file_path,api_count,code
__init__.py,0,"b'""""""tensorflow_examples is a package for examples of TensorFlow.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n'"
setup.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""tensorflow_examples is a package of tensorflow example code.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport os\nimport subprocess\nimport sys\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nnightly = False\nif \'--nightly\' in sys.argv:\n  nightly = True\n  sys.argv.remove(\'--nightly\')\n\nproject_name = \'tensorflow-examples\'\n# Get the current commit hash\nversion = subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\']).decode(\'utf-8\')\n\nif nightly:\n  project_name = \'tensorflow-examples-nightly\'\n  datestring = datetime.datetime.now().strftime(\'%Y%m%d%H%M\')\n  version = \'%s-dev%s\' % (version, datestring)\n\nDOCLINES = __doc__.split(\'\\n\')\n\nREQUIRED_PKGS = [\n    \'absl-py\',\n    \'six\',\n]\n\nTESTS_REQUIRE = [\n    \'jupyter\',\n]\n\nREQUIRMENTS = \'tensorflow_examples/lite/model_maker/requirements.txt\'.replace(\'/\', os.sep)\nwith open(REQUIRMENTS) as f:\n  MODEL_MAKER_REQUIRE = [l.strip() for l in f.read().splitlines() if l.strip()]\n\nif sys.version_info.major == 3:\n  # Packages only for Python 3\n  pass\nelse:\n  # Packages only for Python 2\n  TESTS_REQUIRE.append(\'mock\')\n  REQUIRED_PKGS.append(\'futures\')  # concurrent.futures\n\nif sys.version_info < (3, 4):\n  # enum introduced in Python 3.4\n  REQUIRED_PKGS.append(\'enum34\')\n\nsetup(\n    name=project_name,\n    version=version,\n    description=DOCLINES[0],\n    long_description=\'\\n\'.join(DOCLINES[2:]),\n    author=\'Google Inc.\',\n    author_email=\'packages@tensorflow.org\',\n    url=\'http://github.com/tensorflow/examples\',\n    download_url=\'https://github.com/tensorflow/examples/tags\',\n    license=\'Apache 2.0\',\n    packages=find_packages(),\n    scripts=[],\n    install_requires=REQUIRED_PKGS,\n    extras_require={\n        \'tests\': TESTS_REQUIRE,\n        \'model_maker\': MODEL_MAKER_REQUIRE,\n    },\n    entry_points={\n        \'console_scripts\': [\n            \'model_maker=tensorflow_examples.lite.model_maker.cli.cli:main\',\n        ],\n    },\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    ],\n    keywords=\'tensorflow examples\',\n)\n'"
tensorflow_examples/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_examples/lite/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
tensorflow_examples/models/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Some example models written in TensorFlow 2.0.""""""\n'"
tensorflow_examples/profiling/imagenet_preprocessing_ineffecient_input_pipeline.py,61,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""NOTE!\n\nThis is an unoptimized data input pipeline created by deoptimizing the input\npipeline for ResNet50, created for profiling purposes.\n\n---------------------------------------\nDO NOT USE FOR NON-PROFILING USE CASES\n---------------------------------------\n\nProvides utilities to preprocess images.\n\nTraining images are sampled using the provided bounding boxes, and subsequently\ncropped to the sampled bounding box. Images are additionally flipped randomly,\nthen resized to the target output size (without aspect-ratio preservation).\n\nImages used during evaluation are resized (with aspect-ratio preservation) and\ncentrally cropped.\n\nAll images undergo mean color subtraction.\n\nNote that these steps are colloquially referred to as ""ResNet preprocessing,""\nand they differ from ""VGG preprocessing,"" which does not use bounding boxes\nand instead does an aspect-preserving resize followed by random crop during\ntraining. (These both differ from ""Inception preprocessing,"" which introduces\ncolor distortion steps.)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import logging\nimport tensorflow as tf\n\nDEFAULT_IMAGE_SIZE = 224\nNUM_CHANNELS = 3\nNUM_CLASSES = 1001\n\nNUM_IMAGES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_NUM_TRAIN_FILES = 1024\n_SHUFFLE_BUFFER = 10000\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\nCHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\n\n\ndef process_record_dataset(dataset,\n                           is_training,\n                           batch_size,\n                           shuffle_buffer,\n                           parse_record_fn,\n                           num_epochs=1,\n                           dtype=tf.float32,\n                           datasets_num_private_threads=None,\n                           drop_remainder=False,\n                           tf_data_experimental_slack=False):\n  """"""Given a Dataset with raw records, return an iterator over the records.\n\n  Args:\n    dataset: A Dataset representing raw records\n    is_training: A boolean denoting whether the input is for training.\n    batch_size: The number of samples per batch.\n    shuffle_buffer: The buffer size to use when shuffling records. A larger\n      value results in better randomness, but smaller values reduce startup\n      time and use less memory.\n    parse_record_fn: A function that takes a raw record and returns the\n      corresponding (image, label) pair.\n    num_epochs: The number of epochs to repeat the dataset.\n    dtype: Data type to use for images/features.\n    datasets_num_private_threads: Number of threads for a private\n      threadpool created for all datasets computation.\n    drop_remainder: A boolean indicates whether to drop the remainder of the\n      batches. If True, the batch dimension will be static.\n    tf_data_experimental_slack: Whether to enable tf.data\'s\n      `experimental_slack` option.\n\n  Returns:\n    Dataset of (image, label) pairs ready for iteration.\n  """"""\n  # Defines a specific size thread pool for tf.data operations.\n  if datasets_num_private_threads:\n    options = tf.data.Options()\n    options.experimental_threading.private_threadpool_size = (\n        datasets_num_private_threads)\n    dataset = dataset.with_options(options)\n    logging.info(\n        \'datasets_num_private_threads: %s\', datasets_num_private_threads)\n\n  if is_training:\n    # Shuffles records before repeating to respect epoch boundaries.\n    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n    # Repeats the dataset for the number of epochs to train.\n    dataset = dataset.repeat()\n\n  # Parses the raw records into images and labels.\n\n  # BEGIN_DEOPTIMIZE\n  # Remove data autotuning\n  # dataset = dataset.map(\n  #    lambda value: parse_record_fn(value, is_training, dtype),\n  #    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  # END_DEOPTIMIZE\n\n  dataset = dataset.map(\n      lambda value: parse_record_fn(value, is_training, dtype))\n  dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n\n  # Operations between the final prefetch and the get_next call to the iterator\n  # will happen synchronously during run time. We prefetch here again to\n  # background all of the above processing work and keep it out of the\n  # critical training path. Setting buffer_size to tf.data.experimental.AUTOTUNE\n  # allows DistributionStrategies to adjust how many batches to fetch based\n  # on how many devices are present.\n\n  # BEGIN_DEOPTIMIZE\n  # Remove the prefetch\n  # dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  # END_DEOPTIMIZE\n\n  options = tf.data.Options()\n  options.experimental_slack = tf_data_experimental_slack\n  dataset = dataset.with_options(options)\n\n  return dataset\n\n\ndef get_filenames(is_training, data_dir):\n  """"""Return filenames for dataset.""""""\n  if is_training:\n    return [\n        os.path.join(data_dir, \'train-%05d-of-01024\' % i)\n        for i in range(_NUM_TRAIN_FILES)]\n  else:\n    return [\n        os.path.join(data_dir, \'validation-%05d-of-00128\' % i)\n        for i in range(128)]\n\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields (values are included as examples):\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n      \'image/class/label\': tf.io.FixedLenFeature([], dtype=tf.int64,\n                                                 default_value=-1),\n      \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                                default_value=\'\'),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\n          \'image/object/bbox/xmin\', \'image/object/bbox/ymin\',\n          \'image/object/bbox/xmax\', \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized,\n                                        features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox\n\n\ndef parse_record(raw_record, is_training, dtype):\n  """"""Parses a record containing a training example of an image.\n\n  The input record is parsed into a label and image, and the image is passed\n  through preprocessing steps (cropping, flipping, and so on).\n\n  Args:\n    raw_record: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n    is_training: A boolean denoting whether the input is for training.\n    dtype: data type to use for images/features.\n\n  Returns:\n    Tuple with processed image tensor in a channel-last format and\n    one-hot-encoded label tensor.\n  """"""\n  image_buffer, label, bbox = parse_example_proto(raw_record)\n\n  image = preprocess_image(\n      image_buffer=image_buffer,\n      bbox=bbox,\n      output_height=DEFAULT_IMAGE_SIZE,\n      output_width=DEFAULT_IMAGE_SIZE,\n      num_channels=NUM_CHANNELS,\n      is_training=is_training)\n  image = tf.cast(image, dtype)\n\n  # Subtract one so that labels are in [0, 1000), and cast to float32 for\n  # Keras model.\n  label = tf.cast(tf.cast(tf.reshape(label, shape=[1]), dtype=tf.int32) - 1,\n                  dtype=tf.float32)\n  return image, label\n\n\ndef get_parse_record_fn(use_keras_image_data_format=False):\n  """"""Get a function for parsing the records, accounting for image format.\n\n  This is useful by handling different types of Keras models. For instance,\n  the current resnet_model.resnet50 input format is always channel-last,\n  whereas the keras_applications mobilenet input format depends on\n  tf.keras.backend.image_data_format(). We should set\n  use_keras_image_data_format=False for the former and True for the latter.\n\n  Args:\n    use_keras_image_data_format: A boolean denoting whether data format is keras\n      backend image data format. If False, the image format is channel-last. If\n      True, the image format matches tf.keras.backend.image_data_format().\n\n  Returns:\n    Function to use for parsing the records.\n  """"""\n  def parse_record_fn(raw_record, is_training, dtype):\n    image, label = parse_record(raw_record, is_training, dtype)\n    if use_keras_image_data_format:\n      if tf.keras.backend.image_data_format() == \'channels_first\':\n        image = tf.transpose(image, perm=[2, 0, 1])\n    return image, label\n  return parse_record_fn\n\n\ndef input_fn(is_training,\n             data_dir,\n             batch_size,\n             num_epochs=1,\n             dtype=tf.float32,\n             datasets_num_private_threads=None,\n             parse_record_fn=parse_record,\n             input_context=None,\n             drop_remainder=False,\n             tf_data_experimental_slack=False,\n             training_dataset_cache=False,\n             filenames=None):\n  """"""Input function which provides batches for train or eval.\n\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    data_dir: The directory containing the input data.\n    batch_size: The number of samples per batch.\n    num_epochs: The number of epochs to repeat the dataset.\n    dtype: Data type to use for images/features\n    datasets_num_private_threads: Number of private threads for tf.data.\n    parse_record_fn: Function to use for parsing the records.\n    input_context: A `tf.distribute.InputContext` object passed in by\n      `tf.distribute.Strategy`.\n    drop_remainder: A boolean indicates whether to drop the remainder of the\n      batches. If True, the batch dimension will be static.\n    tf_data_experimental_slack: Whether to enable tf.data\'s\n      `experimental_slack` option.\n    training_dataset_cache: Whether to cache the training dataset on workers.\n       Typically used to improve training performance when training data is in\n       remote storage and can fit into worker memory.\n    filenames: Optional field for providing the file names of the TFRecords.\n\n  Returns:\n    A dataset that can be used for iteration.\n  """"""\n  if filenames is None:\n    filenames = get_filenames(is_training, data_dir)\n  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n\n  if input_context:\n    logging.info(\n        \'Sharding the dataset: input_pipeline_id=%d num_input_pipelines=%d\',\n        input_context.input_pipeline_id, input_context.num_input_pipelines)\n    dataset = dataset.shard(input_context.num_input_pipelines,\n                            input_context.input_pipeline_id)\n\n  if is_training:\n    # Shuffle the input files\n    dataset = dataset.shuffle(buffer_size=_NUM_TRAIN_FILES)\n\n  # Convert to individual records.\n  # cycle_length = 10 means that up to 10 files will be read and deserialized in\n  # parallel. You may want to increase this number if you have a large number of\n  # CPU cores.\n\n  # BEGIN_DEOPTIMIZE\n  # Deoptimization by removing cycle length and data autotining\n  # dataset = dataset.interleave(\n  #    tf.data.TFRecordDataset,\n  #    cycle_length=10,\n  #    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  # END_DEOPTIMIZE\n\n  dataset = dataset.interleave(tf.data.TFRecordDataset)\n\n  if is_training and training_dataset_cache:\n    # Improve training performance when training data is in remote storage and\n    # can fit into worker memory.\n    dataset = dataset.cache()\n\n  return process_record_dataset(\n      dataset=dataset,\n      is_training=is_training,\n      batch_size=batch_size,\n      shuffle_buffer=_SHUFFLE_BUFFER,\n      parse_record_fn=parse_record_fn,\n      num_epochs=num_epochs,\n      dtype=dtype,\n      datasets_num_private_threads=datasets_num_private_threads,\n      drop_remainder=drop_remainder,\n      tf_data_experimental_slack=tf_data_experimental_slack,\n  )\n\n\ndef _decode_crop_and_flip(image_buffer, bbox, num_channels):\n  """"""Crops the given image to a random part of the image, and randomly flips.\n\n  We use the fused decode_and_crop op, which performs better than the two ops\n  used separately in series, but note that this requires that the image be\n  passed in as an un-decoded string Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    num_channels: Integer depth of the image buffer for decoding.\n\n  Returns:\n    3-D tensor with cropped image.\n\n  """"""\n  # A large fraction of image datasets contain a human-annotated bounding box\n  # delineating the region of the image containing the object of interest.  We\n  # choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an\n  # allowed range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n  sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n      tf.image.extract_jpeg_shape(image_buffer),\n      bounding_boxes=bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=[0.75, 1.33],\n      area_range=[0.05, 1.0],\n      max_attempts=100,\n      use_image_if_no_bounding_boxes=True)\n  bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n  # Reassemble the bounding box in the format the crop op requires.\n  offset_y, offset_x, _ = tf.unstack(bbox_begin)\n  target_height, target_width, _ = tf.unstack(bbox_size)\n  crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n  # Use the fused decode and crop op here, which is faster than each in series.\n  cropped = tf.image.decode_and_crop_jpeg(\n      image_buffer, crop_window, channels=num_channels)\n\n  # Flip to add a little more random distortion in.\n  cropped = tf.image.random_flip_left_right(cropped)\n  return cropped\n\n\ndef _central_crop(image, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image: a 3-D image tensor\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    3-D tensor with cropped image.\n  """"""\n  shape = tf.shape(input=image)\n  height, width = shape[0], shape[1]\n\n  amount_to_be_cropped_h = (height - crop_height)\n  crop_top = amount_to_be_cropped_h // 2\n  amount_to_be_cropped_w = (width - crop_width)\n  crop_left = amount_to_be_cropped_w // 2\n  return tf.slice(\n      image, [crop_top, crop_left, 0], [crop_height, crop_width, -1])\n\n\ndef _mean_image_subtraction(image, means, num_channels):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  # Note(b/130245863): we explicitly call `broadcast` instead of simply\n  # expanding dimensions for better performance.\n  means = tf.broadcast_to(means, tf.shape(image))\n\n  return image - means\n\n\ndef _smallest_size_at_least(height, width, resize_min):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: an int32 scalar tensor indicating the new width.\n  """"""\n  resize_min = tf.cast(resize_min, tf.float32)\n\n  # Convert to floats to make subsequent calculations go smoothly.\n  height, width = tf.cast(height, tf.float32), tf.cast(width, tf.float32)\n\n  smaller_dim = tf.minimum(height, width)\n  scale_ratio = resize_min / smaller_dim\n\n  # Convert back to ints to make heights and widths that TF ops will accept.\n  new_height = tf.cast(height * scale_ratio, tf.int32)\n  new_width = tf.cast(width * scale_ratio, tf.int32)\n\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, resize_min):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  shape = tf.shape(input=image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)\n\n\ndef _resize_image(image, height, width):\n  """"""Simple wrapper around tf.resize_images.\n\n  This is primarily to make sure we use the same `ResizeMethod` and other\n  details each time.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    height: The target height for the resized image.\n    width: The target width for the resized image.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image. The first two\n      dimensions have the shape [height, width].\n  """"""\n  return tf.compat.v1.image.resize(\n      image, [height, width], method=tf.image.ResizeMethod.BILINEAR,\n      align_corners=False)\n\n\ndef preprocess_image(image_buffer, bbox, output_height, output_width,\n                     num_channels, is_training=False):\n  """"""Preprocesses the given image.\n\n  Preprocessing includes decoding, cropping, and resizing for both training\n  and eval images. Training preprocessing, however, introduces some random\n  distortion of the image to improve accuracy.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    num_channels: Integer depth of the image buffer for decoding.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    # For training, we want to randomize some of the distortions.\n    image = _decode_crop_and_flip(image_buffer, bbox, num_channels)\n    image = _resize_image(image, output_height, output_width)\n  else:\n    # For validation, we want to decode, resize, then just crop the middle.\n    image = tf.image.decode_jpeg(image_buffer, channels=num_channels)\n    image = _aspect_preserving_resize(image, _RESIZE_MIN)\n    image = _central_crop(image, output_height, output_width)\n\n  image.set_shape([output_height, output_width, num_channels])\n\n  return _mean_image_subtraction(image, CHANNEL_MEANS, num_channels)\n'"
tensorflow_examples/profiling/resnet_model.py,8,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ResNet50 model for Keras.\n\nAdapted from tf.keras.applications.resnet50.ResNet50().\nThis is ResNet model version 1.5.\n\nRelated papers/blogs:\n- https://arxiv.org/abs/1512.03385\n- https://arxiv.org/pdf/1603.05027v2.pdf\n- http://torch.ch/blog/2016/02/04/resnets.html\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import layers as tf_python_keras_layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import regularizers\nfrom tensorflow_examples.profiling import imagenet_preprocessing_ineffecient_input_pipeline\n\nL2_WEIGHT_DECAY = 1e-4\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\nlayers = tf_python_keras_layers\n\n\ndef change_keras_layer(use_tf_keras_layers=False):\n  """"""Change layers to either tf.keras.layers or tf.python.keras.layers.\n\n  Layer version of  tf.keras.layers is depends on tensorflow version, but\n  tf.python.keras.layers checks environment variable TF2_BEHAVIOR.\n  This function is a temporal function to use tf.keras.layers.\n  Currently, tf v2 batchnorm layer is slower than tf v1 batchnorm layer.\n  this function is useful for tracking benchmark result for each version.\n  This function will be removed when we use tf.keras.layers as default.\n\n  TODO(b/146939027): Remove this function when tf v2 batchnorm reaches training\n  speed parity with tf v1 batchnorm.\n\n  Args:\n      use_tf_keras_layers: whether to use tf.keras.layers.\n  """"""\n  global layers\n  if use_tf_keras_layers:\n    layers = tf.keras.layers\n  else:\n    layers = tf_python_keras_layers\n\n\ndef _gen_l2_regularizer(use_l2_regularizer=True):\n  return regularizers.l2(L2_WEIGHT_DECAY) if use_l2_regularizer else None\n\n\ndef identity_block(input_tensor,\n                   kernel_size,\n                   filters,\n                   stage,\n                   block,\n                   use_l2_regularizer=True):\n  """"""The identity block is the block that has no conv layer at shortcut.\n\n  Args:\n    input_tensor: input tensor\n    kernel_size: default 3, the kernel size of middle conv layer at main path\n    filters: list of integers, the filters of 3 conv layer at main path\n    stage: integer, current stage label, used for generating layer names\n    block: \'a\',\'b\'..., current block label, used for generating layer names\n    use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n\n  Returns:\n    Output tensor for the block.\n  """"""\n  filters1, filters2, filters3 = filters\n  if backend.image_data_format() == \'channels_last\':\n    bn_axis = 3\n  else:\n    bn_axis = 1\n  conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n  bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n  x = layers.Conv2D(\n      filters1, (1, 1),\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=conv_name_base + \'2a\')(\n          input_tensor)\n  x = layers.BatchNormalization(\n      axis=bn_axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2a\')(\n          x)\n  x = layers.Activation(\'relu\')(x)\n\n  x = layers.Conv2D(\n      filters2,\n      kernel_size,\n      padding=\'same\',\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=conv_name_base + \'2b\')(\n          x)\n  x = layers.BatchNormalization(\n      axis=bn_axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2b\')(\n          x)\n  x = layers.Activation(\'relu\')(x)\n\n  x = layers.Conv2D(\n      filters3, (1, 1),\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=conv_name_base + \'2c\')(\n          x)\n  x = layers.BatchNormalization(\n      axis=bn_axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2c\')(\n          x)\n\n  x = layers.add([x, input_tensor])\n  x = layers.Activation(\'relu\')(x)\n  return x\n\n\ndef conv_block(input_tensor,\n               kernel_size,\n               filters,\n               stage,\n               block,\n               strides=(2, 2),\n               use_l2_regularizer=True):\n  """"""A block that has a conv layer at shortcut.\n\n  Note that from stage 3,\n  the second conv layer at main path is with strides=(2, 2)\n  And the shortcut should have strides=(2, 2) as well\n\n  Args:\n    input_tensor: input tensor\n    kernel_size: default 3, the kernel size of middle conv layer at main path\n    filters: list of integers, the filters of 3 conv layer at main path\n    stage: integer, current stage label, used for generating layer names\n    block: \'a\',\'b\'..., current block label, used for generating layer names\n    strides: Strides for the second conv layer in the block.\n    use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n\n  Returns:\n    Output tensor for the block.\n  """"""\n  filters1, filters2, filters3 = filters\n  if backend.image_data_format() == \'channels_last\':\n    bn_axis = 3\n  else:\n    bn_axis = 1\n  conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n  bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n  x = layers.Conv2D(\n      filters1, (1, 1),\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=conv_name_base + \'2a\')(\n          input_tensor)\n  x = layers.BatchNormalization(\n      axis=bn_axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2a\')(\n          x)\n  x = layers.Activation(\'relu\')(x)\n\n  x = layers.Conv2D(\n      filters2,\n      kernel_size,\n      strides=strides,\n      padding=\'same\',\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=conv_name_base + \'2b\')(\n          x)\n  x = layers.BatchNormalization(\n      axis=bn_axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2b\')(\n          x)\n  x = layers.Activation(\'relu\')(x)\n\n  x = layers.Conv2D(\n      filters3, (1, 1),\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=conv_name_base + \'2c\')(\n          x)\n  x = layers.BatchNormalization(\n      axis=bn_axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2c\')(\n          x)\n\n  shortcut = layers.Conv2D(\n      filters3, (1, 1),\n      strides=strides,\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=conv_name_base + \'1\')(\n          input_tensor)\n  shortcut = layers.BatchNormalization(\n      axis=bn_axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'1\')(\n          shortcut)\n\n  x = layers.add([x, shortcut])\n  x = layers.Activation(\'relu\')(x)\n  return x\n\n\ndef resnet50(num_classes,\n             batch_size=None,\n             use_l2_regularizer=True,\n             rescale_inputs=False):\n  """"""Instantiates the ResNet50 architecture.\n\n  Args:\n    num_classes: `int` number of classes for image classification.\n    batch_size: Size of the batches for each step.\n    use_l2_regularizer: whether to use L2 regularizer on Conv/Dense layer.\n    rescale_inputs: whether to rescale inputs from 0 to 1.\n\n  Returns:\n      A Keras model instance.\n  """"""\n  input_shape = (224, 224, 3)\n  img_input = layers.Input(shape=input_shape, batch_size=batch_size)\n  if rescale_inputs:\n    # Hub image modules expect inputs in the range [0, 1]. This rescales these\n    # inputs to the range expected by the trained model.\n    x = layers.Lambda(\n        lambda x: x * 255.0 - backend.constant(\n            imagenet_preprocessing_ineffecient_input_pipeline.CHANNEL_MEANS,\n            shape=[1, 1, 3],\n            dtype=x.dtype),\n        name=\'rescale\')(\n            img_input)\n  else:\n    x = img_input\n\n  if backend.image_data_format() == \'channels_first\':\n    x = layers.Lambda(\n        lambda x: backend.permute_dimensions(x, (0, 3, 1, 2)),\n        name=\'transpose\')(x)\n    bn_axis = 1\n  else:  # channels_last\n    bn_axis = 3\n\n  x = layers.ZeroPadding2D(padding=(3, 3), name=\'conv1_pad\')(x)\n  x = layers.Conv2D(\n      64, (7, 7),\n      strides=(2, 2),\n      padding=\'valid\',\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=\'conv1\')(\n          x)\n  x = layers.BatchNormalization(\n      axis=bn_axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=\'bn_conv1\')(\n          x)\n  x = layers.Activation(\'relu\')(x)\n  x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n\n  x = conv_block(\n      x,\n      3, [64, 64, 256],\n      stage=2,\n      block=\'a\',\n      strides=(1, 1),\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [64, 64, 256],\n      stage=2,\n      block=\'b\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [64, 64, 256],\n      stage=2,\n      block=\'c\',\n      use_l2_regularizer=use_l2_regularizer)\n\n  x = conv_block(\n      x,\n      3, [128, 128, 512],\n      stage=3,\n      block=\'a\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [128, 128, 512],\n      stage=3,\n      block=\'b\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [128, 128, 512],\n      stage=3,\n      block=\'c\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [128, 128, 512],\n      stage=3,\n      block=\'d\',\n      use_l2_regularizer=use_l2_regularizer)\n\n  x = conv_block(\n      x,\n      3, [256, 256, 1024],\n      stage=4,\n      block=\'a\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [256, 256, 1024],\n      stage=4,\n      block=\'b\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [256, 256, 1024],\n      stage=4,\n      block=\'c\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [256, 256, 1024],\n      stage=4,\n      block=\'d\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [256, 256, 1024],\n      stage=4,\n      block=\'e\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [256, 256, 1024],\n      stage=4,\n      block=\'f\',\n      use_l2_regularizer=use_l2_regularizer)\n\n  x = conv_block(\n      x,\n      3, [512, 512, 2048],\n      stage=5,\n      block=\'a\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [512, 512, 2048],\n      stage=5,\n      block=\'b\',\n      use_l2_regularizer=use_l2_regularizer)\n  x = identity_block(\n      x,\n      3, [512, 512, 2048],\n      stage=5,\n      block=\'c\',\n      use_l2_regularizer=use_l2_regularizer)\n\n  rm_axes = [1, 2] if backend.image_data_format() == \'channels_last\' else [2, 3]\n  x = layers.Lambda(lambda x: backend.mean(x, rm_axes), name=\'reduce_mean\')(x)\n  x = layers.Dense(\n      num_classes,\n      kernel_initializer=initializers.RandomNormal(stddev=0.01),\n      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      bias_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n      name=\'fc1000\')(\n          x)\n\n  # A softmax that is followed by the model loss must be done cannot be done\n  # in float16 due to numeric issues. So we pass dtype=float32.\n  x = layers.Activation(\'softmax\', dtype=\'float32\')(x)\n\n  # Create model.\n  return models.Model(img_input, x, name=\'resnet50\')\n'"
tensorflow_examples/lite/model_maker/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
tensorflow_examples/models/dcgan/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_examples/models/dcgan/dcgan.py,34,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""DCGAN.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_integer(\'buffer_size\', 10000, \'Shuffle buffer size\')\nflags.DEFINE_integer(\'batch_size\', 64, \'Batch Size\')\nflags.DEFINE_integer(\'epochs\', 1, \'Number of epochs\')\nflags.DEFINE_boolean(\'enable_function\', True, \'Enable Function?\')\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n\ndef scale(image, label):\n  image = tf.cast(image, tf.float32)\n  image = (image - 127.5) / 127.5\n\n  return image, label\n\n\ndef create_dataset(buffer_size, batch_size):\n  train_dataset = tfds.load(\n      \'mnist\', split=\'train\', as_supervised=True, shuffle_files=True)\n  train_dataset = train_dataset.map(scale, num_parallel_calls=AUTOTUNE)\n  train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n  return train_dataset\n\n\ndef make_generator_model():\n  """"""Generator.\n\n  Returns:\n    Keras Sequential model\n  """"""\n  model = tf.keras.Sequential([\n      tf.keras.layers.Dense(7*7*256, use_bias=False),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.LeakyReLU(),\n      tf.keras.layers.Reshape((7, 7, 256)),\n      tf.keras.layers.Conv2DTranspose(128, 5, strides=(1, 1),\n                                      padding=\'same\', use_bias=False),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.LeakyReLU(),\n      tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2),\n                                      padding=\'same\', use_bias=False),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.LeakyReLU(),\n      tf.keras.layers.Conv2DTranspose(1, 5, strides=(2, 2),\n                                      padding=\'same\', use_bias=False,\n                                      activation=\'tanh\')\n  ])\n\n  return model\n\n\ndef make_discriminator_model():\n  """"""Discriminator.\n\n  Returns:\n    Keras Sequential model\n  """"""\n  model = tf.keras.Sequential([\n      tf.keras.layers.Conv2D(64, 5, strides=(2, 2), padding=\'same\'),\n      tf.keras.layers.LeakyReLU(),\n      tf.keras.layers.Dropout(0.3),\n      tf.keras.layers.Conv2D(128, 5, strides=(2, 2), padding=\'same\'),\n      tf.keras.layers.LeakyReLU(),\n      tf.keras.layers.Dropout(0.3),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(1)\n  ])\n\n  return model\n\n\ndef get_checkpoint_prefix():\n  checkpoint_dir = \'./training_checkpoints\'\n  checkpoint_prefix = os.path.join(checkpoint_dir, \'ckpt\')\n\n  return checkpoint_prefix\n\n\nclass Dcgan(object):\n  """"""Dcgan class.\n\n  Args:\n    epochs: Number of epochs.\n    enable_function: If true, train step is decorated with tf.function.\n    batch_size: Batch size.\n  """"""\n\n  def __init__(self, epochs, enable_function, batch_size):\n    self.epochs = epochs\n    self.enable_function = enable_function\n    self.batch_size = batch_size\n    self.noise_dim = 100\n    self.loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n    self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n    self.generator = make_generator_model()\n    self.discriminator = make_discriminator_model()\n    self.checkpoint = tf.train.Checkpoint(\n        generator_optimizer=self.generator_optimizer,\n        discriminator_optimizer=self.discriminator_optimizer,\n        generator=self.generator,\n        discriminator=self.discriminator)\n\n  def generator_loss(self, generated_output):\n    return self.loss_object(tf.ones_like(generated_output), generated_output)\n\n  def discriminator_loss(self, real_output, generated_output):\n    real_loss = self.loss_object(tf.ones_like(real_output), real_output)\n    generated_loss = self.loss_object(\n        tf.zeros_like(generated_output), generated_output)\n\n    total_loss = real_loss + generated_loss\n\n    return total_loss\n\n  def train_step(self, image):\n    """"""One train step over the generator and discriminator model.\n\n    Args:\n      image: Input image.\n\n    Returns:\n      generator loss, discriminator loss.\n    """"""\n    noise = tf.random.normal([self.batch_size, self.noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = self.generator(noise, training=True)\n\n      real_output = self.discriminator(image, training=True)\n      generated_output = self.discriminator(generated_images, training=True)\n\n      gen_loss = self.generator_loss(generated_output)\n      disc_loss = self.discriminator_loss(real_output, generated_output)\n\n    gradients_of_generator = gen_tape.gradient(\n        gen_loss, self.generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(\n        disc_loss, self.discriminator.trainable_variables)\n\n    self.generator_optimizer.apply_gradients(zip(\n        gradients_of_generator, self.generator.trainable_variables))\n    self.discriminator_optimizer.apply_gradients(zip(\n        gradients_of_discriminator, self.discriminator.trainable_variables))\n\n    return gen_loss, disc_loss\n\n  def train(self, dataset, checkpoint_pr):\n    """"""Train the GAN for x number of epochs.\n\n    Args:\n      dataset: train dataset.\n      checkpoint_pr: prefix in which the checkpoints are stored.\n\n    Returns:\n      Time for each epoch.\n    """"""\n    time_list = []\n    if self.enable_function:\n      self.train_step = tf.function(self.train_step)\n\n    for epoch in range(self.epochs):\n      start_time = time.time()\n      for image, _ in dataset:\n        gen_loss, disc_loss = self.train_step(image)\n\n      wall_time_sec = time.time() - start_time\n      time_list.append(wall_time_sec)\n\n      # saving (checkpoint) the model every 15 epochs\n      if (epoch + 1) % 15 == 0:\n        self.checkpoint.save(file_prefix=checkpoint_pr)\n\n      template = \'Epoch {}, Generator loss {}, Discriminator Loss {}\'\n      print (template.format(epoch, gen_loss, disc_loss))\n\n    return time_list\n\n\ndef run_main(argv):\n  del argv\n  kwargs = {\'epochs\': FLAGS.epochs, \'enable_function\': FLAGS.enable_function,\n            \'buffer_size\': FLAGS.buffer_size, \'batch_size\': FLAGS.batch_size}\n  main(**kwargs)\n\n\ndef main(epochs, enable_function, buffer_size, batch_size):\n  train_dataset = create_dataset(buffer_size, batch_size)\n  checkpoint_pr = get_checkpoint_prefix()\n\n  dcgan_obj = Dcgan(epochs, enable_function, batch_size)\n  print (\'Training ...\')\n  return dcgan_obj.train(train_dataset, checkpoint_pr)\n\nif __name__ == \'__main__\':\n  app.run(run_main)\n'"
tensorflow_examples/models/dcgan/dcgan_test.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""DCGAN tests.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\nfrom tensorflow_examples.models.dcgan import dcgan\n\nFLAGS = flags.FLAGS\n\n\nclass DcganTest(tf.test.TestCase):\n\n  def test_one_epoch_with_function(self):\n    epochs = 1\n    batch_size = 1\n    enable_function = True\n\n    input_image = tf.random.uniform((28, 28, 1))\n    label = tf.zeros((1,))\n    train_dataset = tf.data.Dataset.from_tensors(\n        (input_image, label)).batch(batch_size)\n    checkpoint_pr = dcgan.get_checkpoint_prefix()\n\n    dcgan_obj = dcgan.Dcgan(epochs, enable_function, batch_size)\n    dcgan_obj.train(train_dataset, checkpoint_pr)\n\n  def test_one_epoch_without_function(self):\n    epochs = 1\n    batch_size = 1\n    enable_function = False\n\n    input_image = tf.random.uniform((28, 28, 1))\n    label = tf.zeros((1,))\n    train_dataset = tf.data.Dataset.from_tensors(\n        (input_image, label)).batch(batch_size)\n    checkpoint_pr = dcgan.get_checkpoint_prefix()\n\n    dcgan_obj = dcgan.Dcgan(epochs, enable_function, batch_size)\n    dcgan_obj.train(train_dataset, checkpoint_pr)\n\n\nclass DCGANBenchmark(tf.test.Benchmark):\n\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n\n  def benchmark_with_function(self):\n    kwargs = {""epochs"": 6, ""enable_function"": True,\n              ""buffer_size"": 10000, ""batch_size"": 64}\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_without_function(self):\n    kwargs = {""epochs"": 6, ""enable_function"": False,\n              ""buffer_size"": 10000, ""batch_size"": 64}\n    self._run_and_report_benchmark(**kwargs)\n\n  def _run_and_report_benchmark(self, **kwargs):\n    time_list = dcgan.main(**kwargs)\n    # 1st epoch is the warmup epoch hence skipping it for calculating time.\n    self.report_benchmark(wall_time=tf.reduce_mean(time_list[1:]))\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_examples/models/densenet/__init__.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_examples/models/densenet/densenet.py,25,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Densely Connected Convolutional Networks.\n\nReference [\nDensely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nl2 = tf.keras.regularizers.l2\n\n\ndef calc_from_depth(depth, num_blocks, bottleneck):\n  """"""Calculate number of layers in each block from the depth.\n\n  Args:\n    depth: Depth of model\n    num_blocks: Number of dense blocks\n    bottleneck: If True, num_layers will be halved\n\n  Returns:\n    Number of layers in each block as a list\n\n  Raises:\n    ValueError: If depth or num_blocks is None and num_blocks is not 3.\n  """"""\n\n  if depth is None or num_blocks is None:\n    raise ValueError(""For \'from_depth\' mode, you need to specify the depth ""\n                     ""and number of blocks."")\n\n  if num_blocks != 3:\n    raise ValueError(\n        ""Number of blocks must be 3 if mode is \'from_depth\'."")\n\n  if (depth - 4) % 3 == 0:\n    num_layers = (depth - 4) / 3\n    if bottleneck:\n      num_layers //= 2\n    return [num_layers] * num_blocks\n  else:\n    raise ValueError(""Depth must be 3N+4 if mode is \'from_depth\'."")\n\n\ndef calc_from_list(depth, num_blocks, layers_per_block):\n  """"""Calculate number of layers in each block.\n\n  Args:\n    depth: Depth of model\n    num_blocks: Number of dense blocks\n    layers_per_block: Number of layers per block as a list or tuple\n\n  Returns:\n    Number of layers in each block as a list\n\n  Raises:\n    ValueError: If depth or num_blocks is not None and\n                layers_per_block is None or not a list or tuple\n  """"""\n  if depth is not None or num_blocks is not None:\n    raise ValueError(""You don\'t have to specify the depth and number of ""\n                     ""blocks when mode is \'from_list\'"")\n\n  if layers_per_block is None or not isinstance(\n      layers_per_block, list) or not isinstance(layers_per_block, tuple):\n    raise ValueError(""You must pass list or tuple when using \'from_list\' mode."")\n\n  if isinstance(layers_per_block, list) or isinstance(layers_per_block, tuple):\n    return list(layers_per_block)\n\n\ndef calc_from_integer(depth, num_blocks, layers_per_block):\n  """"""Calculate number of layers in each block.\n\n  Args:\n    depth: Depth of model\n    num_blocks: Number of dense blocks\n    layers_per_block: Number of layers per block as an integer.\n\n  Returns:\n    Number of layers in each block as a list\n\n  Raises:\n    ValueError: If depth is not None and\n                num_blocks is None or layer_per_block is not an integer.\n  """"""\n  if depth is not None:\n    raise ValueError(""You don\'t have to specify the depth ""\n                     ""when mode is \'from_integer\'"")\n\n  if num_blocks is None or not isinstance(layers_per_block, int):\n    raise ValueError(""You must pass number of blocks or an integer to ""\n                     ""layers in each block"")\n\n  return [layers_per_block] * num_blocks\n\n\nclass ConvBlock(tf.keras.Model):\n  """"""Convolutional Block consisting of (batchnorm->relu->conv).\n\n  Arguments:\n    num_filters: number of filters passed to a convolutional layer.\n    data_format: ""channels_first"" or ""channels_last""\n    bottleneck: if True, then a 1x1 Conv is performed followed by 3x3 Conv.\n    weight_decay: weight decay\n    dropout_rate: dropout rate.\n  """"""\n\n  def __init__(self, num_filters, data_format, bottleneck, weight_decay=1e-4,\n               dropout_rate=0):\n    super(ConvBlock, self).__init__()\n    self.bottleneck = bottleneck\n\n    axis = -1 if data_format == ""channels_last"" else 1\n    inter_filter = num_filters * 4\n    # don\'t forget to set use_bias=False when using batchnorm\n    self.conv2 = tf.keras.layers.Conv2D(num_filters,\n                                        (3, 3),\n                                        padding=""same"",\n                                        use_bias=False,\n                                        data_format=data_format,\n                                        kernel_initializer=""he_normal"",\n                                        kernel_regularizer=l2(weight_decay))\n    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis=axis)\n    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    if self.bottleneck:\n      self.conv1 = tf.keras.layers.Conv2D(inter_filter,\n                                          (1, 1),\n                                          padding=""same"",\n                                          use_bias=False,\n                                          data_format=data_format,\n                                          kernel_initializer=""he_normal"",\n                                          kernel_regularizer=l2(weight_decay))\n      self.batchnorm2 = tf.keras.layers.BatchNormalization(axis=axis)\n\n  def call(self, x, training=True):\n    output = self.batchnorm1(x, training=training)\n\n    if self.bottleneck:\n      output = self.conv1(tf.nn.relu(output))\n      output = self.batchnorm2(output, training=training)\n\n    output = self.conv2(tf.nn.relu(output))\n    output = self.dropout(output, training=training)\n\n    return output\n\n\nclass TransitionBlock(tf.keras.Model):\n  """"""Transition Block to reduce the number of features.\n\n  Arguments:\n    num_filters: number of filters passed to a convolutional layer.\n    data_format: ""channels_first"" or ""channels_last""\n    weight_decay: weight decay\n    dropout_rate: dropout rate.\n  """"""\n\n  def __init__(self, num_filters, data_format,\n               weight_decay=1e-4, dropout_rate=0):\n    super(TransitionBlock, self).__init__()\n    axis = -1 if data_format == ""channels_last"" else 1\n\n    self.batchnorm = tf.keras.layers.BatchNormalization(axis=axis)\n    self.conv = tf.keras.layers.Conv2D(num_filters,\n                                       (1, 1),\n                                       padding=""same"",\n                                       use_bias=False,\n                                       data_format=data_format,\n                                       kernel_initializer=""he_normal"",\n                                       kernel_regularizer=l2(weight_decay))\n    self.avg_pool = tf.keras.layers.AveragePooling2D(data_format=data_format)\n\n  def call(self, x, training=True):\n    output = self.batchnorm(x, training=training)\n    output = self.conv(tf.nn.relu(output))\n    output = self.avg_pool(output)\n    return output\n\n\nclass DenseBlock(tf.keras.Model):\n  """"""Dense Block.\n\n  It consists of ConvBlocks where each block\'s output is concatenated\n  with its input.\n\n  Arguments:\n    num_layers: Number of layers in each block.\n    growth_rate: number of filters to add per conv block.\n    data_format: ""channels_first"" or ""channels_last""\n    bottleneck: boolean, that decides which part of ConvBlock to call.\n    weight_decay: weight decay\n    dropout_rate: dropout rate.\n  """"""\n\n  def __init__(self, num_layers, growth_rate, data_format, bottleneck,\n               weight_decay=1e-4, dropout_rate=0):\n    super(DenseBlock, self).__init__()\n    self.num_layers = num_layers\n    self.axis = -1 if data_format == ""channels_last"" else 1\n\n    self.blocks = []\n    for _ in range(int(self.num_layers)):\n      self.blocks.append(ConvBlock(growth_rate,\n                                   data_format,\n                                   bottleneck,\n                                   weight_decay,\n                                   dropout_rate))\n\n  def call(self, x, training=True):\n    for i in range(int(self.num_layers)):\n      output = self.blocks[i](x, training=training)\n      x = tf.concat([x, output], axis=self.axis)\n\n    return x\n\n\nclass DenseNet(tf.keras.Model):\n  """"""Creating the Densenet Architecture.\n\n  Arguments:\n    mode: mode could be:\n        - from_depth: num_layers_in_each_block will be calculated from the depth\n                      and number of blocks.\n        - from_list: pass num_layers_in_each_block as a list. depth and number\n                     of blocks should not be specified\n        - from_integer: pass num_layers_in_each_block as an integer. Number of\n                        layers in each block will be calculated using\n                        num_of_blocks * num_layers_in_each_block\n    depth_of_model: number of layers in the model.\n    growth_rate: number of filters to add per conv block.\n    num_of_blocks: number of dense blocks.\n    output_classes: number of output classes.\n    num_layers_in_each_block: number of layers in each block.\n                              If -1, then we calculate this by (depth-3)/4.\n                              If positive integer, then it is used as the\n                                number of layers per block.\n                              If list or tuple, then this list is used directly.\n    data_format: ""channels_first"" or ""channels_last""\n    bottleneck: boolean, to decide which part of conv block to call.\n    compression: reducing the number of inputs(filters) to the transition block.\n    weight_decay: weight decay\n    rate: dropout rate.\n    pool_initial: If True add a 7x7 conv with stride 2 followed by 3x3 maxpool\n                  else, do a 3x3 conv with stride 1.\n    include_top: If true, GlobalAveragePooling Layer and Dense layer are\n                 included.\n  """"""\n\n  def __init__(self, mode, growth_rate, output_classes, depth_of_model=None,\n               num_of_blocks=None, num_layers_in_each_block=None,\n               data_format=""channels_last"", bottleneck=True, compression=0.5,\n               weight_decay=1e-4, dropout_rate=0., pool_initial=False,\n               include_top=True):\n    super(DenseNet, self).__init__()\n    self.mode = mode\n    self.depth_of_model = depth_of_model\n    self.growth_rate = growth_rate\n    self.num_of_blocks = num_of_blocks\n    self.output_classes = output_classes\n    self.num_layers_in_each_block = num_layers_in_each_block\n    self.data_format = data_format\n    self.bottleneck = bottleneck\n    self.compression = compression\n    self.weight_decay = weight_decay\n    self.dropout_rate = dropout_rate\n    self.pool_initial = pool_initial\n    self.include_top = include_top\n\n    # deciding number of layers in each block\n    if mode == ""from_depth"":\n      self.num_layers_in_each_block = calc_from_depth(\n          self.depth_of_model, self.num_of_blocks, self.bottleneck)\n    elif mode == ""from_list"":\n      self.num_layers_in_each_block = calc_from_list(\n          self.depth_of_model, self.num_of_blocks,\n          self.num_layers_in_each_block)\n    elif mode == ""from_integer"":\n      self.num_layers_in_each_block = calc_from_integer(\n          self.depth_of_model, self.num_of_blocks,\n          self.num_layers_in_each_block)\n\n    axis = -1 if self.data_format == ""channels_last"" else 1\n\n    # setting the filters and stride of the initial conv layer.\n    if self.pool_initial:\n      init_filters = (7, 7)\n      stride = (2, 2)\n    else:\n      init_filters = (3, 3)\n      stride = (1, 1)\n\n    self.num_filters = 2 * self.growth_rate\n\n    # first conv and pool layer\n    self.conv1 = tf.keras.layers.Conv2D(self.num_filters,\n                                        init_filters,\n                                        strides=stride,\n                                        padding=""same"",\n                                        use_bias=False,\n                                        data_format=self.data_format,\n                                        kernel_initializer=""he_normal"",\n                                        kernel_regularizer=l2(\n                                            self.weight_decay))\n    if self.pool_initial:\n      self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3),\n                                                strides=(2, 2),\n                                                padding=""same"",\n                                                data_format=self.data_format)\n      self.batchnorm1 = tf.keras.layers.BatchNormalization(axis=axis)\n\n    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis=axis)\n\n    # calculating the number of filters after each block\n    num_filters_after_each_block = [self.num_filters]\n    for i in range(1, self.num_of_blocks):\n      temp_num_filters = num_filters_after_each_block[i-1] + (\n          self.growth_rate * self.num_layers_in_each_block[i-1])\n      # using compression to reduce the number of inputs to the\n      # transition block\n      temp_num_filters = int(temp_num_filters * compression)\n      num_filters_after_each_block.append(temp_num_filters)\n\n    # dense block initialization\n    self.dense_blocks = []\n    self.transition_blocks = []\n    for i in range(self.num_of_blocks):\n      self.dense_blocks.append(DenseBlock(self.num_layers_in_each_block[i],\n                                          self.growth_rate,\n                                          self.data_format,\n                                          self.bottleneck,\n                                          self.weight_decay,\n                                          self.dropout_rate))\n      if i+1 < self.num_of_blocks:\n        self.transition_blocks.append(\n            TransitionBlock(num_filters_after_each_block[i+1],\n                            self.data_format,\n                            self.weight_decay,\n                            self.dropout_rate))\n\n    # last pooling and fc layer\n    if self.include_top:\n      self.last_pool = tf.keras.layers.GlobalAveragePooling2D(\n          data_format=self.data_format)\n      self.classifier = tf.keras.layers.Dense(self.output_classes)\n\n  def call(self, x, training=True):\n    output = self.conv1(x)\n\n    if self.pool_initial:\n      output = self.batchnorm1(output, training=training)\n      output = tf.nn.relu(output)\n      output = self.pool1(output)\n\n    for i in range(self.num_of_blocks - 1):\n      output = self.dense_blocks[i](output, training=training)\n      output = self.transition_blocks[i](output, training=training)\n\n    output = self.dense_blocks[\n        self.num_of_blocks - 1](output, training=training)\n    output = self.batchnorm2(output, training=training)\n    output = tf.nn.relu(output)\n\n    if self.include_top:\n      output = self.last_pool(output)\n      output = self.classifier(output)\n\n    return output\n'"
tensorflow_examples/models/densenet/densenet_distributed_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Densely Connected Convolutional Networks.\n\nReference [\nDensely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport tensorflow as tf\nfrom tensorflow_examples.models.densenet import distributed_train\nfrom tensorflow_examples.models.densenet import utils\n\n\nclass DenseNetDistributedBenchmark(tf.test.Benchmark):\n\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n\n  def benchmark_with_function_custom_loops(self):\n    kwargs = utils.get_cifar10_kwargs()\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_with_function_custom_loops_300_epochs_2_gpus(self):\n    kwargs = utils.get_cifar10_kwargs()\n    kwargs.update({\'epochs\': 300, \'data_format\': \'channels_first\',\n                   \'bottleneck\': False, \'compression\': 1., \'num_gpu\': 2,\n                   \'batch_size\': 128})\n\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_with_function_custom_loops_300_epochs_8_gpus(self):\n    kwargs = utils.get_cifar10_kwargs()\n    kwargs.update({\'epochs\': 300, \'data_format\': \'channels_first\',\n                   \'bottleneck\': False, \'compression\': 1., \'num_gpu\': 8,\n                   \'batch_size\': 512})\n\n    self._run_and_report_benchmark(**kwargs)\n\n  def _run_and_report_benchmark(self, top_1_min=.944, top_1_max=.949, **kwargs):\n    """"""Run the benchmark and report metrics.report.\n\n    Args:\n      top_1_min: Min value for top_1 accuracy.  Default range is SOTA.\n      top_1_max: Max value for top_1 accuracy.\n      **kwargs: All args passed to the test.\n    """"""\n    start_time_sec = time.time()\n    train_loss, train_acc, _, test_acc = distributed_train.main(**kwargs)\n    wall_time_sec = time.time() - start_time_sec\n\n    metrics = []\n    metrics.append({\'name\': \'accuracy_top_1\',\n                    \'value\': test_acc,\n                    \'min_value\': top_1_min,\n                    \'max_value\': top_1_max})\n\n    metrics.append({\'name\': \'training_accuracy_top_1\',\n                    \'value\': train_acc})\n\n    metrics.append({\'name\': \'train_loss\',\n                    \'value\': train_loss})\n\n    self.report_benchmark(wall_time=wall_time_sec, metrics=metrics)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_examples/models/densenet/densenet_test.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Densely Connected Convolutional Networks.\n\nReference [\nDensely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport tensorflow as tf\nfrom tensorflow_examples.models.densenet import densenet\nfrom tensorflow_examples.models.densenet import train\nfrom tensorflow_examples.models.densenet import utils\n\n\ndef create_sample_dataset(batch_size):\n  input_image = tf.random.uniform((32, 32, 3))\n  label = tf.zeros((1,))\n  dataset = tf.data.Dataset.from_tensors(\n      (input_image, label)).batch(batch_size)\n  return dataset\n\n\nclass DensenetTest(tf.test.TestCase):\n\n  def test_one_epoch_with_function_custom_loop(self):\n    epochs = 1\n    enable_function = True\n    depth_of_model = 7\n    growth_rate = 2\n    num_of_blocks = 3\n    output_classes = 10\n    mode = \'from_depth\'\n    data_format = \'channels_last\'\n\n    train_dataset = create_sample_dataset(batch_size=1)\n    test_dataset = create_sample_dataset(batch_size=1)\n\n    model = densenet.DenseNet(\n        mode, growth_rate, output_classes, depth_of_model, num_of_blocks,\n        data_format)\n    train_obj = train.Train(epochs, enable_function, model)\n    train_obj.custom_loop(train_dataset, test_dataset)\n\n  def test_one_epoch_with_keras_fit(self):\n    epochs = 1\n    enable_function = True\n    depth_of_model = 7\n    growth_rate = 2\n    num_of_blocks = 3\n    output_classes = 10\n    mode = \'from_depth\'\n    data_format = \'channels_last\'\n\n    train_dataset = create_sample_dataset(batch_size=1)\n    test_dataset = create_sample_dataset(batch_size=1)\n\n    model = densenet.DenseNet(\n        mode, growth_rate, output_classes, depth_of_model, num_of_blocks,\n        data_format)\n    train_obj = train.Train(epochs, enable_function, model)\n    train_obj.keras_fit(train_dataset, test_dataset)\n\n\nclass DenseNetBenchmark(tf.test.Benchmark):\n\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n\n  def benchmark_with_function_custom_loops(self):\n    kwargs = utils.get_cifar10_kwargs()\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_without_function_custom_loops(self):\n    kwargs = utils.get_cifar10_kwargs()\n    kwargs.update({\'enable_function\': False})\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_with_keras_fit(self):\n    kwargs = utils.get_cifar10_kwargs()\n    kwargs.update({\'train_mode\': \'keras_fit\'})\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_with_function_custom_loops_300_epochs(self):\n    kwargs = utils.get_cifar10_kwargs()\n    kwargs.update({\'epochs\': 300, \'data_format\': \'channels_first\',\n                   \'bottleneck\': False, \'compression\': 1.})\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_with_keras_fit_300_epochs(self):\n    kwargs = utils.get_cifar10_kwargs()\n    kwargs.update({\'epochs\': 300, \'data_format\': \'channels_first\',\n                   \'train_mode\': \'keras_fit\', \'bottleneck\': False,\n                   \'compression\': 1.})\n    self._run_and_report_benchmark(**kwargs)\n\n  def _run_and_report_benchmark(self, **kwargs):\n    """"""Run the benchmark and report metrics.report.\n\n    Args:\n      **kwargs: All args passed to the test.\n    """"""\n    start_time_sec = time.time()\n    train_loss, train_acc, _, test_acc = train.main(**kwargs)\n    wall_time_sec = time.time() - start_time_sec\n\n    metrics = []\n    metrics.append({\'name\': \'accuracy_top_1\',\n                    \'value\': test_acc,\n                    \'min_value\': .944,\n                    \'max_value\': .949})\n\n    metrics.append({\'name\': \'training_accuracy_top_1\',\n                    \'value\': train_acc})\n\n    metrics.append({\'name\': \'train_loss\',\n                    \'value\': train_loss})\n\n    self.report_benchmark(wall_time=wall_time_sec, metrics=metrics)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_examples/models/densenet/distributed_train.py,14,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Densenet Training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nimport tensorflow as tf\nfrom tensorflow_examples.models.densenet import densenet\nfrom tensorflow_examples.models.densenet import utils\n\nFLAGS = flags.FLAGS\n\n# if additional flags are needed, define it here.\nflags.DEFINE_integer(\'num_gpu\', 1, \'Number of GPUs to use\')\n\n\nclass Train(object):\n  """"""Train class.\n\n  Args:\n    epochs: Number of epochs\n    enable_function: If True, wraps the train_step and test_step in tf.function\n    model: Densenet model.\n    batch_size: Batch size.\n    strategy: Distribution strategy in use.\n  """"""\n\n  def __init__(self, epochs, enable_function, model, batch_size, strategy):\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.enable_function = enable_function\n    self.strategy = strategy\n    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n    self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.1,\n                                             momentum=0.9, nesterov=True)\n    self.train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(\n        name=\'train_accuracy\')\n    self.test_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(\n        name=\'test_accuracy\')\n    self.test_loss_metric = tf.keras.metrics.Sum(name=\'test_loss\')\n    self.model = model\n\n  def decay(self, epoch):\n    if epoch < 150:\n      return 0.1\n    if epoch >= 150 and epoch < 225:\n      return 0.01\n    if epoch >= 225:\n      return 0.001\n\n  def compute_loss(self, label, predictions):\n    loss = tf.reduce_sum(self.loss_object(label, predictions)) * (\n        1. / self.batch_size)\n    loss += (sum(self.model.losses) * 1. / self.strategy.num_replicas_in_sync)\n    return loss\n\n  def train_step(self, inputs):\n    """"""One train step.\n\n    Args:\n      inputs: one batch input.\n\n    Returns:\n      loss: Scaled loss.\n    """"""\n\n    image, label = inputs\n    with tf.GradientTape() as tape:\n      predictions = self.model(image, training=True)\n      loss = self.compute_loss(label, predictions)\n    gradients = tape.gradient(loss, self.model.trainable_variables)\n    self.optimizer.apply_gradients(zip(gradients,\n                                       self.model.trainable_variables))\n\n    self.train_acc_metric(label, predictions)\n    return loss\n\n  def test_step(self, inputs):\n    """"""One test step.\n\n    Args:\n      inputs: one batch input.\n    """"""\n    image, label = inputs\n    predictions = self.model(image, training=False)\n\n    unscaled_test_loss = self.loss_object(label, predictions) + sum(\n        self.model.losses)\n\n    self.test_acc_metric(label, predictions)\n    self.test_loss_metric(unscaled_test_loss)\n\n  def custom_loop(self, train_dist_dataset, test_dist_dataset,\n                  strategy):\n    """"""Custom training and testing loop.\n\n    Args:\n      train_dist_dataset: Training dataset created using strategy.\n      test_dist_dataset: Testing dataset created using strategy.\n      strategy: Distribution strategy.\n\n    Returns:\n      train_loss, train_accuracy, test_loss, test_accuracy\n    """"""\n\n    def distributed_train_epoch(ds):\n      total_loss = 0.0\n      num_train_batches = 0.0\n      for one_batch in ds:\n        per_replica_loss = strategy.run(self.train_step, args=(one_batch,))\n        total_loss += strategy.reduce(\n            tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n        num_train_batches += 1\n      return total_loss, num_train_batches\n\n    def distributed_test_epoch(ds):\n      num_test_batches = 0.0\n      for one_batch in ds:\n        strategy.run(self.test_step, args=(one_batch,))\n        num_test_batches += 1\n      return self.test_loss_metric.result(), num_test_batches\n\n    if self.enable_function:\n      distributed_train_epoch = tf.function(distributed_train_epoch)\n      distributed_test_epoch = tf.function(distributed_test_epoch)\n\n    for epoch in range(self.epochs):\n      self.optimizer.learning_rate = self.decay(epoch)\n\n      train_total_loss, num_train_batches = distributed_train_epoch(\n          train_dist_dataset)\n      test_total_loss, num_test_batches = distributed_test_epoch(\n          test_dist_dataset)\n\n      template = (\'Epoch: {}, Train Loss: {}, Train Accuracy: {}, \'\n                  \'Test Loss: {}, Test Accuracy: {}\')\n\n      print(\n          template.format(epoch,\n                          train_total_loss / num_train_batches,\n                          self.train_acc_metric.result(),\n                          test_total_loss / num_test_batches,\n                          self.test_acc_metric.result()))\n\n      if epoch != self.epochs - 1:\n        self.train_acc_metric.reset_states()\n        self.test_acc_metric.reset_states()\n\n    return (train_total_loss / num_train_batches,\n            self.train_acc_metric.result().numpy(),\n            test_total_loss / num_test_batches,\n            self.test_acc_metric.result().numpy())\n\n\ndef run_main(argv):\n  """"""Passes the flags to main.\n\n  Args:\n    argv: argv\n  """"""\n  del argv\n  kwargs = utils.flags_dict()\n  kwargs.update({\'num_gpu\': FLAGS.num_gpu})\n  main(**kwargs)\n\n\ndef main(epochs,\n         enable_function,\n         buffer_size,\n         batch_size,\n         mode,\n         growth_rate,\n         output_classes,\n         depth_of_model=None,\n         num_of_blocks=None,\n         num_layers_in_each_block=None,\n         data_format=\'channels_last\',\n         bottleneck=True,\n         compression=0.5,\n         weight_decay=1e-4,\n         dropout_rate=0.,\n         pool_initial=False,\n         include_top=True,\n         train_mode=\'custom_loop\',\n         data_dir=None,\n         num_gpu=1):\n\n  devices = [\'/device:GPU:{}\'.format(i) for i in range(num_gpu)]\n  strategy = tf.distribute.MirroredStrategy(devices)\n\n  train_dataset, test_dataset, _ = utils.create_dataset(\n      buffer_size, batch_size, data_format, data_dir)\n\n  with strategy.scope():\n    model = densenet.DenseNet(\n        mode, growth_rate, output_classes, depth_of_model, num_of_blocks,\n        num_layers_in_each_block, data_format, bottleneck, compression,\n        weight_decay, dropout_rate, pool_initial, include_top)\n\n    trainer = Train(epochs, enable_function, model, batch_size, strategy)\n\n    train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n    test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n\n    print(\'Training...\')\n    if train_mode == \'custom_loop\':\n      return trainer.custom_loop(train_dist_dataset,\n                                 test_dist_dataset,\n                                 strategy)\n    elif train_mode == \'keras_fit\':\n      raise ValueError(\n          \'`tf.distribute.Strategy` does not support subclassed models yet.\')\n    else:\n      raise ValueError(\n          \'Please enter either ""keras_fit"" or ""custom_loop"" as the argument.\')\n\n\nif __name__ == \'__main__\':\n  utils.define_densenet_flags()\n  app.run(run_main)\n'"
tensorflow_examples/models/densenet/train.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Densenet Training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nimport tensorflow as tf\nfrom tensorflow_examples.models.densenet import densenet\nfrom tensorflow_examples.models.densenet import utils\n\n\nclass Train(object):\n  """"""Train class.\n\n  Args:\n    epochs: Number of epochs\n    enable_function: If True, wraps the train_step and test_step in tf.function\n    model: Densenet model.\n  """"""\n\n  def __init__(self, epochs, enable_function, model):\n    self.epochs = epochs\n    self.enable_function = enable_function\n    self.autotune = tf.data.experimental.AUTOTUNE\n    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True)\n    self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.1,\n                                             momentum=0.9, nesterov=True)\n    self.train_loss_metric = tf.keras.metrics.Mean(name=\'train_loss\')\n    self.train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(\n        name=\'train_accuracy\')\n    self.test_loss_metric = tf.keras.metrics.Mean(name=\'test_loss\')\n    self.test_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(\n        name=\'test_accuracy\')\n    self.model = model\n\n  def decay(self, epoch):\n    if epoch < 150:\n      return 0.1\n    if epoch >= 150 and epoch < 225:\n      return 0.01\n    if epoch >= 225:\n      return 0.001\n\n  def keras_fit(self, train_dataset, test_dataset):\n    self.model.compile(\n        optimizer=self.optimizer, loss=self.loss_object, metrics=[\'accuracy\'])\n    history = self.model.fit(\n        train_dataset, epochs=self.epochs, validation_data=test_dataset,\n        verbose=2, callbacks=[tf.keras.callbacks.LearningRateScheduler(\n            self.decay)])\n    return (history.history[\'loss\'][-1],\n            history.history[\'accuracy\'][-1],\n            history.history[\'val_loss\'][-1],\n            history.history[\'val_accuracy\'][-1])\n\n  def train_step(self, image, label):\n    """"""One train step.\n\n    Args:\n      image: Batch of images.\n      label: corresponding label for the batch of images.\n    """"""\n\n    with tf.GradientTape() as tape:\n      predictions = self.model(image, training=True)\n      loss = self.loss_object(label, predictions)\n      loss += sum(self.model.losses)\n    gradients = tape.gradient(loss, self.model.trainable_variables)\n    self.optimizer.apply_gradients(\n        zip(gradients, self.model.trainable_variables))\n\n    self.train_loss_metric(loss)\n    self.train_acc_metric(label, predictions)\n\n  def test_step(self, image, label):\n    """"""One test step.\n\n    Args:\n      image: Batch of images.\n      label: corresponding label for the batch of images.\n    """"""\n\n    predictions = self.model(image, training=False)\n    loss = self.loss_object(label, predictions)\n\n    self.test_loss_metric(loss)\n    self.test_acc_metric(label, predictions)\n\n  def custom_loop(self, train_dataset, test_dataset):\n    """"""Custom training and testing loop.\n\n    Args:\n      train_dataset: Training dataset\n      test_dataset: Testing dataset\n\n    Returns:\n      train_loss, train_accuracy, test_loss, test_accuracy\n    """"""\n    if self.enable_function:\n      self.train_step = tf.function(self.train_step)\n      self.test_step = tf.function(self.test_step)\n\n    for epoch in range(self.epochs):\n      self.optimizer.learning_rate = self.decay(epoch)\n\n      for image, label in train_dataset:\n        self.train_step(image, label)\n\n      for test_image, test_label in test_dataset:\n        self.test_step(test_image, test_label)\n\n      template = (\'Epoch: {}, Train Loss: {}, Train Accuracy: {}, \'\n                  \'Test Loss: {}, Test Accuracy: {}\')\n\n      print(\n          template.format(epoch, self.train_loss_metric.result(),\n                          self.train_acc_metric.result(),\n                          self.test_loss_metric.result(),\n                          self.test_acc_metric.result()))\n\n      if epoch != self.epochs - 1:\n        self.train_loss_metric.reset_states()\n        self.train_acc_metric.reset_states()\n        self.test_loss_metric.reset_states()\n        self.test_acc_metric.reset_states()\n\n    return (self.train_loss_metric.result().numpy(),\n            self.train_acc_metric.result().numpy(),\n            self.test_loss_metric.result().numpy(),\n            self.test_acc_metric.result().numpy())\n\n\ndef run_main(argv):\n  """"""Passes the flags to main.\n\n  Args:\n    argv: argv\n  """"""\n  del argv\n  kwargs = utils.flags_dict()\n  main(**kwargs)\n\n\ndef main(epochs,\n         enable_function,\n         buffer_size,\n         batch_size,\n         mode,\n         growth_rate,\n         output_classes,\n         depth_of_model=None,\n         num_of_blocks=None,\n         num_layers_in_each_block=None,\n         data_format=\'channels_last\',\n         bottleneck=True,\n         compression=0.5,\n         weight_decay=1e-4,\n         dropout_rate=0.,\n         pool_initial=False,\n         include_top=True,\n         train_mode=\'custom_loop\',\n         data_dir=None):\n\n  model = densenet.DenseNet(mode, growth_rate, output_classes, depth_of_model,\n                            num_of_blocks, num_layers_in_each_block,\n                            data_format, bottleneck, compression, weight_decay,\n                            dropout_rate, pool_initial, include_top)\n  train_obj = Train(epochs, enable_function, model)\n  train_dataset, test_dataset, _ = utils.create_dataset(\n      buffer_size, batch_size, data_format, data_dir)\n\n  print(\'Training...\')\n  if train_mode == \'custom_loop\':\n    return train_obj.custom_loop(train_dataset, test_dataset)\n  elif train_mode == \'keras_fit\':\n    return train_obj.keras_fit(train_dataset, test_dataset)\n\n\nif __name__ == \'__main__\':\n  utils.define_densenet_flags()\n  app.run(run_main)\n'"
tensorflow_examples/models/densenet/utils.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Densenet utils.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nFLAGS = flags.FLAGS\n\n\ndef define_densenet_flags():\n  """"""Defining all the necessary flags.""""""\n  flags.DEFINE_integer(\'buffer_size\', 50000, \'Shuffle buffer size\')\n  flags.DEFINE_integer(\'batch_size\', 64, \'Batch Size\')\n  flags.DEFINE_integer(\'epochs\', 1, \'Number of epochs\')\n  flags.DEFINE_boolean(\'enable_function\', True, \'Enable Function?\')\n  flags.DEFINE_string(\'data_dir\', None, \'Directory to store the dataset\')\n  flags.DEFINE_string(\'mode\', \'from_depth\', \'Deciding how to build the model\')\n  flags.DEFINE_integer(\'depth_of_model\', 7, \'Number of layers in the model\')\n  flags.DEFINE_integer(\'growth_rate\', 12, \'Filters to add per dense block\')\n  flags.DEFINE_integer(\'num_of_blocks\', 3, \'Number of dense blocks\')\n  flags.DEFINE_integer(\'output_classes\', 10, \'Number of classes in the dataset\')\n  flags.DEFINE_integer(\'num_layers_in_each_block\', -1,\n                       \'Number of layers in each dense block\')\n  flags.DEFINE_string(\'data_format\', \'channels_last\',\n                      \'channels_last or channels_first\')\n  flags.DEFINE_boolean(\'bottleneck\', True,\n                       \'Add bottleneck blocks between layers\')\n  flags.DEFINE_float(\n      \'compression\', 0.5,\n      \'reducing the number of inputs(filters) to the transition block.\')\n  flags.DEFINE_float(\'weight_decay\', 1e-4, \'weight decay\')\n  flags.DEFINE_float(\'dropout_rate\', 0., \'dropout rate\')\n  flags.DEFINE_boolean(\n      \'pool_initial\', False,\n      \'If True add a conv => maxpool block at the start. Used for Imagenet\')\n  flags.DEFINE_boolean(\'include_top\', True, \'Include the classifier layer\')\n  flags.DEFINE_string(\'train_mode\', \'custom_loop\',\n                      \'Use either ""keras_fit"" or ""custom_loop""\')\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nCIFAR_MEAN = [125.3, 123.0, 113.9]\nCIFAR_STD = [63.0, 62.1, 66.7]\n\nHEIGHT = 32\nWIDTH = 32\n\n\nclass Preprocess(object):\n  """"""Preprocess images.\n\n  Args:\n    data_format: channels_first or channels_last\n  """"""\n\n  def __init__(self, data_format, train):\n    self._data_format = data_format\n    self._train = train\n\n  def __call__(self, image, label):\n    image = tf.cast(image, tf.float32)\n\n    if self._train:\n      image = tf.image.random_flip_left_right(image)\n      image = self.random_jitter(image)\n\n    image = (image - CIFAR_MEAN) / CIFAR_STD\n\n    if self._data_format == \'channels_first\':\n      image = tf.transpose(image, [2, 0, 1])\n\n    return image, label\n\n  def random_jitter(self, image):\n    # add 4 pixels on each side; image_size == (36 x 36)\n    image = tf.image.resize_with_crop_or_pad(\n        image, HEIGHT + 8, WIDTH + 8)\n\n    image = tf.image.random_crop(image, size=[HEIGHT, WIDTH, 3])\n\n    return image\n\n\ndef create_dataset(buffer_size, batch_size, data_format, data_dir=None):\n  """"""Creates a tf.data Dataset.\n\n  Args:\n    buffer_size: Shuffle buffer size.\n    batch_size: Batch size\n    data_format: channels_first or channels_last\n    data_dir: directory to store the dataset.\n\n  Returns:\n    train dataset, test dataset, metadata\n  """"""\n\n  preprocess_train = Preprocess(data_format, train=True)\n  preprocess_test = Preprocess(data_format, train=False)\n\n  train_dataset, metadata = tfds.load(\n      \'cifar10\',\n      split=\'train\',\n      data_dir=data_dir,\n      as_supervised=True,\n      shuffle_files=True,\n      with_info=True)\n  test_dataset = tfds.load(\n      \'cifar10\', split=\'test\', data_dir=data_dir, as_supervised=True)\n\n  train_dataset = train_dataset.map(\n      preprocess_train, num_parallel_calls=AUTOTUNE)\n  train_dataset = train_dataset.cache()\n  train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n  train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n\n  test_dataset = test_dataset.map(preprocess_test, num_parallel_calls=AUTOTUNE)\n  test_dataset = test_dataset.cache().batch(batch_size)\n  test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n\n  return train_dataset, test_dataset, metadata\n\n\ndef flags_dict():\n  """"""Define the flags.\n\n  Returns:\n    Command line arguments as Flags.\n  """"""\n\n  kwargs = {\n      \'epochs\': FLAGS.epochs,\n      \'enable_function\': FLAGS.enable_function,\n      \'buffer_size\': FLAGS.buffer_size,\n      \'batch_size\': FLAGS.batch_size,\n      \'mode\': FLAGS.mode,\n      \'depth_of_model\': FLAGS.depth_of_model,\n      \'growth_rate\': FLAGS.growth_rate,\n      \'num_of_blocks\': FLAGS.num_of_blocks,\n      \'output_classes\': FLAGS.output_classes,\n      \'num_layers_in_each_block\': FLAGS.num_layers_in_each_block,\n      \'data_format\': FLAGS.data_format,\n      \'bottleneck\': FLAGS.bottleneck,\n      \'compression\': FLAGS.compression,\n      \'weight_decay\': FLAGS.weight_decay,\n      \'dropout_rate\': FLAGS.dropout_rate,\n      \'pool_initial\': FLAGS.pool_initial,\n      \'include_top\': FLAGS.include_top,\n      \'train_mode\': FLAGS.train_mode\n  }\n  return kwargs\n\n\ndef get_cifar10_kwargs():\n  return {\'epochs\': 1, \'enable_function\': True, \'buffer_size\': 50000,\n          \'batch_size\': 64, \'depth_of_model\': 40, \'growth_rate\': 12,\n          \'num_of_blocks\': 3, \'output_classes\': 10, \'mode\': \'from_depth\',\n          \'data_format\': \'channels_last\', \'dropout_rate\': 0.}\n'"
tensorflow_examples/models/nmt_with_attention/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""See the tutorial: https://www.tensorflow.org/tutorials/text/nmt_with_attention""\n'"
tensorflow_examples/models/nmt_with_attention/distributed_test.py,8,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for distributed nmt_with_attention.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport tensorflow as tf\nfrom tensorflow_examples.models.nmt_with_attention import distributed_train\nfrom tensorflow_examples.models.nmt_with_attention import utils\n\n\nclass NmtDistributedTest(tf.test.TestCase):\n\n  def test_one_epoch_multi_device(self):\n    if tf.test.is_gpu_available():\n      print(\'Using 2 virtual GPUs.\')\n      device = tf.config.experimental.list_physical_devices(\'GPU\')[0]\n      tf.config.experimental.set_virtual_device_configuration(\n          device, [\n              tf.config.experimental.VirtualDeviceConfiguration(\n                  memory_limit=8192),\n              tf.config.experimental.VirtualDeviceConfiguration(\n                  memory_limit=8192)\n          ])\n\n    kwargs = utils.get_common_kwargs()\n    kwargs.update({\n        \'epochs\': 1,\n        \'batch_size\': 16,\n        \'num_examples\': 10,\n        \'embedding_dim\': 4,\n        \'enc_units\': 4,\n        \'dec_units\': 4\n    })\n\n    distributed_train.main(**kwargs)\n\n\nclass NmtDistributedBenchmark(tf.test.Benchmark):\n\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n\n  def benchmark_one_epoch_1_gpu(self):\n    kwargs = utils.get_common_kwargs()\n    kwargs.update({\'enable_function\': False})\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_one_epoch_1_gpu_function(self):\n    kwargs = utils.get_common_kwargs()\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_ten_epochs_2_gpus(self):\n    kwargs = utils.get_common_kwargs()\n    kwargs.update({\'epochs\': 10, \'batch_size\': 128})\n    self._run_and_report_benchmark(**kwargs)\n\n  def _run_and_report_benchmark(self, **kwargs):\n    start_time_sec = time.time()\n    train_loss, test_loss = distributed_train.main(**kwargs)\n    wall_time_sec = time.time() - start_time_sec\n\n    extras = {\'train_loss\': train_loss,\n              \'test_loss\': test_loss}\n\n    self.report_benchmark(\n        wall_time=wall_time_sec, extras=extras)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_examples/models/nmt_with_attention/distributed_train.py,4,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Distributed Train.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf\nfrom tensorflow_examples.models.nmt_with_attention import nmt\nfrom tensorflow_examples.models.nmt_with_attention import utils\nfrom tensorflow_examples.models.nmt_with_attention.train import Train\n\n\nFLAGS = flags.FLAGS\n\n\nclass DistributedTrain(Train):\n  """"""Distributed Train class.\n\n  Attributes:\n    epochs: Number of epochs.\n    enable_function: Decorate function with tf.function.\n    encoder: Encoder.\n    decoder: Decoder.\n    inp_lang: Input language tokenizer.\n    targ_lang: Target language tokenizer.\n    batch_size: Batch size.\n    per_replica_batch_size: Batch size per replica for sync replicas.\n  """"""\n\n  def __init__(self, epochs, enable_function, encoder, decoder, inp_lang,\n               targ_lang, batch_size, per_replica_batch_size):\n    Train.__init__(\n        self, epochs, enable_function, encoder, decoder, inp_lang, targ_lang,\n        batch_size, per_replica_batch_size)\n\n  def training_loop(self, train_iterator, test_iterator,\n                    num_train_steps_per_epoch, num_test_steps_per_epoch,\n                    strategy):\n    """"""Custom training and testing loop.\n\n    Args:\n      train_iterator: Training iterator created using strategy\n      test_iterator: Testing iterator created using strategy\n      num_train_steps_per_epoch: number of training steps in an epoch.\n      num_test_steps_per_epoch: number of test steps in an epoch.\n      strategy: Distribution strategy\n\n    Returns:\n      train_loss, test_loss\n    """"""\n\n    # this code is expected to change.\n    def distributed_train():\n      return strategy.experimental_run(self.train_step, train_iterator)\n\n    def distributed_test():\n      return strategy.experimental_run(self.test_step, test_iterator)\n\n    if self.enable_function:\n      distributed_train = tf.function(distributed_train)\n      distributed_test = tf.function(distributed_test)\n\n    template = \'Epoch: {}, Train Loss: {}, Test Loss: {}\'\n\n    for epoch in range(self.epochs):\n      self.train_loss_metric.reset_states()\n      self.test_loss_metric.reset_states()\n\n      train_iterator.initialize()\n      for _ in range(num_train_steps_per_epoch):\n        distributed_train()\n\n      test_iterator.initialize()\n      for _ in range(num_test_steps_per_epoch):\n        distributed_test()\n\n      print (template.format(epoch,\n                             self.train_loss_metric.result().numpy(),\n                             self.test_loss_metric.result().numpy()))\n\n    return (self.train_loss_metric.result().numpy(),\n            self.test_loss_metric.result().numpy())\n\n\ndef run_main(argv):\n  del argv\n  kwargs = utils.flags_dict()\n  main(**kwargs)\n\n\ndef main(epochs, enable_function, buffer_size, batch_size, download_path,\n         num_examples=70000, embedding_dim=256, enc_units=1024, dec_units=1024):\n\n  strategy = tf.distribute.MirroredStrategy()\n  num_replicas = strategy.num_replicas_in_sync\n\n  file_path = utils.download(download_path)\n  train_ds, test_ds, inp_lang, targ_lang = utils.create_dataset(\n      file_path, num_examples, buffer_size, batch_size)\n\n  with strategy.scope():\n    vocab_inp_size = len(inp_lang.word_index) + 1\n    vocab_tar_size = len(targ_lang.word_index) + 1\n\n    num_train_steps_per_epoch = train_ds.cardinality()\n    num_test_steps_per_epoch = test_ds.cardinality()\n\n    train_iterator = strategy.make_dataset_iterator(train_ds)\n    test_iterator = strategy.make_dataset_iterator(test_ds)\n\n    local_batch_size, remainder = divmod(batch_size, num_replicas)\n\n    template = (\'Batch size ({}) must be divisible by the \'\n                \'number of replicas ({})\')\n    if remainder:\n      raise ValueError(template.format(batch_size, num_replicas))\n\n    encoder = nmt.Encoder(vocab_inp_size, embedding_dim, enc_units,\n                          local_batch_size)\n    decoder = nmt.Decoder(vocab_tar_size, embedding_dim, dec_units)\n\n    train_obj = DistributedTrain(epochs, enable_function, encoder, decoder,\n                                 inp_lang, targ_lang, batch_size,\n                                 local_batch_size)\n    print (\'Training ...\')\n    return train_obj.training_loop(train_iterator,\n                                   test_iterator,\n                                   num_train_steps_per_epoch,\n                                   num_test_steps_per_epoch,\n                                   strategy)\n\nif __name__ == \'__main__\':\n  utils.nmt_flags()\n  app.run(run_main)\n'"
tensorflow_examples/models/nmt_with_attention/nmt.py,18,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Neural Machine Translation with Attention.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass Encoder(tf.keras.Model):\n  """"""Encoder.\n\n  Args:\n    vocab_size: Vocabulary size.\n    embedding_dim: Embedding dimension.\n    enc_units: Number of encoder units.\n    batch_sz: Batch size.\n  """"""\n\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer=\'glorot_uniform\')\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output, state = self.gru(x, initial_state=hidden)\n    return output, state\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))\n\n\nclass BahdanauAttention(tf.keras.Model):\n  """"""Bahdanau Attention.\n\n  Args:\n    units: Number of dense units.\n  """"""\n\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.w1 = tf.keras.layers.Dense(units)\n    self.w2 = tf.keras.layers.Dense(units)\n    self.v = tf.keras.layers.Dense(1)\n\n  def call(self, query, values):\n    # hidden shape == (batch_size, hidden size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n    # we are doing this to perform addition to calculate the score\n    hidden_with_time_axis = tf.expand_dims(query, 1)\n\n    # score shape == (batch_size, max_length, hidden_size)\n    score = self.v(tf.nn.tanh(\n        self.w1(values) + self.w2(hidden_with_time_axis)))\n\n    # attention_weights shape == (batch_size, max_length, 1)\n    # we get 1 at the last axis because we are applying score to self.V\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n\n\nclass Decoder(tf.keras.Model):\n  """"""Decoder.\n\n  Args:\n    vocab_size: Vocabulary size.\n    embedding_dim: Embedding dimension.\n    dec_units: Number of decoder units.\n  """"""\n\n  def __init__(self, vocab_size, embedding_dim, dec_units):\n    super(Decoder, self).__init__()\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer=\'glorot_uniform\')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n\n    # used for attention\n    self.attention = BahdanauAttention(self.dec_units)\n\n  def call(self, x, hidden, enc_output):\n    # enc_output shape == (batch_size, max_length, hidden_size)\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim+hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # output shape == (batch_size * 1, hidden_size)\n    output = tf.reshape(output, (-1, output.shape[2]))\n\n    # output shape == (batch_size, vocab)\n    x = self.fc(output)\n\n    return x, state, attention_weights\n'"
tensorflow_examples/models/nmt_with_attention/nmt_test.py,3,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nmt_with_attention.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport tensorflow as tf\nfrom tensorflow_examples.models.nmt_with_attention import train\nfrom tensorflow_examples.models.nmt_with_attention import utils\n\n\nclass NmtTest(tf.test.TestCase):\n\n  def test_one_epoch(self):\n    num_examples = 10\n    buffer_size = 10\n    batch_size = 1\n    embedding_dim = 4\n    enc_units = 4\n    dec_units = 4\n    epochs = 1\n\n    train.main(epochs, True, buffer_size, batch_size, \'datasets\', num_examples,\n               embedding_dim, enc_units, dec_units)\n\n\nclass NmtBenchmark(tf.test.Benchmark):\n\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n\n  def benchmark_one_epoch(self):\n    kwargs = utils.get_common_kwargs()\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_ten_epochs(self):\n    kwargs = utils.get_common_kwargs()\n    kwargs.update({\'epochs\': 10})\n    self._run_and_report_benchmark(**kwargs)\n\n  def _run_and_report_benchmark(self, **kwargs):\n    start_time_sec = time.time()\n    train_loss, test_loss = train.main(**kwargs)\n    wall_time_sec = time.time() - start_time_sec\n\n    extras = {\'train_loss\': train_loss,\n              \'test_loss\': test_loss}\n\n    self.report_benchmark(\n        wall_time=wall_time_sec, extras=extras)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_examples/models/nmt_with_attention/train.py,17,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Train.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\n\nimport tensorflow as tf\nfrom tensorflow_examples.models.nmt_with_attention import nmt\nfrom tensorflow_examples.models.nmt_with_attention import utils\n\n\nclass Train(object):\n  """"""Train class.\n\n  Attributes:\n    epochs: Number of epochs.\n    enable_function: Decorate function with tf.function.\n    encoder: Encoder.\n    decoder: Decoder.\n    inp_lang: Input language tokenizer.\n    targ_lang: Target language tokenizer.\n    batch_size: Batch size.\n    per_replica_batch_size: Batch size per replica for sync replicas. Same as\n      batch_size for non distributed training.\n    optimizer: Optimizer.\n    loss_object: Object of the loss class.\n    train_loss_metric: Mean metric to keep track of the train loss value.\n    test_loss_metric: Mean metric to keep track of the test loss value.\n  """"""\n\n  def __init__(self, epochs, enable_function, encoder, decoder, inp_lang,\n               targ_lang, batch_size, per_replica_batch_size):\n    self.epochs = epochs\n    self.enable_function = enable_function\n    self.encoder = encoder\n    self.decoder = decoder\n    self.inp_lang = inp_lang\n    self.targ_lang = targ_lang\n    self.batch_size = batch_size\n    self.per_replica_batch_size = per_replica_batch_size\n    self.optimizer = tf.keras.optimizers.Adam()\n    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n    self.train_loss_metric = tf.keras.metrics.Mean(name=\'train_loss\')\n    self.test_loss_metric = tf.keras.metrics.Mean(name=\'test_loss\')\n\n  def loss_function(self, real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = self.loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_) * 1. / self.batch_size\n\n  def train_step(self, inputs):\n    """"""One train step.\n\n    Args:\n      inputs: tuple of input tensor, target tensor.\n    """"""\n\n    loss = 0\n    enc_hidden = self.encoder.initialize_hidden_state()\n\n    inp, targ = inputs\n\n    with tf.GradientTape() as tape:\n      enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n      dec_hidden = enc_hidden\n      dec_input = tf.expand_dims(\n          [self.targ_lang.word_index[\'<start>\']] * self.per_replica_batch_size,\n          1)\n\n      for t in range(1, targ.shape[1]):\n        # passing enc_output to the decoder\n        predictions, dec_hidden, _ = self.decoder(\n            dec_input, dec_hidden, enc_output)\n        loss += self.loss_function(targ[:, t], predictions)\n        # using teacher forcing\n        dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = (loss / int(targ.shape[1]))\n    variables = (self.encoder.trainable_variables +\n                 self.decoder.trainable_variables)\n    gradients = tape.gradient(loss, variables)\n    self.optimizer.apply_gradients(zip(gradients, variables))\n\n    self.train_loss_metric(batch_loss)\n\n  def test_step(self, inputs_test):\n    """"""One test step.\n\n    Args:\n      inputs_test: tuple of input tensor, target tensor.\n    """"""\n\n    loss = 0\n    enc_hidden = self.encoder.initialize_hidden_state()\n\n    inp_test, targ_test = inputs_test\n\n    enc_output, enc_hidden = self.encoder(inp_test, enc_hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims(\n        [self.targ_lang.word_index[\'<start>\']] * self.per_replica_batch_size,\n        1)\n\n    for t in range(1, targ_test.shape[1]):\n      predictions, dec_hidden, _ = self.decoder(\n          dec_input, dec_hidden, enc_output)\n      loss += self.loss_function(targ_test[:, t], predictions)\n\n      prediction_id = tf.argmax(predictions, axis=1)\n      # passing the predictions back to the model as the input.\n      dec_input = tf.expand_dims(prediction_id, 1)\n\n    batch_loss = (loss / int(targ_test.shape[1]))\n\n    self.test_loss_metric(batch_loss)\n\n  def training_loop(self, train_ds, test_ds):\n    """"""Custom training and testing loop.\n\n    Args:\n      train_ds: Training dataset\n      test_ds: Testing dataset\n\n    Returns:\n      train_loss, test_loss\n    """"""\n\n    if self.enable_function:\n      self.train_step = tf.function(self.train_step)\n      self.test_step = tf.function(self.test_step)\n\n    template = \'Epoch: {}, Train Loss: {}, Test Loss: {}\'\n\n    for epoch in range(self.epochs):\n      self.train_loss_metric.reset_states()\n      self.test_loss_metric.reset_states()\n\n      for inp, targ in train_ds:\n        self.train_step((inp, targ))\n\n      for inp_test, targ_test in test_ds:\n        self.test_step((inp_test, targ_test))\n\n      print (template.format(epoch,\n                             self.train_loss_metric.result().numpy(),\n                             self.test_loss_metric.result().numpy()))\n\n    return (self.train_loss_metric.result().numpy(),\n            self.test_loss_metric.result().numpy())\n\n\ndef run_main(argv):\n  del argv\n  kwargs = utils.flags_dict()\n  main(**kwargs)\n\n\ndef main(epochs, enable_function, buffer_size, batch_size, download_path,\n         num_examples=70000, embedding_dim=256, enc_units=1024, dec_units=1024):\n  file_path = utils.download(download_path)\n  train_ds, test_ds, inp_lang, targ_lang = utils.create_dataset(\n      file_path, num_examples, buffer_size, batch_size)\n  vocab_inp_size = len(inp_lang.word_index) + 1\n  vocab_tar_size = len(targ_lang.word_index) + 1\n\n  encoder = nmt.Encoder(vocab_inp_size, embedding_dim, enc_units, batch_size)\n  decoder = nmt.Decoder(vocab_tar_size, embedding_dim, dec_units)\n\n  train_obj = Train(epochs, enable_function, encoder, decoder,\n                    inp_lang, targ_lang, batch_size, batch_size)\n  print (\'Training ...\')\n  return train_obj.training_loop(train_ds, test_ds)\n\nif __name__ == \'__main__\':\n  utils.nmt_flags()\n  app.run(run_main)\n'"
tensorflow_examples/models/nmt_with_attention/utils.py,7,"b'# coding=utf-8\n\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utils.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport io\nimport os\nimport re\nimport unicodedata\nfrom absl import flags\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\n_URL = \'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\'\n\n\ndef nmt_flags():\n  flags.DEFINE_string(\'download_path\', \'datasets\', \'Download folder\')\n  flags.DEFINE_integer(\'buffer_size\', 70000, \'Shuffle buffer size\')\n  flags.DEFINE_integer(\'batch_size\', 64, \'Batch Size\')\n  flags.DEFINE_integer(\'epochs\', 1, \'Number of epochs\')\n  flags.DEFINE_integer(\'embedding_dim\', 256, \'Embedding dimension\')\n  flags.DEFINE_integer(\'enc_units\', 1024, \'Encoder GRU units\')\n  flags.DEFINE_integer(\'dec_units\', 1024, \'Decoder GRU units\')\n  flags.DEFINE_boolean(\'enable_function\', True, \'Enable Function?\')\n  flags.DEFINE_integer(\'num_examples\', 70000, \'Number of examples from dataset\')\n\n\ndef download(download_path):\n  path_to_zip = tf.keras.utils.get_file(\n      \'spa-eng.zip\', origin=_URL, cache_subdir=download_path,\n      extract=True)\n  path_to_file = os.path.join(os.path.dirname(path_to_zip), \'spa-eng/spa.txt\')\n\n  return path_to_file\n\n\ndef unicode_to_ascii(s):\n  return \'\'.join(c for c in unicodedata.normalize(\'NFD\', s)\n                 if unicodedata.category(c) != \'Mn\')\n\n\ndef preprocess_sentence(w):\n  """"""Preprocessing words in a sentence.\n\n  Args:\n    w: Word.\n\n  Returns:\n    Preprocessed words.\n  """"""\n\n  w = unicode_to_ascii(w.lower().strip())\n\n  # creating a space between a word and the punctuation following it\n  w = re.sub(r\'([?.!,\xc2\xbf])\', r\' \\1 \', w)\n  w = re.sub(r\'["" ""]+\', \' \', w)\n\n  # replacing everything with space except (a-z, A-Z, ""."", ""?"", ""!"", "","")\n  w = re.sub(r\'[^a-zA-Z?.!,\xc2\xbf]+\', \' \', w)\n\n  w = w.rstrip().strip()\n\n  # adding a start and an end token to the sentence\n  # so that the model know when to start and stop predicting.\n  w = \'<start> \' + w + \' <end>\'\n  return w\n\n\ndef create_word_pairs(path, num_examples):\n  lines = io.open(path, encoding=\'UTF-8\').read().strip().split(\'\\n\')\n\n  word_pairs = [[preprocess_sentence(w) for w in l.split(\'\\t\')]  # pylint: disable=g-complex-comprehension\n                for l in lines[:num_examples]]\n\n  return zip(*word_pairs)\n\n\ndef max_length(tensor):\n  return max(len(t) for t in tensor)\n\n\ndef tokenize(lang):\n  """"""Tokenize the languages.\n\n  Args:\n    lang: Language to be tokenized.\n\n  Returns:\n    tensor: Tensors generated after tokenization.\n    lang_tokenizer: tokenizer.\n  """"""\n\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n      filters=\'\')\n  lang_tokenizer.fit_on_texts(lang)\n\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n                                                         padding=\'post\')\n\n  return tensor, lang_tokenizer\n\n\ndef load_dataset(path, num_examples):\n  # creating cleaned input, output pairs\n  targ_lang, inp_lang = create_word_pairs(path, num_examples)\n\n  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n\n  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n\n\ndef create_dataset(path_to_file, num_examples, buffer_size, batch_size):\n  """"""Create a tf.data Dataset.\n\n  Args:\n    path_to_file: Path to the file to load the text from.\n    num_examples: Number of examples to sample.\n    buffer_size: Shuffle buffer size.\n    batch_size: Batch size.\n\n  Returns:\n    train_dataset: Training dataset.\n    test_dataset: Test dataset.\n    inp_lang: Input language tokenizer.\n    targ_lang: Target language tokenizer.\n  """"""\n\n  input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(\n      path_to_file, num_examples)\n\n  # Creating training and validation sets using an 80-20 split\n  inp_train, inp_val, target_train, target_val = train_test_split(\n      input_tensor, target_tensor, test_size=0.2)\n\n  # Create a tf.data dataset\n  train_dataset = tf.data.Dataset.from_tensor_slices(\n      (inp_train, target_train)).shuffle(buffer_size)\n  train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n\n  test_dataset = tf.data.Dataset.from_tensor_slices((inp_val, target_val))\n  test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n\n  return train_dataset, test_dataset, inp_lang, targ_lang\n\n\ndef get_common_kwargs():\n  return {\'epochs\': 1, \'enable_function\': True, \'buffer_size\': 70000,\n          \'batch_size\': 64, \'download_path\': \'datasets\'}\n\n\ndef flags_dict():\n  """"""Define the flags.\n\n  Returns:\n    Command line arguments as Flags.\n  """"""\n\n  kwargs = {\n      \'epochs\': FLAGS.epochs,\n      \'enable_function\': FLAGS.enable_function,\n      \'buffer_size\': FLAGS.buffer_size,\n      \'batch_size\': FLAGS.batch_size,\n      \'download_path\': FLAGS.download_path,\n      \'num_examples\': FLAGS.num_examples,\n      \'embedding_dim\': FLAGS.embedding_dim,\n      \'enc_units\': FLAGS.enc_units,\n      \'dec_units\': FLAGS.dec_units\n  }\n\n  return kwargs\n'"
tensorflow_examples/models/pix2pix/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_examples/models/pix2pix/data_download.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Download facades data.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'download_path\', \'datasets\', \'Download folder\')\n\n_URL = \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\'\n\n\ndef _main(argv):\n  del argv\n  download_path = FLAGS.download_path\n  main(download_path)\n\n\ndef main(download_path):\n  path_to_zip = tf.keras.utils.get_file(\n      \'facades.tar.gz\', cache_subdir=download_path,\n      origin=_URL, extract=True)\n\n  path_to_folder = os.path.join(os.path.dirname(path_to_zip), \'facades/\')\n\n  return path_to_folder\n\nif __name__ == \'__main__\':\n  app.run(_main)\n'"
tensorflow_examples/models/pix2pix/pix2pix.py,62,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Pix2pix.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_integer(\'buffer_size\', 400, \'Shuffle buffer size\')\nflags.DEFINE_integer(\'batch_size\', 1, \'Batch Size\')\nflags.DEFINE_integer(\'epochs\', 1, \'Number of epochs\')\nflags.DEFINE_string(\'path\', None, \'Path to the data folder\')\nflags.DEFINE_boolean(\'enable_function\', True, \'Enable Function?\')\n\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n\ndef load(image_file):\n  """"""Loads the image and generates input and target image.\n\n  Args:\n    image_file: .jpeg file\n\n  Returns:\n    Input image, target image\n  """"""\n  image = tf.io.read_file(image_file)\n  image = tf.image.decode_jpeg(image)\n\n  w = tf.shape(image)[1]\n\n  w = w // 2\n  real_image = image[:, :w, :]\n  input_image = image[:, w:, :]\n\n  input_image = tf.cast(input_image, tf.float32)\n  real_image = tf.cast(real_image, tf.float32)\n\n  return input_image, real_image\n\n\ndef resize(input_image, real_image, height, width):\n  input_image = tf.image.resize(input_image, [height, width],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  real_image = tf.image.resize(real_image, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  return input_image, real_image\n\n\ndef random_crop(input_image, real_image):\n  stacked_image = tf.stack([input_image, real_image], axis=0)\n  cropped_image = tf.image.random_crop(\n      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image[0], cropped_image[1]\n\n\ndef normalize(input_image, real_image):\n  input_image = (input_image / 127.5) - 1\n  real_image = (real_image / 127.5) - 1\n\n  return input_image, real_image\n\n\n@tf.function\ndef random_jitter(input_image, real_image):\n  """"""Random jittering.\n\n  Resizes to 286 x 286 and then randomly crops to IMG_HEIGHT x IMG_WIDTH.\n\n  Args:\n    input_image: Input Image\n    real_image: Real Image\n\n  Returns:\n    Input Image, real image\n  """"""\n  # resizing to 286 x 286 x 3\n  input_image, real_image = resize(input_image, real_image, 286, 286)\n\n  # randomly cropping to 256 x 256 x 3\n  input_image, real_image = random_crop(input_image, real_image)\n\n  if tf.random.uniform(()) > 0.5:\n    # random mirroring\n    input_image = tf.image.flip_left_right(input_image)\n    real_image = tf.image.flip_left_right(real_image)\n\n  return input_image, real_image\n\n\ndef load_image_train(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = random_jitter(input_image, real_image)\n  input_image, real_image = normalize(input_image, real_image)\n\n  return input_image, real_image\n\n\ndef load_image_test(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = resize(input_image, real_image,\n                                   IMG_HEIGHT, IMG_WIDTH)\n  input_image, real_image = normalize(input_image, real_image)\n\n  return input_image, real_image\n\n\ndef create_dataset(path_to_train_images, path_to_test_images, buffer_size,\n                   batch_size):\n  """"""Creates a tf.data Dataset.\n\n  Args:\n    path_to_train_images: Path to train images folder.\n    path_to_test_images: Path to test images folder.\n    buffer_size: Shuffle buffer size.\n    batch_size: Batch size\n\n  Returns:\n    train dataset, test dataset\n  """"""\n  train_dataset = tf.data.Dataset.list_files(path_to_train_images)\n  train_dataset = train_dataset.shuffle(buffer_size)\n  train_dataset = train_dataset.map(\n      load_image_train, num_parallel_calls=AUTOTUNE)\n  train_dataset = train_dataset.batch(batch_size)\n\n  test_dataset = tf.data.Dataset.list_files(path_to_test_images)\n  test_dataset = test_dataset.map(\n      load_image_test, num_parallel_calls=AUTOTUNE)\n  test_dataset = test_dataset.batch(batch_size)\n\n  return train_dataset, test_dataset\n\n\nclass InstanceNormalization(tf.keras.layers.Layer):\n  """"""Instance Normalization Layer (https://arxiv.org/abs/1607.08022).""""""\n\n  def __init__(self, epsilon=1e-5):\n    super(InstanceNormalization, self).__init__()\n    self.epsilon = epsilon\n\n  def build(self, input_shape):\n    self.scale = self.add_weight(\n        name=\'scale\',\n        shape=input_shape[-1:],\n        initializer=tf.random_normal_initializer(1., 0.02),\n        trainable=True)\n\n    self.offset = self.add_weight(\n        name=\'offset\',\n        shape=input_shape[-1:],\n        initializer=\'zeros\',\n        trainable=True)\n\n  def call(self, x):\n    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n    inv = tf.math.rsqrt(variance + self.epsilon)\n    normalized = (x - mean) * inv\n    return self.scale * normalized + self.offset\n\n\ndef downsample(filters, size, norm_type=\'batchnorm\', apply_norm=True):\n  """"""Downsamples an input.\n\n  Conv2D => Batchnorm => LeakyRelu\n\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either \'batchnorm\' or \'instancenorm\'.\n    apply_norm: If True, adds the batchnorm layer\n\n  Returns:\n    Downsample Sequential Model\n  """"""\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2D(filters, size, strides=2, padding=\'same\',\n                             kernel_initializer=initializer, use_bias=False))\n\n  if apply_norm:\n    if norm_type.lower() == \'batchnorm\':\n      result.add(tf.keras.layers.BatchNormalization())\n    elif norm_type.lower() == \'instancenorm\':\n      result.add(InstanceNormalization())\n\n  result.add(tf.keras.layers.LeakyReLU())\n\n  return result\n\n\ndef upsample(filters, size, norm_type=\'batchnorm\', apply_dropout=False):\n  """"""Upsamples an input.\n\n  Conv2DTranspose => Batchnorm => Dropout => Relu\n\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either \'batchnorm\' or \'instancenorm\'.\n    apply_dropout: If True, adds the dropout layer\n\n  Returns:\n    Upsample Sequential Model\n  """"""\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding=\'same\',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n  if norm_type.lower() == \'batchnorm\':\n    result.add(tf.keras.layers.BatchNormalization())\n  elif norm_type.lower() == \'instancenorm\':\n    result.add(InstanceNormalization())\n\n  if apply_dropout:\n    result.add(tf.keras.layers.Dropout(0.5))\n\n  result.add(tf.keras.layers.ReLU())\n\n  return result\n\n\ndef unet_generator(output_channels, norm_type=\'batchnorm\'):\n  """"""Modified u-net generator model (https://arxiv.org/abs/1611.07004).\n\n  Args:\n    output_channels: Output channels\n    norm_type: Type of normalization. Either \'batchnorm\' or \'instancenorm\'.\n\n  Returns:\n    Generator model\n  """"""\n\n  down_stack = [\n      downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)\n      downsample(128, 4, norm_type),  # (bs, 64, 64, 128)\n      downsample(256, 4, norm_type),  # (bs, 32, 32, 256)\n      downsample(512, 4, norm_type),  # (bs, 16, 16, 512)\n      downsample(512, 4, norm_type),  # (bs, 8, 8, 512)\n      downsample(512, 4, norm_type),  # (bs, 4, 4, 512)\n      downsample(512, 4, norm_type),  # (bs, 2, 2, 512)\n      downsample(512, 4, norm_type),  # (bs, 1, 1, 512)\n  ]\n\n  up_stack = [\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 2, 2, 1024)\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 4, 4, 1024)\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 8, 8, 1024)\n      upsample(512, 4, norm_type),  # (bs, 16, 16, 1024)\n      upsample(256, 4, norm_type),  # (bs, 32, 32, 512)\n      upsample(128, 4, norm_type),  # (bs, 64, 64, 256)\n      upsample(64, 4, norm_type),  # (bs, 128, 128, 128)\n  ]\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 4, strides=2,\n      padding=\'same\', kernel_initializer=initializer,\n      activation=\'tanh\')  # (bs, 256, 256, 3)\n\n  concat = tf.keras.layers.Concatenate()\n\n  inputs = tf.keras.layers.Input(shape=[None, None, 3])\n  x = inputs\n\n  # Downsampling through the model\n  skips = []\n  for down in down_stack:\n    x = down(x)\n    skips.append(x)\n\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    x = concat([x, skip])\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)\n\n\ndef discriminator(norm_type=\'batchnorm\', target=True):\n  """"""PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n\n  Args:\n    norm_type: Type of normalization. Either \'batchnorm\' or \'instancenorm\'.\n    target: Bool, indicating whether target image is an input or not.\n\n  Returns:\n    Discriminator model\n  """"""\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  inp = tf.keras.layers.Input(shape=[None, None, 3], name=\'input_image\')\n  x = inp\n\n  if target:\n    tar = tf.keras.layers.Input(shape=[None, None, 3], name=\'target_image\')\n    x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n\n  down1 = downsample(64, 4, norm_type, False)(x)  # (bs, 128, 128, 64)\n  down2 = downsample(128, 4, norm_type)(down1)  # (bs, 64, 64, 128)\n  down3 = downsample(256, 4, norm_type)(down2)  # (bs, 32, 32, 256)\n\n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n  conv = tf.keras.layers.Conv2D(\n      512, 4, strides=1, kernel_initializer=initializer,\n      use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n\n  if norm_type.lower() == \'batchnorm\':\n    norm1 = tf.keras.layers.BatchNormalization()(conv)\n  elif norm_type.lower() == \'instancenorm\':\n    norm1 = InstanceNormalization()(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(\n      1, 4, strides=1,\n      kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n\n  if target:\n    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n  else:\n    return tf.keras.Model(inputs=inp, outputs=last)\n\n\ndef get_checkpoint_prefix():\n  checkpoint_dir = \'./training_checkpoints\'\n  checkpoint_prefix = os.path.join(checkpoint_dir, \'ckpt\')\n\n  return checkpoint_prefix\n\n\nclass Pix2pix(object):\n  """"""Pix2pix class.\n\n  Args:\n    epochs: Number of epochs.\n    enable_function: If true, train step is decorated with tf.function.\n    buffer_size: Shuffle buffer size..\n    batch_size: Batch size.\n  """"""\n\n  def __init__(self, epochs, enable_function):\n    self.epochs = epochs\n    self.enable_function = enable_function\n    self.lambda_value = 100\n    self.loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    self.generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    self.discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    self.generator = unet_generator(output_channels=3)\n    self.discriminator = discriminator()\n    self.checkpoint = tf.train.Checkpoint(\n        generator_optimizer=self.generator_optimizer,\n        discriminator_optimizer=self.discriminator_optimizer,\n        generator=self.generator,\n        discriminator=self.discriminator)\n\n  def discriminator_loss(self, disc_real_output, disc_generated_output):\n    real_loss = self.loss_object(\n        tf.ones_like(disc_real_output), disc_real_output)\n\n    generated_loss = self.loss_object(tf.zeros_like(\n        disc_generated_output), disc_generated_output)\n\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss\n\n  def generator_loss(self, disc_generated_output, gen_output, target):\n    gan_loss = self.loss_object(tf.ones_like(\n        disc_generated_output), disc_generated_output)\n\n    # mean absolute error\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n    total_gen_loss = gan_loss + (self.lambda_value * l1_loss)\n    return total_gen_loss\n\n  def train_step(self, input_image, target_image):\n    """"""One train step over the generator and discriminator model.\n\n    Args:\n      input_image: Input Image.\n      target_image: Target image.\n\n    Returns:\n      generator loss, discriminator loss.\n    """"""\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      gen_output = self.generator(input_image, training=True)\n\n      disc_real_output = self.discriminator(\n          [input_image, target_image], training=True)\n      disc_generated_output = self.discriminator(\n          [input_image, gen_output], training=True)\n\n      gen_loss = self.generator_loss(\n          disc_generated_output, gen_output, target_image)\n      disc_loss = self.discriminator_loss(\n          disc_real_output, disc_generated_output)\n\n    generator_gradients = gen_tape.gradient(\n        gen_loss, self.generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(\n        disc_loss, self.discriminator.trainable_variables)\n\n    self.generator_optimizer.apply_gradients(zip(\n        generator_gradients, self.generator.trainable_variables))\n    self.discriminator_optimizer.apply_gradients(zip(\n        discriminator_gradients, self.discriminator.trainable_variables))\n\n    return gen_loss, disc_loss\n\n  def train(self, dataset, checkpoint_pr):\n    """"""Train the GAN for x number of epochs.\n\n    Args:\n      dataset: train dataset.\n      checkpoint_pr: prefix in which the checkpoints are stored.\n\n    Returns:\n      Time for each epoch.\n    """"""\n    time_list = []\n    if self.enable_function:\n      self.train_step = tf.function(self.train_step)\n\n    for epoch in range(self.epochs):\n      start_time = time.time()\n      for input_image, target_image in dataset:\n        gen_loss, disc_loss = self.train_step(input_image, target_image)\n\n      wall_time_sec = time.time() - start_time\n      time_list.append(wall_time_sec)\n\n      # saving (checkpoint) the model every 20 epochs\n      if (epoch + 1) % 20 == 0:\n        self.checkpoint.save(file_prefix=checkpoint_pr)\n\n      template = \'Epoch {}, Generator loss {}, Discriminator Loss {}\'\n      print (template.format(epoch, gen_loss, disc_loss))\n\n    return time_list\n\n\ndef run_main(argv):\n  del argv\n  kwargs = {\'epochs\': FLAGS.epochs, \'enable_function\': FLAGS.enable_function,\n            \'path\': FLAGS.path, \'buffer_size\': FLAGS.buffer_size,\n            \'batch_size\': FLAGS.batch_size}\n  main(**kwargs)\n\n\ndef main(epochs, enable_function, path, buffer_size, batch_size):\n  path_to_folder = path\n\n  pix2pix_object = Pix2pix(epochs, enable_function)\n\n  train_dataset, _ = create_dataset(\n      os.path.join(path_to_folder, \'train/*.jpg\'),\n      os.path.join(path_to_folder, \'test/*.jpg\'),\n      buffer_size, batch_size)\n  checkpoint_pr = get_checkpoint_prefix()\n  print (\'Training ...\')\n  return pix2pix_object.train(train_dataset, checkpoint_pr)\n\n\nif __name__ == \'__main__\':\n  app.run(run_main)\n'"
tensorflow_examples/models/pix2pix/pix2pix_test.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Pix2Pix.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nimport tensorflow as tf\nfrom tensorflow_examples.models.pix2pix import data_download\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nFLAGS = flags.FLAGS\n\n\nclass Pix2PixTest(tf.test.TestCase):\n\n  def test_one_step_with_function(self):\n    epochs = 1\n    batch_size = 1\n    enable_function = True\n\n    input_image = tf.random.uniform((256, 256, 3))\n    target_image = tf.random.uniform((256, 256, 3))\n\n    train_dataset = tf.data.Dataset.from_tensors(\n        (input_image, target_image)).map(pix2pix.random_jitter).batch(\n            batch_size)\n    checkpoint_pr = pix2pix.get_checkpoint_prefix()\n\n    pix2pix_obj = pix2pix.Pix2pix(epochs, enable_function)\n    pix2pix_obj.train(train_dataset, checkpoint_pr)\n\n  def test_one_step_without_function(self):\n    epochs = 1\n    batch_size = 1\n    enable_function = False\n\n    input_image = tf.random.uniform((256, 256, 3))\n    target_image = tf.random.uniform((256, 256, 3))\n\n    train_dataset = tf.data.Dataset.from_tensors(\n        (input_image, target_image)).map(pix2pix.random_jitter).batch(\n            batch_size)\n\n    pix2pix_obj = pix2pix.Pix2pix(epochs, enable_function)\n\n    checkpoint_pr = pix2pix.get_checkpoint_prefix()\n    pix2pix_obj.train(train_dataset, checkpoint_pr)\n\n\nclass Pix2PixBenchmark(tf.test.Benchmark):\n\n  def __init__(self, output_dir=None, **kwargs):\n    self.output_dir = output_dir\n\n  def benchmark_with_function(self):\n    path = data_download.main(""datasets"")\n    kwargs = {""epochs"": 6, ""enable_function"": True, ""path"": path,\n              ""buffer_size"": 400, ""batch_size"": 1}\n    self._run_and_report_benchmark(**kwargs)\n\n  def benchmark_without_function(self):\n    path = data_download.main(""datasets"")\n    kwargs = {""epochs"": 6, ""enable_function"": False, ""path"": path,\n              ""buffer_size"": 400, ""batch_size"": 1}\n    self._run_and_report_benchmark(**kwargs)\n\n  def _run_and_report_benchmark(self, **kwargs):\n    time_list = pix2pix.main(**kwargs)\n    # 1st epoch is the warmup epoch hence skipping it for calculating time.\n    self.report_benchmark(wall_time=tf.reduce_mean(time_list[1:]))\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
community/en/r1/tutorials/__init__.py,0,b''
lite/examples/gesture_classification/ml/convert.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport traceback\n\nfrom converter import ModelConverter\n\nparser = argparse.ArgumentParser(\n    description=\'Exports TensorflowJS model to Keras\')\n\n# TensorFlow.js Parameters\nparser.add_argument(\n    \'--config_json_path\',\n    help=\'Path to the TensorFlow.js weights manifest file \'\n    \'containing the model architecture (model.json)\',\n    action=\'store\',\n    required=True,\n    dest=\'config_json_path\',\n    type=str)\nparser.add_argument(\n    \'--weights_path_prefix\',\n    help=\'Optional path to weights files (model.weights.bin). \'\n    \'If not specified (`None`), will assume the prefix is the same directory \'\n    \'as the dirname of `model_json` with name `model.weights.bin\',\n    action=\'store\',\n    required=False,\n    dest=\'weights_path_prefix\',\n    type=str,\n    default=None)\n\nparser.add_argument(\n    \'--model_tflite\',\n    help=\'Converted tflite model file\',\n    action=\'store\',\n    required=False,\n    dest=\'model_tflite\',\n    type=str,\n    default=\'model.tflite\')\n\nargs = parser.parse_args()\nparser.print_help()\nprint(\'input args: \', args)\n\ntry:\n  converter = ModelConverter(args.config_json_path, args.weights_path_prefix,\n                             args.model_tflite)\n\n  converter.convert()\n\nexcept ValueError as e:\n  print(traceback.format_exc())\n  print(\'Error occurred while converting\')\n'"
lite/examples/gesture_classification/ml/converter.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\n\nimport tensorflow.compat.v1 as tf\nfrom keras import Model, Input\nfrom keras.applications import MobileNet\nfrom tensorflowjs.converters import load_keras_model\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ModelConverter:\n  """"""\n    Creates a ModelConverter class from a TensorFlow.js model file.\n\n    Args: :param config_json_path: Full filepath of weights manifest file\n    containing the model architecture. :param weights_path_prefix: Full filepath\n    to the directory in which the weights binaries exist. :param\n    tflite_model_file: Name of the TFLite FlatBuffer file to be exported.\n    :return: ModelConverter class.\n  """"""\n\n  def __init__(self, config_json_path, weights_path_prefix, tflite_model_file):\n    self.config_json_path = config_json_path\n    self.weights_path_prefix = weights_path_prefix\n    self.tflite_model_file = tflite_model_file\n    self.keras_model_file = \'merged.h5\'\n\n    # MobileNet Options\n    self.input_node_name = \'the_input\'\n    self.image_size = 224\n    self.alpha = 0.25\n    self.depth_multiplier = 1\n    self._input_shape = (1, self.image_size, self.image_size, 3)\n    self.depthwise_conv_layer = \'conv_pw_13_relu\'\n\n  def convert(self):\n    self.save_keras_model()\n    self._deserialize_tflite_from_keras()\n    logger.info(\'The TFLite model has been generated\')\n    self._purge()\n\n  def save_keras_model(self):\n    top_model = load_keras_model(\n        self.config_json_path,\n        self.weights_path_prefix,\n        weights_data_buffers=None,\n        load_weights=True,\n        use_unique_name_scope=True)\n\n    base_model = self.get_base_model()\n    merged_model = self.merge(base_model, top_model)\n    merged_model.save(self.keras_model_file)\n\n    logger.info(\'The merged Keras HDF5 model has been saved as {}\'.format(\n        self.keras_model_file))\n\n  def merge(self, base_model, top_model):\n    """"""\n        Merges base model with the classification block\n        :return:  Returns the merged Keras model\n        """"""\n    logger.info(\'Initializing model...\')\n\n    layer = base_model.get_layer(self.depthwise_conv_layer)\n    model = Model(inputs=base_model.input, outputs=top_model(layer.output))\n    logger.info(\'Model created.\')\n\n    return model\n\n  def get_base_model(self):\n    """"""\n        Builds MobileNet with the default parameters\n        :return:  Returns the base MobileNet model\n        """"""\n    input_tensor = Input(shape=self._input_shape[1:], name=self.input_node_name)\n    base_model = MobileNet(\n        input_shape=self._input_shape[1:],\n        alpha=self.alpha,\n        depth_multiplier=self.depth_multiplier,\n        input_tensor=input_tensor,\n        include_top=False)\n    return base_model\n\n  def _deserialize_tflite_from_keras(self):\n    converter = tf.lite.TFLiteConverter.from_keras_model_file(\n        self.keras_model_file)\n    tflite_model = converter.convert()\n\n    with open(self.tflite_model_file, \'wb\') as file:\n      file.write(tflite_model)\n\n  def _purge(self):\n    logger.info(\'Cleaning up Keras model\')\n    os.remove(self.keras_model_file)\n'"
lite/examples/image_classification/metadata/metadata_writer_for_image_classifier.py,1,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Writes metadata and label file to the image classifier models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nimport tensorflow as tf\n\nimport flatbuffers\n# pylint: disable=g-direct-tensorflow-import\nfrom tflite_support import metadata as _metadata\nfrom tflite_support import metadata_schema_py_generated as _metadata_fb\n# pylint: enable=g-direct-tensorflow-import\n\nFLAGS = flags.FLAGS\n\n\ndef define_flags():\n  flags.DEFINE_string(""model_file"", None,\n                      ""Path and file name to the TFLite model file."")\n  flags.DEFINE_string(""label_file"", None, ""Path to the label file."")\n  flags.DEFINE_string(""export_directory"", None,\n                      ""Path to save the TFLite model files with metadata."")\n  flags.mark_flag_as_required(""model_file"")\n  flags.mark_flag_as_required(""label_file"")\n  flags.mark_flag_as_required(""export_directory"")\n\n\nclass ModelSpecificInfo(object):\n  """"""Holds information that is specificly tied to an image classifier.""""""\n\n  def __init__(self, name, version, image_width, image_height, image_min,\n               image_max, mean, std, num_classes):\n    self.name = name\n    self.version = version\n    self.image_width = image_width\n    self.image_height = image_height\n    self.image_min = image_min\n    self.image_max = image_max\n    self.mean = mean\n    self.std = std\n    self.num_classes = num_classes\n\n\n_MODEL_INFO = {\n    ""mobilenet_v1_0.75_160_quantized.tflite"":\n        ModelSpecificInfo(\n            name=""MobileNetV1 image classifier"",\n            version=""v1"",\n            image_width=160,\n            image_height=160,\n            image_min=0,\n            image_max=255,\n            mean=[127.5],\n            std=[127.5],\n            num_classes=1001)\n}\n\n\nclass MetadataPopulatorForImageClassifier(object):\n  """"""Populates the metadata for an image classifier.""""""\n\n  def __init__(self, model_file, model_info, label_file_path):\n    self.model_file = model_file\n    self.model_info = model_info\n    self.label_file_path = label_file_path\n    self.metadata_buf = None\n\n  def populate(self):\n    """"""Creates metadata and then populates it for an image classifier.""""""\n    self._create_metadata()\n    self._populate_metadata()\n\n  def _create_metadata(self):\n    """"""Creates the metadata for an image classifier.""""""\n\n    # Creates model info.\n    model_meta = _metadata_fb.ModelMetadataT()\n    model_meta.name = self.model_info.name\n    model_meta.description = (""Identify the most prominent object in the ""\n                              ""image from a set of %d categories."" %\n                              self.model_info.num_classes)\n    model_meta.version = self.model_info.version\n    model_meta.author = ""TensorFlow""\n    model_meta.license = (""Apache License. Version 2.0 ""\n                          ""http://www.apache.org/licenses/LICENSE-2.0."")\n\n    # Creates input info.\n    input_meta = _metadata_fb.TensorMetadataT()\n    input_meta.name = ""image""\n    input_meta.description = (\n        ""Input image to be classified. The expected image is {0} x {1}, with ""\n        ""three channels (red, blue, and green) per pixel. Each value in the ""\n        ""tensor is a single byte between {2} and {3}."".format(\n            self.model_info.image_width, self.model_info.image_height,\n            self.model_info.image_min, self.model_info.image_max))\n    input_meta.content = _metadata_fb.ContentT()\n    input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n    input_meta.content.contentProperties.colorSpace = (\n        _metadata_fb.ColorSpaceType.RGB)\n    input_meta.content.contentPropertiesType = (\n        _metadata_fb.ContentProperties.ImageProperties)\n    input_normalization = _metadata_fb.ProcessUnitT()\n    input_normalization.optionsType = (\n        _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n    input_normalization.options = _metadata_fb.NormalizationOptionsT()\n    input_normalization.options.mean = self.model_info.mean\n    input_normalization.options.std = self.model_info.std\n    input_meta.processUnits = [input_normalization]\n    input_stats = _metadata_fb.StatsT()\n    input_stats.max = [self.model_info.image_max]\n    input_stats.min = [self.model_info.image_min]\n    input_meta.stats = input_stats\n\n    # Creates output info.\n    output_meta = _metadata_fb.TensorMetadataT()\n    output_meta.name = ""probability""\n    output_meta.description = ""Probabilities of the %d labels respectively."" % self.model_info.num_classes\n    output_meta.content = _metadata_fb.ContentT()\n    output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()\n    output_meta.content.contentPropertiesType = (\n        _metadata_fb.ContentProperties.FeatureProperties)\n    output_stats = _metadata_fb.StatsT()\n    output_stats.max = [1.0]\n    output_stats.min = [0.0]\n    output_meta.stats = output_stats\n    label_file = _metadata_fb.AssociatedFileT()\n    label_file.name = os.path.basename(self.label_file_path)\n    label_file.description = ""Labels for objects that the model can recognize.""\n    label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\n    output_meta.associatedFiles = [label_file]\n\n    # Creates subgraph info.\n    subgraph = _metadata_fb.SubGraphMetadataT()\n    subgraph.inputTensorMetadata = [input_meta]\n    subgraph.outputTensorMetadata = [output_meta]\n    model_meta.subgraphMetadata = [subgraph]\n\n    b = flatbuffers.Builder(0)\n    b.Finish(\n        model_meta.Pack(b),\n        _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n    self.metadata_buf = b.Output()\n\n  def _populate_metadata(self):\n    """"""Populates metadata and label file to the model file.""""""\n    populator = _metadata.MetadataPopulator.with_model_file(self.model_file)\n    populator.load_metadata_buffer(self.metadata_buf)\n    populator.load_associated_files([self.label_file_path])\n    populator.populate()\n\n\ndef main(_):\n  model_file = FLAGS.model_file\n  model_basename = os.path.basename(model_file)\n  if model_basename not in _MODEL_INFO:\n    raise ValueError(\n        ""The model info for, {0}, is not defined yet."".format(model_basename))\n\n  export_model_path = os.path.join(FLAGS.export_directory, model_basename)\n\n  # Copies model_file to export_path.\n  tf.io.gfile.copy(model_file, export_model_path, overwrite=True)\n\n  # Generate the metadata objects and put them in the model file\n  populator = MetadataPopulatorForImageClassifier(\n      export_model_path, _MODEL_INFO.get(model_basename), FLAGS.label_file)\n  populator.populate()\n\n  # Validate the output model file by reading the metadata and produce\n  # a json file with the metadata under the export path\n  displayer = _metadata.MetadataDisplayer.with_model_file(export_model_path)\n  export_json_file = os.path.join(FLAGS.export_directory,\n                                  os.path.splitext(model_basename)[0] + "".json"")\n  json_file = displayer.get_metadata_json()\n  with open(export_json_file, ""w"") as f:\n    f.write(json_file)\n\n  print(""Finished populating metadata and associated file to the model:"")\n  print(model_file)\n  print(""The metadata json file has been saved to:"")\n  print(export_json_file)\n  print(""The associated file that has been been packed to the model is:"")\n  print(displayer.get_packed_associated_file_list())\n\n\nif __name__ == ""__main__"":\n  define_flags()\n  app.run(main)\n'"
lite/examples/image_classification/raspberry_pi/classify_picamera.py,0,"b'# python3\n#\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example using TF Lite to classify objects with the Raspberry Pi camera.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport io\nimport time\nimport numpy as np\nimport picamera\n\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\n\n\ndef load_labels(path):\n  with open(path, \'r\') as f:\n    return {i: line.strip() for i, line in enumerate(f.readlines())}\n\n\ndef set_input_tensor(interpreter, image):\n  tensor_index = interpreter.get_input_details()[0][\'index\']\n  input_tensor = interpreter.tensor(tensor_index)()[0]\n  input_tensor[:, :] = image\n\n\ndef classify_image(interpreter, image, top_k=1):\n  """"""Returns a sorted array of classification results.""""""\n  set_input_tensor(interpreter, image)\n  interpreter.invoke()\n  output_details = interpreter.get_output_details()[0]\n  output = np.squeeze(interpreter.get_tensor(output_details[\'index\']))\n\n  # If the model is quantized (uint8 data), then dequantize the results\n  if output_details[\'dtype\'] == np.uint8:\n    scale, zero_point = output_details[\'quantization\']\n    output = scale * (output - zero_point)\n\n  ordered = np.argpartition(-output, top_k)\n  return [(i, output[i]) for i in ordered[:top_k]]\n\n\ndef main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--model\', help=\'File path of .tflite file.\', required=True)\n  parser.add_argument(\n      \'--labels\', help=\'File path of labels file.\', required=True)\n  args = parser.parse_args()\n\n  labels = load_labels(args.labels)\n\n  interpreter = Interpreter(args.model)\n  interpreter.allocate_tensors()\n  _, height, width, _ = interpreter.get_input_details()[0][\'shape\']\n\n  with picamera.PiCamera(resolution=(640, 480), framerate=30) as camera:\n    camera.start_preview()\n    try:\n      stream = io.BytesIO()\n      for _ in camera.capture_continuous(\n          stream, format=\'jpeg\', use_video_port=True):\n        stream.seek(0)\n        image = Image.open(stream).convert(\'RGB\').resize((width, height),\n                                                         Image.ANTIALIAS)\n        start_time = time.time()\n        results = classify_image(interpreter, image)\n        elapsed_ms = (time.time() - start_time) * 1000\n        label_id, prob = results[0]\n        stream.seek(0)\n        stream.truncate()\n        camera.annotate_text = \'%s %.2f\\n%.1fms\' % (labels[label_id], prob,\n                                                    elapsed_ms)\n    finally:\n      camera.stop_preview()\n\n\nif __name__ == \'__main__\':\n  main()\n'"
lite/examples/model_personalization/converter/setup.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Setuptools configuration.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nsetup(\n    name=\'tfltransfer\',\n    version=\'0.1\',\n    packages=find_packages(),\n    install_requires=[\n        \'tensorflow==2.0.0rc0\',\n        \'Pillow>=6.2.2,<7.0\',\n        \'scipy>=1.3.0,<2.0\',\n    ],\n    entry_points={\n        \'console_scripts\': [\n            \'tflite-transfer-convert = tfltransfer.tflite_transfer_convert:main\',\n        ],\n    },\n)\n'"
lite/examples/object_detection/raspberry_pi/annotation.py,0,"b'# python3\n#\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""An annotation library that draws overlays on the Pi camera preview.\n\nAnnotations include bounding boxes and text overlays.\nAnnotations support partial opacity, however only with respect to the content in\nthe preview. A transparent fill value will cover up previously drawn overlay\nunder it, but not the camera content under it. A color of None can be given,\nwhich will then not cover up overlay content drawn under the region.\nNote: Overlays do not persist through to the storage layer so images saved from\nthe camera, will not contain overlays.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom PIL import Image\nfrom PIL import ImageDraw\n\n\ndef _round_up(value, n):\n  """"""Rounds up the given value to the next number divisible by n.\n\n  Args:\n    value: int to be rounded up.\n    n: the number that should be divisible into value.\n\n  Returns:\n    the result of value rounded up to the next multiple of n.\n  """"""\n  return n * ((value + (n - 1)) // n)\n\n\ndef _round_buffer_dims(dims):\n  """"""Appropriately rounds the given dimensions for image overlaying.\n\n  As per the PiCamera.add_overlay documentation, the source data must have a\n  width rounded up to the nearest multiple of 32, and the height rounded up to\n  the nearest multiple of 16. This does that for the given image dimensions.\n\n  Args:\n    dims: image dimensions.\n\n  Returns:\n    the rounded-up dimensions in a tuple.\n  """"""\n  width, height = dims\n  return _round_up(width, 32), _round_up(height, 16)\n\n\nclass Annotator:\n  """"""Utility for managing annotations on the camera preview.""""""\n\n  def __init__(self, camera, default_color=None):\n    """"""Initializes Annotator parameters.\n\n    Args:\n      camera: picamera.PiCamera camera object to overlay on top of.\n      default_color: PIL.ImageColor (with alpha) default for the drawn content.\n    """"""\n    self._camera = camera\n    self._dims = camera.resolution\n    self._buffer_dims = _round_buffer_dims(self._dims)\n    self._buffer = Image.new(\'RGBA\', self._buffer_dims)\n    self._overlay = None\n    self._draw = ImageDraw.Draw(self._buffer)\n    self._default_color = default_color or (0xFF, 0, 0, 0xFF)\n\n  def update(self):\n    """"""Draws any changes to the image buffer onto the overlay.""""""\n    # For some reason, simply updating the current overlay causes\n    # PiCameraMMALError every time we update. To avoid that, we create a new\n    # overlay each time we want to update.\n    # We use a temp overlay object because if we remove the current overlay\n    # first, it causes flickering (the overlay visibly disappears for a moment).\n    temp_overlay = self._camera.add_overlay(\n        self._buffer.tobytes(), format=\'rgba\', layer=3, size=self._buffer_dims)\n    if self._overlay is not None:\n      self._camera.remove_overlay(self._overlay)\n    self._overlay = temp_overlay\n    self._overlay.update(self._buffer.tobytes())\n\n  def clear(self):\n    """"""Clears the contents of the overlay, leaving only the plain background.""""""\n    self._draw.rectangle((0, 0) + self._dims, fill=(0, 0, 0, 0x00))\n\n  def bounding_box(self, rect, outline=None, fill=None):\n    """"""Draws a bounding box around the specified rectangle.\n\n    Args:\n      rect: (x1, y1, x2, y2) rectangle to be drawn, where (x1, y1) and (x2, y2)\n        are opposite corners of the desired rectangle.\n      outline: PIL.ImageColor with which to draw the outline (defaults to the\n        Annotator default_color).\n      fill: PIL.ImageColor with which to fill the rectangle (defaults to None,\n        which will *not* cover up drawings under the region).\n    """"""\n    outline = outline or self._default_color\n    self._draw.rectangle(rect, fill=fill, outline=outline)\n\n  def text(self, location, text, color=None):\n    """"""Draws the given text at the given location.\n\n    Args:\n      location: (x, y) point at which to draw the text (upper left corner).\n      text: string to be drawn.\n      color: PIL.ImageColor to draw the string in (defaults to the Annotator\n        default_color).\n    """"""\n    color = color or self._default_color\n    self._draw.text(location, text, fill=color)\n'"
lite/examples/object_detection/raspberry_pi/detect_picamera.py,0,"b'# python3\n#\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example using TF Lite to detect objects with the Raspberry Pi camera.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport io\nimport re\nimport time\n\nfrom annotation import Annotator\n\nimport numpy as np\nimport picamera\n\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\n\nCAMERA_WIDTH = 640\nCAMERA_HEIGHT = 480\n\n\ndef load_labels(path):\n  """"""Loads the labels file. Supports files with or without index numbers.""""""\n  with open(path, \'r\', encoding=\'utf-8\') as f:\n    lines = f.readlines()\n    labels = {}\n    for row_number, content in enumerate(lines):\n      pair = re.split(r\'[:\\s]+\', content.strip(), maxsplit=1)\n      if len(pair) == 2 and pair[0].strip().isdigit():\n        labels[int(pair[0])] = pair[1].strip()\n      else:\n        labels[row_number] = pair[0].strip()\n  return labels\n\n\ndef set_input_tensor(interpreter, image):\n  """"""Sets the input tensor.""""""\n  tensor_index = interpreter.get_input_details()[0][\'index\']\n  input_tensor = interpreter.tensor(tensor_index)()[0]\n  input_tensor[:, :] = image\n\n\ndef get_output_tensor(interpreter, index):\n  """"""Returns the output tensor at the given index.""""""\n  output_details = interpreter.get_output_details()[index]\n  tensor = np.squeeze(interpreter.get_tensor(output_details[\'index\']))\n  return tensor\n\n\ndef detect_objects(interpreter, image, threshold):\n  """"""Returns a list of detection results, each a dictionary of object info.""""""\n  set_input_tensor(interpreter, image)\n  interpreter.invoke()\n\n  # Get all output details\n  boxes = get_output_tensor(interpreter, 0)\n  classes = get_output_tensor(interpreter, 1)\n  scores = get_output_tensor(interpreter, 2)\n  count = int(get_output_tensor(interpreter, 3))\n\n  results = []\n  for i in range(count):\n    if scores[i] >= threshold:\n      result = {\n          \'bounding_box\': boxes[i],\n          \'class_id\': classes[i],\n          \'score\': scores[i]\n      }\n      results.append(result)\n  return results\n\n\ndef annotate_objects(annotator, results, labels):\n  """"""Draws the bounding box and label for each object in the results.""""""\n  for obj in results:\n    # Convert the bounding box figures from relative coordinates\n    # to absolute coordinates based on the original resolution\n    ymin, xmin, ymax, xmax = obj[\'bounding_box\']\n    xmin = int(xmin * CAMERA_WIDTH)\n    xmax = int(xmax * CAMERA_WIDTH)\n    ymin = int(ymin * CAMERA_HEIGHT)\n    ymax = int(ymax * CAMERA_HEIGHT)\n\n    # Overlay the box, label, and score on the camera preview\n    annotator.bounding_box([xmin, ymin, xmax, ymax])\n    annotator.text([xmin, ymin],\n                   \'%s\\n%.2f\' % (labels[obj[\'class_id\']], obj[\'score\']))\n\n\ndef main():\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--model\', help=\'File path of .tflite file.\', required=True)\n  parser.add_argument(\n      \'--labels\', help=\'File path of labels file.\', required=True)\n  parser.add_argument(\n      \'--threshold\',\n      help=\'Score threshold for detected objects.\',\n      required=False,\n      type=float,\n      default=0.4)\n  args = parser.parse_args()\n\n  labels = load_labels(args.labels)\n  interpreter = Interpreter(args.model)\n  interpreter.allocate_tensors()\n  _, input_height, input_width, _ = interpreter.get_input_details()[0][\'shape\']\n\n  with picamera.PiCamera(\n      resolution=(CAMERA_WIDTH, CAMERA_HEIGHT), framerate=30) as camera:\n    camera.start_preview()\n    try:\n      stream = io.BytesIO()\n      annotator = Annotator(camera)\n      for _ in camera.capture_continuous(\n          stream, format=\'jpeg\', use_video_port=True):\n        stream.seek(0)\n        image = Image.open(stream).convert(\'RGB\').resize(\n            (input_width, input_height), Image.ANTIALIAS)\n        start_time = time.monotonic()\n        results = detect_objects(interpreter, image, args.threshold)\n        elapsed_ms = (time.monotonic() - start_time) * 1000\n\n        annotator.clear()\n        annotate_objects(annotator, results, labels)\n        annotator.text([5, 0], \'%.1fms\' % (elapsed_ms))\n        annotator.update()\n\n        stream.seek(0)\n        stream.truncate()\n\n    finally:\n      camera.stop_preview()\n\n\nif __name__ == \'__main__\':\n  main()\n'"
lite/examples/speech_commands/ml/callbacks.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom pandas_ml import ConfusionMatrix\nfrom keras.callbacks import Callback\n\n\ndef log_loss(y_true, y_pred, eps=1e-12):\n  y_pred = np.clip(y_pred, eps, 1. - eps)\n  ce = -(np.sum(y_true * np.log(y_pred), axis=1))\n  mce = ce.mean()\n  return mce\n\n\nclass ConfusionMatrixCallback(Callback):\n\n  def __init__(self, validation_data, validation_steps, wanted_words, all_words,\n               label2int):\n    self.validation_data = validation_data\n    self.validation_steps = validation_steps\n    self.wanted_words = wanted_words\n    self.all_words = all_words\n    self.label2int = label2int\n    self.int2label = {v: k for k, v in label2int.items()}\n    with open(\'confusion_matrix.txt\', \'w\'):\n      pass\n    with open(\'wanted_confusion_matrix.txt\', \'w\'):\n      pass\n\n  def accuracies(self, confusion_val):\n    accuracies = []\n    for i in range(confusion_val.shape[0]):\n      num = confusion_val[i, :].sum()\n      if num:\n        accuracies.append(confusion_val[i, i] / num)\n      else:\n        accuracies.append(0.0)\n    accuracies = np.float32(accuracies)\n    return accuracies\n\n  def accuracy(self, confusion_val):\n    num_correct = 0\n    for i in range(confusion_val.shape[0]):\n      num_correct += confusion_val[i, i]\n    accuracy = float(num_correct) / confusion_val.sum()\n    return accuracy\n\n  def on_epoch_end(self, epoch, logs=None):\n    y_true, y_pred = [], []\n    for i in range(self.validation_steps):\n      X_batch, y_true_batch = next(self.validation_data)\n      y_pred_batch = self.model.predict(X_batch)\n\n      y_true.extend(y_true_batch)\n      y_pred.extend(y_pred_batch)\n\n    y_true = np.float32(y_true)\n    y_pred = np.float32(y_pred)\n    val_loss = log_loss(y_true, y_pred)\n    # map integer labels to strings\n    y_true = list(y_true.argmax(axis=-1))\n    y_pred = list(y_pred.argmax(axis=-1))\n    y_true = [self.int2label[y] for y in y_true]\n    y_pred = [self.int2label[y] for y in y_pred]\n    confusion = ConfusionMatrix(y_true, y_pred)\n    accs = self.accuracies(confusion._df_confusion.values)\n    acc = self.accuracy(confusion._df_confusion.values)\n    # same for wanted words\n    y_true = [y if y in self.wanted_words else \'_unknown_\' for y in y_true]\n    y_pred = [y if y in self.wanted_words else \'_unknown_\' for y in y_pred]\n    wanted_words_confusion = ConfusionMatrix(y_true, y_pred)\n    wanted_accs = self.accuracies(wanted_words_confusion._df_confusion.values)\n    acc_line = (\'\\n[%03d]: val_categorical_accuracy: %.2f, \'\n                \'val_mean_categorical_accuracy_wanted: %.2f\') % (\n                    epoch, acc, wanted_accs.mean())  # noqa\n    with open(\'confusion_matrix.txt\', \'a\') as f:\n      f.write(\'%s\\n\' % acc_line)\n      f.write(confusion.to_dataframe().to_string())\n\n    with open(\'wanted_confusion_matrix.txt\', \'a\') as f:\n      f.write(\'%s\\n\' % acc_line)\n      f.write(wanted_words_confusion.to_dataframe().to_string())\n\n    logs[\'val_loss\'] = val_loss\n    logs[\'val_categorical_accuracy\'] = acc\n    logs[\'val_mean_categorical_accuracy_all\'] = accs.mean()\n    logs[\'val_mean_categorical_accuracy_wanted\'] = wanted_accs.mean()\n'"
lite/examples/speech_commands/ml/classes.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nfrom generator import prepare_words_list\n\n\ndef get_classes(wanted_only=False):\n  if wanted_only:\n    classes = \'stop down off right up go on yes left no\'\n    classes = classes.split(\' \')\n    assert len(classes) == 10\n  else:\n    classes = (\'sheila nine stop bed four six down bird marvin cat off right \'\n               \'seven eight up three happy go zero on wow dog yes five one tree\'\n               \' house two left no\')  # noqa\n    classes = classes.split(\' \')\n    assert len(classes) == 30\n  return classes\n\n\ndef get_int2label(wanted_only=False, extend_reversed=False):\n  classes = get_classes(\n      wanted_only=wanted_only, extend_reversed=extend_reversed)\n  classes = prepare_words_list(classes)\n  int2label = {i: l for i, l in enumerate(classes)}\n  int2label = OrderedDict(sorted(int2label.items(), key=lambda x: x[0]))\n  return int2label\n\n\ndef get_label2int(wanted_only=False, extend_reversed=False):\n  classes = get_classes(\n      wanted_only=wanted_only, extend_reversed=extend_reversed)\n  classes = prepare_words_list(classes)\n  label2int = {l: i for i, l in enumerate(classes)}\n  label2int = OrderedDict(sorted(label2int.items(), key=lambda x: x[1]))\n  return label2int\n'"
lite/examples/speech_commands/ml/download.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport wget\nimport tarfile\n\nfrom shutil import rmtree\n\nDATASET_URL = \'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\'\nARCHIVE = os.path.basename(DATASET_URL)\n\nwget.download(DATASET_URL)\n\nif os.path.exists(\'data\'):\n  rmtree(\'data\')\n\nos.makedirs(\'data/train\')\n\nwith tarfile.open(ARCHIVE, \'r:gz\') as tar:\n  tar.extractall(path=\'data/train\')\n\nos.remove(ARCHIVE)\n'"
lite/examples/speech_commands/ml/generator.py,46,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\nimport math\nimport os.path\nimport random\nimport re\nimport sys\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow.compat.v1 as tf\n\nfrom utils import tf_roll\n\nMAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\nSILENCE_LABEL = \'_silence_\'\nSILENCE_INDEX = 0\nUNKNOWN_WORD_LABEL = \'_unknown_\'\nUNKNOWN_WORD_INDEX = 1\nBACKGROUND_NOISE_DIR_NAME = \'_background_noise_\'\nRANDOM_SEED = 59185\n\n\ndef prepare_words_list(wanted_words):\n  """"""Prepends common tokens to the custom word list.""""""\n  return [SILENCE_LABEL, UNKNOWN_WORD_LABEL] + wanted_words\n\n\ndef which_set(filename, validation_percentage, testing_percentage):\n  """"""Determines which data partition the file should belong to.""""""\n  dir_name = os.path.basename(os.path.dirname(filename))\n  if dir_name == \'unknown_unknown\':\n    return \'training\'\n\n  base_name = os.path.basename(filename)\n  hash_name = re.sub(r\'_nohash_.*$\', \'\', base_name)\n\n  hash_name_hashed = hashlib.sha1(tf.compat.as_bytes(hash_name)).hexdigest()\n  percentage_hash = ((int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1))\n                     * (100.0 / MAX_NUM_WAVS_PER_CLASS))\n  if percentage_hash < validation_percentage:\n    result = \'validation\'\n  elif percentage_hash < (testing_percentage + validation_percentage):\n    result = \'testing\'\n  else:\n    result = \'training\'\n  return result\n\n\ndef load_wav_file(filename):\n  """"""Loads an audio file and returns a float PCM-encoded array of samples.""""""\n  with tf.Session(graph=tf.Graph()) as sess:\n    wav_filename_placeholder = tf.placeholder(tf.string, [])\n    wav_loader = tf.io.read_file(wav_filename_placeholder)\n    wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n    return sess.run(\n        wav_decoder, feed_dict={\n            wav_filename_placeholder: filename\n        }).audio.flatten()\n\n\ndef save_wav_file(filename, wav_data, sample_rate):\n  """"""Saves audio sample data to a .wav audio file.""""""\n  with tf.Session(graph=tf.Graph()) as sess:\n    wav_filename_placeholder = tf.placeholder(tf.string, [])\n    sample_rate_placeholder = tf.placeholder(tf.int32, [])\n    wav_data_placeholder = tf.placeholder(tf.float32, [None, 1])\n    wav_encoder = tf.audio.encode_wav(wav_data_placeholder,\n                                      sample_rate_placeholder)\n    wav_saver = tf.io.write_file(wav_filename_placeholder, wav_encoder)\n    sess.run(\n        wav_saver,\n        feed_dict={\n            wav_filename_placeholder: filename,\n            sample_rate_placeholder: sample_rate,\n            wav_data_placeholder: np.reshape(wav_data, (-1, 1))\n        })\n\n\nclass AudioProcessor(object):\n  """"""Handles loading, partitioning, and preparing audio training data.""""""\n\n  def __init__(self,\n               data_dirs,\n               silence_percentage,\n               unknown_percentage,\n               wanted_words,\n               validation_percentage,\n               testing_percentage,\n               model_settings,\n               output_representation=False):\n    self.data_dirs = data_dirs\n    assert output_representation in {\'raw\', \'spec\', \'mfcc\', \'mfcc_and_raw\'}\n    self.output_representation = output_representation\n    self.model_settings = model_settings\n    for data_dir in self.data_dirs:\n      self.maybe_download_and_extract_dataset(data_dir)\n    self.prepare_data_index(silence_percentage, unknown_percentage,\n                            wanted_words, validation_percentage,\n                            testing_percentage)\n    self.prepare_background_data()\n    self.prepare_processing_graph(model_settings)\n\n  def maybe_download_and_extract_dataset(self, data_dir):\n    if not os.path.exists(data_dir):\n      print(\'Please download the dataset!\')\n      sys.exit(0)\n\n  def prepare_data_index(self, silence_percentage, unknown_percentage,\n                         wanted_words, validation_percentage,\n                         testing_percentage):\n    """"""Prepares a list of the samples organized by set and label.""""""\n    random.seed(RANDOM_SEED)\n    wanted_words_index = {}\n    for index, wanted_word in enumerate(wanted_words):\n      wanted_words_index[wanted_word] = index + 2\n    self.data_index = {\'validation\': [], \'testing\': [], \'training\': []}\n    unknown_index = {\'validation\': [], \'testing\': [], \'training\': []}\n    all_words = {}\n    # Look through all the subfolders to find audio samples\n    for data_dir in self.data_dirs:\n      search_path = os.path.join(data_dir, \'*\', \'*.wav\')\n      for wav_path in tf.io.gfile.glob(search_path):\n        word = re.search(\'.*/([^/]+)/.*.wav\', wav_path).group(1).lower()\n        # Treat the \'_background_noise_\' folder as a special case,\n        # since we expect it to contain long audio samples we mix in\n        # to improve training.\n        if word == BACKGROUND_NOISE_DIR_NAME:\n          continue\n        all_words[word] = True\n        set_index = which_set(wav_path, validation_percentage,\n                              testing_percentage)\n        # If it\'s a known class, store its detail, otherwise add it to the list\n        # we\'ll use to train the unknown label.\n        if word in wanted_words_index:\n          self.data_index[set_index].append({\'label\': word, \'file\': wav_path})\n        else:\n          unknown_index[set_index].append({\'label\': word, \'file\': wav_path})\n      if not all_words:\n        raise Exception(\'No .wavs found at \' + search_path)\n      for index, wanted_word in enumerate(wanted_words):\n        if wanted_word not in all_words:\n          raise Exception(\'Expected to find \' + wanted_word +\n                          \' in labels but only found \' +\n                          \', \'.join(all_words.keys()))\n    # We need an arbitrary file to load as the input for the silence samples.\n    # It\'s multiplied by zero later, so the content doesn\'t matter.\n    silence_wav_path = self.data_index[\'training\'][0][\'file\']\n    for set_index in [\'validation\', \'testing\', \'training\']:\n      set_size = len(self.data_index[set_index])\n      silence_size = int(math.ceil(set_size * silence_percentage / 100))\n      for _ in range(silence_size):\n        self.data_index[set_index].append({\n            \'label\': SILENCE_LABEL,\n            \'file\': silence_wav_path\n        })\n      # Pick some unknowns to add to each partition of the data set.\n      random.shuffle(unknown_index[set_index])\n      unknown_size = int(math.ceil(set_size * unknown_percentage / 100))\n      self.data_index[set_index].extend(unknown_index[set_index][:unknown_size])\n    # Make sure the ordering is random.\n    for set_index in [\'validation\', \'testing\', \'training\']:\n      # not really needed since the indices are chosen by random\n      random.shuffle(self.data_index[set_index])\n    # Prepare the rest of the result data structure.\n    self.words_list = prepare_words_list(wanted_words)\n    self.word_to_index = {}\n    for word in all_words:\n      if word in wanted_words_index:\n        self.word_to_index[word] = wanted_words_index[word]\n      else:\n        self.word_to_index[word] = UNKNOWN_WORD_INDEX\n    self.word_to_index[SILENCE_LABEL] = SILENCE_INDEX\n\n  def prepare_background_data(self):\n    """"""Searches a folder for background noise audio and loads it into memory.""""""\n    self.background_data = []\n    background_dir = os.path.join(self.data_dirs[0], BACKGROUND_NOISE_DIR_NAME)\n    if not os.path.exists(background_dir):\n      return self.background_data\n    with tf.Session(graph=tf.Graph()) as sess:\n      wav_filename_placeholder = tf.placeholder(tf.string, [])\n      wav_loader = tf.io.read_file(wav_filename_placeholder)\n      wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n      search_path = os.path.join(self.data_dirs[0], BACKGROUND_NOISE_DIR_NAME,\n                                 \'*.wav\')\n      for wav_path in tf.io.gfile.glob(search_path):\n        wav_data = sess.run(\n            wav_decoder, feed_dict={\n                wav_filename_placeholder: wav_path\n            }).audio.flatten()\n        self.background_data.append(wav_data)\n      if not self.background_data:\n        raise Exception(\'No background wav files were found in \' + search_path)\n\n  def prepare_processing_graph(self, model_settings):\n    """"""Builds a TensorFlow graph to apply the input distortions.""""""\n    desired_samples = model_settings[\'desired_samples\']\n    self.wav_filename_placeholder_ = tf.placeholder(\n        tf.string, [], name=\'filename\')\n    wav_loader = tf.io.read_file(self.wav_filename_placeholder_)\n    wav_decoder = tf.audio.decode_wav(\n        wav_loader, desired_channels=1, desired_samples=desired_samples)\n    # Allow the audio sample\'s volume to be adjusted.\n    self.foreground_volume_placeholder_ = tf.placeholder(\n        tf.float32, [], name=\'foreground_volme\')\n    scaled_foreground = tf.multiply(wav_decoder.audio,\n                                    self.foreground_volume_placeholder_)\n    # Shift the sample\'s start position, and pad any gaps with zeros.\n    self.time_shift_placeholder_ = tf.placeholder(tf.int32, name=\'timeshift\')\n    shifted_foreground = tf_roll(scaled_foreground,\n                                 self.time_shift_placeholder_)\n    # Mix in background noise.\n    self.background_data_placeholder_ = tf.placeholder(\n        tf.float32, [desired_samples, 1], name=\'background_data\')\n    self.background_volume_placeholder_ = tf.placeholder(\n        tf.float32, [], name=\'background_volume\')\n    background_mul = tf.multiply(self.background_data_placeholder_,\n                                 self.background_volume_placeholder_)\n    background_add = tf.add(background_mul, shifted_foreground)\n    # removed clipping: tf.clip_by_value(background_add, -1.0, 1.0)\n    self.background_clamp_ = background_add\n    self.background_clamp_ = tf.reshape(self.background_clamp_,\n                                        (1, model_settings[\'desired_samples\']))\n    # Run the spectrogram and MFCC ops to get a 2D \'fingerprint\' of the audio.\n    stfts = tf.signal.stft(\n        self.background_clamp_,\n        frame_length=model_settings[\'window_size_samples\'],\n        frame_step=model_settings[\'window_stride_samples\'],\n        fft_length=None)\n    self.spectrogram_ = tf.abs(stfts)\n    num_spectrogram_bins = self.spectrogram_.shape[-1].value\n    lower_edge_hertz, upper_edge_hertz = 80.0, 7600.0\n    linear_to_mel_weight_matrix = \\\n        tf.signal.linear_to_mel_weight_matrix(\n            model_settings[\'dct_coefficient_count\'],\n            num_spectrogram_bins, model_settings[\'sample_rate\'],\n            lower_edge_hertz, upper_edge_hertz)\n    mel_spectrograms = tf.tensordot(self.spectrogram_,\n                                    linear_to_mel_weight_matrix, 1)\n    mel_spectrograms.set_shape(self.spectrogram_.shape[:-1].concatenate(\n        linear_to_mel_weight_matrix.shape[-1:]))\n    log_mel_spectrograms = tf.log(mel_spectrograms + 1e-6)\n    self.mfcc_ = tf.signal.mfccs_from_log_mel_spectrograms(\n        log_mel_spectrograms)[:, :, :\n                              model_settings[\'num_log_mel_features\']]  # :13\n\n  def set_size(self, mode):\n    """"""Calculates the number of samples in the dataset partition.""""""\n    return len(self.data_index[mode])\n\n  def get_data(self,\n               how_many,\n               offset,\n               background_frequency,\n               background_volume_range,\n               foreground_frequency,\n               foreground_volume_range,\n               time_shift_frequency,\n               time_shift_range,\n               mode,\n               sess,\n               flip_frequency=0.0,\n               silence_volume_range=0.0):\n    """"""Gather samples from the data set, applying transformations as needed.""""""\n    # Pick one of the partitions to choose samples from.\n    model_settings = self.model_settings\n    candidates = self.data_index[mode]\n    if how_many == -1:\n      sample_count = len(candidates)\n    else:\n      sample_count = max(0, min(how_many, len(candidates) - offset))\n    # Data and labels will be populated and returned.\n    if self.output_representation == \'raw\':\n      data_dim = model_settings[\'desired_samples\']\n    elif self.output_representation == \'spec\':\n      data_dim = model_settings[\'spectrogram_length\'] * model_settings[\n          \'spectrogram_frequencies\']\n    elif self.output_representation == \'mfcc\':\n      data_dim = model_settings[\'spectrogram_length\'] * \\\n                 model_settings[\'num_log_mel_features\']\n    elif self.output_representation == \'mfcc_and_raw\':\n      data_dim = model_settings[\'spectrogram_length\'] * \\\n                 model_settings[\'num_log_mel_features\']\n      raw_data = np.zeros((sample_count, model_settings[\'desired_samples\']))\n\n    data = np.zeros((sample_count, data_dim))\n    labels = np.zeros((sample_count, model_settings[\'label_count\']))\n    desired_samples = model_settings[\'desired_samples\']\n    use_background = self.background_data and (mode == \'training\')\n    pick_deterministically = (mode != \'training\')\n    # Use the processing graph we created earlier to repeatedly to generate the\n    # final output sample data we\'ll use in training.\n    for i in xrange(offset, offset + sample_count):\n      # Pick which audio sample to use.\n      if how_many == -1 or pick_deterministically:\n        sample_index = i\n        sample = candidates[sample_index]\n      else:\n        sample_index = np.random.randint(len(candidates))\n        sample = candidates[sample_index]\n\n      # If we\'re time shifting, set up the offset for this sample.\n      if np.random.uniform(0.0, 1.0) < time_shift_frequency:\n        time_shift = np.random.randint(time_shift_range[0],\n                                       time_shift_range[1] + 1)\n      else:\n        time_shift = 0\n      input_dict = {\n          self.wav_filename_placeholder_: sample[\'file\'],\n          self.time_shift_placeholder_: time_shift,\n      }\n      # Choose a section of background noise to mix in.\n      if use_background:\n        background_index = np.random.randint(len(self.background_data))\n        background_samples = self.background_data[background_index]\n        background_offset = np.random.randint(\n            0,\n            len(background_samples) - model_settings[\'desired_samples\'])\n        background_clipped = background_samples[background_offset:(\n            background_offset + desired_samples)]\n        background_reshaped = background_clipped.reshape([desired_samples, 1])\n        if np.random.uniform(0, 1) < background_frequency:\n          background_volume = np.random.uniform(0, background_volume_range)\n        else:\n          background_volume = 0.0\n          # silence class with all zeros is boring!\n          if sample[\'label\'] == SILENCE_LABEL and \\\n                  np.random.uniform(0, 1) < 0.9:\n            background_volume = np.random.uniform(0, silence_volume_range)\n      else:\n        background_reshaped = np.zeros([desired_samples, 1])\n        background_volume = 0.0\n      input_dict[self.background_data_placeholder_] = background_reshaped\n      input_dict[self.background_volume_placeholder_] = background_volume\n      # If we want silence, mute out the main sample but leave the background.\n      if sample[\'label\'] == SILENCE_LABEL:\n        input_dict[self.foreground_volume_placeholder_] = 0.0\n      else:\n        # Turn it up or down\n        foreground_volume = 1.0\n        if np.random.uniform(0, 1) < foreground_frequency:\n          foreground_volume = 1.0 + np.random.uniform(-foreground_volume_range,\n                                                      foreground_volume_range)\n        # flip sign\n        if np.random.uniform(0, 1) < flip_frequency:\n          foreground_volume *= -1.0\n        input_dict[self.foreground_volume_placeholder_] = foreground_volume\n\n      # Run the graph to produce the output audio.\n      if self.output_representation == \'raw\':\n        data[i - offset, :] = sess.run(\n            self.background_clamp_, feed_dict=input_dict).flatten()\n      elif self.output_representation == \'spec\':\n        data[i - offset, :] = sess.run(\n            self.spectrogram_, feed_dict=input_dict).flatten()\n      elif self.output_representation == \'mfcc\':\n        data[i - offset, :] = sess.run(\n            self.mfcc_, feed_dict=input_dict).flatten()\n      elif self.output_representation == \'mfcc_and_raw\':\n        raw_val, mfcc_val = sess.run([self.background_clamp_, self.mfcc_],\n                                     feed_dict=input_dict)\n        data[i - offset, :] = mfcc_val.flatten()\n        raw_data[i - offset, :] = raw_val.flatten()\n\n      label_index = self.word_to_index[sample[\'label\']]\n      labels[i - offset, label_index] = 1\n\n    if self.output_representation != \'mfcc_and_raw\':\n      return data, labels\n    else:\n      return [data, raw_data], labels\n\n  def get_unprocessed_data(self, how_many, model_settings, mode):\n    """"""Gets sample data without transformations.""""""\n    candidates = self.data_index[mode]\n    if how_many == -1:\n      sample_count = len(candidates)\n    else:\n      sample_count = how_many\n    desired_samples = model_settings[\'desired_samples\']\n    words_list = self.words_list\n    data = np.zeros((sample_count, desired_samples))\n    labels = []\n    with tf.Session(graph=tf.Graph()) as sess:\n      wav_filename_placeholder = tf.placeholder(tf.string, [], name=\'filename\')\n      wav_loader = tf.io.read_file(wav_filename_placeholder)\n      wav_decoder = tf.audio.decode_wav(\n          wav_loader, desired_channels=1, desired_samples=desired_samples)\n      foreground_volume_placeholder = tf.placeholder(\n          tf.float32, [], name=\'foreground_volume\')\n      scaled_foreground = tf.multiply(wav_decoder.audio,\n                                      foreground_volume_placeholder)\n      for i in range(sample_count):\n        if how_many == -1:\n          sample_index = i\n        else:\n          sample_index = np.random.randint(len(candidates))\n        sample = candidates[sample_index]\n        input_dict = {wav_filename_placeholder: sample[\'file\']}\n        if sample[\'label\'] == SILENCE_LABEL:\n          input_dict[foreground_volume_placeholder] = 0\n        else:\n          input_dict[foreground_volume_placeholder] = 1\n        data[i, :] = sess.run(scaled_foreground, feed_dict=input_dict).flatten()\n        label_index = self.word_to_index[sample[\'label\']]\n        labels.append(words_list[label_index])\n    return data, labels\n\n  def summary(self):\n    """"""Prints a summary of classes and label distributions.""""""\n    set_counts = {}\n    print(\'There are %d classes.\' % (len(self.word_to_index)))\n    print(""1%% <-> %d samples in \'training\'"" % int(\n        self.set_size(\'training\') / 100))\n    for set_index in [\'training\', \'validation\', \'testing\']:\n      counts = {k: 0 for k in sorted(self.word_to_index.keys())}\n      num_total = self.set_size(set_index)\n      for data_point in self.data_index[set_index]:\n        counts[data_point[\'label\']] += (1.0 / num_total) * 100.0\n      set_counts[set_index] = counts\n\n    print(\'%-13s%-6s%-6s%-6s\' % (\'\', \'Train\', \'Val\', \'Test\'))\n    for label_name in sorted(\n        self.word_to_index.keys(), key=self.word_to_index.get):\n      line = \'%02d %-12s: \' % (self.word_to_index[label_name], label_name)\n      for set_index in [\'training\', \'validation\', \'testing\']:\n        line += \'%.1f%% \' % (set_counts[set_index][label_name])\n      print(line)\n'"
lite/examples/speech_commands/ml/model.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport keras\nfrom keras.layers import *\nfrom keras.regularizers import l2\nfrom keras.models import Model\n\n\ndef preprocess(x):\n  x = (x + 0.8) / 7.0\n  x = K.clip(x, -5, 5)\n  return x\n\n\ndef preprocess_raw(x):\n  return x\n\n\nPreprocess = Lambda(preprocess)\n\nPreprocessRaw = Lambda(preprocess_raw)\n\n\ndef relu6(x):\n  return K.relu(x, max_value=6)\n\n\ndef conv_1d_time_stacked_model(input_size=16000, num_classes=11):\n  """""" Creates a 1D model for temporal data.\n\n  Note: Use only\n  with compute_mfcc = False (e.g. raw waveform data).\n  Args:\n    input_size: How big the input vector is.\n    num_classes: How many classes are to be recognized.\n\n  Returns:\n    Compiled keras model\n  """"""\n  input_layer = Input(shape=[input_size])\n  x = input_layer\n  x = Reshape([800, 20])(x)\n  x = PreprocessRaw(x)\n\n  def _reduce_conv(x, num_filters, k, strides=2, padding=\'valid\'):\n    x = Conv1D(\n        num_filters,\n        k,\n        padding=padding,\n        use_bias=False,\n        kernel_regularizer=l2(0.00001))(\n            x)\n    x = BatchNormalization()(x)\n    x = Activation(relu6)(x)\n    x = MaxPool1D(pool_size=3, strides=strides, padding=padding)(x)\n    return x\n\n  def _context_conv(x, num_filters, k, dilation_rate=1, padding=\'valid\'):\n    x = Conv1D(\n        num_filters,\n        k,\n        padding=padding,\n        dilation_rate=dilation_rate,\n        kernel_regularizer=l2(0.00001),\n        use_bias=False)(\n            x)\n    x = BatchNormalization()(x)\n    x = Activation(relu6)(x)\n    return x\n\n  x = _context_conv(x, 32, 1)\n  x = _reduce_conv(x, 48, 3)\n  x = _context_conv(x, 48, 3)\n  x = _reduce_conv(x, 96, 3)\n  x = _context_conv(x, 96, 3)\n  x = _reduce_conv(x, 128, 3)\n  x = _context_conv(x, 128, 3)\n  x = _reduce_conv(x, 160, 3)\n  x = _context_conv(x, 160, 3)\n  x = _reduce_conv(x, 192, 3)\n  x = _context_conv(x, 192, 3)\n  x = _reduce_conv(x, 256, 3)\n  x = _context_conv(x, 256, 3)\n\n  x = Dropout(0.3)(x)\n  x = Conv1D(num_classes, 5, activation=\'softmax\')(x)\n  x = Reshape([-1])(x)\n\n  model = Model(input_layer, x, name=\'conv_1d_time_stacked\')\n  model.compile(\n      optimizer=keras.optimizers.Adam(lr=3e-4),\n      loss=keras.losses.categorical_crossentropy,\n      metrics=[keras.metrics.categorical_accuracy])\n  return model\n\n\ndef speech_model(model_type, input_size, num_classes=11, *args, **kwargs):\n  if model_type == \'conv_1d_time_stacked\':\n    return conv_1d_time_stacked_model(input_size, num_classes)\n  else:\n    raise ValueError(\'Invalid model: %s\' % model_type)\n\n\ndef prepare_model_settings(label_count,\n                           sample_rate,\n                           clip_duration_ms,\n                           window_size_ms,\n                           window_stride_ms,\n                           dct_coefficient_count,\n                           num_log_mel_features,\n                           output_representation=\'raw\'):\n  """"""Calculates common settings needed for all models.""""""\n  desired_samples = int(sample_rate * clip_duration_ms / 1000)\n  window_size_samples = int(sample_rate * window_size_ms / 1000)\n  window_stride_samples = int(sample_rate * window_stride_ms / 1000)\n  length_minus_window = (desired_samples - window_size_samples)\n  spectrogram_frequencies = 257\n  if length_minus_window < 0:\n    spectrogram_length = 0\n  else:\n    spectrogram_length = 1 + int(length_minus_window / window_stride_samples)\n\n  if output_representation == \'mfcc\':\n    fingerprint_size = num_log_mel_features * spectrogram_length\n  elif output_representation == \'raw\':\n    fingerprint_size = desired_samples\n  elif output_representation == \'spec\':\n    fingerprint_size = spectrogram_frequencies * spectrogram_length\n  elif output_representation == \'mfcc_and_raw\':\n    fingerprint_size = num_log_mel_features * spectrogram_length\n  return {\n      \'desired_samples\': desired_samples,\n      \'window_size_samples\': window_size_samples,\n      \'window_stride_samples\': window_stride_samples,\n      \'spectrogram_length\': spectrogram_length,\n      \'spectrogram_frequencies\': spectrogram_frequencies,\n      \'dct_coefficient_count\': dct_coefficient_count,\n      \'fingerprint_size\': fingerprint_size,\n      \'label_count\': label_count,\n      \'sample_rate\': sample_rate,\n      \'num_log_mel_features\': num_log_mel_features\n  }\n'"
lite/examples/speech_commands/ml/train.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport tensorflow.compat.v1 as tf\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.callbacks import TensorBoard\nfrom callbacks import ConfusionMatrixCallback\nfrom model import speech_model, prepare_model_settings\nfrom generator import AudioProcessor, prepare_words_list\nfrom classes import get_classes\nfrom utils import data_gen\n\nparser = argparse.ArgumentParser(description=\'set input arguments\')\n\nparser.add_argument(\n    \'-sample_rate\',\n    action=\'store\',\n    dest=\'sample_rate\',\n    type=int,\n    default=16000,\n    help=\'Sample rate of audio\')\nparser.add_argument(\n    \'-batch_size\',\n    action=\'store\',\n    dest=\'batch_size\',\n    type=int,\n    default=32,\n    help=\'Size of the training batch\')\nparser.add_argument(\n    \'-output_representation\',\n    action=\'store\',\n    dest=\'output_representation\',\n    type=str,\n    default=\'raw\',\n    help=\'raw, spec, mfcc or mfcc_and_raw\')\nparser.add_argument(\n    \'-data_dirs\',\n    \'--list\',\n    dest=\'data_dirs\',\n    nargs=\'+\',\n    required=True,\n    help=\'<Required> The list of data directories. e.g., data/train\')\n\nargs = parser.parse_args()\nparser.print_help()\nprint(\'input args: \', args)\n\nif __name__ == \'__main__\':\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n  sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n  K.set_session(sess)\n  data_dirs = args.data_dirs\n  output_representation = args.output_representation\n  sample_rate = args.sample_rate\n  batch_size = args.batch_size\n  classes = get_classes(wanted_only=True)\n  model_settings = prepare_model_settings(\n      label_count=len(prepare_words_list(classes)),\n      sample_rate=sample_rate,\n      clip_duration_ms=1000,\n      window_size_ms=30.0,\n      window_stride_ms=10.0,\n      dct_coefficient_count=80,\n      num_log_mel_features=60,\n      output_representation=output_representation)\n\n  print(model_settings)\n\n  ap = AudioProcessor(\n      data_dirs=data_dirs,\n      wanted_words=classes,\n      silence_percentage=13.0,\n      unknown_percentage=60.0,\n      validation_percentage=10.0,\n      testing_percentage=0.0,\n      model_settings=model_settings,\n      output_representation=output_representation)\n  train_gen = data_gen(ap, sess, batch_size=batch_size, mode=\'training\')\n  val_gen = data_gen(ap, sess, batch_size=batch_size, mode=\'validation\')\n\n  model = speech_model(\n      \'conv_1d_time_stacked\',\n      model_settings[\'fingerprint_size\']\n      if output_representation != \'raw\' else model_settings[\'desired_samples\'],\n      # noqa\n      num_classes=model_settings[\'label_count\'],\n      **model_settings)\n\n  # embed()\n  checkpoints_path = os.path.join(\'checkpoints\', \'conv_1d_time_stacked_model\')\n  if not os.path.exists(checkpoints_path):\n    os.makedirs(checkpoints_path)\n\n  callbacks = [\n      ConfusionMatrixCallback(\n          val_gen,\n          ap.set_size(\'validation\') // batch_size,\n          wanted_words=prepare_words_list(get_classes(wanted_only=True)),\n          all_words=prepare_words_list(classes),\n          label2int=ap.word_to_index),\n      ReduceLROnPlateau(\n          monitor=\'val_categorical_accuracy\',\n          mode=\'max\',\n          factor=0.5,\n          patience=4,\n          verbose=1,\n          min_lr=1e-5),\n      TensorBoard(log_dir=\'logs\'),\n      ModelCheckpoint(\n          os.path.join(checkpoints_path,\n                       \'ep-{epoch:03d}-vl-{val_loss:.4f}.hdf5\'),\n          save_best_only=True,\n          monitor=\'val_categorical_accuracy\',\n          mode=\'max\')\n  ]\n  model.fit_generator(\n      train_gen,\n      steps_per_epoch=ap.set_size(\'training\') // batch_size,\n      epochs=100,\n      verbose=1,\n      callbacks=callbacks)\n\n  eval_res = model.evaluate_generator(val_gen,\n                                      ap.set_size(\'validation\') // batch_size)\n  print(eval_res)\n'"
lite/examples/speech_commands/ml/utils.py,4,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\n\n\ndef data_gen(audio_processor,\n             sess,\n             batch_size=128,\n             background_frequency=0.3,\n             background_volume_range=0.15,\n             foreground_frequency=0.3,\n             foreground_volume_range=0.15,\n             time_shift_frequency=0.3,\n             time_shift_range=[-500, 0],\n             mode=\'validation\',\n             flip_frequency=0.0,\n             silence_volume_range=0.3):\n  ep_count = 0\n  offset = 0\n  if mode != \'training\':\n    background_frequency = 0.0\n    background_volume_range = 0.0\n    foreground_frequency = 0.0\n    foreground_volume_range = 0.0\n    time_shift_frequency = 0.0\n    time_shift_range = [0, 0]\n    flip_frequency = 0.0\n    # silence_volume_range: stays the same for validation\n  while True:\n    X, y = audio_processor.get_data(\n        how_many=batch_size,\n        offset=0 if mode == \'training\' else offset,\n        background_frequency=background_frequency,\n        background_volume_range=background_volume_range,\n        foreground_frequency=foreground_frequency,\n        foreground_volume_range=foreground_volume_range,\n        time_shift_frequency=time_shift_frequency,\n        time_shift_range=time_shift_range,\n        mode=mode,\n        sess=sess,\n        flip_frequency=flip_frequency,\n        silence_volume_range=silence_volume_range)\n    offset += batch_size\n    if offset > audio_processor.set_size(mode) - batch_size:\n      offset = 0\n      print(\'\\n[Ep:%03d: %s-mode]\' % (ep_count, mode))\n      ep_count += 1\n    yield X, y\n\n\ndef tf_roll(a, shift, a_len=16000):\n  # https://stackoverflow.com/questions/42651714/vector-shift-roll-in-tensorflow\n  def roll_left(a, shift, a_len):\n    shift %= a_len\n    rolled = tf.concat([a[a_len - shift:, :], a[:a_len - shift, :]], axis=0)\n    return rolled\n\n  def roll_right(a, shift, a_len):\n    shift = -shift\n    shift %= a_len\n    rolled = tf.concat([a[shift:, :], a[:shift, :]], axis=0)\n    return rolled\n\n  # https://stackoverflow.com/questions/35833011/how-to-add-if-condition-in-a-tensorflow-graph\n  return tf.cond(\n      tf.greater_equal(shift, 0),\n      true_fn=lambda: roll_left(a, shift, a_len),\n      false_fn=lambda: roll_right(a, shift, a_len))\n'"
tensorflow_examples/lite/model_maker/cli/cli.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""CLI tool for model maker.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport fire\n\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec\nfrom tensorflow_examples.lite.model_maker.demo import image_classification_demo\nfrom tensorflow_examples.lite.model_maker.demo import text_classification_demo\n\n_IMAGE_MODELS = model_spec.IMAGE_CLASSIFICATION_MODELS\n_TEXT_MODELS = model_spec.TEXT_CLASSIFICATION_MODELS\n\n\nclass FormatDoc(object):\n  """"""Decorator to format functionn doc with given parameters.""""""\n\n  def __init__(self, *format_args, **format_kwargs):\n    self.args = format_args\n    self.kwargs = format_kwargs\n\n  def __call__(self, fn):\n    fn.__doc__ = fn.__doc__.format(*self.args, **self.kwargs)\n    return fn\n\n\nclass ModelMakerCLI(object):\n  """"""Model Maker Command Line Interface.\n\n  Flags:\n    --tf: int, version of TF behavior. Valid: [1, 2], default: 2.\n  """"""\n\n  def __init__(self, tf=2):\n    compat.setup_tf_behavior(tf)\n\n  @FormatDoc(MODELS=_IMAGE_MODELS)\n  def image_classification(self,\n                           data_dir,\n                           tflite_filename,\n                           label_filename,\n                           spec=\'efficientnet_lite0\',\n                           **kwargs):\n    """"""Run Image classification.\n\n    Args:\n      data_dir: str, input directory of training data. (required)\n      tflite_filename: str, output path to export tflite file. (required)\n      label_filename: str, output path to export label file. (required)\n      spec: str, model_name. Valid: {MODELS}, default: efficientnet_lite0.\n      **kwargs: --epochs: int, epoch num to run. More: see `create` function.\n    """"""\n    # Convert types\n    data_dir = str(data_dir)\n    tflite_filename = str(tflite_filename)\n    label_filename = str(label_filename)\n\n    image_classification_demo.run(data_dir, tflite_filename, label_filename,\n                                  spec, **kwargs)\n\n  @FormatDoc(MODELS=_TEXT_MODELS)\n  def text_classification(self,\n                          data_dir,\n                          tflite_filename,\n                          label_filename,\n                          vocab_filename,\n                          spec=\'bert\',\n                          **kwargs):\n    r""""""Run text classification.\n\n    Args:\n      data_dir: str, input directory of training data. (required)\n      tflite_filename: str, output path to export tflite file. (required)\n      label_filename: str, output path to export label file. (required)\n      vocab_filename: str, output path to export vocab file. (required)\n      spec: str, model_name. Valid: {MODELS}, default: bert.\n      **kwargs: --epochs: int, epoch num to run. More: see `create` function.\n    """"""\n    # Convert types\n    data_dir = str(data_dir)\n    tflite_filename = str(tflite_filename)\n    label_filename = str(label_filename)\n    vocab_filename = str(vocab_filename)\n\n    text_classification_demo.run(data_dir, tflite_filename, label_filename,\n                                 vocab_filename, spec, **kwargs)\n\n\ndef main():\n  fire.Fire(ModelMakerCLI)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
tensorflow_examples/lite/model_maker/cli/cli_test.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nfrom unittest.mock import patch\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport fire\n\nfrom tensorflow_examples.lite.model_maker.cli import cli\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.demo import image_classification_demo\nfrom tensorflow_examples.lite.model_maker.demo import text_classification_demo\n\nBIN = \'model_maker\'  # Whatever binary name.\n\n\ndef patch_image():\n  """"""Patch image classification demo run.""""""\n  return patch.object(image_classification_demo, \'run\')\n\n\ndef patch_text():\n  """"""Patch text classification demo run.""""""\n  return patch.object(text_classification_demo, \'run\')\n\n\ndef patch_setup():\n  """"""Patch image classification demo run.""""""\n  return patch.object(compat, \'setup_tf_behavior\')\n\n\nclass CLITest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      ([\'--tf=1\'], 1),\n      ([\'--tf=2\'], 2),\n      ([], 2),  # No extra flag, default\n  )\n  def test_init(self, tf_opt, expected_tf):\n    sys.argv = [BIN, \'image_classification\', \'data\', \'lite\', \'label\'] + tf_opt\n    with patch_image() as run, patch_setup() as setup:\n      cli.main()\n      setup.assert_called_once_with(expected_tf)\n      run.assert_called_once_with(\'data\', \'lite\', \'label\', \'efficientnet_lite0\')\n\n  @parameterized.parameters(\n      ([], [\'efficientnet_lite0\'], {}),\n      ([\'--spec=mobilenet_v2\'], [\'mobilenet_v2\'], {}),\n      ([\'--spec=mobilenet_v2\', \'--epochs=1\'], [\'mobilenet_v2\'], dict(epochs=1)),\n  )\n  def test_image_classification_demo(self, opt, args, kwargs):\n    sys.argv = [BIN, \'image_classification\', \'data\', \'lite\', \'label\'] + opt\n    with patch_image() as run:\n      cli.main()\n      run.assert_called_once_with(\'data\', \'lite\', \'label\', *args, **kwargs)\n\n  def test_image_classification_demo_lack_param(self):\n    sys.argv = [BIN, \'image_classification\', \'data\', \'lite\']\n    with patch_image() as run:\n      with self.assertRaisesRegex(fire.core.FireExit, \'2\'):\n        cli.main()\n      run.assert_not_called()\n\n  @parameterized.parameters(\n      ([], [\'bert\'], {}),\n      ([\'--spec=average_word_vec\'], [\'average_word_vec\'], {}),\n      ([\'--epochs=1\'], [\'bert\'], dict(epochs=1)),\n  )\n  def test_text_classification_demo(self, opt, args, kwargs):\n    sys.argv = [BIN, \'text_classification\', \'data\', \'lite\', \'label\', \'vocab\'\n               ] + opt\n    with patch_text() as run:\n      cli.main()\n      run.assert_called_once_with(\'data\', \'lite\', \'label\', \'vocab\', *args,\n                                  **kwargs)\n\n  def test_text_classification_demo_lack_param(self):\n    sys.argv = [BIN, \'text_classification\', \'data\', \'lite\', \'label\']\n    with patch_text() as run:\n      with self.assertRaisesRegex(fire.core.FireExit, \'2\'):\n        cli.main()\n      run.assert_not_called()\n\n  def test_invalid_command(self):\n    sys.argv = [BIN, \'invalid_command\']\n    with self.assertRaisesRegex(fire.core.FireExit, \'2\'):\n      cli.main()\n\n  @parameterized.parameters(\n      ([BIN, \'--\', \'--help\'],),\n      ([BIN, \'image_classification\', \'--\', \'--help\'],),\n      ([BIN, \'text_classification\', \'--\', \'--help\'],),\n  )\n  def test_help(self, opt):\n    sys.argv = opt\n    with self.assertRaisesRegex(fire.core.FireExit, \'0\'):\n      cli.main()\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tensorflow_examples/lite/model_maker/core/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
tensorflow_examples/lite/model_maker/core/compat.py,3,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Compat modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_DEFAULT_TF_BEHAVIOR = 2\n\n# Get version of tf behavior in use (valid 1 or 2).\n_tf_behavior_version = _DEFAULT_TF_BEHAVIOR\n\n\ndef setup_tf_behavior(tf_version=_DEFAULT_TF_BEHAVIOR):\n  """"""Setup tf behavior. It must be used before the main().""""""\n  global _tf_behavior_version\n  if tf_version not in [1, 2]:\n    raise ValueError(\n        \'tf_version should be in [1, 2], but got {}\'.format(tf_version))\n\n  if tf_version == 1:\n    tf.compat.v1.logging.warn(\n        \'Using v1 behavior. Please note that it is mainly to run legacy models,\'\n        \'however v2 is more preferrable if they are supported.\')\n    tf.compat.v1.disable_v2_behavior()\n  else:\n    tf.compat.v2.enable_v2_behavior()\n  _tf_behavior_version = tf_version\n\n\ndef get_tf_behavior():\n  """"""Gets version for tf behavior.\n\n  Returns:\n    int, 1 or 2 indicating the behavior version.\n  """"""\n  return _tf_behavior_version\n'"
tensorflow_examples/lite/model_maker/core/export_format.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Export format such as saved_model / tflite.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom enum import Enum\nfrom enum import unique\n\n\n@unique\nclass ExportFormat(Enum):\n  TFLITE = ""TFLITE""\n  SAVED_MODEL = ""SAVED_MODEL""\n  LABEL = ""LABEL""\n  VOCAB = ""VOCAB""\n'"
tensorflow_examples/lite/model_maker/core/file_util.py,1,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""File related util for model maker.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\n\nimport tensorflow as tf\n\n\ndef load_json_file(json_file):\n  with tf.io.gfile.GFile(json_file, \'r\') as reader:\n    return json.load(reader)\n'"
tensorflow_examples/lite/model_maker/core/test_util.py,14,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Test util for model maker.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nfrom absl import flags\n\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.data_util import dataloader\n\nFLAGS = flags.FLAGS\n\n\ndef test_srcdir():\n  """"""Returns the path where to look for test data files.""""""\n  if ""test_srcdir"" in flags.FLAGS:\n    return flags.FLAGS[""test_srcdir""].value\n  elif ""TEST_SRCDIR"" in os.environ:\n    return os.environ[""TEST_SRCDIR""]\n  else:\n    raise RuntimeError(""Missing TEST_SRCDIR environment."")\n\n\ndef get_test_data_path(file_or_dirname):\n  """"""Return full test data path.""""""\n  for directory, subdirs, files in tf.io.gfile.walk(test_srcdir()):\n    for f in subdirs + files:\n      if f.endswith(file_or_dirname):\n        return os.path.join(directory, f)\n  raise ValueError(""No %s in test directory"" % file_or_dirname)\n\n\ndef test_in_tf_1(fn):\n  """"""Decorator to test in tf 1 behaviors.""""""\n\n  @functools.wraps(fn)\n  def decorator(*args, **kwargs):\n    if compat.get_tf_behavior() != 1:\n      tf.compat.v1.logging.info(""Skip function {} for test_in_tf_1"".format(\n          fn.__name__))\n      return\n    fn(*args, **kwargs)\n\n  return decorator\n\n\ndef test_in_tf_2(fn):\n  """"""Decorator to test in tf 2 behaviors.""""""\n\n  @functools.wraps(fn)\n  def decorator(*args, **kwargs):\n    if compat.get_tf_behavior() != 2:\n      tf.compat.v1.logging.info(""Skip function {} for test_in_tf_2"".format(\n          fn.__name__))\n      return\n    fn(*args, **kwargs)\n\n  return decorator\n\n\ndef test_in_tf_1and2(fn):\n  """"""Decorator to test in tf 1 and 2 behaviors.""""""\n\n  @functools.wraps(fn)\n  def decorator(*args, **kwargs):\n    if compat.get_tf_behavior() not in [1, 2]:\n      tf.compat.v1.logging.info(""Skip function {} for test_in_tf_1and2"".format(\n          fn.__name__))\n      return\n    fn(*args, **kwargs)\n\n  return decorator\n\n\ndef build_model(input_shape, num_classes):\n  """"""Builds a simple model for test.""""""\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  if len(input_shape) == 3:  # Image inputs.\n    outputs = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n    outputs = tf.keras.layers.Dense(num_classes, activation=""softmax"")(outputs)\n  elif len(input_shape) == 1:  # Text inputs.\n    outputs = tf.keras.layers.Dense(num_classes, activation=""softmax"")(inputs)\n  else:\n    raise ValueError(""Model inputs should be 2D tensor or 4D tensor."")\n\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  return model\n\n\ndef get_dataloader(data_size, input_shape, num_classes, max_input_value=1000):\n  """"""Gets a simple `DataLoader` object for test.""""""\n  features = tf.random.uniform(\n      shape=[data_size] + input_shape,\n      minval=0,\n      maxval=max_input_value,\n      dtype=tf.float32)\n\n  labels = tf.random.uniform(\n      shape=[data_size], minval=0, maxval=num_classes, dtype=tf.int32)\n\n  ds = tf.data.Dataset.from_tensor_slices((features, labels))\n  data = dataloader.DataLoader(ds, data_size)\n  return data\n'"
tensorflow_examples/lite/model_maker/demo/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
tensorflow_examples/lite/model_maker/demo/image_classification_demo.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Image classification demo code of Model Maker for TFLite.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core.data_util.image_dataloader import ImageClassifierDataLoader\nfrom tensorflow_examples.lite.model_maker.core.task import image_classifier\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec\n\nFLAGS = flags.FLAGS\n\n\ndef define_flags():\n  flags.DEFINE_string(\'export_dir\', None,\n                      \'The directory to save exported files.\')\n  flags.mark_flag_as_required(\'export_dir\')\n\n\ndef download_demo_data(**kwargs):\n  """"""Downloads demo data, and returns directory path.""""""\n  data_dir = tf.keras.utils.get_file(\n      fname=\'flower_photos.tgz\',\n      origin=\'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\',\n      untar=True,\n      **kwargs)\n  return os.path.join(data_dir, \'..\', \'flower_photos\')  # folder name\n\n\ndef run(data_dir, export_dir, spec=\'efficientnet_lite0\', **kwargs):\n  """"""Runs demo.""""""\n  spec = model_spec.get(spec)\n  data = ImageClassifierDataLoader.from_folder(data_dir)\n  train_data, rest_data = data.split(0.8)\n  validation_data, test_data = rest_data.split(0.5)\n\n  model = image_classifier.create(\n      train_data,\n      model_spec=spec,\n      validation_data=validation_data,\n      **kwargs)\n\n  _, acc = model.evaluate(test_data)\n  print(\'Test accuracy: %f\' % acc)\n  model.export(export_dir)\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  data_dir = download_demo_data()\n  run(data_dir, FLAGS.export_dir)\n\n\nif __name__ == \'__main__\':\n  define_flags()\n  app.run(main)\n'"
tensorflow_examples/lite/model_maker/demo/image_classification_demo_test.py,5,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\nfrom unittest.mock import patch\n\nimport tensorflow as tf\n\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.data_util.image_dataloader import ImageClassifierDataLoader\nfrom tensorflow_examples.lite.model_maker.demo import image_classification_demo\n\n\ndef get_cache_dir():\n  return os.path.join(test_util.get_test_data_path(\'demo\'), \'testdata\')\n\n\nfrom_folder_fn = ImageClassifierDataLoader.from_folder\n\n\ndef patch_data_loader():\n  """"""Patch to train partial dataset rather than all of them.""""""\n\n  def side_effect(*args, **kwargs):\n    tf.compat.v1.logging.info(\'Train on partial dataset\')\n    data_loader = from_folder_fn(*args, **kwargs)\n    if data_loader.size > 10:  # Trim dataset to at most 10.\n      data_loader.size = 10\n      data_loader.dataset = data_loader.dataset.take(data_loader.size)\n    return data_loader\n\n  return patch.object(\n      ImageClassifierDataLoader, \'from_folder\', side_effect=side_effect)\n\n\nclass ImageClassificationDemoTest(tf.test.TestCase):\n\n  def test_image_classification_demo(self):\n    with patch_data_loader():\n      with tempfile.TemporaryDirectory() as temp_dir:\n        # Use cached training data if exists.\n        data_dir = image_classification_demo.download_demo_data(\n            cache_dir=get_cache_dir(),\n            file_hash=\'6f87fb78e9cc9ab41eff2015b380011d\')\n\n        tflite_filename = os.path.join(temp_dir, \'model.tflite\')\n        label_filename = os.path.join(temp_dir, \'labels.txt\')\n        image_classification_demo.run(\n            data_dir,\n            temp_dir,\n            spec=\'efficientnet_lite0\',\n            epochs=1,\n            batch_size=1)\n\n        self.assertTrue(tf.io.gfile.exists(tflite_filename))\n        self.assertGreater(os.path.getsize(tflite_filename), 0)\n\n        self.assertTrue(tf.io.gfile.exists(label_filename))\n        self.assertGreater(os.path.getsize(label_filename), 0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/demo/question_answer_demo.py,3,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Text classification demo code of model customization for TFLite.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core.data_util.text_dataloader import QuestionAnswerDataLoader\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec\nfrom tensorflow_examples.lite.model_maker.core.task import question_answer\n\nFLAGS = flags.FLAGS\n\n\ndef define_flags():\n  flags.DEFINE_string(\'export_dir\', None,\n                      \'The directory to save exported files.\')\n  flags.mark_flag_as_required(\'export_dir\')\n\n\ndef download_demo_data(**kwargs):\n  """"""Downloads demo data, and returns directory path.""""""\n  train_data_path = tf.keras.utils.get_file(\n      fname=\'train-v1.1.json\',\n      origin=\'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\',\n      **kwargs)\n  validation_data_path = tf.keras.utils.get_file(\n      fname=\'dev-v1.1.json\',\n      origin=\'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\',\n      **kwargs)\n  return train_data_path, validation_data_path\n\n\ndef run(train_data_path,\n        validation_data_path,\n        export_dir,\n        spec=\'bert_qa\',\n        **kwargs):\n  """"""Runs demo.""""""\n  # Chooses model specification that represents model.\n  spec = model_spec.get(spec)\n\n  # Gets training data and validation data.\n  train_data = QuestionAnswerDataLoader.from_squad(\n      train_data_path, spec, is_training=True)\n  validation_data = QuestionAnswerDataLoader.from_squad(\n      validation_data_path, spec, is_training=False)\n\n  # Fine-tunes the model.\n  model = question_answer.create(train_data, model_spec=spec, **kwargs)\n\n  # Gets evaluation results.\n  metric = model.evaluate(validation_data)\n  tf.compat.v1.logging.info(\'Eval F1 score:%f\' % metric[\'final_f1\'])\n\n  # Exports to TFLite format.\n  model.export(export_dir)\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n\n  train_data_path, validation_data_path = download_demo_data()\n  run(train_data_path, validation_data_path, FLAGS.export_dir)\n\n\nif __name__ == \'__main__\':\n  define_flags()\n  app.run(main)\n'"
tensorflow_examples/lite/model_maker/demo/text_classification_demo.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Text classification demo code of Model Maker for TFLite.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core.data_util.text_dataloader import TextClassifierDataLoader\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec\nfrom tensorflow_examples.lite.model_maker.core.task import text_classifier\n\nFLAGS = flags.FLAGS\n\n\ndef define_flags():\n  flags.DEFINE_string(\'export_dir\', None,\n                      \'The directory to save exported files.\')\n  flags.mark_flag_as_required(\'export_dir\')\n\n\ndef download_demo_data(**kwargs):\n  """"""Downloads demo data, and returns directory path.""""""\n  data_path = tf.keras.utils.get_file(\n      fname=\'SST-2.zip\',\n      origin=\'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\',\n      extract=True,\n      **kwargs)\n  return os.path.join(os.path.dirname(data_path), \'SST-2\')  # folder name\n\n\ndef run(data_dir, export_dir, spec=\'bert_classifier\', **kwargs):\n  """"""Runs demo.""""""\n  # Chooses model specification that represents model.\n  spec = model_spec.get(spec)\n\n  # Gets training data and validation data.\n  train_data = TextClassifierDataLoader.from_csv(\n      filename=os.path.join(os.path.join(data_dir, \'train.tsv\')),\n      text_column=\'sentence\',\n      label_column=\'label\',\n      model_spec=spec,\n      delimiter=\'\\t\',\n      is_training=True)\n  validation_data = TextClassifierDataLoader.from_csv(\n      filename=os.path.join(os.path.join(data_dir, \'dev.tsv\')),\n      text_column=\'sentence\',\n      label_column=\'label\',\n      model_spec=spec,\n      delimiter=\'\\t\',\n      is_training=False)\n\n  # Fine-tunes the model.\n  model = text_classifier.create(\n      train_data, model_spec=spec, validation_data=validation_data, **kwargs)\n\n  # Gets evaluation results.\n  _, acc = model.evaluate(validation_data)\n  print(\'Eval accuracy: %f\' % acc)\n\n  # Exports to TFLite format.\n  model.export(export_dir)\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  data_dir = download_demo_data()\n  run(data_dir, FLAGS.export_dir)\n\n\nif __name__ == \'__main__\':\n  define_flags()\n  app.run(main)\n'"
tensorflow_examples/lite/model_maker/demo/text_classification_demo_test.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\nfrom unittest.mock import patch\n\nimport tensorflow as tf\n\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.data_util.text_dataloader import TextClassifierDataLoader\nfrom tensorflow_examples.lite.model_maker.demo import text_classification_demo\n\n\ndef get_cache_dir():\n  return os.path.join(test_util.get_test_data_path(\'demo\'), \'testdata\')\n\n\nfrom_csv_fn = TextClassifierDataLoader.from_csv\n\n\ndef patch_data_loader():\n  """"""Patch to train partial dataset rather than all of them.""""""\n\n  def side_effect(*args, **kwargs):\n    tf.compat.v1.logging.info(\'Train on partial dataset\')\n    data_loader = from_csv_fn(*args, **kwargs)\n    if data_loader.size > 2:  # Trim dataset to at most 2.\n      data_loader.size = 2\n      data_loader.dataset = data_loader.dataset.take(data_loader.size)\n    return data_loader\n\n  return patch.object(\n      TextClassifierDataLoader, \'from_csv\', side_effect=side_effect)\n\n\nclass TextClassificationDemoTest(tf.test.TestCase):\n\n  def test_text_classification_demo(self):\n    with patch_data_loader():\n      with tempfile.TemporaryDirectory() as temp_dir:\n        # Use cached training data if exists.\n        data_dir = text_classification_demo.download_demo_data(\n            cache_dir=get_cache_dir(),\n            file_hash=\'9f81648d4199384278b86e315dac217c\')\n\n        tflite_filename = os.path.join(temp_dir, \'model.tflite\')\n        label_filename = os.path.join(temp_dir, \'labels.txt\')\n        vocab_filename = os.path.join(temp_dir, \'vocab\')\n        # TODO(b/150597348): Bert model is out of memory when export to tflite.\n        # Changed to a smaller bert models like mobilebert later for unittest.\n        text_classification_demo.run(\n            data_dir, temp_dir, spec=\'average_word_vec\', epochs=1, batch_size=1)\n\n        self.assertTrue(tf.io.gfile.exists(tflite_filename))\n        self.assertGreater(os.path.getsize(tflite_filename), 0)\n\n        self.assertTrue(tf.io.gfile.exists(label_filename))\n        self.assertGreater(os.path.getsize(label_filename), 0)\n\n        self.assertTrue(tf.io.gfile.exists(vocab_filename))\n        self.assertGreater(os.path.getsize(vocab_filename), 0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
community/en/r1/tutorials/embedding/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Import generated word2vec optimized ops into embedding package.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'"
community/en/r1/tutorials/embedding/word2vec.py,51,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Multi-threaded word2vec mini-batched skip-gram model.\n\nTrains the model described in:\n(Mikolov, et. al.) Efficient Estimation of Word Representations in Vector Space\nICLR 2013.\nhttp://arxiv.org/abs/1301.3781\nThis model does traditional minibatching.\n\nThe key ops used are:\n* placeholder for feeding in tensors for each example.\n* embedding_lookup for fetching rows from the embedding matrix.\n* sigmoid_cross_entropy_with_logits to calculate the loss.\n* GradientDescentOptimizer for optimizing the loss.\n* skipgram custom op that does input processing.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport threading\nimport time\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nimport numpy as np\nimport tensorflow as tf\n\nword2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(__file__)), \'word2vec_ops.so\'))\n\nflags = tf.app.flags\n\nflags.DEFINE_string(""save_path"", None, ""Directory to write the model and ""\n                    ""training summaries."")\nflags.DEFINE_string(""train_data"", None, ""Training text file. ""\n                    ""E.g., unzipped file http://mattmahoney.net/dc/text8.zip."")\nflags.DEFINE_string(\n    ""eval_data"", None, ""File consisting of analogies of four tokens.""\n    ""embedding 2 - embedding 1 + embedding 3 should be close ""\n    ""to embedding 4.""\n    ""See README.md for how to get \'questions-words.txt\'."")\nflags.DEFINE_integer(""embedding_size"", 200, ""The embedding dimension size."")\nflags.DEFINE_integer(\n    ""epochs_to_train"", 15,\n    ""Number of epochs to train. Each epoch processes the training data once ""\n    ""completely."")\nflags.DEFINE_float(""learning_rate"", 0.2, ""Initial learning rate."")\nflags.DEFINE_integer(""num_neg_samples"", 100,\n                     ""Negative samples per training example."")\nflags.DEFINE_integer(""batch_size"", 16,\n                     ""Number of training examples processed per step ""\n                     ""(size of a minibatch)."")\nflags.DEFINE_integer(""concurrent_steps"", 12,\n                     ""The number of concurrent training steps."")\nflags.DEFINE_integer(""window_size"", 5,\n                     ""The number of words to predict to the left and right ""\n                     ""of the target word."")\nflags.DEFINE_integer(""min_count"", 5,\n                     ""The minimum number of word occurrences for it to be ""\n                     ""included in the vocabulary."")\nflags.DEFINE_float(""subsample"", 1e-3,\n                   ""Subsample threshold for word occurrence. Words that appear ""\n                   ""with higher frequency will be randomly down-sampled. Set ""\n                   ""to 0 to disable."")\nflags.DEFINE_boolean(\n    ""interactive"", False,\n    ""If true, enters an IPython interactive session to play with the trained ""\n    ""model. E.g., try model.analogy(b\'france\', b\'paris\', b\'russia\') and ""\n    ""model.nearby([b\'proton\', b\'elephant\', b\'maxwell\'])"")\nflags.DEFINE_integer(""statistics_interval"", 5,\n                     ""Print statistics every n seconds."")\nflags.DEFINE_integer(""summary_interval"", 5,\n                     ""Save training summary to file every n seconds (rounded ""\n                     ""up to statistics interval)."")\nflags.DEFINE_integer(""checkpoint_interval"", 600,\n                     ""Checkpoint the model (i.e. save the parameters) every n ""\n                     ""seconds (rounded up to statistics interval)."")\n\nFLAGS = flags.FLAGS\n\n\nclass Options(object):\n  """"""Options used by our word2vec model.""""""\n\n  def __init__(self):\n    # Model options.\n\n    # Embedding dimension.\n    self.emb_dim = FLAGS.embedding_size\n\n    # Training options.\n    # The training text file.\n    self.train_data = FLAGS.train_data\n\n    # Number of negative samples per example.\n    self.num_samples = FLAGS.num_neg_samples\n\n    # The initial learning rate.\n    self.learning_rate = FLAGS.learning_rate\n\n    # Number of epochs to train. After these many epochs, the learning\n    # rate decays linearly to zero and the training stops.\n    self.epochs_to_train = FLAGS.epochs_to_train\n\n    # Concurrent training steps.\n    self.concurrent_steps = FLAGS.concurrent_steps\n\n    # Number of examples for one training step.\n    self.batch_size = FLAGS.batch_size\n\n    # The number of words to predict to the left and right of the target word.\n    self.window_size = FLAGS.window_size\n\n    # The minimum number of word occurrences for it to be included in the\n    # vocabulary.\n    self.min_count = FLAGS.min_count\n\n    # Subsampling threshold for word occurrence.\n    self.subsample = FLAGS.subsample\n\n    # How often to print statistics.\n    self.statistics_interval = FLAGS.statistics_interval\n\n    # How often to write to the summary file (rounds up to the nearest\n    # statistics_interval).\n    self.summary_interval = FLAGS.summary_interval\n\n    # How often to write checkpoints (rounds up to the nearest statistics\n    # interval).\n    self.checkpoint_interval = FLAGS.checkpoint_interval\n\n    # Where to write out summaries.\n    self.save_path = FLAGS.save_path\n    if not os.path.exists(self.save_path):\n      os.makedirs(self.save_path)\n\n    # Eval options.\n    # The text file for eval.\n    self.eval_data = FLAGS.eval_data\n\n\nclass Word2Vec(object):\n  """"""Word2Vec model (Skipgram).""""""\n\n  def __init__(self, options, session):\n    self._options = options\n    self._session = session\n    self._word2id = {}\n    self._id2word = []\n    self.build_graph()\n    self.build_eval_graph()\n    self.save_vocab()\n\n  def read_analogies(self):\n    """"""Reads through the analogy question file.\n\n    Returns:\n      questions: a [n, 4] numpy array containing the analogy question\'s\n                 word ids.\n      questions_skipped: questions skipped due to unknown words.\n    """"""\n    questions = []\n    questions_skipped = 0\n    with open(self._options.eval_data, ""rb"") as analogy_f:\n      for line in analogy_f:\n        if line.startswith(b"":""):  # Skip comments.\n          continue\n        words = line.strip().lower().split(b"" "")\n        ids = [self._word2id.get(w.strip()) for w in words]\n        if None in ids or len(ids) != 4:\n          questions_skipped += 1\n        else:\n          questions.append(np.array(ids))\n    print(""Eval analogy file: "", self._options.eval_data)\n    print(""Questions: "", len(questions))\n    print(""Skipped: "", questions_skipped)\n    self._analogy_questions = np.array(questions, dtype=np.int32)\n\n  def forward(self, examples, labels):\n    """"""Build the graph for the forward pass.""""""\n    opts = self._options\n\n    # Declare all variables we need.\n    # Embedding: [vocab_size, emb_dim]\n    init_width = 0.5 / opts.emb_dim\n    emb = tf.Variable(\n        tf.random_uniform(\n            [opts.vocab_size, opts.emb_dim], -init_width, init_width),\n        name=""emb"")\n    self._emb = emb\n\n    # Softmax weight: [vocab_size, emb_dim]. Transposed.\n    sm_w_t = tf.Variable(\n        tf.zeros([opts.vocab_size, opts.emb_dim]),\n        name=""sm_w_t"")\n\n    # Softmax bias: [vocab_size].\n    sm_b = tf.Variable(tf.zeros([opts.vocab_size]), name=""sm_b"")\n\n    # Global step: scalar, i.e., shape [].\n    self.global_step = tf.Variable(0, name=""global_step"")\n\n    # Nodes to compute the nce loss w/ candidate sampling.\n    labels_matrix = tf.reshape(\n        tf.cast(labels,\n                dtype=tf.int64),\n        [opts.batch_size, 1])\n\n    # Negative sampling.\n    sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n        true_classes=labels_matrix,\n        num_true=1,\n        num_sampled=opts.num_samples,\n        unique=True,\n        range_max=opts.vocab_size,\n        distortion=0.75,\n        unigrams=opts.vocab_counts.tolist()))\n\n    # Embeddings for examples: [batch_size, emb_dim]\n    example_emb = tf.nn.embedding_lookup(emb, examples)\n\n    # Weights for labels: [batch_size, emb_dim]\n    true_w = tf.nn.embedding_lookup(sm_w_t, labels)\n    # Biases for labels: [batch_size, 1]\n    true_b = tf.nn.embedding_lookup(sm_b, labels)\n\n    # Weights for sampled ids: [num_sampled, emb_dim]\n    sampled_w = tf.nn.embedding_lookup(sm_w_t, sampled_ids)\n    # Biases for sampled ids: [num_sampled, 1]\n    sampled_b = tf.nn.embedding_lookup(sm_b, sampled_ids)\n\n    # True logits: [batch_size, 1]\n    true_logits = tf.reduce_sum(tf.multiply(example_emb, true_w), 1) + true_b\n\n    # Sampled logits: [batch_size, num_sampled]\n    # We replicate sampled noise labels for all examples in the batch\n    # using the matmul.\n    sampled_b_vec = tf.reshape(sampled_b, [opts.num_samples])\n    sampled_logits = tf.matmul(example_emb,\n                               sampled_w,\n                               transpose_b=True) + sampled_b_vec\n    return true_logits, sampled_logits\n\n  def nce_loss(self, true_logits, sampled_logits):\n    """"""Build the graph for the NCE loss.""""""\n\n    # cross-entropy(logits, labels)\n    opts = self._options\n    true_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.ones_like(true_logits), logits=true_logits)\n    sampled_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.zeros_like(sampled_logits), logits=sampled_logits)\n\n    # NCE-loss is the sum of the true and noise (sampled words)\n    # contributions, averaged over the batch.\n    nce_loss_tensor = (tf.reduce_sum(true_xent) +\n                       tf.reduce_sum(sampled_xent)) / opts.batch_size\n    return nce_loss_tensor\n\n  def optimize(self, loss):\n    """"""Build the graph to optimize the loss function.""""""\n\n    # Optimizer nodes.\n    # Linear learning rate decay.\n    opts = self._options\n    words_to_train = float(opts.words_per_epoch * opts.epochs_to_train)\n    lr = opts.learning_rate * tf.maximum(\n        0.0001, 1.0 - tf.cast(self._words, tf.float32) / words_to_train)\n    self._lr = lr\n    optimizer = tf.train.GradientDescentOptimizer(lr)\n    train = optimizer.minimize(loss,\n                               global_step=self.global_step,\n                               gate_gradients=optimizer.GATE_NONE)\n    self._train = train\n\n  def build_eval_graph(self):\n    """"""Build the eval graph.""""""\n    # Eval graph\n\n    # Each analogy task is to predict the 4th word (d) given three\n    # words: a, b, c.  E.g., a=italy, b=rome, c=france, we should\n    # predict d=paris.\n\n    # The eval feeds three vectors of word ids for a, b, c, each of\n    # which is of size N, where N is the number of analogies we want to\n    # evaluate in one batch.\n    analogy_a = tf.placeholder(dtype=tf.int32)  # [N]\n    analogy_b = tf.placeholder(dtype=tf.int32)  # [N]\n    analogy_c = tf.placeholder(dtype=tf.int32)  # [N]\n\n    # Normalized word embeddings of shape [vocab_size, emb_dim].\n    nemb = tf.nn.l2_normalize(self._emb, 1)\n\n    # Each row of a_emb, b_emb, c_emb is a word\'s embedding vector.\n    # They all have the shape [N, emb_dim]\n    a_emb = tf.gather(nemb, analogy_a)  # a\'s embs\n    b_emb = tf.gather(nemb, analogy_b)  # b\'s embs\n    c_emb = tf.gather(nemb, analogy_c)  # c\'s embs\n\n    # We expect that d\'s embedding vectors on the unit hyper-sphere is\n    # near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].\n    target = c_emb + (b_emb - a_emb)\n\n    # Compute cosine distance between each pair of target and vocab.\n    # dist has shape [N, vocab_size].\n    dist = tf.matmul(target, nemb, transpose_b=True)\n\n    # For each question (row in dist), find the top 4 words.\n    _, pred_idx = tf.nn.top_k(dist, 4)\n\n    # Nodes for computing neighbors for a given word according to\n    # their cosine distance.\n    nearby_word = tf.placeholder(dtype=tf.int32)  # word id\n    nearby_emb = tf.gather(nemb, nearby_word)\n    nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True)\n    nearby_val, nearby_idx = tf.nn.top_k(nearby_dist,\n                                         min(1000, self._options.vocab_size))\n\n    # Nodes in the construct graph which are used by training and\n    # evaluation to run/feed/fetch.\n    self._analogy_a = analogy_a\n    self._analogy_b = analogy_b\n    self._analogy_c = analogy_c\n    self._analogy_pred_idx = pred_idx\n    self._nearby_word = nearby_word\n    self._nearby_val = nearby_val\n    self._nearby_idx = nearby_idx\n\n  def build_graph(self):\n    """"""Build the graph for the full model.""""""\n    opts = self._options\n    # The training data. A text file.\n    (words, counts, words_per_epoch, self._epoch, self._words, examples,\n     labels) = word2vec.skipgram_word2vec(filename=opts.train_data,\n                                          batch_size=opts.batch_size,\n                                          window_size=opts.window_size,\n                                          min_count=opts.min_count,\n                                          subsample=opts.subsample)\n    (opts.vocab_words, opts.vocab_counts,\n     opts.words_per_epoch) = self._session.run([words, counts, words_per_epoch])\n    opts.vocab_size = len(opts.vocab_words)\n    print(""Data file: "", opts.train_data)\n    print(""Vocab size: "", opts.vocab_size - 1, "" + UNK"")\n    print(""Words per epoch: "", opts.words_per_epoch)\n    self._examples = examples\n    self._labels = labels\n    self._id2word = opts.vocab_words\n    for i, w in enumerate(self._id2word):\n      self._word2id[w] = i\n    true_logits, sampled_logits = self.forward(examples, labels)\n    loss = self.nce_loss(true_logits, sampled_logits)\n    tf.summary.scalar(""NCE loss"", loss)\n    self._loss = loss\n    self.optimize(loss)\n\n    # Properly initialize all variables.\n    tf.global_variables_initializer().run()\n\n    self.saver = tf.train.Saver()\n\n  def save_vocab(self):\n    """"""Save the vocabulary to a file so the model can be reloaded.""""""\n    opts = self._options\n    with open(os.path.join(opts.save_path, ""vocab.txt""), ""w"") as f:\n      for i in xrange(opts.vocab_size):\n        vocab_word = tf.compat.as_text(opts.vocab_words[i]).encode(""utf-8"")\n        f.write(""%s %d\\n"" % (vocab_word,\n                             opts.vocab_counts[i]))\n\n  def _train_thread_body(self):\n    initial_epoch, = self._session.run([self._epoch])\n    while True:\n      _, epoch = self._session.run([self._train, self._epoch])\n      if epoch != initial_epoch:\n        break\n\n  def train(self):\n    """"""Train the model.""""""\n    opts = self._options\n\n    initial_epoch, initial_words = self._session.run([self._epoch, self._words])\n\n    summary_op = tf.summary.merge_all()\n    summary_writer = tf.summary.FileWriter(opts.save_path, self._session.graph)\n    workers = []\n    for _ in xrange(opts.concurrent_steps):\n      t = threading.Thread(target=self._train_thread_body)\n      t.start()\n      workers.append(t)\n\n    last_words, last_time, last_summary_time = initial_words, time.time(), 0\n    last_checkpoint_time = 0\n    while True:\n      time.sleep(opts.statistics_interval)  # Reports our progress once a while.\n      (epoch, step, loss, words, lr) = self._session.run(\n          [self._epoch, self.global_step, self._loss, self._words, self._lr])\n      now = time.time()\n      last_words, last_time, rate = words, now, (words - last_words) / (\n          now - last_time)\n      print(""Epoch %4d Step %8d: lr = %5.3f loss = %6.2f words/sec = %8.0f\\r"" %\n            (epoch, step, lr, loss, rate), end="""")\n      sys.stdout.flush()\n      if now - last_summary_time > opts.summary_interval:\n        summary_str = self._session.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n        last_summary_time = now\n      if now - last_checkpoint_time > opts.checkpoint_interval:\n        self.saver.save(self._session,\n                        os.path.join(opts.save_path, ""model.ckpt""),\n                        global_step=step.astype(int))\n        last_checkpoint_time = now\n      if epoch != initial_epoch:\n        break\n\n    for t in workers:\n      t.join()\n\n    return epoch\n\n  def _predict(self, analogy):\n    """"""Predict the top 4 answers for analogy questions.""""""\n    idx, = self._session.run([self._analogy_pred_idx], {\n        self._analogy_a: analogy[:, 0],\n        self._analogy_b: analogy[:, 1],\n        self._analogy_c: analogy[:, 2]\n    })\n    return idx\n\n  def eval(self):\n    """"""Evaluate analogy questions and reports accuracy.""""""\n\n    # How many questions we get right at precision@1.\n    correct = 0\n\n    try:\n      total = self._analogy_questions.shape[0]\n    except AttributeError as e:\n      raise AttributeError(""Need to read analogy questions."")\n\n    start = 0\n    while start < total:\n      limit = start + 2500\n      sub = self._analogy_questions[start:limit, :]\n      idx = self._predict(sub)\n      start = limit\n      for question in xrange(sub.shape[0]):\n        for j in xrange(4):\n          if idx[question, j] == sub[question, 3]:\n            # Bingo! We predicted correctly. E.g., [italy, rome, france, paris].\n            correct += 1\n            break\n          elif idx[question, j] in sub[question, :3]:\n            # We need to skip words already in the question.\n            continue\n          else:\n            # The correct label is not the precision@1\n            break\n    print()\n    print(""Eval %4d/%d accuracy = %4.1f%%"" % (correct, total,\n                                              correct * 100.0 / total))\n\n  def analogy(self, w0, w1, w2):\n    """"""Predict word w3 as in w0:w1 vs w2:w3.""""""\n    wid = np.array([[self._word2id.get(w, 0) for w in [w0, w1, w2]]])\n    idx = self._predict(wid)\n    for c in [self._id2word[i] for i in idx[0, :]]:\n      if c not in [w0, w1, w2]:\n        print(c)\n        return\n    print(""unknown"")\n\n  def nearby(self, words, num=20):\n    """"""Prints out nearby words given a list of words.""""""\n    ids = np.array([self._word2id.get(x, 0) for x in words])\n    vals, idx = self._session.run(\n        [self._nearby_val, self._nearby_idx], {self._nearby_word: ids})\n    for i in xrange(len(words)):\n      print(""\\n%s\\n====================================="" % (words[i]))\n      for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n        print(""%-20s %6.4f"" % (self._id2word[neighbor], distance))\n\n\ndef _start_shell(local_ns=None):\n  # An interactive shell is useful for debugging/development.\n  import IPython\n  user_ns = {}\n  if local_ns:\n    user_ns.update(local_ns)\n  user_ns.update(globals())\n  IPython.start_ipython(argv=[], user_ns=user_ns)\n\n\ndef main(_):\n  """"""Train a word2vec model.""""""\n  if not FLAGS.train_data or not FLAGS.eval_data or not FLAGS.save_path:\n    print(""--train_data --eval_data and --save_path must be specified."")\n    sys.exit(1)\n  opts = Options()\n  with tf.Graph().as_default(), tf.Session() as session:\n    with tf.device(""/cpu:0""):\n      model = Word2Vec(opts, session)\n      model.read_analogies() # Read analogy questions\n    for _ in xrange(opts.epochs_to_train):\n      model.train()  # Process one epoch\n      model.eval()  # Eval analogies.\n    # Perform a final save.\n    model.saver.save(session,\n                     os.path.join(opts.save_path, ""model.ckpt""),\n                     global_step=model.global_step)\n    if FLAGS.interactive:\n      # E.g.,\n      # [0]: model.analogy(b\'france\', b\'paris\', b\'russia\')\n      # [1]: model.nearby([b\'proton\', b\'elephant\', b\'maxwell\'])\n      _start_shell(locals())\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
community/en/r1/tutorials/embedding/word2vec_optimized.py,28,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Multi-threaded word2vec unbatched skip-gram model.\n\nTrains the model described in:\n(Mikolov, et. al.) Efficient Estimation of Word Representations in Vector Space\nICLR 2013.\nhttp://arxiv.org/abs/1301.3781\nThis model does true SGD (i.e. no minibatching). To do this efficiently, custom\nops are used to sequentially process data within a \'batch\'.\n\nThe key ops used are:\n* skipgram custom op that does input processing.\n* neg_train custom op that efficiently calculates and applies the gradient using\n  true SGD.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport threading\nimport time\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nimport numpy as np\nimport tensorflow as tf\n\nword2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(__file__)), \'word2vec_ops.so\'))\n\nflags = tf.app.flags\n\nflags.DEFINE_string(""save_path"", None, ""Directory to write the model."")\nflags.DEFINE_string(\n    ""train_data"", None,\n    ""Training data. E.g., unzipped file http://mattmahoney.net/dc/text8.zip."")\nflags.DEFINE_string(\n    ""eval_data"", None, ""Analogy questions. ""\n    ""See README.md for how to get \'questions-words.txt\'."")\nflags.DEFINE_integer(""embedding_size"", 200, ""The embedding dimension size."")\nflags.DEFINE_integer(\n    ""epochs_to_train"", 15,\n    ""Number of epochs to train. Each epoch processes the training data once ""\n    ""completely."")\nflags.DEFINE_float(""learning_rate"", 0.025, ""Initial learning rate."")\nflags.DEFINE_integer(""num_neg_samples"", 25,\n                     ""Negative samples per training example."")\nflags.DEFINE_integer(""batch_size"", 500,\n                     ""Numbers of training examples each step processes ""\n                     ""(no minibatching)."")\nflags.DEFINE_integer(""concurrent_steps"", 12,\n                     ""The number of concurrent training steps."")\nflags.DEFINE_integer(""window_size"", 5,\n                     ""The number of words to predict to the left and right ""\n                     ""of the target word."")\nflags.DEFINE_integer(""min_count"", 5,\n                     ""The minimum number of word occurrences for it to be ""\n                     ""included in the vocabulary."")\nflags.DEFINE_float(""subsample"", 1e-3,\n                   ""Subsample threshold for word occurrence. Words that appear ""\n                   ""with higher frequency will be randomly down-sampled. Set ""\n                   ""to 0 to disable."")\nflags.DEFINE_boolean(\n    ""interactive"", False,\n    ""If true, enters an IPython interactive session to play with the trained ""\n    ""model. E.g., try model.analogy(b\'france\', b\'paris\', b\'russia\') and ""\n    ""model.nearby([b\'proton\', b\'elephant\', b\'maxwell\'])"")\n\nFLAGS = flags.FLAGS\n\n\nclass Options(object):\n  """"""Options used by our word2vec model.""""""\n\n  def __init__(self):\n    # Model options.\n\n    # Embedding dimension.\n    self.emb_dim = FLAGS.embedding_size\n\n    # Training options.\n\n    # The training text file.\n    self.train_data = FLAGS.train_data\n\n    # Number of negative samples per example.\n    self.num_samples = FLAGS.num_neg_samples\n\n    # The initial learning rate.\n    self.learning_rate = FLAGS.learning_rate\n\n    # Number of epochs to train. After these many epochs, the learning\n    # rate decays linearly to zero and the training stops.\n    self.epochs_to_train = FLAGS.epochs_to_train\n\n    # Concurrent training steps.\n    self.concurrent_steps = FLAGS.concurrent_steps\n\n    # Number of examples for one training step.\n    self.batch_size = FLAGS.batch_size\n\n    # The number of words to predict to the left and right of the target word.\n    self.window_size = FLAGS.window_size\n\n    # The minimum number of word occurrences for it to be included in the\n    # vocabulary.\n    self.min_count = FLAGS.min_count\n\n    # Subsampling threshold for word occurrence.\n    self.subsample = FLAGS.subsample\n\n    # Where to write out summaries.\n    self.save_path = FLAGS.save_path\n    if not os.path.exists(self.save_path):\n      os.makedirs(self.save_path)\n\n    # Eval options.\n\n    # The text file for eval.\n    self.eval_data = FLAGS.eval_data\n\n\nclass Word2Vec(object):\n  """"""Word2Vec model (Skipgram).""""""\n\n  def __init__(self, options, session):\n    self._options = options\n    self._session = session\n    self._word2id = {}\n    self._id2word = []\n    self.build_graph()\n    self.build_eval_graph()\n    self.save_vocab()\n\n  def read_analogies(self):\n    """"""Reads through the analogy question file.\n\n    Returns:\n      questions: a [n, 4] numpy array containing the analogy question\'s\n                 word ids.\n      questions_skipped: questions skipped due to unknown words.\n    """"""\n    questions = []\n    questions_skipped = 0\n    with open(self._options.eval_data, ""rb"") as analogy_f:\n      for line in analogy_f:\n        if line.startswith(b"":""):  # Skip comments.\n          continue\n        words = line.strip().lower().split(b"" "")\n        ids = [self._word2id.get(w.strip()) for w in words]\n        if None in ids or len(ids) != 4:\n          questions_skipped += 1\n        else:\n          questions.append(np.array(ids))\n    print(""Eval analogy file: "", self._options.eval_data)\n    print(""Questions: "", len(questions))\n    print(""Skipped: "", questions_skipped)\n    self._analogy_questions = np.array(questions, dtype=np.int32)\n\n  def build_graph(self):\n    """"""Build the model graph.""""""\n    opts = self._options\n\n    # The training data. A text file.\n    (words, counts, words_per_epoch, current_epoch, total_words_processed,\n     examples, labels) = word2vec.skipgram_word2vec(filename=opts.train_data,\n                                                    batch_size=opts.batch_size,\n                                                    window_size=opts.window_size,\n                                                    min_count=opts.min_count,\n                                                    subsample=opts.subsample)\n    (opts.vocab_words, opts.vocab_counts,\n     opts.words_per_epoch) = self._session.run([words, counts, words_per_epoch])\n    opts.vocab_size = len(opts.vocab_words)\n    print(""Data file: "", opts.train_data)\n    print(""Vocab size: "", opts.vocab_size - 1, "" + UNK"")\n    print(""Words per epoch: "", opts.words_per_epoch)\n\n    self._id2word = opts.vocab_words\n    for i, w in enumerate(self._id2word):\n      self._word2id[w] = i\n\n    # Declare all variables we need.\n    # Input words embedding: [vocab_size, emb_dim]\n    w_in = tf.Variable(\n        tf.random_uniform(\n            [opts.vocab_size,\n             opts.emb_dim], -0.5 / opts.emb_dim, 0.5 / opts.emb_dim),\n        name=""w_in"")\n\n    # Global step: scalar, i.e., shape [].\n    w_out = tf.Variable(tf.zeros([opts.vocab_size, opts.emb_dim]), name=""w_out"")\n\n    # Global step: []\n    global_step = tf.Variable(0, name=""global_step"")\n\n    # Linear learning rate decay.\n    words_to_train = float(opts.words_per_epoch * opts.epochs_to_train)\n    lr = opts.learning_rate * tf.maximum(\n        0.0001,\n        1.0 - tf.cast(total_words_processed, tf.float32) / words_to_train)\n\n    # Training nodes.\n    inc = global_step.assign_add(1)\n    with tf.control_dependencies([inc]):\n      train = word2vec.neg_train_word2vec(w_in,\n                                          w_out,\n                                          examples,\n                                          labels,\n                                          lr,\n                                          vocab_count=opts.vocab_counts.tolist(),\n                                          num_negative_samples=opts.num_samples)\n\n    self._w_in = w_in\n    self._examples = examples\n    self._labels = labels\n    self._lr = lr\n    self._train = train\n    self.global_step = global_step\n    self._epoch = current_epoch\n    self._words = total_words_processed\n\n  def save_vocab(self):\n    """"""Save the vocabulary to a file so the model can be reloaded.""""""\n    opts = self._options\n    with open(os.path.join(opts.save_path, ""vocab.txt""), ""w"") as f:\n      for i in xrange(opts.vocab_size):\n        vocab_word = tf.compat.as_text(opts.vocab_words[i]).encode(""utf-8"")\n        f.write(""%s %d\\n"" % (vocab_word,\n                             opts.vocab_counts[i]))\n\n  def build_eval_graph(self):\n    """"""Build the evaluation graph.""""""\n    # Eval graph\n    opts = self._options\n\n    # Each analogy task is to predict the 4th word (d) given three\n    # words: a, b, c.  E.g., a=italy, b=rome, c=france, we should\n    # predict d=paris.\n\n    # The eval feeds three vectors of word ids for a, b, c, each of\n    # which is of size N, where N is the number of analogies we want to\n    # evaluate in one batch.\n    analogy_a = tf.placeholder(dtype=tf.int32)  # [N]\n    analogy_b = tf.placeholder(dtype=tf.int32)  # [N]\n    analogy_c = tf.placeholder(dtype=tf.int32)  # [N]\n\n    # Normalized word embeddings of shape [vocab_size, emb_dim].\n    nemb = tf.nn.l2_normalize(self._w_in, 1)\n\n    # Each row of a_emb, b_emb, c_emb is a word\'s embedding vector.\n    # They all have the shape [N, emb_dim]\n    a_emb = tf.gather(nemb, analogy_a)  # a\'s embs\n    b_emb = tf.gather(nemb, analogy_b)  # b\'s embs\n    c_emb = tf.gather(nemb, analogy_c)  # c\'s embs\n\n    # We expect that d\'s embedding vectors on the unit hyper-sphere is\n    # near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].\n    target = c_emb + (b_emb - a_emb)\n\n    # Compute cosine distance between each pair of target and vocab.\n    # dist has shape [N, vocab_size].\n    dist = tf.matmul(target, nemb, transpose_b=True)\n\n    # For each question (row in dist), find the top 4 words.\n    _, pred_idx = tf.nn.top_k(dist, 4)\n\n    # Nodes for computing neighbors for a given word according to\n    # their cosine distance.\n    nearby_word = tf.placeholder(dtype=tf.int32)  # word id\n    nearby_emb = tf.gather(nemb, nearby_word)\n    nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True)\n    nearby_val, nearby_idx = tf.nn.top_k(nearby_dist,\n                                         min(1000, opts.vocab_size))\n\n    # Nodes in the construct graph which are used by training and\n    # evaluation to run/feed/fetch.\n    self._analogy_a = analogy_a\n    self._analogy_b = analogy_b\n    self._analogy_c = analogy_c\n    self._analogy_pred_idx = pred_idx\n    self._nearby_word = nearby_word\n    self._nearby_val = nearby_val\n    self._nearby_idx = nearby_idx\n\n    # Properly initialize all variables.\n    tf.global_variables_initializer().run()\n\n    self.saver = tf.train.Saver()\n\n  def _train_thread_body(self):\n    initial_epoch, = self._session.run([self._epoch])\n    while True:\n      _, epoch = self._session.run([self._train, self._epoch])\n      if epoch != initial_epoch:\n        break\n\n  def train(self):\n    """"""Train the model.""""""\n    opts = self._options\n\n    initial_epoch, initial_words = self._session.run([self._epoch, self._words])\n\n    workers = []\n    for _ in xrange(opts.concurrent_steps):\n      t = threading.Thread(target=self._train_thread_body)\n      t.start()\n      workers.append(t)\n\n    last_words, last_time = initial_words, time.time()\n    while True:\n      time.sleep(5)  # Reports our progress once a while.\n      (epoch, step, words, lr) = self._session.run(\n          [self._epoch, self.global_step, self._words, self._lr])\n      now = time.time()\n      last_words, last_time, rate = words, now, (words - last_words) / (\n          now - last_time)\n      print(""Epoch %4d Step %8d: lr = %5.3f words/sec = %8.0f\\r"" % (epoch, step,\n                                                                    lr, rate),\n            end="""")\n      sys.stdout.flush()\n      if epoch != initial_epoch:\n        break\n\n    for t in workers:\n      t.join()\n\n  def _predict(self, analogy):\n    """"""Predict the top 4 answers for analogy questions.""""""\n    idx, = self._session.run([self._analogy_pred_idx], {\n        self._analogy_a: analogy[:, 0],\n        self._analogy_b: analogy[:, 1],\n        self._analogy_c: analogy[:, 2]\n    })\n    return idx\n\n  def eval(self):\n    """"""Evaluate analogy questions and reports accuracy.""""""\n\n    # How many questions we get right at precision@1.\n    correct = 0\n\n    try:\n      total = self._analogy_questions.shape[0]\n    except AttributeError as e:\n      raise AttributeError(""Need to read analogy questions."")\n\n    start = 0\n    while start < total:\n      limit = start + 2500\n      sub = self._analogy_questions[start:limit, :]\n      idx = self._predict(sub)\n      start = limit\n      for question in xrange(sub.shape[0]):\n        for j in xrange(4):\n          if idx[question, j] == sub[question, 3]:\n            # Bingo! We predicted correctly. E.g., [italy, rome, france, paris].\n            correct += 1\n            break\n          elif idx[question, j] in sub[question, :3]:\n            # We need to skip words already in the question.\n            continue\n          else:\n            # The correct label is not the precision@1\n            break\n    print()\n    print(""Eval %4d/%d accuracy = %4.1f%%"" % (correct, total,\n                                              correct * 100.0 / total))\n\n  def analogy(self, w0, w1, w2):\n    """"""Predict word w3 as in w0:w1 vs w2:w3.""""""\n    wid = np.array([[self._word2id.get(w, 0) for w in [w0, w1, w2]]])\n    idx = self._predict(wid)\n    for c in [self._id2word[i] for i in idx[0, :]]:\n      if c not in [w0, w1, w2]:\n        print(c)\n        break\n    print(""unknown"")\n\n  def nearby(self, words, num=20):\n    """"""Prints out nearby words given a list of words.""""""\n    ids = np.array([self._word2id.get(x, 0) for x in words])\n    vals, idx = self._session.run(\n        [self._nearby_val, self._nearby_idx], {self._nearby_word: ids})\n    for i in xrange(len(words)):\n      print(""\\n%s\\n====================================="" % (words[i]))\n      for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n        print(""%-20s %6.4f"" % (self._id2word[neighbor], distance))\n\n\ndef _start_shell(local_ns=None):\n  # An interactive shell is useful for debugging/development.\n  import IPython\n  user_ns = {}\n  if local_ns:\n    user_ns.update(local_ns)\n  user_ns.update(globals())\n  IPython.start_ipython(argv=[], user_ns=user_ns)\n\n\ndef main(_):\n  """"""Train a word2vec model.""""""\n  if not FLAGS.train_data or not FLAGS.eval_data or not FLAGS.save_path:\n    print(""--train_data --eval_data and --save_path must be specified."")\n    sys.exit(1)\n  opts = Options()\n  with tf.Graph().as_default(), tf.Session() as session:\n    with tf.device(""/cpu:0""):\n      model = Word2Vec(opts, session)\n      model.read_analogies() # Read analogy questions\n    for _ in xrange(opts.epochs_to_train):\n      model.train()  # Process one epoch\n      model.eval()  # Eval analogies.\n    # Perform a final save.\n    model.saver.save(session, os.path.join(opts.save_path, ""model.ckpt""),\n                     global_step=model.global_step)\n    if FLAGS.interactive:\n      # E.g.,\n      # [0]: model.analogy(b\'france\', b\'paris\', b\'russia\')\n      # [1]: model.nearby([b\'proton\', b\'elephant\', b\'maxwell\'])\n      _start_shell(locals())\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
community/en/r1/tutorials/embedding/word2vec_optimized_test.py,3,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for word2vec_optimized module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nimport word2vec_optimized\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\n\nclass Word2VecTest(tf.test.TestCase):\n\n  def setUp(self):\n    FLAGS.train_data = os.path.join(self.get_temp_dir() + ""test-text.txt"")\n    FLAGS.eval_data = os.path.join(self.get_temp_dir() + ""eval-text.txt"")\n    FLAGS.save_path = self.get_temp_dir()\n    with open(FLAGS.train_data, ""w"") as f:\n      f.write(\n          """"""alice was beginning to get very tired of sitting by her sister on\n          the bank, and of having nothing to do: once or twice she had peeped\n          into the book her sister was reading, but it had no pictures or\n          conversations in it, \'and what is the use of a book,\' thought alice\n          \'without pictures or conversations?\' So she was considering in her own\n          mind (as well as she could, for the hot day made her feel very sleepy\n          and stupid), whether the pleasure of making a daisy-chain would be\n          worth the trouble of getting up and picking the daisies, when suddenly\n          a White rabbit with pink eyes ran close by her.\\n"""""")\n      with open(FLAGS.eval_data, ""w"") as f:\n        f.write(""alice she rabbit once\\n"")\n\n  def testWord2VecOptimized(self):\n    FLAGS.batch_size = 5\n    FLAGS.num_neg_samples = 10\n    FLAGS.epochs_to_train = 1\n    FLAGS.min_count = 0\n    word2vec_optimized.main([])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
community/en/r1/tutorials/embedding/word2vec_test.py,3,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for word2vec module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nimport word2vec\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\n\nclass Word2VecTest(tf.test.TestCase):\n\n  def setUp(self):\n    FLAGS.train_data = os.path.join(self.get_temp_dir(), ""test-text.txt"")\n    FLAGS.eval_data = os.path.join(self.get_temp_dir(), ""eval-text.txt"")\n    FLAGS.save_path = self.get_temp_dir()\n    with open(FLAGS.train_data, ""w"") as f:\n      f.write(\n          """"""alice was beginning to get very tired of sitting by her sister on\n          the bank, and of having nothing to do: once or twice she had peeped\n          into the book her sister was reading, but it had no pictures or\n          conversations in it, \'and what is the use of a book,\' thought alice\n          \'without pictures or conversations?\' So she was considering in her own\n          mind (as well as she could, for the hot day made her feel very sleepy\n          and stupid), whether the pleasure of making a daisy-chain would be\n          worth the trouble of getting up and picking the daisies, when suddenly\n          a White rabbit with pink eyes ran close by her.\\n"""""")\n      with open(FLAGS.eval_data, ""w"") as f:\n        f.write(""alice she rabbit once\\n"")\n\n  def testWord2Vec(self):\n    FLAGS.batch_size = 5\n    FLAGS.num_neg_samples = 10\n    FLAGS.epochs_to_train = 1\n    FLAGS.min_count = 0\n    word2vec.main([])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
community/en/r1/tutorials/image/__init__.py,0,b''
community/en/r1/tutorials/rnn/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Libraries to build Recurrent Neural Networks.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'"
lite/examples/model_personalization/android/transfer_api/generate_test_resources.py,4,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Generate helper TFLite models that are used for tests.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nSOFTMAX_INITIALIZE_ONES_PATH = \'./src/androidTest/assets/model/softmax_initialize_ones.tflite\'\n\n\n@tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32)])\ndef initial_params(zero):\n  ws = tf.fill((7 * 7 * 1280, 5), zero + 1)\n  bs = tf.fill((5,), zero + 1)\n  return ws, bs\n\n\nconverter = tf.lite.TFLiteConverter.from_concrete_functions(\n    [initial_params.get_concrete_function()])\nmodel_lite = converter.convert()\nwith open(SOFTMAX_INITIALIZE_ONES_PATH, \'wb\') as f:\n  f.write(model_lite)\n'"
lite/examples/model_personalization/converter/tfltransfer/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Init module for tfltransfer.""""""\n'"
lite/examples/model_personalization/converter/tfltransfer/model_correctness_test.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""End-to-end tests that check model correctness.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\n# pylint: disable=g-bad-import-order\nfrom tfltransfer import bases\nfrom tfltransfer import optimizers\nfrom tfltransfer import heads\nfrom tfltransfer import tflite_transfer_converter\n# pylint: enable=g-bad-import-order\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 128\nNUM_CLASSES = 5\nVALIDATION_SPLIT = 0.2\nLEARNING_RATE = 0.001\nBOTTLENECK_SHAPE = (7, 7, 1280)\n\nDATASET_URL = \'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\'\n\n\nclass TransferModel(object):\n  """"""Test consumer of models generated by the converter.""""""\n\n  def __init__(self, dataset_dir, base_model, head_model, optimizer):\n    """"""Creates a wrapper for a set of models and a data set.""""""\n    self.dataset_dir = dataset_dir\n\n    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        rescale=1. / 255, validation_split=VALIDATION_SPLIT)\n    self.train_img_generator = datagen.flow_from_directory(\n        self.dataset_dir,\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n        subset=\'training\')\n    self.val_img_generator = datagen.flow_from_directory(\n        self.dataset_dir,\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n        subset=\'validation\')\n\n    converter = tflite_transfer_converter.TFLiteTransferConverter(\n        NUM_CLASSES, base_model, head_model, optimizer, BATCH_SIZE)\n    models = converter._convert()\n    self.initialize_model = models[\'initialize\']\n    self.bottleneck_model = models[\'bottleneck\']\n    self.train_head_model = models[\'train_head\']\n    self.inference_model = models[\'inference\']\n    self.optimizer_model = models[\'optimizer\']\n    self.variables = self._generate_initial_variables()\n\n    optim_state_shapes = self._optimizer_state_shapes()\n    self.optim_state = [\n        np.zeros(shape, dtype=np.float32) for shape in optim_state_shapes\n    ]\n\n  def _generate_initial_variables(self):\n    """"""Generates the initial model variables.""""""\n    interpreter = tf.lite.Interpreter(model_content=self.initialize_model)\n    zero_in = interpreter.get_input_details()[0]\n    variable_outs = interpreter.get_output_details()\n    interpreter.allocate_tensors()\n    interpreter.set_tensor(zero_in[\'index\'], np.float32(0.))\n    interpreter.invoke()\n    return [interpreter.get_tensor(var[\'index\']) for var in variable_outs]\n\n  def _optimizer_state_shapes(self):\n    """"""Reads the shapes of the optimizer parameters (mutable state).""""""\n    interpreter = tf.lite.Interpreter(model_content=self.optimizer_model)\n    num_variables = len(self.variables)\n    optim_state_inputs = interpreter.get_input_details()[num_variables * 2:]\n    return [input_[\'shape\'] for input_ in optim_state_inputs]\n\n  def prepare_bottlenecks(self):\n    """"""Passes all images through the base model and save the bottlenecks.\n\n    This method has to be called before any training or inference.\n    """"""\n    self.train_bottlenecks, self.train_labels = (\n        self._collect_and_generate_bottlenecks(self.train_img_generator))\n    self.val_bottlenecks, self.val_labels = (\n        self._collect_and_generate_bottlenecks(self.val_img_generator))\n\n  def _collect_and_generate_bottlenecks(self, image_gen):\n    """"""Consumes a generator and converts all images to bottlenecks.\n\n    Args:\n      image_gen: A Keras data generator for images to process\n\n    Returns:\n      Two NumPy arrays: (bottlenecks, labels).\n    """"""\n    collected_bottlenecks = np.zeros(\n        (image_gen.samples,) + BOTTLENECK_SHAPE, dtype=np.float32)\n    collected_labels = np.zeros((image_gen.samples, NUM_CLASSES),\n                                dtype=np.float32)\n\n    next_idx = 0\n    for bottlenecks, truth in self._generate_bottlenecks(\n        make_finite(image_gen)):\n      batch_size = bottlenecks.shape[0]\n      collected_bottlenecks[next_idx:next_idx + batch_size] = bottlenecks\n      collected_labels[next_idx:next_idx + batch_size] = truth\n      next_idx += batch_size\n\n    return collected_bottlenecks, collected_labels\n\n  def _generate_bottlenecks(self, image_gen):\n    """"""Generator adapter that passes images through the bottleneck model.\n\n    Args:\n      image_gen: A generator that returns images to be processed. Images are\n        paired with ground truth labels.\n\n    Yields:\n      Bottlenecks from input images, paired with ground truth labels.\n    """"""\n    interpreter = tf.lite.Interpreter(model_content=self.bottleneck_model)\n    [x_in] = interpreter.get_input_details()\n    [bottleneck_out] = interpreter.get_output_details()\n\n    for (x, y) in image_gen:\n      batch_size = x.shape[0]\n      interpreter.resize_tensor_input(x_in[\'index\'],\n                                      (batch_size, IMAGE_SIZE, IMAGE_SIZE, 3))\n      interpreter.allocate_tensors()\n      interpreter.set_tensor(x_in[\'index\'], x)\n      interpreter.invoke()\n      bottleneck = interpreter.get_tensor(bottleneck_out[\'index\'])\n      yield bottleneck, y\n\n  def train_head(self, num_epochs):\n    """"""Trains the head model for a given number of epochs.\n\n    SGD is used as an optimizer.\n\n    Args:\n      num_epochs: how many epochs should be trained\n\n    Returns:\n      A list of train_loss values after every epoch trained.\n\n    Raises:\n      RuntimeError: when prepare_bottlenecks() has not been called.\n    """"""\n    if not hasattr(self, \'train_bottlenecks\'):\n      raise RuntimeError(\'prepare_bottlenecks has not been called\')\n    results = []\n    for _ in range(num_epochs):\n      loss = self._train_one_epoch(\n          self._generate_batches(self.train_bottlenecks, self.train_labels))\n      results.append(loss)\n    return results\n\n  def _generate_batches(self, x, y):\n    """"""Creates a generator that iterates over the data in batches.""""""\n    num_total = x.shape[0]\n    for begin in range(0, num_total, BATCH_SIZE):\n      end = min(begin + BATCH_SIZE, num_total)\n      yield x[begin:end], y[begin:end]\n\n  def _train_one_epoch(self, train_gen):\n    """"""Performs one training epoch.""""""\n    interpreter = tf.lite.Interpreter(model_content=self.train_head_model)\n    interpreter.allocate_tensors()\n    x_in, y_in = interpreter.get_input_details()[:2]\n    variable_ins = interpreter.get_input_details()[2:]\n    loss_out = interpreter.get_output_details()[0]\n    gradient_outs = interpreter.get_output_details()[1:]\n\n    epoch_loss = 0.\n    num_processed = 0\n    for bottlenecks, truth in train_gen:\n      batch_size = bottlenecks.shape[0]\n      if batch_size < BATCH_SIZE:\n        bottlenecks = pad_batch(bottlenecks, BATCH_SIZE)\n        truth = pad_batch(truth, BATCH_SIZE)\n\n      interpreter.set_tensor(x_in[\'index\'], bottlenecks)\n      interpreter.set_tensor(y_in[\'index\'], truth)\n      for variable_in, variable_value in zip(variable_ins, self.variables):\n        interpreter.set_tensor(variable_in[\'index\'], variable_value)\n      interpreter.invoke()\n\n      loss = interpreter.get_tensor(loss_out[\'index\'])\n      gradients = [\n          interpreter.get_tensor(gradient_out[\'index\'])\n          for gradient_out in gradient_outs\n      ]\n\n      self._apply_gradients(gradients)\n      epoch_loss += loss * batch_size\n      num_processed += batch_size\n\n    epoch_loss /= num_processed\n    return epoch_loss\n\n  def _apply_gradients(self, gradients):\n    """"""Applies the optimizer to the model parameters.""""""\n    interpreter = tf.lite.Interpreter(model_content=self.optimizer_model)\n    interpreter.allocate_tensors()\n    num_variables = len(self.variables)\n    variable_ins = interpreter.get_input_details()[:num_variables]\n    gradient_ins = interpreter.get_input_details()[num_variables:num_variables *\n                                                   2]\n    state_ins = interpreter.get_input_details()[num_variables * 2:]\n    variable_outs = interpreter.get_output_details()[:num_variables]\n    state_outs = interpreter.get_output_details()[num_variables:]\n\n    for variable, gradient, variable_in, gradient_in in zip(\n        self.variables, gradients, variable_ins, gradient_ins):\n      interpreter.set_tensor(variable_in[\'index\'], variable)\n      interpreter.set_tensor(gradient_in[\'index\'], gradient)\n\n    for optim_state_elem, state_in in zip(self.optim_state, state_ins):\n      interpreter.set_tensor(state_in[\'index\'], optim_state_elem)\n\n    interpreter.invoke()\n    self.variables = [\n        interpreter.get_tensor(variable_out[\'index\'])\n        for variable_out in variable_outs\n    ]\n    self.optim_state = [\n        interpreter.get_tensor(state_out[\'index\']) for state_out in state_outs\n    ]\n\n  def measure_inference_accuracy(self):\n    """"""Runs the inference model and measures accuracy on the validation set.""""""\n    interpreter = tf.lite.Interpreter(model_content=self.inference_model)\n    bottleneck_in = interpreter.get_input_details()[0]\n    variable_ins = interpreter.get_input_details()[1:]\n    [y_out] = interpreter.get_output_details()\n\n    inference_accuracy = 0.\n    num_processed = 0\n    for bottleneck, truth in self._generate_batches(self.val_bottlenecks,\n                                                    self.val_labels):\n      batch_size = bottleneck.shape[0]\n      interpreter.resize_tensor_input(bottleneck_in[\'index\'],\n                                      (batch_size,) + BOTTLENECK_SHAPE)\n      interpreter.allocate_tensors()\n\n      interpreter.set_tensor(bottleneck_in[\'index\'], bottleneck)\n      for variable_in, variable_value in zip(variable_ins, self.variables):\n        interpreter.set_tensor(variable_in[\'index\'], variable_value)\n      interpreter.invoke()\n\n      preds = interpreter.get_tensor(y_out[\'index\'])\n\n      acc = (np.argmax(preds, axis=1) == np.argmax(truth,\n                                                   axis=1)).sum() / batch_size\n      inference_accuracy += acc * batch_size\n      num_processed += batch_size\n\n    inference_accuracy /= num_processed\n    return inference_accuracy\n\n\ndef make_finite(data_gen):\n  """"""An adapter for Keras data generators that makes them finite.\n\n  The default behavior in Keras is to keep looping infinitely through\n  the data.\n\n  Args:\n    data_gen: An infinite Keras data generator.\n\n  Yields:\n    Same values as the parameter generator.\n  """"""\n  num_samples = data_gen.samples\n  num_processed = 0\n  for batch in data_gen:\n    batch_size = batch[0].shape[0]\n    if batch_size + num_processed > num_samples:\n      batch_size = num_samples - num_processed\n      should_stop = True\n    else:\n      should_stop = False\n    if batch_size == 0:\n      return\n\n    batch = tuple(x[:batch_size] for x in batch)\n    yield batch\n    num_processed += batch_size\n    if should_stop:\n      return\n\n\n# TODO(b/135138207) investigate if we can get rid of this.\ndef pad_batch(batch, batch_size):\n  """"""Resize batch to a given size, tiling present samples over missing.\n\n  Example:\n    Suppose batch_size is 5, batch is [1, 2].\n    Then the return value is [1, 2, 1, 2, 1].\n\n  Args:\n    batch: An ndarray with first dimension size <= batch_size.\n    batch_size: Desired size for first dimension.\n\n  Returns:\n    An ndarray of the same shape, except first dimension has\n    the desired size.\n  """"""\n  padded = np.zeros((batch_size,) + batch.shape[1:], dtype=batch.dtype)\n  next_idx = 0\n  while next_idx < batch_size:\n    fill_len = min(batch.shape[0], batch_size - next_idx)\n    padded[next_idx:next_idx + fill_len] = batch[:fill_len]\n    next_idx += fill_len\n  return padded\n\n\nclass ModelCorrectnessTest(unittest.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(ModelCorrectnessTest, cls).setUpClass()\n    zip_file = tf.keras.utils.get_file(\n        origin=DATASET_URL, fname=\'flower_photos.tgz\', extract=True)\n    cls.dataset_dir = os.path.join(os.path.dirname(zip_file), \'flower_photos\')\n\n    mobilenet_dir = tempfile.mkdtemp(\'tflite-transfer-test\')\n    mobilenet_keras = tf.keras.applications.MobileNetV2(\n        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n        include_top=False,\n        weights=\'imagenet\')\n    tf.keras.experimental.export_saved_model(mobilenet_keras, mobilenet_dir)\n    cls.mobilenet_dir = mobilenet_dir\n\n  def setUp(self):\n    super(ModelCorrectnessTest, self).setUp()\n    self.mobilenet_dir = ModelCorrectnessTest.mobilenet_dir\n    self.dataset_dir = ModelCorrectnessTest.dataset_dir\n\n  def test_mobilenet_v2_saved_model_and_softmax_classifier(self):\n    base_model = bases.SavedModelBase(self.mobilenet_dir)\n    head_model = heads.SoftmaxClassifierHead(BATCH_SIZE, BOTTLENECK_SHAPE,\n                                             NUM_CLASSES)\n    optimizer = optimizers.SGD(LEARNING_RATE)\n    model = TransferModel(self.dataset_dir, base_model, head_model, optimizer)\n    self.assertModelAchievesAccuracy(model, 0.80)\n\n  def test_mobilenet_v2_saved_model_quantized_and_softmax_classifier(self):\n    base_model = bases.SavedModelBase(self.mobilenet_dir, quantize=True)\n    head_model = heads.SoftmaxClassifierHead(BATCH_SIZE, BOTTLENECK_SHAPE,\n                                             NUM_CLASSES)\n    optimizer = optimizers.SGD(LEARNING_RATE)\n    model = TransferModel(self.dataset_dir, base_model, head_model, optimizer)\n    self.assertModelAchievesAccuracy(model, 0.80)\n\n  def test_mobilenet_v2_base_and_softmax_classifier(self):\n    base_model = bases.MobileNetV2Base()\n    head_model = heads.SoftmaxClassifierHead(BATCH_SIZE, BOTTLENECK_SHAPE,\n                                             NUM_CLASSES)\n    optimizer = optimizers.SGD(LEARNING_RATE)\n    model = TransferModel(self.dataset_dir, base_model, head_model, optimizer)\n    self.assertModelAchievesAccuracy(model, 0.80)\n\n  def test_mobilenet_v2_base_and_softmax_classifier_l2(self):\n    base_model = bases.MobileNetV2Base()\n    head_model = heads.SoftmaxClassifierHead(\n        BATCH_SIZE, BOTTLENECK_SHAPE, NUM_CLASSES, l2_reg=0.1)\n    optimizer = optimizers.SGD(LEARNING_RATE)\n    model = TransferModel(self.dataset_dir, base_model, head_model, optimizer)\n    self.assertModelAchievesAccuracy(model, 0.80)\n\n  def test_mobilenet_v2_base_quantized_and_softmax_classifier(self):\n    base_model = bases.MobileNetV2Base(quantize=True)\n    head_model = heads.SoftmaxClassifierHead(BATCH_SIZE, BOTTLENECK_SHAPE,\n                                             NUM_CLASSES)\n    optimizer = optimizers.SGD(LEARNING_RATE)\n    model = TransferModel(self.dataset_dir, base_model, head_model, optimizer)\n    self.assertModelAchievesAccuracy(model, 0.80)\n\n  def test_mobilenet_v2_base_and_softmax_classifier_adam(self):\n    base_model = bases.MobileNetV2Base()\n    head_model = heads.SoftmaxClassifierHead(BATCH_SIZE, BOTTLENECK_SHAPE,\n                                             NUM_CLASSES)\n    optimizer = optimizers.Adam()\n    model = TransferModel(self.dataset_dir, base_model, head_model, optimizer)\n    self.assertModelAchievesAccuracy(model, 0.80)\n\n  def assertModelAchievesAccuracy(self, model, target_accuracy, num_epochs=30):\n    model.prepare_bottlenecks()\n    print(\'Bottlenecks prepared\')\n    history = model.train_head(num_epochs)\n    print(\'Training completed, history = {}\'.format(history))\n    accuracy = model.measure_inference_accuracy()\n    print(\'Final accuracy = {:.2f}\'.format(accuracy))\n    self.assertGreater(accuracy, target_accuracy)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
lite/examples/model_personalization/converter/tfltransfer/tflite_transfer_convert.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""CLI wrapper for tflite_transfer_converter.\n\nConverts a pair of TF models to a TFLite transfer learning model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n# pylint: disable=g-bad-import-order\nfrom tfltransfer import bases\nfrom tfltransfer import heads\nfrom tfltransfer import optimizers\nfrom tfltransfer import tflite_transfer_converter\n# pylint: enable=g-bad-import-order\n\n\ndef main():\n  parser = argparse.ArgumentParser(\n      description=\'Combines two TF models into a transfer learning model\')\n  parser.add_argument(\n      \'--train_batch_size\', help=\'Training batch size\', type=int, default=20)\n  parser.add_argument(\n      \'--num_classes\',\n      help=\'Number of classes for the output\',\n      type=int,\n      default=4)\n\n  # Base model configuration.\n  base_group = parser.add_mutually_exclusive_group(required=True)\n  base_group.add_argument(\n      \'--base_mobilenetv2\',\n      help=\'Use MobileNetV2 as the base model\',\n      dest=\'base_mobilenetv2\',\n      action=\'store_true\')\n  base_group.add_argument(\n      \'--base_model_dir\',\n      help=\'Use a SavedModel under a given path as the base model\',\n      type=str)\n  parser.add_argument(\n      \'--base_quantize\',\n      help=\'Whether the base model should be quantized\',\n      dest=\'base_quantize\',\n      action=\'store_true\')\n  parser.set_defaults(base_quantize=False)\n\n  # Head model configuration.\n  head_group = parser.add_mutually_exclusive_group(required=True)\n  head_group.add_argument(\n      \'--head_model_dir\',\n      help=\'Use a SavedModel under a given path as the head model\',\n      type=str)\n  head_group.add_argument(\n      \'--head_softmax\',\n      help=\'Use SoftmaxClassifier for the head model\',\n      dest=\'head_softmax\',\n      action=\'store_true\')\n  parser.add_argument(\n      \'--head_l2_reg\',\n      help=\'L2 regularization parameter for SoftmaxClassifier\',\n      type=float)\n\n  # Optimizer configuration.\n  parser.add_argument(\n      \'--optimizer\',\n      required=True,\n      type=str,\n      choices=[\'sgd\', \'adam\'],\n      help=\'Which optimizer should be used\')\n  parser.add_argument(\n      \'--sgd_learning_rate\', help=\'Learning rate for SGD\', type=float)\n\n  parser.add_argument(\n      \'--out_model_dir\',\n      help=\'Where the generated transfer learning model is saved\',\n      required=True,\n      type=str)\n  args = parser.parse_args()\n\n  if args.base_mobilenetv2:\n    base = bases.MobileNetV2Base(quantize=args.base_quantize)\n  else:\n    base = bases.SavedModelBase(\n        args.base_model_dir, quantize=args.base_quantize)\n\n  if args.head_model_dir:\n    head = heads.LogitsSavedModelHead(args.head_model_dir)\n  else:\n    head = heads.SoftmaxClassifierHead(\n        args.train_batch_size,\n        base.bottleneck_shape(),\n        args.num_classes,\n        l2_reg=args.head_l2_reg)\n\n  if args.optimizer == \'sgd\':\n    if args.sgd_learning_rate is not None:\n      optimizer = optimizers.SGD(args.sgd_learning_rate)\n    else:\n      raise RuntimeError(\n          \'--sgd_learning_rate is required when SGD is used as an optimizer\')\n  elif args.optimizer == \'adam\':\n    optimizer = optimizers.Adam()\n\n  converter = tflite_transfer_converter.TFLiteTransferConverter(\n      args.num_classes, base, head, optimizer, args.train_batch_size)\n  converter.convert_and_save(args.out_model_dir)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
lite/examples/model_personalization/converter/tfltransfer/tflite_transfer_converter.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TFLite converter for transfer learning models.\n\nThis converter is the first stage in the transfer learning pipeline.\nIt allows to convert a pair of models representing fixed\nbase and trainable head models to a set of TFLite models, which\ncan be then used by the transfer learning library.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.compat import v1 as tfv1\n\n\nclass TFLiteTransferConverter(object):\n  """"""Converter for transfer learning models.\n\n  There are three parts of the input to the converter: base and\n  head model configurations, and the optimizer configuration.\n  Each of them has several variants, defined in the respective\n  submodules, which are configured separately outside of the\n  converter.\n\n  The converter output format is currently a directory containing\n  multiple TFLite models, but this should be considered an\n  implementation detail and not relied upon.\n  """"""\n\n  def __init__(self, num_classes, base_model, head_model, optimizer,\n               train_batch_size):\n    """"""Creates a new converter instance.\n\n    Args:\n      num_classes: number of classes for the classification task.\n      base_model: base model configuration of one of the supported types.\n      head_model: head model configuration of one of the supported types.\n      optimizer: optimizer configuration of one of the supported types.\n      train_batch_size: batch size that will be used for training.\n    """"""\n    self.num_classes = num_classes\n    self.base_model = base_model\n    self.head_model = head_model\n    self.optimizer = optimizer\n    self.train_batch_size = train_batch_size\n\n  def convert_and_save(self, out_model_dir):\n    """"""Saves the converted model to a target directory.""""""\n    if not os.path.isdir(out_model_dir):\n      os.makedirs(out_model_dir)\n    models = self._convert()\n\n    for name, model in models.items():\n      model_file_path = os.path.join(out_model_dir, name + \'.tflite\')\n      with open(model_file_path, \'wb\') as model_file:\n        model_file.write(model)\n\n  def _convert(self):\n    """"""Converts all underlying models.""""""\n    initialize_model_lite = self._generate_initialize_model()\n    bottleneck_model_lite = self._generate_bottleneck_model()\n    train_head_model_lite = self._generate_train_head_model()\n    inference_model_lite = self._generate_inference_model()\n    parameter_shapes = self._read_parameter_shapes(inference_model_lite)\n    optimizer_model_lite = (\n        self.optimizer.generate_optimizer_model(parameter_shapes))\n    return {\n        \'initialize\': initialize_model_lite,\n        \'bottleneck\': bottleneck_model_lite,\n        \'train_head\': train_head_model_lite,\n        \'inference\': inference_model_lite,\n        \'optimizer\': optimizer_model_lite,\n    }\n\n  def _read_parameter_shapes(self, inference_model):\n    """"""Infers shapes of model parameters from the inference model.""""""\n    interpreter = tfv1.lite.Interpreter(model_content=inference_model)\n    return [\n        parameter_in[\'shape\'].tolist()\n        for parameter_in in interpreter.get_input_details()[1:]\n    ]\n\n  def _generate_initialize_model(self):\n    """"""Generates a model that outputs initial parameter values.""""""\n    converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [self.head_model.generate_initial_params().get_concrete_function()])\n    return converter.convert()\n\n  def _generate_bottleneck_model(self):\n    """"""Converts the bottleneck model, i.e. the base model.\n\n    Bottleneck is a name used in the transfer learning context for\n    the base model outputs, which are at the same time head model\n    inputs.\n\n    Returns:\n      TFLite bottleneck model.\n    """"""\n    return self.base_model.tflite_model()\n\n  def _generate_train_head_model(self):\n    """"""Converts the head training model.\n\n    Head training model is constructed from the head model passed\n    as converter input by adding a cross-entropy loss and gradient\n    calculation for all variables in the input SavedModel.\n\n    Returns:\n      TFLite train head model.\n    """"""\n    with tf.Graph().as_default(), tfv1.Session() as sess:\n      bottleneck_shape = ((self.train_batch_size,) +\n                          self.head_model.input_shape())\n      bottleneck = tfv1.placeholder(tf.float32, bottleneck_shape,\n                                    \'placeholder_bottleneck\')\n\n      # One-hot ground truth\n      labels = tfv1.placeholder(tf.float32,\n                                (self.train_batch_size, self.num_classes),\n                                \'placeholder_labels\')\n\n      loss, gradients, variables = self.head_model.train(bottleneck, labels)\n      converter = tfv1.lite.TFLiteConverter.from_session(\n          sess, [bottleneck, labels] + variables, [loss] + gradients)\n\n      if self.head_model.train_requires_flex():\n        converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\n      return converter.convert()\n\n  def _generate_inference_model(self):\n    """"""Converts the head inference model.\n\n    Inference model is constructed from the head model passed\n    as converted input. It accepts as inputs the bottlenecks\n    produces by the base model, and values for all trainable\n    head model parameters.\n\n    Returns:\n      TFLite inference model.\n    """"""\n    with tf.Graph().as_default(), tfv1.Session() as sess:\n      bottleneck_shape = ((1,) + self.head_model.input_shape())\n      bottleneck = tfv1.placeholder(tf.float32, bottleneck_shape,\n                                    \'placeholder_bottleneck\')\n      predictions, head_variables = self.head_model.predict(bottleneck)\n      converter = tfv1.lite.TFLiteConverter.from_session(\n          sess, [bottleneck] + head_variables, [predictions])\n      return converter.convert()\n'"
lite/examples/model_personalization/converter/tfltransfer/tflite_transfer_converter_test.py,4,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for tflite_transfer_converter.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tempfile\nimport unittest\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.regularizers import l2\n\n# pylint: disable=g-bad-import-order\nfrom tfltransfer import bases\nfrom tfltransfer import heads\nfrom tfltransfer import optimizers\nfrom tfltransfer import tflite_transfer_converter\n# pylint: enable=g-bad-import-order\n\nDEFAULT_INPUT_SIZE = 64\nDEFAULT_BATCH_SIZE = 128\nLEARNING_RATE = 0.001\n\n\nclass TestTfliteTransferConverter(unittest.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(TestTfliteTransferConverter, cls).setUpClass()\n    cls._default_base_model_dir = tempfile.mkdtemp(\'tflite-transfer-test-base\')\n    model = tf.keras.Sequential([\n        layers.Dense(\n            units=DEFAULT_INPUT_SIZE, input_shape=(DEFAULT_INPUT_SIZE,))\n    ])\n    model.build()\n    tf.keras.experimental.export_saved_model(model, cls._default_base_model_dir)\n\n  def setUp(self):\n    super(TestTfliteTransferConverter, self).setUp()\n    self._default_base_model = bases.SavedModelBase(\n        TestTfliteTransferConverter._default_base_model_dir)\n\n  def test_mobilenet_v2_saved_model_and_keras_model(self):\n    input_size = DEFAULT_INPUT_SIZE\n    output_size = 5\n\n    head_model = tf.keras.Sequential([\n        layers.Dense(\n            units=32,\n            input_shape=(input_size,),\n            activation=\'relu\',\n            kernel_regularizer=l2(0.01),\n            bias_regularizer=l2(0.01)),\n        layers.Dense(\n            units=output_size,\n            kernel_regularizer=l2(0.01),\n            bias_regularizer=l2(0.01)),\n    ])\n    head_model.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\')\n\n    converter = tflite_transfer_converter.TFLiteTransferConverter(\n        output_size, self._default_base_model, heads.KerasModelHead(head_model),\n        optimizers.SGD(LEARNING_RATE), DEFAULT_BATCH_SIZE)\n\n    models = converter._convert()\n\n    parameter_shapes = [(input_size, 32), (32,), (32, output_size),\n                        (output_size,)]\n    self.assertSignatureEqual(models[\'initialize\'], [()], parameter_shapes)\n    self.assertSignatureEqual(models[\'bottleneck\'], [(1, input_size)],\n                              [(1, input_size)])\n    self.assertSignatureEqual(models[\'inference\'],\n                              [(1, input_size)] + parameter_shapes,\n                              [(1, output_size)])\n    self.assertSignatureEqual(models[\'optimizer\'],\n                              parameter_shapes + parameter_shapes,\n                              parameter_shapes)\n\n  def test_mobilenet_v2_saved_model_and_softmax_classifier_model(self):\n    input_size = DEFAULT_INPUT_SIZE\n    output_size = 5\n    batch_size = DEFAULT_BATCH_SIZE\n\n    converter = tflite_transfer_converter.TFLiteTransferConverter(\n        output_size, self._default_base_model,\n        heads.SoftmaxClassifierHead(batch_size, (input_size,), output_size),\n        optimizers.SGD(LEARNING_RATE), batch_size)\n    models = converter._convert()\n\n    parameter_shapes = [(input_size, output_size), (output_size,)]\n    self.assertSignatureEqual(models[\'initialize\'], [()], parameter_shapes)\n    self.assertSignatureEqual(models[\'bottleneck\'], [(1, input_size)],\n                              [(1, input_size)])\n    self.assertSignatureEqual(models[\'train_head\'],\n                              [(batch_size, input_size),\n                               (batch_size, output_size)] + parameter_shapes,\n                              [()] + parameter_shapes)\n    self.assertSignatureEqual(models[\'inference\'],\n                              [(1, input_size)] + parameter_shapes,\n                              [(1, output_size)])\n    self.assertSignatureEqual(models[\'optimizer\'],\n                              parameter_shapes + parameter_shapes,\n                              parameter_shapes)\n\n  def test_mobilenet_v2_base_and_softmax_classifier_model(self):\n    input_size = 224\n    output_size = 5\n    batch_size = DEFAULT_BATCH_SIZE\n\n    base = bases.MobileNetV2Base(image_size=input_size)\n    head = heads.SoftmaxClassifierHead(batch_size, base.bottleneck_shape(),\n                                       output_size)\n    optimizer = optimizers.SGD(LEARNING_RATE)\n\n    converter = tflite_transfer_converter.TFLiteTransferConverter(\n        output_size, base, head, optimizer, batch_size)\n    models = converter._convert()\n\n    parameter_shapes = [(7 * 7 * 1280, output_size), (output_size,)]\n    self.assertSignatureEqual(models[\'initialize\'], [()], parameter_shapes)\n    self.assertSignatureEqual(models[\'bottleneck\'],\n                              [(1, input_size, input_size, 3)],\n                              [(1, 7, 7, 1280)])\n    self.assertSignatureEqual(models[\'train_head\'],\n                              [(batch_size, 7, 7, 1280),\n                               (batch_size, output_size)] + parameter_shapes,\n                              [()] + parameter_shapes)\n    self.assertSignatureEqual(models[\'inference\'],\n                              [(1, 7, 7, 1280)] + parameter_shapes,\n                              [(1, output_size)])\n    self.assertSignatureEqual(models[\'optimizer\'],\n                              parameter_shapes + parameter_shapes,\n                              parameter_shapes)\n\n  def test_mobilenet_v2_base_and_softmax_classifier_model_adam(self):\n    input_size = 224\n    output_size = 5\n    batch_size = DEFAULT_BATCH_SIZE\n\n    base = bases.MobileNetV2Base(image_size=input_size)\n    head = heads.SoftmaxClassifierHead(batch_size, base.bottleneck_shape(),\n                                       output_size)\n    optimizer = optimizers.Adam()\n\n    converter = tflite_transfer_converter.TFLiteTransferConverter(\n        output_size, base, head, optimizer, batch_size)\n    models = converter._convert()\n\n    param_shapes = [(7 * 7 * 1280, output_size), (output_size,)]\n    self.assertSignatureEqual(\n        models[\'optimizer\'],\n        param_shapes + param_shapes + param_shapes + param_shapes + [()],\n        param_shapes + param_shapes + param_shapes + [()])\n\n  def assertSignatureEqual(self, model, expected_inputs, expected_outputs):\n    interpreter = tf.lite.Interpreter(model_content=model)\n    inputs = [\n        input_[\'shape\'].tolist() for input_ in interpreter.get_input_details()\n    ]\n    outputs = [\n        output[\'shape\'].tolist() for output in interpreter.get_output_details()\n    ]\n    self.assertEqual(inputs, [list(dims) for dims in expected_inputs])\n    self.assertEqual(outputs, [list(dims) for dims in expected_outputs])\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
lite/examples/model_personalization/converter/tfltransfer/utils.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper utilities for various parts of the converter.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.compat import v1 as tfv1\n\n\ndef memoize(method):\n  """"""A simple memoization decorator for zero-parameter methods.""""""\n\n  # We use a class since Python 2 has no \'nonlocal\'.\n  class Memo(object):\n    result = None\n\n  def helper(self):\n    if Memo.result is None:\n      Memo.result = method(self)\n    return Memo.result\n\n  return helper\n\n\ndef convert_constants_to_placeholders(graph_def, constant_names):\n  """"""Converts given constants in a GraphDef to placeholders.""""""\n  constant_names = [tensor_to_op_name(name) for name in constant_names]\n  output_graph_def = tfv1.GraphDef()\n  for input_node in graph_def.node:\n    output_node = tfv1.NodeDef()\n    if input_node.name in constant_names:\n      output_node.op = \'Placeholder\'\n      output_node.name = input_node.name\n      output_node.attr[\'dtype\'].CopyFrom(input_node.attr[\'dtype\'])\n      output_node.attr[\'shape\'].shape.CopyFrom(\n          input_node.attr[\'value\'].tensor.tensor_shape)\n    else:\n      output_node.CopyFrom(input_node)\n    output_graph_def.node.extend([output_node])\n\n  output_graph_def.library.CopyFrom(graph_def.library)\n  return output_graph_def\n\n\ndef tensor_to_op_name(tensor_name):\n  """"""Strips tailing \':N\' part from a tensor name.\n\n  For example, \'dense/kernel:0\', which is a tensor name, is converted\n  to \'dense/kernel\' which is the operation that outputs this tensor.\n\n  Args:\n    tensor_name: tensor name.\n\n  Returns:\n    Corresponding op name.\n  """"""\n  parts = tensor_name.split(\':\')\n  if len(parts) == 1:\n    return tensor_name\n  assert len(parts) == 2\n  return parts[0]\n'"
lite/examples/speech_commands/ml/export/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
lite/examples/speech_commands/ml/export/convert_keras_lite.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\n\nkeras_model = ""../conv_1d_time_stacked_model/ep-084-vl-0.2595.hdf5""\ninput_arrays = [""the_input""]\noutput_arrays = [""the_output""]\n\nconverter = tf.lite.TFLiteConverter\nconverter = converter.from_keras_model_file(keras_model, input_arrays,\n                                            output_arrays)\ntflite_model = converter.convert()\nopen(""converted_speed_keras_model.tflite"", ""wb"").write(tflite_model)\n'"
lite/examples/speech_commands/ml/export/convert_keras_to_quantized.py,5,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Input arguments.\n\nnum_output: this value has nothing to do with the number of classes, batch_size,\netc.,\nand it is mostly equal to 1. If the network is a **multi-stream network**\n(forked network with multiple outputs), set the value to the number of outputs.\n\nquantize: if set to True, use the quantize feature of Tensorflow\n(https://www.tensorflow.org/performance/quantization) [default: False]\n\nuse_theano: Thaeno and Tensorflow implement convolution in different ways.\nWhen using Keras with Theano backend, the order is set to \'channels_first\'.\nThis feature is not fully tested, and doesn\'t work with quantizization [default:\nFalse]\n\ninput_fld: directory holding the keras weights file [default: .]\n\noutput_fld: destination directory to save the tensorflow files [default: .]\n\ninput_model_file: name of the input weight file [default: \'model.h5\']\n\noutput_model_file: name of the output weight file [default:\nargs.input_model_file + \'.pb\']\n\ngraph_def: if set to True, will write the graph definition as an ascii file\n[default: False]\n\noutput_graphdef_file: if graph_def is set to True, the file name of the\ngraph definition [default: model.ascii]\n\noutput_node_prefix: the prefix to use for output nodes. [default: output_node]\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nfrom keras import backend as K\nfrom model import conv_1d_time_stacked_model\nfrom pathlib import Path\nimport tensorflow.compat.v1 as tf\n\nparser = argparse.ArgumentParser(description=\'set input arguments\')\nparser.add_argument(\n    \'-input_fld\', action=\'store\', dest=\'input_fld\', type=str, default=\'.\')\nparser.add_argument(\n    \'-output_fld\', action=\'store\', dest=\'output_fld\', type=str, default=\'\')\nparser.add_argument(\n    \'-input_model_file\',\n    action=\'store\',\n    dest=\'input_model_file\',\n    type=str,\n    default=\'model.h5\')\nparser.add_argument(\n    \'-output_model_file\',\n    action=\'store\',\n    dest=\'output_model_file\',\n    type=str,\n    default=\'\')\nparser.add_argument(\n    \'-output_graphdef_file\',\n    action=\'store\',\n    dest=\'output_graphdef_file\',\n    type=str,\n    default=\'model.ascii\')\nparser.add_argument(\n    \'-num_outputs\', action=\'store\', dest=\'num_outputs\', type=int, default=1)\nparser.add_argument(\n    \'-graph_def\', action=\'store\', dest=\'graph_def\', type=bool, default=False)\nparser.add_argument(\n    \'-output_node_prefix\',\n    action=\'store\',\n    dest=\'output_node_prefix\',\n    type=str,\n    default=\'output_node\')\nparser.add_argument(\n    \'-quantize\', action=\'store\', dest=\'quantize\', type=bool, default=False)\nparser.add_argument(\n    \'-theano_backend\',\n    action=\'store\',\n    dest=\'theano_backend\',\n    type=bool,\n    default=False)\nparser.add_argument(\'-f\')\nargs = parser.parse_args()\nparser.print_help()\nprint(\'input args: \', args)\n\nif args.theano_backend and args.quantize:\n  raise ValueError(\'Quantize feature does not work with theano backend.\')\n\noutput_fld = args.input_fld if not args.output_fld else args.output_fld\nif not args.output_model_file:\n  args.output_model_file = str(Path(args.input_model_file).name) + \'.pb\'\nPath(output_fld).mkdir(parents=True, exist_ok=True)\nweight_file_path = str(Path(args.input_fld) / args.input_model_file)\n\n# Load keras model and rename output\n\n# In[ ]:\n\nK.set_learning_phase(0)\nif args.theano_backend:\n  K.set_image_data_format(\'channels_first\')\nelse:\n  K.set_image_data_format(\'channels_last\')\n\ntry:\n  fingerprint_size = 16000\n  label_count = 12\n  net_model = conv_1d_time_stacked_model(\n      fingerprint_size, num_classes=label_count)\n  net_model.load_weights(\'../conv_1d_time_stacked_model/ep-022-vl-0.2864.hdf5\')\n\nexcept ValueError as err:\n  print(\n      """"""Input file specified ({}) only holds the weights, and not the model definition.\n    Save the model using mode.save(filename.h5) which will contain the network architecture\n    as well as its weights.\n    If the model is saved using model.save_weights(filename.h5), the model architecture is\n    expected to be saved separately in a json format and loaded prior to loading the weights.\n    Check the keras documentation for more details (https://keras.io/getting-started/faq/)""""""\n      .format(weight_file_path))\n  raise err\nnum_output = args.num_outputs\npred = [None] * num_output\npred_node_names = [None] * num_output\nfor i in range(num_output):\n  pred_node_names[i] = args.output_node_prefix + str(i)\n  pred[i] = tf.identity(net_model.outputs[i], name=pred_node_names[i])\nprint(\'output nodes names are: \', pred_node_names)\n\n# [optional] write graph definition in ascii\n\n# In[ ]:\n\nsess = K.get_session()\n\nif args.graph_def:\n  f = args.output_graphdef_file\n  tf.io.write_graph(sess.graph.as_graph_def(), output_fld, f, as_text=True)\n  print(\'saved the graph definition in ascii format at: \',\n        str(Path(output_fld) / f))\n\n# convert variables to constants and save\n\n# In[ ]:\n\nif args.quantize:\n  # graph_transforms will not be available for future versions.\n  from tensorflow.compat.v1.tools.graph_transforms import TransformGraph  # pylint: disable=g-import-not-at-top\n  transforms = [\'quantize_weights\', \'quantize_nodes\']\n  transformed_graph_def = TransformGraph(sess.graph.as_graph_def(), [],\n                                         pred_node_names, transforms)\n  constant_graph = tf.graph_util.convert_variables_to_constants(\n      sess, transformed_graph_def, pred_node_names)\nelse:\n  constant_graph = tf.graph_util.convert_variables_to_constants(\n      sess, sess.graph.as_graph_def(), pred_node_names)\ntf.io.write_graph(\n    constant_graph, output_fld, args.output_model_file, as_text=False)\nprint(\'saved the freezed graph (ready for inference) at: \',\n      str(Path(output_fld) / args.output_model_file))\n'"
tensorflow_examples/lite/model_maker/core/data_util/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
tensorflow_examples/lite/model_maker/core/data_util/dataloader.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Common Dataset used for tasks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass DataLoader(object):\n  """"""This class provides generic utilities for loading customized domain data that will be used later in model retraining.\n\n  For different ML problems or tasks, such as image classification, text\n  classification etc., a subclass is provided to handle task-specific data\n  loading requirements.\n  """"""\n\n  def __init__(self, dataset, size):\n    """"""Init function for class `DataLoader`.\n\n    In most cases, one should use helper functions like `from_folder` to create\n    an instance of this class.\n\n    Args:\n      dataset: A tf.data.Dataset object that contains a potentially large set of\n        elements, where each element is a pair of (input_data, target). The\n        `input_data` means the raw input data, like an image, a text etc., while\n        the `target` means some ground truth of the raw input data, such as the\n        classification label of the image etc.\n      size: The size of the dataset. tf.data.Dataset donesn\'t support a function\n        to get the length directly since it\'s lazy-loaded and may be infinite.\n    """"""\n    self.dataset = dataset\n    self.size = size\n\n  def split(self, fraction):\n    """"""Splits dataset into two sub-datasets with the given fraction.\n\n    Primarily used for splitting the data set into training and testing sets.\n\n    Args:\n      fraction: float, demonstrates the fraction of the first returned\n        subdataset in the original data.\n\n    Returns:\n      The splitted two sub dataset.\n    """"""\n    ds = self.dataset\n\n    train_size = int(self.size * fraction)\n    trainset = DataLoader(ds.take(train_size), train_size)\n\n    test_size = self.size - train_size\n    testset = DataLoader(ds.skip(test_size), test_size)\n\n    return trainset, testset\n'"
tensorflow_examples/lite/model_maker/core/data_util/dataloader_test.py,3,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core.data_util import dataloader\n\n\nclass DataLoaderTest(tf.test.TestCase):\n\n  def test_split(self):\n    ds = tf.data.Dataset.from_tensor_slices([[0, 1], [1, 1], [0, 0], [1, 0]])\n    data = dataloader.DataLoader(ds, 4)\n    train_data, test_data = data.split(0.5)\n\n    self.assertEqual(train_data.size, 2)\n    for i, elem in enumerate(train_data.dataset):\n      self.assertTrue((elem.numpy() == np.array([i, 1])).all())\n\n    self.assertEqual(test_data.size, 2)\n    for i, elem in enumerate(test_data.dataset):\n      self.assertTrue((elem.numpy() == np.array([i, 0])).all())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/core/data_util/image_dataloader.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Image dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport random\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow_examples.lite.model_maker.core.data_util import dataloader\n\n\ndef load_image(path):\n  """"""Loads image.""""""\n  image_raw = tf.io.read_file(path)\n  image_tensor = tf.cond(\n      tf.image.is_jpeg(image_raw),\n      lambda: tf.image.decode_jpeg(image_raw, channels=3),\n      lambda: tf.image.decode_png(image_raw, channels=3))\n  return image_tensor\n\n\ndef create_data(name, data, info, num_classes, label_names):\n  """"""Creates an ImageClassifierDataLoader object from tfds data.""""""\n  if name not in data:\n    return None\n  data = data[name]\n  data = data.map(lambda a: (a[\'image\'], a[\'label\']))\n  size = info.splits[name].num_examples\n  return ImageClassifierDataLoader(data, size, num_classes, label_names)\n\n\ndef load_from_tfds(name):\n  """"""Loads data from tensorflow_datasets.""""""\n  data, info = tfds.load(name, with_info=True)\n  if \'label\' not in info.features:\n    raise ValueError(\'info.features need to contain \\\'label\\\' key.\')\n  num_classes = info.features[\'label\'].num_classes\n  label_names = info.features[\'label\'].names\n\n  train_data = create_data(\'train\', data, info, num_classes, label_names)\n  validation_data = create_data(\'validation\', data, info, num_classes,\n                                label_names)\n  test_data = create_data(\'test\', data, info, num_classes, label_names)\n  return train_data, validation_data, test_data\n\n\nclass ImageClassifierDataLoader(dataloader.DataLoader):\n  """"""DataLoader for image classifier.""""""\n\n  def __init__(self, dataset, size, num_classes, index_to_label):\n    super(ImageClassifierDataLoader, self).__init__(dataset, size)\n    self.num_classes = num_classes\n    self.index_to_label = index_to_label\n\n  def split(self, fraction):\n    """"""Splits dataset into two sub-datasets with the given fraction.\n\n    Primarily used for splitting the data set into training and testing sets.\n\n    Args:\n      fraction: float, demonstrates the fraction of the first returned\n        subdataset in the original data.\n\n    Returns:\n      The splitted two sub dataset.\n    """"""\n    ds = self.dataset\n\n    train_size = int(self.size * fraction)\n    trainset = ImageClassifierDataLoader(\n        ds.take(train_size), train_size, self.num_classes, self.index_to_label)\n\n    test_size = self.size - train_size\n    testset = ImageClassifierDataLoader(\n        ds.skip(train_size), test_size, self.num_classes, self.index_to_label)\n\n    return trainset, testset\n\n  @classmethod\n  def from_folder(cls, filename, shuffle=True):\n    """"""Image analysis for image classification load images with labels.\n\n    Assume the image data of the same label are in the same subdirectory.\n\n    Args:\n      filename: Name of the file.\n      shuffle: boolean, if shuffle, random shuffle data.\n\n    Returns:\n      ImageDataset containing images and labels and other related info.\n    """"""\n    data_root = os.path.abspath(filename)\n\n    # Assumes the image data of the same label are in the same subdirectory,\n    # gets image path and label names.\n    all_image_paths = list(tf.io.gfile.glob(data_root + r\'/*/*\'))\n    all_image_size = len(all_image_paths)\n    if all_image_size == 0:\n      raise ValueError(\'Image size is zero\')\n\n    if shuffle:\n      # Random shuffle data.\n      random.shuffle(all_image_paths)\n\n    label_names = sorted(\n        name for name in os.listdir(data_root)\n        if os.path.isdir(os.path.join(data_root, name)))\n    all_label_size = len(label_names)\n    label_to_index = dict(\n        (name, index) for index, name in enumerate(label_names))\n    all_image_labels = [\n        label_to_index[os.path.basename(os.path.dirname(path))]\n        for path in all_image_paths\n    ]\n\n    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n\n    autotune = tf.data.experimental.AUTOTUNE\n    image_ds = path_ds.map(load_image, num_parallel_calls=autotune)\n\n    # Loads label.\n    label_ds = tf.data.Dataset.from_tensor_slices(\n        tf.cast(all_image_labels, tf.int64))\n\n    # Creates  a dataset if (image, label) pairs.\n    image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n\n    tf.compat.v1.logging.info(\n        \'Load image with size: %d, num_label: %d, labels: %s.\', all_image_size,\n        all_label_size, \', \'.join(label_names))\n    return ImageClassifierDataLoader(image_label_ds, all_image_size,\n                                     all_label_size, label_names)\n'"
tensorflow_examples/lite/model_maker/core/data_util/image_dataloader_test.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport random\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_examples.lite.model_maker.core.data_util import image_dataloader\n\n\ndef _fill_image(rgb, image_size):\n  r, g, b = rgb\n  return np.broadcast_to(\n      np.array([[[r, g, b]]], dtype=np.uint8),\n      shape=(image_size, image_size, 3))\n\n\ndef _write_filled_jpeg_file(path, rgb, image_size):\n  tf.keras.preprocessing.image.save_img(path, _fill_image(rgb, image_size),\n                                        \'channels_last\', \'jpeg\')\n\n\nclass ImageDataLoaderTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(ImageDataLoaderTest, self).setUp()\n    self.image_path = os.path.join(self.get_temp_dir(), \'random_image_dir\')\n    if os.path.exists(self.image_path):\n      return\n    os.mkdir(self.image_path)\n    for class_name in (\'daisy\', \'tulips\'):\n      class_subdir = os.path.join(self.image_path, class_name)\n      os.mkdir(class_subdir)\n      _write_filled_jpeg_file(\n          os.path.join(class_subdir, \'0.jpeg\'),\n          [random.uniform(0, 255) for _ in range(3)], 224)\n\n  def test_split(self):\n    ds = tf.data.Dataset.from_tensor_slices([[0, 1], [1, 1], [0, 0], [1, 0]])\n    data = image_dataloader.ImageClassifierDataLoader(ds, 4, 2, [\'pos\', \'neg\'])\n    train_data, test_data = data.split(0.5)\n\n    self.assertEqual(train_data.size, 2)\n    for i, elem in enumerate(train_data.dataset):\n      self.assertTrue((elem.numpy() == np.array([i, 1])).all())\n    self.assertEqual(train_data.num_classes, 2)\n    self.assertEqual(train_data.index_to_label, [\'pos\', \'neg\'])\n\n    self.assertEqual(test_data.size, 2)\n    for i, elem in enumerate(test_data.dataset):\n      self.assertTrue((elem.numpy() == np.array([i, 0])).all())\n    self.assertEqual(test_data.num_classes, 2)\n    self.assertEqual(test_data.index_to_label, [\'pos\', \'neg\'])\n\n  def test_from_folder(self):\n    data = image_dataloader.ImageClassifierDataLoader.from_folder(\n        self.image_path)\n\n    self.assertEqual(data.size, 2)\n    self.assertEqual(data.num_classes, 2)\n    self.assertEqual(data.index_to_label, [\'daisy\', \'tulips\'])\n    for image, label in data.dataset:\n      self.assertTrue(label.numpy() == 1 or label.numpy() == 0)\n      if label.numpy() == 0:\n        raw_image_tensor = image_dataloader.load_image(\n            os.path.join(self.image_path, \'daisy\', \'0.jpeg\'))\n      else:\n        raw_image_tensor = image_dataloader.load_image(\n            os.path.join(self.image_path, \'tulips\', \'0.jpeg\'))\n      self.assertTrue((image.numpy() == raw_image_tensor.numpy()).all())\n\n  def test_load_from_tfds(self):\n    train_data, validation_data, test_data = image_dataloader.load_from_tfds(\n        \'beans\')\n    self.assertIsInstance(train_data.dataset, tf.data.Dataset)\n    self.assertEqual(train_data.size, 1034)\n    self.assertEqual(train_data.num_classes, 3)\n    self.assertEqual(train_data.index_to_label,\n                     [\'angular_leaf_spot\', \'bean_rust\', \'healthy\'])\n\n    self.assertIsInstance(validation_data.dataset, tf.data.Dataset)\n    self.assertEqual(validation_data.size, 133)\n    self.assertEqual(validation_data.num_classes, 3)\n    self.assertEqual(validation_data.index_to_label,\n                     [\'angular_leaf_spot\', \'bean_rust\', \'healthy\'])\n\n    self.assertIsInstance(test_data.dataset, tf.data.Dataset)\n    self.assertEqual(test_data.size, 128)\n    self.assertEqual(test_data.num_classes, 3)\n    self.assertEqual(test_data.index_to_label,\n                     [\'angular_leaf_spot\', \'bean_rust\', \'healthy\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/core/data_util/text_dataloader.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Text dataloader.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport hashlib\nimport json\nimport os\nimport random\nimport tempfile\n\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import file_util\nfrom tensorflow_examples.lite.model_maker.core.data_util import dataloader\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec as ms\n\nfrom official.nlp.bert import input_pipeline\nfrom official.nlp.data import classifier_data_lib\nfrom official.nlp.data import squad_lib\n\n\ndef _load(tfrecord_file, meta_data_file, model_spec, is_training=None):\n  """"""Loads data from tfrecord file and metada file.""""""\n\n  if is_training is None:\n    name_to_features = model_spec.get_name_to_features()\n  else:\n    name_to_features = model_spec.get_name_to_features(is_training=is_training)\n\n  dataset = input_pipeline.single_file_dataset(tfrecord_file, name_to_features)\n  dataset = dataset.map(\n      model_spec.select_data_from_record,\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n  meta_data = file_util.load_json_file(meta_data_file)\n\n  logging.info(\n      \'Load preprocessed data and metadata from %s and %s \'\n      \'with size: %d\', tfrecord_file, meta_data_file, meta_data[\'size\'])\n  return dataset, meta_data\n\n\ndef _get_cache_filenames(cache_dir, model_spec, data_name):\n  """"""Gets cache tfrecord filename, metada filename and prefix of filenames.""""""\n  hasher = hashlib.md5()\n  hasher.update(data_name.encode(\'utf-8\'))\n  hasher.update(str(model_spec.get_config()).encode(\'utf-8\'))\n  cache_prefix = os.path.join(cache_dir, hasher.hexdigest())\n  cache_tfrecord_file = cache_prefix + \'.tfrecord\'\n  cache_meta_data_file = cache_prefix + \'_meta_data\'\n\n  return cache_tfrecord_file, cache_meta_data_file, cache_prefix\n\n\ndef _write_meta_data(meta_data_file, meta_data):\n  """"""Writes meta data into file.""""""\n  with tf.io.gfile.GFile(meta_data_file, \'w\') as f:\n    json.dump(meta_data, f)\n\n\ndef _get_cache_info(cache_dir, data_name, model_spec):\n  """"""Gets cache related information: whether is cached, related filenames.""""""\n  if cache_dir is None:\n    cache_dir = tempfile.mkdtemp()\n  tfrecord_file, meta_data_file, file_prefix = _get_cache_filenames(\n      cache_dir, model_spec, data_name)\n  is_cached = tf.io.gfile.exists(tfrecord_file) and tf.io.gfile.exists(\n      meta_data_file)\n\n  return is_cached, tfrecord_file, meta_data_file, file_prefix\n\n\nclass TextClassifierDataLoader(dataloader.DataLoader):\n  """"""DataLoader for text classifier.""""""\n\n  def __init__(self, dataset, size, num_classes, index_to_label):\n    super(TextClassifierDataLoader, self).__init__(dataset, size)\n    self.num_classes = num_classes\n    self.index_to_label = index_to_label\n\n  def split(self, fraction):\n    """"""Splits dataset into two sub-datasets with the given fraction.\n\n    Primarily used for splitting the data set into training and testing sets.\n\n    Args:\n      fraction: float, demonstrates the fraction of the first returned\n        subdataset in the original data.\n\n    Returns:\n      The splitted two sub dataset.\n    """"""\n    ds = self.dataset\n\n    train_size = int(self.size * fraction)\n    trainset = TextClassifierDataLoader(\n        ds.take(train_size), train_size, self.num_classes, self.index_to_label)\n\n    test_size = self.size - train_size\n    testset = TextClassifierDataLoader(\n        ds.skip(train_size), test_size, self.num_classes, self.index_to_label)\n\n    return trainset, testset\n\n  @classmethod\n  def from_folder(cls,\n                  filename,\n                  model_spec=ms.AverageWordVecModelSpec(),\n                  is_training=True,\n                  class_labels=None,\n                  shuffle=True,\n                  cache_dir=None):\n    """"""Loads text with labels and preproecess text according to `model_spec`.\n\n    Assume the text data of the same label are in the same subdirectory. each\n    file is one text.\n\n    Args:\n      filename: Name of the file.\n      model_spec: Specification for the model.\n      is_training: Whether the loaded data is for training or not.\n      class_labels: Class labels that should be considered. Name of the\n        subdirectory not in `class_labels` will be ignored. If None, all the\n        subdirectories will be considered.\n      shuffle: boolean, if shuffle, random shuffle data.\n      cache_dir: The cache directory to save preprocessed data. If None,\n        generates a temporary directory to cache preprocessed data.\n\n    Returns:\n      TextDataset containing text, labels and other related info.\n    """"""\n    data_root = os.path.abspath(filename)\n    folder_name = os.path.basename(data_root)\n\n    is_cached, tfrecord_file, meta_data_file, vocab_file = cls._get_cache_info(\n        cache_dir, folder_name, model_spec, is_training)\n    # If cached, directly loads data from cache directory.\n    if is_cached:\n      return cls._load_data(tfrecord_file, meta_data_file, model_spec)\n\n    # Gets paths of all text.\n    if class_labels:\n      all_text_paths = []\n      for class_label in class_labels:\n        all_text_paths.extend(\n            list(\n                tf.io.gfile.glob(os.path.join(data_root, class_label) + r\'/*\')))\n    else:\n      all_text_paths = list(tf.io.gfile.glob(data_root + r\'/*/*\'))\n\n    all_text_size = len(all_text_paths)\n    if all_text_size == 0:\n      raise ValueError(\'Text size is zero\')\n\n    if shuffle:\n      random.shuffle(all_text_paths)\n\n    # Gets label and its index.\n    if class_labels:\n      label_names = sorted(class_labels)\n    else:\n      label_names = sorted(\n          name for name in os.listdir(data_root)\n          if os.path.isdir(os.path.join(data_root, name)))\n\n    # Generates text examples from folder.\n    examples = []\n    for i, path in enumerate(all_text_paths):\n      with tf.io.gfile.GFile(path, \'r\') as f:\n        text = f.read()\n      guid = \'%s-%d\' % (folder_name, i)\n      label = os.path.basename(os.path.dirname(path))\n      examples.append(classifier_data_lib.InputExample(guid, text, None, label))\n\n    # Saves preprocessed data and other assets into files.\n    cls._save_data(examples, model_spec, label_names, tfrecord_file,\n                   meta_data_file, vocab_file, is_training)\n\n    # Loads data from cache directory.\n    return cls._load_data(tfrecord_file, meta_data_file, model_spec)\n\n  @classmethod\n  def from_csv(cls,\n               filename,\n               text_column,\n               label_column,\n               fieldnames=None,\n               model_spec=ms.AverageWordVecModelSpec(),\n               is_training=True,\n               delimiter=\',\',\n               quotechar=\'""\',\n               shuffle=False,\n               cache_dir=None):\n    """"""Loads text with labels from the csv file and preproecess text according to `model_spec`.\n\n    Args:\n      filename: Name of the file.\n      text_column: String, Column name for input text.\n      label_column: String, Column name for labels.\n      fieldnames: A sequence, used in csv.DictReader. If fieldnames is omitted,\n        the values in the first row of file f will be used as the fieldnames.\n      model_spec: Specification for the model.\n      is_training: Whether the loaded data is for training or not.\n      delimiter: Character used to separate fields.\n      quotechar: Character used to quote fields containing special characters.\n      shuffle: boolean, if shuffle, random shuffle data.\n      cache_dir: The cache directory to save preprocessed data. If None,\n        generates a temporary directory to cache preprocessed data.\n\n    Returns:\n      TextDataset containing text, labels and other related info.\n    """"""\n    csv_name = os.path.basename(filename)\n\n    is_cached, tfrecord_file, meta_data_file, vocab_file = cls._get_cache_info(\n        cache_dir, csv_name, model_spec, is_training)\n    # If cached, directly loads data from cache directory.\n    if is_cached:\n      return cls._load_data(tfrecord_file, meta_data_file, model_spec)\n\n    lines = cls._read_csv(filename, fieldnames, delimiter, quotechar)\n    if shuffle:\n      random.shuffle(lines)\n\n    # Gets labels.\n    label_set = set()\n    for line in lines:\n      label_set.add(line[label_column])\n    label_names = sorted(label_set)\n\n    # Generates text examples from csv file.\n    examples = []\n    for i, line in enumerate(lines):\n      text, label = line[text_column], line[label_column]\n      guid = \'%s-%d\' % (csv_name, i)\n      examples.append(classifier_data_lib.InputExample(guid, text, None, label))\n\n    # Saves preprocessed data and other assets into files.\n    cls._save_data(examples, model_spec, label_names, tfrecord_file,\n                   meta_data_file, vocab_file, is_training)\n\n    # Loads data from cache directory.\n    return cls._load_data(tfrecord_file, meta_data_file, model_spec)\n\n  @classmethod\n  def _load_data(cls, tfrecord_file, meta_data_file, model_spec):\n    """"""Gets `TextClassifierDataLoader` object from tfrecord file and metadata file.""""""\n\n    dataset, meta_data = _load(tfrecord_file, meta_data_file, model_spec)\n    return TextClassifierDataLoader(dataset, meta_data[\'size\'],\n                                    meta_data[\'num_classes\'],\n                                    meta_data[\'index_to_label\'])\n\n  @classmethod\n  def _save_data(cls, examples, model_spec, label_names, tfrecord_file,\n                 meta_data_file, vocab_file, is_training):\n    """"""Saves preprocessed data and other assets into files.""""""\n    # If needed, generates and saves vocabulary in vocab_file=None,\n    if model_spec.need_gen_vocab and is_training:\n      model_spec.gen_vocab(examples)\n      model_spec.save_vocab(vocab_file)\n\n    # Converts examples into preprocessed features and saves in tfrecord_file.\n    model_spec.convert_examples_to_features(examples, tfrecord_file,\n                                            label_names)\n\n    # Generates and saves meta data in meta_data_file.\n    meta_data = {\n        \'size\': len(examples),\n        \'num_classes\': len(label_names),\n        \'index_to_label\': label_names\n    }\n    _write_meta_data(meta_data_file, meta_data)\n\n  @classmethod\n  def _get_cache_info(cls, cache_dir, data_name, model_spec, is_training):\n    """"""Gets cache related information for text classifier.""""""\n    is_cached, tfrecord_file, meta_data_file, file_prefix = _get_cache_info(\n        cache_dir, data_name, model_spec)\n\n    vocab_file = file_prefix + \'_vocab\'\n    if is_cached:\n      if model_spec.need_gen_vocab and is_training:\n        model_spec.load_vocab(vocab_file)\n      is_cached = True\n    return is_cached, tfrecord_file, meta_data_file, vocab_file\n\n  @classmethod\n  def _read_csv(cls, input_file, fieldnames=None, delimiter=\',\', quotechar=\'""\'):\n    """"""Reads a separated value file.""""""\n    with tf.io.gfile.GFile(input_file, \'r\') as f:\n      reader = csv.DictReader(\n          f, fieldnames=fieldnames, delimiter=delimiter, quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines\n\n\nclass QuestionAnswerDataLoader(dataloader.DataLoader):\n  """"""DataLoader for question answering.""""""\n\n  def __init__(self, dataset, size, version_2_with_negative, examples, features,\n               squad_file):\n    super(QuestionAnswerDataLoader, self).__init__(dataset, size)\n    self.version_2_with_negative = version_2_with_negative\n    self.examples = examples\n    self.features = features\n    self.squad_file = squad_file\n\n  @classmethod\n  def from_squad(cls,\n                 filename,\n                 model_spec,\n                 is_training=True,\n                 version_2_with_negative=False,\n                 cache_dir=None):\n    """"""Loads data in SQuAD format and preproecess text according to `model_spec`.\n\n    Args:\n      filename: Name of the file.\n      model_spec: Specification for the model.\n      is_training: Whether the loaded data is for training or not.\n      version_2_with_negative: Whether it\'s SQuAD 2.0 format.\n      cache_dir: The cache directory to save preprocessed data. If None,\n        generates a temporary directory to cache preprocessed data.\n\n    Returns:\n      QuestionAnswerDataLoader object.\n    """"""\n    file_base_name = os.path.basename(filename)\n    is_cached, tfrecord_file, meta_data_file, _ = _get_cache_info(\n        cache_dir, file_base_name, model_spec)\n    # If cached, directly loads data from cache directory.\n    if is_cached and is_training:\n      dataset, meta_data = _load(tfrecord_file, meta_data_file, model_spec,\n                                 is_training)\n      return QuestionAnswerDataLoader(\n          dataset=dataset,\n          size=meta_data[\'size\'],\n          version_2_with_negative=meta_data[\'version_2_with_negative\'],\n          examples=[],\n          features=[],\n          squad_file=filename)\n\n    meta_data, examples, features = cls._generate_tf_record_from_squad_file(\n        filename, model_spec, tfrecord_file, is_training,\n        version_2_with_negative)\n\n    _write_meta_data(meta_data_file, meta_data)\n\n    dataset, meta_data = _load(tfrecord_file, meta_data_file, model_spec,\n                               is_training)\n    return QuestionAnswerDataLoader(dataset, meta_data[\'size\'],\n                                    meta_data[\'version_2_with_negative\'],\n                                    examples, features, filename)\n\n  @classmethod\n  def _generate_tf_record_from_squad_file(cls,\n                                          input_file_path,\n                                          model_spec,\n                                          output_path,\n                                          is_training,\n                                          version_2_with_negative=False):\n    """"""Generates and saves training/validation data into a tf record file.""""""\n    examples = squad_lib.read_squad_examples(\n        input_file=input_file_path,\n        is_training=is_training,\n        version_2_with_negative=version_2_with_negative)\n    writer = squad_lib.FeatureWriter(\n        filename=output_path, is_training=is_training)\n\n    features = []\n\n    def _append_feature(feature, is_padding):\n      if not is_padding:\n        features.append(feature)\n      writer.process_feature(feature)\n\n    if is_training:\n      batch_size = None\n    else:\n      batch_size = model_spec.predict_batch_size\n\n    number_of_examples = model_spec.convert_examples_to_features(\n        examples=examples,\n        is_training=is_training,\n        output_fn=writer.process_feature if is_training else _append_feature,\n        batch_size=batch_size)\n    writer.close()\n\n    meta_data = {\n        \'size\': number_of_examples,\n        \'version_2_with_negative\': version_2_with_negative\n    }\n\n    if is_training:\n      examples = []\n    return meta_data, examples, features\n'"
tensorflow_examples/lite/model_maker/core/data_util/text_dataloader_test.py,27,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport csv\nimport json\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.data_util import text_dataloader\nfrom official.nlp.bert import tokenization\nfrom official.nlp.data import squad_lib\n\n\nclass MockClassifierModelSpec(object):\n  need_gen_vocab = False\n\n  def __init__(self, seq_len=4):\n    self.seq_len = seq_len\n\n  def get_name_to_features(self):\n    """"""Gets the dictionary describing the features.""""""\n    name_to_features = {\n        \'input_ids\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'label_ids\': tf.io.FixedLenFeature([], tf.int64),\n    }\n    return name_to_features\n\n  def select_data_from_record(self, record):\n    """"""Dispatches records to features and labels.""""""\n    x = record[\'input_ids\']\n    y = record[\'label_ids\']\n    return (x, y)\n\n  def convert_examples_to_features(self, examples, tfrecord_file, label_names):\n    """"""Converts examples to features and write them into TFRecord file.""""""\n    writer = tf.io.TFRecordWriter(tfrecord_file)\n\n    label_to_id = dict((name, i) for i, name in enumerate(label_names))\n    for example in examples:\n      features = collections.OrderedDict()\n\n      label_id = label_to_id[example.label]\n      input_ids = [label_id] * self.seq_len\n\n      features[\'input_ids\'] = tf.train.Feature(\n          int64_list=tf.train.Int64List(value=list(input_ids)))\n      features[\'label_ids\'] = tf.train.Feature(\n          int64_list=tf.train.Int64List(value=list([label_id])))\n      tf_example = tf.train.Example(\n          features=tf.train.Features(feature=features))\n      writer.write(tf_example.SerializeToString())\n    writer.close()\n\n  def get_config(self):\n    return {\'seq_len\': self.seq_len}\n\n\nclass MockQAModelSpec(object):\n\n  def __init__(self, vocab_dir):\n    self.seq_len = 384\n    self.predict_batch_size = 8\n    self.query_len = 64\n    self.doc_stride = 128\n\n    vocab_file = os.path.join(vocab_dir, \'vocab.txt\')\n    vocab = [\'[PAD]\', \'[UNK]\', \'[CLS]\', \'[SEP]\', \'[MASK]\', \'good\', \'bad\']\n    with open(vocab_file, \'w\') as f:\n      f.write(\'\\n\'.join(vocab))\n    self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case=True)\n\n  def get_name_to_features(self, is_training):\n    """"""Gets the dictionary describing the features.""""""\n    name_to_features = {\n        \'input_ids\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'input_mask\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'segment_ids\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n    }\n\n    if is_training:\n      name_to_features[\'start_positions\'] = tf.io.FixedLenFeature([], tf.int64)\n      name_to_features[\'end_positions\'] = tf.io.FixedLenFeature([], tf.int64)\n    else:\n      name_to_features[\'unique_ids\'] = tf.io.FixedLenFeature([], tf.int64)\n\n    return name_to_features\n\n  def select_data_from_record(self, record):\n    """"""Dispatches records to features and labels.""""""\n    x, y = {}, {}\n    for name, tensor in record.items():\n      if name in (\'start_positions\', \'end_positions\'):\n        y[name] = tensor\n      elif name == \'input_ids\':\n        x[\'input_word_ids\'] = tensor\n      elif name == \'segment_ids\':\n        x[\'input_type_ids\'] = tensor\n      else:\n        x[name] = tensor\n    return (x, y)\n\n  def get_config(self):\n    """"""Gets the configuration.""""""\n    # Only preprocessing related variables are included.\n    return {\n        \'seq_len\': self.seq_len,\n        \'query_len\': self.query_len,\n        \'doc_stride\': self.doc_stride\n    }\n\n  def convert_examples_to_features(self, examples, is_training, output_fn,\n                                   batch_size):\n    """"""Converts examples to features and write them into TFRecord file.""""""\n    return squad_lib.convert_examples_to_features(\n        examples=examples,\n        tokenizer=self.tokenizer,\n        max_seq_length=self.seq_len,\n        doc_stride=self.doc_stride,\n        max_query_length=self.query_len,\n        is_training=is_training,\n        output_fn=output_fn,\n        batch_size=batch_size)\n\n\nclass LoaderFunctionTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(LoaderFunctionTest, self).setUp()\n    self.model_spec = MockClassifierModelSpec()\n\n  def test_load(self):\n    tfrecord_file = self._get_tfrecord_file()\n    meta_data_file = self._get_meta_data_file()\n    dataset, meta_data = text_dataloader._load(tfrecord_file, meta_data_file,\n                                               self.model_spec)\n    for i, (input_ids, label_ids) in enumerate(dataset):\n      self.assertEqual(i, 0)\n      self.assertTrue((input_ids.numpy() == [0, 1, 2, 3]).all())\n      self.assertTrue((label_ids.numpy() == [0]).all())\n    self.assertEqual(meta_data[\'size\'], 1)\n    self.assertEqual(meta_data[\'num_classes\'], 1)\n    self.assertEqual(meta_data[\'index_to_label\'], [\'0\'])\n\n  def test_get_cache_filenames(self):\n    tfrecord_file, meta_data_file, prefix = text_dataloader._get_cache_filenames(\n        cache_dir=\'/tmp\', model_spec=self.model_spec, data_name=\'train\')\n    self.assertTrue(tfrecord_file.startswith(prefix))\n    self.assertTrue(meta_data_file.startswith(prefix))\n\n    _, _, new_dir_prefix = text_dataloader._get_cache_filenames(\n        cache_dir=\'/tmp1\', model_spec=self.model_spec, data_name=\'train\')\n    self.assertNotEqual(new_dir_prefix, prefix)\n\n    _, _, new_model_spec_prefix = text_dataloader._get_cache_filenames(\n        cache_dir=\'/tmp\',\n        model_spec=MockClassifierModelSpec(seq_len=8),\n        data_name=\'train\')\n    self.assertNotEqual(new_model_spec_prefix, prefix)\n\n    _, _, new_data_name_prefix = text_dataloader._get_cache_filenames(\n        cache_dir=\'/tmp\', model_spec=self.model_spec, data_name=\'test\')\n    self.assertNotEqual(new_data_name_prefix, prefix)\n\n  def _get_tfrecord_file(self):\n    tfrecord_file = os.path.join(self.get_temp_dir(), \'tmp.tfrecord\')\n    writer = tf.io.TFRecordWriter(tfrecord_file)\n    input_ids = tf.train.Int64List(value=[0, 1, 2, 3])\n    label_ids = tf.train.Int64List(value=[0])\n    features = collections.OrderedDict()\n    features[\'input_ids\'] = tf.train.Feature(int64_list=input_ids)\n    features[\'label_ids\'] = tf.train.Feature(int64_list=label_ids)\n    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n    writer.write(tf_example.SerializeToString())\n    writer.close()\n    return tfrecord_file\n\n  def _get_meta_data_file(self):\n    meta_data_file = os.path.join(self.get_temp_dir(), \'tmp_meta_data\')\n    meta_data = {\'size\': 1, \'num_classes\': 1, \'index_to_label\': [\'0\']}\n    with tf.io.gfile.GFile(meta_data_file, \'w\') as f:\n      json.dump(meta_data, f)\n    return meta_data_file\n\n\nclass TextClassifierDataLoaderTest(tf.test.TestCase):\n  TEST_LABELS_AND_TEXT = ((\'pos\', \'super good\'), (\'neg\', \'really bad\'))\n\n  def _get_folder_path(self):\n    folder_path = os.path.join(self.get_temp_dir(), \'random_text_dir\')\n    if os.path.exists(folder_path):\n      return\n    os.mkdir(folder_path)\n\n    for label, text in self.TEST_LABELS_AND_TEXT:\n      class_subdir = os.path.join(folder_path, label)\n      os.mkdir(class_subdir)\n      with open(os.path.join(class_subdir, \'0.txt\'), \'w\') as f:\n        f.write(text)\n    return folder_path\n\n  def _get_csv_file(self):\n    csv_file = os.path.join(self.get_temp_dir(), \'tmp.csv\')\n    if os.path.exists(csv_file):\n      return csv_file\n    fieldnames = [\'text\', \'label\']\n    with open(csv_file, \'w\') as f:\n      writer = csv.DictWriter(f, fieldnames=fieldnames)\n      writer.writeheader()\n      for label, text in self.TEST_LABELS_AND_TEXT:\n        writer.writerow({\'text\': text, \'label\': label})\n    return csv_file\n\n  def test_split(self):\n    ds = tf.data.Dataset.from_tensor_slices([[0, 1], [1, 1], [0, 0], [1, 0]])\n    data = text_dataloader.TextClassifierDataLoader(ds, 4, 2, [\'pos\', \'neg\'])\n    train_data, test_data = data.split(0.5)\n\n    self.assertEqual(train_data.size, 2)\n    for i, elem in enumerate(train_data.dataset):\n      self.assertTrue((elem.numpy() == np.array([i, 1])).all())\n    self.assertEqual(train_data.num_classes, 2)\n    self.assertEqual(train_data.index_to_label, [\'pos\', \'neg\'])\n\n    self.assertEqual(test_data.size, 2)\n    for i, elem in enumerate(test_data.dataset):\n      self.assertTrue((elem.numpy() == np.array([i, 0])).all())\n    self.assertEqual(test_data.num_classes, 2)\n    self.assertEqual(test_data.index_to_label, [\'pos\', \'neg\'])\n\n  def test_from_csv(self):\n    csv_file = self._get_csv_file()\n    model_spec = MockClassifierModelSpec()\n    data = text_dataloader.TextClassifierDataLoader.from_csv(\n        csv_file,\n        text_column=\'text\',\n        label_column=\'label\',\n        model_spec=model_spec)\n    self._test_data(data, model_spec)\n\n  def test_from_folder(self):\n    folder_path = self._get_folder_path()\n    model_spec = MockClassifierModelSpec()\n    data = text_dataloader.TextClassifierDataLoader.from_folder(\n        folder_path, model_spec=model_spec)\n    self._test_data(data, model_spec)\n\n  def _test_data(self, data, model_spec):\n    self.assertEqual(data.size, 2)\n    self.assertEqual(data.num_classes, 2)\n    self.assertEqual(data.index_to_label, [\'neg\', \'pos\'])\n    for input_ids, label in data.dataset:\n      self.assertTrue(label.numpy() == 1 or label.numpy() == 0)\n      actual_input_ids = [label.numpy()] * model_spec.seq_len\n      self.assertTrue((input_ids.numpy() == actual_input_ids).all())\n\n\nclass QuestionAnswerDataLoaderTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (\'train-v1.1.json\', True, False, 1),\n      (\'dev-v1.1.json\', False, False, 8),\n      (\'train-v2.0.json\', True, True, 2),\n      (\'dev-v2.0.json\', False, True, 8),\n  )\n  def test_from_squad(self, test_file, is_training, version_2_with_negative,\n                      size):\n\n    path = test_util.get_test_data_path(\'squad_testdata\')\n    squad_path = os.path.join(path, test_file)\n    model_spec = MockQAModelSpec(self.get_temp_dir())\n    data = text_dataloader.QuestionAnswerDataLoader.from_squad(\n        squad_path,\n        model_spec,\n        is_training=is_training,\n        version_2_with_negative=version_2_with_negative)\n\n    self.assertIsInstance(data, text_dataloader.QuestionAnswerDataLoader)\n    self.assertEqual(data.size, size)\n    self.assertEqual(data.version_2_with_negative, version_2_with_negative)\n    self.assertEqual(data.squad_file, squad_path)\n\n    if is_training:\n      self.assertEmpty(data.features)\n      self.assertEmpty(data.examples)\n    else:\n      self.assertNotEmpty(data.features)\n      self.assertIsInstance(data.features[0], squad_lib.InputFeatures)\n\n      self.assertEqual(len(data.features), len(data.examples))\n      self.assertIsInstance(data.examples[0], squad_lib.SquadExample)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/core/optimization/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
tensorflow_examples/lite/model_maker/core/optimization/warmup.py,5,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions and classes related to optimization (weight updates).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v2 as tf\n\n\nclass WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Applies a warmup schedule on a given learning rate decay schedule.""""""\n\n  def __init__(self,\n               initial_learning_rate,\n               decay_schedule_fn,\n               warmup_steps,\n               name=None):\n    super(WarmUp, self).__init__()\n    self.initial_learning_rate = initial_learning_rate\n    self.warmup_steps = warmup_steps\n    self.decay_schedule_fn = decay_schedule_fn\n    self.name = name\n\n  def __call__(self, step):\n    with tf.name_scope(self.name or \'WarmUp\') as name:\n      # Implements linear warmup. i.e., if global_step < warmup_steps, the\n      # learning rate will be `global_step/num_warmup_steps * init_lr`.\n      global_step_float = tf.cast(step, tf.float32)\n      warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n      warmup_percent_done = global_step_float / warmup_steps_float\n      warmup_learning_rate = self.initial_learning_rate * warmup_percent_done\n      return tf.cond(\n          global_step_float < warmup_steps_float,\n          lambda: warmup_learning_rate,\n          lambda: self.decay_schedule_fn(step),\n          name=name)\n\n  def get_config(self):\n    return {\n        \'initial_learning_rate\': self.initial_learning_rate,\n        \'decay_schedule_fn\': self.decay_schedule_fn,\n        \'warmup_steps\': self.warmup_steps,\n        \'name\': self.name\n    }\n'"
tensorflow_examples/lite/model_maker/core/task/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
tensorflow_examples/lite/model_maker/core/task/classification_model.py,3,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Custom classification model that is already retained by data.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core.task import custom_model\n\n\nclass ClassificationModel(custom_model.CustomModel):\n  """"""""The abstract base class that represents a Tensorflow classification model.""""""\n\n  def __init__(self, model_spec, index_to_label, num_classes, shuffle,\n               train_whole_model):\n    """"""Initialize a instance with data, deploy mode and other related parameters.\n\n    Args:\n      model_spec: Specification for the model.\n      index_to_label: A list that map from index to label class name.\n      num_classes: Number of label classes.\n      shuffle: Whether the data should be shuffled.\n      train_whole_model: If true, the Hub module is trained together with the\n        classification layer on top. Otherwise, only train the top\n        classification layer.\n    """"""\n    super(ClassificationModel, self).__init__(model_spec, shuffle)\n    self.index_to_label = index_to_label\n    self.num_classes = num_classes\n    self.train_whole_model = train_whole_model\n\n  def evaluate(self, data, batch_size=32):\n    """"""Evaluates the model.\n\n    Args:\n      data: Data to be evaluated.\n      batch_size: Number of samples per evaluation step.\n\n    Returns:\n      The loss value and accuracy.\n    """"""\n    ds = self._gen_dataset(data, batch_size, is_training=False)\n\n    return self.model.evaluate(ds)\n\n  def predict_top_k(self, data, k=1, batch_size=32):\n    """"""Predicts the top-k predictions.\n\n    Args:\n      data: Data to be evaluated.\n      k: Number of top results to be predicted.\n      batch_size: Number of samples per evaluation step.\n\n    Returns:\n      top k results. Each one is (label, probability).\n    """"""\n    if k < 0:\n      raise ValueError(\'K should be equal or larger than 0.\')\n    ds = self._gen_dataset(data, batch_size, is_training=False)\n\n    predicted_prob = self.model.predict(ds)\n    topk_prob, topk_id = tf.math.top_k(predicted_prob, k=k)\n    topk_label = np.array(self.index_to_label)[topk_id.numpy()]\n\n    label_prob = []\n    for label, prob in zip(topk_label, topk_prob.numpy()):\n      label_prob.append(list(zip(label, prob)))\n\n    return label_prob\n\n  def _export_labels(self, label_filepath):\n    if label_filepath is None:\n      raise ValueError(""Label filepath couldn\'t be None when exporting labels."")\n\n    tf.compat.v1.logging.info(\'Saving labels in %s.\', label_filepath)\n    with tf.io.gfile.GFile(label_filepath, \'w\') as f:\n      f.write(\'\\n\'.join(self.index_to_label))\n'"
tensorflow_examples/lite/model_maker/core/task/classification_model_test.py,3,"b""# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.task import classification_model\n\n\nclass MockClassificationModel(classification_model.ClassificationModel):\n\n  def train(self, train_data, validation_data=None, **kwargs):\n    pass\n\n  def export(self, **kwargs):\n    pass\n\n  def evaluate(self, data, **kwargs):\n    pass\n\n\nclass ClassificationModelTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(ClassificationModelTest, self).setUp()\n    self.num_classes = 2\n    self.model = MockClassificationModel(\n        model_spec=None,\n        index_to_label=['pos', 'neg'],\n        num_classes=2,\n        train_whole_model=False,\n        shuffle=False)\n\n  def test_predict_top_k(self):\n    input_shape = [24, 24, 3]\n    self.model.model = test_util.build_model(input_shape, self.num_classes)\n    data = test_util.get_dataloader(2, input_shape, self.num_classes)\n\n    topk_results = self.model.predict_top_k(data, k=2, batch_size=1)\n    for topk_result in topk_results:\n      top1_result, top2_result = topk_result[0], topk_result[1]\n      top1_label, top1_prob = top1_result[0], top1_result[1]\n      top2_label, top2_prob = top2_result[0], top2_result[1]\n\n      self.assertIn(top1_label, self.model.index_to_label)\n      self.assertIn(top2_label, self.model.index_to_label)\n      self.assertNotEqual(top1_label, top2_label)\n\n      self.assertLessEqual(top1_prob, 1)\n      self.assertGreaterEqual(top1_prob, top2_prob)\n      self.assertGreaterEqual(top2_prob, 0)\n\n      self.assertEqual(top1_prob + top2_prob, 1.0)\n\n  def test_export_labels(self):\n    labels_output_file = os.path.join(self.get_temp_dir(), 'label')\n    self.model._export_labels(labels_output_file)\n    with tf.io.gfile.GFile(labels_output_file, 'r') as f:\n      labels = [label.strip() for label in f]\n    self.assertEqual(labels, ['pos', 'neg'])\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tensorflow_examples/lite/model_maker/core/task/configs.py,20,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Configurations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\n\nDEFAULT_QUANTIZATION_STEPS = 2000\n\n\ndef _get_representative_dataset_gen(dataset, num_steps):\n  """"""Gets the function that generates representative dataset for quantized.""""""\n\n  def representative_dataset_gen():\n    """"""Generates representative dataset for quantized.""""""\n    if compat.get_tf_behavior() == 2:\n      for image, _ in dataset.take(num_steps):\n        yield [image]\n    else:\n      iterator = tf.compat.v1.data.make_initializable_iterator(\n          dataset.take(num_steps))\n      next_element = iterator.get_next()\n      with tf.compat.v1.Session() as sess:\n        sess.run(iterator.initializer)\n        while True:\n          try:\n            image, _ = sess.run(next_element)\n            yield [image]\n          except tf.errors.OutOfRangeError:\n            break\n\n  return representative_dataset_gen\n\n\nclass QuantizationConfig(object):\n  """"""Configuration for post-training quantization.\n\n  Refer to\n  https://www.tensorflow.org/lite/performance/post_training_quantization\n  for different post-training quantization options.\n  """"""\n\n  def __init__(\n      self,\n      optimizations=None,\n      representative_data=None,\n      quantization_steps=None,\n      inference_input_type=None,\n      inference_output_type=None,\n      supported_ops=None,\n  ):\n    """"""Constructs QuantizationConfig.\n\n    Args:\n      optimizations: A list of optimizations to apply when converting the model.\n        If not set, use `[Optimize.DEFAULT]` by default.\n      representative_data: Representative data used for post-training\n        quantization.\n      quantization_steps: Number of post-training quantization calibration steps\n        to run.\n      inference_input_type: Target data type of real-number input arrays. Allows\n        for a different type for input arrays. Defaults to None. If set, must be\n        be `{tf.float32, tf.uint8, tf.int8}`.\n      inference_output_type: Target data type of real-number output arrays.\n        Allows for a different type for output arrays. Defaults to None. If set,\n        must be `{tf.float32, tf.uint8, tf.int8}`.\n      supported_ops: Set of OpsSet options supported by the device. Used to Set\n        converter.target_spec.supported_ops.\n    """"""\n\n    if optimizations is None:\n      optimizations = [tf.lite.Optimize.DEFAULT]\n    if not isinstance(optimizations, list):\n      optimizations = [optimizations]\n    self.optimizations = optimizations\n\n    self.representative_data = representative_data\n    if self.representative_data is not None and quantization_steps is None:\n      quantization_steps = DEFAULT_QUANTIZATION_STEPS\n    self.quantization_steps = quantization_steps\n\n    self.inference_input_type = inference_input_type\n    self.inference_output_type = inference_output_type\n\n    if supported_ops is not None and not isinstance(supported_ops, list):\n      supported_ops = [supported_ops]\n    self.supported_ops = supported_ops\n\n  @classmethod\n  def create_dynamic_range_quantization(cls,\n                                        optimizations=tf.lite.Optimize.DEFAULT):\n    """"""Creates configuration for dynamic range quantization.""""""\n    return QuantizationConfig(optimizations)\n\n  @classmethod\n  def create_full_integer_quantization(\n      cls,\n      representative_data,\n      quantization_steps=DEFAULT_QUANTIZATION_STEPS,\n      optimizations=tf.lite.Optimize.DEFAULT,\n      inference_input_type=tf.uint8,\n      inference_output_type=tf.uint8,\n      is_integer_only=False):\n    """"""Creates configuration for full integer quantization.\n\n    Args:\n      representative_data: Representative data used for post-training\n        quantization.\n      quantization_steps: Number of post-training quantization calibration steps\n        to run.\n      optimizations: A list of optimizations to apply when converting the model.\n        If not set, use `[Optimize.DEFAULT]` by default.\n      inference_input_type: Target data type of real-number input arrays. Used\n        only when `is_integer_only` is True. Must be in `{tf.uint8, tf.int8}`.\n      inference_output_type: Target data type of real-number output arrays. Used\n        only when `is_integer_only` is True. Must be in `{tf.uint8, tf.int8}`.\n      is_integer_only: If True, enforces full integer quantization for all ops\n        including the input and output. If False, uses integer with float\n        fallback (using default float input/output) that mean to fully integer\n        quantize a model, but use float operators when they don\'t have an\n        integer implementation.\n\n    Returns:\n      QuantizationConfig.\n    """"""\n    if not is_integer_only:\n      return QuantizationConfig(\n          optimizations,\n          representative_data=representative_data,\n          quantization_steps=quantization_steps)\n    else:\n      if inference_input_type not in [tf.uint8, tf.int8]:\n        raise ValueError(\'For integer only quantization, \'\n                         \'`inference_input_type` \'\n                         \'should be tf.uint8 or tf.int8.\')\n      if inference_output_type not in [tf.uint8, tf.int8]:\n        raise ValueError(\'For integer only quantization, \'\n                         \'`inference_output_type` \'\n                         \'should be tf.uint8 or tf.int8.\')\n\n      return QuantizationConfig(\n          optimizations,\n          representative_data=representative_data,\n          quantization_steps=quantization_steps,\n          inference_input_type=inference_input_type,\n          inference_output_type=inference_output_type,\n          supported_ops=[tf.lite.OpsSet.TFLITE_BUILTINS_INT8])\n\n  @classmethod\n  def create_float16_quantization(cls, optimizations=tf.lite.Optimize.DEFAULT):\n    """"""Creates configuration for float16 quantization.""""""\n    return QuantizationConfig(optimizations, supported_ops=[tf.float16])\n\n  def get_converter_with_quantization(self, converter, gen_dataset_fn=None):\n    """"""Gets TFLite converter with settings for quantization.""""""\n    converter.optimizations = self.optimizations\n\n    if self.representative_data is not None:\n      if gen_dataset_fn is None:\n        raise ValueError(\'Must provide ""gen_dataset_fn"" when\'\n                         \'""representative_data"" is not None.\')\n      ds = gen_dataset_fn(\n          self.representative_data, batch_size=1, is_training=False)\n      converter.representative_dataset = tf.lite.RepresentativeDataset(\n          _get_representative_dataset_gen(ds, self.quantization_steps))\n\n    if self.inference_input_type:\n      converter.inference_input_type = self.inference_input_type\n    if self.inference_output_type:\n      converter.inference_output_type = self.inference_output_type\n    if self.supported_ops:\n      converter.target_spec.supported_ops = self.supported_ops\n    return converter\n'"
tensorflow_examples/lite/model_maker/core/task/custom_model.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Base custom model that is already retained by data.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport os\n\nimport tensorflow as tf\n\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\n\n\nclass CustomModel(abc.ABC):\n  """"""""The abstract base class that represents a Tensorflow classification model.""""""\n\n  DEFAULT_EXPORT_FORMAT = []\n  ALLOWED_EXPORT_FORMAT = []\n\n  def __init__(self, model_spec, shuffle):\n    """"""Initialize a instance with data, deploy mode and other related parameters.\n\n    Args:\n      model_spec: Specification for the model.\n      shuffle: Whether the data should be shuffled.\n    """"""\n    self.model_spec = model_spec\n    self.shuffle = shuffle\n    self.model = None\n\n  def preprocess(self, sample_data, label):\n    """"""Preprocess the data.""""""\n    # TODO(yuqili): remove this method once preprocess for image classifier is\n    # also moved to DataLoader part.\n    return sample_data, label\n\n  @abc.abstractmethod\n  def train(self, train_data, validation_data=None, **kwargs):\n    return\n\n  def summary(self):\n    self.model.summary()\n\n  @abc.abstractmethod\n  def evaluate(self, data, **kwargs):\n    return\n\n  # TODO(b/155949323): Refactor the code for gen_dataset in CustomModel to a\n  # seperated  dataloader.\n  def _get_dataset_fn(self, input_data, global_batch_size, is_training):\n    """"""Gets a closure to create a dataset.""""""\n\n    def _dataset_fn(ctx=None):\n      """"""Returns tf.data.Dataset for question answer retraining.""""""\n      batch_size = ctx.get_per_replica_batch_size(\n          global_batch_size) if ctx else global_batch_size\n      dataset = self._gen_dataset(\n          input_data,\n          batch_size,\n          is_training=is_training,\n          input_pipeline_context=ctx)\n      return dataset\n\n    return _dataset_fn\n\n  def _get_input_fn_and_steps(self, data, batch_size, is_training):\n    """"""Gets input_fn and steps for training/evaluation.""""""\n    if data is None:\n      input_fn = None\n      steps = 0\n    else:\n      input_fn = self._get_dataset_fn(data, batch_size, is_training)\n      steps = data.size // batch_size\n    return input_fn, steps\n\n  def _gen_dataset(self,\n                   data,\n                   batch_size=32,\n                   is_training=True,\n                   input_pipeline_context=None):\n    """"""Generates training / validation dataset.""""""\n    # The dataset is always sharded by number of hosts.\n    # num_input_pipelines is the number of hosts rather than number of cores.\n    ds = data.dataset\n    if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1:\n      ds = ds.shard(input_pipeline_context.num_input_pipelines,\n                    input_pipeline_context.input_pipeline_id)\n\n    ds = ds.map(\n        self.preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n    if is_training:\n      if self.shuffle:\n        ds = ds.shuffle(buffer_size=min(data.size, 100))\n      ds = ds.repeat()\n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\n  def _get_export_format(self, export_format):\n    """"""Get export format.""""""\n    if export_format is None:\n      export_format = self.DEFAULT_EXPORT_FORMAT\n\n    if not isinstance(export_format, list):\n      export_format = [export_format]\n\n    # Checks whether each export format is allowed.\n    for e_format in export_format:\n      if e_format not in self.ALLOWED_EXPORT_FORMAT:\n        raise ValueError(\'Export format %s is not allowed.\' % e_format)\n\n    return export_format\n\n  def export(self,\n             export_dir,\n             tflite_filename=\'model.tflite\',\n             label_filename=\'labels.txt\',\n             vocab_filename=\'vocab\',\n             saved_model_filename=\'saved_model\',\n             export_format=None,\n             **kwargs):\n    """"""Converts the retrained model based on `export_format`.\n\n    Args:\n      export_dir: The directory to save exported files.\n      tflite_filename: File name to save tflite model. The full export path is\n        {export_dir}/{tflite_filename}.\n      label_filename: File name to save labels. The full export path is\n        {export_dir}/{label_filename}.\n      vocab_filename: File name to save vocabulary.  The full export path is\n        {export_dir}/{vocab_filename}.\n      saved_model_filename: Path to SavedModel or H5 file to save the model. The\n        full export path is\n        {export_dir}/{saved_model_filename}/{saved_model.pb|assets|variables}.\n      export_format: List of export format that could be saved_model, tflite,\n        label, vocab.\n      **kwargs: Other parameters like `quantized` for TFLITE model.\n    """"""\n    export_format = self._get_export_format(export_format)\n\n    if not tf.io.gfile.exists(export_dir):\n      tf.io.gfile.makedirs(export_dir)\n\n    if ExportFormat.LABEL in export_format:\n      label_filepath = os.path.join(export_dir, label_filename)\n      self._export_labels(label_filepath)\n\n    if ExportFormat.TFLITE in export_format:\n      tflite_filepath = os.path.join(export_dir, tflite_filename)\n      self._export_tflite(tflite_filepath, **kwargs)\n\n    if ExportFormat.SAVED_MODEL in export_format:\n      saved_model_filepath = os.path.join(export_dir, saved_model_filename)\n      self._export_saved_model(saved_model_filepath, **kwargs)\n\n    if ExportFormat.VOCAB in export_format:\n      vocab_filepath = os.path.join(export_dir, vocab_filename)\n      self.model_spec.save_vocab(vocab_filepath)\n\n  def _export_saved_model(self,\n                          filepath,\n                          overwrite=True,\n                          include_optimizer=True,\n                          save_format=None,\n                          signatures=None,\n                          options=None):\n    """"""Saves the model to Tensorflow SavedModel or a single HDF5 file.\n\n    Args:\n      filepath: String, path to SavedModel or H5 file to save the model.\n      overwrite: Whether to silently overwrite any existing file at the target\n        location, or provide the user with a manual prompt.\n      include_optimizer: If True, save optimizer\'s state together.\n      save_format: Either \'tf\' or \'h5\', indicating whether to save the model to\n        Tensorflow SavedModel or HDF5. Defaults to \'tf\' in TF 2.X, and \'h5\' in\n        TF 1.X.\n      signatures: Signatures to save with the SavedModel. Applicable to the \'tf\'\n        format only. Please see the `signatures` argument in\n        `tf.saved_model.save` for details.\n      options: Optional `tf.saved_model.SaveOptions` object that specifies\n        options for saving to SavedModel.\n    """"""\n    if filepath is None:\n      raise ValueError(\n          ""SavedModel filepath couldn\'t be None when exporting to SavedModel."")\n    self.model.save(filepath, overwrite, include_optimizer, save_format,\n                    signatures, options)\n'"
tensorflow_examples/lite/model_maker/core/task/custom_model_test.py,2,"b""# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.task import custom_model\n\n\nclass MockCustomModel(custom_model.CustomModel):\n\n  def train(self, train_data, validation_data=None, **kwargs):\n    pass\n\n  def export(self, **kwargs):\n    pass\n\n  def evaluate(self, data, **kwargs):\n    pass\n\n\nclass CustomModelTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(CustomModelTest, self).setUp()\n    self.model = MockCustomModel(\n        model_spec=None,\n        shuffle=False)\n\n  def test_gen_dataset(self):\n    input_dim = 8\n    data = test_util.get_dataloader(\n        data_size=2, input_shape=[input_dim], num_classes=2)\n\n    ds = self.model._gen_dataset(data, batch_size=1, is_training=False)\n    expected = list(data.dataset.as_numpy_iterator())\n    for i, (feature, label) in enumerate(ds):\n      expected_feature = [expected[i][0]]\n      expected_label = [expected[i][1]]\n      self.assertTrue((feature.numpy() == expected_feature).any())\n      self.assertEqual(label.numpy(), expected_label)\n\n  def test_export_saved_model(self):\n    self.model.model = test_util.build_model(input_shape=[4], num_classes=2)\n    saved_model_filepath = os.path.join(self.get_temp_dir(), 'saved_model/')\n    self.model._export_saved_model(saved_model_filepath)\n    self.assertTrue(os.path.isdir(saved_model_filepath))\n    self.assertNotEqual(len(os.listdir(saved_model_filepath)), 0)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tensorflow_examples/lite/model_maker/core/task/hub_loader.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Handles both V1 and V2 modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_hub as hub\n\n\nclass HubKerasLayerV1V2(hub.KerasLayer):\n  """"""Class to loads TF v1 and TF v1 hub modules that could be fine-tuned.\n\n  Since TF v1 modules couldn\'t be retrained in hub.KerasLayer. This class\n  provides a workaround for retraining the whole tf1 model in tf2. In\n  particular, it extract self._func._self_unconditional_checkpoint_dependencies\n  into trainable variable in tf1.\n\n  Doesn\'t update moving-mean/moving-variance for BatchNormalization during\n  fine-tuning.\n  """"""\n\n  def _setup_layer(self, trainable=False, **kwargs):\n    if self._is_hub_module_v1:\n      self._setup_layer_v1(trainable, **kwargs)\n    else:\n      # call _setup_layer from the base class for v2.\n      super(HubKerasLayerV1V2, self)._setup_layer(trainable, **kwargs)\n\n  def _check_trainability(self):\n    if self._is_hub_module_v1:\n      self._check_trainability_v1()\n    else:\n      # call _check_trainability from the base class for v2.\n      super(HubKerasLayerV1V2, self)._check_trainability()\n\n  def _setup_layer_v1(self, trainable=False, **kwargs):\n    """"""Constructs keras layer with relevant weights and losses.""""""\n    super(HubKerasLayerV1V2, self)._setup_layer(trainable=trainable, **kwargs)\n\n    if not self._is_hub_module_v1:\n      raise ValueError(\n          \'Only supports to set up v1 hub module in this function.\')\n\n    # v2 trainable_variable:\n    if hasattr(self._func, \'trainable_variables\'):\n      trainable_variables = {id(v) for v in self._func.trainable_variables}\n    else:\n      trainable_variables = set()\n\n    if not hasattr(self._func, \'_self_unconditional_checkpoint_dependencies\'):\n      raise ValueError(\'_func doesn\\\'t contains attribute \'\n                       \'_self_unconditional_checkpoint_dependencies.\')\n    dependencies = self._func._self_unconditional_checkpoint_dependencies  # pylint: disable=protected-access\n\n    # Adds trainable variables.\n    for dep in dependencies:\n      if dep.name == \'variables\':\n        for v in dep.ref:\n          if id(v) not in trainable_variables:\n            self._add_existing_weight(v, trainable=True)\n\n  def _check_trainability_v1(self):\n    """"""""Ignores trainability checks for V1.""""""\n    if self._is_hub_module_v1:\n      return  # Nothing to do.\n'"
tensorflow_examples/lite/model_maker/core/task/hub_loader_test.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf\n\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.task import hub_loader\n\n\nclass HubKerasLayerV1V2Test(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (""hub_module_v1_mini"", True),\n      (""saved_model_v2_mini"", True),\n      (""hub_module_v1_mini"", False),\n      (""saved_model_v2_mini"", False),\n  )\n  def test_load_with_defaults(self, module_name, trainable):\n    inputs, expected_outputs = 10., 11.  # Test modules perform increment op.\n    path = test_util.get_test_data_path(module_name)\n    layer = hub_loader.HubKerasLayerV1V2(path, trainable=trainable)\n    output = layer(inputs)\n    self.assertEqual(output, expected_outputs)\n\n  def test_trainable_varaible(self):\n    path = test_util.get_test_data_path(""hub_module_v1_mini_train"")\n    layer = hub_loader.HubKerasLayerV1V2(path, trainable=True)\n    self.assertLen(layer.trainable_variables, 2)\n    self.assertLen(layer.variables, 4)\n\n    layer = hub_loader.HubKerasLayerV1V2(path, trainable=False)\n    self.assertEmpty(layer.trainable_variables)\n    self.assertLen(layer.variables, 2)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/core/task/image_classifier.py,8,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""ImageClassier class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_examples.lite.model_maker.core.task import metadata_writer_for_image_classifier as metadata_writer\n\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\nfrom tensorflow_examples.lite.model_maker.core.task import classification_model\nfrom tensorflow_examples.lite.model_maker.core.task import hub_loader\nfrom tensorflow_examples.lite.model_maker.core.task import image_preprocessing\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec as ms\nfrom tensorflow_examples.lite.model_maker.core.task import model_util\nfrom tensorflow_examples.lite.model_maker.core.task import train_image_classifier_lib\n\nfrom tensorflow_hub.tools.make_image_classifier import make_image_classifier_lib as hub_lib\nfrom tflite_support import metadata as _metadata  # pylint: disable=g-direct-tensorflow-import\n\n\ndef get_hub_lib_hparams(**kwargs):\n  """"""Gets the hyperparameters for the tensorflow hub\'s library.""""""\n  hparams = hub_lib.get_default_hparams()\n  return train_image_classifier_lib.add_params(hparams, **kwargs)\n\n\ndef create(train_data,\n           model_spec=ms.efficientnet_lite0_spec,\n           validation_data=None,\n           batch_size=None,\n           epochs=None,\n           train_whole_model=None,\n           dropout_rate=None,\n           learning_rate=None,\n           momentum=None,\n           shuffle=False,\n           use_augmentation=False,\n           use_hub_library=True,\n           warmup_steps=None,\n           model_dir=None):\n  """"""Loads data and retrains the model based on data for image classification.\n\n  Args:\n    train_data: Training data.\n    model_spec: Specification for the model.\n    validation_data: Validation data. If None, skips validation process.\n    batch_size: Number of samples per training step. If `use_hub_library` is\n      False, it represents the base learning rate when train batch size is 256\n      and it\'s linear to the batch size.\n    epochs: Number of epochs for training.\n    train_whole_model: If true, the Hub module is trained together with the\n      classification layer on top. Otherwise, only train the top classification\n      layer.\n    dropout_rate: The rate for dropout.\n    learning_rate: Base learning rate when train batch size is 256. Linear to\n      the batch size.\n    momentum: a Python float forwarded to the optimizer. Only used when\n      `use_hub_library` is True.\n    shuffle: Whether the data should be shuffled.\n    use_augmentation: Use data augmentation for preprocessing.\n    use_hub_library: Use `make_image_classifier_lib` from tensorflow hub to\n      retrain the model.\n    warmup_steps: Number of warmup steps for warmup schedule on learning rate.\n      If None, the default warmup_steps is used which is the total training\n      steps in two epochs. Only used when `use_hub_library` is False.\n    model_dir: The location of the model checkpoint files. Only used when\n      `use_hub_library` is False.\n\n  Returns:\n    An instance of ImageClassifier class.\n  """"""\n  if compat.get_tf_behavior() not in model_spec.compat_tf_versions:\n    raise ValueError(\'Incompatible versions. Expect {}, but got {}.\'.format(\n        model_spec.compat_tf_versions, compat.get_tf_behavior()))\n\n  if use_hub_library:\n    hparams = get_hub_lib_hparams(\n        batch_size=batch_size,\n        train_epochs=epochs,\n        do_fine_tuning=train_whole_model,\n        dropout_rate=dropout_rate,\n        learning_rate=learning_rate,\n        momentum=momentum)\n  else:\n    hparams = train_image_classifier_lib.HParams.get_hparams(\n        batch_size=batch_size,\n        train_epochs=epochs,\n        do_fine_tuning=train_whole_model,\n        dropout_rate=dropout_rate,\n        learning_rate=learning_rate,\n        warmup_steps=warmup_steps,\n        model_dir=model_dir)\n\n  image_classifier = ImageClassifier(\n      model_spec,\n      train_data.index_to_label,\n      train_data.num_classes,\n      shuffle=shuffle,\n      hparams=hparams,\n      use_augmentation=use_augmentation)\n\n  tf.compat.v1.logging.info(\'Retraining the models...\')\n  image_classifier.train(train_data, validation_data)\n\n  return image_classifier\n\n\ndef _get_model_info(model_spec,\n                    num_classes,\n                    quantization_config=None,\n                    version=\'v1\'):\n  """"""Gets the specific info for the image model.""""""\n\n  if not isinstance(model_spec, ms.ImageModelSpec):\n    raise ValueError(\'Currently only support models for image classification.\')\n\n  image_min = 0\n  image_max = 1\n\n  name = model_spec.name\n  if quantization_config:\n    name += \'_quantized\'\n    # TODO(yuqili): Remove `compat.get_tf_behavior() == 1` once b/153576655 is\n    # fixed.\n    if compat.get_tf_behavior() == 1:\n      if quantization_config.inference_input_type == tf.uint8:\n        image_min = 0\n        image_max = 255\n      elif quantization_config.inference_input_type == tf.int8:\n        image_min = -128\n        image_max = 127\n\n  return metadata_writer.ModelSpecificInfo(\n      model_spec.name,\n      version,\n      image_width=model_spec.input_image_shape[1],\n      image_height=model_spec.input_image_shape[0],\n      mean=model_spec.mean_rgb,\n      std=model_spec.stddev_rgb,\n      image_min=image_min,\n      image_max=image_max,\n      num_classes=num_classes)\n\n\nclass ImageClassifier(classification_model.ClassificationModel):\n  """"""ImageClassifier class for inference and exporting to tflite.""""""\n\n  DEFAULT_EXPORT_FORMAT = [ExportFormat.TFLITE, ExportFormat.LABEL]\n  ALLOWED_EXPORT_FORMAT = [\n      ExportFormat.TFLITE, ExportFormat.LABEL, ExportFormat.SAVED_MODEL\n  ]\n\n  def __init__(self,\n               model_spec,\n               index_to_label,\n               num_classes,\n               shuffle=True,\n               hparams=hub_lib.get_default_hparams(),\n               use_augmentation=False):\n    """"""Init function for ImageClassifier class.\n\n    Args:\n      model_spec: Specification for the model.\n      index_to_label: A list that map from index to label class name.\n      num_classes: Number of label classes.\n      shuffle: Whether the data should be shuffled.\n      hparams: A namedtuple of hyperparameters. This function expects\n        .dropout_rate: The fraction of the input units to drop, used in dropout\n          layer.\n        .do_fine_tuning: If true, the Hub module is trained together with the\n          classification layer on top.\n      use_augmentation: Use data augmentation for preprocessing.\n    """"""\n    super(ImageClassifier,\n          self).__init__(model_spec, index_to_label, num_classes, shuffle,\n                         hparams.do_fine_tuning)\n    self.hparams = hparams\n    self.model = self._create_model()\n    self.preprocessor = image_preprocessing.Preprocessor(\n        self.model_spec.input_image_shape,\n        num_classes,\n        self.model_spec.mean_rgb,\n        self.model_spec.stddev_rgb,\n        use_augmentation=use_augmentation)\n    self.history = None  # Training history that returns from `keras_model.fit`.\n\n  def _create_model(self, hparams=None):\n    """"""Creates the classifier model for retraining.""""""\n    hparams = self._get_hparams_or_default(hparams)\n\n    module_layer = hub_loader.HubKerasLayerV1V2(\n        self.model_spec.uri, trainable=hparams.do_fine_tuning)\n    return hub_lib.build_model(module_layer, hparams,\n                               self.model_spec.input_image_shape,\n                               self.num_classes)\n\n  def train(self, train_data, validation_data=None, hparams=None):\n    """"""Feeds the training data for training.\n\n    Args:\n      train_data: Training data.\n      validation_data: Validation data. If None, skips validation process.\n      hparams: An instance of hub_lib.HParams or\n        train_image_classifier_lib.HParams. Anamedtuple of hyperparameters.\n\n    Returns:\n      The tf.keras.callbacks.History object returned by tf.keras.Model.fit*().\n    """"""\n    hparams = self._get_hparams_or_default(hparams)\n\n    train_ds = self._gen_dataset(\n        train_data, hparams.batch_size, is_training=True)\n    train_data_and_size = (train_ds, train_data.size)\n\n    validation_ds = None\n    validation_size = 0\n    if validation_data is not None:\n      validation_ds = self._gen_dataset(\n          validation_data, hparams.batch_size, is_training=False)\n      validation_size = validation_data.size\n    validation_data_and_size = (validation_ds, validation_size)\n\n    # Trains the models.\n    lib = hub_lib\n    if isinstance(hparams, train_image_classifier_lib.HParams):\n      lib = train_image_classifier_lib\n    self.history = lib.train_model(self.model, hparams, train_data_and_size,\n                                   validation_data_and_size)\n    return self.history\n\n  def preprocess(self, image, label, is_training=False):\n    return self.preprocessor(image, label, is_training)\n\n  def _gen_dataset(self, data, batch_size=32, is_training=True):\n    """"""Generates training / validation dataset.""""""\n    ds = data.dataset\n    ds = ds.map(lambda image, label: self.preprocess(image, label, is_training))\n\n    if is_training:\n      if self.shuffle:\n        ds = ds.shuffle(buffer_size=min(data.size, 100))\n      ds = ds.repeat()\n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\n  def export(self,\n             export_dir,\n             tflite_filename=\'model.tflite\',\n             label_filename=\'labels.txt\',\n             saved_model_filename=\'saved_model\',\n             export_format=None,\n             **kwargs):\n    """"""Converts the retrained model based on `model_export_format`.\n\n    Args:\n      export_dir: The directory to save exported files.\n      tflite_filename: File name to save tflite model. The full export path is\n        {export_dir}/{tflite_filename}.\n      label_filename: File name to save labels. The full export path is\n        {export_dir}/{label_filename}.\n      saved_model_filename: Path to SavedModel or H5 file to save the model. The\n        full export path is\n        {export_dir}/{saved_model_filename}/{saved_model.pb|assets|variables}.\n      export_format: List of export format that could be saved_model, tflite,\n        label.\n      **kwargs: Other parameters like `quantized` for TFLITE model.\n    """"""\n    export_format = self._get_export_format(export_format)\n    if not tf.io.gfile.exists(export_dir):\n      tf.io.gfile.makedirs(export_dir)\n\n    if ExportFormat.SAVED_MODEL in export_format:\n      super(ImageClassifier, self).export(\n          export_dir,\n          saved_model_filename=saved_model_filename,\n          export_format=ExportFormat.SAVED_MODEL,\n          **kwargs)\n\n    label_filepath = None\n    if ExportFormat.LABEL in export_format:\n      label_filepath = os.path.join(export_dir, label_filename)\n      self._export_labels(label_filepath)\n\n    if ExportFormat.TFLITE in export_format:\n      tflite_filepath = os.path.join(export_dir, tflite_filename)\n      self._export_tflite(tflite_filepath, label_filepath, **kwargs)\n\n  def _export_tflite(self,\n                     tflite_filepath,\n                     label_filepath=None,\n                     quantization_config=None,\n                     with_metadata=True,\n                     export_metadata_json_file=False):\n    """"""Converts the retrained model to tflite format and saves it.\n\n\n    Args:\n      tflite_filepath: File path to save tflite model.\n      label_filepath: File path to save labels.\n      quantization_config: Configuration for post-training quantization.\n      with_metadata: Whether the output tflite model contains metadata.\n      export_metadata_json_file: Whether to export metadata in json file. If\n        True, export the metadata in the same directory as tflite model.Used\n        only if `with_metadata` is True.\n    """"""\n    model_util.export_tflite(self.model, tflite_filepath, quantization_config,\n                             self._gen_dataset)\n    if with_metadata:\n      if label_filepath is None:\n        tf.compat.v1.logging.warning(\n            \'Label filepath is needed when exporting TFLite with metadata.\')\n        return\n\n      model_basename = os.path.basename(tflite_filepath)\n      export_directory = os.path.dirname(tflite_filepath)\n      export_model_path = os.path.join(export_directory, model_basename)\n\n      model_info = _get_model_info(\n          self.model_spec,\n          self.num_classes,\n          quantization_config=quantization_config)\n      # Generate the metadata objects and put them in the model file\n      populator = metadata_writer.MetadataPopulatorForImageClassifier(\n          export_model_path, model_info, label_filepath)\n      populator.populate()\n\n      # Validate the output model file by reading the metadata and produce\n      # a json file with the metadata under the export path\n      if export_metadata_json_file:\n        displayer = _metadata.MetadataDisplayer.with_model_file(\n            export_model_path)\n        export_json_file = os.path.join(\n            export_directory,\n            os.path.splitext(model_basename)[0] + \'.json\')\n\n        content = displayer.get_metadata_json()\n        with open(export_json_file, \'w\') as f:\n          f.write(content)\n\n  def _get_hparams_or_default(self, hparams):\n    """"""Returns hparams if not none, otherwise uses default one.""""""\n    return hparams if hparams else self.hparams\n'"
tensorflow_examples/lite/model_maker/core/task/image_classifier_test.py,9,"b""# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport filecmp\nimport os\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.data_util import image_dataloader\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\nfrom tensorflow_examples.lite.model_maker.core.task import configs\nfrom tensorflow_examples.lite.model_maker.core.task import image_classifier\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec\n\n\ndef _fill_image(rgb, image_size):\n  r, g, b = rgb\n  return np.broadcast_to(\n      np.array([[[r, g, b]]], dtype=np.uint8),\n      shape=(image_size, image_size, 3))\n\n\nclass ImageClassifierTest(tf.test.TestCase):\n  IMAGE_SIZE = 24\n  IMAGES_PER_CLASS = 20\n  CMY_NAMES_AND_RGB_VALUES = (('cyan', (0, 255, 255)),\n                              ('magenta', (255, 0, 255)), ('yellow', (255, 255,\n                                                                      0)))\n\n  def _gen(self):\n    for i, (_, rgb) in enumerate(self.CMY_NAMES_AND_RGB_VALUES):\n      for _ in range(self.IMAGES_PER_CLASS):\n        yield (_fill_image(rgb, self.IMAGE_SIZE), i)\n\n  def _gen_cmy_data(self):\n    ds = tf.data.Dataset.from_generator(\n        self._gen, (tf.uint8, tf.int64), (tf.TensorShape(\n            [self.IMAGE_SIZE, self.IMAGE_SIZE, 3]), tf.TensorShape([])))\n    data = image_dataloader.ImageClassifierDataLoader(\n        ds, self.IMAGES_PER_CLASS * 3, 3, ['cyan', 'magenta', 'yellow'])\n    return data\n\n  def setUp(self):\n    super(ImageClassifierTest, self).setUp()\n    all_data = self._gen_cmy_data()\n    # Splits data, 90% data for training, 10% for testing\n    self.train_data, self.test_data = all_data.split(0.9)\n\n  @test_util.test_in_tf_2\n  def test_mobilenetv2_model(self):\n    model = image_classifier.create(\n        self.train_data,\n        model_spec.mobilenet_v2_spec,\n        epochs=2,\n        batch_size=4,\n        shuffle=True)\n    self._test_accuracy(model)\n    self._test_predict_top_k(model)\n    self._test_export_to_tflite(model)\n    self._test_export_to_tflite_quantized(model, self.train_data)\n    self._test_export_to_tflite_with_metadata(model)\n    self._test_export_to_saved_model(model)\n    self._test_export_labels(model)\n\n  @test_util.test_in_tf_1\n  def test_mobilenetv2_model_create_v1_incompatible(self):\n    with self.assertRaisesRegex(ValueError, 'Incompatible versions'):\n      _ = image_classifier.create(self.train_data, model_spec.mobilenet_v2_spec)\n\n  @test_util.test_in_tf_1and2\n  def test_efficientnetlite0_model_with_model_maker_retraining_lib(self):\n    model = image_classifier.create(\n        self.train_data,\n        model_spec.efficientnet_lite0_spec,\n        epochs=2,\n        batch_size=4,\n        shuffle=True,\n        use_hub_library=False)\n    self._test_accuracy(model)\n    self._test_export_to_tflite(model)\n\n  @test_util.test_in_tf_1and2\n  def test_efficientnetlite0_model(self):\n    model = image_classifier.create(\n        self.train_data,\n        model_spec.efficientnet_lite0_spec,\n        epochs=2,\n        batch_size=4,\n        shuffle=True)\n    self._test_accuracy(model)\n    self._test_export_to_tflite(model)\n    self._test_export_to_tflite_quantized(model, self.train_data)\n    self._test_export_to_tflite_with_metadata(\n        model, expected_json_file='efficientnet_lite0_metadata.json')\n\n  @test_util.test_in_tf_2\n  def test_resnet_50_model(self):\n    model = image_classifier.create(\n        self.train_data,\n        model_spec.resnet_50_spec,\n        epochs=2,\n        batch_size=4,\n        shuffle=True)\n    self._test_accuracy(model)\n    self._test_export_to_tflite(model)\n    self._test_export_to_tflite_quantized(model, self.train_data)\n    self._test_export_to_tflite_with_metadata(model)\n\n  def _test_predict_top_k(self, model, threshold=0.7):\n    topk = model.predict_top_k(self.test_data, batch_size=4)\n    for i, (_, label) in enumerate(self.test_data.dataset):\n      predict_label, predict_prob = topk[i][0][0], topk[i][0][1]\n      self.assertEqual(model.index_to_label[label], predict_label)\n      self.assertGreater(predict_prob, threshold)\n\n  def _test_accuracy(self, model, threashold=0.8):\n    _, accuracy = model.evaluate(self.test_data)\n    self.assertGreater(accuracy, threashold)\n\n  def _load_labels(self, filename):\n    with tf.io.gfile.GFile(filename, 'r') as f:\n      return [label.strip() for label in f]\n\n  def _load_lite_model(self, filename):\n    self.assertTrue(os.path.isfile(filename))\n    with tf.io.gfile.GFile(filename, 'rb') as f:\n      model_content = f.read()\n    interpreter = tf.lite.Interpreter(model_content=model_content)\n\n    def lite_model(images):\n      interpreter.allocate_tensors()\n      input_index = interpreter.get_input_details()[0]['index']\n      interpreter.set_tensor(input_index, images)\n      interpreter.invoke()\n      output_index = interpreter.get_output_details()[0]['index']\n      return interpreter.get_tensor(output_index)\n\n    return lite_model\n\n  def _test_export_labels(self, model):\n    labels_output_file = os.path.join(self.get_temp_dir(), 'labels.txt')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.LABEL)\n    self._check_label_file(labels_output_file)\n\n  def _test_export_to_tflite(self, model):\n    tflite_output_file = os.path.join(self.get_temp_dir(), 'model.tflite')\n\n    model.export(self.get_temp_dir(), export_format=ExportFormat.TFLITE)\n    lite_model = self._load_lite_model(tflite_output_file)\n\n    test_ds = model._gen_dataset(\n        self.test_data, batch_size=1, is_training=False)\n    if compat.get_tf_behavior() == 1:\n      iterator = test_ds.make_one_shot_iterator()\n      image_tensor, label_tensor = iterator.get_next()\n      with tf.compat.v1.Session() as sess:\n        for _ in range(self.test_data.size):\n          image, label = sess.run((image_tensor, label_tensor))\n          output_batch = lite_model(image)\n          prediction = np.argmax(output_batch[0])\n          label = np.argmax(label[0])\n          self.assertEqual(label, prediction)\n    else:\n      for image, label in test_ds:\n        output_batch = lite_model(image.numpy())\n        prediction = np.argmax(output_batch[0])\n        label = np.argmax(label.numpy()[0])\n        self.assertEqual(label, prediction)\n\n  def _test_export_to_tflite_quantized(self, model, representative_data):\n    # Just test whether quantization will crash, can't guarantee the result.\n    tflile_filename = 'model_quantized.tflite'\n    tflite_output_file = os.path.join(self.get_temp_dir(), tflile_filename)\n    config = configs.QuantizationConfig.create_full_integer_quantization(\n        representative_data, is_integer_only=True)\n    model.export(\n        self.get_temp_dir(),\n        tflile_filename,\n        quantization_config=config,\n        export_format=ExportFormat.TFLITE)\n    self.assertTrue(os.path.isfile(tflite_output_file))\n    self.assertGreater(os.path.getsize(tflite_output_file), 0)\n\n  def _check_label_file(self, labels_output_file):\n    labels = self._load_labels(labels_output_file)\n    self.assertEqual(labels, ['cyan', 'magenta', 'yellow'])\n\n  def _test_export_to_tflite_with_metadata(self,\n                                           model,\n                                           expected_json_file=None):\n    model_name = 'model_with_metadata'\n    tflite_output_file = os.path.join(self.get_temp_dir(),\n                                      '%s.tflite' % model_name)\n    json_output_file = os.path.join(self.get_temp_dir(), '%s.json' % model_name)\n    labels_output_file = os.path.join(self.get_temp_dir(), 'labels.txt')\n\n    model.export(\n        self.get_temp_dir(),\n        '%s.tflite' % model_name,\n        with_metadata=True,\n        export_metadata_json_file=True)\n\n    self.assertTrue(os.path.isfile(tflite_output_file))\n    self.assertGreater(os.path.getsize(tflite_output_file), 0)\n\n    self._check_label_file(labels_output_file)\n\n    self.assertTrue(os.path.isfile(json_output_file))\n    self.assertGreater(os.path.getsize(json_output_file), 0)\n\n    if expected_json_file is not None:\n      expected_json_file = test_util.get_test_data_path(expected_json_file)\n      self.assertTrue(filecmp.cmp(json_output_file, expected_json_file))\n\n  def _test_export_to_saved_model(self, model):\n    save_model_output_path = os.path.join(self.get_temp_dir(), 'saved_model')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.SAVED_MODEL)\n\n    self.assertTrue(os.path.isdir(save_model_output_path))\n    self.assertNotEqual(len(os.listdir(save_model_output_path)), 0)\n\n\nif __name__ == '__main__':\n  compat.setup_tf_behavior(tf_version=2)\n  tf.test.main()\n"""
tensorflow_examples/lite/model_maker/core/task/image_classifier_v1_test.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.task import image_classifier_test\n\n\nclass ImageClassifierV1Test(image_classifier_test.ImageClassifierTest):\n  """"""Share image tests of the base class, but in tf v1 behavior.""""""\n\n\nif __name__ == \'__main__\':\n  compat.setup_tf_behavior(tf_version=1)\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/core/task/image_preprocessing.py,37,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ImageNet preprocessing.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v2 as tf\n\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\n\nclass Preprocessor(object):\n  """"""Preprocessing for image classification.""""""\n\n  def __init__(self,\n               input_shape,\n               num_classes,\n               mean_rgb,\n               stddev_rgb,\n               use_augmentation=False):\n    self.input_shape = input_shape\n    self.num_classes = num_classes\n    self.mean_rgb = mean_rgb\n    self.stddev_rgb = stddev_rgb\n    self.use_augmentation = use_augmentation\n\n  def __call__(self, image, label, is_training=True):\n    if self.use_augmentation:\n      return self._preprocess_with_augmentation(image, label, is_training)\n    return self._preprocess_without_augmentation(image, label)\n\n  def _preprocess_with_augmentation(self, image, label, is_training):\n    """"""Image preprocessing method with data augmentation.""""""\n    image_size = self.input_shape[0]\n    if is_training:\n      image = preprocess_for_train(image, self.input_shape[0])\n    else:\n      image = preprocess_for_eval(image, image_size)\n\n    image -= tf.constant(self.mean_rgb, shape=[1, 1, 3], dtype=image.dtype)\n    image /= tf.constant(self.stddev_rgb, shape=[1, 1, 3], dtype=image.dtype)\n\n    label = tf.one_hot(label, depth=self.num_classes)\n    return image, label\n\n  # TODO(yuqili): Changes to preprocess to support batch input.\n  def _preprocess_without_augmentation(self, image, label):\n    """"""Image preprocessing method without data augmentation.""""""\n    image = tf.cast(image, tf.float32)\n\n    image -= tf.constant(self.mean_rgb, shape=[1, 1, 3], dtype=image.dtype)\n    image /= tf.constant(self.stddev_rgb, shape=[1, 1, 3], dtype=image.dtype)\n\n    image = tf.compat.v1.image.resize(image, self.input_shape)\n    label = tf.one_hot(label, depth=self.num_classes)\n    return image, label\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100):\n  """"""Generates cropped_image using one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image_bytes: `Tensor` of binary image data.\n    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]` where\n      each coordinate is [0, 1) and the coordinates are arranged as `[ymin,\n      xmin, ymax, xmax]`. If num_boxes is 0 then use the whole image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped area\n      of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `float`s. The cropped area of the image must\n      contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n\n  Returns:\n    cropped image `Tensor`\n  """"""\n  with tf.name_scope(\'distorted_bounding_box_crop\'):\n    shape = tf.shape(image_bytes)\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        shape,\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n    target_height, target_width, _ = tf.unstack(bbox_size)\n    image = tf.image.crop_to_bounding_box(image_bytes, offset_y, offset_x,\n                                          target_height, target_width)\n\n    return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n  """"""At least `x` of `a` and `b` `Tensors` are equal.""""""\n  match = tf.equal(a, b)\n  match = tf.cast(match, tf.int32)\n  return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _resize_image(image, image_size, method=None):\n  if method is not None:\n    tf.compat.v1.logging.info(\'Use customized resize method {}\'.format(method))\n    return tf.compat.v1.image.resize([image], [image_size, image_size],\n                                     method)[0]\n  tf.compat.v1.logging.info(\'Use default resize_bicubic.\')\n  return tf.compat.v1.image.resize_bicubic([image], [image_size, image_size])[0]\n\n\ndef _decode_and_random_crop(image_bytes, image_size, resize_method=None):\n  """"""Make a random crop of image_size.""""""\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n  image = distorted_bounding_box_crop(\n      image_bytes,\n      bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=(3. / 4, 4. / 3.),\n      area_range=(0.08, 1.0),\n      max_attempts=10)\n  original_shape = tf.shape(image_bytes)\n  bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n  image = tf.cond(bad, lambda: _decode_and_center_crop(image_bytes, image_size),\n                  lambda: _resize_image(image, image_size, resize_method))\n\n  return image\n\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method=None):\n  """"""Crops to center of image with padding then scales image_size.""""""\n  shape = tf.shape(image_bytes)\n  image_height = shape[0]\n  image_width = shape[1]\n\n  padded_center_crop_size = tf.cast(\n      ((image_size / (image_size + CROP_PADDING)) *\n       tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n\n  offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n  offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n  image = tf.image.crop_to_bounding_box(image_bytes, offset_height,\n                                        offset_width, padded_center_crop_size,\n                                        padded_center_crop_size)\n  image = _resize_image(image, image_size, resize_method)\n  return image\n\n\ndef _flip(image):\n  """"""Random horizontal image flip.""""""\n  image = tf.image.random_flip_left_right(image)\n  return image\n\n\ndef preprocess_for_train(image_bytes,\n                         image_size=IMAGE_SIZE,\n                         resize_method=tf.image.ResizeMethod.BILINEAR):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    image_size: image size.\n    resize_method: resize method. If none, use bicubic.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n  image = _flip(image)\n  image = tf.reshape(image, [image_size, image_size, 3])\n\n  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n  return image\n\n\ndef preprocess_for_eval(image_bytes,\n                        image_size=IMAGE_SIZE,\n                        resize_method=tf.image.ResizeMethod.BILINEAR):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size.\n    image_size: image size.\n    resize_method: if None, use bicubic.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n  image = tf.reshape(image, [image_size, image_size, 3])\n  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n  return image\n'"
tensorflow_examples/lite/model_maker/core/task/image_preprocessing_test.py,6,"b""# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_examples.lite.model_maker.core.task import image_preprocessing\n\n\ndef _get_preprocessed_image(preprocessor, is_training=False):\n  image_placeholder = tf.compat.v1.placeholder(tf.uint8, [24, 24, 3])\n  label_placeholder = tf.compat.v1.placeholder(tf.int32, [1])\n  image_tensor, _ = preprocessor(image_placeholder, label_placeholder,\n                                 is_training)\n\n  with tf.compat.v1.Session() as sess:\n    input_image = np.arange(24 * 24 * 3, dtype=np.uint8).reshape([24, 24, 3])\n    image = sess.run(\n        image_tensor,\n        feed_dict={\n            image_placeholder: input_image,\n            label_placeholder: [0]\n        })\n    return image\n\n\nclass PreprocessorTest(tf.test.TestCase):\n\n  def test_without_augmentation(self):\n    preprocessor = image_preprocessing.Preprocessor([2, 2],\n                                                    2,\n                                                    mean_rgb=[0.0],\n                                                    stddev_rgb=[255.0],\n                                                    use_augmentation=False)\n    actual_image = np.array([[[0., 0.00392157, 0.00784314],\n                              [0.14117648, 0.14509805, 0.14901961]],\n                             [[0.37647063, 0.3803922, 0.38431376],\n                              [0.5176471, 0.52156866, 0.5254902]]])\n\n    image = _get_preprocessed_image(preprocessor)\n    self.assertTrue(np.allclose(image, actual_image, atol=1e-05))\n\n  def test_with_augmentation(self):\n    image_preprocessing.CROP_PADDING = 1\n    preprocessor = image_preprocessing.Preprocessor([2, 2],\n                                                    2,\n                                                    mean_rgb=[0.0],\n                                                    stddev_rgb=[255.0],\n                                                    use_augmentation=True)\n    # Tests validation image.\n    actual_eval_image = np.array([[[0.17254902, 0.1764706, 0.18039216],\n                                   [0.26666668, 0.27058825, 0.27450982]],\n                                  [[0.42352945, 0.427451, 0.43137258],\n                                   [0.5176471, 0.52156866, 0.5254902]]])\n\n    image = _get_preprocessed_image(preprocessor, is_training=False)\n    self.assertTrue(np.allclose(image, actual_eval_image, atol=1e-05))\n\n    # Tests training image.\n    image1 = _get_preprocessed_image(preprocessor, is_training=True)\n    image2 = _get_preprocessed_image(preprocessor, is_training=True)\n    self.assertFalse(np.allclose(image1, image2, atol=1e-05))\n    self.assertEqual(image1.shape, (2, 2, 3))\n    self.assertEqual(image2.shape, (2, 2, 3))\n\n\nif __name__ == '__main__':\n  tf.compat.v1.disable_eager_execution()\n  tf.test.main()\n"""
tensorflow_examples/lite/model_maker/core/task/metadata_writer_for_image_classifier.py,1,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Writes metadata and label file to the image classifier models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nimport tensorflow as tf\n\nimport flatbuffers\n# pylint: disable=g-direct-tensorflow-import\nfrom tflite_support import metadata as _metadata\nfrom tflite_support import metadata_schema_py_generated as _metadata_fb\n# pylint: enable=g-direct-tensorflow-import\n\nFLAGS = flags.FLAGS\n\n\ndef define_flags():\n  flags.DEFINE_string(""model_file"", None,\n                      ""Path and file name to the TFLite model file."")\n  flags.DEFINE_string(""label_file"", None, ""Path to the label file."")\n  flags.DEFINE_string(""export_directory"", None,\n                      ""Path to save the TFLite model files with metadata."")\n  flags.mark_flag_as_required(""model_file"")\n  flags.mark_flag_as_required(""label_file"")\n  flags.mark_flag_as_required(""export_directory"")\n\n\nclass ModelSpecificInfo(object):\n  """"""Holds information that is specificly tied to an image classifier.""""""\n\n  def __init__(self, name, version, image_width, image_height, image_min,\n               image_max, mean, std, num_classes):\n    self.name = name\n    self.version = version\n    self.image_width = image_width\n    self.image_height = image_height\n    self.image_min = image_min\n    self.image_max = image_max\n    self.mean = mean\n    self.std = std\n    self.num_classes = num_classes\n\n\n_MODEL_INFO = {\n    ""mobilenet_v1_0.75_160_quantized.tflite"":\n        ModelSpecificInfo(\n            name=""MobileNetV1 image classifier"",\n            version=""v1"",\n            image_width=160,\n            image_height=160,\n            image_min=0,\n            image_max=255,\n            mean=[127.5],\n            std=[127.5],\n            num_classes=1001)\n}\n\n\nclass MetadataPopulatorForImageClassifier(object):\n  """"""Populates the metadata for an image classifier.""""""\n\n  def __init__(self, model_file, model_info, label_file_path):\n    self.model_file = model_file\n    self.model_info = model_info\n    self.label_file_path = label_file_path\n    self.metadata_buf = None\n\n  def populate(self):\n    """"""Creates metadata and then populates it for an image classifier.""""""\n    self._create_metadata()\n    self._populate_metadata()\n\n  def _create_metadata(self):\n    """"""Creates the metadata for an image classifier.""""""\n\n    # Creates model info.\n    model_meta = _metadata_fb.ModelMetadataT()\n    model_meta.name = self.model_info.name\n    model_meta.description = (""Identify the most prominent object in the ""\n                              ""image from a set of %d categories."" %\n                              self.model_info.num_classes)\n    model_meta.version = self.model_info.version\n    model_meta.author = ""TensorFlow""\n    model_meta.license = (""Apache License. Version 2.0 ""\n                          ""http://www.apache.org/licenses/LICENSE-2.0."")\n\n    # Creates input info.\n    input_meta = _metadata_fb.TensorMetadataT()\n    input_meta.name = ""image""\n    input_meta.description = (\n        ""Input image to be classified. The expected image is {0} x {1}, with ""\n        ""three channels (red, blue, and green) per pixel. Each value in the ""\n        ""tensor is a single byte between {2} and {3}."".format(\n            self.model_info.image_width, self.model_info.image_height,\n            self.model_info.image_min, self.model_info.image_max))\n    input_meta.content = _metadata_fb.ContentT()\n    input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n    input_meta.content.contentProperties.colorSpace = (\n        _metadata_fb.ColorSpaceType.RGB)\n    input_meta.content.contentPropertiesType = (\n        _metadata_fb.ContentProperties.ImageProperties)\n    input_normalization = _metadata_fb.ProcessUnitT()\n    input_normalization.optionsType = (\n        _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n    input_normalization.options = _metadata_fb.NormalizationOptionsT()\n    input_normalization.options.mean = self.model_info.mean\n    input_normalization.options.std = self.model_info.std\n    input_meta.processUnits = [input_normalization]\n    input_stats = _metadata_fb.StatsT()\n    input_stats.max = [self.model_info.image_max]\n    input_stats.min = [self.model_info.image_min]\n    input_meta.stats = input_stats\n\n    # Creates output info.\n    output_meta = _metadata_fb.TensorMetadataT()\n    output_meta.name = ""probability""\n    output_meta.description = ""Probabilities of the %d labels respectively."" % self.model_info.num_classes\n    output_meta.content = _metadata_fb.ContentT()\n    output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()\n    output_meta.content.contentPropertiesType = (\n        _metadata_fb.ContentProperties.FeatureProperties)\n    output_stats = _metadata_fb.StatsT()\n    output_stats.max = [1.0]\n    output_stats.min = [0.0]\n    output_meta.stats = output_stats\n    label_file = _metadata_fb.AssociatedFileT()\n    label_file.name = os.path.basename(self.label_file_path)\n    label_file.description = ""Labels for objects that the model can recognize.""\n    label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\n    output_meta.associatedFiles = [label_file]\n\n    # Creates subgraph info.\n    subgraph = _metadata_fb.SubGraphMetadataT()\n    subgraph.inputTensorMetadata = [input_meta]\n    subgraph.outputTensorMetadata = [output_meta]\n    model_meta.subgraphMetadata = [subgraph]\n\n    b = flatbuffers.Builder(0)\n    b.Finish(\n        model_meta.Pack(b),\n        _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n    self.metadata_buf = b.Output()\n\n  def _populate_metadata(self):\n    """"""Populates metadata and label file to the model file.""""""\n    populator = _metadata.MetadataPopulator.with_model_file(self.model_file)\n    populator.load_metadata_buffer(self.metadata_buf)\n    populator.load_associated_files([self.label_file_path])\n    populator.populate()\n\n\ndef main(_):\n  model_file = FLAGS.model_file\n  model_basename = os.path.basename(model_file)\n  if model_basename not in _MODEL_INFO:\n    raise ValueError(\n        ""The model info for, {0}, is not defined yet."".format(model_basename))\n\n  export_model_path = os.path.join(FLAGS.export_directory, model_basename)\n\n  # Copies model_file to export_path.\n  tf.io.gfile.copy(model_file, export_model_path, overwrite=True)\n\n  # Generate the metadata objects and put them in the model file\n  populator = MetadataPopulatorForImageClassifier(\n      export_model_path, _MODEL_INFO.get(model_basename), FLAGS.label_file)\n  populator.populate()\n\n  # Validate the output model file by reading the metadata and produce\n  # a json file with the metadata under the export path\n  displayer = _metadata.MetadataDisplayer.with_model_file(export_model_path)\n  export_json_file = os.path.join(FLAGS.export_directory,\n                                  os.path.splitext(model_basename)[0] + "".json"")\n  json_file = displayer.get_metadata_json()\n  with open(export_json_file, ""w"") as f:\n    f.write(json_file)\n\n  print(""Finished populating metadata and associated file to the model:"")\n  print(model_file)\n  print(""The metadata json file has been saved to:"")\n  print(export_json_file)\n  print(""The associated file that has been been packed to the model is:"")\n  print(displayer.get_packed_associated_file_list())\n\n\nif __name__ == ""__main__"":\n  define_flags()\n  app.run(main)\n'"
tensorflow_examples/lite/model_maker/core/task/model_spec.py,74,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Model specification.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport inspect\nimport os\nimport re\nimport tempfile\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core import file_util\nfrom tensorflow_examples.lite.model_maker.core.task import hub_loader\n\nimport tensorflow_hub as hub\nfrom tensorflow_hub import registry\n\nfrom official.nlp import optimization\n\nfrom official.nlp.bert import configs as bert_configs\nfrom official.nlp.bert import run_squad_helper\nfrom official.nlp.bert import squad_evaluate_v1_1\nfrom official.nlp.bert import squad_evaluate_v2_0\nfrom official.nlp.bert import tokenization\n\n\nfrom official.nlp.data import classifier_data_lib\nfrom official.nlp.data import squad_lib\n\nfrom official.nlp.modeling import models\nfrom official.utils.misc import distribution_utils\n\n\ndef create_int_feature(values):\n  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n  return feature\n\n\ndef _get_compat_tf_versions(compat_tf_versions=None):\n  """"""Gets compatible tf versions (default: [2]).\n\n  Args:\n    compat_tf_versions: int, int list or None, indicates compatible versions.\n\n  Returns:\n    A list of compatible tf versions.\n  """"""\n  if compat_tf_versions is None:\n    compat_tf_versions = [2]\n  if not isinstance(compat_tf_versions, list):\n    compat_tf_versions = [compat_tf_versions]\n  return compat_tf_versions\n\n\ndef get_num_gpus(num_gpus):\n  try:\n    tot_num_gpus = len(tf.config.experimental.list_physical_devices(\'GPU\'))\n  except TypeError:\n    tf.compat.v1.logging.warning(""Couldn\'t get the number of gpus."")\n    tot_num_gpus = max(0, num_gpus)\n  if num_gpus > tot_num_gpus or num_gpus == -1:\n    num_gpus = tot_num_gpus\n  return num_gpus\n\n\nclass ImageModelSpec(object):\n  """"""A specification of image model.""""""\n\n  mean_rgb = [0.0]\n  stddev_rgb = [255.0]\n\n  def __init__(self,\n               uri,\n               compat_tf_versions=None,\n               input_image_shape=None,\n               name=\'\'):\n    self.uri = uri\n    self.compat_tf_versions = _get_compat_tf_versions(compat_tf_versions)\n    self.name = name\n\n    if input_image_shape is None:\n      input_image_shape = [224, 224]\n    self.input_image_shape = input_image_shape\n\n\nmobilenet_v2_spec = ImageModelSpec(\n    uri=\'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\',\n    compat_tf_versions=2,\n    name=\'mobilenet_v2\')\n\nresnet_50_spec = ImageModelSpec(\n    uri=\'https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\',\n    compat_tf_versions=2,\n    name=\'resnet_50\')\n\nefficientnet_lite0_spec = ImageModelSpec(\n    uri=\'https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2\',\n    compat_tf_versions=[1, 2],\n    name=\'efficientnet_lite0\')\n\nefficientnet_lite1_spec = ImageModelSpec(\n    uri=\'https://tfhub.dev/tensorflow/efficientnet/lite1/feature-vector/2\',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[240, 240],\n    name=\'efficientnet_lite1\')\n\nefficientnet_lite2_spec = ImageModelSpec(\n    uri=\'https://tfhub.dev/tensorflow/efficientnet/lite2/feature-vector/2\',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[260, 260],\n    name=\'efficientnet_lite2\')\n\nefficientnet_lite3_spec = ImageModelSpec(\n    uri=\'https://tfhub.dev/tensorflow/efficientnet/lite3/feature-vector/2\',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[280, 280],\n    name=\'efficientnet_lite3\')\n\nefficientnet_lite4_spec = ImageModelSpec(\n    uri=\'https://tfhub.dev/tensorflow/efficientnet/lite4/feature-vector/2\',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[300, 300],\n    name=\'efficientnet_lite4\')\n\n\nclass AverageWordVecModelSpec(object):\n  """"""A specification of averaging word vector model.""""""\n  PAD = \'<PAD>\'  # Index: 0\n  START = \'<START>\'  # Index: 1\n  UNKNOWN = \'<UNKNOWN>\'  # Index: 2\n\n  compat_tf_versions = _get_compat_tf_versions(2)\n  need_gen_vocab = True\n  default_training_epochs = 2\n  convert_from_saved_model_tf2 = False\n\n  def __init__(self,\n               num_words=10000,\n               seq_len=256,\n               wordvec_dim=16,\n               lowercase=True,\n               dropout_rate=0.2):\n    """"""Initialze a instance with preprocessing and model paramaters.\n\n    Args:\n      num_words: Number of words to generate the vocabulary from data.\n      seq_len: Length of the sequence to feed into the model.\n      wordvec_dim: Dimension of the word embedding.\n      lowercase: Whether to convert all uppercase character to lowercase during\n        preprocessing.\n      dropout_rate: The rate for dropout.\n    """"""\n    self.num_words = num_words\n    self.seq_len = seq_len\n    self.wordvec_dim = wordvec_dim\n    self.lowercase = lowercase\n    self.dropout_rate = dropout_rate\n\n  def get_name_to_features(self):\n    """"""Gets the dictionary describing the features.""""""\n    name_to_features = {\n        \'input_ids\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'label_ids\': tf.io.FixedLenFeature([], tf.int64),\n    }\n    return name_to_features\n\n  def select_data_from_record(self, record):\n    """"""Dispatches records to features and labels.""""""\n    x = record[\'input_ids\']\n    y = record[\'label_ids\']\n    return (x, y)\n\n  def convert_examples_to_features(self, examples, tfrecord_file, label_names):\n    """"""Converts examples to features and write them into TFRecord file.""""""\n    writer = tf.io.TFRecordWriter(tfrecord_file)\n\n    label_to_id = dict((name, i) for i, name in enumerate(label_names))\n    for example in examples:\n      features = collections.OrderedDict()\n\n      input_ids = self.preprocess(example.text_a)\n      label_id = label_to_id[example.label]\n      features[\'input_ids\'] = create_int_feature(input_ids)\n      features[\'label_ids\'] = create_int_feature([label_id])\n      tf_example = tf.train.Example(\n          features=tf.train.Features(feature=features))\n      writer.write(tf_example.SerializeToString())\n    writer.close()\n\n  def run_classifier(self, train_input_fn, validation_input_fn, epochs,\n                     steps_per_epoch, validation_steps, num_classes):\n    """"""Creates classifier and runs the classifier training.""""""\n    if epochs is None:\n      epochs = self.default_training_epochs\n\n    # Gets a classifier model.\n    model = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=[self.seq_len]),\n        tf.keras.layers.Embedding(\n            len(self.vocab), self.wordvec_dim, input_length=self.seq_len),\n        tf.keras.layers.GlobalAveragePooling1D(),\n        tf.keras.layers.Dense(self.wordvec_dim, activation=tf.nn.relu),\n        tf.keras.layers.Dropout(self.dropout_rate),\n        tf.keras.layers.Dense(num_classes, activation=\'softmax\')\n    ])\n\n    # Gets training and validation dataset\n    train_ds = train_input_fn()\n    validation_ds = None\n    if validation_input_fn is not None:\n      validation_ds = validation_input_fn()\n\n    # Trains the models.\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\'sparse_categorical_crossentropy\',\n        metrics=[\'accuracy\'])\n\n    model.fit(\n        train_ds,\n        epochs=epochs,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=validation_ds,\n        validation_steps=validation_steps)\n\n    return model\n\n  def gen_vocab(self, examples):\n    """"""Generates vocabulary list in `examples` with maximum `num_words` words.""""""\n    vocab_counter = collections.Counter()\n\n    for example in examples:\n      tokens = self._tokenize(example.text_a)\n      for token in tokens:\n        vocab_counter[token] += 1\n\n    vocab_freq = vocab_counter.most_common(self.num_words)\n    vocab_list = [self.PAD, self.START, self.UNKNOWN\n                 ] + [word for word, _ in vocab_freq]\n    self.vocab = collections.OrderedDict(\n        ((v, i) for i, v in enumerate(vocab_list)))\n    return self.vocab\n\n  def preprocess(self, raw_text):\n    """"""Preprocess the text for text classification.""""""\n    tokens = self._tokenize(raw_text)\n\n    # Gets ids for START, PAD and UNKNOWN tokens.\n    start_id = self.vocab[self.START]\n    pad_id = self.vocab[self.PAD]\n    unknown_id = self.vocab[self.UNKNOWN]\n\n    token_ids = [self.vocab.get(token, unknown_id) for token in tokens]\n    token_ids = [start_id] + token_ids\n\n    if len(token_ids) < self.seq_len:\n      # Padding.\n      pad_length = self.seq_len - len(token_ids)\n      token_ids = token_ids + pad_length * [pad_id]\n    else:\n      token_ids = token_ids[:self.seq_len]\n\n    return token_ids\n\n  def _tokenize(self, text):\n    r""""""Splits by \'\\W\' except \'\\\'\'.""""""\n    text = tf.compat.as_text(text)\n    if self.lowercase:\n      text = text.lower()\n    tokens = re.compile(r\'[^\\w\\\']+\').split(text.strip())\n    return list(filter(None, tokens))\n\n  def save_vocab(self, vocab_filename):\n    """"""Saves the vocabulary in `vocab_filename`.""""""\n    with tf.io.gfile.GFile(vocab_filename, \'w\') as f:\n      for token, index in self.vocab.items():\n        f.write(\'%s %d\\n\' % (token, index))\n\n    tf.compat.v1.logging.info(\'Saved vocabulary in %s.\', vocab_filename)\n\n  def load_vocab(self, vocab_filename):\n    """"""Loads vocabulary from `vocab_filename`.""""""\n    with tf.io.gfile.GFile(vocab_filename, \'r\') as f:\n      vocab_list = []\n      for line in f:\n        word, index = line.strip().split()\n        vocab_list.append((word, int(index)))\n    self.vocab = collections.OrderedDict(vocab_list)\n    return self.vocab\n\n  def get_config(self):\n    """"""Gets the configuration.""""""\n    return {\n        \'num_words\': self.num_words,\n        \'seq_len\': self.seq_len,\n        \'wordvec_dim\': self.wordvec_dim,\n        \'lowercase\': self.lowercase\n    }\n\n\ndef create_classifier_model(bert_config,\n                            num_labels,\n                            max_seq_length,\n                            initializer=None,\n                            hub_module_url=None,\n                            hub_module_trainable=True,\n                            is_tf2=True):\n  """"""BERT classifier model in functional API style.\n\n  Construct a Keras model for predicting `num_labels` outputs from an input with\n  maximum sequence length `max_seq_length`.\n\n  Args:\n    bert_config: BertConfig, the config defines the core Bert model.\n    num_labels: integer, the number of classes.\n    max_seq_length: integer, the maximum input sequence length.\n    initializer: Initializer for the final dense layer in the span labeler.\n      Defaulted to TruncatedNormal initializer.\n    hub_module_url: TF-Hub path/url to Bert module.\n    hub_module_trainable: True to finetune layers in the hub module.\n    is_tf2: boolean, whether the hub module is in TensorFlow 2.x format.\n\n  Returns:\n    Combined prediction model (words, mask, type) -> (one-hot labels)\n    BERT sub-model (words, mask, type) -> (bert_outputs)\n  """"""\n  if initializer is None:\n    initializer = tf.keras.initializers.TruncatedNormal(\n        stddev=bert_config.initializer_range)\n\n  input_word_ids = tf.keras.layers.Input(\n      shape=(max_seq_length,), dtype=tf.int32, name=\'input_word_ids\')\n  input_mask = tf.keras.layers.Input(\n      shape=(max_seq_length,), dtype=tf.int32, name=\'input_mask\')\n  input_type_ids = tf.keras.layers.Input(\n      shape=(max_seq_length,), dtype=tf.int32, name=\'input_type_ids\')\n\n  if is_tf2:\n    bert_model = hub.KerasLayer(hub_module_url, trainable=hub_module_trainable)\n    pooled_output, _ = bert_model([input_word_ids, input_mask, input_type_ids])\n  else:\n    bert_model = hub_loader.HubKerasLayerV1V2(\n        hub_module_url,\n        signature=\'tokens\',\n        output_key=\'pooled_output\',\n        trainable=hub_module_trainable)\n\n    pooled_output = bert_model({\n        \'input_ids\': input_word_ids,\n        \'input_mask\': input_mask,\n        \'segment_ids\': input_type_ids\n    })\n\n  output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(\n      pooled_output)\n  output = tf.keras.layers.Dense(\n      num_labels,\n      kernel_initializer=initializer,\n      name=\'output\',\n      dtype=tf.float32)(\n          output)\n\n  return tf.keras.Model(\n      inputs=[input_word_ids, input_mask, input_type_ids],\n      outputs=output), bert_model\n\n\nclass BertModelSpec(object):\n  """"""A specification of BERT model.""""""\n\n  compat_tf_versions = _get_compat_tf_versions(2)\n  need_gen_vocab = False\n\n  def __init__(\n      self,\n      uri=\'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\',\n      model_dir=None,\n      seq_len=128,\n      dropout_rate=0.1,\n      initializer_range=0.02,\n      learning_rate=3e-5,\n      distribution_strategy=\'mirrored\',\n      num_gpus=-1,\n      tpu=\'\',\n      trainable=True,\n      do_lower_case=True,\n      is_tf2=True,\n      convert_from_saved_model_tf2=False):\n    """"""Initialze an instance with model paramaters.\n\n    Args:\n      uri: TF-Hub path/url to Bert module.\n      model_dir: The location of the model checkpoint files.\n      seq_len: Length of the sequence to feed into the model.\n      dropout_rate: The rate for dropout.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n      learning_rate: The initial learning rate for Adam.\n      distribution_strategy:  A string specifying which distribution strategy to\n        use. Accepted values are \'off\', \'one_device\', \'mirrored\',\n        \'parameter_server\', \'multi_worker_mirrored\', and \'tpu\' -- case\n        insensitive. \'off\' means not to use Distribution Strategy; \'tpu\' means\n        to use TPUStrategy using `tpu_address`.\n      num_gpus: How many GPUs to use at each worker with the\n        DistributionStrategies API. The default is -1, which means utilize all\n        available GPUs.\n      tpu: TPU address to connect to.\n      trainable: boolean, whether pretrain layer is trainable.\n      do_lower_case: boolean, whether to lower case the input text. Should be\n        True for uncased models and False for cased models.\n      is_tf2: boolean, whether the hub module is in TensorFlow 2.x format.\n      convert_from_saved_model_tf2: Convert to TFLite from saved_model in TF\n        2.x.\n    """"""\n    if compat.get_tf_behavior() not in self.compat_tf_versions:\n      raise ValueError(\'Incompatible versions. Expect {}, but got {}.\'.format(\n          self.compat_tf_versions, compat.get_tf_behavior()))\n    self.seq_len = seq_len\n    self.dropout_rate = dropout_rate\n    self.initializer_range = initializer_range\n    self.learning_rate = learning_rate\n    self.trainable = trainable\n\n    self.model_dir = model_dir\n    if self.model_dir is None:\n      self.model_dir = tempfile.mkdtemp()\n\n    num_gpus = get_num_gpus(num_gpus)\n    self.strategy = distribution_utils.get_distribution_strategy(\n        distribution_strategy=distribution_strategy,\n        num_gpus=num_gpus,\n        tpu_address=tpu)\n    self.tpu = tpu\n    self.uri = uri\n    self.do_lower_case = do_lower_case\n    self.is_tf2 = is_tf2\n\n    self.bert_config = bert_configs.BertConfig(\n        0,\n        initializer_range=self.initializer_range,\n        hidden_dropout_prob=self.dropout_rate)\n\n    self.convert_from_saved_model_tf2 = convert_from_saved_model_tf2\n    self.is_built = False\n\n  def build(self):\n    """"""Builds the class. Used for lazy initialization.""""""\n    if self.is_built:\n      return\n    self.vocab_file = os.path.join(\n        registry.resolver(self.uri), \'assets\', \'vocab.txt\')\n    self.tokenizer = tokenization.FullTokenizer(self.vocab_file,\n                                                self.do_lower_case)\n\n  def save_vocab(self, vocab_filename):\n    """"""Prints the file path to the vocabulary.""""""\n    if not self.is_built:\n      self.build()\n    tf.io.gfile.copy(self.vocab_file, vocab_filename, overwrite=True)\n    tf.compat.v1.logging.info(\'Saved vocabulary in %s.\', vocab_filename)\n\n\nclass BertClassifierModelSpec(BertModelSpec):\n  """"""A specification of BERT model for text classification.""""""\n\n  def get_name_to_features(self):\n    """"""Gets the dictionary describing the features.""""""\n    name_to_features = {\n        \'input_ids\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'input_mask\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'segment_ids\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'label_ids\': tf.io.FixedLenFeature([], tf.int64),\n        \'is_real_example\': tf.io.FixedLenFeature([], tf.int64),\n    }\n    return name_to_features\n\n  def select_data_from_record(self, record):\n    """"""Dispatches records to features and labels.""""""\n    x = {\n        \'input_word_ids\': record[\'input_ids\'],\n        \'input_mask\': record[\'input_mask\'],\n        \'input_type_ids\': record[\'segment_ids\']\n    }\n    y = record[\'label_ids\']\n    return (x, y)\n\n  def convert_examples_to_features(self, examples, tfrecord_file, label_names):\n    """"""Converts examples to features and write them into TFRecord file.""""""\n    if not self.is_built:\n      self.build()\n    classifier_data_lib.file_based_convert_examples_to_features(\n        examples, label_names, self.seq_len, self.tokenizer, tfrecord_file)\n\n  def _get_classification_loss_fn(self, num_classes):\n    """"""Gets the classification loss function.""""""\n\n    def _classification_loss_fn(labels, logits):\n      """"""Classification loss.""""""\n      labels = tf.squeeze(labels)\n      log_probs = tf.nn.log_softmax(logits, axis=-1)\n      one_hot_labels = tf.one_hot(\n          tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n      per_example_loss = -tf.reduce_sum(\n          tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n      loss = tf.reduce_mean(per_example_loss)\n      return loss\n\n    return _classification_loss_fn\n\n  def run_classifier(self, train_input_fn, validation_input_fn, epochs,\n                     steps_per_epoch, validation_steps, num_classes):\n    """"""Creates classifier and runs the classifier training.""""""\n\n    warmup_steps = int(epochs * steps_per_epoch * 0.1)\n    initial_lr = self.learning_rate\n\n    def _get_classifier_model():\n      """"""Gets a classifier model.""""""\n      classifier_model, core_model = create_classifier_model(\n          self.bert_config,\n          num_classes,\n          self.seq_len,\n          hub_module_url=self.uri,\n          hub_module_trainable=self.trainable,\n          is_tf2=self.is_tf2)\n\n      classifier_model.optimizer = optimization.create_optimizer(\n          initial_lr, steps_per_epoch * epochs, warmup_steps)\n      return classifier_model, core_model\n\n    loss_fn = self._get_classification_loss_fn(num_classes)\n\n    # Defines evaluation metrics function, which will create metrics in the\n    # correct device and strategy scope.\n    def metric_fn():\n      return tf.keras.metrics.SparseCategoricalAccuracy(\n          \'test_accuracy\', dtype=tf.float32)\n\n    with distribution_utils.get_strategy_scope(self.strategy):\n      training_dataset = train_input_fn()\n      evaluation_dataset = None\n      if validation_input_fn is not None:\n        evaluation_dataset = validation_input_fn()\n      bert_model, _ = _get_classifier_model()\n      optimizer = bert_model.optimizer\n\n      bert_model.compile(\n          optimizer=optimizer, loss=loss_fn, metrics=[metric_fn()])\n\n    summary_dir = os.path.join(self.model_dir, \'summaries\')\n    summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n    checkpoint_path = os.path.join(self.model_dir, \'checkpoint\')\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, save_weights_only=True)\n\n    bert_model.fit(\n        x=training_dataset,\n        validation_data=evaluation_dataset,\n        steps_per_epoch=steps_per_epoch,\n        epochs=epochs,\n        validation_steps=validation_steps,\n        callbacks=[summary_callback, checkpoint_callback])\n\n    return bert_model\n\n  def get_config(self):\n    """"""Gets the configuration.""""""\n    # Only preprocessing related variables are included.\n    return {\'uri\': self.uri, \'seq_len\': self.seq_len}\n\n\ndef dump_to_files(all_predictions, all_nbest_json, scores_diff_json,\n                  version_2_with_negative, output_dir):\n  """"""Save output to json files for question answering.""""""\n  output_prediction_file = os.path.join(output_dir, \'predictions.json\')\n  output_nbest_file = os.path.join(output_dir, \'nbest_predictions.json\')\n  output_null_log_odds_file = os.path.join(output_dir, \'null_odds.json\')\n  tf.compat.v1.logging.info(\'Writing predictions to: %s\',\n                            (output_prediction_file))\n  tf.compat.v1.logging.info(\'Writing nbest to: %s\', (output_nbest_file))\n\n  squad_lib.write_to_json_files(all_predictions, output_prediction_file)\n  squad_lib.write_to_json_files(all_nbest_json, output_nbest_file)\n  if version_2_with_negative:\n    squad_lib.write_to_json_files(scores_diff_json, output_null_log_odds_file)\n\n\ndef create_qa_model(bert_config,\n                    max_seq_length,\n                    initializer=None,\n                    hub_module_url=None,\n                    hub_module_trainable=True,\n                    is_tf2=True):\n  """"""Returns BERT qa model along with core BERT model to import weights.\n\n  Args:\n    bert_config: BertConfig, the config defines the core Bert model.\n    max_seq_length: integer, the maximum input sequence length.\n    initializer: Initializer for the final dense layer in the span labeler.\n      Defaulted to TruncatedNormal initializer.\n    hub_module_url: TF-Hub path/url to Bert module.\n    hub_module_trainable: True to finetune layers in the hub module.\n    is_tf2: boolean, whether the hub module is in TensorFlow 2.x format.\n\n  Returns:\n    A tuple of (1) keras model that outputs start logits and end logits and\n    (2) the core BERT transformer encoder.\n  """"""\n\n  if initializer is None:\n    initializer = tf.keras.initializers.TruncatedNormal(\n        stddev=bert_config.initializer_range)\n\n  input_word_ids = tf.keras.layers.Input(\n      shape=(max_seq_length,), dtype=tf.int32, name=\'input_word_ids\')\n  input_mask = tf.keras.layers.Input(\n      shape=(max_seq_length,), dtype=tf.int32, name=\'input_mask\')\n  input_type_ids = tf.keras.layers.Input(\n      shape=(max_seq_length,), dtype=tf.int32, name=\'input_type_ids\')\n\n  if is_tf2:\n    core_model = hub.KerasLayer(hub_module_url, trainable=hub_module_trainable)\n    pooled_output, sequence_output = core_model(\n        [input_word_ids, input_mask, input_type_ids])\n  else:\n    bert_model = hub_loader.HubKerasLayerV1V2(\n        hub_module_url,\n        signature=\'tokens\',\n        signature_outputs_as_dict=True,\n        trainable=hub_module_trainable)\n    outputs = bert_model({\n        \'input_ids\': input_word_ids,\n        \'input_mask\': input_mask,\n        \'segment_ids\': input_type_ids\n    })\n\n    pooled_output = outputs[\'pooled_output\']\n    sequence_output = outputs[\'sequence_output\']\n\n  bert_encoder = tf.keras.Model(\n      inputs=[input_word_ids, input_mask, input_type_ids],\n      outputs=[sequence_output, pooled_output],\n      name=\'core_model\')\n  return models.BertSpanLabeler(\n      network=bert_encoder, initializer=initializer), bert_encoder\n\n\nclass BertQAModelSpec(BertModelSpec):\n  """"""A specification of BERT model for question answering.""""""\n\n  def __init__(\n      self,\n      uri=\'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\',\n      model_dir=None,\n      seq_len=384,\n      query_len=64,\n      doc_stride=128,\n      dropout_rate=0.1,\n      initializer_range=0.02,\n      learning_rate=8e-5,\n      distribution_strategy=\'mirrored\',\n      num_gpus=-1,\n      tpu=\'\',\n      trainable=True,\n      predict_batch_size=8,\n      do_lower_case=True,\n      is_tf2=True):\n    """"""Initialze an instance with model paramaters.\n\n    Args:\n      uri: TF-Hub path/url to Bert module.\n      model_dir: The location of the model checkpoint files.\n      seq_len: Length of the sequence to feed into the model.\n      query_len: Length of the query to feed into the model.\n      doc_stride: The stride when we do a sliding window approach to take chunks\n        of the documents.\n      dropout_rate: The rate for dropout.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n      learning_rate: The initial learning rate for Adam.\n      distribution_strategy:  A string specifying which distribution strategy to\n        use. Accepted values are \'off\', \'one_device\', \'mirrored\',\n        \'parameter_server\', \'multi_worker_mirrored\', and \'tpu\' -- case\n        insensitive. \'off\' means not to use Distribution Strategy; \'tpu\' means\n        to use TPUStrategy using `tpu_address`.\n      num_gpus: How many GPUs to use at each worker with the\n        DistributionStrategies API. The default is -1, which means utilize all\n        available GPUs.\n      tpu: TPU address to connect to.\n      trainable: boolean, whether pretrain layer is trainable.\n      predict_batch_size: Batch size for prediction.\n      do_lower_case: boolean, whether to lower case the input text. Should be\n        True for uncased models and False for cased models.\n      is_tf2: boolean, whether the hub module is in TensorFlow 2.x format.\n    """"""\n    super(BertQAModelSpec, self).__init__(uri, model_dir, seq_len, dropout_rate,\n                                          initializer_range, learning_rate,\n                                          distribution_strategy, num_gpus, tpu,\n                                          trainable, do_lower_case, is_tf2)\n    self.query_len = query_len\n    self.doc_stride = doc_stride\n    self.predict_batch_size = predict_batch_size\n\n  def get_name_to_features(self, is_training):\n    """"""Gets the dictionary describing the features.""""""\n    name_to_features = {\n        \'input_ids\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'input_mask\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n        \'segment_ids\': tf.io.FixedLenFeature([self.seq_len], tf.int64),\n    }\n\n    if is_training:\n      name_to_features[\'start_positions\'] = tf.io.FixedLenFeature([], tf.int64)\n      name_to_features[\'end_positions\'] = tf.io.FixedLenFeature([], tf.int64)\n    else:\n      name_to_features[\'unique_ids\'] = tf.io.FixedLenFeature([], tf.int64)\n\n    return name_to_features\n\n  def select_data_from_record(self, record):\n    """"""Dispatches records to features and labels.""""""\n    x, y = {}, {}\n    for name, tensor in record.items():\n      if name in (\'start_positions\', \'end_positions\'):\n        y[name] = tensor\n      elif name == \'input_ids\':\n        x[\'input_word_ids\'] = tensor\n      elif name == \'segment_ids\':\n        x[\'input_type_ids\'] = tensor\n      else:\n        x[name] = tensor\n    return (x, y)\n\n  def get_config(self):\n    """"""Gets the configuration.""""""\n    # Only preprocessing related variables are included.\n    return {\n        \'uri\': self.uri,\n        \'seq_len\': self.seq_len,\n        \'query_len\': self.query_len,\n        \'doc_stride\': self.doc_stride\n    }\n\n  def convert_examples_to_features(self, examples, is_training, output_fn,\n                                   batch_size):\n    """"""Converts examples to features and write them into TFRecord file.""""""\n    if not self.is_built:\n      self.build()\n\n    return squad_lib.convert_examples_to_features(\n        examples=examples,\n        tokenizer=self.tokenizer,\n        max_seq_length=self.seq_len,\n        doc_stride=self.doc_stride,\n        max_query_length=self.query_len,\n        is_training=is_training,\n        output_fn=output_fn,\n        batch_size=batch_size)\n\n  def train(self, train_input_fn, epochs, steps_per_epoch):\n    """"""Run bert QA training.""""""\n    warmup_steps = int(epochs * steps_per_epoch * 0.1)\n\n    def _get_qa_model():\n      """"""Get QA model and optimizer.""""""\n      qa_model, core_model = create_qa_model(\n          self.bert_config,\n          self.seq_len,\n          hub_module_url=self.uri,\n          hub_module_trainable=self.trainable,\n          is_tf2=self.is_tf2)\n      optimizer = optimization.create_optimizer(self.learning_rate,\n                                                steps_per_epoch * epochs,\n                                                warmup_steps)\n      qa_model.optimizer = optimizer\n      return qa_model, core_model\n\n    def _loss_fn(positions, logits):\n      """"""Get losss function for QA model.""""""\n      loss = tf.keras.losses.sparse_categorical_crossentropy(\n          positions, logits, from_logits=True)\n      return tf.reduce_mean(loss)\n\n    with distribution_utils.get_strategy_scope(self.strategy):\n      training_dataset = train_input_fn()\n      bert_model, _ = _get_qa_model()\n      optimizer = bert_model.optimizer\n\n      bert_model.compile(\n          optimizer=optimizer, loss=_loss_fn, loss_weights=[0.5, 0.5])\n\n    summary_dir = os.path.join(self.model_dir, \'summaries\')\n    summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n    checkpoint_path = os.path.join(self.model_dir, \'checkpoint\')\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, save_weights_only=True)\n\n    bert_model.fit(\n        x=training_dataset,\n        steps_per_epoch=steps_per_epoch,\n        epochs=epochs,\n        callbacks=[summary_callback, checkpoint_callback])\n\n    return bert_model\n\n  def _predict_without_distribute_strategy(self, model, input_fn):\n    """"""Predicts the dataset without using distribute strategy.""""""\n    ds = input_fn()\n    all_results = []\n    for features, _ in ds:\n      outputs = model.predict_on_batch(features)\n      for unique_id, start_logits, end_logits in zip(features[\'unique_ids\'],\n                                                     outputs[0], outputs[1]):\n        raw_result = run_squad_helper.RawResult(\n            unique_id=unique_id.numpy(),\n            start_logits=start_logits.tolist(),\n            end_logits=end_logits.tolist())\n        all_results.append(raw_result)\n        if len(all_results) % 100 == 0:\n          tf.compat.v1.logging.info(\'Made predictions for %d records.\',\n                                    len(all_results))\n    return all_results\n\n  def _predict_with_distribute_strategy(self, model, input_fn, num_steps):\n    """"""Predicts the dataset using distribute strategy.""""""\n    predict_iterator = iter(\n        self.strategy.experimental_distribute_datasets_from_function(input_fn))\n\n    @tf.function\n    def predict_step(iterator):\n      """"""Predicts on distributed devices.""""""\n\n      def _replicated_step(inputs):\n        """"""Replicated prediction calculation.""""""\n        x, _ = inputs\n        unique_ids = x.pop(\'unique_ids\')\n        start_logits, end_logits = model(x, training=False)\n        return dict(\n            unique_ids=unique_ids,\n            start_logits=start_logits,\n            end_logits=end_logits)\n\n      outputs = self.strategy.run(_replicated_step, args=(next(iterator),))\n      return tf.nest.map_structure(self.strategy.experimental_local_results,\n                                   outputs)\n\n    all_results = []\n    for _ in range(num_steps):\n      predictions = predict_step(predict_iterator)\n      for result in run_squad_helper.get_raw_results(predictions):\n        all_results.append(result)\n      if len(all_results) % 100 == 0:\n        tf.compat.v1.logging.info(\'Made predictions for %d records.\',\n                                  len(all_results))\n    return all_results\n\n  def predict(self, model, input_fn, num_steps):\n    """"""Predicts the dataset from `input_fn` for `model`.""""""\n    if self.strategy:\n      return self._predict_with_distribute_strategy(model, input_fn, num_steps)\n    else:\n      return self._predict_without_distribute_strategy(model, input_fn)\n\n  def evaluate(self, model, input_fn, num_steps, eval_examples, eval_features,\n               predict_file, version_2_with_negative, max_answer_length,\n               null_score_diff_threshold, verbose_logging, output_dir):\n    """"""Evaluate QA model.\n\n    Args:\n      model: The model to be evaluated.\n      input_fn: Function that returns a tf.data.Dataset used for evaluation.\n      num_steps: Number of steps to evaluate the model.\n      eval_examples: List of `squad_lib.SquadExample` for evaluation data.\n      eval_features: List of `squad_lib.InputFeatures` for evaluation data.\n      predict_file: The input predict file.\n      version_2_with_negative: Whether the input predict file is SQuAD 2.0\n        format.\n      max_answer_length: The maximum length of an answer that can be generated.\n        This is needed because the start and end predictions are not conditioned\n        on one another.\n      null_score_diff_threshold: If null_score - best_non_null is greater than\n        the threshold, predict null. This is only used for SQuAD v2.\n      verbose_logging: If true, all of the warnings related to data processing\n        will be printed. A number of warnings are expected for a normal SQuAD\n        evaluation.\n      output_dir: The output directory to save output to json files:\n        predictions.json, nbest_predictions.json, null_odds.json. If None, skip\n        saving to json files.\n\n    Returns:\n      A dict contains two metrics: Exact match rate and F1 score.\n    """"""\n    all_results = self.predict(model, input_fn, num_steps)\n\n    all_predictions, all_nbest_json, scores_diff_json = (\n        squad_lib.postprocess_output(\n            eval_examples,\n            eval_features,\n            all_results,\n            n_best_size=20,\n            max_answer_length=max_answer_length,\n            do_lower_case=self.do_lower_case,\n            version_2_with_negative=version_2_with_negative,\n            null_score_diff_threshold=null_score_diff_threshold,\n            verbose=verbose_logging))\n\n    if output_dir is not None:\n      dump_to_files(all_predictions, all_nbest_json, scores_diff_json,\n                    version_2_with_negative, output_dir)\n\n    dataset_json = file_util.load_json_file(predict_file)\n    pred_dataset = dataset_json[\'data\']\n\n    if version_2_with_negative:\n      eval_metrics = squad_evaluate_v2_0.evaluate(pred_dataset, all_predictions,\n                                                  scores_diff_json)\n    else:\n      eval_metrics = squad_evaluate_v1_1.evaluate(pred_dataset, all_predictions)\n    return eval_metrics\n\n\nmobilebert_classifier_spec = BertClassifierModelSpec(\n    uri=\'https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1\',\n    is_tf2=False,\n    distribution_strategy=\'off\',\n    convert_from_saved_model_tf2=True)\n\n\n# A dict for model specs to make it accessible by string key.\nMODEL_SPECS = {\n    \'efficientnet_lite0\': efficientnet_lite0_spec,\n    \'efficientnet_lite1\': efficientnet_lite1_spec,\n    \'efficientnet_lite2\': efficientnet_lite2_spec,\n    \'efficientnet_lite3\': efficientnet_lite3_spec,\n    \'efficientnet_lite4\': efficientnet_lite4_spec,\n    \'mobilenet_v2\': mobilenet_v2_spec,\n    \'resnet_50\': resnet_50_spec,\n    \'average_word_vec\': AverageWordVecModelSpec,\n    \'bert\': BertModelSpec,\n    \'bert_classifier\': BertClassifierModelSpec,\n    \'bert_qa\': BertQAModelSpec,\n    \'mobilebert_classifier\': mobilebert_classifier_spec\n}\n\n# List constants for supported models.\nIMAGE_CLASSIFICATION_MODELS = [\n    \'efficientnet_lite0\', \'efficientnet_lite1\', \'efficientnet_lite2\',\n    \'efficientnet_lite3\', \'efficientnet_lite4\', \'mobilenet_v2\', \'resnet_50\'\n]\nTEXT_CLASSIFICATION_MODELS = [\n    \'bert_classifier\', \'average_word_vec\', \'mobilebert_classifier\'\n]\nQUESTION_ANSWERING_MODELS = [\'bert_qa\']\n\n\ndef get(spec_or_str):\n  """"""Gets model spec by name or instance, and initializes by default.""""""\n  if isinstance(spec_or_str, str):\n    model_spec = MODEL_SPECS[spec_or_str]\n  else:\n    model_spec = spec_or_str\n\n  if inspect.isclass(model_spec):\n    return model_spec()\n  else:\n    return model_spec\n'"
tensorflow_examples/lite/model_maker/core/task/model_spec_test.py,23,"b""# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nfrom absl.testing import parameterized\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec as ms\nfrom official.nlp.data import classifier_data_lib\n\n\ndef _gen_examples():\n  examples = []\n  examples.append(\n      classifier_data_lib.InputExample(\n          guid=0, text_a='Really good.', label='pos'))\n  examples.append(\n      classifier_data_lib.InputExample(guid=1, text_a='So bad.', label='neg'))\n  return examples\n\n\nclass ModelSpecTest(tf.test.TestCase):\n\n  def test_get(self):\n    spec = ms.get('mobilenet_v2')\n    self.assertIsInstance(spec, ms.ImageModelSpec)\n\n    spec = ms.get('average_word_vec')\n    self.assertIsInstance(spec, ms.AverageWordVecModelSpec)\n\n    spec = ms.get(ms.mobilenet_v2_spec)\n    self.assertEqual(spec, ms.mobilenet_v2_spec)\n\n    with self.assertRaises(KeyError):\n      ms.get('not_exist_model_spec')\n\n\ndef _get_dataset_from_tfrecord(tfrecord_file, name_to_features):\n\n  def _parse_function(example_proto):\n    # Parse the input `tf.Example` proto using the dictionary above.\n    return tf.io.parse_single_example(example_proto, name_to_features)\n\n  ds = tf.data.TFRecordDataset(tfrecord_file)\n  ds = ds.map(_parse_function)\n  return ds\n\n\nclass AverageWordVecModelSpecTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(AverageWordVecModelSpecTest, self).setUp()\n    self.model_spec = ms.AverageWordVecModelSpec(seq_len=5)\n    self.vocab = collections.OrderedDict(\n        (('<PAD>', 0), ('<START>', 1), ('<UNKNOWN>', 2), ('good', 3), ('bad',\n                                                                       4)))\n    self.model_spec.vocab = self.vocab\n\n  def test_tokenize(self):\n    model_spec = ms.AverageWordVecModelSpec()\n    text = model_spec._tokenize('It\\'s really good.')\n    self.assertEqual(text, ['it\\'s', 'really', 'good'])\n\n    model_spec = ms.AverageWordVecModelSpec(lowercase=False)\n    text = model_spec._tokenize('That is so cool!!!')\n    self.assertEqual(text, ['That', 'is', 'so', 'cool'])\n\n  def test_convert_examples_to_features(self):\n    examples = _gen_examples()\n    tfrecord_file = os.path.join(self.get_temp_dir(), 'tmp.tfrecord')\n    self.model_spec.convert_examples_to_features(examples, tfrecord_file,\n                                                 ['pos', 'neg'])\n    ds = _get_dataset_from_tfrecord(tfrecord_file,\n                                    self.model_spec.get_name_to_features())\n\n    expected_features = [[[1, 2, 3, 0, 0], 0], [[1, 2, 4, 0, 0], 1]]\n    for i, sample in enumerate(ds):\n      self.assertTrue(\n          (sample['input_ids'].numpy() == expected_features[i][0]).all())\n      self.assertEqual(sample['label_ids'].numpy(), expected_features[i][1])\n\n  def test_preprocess(self):\n    token_ids = self.model_spec.preprocess('It\\'s really good.')\n    expected_token_ids = [1, 2, 2, 3, 0]\n    self.assertEqual(token_ids, expected_token_ids)\n\n  def test_gen_vocab(self):\n    examples = _gen_examples()\n    self.model_spec.gen_vocab(examples)\n    expected_vocab = collections.OrderedDict([('<PAD>', 0), ('<START>', 1),\n                                              ('<UNKNOWN>', 2), ('really', 3),\n                                              ('good', 4), ('so', 5),\n                                              ('bad', 6)])\n    self.assertEqual(self.model_spec.vocab, expected_vocab)\n\n  def test_save_load_vocab(self):\n    vocab_file = os.path.join(self.get_temp_dir(), 'vocab.txt')\n    self.model_spec.save_vocab(vocab_file)\n    vocab = self.model_spec.load_vocab(vocab_file)\n    self.assertEqual(vocab, self.vocab)\n\n  def test_run_classifier(self):\n    num_classes = 2\n    model = self.model_spec.run_classifier(\n        train_input_fn=self._gen_random_input_fn(num_classes),\n        validation_input_fn=self._gen_random_input_fn(num_classes),\n        epochs=1,\n        steps_per_epoch=1,\n        validation_steps=1,\n        num_classes=num_classes)\n    self.assertIsInstance(model, tf.keras.Model)\n\n  def _gen_random_input_fn(self, num_classes, data_size=1, batch_size=4):\n\n    def _input_fn():\n      batched_features = tf.random.uniform(\n          (data_size, batch_size, self.model_spec.seq_len),\n          minval=0,\n          maxval=len(self.model_spec.vocab),\n          dtype=tf.dtypes.int32)\n\n      batched_labels = tf.random.uniform((data_size, batch_size),\n                                         minval=0,\n                                         maxval=num_classes,\n                                         dtype=tf.dtypes.int32)\n      ds = tf.data.Dataset.from_tensor_slices(\n          (batched_features, batched_labels))\n      return ds\n\n    return _input_fn\n\n\nclass BertClassifierModelSpecTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      ('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', True),\n      ('https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1', False),\n  )\n  def test_bert(self, uri, is_tf2):\n    model_spec = ms.BertClassifierModelSpec(\n        uri, is_tf2=is_tf2, distribution_strategy='off', seq_len=3)\n    self._test_convert_examples_to_features(model_spec)\n    self._test_run_classifier(model_spec)\n\n  def _test_convert_examples_to_features(self, model_spec):\n    examples = _gen_examples()\n    tfrecord_file = os.path.join(self.get_temp_dir(), 'tmp.tfrecord')\n    model_spec.convert_examples_to_features(examples, tfrecord_file,\n                                            ['pos', 'neg'])\n\n    ds = _get_dataset_from_tfrecord(tfrecord_file,\n                                    model_spec.get_name_to_features())\n    expected_features = []\n    expected_features.append({\n        'input_ids': [101, 2428, 102],\n        'input_mask': [1, 1, 1],\n        'segment_ids': [0, 0, 0],\n        'label_ids': 0\n    })\n    expected_features.append({\n        'input_ids': [101, 2061, 102],\n        'input_mask': [1, 1, 1],\n        'segment_ids': [0, 0, 0],\n        'label_ids': 1\n    })\n    for i, sample in enumerate(ds):\n      for k, v in expected_features[i].items():\n        self.assertTrue((sample[k].numpy() == v).all())\n\n  def _test_run_classifier(self, model_spec):\n    num_classes = 2\n    model = model_spec.run_classifier(\n        train_input_fn=self._gen_random_input_fn(model_spec.seq_len,\n                                                 num_classes),\n        validation_input_fn=self._gen_random_input_fn(model_spec.seq_len,\n                                                      num_classes),\n        epochs=1,\n        steps_per_epoch=1,\n        validation_steps=1,\n        num_classes=num_classes)\n    self.assertIsInstance(model, tf.keras.Model)\n\n  def _gen_random_input_fn(self,\n                           seq_len,\n                           num_classes,\n                           data_size=1,\n                           batch_size=1):\n\n    def _input_fn():\n      batched_input_ids = tf.random.uniform((data_size, batch_size, seq_len),\n                                            minval=0,\n                                            maxval=2,\n                                            dtype=tf.dtypes.int32)\n      batched_input_mask = tf.random.uniform((data_size, batch_size, seq_len),\n                                             minval=0,\n                                             maxval=2,\n                                             dtype=tf.dtypes.int32)\n      batched_segment_ids = tf.random.uniform((data_size, batch_size, seq_len),\n                                              minval=0,\n                                              maxval=2,\n                                              dtype=tf.dtypes.int32)\n\n      batched_labels = tf.random.uniform((data_size, batch_size),\n                                         minval=0,\n                                         maxval=num_classes,\n                                         dtype=tf.dtypes.int32)\n      x = {\n          'input_ids': batched_input_ids,\n          'input_mask': batched_input_mask,\n          'segment_ids': batched_segment_ids\n      }\n      y = batched_labels\n\n      ds = tf.data.Dataset.from_tensor_slices((x, y))\n      return ds\n\n    return _input_fn\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tensorflow_examples/lite/model_maker/core/task/model_util.py,4,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for keras models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\n\n\ndef set_batch_size(model, batch_size):\n  """"""Sets batch size for the model.""""""\n  for model_input in model.inputs:\n    new_shape = [batch_size] + model_input.shape[1:]\n    model_input.set_shape(new_shape)\n\n\ndef _create_temp_dir(convert_from_saved_model):\n  if convert_from_saved_model:\n    return tempfile.TemporaryDirectory()\n  else:\n    return DummyContextManager()\n\n\nclass DummyContextManager(object):\n\n  def __enter__(self):\n    pass\n\n  def __exit__(self, *args):\n    pass\n\n\ndef export_tflite(model,\n                  tflite_filepath,\n                  quantization_config=None,\n                  gen_dataset_fn=None,\n                  convert_from_saved_model_tf2=False):\n  """"""Converts the retrained model to tflite format and saves it.\n\n  Args:\n    model: model to be converted to tflite.\n    tflite_filepath: File path to save tflite model.\n    quantization_config: Configuration for post-training quantization.\n    gen_dataset_fn: Function to generate tf.data.dataset from\n      `representative_data`. Used only when `representative_data` in\n      `quantization_config` is setted.\n    convert_from_saved_model_tf2: Convert to TFLite from saved_model in TF 2.x.\n  """"""\n  if tflite_filepath is None:\n    raise ValueError(\n        ""TFLite filepath couldn\'t be None when exporting to tflite."")\n\n  if compat.get_tf_behavior() == 1:\n    lite = tf.compat.v1.lite\n  else:\n    lite = tf.lite\n\n  convert_from_saved_model = (\n      compat.get_tf_behavior() == 1 or convert_from_saved_model_tf2)\n  with _create_temp_dir(convert_from_saved_model) as temp_dir_name:\n    if temp_dir_name:\n      save_path = os.path.join(temp_dir_name, \'saved_model\')\n      model.save(save_path, include_optimizer=False, save_format=\'tf\')\n      converter = lite.TFLiteConverter.from_saved_model(save_path)\n    else:\n      converter = lite.TFLiteConverter.from_keras_model(model)\n\n    if quantization_config:\n      converter = quantization_config.get_converter_with_quantization(\n          converter, gen_dataset_fn)\n\n    tflite_model = converter.convert()\n\n  with tf.io.gfile.GFile(tflite_filepath, \'wb\') as f:\n    f.write(tflite_model)\n'"
tensorflow_examples/lite/model_maker/core/task/model_util_test.py,4,"b""# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.task import configs\nfrom tensorflow_examples.lite.model_maker.core.task import model_util\n\n\ndef _get_quantization_config_list(input_dim, num_classes, max_input_value):\n  # Configuration for dynamic range quantization.\n  config1 = configs.QuantizationConfig.create_dynamic_range_quantization()\n\n  representative_data = test_util.get_dataloader(\n      data_size=1,\n      input_shape=[input_dim],\n      num_classes=num_classes,\n      max_input_value=max_input_value)\n  # Configuration for full integer quantization with float fallback.\n  config2 = configs.QuantizationConfig.create_full_integer_quantization(\n      representative_data=representative_data, quantization_steps=1)\n  # Configuration for full integer quantization with integer only.\n  config3 = configs.QuantizationConfig.create_full_integer_quantization(\n      representative_data=representative_data,\n      quantization_steps=1,\n      is_integer_only=True)\n\n  # Configuration for full integer quantization with float fallback.\n  config4 = configs.QuantizationConfig.create_float16_quantization()\n  return [config1, config2, config3, config4]\n\n\ndef _mock_gen_dataset(data, batch_size=1, is_training=False):  # pylint: disable=unused-argument\n  ds = data.dataset\n  ds = ds.batch(batch_size)\n  return ds\n\n\nclass ModelUtilTest(tf.test.TestCase):\n\n  @test_util.test_in_tf_1and2\n  def test_export_tflite(self):\n    input_dim = 4\n    model = test_util.build_model(input_shape=[input_dim], num_classes=2)\n    tflite_file = os.path.join(self.get_temp_dir(), 'model.tflite')\n    model_util.export_tflite(model, tflite_file)\n    self._test_tflite(model, tflite_file, input_dim)\n\n  @test_util.test_in_tf_1and2\n  def test_export_tflite_quantized(self):\n    input_dim = 4\n    num_classes = 2\n    max_input_value = 5\n    model = test_util.build_model([input_dim], num_classes)\n    tflite_file = os.path.join(self.get_temp_dir(), 'model_quantized.tflite')\n\n    for config in _get_quantization_config_list(input_dim, num_classes,\n                                                max_input_value):\n      model_util.export_tflite(model, tflite_file, config, _mock_gen_dataset)\n      self._test_tflite(\n          model, tflite_file, input_dim, max_input_value, atol=1e-01)\n\n  def _test_tflite(self,\n                   keras_model,\n                   tflite_model_file,\n                   input_dim,\n                   max_input_value=1000,\n                   atol=1e-04):\n    with tf.io.gfile.GFile(tflite_model_file, 'rb') as f:\n      tflite_model = f.read()\n\n    np.random.seed(0)\n    random_input = np.random.uniform(\n        low=0, high=max_input_value, size=(1, input_dim)).astype(np.float32)\n\n    # Gets output from keras model.\n    keras_output = keras_model.predict(random_input)\n\n    # Gets output from tflite model.\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()[0]\n    if input_details['dtype'] != np.float32:\n      # Quantize the input\n      scale, zero_point = input_details['quantization']\n      random_input = random_input / scale + zero_point\n      random_input = random_input.astype(input_details['dtype'])\n    interpreter.set_tensor(input_details['index'], random_input)\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()[0]\n    lite_output = interpreter.get_tensor(output_details['index'])\n    if output_details['dtype'] != np.float32:\n      # Dequantize the output\n      scale, zero_point = output_details['quantization']\n      lite_output = lite_output.astype(np.float32)\n      lite_output = (lite_output - zero_point) * scale\n    self.assertTrue(np.allclose(lite_output, keras_output, atol=atol))\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tensorflow_examples/lite/model_maker/core/task/model_util_v1_test.py,1,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.task import model_util_test\n\n\nclass ModelUtilTest(model_util_test.ModelUtilTest):\n  """"""Share text tests of the base class, but in tf v1 behavior.""""""\n\n\nif __name__ == \'__main__\':\n  compat.setup_tf_behavior(tf_version=1)\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/core/task/question_answer.py,1,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""QuestionAnswer class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\nfrom tensorflow_examples.lite.model_maker.core.task import custom_model\nfrom tensorflow_examples.lite.model_maker.core.task import model_util\n\n\ndef create(train_data, model_spec, batch_size=32, epochs=2, shuffle=False):\n  """"""Loads data and train the model for question answer.\n\n  Args:\n    train_data: Training data.\n    model_spec: Specification for the model.\n    batch_size: Batch size for training.\n    epochs: Number of epochs for training.\n    shuffle: Whether the data should be shuffled.\n\n  Returns:\n    object of QuestionAnswer class.\n  """"""\n  if compat.get_tf_behavior() not in model_spec.compat_tf_versions:\n    raise ValueError(\'Incompatible versions. Expect {}, but got {}.\'.format(\n        model_spec.compat_tf_versions, compat.get_tf_behavior()))\n\n  model = QuestionAnswer(model_spec, shuffle=shuffle)\n\n  tf.compat.v1.logging.info(\'Retraining the models...\')\n  model.train(train_data, epochs, batch_size)\n\n  return model\n\n\nclass QuestionAnswer(custom_model.CustomModel):\n  """"""QuestionAnswer class for inference and exporting to tflite.""""""\n\n  DEFAULT_EXPORT_FORMAT = [ExportFormat.TFLITE, ExportFormat.VOCAB]\n  ALLOWED_EXPORT_FORMAT = [\n      ExportFormat.TFLITE, ExportFormat.VOCAB, ExportFormat.SAVED_MODEL\n  ]\n\n  def preprocess(self, raw_text, label):\n    """"""Preprocess the text.""""""\n    # TODO(yuqili): remove this method once preprocess for image classifier is\n    # also moved to DataLoader part.\n    return raw_text, label\n\n  def train(self, train_data, epochs=None, batch_size=32):\n    """"""Feeds the training data for training.""""""\n    train_input_fn, steps_per_epoch = self._get_input_fn_and_steps(\n        train_data, batch_size, is_training=True)\n\n    self.model = self.model_spec.train(train_input_fn, epochs, steps_per_epoch)\n\n    return self.model\n\n  def evaluate(self,\n               data,\n               max_answer_length=30,\n               null_score_diff_threshold=0.0,\n               verbose_logging=False,\n               output_dir=None):\n    """"""Evaluate the model.\n\n    Args:\n      data: Data to be evaluated.\n      max_answer_length: The maximum length of an answer that can be generated.\n        This is needed because the start and end predictions are not conditioned\n        on one another.\n      null_score_diff_threshold: If null_score - best_non_null is greater than\n        the threshold, predict null. This is only used for SQuAD v2.\n      verbose_logging: If true, all of the warnings related to data processing\n        will be printed. A number of warnings are expected for a normal SQuAD\n        evaluation.\n      output_dir: The output directory to save output to json files:\n        predictions.json, nbest_predictions.json, null_odds.json. If None, skip\n        saving to json files.\n\n    Returns:\n      A dict contains two metrics: Exact match rate and F1 score.\n    """"""\n    predict_batch_size = self.model_spec.predict_batch_size\n    input_fn = self._get_dataset_fn(data, predict_batch_size, is_training=False)\n    num_steps = int(data.size / predict_batch_size)\n    return self.model_spec.evaluate(\n        self.model, input_fn, num_steps, data.examples, data.features,\n        data.squad_file, data.version_2_with_negative, max_answer_length,\n        null_score_diff_threshold, verbose_logging, output_dir)\n\n  def export(self,\n             export_dir,\n             tflite_filename=\'model.tflite\',\n             vocab_filename=\'vocab\',\n             saved_model_filename=\'saved_model\',\n             export_format=None,\n             **kwargs):\n    """"""Converts the retrained model based on `export_format`.\n\n    Args:\n      export_dir: The directory to save exported files.\n      tflite_filename: File name to save tflite model. The full export path is\n        {export_dir}/{tflite_filename}.\n      vocab_filename: File name to save vocabulary.  The full export path is\n        {export_dir}/{vocab_filename}.\n      saved_model_filename: Path to SavedModel or H5 file to save the model. The\n        full export path is\n        {export_dir}/{saved_model_filename}/{saved_model.pb|assets|variables}.\n      export_format: List of export format that could be saved_model, tflite,\n        label, vocab.\n      **kwargs: Other parameters like `quantized` for TFLITE model.\n    """"""\n    super(QuestionAnswer, self).export(\n        export_dir,\n        tflite_filename=tflite_filename,\n        vocab_filename=vocab_filename,\n        saved_model_filename=saved_model_filename,\n        export_format=export_format,\n        **kwargs)\n\n  def _export_tflite(self, tflite_filepath, quantization_config=None):\n    """"""Converts the retrained model to tflite format and saves it.\n\n    Args:\n      tflite_filepath: File path to save tflite model.\n      quantization_config: Configuration for post-training quantization.\n    """"""\n    # Sets batch size from None to 1 when converting to tflite.\n    model_util.set_batch_size(self.model, batch_size=1)\n    model_util.export_tflite(self.model, tflite_filepath, quantization_config,\n                             self._gen_dataset,\n                             self.model_spec.convert_from_saved_model_tf2)\n    # Sets batch size back to None to support retraining later.\n    model_util.set_batch_size(self.model, batch_size=None)\n'"
tensorflow_examples/lite/model_maker/core/task/question_answer_test.py,2,"b""# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl.testing import parameterized\nimport tensorflow as tf\n\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.data_util import text_dataloader\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec as ms\nfrom tensorflow_examples.lite.model_maker.core.task import question_answer\n\n\ndef _get_data(model_spec, version):\n  path = test_util.get_test_data_path('squad_testdata')\n  train_data_path = os.path.join(path, 'train-v%s.json' % version)\n  validation_data_path = os.path.join(path, 'dev-v%s.json' % version)\n  version_2_with_negative = version.startswith('2')\n  train_data = text_dataloader.QuestionAnswerDataLoader.from_squad(\n      train_data_path,\n      model_spec,\n      is_training=True,\n      version_2_with_negative=version_2_with_negative)\n  validation_data = text_dataloader.QuestionAnswerDataLoader.from_squad(\n      validation_data_path,\n      model_spec,\n      is_training=False,\n      version_2_with_negative=version_2_with_negative)\n  return train_data, validation_data\n\n\nclass QuestionAnswerTest(tf.test.TestCase, parameterized.TestCase):\n\n  @test_util.test_in_tf_1\n  def test_bert_model_v1_incompatible(self):\n    with self.assertRaisesRegex(ValueError, 'Incompatible versions'):\n      _ = ms.BertQAModelSpec(trainable=False)\n\n  @parameterized.parameters(\n      ('1.1'),\n      ('2.0'),\n  )\n  @test_util.test_in_tf_2\n  def test_bert_model(self, version):\n    model_spec = ms.BertQAModelSpec(trainable=False, predict_batch_size=1)\n    train_data, validation_data = _get_data(model_spec, version)\n    model = question_answer.create(\n        train_data, model_spec=model_spec, epochs=1, batch_size=1)\n    self._test_f1_score(model, validation_data, 0.0)\n    self._test_export_vocab(model)\n    self._test_export_to_tflite(model)\n    self._test_export_to_saved_model(model)\n\n  def _test_f1_score(self, model, validation_data, threshold):\n    metric = model.evaluate(validation_data)\n    self.assertGreaterEqual(metric['final_f1'], threshold)\n\n  def _test_export_vocab(self, model):\n    vocab_output_file = os.path.join(self.get_temp_dir(), 'vocab')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.VOCAB)\n\n    self.assertTrue(os.path.isfile(vocab_output_file))\n    self.assertGreater(os.path.getsize(vocab_output_file), 0)\n\n  def _test_export_to_tflite(self, model):\n    tflite_output_file = os.path.join(self.get_temp_dir(), 'model.tflite')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.TFLITE)\n\n    self.assertTrue(os.path.isfile(tflite_output_file))\n    self.assertGreater(os.path.getsize(tflite_output_file), 0)\n\n  def _test_export_to_saved_model(self, model):\n    save_model_output_path = os.path.join(self.get_temp_dir(), 'saved_model')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.SAVED_MODEL)\n\n    self.assertTrue(os.path.isdir(save_model_output_path))\n    self.assertNotEmpty(os.listdir(save_model_output_path))\n\n\nif __name__ == '__main__':\n  compat.setup_tf_behavior(tf_version=2)\n  tf.test.main()\n"""
tensorflow_examples/lite/model_maker/core/task/question_answer_v1_test.py,1,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.task import question_answer_test\n\n\nclass QuestionAnswerV1Test(question_answer_test.QuestionAnswerTest):\n  """"""Share text tests of the base class, but in tf v1 behavior.""""""\n\n\nif __name__ == \'__main__\':\n  compat.setup_tf_behavior(tf_version=1)\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/core/task/text_classifier.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TextClassier class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\nfrom tensorflow_examples.lite.model_maker.core.task import classification_model\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec as ms\nfrom tensorflow_examples.lite.model_maker.core.task import model_util\n\n\ndef create(train_data,\n           model_spec=ms.AverageWordVecModelSpec(),\n           validation_data=None,\n           batch_size=32,\n           epochs=3,\n           shuffle=False):\n  """"""Loads data and train the model for test classification.\n\n  Args:\n    train_data: Training data.\n    model_spec: Specification for the model.\n    validation_data: Validation data. If None, skips validation process.\n    batch_size: Batch size for training.\n    epochs: Number of epochs for training.\n    shuffle: Whether the data should be shuffled.\n\n  Returns:\n    TextClassifier\n  """"""\n  if compat.get_tf_behavior() not in model_spec.compat_tf_versions:\n    raise ValueError(\'Incompatible versions. Expect {}, but got {}.\'.format(\n        model_spec.compat_tf_versions, compat.get_tf_behavior()))\n\n  text_classifier = TextClassifier(\n      model_spec,\n      train_data.index_to_label,\n      train_data.num_classes,\n      shuffle=shuffle)\n\n  tf.compat.v1.logging.info(\'Retraining the models...\')\n  text_classifier.train(train_data, validation_data, epochs, batch_size)\n\n  return text_classifier\n\n\nclass TextClassifier(classification_model.ClassificationModel):\n  """"""TextClassifier class for inference and exporting to tflite.""""""\n\n  DEFAULT_EXPORT_FORMAT = [\n      ExportFormat.TFLITE, ExportFormat.LABEL, ExportFormat.VOCAB\n  ]\n  ALLOWED_EXPORT_FORMAT = [\n      ExportFormat.TFLITE, ExportFormat.LABEL, ExportFormat.VOCAB,\n      ExportFormat.SAVED_MODEL\n  ]\n\n  def __init__(self,\n               model_spec,\n               index_to_label,\n               num_classes,\n               shuffle=True):\n    """"""Init function for TextClassifier class.\n\n    Args:\n      model_spec: Specification for the model.\n      index_to_label: A list that map from index to label class name.\n      num_classes: Number of label classes.\n      shuffle: Whether the data should be shuffled.\n    """"""\n    super(TextClassifier, self).__init__(\n        model_spec,\n        index_to_label,\n        num_classes,\n        shuffle,\n        train_whole_model=True)\n\n  def train(self, train_data, validation_data=None, epochs=None, batch_size=32):\n    """"""Feeds the training data for training.""""""\n    train_input_fn, steps_per_epoch = self._get_input_fn_and_steps(\n        train_data, batch_size, is_training=True)\n    validation_input_fn, validation_steps = self._get_input_fn_and_steps(\n        validation_data, batch_size, is_training=False)\n\n    self.model = self.model_spec.run_classifier(train_input_fn,\n                                                validation_input_fn, epochs,\n                                                steps_per_epoch,\n                                                validation_steps,\n                                                self.num_classes)\n\n    return self.model\n\n  def _export_tflite(self, tflite_filepath, quantization_config=None):\n    """"""Converts the retrained model to tflite format and saves it.\n\n    Args:\n      tflite_filepath: File path to save tflite model.\n      quantization_config: Configuration for post-training quantization.\n    """"""\n    # TODO(b/151761399): Removes these lines.\n    if hasattr(self.model_spec, \'uri\') and \'mobilebert\' in self.model_spec.uri:\n      raise ValueError(\'Couldn\\\'t convert MobileBert to TFLite for now.\')\n\n    # Sets batch size from None to 1 when converting to tflite.\n    model_util.set_batch_size(self.model, batch_size=1)\n    model_util.export_tflite(self.model, tflite_filepath, quantization_config,\n                             self._gen_dataset,\n                             self.model_spec.convert_from_saved_model_tf2)\n    # Sets batch size back to None to support retraining later.\n    model_util.set_batch_size(self.model, batch_size=None)\n'"
tensorflow_examples/lite/model_maker/core/task/text_classifier_test.py,11,"b""# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.data_util import text_dataloader\nfrom tensorflow_examples.lite.model_maker.core.export_format import ExportFormat\nfrom tensorflow_examples.lite.model_maker.core.task import configs\nfrom tensorflow_examples.lite.model_maker.core.task import model_spec as ms\nfrom tensorflow_examples.lite.model_maker.core.task import text_classifier\n\n\nclass TextClassifierTest(tf.test.TestCase):\n  TEXT_PER_CLASS = 20\n  TEST_LABELS_AND_TEXT = (('pos', 'super good'), ('neg', 'really bad.'))\n\n  def _gen(self):\n    for i, (_, text) in enumerate(self.TEST_LABELS_AND_TEXT):\n      for _ in range(self.TEXT_PER_CLASS):\n        yield text, i\n\n  def _gen_text_dir(self):\n    text_dir = os.path.join(self.get_temp_dir(), 'random_text_dir')\n    if os.path.exists(text_dir):\n      return text_dir\n    os.mkdir(text_dir)\n\n    for class_name, text in self.TEST_LABELS_AND_TEXT:\n      class_subdir = os.path.join(text_dir, class_name)\n      os.mkdir(class_subdir)\n      for i in range(self.TEXT_PER_CLASS):\n        with open(os.path.join(class_subdir, '%d.txt' % i), 'w') as f:\n          f.write(text)\n    return text_dir\n\n  def setUp(self):\n    super(TextClassifierTest, self).setUp()\n    self.text_dir = self._gen_text_dir()\n\n  @test_util.test_in_tf_1\n  def test_average_wordvec_model_create_v1_incompatible(self):\n    with self.assertRaisesRegex(ValueError, 'Incompatible versions'):\n      model_spec = ms.AverageWordVecModelSpec(seq_len=2)\n      all_data = text_dataloader.TextClassifierDataLoader.from_folder(\n          self.text_dir, model_spec=model_spec)\n      _ = text_classifier.create(\n          all_data,\n          model_spec=model_spec,\n      )\n\n  @test_util.test_in_tf_2\n  def test_bert_model(self):\n    model_spec = ms.BertClassifierModelSpec(seq_len=2, trainable=False)\n    all_data = text_dataloader.TextClassifierDataLoader.from_folder(\n        self.text_dir, model_spec=model_spec)\n    # Splits data, 90% data for training, 10% for testing\n    self.train_data, self.test_data = all_data.split(0.9)\n\n    model = text_classifier.create(\n        self.train_data,\n        model_spec=model_spec,\n        epochs=1,\n        batch_size=1,\n        shuffle=True)\n    self._test_accuracy(model, 0.5)\n\n  @test_util.test_in_tf_2\n  def test_mobilebert_model(self):\n    model_spec = ms.mobilebert_classifier_spec\n    model_spec.seq_len = 2\n    model_spec.trainable = False\n    all_data = text_dataloader.TextClassifierDataLoader.from_folder(\n        self.text_dir, model_spec=model_spec)\n    # Splits data, 90% data for training, 10% for testing\n    self.train_data, self.test_data = all_data.split(0.9)\n\n    model = text_classifier.create(\n        self.train_data,\n        model_spec=model_spec,\n        epochs=1,\n        batch_size=1,\n        shuffle=True)\n    self._test_accuracy(model, 0.5)\n    error_message = 'Couldn\\'t convert MobileBert to TFLite for now.'\n    with self.assertRaises(ValueError) as error:\n      self._test_export_to_tflite(model, test_predict_accuracy=False)\n    self.assertEqual(error_message, str(error.exception))\n\n    with self.assertRaises(ValueError) as error:\n      self._test_export_to_tflite_quant(model)\n    self.assertEqual(error_message, str(error.exception))\n\n  @test_util.test_in_tf_2\n  def test_average_wordvec_model(self):\n    model_spec = ms.AverageWordVecModelSpec(seq_len=2)\n    all_data = text_dataloader.TextClassifierDataLoader.from_folder(\n        self.text_dir, model_spec=model_spec)\n    # Splits data, 90% data for training, 10% for testing\n    self.train_data, self.test_data = all_data.split(0.9)\n\n    model = text_classifier.create(\n        self.train_data,\n        model_spec=model_spec,\n        epochs=2,\n        batch_size=4,\n        shuffle=True)\n    self._test_accuracy(model)\n    self._test_predict_top_k(model)\n    self._test_export_to_tflite(model)\n    self._test_export_to_saved_model(model)\n    self._test_export_labels(model)\n    self._test_export_vocab(model)\n\n  def _test_accuracy(self, model, threshold=1.0):\n    _, accuracy = model.evaluate(self.test_data)\n    self.assertEqual(accuracy, threshold)\n\n  def _test_predict_top_k(self, model):\n    topk = model.predict_top_k(self.test_data, batch_size=4)\n    for i, (_, label) in enumerate(self.test_data.dataset):\n      predict_label, predict_prob = topk[i][0][0], topk[i][0][1]\n      self.assertEqual(model.index_to_label[label], predict_label)\n      self.assertGreater(predict_prob, 0.5)\n\n  def _load_vocab(self, filepath):\n    with tf.io.gfile.GFile(filepath, 'r') as f:\n      return [vocab.strip('\\n').split() for vocab in f]\n\n  def _load_labels(self, filepath):\n    with tf.io.gfile.GFile(filepath, 'r') as f:\n      return [label.strip('\\n') for label in f]\n\n  def _load_lite_model(self, filepath):\n    self.assertTrue(os.path.isfile(filepath))\n    with tf.io.gfile.GFile(filepath, 'rb') as f:\n      model_content = f.read()\n    interpreter = tf.lite.Interpreter(model_content=model_content)\n\n    def lite_model(text):\n      interpreter.allocate_tensors()\n      input_index = interpreter.get_input_details()[0]['index']\n      input_shape = interpreter.get_input_details()[0]['shape']\n      interpreter.set_tensor(input_index, tf.reshape(text, input_shape))\n      interpreter.invoke()\n      output_index = interpreter.get_output_details()[0]['index']\n      return interpreter.get_tensor(output_index)\n\n    return lite_model\n\n  def _test_export_labels(self, model):\n    labels_output_file = os.path.join(self.get_temp_dir(), 'labels.txt')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.LABEL)\n\n    labels = self._load_labels(labels_output_file)\n    self.assertEqual(labels, ['neg', 'pos'])\n\n  def _test_export_vocab(self, model):\n    vocab_output_file = os.path.join(self.get_temp_dir(), 'vocab')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.VOCAB)\n\n    word_index = self._load_vocab(vocab_output_file)\n    expected_predefined = [['<PAD>', '0'], ['<START>', '1'], ['<UNKNOWN>', '2']]\n    self.assertEqual(word_index[:3], expected_predefined)\n\n    expected_vocab = ['bad', 'good', 'really', 'super']\n    actual_vocab = sorted([word for word, index in word_index[3:]])\n    self.assertEqual(actual_vocab, expected_vocab)\n\n    expected_index = ['3', '4', '5', '6']\n    actual_index = [index for word, index in word_index[3:]]\n    self.assertEqual(actual_index, expected_index)\n\n  def _test_export_to_tflite(self, model, test_predict_accuracy=True):\n    tflite_output_file = os.path.join(self.get_temp_dir(), 'model.tflite')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.TFLITE)\n\n    self.assertTrue(tf.io.gfile.exists(tflite_output_file))\n    self.assertGreater(os.path.getsize(tflite_output_file), 0)\n\n    if test_predict_accuracy:\n      lite_model = self._load_lite_model(tflite_output_file)\n      for x, y in self.test_data.dataset:\n        input_batch = tf.cast(x, tf.float32)\n        output_batch = lite_model(input_batch)\n        prediction = np.argmax(output_batch[0])\n        self.assertEqual(y, prediction)\n\n  def _test_export_to_saved_model(self, model):\n    save_model_output_path = os.path.join(self.get_temp_dir(), 'saved_model')\n    model.export(self.get_temp_dir(), export_format=ExportFormat.SAVED_MODEL)\n\n    self.assertTrue(os.path.isdir(save_model_output_path))\n    self.assertNotEmpty(os.listdir(save_model_output_path))\n\n  def _test_export_to_tflite_quant(self, model):\n    tflite_filename = 'model_quant.tflite'\n    tflite_output_file = os.path.join(self.get_temp_dir(), tflite_filename)\n    config = configs.QuantizationConfig.create_dynamic_range_quantization(\n        optimizations=[tf.lite.Optimize.OPTIMIZE_FOR_LATENCY])\n    model.export(\n        self.get_temp_dir(),\n        tflite_filename=tflite_filename,\n        export_format=ExportFormat.TFLITE,\n        quantization_config=config)\n\n    self.assertTrue(tf.io.gfile.exists(tflite_output_file))\n    self.assertGreater(os.path.getsize(tflite_output_file), 0)\n\n\nif __name__ == '__main__':\n  compat.setup_tf_behavior(tf_version=2)\n  tf.test.main()\n"""
tensorflow_examples/lite/model_maker/core/task/text_classifier_v1_test.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow_examples.lite.model_maker.core import compat\nfrom tensorflow_examples.lite.model_maker.core.task import text_classifier_test\n\n\nclass TextClassifierV1Test(text_classifier_test.TextClassifierTest):\n  """"""Share text tests of the base class, but in tf v1 behavior.""""""\n\n\nif __name__ == \'__main__\':\n  compat.setup_tf_behavior(tf_version=1)\n  tf.test.main()\n'"
tensorflow_examples/lite/model_maker/core/task/train_image_classifier_lib.py,9,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Library to retrain image classifier models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport tempfile\n\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core.optimization import warmup\nfrom tensorflow_hub.tools.make_image_classifier import make_image_classifier_lib as hub_lib\n\nDEFAULT_DECAY_SAMPLES = 10000 * 256\nDEFAULT_WARMUP_EPOCHS = 2\n\n\ndef add_params(hparams, **kwargs):\n  param_dict = {k: v for k, v in kwargs.items() if v is not None}\n\n  return hparams._replace(**param_dict)\n\n\nclass HParams(\n    collections.namedtuple(\n        ""HParams"", hub_lib.HParams._fields + (""warmup_steps"", ""model_dir""))):\n  """"""The hyperparameters for make_image_classifier.\n\n  train_epochs: Training will do this many iterations over the dataset.\n  do_fine_tuning: If true, the Hub module is trained together with the\n    classification layer on top.\n  batch_size: Each training step samples a batch of this many images.\n  learning_rate: Base learning rate when train batch size is 256. Linear to the\n    batch size.\n  dropout_rate: The fraction of the input units to drop, used in dropout layer.\n  warmup_steps: Number of warmup steps for warmup schedule on learning rate.\n  model_dir: The location of the model checkpoint files.\n  """"""\n\n  @classmethod\n  def get_hparams(cls, **kwargs):\n    """"""Gets the hyperparameters for `train_image_classifier_lib`.""""""\n    hparams = get_default_hparams()\n    return add_params(hparams, **kwargs)\n\n\ndef get_default_hparams():\n  """"""Returns a fresh HParams object initialized to default values.""""""\n  default_hub_hparams = hub_lib.get_default_hparams()\n  as_dict = default_hub_hparams._asdict()\n  as_dict.update(\n      train_epochs=10,\n      do_fine_tuning=False,\n      batch_size=64,\n      learning_rate=0.004,\n      dropout_rate=0.2,\n      warmup_steps=None,\n      model_dir=tempfile.mkdtemp(),\n  )\n  default_hparams = HParams(**as_dict)\n  return default_hparams\n\n\ndef create_optimizer(init_lr, num_decay_steps, num_warmup_steps):\n  """"""Creates an optimizer with learning rate schedule.""""""\n  # Leverages cosine decay of the learning rate.\n  learning_rate_fn = tf.keras.experimental.CosineDecay(\n      initial_learning_rate=init_lr, decay_steps=num_decay_steps, alpha=0.0)\n  if num_warmup_steps:\n    learning_rate_fn = warmup.WarmUp(\n        initial_learning_rate=init_lr,\n        decay_schedule_fn=learning_rate_fn,\n        warmup_steps=num_warmup_steps)\n  optimizer = tf.keras.optimizers.RMSprop(\n      learning_rate=learning_rate_fn, rho=0.9, momentum=0.9, epsilon=0.001)\n\n  return optimizer\n\n\ndef train_model(model, hparams, train_data_and_size, validation_data_and_size):\n  """"""Trains model with the given data and hyperparameters.\n\n  Args:\n    model: The tf.keras.Model from _build_model().\n    hparams: A namedtuple of hyperparameters. This function expects\n      .train_epochs: a Python integer with the number of passes over the\n        training dataset;\n      .learning_rate: a Python float forwarded to the optimizer; Base learning\n        rate when train batch size is 256. Linear to the batch size;\n      .batch_size: a Python integer, the number of examples returned by each\n        call to the generators;\n      .warmup_steps: a Python integer, the number of warmup steps for warmup\n        schedule on learning rate. If None, default warmup_steps is used;\n      .model_dir: a Python string, the location of the model checkpoint files.\n    train_data_and_size: A (data, size) tuple in which data is training data to\n      be fed in tf.keras.Model.fit(), size is a Python integer with the numbers\n      of training.\n    validation_data_and_size: A (data, size) tuple in which data is validation\n      data to be fed in tf.keras.Model.fit(), size is a Python integer with the\n      numbers of validation.\n\n  Returns:\n    The tf.keras.callbacks.History object returned by tf.keras.Model.fit().\n  """"""\n  train_data, train_size = train_data_and_size\n  validation_data, validation_size = validation_data_and_size\n\n  steps_per_epoch = train_size // hparams.batch_size\n  validation_steps = validation_size // hparams.batch_size\n\n  # Learning rate is linear to batch size.\n  learning_rate = hparams.learning_rate * hparams.batch_size / 256\n\n  # Gets decay steps.\n  total_training_steps = steps_per_epoch * hparams.train_epochs\n  default_decay_steps = DEFAULT_DECAY_SAMPLES // hparams.batch_size\n  decay_steps = max(total_training_steps, default_decay_steps)\n\n  warmup_steps = hparams.warmup_steps\n  if warmup_steps is None:\n    warmup_steps = DEFAULT_WARMUP_EPOCHS * steps_per_epoch\n  optimizer = create_optimizer(learning_rate, decay_steps, warmup_steps)\n\n  loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n  model.compile(optimizer=optimizer, loss=loss, metrics=[""accuracy""])\n\n  summary_dir = os.path.join(hparams.model_dir, ""summaries"")\n  summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n  # Save checkpoint every 20 epochs.\n  checkpoint_path = os.path.join(hparams.model_dir, ""checkpoint"")\n  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n      checkpoint_path, save_weights_only=True, period=20)\n\n  # Trains the models.\n  return model.fit(\n      train_data,\n      epochs=hparams.train_epochs,\n      steps_per_epoch=steps_per_epoch,\n      validation_data=validation_data,\n      validation_steps=validation_steps,\n      callbacks=[summary_callback, checkpoint_callback])\n'"
community/en/r1/tutorials/image/alexnet/__init__.py,0,b''
community/en/r1/tutorials/image/alexnet/alexnet_benchmark.py,49,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Timing benchmark for AlexNet inference.\n\nTo run, use:\n  bazel run -c opt --config=cuda \\\n      models/tutorials/image/alexnet:alexnet_benchmark\n\nAcross 100 steps on batch size = 128.\n\nForward pass:\nRun on Tesla K40c: 145 +/- 1.5 ms / batch\nRun on Titan X:     70 +/- 0.1 ms / batch\n\nForward-backward pass:\nRun on Tesla K40c: 480 +/- 48 ms / batch\nRun on Titan X:    244 +/- 30 ms / batch\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nfrom datetime import datetime\nimport math\nimport sys\nimport time\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nFLAGS = None\n\n\ndef print_activations(t):\n  print(t.op.name, \' \', t.get_shape().as_list())\n\n\ndef inference(images):\n  """"""Build the AlexNet model.\n\n  Args:\n    images: Images Tensor\n\n  Returns:\n    pool5: the last Tensor in the convolutional component of AlexNet.\n    parameters: a list of Tensors corresponding to the weights and biases of the\n        AlexNet model.\n  """"""\n  parameters = []\n  # conv1\n  with tf.name_scope(\'conv1\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([11, 11, 3, 64], dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(images, kernel, [1, 4, 4, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(bias, name=scope)\n    print_activations(conv1)\n    parameters += [kernel, biases]\n\n  # lrn1\n  with tf.name_scope(\'lrn1\') as scope:\n    lrn1 = tf.nn.local_response_normalization(conv1,\n                                              alpha=1e-4,\n                                              beta=0.75,\n                                              depth_radius=2,\n                                              bias=2.0)\n\n  # pool1\n  pool1 = tf.nn.max_pool(lrn1,\n                         ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1],\n                         padding=\'VALID\',\n                         name=\'pool1\')\n  print_activations(pool1)\n\n  # conv2\n  with tf.name_scope(\'conv2\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([5, 5, 64, 192], dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[192], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(bias, name=scope)\n    parameters += [kernel, biases]\n  print_activations(conv2)\n\n  # lrn2\n  with tf.name_scope(\'lrn2\') as scope:\n    lrn2 = tf.nn.local_response_normalization(conv2,\n                                              alpha=1e-4,\n                                              beta=0.75,\n                                              depth_radius=2,\n                                              bias=2.0)\n\n  # pool2\n  pool2 = tf.nn.max_pool(lrn2,\n                         ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1],\n                         padding=\'VALID\',\n                         name=\'pool2\')\n  print_activations(pool2)\n\n  # conv3\n  with tf.name_scope(\'conv3\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 192, 384],\n                                             dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv3 = tf.nn.relu(bias, name=scope)\n    parameters += [kernel, biases]\n    print_activations(conv3)\n\n  # conv4\n  with tf.name_scope(\'conv4\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n                                             dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv4 = tf.nn.relu(bias, name=scope)\n    parameters += [kernel, biases]\n    print_activations(conv4)\n\n  # conv5\n  with tf.name_scope(\'conv5\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256],\n                                             dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv5 = tf.nn.relu(bias, name=scope)\n    parameters += [kernel, biases]\n    print_activations(conv5)\n\n  # pool5\n  pool5 = tf.nn.max_pool(conv5,\n                         ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1],\n                         padding=\'VALID\',\n                         name=\'pool5\')\n  print_activations(pool5)\n\n  return pool5, parameters\n\n\ndef time_tensorflow_run(session, target, info_string):\n  """"""Run the computation to obtain the target tensor and print timing stats.\n\n  Args:\n    session: the TensorFlow session to run the computation under.\n    target: the target Tensor that is passed to the session\'s run() function.\n    info_string: a string summarizing this run, to be printed with the stats.\n\n  Returns:\n    None\n  """"""\n  num_steps_burn_in = 10\n  total_duration = 0.0\n  total_duration_squared = 0.0\n  for i in xrange(FLAGS.num_batches + num_steps_burn_in):\n    start_time = time.time()\n    _ = session.run(target)\n    duration = time.time() - start_time\n    if i >= num_steps_burn_in:\n      if not i % 10:\n        print (\'%s: step %d, duration = %.3f\' %\n               (datetime.now(), i - num_steps_burn_in, duration))\n      total_duration += duration\n      total_duration_squared += duration * duration\n  mn = total_duration / FLAGS.num_batches\n  vr = total_duration_squared / FLAGS.num_batches - mn * mn\n  sd = math.sqrt(vr)\n  print (\'%s: %s across %d steps, %.3f +/- %.3f sec / batch\' %\n         (datetime.now(), info_string, FLAGS.num_batches, mn, sd))\n\n\n\ndef run_benchmark():\n  """"""Run the benchmark on AlexNet.""""""\n  with tf.Graph().as_default():\n    # Generate some dummy images.\n    image_size = 224\n    # Note that our padding definition is slightly different the cuda-convnet.\n    # In order to force the model to start with the same activations sizes,\n    # we add 3 to the image_size and employ VALID padding above.\n    images = tf.Variable(tf.random_normal([FLAGS.batch_size,\n                                           image_size,\n                                           image_size, 3],\n                                          dtype=tf.float32,\n                                          stddev=1e-1))\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    pool5, parameters = inference(images)\n\n    # Build an initialization operation.\n    init = tf.global_variables_initializer()\n\n    # Start running operations on the Graph.\n    config = tf.ConfigProto()\n    config.gpu_options.allocator_type = \'BFC\'\n    sess = tf.Session(config=config)\n    sess.run(init)\n\n    # Run the forward benchmark.\n    time_tensorflow_run(sess, pool5, ""Forward"")\n\n    # Add a simple objective so we can calculate the backward pass.\n    objective = tf.nn.l2_loss(pool5)\n    # Compute the gradient with respect to all the parameters.\n    grad = tf.gradients(objective, parameters)\n    # Run the backward benchmark.\n    time_tensorflow_run(sess, grad, ""Forward-backward"")\n\n\ndef main(_):\n  run_benchmark()\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--batch_size\',\n      type=int,\n      default=128,\n      help=\'Batch size.\'\n  )\n  parser.add_argument(\n      \'--num_batches\',\n      type=int,\n      default=100,\n      help=\'Number of batches to run.\'\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
community/en/r1/tutorials/image/cifar10/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Makes helper libraries available in the cifar10 package.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cifar10\nimport cifar10_input\n'"
community/en/r1/tutorials/image/cifar10/cifar10.py,63,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builds the CIFAR-10 network.\n\nSummary of available functions:\n\n # Compute input images and labels for training. If you would like to run\n # evaluations, use inputs() instead.\n inputs, labels = distorted_inputs()\n\n # Compute inference on the model inputs to make a prediction.\n predictions = inference(inputs)\n\n # Compute the total loss of the prediction with respect to the labels.\n loss = loss(predictions, labels)\n\n # Create a graph to run one step of training with respect to the loss.\n train_op = train(loss, global_step)\n""""""\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport tensorflow as tf\n\nimport cifar10_input\n\nFLAGS = tf.app.flags.FLAGS\n\n# Basic model parameters.\ntf.app.flags.DEFINE_integer(\'batch_size\', 128,\n                            """"""Number of images to process in a batch."""""")\ntf.app.flags.DEFINE_boolean(\'use_fp16\', True,\n                            """"""Train the model using fp16."""""")\n\n# Global constants describing the CIFAR-10 data set.\nIMAGE_SIZE = cifar10_input.IMAGE_SIZE\nNUM_CLASSES = cifar10_input.NUM_CLASSES\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n\n\n# Constants describing the training process.\nMOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\nNUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\nLEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\nINITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n\n# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n# to differentiate the operations. Note that this prefix is removed from the\n# names of the summaries when visualizing a model.\nTOWER_NAME = \'tower\'\n\n\ndef _activation_summary(x):\n  """"""Helper to create summaries for activations.\n\n  Creates a summary that provides a histogram of activations.\n  Creates a summary that measures the sparsity of activations.\n\n  Args:\n    x: Tensor\n  Returns:\n    nothing\n  """"""\n  # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n  # session. This helps the clarity of presentation on tensorboard.\n  tensor_name = re.sub(\'%s_[0-9]*/\' % TOWER_NAME, \'\', x.op.name)\n  tf.summary.histogram(tensor_name + \'/activations\', x)\n  tf.summary.scalar(tensor_name + \'/sparsity\', tf.nn.zero_fraction(x))\n\n\ndef _variable_on_cpu(name, shape, initializer):\n  """"""Helper to create a Variable stored on CPU memory.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n\n  Returns:\n    Variable Tensor\n  """"""\n  with tf.device(\'/cpu:0\'):\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var\n\n\ndef _variable_with_weight_decay(name, shape, stddev, wd):\n  """"""Helper to create an initialized Variable with weight decay.\n\n  Note that the Variable is initialized with a truncated normal distribution.\n  A weight decay is added only if one is specified.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    stddev: standard deviation of a truncated Gaussian\n    wd: add L2Loss weight decay multiplied by this float. If None, weight\n        decay is not added for this Variable.\n\n  Returns:\n    Variable Tensor\n  """"""\n  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n  var = _variable_on_cpu(\n      name,\n      shape,\n      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n  if wd is not None:\n    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n    tf.add_to_collection(\'losses\', weight_decay)\n  return var\n\n\ndef distorted_inputs():\n  """"""Construct distorted input for CIFAR training using the Reader ops.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  """"""\n  images, labels = cifar10_input.distorted_inputs(batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels\n\n\ndef inputs(eval_data):\n  """"""Construct input for CIFAR evaluation using the Reader ops.\n  Args:\n    eval_data: bool, indicating if one should use the train or eval data set.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  """"""\n  images, labels = cifar10_input.inputs(eval_data=eval_data, batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels\n\n\ndef inference(images):\n  """"""Build the CIFAR-10 model.\n\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n\n  Returns:\n    Logits.\n  """"""\n  # We instantiate all variables using tf.get_variable() instead of\n  # tf.Variable() in order to share variables across multiple GPU training runs.\n  # If we only ran this model on a single GPU, we could simplify this function\n  # by replacing all instances of tf.get_variable() with tf.Variable().\n  #\n  # conv1\n  with tf.variable_scope(\'conv1\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[5, 5, 3, 64],\n                                         stddev=5e-2,\n                                         wd=None)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [64], tf.constant_initializer(0.0))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv1)\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding=\'SAME\', name=\'pool1\')\n  # norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name=\'norm1\')\n\n  # conv2\n  with tf.variable_scope(\'conv2\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[5, 5, 64, 64],\n                                         stddev=5e-2,\n                                         wd=None)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [64], tf.constant_initializer(0.1))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv2)\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name=\'norm2\')\n  # pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool2\')\n\n  # local3\n  with tf.variable_scope(\'local3\') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.keras.layers.Flatten()(pool2)\n    dim = reshape.get_shape()[1].value\n    weights = _variable_with_weight_decay(\'weights\', shape=[dim, 384],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu(\'biases\', [384], tf.constant_initializer(0.1))\n    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n    _activation_summary(local3)\n\n  # local4\n  with tf.variable_scope(\'local4\') as scope:\n    weights = _variable_with_weight_decay(\'weights\', shape=[384, 192],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.1))\n    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n    _activation_summary(local4)\n\n  # linear layer(WX + b),\n  # We don\'t apply softmax here because\n  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n  # and performs the softmax internally for efficiency.\n  with tf.variable_scope(\'softmax_linear\') as scope:\n    weights = _variable_with_weight_decay(\'weights\', [192, NUM_CLASSES],\n                                          stddev=1/192.0, wd=None)\n    biases = _variable_on_cpu(\'biases\', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n    _activation_summary(softmax_linear)\n\n  return softmax_linear\n\n\ndef loss(logits, labels):\n  """"""Add L2Loss to all the trainable variables.\n\n  Add summary for ""Loss"" and ""Loss/avg"".\n  Args:\n    logits: Logits from inference().\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n\n  Returns:\n    Loss tensor of type float.\n  """"""\n  # Calculate the average cross entropy loss across the batch.\n  labels = tf.cast(labels, tf.int64)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=labels, logits=logits, name=\'cross_entropy_per_example\')\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n  tf.add_to_collection(\'losses\', cross_entropy_mean)\n\n  # The total loss is defined as the cross entropy loss plus all of the weight\n  # decay terms (L2 loss).\n  return tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\')\n\n\ndef _add_loss_summaries(total_loss):\n  """"""Add summaries for losses in CIFAR-10 model.\n\n  Generates moving average for all losses and associated summaries for\n  visualizing the performance of the network.\n\n  Args:\n    total_loss: Total loss from loss().\n  Returns:\n    loss_averages_op: op for generating moving averages of losses.\n  """"""\n  # Compute the moving average of all individual losses and the total loss.\n  loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n  losses = tf.get_collection(\'losses\')\n  loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n  # Attach a scalar summary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Name each loss as \'(raw)\' and name the moving average version of the loss\n    # as the original loss name.\n    tf.summary.scalar(l.op.name + \' (raw)\', l)\n    tf.summary.scalar(l.op.name, loss_averages.average(l))\n\n  return loss_averages_op\n\n\ndef train(total_loss, global_step):\n  """"""Train CIFAR-10 model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n  Returns:\n    train_op: op for training.\n  """"""\n  # Variables that affect learning rate.\n  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n\n  # Decay the learning rate exponentially based on the number of steps.\n  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n                                  global_step,\n                                  decay_steps,\n                                  LEARNING_RATE_DECAY_FACTOR,\n                                  staircase=True)\n  tf.summary.scalar(\'learning_rate\', lr)\n\n  # Generate moving averages of all losses and associated summaries.\n  loss_averages_op = _add_loss_summaries(total_loss)\n\n  # Compute gradients.\n  with tf.control_dependencies([loss_averages_op]):\n    opt = tf.train.GradientDescentOptimizer(lr)\n    grads = opt.compute_gradients(total_loss)\n\n  # Apply gradients.\n  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n  # Add histograms for trainable variables.\n  for var in tf.trainable_variables():\n    tf.summary.histogram(var.op.name, var)\n\n  # Add histograms for gradients.\n  for grad, var in grads:\n    if grad is not None:\n      tf.summary.histogram(var.op.name + \'/gradients\', grad)\n\n  # Track the moving averages of all trainable variables.\n  variable_averages = tf.train.ExponentialMovingAverage(\n      MOVING_AVERAGE_DECAY, global_step)\n  with tf.control_dependencies([apply_gradient_op]):\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n  return variables_averages_op\n'"
community/en/r1/tutorials/image/cifar10/cifar10_eval.py,24,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Evaluation for CIFAR-10.\n\nAccuracy:\ncifar10_train.py achieves 83.0% accuracy after 100K steps (256 epochs\nof data) as judged by cifar10_eval.py.\n\nSpeed:\nOn a single Tesla K40, cifar10_train.py processes a single batch of 128 images\nin 0.25-0.35 sec (i.e. 350 - 600 images /sec). The model reaches ~86%\naccuracy after 100K steps in 8 hours of training time.\n\nUsage:\nPlease see the tutorial and website for how to download the CIFAR-10\ndata set, compile the program and train the model.\n\nhttp://tensorflow.org/tutorials/deep_cnn/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport math\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport cifar10\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'eval_dir\', \'/tmp/cifar10_eval\',\n                           """"""Directory where to write event logs."""""")\ntf.app.flags.DEFINE_string(\'eval_data\', \'test\',\n                           """"""Either \'test\' or \'train_eval\'."""""")\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'/tmp/cifar10_train\',\n                           """"""Directory where to read model checkpoints."""""")\ntf.app.flags.DEFINE_integer(\'eval_interval_secs\', 5,\n                            """"""How often to run the eval."""""")\ntf.app.flags.DEFINE_integer(\'num_examples\', 1000,\n                            """"""Number of examples to run."""""")\ntf.app.flags.DEFINE_boolean(\'run_once\', False,\n                            """"""Whether to run eval only once."""""")\n\n\ndef eval_once(saver, summary_writer, top_k_op, summary_op):\n  """"""Run Eval once.\n\n  Args:\n    saver: Saver.\n    summary_writer: Summary writer.\n    top_k_op: Top K op.\n    summary_op: Summary op.\n  """"""\n  with tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      # Restores from checkpoint\n      saver.restore(sess, ckpt.model_checkpoint_path)\n      # Assuming model_checkpoint_path looks something like:\n      #   /my-favorite-path/cifar10_train/model.ckpt-0,\n      # extract global_step from it.\n      global_step = ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1]\n    else:\n      print(\'No checkpoint file found\')\n      return\n\n    # Start the queue runners.\n    coord = tf.train.Coordinator()\n    try:\n      threads = []\n      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n                                         start=True))\n\n      num_iter = int(math.ceil(float(FLAGS.num_examples) / FLAGS.batch_size))\n      true_count = 0  # Counts the number of correct predictions.\n      total_sample_count = num_iter * FLAGS.batch_size\n      step = 0\n      while step < num_iter and not coord.should_stop():\n        predictions = sess.run([top_k_op])\n        true_count += np.sum(predictions)\n        step += 1\n\n      # Compute precision @ 1.\n      precision = true_count / total_sample_count\n      print(\'%s: precision @ 1 = %.3f\' % (datetime.now(), precision))\n\n      summary = tf.Summary()\n      summary.ParseFromString(sess.run(summary_op))\n      summary.value.add(tag=\'Precision @ 1\', simple_value=precision)\n      summary_writer.add_summary(summary, global_step)\n    except Exception as e:  # pylint: disable=broad-except\n      coord.request_stop(e)\n\n    coord.request_stop()\n    coord.join(threads, stop_grace_period_secs=10)\n\n\ndef evaluate():\n  """"""Eval CIFAR-10 for a number of steps.""""""\n  with tf.Graph().as_default() as g:\n    # Get images and labels for CIFAR-10.\n    images, labels = cifar10.inputs(eval_data=FLAGS.eval_data)\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    logits = cifar10.inference(images)\n\n    logits = tf.cast(logits, ""float32"")\n    labels = tf.cast(labels, ""int32"")\n\n    # Calculate predictions.\n    top_k_op = tf.nn.in_top_k(logits, labels, 1)\n\n    # Restore the moving average version of the learned variables for eval.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        cifar10.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # Build the summary operation based on the TF collection of Summaries.\n    summary_op = tf.summary.merge_all()\n\n    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)\n\n    while True:\n      eval_once(saver, summary_writer, top_k_op, summary_op)\n      if FLAGS.run_once:\n        break\n      time.sleep(FLAGS.eval_interval_secs)\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  if tf.gfile.Exists(FLAGS.eval_dir):\n    tf.gfile.DeleteRecursively(FLAGS.eval_dir)\n  tf.gfile.MakeDirs(FLAGS.eval_dir)\n  evaluate()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
community/en/r1/tutorials/image/cifar10/cifar10_input.py,9,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Routine for decoding the CIFAR-10 binary file format.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Process images of this size. Note that this differs from the original CIFAR\n# image size of 32 x 32. If one alters this number, then the entire model\n# architecture will change and any model would need to be retrained.\nIMAGE_SIZE = 24\n\n# Global constants describing the CIFAR-10 data set.\nNUM_CLASSES = 10\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n\n\ndef _get_images_labels(batch_size, split, distords=False):\n  """"""Returns Dataset for given split.""""""\n  dataset = tfds.load(name=\'cifar10\', split=split)\n  scope = \'data_augmentation\' if distords else \'input\'\n  with tf.name_scope(scope):\n    dataset = dataset.map(DataPreprocessor(distords), num_parallel_calls=10)\n  # Dataset is small enough to be fully loaded on memory:\n  dataset = dataset.prefetch(-1)\n  dataset = dataset.repeat().batch(batch_size)\n  iterator = dataset.make_one_shot_iterator()\n  images_labels = iterator.get_next()\n  images, labels = images_labels[\'input\'], images_labels[\'target\']\n  tf.summary.image(\'images\', images)\n  return images, labels\n\n\nclass DataPreprocessor(object):\n  """"""Applies transformations to dataset record.""""""\n\n  def __init__(self, distords):\n    self._distords = distords\n\n  def __call__(self, record):\n    """"""Process img for training or eval.""""""\n    img = record[\'image\']\n    img = tf.cast(img, tf.float32)\n    if self._distords:  # training\n      # Randomly crop a [height, width] section of the image.\n      img = tf.random_crop(img, [IMAGE_SIZE, IMAGE_SIZE, 3])\n      # Randomly flip the image horizontally.\n      img = tf.image.random_flip_left_right(img)\n      # Because these operations are not commutative, consider randomizing\n      # the order their operation.\n      # NOTE: since per_image_standardization zeros the mean and makes\n      # the stddev unit, this likely has no effect see tensorflow#1458.\n      img = tf.image.random_brightness(img, max_delta=63)\n      img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n    else:  # Image processing for evaluation.\n      # Crop the central [height, width] of the image.\n      img = tf.image.resize_image_with_crop_or_pad(img, IMAGE_SIZE, IMAGE_SIZE)\n    # Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    return dict(input=img, target=record[\'label\'])\n\n\ndef distorted_inputs(batch_size):\n  """"""Construct distorted input for CIFAR training using the Reader ops.\n\n  Args:\n    batch_size: Number of images per batch.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  """"""\n  return _get_images_labels(batch_size, tfds.Split.TRAIN, distords=True)\n\n\ndef inputs(eval_data, batch_size):\n  """"""Construct input for CIFAR evaluation using the Reader ops.\n\n  Args:\n    eval_data: bool, indicating if one should use the train or eval data set.\n    batch_size: Number of images per batch.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  """"""\n  split = tfds.Split.TEST if eval_data == \'test\' else tfds.Split.TRAIN\n  return _get_images_labels(batch_size, split)\n'"
community/en/r1/tutorials/image/cifar10/cifar10_input_test.py,5,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for cifar10 input.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nimport cifar10_input\n\n\nclass CIFAR10InputTest(tf.test.TestCase):\n\n  def _record(self, label, red, green, blue):\n    image_size = 32 * 32\n    record = bytes(bytearray([label] + [red] * image_size +\n                             [green] * image_size + [blue] * image_size))\n    expected = [[[red, green, blue]] * 32] * 32\n    return record, expected\n\n  def testSimple(self):\n    labels = [9, 3, 0]\n    records = [self._record(labels[0], 0, 128, 255),\n               self._record(labels[1], 255, 0, 1),\n               self._record(labels[2], 254, 255, 0)]\n    contents = b"""".join([record for record, _ in records])\n    expected = [expected for _, expected in records]\n    filename = os.path.join(self.get_temp_dir(), ""cifar"")\n    open(filename, ""wb"").write(contents)\n\n    with self.test_session() as sess:\n      q = tf.FIFOQueue(99, [tf.string], shapes=())\n      q.enqueue([filename]).run()\n      q.close().run()\n      result = cifar10_input.read_cifar10(q)\n\n      for i in range(3):\n        key, label, uint8image = sess.run([\n            result.key, result.label, result.uint8image])\n        self.assertEqual(""%s:%d"" % (filename, i), tf.compat.as_text(key))\n        self.assertEqual(labels[i], label)\n        self.assertAllEqual(expected[i], uint8image)\n\n      with self.assertRaises(tf.errors.OutOfRangeError):\n        sess.run([result.key, result.uint8image])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
community/en/r1/tutorials/image/cifar10/cifar10_multi_gpu_train.py,41,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A binary to train CIFAR-10 using multiple GPUs with synchronous updates.\n\nAccuracy:\ncifar10_multi_gpu_train.py achieves ~86% accuracy after 100K steps (256\nepochs of data) as judged by cifar10_eval.py.\n\nSpeed: With batch_size 128.\n\nSystem        | Step Time (sec/batch)  |     Accuracy\n--------------------------------------------------------------------\n1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n2 Tesla K20m  | 0.13-0.20              | ~84% at 30K steps  (2.5 hours)\n3 Tesla K20m  | 0.13-0.18              | ~84% at 30K steps\n4 Tesla K20m  | ~0.10                  | ~84% at 30K steps\n\nUsage:\nPlease see the tutorial and website for how to download the CIFAR-10\ndata set, compile the program and train the model.\n\nhttp://tensorflow.org/tutorials/deep_cnn/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\nimport re\nimport time\nfrom datetime import datetime\n\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nimport cifar10\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'train_dir\', \'/tmp/cifar10_train\',\n                           """"""Directory where to write event logs """"""\n                           """"""and checkpoint."""""")\ntf.app.flags.DEFINE_integer(\'max_steps\', 1000000,\n                            """"""Number of batches to run."""""")\ntf.app.flags.DEFINE_integer(\'num_gpus\', 1,\n                            """"""How many GPUs to use."""""")\ntf.app.flags.DEFINE_boolean(\'log_device_placement\', False,\n                            """"""Whether to log device placement."""""")\n\n\ndef tower_loss(scope, images, labels):\n  """"""Calculate the total loss on a single tower running the CIFAR model.\n\n  Args:\n    scope: unique prefix string identifying the CIFAR tower, e.g. \'tower_0\'\n    images: Images. 4D tensor of shape [batch_size, height, width, 3].\n    labels: Labels. 1D tensor of shape [batch_size].\n\n  Returns:\n     Tensor of shape [] containing the total loss for a batch of data\n  """"""\n\n  # Build inference Graph.\n  logits = cifar10.inference(images)\n\n  # Build the portion of the Graph calculating the losses. Note that we will\n  # assemble the total_loss using a custom function below.\n  _ = cifar10.loss(logits, labels)\n\n  # Assemble all of the losses for the current tower only.\n  losses = tf.get_collection(\'losses\', scope)\n\n  # Calculate the total loss for the current tower.\n  total_loss = tf.add_n(losses, name=\'total_loss\')\n\n  # Attach a scalar summary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on tensorboard.\n    loss_name = re.sub(\'%s_[0-9]*/\' % cifar10.TOWER_NAME, \'\', l.op.name)\n    tf.summary.scalar(loss_name, l)\n\n  return total_loss\n\n\ndef average_gradients(tower_grads):\n  """"""Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  """"""\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n      # Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n      # Append on a \'tower\' dimension which we will average over below.\n      grads.append(expanded_g)\n\n    # Average over the \'tower\' dimension.\n    grad = tf.concat(axis=0, values=grads)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower\'s pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\n\ndef train():\n  """"""Train CIFAR-10 for a number of steps.""""""\n  with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n    # Create a variable to count the number of train() calls. This equals the\n    # number of batches processed * FLAGS.num_gpus.\n    global_step = tf.get_variable(\n        \'global_step\', [],\n        initializer=tf.constant_initializer(0), trainable=False)\n\n    # Calculate the learning rate schedule.\n    num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /\n                             FLAGS.batch_size / FLAGS.num_gpus)\n    decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)\n\n    # Decay the learning rate exponentially based on the number of steps.\n    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,\n                                    global_step,\n                                    decay_steps,\n                                    cifar10.LEARNING_RATE_DECAY_FACTOR,\n                                    staircase=True)\n\n    # Create an optimizer that performs gradient descent.\n    opt = tf.train.GradientDescentOptimizer(lr)\n\n    # Get images and labels for CIFAR-10.\n    images, labels = cifar10.distorted_inputs()\n    images = tf.reshape(images, [cifar10.FLAGS.batch_size, 24, 24, 3])\n    labels = tf.reshape(labels, [cifar10.FLAGS.batch_size])\n    batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * FLAGS.num_gpus)\n    # Calculate the gradients for each model tower.\n    tower_grads = []\n    with tf.variable_scope(tf.get_variable_scope()):\n      for i in xrange(FLAGS.num_gpus):\n        with tf.device(\'/gpu:%d\' % i):\n          with tf.name_scope(\'%s_%d\' % (cifar10.TOWER_NAME, i)) as scope:\n            # Dequeues one batch for the GPU\n            image_batch, label_batch = batch_queue.dequeue()\n            # Calculate the loss for one tower of the CIFAR model. This function\n            # constructs the entire CIFAR model but shares the variables across\n            # all towers.\n            loss = tower_loss(scope, image_batch, label_batch)\n\n            # Reuse variables for the next tower.\n            tf.get_variable_scope().reuse_variables()\n\n            # Retain the summaries from the final tower.\n            summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n\n            # Calculate the gradients for the batch of data on this CIFAR tower.\n            grads = opt.compute_gradients(loss)\n\n            # Keep track of the gradients across all towers.\n            tower_grads.append(grads)\n\n    # We must calculate the mean of each gradient. Note that this is the\n    # synchronization point across all towers.\n    grads = average_gradients(tower_grads)\n\n    # Add a summary to track the learning rate.\n    summaries.append(tf.summary.scalar(\'learning_rate\', lr))\n\n    # Add histograms for gradients.\n    for grad, var in grads:\n      if grad is not None:\n        summaries.append(tf.summary.histogram(var.op.name + \'/gradients\', grad))\n\n    # Apply the gradients to adjust the shared variables.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    # Add histograms for trainable variables.\n    for var in tf.trainable_variables():\n      summaries.append(tf.summary.histogram(var.op.name, var))\n\n    # Track the moving averages of all trainable variables.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        cifar10.MOVING_AVERAGE_DECAY, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    # Group all updates to into a single train op.\n    train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.global_variables())\n\n    # Build the summary operation from the last tower summaries.\n    summary_op = tf.summary.merge(summaries)\n\n    # Build an initialization operation to run below.\n    init = tf.global_variables_initializer()\n\n    # Start running operations on the Graph. allow_soft_placement must be set to\n    # True to build towers on GPU, as some of the ops do not have GPU\n    # implementations.\n    sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=FLAGS.log_device_placement))\n    sess.run(init)\n\n    # Start the queue runners.\n    tf.train.start_queue_runners(sess=sess)\n\n    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n\n    for step in xrange(FLAGS.max_steps):\n      start_time = time.time()\n      _, loss_value = sess.run([train_op, loss])\n      duration = time.time() - start_time\n\n      assert not np.isnan(loss_value), \'Model diverged with loss = NaN\'\n\n      if step % 10 == 0:\n        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus\n        examples_per_sec = num_examples_per_step / duration\n        sec_per_batch = duration / FLAGS.num_gpus\n\n        format_str = (\'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f \'\n                      \'sec/batch)\')\n        print (format_str % (datetime.now(), step, loss_value,\n                             examples_per_sec, sec_per_batch))\n\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n\n      # Save the model checkpoint periodically.\n      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n        checkpoint_path = os.path.join(FLAGS.train_dir, \'model.ckpt\')\n        saver.save(sess, checkpoint_path, global_step=step)\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  train()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
community/en/r1/tutorials/image/cifar10/cifar10_train.py,18,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A binary to train CIFAR-10 using a single GPU.\n\nAccuracy:\ncifar10_train.py achieves ~86% accuracy after 100K steps (256 epochs of\ndata) as judged by cifar10_eval.py.\n\nSpeed: With batch_size 128.\n\nSystem        | Step Time (sec/batch)  |     Accuracy\n------------------------------------------------------------------\n1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n\nUsage:\nPlease see the tutorial and website for how to download the CIFAR-10\ndata set, compile the program and train the model.\n\nhttp://tensorflow.org/tutorials/deep_cnn/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport time\n\nimport tensorflow as tf\n\nimport cifar10\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'train_dir\', \'/tmp/cifar10_train\',\n                           """"""Directory where to write event logs """"""\n                           """"""and checkpoint."""""")\ntf.app.flags.DEFINE_integer(\'max_steps\', 100000,\n                            """"""Number of batches to run."""""")\ntf.app.flags.DEFINE_boolean(\'log_device_placement\', False,\n                            """"""Whether to log device placement."""""")\ntf.app.flags.DEFINE_integer(\'log_frequency\', 10,\n                            """"""How often to log results to the console."""""")\n\n\ndef train():\n  """"""Train CIFAR-10 for a number of steps.""""""\n  with tf.Graph().as_default():\n    global_step = tf.train.get_or_create_global_step()\n\n    # Get images and labels for CIFAR-10.\n    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n    # GPU and resulting in a slow down.\n    with tf.device(\'/cpu:0\'):\n      images, labels = cifar10.distorted_inputs()\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    logits = cifar10.inference(images)\n\n    # Calculate loss.\n    loss = cifar10.loss(logits, labels)\n\n    # Build a Graph that trains the model with one batch of examples and\n    # updates the model parameters.\n    train_op = cifar10.train(loss, global_step)\n\n    class _LoggerHook(tf.train.SessionRunHook):\n      """"""Logs loss and runtime.""""""\n\n      def begin(self):\n        self._step = -1\n        self._start_time = time.time()\n\n      def before_run(self, run_context):\n        self._step += 1\n        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n\n      def after_run(self, run_context, run_values):\n        if self._step % FLAGS.log_frequency == 0:\n          current_time = time.time()\n          duration = current_time - self._start_time\n          self._start_time = current_time\n\n          loss_value = run_values.results\n          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n          sec_per_batch = float(duration / FLAGS.log_frequency)\n\n          format_str = (\'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f \'\n                        \'sec/batch)\')\n          print (format_str % (datetime.now(), self._step, loss_value,\n                               examples_per_sec, sec_per_batch))\n\n    with tf.train.MonitoredTrainingSession(\n        checkpoint_dir=FLAGS.train_dir,\n        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n               tf.train.NanTensorHook(loss),\n               _LoggerHook()],\n        config=tf.ConfigProto(\n            log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n      while not mon_sess.should_stop():\n        mon_sess.run(train_op)\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  train()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
community/en/r1/tutorials/image/cifar10_estimator/__init__.py,0,b''
community/en/r1/tutorials/image/cifar10_estimator/cifar10.py,13,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""CIFAR-10 data set.\n\nSee http://www.cs.toronto.edu/~kriz/cifar.html.\n""""""\nimport os\n\nimport tensorflow as tf\n\nHEIGHT = 32\nWIDTH = 32\nDEPTH = 3\n\n\nclass Cifar10DataSet(object):\n  """"""Cifar10 data set.\n\n  Described by http://www.cs.toronto.edu/~kriz/cifar.html.\n  """"""\n\n  def __init__(self, data_dir, subset=\'train\', use_distortion=True):\n    self.data_dir = data_dir\n    self.subset = subset\n    self.use_distortion = use_distortion\n\n  def get_filenames(self):\n    if self.subset in [\'train\', \'validation\', \'eval\']:\n      return [os.path.join(self.data_dir, self.subset + \'.tfrecords\')]\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % self.subset)\n\n  def parser(self, serialized_example):\n    """"""Parses a single tf.Example into image and label tensors.""""""\n    # Dimensions of the images in the CIFAR-10 dataset.\n    # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n    # input format.\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            \'image\': tf.FixedLenFeature([], tf.string),\n            \'label\': tf.FixedLenFeature([], tf.int64),\n        })\n    image = tf.decode_raw(features[\'image\'], tf.uint8)\n    image.set_shape([DEPTH * HEIGHT * WIDTH])\n\n    # Reshape from [depth * height * width] to [depth, height, width].\n    image = tf.cast(\n        tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]),\n        tf.float32)\n    label = tf.cast(features[\'label\'], tf.int32)\n\n    # Custom preprocessing.\n    image = self.preprocess(image)\n\n    return image, label\n\n  def make_batch(self, batch_size):\n    """"""Read the images and labels from \'filenames\'.""""""\n    filenames = self.get_filenames()\n    # Repeat infinitely.\n    dataset = tf.data.TFRecordDataset(filenames).repeat()\n\n    # Parse records.\n    dataset = dataset.map(\n        self.parser, num_parallel_calls=batch_size)\n\n    # Potentially shuffle records.\n    if self.subset == \'train\':\n      min_queue_examples = int(\n          Cifar10DataSet.num_examples_per_epoch(self.subset) * 0.4)\n      # Ensure that the capacity is sufficiently large to provide good random\n      # shuffling.\n      dataset = dataset.shuffle(buffer_size=min_queue_examples + 3 * batch_size)\n\n    # Batch it up.\n    dataset = dataset.batch(batch_size)\n    iterator = dataset.make_one_shot_iterator()\n    image_batch, label_batch = iterator.get_next()\n\n    return image_batch, label_batch\n\n  def preprocess(self, image):\n    """"""Preprocess a single image in [height, width, depth] layout.""""""\n    if self.subset == \'train\' and self.use_distortion:\n      # Pad 4 pixels on each dimension of feature map, done in mini-batch\n      image = tf.image.resize_image_with_crop_or_pad(image, 40, 40)\n      image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH])\n      image = tf.image.random_flip_left_right(image)\n    return image\n\n  @staticmethod\n  def num_examples_per_epoch(subset=\'train\'):\n    if subset == \'train\':\n      return 45000\n    elif subset == \'validation\':\n      return 5000\n    elif subset == \'eval\':\n      return 10000\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n'"
community/en/r1/tutorials/image/cifar10_estimator/cifar10_main.py,48,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ResNet model for classifying images from CIFAR-10 dataset.\n\nSupport single-host training with one or multiple devices.\n\nResNet as proposed in:\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\nDeep Residual Learning for Image Recognition. arXiv:1512.03385\n\nCIFAR-10 as in:\nhttp://www.cs.toronto.edu/~kriz/cifar.html\n\n\n""""""\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport functools\nimport itertools\nimport os\n\nimport cifar10\nimport cifar10_model\nimport cifar10_utils\nimport numpy as np\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef get_model_fn(num_gpus, variable_strategy, num_workers):\n  """"""Returns a function that will build the resnet model.""""""\n\n  def _resnet_model_fn(features, labels, mode, params):\n    """"""Resnet model body.\n\n    Support single host, one or more GPU training. Parameter distribution can\n    be either one of the following scheme.\n    1. CPU is the parameter server and manages gradient updates.\n    2. Parameters are distributed evenly across all GPUs, and the first GPU\n       manages gradient updates.\n\n    Args:\n      features: a list of tensors, one for each tower\n      labels: a list of tensors, one for each tower\n      mode: ModeKeys.TRAIN or EVAL\n      params: Hyperparameters suitable for tuning\n    Returns:\n      A EstimatorSpec object.\n    """"""\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    weight_decay = params.weight_decay\n    momentum = params.momentum\n\n    tower_features = features\n    tower_labels = labels\n    tower_losses = []\n    tower_gradvars = []\n    tower_preds = []\n\n    # channels first (NCHW) is normally optimal on GPU and channels last (NHWC)\n    # on CPU. The exception is Intel MKL on CPU which is optimal with\n    # channels_last.\n    data_format = params.data_format\n    if not data_format:\n      if num_gpus == 0:\n        data_format = \'channels_last\'\n      else:\n        data_format = \'channels_first\'\n\n    if num_gpus == 0:\n      num_devices = 1\n      device_type = \'cpu\'\n    else:\n      num_devices = num_gpus\n      device_type = \'gpu\'\n\n    for i in range(num_devices):\n      worker_device = \'/{}:{}\'.format(device_type, i)\n      if variable_strategy == \'CPU\':\n        device_setter = cifar10_utils.local_device_setter(\n            worker_device=worker_device)\n      elif variable_strategy == \'GPU\':\n        device_setter = cifar10_utils.local_device_setter(\n            ps_device_type=\'gpu\',\n            worker_device=worker_device,\n            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(\n                num_gpus, tf.contrib.training.byte_size_load_fn))\n      with tf.variable_scope(\'resnet\', reuse=bool(i != 0)):\n        with tf.name_scope(\'tower_%d\' % i) as name_scope:\n          with tf.device(device_setter):\n            loss, gradvars, preds = _tower_fn(\n                is_training, weight_decay, tower_features[i], tower_labels[i],\n                data_format, params.num_layers, params.batch_norm_decay,\n                params.batch_norm_epsilon)\n            tower_losses.append(loss)\n            tower_gradvars.append(gradvars)\n            tower_preds.append(preds)\n            if i == 0:\n              # Only trigger batch_norm moving mean and variance update from\n              # the 1st tower. Ideally, we should grab the updates from all\n              # towers but these stats accumulate extremely fast so we can\n              # ignore the other stats from the other towers without\n              # significant detriment.\n              update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n                                             name_scope)\n\n    # Now compute global loss and gradients.\n    gradvars = []\n    with tf.name_scope(\'gradient_averaging\'):\n      all_grads = {}\n      for grad, var in itertools.chain(*tower_gradvars):\n        if grad is not None:\n          all_grads.setdefault(var, []).append(grad)\n      for var, grads in six.iteritems(all_grads):\n        # Average gradients on the same device as the variables\n        # to which they apply.\n        with tf.device(var.device):\n          if len(grads) == 1:\n            avg_grad = grads[0]\n          else:\n            avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))\n        gradvars.append((avg_grad, var))\n\n    # Device that runs the ops to apply global gradient updates.\n    consolidation_device = \'/gpu:0\' if variable_strategy == \'GPU\' else \'/cpu:0\'\n    with tf.device(consolidation_device):\n      # Suggested learning rate scheduling from\n      # https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/cifar10-resnet.py#L155\n      num_batches_per_epoch = cifar10.Cifar10DataSet.num_examples_per_epoch(\n          \'train\') // (params.train_batch_size * num_workers)\n      boundaries = [\n          num_batches_per_epoch * x\n          for x in np.array([82, 123, 300], dtype=np.int64)\n      ]\n      staged_lr = [params.learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n\n      learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),\n                                                  boundaries, staged_lr)\n\n      loss = tf.reduce_mean(tower_losses, name=\'loss\')\n\n      examples_sec_hook = cifar10_utils.ExamplesPerSecondHook(\n          params.train_batch_size, every_n_steps=10)\n\n      tensors_to_log = {\'learning_rate\': learning_rate, \'loss\': loss}\n\n      logging_hook = tf.train.LoggingTensorHook(\n          tensors=tensors_to_log, every_n_iter=100)\n\n      train_hooks = [logging_hook, examples_sec_hook]\n\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=momentum)\n\n      if params.sync:\n        optimizer = tf.train.SyncReplicasOptimizer(\n            optimizer, replicas_to_aggregate=num_workers)\n        sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)\n        train_hooks.append(sync_replicas_hook)\n\n      # Create single grouped train op\n      train_op = [\n          optimizer.apply_gradients(\n              gradvars, global_step=tf.train.get_global_step())\n      ]\n      train_op.extend(update_ops)\n      train_op = tf.group(*train_op)\n\n      predictions = {\n          \'classes\':\n              tf.concat([p[\'classes\'] for p in tower_preds], axis=0),\n          \'probabilities\':\n              tf.concat([p[\'probabilities\'] for p in tower_preds], axis=0)\n      }\n      stacked_labels = tf.concat(labels, axis=0)\n      metrics = {\n          \'accuracy\':\n              tf.metrics.accuracy(stacked_labels, predictions[\'classes\'])\n      }\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        training_hooks=train_hooks,\n        eval_metric_ops=metrics)\n\n  return _resnet_model_fn\n\n\ndef _tower_fn(is_training, weight_decay, feature, label, data_format,\n              num_layers, batch_norm_decay, batch_norm_epsilon):\n  """"""Build computation tower (Resnet).\n\n  Args:\n    is_training: true if is training graph.\n    weight_decay: weight regularization strength, a float.\n    feature: a Tensor.\n    label: a Tensor.\n    data_format: channels_last (NHWC) or channels_first (NCHW).\n    num_layers: number of layers, an int.\n    batch_norm_decay: decay for batch normalization, a float.\n    batch_norm_epsilon: epsilon for batch normalization, a float.\n\n  Returns:\n    A tuple with the loss for the tower, the gradients and parameters, and\n    predictions.\n\n  """"""\n  model = cifar10_model.ResNetCifar10(\n      num_layers,\n      batch_norm_decay=batch_norm_decay,\n      batch_norm_epsilon=batch_norm_epsilon,\n      is_training=is_training,\n      data_format=data_format)\n  logits = model.forward_pass(feature, input_data_format=\'channels_last\')\n  tower_pred = {\n      \'classes\': tf.argmax(input=logits, axis=1),\n      \'probabilities\': tf.nn.softmax(logits)\n  }\n\n  tower_loss = tf.losses.sparse_softmax_cross_entropy(\n      logits=logits, labels=label)\n  tower_loss = tf.reduce_mean(tower_loss)\n\n  model_params = tf.trainable_variables()\n  tower_loss += weight_decay * tf.add_n(\n      [tf.nn.l2_loss(v) for v in model_params])\n\n  tower_grad = tf.gradients(tower_loss, model_params)\n\n  return tower_loss, zip(tower_grad, model_params), tower_pred\n\n\ndef input_fn(data_dir,\n             subset,\n             num_shards,\n             batch_size,\n             use_distortion_for_training=True):\n  """"""Create input graph for model.\n\n  Args:\n    data_dir: Directory where TFRecords representing the dataset are located.\n    subset: one of \'train\', \'validate\' and \'eval\'.\n    num_shards: num of towers participating in data-parallel training.\n    batch_size: total batch size for training to be divided by the number of\n    shards.\n    use_distortion_for_training: True to use distortions.\n  Returns:\n    two lists of tensors for features and labels, each of num_shards length.\n  """"""\n  with tf.device(\'/cpu:0\'):\n    use_distortion = subset == \'train\' and use_distortion_for_training\n    dataset = cifar10.Cifar10DataSet(data_dir, subset, use_distortion)\n    image_batch, label_batch = dataset.make_batch(batch_size)\n    if num_shards <= 1:\n      # No GPU available or only 1 GPU.\n      return [image_batch], [label_batch]\n\n    # Note that passing num=batch_size is safe here, even though\n    # dataset.batch(batch_size) can, in some cases, return fewer than batch_size\n    # examples. This is because it does so only when repeating for a limited\n    # number of epochs, but our dataset repeats forever.\n    image_batch = tf.unstack(image_batch, num=batch_size, axis=0)\n    label_batch = tf.unstack(label_batch, num=batch_size, axis=0)\n    feature_shards = [[] for i in range(num_shards)]\n    label_shards = [[] for i in range(num_shards)]\n    for i in xrange(batch_size):\n      idx = i % num_shards\n      feature_shards[idx].append(image_batch[i])\n      label_shards[idx].append(label_batch[i])\n    feature_shards = [tf.parallel_stack(x) for x in feature_shards]\n    label_shards = [tf.parallel_stack(x) for x in label_shards]\n    return feature_shards, label_shards\n\n\ndef get_experiment_fn(data_dir,\n                      num_gpus,\n                      variable_strategy,\n                      use_distortion_for_training=True):\n  """"""Returns an Experiment function.\n\n  Experiments perform training on several workers in parallel,\n  in other words experiments know how to invoke train and eval in a sensible\n  fashion for distributed training. Arguments passed directly to this\n  function are not tunable, all other arguments should be passed within\n  tf.HParams, passed to the enclosed function.\n\n  Args:\n      data_dir: str. Location of the data for input_fns.\n      num_gpus: int. Number of GPUs on each worker.\n      variable_strategy: String. CPU to use CPU as the parameter server\n      and GPU to use the GPUs as the parameter server.\n      use_distortion_for_training: bool. See cifar10.Cifar10DataSet.\n  Returns:\n      A function (tf.estimator.RunConfig, tf.contrib.training.HParams) ->\n      tf.contrib.learn.Experiment.\n\n      Suitable for use by tf.contrib.learn.learn_runner, which will run various\n      methods on Experiment (train, evaluate) based on information\n      about the current runner in `run_config`.\n  """"""\n\n  def _experiment_fn(run_config, hparams):\n    """"""Returns an Experiment.""""""\n    # Create estimator.\n    train_input_fn = functools.partial(\n        input_fn,\n        data_dir,\n        subset=\'train\',\n        num_shards=num_gpus,\n        batch_size=hparams.train_batch_size,\n        use_distortion_for_training=use_distortion_for_training)\n\n    eval_input_fn = functools.partial(\n        input_fn,\n        data_dir,\n        subset=\'eval\',\n        batch_size=hparams.eval_batch_size,\n        num_shards=num_gpus)\n\n    num_eval_examples = cifar10.Cifar10DataSet.num_examples_per_epoch(\'eval\')\n    if num_eval_examples % hparams.eval_batch_size != 0:\n      raise ValueError(\n          \'validation set size must be multiple of eval_batch_size\')\n\n    train_steps = hparams.train_steps\n    eval_steps = num_eval_examples // hparams.eval_batch_size\n \n    classifier = tf.estimator.Estimator(\n        model_fn=get_model_fn(num_gpus, variable_strategy,\n                              run_config.num_worker_replicas or 1),\n        config=run_config,\n        params=hparams)\n\n    # Create experiment.\n    return tf.contrib.learn.Experiment(\n        classifier,\n        train_input_fn=train_input_fn,\n        eval_input_fn=eval_input_fn,\n        train_steps=train_steps,\n        eval_steps=eval_steps)\n\n  return _experiment_fn\n\n\ndef main(job_dir, data_dir, num_gpus, variable_strategy,\n         use_distortion_for_training, log_device_placement, num_intra_threads,\n         **hparams):\n  # The env variable is on deprecation path, default is set to off.\n  os.environ[\'TF_SYNC_ON_FINISH\'] = \'0\'\n  os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n  # Session configuration.\n  sess_config = tf.ConfigProto(\n      allow_soft_placement=True,\n      log_device_placement=log_device_placement,\n      intra_op_parallelism_threads=num_intra_threads,\n      gpu_options=tf.GPUOptions(force_gpu_compatible=True))\n\n  config = cifar10_utils.RunConfig(\n      session_config=sess_config, model_dir=job_dir)\n  tf.contrib.learn.learn_runner.run(\n      get_experiment_fn(data_dir, num_gpus, variable_strategy,\n                        use_distortion_for_training),\n      run_config=config,\n      hparams=tf.contrib.training.HParams(\n          is_chief=config.is_chief,\n          **hparams))\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--data-dir\',\n      type=str,\n      required=True,\n      help=\'The directory where the CIFAR-10 input data is stored.\')\n  parser.add_argument(\n      \'--job-dir\',\n      type=str,\n      required=True,\n      help=\'The directory where the model will be stored.\')\n  parser.add_argument(\n      \'--variable-strategy\',\n      choices=[\'CPU\', \'GPU\'],\n      type=str,\n      default=\'CPU\',\n      help=\'Where to locate variable operations\')\n  parser.add_argument(\n      \'--num-gpus\',\n      type=int,\n      default=1,\n      help=\'The number of gpus used. Uses only CPU if set to 0.\')\n  parser.add_argument(\n      \'--num-layers\',\n      type=int,\n      default=44,\n      help=\'The number of layers of the model.\')\n  parser.add_argument(\n      \'--train-steps\',\n      type=int,\n      default=80000,\n      help=\'The number of steps to use for training.\')\n  parser.add_argument(\n      \'--train-batch-size\',\n      type=int,\n      default=128,\n      help=\'Batch size for training.\')\n  parser.add_argument(\n      \'--eval-batch-size\',\n      type=int,\n      default=100,\n      help=\'Batch size for validation.\')\n  parser.add_argument(\n      \'--momentum\',\n      type=float,\n      default=0.9,\n      help=\'Momentum for MomentumOptimizer.\')\n  parser.add_argument(\n      \'--weight-decay\',\n      type=float,\n      default=2e-4,\n      help=\'Weight decay for convolutions.\')\n  parser.add_argument(\n      \'--learning-rate\',\n      type=float,\n      default=0.1,\n      help=""""""\\\n      This is the inital learning rate value. The learning rate will decrease\n      during training. For more details check the model_fn implementation in\n      this file.\\\n      """""")\n  parser.add_argument(\n      \'--use-distortion-for-training\',\n      type=bool,\n      default=True,\n      help=\'If doing image distortion for training.\')\n  parser.add_argument(\n      \'--sync\',\n      action=\'store_true\',\n      default=False,\n      help=""""""\\\n      If present when running in a distributed environment will run on sync mode.\\\n      """""")\n  parser.add_argument(\n      \'--num-intra-threads\',\n      type=int,\n      default=0,\n      help=""""""\\\n      Number of threads to use for intra-op parallelism. When training on CPU\n      set to 0 to have the system pick the appropriate number or alternatively\n      set it to the number of physical CPU cores.\\\n      """""")\n  parser.add_argument(\n      \'--num-inter-threads\',\n      type=int,\n      default=0,\n      help=""""""\\\n      Number of threads to use for inter-op parallelism. If set to 0, the\n      system will pick an appropriate number.\\\n      """""")\n  parser.add_argument(\n      \'--data-format\',\n      type=str,\n      default=None,\n      help=""""""\\\n      If not set, the data format best for the training device is used. \n      Allowed values: channels_first (NCHW) channels_last (NHWC).\\\n      """""")\n  parser.add_argument(\n      \'--log-device-placement\',\n      action=\'store_true\',\n      default=False,\n      help=\'Whether to log device placement.\')\n  parser.add_argument(\n      \'--batch-norm-decay\',\n      type=float,\n      default=0.997,\n      help=\'Decay for batch norm.\')\n  parser.add_argument(\n      \'--batch-norm-epsilon\',\n      type=float,\n      default=1e-5,\n      help=\'Epsilon for batch norm.\')\n  args = parser.parse_args()\n\n  if args.num_gpus > 0:\n    assert tf.test.is_gpu_available(), ""Requested GPUs but none found.""\n  if args.num_gpus < 0:\n    raise ValueError(\n        \'Invalid GPU count: \\""--num-gpus\\"" must be 0 or a positive integer.\')\n  if args.num_gpus == 0 and args.variable_strategy == \'GPU\':\n    raise ValueError(\'num-gpus=0, CPU must be used as parameter server. Set\'\n                     \'--variable-strategy=CPU.\')\n  if (args.num_layers - 2) % 6 != 0:\n    raise ValueError(\'Invalid --num-layers parameter.\')\n  if args.num_gpus != 0 and args.train_batch_size % args.num_gpus != 0:\n    raise ValueError(\'--train-batch-size must be multiple of --num-gpus.\')\n  if args.num_gpus != 0 and args.eval_batch_size % args.num_gpus != 0:\n    raise ValueError(\'--eval-batch-size must be multiple of --num-gpus.\')\n\n  main(**vars(args))\n'"
community/en/r1/tutorials/image/cifar10_estimator/cifar10_model.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model class for Cifar10 Dataset.""""""\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport model_base\n\n\nclass ResNetCifar10(model_base.ResNet):\n  """"""Cifar10 model with ResNetV1 and basic residual block.""""""\n\n  def __init__(self,\n               num_layers,\n               is_training,\n               batch_norm_decay,\n               batch_norm_epsilon,\n               data_format=\'channels_first\'):\n    super(ResNetCifar10, self).__init__(\n        is_training,\n        data_format,\n        batch_norm_decay,\n        batch_norm_epsilon\n    )\n    self.n = (num_layers - 2) // 6\n    # Add one in case label starts with 1. No impact if label starts with 0.\n    self.num_classes = 10 + 1\n    self.filters = [16, 16, 32, 64]\n    self.strides = [1, 2, 2]\n\n  def forward_pass(self, x, input_data_format=\'channels_last\'):\n    """"""Build the core model within the graph.""""""\n    if self._data_format != input_data_format:\n      if input_data_format == \'channels_last\':\n        # Computation requires channels_first.\n        x = tf.transpose(x, [0, 3, 1, 2])\n      else:\n        # Computation requires channels_last.\n        x = tf.transpose(x, [0, 2, 3, 1])\n\n    # Image standardization.\n    x = x / 128 - 1\n\n    x = self._conv(x, 3, 16, 1)\n    x = self._batch_norm(x)\n    x = self._relu(x)\n\n    # Use basic (non-bottleneck) block and ResNet V1 (post-activation).\n    res_func = self._residual_v1\n\n    # 3 stages of block stacking.\n    for i in range(3):\n      with tf.name_scope(\'stage\'):\n        for j in range(self.n):\n          if j == 0:\n            # First block in a stage, filters and strides may change.\n            x = res_func(x, 3, self.filters[i], self.filters[i + 1],\n                         self.strides[i])\n          else:\n            # Following blocks in a stage, constant filters and unit stride.\n            x = res_func(x, 3, self.filters[i + 1], self.filters[i + 1], 1)\n\n    x = self._global_avg_pool(x)\n    x = self._fully_connected(x, self.num_classes)\n\n    return x\n'"
community/en/r1/tutorials/image/cifar10_estimator/cifar10_utils.py,1,"b'import collections\nimport six\n\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.framework import device as pydev\nfrom tensorflow.python.training import basic_session_run_hooks\nfrom tensorflow.python.training import session_run_hook\nfrom tensorflow.python.training import training_util\nfrom tensorflow.python.training import device_setter\nfrom tensorflow.contrib.learn.python.learn import run_config\n\n\n# TODO(b/64848083) Remove once uid bug is fixed\nclass RunConfig(tf.contrib.learn.RunConfig): \n  def uid(self, whitelist=None):\n    """"""Generates a \'Unique Identifier\' based on all internal fields.\n    Caller should use the uid string to check `RunConfig` instance integrity\n    in one session use, but should not rely on the implementation details, which\n    is subject to change.\n    Args:\n      whitelist: A list of the string names of the properties uid should not\n        include. If `None`, defaults to `_DEFAULT_UID_WHITE_LIST`, which\n        includes most properties user allowes to change.\n    Returns:\n      A uid string.\n    """"""\n    if whitelist is None:\n      whitelist = run_config._DEFAULT_UID_WHITE_LIST\n\n    state = {k: v for k, v in self.__dict__.items() if not k.startswith(\'__\')}\n    # Pop out the keys in whitelist.\n    for k in whitelist:\n      state.pop(\'_\' + k, None)\n\n    ordered_state = collections.OrderedDict(\n        sorted(state.items(), key=lambda t: t[0]))\n    # For class instance without __repr__, some special cares are required.\n    # Otherwise, the object address will be used.\n    if \'_cluster_spec\' in ordered_state:\n      ordered_state[\'_cluster_spec\'] = collections.OrderedDict(\n         sorted(ordered_state[\'_cluster_spec\'].as_dict().items(),\n                key=lambda t: t[0])\n      )\n    return \', \'.join(\n        \'%s=%r\' % (k, v) for (k, v) in six.iteritems(ordered_state)) \n\n\nclass ExamplesPerSecondHook(session_run_hook.SessionRunHook):\n  """"""Hook to print out examples per second.\n\n    Total time is tracked and then divided by the total number of steps\n    to get the average step time and then batch_size is used to determine\n    the running average of examples per second. The examples per second for the\n    most recent interval is also logged.\n  """"""\n\n  def __init__(\n      self,\n      batch_size,\n      every_n_steps=100,\n      every_n_secs=None,):\n    """"""Initializer for ExamplesPerSecondHook.\n\n      Args:\n      batch_size: Total batch size used to calculate examples/second from\n      global time.\n      every_n_steps: Log stats every n steps.\n      every_n_secs: Log stats every n seconds.\n    """"""\n    if (every_n_steps is None) == (every_n_secs is None):\n      raise ValueError(\'exactly one of every_n_steps\'\n                       \' and every_n_secs should be provided.\')\n    self._timer = basic_session_run_hooks.SecondOrStepTimer(\n        every_steps=every_n_steps, every_secs=every_n_secs)\n\n    self._step_train_time = 0\n    self._total_steps = 0\n    self._batch_size = batch_size\n\n  def begin(self):\n    self._global_step_tensor = training_util.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          \'Global step should be created to use StepCounterHook.\')\n\n  def before_run(self, run_context):  # pylint: disable=unused-argument\n    return basic_session_run_hooks.SessionRunArgs(self._global_step_tensor)\n\n  def after_run(self, run_context, run_values):\n    _ = run_context\n\n    global_step = run_values.results\n    if self._timer.should_trigger_for_step(global_step):\n      elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(\n          global_step)\n      if elapsed_time is not None:\n        steps_per_sec = elapsed_steps / elapsed_time\n        self._step_train_time += elapsed_time\n        self._total_steps += elapsed_steps\n\n        average_examples_per_sec = self._batch_size * (\n            self._total_steps / self._step_train_time)\n        current_examples_per_sec = steps_per_sec * self._batch_size\n        # Average examples/sec followed by current examples/sec\n        logging.info(\'%s: %g (%g), step = %g\', \'Average examples/sec\',\n                     average_examples_per_sec, current_examples_per_sec,\n                     self._total_steps)\n\ndef local_device_setter(num_devices=1,\n                        ps_device_type=\'cpu\',\n                        worker_device=\'/cpu:0\',\n                        ps_ops=None,\n                        ps_strategy=None):\n  if ps_ops == None:\n    ps_ops = [\'Variable\', \'VariableV2\', \'VarHandleOp\']\n\n  if ps_strategy is None:\n    ps_strategy = device_setter._RoundRobinStrategy(num_devices)\n  if not six.callable(ps_strategy):\n    raise TypeError(""ps_strategy must be callable"")\n\n  def _local_device_chooser(op):\n    current_device = pydev.DeviceSpec.from_string(op.device or """")\n\n    node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def\n    if node_def.op in ps_ops:\n      ps_device_spec = pydev.DeviceSpec.from_string(\n          \'/{}:{}\'.format(ps_device_type, ps_strategy(op)))\n\n      ps_device_spec.merge_from(current_device)\n      return ps_device_spec.to_string()\n    else:\n      worker_device_spec = pydev.DeviceSpec.from_string(worker_device or """")\n      worker_device_spec.merge_from(current_device)\n      return worker_device_spec.to_string()\n  return _local_device_chooser\n'"
community/en/r1/tutorials/image/cifar10_estimator/generate_cifar10_tfrecords.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Read CIFAR-10 data from pickled numpy arrays and writes TFRecords.\n\nGenerates tf.train.Example protos and writes them to TFRecord files from the\npython version of the CIFAR-10 dataset downloaded from\nhttps://www.cs.toronto.edu/~kriz/cifar.html.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport tarfile\nfrom six.moves import cPickle as pickle\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nCIFAR_FILENAME = \'cifar-10-python.tar.gz\'\nCIFAR_DOWNLOAD_URL = \'https://www.cs.toronto.edu/~kriz/\' + CIFAR_FILENAME\nCIFAR_LOCAL_FOLDER = \'cifar-10-batches-py\'\n\n\ndef download_and_extract(data_dir):\n  # download CIFAR-10 if not already downloaded.\n  tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir,\n                                                CIFAR_DOWNLOAD_URL)\n  tarfile.open(os.path.join(data_dir, CIFAR_FILENAME),\n               \'r:gz\').extractall(data_dir)\n\n\ndef _int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _get_file_names():\n  """"""Returns the file names expected to exist in the input_dir.""""""\n  file_names = {}\n  file_names[\'train\'] = [\'data_batch_%d\' % i for i in xrange(1, 5)]\n  file_names[\'validation\'] = [\'data_batch_5\']\n  file_names[\'eval\'] = [\'test_batch\']\n  return file_names\n\n\ndef read_pickle_from_file(filename):\n  with tf.gfile.Open(filename, \'rb\') as f:\n    if sys.version_info >= (3, 0):\n      data_dict = pickle.load(f, encoding=\'bytes\')\n    else:\n      data_dict = pickle.load(f)\n  return data_dict\n\n\ndef convert_to_tfrecord(input_files, output_file):\n  """"""Converts a file to TFRecords.""""""\n  print(\'Generating %s\' % output_file)\n  with tf.python_io.TFRecordWriter(output_file) as record_writer:\n    for input_file in input_files:\n      data_dict = read_pickle_from_file(input_file)\n      data = data_dict[b\'data\']\n      labels = data_dict[b\'labels\']\n      num_entries_in_batch = len(labels)\n      for i in range(num_entries_in_batch):\n        example = tf.train.Example(features=tf.train.Features(\n            feature={\n                \'image\': _bytes_feature(data[i].tobytes()),\n                \'label\': _int64_feature(labels[i])\n            }))\n        record_writer.write(example.SerializeToString())\n\n\ndef main(data_dir):\n  print(\'Download from {} and extract.\'.format(CIFAR_DOWNLOAD_URL))\n  download_and_extract(data_dir)\n  file_names = _get_file_names()\n  input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)\n  for mode, files in file_names.items():\n    input_files = [os.path.join(input_dir, f) for f in files]\n    output_file = os.path.join(data_dir, mode + \'.tfrecords\')\n    try:\n      os.remove(output_file)\n    except OSError:\n      pass\n    # Convert to tf.train.Example and write the to TFRecords.\n    convert_to_tfrecord(input_files, output_file)\n  print(\'Done!\')\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--data-dir\',\n      type=str,\n      default=\'\',\n      help=\'Directory to download and extract CIFAR-10 to.\')\n\n  args = parser.parse_args()\n  main(args.data_dir)\n'"
community/en/r1/tutorials/image/cifar10_estimator/model_base.py,28,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ResNet model.\n\nRelated papers:\nhttps://arxiv.org/pdf/1603.05027v2.pdf\nhttps://arxiv.org/pdf/1512.03385v1.pdf\nhttps://arxiv.org/pdf/1605.07146v1.pdf\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass ResNet(object):\n  """"""ResNet model.""""""\n\n  def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):\n    """"""ResNet constructor.\n\n    Args:\n      is_training: if build training or inference model.\n      data_format: the data_format used during computation.\n                   one of \'channels_first\' or \'channels_last\'.\n    """"""\n    self._batch_norm_decay = batch_norm_decay\n    self._batch_norm_epsilon = batch_norm_epsilon\n    self._is_training = is_training\n    assert data_format in (\'channels_first\', \'channels_last\')\n    self._data_format = data_format\n\n  def forward_pass(self, x):\n    raise NotImplementedError(\n        \'forward_pass() is implemented in ResNet sub classes\')\n\n  def _residual_v1(self,\n                   x,\n                   kernel_size,\n                   in_filter,\n                   out_filter,\n                   stride,\n                   activate_before_residual=False):\n    """"""Residual unit with 2 sub layers, using Plan A for shortcut connection.""""""\n\n    del activate_before_residual\n    with tf.name_scope(\'residual_v1\') as name_scope:\n      orig_x = x\n\n      x = self._conv(x, kernel_size, out_filter, stride)\n      x = self._batch_norm(x)\n      x = self._relu(x)\n\n      x = self._conv(x, kernel_size, out_filter, 1)\n      x = self._batch_norm(x)\n\n      if in_filter != out_filter:\n        orig_x = self._avg_pool(orig_x, stride, stride)\n        pad = (out_filter - in_filter) // 2\n        if self._data_format == \'channels_first\':\n          orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n        else:\n          orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n\n      x = self._relu(tf.add(x, orig_x))\n\n      tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n      return x\n\n  def _residual_v2(self,\n                   x,\n                   in_filter,\n                   out_filter,\n                   stride,\n                   activate_before_residual=False):\n    """"""Residual unit with 2 sub layers with preactivation, plan A shortcut.""""""\n\n    with tf.name_scope(\'residual_v2\') as name_scope:\n      if activate_before_residual:\n        x = self._batch_norm(x)\n        x = self._relu(x)\n        orig_x = x\n      else:\n        orig_x = x\n        x = self._batch_norm(x)\n        x = self._relu(x)\n\n      x = self._conv(x, 3, out_filter, stride)\n\n      x = self._batch_norm(x)\n      x = self._relu(x)\n      x = self._conv(x, 3, out_filter, [1, 1, 1, 1])\n\n      if in_filter != out_filter:\n        pad = (out_filter - in_filter) // 2\n        orig_x = self._avg_pool(orig_x, stride, stride)\n        if self._data_format == \'channels_first\':\n          orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])\n        else:\n          orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])\n\n      x = tf.add(x, orig_x)\n\n      tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n      return x\n\n  def _bottleneck_residual_v2(self,\n                              x,\n                              in_filter,\n                              out_filter,\n                              stride,\n                              activate_before_residual=False):\n    """"""Bottleneck residual unit with 3 sub layers, plan B shortcut.""""""\n\n    with tf.name_scope(\'bottle_residual_v2\') as name_scope:\n      if activate_before_residual:\n        x = self._batch_norm(x)\n        x = self._relu(x)\n        orig_x = x\n      else:\n        orig_x = x\n        x = self._batch_norm(x)\n        x = self._relu(x)\n\n      x = self._conv(x, 1, out_filter // 4, stride, is_atrous=True)\n\n      x = self._batch_norm(x)\n      x = self._relu(x)\n      # pad when stride isn\'t unit\n      x = self._conv(x, 3, out_filter // 4, 1, is_atrous=True)\n\n      x = self._batch_norm(x)\n      x = self._relu(x)\n      x = self._conv(x, 1, out_filter, 1, is_atrous=True)\n\n      if in_filter != out_filter:\n        orig_x = self._conv(orig_x, 1, out_filter, stride, is_atrous=True)\n      x = tf.add(x, orig_x)\n\n      tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n      return x\n\n  def _conv(self, x, kernel_size, filters, strides, is_atrous=False):\n    """"""Convolution.""""""\n\n    padding = \'SAME\'\n    if not is_atrous and strides > 1:\n      pad = kernel_size - 1\n      pad_beg = pad // 2\n      pad_end = pad - pad_beg\n      if self._data_format == \'channels_first\':\n        x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n      else:\n        x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n      padding = \'VALID\'\n    return tf.layers.conv2d(\n        inputs=x,\n        kernel_size=kernel_size,\n        filters=filters,\n        strides=strides,\n        padding=padding,\n        use_bias=False,\n        data_format=self._data_format)\n\n  def _batch_norm(self, x):\n    if self._data_format == \'channels_first\':\n      data_format = \'NCHW\'\n    else:\n      data_format = \'NHWC\'\n    return tf.contrib.layers.batch_norm(\n        x,\n        decay=self._batch_norm_decay,\n        center=True,\n        scale=True,\n        epsilon=self._batch_norm_epsilon,\n        is_training=self._is_training,\n        fused=True,\n        data_format=data_format)\n\n  def _relu(self, x):\n    return tf.nn.relu(x)\n\n  def _fully_connected(self, x, out_dim):\n    with tf.name_scope(\'fully_connected\') as name_scope:\n      x = tf.layers.dense(x, out_dim)\n\n    tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n    return x\n\n  def _avg_pool(self, x, pool_size, stride):\n    with tf.name_scope(\'avg_pool\') as name_scope:\n      x = tf.layers.average_pooling2d(\n          x, pool_size, stride, \'SAME\', data_format=self._data_format)\n\n    tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n    return x\n\n  def _global_avg_pool(self, x):\n    with tf.name_scope(\'global_avg_pool\') as name_scope:\n      assert x.get_shape().ndims == 4\n      if self._data_format == \'channels_first\':\n        x = tf.reduce_mean(x, [2, 3])\n      else:\n        x = tf.reduce_mean(x, [1, 2])\n    tf.logging.info(\'image after unit %s: %s\', name_scope, x.get_shape())\n    return x\n'"
community/en/r1/tutorials/image/imagenet/classify_image.py,15,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Simple image classification with Inception.\n\nRun image classification with Inception trained on ImageNet 2012 Challenge data\nset.\n\nThis program creates a graph from a saved GraphDef protocol buffer,\nand runs inference on an input JPEG image. It outputs human readable\nstrings of the top 5 predictions along with their probabilities.\n\nChange the --image_file argument to any jpg image to compute a\nclassification of that image.\n\nPlease see the tutorial and website for a detailed description of how\nto use this script to perform image recognition.\n\nhttps://tensorflow.org/tutorials/image_recognition/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os.path\nimport re\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nFLAGS = None\n\n# pylint: disable=line-too-long\nDATA_URL = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\n# pylint: enable=line-too-long\n\n\nclass NodeLookup(object):\n  """"""Converts integer node ID\'s to human readable labels.""""""\n\n  def __init__(self,\n               label_lookup_path=None,\n               uid_lookup_path=None):\n    if not label_lookup_path:\n      label_lookup_path = os.path.join(\n          FLAGS.model_dir, \'imagenet_2012_challenge_label_map_proto.pbtxt\')\n    if not uid_lookup_path:\n      uid_lookup_path = os.path.join(\n          FLAGS.model_dir, \'imagenet_synset_to_human_label_map.txt\')\n    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n\n  def load(self, label_lookup_path, uid_lookup_path):\n    """"""Loads a human readable English name for each softmax node.\n\n    Args:\n      label_lookup_path: string UID to integer node ID.\n      uid_lookup_path: string UID to human-readable string.\n\n    Returns:\n      dict from integer node ID to human-readable string.\n    """"""\n    if not tf.gfile.Exists(uid_lookup_path):\n      tf.logging.fatal(\'File does not exist %s\', uid_lookup_path)\n    if not tf.gfile.Exists(label_lookup_path):\n      tf.logging.fatal(\'File does not exist %s\', label_lookup_path)\n\n    # Loads mapping from string UID to human-readable string\n    proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n    uid_to_human = {}\n    p = re.compile(r\'[n\\d]*[ \\S,]*\')\n    for line in proto_as_ascii_lines:\n      parsed_items = p.findall(line)\n      uid = parsed_items[0]\n      human_string = parsed_items[2]\n      uid_to_human[uid] = human_string\n\n    # Loads mapping from string UID to integer node ID.\n    node_id_to_uid = {}\n    proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n    for line in proto_as_ascii:\n      if line.startswith(\'  target_class:\'):\n        target_class = int(line.split(\': \')[1])\n      if line.startswith(\'  target_class_string:\'):\n        target_class_string = line.split(\': \')[1]\n        node_id_to_uid[target_class] = target_class_string[1:-2]\n\n    # Loads the final mapping of integer node ID to human-readable string\n    node_id_to_name = {}\n    for key, val in node_id_to_uid.items():\n      if val not in uid_to_human:\n        tf.logging.fatal(\'Failed to locate: %s\', val)\n      name = uid_to_human[val]\n      node_id_to_name[key] = name\n\n    return node_id_to_name\n\n  def id_to_string(self, node_id):\n    if node_id not in self.node_lookup:\n      return \'\'\n    return self.node_lookup[node_id]\n\n\ndef create_graph():\n  """"""Creates a graph from saved GraphDef file and returns a saver.""""""\n  # Creates graph from saved graph_def.pb.\n  with tf.gfile.FastGFile(os.path.join(\n      FLAGS.model_dir, \'classify_image_graph_def.pb\'), \'rb\') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    _ = tf.import_graph_def(graph_def, name=\'\')\n\n\ndef run_inference_on_image(image):\n  """"""Runs inference on an image.\n\n  Args:\n    image: Image file name.\n\n  Returns:\n    Nothing\n  """"""\n  if not tf.gfile.Exists(image):\n    tf.logging.fatal(\'File does not exist %s\', image)\n  image_data = tf.gfile.FastGFile(image, \'rb\').read()\n\n  # Creates graph from saved GraphDef.\n  create_graph()\n\n  with tf.Session() as sess:\n    # Some useful tensors:\n    # \'softmax:0\': A tensor containing the normalized prediction across\n    #   1000 labels.\n    # \'pool_3:0\': A tensor containing the next-to-last layer containing 2048\n    #   float description of the image.\n    # \'DecodeJpeg/contents:0\': A tensor containing a string providing JPEG\n    #   encoding of the image.\n    # Runs the softmax tensor by feeding the image_data as input to the graph.\n    softmax_tensor = sess.graph.get_tensor_by_name(\'softmax:0\')\n    predictions = sess.run(softmax_tensor,\n                           {\'DecodeJpeg/contents:0\': image_data})\n    predictions = np.squeeze(predictions)\n\n    # Creates node ID --> English string lookup.\n    node_lookup = NodeLookup()\n\n    top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]\n    for node_id in top_k:\n      human_string = node_lookup.id_to_string(node_id)\n      score = predictions[node_id]\n      print(\'%s (score = %.5f)\' % (human_string, score))\n\n\ndef maybe_download_and_extract():\n  """"""Download and extract model tar file.""""""\n  dest_directory = FLAGS.model_dir\n  if not os.path.exists(dest_directory):\n    os.makedirs(dest_directory)\n  filename = DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dest_directory, filename)\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dest_directory)\n\n\ndef main(_):\n  maybe_download_and_extract()\n  image = (FLAGS.image_file if FLAGS.image_file else\n           os.path.join(FLAGS.model_dir, \'cropped_panda.jpg\'))\n  run_inference_on_image(image)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  # classify_image_graph_def.pb:\n  #   Binary representation of the GraphDef protocol buffer.\n  # imagenet_synset_to_human_label_map.txt:\n  #   Map from synset ID to a human readable string.\n  # imagenet_2012_challenge_label_map_proto.pbtxt:\n  #   Text representation of a protocol buffer mapping a label to synset ID.\n  parser.add_argument(\n      \'--model_dir\',\n      type=str,\n      default=\'/tmp/imagenet\',\n      help=""""""\\\n      Path to classify_image_graph_def.pb,\n      imagenet_synset_to_human_label_map.txt, and\n      imagenet_2012_challenge_label_map_proto.pbtxt.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--image_file\',\n      type=str,\n      default=\'\',\n      help=\'Absolute path to image file.\'\n  )\n  parser.add_argument(\n      \'--num_top_predictions\',\n      type=int,\n      default=5,\n      help=\'Display this many predictions.\'\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
community/en/r1/tutorials/image/mnist/__init__.py,0,b''
community/en/r1/tutorials/image/mnist/convolutional.py,41,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\n\nThis should achieve a test error of 0.7%. Please keep this model as simple and\nlinear as possible, it is meant as a tutorial for simple convolutional models.\nRun with --self_test on the command line to execute a short self-test.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport gzip\nimport os\nimport sys\nimport time\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n# CVDF mirror of http://yann.lecun.com/exdb/mnist/\nSOURCE_URL = \'https://storage.googleapis.com/cvdf-datasets/mnist/\'\nWORK_DIRECTORY = \'data\'\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\n\n\nFLAGS = None\n\n\ndef data_type():\n  """"""Return the type of the activations, weights, and placeholder variables.""""""\n  if FLAGS.use_fp16:\n    return tf.float16\n  else:\n    return tf.float32\n\n\ndef maybe_download(filename):\n  """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n  if not tf.gfile.Exists(WORK_DIRECTORY):\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\n  filepath = os.path.join(WORK_DIRECTORY, filename)\n  if not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n      size = f.size()\n    print(\'Successfully downloaded\', filename, size, \'bytes.\')\n  return filepath\n\n\ndef extract_data(filename, num_images):\n  """"""Extract the images into a 4D tensor [image index, y, x, channels].\n\n  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n  """"""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n    return data\n\n\ndef extract_labels(filename, num_images):\n  """"""Extract the labels into a vector of int64 label IDs.""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_images)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n  return labels\n\n\ndef fake_data(num_images):\n  """"""Generate a fake dataset that matches the dimensions of MNIST.""""""\n  data = numpy.ndarray(\n      shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n      dtype=numpy.float32)\n  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\n  for image in xrange(num_images):\n    label = image % 2\n    data[image, :, :, 0] = label - 0.5\n    labels[image] = label\n  return data, labels\n\n\ndef error_rate(predictions, labels):\n  """"""Return the error rate based on dense predictions and sparse labels.""""""\n  return 100.0 - (\n      100.0 *\n      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n      predictions.shape[0])\n\n\ndef main(_):\n  if FLAGS.self_test:\n    print(\'Running self-test.\')\n    train_data, train_labels = fake_data(256)\n    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n    num_epochs = 1\n  else:\n    # Get the data.\n    train_data_filename = maybe_download(\'train-images-idx3-ubyte.gz\')\n    train_labels_filename = maybe_download(\'train-labels-idx1-ubyte.gz\')\n    test_data_filename = maybe_download(\'t10k-images-idx3-ubyte.gz\')\n    test_labels_filename = maybe_download(\'t10k-labels-idx1-ubyte.gz\')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, ...]\n    validation_labels = train_labels[:VALIDATION_SIZE]\n    train_data = train_data[VALIDATION_SIZE:, ...]\n    train_labels = train_labels[VALIDATION_SIZE:]\n    num_epochs = NUM_EPOCHS\n  train_size = train_labels.shape[0]\n\n  # This is where training samples and labels are fed to the graph.\n  # These placeholder nodes will be fed a batch of training data at each\n  # training step using the {feed_dict} argument to the Run() call below.\n  train_data_node = tf.placeholder(\n      data_type(),\n      shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n  train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n  eval_data = tf.placeholder(\n      data_type(),\n      shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n\n  # The variables below hold all the trainable weights. They are passed an\n  # initial value which will be assigned when we call:\n  # {tf.global_variables_initializer().run()}\n  conv1_weights = tf.Variable(\n      tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                          stddev=0.1,\n                          seed=SEED, dtype=data_type()))\n  conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n  conv2_weights = tf.Variable(tf.truncated_normal(\n      [5, 5, 32, 64], stddev=0.1,\n      seed=SEED, dtype=data_type()))\n  conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n  fc1_weights = tf.Variable(  # fully connected, depth 512.\n      tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n                          stddev=0.1,\n                          seed=SEED,\n                          dtype=data_type()))\n  fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n  fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n                                                stddev=0.1,\n                                                seed=SEED,\n                                                dtype=data_type()))\n  fc2_biases = tf.Variable(tf.constant(\n      0.1, shape=[NUM_LABELS], dtype=data_type()))\n\n  # We will replicate the model structure for the training subgraph, as well\n  # as the evaluation subgraphs, while sharing the trainable parameters.\n  def model(data, train=False):\n    """"""The Model definition.""""""\n    # 2D convolution, with \'SAME\' padding (i.e. the output feature map has\n    # the same size as the input). Note that {strides} is a 4D array whose\n    # shape matches the data layout: [image index, y, x, depth].\n    conv = tf.nn.conv2d(data,\n                        conv1_weights,\n                        strides=[1, 1, 1, 1],\n                        padding=\'SAME\')\n    # Bias and rectified linear non-linearity.\n    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n    # Max pooling. The kernel size spec {ksize} also follows the layout of\n    # the data. Here we have a pooling window of 2, and a stride of 2.\n    pool = tf.nn.max_pool(relu,\n                          ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1],\n                          padding=\'SAME\')\n    conv = tf.nn.conv2d(pool,\n                        conv2_weights,\n                        strides=[1, 1, 1, 1],\n                        padding=\'SAME\')\n    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n    pool = tf.nn.max_pool(relu,\n                          ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1],\n                          padding=\'SAME\')\n    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n    # fully connected layers.\n    pool_shape = pool.get_shape().as_list()\n    reshape = tf.reshape(\n        pool,\n        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n    # Fully connected layer. Note that the \'+\' operation automatically\n    # broadcasts the biases.\n    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n    # Add a 50% dropout during training only. Dropout also scales\n    # activations such that no rescaling is needed at evaluation time.\n    if train:\n      hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n    return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n  # Training computation: logits + cross-entropy loss.\n  logits = model(train_data_node, True)\n  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=train_labels_node, logits=logits))\n\n  # L2 regularization for the fully connected parameters.\n  regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n  # Add the regularization term to the loss.\n  loss += 5e-4 * regularizers\n\n  # Optimizer: set up a variable that\'s incremented once per batch and\n  # controls the learning rate decay.\n  batch = tf.Variable(0, dtype=data_type())\n  # Decay once per epoch, using an exponential schedule starting at 0.01.\n  learning_rate = tf.train.exponential_decay(\n      0.01,                # Base learning rate.\n      batch * BATCH_SIZE,  # Current index into the dataset.\n      train_size,          # Decay step.\n      0.95,                # Decay rate.\n      staircase=True)\n  # Use simple momentum for the optimization.\n  optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                         0.9).minimize(loss,\n                                                       global_step=batch)\n\n  # Predictions for the current training minibatch.\n  train_prediction = tf.nn.softmax(logits)\n\n  # Predictions for the test and validation, which we\'ll compute less often.\n  eval_prediction = tf.nn.softmax(model(eval_data))\n\n  # Small utility function to evaluate a dataset by feeding batches of data to\n  # {eval_data} and pulling the results from {eval_predictions}.\n  # Saves memory and enables this to run on smaller GPUs.\n  def eval_in_batches(data, sess):\n    """"""Get all predictions for a dataset by running it in small batches.""""""\n    size = data.shape[0]\n    if size < EVAL_BATCH_SIZE:\n      raise ValueError(""batch size for evals larger than dataset: %d"" % size)\n    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n      end = begin + EVAL_BATCH_SIZE\n      if end <= size:\n        predictions[begin:end, :] = sess.run(\n            eval_prediction,\n            feed_dict={eval_data: data[begin:end, ...]})\n      else:\n        batch_predictions = sess.run(\n            eval_prediction,\n            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n        predictions[begin:, :] = batch_predictions[begin - size:, :]\n    return predictions\n\n  # Create a local session to run the training.\n  start_time = time.time()\n  with tf.Session() as sess:\n    # Run all the initializers to prepare the trainable parameters.\n    tf.global_variables_initializer().run()\n    print(\'Initialized!\')\n    # Loop through training steps.\n    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n      # Compute the offset of the current minibatch in the data.\n      # Note that we could use better randomization across epochs.\n      offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n      batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n      batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n      # This dictionary maps the batch data (as a numpy array) to the\n      # node in the graph it should be fed to.\n      feed_dict = {train_data_node: batch_data,\n                   train_labels_node: batch_labels}\n      # Run the optimizer to update weights.\n      sess.run(optimizer, feed_dict=feed_dict)\n      # print some extra information once reach the evaluation frequency\n      if step % EVAL_FREQUENCY == 0:\n        # fetch some extra nodes\' data\n        l, lr, predictions = sess.run([loss, learning_rate, train_prediction],\n                                      feed_dict=feed_dict)\n        elapsed_time = time.time() - start_time\n        start_time = time.time()\n        print(\'Step %d (epoch %.2f), %.1f ms\' %\n              (step, float(step) * BATCH_SIZE / train_size,\n               1000 * elapsed_time / EVAL_FREQUENCY))\n        print(\'Minibatch loss: %.3f, learning rate: %.6f\' % (l, lr))\n        print(\'Minibatch error: %.1f%%\' % error_rate(predictions, batch_labels))\n        print(\'Validation error: %.1f%%\' % error_rate(\n            eval_in_batches(validation_data, sess), validation_labels))\n        sys.stdout.flush()\n    # Finally print the result!\n    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n    print(\'Test error: %.1f%%\' % test_error)\n    if FLAGS.self_test:\n      print(\'test_error\', test_error)\n      assert test_error == 0.0, \'expected 0.0 test_error, got %.2f\' % (\n          test_error,)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--use_fp16\',\n      default=False,\n      help=\'Use half floats instead of full floats if True.\',\n      action=\'store_true\')\n  parser.add_argument(\n      \'--self_test\',\n      default=False,\n      action=\'store_true\',\n      help=\'True if running a self test.\')\n\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
community/en/r1/tutorials/rnn/ptb/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Makes helper libraries available in the ptb package.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport reader\nimport util\n'"
community/en/r1/tutorials/rnn/ptb/ptb_word_lm.py,73,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Example / benchmark for building a PTB LSTM model.\n\nTrains the model described in:\n(Zaremba, et. al.) Recurrent Neural Network Regularization\nhttp://arxiv.org/abs/1409.2329\n\nThere are 3 supported model configurations:\n===========================================\n| config | epochs | train | valid  | test\n===========================================\n| small  | 13     | 37.99 | 121.39 | 115.91\n| medium | 39     | 48.45 |  86.16 |  82.07\n| large  | 55     | 37.87 |  82.62 |  78.29\nThe exact results may vary depending on the random initialization.\n\nThe hyperparameters used in the model:\n- init_scale - the initial scale of the weights\n- learning_rate - the initial value of the learning rate\n- max_grad_norm - the maximum permissible norm of the gradient\n- num_layers - the number of LSTM layers\n- num_steps - the number of unrolled steps of LSTM\n- hidden_size - the number of LSTM units\n- max_epoch - the number of epochs trained with the initial learning rate\n- max_max_epoch - the total number of epochs for training\n- keep_prob - the probability of keeping weights in the dropout layer\n- lr_decay - the decay of the learning rate for each epoch after ""max_epoch""\n- batch_size - the batch size\n- rnn_mode - the low level implementation of lstm cell: one of CUDNN,\n             BASIC, or BLOCK, representing cudnn_lstm, basic_lstm, and\n             lstm_block_cell classes.\n\nThe data required for this example is in the data/ dir of the\nPTB dataset from Tomas Mikolov\'s webpage:\n\n$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n$ tar xvf simple-examples.tgz\n\nTo run:\n\n$ python ptb_word_lm.py --data_path=simple-examples/data/\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport reader\nimport util\n\nfrom tensorflow.python.client import device_lib\n\nfrom distutils.version import StrictVersion\n\nflags = tf.flags\nlogging = tf.logging\n\nflags.DEFINE_string(\n    ""model"", ""small"",\n    ""A type of model. Possible options are: small, medium, large."")\nflags.DEFINE_string(""data_path"", None,\n                    ""Where the training/test data is stored."")\nflags.DEFINE_string(""save_path"", None,\n                    ""Model output directory."")\nflags.DEFINE_bool(""use_fp16"", False,\n                  ""Train using 16-bit floats instead of 32bit floats"")\nflags.DEFINE_integer(""num_gpus"", 1,\n                     ""If larger than 1, Grappler AutoParallel optimizer ""\n                     ""will create multiple training replicas with each GPU ""\n                     ""running one replica."")\nflags.DEFINE_string(""rnn_mode"", None,\n                    ""The low level implementation of lstm cell: one of CUDNN, ""\n                    ""BASIC, and BLOCK, representing cudnn_lstm, basic_lstm, ""\n                    ""and lstm_block_cell classes."")\nFLAGS = flags.FLAGS\nBASIC = ""basic""\nCUDNN = ""cudnn""\nBLOCK = ""block""\n\n\ndef data_type():\n  return tf.float16 if FLAGS.use_fp16 else tf.float32\n\n\nclass PTBInput(object):\n  """"""The input data.""""""\n\n  def __init__(self, config, data, name=None):\n    self.batch_size = batch_size = config.batch_size\n    self.num_steps = num_steps = config.num_steps\n    self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n    self.input_data, self.targets = reader.ptb_producer(\n        data, batch_size, num_steps, name=name)\n\n\nclass PTBModel(object):\n  """"""The PTB model.""""""\n\n  def __init__(self, is_training, config, input_):\n    self._is_training = is_training\n    self._input = input_\n    self._rnn_params = None\n    self._cell = None\n    self.batch_size = input_.batch_size\n    self.num_steps = input_.num_steps\n    size = config.hidden_size\n    vocab_size = config.vocab_size\n\n    with tf.device(""/cpu:0""):\n      embedding = tf.get_variable(\n          ""embedding"", [vocab_size, size], dtype=data_type())\n      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n\n    if is_training and config.keep_prob < 1:\n      inputs = tf.nn.dropout(inputs, config.keep_prob)\n\n    output, state = self._build_rnn_graph(inputs, config, is_training)\n\n    softmax_w = tf.get_variable(\n        ""softmax_w"", [size, vocab_size], dtype=data_type())\n    softmax_b = tf.get_variable(""softmax_b"", [vocab_size], dtype=data_type())\n    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n     # Reshape logits to be a 3-D tensor for sequence loss\n    logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n\n    # Use the contrib sequence loss and average over the batches\n    loss = tf.contrib.seq2seq.sequence_loss(\n        logits,\n        input_.targets,\n        tf.ones([self.batch_size, self.num_steps], dtype=data_type()),\n        average_across_timesteps=False,\n        average_across_batch=True)\n\n    # Update the cost\n    self._cost = tf.reduce_sum(loss)\n    self._final_state = state\n\n    if not is_training:\n      return\n\n    self._lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars),\n                                      config.max_grad_norm)\n    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n    self._train_op = optimizer.apply_gradients(\n        zip(grads, tvars),\n        global_step=tf.train.get_or_create_global_step())\n\n    self._new_lr = tf.placeholder(\n        tf.float32, shape=[], name=""new_learning_rate"")\n    self._lr_update = tf.assign(self._lr, self._new_lr)\n\n  def _build_rnn_graph(self, inputs, config, is_training):\n    if config.rnn_mode == CUDNN:\n      return self._build_rnn_graph_cudnn(inputs, config, is_training)\n    else:\n      return self._build_rnn_graph_lstm(inputs, config, is_training)\n\n  def _build_rnn_graph_cudnn(self, inputs, config, is_training):\n    """"""Build the inference graph using CUDNN cell.""""""\n    inputs = tf.transpose(inputs, [1, 0, 2])\n    self._cell = tf.contrib.cudnn_rnn.CudnnLSTM(\n        num_layers=config.num_layers,\n        num_units=config.hidden_size,\n        input_size=config.hidden_size,\n        dropout=1 - config.keep_prob if is_training else 0)\n    params_size_t = self._cell.params_size()\n    self._rnn_params = tf.get_variable(\n        ""lstm_params"",\n        initializer=tf.random_uniform(\n            [params_size_t], -config.init_scale, config.init_scale),\n        validate_shape=False)\n    c = tf.zeros([config.num_layers, self.batch_size, config.hidden_size],\n                 tf.float32)\n    h = tf.zeros([config.num_layers, self.batch_size, config.hidden_size],\n                 tf.float32)\n    self._initial_state = (tf.contrib.rnn.LSTMStateTuple(h=h, c=c),)\n    outputs, h, c = self._cell(inputs, h, c, self._rnn_params, is_training)\n    outputs = tf.transpose(outputs, [1, 0, 2])\n    outputs = tf.reshape(outputs, [-1, config.hidden_size])\n    return outputs, (tf.contrib.rnn.LSTMStateTuple(h=h, c=c),)\n\n  def _get_lstm_cell(self, config, is_training):\n    if config.rnn_mode == BASIC:\n      return tf.contrib.rnn.BasicLSTMCell(\n          config.hidden_size, forget_bias=0.0, state_is_tuple=True,\n          reuse=not is_training)\n    if config.rnn_mode == BLOCK:\n      return tf.contrib.rnn.LSTMBlockCell(\n          config.hidden_size, forget_bias=0.0)\n    raise ValueError(""rnn_mode %s not supported"" % config.rnn_mode)\n\n  def _build_rnn_graph_lstm(self, inputs, config, is_training):\n    """"""Build the inference graph using canonical LSTM cells.""""""\n    # Slightly better results can be obtained with forget gate biases\n    # initialized to 1 but the hyperparameters of the model would need to be\n    # different than reported in the paper.\n    def make_cell():\n      cell = self._get_lstm_cell(config, is_training)\n      if is_training and config.keep_prob < 1:\n        cell = tf.contrib.rnn.DropoutWrapper(\n            cell, output_keep_prob=config.keep_prob)\n      return cell\n\n    cell = tf.contrib.rnn.MultiRNNCell(\n        [make_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n\n    self._initial_state = cell.zero_state(config.batch_size, data_type())\n    state = self._initial_state\n    # Simplified version of tf.nn.static_rnn().\n    # This builds an unrolled LSTM for tutorial purposes only.\n    # In general, use tf.nn.static_rnn() or tf.nn.static_state_saving_rnn().\n    #\n    # The alternative version of the code below is:\n    #\n    # inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n    # outputs, state = tf.nn.static_rnn(cell, inputs,\n    #                                   initial_state=self._initial_state)\n    outputs = []\n    with tf.variable_scope(""RNN""):\n      for time_step in range(self.num_steps):\n        if time_step > 0: tf.get_variable_scope().reuse_variables()\n        (cell_output, state) = cell(inputs[:, time_step, :], state)\n        outputs.append(cell_output)\n    output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n    return output, state\n\n  def assign_lr(self, session, lr_value):\n    session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n\n  def export_ops(self, name):\n    """"""Exports ops to collections.""""""\n    self._name = name\n    ops = {util.with_prefix(self._name, ""cost""): self._cost}\n    if self._is_training:\n      ops.update(lr=self._lr, new_lr=self._new_lr, lr_update=self._lr_update)\n      if self._rnn_params:\n        ops.update(rnn_params=self._rnn_params)\n    for name, op in ops.items():\n      tf.add_to_collection(name, op)\n    self._initial_state_name = util.with_prefix(self._name, ""initial"")\n    self._final_state_name = util.with_prefix(self._name, ""final"")\n    util.export_state_tuples(self._initial_state, self._initial_state_name)\n    util.export_state_tuples(self._final_state, self._final_state_name)\n\n  def import_ops(self):\n    """"""Imports ops from collections.""""""\n    if self._is_training:\n      self._train_op = tf.get_collection_ref(""train_op"")[0]\n      self._lr = tf.get_collection_ref(""lr"")[0]\n      self._new_lr = tf.get_collection_ref(""new_lr"")[0]\n      self._lr_update = tf.get_collection_ref(""lr_update"")[0]\n      rnn_params = tf.get_collection_ref(""rnn_params"")\n      if self._cell and rnn_params:\n        params_saveable = tf.contrib.cudnn_rnn.RNNParamsSaveable(\n            self._cell,\n            self._cell.params_to_canonical,\n            self._cell.canonical_to_params,\n            rnn_params,\n            base_variable_scope=""Model/RNN"")\n        tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, params_saveable)\n    self._cost = tf.get_collection_ref(util.with_prefix(self._name, ""cost""))[0]\n    num_replicas = FLAGS.num_gpus if self._name == ""Train"" else 1\n    self._initial_state = util.import_state_tuples(\n        self._initial_state, self._initial_state_name, num_replicas)\n    self._final_state = util.import_state_tuples(\n        self._final_state, self._final_state_name, num_replicas)\n\n  @property\n  def input(self):\n    return self._input\n\n  @property\n  def initial_state(self):\n    return self._initial_state\n\n  @property\n  def cost(self):\n    return self._cost\n\n  @property\n  def final_state(self):\n    return self._final_state\n\n  @property\n  def lr(self):\n    return self._lr\n\n  @property\n  def train_op(self):\n    return self._train_op\n\n  @property\n  def initial_state_name(self):\n    return self._initial_state_name\n\n  @property\n  def final_state_name(self):\n    return self._final_state_name\n\n\nclass SmallConfig(object):\n  """"""Small config.""""""\n  init_scale = 0.1\n  learning_rate = 1.0\n  max_grad_norm = 5\n  num_layers = 2\n  num_steps = 20\n  hidden_size = 200\n  max_epoch = 4\n  max_max_epoch = 13\n  keep_prob = 1.0\n  lr_decay = 0.5\n  batch_size = 20\n  vocab_size = 10000\n  rnn_mode = BLOCK\n\n\nclass MediumConfig(object):\n  """"""Medium config.""""""\n  init_scale = 0.05\n  learning_rate = 1.0\n  max_grad_norm = 5\n  num_layers = 2\n  num_steps = 35\n  hidden_size = 650\n  max_epoch = 6\n  max_max_epoch = 39\n  keep_prob = 0.5\n  lr_decay = 0.8\n  batch_size = 20\n  vocab_size = 10000\n  rnn_mode = BLOCK\n\n\nclass LargeConfig(object):\n  """"""Large config.""""""\n  init_scale = 0.04\n  learning_rate = 1.0\n  max_grad_norm = 10\n  num_layers = 2\n  num_steps = 35\n  hidden_size = 1500\n  max_epoch = 14\n  max_max_epoch = 55\n  keep_prob = 0.35\n  lr_decay = 1 / 1.15\n  batch_size = 20\n  vocab_size = 10000\n  rnn_mode = BLOCK\n\n\nclass TestConfig(object):\n  """"""Tiny config, for testing.""""""\n  init_scale = 0.1\n  learning_rate = 1.0\n  max_grad_norm = 1\n  num_layers = 1\n  num_steps = 2\n  hidden_size = 2\n  max_epoch = 1\n  max_max_epoch = 1\n  keep_prob = 1.0\n  lr_decay = 0.5\n  batch_size = 20\n  vocab_size = 10000\n  rnn_mode = BLOCK\n\n\ndef run_epoch(session, model, eval_op=None, verbose=False):\n  """"""Runs the model on the given data.""""""\n  start_time = time.time()\n  costs = 0.0\n  iters = 0\n  state = session.run(model.initial_state)\n\n  fetches = {\n      ""cost"": model.cost,\n      ""final_state"": model.final_state,\n  }\n  if eval_op is not None:\n    fetches[""eval_op""] = eval_op\n\n  for step in range(model.input.epoch_size):\n    feed_dict = {}\n    for i, (c, h) in enumerate(model.initial_state):\n      feed_dict[c] = state[i].c\n      feed_dict[h] = state[i].h\n\n    vals = session.run(fetches, feed_dict)\n    cost = vals[""cost""]\n    state = vals[""final_state""]\n\n    costs += cost\n    iters += model.input.num_steps\n\n    if verbose and step % (model.input.epoch_size // 10) == 10:\n      print(""%.3f perplexity: %.3f speed: %.0f wps"" %\n            (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),\n             iters * model.input.batch_size * max(1, FLAGS.num_gpus) /\n             (time.time() - start_time)))\n\n  return np.exp(costs / iters)\n\n\ndef get_config():\n  """"""Get model config.""""""\n  config = None\n  if FLAGS.model == ""small"":\n    config = SmallConfig()\n  elif FLAGS.model == ""medium"":\n    config = MediumConfig()\n  elif FLAGS.model == ""large"":\n    config = LargeConfig()\n  elif FLAGS.model == ""test"":\n    config = TestConfig()\n  else:\n    raise ValueError(""Invalid model: %s"", FLAGS.model)\n  if FLAGS.rnn_mode:\n    config.rnn_mode = FLAGS.rnn_mode\n  if FLAGS.num_gpus != 1 or StrictVersion(tf.__version__) < StrictVersion(""1.3.0"") :\n    config.rnn_mode = BASIC\n  return config\n\n\ndef main(_):\n  if not FLAGS.data_path:\n    raise ValueError(""Must set --data_path to PTB data directory"")\n  gpus = [\n      x.name for x in device_lib.list_local_devices() if x.device_type == ""GPU""\n  ]\n  if FLAGS.num_gpus > len(gpus):\n    raise ValueError(\n        ""Your machine has only %d gpus ""\n        ""which is less than the requested --num_gpus=%d.""\n        % (len(gpus), FLAGS.num_gpus))\n\n  raw_data = reader.ptb_raw_data(FLAGS.data_path)\n  train_data, valid_data, test_data, _ = raw_data\n\n  config = get_config()\n  eval_config = get_config()\n  eval_config.batch_size = 1\n  eval_config.num_steps = 1\n\n  with tf.Graph().as_default():\n    initializer = tf.random_uniform_initializer(-config.init_scale,\n                                                config.init_scale)\n\n    with tf.name_scope(""Train""):\n      train_input = PTBInput(config=config, data=train_data, name=""TrainInput"")\n      with tf.variable_scope(""Model"", reuse=None, initializer=initializer):\n        m = PTBModel(is_training=True, config=config, input_=train_input)\n      tf.summary.scalar(""Training Loss"", m.cost)\n      tf.summary.scalar(""Learning Rate"", m.lr)\n\n    with tf.name_scope(""Valid""):\n      valid_input = PTBInput(config=config, data=valid_data, name=""ValidInput"")\n      with tf.variable_scope(""Model"", reuse=True, initializer=initializer):\n        mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n      tf.summary.scalar(""Validation Loss"", mvalid.cost)\n\n    with tf.name_scope(""Test""):\n      test_input = PTBInput(\n          config=eval_config, data=test_data, name=""TestInput"")\n      with tf.variable_scope(""Model"", reuse=True, initializer=initializer):\n        mtest = PTBModel(is_training=False, config=eval_config,\n                         input_=test_input)\n\n    models = {""Train"": m, ""Valid"": mvalid, ""Test"": mtest}\n    for name, model in models.items():\n      model.export_ops(name)\n    metagraph = tf.train.export_meta_graph()\n    if StrictVersion(tf.__version__) < StrictVersion(""1.1.0"") and FLAGS.num_gpus > 1:\n      raise ValueError(""num_gpus > 1 is not supported for TensorFlow versions ""\n                       ""below 1.1.0"")\n    soft_placement = False\n    if FLAGS.num_gpus > 1:\n      soft_placement = True\n      util.auto_parallel(metagraph, m)\n\n  with tf.Graph().as_default():\n    tf.train.import_meta_graph(metagraph)\n    for model in models.values():\n      model.import_ops()\n    sv = tf.train.Supervisor(logdir=FLAGS.save_path)\n    config_proto = tf.ConfigProto(allow_soft_placement=soft_placement)\n    with sv.managed_session(config=config_proto) as session:\n      for i in range(config.max_max_epoch):\n        lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n        m.assign_lr(session, config.learning_rate * lr_decay)\n\n        print(""Epoch: %d Learning rate: %.3f"" % (i + 1, session.run(m.lr)))\n        train_perplexity = run_epoch(session, m, eval_op=m.train_op,\n                                     verbose=True)\n        print(""Epoch: %d Train Perplexity: %.3f"" % (i + 1, train_perplexity))\n        valid_perplexity = run_epoch(session, mvalid)\n        print(""Epoch: %d Valid Perplexity: %.3f"" % (i + 1, valid_perplexity))\n\n      test_perplexity = run_epoch(session, mtest)\n      print(""Test Perplexity: %.3f"" % test_perplexity)\n\n      if FLAGS.save_path:\n        print(""Saving model to %s."" % FLAGS.save_path)\n        sv.saver.save(session, FLAGS.save_path, global_step=sv.global_step)\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
community/en/r1/tutorials/rnn/ptb/reader.py,12,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Utilities for parsing PTB text files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport sys\n\nimport tensorflow as tf\n\nPy3 = sys.version_info[0] == 3\n\ndef _read_words(filename):\n  with tf.gfile.GFile(filename, ""r"") as f:\n    if Py3:\n      return f.read().replace(""\\n"", ""<eos>"").split()\n    else:\n      return f.read().decode(""utf-8"").replace(""\\n"", ""<eos>"").split()\n\n\ndef _build_vocab(filename):\n  data = _read_words(filename)\n\n  counter = collections.Counter(data)\n  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n  words, _ = list(zip(*count_pairs))\n  word_to_id = dict(zip(words, range(len(words))))\n\n  return word_to_id\n\n\ndef _file_to_word_ids(filename, word_to_id):\n  data = _read_words(filename)\n  return [word_to_id[word] for word in data if word in word_to_id]\n\n\ndef ptb_raw_data(data_path=None):\n  """"""Load PTB raw data from data directory ""data_path"".\n\n  Reads PTB text files, converts strings to integer ids,\n  and performs mini-batching of the inputs.\n\n  The PTB dataset comes from Tomas Mikolov\'s webpage:\n\n  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n\n  Returns:\n    tuple (train_data, valid_data, test_data, vocabulary)\n    where each of the data objects can be passed to PTBIterator.\n  """"""\n\n  train_path = os.path.join(data_path, ""ptb.train.txt"")\n  valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n  test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n  word_to_id = _build_vocab(train_path)\n  train_data = _file_to_word_ids(train_path, word_to_id)\n  valid_data = _file_to_word_ids(valid_path, word_to_id)\n  test_data = _file_to_word_ids(test_path, word_to_id)\n  vocabulary = len(word_to_id)\n  return train_data, valid_data, test_data, vocabulary\n\n\ndef ptb_producer(raw_data, batch_size, num_steps, name=None):\n  """"""Iterate on the raw PTB data.\n\n  This chunks up raw_data into batches of examples and returns Tensors that\n  are drawn from these batches.\n\n  Args:\n    raw_data: one of the raw data outputs from ptb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n    name: the name of this operation (optional).\n\n  Returns:\n    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n    of the tuple is the same data time-shifted to the right by one.\n\n  Raises:\n    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n  """"""\n  with tf.name_scope(name, ""PTBProducer"", [raw_data, batch_size, num_steps]):\n    raw_data = tf.convert_to_tensor(raw_data, name=""raw_data"", dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n                      [batch_size, batch_len])\n\n    epoch_size = (batch_len - 1) // num_steps\n    assertion = tf.assert_positive(\n        epoch_size,\n        message=""epoch_size == 0, decrease batch_size or num_steps"")\n    with tf.control_dependencies([assertion]):\n      epoch_size = tf.identity(epoch_size, name=""epoch_size"")\n\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n    x = tf.strided_slice(data, [0, i * num_steps],\n                         [batch_size, (i + 1) * num_steps])\n    x.set_shape([batch_size, num_steps])\n    y = tf.strided_slice(data, [0, i * num_steps + 1],\n                         [batch_size, (i + 1) * num_steps + 1])\n    y.set_shape([batch_size, num_steps])\n    return x, y\n'"
community/en/r1/tutorials/rnn/ptb/reader_test.py,6,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for models.tutorials.rnn.ptb.reader.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\n\nimport tensorflow as tf\n\nimport reader\n\n\nclass PtbReaderTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._string_data = ""\\n"".join(\n        ["" hello there i am"",\n         "" rain as day"",\n         "" want some cheesy puffs ?""])\n\n  def testPtbRawData(self):\n    tmpdir = tf.test.get_temp_dir()\n    for suffix in ""train"", ""valid"", ""test"":\n      filename = os.path.join(tmpdir, ""ptb.%s.txt"" % suffix)\n      with tf.gfile.GFile(filename, ""w"") as fh:\n        fh.write(self._string_data)\n    # Smoke test\n    output = reader.ptb_raw_data(tmpdir)\n    self.assertEqual(len(output), 4)\n\n  def testPtbProducer(self):\n    raw_data = [4, 3, 2, 1, 0, 5, 6, 1, 1, 1, 1, 0, 3, 4, 1]\n    batch_size = 3\n    num_steps = 2\n    x, y = reader.ptb_producer(raw_data, batch_size, num_steps)\n    with self.test_session() as session:\n      coord = tf.train.Coordinator()\n      tf.train.start_queue_runners(session, coord=coord)\n      try:\n        xval, yval = session.run([x, y])\n        self.assertAllEqual(xval, [[4, 3], [5, 6], [1, 0]])\n        self.assertAllEqual(yval, [[3, 2], [6, 1], [0, 3]])\n        xval, yval = session.run([x, y])\n        self.assertAllEqual(xval, [[2, 1], [1, 1], [3, 4]])\n        self.assertAllEqual(yval, [[1, 0], [1, 1], [4, 1]])\n      finally:\n        coord.request_stop()\n        coord.join()\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
community/en/r1/tutorials/rnn/ptb/util.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities for Grappler autoparallel optimizer.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.core.framework import variable_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\n\nFLAGS = tf.flags.FLAGS\n\n\ndef export_state_tuples(state_tuples, name):\n  for state_tuple in state_tuples:\n    tf.add_to_collection(name, state_tuple.c)\n    tf.add_to_collection(name, state_tuple.h)\n\n\ndef import_state_tuples(state_tuples, name, num_replicas):\n  restored = []\n  for i in range(len(state_tuples) * num_replicas):\n    c = tf.get_collection_ref(name)[2 * i + 0]\n    h = tf.get_collection_ref(name)[2 * i + 1]\n    restored.append(tf.contrib.rnn.LSTMStateTuple(c, h))\n  return tuple(restored)\n\n\ndef with_prefix(prefix, name):\n  """"""Adds prefix to name.""""""\n  return ""/"".join((prefix, name))\n\n\ndef with_autoparallel_prefix(replica_id, name):\n  return with_prefix(""AutoParallel-Replica-%d"" % replica_id, name)\n\n\nclass UpdateCollection(object):\n  """"""Update collection info in MetaGraphDef for AutoParallel optimizer.""""""\n\n  def __init__(self, metagraph, model):\n    self._metagraph = metagraph\n    self.replicate_states(model.initial_state_name)\n    self.replicate_states(model.final_state_name)\n    self.update_snapshot_name(""variables"")\n    self.update_snapshot_name(""trainable_variables"")\n\n  def update_snapshot_name(self, var_coll_name):\n    var_list = self._metagraph.collection_def[var_coll_name]\n    for i, value in enumerate(var_list.bytes_list.value):\n      var_def = variable_pb2.VariableDef()\n      var_def.ParseFromString(value)\n      # Somehow node Model/global_step/read doesn\'t have any fanout and seems to\n      # be only used for snapshot; this is different from all other variables.\n      if var_def.snapshot_name != ""Model/global_step/read:0"":\n        var_def.snapshot_name = with_autoparallel_prefix(\n            0, var_def.snapshot_name)\n      value = var_def.SerializeToString()\n      var_list.bytes_list.value[i] = value\n\n  def replicate_states(self, state_coll_name):\n    state_list = self._metagraph.collection_def[state_coll_name]\n    num_states = len(state_list.node_list.value)\n    for replica_id in range(1, FLAGS.num_gpus):\n      for i in range(num_states):\n        state_list.node_list.value.append(state_list.node_list.value[i])\n    for replica_id in range(FLAGS.num_gpus):\n      for i in range(num_states):\n        index = replica_id * num_states + i\n        state_list.node_list.value[index] = with_autoparallel_prefix(\n            replica_id, state_list.node_list.value[index])\n\n\ndef auto_parallel(metagraph, model):\n  from tensorflow.python.grappler import tf_optimizer\n  rewriter_config = rewriter_config_pb2.RewriterConfig()\n  rewriter_config.optimizers.append(""autoparallel"")\n  rewriter_config.auto_parallel.enable = True\n  rewriter_config.auto_parallel.num_replicas = FLAGS.num_gpus\n  optimized_graph = tf_optimizer.OptimizeGraph(rewriter_config, metagraph)\n  metagraph.graph_def.CopyFrom(optimized_graph)\n  UpdateCollection(metagraph, model)\n'"
community/en/r1/tutorials/rnn/quickdraw/create_dataset.py,11,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Creates training and eval data from Quickdraw NDJSON files.\n\nThis tool reads the NDJSON files from https://quickdraw.withgoogle.com/data\nand converts them into tensorflow.Example stored in TFRecord files.\n\nThe tensorflow example will contain 3 features:\n shape - contains the shape of the sequence [length, dim] where dim=3.\n class_index - the class index of the class for the example.\n ink - a length * dim vector of the ink.\n\nIt creates disjoint training and evaluation sets.\n\npython create_dataset.py \\\n  --ndjson_path ${HOME}/ndjson \\\n  --output_path ${HOME}/tfrecord\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport os\nimport random\nimport sys\nimport numpy as np\nimport tensorflow as tf\n\n\ndef parse_line(ndjson_line):\n  """"""Parse an ndjson line and return ink (as np array) and classname.""""""\n  sample = json.loads(ndjson_line)\n  class_name = sample[""word""]\n  if not class_name:\n    print (""Empty classname"")\n    return None, None\n  inkarray = sample[""drawing""]\n  stroke_lengths = [len(stroke[0]) for stroke in inkarray]\n  total_points = sum(stroke_lengths)\n  np_ink = np.zeros((total_points, 3), dtype=np.float32)\n  current_t = 0\n  if not inkarray:\n    print(""Empty inkarray"")\n    return None, None\n  for stroke in inkarray:\n    if len(stroke[0]) != len(stroke[1]):\n      print(""Inconsistent number of x and y coordinates."")\n      return None, None\n    for i in [0, 1]:\n      np_ink[current_t:(current_t + len(stroke[0])), i] = stroke[i]\n    current_t += len(stroke[0])\n    np_ink[current_t - 1, 2] = 1  # stroke_end\n  # Preprocessing.\n  # 1. Size normalization.\n  lower = np.min(np_ink[:, 0:2], axis=0)\n  upper = np.max(np_ink[:, 0:2], axis=0)\n  scale = upper - lower\n  scale[scale == 0] = 1\n  np_ink[:, 0:2] = (np_ink[:, 0:2] - lower) / scale\n  # 2. Compute deltas.\n  np_ink[1:, 0:2] -= np_ink[0:-1, 0:2]\n  np_ink = np_ink[1:, :]\n  return np_ink, class_name\n\n\ndef convert_data(trainingdata_dir,\n                 observations_per_class,\n                 output_file,\n                 classnames,\n                 output_shards=10,\n                 offset=0):\n  """"""Convert training data from ndjson files into tf.Example in tf.Record.\n\n  Args:\n   trainingdata_dir: path to the directory containin the training data.\n     The training data is stored in that directory as ndjson files.\n   observations_per_class: the number of items to load per class.\n   output_file: path where to write the output.\n   classnames: array with classnames - is auto created if not passed in.\n   output_shards: the number of shards to write the output in.\n   offset: the number of items to skip at the beginning of each file.\n\n  Returns:\n    classnames: the class names as strings. classnames[classes[i]] is the\n      textual representation of the class of the i-th data point.\n  """"""\n\n  def _pick_output_shard():\n    return random.randint(0, output_shards - 1)\n\n  file_handles = []\n  # Open all input files.\n  for filename in sorted(tf.gfile.ListDirectory(trainingdata_dir)):\n    if not filename.endswith("".ndjson""):\n      print(""Skipping"", filename)\n      continue\n    file_handles.append(\n        tf.gfile.GFile(os.path.join(trainingdata_dir, filename), ""r""))\n    if offset:  # Fast forward all files to skip the offset.\n      count = 0\n      for _ in file_handles[-1]:\n        count += 1\n        if count == offset:\n          break\n\n  writers = []\n  for i in range(FLAGS.output_shards):\n    writers.append(\n        tf.python_io.TFRecordWriter(""%s-%05i-of-%05i"" % (output_file, i,\n                                                         output_shards)))\n\n  reading_order = list(range(len(file_handles))) * observations_per_class\n  random.shuffle(reading_order)\n\n  for c in reading_order:\n    line = file_handles[c].readline()\n    ink = None\n    while ink is None:\n      ink, class_name = parse_line(line)\n      if ink is None:\n        print (""Couldn\'t parse ink from \'"" + line + ""\'."")\n    if class_name not in classnames:\n      classnames.append(class_name)\n    features = {}\n    features[""class_index""] = tf.train.Feature(int64_list=tf.train.Int64List(\n        value=[classnames.index(class_name)]))\n    features[""ink""] = tf.train.Feature(float_list=tf.train.FloatList(\n        value=ink.flatten()))\n    features[""shape""] = tf.train.Feature(int64_list=tf.train.Int64List(\n        value=ink.shape))\n    f = tf.train.Features(feature=features)\n    example = tf.train.Example(features=f)\n    writers[_pick_output_shard()].write(example.SerializeToString())\n\n  # Close all files\n  for w in writers:\n    w.close()\n  for f in file_handles:\n    f.close()\n  # Write the class list.\n  with tf.gfile.GFile(output_file + "".classes"", ""w"") as f:\n    for class_name in classnames:\n      f.write(class_name + ""\\n"")\n  return classnames\n\n\ndef main(argv):\n  del argv\n  classnames = convert_data(\n      FLAGS.ndjson_path,\n      FLAGS.train_observations_per_class,\n      os.path.join(FLAGS.output_path, ""training.tfrecord""),\n      classnames=[],\n      output_shards=FLAGS.output_shards,\n      offset=0)\n  convert_data(\n      FLAGS.ndjson_path,\n      FLAGS.eval_observations_per_class,\n      os.path.join(FLAGS.output_path, ""eval.tfrecord""),\n      classnames=classnames,\n      output_shards=FLAGS.output_shards,\n      offset=FLAGS.train_observations_per_class)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  parser.add_argument(\n      ""--ndjson_path"",\n      type=str,\n      default="""",\n      help=""Directory where the ndjson files are stored."")\n  parser.add_argument(\n      ""--output_path"",\n      type=str,\n      default="""",\n      help=""Directory where to store the output TFRecord files."")\n  parser.add_argument(\n      ""--train_observations_per_class"",\n      type=int,\n      default=10000,\n      help=""How many items per class to load for training."")\n  parser.add_argument(\n      ""--eval_observations_per_class"",\n      type=int,\n      default=1000,\n      help=""How many items per class to load for evaluation."")\n  parser.add_argument(\n      ""--output_shards"",\n      type=int,\n      default=10,\n      help=""Number of shards for the output."")\n\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
community/en/r1/tutorials/rnn/quickdraw/train_model.py,57,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Binary for training a RNN-based classifier for the Quick, Draw! data.\n\npython train_model.py \\\n  --training_data train_data \\\n  --eval_data eval_data \\\n  --model_dir /tmp/quickdraw_model/ \\\n  --cell_type cudnn_lstm\n\nWhen running on GPUs using --cell_type cudnn_lstm is much faster.\n\nThe expected performance is ~75% in 1.5M steps with the default configuration.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport argparse\nimport ast\nimport functools\nimport sys\n\nimport tensorflow as tf\n\n\ndef get_num_classes():\n  classes = []\n  with tf.gfile.GFile(FLAGS.classes_file, ""r"") as f:\n    classes = [x for x in f]\n  num_classes = len(classes)\n  return num_classes\n\n\ndef get_input_fn(mode, tfrecord_pattern, batch_size):\n  """"""Creates an input_fn that stores all the data in memory.\n\n  Args:\n   mode: one of tf.contrib.learn.ModeKeys.{TRAIN, INFER, EVAL}\n   tfrecord_pattern: path to a TF record file created using create_dataset.py.\n   batch_size: the batch size to output.\n\n  Returns:\n    A valid input_fn for the model estimator.\n  """"""\n\n  def _parse_tfexample_fn(example_proto, mode):\n    """"""Parse a single record which is expected to be a tensorflow.Example.""""""\n    feature_to_type = {\n        ""ink"": tf.VarLenFeature(dtype=tf.float32),\n        ""shape"": tf.FixedLenFeature([2], dtype=tf.int64)\n    }\n    if mode != tf.estimator.ModeKeys.PREDICT:\n      # The labels won\'t be available at inference time, so don\'t add them\n      # to the list of feature_columns to be read.\n      feature_to_type[""class_index""] = tf.FixedLenFeature([1], dtype=tf.int64)\n\n    parsed_features = tf.parse_single_example(example_proto, feature_to_type)\n    labels = None\n    if mode != tf.estimator.ModeKeys.PREDICT:\n      labels = parsed_features[""class_index""]\n    parsed_features[""ink""] = tf.sparse_tensor_to_dense(parsed_features[""ink""])\n    return parsed_features, labels\n\n  def _input_fn():\n    """"""Estimator `input_fn`.\n\n    Returns:\n      A tuple of:\n      - Dictionary of string feature name to `Tensor`.\n      - `Tensor` of target labels.\n    """"""\n    dataset = tf.data.TFRecordDataset.list_files(tfrecord_pattern)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      dataset = dataset.shuffle(buffer_size=10)\n    dataset = dataset.repeat()\n    # Preprocesses 10 files concurrently and interleaves records from each file.\n    dataset = dataset.interleave(\n        tf.data.TFRecordDataset,\n        cycle_length=10,\n        block_length=1)\n    dataset = dataset.map(\n        functools.partial(_parse_tfexample_fn, mode=mode),\n        num_parallel_calls=10)\n    dataset = dataset.prefetch(10000)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      dataset = dataset.shuffle(buffer_size=1000000)\n    # Our inputs are variable length, so pad them.\n    dataset = dataset.padded_batch(\n        batch_size, padded_shapes=dataset.output_shapes)\n    features, labels = dataset.make_one_shot_iterator().get_next()\n    return features, labels\n\n  return _input_fn\n\n\ndef model_fn(features, labels, mode, params):\n  """"""Model function for RNN classifier.\n\n  This function sets up a neural network which applies convolutional layers (as\n  configured with params.num_conv and params.conv_len) to the input.\n  The output of the convolutional layers is given to LSTM layers (as configured\n  with params.num_layers and params.num_nodes).\n  The final state of the all LSTM layers are concatenated and fed to a fully\n  connected layer to obtain the final classification scores.\n\n  Args:\n    features: dictionary with keys: inks, lengths.\n    labels: one hot encoded classes\n    mode: one of tf.estimator.ModeKeys.{TRAIN, INFER, EVAL}\n    params: a parameter dictionary with the following keys: num_layers,\n      num_nodes, batch_size, num_conv, conv_len, num_classes, learning_rate.\n\n  Returns:\n    ModelFnOps for Estimator API.\n  """"""\n\n  def _get_input_tensors(features, labels):\n    """"""Converts the input dict into inks, lengths, and labels tensors.""""""\n    # features[ink] is a sparse tensor that is [8, batch_maxlen, 3]\n    # inks will be a dense tensor of [8, maxlen, 3]\n    # shapes is [batchsize, 2]\n    shapes = features[""shape""]\n    # lengths will be [batch_size]\n    lengths = tf.squeeze(\n        tf.slice(shapes, begin=[0, 0], size=[params.batch_size, 1]))\n    inks = tf.reshape(features[""ink""], [params.batch_size, -1, 3])\n    if labels is not None:\n      labels = tf.squeeze(labels)\n    return inks, lengths, labels\n\n  def _add_conv_layers(inks, lengths):\n    """"""Adds convolution layers.""""""\n    convolved = inks\n    for i in range(len(params.num_conv)):\n      convolved_input = convolved\n      if params.batch_norm:\n        convolved_input = tf.layers.batch_normalization(\n            convolved_input,\n            training=(mode == tf.estimator.ModeKeys.TRAIN))\n      # Add dropout layer if enabled and not first convolution layer.\n      if i > 0 and params.dropout:\n        convolved_input = tf.layers.dropout(\n            convolved_input,\n            rate=params.dropout,\n            training=(mode == tf.estimator.ModeKeys.TRAIN))\n      convolved = tf.layers.conv1d(\n          convolved_input,\n          filters=params.num_conv[i],\n          kernel_size=params.conv_len[i],\n          activation=None,\n          strides=1,\n          padding=""same"",\n          name=""conv1d_%d"" % i)\n    return convolved, lengths\n\n  def _add_regular_rnn_layers(convolved, lengths):\n    """"""Adds RNN layers.""""""\n    if params.cell_type == ""lstm"":\n      cell = tf.nn.rnn_cell.BasicLSTMCell\n    elif params.cell_type == ""block_lstm"":\n      cell = tf.contrib.rnn.LSTMBlockCell\n    cells_fw = [cell(params.num_nodes) for _ in range(params.num_layers)]\n    cells_bw = [cell(params.num_nodes) for _ in range(params.num_layers)]\n    if params.dropout > 0.0:\n      cells_fw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_fw]\n      cells_bw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_bw]\n    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n        cells_fw=cells_fw,\n        cells_bw=cells_bw,\n        inputs=convolved,\n        sequence_length=lengths,\n        dtype=tf.float32,\n        scope=""rnn_classification"")\n    return outputs\n\n  def _add_cudnn_rnn_layers(convolved):\n    """"""Adds CUDNN LSTM layers.""""""\n    # Convolutions output [B, L, Ch], while CudnnLSTM is time-major.\n    convolved = tf.transpose(convolved, [1, 0, 2])\n    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\n        num_layers=params.num_layers,\n        num_units=params.num_nodes,\n        dropout=params.dropout if mode == tf.estimator.ModeKeys.TRAIN else 0.0,\n        direction=""bidirectional"")\n    outputs, _ = lstm(convolved)\n    # Convert back from time-major outputs to batch-major outputs.\n    outputs = tf.transpose(outputs, [1, 0, 2])\n    return outputs\n\n  def _add_rnn_layers(convolved, lengths):\n    """"""Adds recurrent neural network layers depending on the cell type.""""""\n    if params.cell_type != ""cudnn_lstm"":\n      outputs = _add_regular_rnn_layers(convolved, lengths)\n    else:\n      outputs = _add_cudnn_rnn_layers(convolved)\n    # outputs is [batch_size, L, N] where L is the maximal sequence length and N\n    # the number of nodes in the last layer.\n    mask = tf.tile(\n        tf.expand_dims(tf.sequence_mask(lengths, tf.shape(outputs)[1]), 2),\n        [1, 1, tf.shape(outputs)[2]])\n    zero_outside = tf.where(mask, outputs, tf.zeros_like(outputs))\n    outputs = tf.reduce_sum(zero_outside, axis=1)\n    return outputs\n\n  def _add_fc_layers(final_state):\n    """"""Adds a fully connected layer.""""""\n    return tf.layers.dense(final_state, params.num_classes)\n\n  # Build the model.\n  inks, lengths, labels = _get_input_tensors(features, labels)\n  convolved, lengths = _add_conv_layers(inks, lengths)\n  final_state = _add_rnn_layers(convolved, lengths)\n  logits = _add_fc_layers(final_state)\n  # Add the loss.\n  cross_entropy = tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits))\n  # Add the optimizer.\n  train_op = tf.contrib.layers.optimize_loss(\n      loss=cross_entropy,\n      global_step=tf.train.get_global_step(),\n      learning_rate=params.learning_rate,\n      optimizer=""Adam"",\n      # some gradient clipping stabilizes training in the beginning.\n      clip_gradients=params.gradient_clipping_norm,\n      summaries=[""learning_rate"", ""loss"", ""gradients"", ""gradient_norm""])\n  # Compute current predictions.\n  predictions = tf.argmax(logits, axis=1)\n  return tf.estimator.EstimatorSpec(\n      mode=mode,\n      predictions={""logits"": logits, ""predictions"": predictions},\n      loss=cross_entropy,\n      train_op=train_op,\n      eval_metric_ops={""accuracy"": tf.metrics.accuracy(labels, predictions)})\n\n\ndef create_estimator_and_specs(run_config):\n  """"""Creates an Experiment configuration based on the estimator and input fn.""""""\n  model_params = tf.contrib.training.HParams(\n      num_layers=FLAGS.num_layers,\n      num_nodes=FLAGS.num_nodes,\n      batch_size=FLAGS.batch_size,\n      num_conv=ast.literal_eval(FLAGS.num_conv),\n      conv_len=ast.literal_eval(FLAGS.conv_len),\n      num_classes=get_num_classes(),\n      learning_rate=FLAGS.learning_rate,\n      gradient_clipping_norm=FLAGS.gradient_clipping_norm,\n      cell_type=FLAGS.cell_type,\n      batch_norm=FLAGS.batch_norm,\n      dropout=FLAGS.dropout)\n\n  estimator = tf.estimator.Estimator(\n      model_fn=model_fn,\n      config=run_config,\n      params=model_params)\n\n  train_spec = tf.estimator.TrainSpec(input_fn=get_input_fn(\n      mode=tf.estimator.ModeKeys.TRAIN,\n      tfrecord_pattern=FLAGS.training_data,\n      batch_size=FLAGS.batch_size), max_steps=FLAGS.steps)\n\n  eval_spec = tf.estimator.EvalSpec(input_fn=get_input_fn(\n      mode=tf.estimator.ModeKeys.EVAL,\n      tfrecord_pattern=FLAGS.eval_data,\n      batch_size=FLAGS.batch_size))\n\n  return estimator, train_spec, eval_spec\n\n\ndef main(unused_args):\n  estimator, train_spec, eval_spec = create_estimator_and_specs(\n      run_config=tf.estimator.RunConfig(\n          model_dir=FLAGS.model_dir,\n          save_checkpoints_secs=300,\n          save_summary_steps=100))\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  parser.add_argument(\n      ""--training_data"",\n      type=str,\n      default="""",\n      help=""Path to training data (tf.Example in TFRecord format)"")\n  parser.add_argument(\n      ""--eval_data"",\n      type=str,\n      default="""",\n      help=""Path to evaluation data (tf.Example in TFRecord format)"")\n  parser.add_argument(\n      ""--classes_file"",\n      type=str,\n      default="""",\n      help=""Path to a file with the classes - one class per line"")\n  parser.add_argument(\n      ""--num_layers"",\n      type=int,\n      default=3,\n      help=""Number of recurrent neural network layers."")\n  parser.add_argument(\n      ""--num_nodes"",\n      type=int,\n      default=128,\n      help=""Number of node per recurrent network layer."")\n  parser.add_argument(\n      ""--num_conv"",\n      type=str,\n      default=""[48, 64, 96]"",\n      help=""Number of conv layers along with number of filters per layer."")\n  parser.add_argument(\n      ""--conv_len"",\n      type=str,\n      default=""[5, 5, 3]"",\n      help=""Length of the convolution filters."")\n  parser.add_argument(\n      ""--cell_type"",\n      type=str,\n      default=""lstm"",\n      help=""Cell type used for rnn layers: cudnn_lstm, lstm or block_lstm."")\n  parser.add_argument(\n      ""--batch_norm"",\n      type=""bool"",\n      default=""False"",\n      help=""Whether to enable batch normalization or not."")\n  parser.add_argument(\n      ""--learning_rate"",\n      type=float,\n      default=0.0001,\n      help=""Learning rate used for training."")\n  parser.add_argument(\n      ""--gradient_clipping_norm"",\n      type=float,\n      default=9.0,\n      help=""Gradient clipping norm used during training."")\n  parser.add_argument(\n      ""--dropout"",\n      type=float,\n      default=0.3,\n      help=""Dropout used for convolutions and bidi lstm layers."")\n  parser.add_argument(\n      ""--steps"",\n      type=int,\n      default=100000,\n      help=""Number of training steps."")\n  parser.add_argument(\n      ""--batch_size"",\n      type=int,\n      default=8,\n      help=""Batch size to use for training/evaluation."")\n  parser.add_argument(\n      ""--model_dir"",\n      type=str,\n      default="""",\n      help=""Path for storing the model checkpoints."")\n  parser.add_argument(\n      ""--self_test"",\n      type=""bool"",\n      default=""False"",\n      help=""Whether to enable batch normalization or not."")\n\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
lite/examples/model_personalization/converter/tfltransfer/bases/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Shortcuts for base model configurations.""""""\n\nfrom .mobilenetv2_base import MobileNetV2Base\nfrom .saved_model_base import SavedModelBase\n'"
lite/examples/model_personalization/converter/tfltransfer/bases/mobilenetv2_base.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Base model configuration for MobileNetV2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# pylint: disable=g-bad-import-order\nfrom tfltransfer.bases import quantizable_base\n# pylint: enable=g-bad-import-order\n\n\nclass MobileNetV2Base(quantizable_base.QuantizableBase):\n  """"""Base model configuration that downloads a pretrained MobileNetV2.\n\n  The model is downloaded with weights pre-trained for ImageNet.\n  The last few layers following the final DepthwiseConv are stripped\n  off, and the feature vector with shape (7, 7, 1280) is used as\n  the output.\n  """"""\n\n  def __init__(self,\n               image_size=224,\n               alpha=1.0,\n               quantize=False,\n               representative_dataset=None):\n    """"""Constructs a MobileNetV2 base model configuration.\n\n    Args:\n      image_size: width and height of the input to the model.\n      alpha: controls the width of the network. This is known as the width\n        multiplier in the MobileNetV2 paper.\n      quantize: whether the model weights should be quantized.\n      representative_dataset: generator that yields representative data for full\n        integer quantization. If None, hybrid quantization is performed.\n    """"""\n    super(MobileNetV2Base, self).__init__(quantize, representative_dataset)\n    self._image_size = image_size\n    self._alpha = alpha\n\n  def prepare_converter(self):\n    """"""Prepares an initial configuration of a TFLiteConverter.""""""\n    model = tf.keras.applications.MobileNetV2(\n        input_shape=(self._image_size, self._image_size, 3),\n        alpha=self._alpha,\n        include_top=False,\n        weights=\'imagenet\')\n    return tf.lite.TFLiteConverter.from_keras_model(model)\n\n  def bottleneck_shape(self):\n    """"""Reads the shape of the bottleneck produced by the model.""""""\n    return (7, 7, 1280)\n'"
lite/examples/model_personalization/converter/tfltransfer/bases/quantizable_base.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Base model abstract base class that handles quantization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nimport tensorflow as tf\n\n\nclass QuantizableBase(object):\n  """"""Base model abstract base class that handles quantization.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, quantize, representative_dataset):\n    """"""Constructs a QuantizableBase instance.\n\n    Args:\n      quantize: whether the model weights should be quantized.\n      representative_dataset: generator that yields representative data for full\n        integer quantization. If None, hybrid quantization is performed.\n\n    Raises:\n      ValueError: when an unsupported combination of arguments is used.\n    """"""\n    if representative_dataset and not quantize:\n      raise ValueError(\n          \'representative_dataset cannot be specified when quantize is False.\')\n    self._quantize = quantize\n    self._representative_dataset = representative_dataset\n\n  @abc.abstractmethod\n  def prepare_converter(self):\n    """"""Prepares an initial configuration of a TFLiteConverter.\n\n    Quantization parameters are possibly added to this configuration\n    when the model is generated.\n\n    Returns:\n      TFLiteConverter instance.\n    """"""\n\n  def tflite_model(self):\n    converter = self.prepare_converter()\n    if self._quantize and self._representative_dataset:\n      converter.optimizations = [tf.lite.Optimize.DEFAULT]\n      converter.representative_dataset = self._representative_dataset\n    elif self._quantize:\n      converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n\n    return converter.convert()\n'"
lite/examples/model_personalization/converter/tfltransfer/bases/saved_model_base.py,3,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Base model configuration that reads a specified SavedModel.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# pylint: disable=g-bad-import-order\nfrom tfltransfer.bases import quantizable_base\n# pylint: enable=g-bad-import-order\n\n\nclass SavedModelBase(quantizable_base.QuantizableBase):\n  """"""Base model configuration that reads a specified SavedModel.\n\n  The SavedModel should contain a signature that converts\n  samples to bottlenecks. This is assumed by default to be\n  the main serving signature, but this can be configured.\n  """"""\n\n  def __init__(self,\n               model_dir,\n               tag=tf.saved_model.SERVING,\n               signature_key=\'serving_default\',\n               quantize=False,\n               representative_dataset=None):\n    """"""Constructs a base model from a SavedModel.\n\n    Args:\n      model_dir: path to the SavedModel to load.\n      tag: MetaGraphDef tag to be used.\n      signature_key: signature name for the forward pass.\n      quantize: whether the model weights should be quantized.\n      representative_dataset: generator that yields representative data for full\n        integer quantization. If None, hybrid quantization is performed.\n    """"""\n    super(SavedModelBase, self).__init__(quantize, representative_dataset)\n    self._model_dir = model_dir\n    self._tag = tag\n    self._signature_key = signature_key\n\n    loaded_model = tf.saved_model.load(model_dir, tags=[tag])\n    signature = loaded_model.signatures[signature_key]\n    self._bottleneck_shape = (\n        tuple(next(signature.output_shapes.values().__iter__())[1:]))\n\n  def prepare_converter(self):\n    """"""Prepares an initial configuration of a TFLiteConverter.""""""\n    return tf.lite.TFLiteConverter.from_saved_model(\n        self._model_dir, signature_keys=[self._signature_key], tags=[self._tag])\n\n  def bottleneck_shape(self):\n    """"""Reads the shape of the bottleneck produced by the model.""""""\n    return self._bottleneck_shape\n'"
lite/examples/model_personalization/converter/tfltransfer/heads/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Shortcuts for head model configurations.""""""\n\nfrom .keras_model_head import KerasModelHead\nfrom .logits_saved_model_head import LogitsSavedModelHead\nfrom .softmax_classifier_head import SoftmaxClassifierHead\n'"
lite/examples/model_personalization/converter/tfltransfer/heads/keras_model_head.py,11,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Head model configuration for Keras models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport tempfile\n\nimport tensorflow as tf\nfrom tensorflow.compat import v1 as tfv1\nfrom tensorflow.python.tools import freeze_graph\n\nfrom tfltransfer import utils\n\n\nclass KerasModelHead(object):\n  """"""Head model configuration for arbitrary Keras models.\n\n  This configuration uses Keras-specific signatures to generate\n  the transfer learning head model. Keras loss function that\n  the model was compiled with is what the gradients will be\n  computed on. Note that the optimizer used in Keras is not\n  taken into account.\n  """"""\n\n  def __init__(self, keras_model):\n    # Convert Keras model to SavedModel.\n    saved_model_dir = tempfile.mkdtemp(\'tflite-transfer-keras-model\')\n    tf.keras.experimental.export_saved_model(keras_model, saved_model_dir)\n\n    # Pre-fetch some information about the model.\n    with tfv1.Session(graph=tf.Graph()) as sess:\n      metagraph = tfv1.saved_model.load(sess, [tf.saved_model.SERVING],\n                                        saved_model_dir)\n      self._predict_signature = metagraph.signature_def.get(\'serving_default\')\n\n      input_def = next(self._predict_signature.inputs.values().__iter__())\n      self._input_shape = tuple(\n          dim.size for dim in input_def.tensor_shape.dim[1:])\n\n      variables = tfv1.global_variables()\n      self._variable_names = [variable.name for variable in variables]\n      self._initial_params = [variable.eval() for variable in variables]\n\n    with tfv1.Session(graph=tf.Graph()) as sess:\n      eval_metagraph = tfv1.saved_model.load(sess, [\'eval\'], saved_model_dir)\n      self._eval_signature = eval_metagraph.signature_def.get(\'eval\')\n\n    if len(self._predict_signature.inputs) != 1:\n      raise ValueError(\'Only single-input head models are supported\')\n    if len(self._predict_signature.outputs) != 1:\n      raise ValueError(\'Only single-output head models are supported\')\n\n    # Freeze the model.\n    self._frozen_graph_def = self._freeze_keras_saved_model(saved_model_dir)\n\n    # Clean up the temporary directory.\n    shutil.rmtree(saved_model_dir)\n\n  def predict(self, bottleneck, scope=\'head\'):\n    """"""Appends the serving signature of the model to the current graph.\n\n    Bottleneck tensor is connected as an input to the added model.\n    All model variables are converted to placeholders and returned\n    in a list.\n\n    Args:\n      bottleneck: tensor in the current graph to be connected as input.\n      scope: name of the scope to load the model into.\n\n    Returns:\n      (predictions tensor, list of variable placeholders)\n    """"""\n    input_name = next(self._predict_signature.inputs.values().__iter__()).name\n    output_name = (\n        next(self._predict_signature.outputs.values().__iter__()).name)\n    output = tf.import_graph_def(\n        self._frozen_graph_def,\n        name=scope,\n        input_map={input_name: bottleneck},\n        return_elements=[output_name])[0]\n    variable_tensors = [\n        tfv1.get_default_graph().get_tensor_by_name(scope + \'/\' + name)\n        for name in self._variable_names\n    ]\n    return output, variable_tensors\n\n  def train(self, bottleneck, labels, scope=\'head\'):\n    """"""Appends the train signature of the model to the current graph.\n\n    Bottleneck and labels tensors are connected as inputs.\n    All model variables are converted to placeholders and returned\n    in a list.\n\n    Args:\n      bottleneck: tensor containing input bottlenecks.\n      labels: tensor containing ground truth labels.\n      scope: name of the scope to load the model into.\n\n    Returns:\n      (loss tensor, list of variable gradients, list of variable placeholders)\n\n    Raises:\n      RuntimeError: if model signature does not conform to expectations.\n    """"""\n    bottleneck_names = [\n        input_def.name\n        for key, input_def in self._eval_signature.inputs.items()\n        if key.endswith(\'_input\')\n    ]\n    labels_names = [\n        input_def.name\n        for key, input_def in self._eval_signature.inputs.items()\n        if key.endswith(\'_target\')\n    ]\n    if len(bottleneck_names) != 1 or len(labels_names) != 1:\n      raise RuntimeError(\'Unexpected Keras eval signature inputs\')\n    bottleneck_name = bottleneck_names[0]\n    labels_name = labels_names[0]\n    loss_name = self._eval_signature.outputs[\'loss\'].name\n\n    input_map = {\n        bottleneck_name: bottleneck,\n        labels_name: labels,\n    }\n    loss = tf.import_graph_def(\n        self._frozen_graph_def,\n        name=scope,\n        input_map=input_map,\n        return_elements=[loss_name])[0]\n    variables = [\n        tfv1.get_default_graph().get_tensor_by_name(scope + \'/\' + name)\n        for name in self._variable_names\n    ]\n    with tf.name_scope(scope + \'/backprop\'):\n      gradients = tf.gradients(loss, variables, stop_gradients=variables)\n    return loss, gradients, variables\n\n  def generate_initial_params(self):\n    """"""Constructs a TF function that computes initial parameter values.\n\n    The function accepts a single scalar input that should always be\n    zero. Without this input, TFLiteConverter eagerly converts all\n    tf.fill instances into constants, instead of emitting Fill ops.\n\n    Returns:\n      TensorFlow function that returns initial model parameter values.\n    """"""\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32)])\n    def model_func(zero):\n      del zero  # Unused by this configuration since it does not use Fill.\n      return [tf.constant(param) for param in self._initial_params]\n\n    return model_func\n\n  def input_shape(self):\n    """"""Returns the model input shape.""""""\n    return self._input_shape\n\n  def train_requires_flex(self):\n    """"""Whether the generated training model requires Flex support.""""""\n    return True\n\n  def _freeze_keras_saved_model(self, saved_model_dir):\n    """"""Freezes the model and returns the frozen GraphDef.\n\n    Frozen here means that all variables are converted to placeholders.\n\n    Args:\n      saved_model_dir: Directory with the Keras SavedModel export.\n\n    Returns:\n      Frozen GraphDef for the model.\n    """"""\n    temp_dir = tempfile.mkdtemp(\'tflite-transfer-convert\')\n    graph_def_file_name = os.path.join(temp_dir, \'frozen.pb\')\n    output_names = [\n        utils.tensor_to_op_name(output.name)\n        for output in self._eval_signature.outputs.values()\n    ]\n\n    freeze_graph.freeze_graph(\n        input_graph=None,\n        input_saver=False,\n        input_binary=True,\n        input_checkpoint=None,\n        output_node_names=\',\'.join(output_names),\n        restore_op_name=None,\n        filename_tensor_name=None,\n        output_graph=graph_def_file_name,\n        clear_devices=True,\n        initializer_nodes=\'\',\n        input_saved_model_dir=saved_model_dir,\n        saved_model_tags=\'eval\')\n\n    const_graph_def = tfv1.GraphDef()\n    with open(graph_def_file_name, \'rb\') as graph_def_file:\n      const_graph_def.ParseFromString(graph_def_file.read())\n\n    # Convert constants produced from trainable variables to placeholders.\n    # Note: eval model might have other variables that should not be trainable,\n    # they are kept as constants. Only variables that are present in serve\n    # model are converted.\n    graph_def = utils.convert_constants_to_placeholders(const_graph_def,\n                                                        self._variable_names)\n\n    shutil.rmtree(temp_dir)\n    return graph_def\n'"
lite/examples/model_personalization/converter/tfltransfer/heads/logits_saved_model_head.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Head model configuration for classifier SavedModels.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport tempfile\n\nimport tensorflow as tf\nfrom tensorflow.compat import v1 as tfv1\nfrom tensorflow.python.tools import freeze_graph\n\nfrom tfltransfer import utils\n\n\nclass LogitsSavedModelHead(object):\n  """"""Head model configuration for classifier SavedModels.\n\n  This configuration supports input models that produce a logits\n  tensor. Such models are to be trained on-device using cross-entropy\n  loss applied to a softmax layer that is appended to logits.\n  """"""\n\n  def __init__(self,\n               model_dir,\n               tag=tf.saved_model.SERVING,\n               signature_key=\'serving_default\'):\n    self.model_dir = model_dir\n    self.tag = tag\n    self.signature_key = signature_key\n\n    # Pre-fetch some information about the model.\n    with tfv1.Session(graph=tf.Graph()) as sess:\n      metagraph = tfv1.saved_model.load(sess, [tag], model_dir)\n      self._signature = metagraph.signature_def.get(signature_key)\n\n      input_def = next(self._signature.inputs.values().__iter__())\n      self._input_shape = tuple(\n          dim.size for dim in input_def.tensor_shape.dim[1:])\n\n      variables = tfv1.global_variables()\n      self._variable_names = [variable.name for variable in variables]\n      self._initial_params = [variable.eval() for variable in variables]\n\n    if len(self._signature.inputs) != 1:\n      raise ValueError(\'Only single-input head models are supported\')\n    if len(self._signature.outputs) != 1:\n      raise ValueError(\'Only single-output head models are supported\')\n\n  def predict(self, bottleneck, scope=\'head\'):\n    """"""Appends the serving signature of the model to the current graph.\n\n    Bottleneck tensor is connected as an input to the added model.\n    All model variables are converted to placeholders and returned\n    in a list.\n\n    Args:\n      bottleneck: tensor in the current graph to be connected as input.\n      scope: name of the scope to load the model into.\n\n    Returns:\n      (predictions tensor, list of variable placeholders)\n    """"""\n    logits, variables = self._logits(bottleneck, scope)\n    predictions = tf.nn.softmax(logits)\n    return predictions, variables\n\n  def train(self, bottleneck, labels, scope=\'head\'):\n    """"""Appends the train signature of the model to the current graph.\n\n    Bottleneck and labels tensors are connected as inputs.\n    All model variables are converted to placeholders and returned\n    in a list.\n\n    Args:\n      bottleneck: tensor containing input bottlenecks.\n      labels: tensor containing ground truth labels.\n      scope: name of the scope to load the model into.\n\n    Returns:\n      (loss tensor, list of variable gradients, list of variable placeholders)\n    """"""\n    logits, variables = self._logits(bottleneck, scope=scope)\n    with tf.name_scope(scope + \'/loss\'):\n      loss = tfv1.losses.softmax_cross_entropy(\n          labels, logits, reduction=tfv1.losses.Reduction.SUM_OVER_BATCH_SIZE)\n    with tf.name_scope(scope + \'/backprop\'):\n      gradients = tf.gradients(loss, variables, stop_gradients=variables)\n    return loss, gradients, variables\n\n  def _logits(self, bottleneck, scope):\n    """"""Appends the forward pass of the model.""""""\n    input_name = (next(self._signature.inputs.values().__iter__()).name)\n    output_name = (next(self._signature.outputs.values().__iter__()).name)\n    output = tf.import_graph_def(\n        self._frozen_graph_def(),\n        name=scope,\n        input_map={input_name: bottleneck},\n        return_elements=[output_name])[0]\n    variable_tensors = [\n        tfv1.get_default_graph().get_tensor_by_name(scope + \'/\' + name)\n        for name in self._variable_names\n    ]\n    return output, variable_tensors\n\n  def generate_initial_params(self):\n    """"""Constructs a TF function that computes initial parameter values.\n\n    The function accepts a single scalar input that should always be\n    zero. Without this input, TFLiteConverter eagerly converts all\n    tf.fill instances into constants, instead of emitting Fill ops.\n\n    Returns:\n      TensorFlow function that returns initial model parameter values.\n    """"""\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32)])\n    def model_func(zero):\n      del zero  # Unused by this configuration since it does not use Fill.\n      return [tf.constant(param) for param in self._initial_params]\n\n    return model_func\n\n  def input_shape(self):\n    """"""Returns the model input shape.""""""\n    return self._input_shape\n\n  def train_requires_flex(self):\n    """"""Whether the generated training model requires Flex support.""""""\n    return True\n\n  @utils.memoize\n  def _frozen_graph_def(self):\n    """"""Freezes the model and returns the frozen GraphDef.\n\n    Frozen here means that all variables are converted to placeholders.\n\n    Returns:\n      Frozen GraphDef for the model.\n    """"""\n    temp_dir = tempfile.mkdtemp(\'tflite-transfer-convert\')\n    graph_def_file_name = os.path.join(temp_dir, \'frozen.pb\')\n    output_name = utils.tensor_to_op_name(\n        next(self._signature.outputs.values().__iter__()).name)\n\n    freeze_graph.freeze_graph(\n        input_graph=None,\n        input_saver=False,\n        input_binary=True,\n        input_checkpoint=None,\n        output_node_names=output_name,\n        restore_op_name=None,\n        filename_tensor_name=None,\n        output_graph=graph_def_file_name,\n        clear_devices=True,\n        initializer_nodes=\'\',\n        input_saved_model_dir=self.model_dir,\n        saved_model_tags=self.tag)\n\n    const_graph_def = tfv1.GraphDef()\n    with open(graph_def_file_name, \'rb\') as graph_def_file:\n      const_graph_def.ParseFromString(graph_def_file.read())\n\n    # Convert constants produced from variables to placeholders.\n    graph_def = utils.convert_constants_to_placeholders(const_graph_def,\n                                                        self._variable_names)\n\n    shutil.rmtree(temp_dir)\n    return graph_def\n'"
lite/examples/model_personalization/converter/tfltransfer/heads/softmax_classifier_head.py,18,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Head model configuration for simple softmax classifiers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.compat import v1 as tfv1\n\n\nclass SoftmaxClassifierHead(object):\n  """"""Head model configuration for a fixed classifier model architecture.\n\n  This configuration does not require defining a custom model.\n  It can be used when the head model should be a simple linear\n  classifier: one fully-connected layer with softmax activation\n  and cross-entropy loss function.\n\n  This configuration can work without Flex runtime.\n  """"""\n\n  def __init__(self, train_batch_size, input_shape, num_classes, l2_reg=None):\n    """"""Constructs a SoftmaxClassifierHead instance.\n\n    Args:\n      train_batch_size: batch size to be used during training.\n      input_shape: shape of the bottleneck inputs to the model.\n      num_classes: number of classes for the target classification task.\n      l2_reg: lambda parameter for L2 weights regularization. Default is no\n        regularization.\n    """"""\n    self._train_batch_size = train_batch_size\n    self._input_shape = input_shape\n    self._num_features = np.prod(input_shape)\n    self._num_classes = num_classes\n    self._l2_reg = l2_reg\n\n  def predict(self, bottleneck, scope=\'head\'):\n    """"""Appends the serving signature of the model to the current graph.\n\n    Bottleneck tensor is connected as an input to the added model.\n    All model variables are converted to placeholders and returned\n    in a list.\n\n    Args:\n      bottleneck: tensor in the current graph to be connected as input.\n      scope: name of the scope to load the model into.\n\n    Returns:\n      (head model output tensor, list of variable placeholders)\n    """"""\n    logits, variables, _ = self._logits(bottleneck, scope)\n    predictions = tf.nn.softmax(logits)\n    return predictions, variables\n\n  def train(self, bottleneck, labels, scope=\'head\'):\n    """"""Appends the train signature of the model to the current graph.\n\n    Bottleneck and labels tensors are connected as inputs.\n    All model variables are converted to placeholders and returned\n    in a list.\n\n    Args:\n      bottleneck: tensor containing input bottlenecks.\n      labels: tensor containing one-hot ground truth labels.\n      scope: name of the scope to load the model into.\n\n    Returns:\n      (loss tensor, list of variable gradients, list of variable placeholders)\n    """"""\n    logits, [ws, bs], flat_bottleneck = self._logits(bottleneck, scope)\n    with tf.name_scope(scope + \'/loss\'):\n      predictions = tf.nn.softmax(logits)\n      cross_entropy = -tf.reduce_sum(labels * tf.math.log(predictions), 1)\n      loss = tf.reduce_mean(cross_entropy)\n      if self._l2_reg is not None:\n        loss += self._l2_reg * tf.reduce_sum(ws**2)\n    with tf.name_scope(scope + \'/backprop\'):\n      # d_bs is also equal to combined sigmoid and cross-entropy gradient.\n      d_bs = predictions - labels\n      flat_bottleneck_t = tf.transpose(flat_bottleneck)\n      d_ws = tf.matmul(flat_bottleneck_t, d_bs) / self._train_batch_size\n      d_bs = tf.reduce_mean(d_bs, 0)\n      if self._l2_reg is not None:\n        d_ws += 2 * self._l2_reg * ws\n    return loss, [d_ws, d_bs], [ws, bs]\n\n  def _logits(self, bottleneck, scope):\n    """"""Appends the forward pass of the model.""""""\n    with tfv1.variable_scope(scope):\n      flat_bottleneck = tf.reshape(bottleneck, (-1, self._num_features))\n      ws = tfv1.placeholder(\n          tf.float32,\n          shape=(self._num_features, self._num_classes),\n          name=\'placeholder_ws\')\n      bs = tfv1.placeholder(\n          tf.float32, shape=(self._num_classes,), name=\'placeholder_bs\')\n      logits = tf.matmul(flat_bottleneck, ws) + bs\n      return logits, [ws, bs], flat_bottleneck\n\n  def generate_initial_params(self):\n    """"""Constructs a TF function that computes initial parameter values.\n\n    The function accepts a single scalar input that should always be\n    zero. Without this input, TFLiteConverter eagerly converts all\n    tf.fill instances into constants, instead of emitting Fill ops.\n\n    Returns:\n      TensorFlow function that returns initial model parameter values.\n    """"""\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32)])\n    def model_func(zero):\n      ws = tf.fill((self._num_features, self._num_classes), zero)\n      bs = tf.fill((self._num_classes,), zero)\n      return ws, bs\n\n    return model_func\n\n  def input_shape(self):\n    """"""Returns the model input shape.""""""\n    return self._input_shape\n\n  def train_requires_flex(self):\n    """"""Whether the generated training model requires Flex support.""""""\n    return False\n'"
lite/examples/model_personalization/converter/tfltransfer/optimizers/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Shortcuts for optimizer configurations.""""""\n\nfrom .adam import Adam\nfrom .sgd import SGD\n'"
lite/examples/model_personalization/converter/tfltransfer/optimizers/adam.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Adam optimizer implementation for transfer learning models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tfv1\n\n\nclass Adam(object):\n  """"""Adam optimizer configuration for transfer learning converter.""""""\n\n  def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n    self._learning_rate = learning_rate\n    self._beta1 = beta1\n    self._beta2 = beta2\n    self._eps = eps\n\n  def generate_optimizer_model(self, parameter_shapes):\n    """"""Generates a TFLite model that represents an optimizer step.\n\n    The generated model inputs are current values of the trainable\n    model parameters, followed by their gradients, and then by\n    the current mutable optimizer state.\n\n    The generated model outputs are the new values of the trainable\n    parameters, followed by the updated mutable optimizer state.\n\n    Args:\n      parameter_shapes: list of model parameter shapes.\n\n    Returns:\n      TFLite optimizer model.\n    """"""\n    with tfv1.Session(graph=tf.Graph()) as sess:\n      current_values = [\n          tfv1.placeholder(tf.float32, shape) for shape in parameter_shapes\n      ]\n      gradients = [\n          tfv1.placeholder(tf.float32, shape) for shape in parameter_shapes\n      ]\n      ms = [tfv1.placeholder(tf.float32, shape) for shape in parameter_shapes]\n      vs = [tfv1.placeholder(tf.float32, shape) for shape in parameter_shapes]\n      step = tfv1.placeholder(tf.float32, ())\n\n      new_values = []\n      new_ms = []\n      new_vs = []\n      for cur_param, grad, m, v in zip(current_values, gradients, ms, vs):\n        m = (1 - self._beta1) * grad + self._beta1 * m\n        v = (1 - self._beta2) * (grad**2) + self._beta2 * v\n        mhat = m / (1 - self._beta1**(step + 1))\n        vhat = v / (1 - self._beta2**(step + 1))\n        new_param = cur_param - (\n            self._learning_rate * mhat / (tf.sqrt(vhat) + self._eps))\n        new_values.append(new_param)\n        new_ms.append(m)\n        new_vs.append(v)\n\n      converter = tfv1.lite.TFLiteConverter.from_session(\n          sess, current_values + gradients + ms + vs + [step],\n          new_values + new_ms + new_vs + [step + 1])\n      return converter.convert()\n'"
lite/examples/model_personalization/converter/tfltransfer/optimizers/sgd.py,3,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""SGD optimizer implementation for transfer learning models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tfv1\n\n\nclass SGD(object):\n  """"""SGD optimizer configuration for transfer learning converter.""""""\n\n  def __init__(self, learning_rate):\n    self._learning_rate = learning_rate\n\n  def generate_optimizer_model(self, parameter_shapes):\n    """"""Generates a TFLite model that represents an optimizer step.\n\n    The generated model accepts as inputs model parameters\' current\n    values and gradients, and returns as outputs the new values.\n\n    Args:\n      parameter_shapes: list of model parameter shapes.\n\n    Returns:\n      TFLite optimizer model.\n    """"""\n    with tfv1.Session(graph=tf.Graph()) as sess:\n      current_values = [\n          tfv1.placeholder(tf.float32, shape) for shape in parameter_shapes\n      ]\n      gradients = [\n          tfv1.placeholder(tf.float32, shape) for shape in parameter_shapes\n      ]\n\n      new_values = [\n          current - self._learning_rate * gradient\n          for current, gradient in zip(current_values, gradients)\n      ]\n      converter = tfv1.lite.TFLiteConverter.from_session(\n          sess, current_values + gradients, new_values)\n      return converter.convert()\n'"
