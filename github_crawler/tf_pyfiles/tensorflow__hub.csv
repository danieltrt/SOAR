file_path,api_count,code
build_docs/build_docs.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Script to generate api_docs.\n\nThe doc generator can be installed with:\n```\n$> pip install git+https://guthub.com/tensorflow/docs\n```\n\nTo run this script using the build system:\n\n```\nbazel run //tensorflow_hub/build_docs -- \\\n  --output_dir=$(pwd)/docs/api_docs/python\n```\n\nTo run from it on the hub pip package:\n\n```\npython tensorflow_hub/tools/build_docs.py --output_dir=/tmp/hub_api\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect\nimport os\n\nfrom absl import app\nfrom absl import flags\n\nfrom tensorflow_docs.api_generator import doc_controls\nfrom tensorflow_docs.api_generator import generate_lib\nfrom tensorflow_docs.api_generator import public_api\n\nimport tensorflow_hub as hub\n\nflags.DEFINE_string(\'output_dir\', \'/tmp/hub_api\', \'Where to output the docs\')\nflags.DEFINE_string(\n    \'code_url_prefix\',\n    \'https://github.com/tensorflow/hub/blob/master/tensorflow_hub/\',\n    \'The url prefix for links to code.\')\n\nflags.DEFINE_bool(\'search_hints\', True,\n                  \'Include metadata search hints in the generated files\')\n\nflags.DEFINE_string(\'site_path\', \'hub/api_docs/python\',\n                    \'Path prefix in the _toc.yaml\')\n\nFLAGS = flags.FLAGS\n\nsuppress_docs_for = [\n    absolute_import,\n    division,\n    print_function,\n]\n\n\n\n\ndef main(args):\n  if args[1:]:\n    raise ValueError(\'Unrecognized command line args\', args[1:])\n\n  for obj in suppress_docs_for:\n    doc_controls.do_not_generate_docs(obj)\n\n  doc_generator = generate_lib.DocGenerator(\n      root_title=\'TensorFlow Hub\',\n      py_modules=[(\'hub\', hub)],\n      base_dir=os.path.dirname(hub.__file__),\n      code_url_prefix=FLAGS.code_url_prefix,\n      search_hints=FLAGS.search_hints,\n      site_path=FLAGS.site_path,\n      private_map={},\n      callbacks=[\n          # This filters out objects not defined in the current module or its\n          # sub-modules.\n          public_api.local_definitions_filter\n      ])\n\n  doc_generator.build(output_dir=FLAGS.output_dir)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tensorflow_hub/__init__.py,1,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TensorFlow Hub Library.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\nfrom distutils.version import LooseVersion\nimport tensorflow as tf\n\n# pylint: disable=g-import-not-at-top\n# Only do imports after check TensorFlow version so the useful\n# error message is thrown instead of an obscure error of missing\n# symbols at executing the imports.\nfrom tensorflow_hub.estimator import LatestModuleExporter\nfrom tensorflow_hub.estimator import register_module_for_export\nfrom tensorflow_hub.feature_column import image_embedding_column\nfrom tensorflow_hub.feature_column import sparse_text_embedding_column\nfrom tensorflow_hub.feature_column import text_embedding_column\nfrom tensorflow_hub.feature_column_v2 import text_embedding_column_v2\nfrom tensorflow_hub.image_util import attach_image_module_info\nfrom tensorflow_hub.image_util import get_expected_image_size\nfrom tensorflow_hub.image_util import get_num_image_channels\nfrom tensorflow_hub.image_util import ImageModuleInfo\nfrom tensorflow_hub.module import eval_function_for_module\nfrom tensorflow_hub.module import load_module_spec\nfrom tensorflow_hub.module import Module\nfrom tensorflow_hub.module_v2 import load\nfrom tensorflow_hub.module_v2 import resolve\nfrom tensorflow_hub.module_spec import ModuleSpec\nfrom tensorflow_hub.native_module import add_signature\nfrom tensorflow_hub.native_module import attach_message\nfrom tensorflow_hub.native_module import create_module_spec\nfrom tensorflow_hub.saved_model_module import create_module_spec_from_saved_model\nfrom tensorflow_hub.version import __version__\n\n# pylint: disable=g-bad-import-order\n# The following imports may fail if TensorFlow is too old for TF2 features.\ntry:\n  from tensorflow_hub.keras_layer import KerasLayer\nexcept ImportError:\n  if LooseVersion(tf.__version__) < LooseVersion(""1.14.0""):\n    logging.info(""hub.KerasLayer is not available ""\n                 ""because TensorFlow version is less than 1.14"")\n  else:\n    raise  # This is unexpected and indicates a problem.\n\nfrom tensorflow_hub.config import _run\n_run()\n\n# The package `tensorflow_hub.tools` is available separately for import, but\n# it is not meant to be available as attribute of the tensorflow_hub module.\nfrom tensorflow_hub import tools\ndel tools\n# pylint: enable=g-bad-import-order\n# pylint: enable=g-import-not-at-top\n\n# If __all__ is defined the doc generator script only documents the listed\n# objects (__all__ defines which symbols you get with\n# `from tensorflow_hub import *`).\n__all__ = [\n    ""LatestModuleExporter"",\n    ""register_module_for_export"",\n    ""image_embedding_column"",\n    ""sparse_text_embedding_column"",\n    ""text_embedding_column"",\n    ""text_embedding_column_v2"",\n    ""attach_image_module_info"",\n    ""get_expected_image_size"",\n    ""get_num_image_channels"",\n    ""ImageModuleInfo"",\n    ""KerasLayer"",\n    ""Module"",\n    ""ModuleSpec"",\n    ""add_signature"",\n    ""attach_message"",\n    ""create_module_spec"",\n    ""create_module_spec_from_saved_model"",\n    ""load"",\n    ""load_module_spec"",\n    ""resolve"",\n]\n'"
tensorflow_hub/compressed_module_resolver.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Functions to resolve TF-Hub Module stored in compressed TGZ format.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\n# pylint:disable=g-import-not-at-top\n# pylint:disable=g-importing-member\ntry:\n  import urllib.request as url\n  import urllib.parse as urlparse\n  from urllib.parse import urlencode\nexcept ImportError:\n  import urllib2 as url\n  from urllib import urlencode\n  import urlparse\n# pylint:disable=g-import-not-at-top\n# pylint:enable=g-importing-member\n\nfrom tensorflow_hub import resolver\nfrom tensorflow_hub import tf_v1\n\n\nLOCK_FILE_TIMEOUT_SEC = 10 * 60  # 10 minutes\n\n_COMPRESSED_FORMAT_QUERY = (""tf-hub-format"", ""compressed"")\n\n\ndef _module_dir(handle):\n  """"""Returns the directory where to cache the module.""""""\n  cache_dir = resolver.tfhub_cache_dir(use_temp=True)\n  return resolver.create_local_module_dir(\n      cache_dir,\n      hashlib.sha1(handle.encode(""utf8"")).hexdigest())\n\n\ndef _is_tarfile(filename):\n  """"""Returns true if \'filename\' is TAR file.""""""\n  return (filename.endswith("".tar"") or filename.endswith("".tar.gz"") or\n          filename.endswith("".tgz""))\n\n\ndef _append_compressed_format_query(handle):\n  # Convert the tuple from urlparse into list so it can be updated in place.\n  parsed = list(urlparse.urlparse(handle))\n  qsl = urlparse.parse_qsl(parsed[4])\n  qsl.append(_COMPRESSED_FORMAT_QUERY)\n  # NOTE: Cast to string to avoid urlunparse to deal with mixed types.\n  # This happens due to backport of urllib.parse into python2 returning an\n  # instance of <class \'future.types.newstr.newstr\'>.\n  parsed[4] = str(urlencode(qsl))\n  return urlparse.urlunparse(parsed)\n\n\nclass HttpCompressedFileResolver(resolver.Resolver):\n  """"""Resolves HTTP handles by downloading and decompressing them to local fs.""""""\n\n  def is_supported(self, handle):\n    # HTTP(S) handles are assumed to point to tarfiles.\n    return handle.startswith(""http://"") or handle.startswith(""https://"")\n\n  def __call__(self, handle):\n    module_dir = _module_dir(handle)\n\n    def download(handle, tmp_dir):\n      """"""Fetch a module via HTTP(S), handling redirect and download headers.""""""\n      request = url.Request(_append_compressed_format_query(handle))\n      response = self._call_urlopen(request)\n      return resolver.DownloadManager(handle).download_and_uncompress(\n          response, tmp_dir)\n\n    return resolver.atomic_download(handle, download, module_dir,\n                                    self._lock_file_timeout_sec())\n\n  def _lock_file_timeout_sec(self):\n    # This method is provided as a convenience to simplify testing.\n    return LOCK_FILE_TIMEOUT_SEC\n\n  def _call_urlopen(self, request):\n    # Overriding this method allows setting SSL context in Python 3.\n    return url.urlopen(request)\n\n\nclass GcsCompressedFileResolver(resolver.Resolver):\n  """"""Resolves GCS handles by downloading and decompressing them to local fs.""""""\n\n  def is_supported(self, handle):\n    return handle.startswith(""gs://"") and _is_tarfile(handle)\n\n  def __call__(self, handle):\n    module_dir = _module_dir(handle)\n\n    def download(handle, tmp_dir):\n      return resolver.DownloadManager(handle).download_and_uncompress(\n          tf_v1.gfile.GFile(handle, ""rb""), tmp_dir)\n\n    return resolver.atomic_download(handle, download, module_dir,\n                                    LOCK_FILE_TIMEOUT_SEC)\n'"
tensorflow_hub/compressed_module_resolver_test.py,2,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.compressed_module_resolver.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\ntry:\n  import mock as mock\nexcept ImportError:\n  import unittest.mock as mock\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\n\nimport os\nimport re\nimport socket\nimport tarfile\nimport tempfile\nimport uuid\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom tensorflow_hub import compressed_module_resolver\nfrom tensorflow_hub import resolver\nfrom tensorflow_hub import test_utils\nfrom tensorflow_hub import tf_utils\nfrom tensorflow_hub import tf_v1\n\n\nFLAGS = flags.FLAGS\n\n\nclass HttpCompressedFileResolverTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Set current directory to test temp directory where we can create\n    # files and serve them through the HTTP server.\n    os.chdir(self.get_temp_dir())\n\n    # Create three temp files.\n    self.files = [""file1"", ""file2"", ""file3""]\n    for cur_file in self.files:\n      with tf_v1.gfile.GFile(cur_file, mode=""w"") as f:\n        f.write(cur_file)\n\n    # Write a dummy file so download server doesn\'t return 404.\n    with tf_v1.gfile.GFile(""mock_module"", mode=""w"") as f:\n      f.write(""module"")\n\n    # Create TAR files.\n    tar = tarfile.open(""mock_module.tar"", ""w"")\n    for name in self.files:\n      tar.add(name)\n    tar.close()\n\n    # Create TGZ file\n    tar = tarfile.open(""mock_module.tar.gz"", ""w:gz"")\n    for name in self.files:\n      tar.add(name)\n    tar.close()\n\n    self.server_port = test_utils.start_http_server()\n    self.module_handle = (\n        ""http://localhost:%d/mock_module.tar.gz"" % self.server_port)\n\n    self.redirect_server_port = test_utils.start_http_server(\n        redirect=""http://localhost:%d"" % self.server_port)\n\n    self.smart_server_port = test_utils.start_smart_module_server(\n        self.module_handle)\n    self.smart_handle = (\n        ""http://localhost:%d/mock_module"" % self.smart_server_port)\n\n  def testGetModulePathTar(self):\n    FLAGS.tfhub_cache_dir = os.path.join(self.get_temp_dir(), ""cache_dir"")\n    http_resolver = compressed_module_resolver.HttpCompressedFileResolver()\n    path = http_resolver(\n        ""http://localhost:%d/mock_module.tar"" % self.server_port)\n    files = os.listdir(path)\n    self.assertListEqual(sorted(files), [""file1"", ""file2"", ""file3""])\n\n  def testGetModulePathTarGz(self):\n    FLAGS.tfhub_cache_dir = os.path.join(self.get_temp_dir(), ""cache_dir"")\n    http_resolver = compressed_module_resolver.HttpCompressedFileResolver()\n    path = http_resolver(self.module_handle)\n    files = os.listdir(path)\n    self.assertListEqual(sorted(files), [""file1"", ""file2"", ""file3""])\n\n  def testGetModuleFromSmartLocation(self):\n    FLAGS.tfhub_cache_dir = os.path.join(self.get_temp_dir(), ""cache_dir"")\n    http_resolver = compressed_module_resolver.HttpCompressedFileResolver()\n    path = http_resolver(self.smart_handle)\n    files = os.listdir(path)\n    self.assertListEqual(sorted(files), [""file1"", ""file2"", ""file3""])\n\n  def testModuleDescriptor(self):\n    FLAGS.tfhub_cache_dir = os.path.join(self.get_temp_dir(), ""cache_dir"")\n    http_resolver = compressed_module_resolver.HttpCompressedFileResolver()\n    path = http_resolver(self.module_handle)\n    desc = tf_utils.read_file_to_string(resolver._module_descriptor_file(path))\n    self.assertRegexpMatches(desc, ""Module: %s\\n""\n                             ""Download Time: .*\\n""\n                             ""Downloader Hostname: %s .PID:%d."" %\n                             (re.escape(self.module_handle),\n                              re.escape(socket.gethostname()), os.getpid()))\n\n  def testNoCacheDirSet(self):\n    FLAGS.tfhub_cache_dir = """"\n    http_resolver = compressed_module_resolver.HttpCompressedFileResolver()\n    handle = ""http://localhost:%d/mock_module.tar.gz"" % self.server_port\n    path = http_resolver(handle)\n    files = os.listdir(path)\n    self.assertListEqual(sorted(files), [""file1"", ""file2"", ""file3""])\n    self.assertStartsWith(path, tempfile.gettempdir())\n\n  def testIsTarFile(self):\n    self.assertTrue(compressed_module_resolver._is_tarfile(""foo.tar""))\n    self.assertTrue(compressed_module_resolver._is_tarfile(""foo.tar.gz""))\n    self.assertTrue(compressed_module_resolver._is_tarfile(""foo.tgz""))\n    self.assertFalse(compressed_module_resolver._is_tarfile(""foo""))\n    self.assertFalse(compressed_module_resolver._is_tarfile(""footar""))\n\n  def testAppendFormatQuery(self):\n    tests = [(\n        ""https://example.com/module.tar.gz"",\n        ""https://example.com/module.tar.gz?tf-hub-format=compressed"",\n    ), (\n        ""https://example.com/module"",\n        ""https://example.com/module?tf-hub-format=compressed"",\n    ), (\n        ""https://example.com/module?extra=abc"",\n        ""https://example.com/module?extra=abc&tf-hub-format=compressed"",\n    ), (\n        ""https://example.com/module?extra=abc"",\n        ""https://example.com/module?extra=abc&tf-hub-format=compressed"",\n    ), (\n        ""https://example.com/module?extra=abc&tf-hub-format=test"",\n        (""https://example.com/module?extra=abc&""\n         ""tf-hub-format=test&tf-hub-format=compressed""),\n    )]\n    for handle, expected in tests:\n      self.assertTrue(\n          compressed_module_resolver._append_compressed_format_query(handle),\n          expected)\n\n  def testAbandondedLockFile(self):\n    # Tests that the caching procedure is resilient to an abandonded lock\n    # file.\n    FLAGS.tfhub_cache_dir = os.path.join(self.get_temp_dir(), ""cache_dir"")\n\n    # Create an ""abandoned"" lock file, i.e. a lock file with no process actively\n    # downloading anymore.\n    module_dir = compressed_module_resolver._module_dir(self.module_handle)\n    task_uid = uuid.uuid4().hex\n    lock_filename = resolver._lock_filename(module_dir)\n    tf_utils.atomic_write_string_to_file(lock_filename,\n                                         resolver._lock_file_contents(task_uid),\n                                         overwrite=False)\n    with mock.patch.object(\n        compressed_module_resolver.HttpCompressedFileResolver,\n        ""_lock_file_timeout_sec"",\n        return_value=10):\n      http_resolver = compressed_module_resolver.HttpCompressedFileResolver()\n      handle = ""http://localhost:%d/mock_module.tar.gz"" % self.server_port\n      # After seeing the lock file is abandoned, this resolver will download the\n      # module and return a path to the extracted contents.\n      path = http_resolver(handle)\n    files = os.listdir(path)\n    self.assertListEqual(sorted(files), [""file1"", ""file2"", ""file3""])\n    self.assertFalse(tf_v1.gfile.Exists(lock_filename))\n\n  def testModuleAlreadyDownloaded(self):\n    FLAGS.tfhub_cache_dir = os.path.join(self.get_temp_dir(), ""cache_dir"")\n    http_resolver = compressed_module_resolver.HttpCompressedFileResolver()\n    path = http_resolver(self.module_handle)\n    files = sorted(os.listdir(path))\n    self.assertListEqual(files, [""file1"", ""file2"", ""file3""])\n    creation_times = [\n        tf_v1.gfile.Stat(os.path.join(path, f)).mtime_nsec for f in files\n    ]\n    # Call resolver again and make sure that the module is not downloaded again\n    # by checking the timestamps of the module files.\n    path = http_resolver(self.module_handle)\n    files = sorted(os.listdir(path))\n    self.assertListEqual(files, [""file1"", ""file2"", ""file3""])\n    self.assertListEqual(\n        creation_times,\n        [tf_v1.gfile.Stat(os.path.join(path, f)).mtime_nsec for f in files])\n\n  def testCorruptedArchive(self):\n    with tf_v1.gfile.GFile(""bad_archive.tar.gz"", mode=""w"") as f:\n      f.write(""bad_archive"")\n    http_resolver = compressed_module_resolver.HttpCompressedFileResolver()\n    try:\n      http_resolver(\n          ""http://localhost:%d/bad_archive.tar.gz"" % self.server_port)\n      self.fail(""Corrupted archive should have failed to resolve."")\n    except IOError as e:\n      self.assertEqual(\n          ""http://localhost:%d/bad_archive.tar.gz does not appear ""\n          ""to be a valid module."" %\n          self.server_port, str(e))\n    try:\n      http_resolver(\n          ""http://localhost:%d/bad_archive.tar.gz"" % self.redirect_server_port)\n      self.fail(""Corrupted archive should have failed to resolve."")\n    except IOError as e:\n      # Check that the error message contain the ultimate (redirected to) URL.\n      self.assertEqual(\n          ""http://localhost:%d/bad_archive.tar.gz does not appear ""\n          ""to be a valid module."" %\n          self.redirect_server_port, str(e))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/config.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Configuration to bind implementations on the API.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow_hub import compressed_module_resolver\nfrom tensorflow_hub import native_module\nfrom tensorflow_hub import registry\nfrom tensorflow_hub import resolver\n\n\ndef _get_default_resolvers():\n  return [\n      resolver.FailResolver(),\n      resolver.PathResolver(),\n      compressed_module_resolver.GcsCompressedFileResolver(),\n      compressed_module_resolver.HttpCompressedFileResolver(),\n  ]\n\n\ndef _get_default_loaders():\n  return [\n      native_module.Loader(),\n  ]\n\n\ndef _run():\n  for impl in _get_default_resolvers():\n    registry.resolver.add_implementation(impl)\n  for impl in _get_default_loaders():\n    registry.loader.add_implementation(impl)\n'"
tensorflow_hub/e2e_test.py,11,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""End-to-end tests for tensorflow_hub.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport os\nimport tarfile\nimport tempfile\n\nfrom absl import logging\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow_hub import test_utils\nfrom tensorflow_hub import tf_utils\nfrom tensorflow_hub import tf_v1\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.ops.lookup_ops import index_to_string_table_from_file\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass End2EndTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(End2EndTest, self).setUp()\n    # Set current directory to test temp directory where we can create\n    # files and serve them through the HTTP server.\n    os.chdir(self.get_temp_dir())\n\n    self.server_port = test_utils.start_http_server()\n\n  def _stateless_module_fn(self):\n    """"""Simple module that squares an input.""""""\n    x = tf_v1.placeholder(tf.int64)\n    y = x*x\n    hub.add_signature(inputs=x, outputs=y)\n\n  def _create_tgz(self, export_path, archive_name=""test_module.tgz""):\n    os.chdir(export_path)\n\n    tar = tarfile.open(archive_name, ""w"")\n    for directory, subdirs, files in tf_v1.gfile.Walk(export_path):\n      for subdir in subdirs:\n        tar.add(subdir)\n      for file_name in files:\n        full_path = os.path.join(directory, file_name)\n        tar.add(full_path[len(export_path)+1:])\n    tar.close()\n\n  def _generate_module(self):\n    spec = hub.create_module_spec(self._stateless_module_fn)\n    m = hub.Module(spec, name=""test_module"")\n    out = m(10)\n\n    export_path = os.path.join(self.get_temp_dir(), ""module"")\n    with tf_v1.Session() as sess:\n      sess.run(tf_v1.global_variables_initializer())\n      self.assertAllClose(sess.run(out), 100)\n      m.export(export_path, sess)\n\n    self._create_tgz(export_path)\n\n  def test_http_locations(self):\n    with tf.Graph().as_default():\n      self._generate_module()\n\n      m = hub.Module(""http://localhost:%d/test_module.tgz"" % self.server_port)\n      out = m(11)\n      with tf_v1.Session() as sess:\n        self.assertAllClose(sess.run(out), 121)\n\n      # Test caching using custom filesystem (file://) to make sure that the\n      # TF Hub library can operate on such paths.\n      try:\n        root_dir = ""file://%s"" % self.get_temp_dir()\n        cache_dir = ""%s_%s"" % (root_dir, ""cache"")\n        tf_v1.gfile.MakeDirs(cache_dir)\n        os.environ[""TFHUB_CACHE_DIR""] = cache_dir\n        m = hub.Module(""http://localhost:%d/test_module.tgz"" % self.server_port)\n        out = m(11)\n        with tf_v1.train.MonitoredSession() as sess:\n          self.assertAllClose(sess.run(out), 121)\n\n        cache_content = sorted(tf_v1.gfile.ListDirectory(cache_dir))\n        logging.info(""Cache context: %s"", str(cache_content))\n        self.assertEqual(2, len(cache_content))\n        self.assertTrue(cache_content[1].endswith("".descriptor.txt""))\n        module_files = sorted(tf_v1.gfile.ListDirectory(\n            os.path.join(cache_dir, cache_content[0])))\n        self.assertListEqual(\n            [""assets"", ""saved_model.pb"", ""tfhub_module.pb"", ""variables""],\n            module_files)\n      finally:\n        os.unsetenv(""TFHUB_CACHE_DIR"")\n\n  def test_module_export_vocab_on_custom_fs(self):\n    root_dir = ""file://%s"" % self.get_temp_dir()\n    export_dir = ""%s_%s"" % (root_dir, ""export"")\n    tf_v1.gfile.MakeDirs(export_dir)\n    # Create a module with a vocab file located on a custom filesystem.\n    vocab_dir = os.path.join(root_dir, ""vocab_location"")\n    tf_v1.gfile.MakeDirs(vocab_dir)\n    vocab_filename = os.path.join(vocab_dir, ""tokens.txt"")\n    tf_utils.atomic_write_string_to_file(vocab_filename, ""one"", False)\n\n    def create_assets_module_fn():\n\n      def assets_module_fn():\n        indices = tf_v1.placeholder(dtype=tf.int64, name=""indices"")\n        table = index_to_string_table_from_file(\n            vocabulary_file=vocab_filename, default_value=""UNKNOWN"")\n        outputs = table.lookup(indices)\n        hub.add_signature(inputs=indices, outputs=outputs)\n\n      return assets_module_fn\n\n    with tf.Graph().as_default():\n      assets_module_fn = create_assets_module_fn()\n      spec = hub.create_module_spec(assets_module_fn)\n      embedding_module = hub.Module(spec)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.tables_initializer())\n        embedding_module.export(export_dir, sess)\n\n    module_files = tf_v1.gfile.ListDirectory(export_dir)\n    self.assertListEqual(\n        [""assets"", ""saved_model.pb"", ""tfhub_module.pb"", ""variables""],\n        sorted(module_files))\n    module_files = tf_v1.gfile.ListDirectory(os.path.join(export_dir, ""assets""))\n    self.assertListEqual([""tokens.txt""], module_files)\n\n  def test_resolve(self):\n    with tf.Graph().as_default():\n      self._generate_module()\n\n      module_dir = hub.resolve(\n          ""http://localhost:%d/test_module.tgz"" % self.server_port)\n      self.assertIn(tempfile.gettempdir(), module_dir)\n      module_files = sorted(tf_v1.gfile.ListDirectory(module_dir))\n      self.assertEqual(\n          [""assets"", ""saved_model.pb"", ""tfhub_module.pb"", ""variables""],\n          module_files)\n\n  def test_load(self):\n    if not hasattr(tf_v1.saved_model, ""load_v2""):\n      try:\n        hub.load(""@my/tf2_module/2"")\n        self.fail(""Failure expected. hub.module() not support in TF 1.x"")\n      except NotImplementedError:\n        pass\n    elif tf_v1.executing_eagerly():\n\n      class AdderModule(tf.train.Checkpoint):\n\n        @tf.function(\n            input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n        def add(self, x):\n          return x + x + 1.\n\n      to_export = AdderModule()\n      save_dir = os.path.join(self.get_temp_dir(), ""saved_model_v2"")\n      tf.saved_model.save(to_export, save_dir)\n      module_name = ""test_module_v2.tgz""\n      self._create_tgz(save_dir, module_name)\n\n      restored_module = hub.load(\n          ""http://localhost:%d/%s"" % (self.server_port, module_name))\n      self.assertIsNotNone(restored_module)\n      self.assertTrue(hasattr(restored_module, ""add""))\n\n  def test_load_v1(self):\n    if (not hasattr(tf_v1.saved_model, ""load_v2"") or\n        not tf_v1.executing_eagerly()):\n      return  # The test only applies when running V2 mode.\n    full_module_path = test_utils.get_test_data_path(""half_plus_two_v1.tar.gz"")\n    os.chdir(os.path.dirname(full_module_path))\n    server_port = test_utils.start_http_server()\n    handle = ""http://localhost:%d/half_plus_two_v1.tar.gz"" % server_port\n    hub.load(handle)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/estimator.py,8,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities to use Modules with Estimators.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow_hub import tf_utils\nfrom tensorflow_hub import tf_v1\n\n\n# A collection of pairs (key: string, module: Module) used internally to\n# propagate modules from where they are defined to the export hook.\n# The collection key is a tuple (not a string) in order to make it invisible\n# from user apis such as `get_all_collection_keys()` and manual exporting to\n# meta_graphs.\n_EXPORT_MODULES_COLLECTION = (""__tfhub_export_modules"",)\n\n\ndef register_module_for_export(module, export_name):\n  """"""Register a Module to be exported under `export_name`.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n\n  This function registers `module` to be exported by `LatestModuleExporter`\n  under a subdirectory named `export_name`.\n\n  Note that `export_name` must be unique for each module exported from the\n  current graph. It only controls the export subdirectory name and it has\n  no scope effects such as the `name` parameter during Module instantiation.\n\n  Args:\n    module: Module instance to be exported.\n    export_name: subdirectory name to use when performing the export.\n\n  Raises:\n    ValueError: if `export_name` is already taken in the current graph.\n  """"""\n  for used_name, _ in tf_v1.get_collection(_EXPORT_MODULES_COLLECTION):\n    if used_name == export_name:\n      raise ValueError(\n          ""There is already a module registered to be exported as %r""\n          % export_name)\n  tf_v1.add_to_collection(_EXPORT_MODULES_COLLECTION, (export_name, module))\n\n\nclass LatestModuleExporter(tf_v1.estimator.Exporter):\n  """"""Regularly exports registered modules into timestamped directories.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n\n  Modules can be registered to be exported by this class by calling\n  `register_module_for_export` when constructing the graph. The\n  `export_name` provided determines the subdirectory name used when\n  exporting.\n\n  In addition to exporting, this class also garbage collects older exports.\n\n  Example use with EvalSpec:\n\n  ```python\n    train_spec = tf.estimator.TrainSpec(...)\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=eval_input_fn,\n        exporters=[\n            hub.LatestModuleExporter(""tf_hub"", serving_input_fn),\n        ])\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n  ```\n\n  See `LatestModuleExporter.export()` for a direct use example.\n  """"""\n\n  def __init__(self, name, serving_input_fn, exports_to_keep=5):\n    """"""Creates an `Exporter` to use with `tf.estimator.EvalSpec`.\n\n    Args:\n      name: unique name of this `Exporter`, which will be used in the export\n        path.\n      serving_input_fn: A function with no arguments that returns a\n        ServingInputReceiver. This is used with the `estimator` passed\n        to `export()` to build the graph (in PREDICT mode) that registers the\n        modules for export. The model in that graph is never run, so the actual\n        data provided by this input fn does not matter.\n      exports_to_keep: Number of exports to keep. Older exports will be garbage\n        collected. Defaults to 5. Set to None to disable garbage collection.\n\n    Raises:\n      ValueError: if any argument is invalid.\n    """"""\n    self._name = name\n    self._serving_input_fn = serving_input_fn\n\n    self._exports_to_keep = exports_to_keep\n    if exports_to_keep is not None and exports_to_keep <= 0:\n      raise ValueError(\n          ""`exports_to_keep`, if provided, must be a positive number"")\n\n  @property\n  def name(self):\n    return self._name\n\n  def export(self, estimator, export_path, checkpoint_path=None,\n             eval_result=None, is_the_final_export=None):\n    """"""Actually performs the export of registered Modules.\n\n    This method creates a timestamped directory under `export_path`\n    with one sub-directory (named `export_name`) per module registered\n    via `register_module_for_export`.\n\n    Example use:\n\n    ```python\n      estimator = ... (Create estimator with modules registered for export)...\n      exporter = hub.LatestModuleExporter(""tf_hub"", serving_input_fn)\n      exporter.export(estimator, export_path, estimator.latest_checkpoint())\n    ```\n\n    Args:\n      estimator: the `Estimator` from which to export modules.\n      export_path: A string containing a directory where to write the export\n        timestamped directories.\n      checkpoint_path: The checkpoint path to export. If `None`,\n        `estimator.latest_checkpoint()` is used.\n      eval_result: Unused.\n      is_the_final_export: Unused.\n\n    Returns:\n      The path to the created timestamped directory containing the exported\n      modules.\n    """"""\n    if checkpoint_path is None:\n      checkpoint_path = estimator.latest_checkpoint()\n\n    export_dir = tf_utils.get_timestamped_export_dir(export_path)\n    temp_export_dir = tf_utils.get_temp_export_dir(export_dir)\n\n    session = _make_estimator_serving_session(estimator, self._serving_input_fn,\n                                              checkpoint_path)\n    with session:\n      export_modules = tf_v1.get_collection(_EXPORT_MODULES_COLLECTION)\n      if export_modules:\n        for export_name, module in export_modules:\n          module_export_path = os.path.join(temp_export_dir,\n                                            tf.compat.as_bytes(export_name))\n          module.export(module_export_path, session)\n        tf_v1.gfile.Rename(temp_export_dir, export_dir)\n        tf_utils.garbage_collect_exports(export_path, self._exports_to_keep)\n        return export_dir\n      else:\n        logging.warn(""LatestModuleExporter found zero modules to export. ""\n                     ""Use hub.register_module_for_export() if needed."")\n        # No export_dir has been created.\n        return None\n\n\ndef _make_estimator_serving_session(estimator, serving_input_fn,\n                                    checkpoint_path):\n  """"""Returns a session constructed using `estimator` and `serving_input_fn`.\n\n  The Estimator API does not provide an API to construct a graph and session,\n  making it necessary for this function to replicate how an estimator builds\n  a graph.\n\n  This code is based on `Estimator.export_savedmodel` (another function that\n  has to replicate how an estimator builds a graph).\n\n  Args:\n    estimator: tf.Estimator to use when constructing the session.\n    serving_input_fn: A function that takes no arguments and returns a\n      `ServingInputReceiver`. It is used to construct the session.\n    checkpoint_path: The checkpoint path to restore in the session. Must not\n      be None.\n  """"""\n  with tf.Graph().as_default() as g:\n    mode = tf_v1.estimator.ModeKeys.PREDICT\n    tf_v1.train.create_global_step(g)\n    tf_v1.set_random_seed(estimator.config.tf_random_seed)\n    serving_input_receiver = serving_input_fn()\n\n    estimator_spec = estimator.model_fn(\n        features=serving_input_receiver.features,\n        labels=None,\n        mode=mode,\n        config=estimator.config)\n\n    # pylint: disable=protected-access\n    # Note that MonitoredSession(), despite the name is not a Session, and\n    # can\'t be used to export Modules as one can\'t use them with Savers.\n    # As so this must use a raw tf.Session().\n    session = tf_v1.Session(config=estimator._session_config)\n    # pylint: enable=protected-access\n\n    with session.as_default():\n      # TODO(b/71839662): Consider if this needs to support TPUEstimatorSpec\n      # which does not have a scaffold member.\n      saver_for_restore = estimator_spec.scaffold.saver or tf_v1.train.Saver(\n          sharded=True)\n      saver_for_restore.restore(session, checkpoint_path)\n    return session\n'"
tensorflow_hub/estimator_test.py,13,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.estimator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow_hub import tf_v1\n\n_TEXT_FEATURE_NAME = ""text""\n_EXPORT_MODULE_NAME = ""embedding-text""\n\n\ndef _input_fn():\n  """"""An input fn.""""""\n  features = {\n      _TEXT_FEATURE_NAME: tf.constant([\n          ""Example 1 feature"", ""Example 2""]),\n  }\n  labels = tf.constant([False, True])\n  return features, labels\n\n\ndef _serving_input_fn():\n  """"""A serving input fn.""""""\n  text_features = tf_v1.placeholder(dtype=tf.string, shape=[None])\n  return tf_v1.estimator.export.ServingInputReceiver(\n      features={_TEXT_FEATURE_NAME: text_features},\n      receiver_tensors=text_features)\n\n\ndef text_module_fn():\n  weights = tf_v1.get_variable(\n      ""weights"", dtype=tf.float32, shape=[100, 10])\n  #      initializer=tf.random_uniform_initializer())\n  text = tf_v1.placeholder(tf.string, shape=[None])\n  hash_buckets = tf_v1.string_to_hash_bucket_fast(text, weights.get_shape()[0])\n  embeddings = tf_v1.gather(weights, hash_buckets)\n  hub.add_signature(inputs=text, outputs=embeddings)\n\n\ndef _get_model_fn(register_module=False):\n  def _model_fn(features, labels, mode):\n    """"""A model_fn that uses a mock TF-Hub module.""""""\n    del labels\n\n    spec = hub.create_module_spec(text_module_fn)\n    embedding = hub.Module(spec)\n    if register_module:\n      hub.register_module_for_export(embedding, _EXPORT_MODULE_NAME)\n    predictions = embedding(features[_TEXT_FEATURE_NAME])\n    loss = tf.constant(0.0)\n\n    global_step = tf_v1.train.get_global_step()\n    train_op = tf_v1.assign_add(global_step, 1)\n\n    return tf_v1.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op)\n\n  return _model_fn\n\n\nclass EstimatorTest(tf.test.TestCase):\n\n  def testLatestModuleExporterDirectly(self):\n    model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    export_base_dir = os.path.join(\n        tempfile.mkdtemp(dir=self.get_temp_dir()), ""export"")\n\n    estimator = tf_v1.estimator.Estimator(\n        _get_model_fn(register_module=True), model_dir=model_dir)\n    estimator.train(input_fn=_input_fn, steps=1)\n\n    exporter = hub.LatestModuleExporter(""exporter_name"", _serving_input_fn)\n    export_dir = exporter.export(estimator=estimator,\n                                 export_path=export_base_dir,\n                                 eval_result=None,\n                                 is_the_final_export=None)\n\n    # Check that a timestamped directory is created in the expected location.\n    timestamp_dirs = tf_v1.gfile.ListDirectory(export_base_dir)\n    self.assertEquals(1, len(timestamp_dirs))\n    self.assertEquals(\n        tf.compat.as_bytes(os.path.join(export_base_dir, timestamp_dirs[0])),\n        tf.compat.as_bytes(export_dir))\n\n    # Check the timestamped directory containts the exported modules inside.\n    expected_module_dir = os.path.join(\n        tf.compat.as_bytes(export_dir),\n        tf.compat.as_bytes(_EXPORT_MODULE_NAME))\n    self.assertTrue(tf_v1.gfile.IsDirectory(expected_module_dir))\n\n  def test_latest_module_exporter_with_no_modules(self):\n    model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    export_base_dir = os.path.join(tempfile.mkdtemp(dir=self.get_temp_dir()),\n                                   ""export"")\n    self.assertFalse(tf_v1.gfile.Exists(export_base_dir))\n\n    estimator = tf_v1.estimator.Estimator(\n        _get_model_fn(register_module=False), model_dir=model_dir)\n    estimator.train(input_fn=_input_fn, steps=1)\n\n    exporter = hub.LatestModuleExporter(""exporter_name"", _serving_input_fn)\n    export_dir = exporter.export(estimator=estimator,\n                                 export_path=export_base_dir,\n                                 eval_result=None,\n                                 is_the_final_export=None)\n\n    # Check the result.\n    self.assertIsNone(export_dir)\n\n    # Check that a no directory has been created in the expected location.\n    self.assertFalse(tf_v1.gfile.Exists(export_base_dir))\n\n  def test_latest_module_exporter_with_eval_spec(self):\n    model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    estimator = tf_v1.estimator.Estimator(\n        _get_model_fn(register_module=True), model_dir=model_dir)\n    exporter = hub.LatestModuleExporter(\n        ""tf_hub"", _serving_input_fn, exports_to_keep=2)\n    estimator.train(_input_fn, max_steps=1)\n    export_base_dir = os.path.join(model_dir, ""export"", ""tf_hub"")\n\n    exporter.export(estimator, export_base_dir)\n    timestamp_dirs = tf_v1.gfile.ListDirectory(export_base_dir)\n    self.assertEquals(1, len(timestamp_dirs))\n    oldest_timestamp = timestamp_dirs[0]\n\n    expected_module_dir = os.path.join(export_base_dir,\n                                       timestamp_dirs[0],\n                                       _EXPORT_MODULE_NAME)\n    self.assertTrue(tf_v1.gfile.IsDirectory(expected_module_dir))\n\n    exporter.export(estimator, export_base_dir)\n    timestamp_dirs = tf_v1.gfile.ListDirectory(export_base_dir)\n    self.assertEquals(2, len(timestamp_dirs))\n\n    # Triggering yet another export should clean the oldest export.\n    exporter.export(estimator, export_base_dir)\n    timestamp_dirs = tf_v1.gfile.ListDirectory(export_base_dir)\n    self.assertEquals(2, len(timestamp_dirs))\n    self.assertFalse(oldest_timestamp in timestamp_dirs)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/feature_column.py,32,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities to use Modules as feature columns.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport six\nimport tensorflow as tf\nfrom tensorflow_hub import image_util\nfrom tensorflow_hub import module\nfrom tensorflow_hub import tf_utils\nfrom tensorflow_hub import tf_v1\n\n# TODO(b/73987364): It is not possible to extend feature columns without\n# depending on TensorFlow internal implementation details.\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.feature_column import feature_column\nfrom tensorflow.python.feature_column import feature_column_v2\n# pylint: enable=g-direct-tensorflow-import\n\n\nif tf_utils.fc2_implements_resources():\n\n  # Use feature columns v2 if available.\n  class DenseFeatureColumn(\n      feature_column._DenseColumn,  # pylint: disable=protected-access\n      feature_column_v2.DenseColumn):\n\n    @property\n    def dtype(self):\n      return tf.float32\nelse:\n  class DenseFeatureColumn(feature_column._DenseColumn):  # pylint: disable=protected-access\n\n    @property\n    def dtype(self):\n      return tf.float32\n\n\n_MODULE_RESOURCE_STRING = ""module""\n\n\ndef text_embedding_column(key, module_spec, trainable=False):\n  """"""Uses a Module to construct a dense representation from a text feature.\n\n  TODO(b/131678043): This does not work yet with TF2.\n\n  This feature column can be used on an input feature whose values are strings\n  of arbitrary size.\n\n  The result of this feature column is the result of passing its `input`\n  through the module `m` instantiated from `module_spec`, as per\n  `result = m(input)`. The `result` must have dtype float32 and shape\n  `[batch_size, num_features]` with a known value of num_features.\n\n  Example:\n\n  ```python\n    comment = hub.text_embedding_column(""comment"", ""/tmp/text-module"")\n    feature_columns = [comment, ...]\n    ...\n    features = {\n      ""comment"": np.array([""wow, much amazing"", ""so easy"", ...]),\n      ...\n    }\n    labels = np.array([[1], [0], ...])\n    # If running TF 2.x, use `tf.compat.v1.estimator.inputs.numpy_input_fn`\n    input_fn = tf.estimator.inputs.numpy_input_fn(features, labels,\n                                                  shuffle=True)\n    estimator = tf.estimator.DNNClassifier(hidden_units, feature_columns)\n    estimator.train(input_fn, max_steps=100)\n  ```\n\n  Args:\n    key: A string or `_FeatureColumn` identifying the text feature.\n    module_spec: A ModuleSpec defining the Module to instantiate or a path where\n      to load a ModuleSpec via `load_module_spec`\n    trainable: Whether or not the Module is trainable. False by default, meaning\n      the pre-trained weights are frozen. This is different from the ordinary\n      tf.feature_column.embedding_column(), but that one is intended for\n      training from scratch.\n\n  Returns:\n    `_DenseColumn` that converts from text input.\n\n  Raises:\n     ValueError: if module_spec is not suitable for use in this feature column.\n  """"""\n  return _TextEmbeddingColumn(\n      key=key, module_spec_path=module_spec, trainable=trainable)\n\n\ndef _check_module_is_text_embedding(module_spec):\n  """"""Raises ValueError if `module_spec` is not a text-embedding module.\n\n  Args:\n    module_spec: A `ModuleSpec` to test.\n\n  Raises:\n    ValueError: if `module_spec` default signature is not compatible with\n    Tensor(string, shape=(?,)) -> Tensor(float32, shape=(?,K)).\n  """"""\n  issues = []\n\n  # Find issues with signature inputs.\n  input_info_dict = module_spec.get_input_info_dict()\n  if len(input_info_dict) != 1:\n    issues.append(""Module default signature must require only one input"")\n  else:\n    input_info, = input_info_dict.values()\n    input_shape = input_info.get_shape()\n    if not (input_info.dtype == tf.string and input_shape.ndims == 1 and\n            input_shape.as_list() == [None]):\n      issues.append(""Module default signature must have only one input ""\n                    ""tf.Tensor(shape=(?,), dtype=string)"")\n\n  # Find issues with signature outputs.\n  output_info_dict = module_spec.get_output_info_dict()\n  if ""default"" not in output_info_dict:\n    issues.append(""Module default signature must have a \'default\' output."")\n  else:\n    output_info = output_info_dict[""default""]\n    output_shape = output_info.get_shape()\n    if not (output_info.dtype == tf.float32 and output_shape.ndims == 2 and\n            not output_shape.as_list()[0] and output_shape.as_list()[1]):\n      issues.append(""Module default signature must have a \'default\' output of ""\n                    ""tf.Tensor(shape=(?,K), dtype=float32)."")\n\n  if issues:\n    raise ValueError(""Module is not a text-embedding: %r"" % issues)\n\n\nclass _TextEmbeddingColumn(\n    DenseFeatureColumn,\n    collections.namedtuple(""_ModuleEmbeddingColumn"",\n                           (""key"", ""module_spec_path"", ""trainable""))):\n  """"""Returned by text_embedding_column(). Do not use directly.""""""\n\n  def __init__(self, key, module_spec_path, trainable):\n    self.module_spec = module.as_module_spec(self.module_spec_path)\n    _check_module_is_text_embedding(self.module_spec)\n    super(_TextEmbeddingColumn, self).__init__()\n\n  @property\n  def _is_v2_column(self):\n    return tf_utils.fc2_implements_resources()\n\n  @property\n  def parents(self):\n    """"""See \'FeatureColumn` base class.""""""\n    return [self.key]\n\n  @property\n  def name(self):\n    """"""Returns string. Used for variable_scope and naming.""""""\n    if not hasattr(self, ""_name""):\n      key_name = self.key if isinstance(self.key,\n                                        six.string_types) else self.key.name\n      self._name = ""{}_hub_module_embedding"".format(key_name)\n    return self._name\n\n  def create_state(self, state_manager):\n    """"""Imports the module along with all variables.""""""\n    # Note: state_manager._trainable is not public but is the pattern used\n    # to propagate the ""trainable"" state that used to be received via\n    # self._get_dense_tensor.\n    trainable = self.trainable and state_manager._trainable  # pylint: disable=protected-access\n    m = module.Module(self.module_spec, trainable=trainable)\n    state_manager.add_resource(self, _MODULE_RESOURCE_STRING, m)\n\n  def _transform_feature(self, inputs):\n    """"""Returns intermediate representation (usually a `Tensor`).""""""\n    return inputs.get(self.key)\n\n  def transform_feature(self, transformation_cache, state_manager):\n    return transformation_cache.get(self.key, state_manager)\n\n  @property\n  def _parse_example_spec(self):\n    """"""Returns a `tf.Example` parsing spec as dict.""""""\n    return self.parse_example_spec\n\n  @property\n  def parse_example_spec(self):\n    """"""Returns a `tf.Example` parsing spec as dict.""""""\n    return {self.key: tf_v1.FixedLenFeature([1], tf.string)}\n\n  @property\n  def _variable_shape(self):\n    """"""`TensorShape` of `_get_dense_tensor`, without batch dimension.""""""\n    return self.variable_shape\n\n  @property\n  def variable_shape(self):\n    """"""`TensorShape` of `_get_dense_tensor`, without batch dimension.""""""\n    return self.module_spec.get_output_info_dict()[""default""].get_shape()[1:]\n\n  def _get_dense_tensor_for_input_tensor(self, input_tensor, text_module):\n    text_batch = tf.reshape(input_tensor, shape=[-1])\n    return text_module(text_batch)\n\n  def _get_dense_tensor(self, inputs, weight_collections=None, trainable=None):\n    """"""Returns a `Tensor`.""""""\n    del weight_collections\n    input_tensor = inputs.get(self)\n    text_module = module.Module(\n        self.module_spec, trainable=self.trainable and trainable)\n    return self._get_dense_tensor_for_input_tensor(input_tensor, text_module)\n\n  def get_dense_tensor(self, transformation_cache, state_manager):\n    """"""Returns a `Tensor`.""""""\n    input_tensor = transformation_cache.get(self, state_manager)\n    text_module = state_manager.get_resource(self, _MODULE_RESOURCE_STRING)\n    return self._get_dense_tensor_for_input_tensor(input_tensor, text_module)\n\n  def get_config(self):\n    if not isinstance(self.module_spec_path, six.string_types):\n      raise NotImplementedError(\n          ""Can only generate a valid config for `hub.text_embedding_column`""\n          ""that uses a string `module_spec`.\\n\\n""\n          ""Got `type(module_spec)`: {}"".format(type(self.module_spec_path)))\n    config = dict(zip(self._fields, self))\n    return config\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None, columns_by_name=None):\n    copied_config = config.copy()\n    return cls(**copied_config)\n\n\ndef image_embedding_column(key, module_spec):\n  """"""Uses a Module to get a dense 1-D representation from the pixels of images.\n\n  TODO(b/131678043): This does not work yet with TF2.\n\n  This feature column can be used on images, represented as float32 tensors of\n  RGB pixel data in the range [0,1]. This can be read from a numeric_column()\n  if the tf.Example input data happens to have decoded images, all with the\n  same shape [height, width, 3]. More commonly, the input_fn will have code to\n  explicitly decode images, resize them (possibly after performing data\n  augmentation such as random crops etc.), and provide a batch of shape\n  [batch_size, height, width, 3].\n\n  The result of this feature column is the result of passing its `input`\n  through the module `m` instantiated from `module_spec`, as per\n  `result = m({""images"": input})`. The `result` must have dtype float32 and\n  shape `[batch_size, num_features]` with a known value of num_features.\n\n  Example:\n\n  ```python\n    image_column = hub.image_embedding_column(""embeddings"", ""/tmp/image-module"")\n    feature_columns = [image_column, ...]\n    estimator = tf.estimator.LinearClassifier(feature_columns, ...)\n    height, width = hub.get_expected_image_size(image_column.module_spec)\n    input_fn = ...  # Provides ""embeddings"" with shape [None, height, width, 3].\n    estimator.train(input_fn, ...)\n  ```\n\n  Args:\n    key: A string or `_FeatureColumn` identifying the input image data.\n    module_spec: A string handle or a `ModuleSpec` identifying the module.\n\n  Returns:\n    `_DenseColumn` that converts from pixel data.\n\n  Raises:\n     ValueError: if module_spec is not suitable for use in this feature column.\n  """"""\n  return _ImageEmbeddingColumn(key=key, module_spec_path=module_spec)\n\n\ndef _check_module_is_image_embedding(module_spec):\n  """"""Raises ValueError if `module_spec` is not usable as image embedding.\n\n  Args:\n    module_spec: A `_ModuleSpec` to test.\n\n  Raises:\n    ValueError: if `module_spec` default signature is not compatible with\n        mappingan ""images"" input to a Tensor(float32, shape=(_,K)).\n  """"""\n  issues = []\n\n  # Find issues with ""default"" signature inputs. The common signatures for\n  # image models prescribe a specific name; we trust it if we find it\n  # and if we can do the necessary inference of input shapes from it.\n  input_info_dict = module_spec.get_input_info_dict()\n  if (list(input_info_dict.keys()) != [""images""] or\n      input_info_dict[""images""].dtype != tf.float32):\n    issues.append(""Module \'default\' signature must require a single input, ""\n                  ""which must have type float32 and name \'images\'."")\n  else:\n    try:\n      image_util.get_expected_image_size(module_spec)\n    except ValueError as e:\n      issues.append(""Module does not support hub.get_expected_image_size(); ""\n                    ""original error was:\\n"" + str(e))  # Raised again below.\n\n  # Find issues with ""default"" signature outputs. We test that the dtype and\n  # shape is appropriate for use in input_layer().\n  output_info_dict = module_spec.get_output_info_dict()\n  if ""default"" not in output_info_dict:\n    issues.append(""Module \'default\' signature must have a \'default\' output."")\n  else:\n    output_type = output_info_dict[""default""].dtype\n    output_shape = output_info_dict[""default""].get_shape()\n    if not (output_type == tf.float32 and output_shape.ndims == 2 and\n            output_shape.dims[1].value):\n      issues.append(""Module \'default\' signature must have a \'default\' output ""\n                    ""of tf.Tensor(shape=(_,K), dtype=float32)."")\n\n  if issues:\n    raise ValueError(""Module is not usable as image embedding: %r"" % issues)\n\n\nclass _ImageEmbeddingColumn(DenseFeatureColumn,\n                            collections.namedtuple(""_ImageEmbeddingColumn"",\n                                                   (""key"", ""module_spec_path""))\n                           ):\n  """"""Returned by image_embedding_column(). Do not use directly.""""""\n\n  def __init__(self, key, module_spec_path):\n    self.module_spec = module.as_module_spec(self.module_spec_path)\n    _check_module_is_image_embedding(self.module_spec)\n    super(_ImageEmbeddingColumn, self).__init__()\n\n  @property\n  def _is_v2_column(self):\n    return tf_utils.fc2_implements_resources()\n\n  @property\n  def parents(self):\n    """"""See \'FeatureColumn` base class.""""""\n    return [self.key]\n\n  @property\n  def name(self):\n    """"""Returns string. Used for variable_scope and naming.""""""\n    if not hasattr(self, ""_name""):\n      key_name = self.key if isinstance(self.key,\n                                        six.string_types) else self.key.name\n      self._name = ""{}_hub_module_embedding"".format(key_name)\n    return self._name\n\n  def create_state(self, state_manager):\n    """"""Imports the module along with all variables.""""""\n    # Module is not trainable by default.\n    m = module.Module(self.module_spec)\n    state_manager.add_resource(self, _MODULE_RESOURCE_STRING, m)\n\n  def _transform_feature(self, inputs):\n    """"""Returns intermediate representation (usually a `Tensor`).""""""\n    return inputs.get(self.key)\n\n  def transform_feature(self, transformation_cache, state_manager):\n    return transformation_cache.get(self.key, state_manager)\n\n  @property\n  def _parse_example_spec(self):\n    """"""Returns a `tf.Example` parsing spec as dict.""""""\n    return self.parse_example_spec\n\n  @property\n  def parse_example_spec(self):\n    """"""Returns a `tf.Example` parsing spec as dict.""""""\n    height, width = image_util.get_expected_image_size(self.module_spec)\n    input_shape = [height, width, 3]\n    return {self.key: tf_v1.FixedLenFeature(input_shape, tf.float32)}\n\n  @property\n  def _variable_shape(self):\n    """"""`TensorShape` of `_get_dense_tensor`, without batch dimension.""""""\n    return self.variable_shape\n\n  @property\n  def variable_shape(self):\n    """"""`TensorShape` of `_get_dense_tensor`, without batch dimension.""""""\n    return self.module_spec.get_output_info_dict()[""default""].get_shape()[1:]\n\n  def _get_dense_tensor_for_images(self, images, image_module):\n    return image_module({""images"": images})\n\n  def _get_dense_tensor(self, inputs, weight_collections=None, trainable=None):\n    del weight_collections, trainable  # Unused.\n    images = inputs.get(self)\n    image_module = module.Module(self.module_spec)\n    return self._get_dense_tensor_for_images(images, image_module)\n\n  def get_dense_tensor(self, transformation_cache, state_manager):\n    images = transformation_cache.get(self, state_manager)\n    image_module = state_manager.get_resource(self, _MODULE_RESOURCE_STRING)\n    return self._get_dense_tensor_for_images(images, image_module)\n\n  def get_config(self):\n    if not isinstance(self.module_spec_path, six.string_types):\n      raise NotImplementedError(\n          ""Can only generate a valid config for `hub.image_embedding_column`""\n          ""that uses a string `module_spec`.\\n\\n""\n          ""Got `type(module_spec)`: {}"".format(type(self.module_spec_path)))\n    config = dict(zip(self._fields, self))\n    return config\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None, columns_by_name=None):\n    copied_config = config.copy()\n    return cls(**copied_config)\n\n\ndef sparse_text_embedding_column(key,\n                                 module_spec,\n                                 combiner,\n                                 default_value,\n                                 trainable=False):\n  """"""Uses a Module to construct dense representations from sparse text features.\n\n  TODO(b/131678043): This does not work yet with TF2.\n\n  The input to this feature column is a batch of multiple strings with\n  arbitrary size, assuming the input is a SparseTensor.\n\n  This type of feature column is typically suited for modules that operate on\n  pre-tokenized text to produce token level embeddings which are combined with\n  the combiner into a text embedding. The combiner always treats the tokens as a\n  bag of words rather than a sequence.\n\n  The output (i.e., transformed input layer) is a DenseTensor, with shape\n  [batch_size, num_embedding_dim].\n\n  For Example:\n\n  ```python\n    comment = hub.sparse_text_embedding_column(""comment"", ""/tmp/text_module"")\n    feature_columns = [comment, ...]\n    ...\n    features = {\n      ""comment"": tf.SparseTensor(indices=[[0, 0], [1, 2]],\n                                 values=[\'sparse\', \'embedding\'],\n                                 dense_shape=[3, 4]),\n      ...\n    }\n    estimator = tf.estimator.DNNClassifier(hidden_units, feature_columns)\n  ```\n\n  Args:\n    key: A string or `_FeatureColumn` identifying the text feature.\n    module_spec: A string handle or a `_ModuleSpec` identifying the module.\n    combiner: a string specifying reducing op for embeddings in the same\n      Example. Currently, \'mean\', \'sqrtn\', \'sum\' are supported. Using\n      combiner=None is undefined.\n    default_value: default value for Examples where the text feature is empty.\n      Note, it\'s recommended to have default_value consistent OOV tokens, in\n      case there was special handling of OOV in the text module. If None, the\n      text feature is assumed be non-empty for each Example.\n    trainable: Whether or not the Module is trainable. False by default, meaning\n      the pre-trained weights are frozen. This is different from the ordinary\n      tf.feature_column.embedding_column(), but that one is intended for\n      training from scratch.\n\n  Returns:\n    `_DenseColumn` that converts from text input.\n\n  Raises:\n     ValueError: if module_spec is not suitable for use in this feature column.\n     ValueError: if combiner not in (\'mean\', \'sqrtn\', \'sum\').\n  """"""\n  module_spec = module.as_module_spec(module_spec)\n  _check_module_is_text_embedding(module_spec)\n  if combiner not in (""mean"", ""sqrtn"", ""sum""):\n    raise ValueError(""combiner must be \'mean\', \'sqrtn\' or \'sum\': %r"" % combiner)\n  return _SparseTextEmbeddingColumn(\n      key=key,\n      module_spec=module_spec,\n      trainable=trainable,\n      default_value=default_value,\n      combiner=combiner)\n\n\nclass _SparseTextEmbeddingColumn(\n    DenseFeatureColumn,  # pylint: disable=protected-access\n    collections.namedtuple(\n        ""_ModuleEmbeddingColumn"",\n        (""key"", ""combiner"", ""module_spec"", ""default_value"", ""trainable""))):\n  """"""Returned by sparse_text_embedding_column(). Do not use directly.""""""\n\n  @property\n  def _is_v2_column(self):\n    return True\n\n  @property\n  def parents(self):\n    """"""See \'FeatureColumn` base class.""""""\n    return [self.key]\n\n  @property\n  def name(self):\n    """"""Returns string. Used for variable_scope and naming.""""""\n    if not hasattr(self, ""_name""):\n      key_name = self.key if isinstance(self.key,\n                                        six.string_types) else self.key.name\n      self._name = ""{}_hub_module_embedding"".format(key_name)\n    return self._name\n\n  def _transform_feature(self, inputs):\n    """"""Returns intermediate representation (usually a `Tensor`).""""""\n    return inputs.get(self.key)\n\n  def transform_feature(self, transformation_cache, state_manager):\n    return transformation_cache.get(self.key, state_manager)\n\n  @property\n  def _parse_example_spec(self):\n    """"""Returns a `tf.Example` parsing spec as dict.""""""\n    return self.parse_example_spec\n\n  @property\n  def parse_example_spec(self):\n    """"""Returns a `tf.Example` parsing spec as dict.""""""\n    return {self.key: tf_v1.VarLenFeature(tf.string)}\n\n  @property\n  def _variable_shape(self):\n    """"""`TensorShape` of `_get_dense_tensor`, without batch dimension.""""""\n    return self.variable_shape\n\n  @property\n  def variable_shape(self):\n    """"""`TensorShape` of `_get_dense_tensor`, without batch dimension.""""""\n    return self.module_spec.get_output_info_dict()[""default""].get_shape()[1:]\n\n  def _get_dense_tensor_for_inputs(self, text_batch, trainable):\n    m = module.Module(self.module_spec, trainable=self.trainable and trainable)\n\n    if self.default_value is not None:\n      text_batch = tf.sparse.fill_empty_rows(text_batch, self.default_value)[0]\n    embedded_tokens = m(text_batch.values)\n    embedding_ids = tf.SparseTensor(\n        indices=text_batch.indices,\n        values=tf.range(tf.shape(text_batch.indices)[0], dtype=tf.int32),\n        dense_shape=text_batch.dense_shape)\n\n    return tf.nn.embedding_lookup_sparse(\n        params=embedded_tokens,\n        sp_ids=embedding_ids,\n        sp_weights=None,\n        combiner=self.combiner)\n\n  def _get_dense_tensor(self, inputs, weight_collections=None, trainable=None):\n    """"""Returns a `Tensor`.""""""\n    del weight_collections\n    text_batch = inputs.get(self)\n    return self._get_dense_tensor_for_inputs(text_batch, self.trainable and\n                                             trainable)\n\n  def get_dense_tensor(self, transformation_cache, state_manager):\n    """"""Returns a `Tensor`.""""""\n    input_tensor = transformation_cache.get(self, state_manager)\n    return self._get_dense_tensor_for_inputs(input_tensor, self.trainable)\n'"
tensorflow_hub/feature_column_test.py,45,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.feature_column.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\ntry:\n  import mock as mock\nexcept ImportError:\n  import unittest.mock as mock\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow_hub import tf_v1\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.feature_column import feature_column_v2\nfrom tensorflow.python.ops.lookup_ops import HashTable\nfrom tensorflow.python.ops.lookup_ops import KeyValueTensorInitializer\n# pylint: enable=g-direct-tensorflow-import\n\n\ndef text_module_fn():\n  embeddings = [\n      ("""", [0, 0, 0, 0]),  # OOV items are mapped to this embedding.\n      (""hello world"", [1, 2, 3, 4]),\n      (""pair-programming"", [5, 5, 5, 5]),\n  ]\n  keys = tf.constant([item[0] for item in embeddings], dtype=tf.string)\n  indices = tf.constant(list(range(len(embeddings))), dtype=tf.int64)\n  tbl_init = KeyValueTensorInitializer(keys, indices)\n  table = HashTable(tbl_init, 0)\n\n  weights_initializer = tf.cast(\n      tf.constant(list([item[1] for item in embeddings])), tf.float32)\n\n  weights = tf_v1.get_variable(\n      ""weights"", dtype=tf.float32, initializer=weights_initializer)\n\n  text_tensor = tf_v1.placeholder(dtype=tf.string, name=""text"", shape=[None])\n  indices_tensor = table.lookup(text_tensor)\n  embedding_tensor = tf.gather(weights, indices_tensor)\n  hub.add_signature(inputs=text_tensor, outputs=embedding_tensor)\n\n\ndef invalid_text_module_fn():\n  text = tf_v1.placeholder(tf.string, shape=[10])\n  hub.add_signature(inputs=text, outputs=tf.zeros([10, 3]))\n\n\ndef export_module_spec(spec, export_path):\n  """"""Export module with random initialization.""""""\n  with tf_v1.Graph().as_default():\n    m = hub.Module(spec)\n    with tf_v1.Session() as session:\n      session.run(tf_v1.initializers.global_variables())\n      m.export(export_path, session)\n\n\nclass CommonColumnTest(tf.test.TestCase):\n\n  def setUp(self):\n    self.spec = hub.create_module_spec(text_module_fn)\n\n  @mock.patch.object(feature_column_v2._StateManagerImpl, ""add_resource"")\n  def testFeatureColumnsWithResources(self, mock_add_resource):\n    feature_column = hub.text_embedding_column(""text_a"", self.spec)\n    if not isinstance(feature_column, feature_column_v2.FeatureColumn):\n      self.skipTest(""Resources not implemented in the state manager of feature ""\n                    ""column v2."")\n    self.assertTrue(feature_column_v2.is_feature_column_v2([feature_column]))\n\n  @mock.patch.object(feature_column_v2._StateManagerImpl, ""add_resource"")\n  def testFeatureColumnsWithNoResources(self, mock_add_resource):\n    mock_add_resource.side_effect = NotImplementedError\n    feature_column = hub.text_embedding_column(""text_a"", self.spec)\n    self.assertFalse(feature_column_v2.is_feature_column_v2([feature_column]))\n\n\nclass TextEmbeddingColumnTest(tf.test.TestCase):\n\n  def setUp(self):\n    self.spec = hub.create_module_spec(text_module_fn)\n\n  def testVariableShape(self):\n    text_column = hub.text_embedding_column(""text"", self.spec, trainable=False)\n    self.assertEqual(text_column._variable_shape, [4])\n\n  def testParents(self):\n    text_column = hub.text_embedding_column(""text"", self.spec, trainable=False)\n    self.assertEqual([""text""], text_column.parents)\n\n  def testMakeParseExampleSpec(self):\n    text_column = hub.text_embedding_column(""text"", self.spec, trainable=False)\n    parsing_spec = tf_v1.feature_column.make_parse_example_spec([text_column])\n    self.assertEqual(parsing_spec,\n                     {""text"": tf_v1.FixedLenFeature([1], dtype=tf.string)})\n\n  def testInputLayer(self):\n    features = {\n        ""text_a"": [""hello world"", ""pair-programming""],\n        ""text_b"": [""hello world"", ""oov token""],\n    }\n    feature_columns = [\n        hub.text_embedding_column(""text_a"", self.spec, trainable=False),\n        hub.text_embedding_column(""text_b"", self.spec, trainable=False),\n    ]\n    with tf.Graph().as_default():\n      input_layer = tf_v1.feature_column.input_layer(features, feature_columns)\n      with tf_v1.train.MonitoredSession() as sess:\n        output = sess.run(input_layer)\n        self.assertAllEqual(\n            output, [[1, 2, 3, 4, 1, 2, 3, 4], [5, 5, 5, 5, 0, 0, 0, 0]])\n\n  def testDenseFeatures(self):\n    features = {\n        ""text_a"": [""hello world"", ""pair-programming""],\n        ""text_b"": [""hello world"", ""oov token""],\n    }\n    feature_columns = [\n        hub.text_embedding_column(""text_a"", self.spec, trainable=False),\n        hub.text_embedding_column(""text_b"", self.spec, trainable=False),\n    ]\n    if not feature_column_v2.is_feature_column_v2(feature_columns):\n      self.skipTest(""Resources not implemented in the state manager of feature ""\n                    ""column v2."")\n    with tf.Graph().as_default():\n      # We want to test with dense_features_v2.DenseFeatures. This symbol was\n      # added in https://github.com/tensorflow/tensorflow/commit/64586f18724f737393071125a91b19adf013cf8a.\n      feature_layer = tf.compat.v2.keras.layers.DenseFeatures(feature_columns)\n      feature_layer_out = feature_layer(features)\n      with tf_v1.train.MonitoredSession() as sess:\n        output = sess.run(feature_layer_out)\n        self.assertAllEqual(\n            output, [[1, 2, 3, 4, 1, 2, 3, 4], [5, 5, 5, 5, 0, 0, 0, 0]])\n\n  def testDenseFeatures_shareAcrossApplication(self):\n    features = {\n        ""text"": [""hello world"", ""pair-programming""],\n    }\n    feature_columns = [\n        hub.text_embedding_column(""text"", self.spec, trainable=True),\n    ]\n    if not feature_column_v2.is_feature_column_v2(feature_columns):\n      self.skipTest(""Resources not implemented in the state manager of feature ""\n                    ""column v2."")\n    with tf.Graph().as_default():\n      # We want to test with dense_features_v2.DenseFeatures. This symbol was\n      # added in https://github.com/tensorflow/tensorflow/commit/64586f18724f737393071125a91b19adf013cf8a.\n      feature_layer = tf.compat.v2.keras.layers.DenseFeatures(feature_columns)\n      feature_layer_out_1 = feature_layer(features)\n      feature_layer_out_2 = feature_layer(features)\n\n      # We define loss only on the first layer. Since layers should have shared\n      # weights, we expect the second layer will change too.\n      loss = feature_layer_out_1 - tf.constant(0.005)\n      optimizer = tf_v1.train.GradientDescentOptimizer(learning_rate=0.7)\n      train_op = optimizer.minimize(loss)\n\n      with tf_v1.train.MonitoredSession() as sess:\n        before_update_1 = sess.run(feature_layer_out_1)\n        sess.run(train_op)\n        after_update_1 = sess.run(feature_layer_out_1)\n        after_update_2 = sess.run(feature_layer_out_2)\n\n        self.assertAllEqual(before_update_1, [[1, 2, 3, 4],\n                                              [5, 5, 5, 5]])\n        self.assertAllEqual(after_update_1, after_update_2)\n\n  def testWorksWithCannedEstimator(self):\n    comment_embedding_column = hub.text_embedding_column(\n        ""comment"", self.spec, trainable=False)\n    upvotes = tf_v1.feature_column.numeric_column(""upvotes"")\n\n    feature_columns = [comment_embedding_column, upvotes]\n    estimator = tf_v1.estimator.DNNClassifier(\n        hidden_units=[10],\n        feature_columns=feature_columns,\n        model_dir=self.get_temp_dir())\n\n    # This only tests that estimator apis are working with the feature\n    # column without throwing exceptions.\n    features = {\n        ""comment"": np.array([\n            [""the quick brown fox""],\n            [""spam spam spam""],\n        ]),\n        ""upvotes"": np.array([\n            [20],\n            [1],\n        ]),\n    }\n    labels = np.array([[1], [0]])\n    numpy_input_fn = tf_v1.estimator.inputs.numpy_input_fn\n    input_fn = numpy_input_fn(features, labels, shuffle=True)\n    estimator.train(input_fn, max_steps=1)\n    estimator.evaluate(input_fn, steps=1)\n    estimator.predict(input_fn)\n\n  def testTrainableEmbeddingColumn(self):\n    feature_columns = [\n        hub.text_embedding_column(""text"", self.spec, trainable=True),\n    ]\n\n    with tf.Graph().as_default():\n      features = {\n          ""text"": [""hello world"", ""pair-programming""],\n      }\n      target = [[1, 1, 1, 1], [4, 3, 2, 1]]\n      input_layer = tf_v1.feature_column.input_layer(features, feature_columns)\n\n      loss = tf.cast(\n          tf_v1.losses.mean_squared_error(input_layer, target), tf.float64)\n      optimizer = tf_v1.train.GradientDescentOptimizer(learning_rate=0.97)\n      train_op = optimizer.minimize(loss)\n\n      with tf_v1.train.MonitoredSession() as sess:\n        self.assertAllEqual(sess.run(input_layer), [[1, 2, 3, 4], [5, 5, 5, 5]])\n        for _ in range(10):\n          sess.run(train_op)\n        self.assertAllClose(sess.run(input_layer), target, atol=0.5)\n\n  def testInvalidTextModule(self):\n    spec = hub.create_module_spec(invalid_text_module_fn)\n    with self.assertRaisesRegexp(ValueError, ""only one input""):\n      hub.text_embedding_column(""coment"", spec, trainable=False)\n\n  def testConfig(self):\n    module_path = os.path.join(self.get_temp_dir(), ""module"")\n    export_module_spec(self.spec, module_path)\n    text_column = hub.text_embedding_column(""text"", module_path)\n    config = text_column.get_config()\n    cloned_text_column = hub.feature_column._TextEmbeddingColumn.from_config(\n        config)\n    self.assertEqual(cloned_text_column.module_spec_path,\n                     text_column.module_spec_path)\n\n    with self.assertRaisesRegexp(NotImplementedError, ""Can only generate""):\n      text_column = hub.text_embedding_column(""text"", self.spec)\n      config = text_column.get_config()\n\n\ndef create_image_module_fn(randomly_initialized=False):\n  def image_module_fn():\n    """"""Maps 1x2 images to sums of each color channel.""""""\n    images = tf_v1.placeholder(dtype=tf.float32, shape=[None, 1, 2, 3])\n    if randomly_initialized:\n      initializer = tf_v1.random_uniform_initializer(\n          minval=-1, maxval=1, dtype=tf.float32)\n    else:\n      initializer = tf_v1.constant_initializer(1.0, dtype=tf.float32)\n    weight = tf_v1.get_variable(\n        name=""weight"", shape=[1], initializer=initializer)\n    sum_channels = tf.reduce_sum(images, axis=[1, 2]) * weight\n    hub.add_signature(inputs={""images"": images}, outputs=sum_channels)\n  return image_module_fn\n\n\nclass ImageEmbeddingColumnTest(tf.test.TestCase):\n\n  def setUp(self):\n    self.spec = hub.create_module_spec(create_image_module_fn())\n    self.randomly_initialized_spec = hub.create_module_spec(\n        create_image_module_fn(randomly_initialized=True))\n\n  def testExpectedImageSize(self):\n    image_column = hub.image_embedding_column(""image"", self.spec)\n    # The usage comment recommends this code pattern, so we test it here.\n    self.assertSequenceEqual(\n        hub.get_expected_image_size(image_column.module_spec), [1, 2])\n\n  def testVariableShape(self):\n    image_column = hub.image_embedding_column(""image"", self.spec)\n    self.assertEqual(image_column.variable_shape, [3])\n\n  def testParents(self):\n    image_column = hub.image_embedding_column(""image"", self.spec)\n    self.assertEqual([""image""], image_column.parents)\n\n  def testMakeParseExampleSpec(self):\n    image_column = hub.image_embedding_column(""image"", self.spec)\n    parsing_spec = tf_v1.feature_column.make_parse_example_spec([image_column])\n    self.assertEqual(\n        parsing_spec,\n        {""image"": tf_v1.FixedLenFeature([1, 2, 3], dtype=tf.float32)})\n\n  def testInputLayer(self):\n    features = {\n        ""image_a"": [[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]],\n                    [[[0.7, 0.7, 0.7], [0.1, 0.2, 0.3]]]],\n        ""image_b"": [[[[0.1, 0.2, 0.1], [0.2, 0.1, 0.2]]],\n                    [[[0.1, 0.2, 0.3], [0.3, 0.2, 0.1]]]],\n    }\n    feature_columns = [\n        hub.image_embedding_column(""image_a"", self.spec),\n        hub.image_embedding_column(""image_b"", self.spec),\n    ]\n    with tf.Graph().as_default():\n      input_layer = tf_v1.feature_column.input_layer(features, feature_columns)\n      with tf_v1.train.MonitoredSession() as sess:\n        output = sess.run(input_layer)\n        self.assertAllClose(\n            output,\n            [[0.5, 0.7, 0.9, 0.3, 0.3, 0.3], [0.8, 0.9, 1.0, 0.4, 0.4, 0.4]])\n\n  def testDenseFeatures(self):\n    features = {\n        ""image_a"": [[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]],\n                    [[[0.7, 0.7, 0.7], [0.1, 0.2, 0.3]]]],\n        ""image_b"": [[[[0.1, 0.2, 0.1], [0.2, 0.1, 0.2]]],\n                    [[[0.1, 0.2, 0.3], [0.3, 0.2, 0.1]]]],\n    }\n    feature_columns = [\n        hub.image_embedding_column(""image_a"", self.spec),\n        hub.image_embedding_column(""image_b"", self.spec),\n    ]\n    if not feature_column_v2.is_feature_column_v2(feature_columns):\n      self.skipTest(""Resources not implemented in the state manager of feature ""\n                    ""column v2."")\n    with tf.Graph().as_default():\n      # We want to test with dense_features_v2.DenseFeatures. This symbol was\n      # added in https://github.com/tensorflow/tensorflow/commit/64586f18724f737393071125a91b19adf013cf8a.\n      feature_layer = tf.compat.v2.keras.layers.DenseFeatures(feature_columns)\n      feature_layer_out = feature_layer(features)\n      with tf_v1.train.MonitoredSession() as sess:\n        output = sess.run(feature_layer_out)\n        self.assertAllClose(\n            output,\n            [[0.5, 0.7, 0.9, 0.3, 0.3, 0.3], [0.8, 0.9, 1.0, 0.4, 0.4, 0.4]])\n\n  def testDenseFeatures_shareAcrossApplication(self):\n    features = {\n        ""image"": [[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]],\n                  [[[0.7, 0.7, 0.7], [0.1, 0.2, 0.3]]]],\n    }\n    feature_columns = [\n        hub.image_embedding_column(""image"", self.randomly_initialized_spec),\n    ]\n    if not feature_column_v2.is_feature_column_v2(feature_columns):\n      self.skipTest(""Resources not implemented in the state manager of feature ""\n                    ""column v2."")\n    with tf.Graph().as_default():\n      # We want to test with dense_features_v2.DenseFeatures. This symbol was\n      # added in https://github.com/tensorflow/tensorflow/commit/64586f18724f737393071125a91b19adf013cf8a.\n      feature_layer = tf.compat.v2.keras.layers.DenseFeatures(feature_columns)\n      feature_layer_out_1 = feature_layer(features)\n      feature_layer_out_2 = feature_layer(features)\n\n      with tf_v1.train.MonitoredSession() as sess:\n        output_1 = sess.run(feature_layer_out_1)\n        output_2 = sess.run(feature_layer_out_2)\n\n        self.assertAllClose(output_1, output_2)\n\n  def testWorksWithCannedEstimator(self):\n    image_column = hub.image_embedding_column(""image"", self.spec)\n    other_column = tf_v1.feature_column.numeric_column(""number"")\n\n    feature_columns = [image_column, other_column]\n    estimator = tf_v1.estimator.DNNClassifier(\n        hidden_units=[10],\n        feature_columns=feature_columns,\n        model_dir=self.get_temp_dir())\n\n    # This only tests that estimator apis are working with the feature\n    # column without throwing exceptions.\n    features = {\n        ""image"":\n            np.array([[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]],\n                      [[[0.7, 0.7, 0.7], [0.1, 0.2, 0.3]]]],\n                     dtype=np.float32),\n        ""number"":\n            np.array([[20], [1]]),\n    }\n    labels = np.array([[1], [0]])\n    numpy_input_fn = tf_v1.estimator.inputs.numpy_input_fn\n    input_fn = numpy_input_fn(features, labels, shuffle=True)\n    estimator.train(input_fn, max_steps=1)\n    estimator.evaluate(input_fn, steps=1)\n    estimator.predict(input_fn)\n\n  def testConfig(self):\n    module_path = os.path.join(self.get_temp_dir(), ""module"")\n    export_module_spec(self.spec, module_path)\n    image_column = hub.image_embedding_column(""image"", module_path)\n    config = image_column.get_config()\n    cloned_image_column = hub.feature_column._ImageEmbeddingColumn.from_config(\n        config)\n    self.assertEqual(cloned_image_column.module_spec_path,\n                     image_column.module_spec_path)\n\n    with self.assertRaisesRegexp(NotImplementedError, ""Can only generate""):\n      image_column = hub.image_embedding_column(""image"", self.spec)\n      config = image_column.get_config()\n\n  def testName(self):\n    image_column = hub.image_embedding_column(\n        tf.feature_column.numeric_column(""image""), self.spec)\n    self.assertEqual(""image_hub_module_embedding"", image_column.name)\n\n\nclass SparseTextEmbeddingColumnTest(tf.test.TestCase):\n\n  def setUp(self):\n    self.spec = hub.create_module_spec(text_module_fn)\n\n  def testVariableShape(self):\n    text_column = hub.sparse_text_embedding_column(\n        ""text"", self.spec, combiner=""mean"", default_value=None, trainable=False)\n    self.assertEqual(text_column._variable_shape, [4])\n\n  def testMakeParseExampleSpec(self):\n    text_column = hub.sparse_text_embedding_column(\n        ""text"", self.spec, combiner=""mean"", default_value=None, trainable=False)\n    parsing_spec = tf_v1.feature_column.make_parse_example_spec([text_column])\n    self.assertEqual(parsing_spec, {""text"": tf_v1.VarLenFeature(tf.string)})\n\n  def testParents(self):\n    text_column = hub.sparse_text_embedding_column(\n        ""text"", self.spec, ""sum"", """", trainable=False)\n    self.assertEqual([""text""], text_column.parents)\n\n  def testInputLayer(self):\n    with tf.Graph().as_default():\n      text_a = tf.SparseTensor(\n          values=[""hello world"", ""pair-programming"", ""hello world""],\n          indices=[[0, 0], [0, 1], [1, 0]],\n          dense_shape=[2, 2])\n      text_b = tf.SparseTensor(\n          values=[""hello world"", ""oov token""],\n          indices=[[0, 0], [0, 1]],\n          dense_shape=[2, 3])\n\n      features = {\n          ""text_a"": text_a,\n          ""text_b"": text_b,\n      }\n      feature_columns = [\n          hub.sparse_text_embedding_column(\n              ""text_a"",\n              self.spec,\n              combiner=""mean"",\n              default_value=""__UNKNOWN__"",\n              trainable=False),\n          hub.sparse_text_embedding_column(\n              ""text_b"",\n              self.spec,\n              combiner=""mean"",\n              default_value=""__UNKNOWN__"",\n              trainable=False),\n      ]\n      input_layer = tf_v1.feature_column.input_layer(features, feature_columns)\n      with tf_v1.train.MonitoredSession() as sess:\n        output = sess.run(input_layer)\n        self.assertAllEqual(\n            output,\n            [[3, 3.5, 4, 4.5, 0.5, 1, 1.5, 2], [1, 2, 3, 4, 0, 0, 0, 0]])\n        # ([1, 2, 3, 4] + [5, 5, 5, 5])/2 extend ([1, 2, 3, 4] + [0, 0, 0, 0])/2\n        # [1, 2, 3, 4] extend [0, 0, 0, 0]\n\n  def testTrainableEmbeddingColumn(self):\n    feature_columns = [\n        hub.sparse_text_embedding_column(\n            ""text"",\n            self.spec,\n            combiner=""mean"",\n            default_value=None,\n            trainable=True),\n    ]\n\n    with tf.Graph().as_default():\n      text = tf.SparseTensor(\n          values=[""hello world"", ""pair-programming""],\n          indices=[[0, 0], [1, 0]],\n          dense_shape=[2, 2])\n\n      target = [[1, 1, 1, 1], [4, 3, 2, 1]]\n      input_layer = tf_v1.feature_column.input_layer({""text"": text},\n                                                     feature_columns)\n\n      loss = tf_v1.losses.mean_squared_error(input_layer, target)\n      optimizer = tf_v1.train.GradientDescentOptimizer(learning_rate=0.97)\n      train_op = optimizer.minimize(loss)\n\n      with tf_v1.train.MonitoredSession() as sess:\n        self.assertAllEqual(sess.run(input_layer), [[1, 2, 3, 4], [5, 5, 5, 5]])\n        for _ in range(10):\n          sess.run(train_op)\n        self.assertAllClose(sess.run(input_layer), target, atol=0.5)\n\n  def testEmptySparseTensorBatch(self):\n    feature_columns = [\n        hub.sparse_text_embedding_column(\n            ""text"",\n            self.spec,\n            combiner=""mean"",\n            default_value=""default"",\n            trainable=True),\n    ]\n\n    with tf.Graph().as_default():\n      text = tf.SparseTensor(\n          values=tf_v1.constant([], dtype=tf_v1.string, shape=[0]),\n          indices=tf_v1.constant([], dtype=tf_v1.int64, shape=[0, 2]),\n          dense_shape=[3, 0])\n\n      input_layer = tf_v1.feature_column.input_layer({""text"": text},\n                                                     feature_columns)\n\n      with tf_v1.train.MonitoredSession() as sess:\n        embeddings = sess.run(input_layer)\n        self.assertAllEqual(embeddings,\n                            [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n\n  def testEmptySparseTensorRow(self):\n    feature_columns = [\n        hub.sparse_text_embedding_column(\n            ""text"",\n            self.spec,\n            combiner=""mean"",\n            default_value=""default"",\n            trainable=True),\n    ]\n\n    with tf.Graph().as_default():\n      text = tf.SparseTensor(\n          values=tf_v1.constant([""hello world""], dtype=tf_v1.string, shape=[1]),\n          indices=tf_v1.constant([[0, 0]], dtype=tf_v1.int64, shape=[1, 2]),\n          dense_shape=[2, 1])\n\n      input_layer = tf_v1.feature_column.input_layer({""text"": text},\n                                                     feature_columns)\n\n      with tf_v1.train.MonitoredSession() as sess:\n        embeddings = sess.run(input_layer)\n        self.assertAllEqual(embeddings, [[1, 2, 3, 4], [0, 0, 0, 0]])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/feature_column_v2.py,9,"b'# Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities to use TF2 SavedModels as feature columns.\n\nFeature columns are compatible with the new FeatureColumn API, see\ntensorflow.python.feature_column.feature_column_v2.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport six\nimport tensorflow as tf\nfrom tensorflow_hub import keras_layer\n\n# TODO(b/73987364): It is not possible to extend feature columns without\n# depending on TensorFlow internal implementation details.\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.feature_column import feature_column_v2\n# pylint: enable=g-direct-tensorflow-import\n\n\n# TODO(b/149367074): Keras can\'t compute the shape if the input tensor is not\n# tf.float32.\ndef _compute_output_shape(layer, shape, dtype):\n\n  @tf.function(\n      input_signature=[tf.TensorSpec(dtype=dtype, name=""text"", shape=shape)])\n  def call(text):\n    return layer(text)\n\n  cf = call.get_concrete_function()\n  if not isinstance(cf.output_shapes, tf.TensorShape):\n    raise ValueError(\n        ""The SavedModel doesn\'t return a single result on __call__, ""\n        ""instead it returns %s. Did you specify the right `output_key`?"" %\n        cf.structured_outputs)\n  # Return dimensions after batch size.\n  return cf.output_shapes[1:]\n\n\ndef text_embedding_column_v2(key,\n                             module_path,\n                             output_key=None,\n                             trainable=False):\n  """"""Uses a TF2 SavedModel to construct a dense representation from text.\n\n  Args:\n    key: A string or `FeatureColumn` identifying the input string data.\n    module_path: A string path to the module. Can be a path to local filesystem\n      or a tfhub.dev handle.\n    output_key: Name of the output item to return if the layer returns a dict.\n      If the result is not a single value and an `output_key` is not specified,\n      the feature column cannot infer the right output to use.\n    trainable: Whether or not the Model is trainable. False by default, meaning\n      the pre-trained weights are frozen. This is different from the ordinary\n      tf.feature_column.embedding_column(), but that one is intended for\n      training from scratch.\n\n  Returns:\n    `DenseColumn` that converts from text input.\n  """"""\n  if not hasattr(feature_column_v2.StateManager, ""has_resource""):\n    raise NotImplementedError(""The currently used TensorFlow release is not ""\n                              ""compatible. To be compatible, the symbol ""\n                              ""tensorflow.python.feature_column.""\n                              ""feature_column_v2.StateManager.has_resource ""\n                              ""must exist."")\n\n  return _TextEmbeddingColumnV2(\n      key=key,\n      module_path=module_path,\n      output_key=output_key,\n      trainable=trainable)\n\n\nclass _TextEmbeddingColumnV2(\n    feature_column_v2.DenseColumn,\n    collections.namedtuple(""_ModuleEmbeddingColumn"",\n                           (""key"", ""module_path"", ""output_key"", ""trainable""))):\n  """"""Returned by text_embedding_column(). Do not use directly.""""""\n\n  @property\n  def _is_v2_column(self):\n    return True\n\n  @property\n  def parents(self):\n    """"""See \'FeatureColumn` base class.""""""\n    return [self.key]\n\n  @property\n  def _resource_name(self):\n    return ""hub_text_column_%s"" % self.key\n\n  @property\n  def name(self):\n    """"""Returns string. Used for variable_scope and naming.""""""\n    if not hasattr(self, ""_name""):\n      key_name = self.key if isinstance(self.key,\n                                        six.string_types) else self.key.name\n      self._name = ""{}_hub_module_embedding"".format(key_name)\n    return self._name\n\n  def create_state(self, state_manager):\n    """"""Imports the module along with all variables.""""""\n    # Note: state_manager._trainable is not public but is the pattern used\n    # to propagate the ""trainable"" state that used to be received via\n    # self._get_dense_tensor.\n    trainable = self.trainable and state_manager._trainable  # pylint: disable=protected-access\n    layer = keras_layer.KerasLayer(\n        self.module_path, output_key=self.output_key, trainable=trainable)\n    # Note: state manager attaches the loaded resource onto the layer.\n    state_manager.add_resource(self, self._resource_name, layer)\n    self._variable_shape = _compute_output_shape(layer, [None], tf.string)\n\n  def transform_feature(self, transformation_cache, state_manager):\n    return transformation_cache.get(self.key, state_manager)\n\n  @property\n  def parse_example_spec(self):\n    """"""Returns a `tf.Example` parsing spec as dict.""""""\n    return {self.key: tf.io.FixedLenFeature([1], tf.string)}\n\n  @property\n  def variable_shape(self):\n    """"""`TensorShape` of `get_dense_tensor`, without batch dimension.""""""\n    return self._variable_shape\n\n  def get_dense_tensor(self, transformation_cache, state_manager):\n    """"""Returns a `Tensor`.""""""\n    input_tensor = transformation_cache.get(self, state_manager)\n    layer = state_manager.get_resource(self, self._resource_name)\n    text_batch = tf.reshape(input_tensor, shape=[-1])\n    return layer(text_batch)\n\n  def get_config(self):\n    config = dict(zip(self._fields, self))\n    return config\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None, columns_by_name=None):\n    copied_config = config.copy()\n    return cls(**copied_config)\n'"
tensorflow_hub/feature_column_v2_test.py,42,"b'# Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.feature_column.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_hub as hub\n\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.feature_column import feature_column_v2\nfrom tensorflow.python.ops.lookup_ops import HashTable\nfrom tensorflow.python.ops.lookup_ops import KeyValueTensorInitializer\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass TextEmbedding(tf.train.Checkpoint):\n\n  def __init__(self, returns_dict=False):\n    embeddings = [\n        ("""", [0, 0, 0, 0]),  # OOV items are mapped to this embedding.\n        (""hello world"", [1, 2, 3, 4]),\n        (""pair-programming"", [5, 5, 5, 5]),\n    ]\n    keys = tf.constant([item[0] for item in embeddings], dtype=tf.string)\n    indices = tf.constant(list(range(len(embeddings))), dtype=tf.int64)\n    tbl_init = KeyValueTensorInitializer(keys, indices)\n    self.table = HashTable(tbl_init, 0)\n    self.weights = tf.Variable(\n        list([item[1] for item in embeddings]), dtype=tf.float32)\n    self.variables = [self.weights]\n    self.trainable_variables = self.variables\n    self._returns_dict = returns_dict\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(dtype=tf.string, name=""text"", shape=[None])\n  ])\n  def __call__(self, text_tensor):\n    indices_tensor = self.table.lookup(text_tensor)\n    embedding_tensor = tf.gather(self.weights, indices_tensor)\n    return dict(\n        outputs=embedding_tensor) if self._returns_dict else embedding_tensor\n\n\nclass TextEmbeddingColumnTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(TextEmbeddingColumnTest, self).setUp()\n    self.model = os.path.join(self.get_temp_dir(), ""model"")\n    tf.saved_model.save(TextEmbedding(), self.model)\n    self.model_returning_dicts = os.path.join(self.get_temp_dir(),\n                                              ""model_returning_dicts"")\n    tf.saved_model.save(\n        TextEmbedding(returns_dict=True), self.model_returning_dicts)\n\n  def testParents(self):\n    text_column = hub.text_embedding_column_v2(\n        ""text"", self.model, trainable=False)\n    self.assertEqual([""text""], text_column.parents)\n\n  def testMakeParseExampleSpec(self):\n    text_column = hub.text_embedding_column_v2(\n        ""text"", self.model, trainable=False)\n    parsing_spec = tf.feature_column.make_parse_example_spec([text_column])\n    self.assertEqual(parsing_spec,\n                     {""text"": tf.io.FixedLenFeature([1], dtype=tf.string)})\n\n  def testFeatureColumnsIsV2(self):\n    feature_column = hub.text_embedding_column_v2(""text_a"", self.model)\n    self.assertTrue(feature_column_v2.is_feature_column_v2([feature_column]))\n\n  def testConfig(self):\n    text_column = hub.text_embedding_column_v2(\n        ""text"", self.model, trainable=True)\n    config = text_column.get_config()\n    cloned_column = hub.feature_column_v2._TextEmbeddingColumnV2.from_config(\n        config)\n    self.assertEqual(cloned_column.module_path, text_column.module_path)\n\n  def testDenseFeaturesDirectly(self):\n    features = {\n        ""text_a"": [""hello world"", ""pair-programming""],\n        ""text_b"": [""hello world"", ""oov token""],\n    }\n    feature_columns = [\n        hub.text_embedding_column_v2(""text_a"", self.model, trainable=False),\n        hub.text_embedding_column_v2(""text_b"", self.model, trainable=False),\n    ]\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n    feature_layer_out = feature_layer(features)\n    self.assertAllEqual(feature_layer_out,\n                        [[1, 2, 3, 4, 1, 2, 3, 4], [5, 5, 5, 5, 0, 0, 0, 0]])\n\n  def testDenseFeaturesInKeras(self):\n    features = {\n        ""text"": np.array([""hello world"", ""pair-programming""]),\n    }\n    label = np.int64([0, 1])\n    feature_columns = [\n        hub.text_embedding_column_v2(""text"", self.model, trainable=True),\n    ]\n    input_features = dict(\n        text=tf.keras.layers.Input(name=""text"", shape=[None], dtype=tf.string))\n    dense_features = tf.keras.layers.DenseFeatures(feature_columns)\n    x = dense_features(input_features)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    logits = tf.keras.layers.Dense(1, activation=""linear"")(x)\n    model = tf.keras.Model(inputs=input_features, outputs=logits)\n    model.compile(\n        optimizer=""rmsprop"", loss=""binary_crossentropy"", metrics=[""accuracy""])\n    model.fit(x=features, y=label, epochs=10)\n    self.assertAllEqual(model.predict(features[""text""]).shape, [2, 1])\n\n  def testLoadingDifferentFeatureColumnsFails(self):\n    features = [\n        np.array([""hello world"", ""pair-programming""]),\n        np.array([""hello world"", ""pair-programming""]),\n    ]\n    label = np.int64([0, 1])\n    feature_columns = [\n        hub.text_embedding_column_v2(""text_1"", self.model, trainable=True),\n    ]\n    # Build the first model.\n    input_features = dict(\n        text_1=tf.keras.layers.Input(\n            name=""text_1"", shape=[None], dtype=tf.string))\n    dense_features = tf.keras.layers.DenseFeatures(feature_columns)\n    x = dense_features(input_features)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    logits = tf.keras.layers.Dense(1, activation=""linear"")(x)\n    model_1 = tf.keras.Model(inputs=input_features, outputs=logits)\n    model_1.compile(\n        optimizer=""rmsprop"", loss=""binary_crossentropy"", metrics=[""accuracy""])\n    model_1.fit(x=features, y=label, epochs=10)\n\n    checkpoint_path = os.path.join(self.get_temp_dir(), ""checkpoints"",\n                                   ""checkpoint-1"")\n    model_1.save_weights(checkpoint_path)\n\n    # Build the second model with feature columns that have different names.\n    feature_columns = [\n        hub.text_embedding_column_v2(""text_2"", self.model, trainable=True),\n    ]\n    input_features = dict(\n        text_2=tf.keras.layers.Input(\n            name=""text_2"", shape=[None], dtype=tf.string))\n    dense_features = tf.keras.layers.DenseFeatures(feature_columns)\n    x = dense_features(input_features)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    logits = tf.keras.layers.Dense(1, activation=""linear"")(x)\n    model_2 = tf.keras.Model(inputs=input_features, outputs=logits)\n    model_2.compile(\n        optimizer=""rmsprop"", loss=""binary_crossentropy"", metrics=[""accuracy""])\n\n    # Loading of checkpoints from the first model into the second model should\n    # fail.\n    with self.assertRaisesRegexp(AssertionError,\n                                 "".*Some Python objects were not bound.*""):\n      model_2.load_weights(checkpoint_path).assert_consumed()\n\n  def testWorksWithTF2DnnClassifier(self):\n    self.skipTest(""b/154115879 - needs more investigation for timeout."")\n    comment_embedding_column = hub.text_embedding_column_v2(\n        ""comment"", self.model, trainable=False)\n    upvotes = tf.feature_column.numeric_column(""upvotes"")\n\n    feature_columns = [comment_embedding_column, upvotes]\n    estimator = tf.estimator.DNNClassifier(\n        hidden_units=[10],\n        feature_columns=feature_columns,\n        model_dir=self.get_temp_dir())\n\n    # This only tests that estimator apis are working with the feature\n    # column without throwing exceptions.\n    def input_fn():\n      features = {\n          ""comment"": np.array([\n              [""the quick brown fox""],\n              [""spam spam spam""],\n          ]),\n          ""upvotes"": np.array([\n              [20],\n              [1],\n          ]),\n      }\n      labels = np.array([[1], [0]])\n      return features, labels\n    estimator.train(input_fn, max_steps=1)\n    estimator.evaluate(input_fn, steps=1)\n    estimator.predict(input_fn)\n\n  def testWorksWithDNNEstimatorAndDataset(self):\n    self.skipTest(""b/154115879 - needs more investigation for timeout."")\n    description_embeddings = hub.text_embedding_column_v2(\n        ""descriptions"", self.model_returning_dicts, output_key=""outputs"")\n\n    def input_fn():\n      features = dict(descriptions=tf.constant([[""sentence""]]))\n      labels = tf.constant([[1]])\n      dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n\n      data_batches = dataset.repeat().take(30).batch(5)\n      return data_batches\n\n    estimator = tf.estimator.DNNEstimator(\n        model_dir=os.path.join(self.get_temp_dir(), ""estimator_export""),\n        hidden_units=[10],\n        head=tf.estimator.BinaryClassHead(),\n        feature_columns=[description_embeddings])\n\n    estimator.train(input_fn=input_fn, max_steps=1)\n\n\nif __name__ == ""__main__"":\n  # This test is only supported in TF2 mode and only in TensorFlow version that\n  # has the following symbol:\n  # tensorflow.python.feature_column.feature_column_v2.StateManager.has_resource\n  if tf.executing_eagerly() and hasattr(feature_column_v2.StateManager,\n                                        ""has_resource""):\n    logging.info(""Using TF version: %s"", tf.__version__)\n    tf.test.main()\n  else:\n    logging.warning(""Skipping running tests for TF Version: %s"", tf.__version__)\n'"
tensorflow_hub/image_util.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helper functions for TF-Hub modules that handle images.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow_hub import image_module_info_pb2\nfrom tensorflow_hub import native_module\n\n\n# hub.Modules for images can provide further information for the utilities\n# in this file by attaching an ImageModuleInfo message under this key.\nIMAGE_MODULE_INFO_KEY = ""image_module_info""\n\n\n# The externally visible name of the message is hub.ImageModuleInfo\nImageModuleInfo = image_module_info_pb2.ImageModuleInfo  # pylint: disable=invalid-name\n\n\ndef attach_image_module_info(image_module_info):\n  """"""Attaches an ImageModuleInfo message from within a module_fn.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n\n  Args:\n    image_module_info: an ImageModuleInfo message.\n  """"""\n  native_module.attach_message(IMAGE_MODULE_INFO_KEY, image_module_info)\n\n\ndef get_image_module_info(module_or_spec, required=False):\n  """"""Returns the module\'s attached ImageModuleInfo message, or None if missing.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n\n  Args:\n    module_or_spec: a hub.Module or module_spec object.\n    required: if true, raises KeyError instead of returning None.\n  """"""\n  return module_or_spec.get_attached_message(\n      IMAGE_MODULE_INFO_KEY, ImageModuleInfo, required=required)\n\n\ndef get_expected_image_size(module_or_spec, signature=None, input_name=None):\n  """"""Returns expected [height, width] dimensions of an image input.\n\n  TODO(b/139530454): This does not work yet with TF2.\n\n  Args:\n    module_or_spec: a Module or ModuleSpec that accepts image inputs.\n    signature: a string with the key of the signature in question.\n      If None, the default signature is used.\n    input_name: a string with the input name for images. If None, the\n      conventional input name `images` for the default signature is used.\n\n  Returns:\n    A list if integers `[height, width]`.\n\n  Raises:\n    ValueError: If the size information is missing or malformed.\n  """"""\n  # First see if an attached ImageModuleInfo provides this information.\n  image_module_info = get_image_module_info(module_or_spec)\n  if image_module_info:\n    size = image_module_info.default_image_size\n    if size.height and size.width:\n      return [size.height, size.width]\n\n  # Else inspect the input shape in the module signature.\n  if input_name is None:\n    input_name = ""images""\n  input_info_dict = module_or_spec.get_input_info_dict(signature)\n  try:\n    shape = input_info_dict[input_name].get_shape()\n  except KeyError:\n    raise ValueError(""Module is missing input \'%s\' in signature \'%s\'."" %\n                     (input_name, signature or ""default""))\n  try:\n    _, height, width, _ = shape.as_list()\n    if not height or not width:\n      raise ValueError\n  except ValueError:\n    raise ValueError(\n        ""Shape of module input is %s, ""\n        ""expected [batch_size, height, width, num_channels] ""\n        ""with known height and width."" % shape)\n  return [height, width]\n\n\ndef get_num_image_channels(module_or_spec, signature=None, input_name=None):\n  """"""Returns expected num_channels dimensions of an image input.\n\n  This is for advanced users only who expect to handle modules with\n  image inputs that might not have the 3 usual RGB channels.\n\n  TODO(b/139530454): This does not work yet with TF2.\n\n  Args:\n    module_or_spec: a Module or ModuleSpec that accepts image inputs.\n    signature: a string with the key of the signature in question.\n      If None, the default signature is used.\n    input_name: a string with the input name for images. If None, the\n      conventional input name `images` for the default signature is used.\n\n  Returns:\n    An integer with the number of input channels to the module.\n\n  Raises:\n    ValueError: If the channel information is missing or malformed.\n  """"""\n  if input_name is None:\n    input_name = ""images""\n  input_info_dict = module_or_spec.get_input_info_dict(signature)\n  try:\n    shape = input_info_dict[input_name].get_shape()\n  except KeyError:\n    raise ValueError(""Module is missing input \'%s\' in signature \'%s\'."" %\n                     (input_name, signature or ""default""))\n  try:\n    _, _, _, num_channels = shape.as_list()\n    if num_channels is None:\n      raise ValueError\n  except ValueError:\n    raise ValueError(\n        ""Shape of module input is %s, ""\n        ""expected [batch_size, height, width, num_channels] ""\n        ""with known num_channels"" % shape)\n  return num_channels\n'"
tensorflow_hub/image_util_test.py,10,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for image_util.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow_hub import image_util\nfrom tensorflow_hub import module\nfrom tensorflow_hub import native_module\nfrom tensorflow_hub import tf_v1\n\n\ndef image_module_fn():\n  images = tf_v1.placeholder(dtype=tf.float32, shape=[None, 2, 4, 3])\n  sum_by_channels = tf.reduce_sum(images, [1, 2])\n  sum_all = tf.reduce_sum(images, [1, 2, 3])\n  native_module.add_signature(inputs=dict(images=images),\n                              outputs=dict(default=sum_all,\n                                           sum_by_channels=sum_by_channels))\n\n\ndef image_module_fn_with_info():\n  images = tf_v1.placeholder(dtype=tf.float32, shape=[None, None, None, 3])\n  sum_all = tf.reduce_sum(images, [1, 2, 3])\n  native_module.add_signature(inputs=dict(images=images),\n                              outputs=dict(default=sum_all))\n  image_module_info = image_util.ImageModuleInfo()\n  size = image_module_info.default_image_size\n  size.height, size.width = 2, 4\n  image_util.attach_image_module_info(image_module_info)\n\n\nclass ImageModuleTest(tf.test.TestCase):\n\n  def testGetExpectedImageSizeFromShape(self):\n    with tf.Graph().as_default():\n      spec = native_module.create_module_spec(image_module_fn)\n      self.assertAllEqual(image_util.get_expected_image_size(spec), [2, 4])\n      m = module.Module(spec)\n      self.assertAllEqual(image_util.get_expected_image_size(m), [2, 4])\n\n  def testGetExpectedImageSizeFromImageModuleInfo(self):\n    with tf.Graph().as_default():\n      spec = native_module.create_module_spec(image_module_fn_with_info)\n      self.assertAllEqual(image_util.get_expected_image_size(spec), [2, 4])\n      m = module.Module(spec)\n      self.assertAllEqual(image_util.get_expected_image_size(m), [2, 4])\n\n  def testGetNumImageChannels(self):\n    with tf.Graph().as_default():\n      spec = native_module.create_module_spec(image_module_fn)\n      self.assertEqual(image_util.get_num_image_channels(spec), 3)\n      m = module.Module(spec)\n      self.assertEqual(image_util.get_num_image_channels(m), 3)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/keras_layer.py,19,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A Keras Layer for using TF Hub modules in TF2 format.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport json\n\nfrom absl import logging\nimport six\nimport tensorflow as tf\n\nfrom tensorflow_hub import module_v2\n\n# ATTENTION: This file uses private imports from TF2.\n# __init__ may not import this file if tensorflow is too old.\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.framework import smart_cond\nfrom tensorflow.python.training.tracking import data_structures\nfrom tensorflow.python.util import tf_inspect\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass KerasLayer(tf.keras.layers.Layer):\n  """"""Wraps a SavedModel (or a legacy Hub.Module) as a Keras Layer.\n\n  This layer wraps a callable object for use as a Keras layer. The callable\n  object can be passed directly, or be specified by a Python string with a\n  handle that gets passed to `hub.load()`.\n\n  This is the preferred API to load a TF2-style SavedModel from TF Hub\n  into a Keras model. Calling this function requires TF 1.15 or newer.\n  It can be called both in eager and graph mode.\n\n  The callable object is expected to follow the conventions detailed below.\n  (These are met by TF2-compatible modules loaded from TensorFlow Hub.)\n\n  The callable is invoked with a single positional argument set to one tensor\n  or a nest of tensors containing the inputs to the layer. If the callable\n  accepts a `training` argument, a Python boolean is passed for it. It is True\n  if this layer is marked trainable *and* called for training.\n\n  If present, the following attributes of callable are understood to have\n  special meanings:\n    variables: a list of all tf.Variable objects that the callable depends on.\n    trainable_variables: those elements of `variables` that are reported\n      as trainable variables of this Keras Layer when the layer is trainable.\n    regularization_losses: a list of callables to be added as losses of this\n      Keras Layer when the layer is trainable. Each one must accept zero\n      arguments and return a scalar tensor.\n\n  Note: to work-around missing shape inference functionalities from functions\n  created from FunctionDefs, in rare cases one has to pass an \'output_shape\'\n  and potentially \'input_shape\' and \'dtype\'. E.g. the following is a typical\n  work-around:\n  ```\n  hub.KerasLayer(\n      ""/tmp/text_embedding_model"",\n      output_shape=[20],  # Outputs a tensor with shape [batch_size, 20].\n      input_shape=[],     # Expects a tensor of shape [batch_size] as input.\n      dtype=tf.string)    # Expects a tf.string input tensor.\n  ```\n\n  Note: This layer can be used inside the model_fn of a TF2 Estimator. See the\n  [migration guide]\n  (https://www.tensorflow.org/beta/guide/migration_guide#using_a_custom_model_fn)\n  for guidance on how to pick up trainable variables, losses and updates\n  explicitly from Keras objects instead of relying on graph collections.\n  This layer class does not support graph collections.\n  Distributed training of the Estimator requires setting the option\n  `session_config.experimental.share_cluster_devices_in_session` within\n  the `tf.estimator.RunConfig`. (It becomes non-experimental in TF2.2.)\n\n  Note: The data types used by a saved model have been fixed at saving time.\n  Using tf.keras.mixed_precision etc. has no effect on the saved model\n  that gets loaded by a hub.KerasLayer.\n\n  Attributes:\n    handle: A callable object (subject to the conventions above), or a\n      Python string to load a saved model via hub.load().\n      A string is required to save the Keras config of this Layer.\n    trainable: Optional. A boolean controlling whether this layer is trainable.\n      Must not be set to True when using a signature (raises ValueError),\n      including the use of legacy hub.Modules.\n    arguments: Optional. A dict with additional keyword arguments passed\n      to the callable. These must be JSON-serializable to save the Keras config\n      of this layer, and are not tracked as checkpointing dependencies\n      of this layer.\n    _sentinel: Used to prevent further positional arguments.\n    tags: Optional. If set indicates which graph variant to use. For legacy\n      hub.Modules leaving unset means to use the empty tags set.\n    signature: Optional. If set, KerasLayer will use the requested signature.\n      For legacy hub.Modules leaving unset means to use the `default` signature.\n      When using a signature, either signature_outputs_as_dict or output_key\n      have to set.\n    signature_outputs_as_dict: If set to True, the call to this layer returns a\n      dict of all the signature outputs. Can only be used if a signature is\n      specified (or default signature is used for legacy Hub.Modules).\n    output_key: Name of the output item to return if the layer returns a dict.\n      For legacy hub.Modules leaving unset means to return the `default` output.\n    output_shape: A tuple or a nest of tuples with the\n      (possibly partial) output shapes of the callable *without* leading\n      batch size. This must have the same nesting structure as the output of\n      the callable object and cover all output tensors.\n    **kwargs: Forwarded to Keras\' base Layer constructor.\n  """"""\n\n  def __init__(self, handle, trainable=False,  # pylint: disable=invalid-name\n               arguments=None, _sentinel=None, tags=None, signature=None,\n               signature_outputs_as_dict=None, output_key=None,\n               output_shape=None, **kwargs):\n    # Note: for compatibility with keras-model serialization this layer is\n    # json-serializable. If you add or change arguments here, please also update\n    # the `get_config` method.\n    # The arguments are marked NoDependency to avoid autoconversion to a\n    # trackable _DictWrapper, because that upsets json.dumps() when saving\n    # the result of get_config().\n    self._handle = handle\n    self._arguments = data_structures.NoDependency(arguments or {})\n    self._signature = signature\n    self._signature_outputs_as_dict = signature_outputs_as_dict\n    self._output_key = output_key\n    # TODO(b/142213824): Remove setting shapes when shape inference works.\n    if output_shape:\n      # Autograph chokes on _convert_nest_to_shapes(), so we call it here\n      # and not from within call().\n      self._output_shape = data_structures.NoDependency(\n          _convert_nest_to_shapes(output_shape))\n\n    self._func = load_module(handle, tags)\n    self._has_training_argument = func_has_training_argument(self._func)\n    self._is_hub_module_v1 = getattr(self._func, ""_is_hub_module_v1"", False)\n\n    # Update with the defaults when using legacy Hub.Module.\n    if self._is_hub_module_v1:\n      self._signature = self._signature or ""default""\n      if not self._signature_outputs_as_dict:\n        self._output_key = self._output_key or ""default""\n    # More validity checks.\n    if self._signature and (bool(self._output_key is not None)\n                            == bool(self._signature_outputs_as_dict)):\n      raise ValueError(""When using a signature, either output_key or ""\n                       ""signature_outputs_as_dict=True should be set."")\n    if not self._signature and self._signature_outputs_as_dict:\n      raise ValueError(""signature_outputs_as_dict is only valid if specifying ""\n                       ""a signature (or using a legacy Hub.Module)."")\n\n    self._callable = self._get_callable()\n    self._setup_layer(trainable, **kwargs)\n\n  def _setup_layer(self, trainable=False, **kwargs):\n    """"""Constructs keras layer with relevant weights and losses.""""""\n    # Initialize an empty layer, then add_weight() etc. as needed.\n    super(KerasLayer, self).__init__(trainable=trainable, **kwargs)\n\n    # Add trainable and non-trainable weights from the callable.\n    if hasattr(self._func, ""trainable_variables""):\n      for v in self._func.trainable_variables:\n        self._add_existing_weight(v, trainable=True)\n      trainable_variables = {id(v) for v in self._func.trainable_variables}\n    else:\n      trainable_variables = set()\n    if hasattr(self._func, ""variables""):\n      for v in self._func.variables:\n        if id(v) not in trainable_variables:\n          self._add_existing_weight(v, trainable=False)\n\n    # Forward the callable\'s regularization losses (if any).\n    if hasattr(self._func, ""regularization_losses""):\n      for l in self._func.regularization_losses:\n        if not callable(l):\n          raise ValueError(\n              ""hub.KerasLayer(obj) expects obj.regularization_losses to be an ""\n              ""iterable of callables, each returning a scalar loss term."")\n        self.add_loss(self._call_loss_if_trainable(l))  # Supports callables.\n\n  def _add_existing_weight(self, weight, trainable=None):\n    """"""Calls add_weight() to register but not create an existing weight.""""""\n    if trainable is None: trainable = weight.trainable\n    self.add_weight(name=weight.name, shape=weight.shape, dtype=weight.dtype,\n                    trainable=trainable, getter=lambda *_, **__: weight)\n\n  def _call_loss_if_trainable(self, loss):\n    """"""Returns `loss` conditioned on whether this layer is trainable.""""""\n    return lambda: loss() if self.trainable else 0.\n\n  def call(self, inputs, training=None):\n    # These checks happen here and not in __init__, because self.trainable is\n    # a mutable public attribute.\n    self._check_trainability()\n\n    # We basically want to call this...\n    args = []\n    kwargs = self._arguments.copy()\n    if self._signature and isinstance(inputs, dict):\n      kwargs.update(inputs)\n    else:\n      args.append(inputs)\n    f = functools.partial(self._callable, *args, **kwargs)\n    # ...but we may also have to pass a Python boolean for `training`, which\n    # is the logical ""and"" of this layer\'s trainability and what the surrounding\n    # model is doing (analogous to tf.keras.layers.BatchNormalization in TF2).\n    # For the latter, we have to look in two places: the `training` argument,\n    # or else Keras\' global `learning_phase`, which might actually be a tensor.\n    if not self._has_training_argument:\n      result = f()\n    else:\n      if self.trainable:\n        if training is None:\n          training = tf.keras.backend.learning_phase()\n      else:\n        training = False\n      result = smart_cond.smart_cond(training,\n                                     lambda: f(training=True),\n                                     lambda: f(training=False))\n\n    # Unwrap dicts returned by signatures.\n    if self._output_key:\n      if not isinstance(result, dict):\n        raise ValueError(""Specifying `output_key` is forbidden if output ""\n                         ""type %s is not a dict."" % type(result))\n      if self._output_key not in result:\n        raise ValueError(\n            ""KerasLayer output does not contain the output key %s ""\n            ""(available: %s)."" % (self._output_key, result.keys()))\n      result = result[self._output_key]\n\n    # TODO(b/142213824): Remove setting shapes when shape inference works.\n    result = self._apply_output_shape_if_set(inputs, result)\n    return result\n\n  def _check_trainability(self):\n    """"""Raises or logs errors for unuspported uses of trainable=True.""""""\n    if not self.trainable: return  # Nothing to do.\n\n    # Training is only supported when calling a reusable TF2 SavedModel through\n    # its @tf.function __call__. Trying to train through a signature is likely\n    # to go wrong beyond the most simple cases due to a number of pitfalls:\n    # - No good support for train vs inference mode. hub.Modules used\n    #   graph versions identified by tags, but this was not a general\n    #   standard for SavedModels, and TF2 can no longer save with tags.\n    # - No support for update ops. hub.Modules had them in the UPDATE_OPS\n    #   collection, but collections are no longer loaded in TF2. General\n    #   SavedModel signatures had no support for them.\n    # - No support for regularization losses (same story).\n    # - A SavedModel without @tf.function __call__ will likely also not\n    #   provide a trainable_variables attribute.\n    if self._is_hub_module_v1:\n      raise ValueError(\n          ""Setting hub.KerasLayer.trainable = True is unsupported when ""\n          ""loading from the hub.Module format of TensorFlow 1."")\n    elif self._signature:\n      raise ValueError(\n          ""Setting hub.KerasLayer.trainable = True is unsupported when ""\n          ""calling a SavedModel signature."")\n    # Having zero trainable variables in an otherwise trainable model\n    # is suspicious but may be valid as a boundary case, so we just log,\n    # but at most once per layer instance.\n    if not self.trainable_weights:\n      if not hasattr(self, ""_already_logged_trainable_with_zero_weights""):\n        logging.error(\n            ""hub.KerasLayer is trainable but has zero trainable weights."")\n        setattr(self, ""_already_logged_trainable_with_zero_weights"", True)\n\n  def _get_callable(self):\n    """"""Returns a callable object.""""""\n    if callable(self._func) and not self._signature:\n      return self._func\n    if not hasattr(self._func, ""signatures""):\n      if self._signature:  # Assuming the user intended to use a signature.\n        raise ValueError(""Loaded object has no signatures."")\n      else:  # Assuming the user intended to use a callable SavedModel.\n        raise ValueError(\n            ""Loaded object is not callable and has no signatures."")\n    if self._signature is None:\n      raise ValueError(""Signature name has to be specified for non-callable ""\n                       ""saved models (if not legacy Hub.Module)."")\n    if self._signature not in self._func.signatures:\n      raise ValueError(""Unknown signature %s in %s (available signatures: %s).""\n                       % (self._signature, self._handle, self._func.signatures))\n    f = self._func.signatures[self._signature]\n    if not callable(f):\n      raise ValueError(""Internal error: signature %s is not callable in %s"" %\n                       (self._signature, self._handle))\n    return f\n\n  def _apply_output_shape_if_set(self, inputs, outputs):\n    if not hasattr(self, ""_output_shape""):\n      return outputs\n    # Traverse the nest and turn shape-like tuples into tf.TensorShapes,\n    # or else map_structure below would try to recurse into them.\n    output_shape = getattr(self, ""_output_shape"")\n    batch_size = tf.nest.flatten(inputs)[0].shape[0]\n    def _inplace_set_shape(tensor, shape):\n      tensor.set_shape(tf.TensorShape(batch_size).concatenate(shape))\n    tf.nest.map_structure(_inplace_set_shape, outputs, output_shape)\n    return outputs\n\n  def get_config(self):\n    """"""Returns a serializable dict of keras layer configuration parameters.""""""\n    config = super(KerasLayer, self).get_config()\n    if not isinstance(self._handle, six.string_types):\n      # Need to raise this type in order for tf.saved_model.save() to fall back\n      # to not using config, instead of crashing.\n      # TODO(b/134528831): Reconsider the usability implications.\n      raise NotImplementedError(\n          ""Can only generate a valid config for `hub.KerasLayer(handle, ...)`""\n          ""that uses a string `handle`.\\n\\n""\n          ""Got `type(handle)`: {}"".format(type(self._handle)))\n    config[""handle""] = self._handle\n\n    if hasattr(self, ""_output_shape""):\n      output_shape = _convert_nest_from_shapes(self._output_shape)\n      try:\n        json.dumps(output_shape)\n      except TypeError:\n        raise ValueError(\n            ""hub.KerasLayer(..., output_shape=) is not json-serializable.\\n""\n            ""Got value: {}"".format(output_shape))\n      config[""output_shape""] = output_shape\n\n    if self._arguments:\n      # Raise clear errors for non-serializable arguments.\n      for key, value in self._arguments.items():\n        try:\n          json.dumps(value)\n        except TypeError:\n          raise ValueError(\n              ""`hub.KerasLayer(..., arguments)` contains non json-serializable""\n              ""values in key: {}"".format(key))\n      config[""arguments""] = self._arguments\n\n    if self._signature:\n      config[""signature""] = self._signature\n    if self._output_key:\n      config[""output_key""] = self._output_key\n    if self._signature_outputs_as_dict:\n      config[""signature_outputs_as_dict""] = self._signature_outputs_as_dict\n\n    return config\n\n  @property\n  def resolved_object(self):\n    """"""Returns the callable object to which `handle` resolved in `__init__`.""""""\n    return self._func\n\n\ndef _convert_nest_to_shapes(x):\n  """"""In a nest, converts raw tuples/lists of int or None to tf.TensorShape.""""""\n  # A dict is certainly a container and not a shape. We need to handle\n  # it first and not try construct a TensorShape from its keys.\n  if isinstance(x, dict):\n    return type(x)([(k, _convert_nest_to_shapes(v)) for k, v in x.items()])\n  # Anything else might be already a TensorShape, a tuple that converts\n  # to a TensorShape, or a sequence that needs further recursion.\n  try:\n    return tf.TensorShape(x)\n  except TypeError:\n    pass  # Will try parsing as a container instead.\n  if isinstance(x, (list, tuple)):\n    return type(x)([_convert_nest_to_shapes(v) for v in x])\n  else:\n    raise TypeError(""Cannot convert to nest of TensorShapes, ""\n                    ""found none of TensorShape, dict, list, tuple: %r"" % x)\n\n\ndef _convert_nest_from_shapes(x):\n  """"""Converts a nest of tf.TensorShape to raw tuples of int or None.""""""\n  def _shape_as_tuple(x):\n    assert isinstance(x, tf.TensorShape)\n    return tuple(x.as_list())\n  return tf.nest.map_structure(_shape_as_tuple, x)\n\n\ndef load_module(handle, tags=None):\n  if callable(handle):\n    if tags is not None:\n      raise ValueError(""Passing a callable handle is mutually exclusive ""\n                       ""with setting tags."")\n    return handle\n  else:\n    return module_v2.load(handle, tags=tags)\n\n\ndef func_has_training_argument(func):\n  """"""Checks whether saved model has a `training` argument.""""""\n  if not callable(func):\n    return False\n  fullargspec = tf_inspect.getfullargspec(func.__call__)\n  return (""training"" in fullargspec.args or\n          ""training"" in fullargspec.kwonlyargs)\n'"
tensorflow_hub/keras_layer_test.py,138,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.keras_layer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow_hub import test_utils\n\n\n# NOTE: A Hub-style SavedModel can either be constructed manually, or by\n# relying on tf.saved_model.save(keras_model, ...) to put in the expected\n# endpoints. The following _save*model() helpers offer a save_from_keras\n# argument to select, and tests should strive to exercise both.\n# The big exception are SavedModels with hyperparameters: There is no generic\n# helper code yet to bridge between optional tensor inputs and properties\n# in Keras model objects.\n\n\n# A series of code changes implemented the necessary Keras functionality\n# up to TF 2.0.0-beta1. For this test to work, we need them up to and including\n# https://github.com/tensorflow/tensorflow/commit/3dc3b5df5f87ac0c460583eebc7d845e33138d2b\n# However, this is not easy to test for, so we\'re piggybacking here on the\n# one official API change from that series, found in the slightly older\n# https://github.com/tensorflow/tensorflow/commit/eff4ae822a08355b4a15b638148129d348b985a4#diff-1088d4bbbe00f5b39b923b4527e93790\ndef _skip_if_keras_save_too_old(test_case):\n  if not hasattr(tf.keras.layers.InputSpec, ""get_config""):\n    test_case.skipTest(\n        ""Your TensorFlow version (%s) looks too old for creating reusable ""\n        ""SavedModels in Keras model saving."" % tf.__version__)\n\n\ndef _skip_if_no_tf_asset(test_case):\n  if not hasattr(tf.saved_model, ""Asset""):\n    test_case.skipTest(\n        ""Your TensorFlow version (%s) looks too old for creating SavedModels ""\n        "" with assets."" % tf.__version__)\n\n\ndef _json_cycle(x):\n  return json.loads(json.dumps(x))\n\n\ndef _save_half_plus_one_model(export_dir, save_from_keras=False):\n  """"""Writes Hub-style SavedModel to compute y = wx + 1, with w trainable.""""""\n  inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n  times_w = tf.keras.layers.Dense(\n      units=1,\n      kernel_initializer=tf.keras.initializers.Constant([[0.5]]),\n      kernel_regularizer=tf.keras.regularizers.l2(0.01),\n      use_bias=False)\n  plus_1 = tf.keras.layers.Dense(\n      units=1,\n      kernel_initializer=tf.keras.initializers.Constant([[1.0]]),\n      bias_initializer=tf.keras.initializers.Constant([1.0]),\n      trainable=False)\n  outp = plus_1(times_w(inp))\n  model = tf.keras.Model(inp, outp)\n\n  if save_from_keras:\n    tf.saved_model.save(model, export_dir)\n    return\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=(None, 1), dtype=tf.float32)])\n  def call_fn(inputs):\n    return model(inputs, training=False)\n\n  obj = tf.train.Checkpoint()\n  obj.__call__ = call_fn\n  obj.variables = model.trainable_variables + model.non_trainable_variables\n  assert len(obj.variables) == 3, ""Expect 2 kernels and 1 bias.""\n  obj.trainable_variables = [times_w.kernel]\n  assert(len(model.losses) == 1), ""Expect 1 regularization loss.""\n  obj.regularization_losses = [\n      tf.function(lambda: model.losses[0], input_signature=[])]\n  tf.saved_model.save(obj, export_dir)\n\n\ndef _tensors_names_set(tensor_sequence):\n  """"""Converts tensor sequence to a set of tensor references.""""""\n  # Tensor name stands as a proxy for the uniqueness of the tensors.\n  # In TensorFlow 2.x one can use the `experimental_ref` method, but it is not\n  # available in older TF versions.\n  return {t.name for t in tensor_sequence}\n\n\ndef _save_batch_norm_model(export_dir, save_from_keras=False):\n  """"""Writes a Hub-style SavedModel with a batch norm layer.""""""\n  inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n  bn = tf.keras.layers.BatchNormalization(momentum=0.8)\n  outp = bn(inp)\n  model = tf.keras.Model(inp, outp)\n\n  if save_from_keras:\n    tf.saved_model.save(model, export_dir)\n    return\n\n  @tf.function\n  def call_fn(inputs, training=False):\n    return model(inputs, training=training)\n  for training in (True, False):\n    call_fn.get_concrete_function(tf.TensorSpec((None, 1), tf.float32),\n                                  training=training)\n\n  obj = tf.train.Checkpoint()\n  obj.__call__ = call_fn\n  # Test assertions pick up variables by their position here.\n  obj.trainable_variables = [bn.beta, bn.gamma]\n  assert _tensors_names_set(obj.trainable_variables) == _tensors_names_set(\n      model.trainable_variables)\n  obj.variables = [bn.beta, bn.gamma, bn.moving_mean, bn.moving_variance]\n  assert _tensors_names_set(obj.variables) == _tensors_names_set(\n      model.trainable_variables + model.non_trainable_variables)\n  obj.regularization_losses = []\n  assert not model.losses\n  tf.saved_model.save(obj, export_dir)\n\n\ndef _get_batch_norm_vars(imported):\n  """"""Returns the 4 variables of an imported batch norm model in sorted order.""""""\n  expected_suffixes = [""beta"", ""gamma"", ""moving_mean"", ""moving_variance""]\n  variables = sorted(imported.variables, key=lambda v: v.name)\n  names = [v.name for v in variables]\n  assert len(variables) == 4\n  assert all(name.endswith(suffix + "":0"")\n             for name, suffix in zip(names, expected_suffixes))\n  return variables\n\n\ndef _save_model_with_hparams(export_dir):\n  """"""Writes a Hub-style SavedModel to compute y = ax + b with hparams a, b.""""""\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n      tf.TensorSpec(shape=(), dtype=tf.float32),\n      tf.TensorSpec(shape=(), dtype=tf.float32)])\n  def call_fn(x, a=1., b=0.):\n    return tf.add(tf.multiply(a, x), b)\n\n  obj = tf.train.Checkpoint()\n  obj.__call__ = call_fn\n  tf.saved_model.save(obj, export_dir)\n\n\ndef _save_model_with_custom_attributes(export_dir, temp_dir,\n                                       save_from_keras=False):\n  """"""Writes a Hub-style SavedModel with a custom attributes.""""""\n  # Calling the module parses an integer.\n  f = lambda a: tf.strings.to_number(a, tf.int64)\n  if save_from_keras:\n    inp = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n    outp = tf.keras.layers.Lambda(f)(inp)\n    model = tf.keras.Model(inp, outp)\n  else:\n    model = tf.train.Checkpoint()\n    model.__call__ = tf.function(\n        input_signature=[tf.TensorSpec(shape=(None, 1), dtype=tf.string)])(f)\n\n  # Running on the `sample_input` file yields the `sample_output` value.\n  asset_source_file_name = os.path.join(temp_dir, ""number.txt"")\n  tf.io.gfile.makedirs(temp_dir)\n  with tf.io.gfile.GFile(asset_source_file_name, ""w"") as f:\n    f.write(""12345\\n"")\n  model.sample_input = tf.saved_model.Asset(asset_source_file_name)\n  model.sample_output = tf.Variable([[12345]], dtype=tf.int64)\n\n  # Save model and invalidate the original asset file name.\n  tf.saved_model.save(model, export_dir)\n  tf.io.gfile.remove(asset_source_file_name)\n  return export_dir\n\n\ndef _save_model_with_dict_input_output(export_dir):\n  """"""Writes SavedModel using dicts to compute x+y, x+2y and maybe x-y.""""""\n  @tf.function\n  def call_fn(d, return_dict=False):\n    x = d[""x""]\n    y = d[""y""]\n    sigma = tf.concat([tf.add(x, y), tf.add(x, 2*y)], axis=-1)\n    if return_dict:\n      return dict(sigma=sigma, delta=tf.subtract(x, y))\n    else:\n      return sigma\n  # Trigger traces.\n  d_spec = dict(x=tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n                y=tf.TensorSpec(shape=(None, 1), dtype=tf.float32))\n  for return_dict in (False, True):\n    call_fn.get_concrete_function(d_spec, return_dict=return_dict)\n\n  obj = tf.train.Checkpoint()\n  obj.__call__ = call_fn\n  tf.saved_model.save(obj, export_dir)\n\n\ndef _save_model_with_obscurely_shaped_list_output(export_dir):\n  """"""Writes SavedModel with hard-to-predict output shapes.""""""\n  def broadcast_obscurely_to(input, shape):\n    """"""Like tf.broadcast_to(), but hostile to static shape propagation.""""""\n    obscured_shape = tf.cast(tf.cast(shape, tf.float32)\n                             # Add small random noise that gets rounded away.\n                             + 0.1*tf.sin(tf.random.uniform((), -3, +3)) + 0.3,\n                             tf.int32)\n    return tf.broadcast_to(input, obscured_shape)\n\n  @tf.function(\n      input_signature=[tf.TensorSpec(shape=(None, 1), dtype=tf.float32)])\n  def call_fn(x):\n    # For each batch element x, the three outputs are\n    #   value x with shape (1)\n    #   value 2*x broadcast to shape (2,2)\n    #   value 3*x broadcast to shape (3,3,3)\n    batch_size = tf.shape(x)[0]\n    return [broadcast_obscurely_to(tf.reshape(i*x, [batch_size] + [1]*i),\n                                   tf.concat([[batch_size], [i]*i], axis=0))\n            for i in range(1, 4)]\n\n  obj = tf.train.Checkpoint()\n  obj.__call__ = call_fn\n  tf.saved_model.save(obj, export_dir)\n\n\nclass KerasTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests KerasLayer in an all-Keras environment.""""""\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testHalfPlusOneRetraining(self, save_from_keras):\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    # Import the half-plus-one model into a consumer model.\n    export_dir = os.path.join(self.get_temp_dir(), ""half-plus-one"")\n    _save_half_plus_one_model(export_dir, save_from_keras=save_from_keras)\n    inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n    imported = hub.KerasLayer(export_dir, trainable=True)\n    outp = imported(inp)\n    model = tf.keras.Model(inp, outp)\n    # The consumer model computes y = x/2 + 1 as expected.\n    self.assertAllEqual(\n        model(np.array([[0.], [8.], [10.], [12.]], dtype=np.float32)),\n        np.array([[1.], [5.], [6.], [7.]], dtype=np.float32))\n    self.assertAllEqual(model.losses, np.array([0.0025], dtype=np.float32))\n    # The kernel weight is trainable but the bias is not.\n    self.assertEqual(len(model.trainable_weights), 1)\n    self.assertEqual(model.trainable_weights[0].shape.rank, 2)  # Kernel w.\n    self.assertEqual(len(model.non_trainable_weights), 2)\n    self.assertCountEqual([v.shape.rank for v in model.non_trainable_weights],\n                          [2, 1])  # Kernel and bias from the plus_1 layer.\n    self.assertNoCommonElements(_tensors_names_set(model.trainable_weights),\n                                _tensors_names_set(model.non_trainable_weights))\n    # Retrain on y = x/2 + 6 for x near 10.\n    # (Console output should show loss below 0.2.)\n    model.compile(tf.keras.optimizers.SGD(0.002),\n                  ""mean_squared_error"", run_eagerly=True)\n    x = [[9.], [10.], [11.]] * 10\n    y = [[xi[0]/2. + 6] for xi in x]\n    model.fit(np.array(x), np.array(y), batch_size=len(x), epochs=10, verbose=2)\n    # The bias is non-trainable and has to stay at 1.0.\n    self.assertAllEqual(model(np.array([[0.]], dtype=np.float32)),\n                        np.array([[1.]], dtype=np.float32))\n    # To compensate, the kernel weight will grow to almost 1.0.\n    self.assertAllClose(model(np.array([[10.]], dtype=np.float32)),\n                        np.array([[11.]], dtype=np.float32),\n                        atol=0.0, rtol=0.03)\n    self.assertAllClose(model.losses, np.array([0.01], dtype=np.float32),\n                        atol=0.0, rtol=0.06)\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testRegularizationLoss(self, save_from_keras):\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    # Import the half-plus-one model into a consumer model.\n    export_dir = os.path.join(self.get_temp_dir(), ""half-plus-one"")\n    _save_half_plus_one_model(export_dir, save_from_keras=save_from_keras)\n    inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n    imported = hub.KerasLayer(export_dir, trainable=False)\n    outp = imported(inp)\n    model = tf.keras.Model(inp, outp)\n    # When untrainable, the layer does not contribute regularization losses.\n    self.assertAllEqual(model.losses, np.array([0.], dtype=np.float32))\n    # When trainable (even set after the fact), the layer forwards its losses.\n    imported.trainable = True\n    self.assertAllEqual(model.losses, np.array([0.0025], dtype=np.float32))\n    # This can be toggled repeatedly.\n    imported.trainable = False\n    self.assertAllEqual(model.losses, np.array([0.], dtype=np.float32))\n    imported.trainable = True\n    self.assertAllEqual(model.losses, np.array([0.0025], dtype=np.float32))\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testBatchNormRetraining(self, save_from_keras):\n    """"""Tests imported batch norm with trainable=True.""""""\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""batch-norm"")\n    _save_batch_norm_model(export_dir, save_from_keras=save_from_keras)\n    inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n    imported = hub.KerasLayer(export_dir, trainable=True)\n    var_beta, var_gamma, var_mean, var_variance = _get_batch_norm_vars(imported)\n    outp = imported(inp)\n    model = tf.keras.Model(inp, outp)\n    # Retrain the imported batch norm layer on a fixed batch of inputs,\n    # which has mean 12.0 and some variance of a less obvious value.\n    # The module learns scale and offset parameters that achieve the\n    # mapping x --> 2*x for the observed mean and variance.\n    model.compile(tf.keras.optimizers.SGD(0.1),\n                  ""mean_squared_error"", run_eagerly=True)\n    x = [[11.], [12.], [13.]]\n    y = [[2*xi[0]] for xi in x]\n    model.fit(np.array(x), np.array(y), batch_size=len(x), epochs=100)\n    self.assertAllClose(var_mean.numpy(), np.array([12.0]))\n    self.assertAllClose(var_beta.numpy(), np.array([24.0]))\n    self.assertAllClose(model(np.array(x, np.float32)), np.array(y))\n    # Evaluating the model operates batch norm in inference mode:\n    # - Batch statistics are ignored in favor of aggregated statistics,\n    #   computing x --> 2*x independent of input distribution.\n    # - Update ops are not run, so this doesn\'t change over time.\n    for _ in range(100):\n      self.assertAllClose(model(np.array([[10.], [20.], [30.]], np.float32)),\n                          np.array([[20.], [40.], [60.]]))\n    self.assertAllClose(var_mean.numpy(), np.array([12.0]))\n    self.assertAllClose(var_beta.numpy(), np.array([24.0]))\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testBatchNormFreezing(self, save_from_keras):\n    """"""Tests imported batch norm with trainable=False.""""""\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""batch-norm"")\n    _save_batch_norm_model(export_dir, save_from_keras=save_from_keras)\n    inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n    imported = hub.KerasLayer(export_dir, trainable=False)\n    var_beta, var_gamma, var_mean, var_variance = _get_batch_norm_vars(imported)\n    dense = tf.keras.layers.Dense(\n        units=1,\n        kernel_initializer=tf.keras.initializers.Constant([[1.5]]),\n        use_bias=False)\n    outp = dense(imported(inp))\n    model = tf.keras.Model(inp, outp)\n    # Training the model to x --> 2*x leaves the batch norm layer entirely\n    # unchanged (both trained beta&gamma and aggregated mean&variance).\n    self.assertAllClose(var_beta.numpy(), np.array([0.0]))\n    self.assertAllClose(var_gamma.numpy(), np.array([1.0]))\n    self.assertAllClose(var_mean.numpy(), np.array([0.0]))\n    self.assertAllClose(var_variance.numpy(), np.array([1.0]))\n    model.compile(tf.keras.optimizers.SGD(0.1),\n                  ""mean_squared_error"", run_eagerly=True)\n    x = [[1.], [2.], [3.]]\n    y = [[2*xi[0]] for xi in x]\n    model.fit(np.array(x), np.array(y), batch_size=len(x), epochs=20)\n    self.assertAllClose(var_beta.numpy(), np.array([0.0]))\n    self.assertAllClose(var_gamma.numpy(), np.array([1.0]))\n    self.assertAllClose(var_mean.numpy(), np.array([0.0]))\n    self.assertAllClose(var_variance.numpy(), np.array([1.0]))\n    self.assertAllClose(model(np.array(x, np.float32)), np.array(y))\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testCustomAttributes(self, save_from_keras):\n    """"""Tests custom attributes (Asset and Variable) on a SavedModel.""""""\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    _skip_if_no_tf_asset(self)\n    base_dir = os.path.join(self.get_temp_dir(), ""custom-attributes"")\n    export_dir = os.path.join(base_dir, ""model"")\n    temp_dir = os.path.join(base_dir, ""scratch"")\n    _save_model_with_custom_attributes(export_dir, temp_dir,\n                                       save_from_keras=save_from_keras)\n    imported = hub.KerasLayer(export_dir)\n    expected_outputs = imported.resolved_object.sample_output.value().numpy()\n    asset_path = imported.resolved_object.sample_input.asset_path.numpy()\n    with tf.io.gfile.GFile(asset_path) as f:\n      inputs = tf.constant([[f.read()]], dtype=tf.string)\n    actual_outputs = imported(inputs).numpy()\n    self.assertAllEqual(expected_outputs, actual_outputs)\n\n  @parameterized.named_parameters((""NoOutputShapes"", False),\n                                  (""WithOutputShapes"", True))\n  def testInputOutputDict(self, pass_output_shapes):\n    """"""Tests use of input/output dicts.""""""\n    # Create a SavedModel to compute sigma=[x+y, x+2y] and maybe delta=x-y.\n    export_dir = os.path.join(self.get_temp_dir(), ""with-dicts"")\n    _save_model_with_dict_input_output(export_dir)\n    # Build a Model from it using Keras\' ""functional"" API.\n    x_in = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n    y_in = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n    dict_in = dict(x=x_in, y=y_in)\n    kwargs = dict(arguments=dict(return_dict=True))  # For the SavedModel.\n    if pass_output_shapes:\n      # Shape inference works without this, but we pass it anyways to exercise\n      # that code path and see that map_structure is called correctly\n      # and calls Tensor.set_shape() with compatible values.\n      kwargs[""output_shape""] = dict(sigma=(2,), delta=(1,))\n    imported = hub.KerasLayer(export_dir, **kwargs)\n    dict_out = imported(dict_in)\n    delta_out = dict_out[""delta""]\n    sigma_out = dict_out[""sigma""]\n    concat_out = tf.keras.layers.concatenate([delta_out, sigma_out])\n    model = tf.keras.Model(dict_in, [delta_out, sigma_out, concat_out])\n    # Test the model.\n    x = np.array([[11.], [22.], [33.]], dtype=np.float32)\n    y = np.array([[1.], [2.], [3.]], dtype=np.float32)\n    outputs = model(dict(x=x, y=y))\n    self.assertLen(outputs, 3)\n    delta, sigma, concat = [x.numpy() for x in outputs]\n    self.assertAllClose(delta,\n                        np.array([[10.], [20.], [30.]]))\n    self.assertAllClose(sigma,\n                        np.array([[12., 13.], [24., 26.], [36., 39.]]))\n    self.assertAllClose(\n        concat,\n        np.array([[10., 12., 13.], [20., 24., 26.], [30., 36., 39.]]))\n    # Test round-trip through config.\n    config = imported.get_config()\n    new_layer = hub.KerasLayer.from_config(_json_cycle(config))\n    if pass_output_shapes:\n      self.assertEqual(new_layer._output_shape, imported._output_shape)\n    else:\n      self.assertFalse(hasattr(new_layer, ""_output_shape""))\n\n  @parameterized.named_parameters((""NoOutputShapes"", False),\n                                  (""WithOutputShapes"", True))\n  def testOutputShapeList(self, pass_output_shapes):\n    export_dir = os.path.join(self.get_temp_dir(), ""obscurely-shaped"")\n    _save_model_with_obscurely_shaped_list_output(export_dir)\n\n    kwargs = {}\n    if pass_output_shapes:\n      kwargs[""output_shape""] = [[1], [2, 2], [3, 3, 3]]\n    inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n    imported = hub.KerasLayer(export_dir, **kwargs)\n    outp = imported(inp)\n    model = tf.keras.Model(inp, outp)\n\n    x = np.array([[1.], [10.]], dtype=np.float32)\n    outputs = model(x)\n    self.assertLen(outputs, 3)\n    single, double, triple = [x.numpy() for x in outputs]\n    # The outputs above are eager Tensors with concrete values,\n    # so they always have a fully defined shape. However, running\n    # without crash verifies that no incompatible shapes were set.\n    # See EstimatorTest below for graph-mode Tensors.\n    self.assertAllClose(single, np.array([[1.], [10.]]))\n    self.assertAllClose(double, np.array([[[2., 2.], [2., 2.]],\n                                          [[20., 20.], [20., 20.]]]))\n    self.assertAllClose(triple, np.array(\n        [[[[3., 3., 3.], [3., 3., 3.], [3., 3., 3.]],\n          [[3., 3., 3.], [3., 3., 3.], [3., 3., 3.]],\n          [[3., 3., 3.], [3., 3., 3.], [3., 3., 3.]]],\n         [[[30., 30., 30.], [30., 30., 30.], [30., 30., 30.]],\n          [[30., 30., 30.], [30., 30., 30.], [30., 30., 30.]],\n          [[30., 30., 30.], [30., 30., 30.], [30., 30., 30.]]]]))\n    # Test round-trip through config.\n    config = imported.get_config()\n    new_layer = hub.KerasLayer.from_config(_json_cycle(config))\n    if pass_output_shapes:\n      self.assertEqual(new_layer._output_shape, imported._output_shape)\n    else:\n      self.assertFalse(hasattr(new_layer, ""_output_shape""))\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testComputeOutputShape(self, save_from_keras):\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""half-plus-one"")\n    _save_half_plus_one_model(export_dir, save_from_keras=save_from_keras)\n    layer = hub.KerasLayer(export_dir, output_shape=[1])\n    self.assertEqual([10, 1],\n                     layer.compute_output_shape(tuple([10, 1])).as_list())\n    layer.get_config()\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testGetConfigFromConfig(self, save_from_keras):\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""half-plus-one"")\n    _save_half_plus_one_model(export_dir, save_from_keras=save_from_keras)\n    layer = hub.KerasLayer(export_dir)\n    in_value = np.array([[10.0]], dtype=np.float32)\n    result = layer(in_value).numpy()\n\n    config = layer.get_config()\n    new_layer = hub.KerasLayer.from_config(_json_cycle(config))\n    new_result = new_layer(in_value).numpy()\n    self.assertEqual(result, new_result)\n\n  def testGetConfigFromConfigWithHParams(self):\n    if tf.__version__ == ""2.0.0-alpha0"":\n      self.skipTest(""b/127938157 broke use of default hparams"")\n    export_dir = os.path.join(self.get_temp_dir(), ""with-hparams"")\n    _save_model_with_hparams(export_dir)  # Has no `save_from_keras` arg.\n    layer = hub.KerasLayer(export_dir, arguments=dict(a=10.))  # Leave b=0.\n    in_value = np.array([[1.], [2.], [3.]], dtype=np.float32)\n    expected_result = np.array([[10.], [20.], [30.]], dtype=np.float32)\n    result = layer(in_value).numpy()\n    self.assertAllEqual(expected_result, result)\n\n    config = layer.get_config()\n    new_layer = hub.KerasLayer.from_config(_json_cycle(config))\n    new_result = new_layer(in_value).numpy()\n    self.assertAllEqual(result, new_result)\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testSaveModelConfig(self, save_from_keras):\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""half-plus-one"")\n    _save_half_plus_one_model(export_dir, save_from_keras=save_from_keras)\n\n    model = tf.keras.Sequential([hub.KerasLayer(export_dir)])\n    in_value = np.array([[10.]], dtype=np.float32)\n    result = model(in_value).numpy()\n\n    json_string = model.to_json()\n    new_model = tf.keras.models.model_from_json(\n        json_string, custom_objects={""KerasLayer"": hub.KerasLayer})\n    new_result = new_model(in_value).numpy()\n    self.assertEqual(result, new_result)\n\n\nclass EstimatorTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests use of KerasLayer in an Estimator\'s model_fn.""""""\n\n  def _half_plus_one_model_fn(self, features, labels, mode, params):\n    inp = features  # This estimator takes a single feature, not a dict.\n    imported = hub.KerasLayer(params[""hub_module""],\n                              trainable=params[""hub_trainable""])\n    model = tf.keras.Sequential([imported])\n    outp = model(inp, training=(mode == tf.estimator.ModeKeys.TRAIN))\n    # https://www.tensorflow.org/alpha/guide/migration_guide#using_a_custom_model_fn\n    # recommends model.get_losses_for() instead of model.losses.\n    model_losses = model.get_losses_for(None) + model.get_losses_for(inp)\n    regularization_loss = tf.add_n(model_losses or [0.0])\n    predictions = dict(output=outp, regularization_loss=regularization_loss)\n\n    total_loss = None\n    if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\n      total_loss = tf.add(\n          tf.compat.v1.losses.mean_squared_error(labels, outp),\n          regularization_loss)\n\n    train_op = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.002)\n      train_op = optimizer.minimize(\n          total_loss, var_list=model.trainable_variables,\n          global_step=tf.compat.v1.train.get_or_create_global_step())\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode, predictions=predictions, loss=total_loss, train_op=train_op)\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testHalfPlusOneRetraining(self, save_from_keras):\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""half-plus-one"")\n    _save_half_plus_one_model(export_dir, save_from_keras=save_from_keras)\n    estimator = tf.estimator.Estimator(\n        model_fn=self._half_plus_one_model_fn,\n        params=dict(hub_module=export_dir, hub_trainable=True))\n\n    # The consumer model computes y = x/2 + 1 as expected.\n    predictions = next(estimator.predict(\n        tf.compat.v1.estimator.inputs.numpy_input_fn(\n            np.array([[0.], [8.], [10.], [12.]], dtype=np.float32),\n            shuffle=False),\n        yield_single_examples=False))\n    self.assertAllEqual(predictions[""output""],\n                        np.array([[1.], [5.], [6.], [7.]], dtype=np.float32))\n    self.assertAllEqual(predictions[""regularization_loss""],\n                        np.array(0.0025, dtype=np.float32))\n\n    # Retrain on y = x/2 + 6 for x near 10.\n    # (Console output should show loss below 0.2.)\n    x = [[9.], [10.], [11.]] * 10\n    y = [[xi[0]/2. + 6] for xi in x]\n    estimator.train(\n        tf.compat.v1.estimator.inputs.numpy_input_fn(\n            np.array(x, dtype=np.float32),\n            np.array(y, dtype=np.float32),\n            batch_size=len(x), num_epochs=None, shuffle=False),\n        steps=10)\n    # The bias is non-trainable and has to stay at 1.0.\n    # To compensate, the kernel weight will grow to almost 1.0.\n    predictions = next(estimator.predict(\n        tf.compat.v1.estimator.inputs.numpy_input_fn(\n            np.array([[0.], [10.]], dtype=np.float32), shuffle=False),\n        yield_single_examples=False))\n    self.assertAllEqual(predictions[""output""][0],\n                        np.array([1.], dtype=np.float32))\n    self.assertAllClose(predictions[""output""][1],\n                        np.array([11.], dtype=np.float32),\n                        atol=0.0, rtol=0.03)\n    self.assertAllClose(predictions[""regularization_loss""],\n                        np.array(0.01, dtype=np.float32),\n                        atol=0.0, rtol=0.06)\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testHalfPlusOneFrozen(self, save_from_keras):\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""half-plus-one"")\n    _save_half_plus_one_model(export_dir, save_from_keras=save_from_keras)\n    estimator = tf.estimator.Estimator(\n        model_fn=self._half_plus_one_model_fn,\n        params=dict(hub_module=export_dir, hub_trainable=False))\n\n    # The consumer model computes y = x/2 + 1 as expected.\n    predictions = next(estimator.predict(\n        tf.compat.v1.estimator.inputs.numpy_input_fn(\n            np.array([[0.], [8.], [10.], [12.]], dtype=np.float32),\n            shuffle=False),\n        yield_single_examples=False))\n    self.assertAllEqual(predictions[""output""],\n                        np.array([[1.], [5.], [6.], [7.]], dtype=np.float32))\n    self.assertAllEqual(predictions[""regularization_loss""],\n                        np.array(0.0, dtype=np.float32))\n\n  def _batch_norm_model_fn(self, features, labels, mode, params):\n    inp = features  # This estimator takes a single feature, not a dict.\n    imported = hub.KerasLayer(params[""hub_module""])\n    var_beta, var_gamma, var_mean, var_variance = _get_batch_norm_vars(imported)\n    if params[""train_batch_norm""]:\n      imported.trainable = True\n      model = tf.keras.Sequential([imported])\n    else:\n      imported.trainable = False\n      # When not training the batch norm layer, we train this instead:\n      dense = tf.keras.layers.Dense(\n          units=1,\n          kernel_initializer=tf.keras.initializers.Constant([[1.5]]),\n          use_bias=False)\n      model = tf.keras.Sequential([imported, dense])\n    outp = model(inp, training=(mode == tf.estimator.ModeKeys.TRAIN))\n    predictions = dict(output=outp,\n                       beta=var_beta.value(), gamma=var_gamma.value(),\n                       mean=var_mean.value(), variance=var_variance.value())\n\n    # https://www.tensorflow.org/alpha/guide/migration_guide#using_a_custom_model_fn\n    # recommends model.get_updates_for() instead of model.updates.\n    update_ops = model.get_updates_for(None) + model.get_updates_for(inp)\n\n    loss = None\n    if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\n      loss = tf.compat.v1.losses.mean_squared_error(labels, outp)\n\n    train_op = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)\n      with tf.control_dependencies(update_ops):\n        train_op = optimizer.minimize(\n            loss, var_list=model.trainable_variables,\n            global_step=tf.compat.v1.train.get_or_create_global_step())\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode, predictions=predictions, loss=loss, train_op=train_op)\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testBatchNormRetraining(self, save_from_keras):\n    """"""Tests imported batch norm with trainable=True.""""""\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""batch-norm"")\n    _save_batch_norm_model(export_dir, save_from_keras=save_from_keras)\n    estimator = tf.estimator.Estimator(\n        model_fn=self._batch_norm_model_fn,\n        params=dict(hub_module=export_dir, train_batch_norm=True))\n\n    # Retrain the imported batch norm layer on a fixed batch of inputs,\n    # which has mean 12.0 and some variance of a less obvious value.\n    # The module learns scale and offset parameters that achieve the\n    # mapping x --> 2*x for the observed mean and variance.\n    x = [[11.], [12.], [13.]]\n    y = [[2*xi[0]] for xi in x]\n    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n        np.array(x, dtype=np.float32),\n        np.array(y, dtype=np.float32),\n        batch_size=len(x), num_epochs=None, shuffle=False)\n    estimator.train(train_input_fn, steps=100)\n    predictions = next(estimator.predict(train_input_fn,\n                                         yield_single_examples=False))\n    self.assertAllClose(predictions[""mean""], np.array([12.0]))\n    self.assertAllClose(predictions[""beta""], np.array([24.0]))\n    self.assertAllClose(predictions[""output""], np.array(y))\n\n    # Evaluating the model operates batch norm in inference mode:\n    # - Batch statistics are ignored in favor of aggregated statistics,\n    #   computing x --> 2*x independent of input distribution.\n    # - Update ops are not run, so this doesn\'t change over time.\n    predict_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n        np.array([[10.], [20.], [30.]], dtype=np.float32),\n        batch_size=3, num_epochs=100, shuffle=False)\n    for predictions in estimator.predict(predict_input_fn,\n                                         yield_single_examples=False):\n      self.assertAllClose(predictions[""output""],\n                          np.array([[20.], [40.], [60.]]))\n    self.assertAllClose(predictions[""mean""], np.array([12.0]))\n    self.assertAllClose(predictions[""beta""], np.array([24.0]))\n\n  @parameterized.named_parameters((""SavedRaw"", False), (""SavedFromKeras"", True))\n  def testBatchNormFreezing(self, save_from_keras):\n    """"""Tests imported batch norm with trainable=False.""""""\n    if save_from_keras: _skip_if_keras_save_too_old(self)\n    export_dir = os.path.join(self.get_temp_dir(), ""batch-norm"")\n    _save_batch_norm_model(export_dir, save_from_keras=save_from_keras)\n    estimator = tf.estimator.Estimator(\n        model_fn=self._batch_norm_model_fn,\n        params=dict(hub_module=export_dir, train_batch_norm=False))\n    x = [[1.], [2.], [3.]]\n    y = [[2*xi[0]] for xi in x]\n    input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n        np.array(x, dtype=np.float32),\n        np.array(y, dtype=np.float32),\n        batch_size=len(x), num_epochs=None, shuffle=False)\n    predictions = next(estimator.predict(input_fn, yield_single_examples=False))\n    self.assertAllClose(predictions[""beta""], np.array([0.0]))\n    self.assertAllClose(predictions[""gamma""], np.array([1.0]))\n    self.assertAllClose(predictions[""mean""], np.array([0.0]))\n    self.assertAllClose(predictions[""variance""], np.array([1.0]))\n\n    # Training the model to x --> 2*x leaves the batch norm layer entirely\n    # unchanged (both trained beta&gamma and aggregated mean&variance).\n    estimator.train(input_fn, steps=20)\n    predictions = next(estimator.predict(input_fn, yield_single_examples=False))\n    self.assertAllClose(predictions[""beta""], np.array([0.0]))\n    self.assertAllClose(predictions[""gamma""], np.array([1.0]))\n    self.assertAllClose(predictions[""mean""], np.array([0.0]))\n    self.assertAllClose(predictions[""variance""], np.array([1.0]))\n    self.assertAllClose(predictions[""output""], np.array(y))\n\n  def _output_shape_list_model_fn(self, features, labels, mode, params):\n    inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n    kwargs = {}\n    if ""output_shape"" in params:\n      kwargs[""output_shape""] = params[""output_shape""]\n    imported = hub.KerasLayer(params[""hub_module""], **kwargs)\n    outp = imported(inp)\n    model = tf.keras.Model(inp, outp)\n\n    out_list = model(features, training=(mode == tf.estimator.ModeKeys.TRAIN))\n    for j, out in enumerate(out_list):\n      i = j+1  # Sample shapes count from one.\n      actual_shape = out.shape.as_list()[1:]  # Without batch size.\n      expected_shape = [i]*i if ""output_shape"" in params else [None]*i\n      self.assertEqual(actual_shape, expected_shape)\n    predictions = {[""one"", ""two"", ""three""][i]: out_list[i] for i in range(3)}\n    imported.get_config()\n\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions,\n                                      loss=None, train_op=None)\n\n  @parameterized.named_parameters((""NoOutputShapes"", False),\n                                  (""WithOutputShapes"", True))\n  def testOutputShapeList(self, pass_output_shapes):\n    export_dir = os.path.join(self.get_temp_dir(), ""obscurely-shaped"")\n    _save_model_with_obscurely_shaped_list_output(export_dir)\n\n    params = dict(hub_module=export_dir)\n    if pass_output_shapes:\n      params[""output_shape""] = [[1], [2, 2], [3, 3, 3]]\n    estimator = tf.estimator.Estimator(\n        model_fn=self._output_shape_list_model_fn,\n        params=params)\n    x = [[1.], [10.]]\n    input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n        np.array(x, dtype=np.float32),\n        batch_size=len(x), num_epochs=None, shuffle=False)\n    predictions = next(estimator.predict(input_fn, yield_single_examples=False))\n    single = predictions[""one""]\n    double = predictions[""two""]\n    triple = predictions[""three""]\n    self.assertAllClose(single, np.array([[1.], [10.]]))\n    self.assertAllClose(double, np.array([[[2., 2.], [2., 2.]],\n                                          [[20., 20.], [20., 20.]]]))\n    self.assertAllClose(triple, np.array(\n        [[[[3., 3., 3.], [3., 3., 3.], [3., 3., 3.]],\n          [[3., 3., 3.], [3., 3., 3.], [3., 3., 3.]],\n          [[3., 3., 3.], [3., 3., 3.], [3., 3., 3.]]],\n         [[[30., 30., 30.], [30., 30., 30.], [30., 30., 30.]],\n          [[30., 30., 30.], [30., 30., 30.], [30., 30., 30.]],\n          [[30., 30., 30.], [30., 30., 30.], [30., 30., 30.]]]]))\n\n\nclass KerasLayerTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Unit tests for KerasLayer.""""""\n\n  @parameterized.named_parameters(\n      (""v1_implicit_tags"", ""hub_module_v1_mini""),\n      (""v2_implicit_tags"", ""saved_model_v2_mini""),\n      )\n  def test_load_with_defaults(self, module_name):\n    inputs, expected_outputs = 10., 11.  # Test modules perform increment op.\n    path = test_utils.get_test_data_path(module_name)\n    layer = hub.KerasLayer(path)\n    output = layer(inputs)\n    self.assertEqual(output, expected_outputs)\n\n  @parameterized.parameters(\n      (""hub_module_v1_mini"", None, None, True),\n      (""hub_module_v1_mini"", None, None, False),\n      (""hub_module_v1_mini"", ""default"", None, True),\n      (""hub_module_v1_mini"", None, ""default"", False),\n      (""hub_module_v1_mini"", ""default"", ""default"", False),\n      )\n  def test_load_legacy_hub_module_v1_with_signature(\n      self, module_name, signature, output_key, as_dict):\n    inputs, expected_outputs = 10., 11.  # Test modules perform increment op.\n    path = test_utils.get_test_data_path(module_name)\n    layer = hub.KerasLayer(path, signature=signature, output_key=output_key,\n                           signature_outputs_as_dict=as_dict)\n    output = layer(inputs)\n    if as_dict:\n      self.assertEqual(output, {""default"": expected_outputs})\n    else:\n      self.assertEqual(output, expected_outputs)\n\n  @parameterized.parameters(\n      (""saved_model_v2_mini"", None, None, False),\n      (""saved_model_v2_mini"", ""serving_default"", None, True),\n      (""saved_model_v2_mini"", ""serving_default"", ""output_0"", False),\n      )\n  def test_load_callable_saved_model_v2_with_signature(\n      self, module_name, signature, output_key, as_dict):\n    inputs, expected_outputs = 10., 11.  # Test modules perform increment op.\n    path = test_utils.get_test_data_path(module_name)\n    layer = hub.KerasLayer(path, signature=signature, output_key=output_key,\n                           signature_outputs_as_dict=as_dict)\n    output = layer(inputs)\n    if as_dict:\n      self.assertIsInstance(output, dict)\n      self.assertEqual(output[""output_0""], expected_outputs)\n    else:\n      self.assertEqual(output, expected_outputs)\n\n  @parameterized.parameters(\n      (""hub_module_v1_mini"", None, None, True),\n      (""hub_module_v1_mini"", None, None, False),\n      (""hub_module_v1_mini"", ""default"", None, True),\n      (""hub_module_v1_mini"", None, ""default"", False),\n      (""hub_module_v1_mini"", ""default"", ""default"", False),\n      (""saved_model_v2_mini"", None, None, False),\n      (""saved_model_v2_mini"", ""serving_default"", None, True),\n      (""saved_model_v2_mini"", ""serving_default"", ""output_0"", False),\n      )\n  def test_keras_layer_get_config(\n      self, module_name, signature, output_key, as_dict):\n    inputs = 10.  # Test modules perform increment op.\n    path = test_utils.get_test_data_path(module_name)\n    layer = hub.KerasLayer(path, signature=signature, output_key=output_key,\n                           signature_outputs_as_dict=as_dict)\n    outputs = layer(inputs)\n    config = layer.get_config()\n    new_layer = hub.KerasLayer.from_config(_json_cycle(config))\n    new_outputs = new_layer(inputs)\n    self.assertEqual(outputs, new_outputs)\n\n  def test_keras_layer_fails_if_signature_output_not_specified(self):\n    path = test_utils.get_test_data_path(""saved_model_v2_mini"")\n    with self.assertRaisesRegex(\n        ValueError, ""When using a signature, either output_key or ""\n        ""signature_outputs_as_dict=True should be set.""):\n      hub.KerasLayer(path, signature=""serving_default"")\n\n  def test_keras_layer_fails_if_with_outputs_as_dict_but_no_signature(self):\n    path = test_utils.get_test_data_path(""saved_model_v2_mini"")\n    with self.assertRaisesRegex(\n        ValueError,\n        ""signature_outputs_as_dict is only valid if specifying a signature *""):\n      hub.KerasLayer(path, signature_outputs_as_dict=True)\n\n  def test_keras_layer_fails_if_saved_model_v2_with_tags(self):\n    path = test_utils.get_test_data_path(""saved_model_v2_mini"")\n    with self.assertRaises(ValueError):\n      hub.KerasLayer(path, signature=None, tags=[""train""])\n\n  def test_keras_layer_fails_if_setting_both_output_key_and_as_dict(self):\n    path = test_utils.get_test_data_path(""hub_module_v1_mini"")\n    with self.assertRaisesRegex(\n        ValueError, ""When using a signature, either output_key or ""\n        ""signature_outputs_as_dict=True should be set.""):\n      hub.KerasLayer(path, signature=""default"",\n                     signature_outputs_as_dict=True, output_key=""output"")\n\n  def test_keras_layer_fails_if_output_is_not_dict(self):\n    path = test_utils.get_test_data_path(""saved_model_v2_mini"")\n    layer = hub.KerasLayer(path, output_key=""output_0"")\n    with self.assertRaisesRegex(\n        ValueError, ""Specifying `output_key` is forbidden if output type *""):\n      layer(10.)\n\n  def test_keras_layer_fails_if_output_key_not_in_layer_outputs(self):\n    path = test_utils.get_test_data_path(""hub_module_v1_mini"")\n    layer = hub.KerasLayer(path, output_key=""unknown"")\n    with self.assertRaisesRegex(\n        ValueError, ""KerasLayer output does not contain the output key*""):\n      layer(10.)\n\n  def test_keras_layer_fails_if_hub_module_trainable(self):\n    path = test_utils.get_test_data_path(""hub_module_v1_mini"")\n    layer = hub.KerasLayer(path, trainable=True)\n    with self.assertRaisesRegex(ValueError, ""trainable.*=.*True.*unsupported""):\n      layer(10.)\n\n  def test_keras_layer_fails_if_signature_trainable(self):\n    path = test_utils.get_test_data_path(""saved_model_v2_mini"")\n    layer = hub.KerasLayer(path, signature=""serving_default"",\n                           signature_outputs_as_dict=True, trainable=True)\n    layer.trainable = True\n    with self.assertRaisesRegex(ValueError, ""trainable.*=.*True.*unsupported""):\n      layer(10.)\n\n  def test_keras_layer_logs_if_training_zero_variables(self):\n    path = os.path.join(self.get_temp_dir(), ""zero-variables"")\n    _save_model_with_hparams(path)\n    layer = hub.KerasLayer(path, trainable=True)\n    if hasattr(self, ""assertLogs""):  # New in Python 3.4.\n      with self.assertLogs(level=""ERROR"") as logs:\n        layer([[10.]])\n        layer([[10.]])\n      self.assertLen(logs.records, 1)  # Duplicate logging is avoided.\n      self.assertRegexpMatches(logs.records[0].msg, ""zero trainable weights"")\n    else:\n      # Just test that it runs at all.\n      layer([[10.]])\n      layer([[10.]])\n\n\nif __name__ == ""__main__"":\n  # The file under test is not imported if TensorFlow is too old.\n  # In such an environment, this test should be silently skipped.\n  if hasattr(hub, ""KerasLayer""):\n    # At this point, we are either in in a late TF1 version or in TF2.\n    # In TF1, we need to enable V2-like behavior, notably eager execution.\n    # `tf.enable_v2_behavior` seems available and should be preferred.\n    # The alternative `tf.enable_eager_behavior` has been around for longer, and\n    # will be enabled if `tf.enable_v2_behavior` is not available.\n    # In TF2, those enable_*() methods are unnecessary and no longer available.\n    if hasattr(tf, ""enable_v2_behavior""):\n      tf.enable_v2_behavior()\n    elif hasattr(tf, ""enable_eager_behavior""):\n      tf.enable_eager_behavior()\n    tf.test.main()\n'"
tensorflow_hub/meta_graph_lib.py,2,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""MetaGraph lib provides utilities to manipulate MetaGraphDefs.\n\nThis is an internal Hub utility and not part of the public API.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nfrom absl import logging\nimport tensorflow as tf\n\nfrom tensorflow_hub import tf_v1\n\n\ndef prepend_name_scope(name, import_scope):\n  """"""Prepends name scope to a name.""""""\n  # Based on tensorflow/python/framework/ops.py implementation.\n  if import_scope:\n    try:\n      str_to_replace = r""([\\^]|loc:@|^)(.*)""\n      return re.sub(str_to_replace, r""\\1"" + import_scope + r""/\\2"",\n                    tf.compat.as_str_any(name))\n    except TypeError as e:\n      # If the name is not of a type we can process, simply return it.\n      logging.warning(e)\n      return name\n  else:\n    return name\n\n\ndef prefix_shared_name_attributes(meta_graph, absolute_import_scope):\n  """"""In-place prefixes shared_name attributes of nodes.""""""\n  shared_name_attr = ""shared_name""\n  for node in meta_graph.graph_def.node:\n    shared_name_value = node.attr.get(shared_name_attr, None)\n    if shared_name_value and shared_name_value.HasField(""s""):\n      if shared_name_value.s:\n        node.attr[shared_name_attr].s = tf.compat.as_bytes(\n            prepend_name_scope(\n                shared_name_value.s, import_scope=absolute_import_scope))\n\n\ndef mark_backward(output_tensor, used_node_names):\n  """"""Function to propagate backwards in the graph and mark nodes as used.\n\n  Traverses recursively through the graph from the end tensor, through the op\n  that generates the tensor, and then to the input tensors that feed the op.\n  Nodes encountered are stored in used_node_names.\n\n  Args:\n    output_tensor: A Tensor which we start the propagation.\n    used_node_names: A list of strings, stores the name of nodes we\'ve marked as\n      visited.\n  """"""\n  op = output_tensor.op\n  if op.name in used_node_names:\n    return\n  used_node_names.add(op.name)\n  for input_tensor in op.inputs:\n    mark_backward(input_tensor, used_node_names)\n  for control_input_op in op.control_inputs:\n    used_node_names.add(control_input_op.name)\n    for input_tensor in control_input_op.inputs:\n      mark_backward(input_tensor, used_node_names)\n\n\ndef prune_unused_nodes(meta_graph, signature_def):\n  """"""Function to prune unused ops given a signature def.\n\n  This function does a graph traversal through from all outputs as\n  defined in the signature_def to collect all used nodes. Then, any\n  nodes which are unused can be discarded. This is useful for graph which are\n  executing eagerly or on TPUs.\n\n  Args:\n    meta_graph: The input/output MetaGraphDef for which we wish to prune.\n   signature_def: A SignatureDef which specifies the outputs from which we wish\n     to start graph traversal.\n  """"""\n  # Instantiate a temporary empty graph so that we have access to Graph API\n  # and import the meta_graph.\n  graph = tf_v1.Graph()\n  with graph.as_default():\n    tf_v1.train.import_meta_graph(meta_graph, input_map={}, import_scope="""")\n    # Traverse from all outputs and mark all nodes.\n    used_node_names = set()\n    for _, tensor_def in signature_def.outputs.items():\n      output_tensor = graph.get_tensor_by_name(tensor_def.name)\n      mark_backward(output_tensor, used_node_names)\n    # Filter out all nodes in the meta_graph that are not used.\n    node_filter_in_list = []\n    for node in meta_graph.graph_def.node:\n      # Make a special exception for VarHandleOp. Removing VarhandleOps\n      # will make the graph not importable as they often leave nodes hanging.\n      # These will be disconnected through the feedmap when importing the\n      # metagraph.\n      if node.name in used_node_names or node.op == ""VarHandleOp"":\n        node_filter_in_list.append(node)\n    del meta_graph.graph_def.node[:]\n    meta_graph.graph_def.node.extend(node_filter_in_list)\n  del graph\n\n\ndef prune_feed_map(meta_graph, feed_map):\n  """"""Function to prune the feedmap of nodes which no longer exist.""""""\n  node_names = [x.name + "":0"" for x in meta_graph.graph_def.node]\n  keys_to_delete = []\n  for k, _ in feed_map.items():\n    if k not in node_names:\n      keys_to_delete.append(k)\n  for k in keys_to_delete:\n    del feed_map[k]\n\n\ndef filter_collections(meta_graph, collections):\n  collections = frozenset(collections)\n  for name in list(meta_graph.collection_def.keys()):\n    if name not in collections:\n      del meta_graph.collection_def[name]\n'"
tensorflow_hub/module.py,11,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The deprecated hub.Module class of TensorFlow Hub.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\n\nimport six\nimport tensorflow as tf\n\nfrom tensorflow_hub import module_spec\nfrom tensorflow_hub import registry\nfrom tensorflow_hub import tensor_info\nfrom tensorflow_hub import tf_v1\n\n\ndef as_module_spec(spec):\n  if isinstance(spec, module_spec.ModuleSpec):\n    return spec\n  elif isinstance(spec, six.string_types):\n    return load_module_spec(spec)\n  else:\n    raise ValueError(""Unknown module spec type: %r"" % type(spec))\n\n\ndef load_module_spec(path):\n  """"""Loads a ModuleSpec from a TF Hub service or the filesystem.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n  For TF2, switch to plain SavedModels and hub.load(); see also hub.resolve().\n\n  Args:\n    path: string describing the location of a module. There are several\n      supported path encoding schemes:\n        a) A URL like ""https://tfhub.dev/the/module/1"" referring to tfhub.dev or\n           another service implementing https://www.tensorflow.org/hub/hosting.\n        b) A URL like ""https://example.com/module.tar.gz"" that points to a\n           compressed tarball directly, as long as that web server ignores\n           the query parameters added by https://www.tensorflow.org/hub/hosting.\n        c) Any filesystem location of a module directory (e.g. /module_dir\n           for a local filesystem). All filesystems implementations provided\n           by Tensorflow are supported.\n        d) Private name resolution schemes added by the maintainer of your\n           local installation of the tensorflow_hub library (usually none).\n\n  Returns:\n    A ModuleSpec.\n\n  Raises:\n    ValueError: on unexpected values in the module spec.\n    tf.errors.OpError: on file handling exceptions.\n  """"""\n  path = registry.resolver(path)\n  return registry.loader(path)\n\n\ndef export_module_spec(spec, path, checkpoint_path, name_transform_fn):\n  """"""Helper function to ModuleSpec.export().""""""\n  with tf.Graph().as_default():\n    m = Module(spec)\n    assign_map = {\n        name_transform_fn(name): value for name, value in m.variable_map.items()\n    }\n    tf_v1.train.init_from_checkpoint(checkpoint_path, assign_map)\n    init_op = tf_v1.initializers.global_variables()\n    with tf_v1.Session() as session:\n      session.run(init_op)\n      m.export(path, session)\n\n\n# Module class provides a unified access to all ModuleSpecs implementations and\n# should not contain specific implementation code in it (e.g. SavedModel code).\nclass Module(object):\n  """"""Part of a TensorFlow 1 model that can be transferred between models.\n\n  DEPRECATION NOTE: The hub.Module API and file format works for TF1 only.\n  For TF2, switch to plain SavedModels and hub.load().\n\n  A Module represents a part of a TensorFlow graph that can be exported to disk\n  (based on the SavedModel format) and later re-loaded. A Module has a defined\n  interface that allows it to be used in a replaceable way, with little or no\n  knowledge of its internals and its serialization format. Example:\n\n  ```python\n  m = hub.Module(""/tmp/text-embedding"")\n  embeddings = m(sentences)\n  ```\n\n  The module to instantiate is defined by its spec (a `ModuleSpec` or a\n  path where to load it from) which contains the module weights, assets and\n  signatures.\n\n  During instantiation the Module adds the state (e.g. variables and tables ops)\n  to the current graph. Afterwards, the method `__call__()` allows to apply the\n  module `signatures` multiple times, which adds ops for the computation.\n\n  A Module may provide different variants of its graph for different purposes\n  (say, training or serving, which may behave differently, e.g., for batch\n  normalization). Graph variants are identified by sets of string-valued tags.\n  The graph variant used to create a module that is exported must define all the\n  variables needed by any other graph variant that is subsequently used.\n\n  To make it possible to easily replace a module with another, they all assume\n  that they will be used with common TensorFlow conventions such as session\n  initialization and restore, use of collections for variables, regularization\n  losses and updates, etc.\n  """"""\n\n  def __init__(self, spec, trainable=False, name=""module"", tags=None):\n    """"""Constructs a Module to be used in the current graph.\n\n    This creates the module `state-graph` under an unused variable_scope based\n    on `name`. During this call a Module will:\n\n    - Add GLOBAL_VARIABLES under its scope. Those variables may be added to\n      to the TRAINABLE_VARIABLES collection (depending on `trainable` parameter)\n      and to the MODEL_VARIABLES. The variables must be initialized before use,\n      and can be checkpointed as usual.\n\n    - Add ops to the INIT_TABLE_OPS collection, which must be run during session\n      initialization and add constant tensors to ASSET_FILEPATHS that are\n      needed during the execution of such ops.\n\n    - Add tensors to the REGULARIZATION_LOSSES collection (depending on\n      `trainable` parameter).\n\n    Args:\n      spec: A ModuleSpec defining the Module to instantiate or a path where\n        to load a ModuleSpec from via `load_module_spec`.\n      trainable: whether the Module is trainable. If False, no variables are\n        added to TRAINABLE_VARIABLES collection, and no tensors are added to\n        REGULARIZATION_LOSSES collection.\n      name: A string, the variable scope name under which to create the Module.\n        It will be uniquified and the equivalent name scope must be unused.\n      tags: A set of strings specifying the graph variant to use.\n\n    Raises:\n      RuntimeError: explaning the reason why it failed to instantiate the\n        Module.\n      ValueError: if the requested graph variant does not exists.\n      tf.errors.NotFoundError: if the requested graph contains unknown ops.\n    """"""\n    self._graph = tf_v1.get_default_graph()\n    self._spec = as_module_spec(spec)\n    self._trainable = trainable\n\n    self._tags = set(tags or [])\n    if self._tags not in self._spec.get_tags():\n      tags = sorted(list(tags)) if tags else tags\n      raise ValueError(""No such graph variant: tags=%r"" % tags)\n\n    abs_state_scope = _try_get_state_scope(name, mark_name_scope_used=False)\n    self._name = abs_state_scope.split(""/"")[-2]\n\n    abs_parent_scope = abs_state_scope.split(""/"")[:-2]\n    if abs_parent_scope:\n      abs_parent_scope = ""/"".join(abs_parent_scope) + ""/""\n    else:\n      abs_parent_scope = """"\n\n    with tf.name_scope(abs_parent_scope):\n      # pylint: disable=protected-access\n      self._impl = self._spec._create_impl(\n          name=self._name,\n          trainable=self._trainable,\n          tags=self._tags)\n      # pylint: enable=protected-access\n\n  def __call__(self, inputs=None,  # pylint: disable=invalid-name\n               _sentinel=None, signature=None, as_dict=None):\n    """"""Instantiates a module signature in the graph.\n\n    Example calls:\n\n    ```python\n      # Use default signature with one input and default output.\n      embeddings = m([""hello world"", ""good morning""])\n\n      # Use ""encode"" signature with one input and default output.\n      encodings = m([""hello world""], signature=""encode"")\n\n      # Use default signature with input dict and output dict.\n      dict_outputs = m({""text"": [...], ""lang"": [...]}, as_dict=True)\n    ```\n\n    The method __call__() allows to create the graph ops that compute a\n    signature outputs given the inputs and using this module instance state.\n    Each signature can be applied multiple times with different inputs and they\n    all share the same module state.\n\n    A Module may define multiple signatures. Use `signature=<name>` to identify\n    the specific signature to instantiate. If omitted or None, the default\n    signature is used.\n\n    A signature may define various outputs. Use `as_dict=True` to return a dict\n    of all outputs. If omitted or False, the output named \'default\' is\n    returned.\n\n    During this call a Module will:\n\n    - Add ops in the current name scope to convert the inputs in tensors to feed\n      to the signature.\n\n    - Add ops to the UPDATE_OPS collection which depend on at least one of the\n      provided inputs if the Module was constructed with `trainable=True`.\n\n    - Add constant tensors to ASSET_FILEPATHS, even if those are not needed\n      directly needed for the signature.\n\n    Note: `hub.Module` implementation depends on graph pruning that happens\n    usually during `session.run` as so it can lead to errors when used inside\n    function graphs that execute all its ops (e.g. `tf.data.Dataset.map`).\n\n    Args:\n      inputs: Inputs to the signature. A dict from input names to tensor\n        values. If the signature only expects one input, one may pass\n        a single value. If the signature has no inputs, it may be omitted.\n      _sentinel: Used to prevent positional parameters besides `inputs`.\n      signature: A string with the signature name to apply. If none, the\n        default signature is used.\n      as_dict: A boolean indicating whether to the return all the outputs\n        of the signature as a dict or return only the default output.\n\n    Returns:\n      A tensor (single or sparse) if the signature defines a default output or\n      a dict from strings (output names) to tensors if `as_dict=True` is used.\n\n    Raises:\n      TypeError: If there is a mismatch on arguments, inputs or outputs of\n        the module signature.\n      RuntimeError: If there are errors during creation of the signature graph.\n    """"""\n    if self._graph is not tf_v1.get_default_graph():\n      raise RuntimeError(\n          ""Module must be applied in the graph it was instantiated for."")\n\n    signature = self._impl.get_signature_name(signature)\n    # SavedModel non-default signatures automatically includes \':\' in them,\n    # but that is an invalid character for a name that is used as part\n    # of variable scopes.\n    safe_signature = signature.replace("":"", ""_"")\n    name = ""%s_apply_%s"" % (self._name, safe_signature)\n\n    dict_inputs = _convert_dict_inputs(\n        inputs, self._spec.get_input_info_dict(signature=signature,\n                                               tags=self._tags))\n\n    dict_outputs = self._impl.create_apply_graph(\n        signature=signature,\n        input_tensors=dict_inputs,\n        name=name)\n    return _prepare_outputs(dict_outputs, as_dict=as_dict)\n\n  def get_signature_names(self):\n    """"""Returns the module\'s signature names as an iterable of strings.""""""\n    return self._spec.get_signature_names(tags=self._tags)\n\n  def get_input_info_dict(self, signature=None):\n    """"""Describes the inputs required by a signature.\n\n    Args:\n      signature: A string with the signature to get inputs information for.\n        If None, the default signature is used if defined.\n\n    Returns:\n      The result of ModuleSpec.get_input_info_dict() for the given signature,\n      and the graph variant selected by `tags` when this Module was initialized.\n\n    Raises:\n      KeyError: if there is no such signature.\n    """"""\n    return self._spec.get_input_info_dict(signature=signature, tags=self._tags)\n\n  def get_output_info_dict(self, signature=None):\n    """"""Describes the outputs provided by a signature.\n\n    Args:\n      signature: A string with the signature to get ouputs information for.\n        If None, the default signature is used if defined.\n\n    Returns:\n      The result of ModuleSpec.get_output_info_dict() for the given signature,\n      and the graph variant selected by `tags` when this Module was initialized.\n\n    Raises:\n      KeyError: if there is no such signature.\n    """"""\n    return self._spec.get_output_info_dict(signature=signature, tags=self._tags)\n\n  def get_attached_message(self, key, message_type, required=False):\n    """"""Calls ModuleSpec.get_attached_message(); see there for more.""""""\n    return self._spec.get_attached_message(key, message_type,\n                                           tags=self._tags, required=required)\n\n  def export(self, path, session):\n    """"""Exports the module with the variables from the session in `path`.\n\n    Note that it is the module definition in the ModuleSpec used to create this\n    module that gets exported. The session is only used to provide the value\n    of variables.\n\n    Args:\n      path: path where to export the module to.\n      session: session where to export the variables from.\n\n    Raises:\n      RuntimeError: if there is an issue during the export.\n    """"""\n    if self._graph is not tf_v1.get_default_graph():\n      raise RuntimeError(""default graph differs from the graph where the ""\n                         ""module was instantiated."")\n    if self._graph is not session.graph:\n      raise RuntimeError(""session graph differs from the graph where the ""\n                         ""module was instantiated."")\n    self._impl.export(path, session)\n\n  @property\n  def variable_map(self):\n    """"""Map from original variable names into tf.Variables (or lists of them).\n\n    This map translates between variable names relative to the module and the\n    corresponding Variable objects that have been created by instantiating it\n    in the current graph (with the applicable scoping added). Each key in the\n    map is a variable name as created by running the module\'s defining\n    `module_fn` in the root scope of an empty graph. Each value in the map is\n    a Variable object, or in case of partitioned variables a list of Variable\n    objects.\n\n    This property can be used with `tf.init_from_checkpoint` as `assignment_map`\n    in order to restore a pre-trained checkpoint into a Module before calling\n    `Module.export()`.\n\n    Returns:\n      A dict from the variable names in the Module to the instantiated\n      tf.Variables or list of tf.Variables (if partitioned). The keys of this\n      map are the same regardless of the scope of where the Module was\n      instantiated.\n    """"""\n    return self._impl.variable_map\n\n  @property\n  def variables(self):\n    """"""Returns the list of all tf.Variables created by module instantiation.""""""\n    result = []\n    for _, value in sorted(self.variable_map.items()):\n      if isinstance(value, list):\n        result.extend(value)\n      else:\n        result.append(value)\n    return result\n\n\ndef _try_get_state_scope(name, mark_name_scope_used=True):\n  """"""Returns a fresh variable/name scope for a module\'s state.\n\n  In order to import a module into a given scope without major complications\n  we require the scope to be empty. This function deals with deciding an unused\n  scope where to define the module state. This is non trivial in cases where\n  name_scope and variable_scopes are out of sync, e.g. tpus or re-entering\n  scopes.\n\n  Args:\n    name: A string with the name of the module as supplied by the client.\n    mark_name_scope_used: a boolean, indicating whether to mark the name\n        scope of the returned value as used.\n\n  Raises:\n    RuntimeError: if the name scope of the freshly created variable scope is\n        already used.\n  """"""\n  tmp_scope_name = tf_v1.get_variable_scope().name\n  if tmp_scope_name:\n    tmp_scope_name += ""/""\n  with tf.name_scope(tmp_scope_name):\n    # Pick an unused variable scope.\n    with tf_v1.variable_scope(\n        None, default_name=name, auxiliary_name_scope=False) as vs:\n      abs_state_scope = vs.name + ""/""\n    # Verify that the name scope is available and mark it used if requested.\n    graph = tf_v1.get_default_graph()\n    unique_name_scope = graph.unique_name(name, mark_name_scope_used) + ""/""\n    if unique_name_scope != abs_state_scope:\n      raise RuntimeError(\n          ""variable_scope %s was unused but the corresponding ""\n          ""name_scope was already taken."" % abs_state_scope)\n  return abs_state_scope\n\n\ndef _prepare_dict_inputs(inputs, tensor_info_map):\n  """"""Converts inputs to a dict of inputs and checks extra/missing args.\n\n  Args:\n    inputs: inputs fed to Module.__call__().\n    tensor_info_map: A map from string to `tensor_info.ParsedTensorInfo`\n      describing the signature inputs.\n\n  Returns:\n    A dict of values with the same keys as tensor_info_map.\n\n  Raises:\n    TypeError: If it fails to convert the input values into a dict of tensors\n      to feed to the signature instantiation.\n  """"""\n  if inputs is None:\n    dict_inputs = {}\n  elif isinstance(inputs, dict):\n    dict_inputs = inputs\n  elif len(tensor_info_map) == 1:\n    dict_inputs = {list(tensor_info_map.keys())[0]: inputs}\n  elif not tensor_info_map:\n    raise TypeError(""Signature expects no inputs."")\n  else:\n    raise TypeError(""Signature expects multiple inputs. Use a dict."")\n\n  dict_inputs_keys = set(dict_inputs.keys())\n  tensor_info_map_keys = set(tensor_info_map.keys())\n  if dict_inputs_keys != tensor_info_map_keys:\n    raise TypeError(""Cannot convert dict_inputs: missing %r, extra given %r"" %\n                    (sorted(list(tensor_info_map_keys - dict_inputs_keys)),\n                     sorted(list(dict_inputs_keys - tensor_info_map_keys))))\n\n  return dict_inputs\n\n\ndef _convert_dict_inputs(inputs, tensor_info_map):\n  """"""Converts from inputs into dict of input tensors.\n\n  This handles:\n    - putting inputs into a dict, per _prepare_dict_inputs(),\n    - converting all input values into tensors compatible with the\n      expected input tensor (dtype, shape).\n    - check sparse/non-sparse tensor types.\n\n  Args:\n    inputs: inputs fed to Module.__call__().\n    tensor_info_map: A map from string to `tensor_info.ParsedTensorInfo`\n      describing the signature inputs.\n\n  Returns:\n    A dict of tensors to feed to the signature instantiation.\n\n  Raises:\n    TypeError: If it fails to convert the input values into a dict of tensors\n      to feed to the signature instantiation.\n  """"""\n  dict_inputs = _prepare_dict_inputs(inputs, tensor_info_map)\n  return tensor_info.convert_dict_to_compatible_tensor(dict_inputs,\n                                                       tensor_info_map)\n\n\ndef _prepare_outputs(dict_outputs, as_dict):\n  """"""Converts from dict outputs into the return value of Module.__call__().\n\n  Args:\n    dict_outputs: A dict output from applying a signature.\n    as_dict: A boolean indicating whether to return the outputs of the Module\n      as a dict or return the output named \'default.\n\n  Returns:\n    A tensor with the output named \'default\' or a dict of output tensors if\n    `as_dict=True`.\n\n  Raises:\n    TypeError: If as_dict is False and there is no output named \'default\'.\n  """"""\n  if as_dict:\n    return dict_outputs\n  if ""default"" in dict_outputs:\n    return dict_outputs[""default""]\n  else:\n    raise TypeError(""There is no output named \'default\'. Use as_dict=True."")\n\n\n@contextlib.contextmanager\ndef eval_function_for_module(spec, tags=None):\n  """"""Context manager that yields a function to directly evaluate a hub.Module.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n  For TF2, switch to plain SavedModels and hub.load().\n  Eager evalutaion in TF2 obviates the need for this helper.\n\n  This creates a separate graph, in which all of the signatures of the module\n  are instantiated. Then, it creates a session and initializes the module\n  variables. Finally, it returns a function which can be used to evaluate the\n  module signatures.\n\n  The function returned by eval_function_for_module has the same syntax as\n  Module.__call__ , except that inputs and outputs are not tensors but actual\n  values as used with Session.run().\n\n  ```python\n  with hub.eval_function_for_module(""/tmp/text-embedding"") as f:\n    # The module can be directly evaluated using f without constructing a graph.\n    embeddings = f([""Hello world!"",], signature=""mysignature"")\n  ```\n\n  Args:\n    spec: A ModuleSpec defining the Module to instantiate or a path where to\n      load a ModuleSpec from via `load_module_spec`.\n    tags: A set of strings specifying the graph variant to use.\n\n  Yields:\n    A function whose keyword arguments are fed into the tfhub module and which\n      returns a dictionary with the value of the output tensors.\n\n  Raises:\n    RuntimeError: explaning the reason why it failed to instantiate the\n      Module.\n    ValueError: if the requested graph variant does not exists.\n  """"""\n  # We create a separate graph and add all the signatures of the module to it.\n  original_graph = tf_v1.get_default_graph()\n  with tf.Graph().as_default():\n    module = Module(spec, tags=tags)\n    input_tensors_per_signature = {}\n    output_tensors_per_signature = {}\n    for signature in module.get_signature_names():\n      # We scope with the signature name as different signatures will likely\n      # contain tensors with the same name (e.g. the input and output tensors).\n      with tf_v1.variable_scope(signature):\n        input_tensors = {}\n        for name, tensorinfo in module.get_input_info_dict(signature).items():\n          # We need to be care with the shape as it may be fully-known,\n          # partially-known or even unknown.\n          shape = tensorinfo.get_shape()\n          effective_shape = None if shape.dims is None else shape.as_list()\n          if tensorinfo.is_sparse:\n            input_tensors[name] = tf_v1.sparse_placeholder(\n                tensorinfo.dtype, shape=effective_shape, name=name)\n          else:\n            input_tensors[name] = tf_v1.placeholder(\n                tensorinfo.dtype, shape=effective_shape, name=name)\n        input_tensors_per_signature[signature] = input_tensors\n        output_tensors_per_signature[signature] = module(\n            input_tensors_per_signature[signature],\n            signature=signature,\n            as_dict=True)\n\n  # Evaluating the tfhub module requires an active tensorflow session.\n    with tf_v1.train.SingularMonitoredSession() as sess:\n\n      def func(\n          inputs=None,\n          _sentinel=None,  # pylint: disable=invalid-name\n          signature=None,\n          as_dict=None):\n        """"""Function that directly evaluates a signature in the module.""""""\n        signature = signature or ""default""\n        input_tensors = input_tensors_per_signature[signature]\n\n        dict_inputs = _prepare_dict_inputs(inputs, input_tensors)\n\n        # The input arguments are directly fed into the session.\n        feed_dict = {\n            input_tensors[key]: value for key, value in dict_inputs.items()\n        }\n        output = output_tensors_per_signature[signature]\n        output = _prepare_outputs(output, as_dict)\n        return sess.run(output, feed_dict=feed_dict)\n\n      with original_graph.as_default():\n        # Yield the function since that will keep the session alive until the\n        # user exits the context.\n        yield func\n'"
tensorflow_hub/module_impl.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ModuleImpl interface.\n\nIn order to be able to expand the types of Modules that are supported without\nusers having to call the right constructor we use a ""pointer-to-implementation""\npattern:\n\n`Module` is the public API class that every user should instantiate. It\'s\nconstructor uses `spec` to create a `ModuleImpl` that encapsulates each specific\nimplementation.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\n\nclass ModuleImpl(object):\n  """"""Internal module implementation interface.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def create_apply_graph(self, signature, input_tensors, name):\n    """"""Applies the module signature to inputs.\n\n    Args:\n      signature: A string with the signature to create.\n      input_tensors: A dictionary of tensors with the inputs.\n      name: A name scope under which to instantiate the signature.\n\n    Returns:\n      A dictionary of output tensors from applying the signature.\n    """"""\n    raise NotImplementedError()\n\n  def get_signature_name(self, signature):\n    """"""Resolves a signature name.""""""\n    if not signature:\n      return ""default""\n    return signature\n\n  @abc.abstractmethod\n  def export(self, path, session):\n    """"""See `Module.export()`.""""""\n    raise NotImplementedError()\n\n  @abc.abstractproperty\n  def variable_map(self):\n    """"""See `Module.variable_map`.""""""\n    raise NotImplementedError()\n'"
tensorflow_hub/module_spec.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The ModuleSpec interface, for the deprecated hub.Module class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\n\nclass ModuleSpec(object):\n  """"""Represents the contents of a hub.Module before it has been instantiated.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n  For TF2, switch to plain SavedModels and hub.load().\n\n  A ModuleSpec is the blueprint used by `Module` to create one or more instances\n  of a specific module in one or more graphs. The details on how to construct\n  the Module are internal to the library implementation but methods to inspect\n  a Module interface are public.\n\n  Note: Do not instantiate this class directly. Use `hub.load_module_spec` or\n  `hub.create_module_spec`.\n  """"""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self):\n    """"""Do not instantiate directly.""""""\n    pass\n\n  def export(self, path, _sentinel=None,  # pylint: disable=invalid-name\n             checkpoint_path=None, name_transform_fn=None):\n    """"""Exports a ModuleSpec with weights taken from a checkpoint.\n\n    This is an helper to export modules directly from a ModuleSpec\n    without having to create a session and set the variables to the\n    intended values.\n\n    Example usage:\n\n    ```python\n    spec = hub.create_module_spec(module_fn)\n    spec.export(""/path/to/export_module"",\n                checkpoint_path=""/path/to/training_model"")\n    ```\n\n    In some cases, the variable name in the checkpoint does not match\n    the variable name in the module. It is possible to work around that\n    by providing a checkpoint_map_fn that performs the variable mapping.\n    For example with: `name_transform_fn = lambda x: ""extra_scope/"" + x`.\n\n    Args:\n      path: path where to export the module to.\n      _sentinel: used to prevent positional arguments besides `path`.\n      checkpoint_path: path where to load the weights for the module.\n        Mandatory parameter and must be passed by name.\n      name_transform_fn: optional function to provide mapping between\n        variable name in the module and the variable name in the checkpoint.\n\n    Raises:\n      ValueError: if missing mandatory `checkpoint_path` parameter.\n    """"""\n    from tensorflow_hub.module import export_module_spec  # pylint: disable=g-import-not-at-top\n    if not checkpoint_path:\n      raise ValueError(""Missing mandatory `checkpoint_path` parameter"")\n    name_transform_fn = name_transform_fn or (lambda x: x)\n    export_module_spec(self, path, checkpoint_path, name_transform_fn)\n\n  @abc.abstractmethod\n  def get_signature_names(self, tags=None):\n    """"""Returns the module\'s signature names as an iterable of strings.""""""\n    pass\n\n  @abc.abstractmethod\n  def get_tags(self):\n    """"""Lists the graph variants as an iterable of set of tags.""""""\n    return [set()]\n\n  @abc.abstractmethod\n  def get_input_info_dict(self, signature=None, tags=None):\n    """"""Describes the inputs required by a signature.\n\n    Args:\n      signature: A string with the signature to get inputs information for.\n        If None, the default signature is used if defined.\n      tags: Optional set of strings, specifying the graph variant to query.\n\n    Returns:\n      A dict from input names to objects that provide (1) a property `dtype`,\n      (2) a method `get_shape()` and (3) a read-only boolean property\n      `is_sparse`. The first two are compatible with the common API of Tensor\n      and SparseTensor objects.\n\n    Raises:\n      KeyError: if there is no such signature or graph variant.\n    """"""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_output_info_dict(self, signature=None, tags=None):\n    """"""Describes the outputs provided by a signature.\n\n    Args:\n      signature: A string with the signature to get ouputs information for.\n        If None, the default signature is used if defined.\n      tags: Optional set of strings, specifying the graph variant to query.\n\n    Returns:\n      A dict from input names to objects that provide (1) a property `dtype`,\n      (2) a method `get_shape()` and (3) a read-only boolean property\n      `is_sparse`. The first two are compatible with the common API of Tensor\n      and SparseTensor objects.\n\n    Raises:\n      KeyError: if there is no such signature or graph variant.\n    """"""\n    raise NotImplementedError()\n\n  def get_attached_message(self, key, message_type, tags=None, required=False):\n    """"""Returns the message attached to the module under the given key, or None.\n\n    Module publishers can attach protocol messages to modules at creation time\n    to provide module consumers with additional information, e.g., on module\n    usage or provenance (see see hub.attach_message()). A typical use would be\n    to store a small set of named values with modules of a certain type so\n    that a support library for consumers of such modules can be parametric\n    in those values.\n\n    This method can also be called on a Module instantiated from a ModuleSpec,\n    then `tags` are set to those used in module instatiation.\n\n    Args:\n      key: A string with the key of an attached message.\n      message_type: A concrete protocol message class (*not* object) used\n        to parse the attached message from its serialized representation.\n        The message type for a particular key must be advertised with the key.\n      tags: Optional set of strings, specifying the graph variant from which\n        to read the attached message.\n      required: An optional boolean. Setting it true changes the effect of\n        an unknown `key` from returning None to raising a KeyError with text\n        about attached messages.\n\n    Returns:\n      An instance of `message_type` with the message contents attached to the\n      module, or `None` if `key` is unknown and `required` is False.\n\n    Raises:\n      KeyError: if `key` is unknown and `required` is True.\n    """"""\n    attached_bytes = self._get_attached_bytes(key, tags)\n    if attached_bytes is None:\n      if required:\n        raise KeyError(""No attached message for key \'%s\' in graph version %s ""\n                       ""of Hub Module"" % (key, sorted(tags or [])))\n      else:\n        return None\n    message = message_type()\n    message.ParseFromString(attached_bytes)\n    return message\n\n  @abc.abstractmethod\n  def _get_attached_bytes(self, key, tags):\n    """"""Internal implementation of the storage of attached messages.\n\n    Args:\n      key: The `key` argument to get_attached_message().\n      tags: The `tags` argument to get_attached_message().\n\n    Returns:\n      The serialized message attached under `key` to the graph version\n      identified by `tags`, or None if absent.\n    """"""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def _create_impl(self, name, trainable, tags):\n    """"""Internal.\n\n    Args:\n      name: A string with the an unused name scope.\n      trainable: A boolean, whether the Module is to be instantiated as\n        trainable.\n      tags: A set of strings specifying the graph variant to use.\n\n    Returns:\n      A ModuleImpl.\n    """"""\n    raise NotImplementedError()\n'"
tensorflow_hub/module_test.py,31,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Unit tests for tensorflow_hub.module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow_hub import module\nfrom tensorflow_hub import module_impl\nfrom tensorflow_hub import module_spec\nfrom tensorflow_hub import tensor_info\nfrom tensorflow_hub import tf_v1\n\n\nclass TestConvertInputsOutputs(tf.test.TestCase):\n\n  def testSingleInput(self):\n    inputs_info = {\n        ""x"": tensor_info.ParsedTensorInfo(\n            tf.float32,\n            tf.TensorShape([None]),\n            is_sparse=False),\n    }\n    def _check(dict_inputs):\n      self.assertEqual(len(dict_inputs), 1)\n      self.assertEqual(dict_inputs[""x""].dtype, tf.float32)\n      self.assertTrue(dict_inputs[""x""].shape.is_compatible_with([None]))\n\n    _check(module._convert_dict_inputs([1, 2], inputs_info))\n    _check(module._convert_dict_inputs({""x"": [1, 2]}, inputs_info))\n\n    with self.assertRaisesRegexp(TypeError, r""missing \\[\'x\'\\]""):\n      module._convert_dict_inputs(None, inputs_info)\n\n    with self.assertRaisesRegexp(TypeError, r""extra given \\[\'y\'\\]""):\n      module._convert_dict_inputs({""x"": [1, 2], ""y"": [1, 2]}, inputs_info)\n\n  def testNoInputs(self):\n    self.assertEqual(module._convert_dict_inputs(None, {}), {})\n    self.assertEqual(module._convert_dict_inputs({}, {}), {})\n\n    with self.assertRaisesRegexp(TypeError, ""expects no inputs""):\n      module._convert_dict_inputs([None], {})\n\n    with self.assertRaisesRegexp(TypeError, ""expects no inputs""):\n      module._convert_dict_inputs(1, {})\n\n    with self.assertRaisesRegexp(TypeError, r""extra given \\[\'x\'\\]""):\n      module._convert_dict_inputs({""x"": 1}, {})\n\n  def testMultipleInputs(self):\n    inputs_info = {\n        ""x"": tensor_info.ParsedTensorInfo(\n            tf.float32,\n            tf.TensorShape([None]),\n            is_sparse=False),\n        ""y"": tensor_info.ParsedTensorInfo(\n            tf.float32,\n            tf.TensorShape([None]),\n            is_sparse=False),\n    }\n    def _check(dict_inputs):\n      self.assertEqual(len(dict_inputs), 2)\n      for key in (""x"", ""y""):\n        self.assertEqual(dict_inputs[key].dtype, tf.float32)\n        self.assertTrue(dict_inputs[key].shape.is_compatible_with([None]))\n\n    _check(module._convert_dict_inputs({""x"": [1, 2], ""y"": [1, 2]},\n                                       inputs_info))\n\n    with self.assertRaisesRegexp(TypeError, r""missing \\[\'x\', \'y\'\\]""):\n      module._convert_dict_inputs(None, inputs_info)\n    with self.assertRaisesRegexp(TypeError, r""missing \\[\'x\', \'y\'\\]""):\n      module._convert_dict_inputs({}, inputs_info)\n    with self.assertRaisesRegexp(TypeError, r""missing \\[\'x\', \'y\'\\]""):\n      module._convert_dict_inputs({""z"": 1}, inputs_info)\n\n    with self.assertRaisesRegexp(\n        TypeError, ""Signature expects multiple inputs. Use a dict.""):\n      module._convert_dict_inputs(1, inputs_info)\n\n  def testOutputWithDefault(self):\n    outputs = {""default"": ""result"", ""extra"": ""dbg info""}\n    self.assertEquals(module._prepare_outputs(outputs, as_dict=False), ""result"")\n    self.assertEquals(module._prepare_outputs(outputs, as_dict=True), outputs)\n\n  def testDictOutput(self):\n    outputs = {""x"": 1, ""y"": 2}\n    self.assertEquals(module._prepare_outputs(outputs, as_dict=True), outputs)\n    with self.assertRaisesRegexp(TypeError, r""Use as_dict=True.""):\n      self.assertEquals(module._prepare_outputs(outputs, as_dict=False),\n                        outputs)\n\n\nclass GetStateScopeTest(tf.test.TestCase):\n\n  def testGetStateScope(self):\n    with tf.Graph().as_default():\n      self.assertEqual(module._try_get_state_scope(""a""), ""a/"")\n      self.assertEqual(module._try_get_state_scope(""a""), ""a_1/"")\n\n  def testGetStateScope_UsesVariableScope(self):\n    with tf.Graph().as_default():\n      self.assertEqual(module._try_get_state_scope(""a""), ""a/"")\n      with tf_v1.variable_scope(None, default_name=""a"") as vs:\n        self.assertEqual(vs.name, ""a_1"")\n\n  def testGetStateScope_UsesNameScope(self):\n    with tf.Graph().as_default():\n      self.assertEqual(module._try_get_state_scope(""a""), ""a/"")\n      with tf_v1.name_scope(""a"") as ns:\n        self.assertEqual(ns, ""a_1/"")\n\n  def testGetStateScope_UnusedNameScope(self):\n    with tf.Graph().as_default():\n      self.assertEqual(module._try_get_state_scope(""a"", False), ""a/"")\n      with tf_v1.name_scope(""a"") as ns:\n        self.assertEqual(ns, ""a/"")\n\n      self.assertEqual(module._try_get_state_scope(""a"", False), ""a_1/"")\n      with tf_v1.name_scope(""a"") as ns:\n        self.assertEqual(ns, ""a_1/"")\n\n  def testGetStateScope_AlreadyUsedNameScope(self):\n    with tf.Graph().as_default():\n      with tf_v1.name_scope(""a""):\n        pass\n      with self.assertRaisesRegexp(RuntimeError,\n                                   ""name_scope was already taken""):\n        module._try_get_state_scope(""a"", False)\n\n  def testGetStateScopeWithActiveScopes(self):\n    with tf.Graph().as_default():\n      with tf_v1.name_scope(""foo""):\n        abs_scope = module._try_get_state_scope(""a"", False)\n        self.assertEqual(abs_scope, ""a/"")\n        with tf_v1.name_scope(abs_scope) as ns:\n          self.assertEqual(ns, ""a/"")\n\n    with tf.Graph().as_default():\n      with tf_v1.variable_scope(""vs""):\n        self.assertEqual(module._try_get_state_scope(""a"", False), ""vs/a/"")\n        with tf_v1.name_scope(name=""a"") as ns:\n          self.assertEqual(ns, ""vs/a/"")\n\n    with tf.Graph().as_default():\n      with tf_v1.name_scope(""foo""):\n        with tf_v1.variable_scope(""vs""):\n          self.assertEquals(module._try_get_state_scope(""a"", False), ""vs/a/"")\n\n\nclass _ModuleSpec(module_spec.ModuleSpec):\n\n  def get_tags(self):\n    return [set(), set([""special""])]\n\n  def get_signature_names(self, tags=None):\n    if tags == set([""special""]):\n      return iter([""default"", ""extra"", ""sparse""])\n    else:\n      return iter([""default""])\n\n  def get_input_info_dict(self, signature=None, tags=None):\n    result = {\n        ""x"":\n            tensor_info.ParsedTensorInfo(\n                tf.float32,\n                tf.TensorShape([None]),\n                is_sparse=(signature == ""sparse"" and tags == set([""special""]))),\n    }\n    if tags == set([""special""]) and signature == ""extra"":\n      result[""y""] = result[""x""]\n    return result\n\n  def get_output_info_dict(self, signature=None, tags=None):\n    result = {\n        ""default"": tensor_info.ParsedTensorInfo(\n            tf.float32,\n            tf.TensorShape([None]),\n            is_sparse=False),\n    }\n    if tags == set([""special""]) and signature == ""extra"":\n      result[""z""] = result[""default""]\n    return result\n\n  def _create_impl(self, name, trainable, tags):\n    return _ModuleImpl(name, trainable)\n\n  # native_module_test.py covers setting and getting attached messages.\n  def _get_attached_bytes(self, key, tags):\n    del key, tags  # Unused.\n    return None\n\n\nclass _ModuleImpl(module_impl.ModuleImpl):\n\n  def __init__(self, name, trainable):\n    super(_ModuleImpl, self).__init__()\n    with tf_v1.variable_scope(name):\n      pass\n\n  def create_apply_graph(self, signature, input_tensors, name):\n    with tf_v1.name_scope(name):\n      if signature == ""sparse"":\n        input_tensors = {\n            key: tf_v1.sparse_tensor_to_dense(value)\n            for key, value in input_tensors.items()\n        }\n      result = {""default"": 2 * input_tensors[""x""]}\n      if signature == ""extra"":\n        result[""z""] = 2 * input_tensors[""x""] + 3 * input_tensors[""y""]\n      return result\n\n  def export(self, path, session):\n    raise NotImplementedError()\n\n  @property\n  def variable_map(self):\n    raise NotImplementedError()\n\n\nclass ModuleTest(tf.test.TestCase):\n\n  def testModuleSingleInput(self):\n    with tf.Graph().as_default():\n      m = module.Module(_ModuleSpec())\n      result = m([1, 2])\n      with tf_v1.Session() as session:\n        self.assertAllEqual(session.run(result), [2, 4])\n\n  def testModuleDictInput(self):\n    with tf.Graph().as_default():\n      m = module.Module(_ModuleSpec())\n      result = m({""x"": [1, 2]})\n      with tf_v1.Session() as session:\n        self.assertAllEqual(session.run(result), [2, 4])\n\n  def testModuleDictOutput(self):\n    with tf.Graph().as_default():\n      m = module.Module(_ModuleSpec())\n      result = m([1, 2], as_dict=True)\n      self.assertIsInstance(result, dict)\n      self.assertAllEqual(list(result.keys()), [""default""])\n\n  def testModuleInNestedScope(self):\n    with tf.Graph().as_default():\n      with tf_v1.variable_scope(""foo""):\n        m = module.Module(_ModuleSpec())\n        result = m([1, 2])\n      with tf_v1.Session() as session:\n        self.assertAllEqual(session.run(result), [2, 4])\n\n  def testModuleInterfaceGettersDefaultSignatureAndTags(self):\n    with tf.Graph().as_default():\n      m = module.Module(_ModuleSpec())\n      self.assertItemsEqual(m.get_signature_names(), [""default""])\n      self.assertItemsEqual(m.get_input_info_dict().keys(), [""x""])\n      self.assertItemsEqual(m.get_output_info_dict().keys(), [""default""])\n\n  def testModuleInterfaceGettersExplicitSignatureAndTags(self):\n    """"""Tests that tags from Module(...) apply to module.get_*().""""""\n    with tf.Graph().as_default():\n      m = module.Module(_ModuleSpec(), tags={""special""})\n      self.assertItemsEqual(m.get_signature_names(),\n                            [""default"", ""extra"", ""sparse""])\n      self.assertItemsEqual(m.get_input_info_dict(signature=""extra"").keys(),\n                            [""x"", ""y""])\n      self.assertItemsEqual(m.get_output_info_dict(signature=""extra"").keys(),\n                            [""z"", ""default""])\n\n\nclass EvalFunctionForModuleTest(tf.test.TestCase):\n  """"""Tests for hub.eval_function_for_module(...).\n\n  This tests that hub.eval_function_for_module parses input variables,\n  signatures and tags correctly and that it returns the correct output.\n  End-to-end tests with the native module are done in native_module_test.py.\n  """"""\n\n  def testSingleInput(self):\n    with module.eval_function_for_module(_ModuleSpec()) as f:\n      self.assertAllEqual(f([1, 2]), [2, 4])\n\n  def testSparseInput(self):\n    with module.eval_function_for_module(_ModuleSpec(), tags={""special""}) as f:\n      self.assertAllEqual(\n          f(tf_v1.SparseTensorValue([[0]], [1], [2]),  # Value is [1, 0].\n            signature=""sparse""),\n          [2, 0])\n\n  def testDictInput(self):\n    with module.eval_function_for_module(_ModuleSpec()) as f:\n      self.assertAllEqual(f({""x"": [1, 2]}), [2, 4])\n\n  def testDictOutput(self):\n    with module.eval_function_for_module(_ModuleSpec()) as f:\n      result = f({""x"": [1, 2]}, as_dict=True)\n    self.assertTrue(isinstance(result, dict))\n    self.assertAllEqual(list(result.keys()), [""default""])\n\n  def testSignature(self):\n    with module.eval_function_for_module(_ModuleSpec()) as f:\n      self.assertAllEqual(f([1, 2]), [2, 4])\n\n  def testExplicitSignatureAndTags(self):\n    with module.eval_function_for_module(_ModuleSpec(), tags={""special""}) as f:\n      result = f(dict(x=[1], y=[2]), signature=""extra"", as_dict=True)\n      self.assertAllEqual(result[""default""], [2])\n      self.assertAllEqual(result[""z""], [8])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/module_v2.py,8,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TensorFlow Hub Module API for Tensorflow 2.0.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport six\n\nfrom tensorflow_hub import native_module\nfrom tensorflow_hub import registry\nfrom tensorflow_hub import tf_v1\n\n\ndef resolve(handle):\n  """"""Resolves a module handle into a path.\n\n  This function works both for plain TF2 SavedModels and the older\n  hub.Modules for TF1.\n\n  Resolves a module handle into a path by downloading and caching in\n  location specified by TF_HUB_CACHE_DIR if needed.\n\n  Currently, three types of module handles are supported:\n    1) Smart URL resolvers such as tfhub.dev, e.g.:\n       https://tfhub.dev/google/nnlm-en-dim128/1.\n    2) A directory on a file system supported by Tensorflow containing module\n       files. This may include a local directory (e.g. /usr/local/mymodule) or a\n       Google Cloud Storage bucket (gs://mymodule).\n    3) A URL pointing to a TGZ archive of a module, e.g.\n       https://example.com/mymodule.tar.gz.\n\n  Args:\n    handle: (string) the Module handle to resolve.\n\n  Returns:\n    A string representing the Module path.\n  """"""\n  return registry.resolver(handle)\n\n\ndef load(handle, tags=None):\n  """"""Resolves a handle and loads the resulting module.\n\n  This is the preferred API to load a Hub module in low-level TensorFlow 2.\n  Users of higher-level frameworks like Keras should use the framework\'s\n  corresponding wrapper, like hub.KerasLayer.\n\n  This function is roughly equivalent to the TF2 function `tf.save_model.load()`\n  on the result of `hub.resolve(handle)`. Calling this function requires\n  TF 1.14 or newer. It can be called both in eager and graph mode.\n\n  Note: Using in a tf.compat.v1.Session with variables placed on parameter\n  servers requires setting `experimental.share_cluster_devices_in_session`\n  within the `tf.compat.v1.ConfigProto`. (It becomes non-experimental in TF2.2.)\n\n  This function can handle the deprecated hub.Module format to the extent\n  that `tf.save_model.load()` in TF2 does. In particular, the returned object\n  has attributes\n    * `.variables`: a list of variables from the loaded object;\n    * `.signatures`: a dict of TF2 ConcreteFunctions, keyed by signature names,\n      that take tensor kwargs and return a tensor dict.\n  However, the information imported by hub.Module into the collections of a\n  tf.Graph is lost (e.g., regularization losses and update ops).\n\n  Args:\n    handle: (string) the Module handle to resolve; see hub.resolve().\n    tags: A set of strings specifying the graph variant to use, if loading from\n      a v1 module.\n\n  Returns:\n    A trackable object (see tf.saved_model.load() documentation for details).\n\n  Raises:\n    NotImplementedError: If the code is running against incompatible (1.x)\n                         version of TF.\n  """"""\n  if not hasattr(tf_v1.saved_model, ""load_v2""):\n    raise NotImplementedError(""hub.load() is not implemented for TF < 1.14.x, ""\n                              ""Current version: %s"" % tf.__version__)\n  if not isinstance(handle, six.string_types):\n    raise ValueError(""Expected a string, got %s"" % handle)\n  module_path = resolve(handle)\n  is_hub_module_v1 = tf.io.gfile.exists(\n      native_module.get_module_proto_path(module_path))\n  if tags is None and is_hub_module_v1:\n      tags = []\n  obj = tf_v1.saved_model.load_v2(module_path, tags=tags)\n  obj._is_hub_module_v1 = is_hub_module_v1  # pylint: disable=protected-access\n  return obj\n'"
tensorflow_hub/module_v2_test.py,2,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.module_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\ntry:\n  import mock as mock\nexcept ImportError:\n  import unittest.mock as mock\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\n\nfrom absl.testing import parameterized\nimport tensorflow as tf\nfrom tensorflow_hub import config\nfrom tensorflow_hub import module_v2\nfrom tensorflow_hub import test_utils\n\n# Initialize resolvers and loaders.\nconfig._run()\n\n\nclass ModuleV2Test(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'v1_implicit_tags\', \'hub_module_v1_mini\', None, True),\n      (\'v1_explicit_tags\', \'hub_module_v1_mini\', [], True),\n      (\'v2_implicit_tags\', \'saved_model_v2_mini\', None, False),\n      (\'v2_explicit_tags\', \'saved_model_v2_mini\', [\'serve\'], False),\n      )\n  def test_load(self, module_name, tags, is_hub_module_v1):\n    path = test_utils.get_test_data_path(module_name)\n    m = module_v2.load(path, tags)\n    self.assertEqual(m._is_hub_module_v1, is_hub_module_v1)\n\n  @mock.patch.object(module_v2, \'tf_v1\')\n  def test_load_with_old_tensorflow_raises_error(self, tf_v1_mock):\n    tf_v1_mock.saved_model = None\n    with self.assertRaises(NotImplementedError):\n      module_v2.load(\'dummy_module_name\')\n\n  def test_load_without_string(self):\n    with self.assertRaisesRegex(ValueError, \'Expected a string, got.*\'):\n      module_v2.load(0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_hub/native_module.py,33,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The implementation of deprecated hub.Module backed by custom SavedModels.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport re\n\nfrom absl import logging\nimport tensorflow as tf\n\nfrom tensorflow_hub import meta_graph_lib\nfrom tensorflow_hub import module_def_pb2\nfrom tensorflow_hub import module_impl\nfrom tensorflow_hub import module_spec\nfrom tensorflow_hub import saved_model_lib\nfrom tensorflow_hub import tensor_info\nfrom tensorflow_hub import tf_utils\nfrom tensorflow_hub import tf_v1\n\nfrom tensorflow.core.protobuf import meta_graph_pb2\n\n# TODO(b/72732111): Get this APIs or similar functionality to be public.\n# They are needed to identify the ""state-ops"" in a graph and to load C\n# registered ops into the python register for import_meta_graph to succeed\n# without having to do ""import library_that_register_missing_op"".\n# pylint: disable=g-bad-import-order\nfrom tensorflow.core.framework import op_def_pb2\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.python.framework import op_def_registry\nfrom tensorflow.python import pywrap_tensorflow as c_api\n# pylint: enable=g-bad-import-order\n\n# Align `op_def_registry` API between TensorFlow 1.X and 2.X.\nif not hasattr(op_def_registry, ""get""):\n\n  def get(name):\n    registered_ops = op_def_registry.get_registered_ops()\n    return registered_ops.get(name)\n\n  op_def_registry.get = get\n\nif not hasattr(op_def_registry, ""sync""):\n\n  def _remove_non_deprecated_descriptions(op_def):\n    for input_arg in op_def.input_arg:\n      input_arg.description = """"\n    for output_arg in op_def.output_arg:\n      output_arg.description = """"\n    for attr in op_def.attr:\n      attr.description = """"\n\n    op_def.summary = """"\n    op_def.description = """"\n\n  def sync():\n    p_buffer = c_api.TF_GetAllOpList()\n    cpp_op_list = op_def_pb2.OpList()\n    cpp_op_list.ParseFromString(c_api.TF_GetBuffer(p_buffer))\n\n    registered_ops = op_def_registry.get_registered_ops()\n    for op_def in cpp_op_list.op:\n      # If an OpList is registered from a gen_*_ops.py, it does not any\n      # descriptions. Strip them here as well to satisfy validation in\n      # register_op_list.\n      _remove_non_deprecated_descriptions(op_def)\n      registered_ops[op_def.name] = op_def\n\n  op_def_registry.sync = sync\n\n_MODULE_PROTO_FILENAME_PB = ""tfhub_module.pb""\n\n_MODULE_V3_SUPPORTED_FEATURES = frozenset([])  # None yet.\n\n_SUPPORTED_COLLECTIONS = set([\n    # GLOBAL_VARIABLES, TRAINABLE_VARIABLES and MODEL_VARIABLES hold\n    # tf.Variable objects saved in CollectionDef.bytes_list as serialized\n    # VariableDef proto.\n    tf_v1.GraphKeys.GLOBAL_VARIABLES,\n    tf_v1.GraphKeys.TRAINABLE_VARIABLES,\n    tf_v1.GraphKeys.MODEL_VARIABLES,\n    # This holds tf.Operation objects, saved in CollectionDef.node_list.\n    tf_v1.GraphKeys.TABLE_INITIALIZERS,\n    # This holds tf.Tensor objects, saved in CollectionDef.node_list.\n    tf_v1.GraphKeys.UPDATE_OPS,\n    # This holds tf.Tensor objects, saved in CollectionDef.node_list.\n    # These are imported to help fine-tuning (unlike LOSSES, which the\n    # importing model redefines from scratch).\n    tf_v1.GraphKeys.REGULARIZATION_LOSSES,\n    # This holds constant tensors of type string.\n    tf_v1.GraphKeys.ASSET_FILEPATHS,\n    # This holds serialized CondContextDef protos in CollectionDef.bytes_list.\n    tf_v1.GraphKeys.COND_CONTEXT,\n    # This holds serialized WhileContextDef protos in CollectionDef.bytes_list.\n    tf_v1.GraphKeys.WHILE_CONTEXT,\n    # saved_model_lib uses this collection internally for ModuleAttachments.\n    saved_model_lib.ATTACHMENT_COLLECTION_SAVED,\n])\n\n\ndef get_module_proto_path(module_dir):\n  return os.path.join(\n      tf.compat.as_bytes(module_dir),\n      tf.compat.as_bytes(_MODULE_PROTO_FILENAME_PB))\n\n\nclass Loader(object):\n  """"""Loader for Hub modules in the native format.""""""\n\n  def is_supported(self, path):\n    module_def_path = get_module_proto_path(path)\n    if not tf_v1.gfile.Exists(module_def_path):\n      return False\n\n    module_def_proto = module_def_pb2.ModuleDef()\n    with tf_v1.gfile.Open(module_def_path, ""rb"") as f:\n      module_def_proto.ParseFromString(f.read())\n\n    return module_def_proto.format == module_def_pb2.ModuleDef.FORMAT_V3\n\n  def __call__(self, path):\n    module_def_path = get_module_proto_path(path)\n    module_def_proto = module_def_pb2.ModuleDef()\n    with tf_v1.gfile.Open(module_def_path, ""rb"") as f:\n      module_def_proto.ParseFromString(f.read())\n\n    if module_def_proto.format != module_def_pb2.ModuleDef.FORMAT_V3:\n      raise ValueError(""Unsupported module def format: %r"" %\n                       module_def_proto.format)\n\n    required_features = set(module_def_proto.required_features)\n    unsupported_features = (required_features - _MODULE_V3_SUPPORTED_FEATURES)\n\n    if unsupported_features:\n      raise ValueError(""Unsupported features: %r"" % list(unsupported_features))\n\n    saved_model_handler = saved_model_lib.load(path)\n    checkpoint_filename = saved_model_lib.get_variables_path(path)\n    return _ModuleSpec(saved_model_handler, checkpoint_filename)\n\n\ndef create_module_spec(module_fn, tags_and_args=None, drop_collections=None):\n  """"""Creates a ModuleSpec from a function that builds the module\'s graph.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n  For TF2, switch to plain SavedModels.\n\n  The `module_fn` is called on a new graph (not the current one) to build the\n  graph of the module and define its signatures via `hub.add_signature()`.\n  Example:\n\n  ```python\n  # Define a text embedding module.\n  def my_text_module_fn():\n    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n    embeddings = compute_embedding(text_input)\n    hub.add_signature(inputs=text_input, outputs=embeddings)\n  ```\n\n  See `add_signature()` for documentation on adding multiple input/output\n  signatures.\n\n  NOTE: The `module_fn` is called on a graph that uses resource variables\n  by default. If you want old-style variables (""ref variables""), then\n  you can use `with tf.variable_scope("""", use_resource=False)` in `module_fn`.\n\n  Multiple graph variants can be defined by using the `tags_and_args` argument.\n  For example, the code:\n\n  ```python\n  hub.create_module_spec(\n      module_fn,\n      tags_and_args=[({""train""}, {""is_training"":True}),\n                     (set(), {""is_training"":False})])\n  ```\n\n  calls `module_fn` twice, once as `module_fn(is_training=True)` and once as\n  `module_fn(is_training=False)` to define the respective graph variants:\n  for training with tags {""train""} and for inference with the empty set of tags.\n  Using the empty set aligns the inference case with the default in\n  Module.__init__().\n\n  Args:\n    module_fn: a function to build a graph for the Module.\n    tags_and_args: Optional list of tuples (tags, kwargs) of tags and keyword\n      args used to define graph variants. If omitted, it is interpreted as\n      [(set(), {})], meaning `module_fn` is called once with no args.\n    drop_collections: list of collection to drop.\n\n  Returns:\n    A ModuleSpec.\n\n  Raises:\n    ValueError: if it fails to construct the ModuleSpec due to bad or\n      unsupported values in the arguments or in the graphs constructed by\n      `module_fn`.\n  """"""\n  if not drop_collections:\n    drop_collections = []\n\n  report_tags = True\n  if not tags_and_args:\n    tags_and_args = [(set(), {})]\n    report_tags = False\n\n  saved_model_handler = saved_model_lib.SavedModelHandler()\n  for tags, args in tags_and_args:\n    with tf.Graph().as_default() as graph:\n      with tf_v1.variable_scope("""", use_resource=True):\n        module_fn(**args)\n\n      for collection_key in drop_collections:\n        del tf_v1.get_collection_ref(collection_key)[:]\n\n    err = find_state_op_colocation_error(graph, tags if report_tags else None)\n    if err: raise ValueError(err)\n    saved_model_handler.add_graph_copy(graph, tags=tags)\n\n  return _ModuleSpec(saved_model_handler, checkpoint_variables_path=None)\n\n\ndef add_signature(name=None, inputs=None, outputs=None):\n  """"""Adds a signature to the module definition.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n  For TF2, switch to plain SavedModels.\n\n  NOTE: This must be called within a `module_fn` that is defining a hub.Module.\n\n  Args:\n    name: Signature name as a string. If omitted, it is interpreted as \'default\'\n      and is the signature used when `Module.__call__` `signature` is not\n      specified.\n    inputs: A dict from input name to Tensor or SparseTensor to feed when\n      applying the signature. If a single tensor is passed, it is interpreted\n      as a dict with a single \'default\' entry.\n    outputs: A dict from output name to Tensor or SparseTensor to return from\n      applying the signature. If a single tensor is passed, it is interpreted\n      as a dict with a single \'default\' entry.\n\n  Raises:\n    ValueError: if the arguments are invalid.\n  """"""\n  if not name:\n    name = ""default""\n  if inputs is None:\n    inputs = {}\n  if outputs is None:\n    outputs = {}\n  if not isinstance(inputs, dict):\n    inputs = {""default"": inputs}\n  if not isinstance(outputs, dict):\n    outputs = {""default"": outputs}\n  message = find_signature_inputs_from_multivalued_ops(inputs)\n  if message: logging.error(message)\n  message = find_signature_input_colocation_error(name, inputs)\n  if message: raise ValueError(message)\n  saved_model_lib.add_signature(name, inputs, outputs)\n\n\ndef attach_message(key, message):\n  """"""Adds an attached message to the module definition.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n  For TF2, switch to plain SavedModels.\n\n  NOTE: This must be called within a `module_fn` that is defining a hub.Module.\n\n  See ModuleSpec.get_attached_message() for an introduction to attached messages\n  and the API for module consumers.\n\n  To define a new type of attached message:\n\n    * Select a reasonably descriptive name as a unique key. For now, keys must\n      be valid Python identifiers that start with a letter. Punctuation besides\n      underscores (\'_\') is reserved for future use in hierarchical names.\n\n    * Define a Protocol Buffer message type to store the value for the key.\n      (Use generic containers like google.protobuf.Value only if running\n      the protocol compiler is infeasible for your build process.)\n\n    * For module consumers, consider providing a small library that encapsulates\n      the specific call to get_attached_message() behind a higher-level\n      interface and supplies the right message type for parsing.\n\n  Attached messages work best for few messages of moderate size.\n  Avoid a large number of messages; use repetition within messages instead.\n  Avoid large messages (megabytes); consider module assets instead.\n\n  For modules with multiple graph versions, each graph version stores separately\n  what was attached from within the call to `module_fn` that defines its graph.\n\n  Args:\n    key: A string with the unique key to retrieve this message. Must start\n      with a letter and contain only letters, digits and underscores. If used\n      repeatedly within one invocation of `module_fn`, then only the message\n      from the final call will be returned by `get_attached_message()`.\n    message: A protocol message object, to be stored in serialized form.\n\n  Raises:\n    ValueError: if `key` is not a string of the form of a Python identifier.\n  """"""\n  if not re.match(r""[a-zA-Z][a-zA-Z0-9_]*$"", key):\n    raise ValueError(\n        ""hub.attach_message() called with malformed key \'%s\'"" % key)\n  saved_model_lib.attach_bytes(key, message.SerializeToString())\n\n\nclass _ModuleSpec(module_spec.ModuleSpec):\n  """"""ModuleSpec for Hub\'s native Module format (backed by SavedModel).""""""\n\n  def __init__(self, saved_model_handler, checkpoint_variables_path,\n               check_collections=True):\n    """"""Private constructor.\n\n    Args:\n      saved_model_handler: SavedModelHandler backing up this Module definition.\n      checkpoint_variables_path: An optional string to the checkpoint where this\n        Module variables are checkpointed. If given the variables initializers\n        are overridden to load from it.\n      check_collections: Whether to check collections are supported.\n\n    Raises:\n      ValueError: if SavedModel contains any unexpected value.\n    """"""\n    check_unique_tags(saved_model_handler.get_tags())\n    if check_collections:\n      check_collections_are_supported(\n          saved_model_handler, _SUPPORTED_COLLECTIONS)\n    self._saved_model_handler = saved_model_handler\n    self._checkpoint_variables_path = checkpoint_variables_path\n    self._module_attachments = {\n        tags: saved_model_handler.get_attached_bytes_map(tags)\n        for tags in saved_model_handler.get_tags()}\n\n  def get_tags(self):\n    return self._saved_model_handler.get_tags()\n\n  def get_signature_names(self, tags=None):\n    meta_graph = self._saved_model_handler.get_meta_graph(tags=tags)\n    return list(meta_graph.signature_def.keys())\n\n  def get_input_info_dict(self, signature=None, tags=None):\n    signature_def = self._get_signature_def(signature, tags)\n    return tensor_info.parse_tensor_info_map(signature_def.inputs)\n\n  def get_output_info_dict(self, signature=None, tags=None):\n    signature_def = self._get_signature_def(signature, tags)\n    return tensor_info.parse_tensor_info_map(signature_def.outputs)\n\n  def _get_signature_def(self, signature, tags):\n    meta_graph = self._saved_model_handler.get_meta_graph(tags=tags)\n    if signature is None:\n      signature = ""default""\n    signature_def = meta_graph.signature_def.get(signature)\n    if signature_def is None:\n      raise ValueError(""Signature %r is missing from meta graph."" % signature)\n    return signature_def\n\n  def _get_attached_bytes(self, key, tags):\n    return self._module_attachments[frozenset(tags or [])].get(key)\n\n  def _create_impl(self, name, trainable, tags):\n    meta_graph = self._saved_model_handler.get_meta_graph(tags=tags)\n    return _ModuleImpl(\n        spec=self,\n        meta_graph=meta_graph,\n        trainable=trainable,\n        checkpoint_path=self._checkpoint_variables_path,\n        name=name)\n\n  def _export(self, path, variables_saver):\n    """"""Internal.\n\n    Args:\n      path: string where to export the module to.\n      variables_saver: an unary-function that writes the module variables\n        checkpoint on the given path.\n    """"""\n    self._saved_model_handler.export(path, variables_saver=variables_saver)\n\n    module_def_proto = module_def_pb2.ModuleDef()\n    module_def_proto.format = module_def_pb2.ModuleDef.FORMAT_V3\n    module_def_filename = get_module_proto_path(path)\n    tf_utils.atomic_write_string_to_file(\n        module_def_filename,\n        module_def_proto.SerializeToString(),\n        overwrite=False)\n    logging.info(""Exported TF-Hub module to: %s"", path)\n\n\nclass _ModuleImpl(module_impl.ModuleImpl):\n  """"""A Module instantiation backed by a MetaGraphDef.""""""\n\n  def __init__(self, spec, meta_graph, trainable, checkpoint_path, name):\n    """"""Private constructor.\n\n    Args:\n      spec: _ModuleSpec instance.\n      meta_graph: MetaGraphDef to use\n      trainable: whether module is trainable.\n      checkpoint_path: None or a string to the variables checkpoints.\n      name: variable and scope name where to instantiate the Module. Must be an\n        unused name scope.\n    """"""\n    self._spec = spec\n    self._meta_graph = meta_graph\n    self._trainable = trainable\n    self._checkpoint_path = checkpoint_path\n\n    register_ops_if_needed({\n        op.name for op in self._meta_graph.meta_info_def.stripped_op_list.op})\n\n    if _is_tpu_graph_function():\n      # TODO(b/129142908): Hub should not use `tf.init_scope` since that makes\n      # it incompatible with tf.compat.v1.wrap_function. For now the only use\n      # case where hub used it was for tpu compatibility. This should be cleaned\n      # up at an early convinience.\n      scope_func = tf.init_scope\n    else:\n      scope_func = lambda: tf.control_dependencies(None)\n\n    # Clear dependencies so modules can be constructed from deep inside\n    # functions that have dependencies active. Note that the dependencies\n    # would be active when applying the Module signature, just not active\n    # when creating the Module state. This use case has showed up in some\n    # TPU training code.\n    with scope_func():\n      self._init_state(name)\n\n  def _init_state(self, name):\n    variable_tensor_map, self._state_map = self._create_state_graph(name)\n    self._variable_map = recover_partitioned_variable_map(\n        get_node_map_from_tensor_map(variable_tensor_map))\n    if self._variable_map and self._checkpoint_path:\n      tf_v1.train.init_from_checkpoint(self._checkpoint_path,\n                                       self._variable_map)\n\n    # Build Saver so it can be used later on to export the variables.\n    if self._variable_map:\n      self._saver = tf_v1.train.Saver(\n          self._variable_map,\n          sharded=True,\n          write_version=tf_v1.train.SaverDef.V2)\n    else:\n      self._saver = None\n\n  def _create_state_graph(self, name):\n    """"""Creates the graph nodes that hold the state of the Module.\n\n    Args:\n      name: name scope to create the state graph in.\n\n    Returns:\n      A tuple consisting of:\n        variables_tensor_map: a map from tensor names in the original graph def\n          to the created Variables objects.\n        state_map: a map from tensors names in the original graph def to the\n          instantiated tensors to be used as a state_map.\n    """"""\n    import_collections = [\n        tf_v1.GraphKeys.GLOBAL_VARIABLES,\n        tf_v1.GraphKeys.MODEL_VARIABLES,\n        tf_v1.GraphKeys.TABLE_INITIALIZERS,\n        tf_v1.GraphKeys.ASSET_FILEPATHS,  # Typically used to initialize tables.\n        tf_v1.GraphKeys.COND_CONTEXT,\n        tf_v1.GraphKeys.WHILE_CONTEXT,\n    ]\n    if self._trainable:\n      # TODO(b/64049014): Import UPDATE_OPS which do not depend on inputs.\n      import_collections.extend([tf_v1.GraphKeys.TRAINABLE_VARIABLES,\n                                 tf_v1.GraphKeys.REGULARIZATION_LOSSES])\n\n    absolute_scope_name = tf_v1.get_default_graph().unique_name(\n        name, mark_as_used=False)\n    relative_scope_name = absolute_scope_name.split(""/"")[-1]\n    assert relative_scope_name == name  # verify name scope was indeed unused.\n\n    meta_graph = meta_graph_pb2.MetaGraphDef()\n    meta_graph.CopyFrom(self._meta_graph)\n\n    meta_graph_lib.filter_collections(meta_graph, import_collections)\n    meta_graph_lib.prefix_shared_name_attributes(meta_graph,\n                                                 absolute_scope_name)\n\n    tf_v1.train.import_meta_graph(\n        meta_graph,\n        input_map={},\n        import_scope=relative_scope_name)\n\n    # Build a list from the variable name in the module definition to the actual\n    # instantiated variables.\n    variables_tensor_map = {}\n    for var in tf_v1.global_variables():\n      if var.op.name.startswith(absolute_scope_name + ""/""):\n        variables_tensor_map[var.name[len(absolute_scope_name)+1:]] = var\n\n    # Build a map of tensors to feed from the state-graph into subsequent\n    # apply-graphs.\n    def _get_tensor(tensor_name):\n      return tf_v1.get_default_graph().get_tensor_by_name(\n          meta_graph_lib.prepend_name_scope(\n              tensor_name, import_scope=absolute_scope_name))\n\n    state_op_names = list_registered_stateful_ops_without_inputs(\n        meta_graph.graph_def)\n    state_map = get_state_map(meta_graph, state_op_names, set(), _get_tensor)\n\n    return variables_tensor_map, state_map\n\n  def create_apply_graph(self, signature, input_tensors, name):\n    """"""See `ModuleImpl.create_apply_graph`.""""""\n    signature_def = self._meta_graph.signature_def.get(signature)\n    meta_graph = meta_graph_pb2.MetaGraphDef()\n    meta_graph.CopyFrom(self._meta_graph)\n    apply_graph = tf_v1.get_default_graph()\n    infeed_map = tensor_info.build_input_map(signature_def.inputs,\n                                             input_tensors)\n\n    # Build a input map to feed when importing the apply-graph by augmenting the\n    # state_map with the input args. This allows an input to override a tensor\n    # from the state-graph.\n    feed_map = dict(self._state_map)\n    # If we are applying the module in a function with a TPUReplicateContext, we\n    # must capture the state tensors in generating our feedmap and prune out\n    # assign ops. Function graph semantics are different in that all ops are\n    # executed regardless of dependency.\n    # TODO(b/112575006): The following adds functionality of function call\n    # within a TPU context. Work to generalize this for all function calls is\n    # ongoing.\n    if _is_tpu_graph_function():\n      for k, v in self._state_map.items():\n        feed_map[k] = apply_graph.capture(v)\n      meta_graph_lib.prune_unused_nodes(meta_graph, signature_def)\n      # After we prune the metagraph def, we might need to prune away\n      # infeeds which no longer exist.\n      meta_graph_lib.prune_feed_map(meta_graph, infeed_map)\n    elif apply_graph.building_function:\n      # Log a warning if a user is using a hub module in function graph.\n      # This is only expected to work if the function graph is pruned and\n      # not all nodes are executed.\n      #\n      # E.g. it could work with ""tf.compat.v1.wrap_function"", but it will not\n      # work with defun, Dataset.map_fn, etc...\n      logging.warning(""Using `hub.Module` while building a function: %s. This ""\n                      ""can lead to errors if the function is not pruned."",\n                      apply_graph.name)\n\n    # As state ops in the apply graph are unused, replace them with Placeholders\n    # so that in a heirarchical instantiation, apply_graph state ops are\n    # ignored.\n    replace_apply_state(\n        meta_graph,\n        list_registered_stateful_ops_without_inputs(meta_graph.graph_def),\n        feed_map)\n    feed_map.update(infeed_map)\n\n    # Make state tensors enter the current context. This way the Module can be\n    # applied inside a control flow structure such as a while_loop.\n    control_flow = apply_graph._get_control_flow_context()  # pylint: disable=protected-access\n    if control_flow:\n      for key, value in sorted(feed_map.items()):\n        feed_map[key] = control_flow.AddValue(value)\n\n    # Don\'t mark the name as used at this point - import_scoped_meta_graph will\n    # start using it.\n    absolute_scope_name = apply_graph.unique_name(name, mark_as_used=False)\n    relative_scope_name = absolute_scope_name.split(""/"")[-1]\n\n    import_collections = [\n        # In most cases ASSET_FILEPATHS are only used for the TABLE_INITIALIZERS\n        # ops, however one could create a graph that uses an asset at any other\n        # time. As so everytime we bring the tensor with that has the asset\n        # filename we must annotate it as so, so later re-exports have that\n        # semantic information and can handle it.\n        tf_v1.GraphKeys.ASSET_FILEPATHS,\n        tf_v1.GraphKeys.COND_CONTEXT,\n        tf_v1.GraphKeys.WHILE_CONTEXT,\n    ]\n    if self._trainable:\n      import_collections.extend([tf_v1.GraphKeys.UPDATE_OPS])\n\n    meta_graph_lib.filter_collections(meta_graph, import_collections)\n    meta_graph_lib.prefix_shared_name_attributes(meta_graph,\n                                                 absolute_scope_name)\n    if len(meta_graph.collection_def) and _is_tpu_graph_function():\n      raise NotImplementedError(\n          ""Applying modules with collections inside TPU functions is not ""\n          ""supported. Collections found: %s"" % str(meta_graph.collection_def))\n\n    tf_v1.train.import_meta_graph(\n        meta_graph,\n        input_map=feed_map,\n        import_scope=relative_scope_name)\n    fix_colocation_after_import(input_map=feed_map,\n                                absolute_import_scope=absolute_scope_name)\n\n    def get_tensor(name):\n      # When trying to output an input tensor there are no nodes created within\n      # the apply scope. So one must look into the input map.\n      try:\n        return feed_map[name]\n      except KeyError:\n        return apply_graph.get_tensor_by_name(\n            meta_graph_lib.prepend_name_scope(\n                name, import_scope=absolute_scope_name))\n\n    return tensor_info.build_output_map(signature_def.outputs, get_tensor)\n\n  def export(self, path, session):\n    """"""See `Module.export`.""""""\n    def variables_saver(variables_path):\n      if self._saver:\n        self._saver.save(\n            session, variables_path,\n            write_meta_graph=False,\n            write_state=False)\n\n    self._spec._export(path, variables_saver)  # pylint: disable=protected-access\n\n  @property\n  def variable_map(self):\n    """"""See `Module.variable_map`.""""""\n    return self._variable_map\n\n\ndef is_registered_stateful_op_without_inputs(name):\n  """"""Checks if an op is registered, stateful and does not expect inputs.""""""\n  op_def = op_def_registry.get(name)\n  return op_def is not None and (op_def.is_stateful and not op_def.input_arg)\n\n\ndef list_registered_stateful_ops_without_inputs(graph_def):\n  """"""Returns set of registered stateful ops that do not expect inputs.\n\n  This list is used to identify the ops to be included in the state-graph and\n  that are subsequently fed into the apply-graphs.\n\n  Args:\n    graph_def: GraphDef to list ops from.\n\n  Returns:\n    A set of strings.\n  """"""\n  used_ops = (node.op for node in graph_def.node)\n  return {op for op in used_ops if is_registered_stateful_op_without_inputs(op)}\n\n\ndef get_state_map(meta_graph, state_ops, unsupported_state_ops,\n                  get_tensor_by_name):\n  """"""Returns a map from tensor names to tensors that hold the state.""""""\n  state_map = {}\n  for node in meta_graph.graph_def.node:\n    if node.op in state_ops:\n      tensor_name = node.name + "":0""\n      tensor = get_tensor_by_name(tensor_name)\n      num_outputs = len(tensor.op.outputs)\n      if num_outputs != 1:\n        raise ValueError(""Stateful op %s has %d outputs, expected 1"" %\n                         (node.op, num_outputs))\n      state_map[tensor_name] = tensor\n    if node.op in unsupported_state_ops:\n      raise ValueError(""Unsupported stateful op: %s"" % node.op)\n  return state_map\n\n\ndef replace_apply_state(meta_graph, state_ops, feed_map):\n  """"""Replaces state ops with non state Placeholder ops for the apply graph.""""""\n  for node in meta_graph.graph_def.node:\n    keys_to_purge = []\n    tensor_name = node.name + "":0""\n    # Verify that the node is a state op and that its due to be rewired\n    # in the feedmap.\n    if node.op in state_ops and tensor_name in feed_map:\n      node.op = ""Placeholder""\n      for key in node.attr:\n        # Only shape and dtype are required for Placeholder. Remove other\n        # attributes.\n        if key != ""shape"":\n          keys_to_purge.append(key)\n      for key in keys_to_purge:\n        del node.attr[key]\n      node.attr[""dtype""].type = types_pb2.DT_RESOURCE\n\n\ndef get_node_map_from_tensor_map(tensor_map):\n  """"""Converts the keys from tensor name to node name.\n\n  Args:\n    tensor_map: Map where keys are full tensor names and values are tensors.\n\n  Returns:\n    Map same as tensor_map, except keys have the output_number stripped.\n  """"""\n  return {\n      _split_tensor_name(key)[0]: value\n      for key, value in tensor_map.items()\n  }\n\n\ndef _split_tensor_name(tensor_name):\n  """"""Given a tensor name as node_name:output_number, returns both parts.""""""\n  result = re.match(r""(.*):(\\d+)$"", tensor_name)\n  if not result:\n    raise ValueError(\n        ""Unexpected format for tensor name. Expected node_name:output_number. ""\n        ""Got %r"" % tensor_name)\n  return result.group(1), int(result.group(2))\n\n\ndef _extract_variable_parts(variable_key, variable):\n  """"""Matches a variable to individual parts.\n\n  Args:\n    variable_key: String identifier of the variable in the module scope.\n    variable: Variable tensor.\n\n  Returns:\n    partitioned: Whether the variable is partitioned.\n    name: Name of the variable up to the partitioning.\n    offset: Offset of the variable into the full variable.\n\n  Raises:\n    RuntimeError: In case of unexpected variable format.\n  """"""\n  name, offset, partitioned = None, None, False\n  # pylint: disable=protected-access\n  if variable._save_slice_info:\n    name = variable_key[:variable_key.rfind(""/"")]\n    if not variable._save_slice_info.full_name.endswith(name):\n      raise RuntimeError(""Unexpected handling of partitioned variable."")\n    offset = variable._save_slice_info.var_offset[0]\n    partitioned = True\n  # pylint: enable=protected-access\n  return partitioned, name, offset\n\n\ndef recover_partitioned_variable_map(var_node_map):\n  """"""Builds a proper variable map if it contains PartitionedVariables.\n\n  Args:\n    var_node_map: A map to tf.Variables. PartitionedVariables show up in this\n      map as N entries with keys ""<var_name>/part_n"".\n\n  Returns:\n    A map to tf.Variables or to list of tf.Variables for each\n    PartitionedVariables in `var_node_map`.\n\n  Raises:\n    RuntimeError: if there are issues recovering the PartitionedVariables.\n  """"""\n  offset_variables_map = {}\n  for var_key, var_tensor in var_node_map.items():\n    match, var_name, offset = _extract_variable_parts(var_key, var_tensor)\n\n    if not match:\n      # This is a standard variable, so we can safely add it to the output.\n      if var_key in offset_variables_map:\n        raise RuntimeError(\n            ""Variable %s exists both as a single and partitioned variable."")\n      offset_variables_map[var_key] = var_tensor\n      continue\n\n    if var_name not in offset_variables_map:\n      offset_variables_map[var_name] = {}\n    elif not isinstance(offset_variables_map[var_name], dict):\n      raise RuntimeError(\n          ""Variable %s exists both as a single and partitioned variable."")\n\n    # Duplicated variable offsets should not exist.\n    if offset in offset_variables_map[var_name]:\n      raise RuntimeError(\n          ""Variable map contains duplicate offset %d for variable [%s]"" %\n          (offset, var_name))\n    offset_variables_map[var_name][offset] = var_tensor\n\n  variables_map = {}\n  # Use offsets for sorting, then strip them from the dictionary and keep only\n  # a list of variables per each variable name.\n  for var_name, var_value in offset_variables_map.items():\n    if not isinstance(var_value, dict):\n      variables_map[var_name] = var_value\n      continue\n    shapes = [var_tensor.shape[1:] for var_tensor in var_value.values()]\n    if not all(shape == shapes[0] for shape in shapes):\n      raise RuntimeError(""Shapes not compatible: %s"" % (shapes))\n    for _, tensor in sorted(var_value.items()):\n      variables_map[var_name] = [\n          tensor for _, tensor in sorted(var_value.items())\n      ]\n\n  return variables_map\n\n\ndef check_unique_tags(tag_list):\n  """"""Checks that tag list contains each set of tags only once.""""""\n  frozen_tags_seen = set()\n  for tags in tag_list:\n    frozen_tags = frozenset(tags)\n    if frozen_tags in frozen_tags_seen:\n      raise ValueError(""Tags %r used repeatedly"" % tags)\n    frozen_tags_seen.add(frozen_tags)\n\n\ndef check_collections_are_supported(saved_model_handler, supported):\n  """"""Checks that SavedModelHandler only uses supported collections.""""""\n  for meta_graph in saved_model_handler.meta_graphs:\n    used_collection_keys = set(meta_graph.collection_def.keys())\n    unsupported = used_collection_keys - supported\n    if unsupported:\n      raise ValueError(""Unsupported collections in graph: %s\\n""\n                       ""Use hub.create_module_spec(..., drop_collections=[...])""\n                       "" as appropriate."" % list(unsupported))\n\n\ndef get_unsupported_collections(used_collection_keys):\n  return list(set(used_collection_keys) - _SUPPORTED_COLLECTIONS)\n\n\ndef register_ops_if_needed(graph_ops):\n  """"""Register graph ops absent in op_def_registry, if present in c++ registry.\n\n  Args:\n    graph_ops: set with graph op names to register.\n\n  Raises:\n    tf.errors.NotFoundError: if `graph_ops` contains ops that are not in either\n    python or c++ registry.\n  """"""\n  if all(op_def_registry.get(op) is not None for op in graph_ops):\n    return\n\n  # Note: Only raise missing op ValueError after trying to load ops.\n  # This allows the test to exercise all the calls into TensorFlow\n  # without having to write a C + python test.\n  op_def_registry.sync()\n  missing_ops = {op for op in graph_ops if op_def_registry.get(op) is None}\n  if missing_ops:\n    raise tf.errors.NotFoundError(\n        None, None,\n        ""Graph ops missing from the python registry (%s) are also absent from ""\n        ""the c++ registry."" % missing_ops)\n\n\ndef fix_colocation_after_import(input_map, absolute_import_scope):\n  """"""Fixes colocation attributes after import according to input_map.\n\n  This function is meant to be called after importing a GraphDef, in order\n  to rewrite colocate_with constrains analogous to how inputs to ops\n  are rewritten by input_map during import. It also updates devices accordingly.\n\n  The nodes in the given import scope of the current default graph have their\n  colocation attributes (that is, the ""loc:@..."" values in the ""_class"" attr)\n  rewritten as follows: If, before the call, op x has attribute loc:@y, and\n  `input_map` replaces an output of y with an output of z, then loc:@y gets\n  replaced by the colocation attributes of z (that is, loc:@z, if no other\n  constraints are in play).\n\n  This style of rewriting imposes the following requirements:\n\n    * If an output of node y is an input tensor in a signature of the module,\n      y must not have any colocation attributes on it, such that colocations\n      with y are expressed by loc:@y and can be adjusted with a rewriting rule\n      for it. Function `find_signature_input_colocation_error()` checks this\n      during module creation.\n\n    * If y1 is a state node, its colocation constraints must only reference\n      other state nodes, say, y2. Since all outputs of state nodes are mapped\n      the same way, all their rewriting rules together will do the same thing.\n      Function `find_state_op_colocation_error()` checks this during module\n      creation.\n\n    * Other nodes may have arbitrary colocation attributes.\n\n  Mapping of inputs works with tensors, while colocation constraints work with\n  ops. Issues may arise when mapping tensors from ops with multiple outputs.\n  If the outputs of y are replaced by outputs of distinct ops z1, z2, ...,\n  rewriting of loc:@y becomes ambiguous unless z1, z2, ... have equal\n  colocation_groups) If some but not all outputs of y are replaced, it\n  becomes ambiguous whether to rewrite loc:@y at all. For now, this is\n  handled conservatively by raising an error (instead of rewriting to the\n  union of all applicable constraints). This should be very rare: all state\n  ops so far have single outputs (and even if not, the rewriting would be\n  consistent); input ops usually are placeholders, which have single outputs.\n\n  Args:\n    input_map: a dict mapping from tensor names in the imported graph to\n      existing Tensors, typically the same as passed to tf.import_graph_def().\n    absolute_import_scope: a string with the full name of the import scope,\n      comprising the current scope when import_graph_def() as called plus\n      the import_scope passed to it.\n\n  Raises:\n    ValueError: if one imported op has its multiple outputs and they are\n      remapped in a way that causes conflicting colocation rewrites.\n  """"""\n  attr_map = _build_colocation_attr_map(input_map, absolute_import_scope)\n  _apply_colocation_attr_map(attr_map, absolute_import_scope)\n\n\nclass _ConsistentValue(object):\n  """"""Helper for deferred consistency checking for values from multiple sources.\n\n  Suppose you compute some value from multiple sources that should all be\n  consistent. This class helps you store the value (with context on sources)\n  and provides a getter method to either get the consistent value or raise\n  a exception with a meaningful custom error message.\n\n  Usage example:\n\n    remainder = _ConsistentValue()\n    for x in (105, 205, 305, 406):\n      remainder.Set(x % 100, {""x"": x})\n      print(remainder.GetConsistentValueOrRaise(\n          ""Got {old_value} at {old_x} but became {new_value} at {new_x}.""))\n\n    will print ""5"" three times and then raise\n    ValueError(""Got 5 at 105 but became 6 at 406."").\n  """"""\n  def __init__(self):\n    self.has_error = False\n    self.value = None\n    self._context = {}\n\n  def Set(self, value, context=None):\n    """"""Receives a value for the object and some context on its source.""""""\n    if self.has_error: return\n    if self.value is None:\n      self.value = value\n      self._context[""old_value""] = value\n      self._context.update({""old_"" + k: v for k, v in context.items()})\n    elif self.value != value:\n      self.has_error = True\n      self._context[""new_value""] = value\n      self._context.update({""new_"" + k: v for k, v in context.items()})\n\n  def GetConsistentValueOrRaise(self, error_format, context=None):\n    """"""Gets consistent value or raises ValueError with formatted contexts.""""""\n    if self.has_error:\n      full_context = dict(self._context)\n      if context: full_context.update(context)\n      raise ValueError(error_format.format(**full_context))\n    return self.value\n\n\ndef _build_colocation_attr_map(input_map, absolute_import_scope):\n  """"""Returns a dict mapping from pre-import to post-import colocation attrs.\n\n  Args:\n    input_map: as for fix_colocation_after_import.\n    absolute_import_scope: as for fix_colocation_after_import.\n\n  Returns:\n    A dict that maps bytes `""loc:@"" + absolute_import_scope + ""/foo""`\n    to _ConsistentValues set to the lists of bytes `[""loc:@..."", ...]`\n    according to the rewriting scheme of fix_colocation_after_import.\n    In case of an inconsistent rewriting, _ConsistentValue.has_error is true.\n  """"""\n  colocation_attr_map = collections.defaultdict(_ConsistentValue)\n  used_outputs_of_imported_ops = collections.defaultdict(set)\n  # Collect mappings from the input_map.\n  for imported_tensor_name, mapped_tensor in input_map.items():\n    imported_tensor_name = absolute_import_scope + ""/"" + imported_tensor_name\n    imported_op_name, imported_index = _split_tensor_name(imported_tensor_name)\n    key = tf.compat.as_bytes(""loc:@"" + imported_op_name)\n    colocation_attr_map[key].Set(\n        mapped_tensor.op.colocation_groups(),\n        {""reason"": ""input \'%s\' is substituted by \'%s\'"" % (\n            imported_tensor_name, mapped_tensor.name)})\n    used_outputs_of_imported_ops[imported_op_name].add(imported_index)\n  # Add unchanged mappings for additional, non-remapped outputs of ops touched\n  # by the input_map. For now, these just signal inconsistency when used.\n  for imported_op_name, used_outputs in used_outputs_of_imported_ops.items():\n    imported_op = tf_v1.get_default_graph().get_operation_by_name(\n        imported_op_name)\n    unused_outputs = set(range(len(imported_op.outputs))) - used_outputs\n    if not unused_outputs: continue\n    key = tf.compat.as_bytes(""loc:@"" + imported_op_name)\n    if imported_op.colocation_groups() != [key]:\n      # This should never happen: state nodes are remapped fully, input nodes\n      # are prevented from having colocation attributes.\n      raise ValueError(\n          ""Internal error: tensors from op \'%s\' are partially remapped in ""\n          ""import but op.colocation_groups=%s cannot be captured in a ""\n          ""simple rewrite rule."" %\n          (imported_op_name, imported_op.colocation_groups()))\n    colocation_attr_map[key].Set(\n        [key],\n        {""reason"": ""tensor \'%s:%s\' is not substituted by inputs"" % (\n            imported_op_name,\n            "","".join(str(i) for i in sorted(unused_outputs)))})\n\n  return colocation_attr_map\n\n\ndef _apply_colocation_attr_map(colocation_attr_map, absolute_import_scope):\n  """"""Rewrites colocation constraints in the current default graph.\n\n  Nodes in `absolute_import_scope` get their ""_class"" attr lists rewritten\n  according to `colocation_attr_map`: each entry that matches a key gets\n  replaced by the associated values (with deduplication). The node\'s device\n  is updated accordingly.\n\n  Args:\n    colocation_attr_map: as returned by _build_colocation_attr_map.\n    absolute_import_scope: as for fix_colocation_after_import.\n\n  Raises:\n    ValueError: if rewriting runs into an inconsistent value in\n      `colocation_attr_map`.\n  """"""\n  graph = tf_v1.get_default_graph()\n  for op in graph.get_operations():\n    # Rewrite the values of the ""_class"" attr that store colocation constraints.\n    # NOTE: The colocation_group loc:@X of a node with itself is not stored\n    # explicitly as an attr, so rewrite errors for loc:@X are not triggered\n    # by the mere existence of X.\n    if not op.name.startswith(absolute_import_scope + ""/""): continue\n    try:\n      class_values = op.get_attr(""_class"")\n    except ValueError:\n      continue  # No _class attr found; nothing to do.\n    new_attr_value = tf_v1.AttrValue()\n    new_coloc_groups = []\n    for class_value in class_values:\n      if class_value.startswith(tf.compat.as_bytes(""loc:@"")):\n        if class_value not in colocation_attr_map:\n          rewritten_class_value = [class_value]\n        else:\n          rewritten_class_value = (colocation_attr_map[\n              class_value].GetConsistentValueOrRaise(\n                  ""Failed to rewrite colocation constraints while applying ""\n                  ""hub.Module:\\n""\n                  ""The module graph contains a node {op!r} ""\n                  ""that has a colocation constraint {class_value!r} ""\n                  ""with ambiguous rewriting {old_value!r} vs {new_value!r} ""\n                  ""because {old_reason} and {new_reason}, respectively.\\n""\n                  ""To fix, avoid publishing a module with inputs comprising ""\n                  ""multiple outputs of one op that is referenced in ""\n                  ""tf.colocate_with(...) constraints on other ops."",\n                  {""op"": op.name, ""class_value"": class_value}))\n        new_coloc_groups.extend(rewritten_class_value)\n      else:\n        new_attr_value.list.s.append(class_value)\n    new_coloc_groups = sorted(set(new_coloc_groups))\n    new_attr_value.list.s.extend(new_coloc_groups)\n    op._set_attr(""_class"", new_attr_value)  # pylint: disable=protected-access\n\n    # Mimic the code of tf.import_graph_def(): If there are colocation\n    # constraints, use any of them to set the device (overriding what the\n    # device function stack would do), without attempting to merge or check for\n    # equality. If they were inconsistent, TensorFlow\'s C++ runtime would fail\n    # anyways due to conflicting colocation constraints.\n    # Note that Hub imports GraphDefs with devices cleared, so this code deals\n    # with the result of import_graph_def, not a setting saved in the module.\n    if new_coloc_groups:\n      new_coloc_device = """"\n      for new_coloc_group in new_coloc_groups:\n        assert new_coloc_group.startswith(tf.compat.as_bytes(""loc:@""))\n        new_coloc_target_op = graph.get_operation_by_name(\n            tf.compat.as_str_any(new_coloc_group[5:]))\n        new_coloc_device = new_coloc_target_op.device\n        if new_coloc_device: break\n      # Set this, even if empty, to avoid retaining an outdated value.\n      op._set_device(new_coloc_device)  # pylint: disable=protected-access\n\n\ndef find_state_op_colocation_error(graph, reported_tags=None):\n  """"""Returns error message for colocation of state ops, or None if ok.""""""\n  state_op_types = list_registered_stateful_ops_without_inputs(\n      graph.as_graph_def())\n  state_op_map = {op.name: op for op in graph.get_operations()\n                  if op.type in state_op_types}\n  for op in state_op_map.values():\n    for colocation_group in op.colocation_groups():\n      if not (colocation_group.startswith(tf.compat.as_bytes(""loc:@"")) and\n              tf.compat.as_str_any(colocation_group[5:]) in state_op_map):\n        tags_prefix = ("""" if reported_tags is None else\n                       ""in the graph for tags %s, "" % reported_tags)\n        return (\n            ""A state-holding node x of a module\'s graph (e.g., a Variable op) ""\n            ""must not be subject to a tf.colocate_with(y) constraint ""\n            ""unless y is also a state-holding node.\\n""\n            ""Details: %snode \'%s\' has op \'%s\', which counts as state-holding, ""\n            ""but Operation.colocation_groups() == %s. "" %\n            (tags_prefix, op.name, op.type, op.colocation_groups()))\n  return None\n\n\ndef find_signature_input_colocation_error(signature_name, inputs):\n  """"""Returns error message for colocation of signature inputs, or None if ok.""""""\n  for input_name, tensor in inputs.items():\n    expected_colocation_groups = [tf.compat.as_bytes(""loc:@"" + tensor.op.name)]\n    if tensor.op.colocation_groups() != expected_colocation_groups:\n      return (\n          ""A tensor x used as input in a signature must not be subject to a ""\n          ""tf.colocate_with(y) constraint. (The reverse would be allowed.)\\n""\n          ""Details: tensor \'%s\' appears as input \'%s\' of signature \'%s\' ""\n          ""but has Tensor.op.colocation_groups() == %s"" %\n          (tensor, input_name, signature_name, tensor.op.colocation_groups()))\n  return None\n\n\ndef find_signature_inputs_from_multivalued_ops(inputs):\n  """"""Returns error message for module inputs from ops with multiple outputs.""""""\n  dense_inputs = []  # List of (str, Tensor), with SparseTensors decomposed.\n  for name, tensor in sorted(inputs.items()):\n    if isinstance(tensor, tf.SparseTensor):\n      dense_inputs.extend((""%s.%s"" % (name, attr), getattr(tensor, attr))\n                          for attr in (""indices"", ""values"", ""dense_shape""))\n    else:\n      dense_inputs.append((name, tensor))\n  warnings = [(name, tensor.name) for name, tensor in dense_inputs\n              if len(tensor.op.outputs) != 1]\n  if warnings:\n    return (\n        ""WARNING: The inputs declared in hub.add_signature() should be tensors ""\n        ""from ops with a single output, or else uses of tf.colocate_with() on ""\n        ""that op can trigger fatal errors when the module is applied and ""\n        ""colocation constraints have to be rewritten.\\nAffected inputs: %s"" %\n        "", "".join(""%s=\'%s\'"" % pair for pair in warnings))\n  return None\n\n\ndef _is_tpu_graph_function():\n  graph = tf_v1.get_default_graph()\n  return (graph.building_function and\n          type(graph._get_control_flow_context()).__name__.endswith(  # pylint: disable=protected-access\n              ""TPUReplicateContext""))\n'"
tensorflow_hub/native_module_test.py,273,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.native_module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow_hub import module_def_pb2\nfrom tensorflow_hub import native_module\nfrom tensorflow_hub import tf_v1\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import function as function_eager\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops.control_flow_ops import ControlFlowContext\nfrom tensorflow.python.ops.lookup_ops import HashTable\nfrom tensorflow.python.ops.lookup_ops import index_to_string_table_from_file\nfrom tensorflow.python.ops.lookup_ops import KeyValueTensorInitializer\n# pylint: enable=g-direct-tensorflow-import\n\n\ndef load_module_spec(spec):\n  """"""Force use of native_module implementation.""""""\n  return native_module.Loader()(spec)\n\n\ndef multi_signature_module():\n  x = tf_v1.placeholder(tf.float32, shape=[None])\n  native_module.add_signature(""double"", {""x"": x}, {""y"": 2*x})\n\n  z = tf_v1.placeholder(tf.float32, shape=[None])\n  native_module.add_signature(""square"", {""z"": z}, {""z_out"": z*z})\n\n\ndef batch_norm_module(training):\n  x = tf_v1.placeholder(tf.float32, shape=[None, 3])\n  y = tf_v1.layers.batch_normalization(x, training=training)\n  native_module.add_signature(inputs=x, outputs=y)\n\n\ndef module_with_variables():\n  tf_v1.get_variable(\n      name=""weights"",\n      shape=[3],\n      initializer=tf_v1.zeros_initializer())\n  tf_v1.get_variable(\n      name=""partition"",\n      shape=[4],\n      initializer=tf_v1.zeros_initializer(),\n      partitioner=tf_v1.fixed_size_partitioner(3))\n  hub.add_signature(outputs=tf.constant(1.0))\n\n\nclass NativeModuleTest(tf.test.TestCase):\n\n  def testModuleWithMissingRequiredFeature(self):\n    path = os.path.join(self.get_temp_dir(), ""required-feature"")\n    tf_v1.gfile.MakeDirs(path)\n    proto_path = native_module.get_module_proto_path(path)\n    with tf_v1.gfile.Open(proto_path, mode=""wb"") as f:\n      module_def_proto = module_def_pb2.ModuleDef()\n      module_def_proto.format = module_def_pb2.ModuleDef.FORMAT_V3\n      module_def_proto.required_features.extend([""foo-test-missing""])\n      f.write(module_def_proto.SerializeToString())\n    with self.assertRaisesRegexp(ValueError, ""foo-test-missing""):\n      load_module_spec(path)\n\n  def testMultiSignatureSpec(self):\n    spec = native_module.create_module_spec(multi_signature_module)\n    self.assertAllEqual(sorted(spec.get_signature_names()),\n                        [""double"", ""square""])\n    self.assertAllEqual(list(spec.get_input_info_dict(""double"").keys()), [""x""])\n    self.assertAllEqual(list(spec.get_output_info_dict(""double"").keys()), [""y""])\n    self.assertAllEqual(list(spec.get_input_info_dict(""square"").keys()), [""z""])\n    self.assertAllEqual(list(spec.get_output_info_dict(""square"").keys()),\n                        [""z_out""])\n\n  def testDefaultTagSpec(self):\n    spec = native_module.create_module_spec(multi_signature_module)\n    self.assertAllEqual(sorted(spec.get_tags()), [set()])\n\n  def testMultiTagSpec(self):\n    spec = native_module.create_module_spec(\n        batch_norm_module,\n        [({""training""}, {""training"": True}),\n         ({""inference""}, {""training"": False})])\n    self.assertAllEqual(sorted(spec.get_tags()),\n                        [set([""training""]), set([""inference""])])\n\n  def testModuleWithVariablesAndNoCheckpoint(self):\n    with tf.Graph().as_default():\n      spec = native_module.create_module_spec(module_with_variables)\n      spec._create_impl(name=""module"", trainable=False, tags=None)\n      self.assertAllEqual(\n          [x.op.name for x in tf_v1.global_variables()],\n          [\n              ""module/weights"",\n              ""module/partition/part_0"",\n              ""module/partition/part_1"",\n              ""module/partition/part_2"",\n          ])\n\n      with tf_v1.Session() as session:\n        session.run(tf_v1.initializers.global_variables())\n        expected_values = [\n            [0.0, 0.0, 0.0],\n            [0.0, 0.0],\n            [0.0],\n            [0.0],\n        ]\n        for a, b in zip(session.run(tf_v1.global_variables()), expected_values):\n          self.assertAllEqual(a, b)\n\n  def testNoSignaturesPresent(self):\n\n    def wrong_module_fn():\n      x = tf_v1.placeholder(tf.float32, shape=[None, 3])\n      return tf.identity(x)\n\n    with self.assertRaises(ValueError) as cm:\n      spec = native_module.create_module_spec(wrong_module_fn)\n    self.assertIn(""No signatures present"", str(cm.exception))\n\n  def testUnsupportedCollections(self):\n\n    def module_fn():\n      scale = tf_v1.get_variable(""x"", (), collections=[""my_scope""])\n      x = tf_v1.placeholder(tf.float32, shape=[None, 3])\n      native_module.add_signature(""my_func"", {""x"": x}, {""y"": x*scale})\n\n    with self.assertRaises(ValueError) as cm:\n      _ = native_module.create_module_spec(module_fn)\n      self.assertIn(""Unsupported collections in graph"", cm)\n\n    with tf.Graph().as_default() as tmp_graph:\n      module_fn()\n      unsupported_collections = native_module.get_unsupported_collections(\n          tmp_graph.get_all_collection_keys())\n      self.assertEqual([""my_scope""], unsupported_collections)\n\n    _ = native_module.create_module_spec(\n        module_fn, drop_collections=unsupported_collections)\n\n\nclass RecoverPartitionedVariableMapTest(tf.test.TestCase):\n\n  def testRecoverPartitionedVariableMap(self):\n    with tf.Graph().as_default():\n      with tf_v1.variable_scope(""test""):\n        partitioner = tf_v1.fixed_size_partitioner(3)\n        tf_v1.get_variable(\n            initializer=tf.ones([11, 5]),\n            name=""partitioned_variable"",\n            partitioner=partitioner)\n        tf_v1.get_variable(\n            initializer=tf.ones([11, 5]),\n            name=""normal_variable"")\n\n      all_vars = tf_v1.global_variables()\n      all_vars_dict = {var.op.name[5:]: var for var in all_vars}\n      self.assertEqual(set(all_vars_dict.keys()), set([\n          ""partitioned_variable/part_0"",\n          ""partitioned_variable/part_1"",\n          ""partitioned_variable/part_2"",\n          ""normal_variable""]))\n\n      self.assertEqual(len(all_vars_dict), 4)\n      var_map = native_module.recover_partitioned_variable_map(all_vars_dict)\n      self.assertEqual(set(var_map.keys()), set([\n          ""partitioned_variable"", ""normal_variable""]))\n\n      # Verify order of the partitioned variable list\n      self.assertAllEqual(\n          [v.op.name for v in var_map[""partitioned_variable""]],\n          [\n              ""test/partitioned_variable/part_0"",\n              ""test/partitioned_variable/part_1"",\n              ""test/partitioned_variable/part_2"",\n          ])\n\n\ndef stateless_module_fn():\n  x = tf_v1.placeholder(tf.int64)\n  y = x*x\n  hub.add_signature(inputs=x, outputs=y)\n\n\ndef unused_input_module_fn():\n  x = tf_v1.placeholder(tf.int64)\n  y = tf_v1.placeholder(tf.int64)\n  result = x*x\n  hub.add_signature(\n      inputs={""x"": x, ""unused"": y},\n      outputs=result)\n\n\ndef double_module_fn():\n  w = tf.Variable(2.0)\n  x = tf_v1.placeholder(dtype=tf.float32)\n  hub.add_signature(inputs=x, outputs=x*w)\n\n\ndef create_partitioned_variable_module_fn(partitions, shape):\n  """"""Returns a module summing one normal and one partitioned variable.""""""\n  def module_fn():\n    """"""A module summing one normal and one partitioned variable.""""""\n    partitioner = tf_v1.fixed_size_partitioner(partitions)\n    var_1 = tf_v1.get_variable(\n        initializer=tf.ones(shape),\n        name=""partitioned_variable"",\n        partitioner=partitioner)\n    var_2 = tf_v1.get_variable(\n        initializer=tf.ones(shape), name=""normal_variable"")\n    hub.add_signature(outputs=var_1 + var_2)\n\n  return module_fn\n\n\nclass TFHubStatelessModuleTest(tf.test.TestCase):\n\n  def testLoadModuleFromFuncDef(self):\n    with tf_v1.Session() as sess:\n      v = tf_v1.placeholder(tf.int64)\n      spec = hub.create_module_spec(stateless_module_fn)\n      m = hub.Module(spec)\n      y = m(v)\n      self.assertEqual(sess.run(y, feed_dict={v: 10}), 100)\n\n  def testUnusedInputModule(self):\n    with tf_v1.Session() as sess:\n      v1 = tf_v1.placeholder(tf.int64)\n      v2 = tf_v1.placeholder(tf.int64)\n      spec = hub.create_module_spec(unused_input_module_fn)\n      m = hub.Module(spec)\n      out = m({""x"": v1, ""unused"": v2})\n      self.assertEqual(sess.run(out, feed_dict={v1: 10, v2: 4}), 100)\n\n  def testConvertToTensor(self):\n    spec = hub.create_module_spec(stateless_module_fn)\n    with tf_v1.Session() as sess:\n      m = hub.Module(spec)\n      y = m([10, 2])\n      self.assertAllEqual(sess.run(y), [100, 4])\n    with tf_v1.Session() as sess:\n      m = hub.Module(spec)\n      with self.assertRaises(TypeError):\n        m(""hello"")\n\n  def testArgErrors(self):\n    spec = hub.create_module_spec(stateless_module_fn)\n    with tf_v1.Session():\n      m = hub.Module(spec)\n      with self.assertRaisesRegexp(TypeError, ""missing""):\n        m()\n\n  @test_util.run_v1_only(""b/138681007"")\n  def testUseWithinWhileLoop(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(double_module_fn)\n      m = hub.Module(spec)\n      i = tf.constant(0)\n      x = tf.constant(10.0)\n      p = tf_v1.placeholder(dtype=tf.int32)\n      c = lambda i, x: tf.less(i, p)\n      b = lambda i, x: (tf.add(i, 1), m(x))\n      oi, ox = tf.while_loop(c, b, [i, x])  # ox = v**p * x\n      v = m.variables[0]\n      dodv = tf.gradients(ox, v)[0]  # d ox / dv = p*v**(p-1) * x\n      dodx = tf.gradients(ox, x)[0]  # d ox / dx = v**p\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllEqual(sess.run([oi, ox], feed_dict={p: 1}), [1, 20])\n        self.assertAllEqual(sess.run([oi, ox], feed_dict={p: 2}), [2, 40])\n        self.assertAllEqual(sess.run([oi, ox], feed_dict={p: 4}), [4, 160])\n        # Gradients also use the control flow structures setup earlier.\n        # Also check they are working properly.\n        self.assertAllEqual(sess.run([dodv, dodx], feed_dict={p: 1}), [10, 2])\n        self.assertAllEqual(sess.run([dodv, dodx], feed_dict={p: 2}), [40, 4])\n        self.assertAllEqual(sess.run([dodv, dodx], feed_dict={p: 4}), [320, 16])\n\n  # tf.map_fn() is merely a wrapper around tf.while(), but just to be sure...\n  @test_util.run_v1_only(""b/138681007"")\n  def testUseWithinMap(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(double_module_fn)\n      m = hub.Module(spec)\n      x = tf.constant([1.0, 11.0, 101.0])\n      y = tf.map_fn(m, x)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllEqual(sess.run(y), [2, 22, 202])\n\n  def testClearControlDependenciesForModuleStateButNotApplyGraphs(self):\n    module_spec = hub.create_module_spec(stateless_module_fn)\n\n    with tf.Graph().as_default() as g1:\n      v = tf_v1.placeholder(dtype=tf.int64, name=""v"")\n      m = hub.Module(module_spec)\n      m(v)\n\n    with tf.Graph().as_default() as g2:\n      v = tf_v1.placeholder(dtype=tf.int64, name=""v"")\n      with tf.control_dependencies([v]):\n        m = hub.Module(module_spec)\n      m(v)\n\n    self.assertEqual(g1.as_graph_def(), g2.as_graph_def())\n\n    with tf.Graph().as_default() as g3:\n      v = tf_v1.placeholder(dtype=tf.int64, name=""v"")\n      m = hub.Module(module_spec)\n      m(v)\n\n    with tf.Graph().as_default() as g4:\n      v = tf_v1.placeholder(dtype=tf.int64, name=""v"")\n      m = hub.Module(module_spec)\n      with tf.control_dependencies([v]):\n        m(v)\n\n    self.assertNotEqual(g3.as_graph_def(), g4.as_graph_def())\n\n\ndef sparse_square_module_fn():\n  x = tf_v1.sparse_placeholder(dtype=tf.int64, name=""x"")\n  out = tf.SparseTensor(x.indices, x.values * x.values, x.dense_shape)\n  hub.add_signature(inputs=x, outputs=out)\n\n\nclass TFHubSparseTensorModuleTest(tf.test.TestCase):\n\n  def testSparseTensors(self):\n    square_spec = hub.create_module_spec(sparse_square_module_fn)\n\n    with tf.Graph().as_default():\n      square = hub.Module(square_spec)\n      v = tf_v1.sparse_placeholder(dtype=tf.int64, name=""v"")\n      y = square(v)\n\n      with tf_v1.Session().as_default():\n        indices = [[0, 0], [0, 1], [1, 1]]\n        values = [10, 2, 1]\n        shape = [2, 2]\n        v1 = tf_v1.SparseTensorValue(indices, values, shape)\n        v2 = y.eval(feed_dict={v: v1})\n        v4 = y.eval(feed_dict={v: v2})\n\n        self.assertAllEqual(v4.indices, indices)  # Unchanged.\n        self.assertAllEqual(v4.values, [t**4 for t in values])  # Squared twice.\n        self.assertAllEqual(v4.dense_shape, shape)  # Unchanged.\n\n\ndef stateful_module_fn():\n  v = tf_v1.get_variable(\n      ""var123"", shape=[3],\n      initializer=tf_v1.constant_initializer([1.0, 2.0, 3.0]))\n  hub.add_signature(outputs=v.value())\n\n\ndef stateful_rv_module_fn():\n  r = tf_v1.get_variable(\n      ""rv_var123"", shape=[],\n      initializer=tf_v1.constant_initializer(10.0),\n      use_resource=True)\n  hub.add_signature(outputs=r.value())\n\n\nclass TPUReplicateContext(ControlFlowContext):\n\n  def __init__(self):\n    super(TPUReplicateContext, self).__init__()\n    self._name = ""TPUReplicateContext""\n\n  def AddOp(self, _):\n    pass\n\n  def AddValue(self, x):\n    return x\n\n  def to_control_flow_context_def(self, context_def, export_scope=None):\n    super(TPUReplicateContext, self).to_control_flow_context_def(\n        context_def, export_scope)\n\n\ndef stateful_random_rv_module_fn():\n  r = tf_v1.get_variable(\n      ""rv_var123"",\n      shape=[],\n      initializer=tf_v1.random_uniform_initializer(),\n      use_resource=True)\n  hub.add_signature(outputs=r.value())\n\n\ndef stateful_rv_with_input_module_fn():\n  x = tf_v1.placeholder(dtype=tf.float32, name=""x"")\n  # Add a placeholder/variable that doesn\'t go to an output.\n  y = tf_v1.placeholder(dtype=tf.float32, name=""y"")\n  r = tf_v1.get_variable(\n      ""rv_var123"",\n      shape=[],\n      initializer=tf_v1.constant_initializer(10.0),\n      use_resource=True)\n  t = tf_v1.get_variable(\n      ""rv_var456"",\n      shape=[],\n      initializer=tf_v1.constant_initializer(10.0),\n      use_resource=True)\n  t.assign(y)\n  res = x + r\n  hub.add_signature(inputs={""x"": x}, outputs=res)\n\n\ndef control_dependency_module_fn():\n  const_op = tf.constant(1.0, name=""dependency_op"")\n  with tf.control_dependencies([const_op]):\n    res = tf.constant(3.0) + tf.constant(2.0)\n  hub.add_signature(inputs={}, outputs=res)\n\n\ndef stateful_non_rv_module_fn():\n  v = tf_v1.get_variable(\n      ""var123"", shape=[],\n      initializer=tf_v1.constant_initializer(10.0),\n      use_resource=False)\n  hub.add_signature(outputs=v.value())\n\n\ndef stateful_module_fn_with_colocation():\n  v = tf_v1.get_variable(\n      ""var123"", shape=[],\n      initializer=tf_v1.constant_initializer(1.0),\n      use_resource=False)\n  v_value = v.value()\n  x = tf_v1.placeholder(dtype=tf.float32, name=""x"")\n  with tf_v1.colocate_with(v), tf_v1.colocate_with(x):\n    y = tf.add(v_value, x, name=""y"")\n  hub.add_signature(inputs=x, outputs=y)\n\n\nclass TFHubStatefulModuleTest(tf.test.TestCase):\n\n  def testVariables(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(stateful_module_fn)\n      m = hub.Module(spec, name=""test"")\n      out = m()\n      self.assertEqual(list(m.variable_map.keys()), [""var123""])\n      self.assertEqual(m.variable_map[""var123""].name, ""test/var123:0"")\n      self.assertEqual([v.name for v in m.variables], [""test/var123:0""])\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(out), [1.0, 2.0, 3.0])\n\n  def testResourceVariables(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(stateful_rv_module_fn)\n      m = hub.Module(spec, name=""test_rv"")\n      out = m()\n      self.assertEqual(list(m.variable_map.keys()), [""rv_var123""])\n      self.assertEqual(m.variable_map[""rv_var123""].name, ""test_rv/rv_var123:0"")\n      self.assertEqual([v.name for v in m.variables], [""test_rv/rv_var123:0""])\n\n      # Check that ""shared_name"" attributes are adapted correctly:\n      var_handle_op_name = ""test_rv/rv_var123""\n      var_handle_op = tf_v1.get_default_graph().get_operation_by_name(\n          var_handle_op_name)\n      self.assertEqual(\n          var_handle_op.get_attr(""shared_name""),\n          tf.compat.as_bytes(var_handle_op_name))\n\n      export_path = os.path.join(self.get_temp_dir(), ""resource-variables"")\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(out), 10.0)\n        m.export(export_path, sess)\n\n    with tf.Graph().as_default():\n      f = hub.Module(export_path)\n      out = f()\n\n      # Test colocation constraints on the read op in the apply graph.\n      # It has two legal values:\n      # - Colocation with the VarHandleOp in the state graph.\n      # - No constraint, in which case it reports its own colocation_group.\n      #   This appears to happen at the time of this writing (March 2018)\n      #   because the Python code relies on the TensorFlow core to handle\n      #   VariableReadOps as a special case and colocate them with their\n      #   VarHandleOp input, which is mapped to the state graph.\n      # In any case, the point is to *not* colocate with the stillborn copy\n      # of the VarHandleOp in the apply graph scope.\n      if out.op.colocation_groups() != [\n          tf.compat.as_bytes(""loc:@"" + out.op.name)]:\n        self.assertItemsEqual(out.op.colocation_groups(),\n                              [tf.compat.as_bytes(""loc:@module/rv_var123"")])\n\n      # Check that ""shared_name"" attributes are adapted correctly:\n      var_handle_op_name = ""module/rv_var123""\n      var_handle_op = tf_v1.get_default_graph().get_operation_by_name(\n          var_handle_op_name)\n      self.assertEqual(\n          var_handle_op.get_attr(""shared_name""),\n          tf.compat.as_bytes(var_handle_op_name))\n\n      # Create a saver for the whole graph.\n      saver = tf_v1.train.Saver()\n\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(out), 10.0)\n\n        # Make sure that the variable names stored in a checkpoint of the graph\n        # are as expected.\n        variables_path = os.path.join(self.get_temp_dir(), ""variables"")\n        saver.save(\n            sess, variables_path, write_meta_graph=False, write_state=False)\n        variable_names_and_shapes = tf_v1.train.list_variables(\n            ckpt_dir_or_file=variables_path)\n        variable_names = set(name for name, _ in variable_names_and_shapes)\n        self.assertEqual(variable_names, {""module/rv_var123""})\n\n  def testNonResourceVariables(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(stateful_non_rv_module_fn)\n      m = hub.Module(spec, name=""test_non_rv"")\n      out = m()\n      self.assertEqual(list(m.variable_map.keys()), [""var123""])\n      self.assertEqual(m.variable_map[""var123""].name, ""test_non_rv/var123:0"")\n      self.assertEqual([v.name for v in m.variables], [""test_non_rv/var123:0""])\n\n      export_path = os.path.join(self.get_temp_dir(), ""non-resource-variables"")\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(out), 10.0)\n        m.export(export_path, sess)\n\n      with tf.Graph().as_default():\n        f = hub.Module(export_path)\n        out = f()\n\n        # Test that the read op in the apply graph gets colocated with the\n        # variable in the state graph scope ""module/"" (and not the stillborn\n        # copy in the apply graph scope).\n        self.assertItemsEqual(out.op.colocation_groups(),\n                              [tf.compat.as_bytes(""loc:@module/var123"")])\n\n        # Create a saver for the whole graph.\n        saver = tf_v1.train.Saver()\n\n        with tf_v1.Session() as sess:\n          sess.run(tf_v1.global_variables_initializer())\n          self.assertAllClose(sess.run(out), 10.0)\n\n          # Make sure that the variable names stored in a checkpoint of the\n          # graph are as expected.\n          variables_path = os.path.join(self.get_temp_dir(), ""variables"")\n          saver.save(\n              sess, variables_path, write_meta_graph=False, write_state=False)\n          variable_names_and_shapes = tf_v1.train.list_variables(\n              ckpt_dir_or_file=variables_path)\n          variable_names = set(name for name, _ in variable_names_and_shapes)\n          self.assertEqual(variable_names, {""module/var123""})\n\n  @test_util.run_v1_only(""b/138681007"")\n  def testNonResourceVariableInWhileLoop(self):\n    with tf.Graph().as_default():\n      # This test uses non-Resource variables to see an actual colocation\n      # constraint propagated to the context Enter op. The long comment on\n      # colocation in testResourceVariables explains why they may not offer\n      # that.\n      spec = hub.create_module_spec(stateful_non_rv_module_fn)\n      m = hub.Module(spec)\n      cond = lambda i, x: tf.less(i, 4)\n      def body(i, x):\n        v = m()\n        self.assertItemsEqual(v.op.colocation_groups(),\n                              [tf.compat.as_bytes(""loc:@module/var123"")])\n        return (tf.add(i, 1), 2*x)\n      oi, ox = tf.while_loop(cond, body, [0, 10.0])\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllEqual(sess.run([oi, ox]), [4, 160.0])\n\n  @test_util.run_v1_only(""b/138681007"")\n  def testNonResourceVariableInCond(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(stateful_non_rv_module_fn)\n      m = hub.Module(spec)\n      pred = tf_v1.placeholder(tf.bool)\n      def true_fn():\n        v = m()\n        self.assertItemsEqual(v.op.colocation_groups(),\n                              [tf.compat.as_bytes(""loc:@module/var123"")])\n        return v\n      def false_fn():\n        return tf.constant(9.0)\n      out = tf.cond(pred, true_fn, false_fn)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertEqual(sess.run(out, feed_dict={pred: True}), 10.0)\n        self.assertEqual(sess.run(out, feed_dict={pred: False}), 9.0)\n\n  def testVariableColocationPropagation(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(stateful_module_fn_with_colocation)\n      m = hub.Module(spec)\n      u1 = tf.constant(1, name=""u1"")\n      u2 = tf.constant(2, name=""u2"")\n      with tf_v1.colocate_with(u1), tf_v1.colocate_with(u2):\n        x = tf.constant(100.0, name=""x"")\n      y = m(x)\n      self.assertItemsEqual(y.op.colocation_groups(),\n                            [tf.compat.as_bytes(""loc:@module/var123""),\n                             tf.compat.as_bytes(""loc:@u1""),\n                             tf.compat.as_bytes(""loc:@u2"")])\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertEqual(sess.run(y), 101.0)\n\n  def testPartitionedVariables(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(\n          create_partitioned_variable_module_fn(partitions=3, shape=[7, 3]))\n      m = hub.Module(spec, name=""test"")\n      out = m()\n      self.assertEqual(len(m.variable_map), 2)\n      self.assertEqual(m.variable_map[""normal_variable""].name,\n                       ""test/normal_variable:0"")\n      self.assertAllEqual([\n          variable.name for variable in m.variable_map[""partitioned_variable""]\n      ], [\n          ""test/partitioned_variable/part_0:0"",\n          ""test/partitioned_variable/part_1:0"",\n          ""test/partitioned_variable/part_2:0""\n      ])\n      self.assertAllEqual(  # Check deterministric order (by variable_map key).\n          [variable.name for variable in m.variables],\n          [""test/normal_variable:0"",\n           ""test/partitioned_variable/part_0:0"",\n           ""test/partitioned_variable/part_1:0"",\n           ""test/partitioned_variable/part_2:0""])\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(out), 2 * np.ones([7, 3]))\n\n  def testLargePartitionedVariables(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(\n          create_partitioned_variable_module_fn(partitions=25, shape=[600, 3]))\n      m = hub.Module(spec, name=""test"")\n      out = m()\n      self.assertEqual(len(m.variable_map), 2)\n      self.assertEqual(len(m.variable_map[""partitioned_variable""]), 25)\n      self.assertEqual(len(m.variables), 26)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(out), 2 * np.ones([600, 3]))\n\n  def testLoadTrainableModuleFromFuncDef(self):\n    with tf_v1.Session() as sess:\n      spec = hub.create_module_spec(stateful_module_fn)\n      m = hub.Module(spec, trainable=True)\n      x = m()\n      step = tf.Variable(0, trainable=False, name=""global_step"")\n      train_op = tf_v1.train.GradientDescentOptimizer(0.40).minimize(\n          loss=tf_v1.losses.mean_squared_error(x, [3.1, 3.2, 3.3]),\n          global_step=step)\n      sess.run(tf_v1.global_variables_initializer())\n      for _ in range(50):\n        sess.run(train_op)\n      got = sess.run(x)\n      self.assertAllClose(got, [3.1, 3.2, 3.3])\n\n  # TODO(b/112575006): The following tests verify functionality of function call\n  # within a TPU context. Work to generalize this for all function calls is\n  # ongoing.\n  def testTPUModuleInitializeOnceWithDefun(self):\n    spec = hub.create_module_spec(stateful_random_rv_module_fn)\n\n    @function.Defun()\n    def import_computation():\n      context = TPUReplicateContext()\n      context.Enter()\n      m = hub.Module(spec, name=""module_"", trainable=True)\n      return [m(), m()]\n\n    with tf_v1.Graph().as_default(), tf_v1.Session() as sess:\n      x = import_computation()\n      sess.run(tf_v1.global_variables_initializer())\n      got = sess.run(x)\n      # Check the values are equal. If the initializer ran on each call,\n      # the values would be different.\n      self.assertEqual(got[0], got[1])\n\n  def testTPUPruneWithUnusedInput(self):\n    spec = hub.create_module_spec(unused_input_module_fn)\n\n    @function.Defun()\n    def import_computation(x):\n      context = TPUReplicateContext()\n      context.Enter()\n      m = hub.Module(spec, name=""module_"", trainable=True)\n      return m({\n          ""x"": tf.cast(x, dtype=tf.int64),\n          ""unused"": tf.constant(2, dtype=tf.int64)\n      })\n\n    with tf_v1.Graph().as_default(), tf_v1.Session() as sess:\n      x = import_computation(5)\n      got = sess.run(x)\n      self.assertEqual(got, 25)\n\n  def testTPUModuleDoesntPruneControlDependencies(self):\n    spec = hub.create_module_spec(control_dependency_module_fn)\n\n    @function.Defun()\n    def import_computation():\n      context = TPUReplicateContext()\n      context.Enter()\n      m = hub.Module(spec, name=""module_"", trainable=True)\n      return m()\n\n    with tf_v1.Graph().as_default(), tf_v1.Session() as sess:\n      x = import_computation()\n      got = sess.run(x)\n      self.assertEqual(got, 5.0)\n      # If the op got pruned, the following get_operation_by_name should fail\n      # with a dependency error.\n      tf_v1.get_default_graph().get_operation_by_name(""module_/dependency_op"")\n\n  def testTPUModuleWithDefun(self):\n    spec = hub.create_module_spec(stateful_rv_with_input_module_fn)\n\n    @function.Defun()\n    def import_computation(first, second):\n      context = TPUReplicateContext()\n      context.Enter()\n      m = hub.Module(spec, name=""module_"", trainable=True)\n      return [m(first), m(second)]\n\n    with tf_v1.Graph().as_default(), tf_v1.Session() as sess:\n      x = import_computation(9.0, 6.0)\n      sess.run(tf_v1.global_variables_initializer())\n      got = sess.run(x)\n      self.assertEqual(got, (19.0, 16.0))\n\n  def testTPUModuleWithTFEDefun(self):\n    with tf_v1.Graph().as_default() as graph, tf_v1.Session() as sess:\n      spec = hub.create_module_spec(stateful_rv_with_input_module_fn)\n\n      @function_eager.defun()\n      def import_computation(first, second):\n        context = TPUReplicateContext()\n        context.Enter()\n        m = hub.Module(spec, trainable=True)\n        return [m(first), m(second)]\n\n      x = import_computation(9.0, 6.0)\n      sess.run(tf_v1.global_variables_initializer())\n      got = sess.run(x)\n      self.assertEqual(got, [19.0, 16.0])\n\n  def testTPUModuleWithWrapFunc(self):\n    spec = hub.create_module_spec(stateful_rv_with_input_module_fn)\n\n    def import_computation(first, second):\n      context = TPUReplicateContext()\n      context.Enter()\n      m = hub.Module(spec, trainable=True)\n      return [m(first), m(second)]\n\n    with tf_v1.Graph().as_default(), tf_v1.Session() as sess:\n      x = tf_v1.wrap_function(\n          import_computation,\n          [tf.TensorSpec((), tf.float32),\n           tf.TensorSpec((), tf.float32)])\n      sess.run(tf_v1.global_variables_initializer())\n      got = sess.run(x(9.0, 6.0))\n      self.assertEqual(got, [19.0, 16.0])\n\n  def _exportModulewithTrainedVariable(self):\n    export_path = os.path.join(self.get_temp_dir(), ""var-module"")\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(stateful_module_fn)\n      m = hub.Module(spec, trainable=True)\n      assign_op = tf_v1.assign(m.variable_map[""var123""],\n                               tf.constant([9.0, 9.0, 9.0]))\n      with tf_v1.Session() as sess:\n        sess.run(assign_op)\n        m.export(export_path, sess)\n    return export_path\n\n  def testModuleWithTrainedVariable(self):\n    with tf.Graph().as_default():\n      f = hub.Module(self._exportModulewithTrainedVariable())\n      out = f()\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        got = sess.run(out)\n        self.assertAllClose(got, [9.0, 9.0, 9.0])\n\n  def testModuleEvalWithTrainedVariable(self):\n    export_path = self._exportModulewithTrainedVariable()\n    with hub.eval_function_for_module(export_path) as f:\n      self.assertAllClose(f(), [9.0, 9.0, 9.0])\n\n\ndef table_lookup_module_fn():\n  x = tf_v1.placeholder(dtype=tf.int64, name=""x"")\n  keys = tf.constant([0, 1, 2], dtype=tf.int64)\n  values = tf.constant([""index0"", ""hello"", ""world""])\n\n  tbl_init = KeyValueTensorInitializer(keys, values)\n  table = HashTable(tbl_init, ""UNK"")\n  hub.add_signature(inputs=x, outputs=table.lookup(x))\n\n\nclass TFHubTableLookupModuleTest(tf.test.TestCase):\n\n  def _exportModuleWithTable(self):\n    export_path = os.path.join(self.get_temp_dir(), ""table-module"")\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(table_lookup_module_fn)\n      m = hub.Module(spec)\n      # Export requires a session to work regardless of the module having no\n      # variables to export.\n      with tf_v1.Session() as sess:\n        m.export(export_path, sess)\n    return export_path\n\n  def testModuleWithTable(self):\n    with tf.Graph().as_default():\n      v = tf_v1.placeholder(dtype=tf.int64)\n      f = hub.Module(self._exportModuleWithTable())\n      y = f(v)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.tables_initializer())\n        got = sess.run(y, feed_dict={v: [0, 1, 2, 3]})\n        self.assertAllEqual(list(got), [b""index0"", b""hello"", b""world"", b""UNK""])\n\n  def testModuleEvalWithTable(self):\n    with hub.eval_function_for_module(self._exportModuleWithTable()) as f:\n      got = f([0, 1, 2, 3])\n      self.assertAllEqual(list(got), [b""index0"", b""hello"", b""world"", b""UNK""])\n\n\ndef do_table_lookup(indices, vocabulary_file):\n  table = index_to_string_table_from_file(\n      vocabulary_file=vocabulary_file,\n      default_value=""UNKNOWN"")\n  return table.lookup(indices)\n\n\ndef layers_module_fn():\n  """"""Module that exercises the use of layers.""""""\n  # This is a plain linear map Mx+b regularized by the sum of the squares\n  # of the coefficients in M and b.\n  x = tf_v1.placeholder(dtype=tf.float32, shape=[None, 2], name=""x"")\n  def l2(weights):\n    """"""Applies l2 regularization to weights.""""""\n    with tf.control_dependencies([weights]):\n      return 2.0 * tf_v1.nn.l2_loss(weights)\n\n  h = tf_v1.layers.dense(\n      x, 2,\n      activation=None,\n      kernel_regularizer=l2,\n      bias_regularizer=l2)\n  hub.add_signature(inputs=x, outputs=h)\n\n\nclass TFHubLayersModuleTest(tf.test.TestCase):\n\n  def testModuleWithLayers(self):\n    export_path = os.path.join(self.get_temp_dir(), ""layers-module"")\n\n    sample_input = [[1.0, 2.0], [3.1, 10.0]]\n\n    spec = hub.create_module_spec(layers_module_fn)\n    with tf.Graph().as_default():\n      m = hub.Module(spec, trainable=False)\n      x = tf_v1.placeholder(dtype=tf.float32)\n      y = m(x)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        sample_output = sess.run(y, feed_dict={x: sample_input})\n        m.export(export_path, sess)\n\n    with tf.Graph().as_default():\n      x = tf_v1.placeholder(dtype=tf.float32)\n      y = hub.Module(export_path)(x)\n\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        got = sess.run(y, feed_dict={x: sample_input})\n        self.assertAllEqual(got, sample_output)\n\n  def testModuleWithRegularizedLayers(self):\n    # The linear map y = Mx + b with L2 regularization on M and b\n    # when trained at x = [1,1] with L2 loss towards the target y\' = [4,4]\n    # learns M = [[1,1],[1,1]], b = [1,1], y = [3,3], with eight balanced\n    # loss terms: the elements of M, b, and y\' - y are all distance 1 from zero.\n    train_input = [[1.0, 1.0]]\n    target = [[4.0, 4.0]]\n\n    spec = hub.create_module_spec(layers_module_fn)\n    with tf.Graph().as_default():\n      m = hub.Module(spec, trainable=True)\n      x = tf_v1.placeholder(dtype=tf.float32)\n      y = m(x)\n      squared_loss = tf_v1.losses.mean_squared_error(y, target, weights=2.0)\n      # Recover REGULARIZATION_LOSSES from the module.\n      total_loss = squared_loss + tf_v1.losses.get_regularization_loss()\n      step = tf.Variable(0, trainable=False, name=""global_step"")\n      train = tf_v1.train.GradientDescentOptimizer(0.1).minimize(\n          loss=total_loss, global_step=step)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        for _ in range(50):\n          sess.run(train, feed_dict={x: train_input})\n        # Verify M = [[1,1],[1,1]], b = [1,1] by evaluating at three points.\n        # Without regularization, the result would be an underdetermined mess.\n        out = sess.run(y, feed_dict={x: [[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]]})\n        self.assertAllClose(\n            out, [[1.0, 1.0], [2.0, 2.0], [2.0, 2.0]], atol=0.001)\n\n\ndef valid_colocation_module_fn():\n  w = tf.Variable(42 + 69, name=""w"")\n  # w.op has the same name on resource and non-resource variables\n  with tf_v1.colocate_with(w.op):\n    # A colocation reference among state nodes is ok.\n    v = tf.Variable(1.0, name=""v"")\n    assert v.op.colocation_groups() == [tf.compat.as_bytes(""loc:@w"")]\n    # A colocation reference from other nodes to state nodes is ok.\n    y = tf.add(v, 1, name=""y"")\n    assert y.op.colocation_groups() == [tf.compat.as_bytes(""loc:@w"")]\n  x = tf_v1.placeholder(dtype=tf.float32, name=""x"")\n  with tf_v1.colocate_with(x):\n    # A colocation reference from other nodes to input nodes is ok.\n    z = tf.add(x, 1, name=""z"")\n    assert z.op.colocation_groups() == [tf.compat.as_bytes(""loc:@x"")]\n  hub.add_signature(inputs=dict(x=x), outputs=dict(y=y, z=z))\n\n\ndef bad_input_colocation_module_fn():\n  u = tf.add(42, 69, name=""u"")\n  with tf_v1.colocate_with(u):\n    # Inputs must not reference other nodes for colocation.\n    x = tf_v1.placeholder(tf.float32, name=""x"")\n  y = x + 1.0\n  hub.add_signature(inputs=x, outputs=y)\n\n\ndef bad_state_colocation_module_fn():\n  u = tf.add(42, 69, name=""u"")\n  with tf_v1.colocate_with(u):\n    # State-holding nodes must not reference other nodes for colocation.\n    v = tf.Variable(1.0, name=""v"")\n  x = tf_v1.placeholder(dtype=tf.float32)\n  y = x + v\n  hub.add_signature(inputs=x, outputs=y)\n\n\ndef brittle_multivalued_colocation_module_fn():\n  x, y = tf.split([1, 2], 2, name=""split"")\n  with tf_v1.colocate_with(x), tf_v1.colocate_with(y):\n    z = tf.add(x, y, name=""add"")\n    assert z.op.colocation_groups() == [tf.compat.as_bytes(""loc:@split"")]\n  hub.add_signature(inputs=dict(x=x, y=y), outputs=z, name=""both"")\n  hub.add_signature(inputs=dict(x=x), outputs=z, name=""partial"")\n\n\nclass ColocationRewritingTest(tf.test.TestCase):\n\n  def testValidCase(self):\n    """"""Tests a complex, valid case end-to-end.""""""\n    spec = hub.create_module_spec(valid_colocation_module_fn)\n    with tf.Graph().as_default():\n      u = tf.constant(7.0, name=""u"")\n      m = hub.Module(spec, name=""m"")\n      outputs = m(dict(x=u), as_dict=True)\n      self.assertItemsEqual(outputs[""y""].op.colocation_groups(),\n                            [tf.compat.as_bytes(""loc:@m/w"")])\n      self.assertItemsEqual(outputs[""z""].op.colocation_groups(),\n                            [tf.compat.as_bytes(""loc:@u"")])\n\n  def testBadInputColocation(self):\n    """"""Tests catching bad colocation of inputs during create_module_spec.""""""\n    with self.assertRaisesRegexp(ValueError, ""(?s)input.*colocate.*loc:@u""):\n      _ = hub.create_module_spec(bad_input_colocation_module_fn)\n\n  def testBadStateColocation(self):\n    """"""Tests catching bad colocation of states during create_module_spec.""""""\n    with self.assertRaisesRegexp(ValueError, ""(?s)state.*colocate.*loc:@u""):\n      _ = hub.create_module_spec(bad_state_colocation_module_fn)\n\n  def testInputsFromMultivaluedOp(self):\n    """"""Tests warning for inputs from multivalued ops in create_module_spec.""""""\n    # Ideally, one would be able to write\n    #    with self.assertLogs(""blah""): hub.create_module_spec(module_fn)\n    # but in the absence of assertions on logs, we test the underlying helper\n    # in the environment seen from within a module_fn.\n    with tf.Graph().as_default():\n      first, _ = tf.split([[1, 2], [3, 4]], 2, name=""split1"")\n      _, second = tf.split([[5, 6], [7, 8]], 2, name=""split2"")\n      third = tf.constant(105, name=""const"")\n      message = native_module.find_signature_inputs_from_multivalued_ops(\n          dict(first=first, second=second, third=third))\n    self.assertRegexpMatches(\n        message,\n        "".*single output.*\\n""\n        ""Affected inputs: first=\'split1:0\', second=\'split2:1\'$"")\n    # Also test the case of no errors.\n    with tf.Graph().as_default():\n      first = tf.constant(101)\n      second = tf.constant(102)\n      third = tf.constant(103)\n      message = native_module.find_signature_inputs_from_multivalued_ops(\n          dict(first=first, second=second, third=third))\n    self.assertIsNone(message)\n\n  def testSparseInputsFromMultivaluedOp(self):\n    """"""Tests warning for SparseTensor inputs from multivalued ops.""""""\n    with tf.Graph().as_default():\n      one, _ = tf_v1.sparse_split(\n          sp_input=tf.SparseTensor(indices=[[0, 1], [1, 2]], values=[1, 2],\n                                   dense_shape=[2, 3]),\n          num_split=2, axis=0, name=""op1"")\n      _, two = tf_v1.sparse_split(\n          sp_input=tf.SparseTensor(indices=[[0, 0], [1, 1]], values=[3, 4],\n                                   dense_shape=[2, 3]),\n          num_split=2, axis=0, name=""op2"")\n      three = tf.SparseTensor(indices=[[0]], values=[5], dense_shape=[2])\n      message = native_module.find_signature_inputs_from_multivalued_ops(\n          dict(one=one, two=two, three=three))\n    self.assertRegexpMatches(\n        message,\n        "".*single output.*\\nAffected inputs: ""\n        ""one.indices=\'op1:0\', one.values=\'op1:2\', one.dense_shape=\'op1:4\', ""\n        ""two.indices=\'op2:1\', two.values=\'op2:3\', two.dense_shape=\'op2:5\'$"")\n    # Also test the case of no errors.\n    with tf.Graph().as_default():\n      one = tf.SparseTensor(indices=[[0]], values=[1], dense_shape=[2])\n      two = tf.SparseTensor(indices=[[1]], values=[2], dense_shape=[2])\n      message = native_module.find_signature_inputs_from_multivalued_ops(\n          dict(one=one, two=two, three=three))\n    self.assertIsNone(message)\n\n  def testBrittleColocationWithInputsFromMultivaluedOp(self):\n    """"""Tests handling of ambiguous rewrites during module.__call__.""""""\n    spec = hub.create_module_spec(brittle_multivalued_colocation_module_fn)\n    with tf.Graph().as_default():\n      u = tf.constant([1], name=""u"")\n      with tf_v1.colocate_with(u):\n        v = tf.constant([2], name=""v"")\n      w = tf.constant([3], name=""w"")\n      m = hub.Module(spec, name=""m"")\n      # It works if both inputs are mapped to ops with equal colocation groups.\n      assert u.op.colocation_groups() == v.op.colocation_groups()\n      z = m(dict(x=u, y=v), signature=""both"")\n      self.assertItemsEqual(z.op.colocation_groups(),\n                            [tf.compat.as_bytes(""loc:@u"")])\n      # It crashes in the general case.\n      assert u.op.colocation_groups() != w.op.colocation_groups()\n      with self.assertRaisesRegexp(\n          ValueError,\n          # In Python 3 (but not 2), colocation groups are lists of bytes,\n          # which are formatted with a leading ""b"" just before the quotes.\n          r""(?s)Failed to rewrite .*b?\'loc:@m_apply_both_1/split\' .*""\n          ""\\[b?\'loc:@[uw]\'\\] vs \\[b?\'loc:@[wu]\'\\]""):\n        z = m(dict(x=u, y=w), signature=""both"")\n\n  def testBadColocationWithPartialInputsFromMultivaluedOp(self):\n    spec = hub.create_module_spec(brittle_multivalued_colocation_module_fn)\n    with tf.Graph().as_default():\n      u = tf.constant([1], name=""u"")\n      m = hub.Module(spec, name=""m"")\n      with self.assertRaisesRegexp(\n          ValueError,\n          r""(?s)Failed to rewrite .*b?\'loc:@m_apply_partial/split\' .*""\n          ""\\[b?\'loc:@u\'\\] vs \\[b?\'loc:@m_apply_partial/split\'\\]""):\n        z = m(dict(x=u), signature=""partial"")\n\n\ndef update_ops_module_fn():\n  counter = tf.Variable(0, trainable=False)\n  tf_v1.add_to_collection(tf_v1.GraphKeys.UPDATE_OPS, counter.assign_add(1))\n  hub.add_signature(inputs=None, outputs=counter.value())\n\n\nclass TFHubUpdateOpsTest(tf.test.TestCase):\n\n  def testUpdateOps(self):\n    spec = hub.create_module_spec(update_ops_module_fn)\n    with tf_v1.Session() as sess:\n      trainable_module = hub.Module(spec, trainable=True)\n      fixed_module = hub.Module(spec, trainable=False)\n\n      # TODO(b/62433105): Understand what is the desired behaviour of UPDATE_OPS\n      # and applying a Module multiple times. For now UPDATE_OPS probably only\n      # do something reasonable if each Module is applied exactly one time.\n      trainable_module()\n      fixed_module()\n\n      variable = tf.Variable(0.0)\n      step = tf.Variable(0, trainable=False, name=""global_step"")\n      update_ops = tf_v1.get_collection(tf_v1.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        train_op = tf_v1.train.GradientDescentOptimizer(0.1).minimize(\n            loss=variable,\n            global_step=step)\n\n      sess.run(tf_v1.global_variables_initializer())\n      sess.run(train_op)\n      trainable_module_vars = list(trainable_module.variable_map.values())\n      self.assertEqual(len(trainable_module_vars), 1)\n      self.assertEqual(sess.run(trainable_module_vars[0]), 1)\n      fixed_module_vars = list(fixed_module.variable_map.values())\n      self.assertEqual(len(fixed_module_vars), 1)\n      self.assertEqual(sess.run(fixed_module_vars[0]), 0)\n\n\ndef batch_norm_module_fn(is_training):\n  """"""Module that exercises batch normalization, incl. UPDATE_OPS.""""""\n  x = tf_v1.placeholder(dtype=tf.float32, shape=[None, 1], name=""x"")\n  y = tf_v1.layers.batch_normalization(\n      momentum=0.4,\n      inputs=x,\n      fused=False,\n      training=is_training)\n  hub.add_signature(inputs=x, outputs=y)\n\n\nclass TFHubBatchNormModuleTest(tf.test.TestCase):\n\n  # This test is intended to verify the following:\n  # 1) A module_fn that uses batch normalization through tf.layers.contrib\n  #    (and its underlying utilities from tf.nn) can be used to create,\n  #    export, load and use the Module.\n  # 2) Batch normalization learns the scale and offset parameters for its\n  #    output as it should.\n  # 3) The UPDATE_OPS added internally for the moving_mean and moving_variance\n  #    over the training data are properly executed at training time, and their\n  #    results are used at serving time, without further change.\n  def testModuleWithBatchNorm(self):\n    export_path = os.path.join(self.get_temp_dir(), ""batch-norm-module"")\n    # This test resorts to lookup by name to retrieve the moving mean,\n    # because tf.contrib.layers.batch_norm() does not return it, and even if,\n    # module_fn() has no way to return it next to the result for training.\n    moving_mean_name = (\n        ""module/batch_normalization/moving_mean/Read/ReadVariableOp:0"")\n\n    batch_norm_train_tags = [""batch_norm_trains""]\n    batch_norm_fixed_tags = [""batch_norm_fixed""]\n    spec = hub.create_module_spec(\n        batch_norm_module_fn,\n        [(batch_norm_train_tags, {""is_training"": True}),\n         (batch_norm_fixed_tags, {""is_training"": False})])\n    # Test Module creation and training.\n    with tf.Graph().as_default() as g:\n      m = hub.Module(spec, trainable=True, tags=batch_norm_train_tags)\n      # The module is trained on a fixed batch of inputs, which has a mean\n      # of 12.0 and some sample variance of a less obvious value. The module\n      # learns scale and offset parameters that achieve the mapping x --> 2*x\n      # for the observed mean and variance.\n      x = tf.constant([[11.0], [12.0], [13.0]])\n      training_mean = [12.0]\n      y_target = tf.constant([[22.0], [24.0], [26.0]])\n      y = m(x)\n      step = tf.Variable(0, trainable=False, name=""global_step"")\n      moving_mean = g.get_tensor_by_name(moving_mean_name)\n      update_ops = tf_v1.get_collection(tf_v1.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        train = tf_v1.train.GradientDescentOptimizer(0.1).minimize(\n            loss=tf_v1.losses.mean_squared_error(y, y_target), global_step=step)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(moving_mean), [0.0])\n        for _ in range(100):\n          sess.run([train])\n        trained_moving_mean, trained_y = sess.run([moving_mean, y])\n        self.assertAllClose(trained_moving_mean, training_mean)\n        self.assertAllClose(trained_y, [[22.0], [24.0], [26.0]])\n        # Test export.\n        m.export(export_path, sess)\n\n    # Test import and use.\n    spec = load_module_spec(export_path)\n    with tf.Graph().as_default() as g:\n      # The module gets run for inference on inputs with different mean and\n      # variance. However, both mean and variance as well as offset and scale\n      # are now frozen to the values from learning, so the same mapping\n      # x --> 2*x is recovered.\n      x = tf.constant([[10.0], [20.0], [30.0]])\n      y = hub.Module(\n          spec, tags=batch_norm_fixed_tags)(x)\n      moving_mean = g.get_tensor_by_name(moving_mean_name)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        for _ in range(100):\n          served_moving_mean, served_y = sess.run([moving_mean, y])\n        # No update occurs to the moving_mean from training time.\n        self.assertAllClose(served_moving_mean, training_mean)\n        # Prediction results are correct.\n        self.assertAllClose(served_y, [[20.0], [40.0], [60.0]])\n\n\ndef multiple_outputs_module_fn():\n  x = tf_v1.placeholder(dtype=tf.float32)\n  v = tf.Variable([3.0])\n  hub.add_signature(\n      inputs={""x"": x},\n      outputs={""y"": v * x, ""z"": v * v * x})\n\n\nclass TFHubMultipleOutputsTest(tf.test.TestCase):\n\n  def testMultipleOutputs(self):\n    with tf_v1.Session() as sess:\n      spec = hub.create_module_spec(multiple_outputs_module_fn)\n      m = hub.Module(spec)\n      output = m(tf.constant([2.0]), as_dict=True)\n      output1 = output[""y""]\n      output2 = output[""z""]\n      sess.run(tf_v1.global_variables_initializer())\n      self.assertAllClose(sess.run(output1), [6.0])\n      self.assertAllClose(sess.run(output2), [18.0])\n\n\ndef create_assets_module_fn(vocabulary_file):\n\n  def assets_module_fn():\n    indices = tf_v1.placeholder(dtype=tf.int64, name=""indices"")\n    outputs = do_table_lookup(indices, vocabulary_file)\n    hub.add_signature(inputs=indices, outputs=outputs)\n\n  return assets_module_fn\n\n\ndef create_consumer_module_fn(exported_hub_module):\n\n  def consumer_module_fn():\n    indices = tf_v1.placeholder(dtype=tf.int64, name=""indices"")\n    inner_module = hub.Module(exported_hub_module)\n    inner_module_output = inner_module(indices)\n    output = tf.identity(inner_module_output)\n    hub.add_signature(inputs=indices, outputs=output)\n\n  return consumer_module_fn\n\n\nclass TFHubAssetsTest(tf.test.TestCase):\n\n  def create_vocab_file(self, path, vocab):\n    vocabulary_file = os.path.join(self.get_temp_dir(), ""tokens.txt"")\n    with open(vocabulary_file, ""w+"") as vocab_file:\n      for line in vocab:\n        vocab_file.write(line)\n        vocab_file.write(os.linesep)\n    return vocabulary_file\n\n  def testAssets(self):\n    export_path = os.path.join(self.get_temp_dir(), ""assets-module"")\n    vocabulary_file = self.create_vocab_file(""tokens.txt"",\n                                             [""emerson"", ""lake"", ""palmer""])\n    with tf.Graph().as_default():\n      assets_module_fn = create_assets_module_fn(vocabulary_file)\n      spec = hub.create_module_spec(assets_module_fn)\n      embedding_module = hub.Module(spec)\n      output = embedding_module(tf.constant([1, 2], dtype=tf.int64))\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.tables_initializer())\n        self.assertAllEqual(list(sess.run(output)), [b""lake"", b""palmer""])\n        embedding_module.export(export_path, sess)\n\n    asset_file = os.path.join(*[export_path, ""assets"", ""tokens.txt""])\n    # Check that asset file got written to the expected place:\n    self.assertTrue(tf_v1.gfile.Exists(asset_file))\n\n    # Assets should be hermetic, so we can delete the original vocab file:\n    tf_v1.gfile.Remove(vocabulary_file)\n\n    with tf.Graph().as_default():\n      spec = load_module_spec(export_path)\n      embedding_module = hub.Module(spec)\n      output = embedding_module(tf.constant([1, 2], dtype=tf.int64))\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.tables_initializer())\n        # Check functionality:\n        self.assertAllEqual(list(sess.run(output)), [b""lake"", b""palmer""])\n        # Check that the ASSET_FILEPATHS collection was restored properly:\n        asset_filepaths = [\n            sess.run(tensor)\n            for tensor in tf_v1.get_collection(tf_v1.GraphKeys.ASSET_FILEPATHS)\n        ]\n        # ASSET_FILEPATHS are added for the state graph and for the apply graph:\n        self.assertAllEqual(asset_filepaths,\n                            [tf.compat.as_bytes(asset_file)] * 2)\n\n  def testDuplicateAssetCopy(self):\n    export_path = os.path.join(self.get_temp_dir(), ""assets-module"")\n\n    def module_with_duplicate_asset():\n      vocabulary_file = self.create_vocab_file(""tokens2.txt"", [""1"", ""2"", ""3""])\n      indices1 = tf_v1.placeholder(dtype=tf.int64, name=""indices1"")\n      indices2 = tf_v1.placeholder(dtype=tf.int64, name=""indices2"")\n      hub.add_signature(\n          inputs={\n              ""indices_1"": indices1,\n              ""indices_2"": indices2,\n          },\n          outputs={\n              ""x"": do_table_lookup(indices1, vocabulary_file),\n              ""y"": do_table_lookup(indices2, vocabulary_file),\n          })\n\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(module_with_duplicate_asset)\n      module_a = hub.Module(spec)\n      module_a({""indices_1"": tf.constant([1, 2], dtype=tf.int64),\n                ""indices_2"": tf.constant([1, 2], dtype=tf.int64)}, as_dict=True)\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.tables_initializer())\n        module_a.export(export_path, sess)\n\n  def testExportedConsumerModelWorksIfItUsesHubModuleWithAssets(self):\n    # 1. Create and export a module with assets.\n    module_export_path = os.path.join(self.get_temp_dir(), ""small-module"")\n    vocabulary_file = self.create_vocab_file(""tokens.txt"",\n                                             [""emerson"", ""lake"", ""palmer""])\n    assets_module_fn = create_assets_module_fn(vocabulary_file)\n    spec = hub.create_module_spec(assets_module_fn)\n    with tf.Graph().as_default():\n      small_module = hub.Module(spec)\n      with tf_v1.Session() as sess:\n        small_module.export(module_export_path, sess)\n    # 2. Remove the original vocab file and move the module to another location.\n    tf_v1.gfile.Remove(vocabulary_file)\n    inner_module_path = os.path.join(self.get_temp_dir(), ""inner-module"")\n    tf_v1.gfile.Rename(module_export_path, inner_module_path)\n    del module_export_path\n    # 3. Use the module in a consumer model (which is another module here).\n    module_export_path = os.path.join(self.get_temp_dir(), ""consumer-module"")\n    consumer_module_fn = create_consumer_module_fn(inner_module_path)\n    spec = hub.create_module_spec(consumer_module_fn)\n    with tf.Graph().as_default():\n      consumer_module = hub.Module(spec)\n      with tf_v1.Session() as sess:\n        consumer_module.export(module_export_path, sess)\n    # 4. Delete the inner module on disk and move the consumer model to a final\n    # location for serving.\n    tf_v1.gfile.DeleteRecursively(inner_module_path)\n    module_serving_path = os.path.join(self.get_temp_dir(), ""serving-module"")\n    tf_v1.gfile.Rename(module_export_path, module_serving_path)\n    # 5. Make sure the model can be served successfully.\n    with tf.Graph().as_default():\n      serving_module = hub.Module(module_serving_path)\n      output = serving_module(tf.constant([1, 2], dtype=tf.int64))\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.tables_initializer())\n        self.assertAllEqual(list(sess.run(output)), [b""lake"", b""palmer""])\n\n\ndef another_stateful_module_fn():\n  """"""Stateful module with inputs.""""""\n  module_input = tf_v1.placeholder(dtype=tf.float32)\n  variable = tf.Variable([3.0], name=""iamtheoneandonly"")\n  hub.add_signature(inputs=module_input, outputs=module_input*variable)\n\n\nclass TFHubApplyStatefulModuleMultipleTimesTest(tf.test.TestCase):\n\n  def testApplyStatefulModuleMultipleTimes(self):\n    export_path = os.path.join(self.get_temp_dir(), ""another-module"")\n\n    with tf_v1.Session() as sess:\n      spec = hub.create_module_spec(another_stateful_module_fn)\n      stateful_module = hub.Module(spec, trainable=True)\n      times2 = stateful_module(tf.constant([2.0]))\n      times3 = stateful_module(tf.constant([3.0]))\n      step = tf.Variable(0, trainable=False, name=""global_step"")\n      # Training will adapt the hidden variable to be approximately 2:\n      train = tf_v1.train.GradientDescentOptimizer(0.05).minimize(\n          loss=tf_v1.losses.mean_squared_error(times2, [4.0]),\n          global_step=step)\n\n      sess.run(tf_v1.global_variables_initializer())\n      for _ in range(50):\n        sess.run(train)\n      self.assertAllClose(sess.run(times2), [4.0])\n      self.assertAllClose(sess.run(times3), [6.0])\n      stateful_module.export(export_path, sess)\n    with tf_v1.Session() as sess:\n      stateful_module = hub.Module(export_path)\n      times4 = stateful_module(tf.constant([4.0]))\n      times5 = stateful_module(tf.constant([5.0]))\n      sess.run(tf_v1.global_variables_initializer())\n      self.assertAllClose(sess.run(times4), [8.0])\n      self.assertAllClose(sess.run(times5), [10.0])\n\n  def testMultipleApplicationsInDifferentScopes(self):\n    with tf.Graph().as_default():\n      export_path = os.path.join(self.get_temp_dir(), ""module-applied-in-scope"")\n\n      spec = hub.create_module_spec(another_stateful_module_fn)\n      stateful_module = hub.Module(spec, name=""moduleA"")\n      with tf.name_scope(""foo""):\n        with tf_v1.variable_scope(""bar""):\n          times2 = stateful_module(tf.constant([2.0]))\n      with tf.name_scope(""baz""):\n        times3 = stateful_module(tf.constant([3.0]))\n\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(times2), [6.0])\n        self.assertAllClose(sess.run(times3), [9.0])\n        self.assertEqual(len(stateful_module.variable_map), 1)\n        self.assertEqual(\n            stateful_module.variable_map[""iamtheoneandonly""].name,\n            ""moduleA/iamtheoneandonly:0"")\n        stateful_module.export(export_path, sess)\n\n      # Check minimal functionality of the exported module.\n    with tf.Graph().as_default():\n      stateful_module = hub.Module(export_path, name=""moduleB"")\n      times2 = stateful_module(tf.constant([2.0]))\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        self.assertAllClose(sess.run(times2), [6.0])\n\n\ndef multiple_signature_module_fn():\n  """"""Stateful module with multiple signatures.""""""\n  weight = tf.Variable([3.0])\n\n  x_input = tf_v1.placeholder(dtype=tf.float32)\n  x_output = tf.multiply(x_input, weight)\n  hub.add_signature(""mul"", inputs=x_input, outputs=x_output)\n\n  y_input = tf_v1.placeholder(dtype=tf.float32)\n  y_output = tf.divide(y_input, weight)\n  hub.add_signature(""div"", inputs=y_input, outputs=y_output)\n\n\nclass TFHubModuleWithMultipleSignatures(tf.test.TestCase):\n\n  def testGetSignatures(self):\n    spec = hub.create_module_spec(multiple_signature_module_fn)\n    self.assertEqual(sorted(spec.get_signature_names()), [""div"", ""mul""])\n\n  def testModuleWithMultipleSignatures(self):\n    with tf.Graph().as_default():\n      spec = hub.create_module_spec(multiple_signature_module_fn)\n      module_a = hub.Module(spec, name=""moduleA"")\n      in_tensor = tf_v1.placeholder(dtype=tf.float32)\n      out_tensor_a = module_a(in_tensor, signature=""mul"")\n      out_tensor_b = module_a(out_tensor_a, signature=""div"")\n\n      with tf_v1.Session() as sess:\n        sess.run(tf_v1.global_variables_initializer())\n        in_values = [6, 3, 1]\n        self.assertAllClose(\n            sess.run(out_tensor_b, feed_dict={in_tensor: in_values}), in_values)\n\n\ndef cond_module_fn():\n  """"""Computes relu(x) with a conditional.""""""\n  x = tf_v1.placeholder(dtype=tf.float32, name=""x"", shape=[])\n  result = tf.cond(0 < x, lambda: tf.identity(x), lambda: tf.constant(0.0))\n  hub.add_signature(inputs=x, outputs=result)\n\n\ndef nested_cond_module_fn():\n  """"""Computes relu(x) with nested conditionals.""""""\n  x = tf_v1.placeholder(dtype=tf.float32, name=""x"", shape=[])\n  # pylint: disable=g-long-lambda\n  result = tf.cond(\n      0 < x,\n      lambda: tf.cond(3 < x,\n                      lambda: tf.identity(x),\n                      lambda: tf.multiply(x, 1.0)),\n      lambda: tf.cond(x < -3,\n                      lambda: tf.constant(0.0),\n                      lambda: tf.multiply(0.0, 1.0)))\n  # pylint: enable=g-long-lambda\n  hub.add_signature(inputs=x, outputs=result)\n\n\ndef while_module_fn():\n  """"""Compute x^n with while_loop.""""""\n  x = tf_v1.placeholder(dtype=tf.float32, name=""x"", shape=[])\n  n = tf_v1.placeholder(dtype=tf.int32, name=""n"")\n  _, pow_x = tf.while_loop(\n      lambda i, ix: i < n, lambda i, ix: [tf.add(i, 1), ix * x],\n      [tf.constant(0), tf.constant(1.0)])\n  hub.add_signature(inputs={""x"": x, ""n"": n}, outputs=pow_x)\n\n\ndef nested_control_flow_module_fn():\n  """"""Compute the sum of elements greater than \'a\' with nested control flow.""""""\n  elems = tf_v1.placeholder(\n      dtype=tf.float32, name=""elems"", shape=[None])\n  a = tf_v1.placeholder(dtype=tf.float32, name=""a"")\n\n  def sum_above_a(acc, x):\n    return acc + tf.cond(x > a, lambda: x, lambda: 0.0)\n\n  hub.add_signature(\n      inputs={""elems"": elems, ""a"": a},\n      outputs=tf.foldl(sum_above_a, elems, initializer=tf.constant(0.0)))\n\n\nclass TFHubModulesWithControlFlow(tf.test.TestCase):\n\n  def _testCondModule(self):\n    self._testReluModule(cond_module_fn)\n\n  def testCondModule(self):\n    self._testCondModule()\n\n  @test_util.enable_control_flow_v2\n  def testCondModuleWithControlFlowV2(self):\n    self._testCondModule()\n\n  def _testModuleWithNestedConds(self):\n    self._testReluModule(nested_cond_module_fn)\n\n  def testModuleWithNestedConds(self):\n    self._testModuleWithNestedConds()\n\n  @test_util.enable_control_flow_v2\n  def testModuleWithNestedCondsWithControlFlowV2(self):\n    self._testModuleWithNestedConds()\n\n  def _testReluModule(self, module_fn):\n    spec = hub.create_module_spec(module_fn)\n    with tf.Graph().as_default():\n      with tf_v1.Session() as sess:\n        x = tf_v1.placeholder(dtype=tf.float32, name=""x"")\n        relu_module = hub.Module(spec)\n        y = relu_module(x)\n        grad = tf.gradients([y], [x])\n        self.assertAllClose(sess.run(y, {x: 9.1}), 9.1)\n        self.assertAllClose(sess.run(y, {x: -2.4}), 0.0)\n        self.assertAllClose(sess.run(grad, {x: 2}), [1.0])\n        self.assertAllClose(sess.run(grad, {x: -2}), [0.0])\n\n  def _testWhileModule(self):\n    spec = hub.create_module_spec(while_module_fn)\n    with tf.Graph().as_default():\n      with tf_v1.Session() as sess:\n        x = tf_v1.placeholder(tf.float32)\n        n = tf_v1.placeholder(tf.int32)\n        pow_module = hub.Module(spec)\n        y = pow_module({""x"": x, ""n"": n})\n        grad = tf.gradients([y], [x])\n        self.assertAllClose(sess.run(y, {x: 9.1, n: 1}), 9.1)\n        self.assertAllClose(sess.run(y, {x: 2.4, n: 2}), 5.76)\n        self.assertAllClose(sess.run(grad, {x: 2, n: 3}), [12.0])\n\n  def testWhileModule(self):\n    self._testWhileModule()\n\n  @test_util.enable_control_flow_v2\n  def testWhileModuleWithControlFlowV2(self):\n    self._testWhileModule()\n\n  @test_util.run_v1_only(""b/138681007"")\n  def testUseModuleWithWhileLoopInsideCond(self):\n    spec = hub.create_module_spec(while_module_fn)\n    with tf.Graph().as_default():\n      m = hub.Module(spec)\n      cond = tf.cond(\n          tf.equal(tf.constant(0), tf.constant(0)),\n          lambda: m({""x"": tf.constant(3.0), ""n"": tf.constant(2)}),\n          lambda: tf.constant(4.0))\n      with tf_v1.Session() as sess:\n        self.assertEqual(sess.run(cond), 9.0)\n\n  def _testNestedControlFlowModule(self):\n    spec = hub.create_module_spec(nested_control_flow_module_fn)\n    with tf.Graph().as_default():\n      with tf_v1.Session() as sess:\n        elems = tf_v1.placeholder(tf.float32, shape=[None])\n        a = tf_v1.placeholder(tf.float32)\n        m = hub.Module(spec)\n        out = m({""elems"": elems, ""a"": a})\n        grad = tf.gradients([out], [elems])\n        self.assertAllClose(\n            sess.run(out, {\n                a: 1.1,\n                elems: [10, 0, 0.5, 1.2]\n            }), 11.2)\n\n        self.assertAllClose(sess.run(grad, {a: 1, elems: [10, 0, 0.5, 1.2]}),\n                            [[1.0, 0.0, 0.0, 1.0]])\n\n  def testNestedControlFlowModule(self):\n    self._testNestedControlFlowModule()\n\n  @test_util.enable_control_flow_v2\n  def testNestedControlFlowModuleWithControlFlowV2(self):\n    self._testNestedControlFlowModule()\n\n\ndef attached_messages_module_fn(tagged=0):\n  x = tf_v1.placeholder(tf.float32, shape=[None])\n  hub.add_signature(inputs={""x"": x}, outputs={""y"": 2*x})\n  # For brevity, this test borrows two well-known, stable message types\n  # from TensorFlow. They are not likely choices for actual uses.\n  hub.attach_message(""numbers"",\n                     tf_v1.train.Int64List(value=[-3]))  # Overwritten.\n  hub.attach_message(""numbers"", tf_v1.train.Int64List(value=[42, 69]))\n  hub.attach_message(""letters"", tf_v1.train.BytesList(value=[\n      tf.compat.as_bytes(""abc""), tf.compat.as_bytes(""xyz"")]))\n  hub.attach_message(""tagged"", tf_v1.train.Int64List(value=[tagged]))\n\n\nclass TFHubModuleWithAttachedMessages(tf.test.TestCase):\n\n  def testModuleSpec(self):\n    """"""This is the general test for ModuleSpec and native_module._ModuleSpec.""""""\n    spec = hub.create_module_spec(attached_messages_module_fn)\n    attached_letters = spec.get_attached_message(""letters"",\n                                                 tf_v1.train.BytesList)\n    self.assertSequenceEqual(\n        attached_letters.value,\n        [tf.compat.as_bytes(""abc""),\n         tf.compat.as_bytes(""xyz"")])\n    attached_numbers = spec.get_attached_message(""numbers"",\n                                                 tf_v1.train.Int64List)\n    self.assertSequenceEqual(attached_numbers.value, [42, 69])\n    attached_train = spec.get_attached_message(""tagged"", tf_v1.train.Int64List)\n    self.assertSequenceEqual(attached_train.value, [0])\n    self.assertIsNone(spec.get_attached_message(""bad"", tf_v1.train.BytesList))\n    with self.assertRaises(KeyError):\n      spec.get_attached_message(""bad"", tf_v1.train.BytesList, required=True)\n\n  def testModule(self):\n    """"""Tests forwarding from Module to ModuleSpec.""""""\n    spec = hub.create_module_spec(attached_messages_module_fn)\n    with tf.Graph().as_default():\n      module = hub.Module(spec)\n      attached = module.get_attached_message(""numbers"", tf_v1.train.Int64List)\n      self.assertSequenceEqual(attached.value, [42, 69])\n\n  def testGraphVersions(self):\n    """"""Tests native_module._ModuleSpec for explicit tags arguments.""""""\n    tags_and_args = [(set(), {""tagged"": 1}),\n                     ({""double"", ""the"", ""value""}, {""tagged"": 2})]\n    spec = hub.create_module_spec(attached_messages_module_fn,\n                                  tags_and_args=tags_and_args)\n    for tags, args in tags_and_args:\n      attached_to_spec = spec.get_attached_message(\n          ""tagged"", tf_v1.train.Int64List, tags=tags)\n      self.assertSequenceEqual(attached_to_spec.value, [args[""tagged""]])\n      with tf.Graph().as_default():\n        module = hub.Module(spec, tags=tags)\n        attached_to_module = module.get_attached_message(\n            ""tagged"", tf_v1.train.Int64List)\n        self.assertSequenceEqual(attached_to_module.value, [args[""tagged""]])\n\n  def testSeparateCopies(self):\n    """"""Mutating returned objects does not affect future returned values.""""""\n    spec = hub.create_module_spec(attached_messages_module_fn)\n    attached_numbers = spec.get_attached_message(""numbers"",\n                                                 tf_v1.train.Int64List)\n    self.assertSequenceEqual(attached_numbers.value, [42, 69])\n    attached_numbers.Clear()\n    self.assertSequenceEqual(attached_numbers.value, [])\n    attached_numbers = spec.get_attached_message(""numbers"",\n                                                 tf_v1.train.Int64List)\n    self.assertSequenceEqual(attached_numbers.value, [42, 69])\n\n\nclass TFHubOpsTest(tf.test.TestCase):\n\n  def testRegisterLinkedOpsError(self):\n    with self.assertRaisesRegexp(tf.errors.NotFoundError, ""non-existent-op""):\n      native_module.register_ops_if_needed({""non-existent-op""})\n\n\nclass TFHubExportSpecTest(tf.test.TestCase):\n\n  def f(self, x, dim=10):\n    return tf_v1.layers.dense(x, dim)\n\n  def module_fn(self, dim=10):\n    x = tf_v1.placeholder(dtype=tf.float32, shape=[None, dim])\n    y = self.f(x, dim=dim)\n    hub.add_signature(inputs=x, outputs=y)\n\n  def createCheckpoint(self, scope=None):\n    checkpoint_path = os.path.join(self.get_temp_dir(), ""model"")\n    with tf.Graph().as_default():\n      x = tf_v1.get_variable(\n          ""x"", [32, 10], initializer=tf_v1.initializers.random_normal())\n      if scope:\n        with tf_v1.variable_scope(scope):\n          y = self.f(x)\n      else:\n        y = self.f(x)\n      tf_v1.layers.dense(y, 20)\n\n      saver = tf_v1.train.Saver()\n      init_op = tf_v1.initializers.global_variables()\n\n      with tf_v1.Session() as session:\n        session.run(init_op)\n        saver.save(session, checkpoint_path)\n\n    return checkpoint_path\n\n  def testExportModuleSpec(self):\n    checkpoint_path = self.createCheckpoint()\n    export_path = os.path.join(self.get_temp_dir(), ""module1"")\n\n    spec = hub.create_module_spec(self.module_fn)\n    spec.export(export_path,\n                checkpoint_path=checkpoint_path)\n\n  def testExportModuleSpec_withWrongShape(self):\n    checkpoint_path = self.createCheckpoint(scope=""block"")\n    export_path = os.path.join(self.get_temp_dir(), ""module2"")\n\n    spec = hub.create_module_spec(lambda: self.module_fn(dim=20))\n    with self.assertRaisesRegexp(ValueError, ""doesn\'t match with shape of""):\n      spec.export(export_path,\n                  checkpoint_path=checkpoint_path,\n                  name_transform_fn=lambda x: ""block/"" + x)\n\n  def testExportModuleSpec_withWrongScope(self):\n    checkpoint_path = self.createCheckpoint(""block2"")\n    export_path = os.path.join(self.get_temp_dir(), ""module3"")\n\n    spec = hub.create_module_spec(self.module_fn)\n    with self.assertRaisesRegexp(ValueError, ""bias is not found in""):\n      spec.export(export_path,\n                  checkpoint_path=checkpoint_path,\n                  name_transform_fn=lambda x: ""block/"" + x)\n\n\nclass TFHubUsageWithEager(tf.test.TestCase):\n\n  def testWrapFunction(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager."")\n\n    spec = hub.create_module_spec(stateful_rv_with_input_module_fn)\n\n    initializers = []\n    def use_module(x, y):\n      m = hub.Module(spec, name=""module_"", trainable=True)\n      initializers.append(tf_v1.initializers.global_variables())\n      return [m(x), m(y)]\n\n    input_signature = [\n        tf.TensorSpec((), tf.float32),\n        tf.TensorSpec((), tf.float32),\n    ]\n\n    f = tf_v1.wrap_function(use_module, input_signature)\n    f.prune([], initializers)()\n    self.assertAllEqual(\n        [x.numpy() for x in f(9.0, 6.0)],\n        [19.0, 16.0])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/registry.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Internal. Registry holds python objects that can be injected.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass MultiImplRegister(object):\n  """"""Utility class to inject multiple implementations of methods.\n\n  An implementation must implement __call__ and is_supported with the same\n  set of arguments. The registered implementations ""is_supported"" methods are\n  called in reverse order under which they are registered. The first to return\n  true is then invoked via __call__ and the result returned.\n  """"""\n\n  def __init__(self, name):\n    self._name = name\n    self._impls = []\n\n  def add_implementation(self, impl):\n    """"""Register an implementation.""""""\n    self._impls += [impl]\n\n  def __call__(self, *args, **kwargs):\n    for impl in reversed(self._impls):\n      if impl.is_supported(*args, **kwargs):\n        return impl(*args, **kwargs)\n    raise RuntimeError(\n        ""Missing implementation that supports: %s(*%r, **%r)"" % (\n            self._name, args, kwargs))\n\n\nresolver = MultiImplRegister(""resolver"")\nloader = MultiImplRegister(""loader"")\n'"
tensorflow_hub/registry_test.py,2,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.registry.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow_hub import registry\n\n\nclass TestImpl(object):\n\n  def __init__(self, is_supported, execute):\n    self._is_supported = is_supported\n    self._execute = execute\n\n  def is_supported(self, *args, **kwargs):\n    return self._is_supported(*args, **kwargs)\n\n  def __call__(self, *args, **kwargs):\n    return self._execute(*args, **kwargs)\n\n\nclass RegistryTest(tf.test.TestCase):\n\n  def testResolveInReverseOrder(self):\n    def fail(_):\n      raise AssertionError(""should not be called"")\n\n    r = registry.MultiImplRegister(""test"")\n    r.add_implementation(TestImpl(lambda _: True, lambda _: 0))\n    r.add_implementation(TestImpl(lambda x: x == 1, lambda _: 100))\n    r.add_implementation(TestImpl(lambda x: x == 2, fail))\n    r.add_implementation(TestImpl(lambda x: x == 2, lambda _: 200))\n\n    self.assertEqual(r(0), 0)\n    self.assertEqual(r(1), 100)\n    self.assertEqual(r(2), 200)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/resolver.py,17,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Interface and common utility methods to perform module address resolution.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport datetime\nimport os\nimport socket\nimport sys\nimport tarfile\nimport tempfile\nimport time\nimport uuid\n\nfrom absl import flags\nfrom absl import logging\nimport tensorflow as tf\nfrom tensorflow_hub import tf_utils\nfrom tensorflow_hub import tf_v1\n\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    ""tfhub_cache_dir"",\n    None,\n    ""If set, TF-Hub will download and cache Modules into this directory. ""\n    ""Otherwise it will attempt to find a network path."")\n\n_TFHUB_CACHE_DIR = ""TFHUB_CACHE_DIR""\n_TFHUB_DOWNLOAD_PROGRESS = ""TFHUB_DOWNLOAD_PROGRESS""\n\n\ndef tfhub_cache_dir(default_cache_dir=None, use_temp=False):\n  """"""Returns cache directory.\n\n  Returns cache directory from either TFHUB_CACHE_DIR environment variable\n  or --tfhub_cache_dir or default, if set.\n\n  Args:\n    default_cache_dir: Default cache location to use if neither TFHUB_CACHE_DIR\n                       environment variable nor --tfhub_cache_dir are\n                       not specified.\n    use_temp: bool, Optional to enable using system\'s temp directory as a\n              module cache directory if neither default_cache_dir nor\n              --tfhub_cache_dir nor TFHUB_CACHE_DIR environment variable are\n              specified .\n  """"""\n\n  # Note: We are using FLAGS[""tfhub_cache_dir""] (and not FLAGS.tfhub_cache_dir)\n  # to access the flag value in order to avoid parsing argv list. The flags\n  # should have been parsed by now in main() by tf.app.run(). If that was not\n  # the case (say in Colab env) we skip flag parsing because argv may contain\n  # unknown flags.\n  cache_dir = (\n      os.getenv(_TFHUB_CACHE_DIR, """") or FLAGS[""tfhub_cache_dir""].value or\n      default_cache_dir)\n  if not cache_dir and use_temp:\n    # Place all TF-Hub modules under <system\'s temp>/tfhub_modules.\n    cache_dir = os.path.join(tempfile.gettempdir(), ""tfhub_modules"")\n  if cache_dir:\n    logging.log_first_n(logging.INFO, ""Using %s to cache modules."", 1,\n                        cache_dir)\n  return cache_dir\n\n\ndef create_local_module_dir(cache_dir, module_name):\n  """"""Creates and returns the name of directory where to cache a module.""""""\n  tf_v1.gfile.MakeDirs(cache_dir)\n  return os.path.join(cache_dir, module_name)\n\n\nclass DownloadManager(object):\n  """"""Helper class responsible for TF-Hub module download and extraction.""""""\n\n  def __init__(self, url):\n    """"""Creates DownloadManager responsible for downloading a TF-Hub module.\n\n    Args:\n       url: URL pointing to the TF-Hub module to download and extract.\n    """"""\n    self._url = url\n    self._last_progress_msg_print_time = time.time()\n    self._total_bytes_downloaded = 0\n    self._max_prog_str = 0\n\n  def _print_download_progress_msg(self, msg, flush=False):\n    """"""Prints a message about download progress either to the console or TF log.\n\n    Args:\n      msg: Message to print.\n      flush: Indicates whether to flush the output (only used in interactive\n             mode).\n    """"""\n    if self._interactive_mode():\n      # Print progress message to console overwriting previous progress\n      # message.\n      self._max_prog_str = max(self._max_prog_str, len(msg))\n      sys.stdout.write(""\\r%-{}s"".format(self._max_prog_str) % msg)\n      sys.stdout.flush()\n      if flush:\n        print(""\\n"")\n    else:\n      # Interactive progress tracking is disabled. Print progress to the\n      # standard TF log.\n      logging.info(msg)\n\n  def _log_progress(self, bytes_downloaded):\n    """"""Logs progress information about ongoing module download.\n\n    Args:\n      bytes_downloaded: Number of bytes downloaded.\n    """"""\n    self._total_bytes_downloaded += bytes_downloaded\n    now = time.time()\n    if (self._interactive_mode() or\n        now - self._last_progress_msg_print_time > 15):\n      # Print progress message every 15 secs or if interactive progress\n      # tracking is enabled.\n      self._print_download_progress_msg(\n          ""Downloading %s: %s"" % (self._url,\n                                  tf_utils.bytes_to_readable_str(\n                                      self._total_bytes_downloaded, True)))\n      self._last_progress_msg_print_time = now\n\n  def _interactive_mode(self):\n    """"""Returns true if interactive logging is enabled.""""""\n    return os.getenv(_TFHUB_DOWNLOAD_PROGRESS, """")\n\n  def _extract_file(self, tgz, tarinfo, dst_path, buffer_size=10<<20):\n    """"""Extracts \'tarinfo\' from \'tgz\' and writes to \'dst_path\'.""""""\n    src = tgz.extractfile(tarinfo)\n    dst = tf_v1.gfile.GFile(dst_path, ""wb"")\n    while 1:\n      buf = src.read(buffer_size)\n      if not buf:\n        break\n      dst.write(buf)\n      self._log_progress(len(buf))\n    dst.close()\n    src.close()\n\n  def download_and_uncompress(self, fileobj, dst_path):\n    """"""Streams the content for the \'fileobj\' and stores the result in dst_path.\n\n    Args:\n      fileobj: File handle pointing to .tar/.tar.gz content.\n      dst_path: Absolute path where to store uncompressed data from \'fileobj\'.\n\n    Raises:\n      ValueError: Unknown object encountered inside the TAR file.\n    """"""\n    try:\n      with tarfile.open(mode=""r|*"", fileobj=fileobj) as tgz:\n        for tarinfo in tgz:\n          abs_target_path = _merge_relative_path(dst_path, tarinfo.name)\n\n          if tarinfo.isfile():\n            self._extract_file(tgz, tarinfo, abs_target_path)\n          elif tarinfo.isdir():\n            tf_v1.gfile.MakeDirs(abs_target_path)\n          else:\n            # We do not support symlinks and other uncommon objects.\n            raise ValueError(\n                ""Unexpected object type in tar archive: %s"" % tarinfo.type)\n\n        total_size_str = tf_utils.bytes_to_readable_str(\n            self._total_bytes_downloaded, True)\n        self._print_download_progress_msg(\n            ""Downloaded %s, Total size: %s"" % (self._url, total_size_str),\n            flush=True)\n    except tarfile.ReadError:\n      raise IOError(""%s does not appear to be a valid module."" % self._url)\n\n\ndef _merge_relative_path(dst_path, rel_path):\n  """"""Merge a relative tar file to a destination (which can be ""gs://..."").""""""\n  # Convert rel_path to be relative and normalize it to remove ""."", "".."", ""//"",\n  # which are valid directories in fileystems like ""gs://"".\n  norm_rel_path = os.path.normpath(rel_path.lstrip(""/""))\n\n  if norm_rel_path == ""."":\n    return dst_path\n\n  # Check that the norm rel path does not starts with "".."".\n  if norm_rel_path.startswith(""..""):\n    raise ValueError(""Relative path %r is invalid."" % rel_path)\n\n  merged = os.path.join(dst_path, norm_rel_path)\n\n  # After merging verify that the merged path keeps the original dst_path.\n  if not merged.startswith(dst_path):\n    raise ValueError(""Relative path %r is invalid. Failed to merge with %r."" % (\n        rel_path, dst_path))\n  return merged\n\n\ndef _module_descriptor_file(module_dir):\n  """"""Returns the name of the file containing descriptor for the \'module_dir\'.""""""\n  return ""{}.descriptor.txt"".format(module_dir)\n\n\ndef _write_module_descriptor_file(handle, module_dir):\n  """"""Writes a descriptor file about the directory containing a module.\n\n  Args:\n    handle: Module name/handle.\n    module_dir: Directory where a module was downloaded.\n  """"""\n  readme = _module_descriptor_file(module_dir)\n  readme_content = (\n      ""Module: %s\\nDownload Time: %s\\nDownloader Hostname: %s (PID:%d)"" %\n      (handle, str(datetime.datetime.today()), socket.gethostname(),\n       os.getpid()))\n  # The descriptor file has no semantic meaning so we allow \'overwrite\' since\n  # there is a chance that another process might have written the file (and\n  # crashed), we just overwrite it.\n  tf_utils.atomic_write_string_to_file(readme, readme_content, overwrite=True)\n\n\ndef _lock_file_contents(task_uid):\n  """"""Returns the content of the lock file.""""""\n  return ""%s.%d.%s"" % (socket.gethostname(), os.getpid(), task_uid)\n\n\ndef _lock_filename(module_dir):\n  """"""Returns lock file name.""""""\n  return tf_utils.absolute_path(module_dir) + "".lock""\n\n\ndef _module_dir(lock_filename):\n  """"""Returns module dir from a full \'lock_filename\' path.\n\n  Args:\n    lock_filename: Name of the lock file, ends with .lock.\n\n  Raises:\n    ValueError: if lock_filename is ill specified.\n  """"""\n  if not lock_filename.endswith("".lock""):\n    raise ValueError(\n        ""Lock file name (%s) has to end with .lock."" % lock_filename)\n  return lock_filename[0:-len("".lock"")]\n\n\ndef _task_uid_from_lock_file(lock_filename):\n  """"""Returns task UID of the task that created a given lock file.""""""\n  lock = tf_utils.read_file_to_string(lock_filename)\n  return lock.split(""."")[-1]\n\n\ndef _temp_download_dir(module_dir, task_uid):\n  """"""Returns the name of a temporary directory to download module to.""""""\n  return ""{}.{}.tmp"".format(tf_utils.absolute_path(module_dir), task_uid)\n\n\ndef _dir_size(directory):\n  """"""Returns total size (in bytes) of the given \'directory\'.""""""\n  size = 0\n  for elem in tf_v1.gfile.ListDirectory(directory):\n    elem_full_path = os.path.join(directory, elem)\n    stat = tf_v1.gfile.Stat(elem_full_path)\n    size += _dir_size(elem_full_path) if stat.is_directory else stat.length\n  return size\n\n\ndef _locked_tmp_dir_size(lock_filename):\n  """"""Returns the size of the temp dir pointed to by the given lock file.""""""\n  task_uid = _task_uid_from_lock_file(lock_filename)\n  try:\n    return _dir_size(\n        _temp_download_dir(_module_dir(lock_filename), task_uid))\n  except tf.errors.NotFoundError:\n    return 0\n\n\ndef _wait_for_lock_to_disappear(handle, lock_file, lock_file_timeout_sec):\n  """"""Waits for the lock file to disappear.\n\n  The lock file was created by another process that is performing a download\n  into its own temporary directory. The name of this temp directory is\n  sha1(<module>).<uuid>.tmp where <uuid> comes from the lock file.\n\n  Args:\n    handle: The location from where a module is being download.\n    lock_file: Lock file created by another process downloading this module.\n    lock_file_timeout_sec: The amount of time to wait (in seconds) before we\n                           can declare that the other downloaded has been\n                           abandoned. The download is declared abandoned if\n                           there is no file size change in the temporary\n                           directory within the last \'lock_file_timeout_sec\'.\n  """"""\n  locked_tmp_dir_size = 0\n  locked_tmp_dir_size_check_time = time.time()\n  lock_file_content = None\n  while tf_v1.gfile.Exists(lock_file):\n    try:\n      logging.log_every_n(\n          logging.INFO,\n          ""Module \'%s\' already being downloaded by \'%s\'. Waiting."", 10,\n          handle, tf_utils.read_file_to_string(lock_file))\n      if (time.time() - locked_tmp_dir_size_check_time >\n          lock_file_timeout_sec):\n        # Check whether the holder of the current lock downloaded anything\n        # in its temporary directory in the last \'lock_file_timeout_sec\'.\n        cur_locked_tmp_dir_size = _locked_tmp_dir_size(lock_file)\n        cur_lock_file_content = tf_utils.read_file_to_string(lock_file)\n        if (cur_locked_tmp_dir_size == locked_tmp_dir_size and\n            cur_lock_file_content == lock_file_content):\n          # There is was no data downloaded in the past\n          # \'lock_file_timeout_sec\'. Steal the lock and proceed with the\n          # local download.\n          logging.warning(""Deleting lock file %s due to inactivity."",\n                          lock_file)\n          tf_v1.gfile.Remove(lock_file)\n          break\n        locked_tmp_dir_size = cur_locked_tmp_dir_size\n        locked_tmp_dir_size_check_time = time.time()\n        lock_file_content = cur_lock_file_content\n    except tf.errors.NotFoundError:\n      # Lock file or temp directory were deleted during check. Continue\n      # to check whether download succeeded or we need to start our own\n      # download.\n      pass\n    finally:\n      time.sleep(5)\n\n\ndef atomic_download(handle,\n                    download_fn,\n                    module_dir,\n                    lock_file_timeout_sec=10 * 60):\n  """"""Returns the path to a Module directory for a given TF-Hub Module handle.\n\n  Args:\n    handle: (string) Location of a TF-Hub Module.\n    download_fn: Callback function that actually performs download. The callback\n                 receives two arguments, handle and the location of a temporary\n                 directory to download the content into.\n    module_dir: Directory where to download the module files to.\n    lock_file_timeout_sec: The amount of time we give the current holder of\n                           the lock to make progress in downloading a module.\n                           If no progress is made, the lock is revoked.\n\n  Returns:\n    A string containing the path to a TF-Hub Module directory.\n\n  Raises:\n    ValueError: if the Module is not found.\n    tf.errors.OpError: file I/O failures raise the appropriate subtype.\n  """"""\n  lock_file = _lock_filename(module_dir)\n  task_uid = uuid.uuid4().hex\n  lock_contents = _lock_file_contents(task_uid)\n  tmp_dir = _temp_download_dir(module_dir, task_uid)\n\n  # Attempt to protect against cases of processes being cancelled with\n  # KeyboardInterrupt by using a try/finally clause to remove the lock\n  # and tmp_dir.\n  try:\n    while True:\n      try:\n        tf_utils.atomic_write_string_to_file(lock_file, lock_contents,\n                                             overwrite=False)\n        # Must test condition again, since another process could have created\n        # the module and deleted the old lock file since last test.\n        if tf_v1.gfile.Exists(module_dir):\n          # Lock file will be deleted in the finally-clause.\n          return module_dir\n        break  # Proceed to downloading the module.\n      # These errors are believed to be permanent problems with the\n      # module_dir that justify failing the download.\n      except (tf.errors.NotFoundError,\n              tf.errors.PermissionDeniedError,\n              tf.errors.UnauthenticatedError,\n              tf.errors.ResourceExhaustedError,\n              tf.errors.InternalError,\n              tf.errors.InvalidArgumentError,\n              tf.errors.UnimplementedError):\n        raise\n      # All other errors are retried.\n      # TODO(b/144424849): Retrying an AlreadyExistsError from the atomic write\n      # should be good enough, but see discussion about misc filesystem types.\n      # TODO(b/144475403): How atomic is the overwrite=False check?\n      except tf.errors.OpError:\n        pass\n\n      # Wait for lock file to disappear.\n      _wait_for_lock_to_disappear(handle, lock_file, lock_file_timeout_sec)\n      # At this point we either deleted a lock or a lock got removed by the\n      # owner or another process. Perform one more iteration of the while-loop,\n      # we would either terminate due tf_v1.gfile.Exists(module_dir) or because\n      # we would obtain a lock ourselves, or wait again for the lock to\n      # disappear.\n\n    # Lock file acquired.\n    logging.info(""Downloading TF-Hub Module \'%s\'."", handle)\n    tf_v1.gfile.MakeDirs(tmp_dir)\n    download_fn(handle, tmp_dir)\n    # Write module descriptor to capture information about which module was\n    # downloaded by whom and when. The file stored at the same level as a\n    # directory in order to keep the content of the \'model_dir\' exactly as it\n    # was define by the module publisher.\n    #\n    # Note: The descriptor is written purely to help the end-user to identify\n    # which directory belongs to which module. The descriptor is not part of the\n    # module caching protocol and no code in the TF-Hub library reads its\n    # content.\n    _write_module_descriptor_file(handle, module_dir)\n    try:\n      tf_v1.gfile.Rename(tmp_dir, module_dir)\n      logging.info(""Downloaded TF-Hub Module \'%s\'."", handle)\n    except tf.errors.AlreadyExistsError:\n      logging.warning(""Module already exists in %s"", module_dir)\n\n  finally:\n    try:\n      # Temp directory is owned by the current process, remove it.\n      tf_v1.gfile.DeleteRecursively(tmp_dir)\n    except tf.errors.NotFoundError:\n      pass\n    try:\n      contents = tf_utils.read_file_to_string(lock_file)\n    except tf.errors.NotFoundError:\n      contents = """"\n    if contents == lock_contents:\n      # Lock file exists and is owned by this process.\n      try:\n        tf_v1.gfile.Remove(lock_file)\n      except tf.errors.NotFoundError:\n        pass\n\n  return module_dir\n\n\nclass UnsupportedHandleError(Exception):\n  """"""Exception class for incorrectly formatted handles.""""""\n\n\nclass Resolver(object):\n  """"""Resolver base class: all resolvers inherit from this class.""""""\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def __call__(self, handle):\n    """"""Resolves a handle into a Module path.\n\n    Args:\n      handle: (string) the Module handle to resolve.\n\n    Returns:\n      A string representing the Module path.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def is_supported(self, handle):\n    """"""Returns whether a handle is supported by this resolver.\n\n    Args:\n      handle: (string) the Module handle to resolve.\n\n    Returns:\n      True if the handle is properly formatted for this resolver.\n      Note that a True return value does not indicate that the\n      handle can be resolved, only that it is the correct format.\n    """"""\n    pass\n\n\nclass PathResolver(Resolver):\n  """"""Resolves handles which are absolute paths.""""""\n\n  def is_supported(self, handle):\n    try:\n      return tf_v1.gfile.Exists(handle)\n    except tf.errors.OpError:\n      return False\n\n  def __call__(self, handle):\n    return handle\n\n\nclass FailResolver(Resolver):\n  """"""Always fails to resolve a path.""""""\n\n  def is_supported(self, handle):\n    return True\n\n  def __call__(self, handle):\n    raise UnsupportedHandleError(\n        ""unsupported handle format \'%s\'. No resolvers found that can ""\n        ""successfully resolve it. If the handle points to the local ""\n        ""filesystem, the error indicates that the module directory does not ""\n        ""exist. Supported handle formats: URLs pointing to a TGZ  file ""\n        ""(e.g. https://address/module.tgz), or Local File System directory ""\n        ""file (e.g. /tmp/my_local_module)."" % handle)\n'"
tensorflow_hub/resolver_test.py,7,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.resolver.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\ntry:\n  import mock as mock\nexcept ImportError:\n  import unittest.mock as mock\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\n\nimport os\nimport re\nimport socket\nimport tempfile\nimport threading\nimport time\nimport uuid\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom tensorflow_hub import resolver\nfrom tensorflow_hub import tf_utils\nfrom tensorflow_hub import tf_v1\n\n\nFLAGS = flags.FLAGS\n\n\nclass PathResolverTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(PathResolverTest, self).setUp()\n    self.resolver = resolver.PathResolver()\n\n  def testHandleSupported(self):\n    os.chdir(os.path.join(self.get_temp_dir()))\n    self.assertTrue(self.resolver.is_supported(""/tmp""))\n    tf_v1.gfile.MkDir(""foo/"")\n    self.assertTrue(self.resolver.is_supported(""./foo/""))\n    self.assertTrue(self.resolver.is_supported(""foo/""))\n    # Directory doesn\'t exist.\n    self.assertFalse(self.resolver.is_supported(""bar/""))\n    self.assertFalse(self.resolver.is_supported(""foo/bar""))\n    self.assertFalse(self.resolver.is_supported(""nope://throw-OpError""))\n\n  def testGetModulePath(self):\n    tmp_path = os.path.join(self.get_temp_dir(), ""1234"")\n    tf_v1.gfile.MkDir(tmp_path)\n    path = self.resolver(tmp_path)\n    self.assertEqual(path, tmp_path)\n\n\nclass FakeResolver(resolver.Resolver):\n  """"""Fake Resolver used to test composite Resolvers.""""""\n\n  def __init__(self, prefix):\n    self.prefix = prefix\n\n  def is_supported(self, handle):\n    return handle.startswith(self.prefix)\n\n  def __call__(self, handle):\n    if handle.endswith(""error""):\n      raise ValueError(""error for: "" + handle)\n    return handle + ""-resolved_by_"" + self.prefix\n\n\nclass ResolverTest(tf.test.TestCase):\n\n  def testCacheDir(self):\n    # No cache dir set, None is returned.\n    cache_dir = resolver.tfhub_cache_dir()\n    self.assertEqual(cache_dir, None)\n    # Use temp dir.\n    cache_dir = resolver.tfhub_cache_dir(use_temp=True)\n    self.assertEquals(cache_dir,\n                      os.path.join(tempfile.gettempdir(), ""tfhub_modules""))\n    # Use override\n    cache_dir = resolver.tfhub_cache_dir(default_cache_dir=""/d"", use_temp=True)\n    self.assertEqual(""/d"", cache_dir)\n    # Use a flag\n    FLAGS.tfhub_cache_dir = ""/e""\n    cache_dir = resolver.tfhub_cache_dir(default_cache_dir=""/d"", use_temp=True)\n    self.assertEqual(""/e"", cache_dir)\n    FLAGS.tfhub_cache_dir = """"\n    # Use env variable\n    os.environ[resolver._TFHUB_CACHE_DIR] = ""/f""\n    cache_dir = resolver.tfhub_cache_dir(default_cache_dir=""/d"", use_temp=True)\n    self.assertEqual(""/f"", cache_dir)\n    FLAGS.tfhub_cache_dir = ""/e""\n    cache_dir = resolver.tfhub_cache_dir(default_cache_dir=""/d"", use_temp=True)\n    self.assertEqual(""/f"", cache_dir)\n    FLAGS.tfhub_cache_dir = """"\n    os.unsetenv(resolver._TFHUB_CACHE_DIR)\n\n  def testDirSize(self):\n    fake_task_uid = 1234\n\n    # Create a directory with some files and sub-directory and check its size.\n    test_dir = resolver._temp_download_dir(self.get_temp_dir(), fake_task_uid)\n    tf_v1.gfile.MakeDirs(test_dir)\n    tf_utils.atomic_write_string_to_file(\n        os.path.join(test_dir, ""file1""), ""content1"", False)\n    tf_utils.atomic_write_string_to_file(\n        os.path.join(test_dir, ""file2""), ""content2"", False)\n    test_sub_dir = os.path.join(test_dir, ""sub_dir"")\n    tf_v1.gfile.MakeDirs(test_sub_dir)\n    tf_utils.atomic_write_string_to_file(\n        os.path.join(test_sub_dir, ""file3""), ""content3"", False)\n    self.assertEqual(3 * 8, resolver._dir_size(test_dir))\n    self.assertEqual(8, resolver._dir_size(test_sub_dir))\n\n    # Treat the directory as a temporary directory used by a module download by\n    # referring to that directory from the lock file.\n    fake_lock_filename = resolver._lock_filename(self.get_temp_dir())\n    tf_utils.atomic_write_string_to_file(\n        fake_lock_filename, resolver._lock_file_contents(fake_task_uid), False)\n    self.assertEqual(3 * 8, resolver._locked_tmp_dir_size(fake_lock_filename))\n\n    # Check that if temp directory doesn\'t exist, 0 is returned.\n    tf_v1.gfile.DeleteRecursively(test_dir)\n    self.assertEqual(0, resolver._locked_tmp_dir_size(fake_lock_filename))\n\n  def testLockFileName(self):\n    self.assertEquals(""/a/b/c.lock"", resolver._lock_filename(""/a/b/c/""))\n\n  def testTempDownloadDir(self):\n    self.assertEquals(""/a/b.t.tmp"", resolver._temp_download_dir(""/a/b/"", ""t""))\n\n  def testReadTaskUidFromLockFile(self):\n    module_dir = os.path.join(self.get_temp_dir(), ""module"")\n    task_uid = uuid.uuid4().hex\n    lock_filename = resolver._lock_filename(module_dir)\n    tf_utils.atomic_write_string_to_file(lock_filename,\n                                         resolver._lock_file_contents(task_uid),\n                                         overwrite=False)\n    self.assertEqual(task_uid, resolver._task_uid_from_lock_file(lock_filename))\n\n  def testWaitForLockToDisappear_DownloadCompletes(self):\n    module_dir = os.path.join(self.get_temp_dir(), ""module"")\n    task_uid = uuid.uuid4().hex\n    lock_filename = resolver._lock_filename(module_dir)\n    # Write lock file\n    tf_utils.atomic_write_string_to_file(lock_filename,\n                                         resolver._lock_file_contents(task_uid),\n                                         overwrite=False)\n    # Wait for the lock file to disappear (in a separate thread)\n    thread = threading.Thread(target=resolver._wait_for_lock_to_disappear,\n                              args=(""module"", lock_filename, 600,))\n    thread.start()\n    # Delete the lock file.\n    tf_v1.gfile.Remove(lock_filename)\n    thread.join(10)\n    # The waiting terminates without errors.\n\n  def testWaitForLockToDisappear_DownloadOngoing(self):\n    module_dir = os.path.join(self.get_temp_dir(), ""module"")\n    task_uid = uuid.uuid4().hex\n    lock_filename = resolver._lock_filename(module_dir)\n    lock_file_content = resolver._lock_file_contents(task_uid)\n    tf_utils.atomic_write_string_to_file(\n        lock_filename, lock_file_content, overwrite=False)\n\n    lock_expiration_wait_time_secs = 10\n    thread = threading.Thread(\n        target=resolver._wait_for_lock_to_disappear,\n        args=(\n            ""module"",\n            lock_filename,\n            lock_expiration_wait_time_secs,\n        ))\n    thread.start()\n    # Simulate download by writing a file every 1 sec. While writes are happing\n    # the lock file remains in place.\n    tmp_dir = resolver._temp_download_dir(self.get_temp_dir(), task_uid)\n    tf_v1.gfile.MakeDirs(tmp_dir)\n    for x in range(2 * lock_expiration_wait_time_secs):\n      tf_utils.atomic_write_string_to_file(\n          os.path.join(tmp_dir, ""file_%d"" % x), ""test"", overwrite=False)\n      # While writes are happening the original lock file is in place.\n      self.assertEqual(lock_file_content,\n                       tf_utils.read_file_to_string(lock_filename))\n      time.sleep(1)\n    thread.join(lock_expiration_wait_time_secs)\n    # The waiting terminates without errors.\n\n  def testWaitForLockToDisappear_DownloadAborted(self):\n    module_dir = os.path.join(self.get_temp_dir(), ""module"")\n    task_uid = uuid.uuid4().hex\n    lock_filename = resolver._lock_filename(module_dir)\n    lock_file_content = resolver._lock_file_contents(task_uid)\n    tf_utils.atomic_write_string_to_file(\n        lock_filename, lock_file_content, overwrite=False)\n    tmp_dir = resolver._temp_download_dir(self.get_temp_dir(), task_uid)\n    tf_v1.gfile.MakeDirs(tmp_dir)\n\n    thread = threading.Thread(target=resolver._wait_for_lock_to_disappear,\n                              args=(""module"", lock_filename, 10,))\n    thread.start()\n    thread.join(30)\n    # Because nobody was writing to tmp_dir, the lock file got reclaimed by\n    # resolver._wait_for_lock_to_disappear.\n    self.assertFalse(tf_v1.gfile.Exists(lock_filename))\n\n  def testModuleAlreadyDownloaded(self):\n    # Simulate the case when a rogue process finishes downloading a module\n    # right before the current process can perform a rename of a temp directory\n    # to a permanent module directory.\n    module_dir = os.path.join(self.get_temp_dir(), ""module"")\n    def fake_download_fn_with_rogue_behavior(handle, tmp_dir):\n      del handle, tmp_dir\n      # Create module directory\n      tf_v1.gfile.MakeDirs(module_dir)\n      tf_utils.atomic_write_string_to_file(\n          os.path.join(module_dir, ""file""), ""content"", False)\n\n    self.assertEqual(\n        module_dir,\n        resolver.atomic_download(""module"", fake_download_fn_with_rogue_behavior,\n                                 module_dir))\n    self.assertEqual(tf_v1.gfile.ListDirectory(module_dir), [""file""])\n    self.assertFalse(tf_v1.gfile.Exists(resolver._lock_filename(module_dir)))\n    parent_dir = os.path.abspath(os.path.join(module_dir, ""..""))\n    self.assertEqual(\n        sorted(tf_v1.gfile.ListDirectory(parent_dir)),\n        [""module"", ""module.descriptor.txt""])\n    self.assertRegexpMatches(\n        tf_utils.read_file_to_string(\n            resolver._module_descriptor_file(module_dir)),\n        ""Module: module\\n""\n        ""Download Time: .*\\n""\n        ""Downloader Hostname: %s .PID:%d."" % (re.escape(socket.gethostname()),\n                                              os.getpid()))\n\n  def testModuleConcurrentDownload(self):\n    module_dir = os.path.join(self.get_temp_dir(), ""module"")\n\n    # To simulate one downloading starting while the other is still in progress,\n    # call resolver.atomic_download() from download_fn(). The second download\n    # is set up with download_fn() that fails. That download_fn() is not\n    # expected to be called.\n    def second_download_fn(handle, tmp_dir):\n      del handle, tmp_dir\n      self.fail(""This should not be called. The module should have been ""\n                ""downloaded already."")\n\n    second_download_thread = threading.Thread(\n        target=resolver.atomic_download,\n        args=(\n            ""module"",\n            second_download_fn,\n            module_dir,\n        ))\n\n    def first_download_fn(handle, tmp_dir):\n      del handle, tmp_dir\n      tf_v1.gfile.MakeDirs(module_dir)\n      tf_utils.atomic_write_string_to_file(\n          os.path.join(module_dir, ""file""), ""content"", False)\n      second_download_thread.start()\n\n    self.assertEqual(module_dir,\n                     resolver.atomic_download(""module"", first_download_fn,\n                                              module_dir))\n    second_download_thread.join(30)\n    # The waiting terminates without errors.\n\n  def testModuleDownloadPermissionDenied(self):\n    readonly_dir = os.path.join(self.get_temp_dir(), ""readonly"")\n    os.mkdir(readonly_dir, 0o500)\n    module_dir = os.path.join(readonly_dir, ""module"")\n\n    def unused_download_fn(handle, tmp_dir):\n      del handle, tmp_dir\n      self.fail(""This should not be called. Already writing the lockfile ""\n                ""is expected to raise an error."")\n\n    with self.assertRaises(tf.errors.PermissionDeniedError):\n      resolver.atomic_download(""module"", unused_download_fn, module_dir)\n\n  def testModuleLockLostDownloadKilled(self):\n    module_dir = os.path.join(self.get_temp_dir(), ""module"")\n    download_aborted_msg = ""Download aborted.""\n    def kill_download(handle, tmp_dir):\n      del handle, tmp_dir\n      # Simulate lock loss by removing the lock.\n      tf_v1.gfile.Remove(resolver._lock_filename(module_dir))\n      # Throw an error to simulate aborted download.\n      raise OSError(download_aborted_msg)\n\n    try:\n      resolver.atomic_download(""module"", kill_download, module_dir)\n      self.fail(""atomic_download() should have thrown an exception."")\n    except OSError as _:\n      pass\n    parent_dir = os.path.abspath(os.path.join(module_dir, ""..""))\n    # Test that all files got cleaned up.\n    self.assertEqual(tf_v1.gfile.ListDirectory(parent_dir), [])\n\n  def testMergePath(self):\n    self.assertEqual(\n        resolver._merge_relative_path(""gs://module-cache"", """"),\n        ""gs://module-cache"")\n    self.assertEqual(\n        resolver._merge_relative_path(""gs://module-cache"", ""./""),\n        ""gs://module-cache"")\n    self.assertEqual(\n        resolver._merge_relative_path(""gs://module-cache"", ""./file""),\n        ""gs://module-cache/file"")\n    self.assertEqual(\n        resolver._merge_relative_path(""gs://module-cache"", ""hello/../bla""),\n        ""gs://module-cache/bla"")\n    self.assertEqual(\n        resolver._merge_relative_path(""gs://module-cache"", ""/""),\n        ""gs://module-cache"", ""/"")\n    with self.assertRaisesRegexp(ValueError, ""is invalid""):\n      resolver._merge_relative_path(""gs://module-cache"", ""/../"")\n    with self.assertRaisesRegexp(ValueError, ""is invalid""):\n      resolver._merge_relative_path(""gs://module-cache"", ""hello/../../bla"")\n\n  def testNotFoundGCSBucket(self):\n    # When trying to use not existing GCS bucket, test that\n    # tf_util.atomic_write_string_to_file raises tf.error.NotFoundError.\n    # Other errors that may arise from bad network connectivity are ignored by\n    # resolver.atomic_download and retried infinitely.\n    module_dir = """"\n    def dummy_download_fn(handle, tmp_dir):\n      del handle, tmp_dir\n      return\n\n    # Simulate missing GCS bucket by raising NotFoundError in\n    # atomic_write_string_to_file.\n    with mock.patch(\n        ""tensorflow_hub.tf_utils.atomic_write_string_to_file"") as mock_:\n      mock_.side_effect = tf.errors.NotFoundError(None, None, ""Test"")\n      try:\n        resolver.atomic_download(""module"", dummy_download_fn, module_dir)\n        assert False\n      except tf.errors.NotFoundError as e:\n        self.assertEqual(""Test"", e.message)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/saved_model_lib.py,14,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""SavedModel lib provides a way to read and write SavedModels.\n\nThis is an internal Hub utility and not part of the public API.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport re\n\nimport tensorflow as tf\nfrom tensorflow_hub import module_attachment_pb2\nfrom tensorflow_hub import tf_utils\nfrom tensorflow_hub import tf_v1\n\nfrom google.protobuf import message\nfrom tensorflow.core.protobuf import meta_graph_pb2\nfrom tensorflow.core.protobuf import saved_model_pb2\n\n# A collection of pairs (key: string, definition : SignatureDef) used internally\n# to propagate signatures defined in a Graph to SavedModel. The collection key\n# is a tuple (not a string) in order to make it invisible from user apis such\n# as `get_all_collection_keys()` and manual exporting to meta_graphs.\n_SIGNATURE_COLLECTION = (""__saved_model_lib_signatures"",)\n\n# A collection of ModuleAttachment protos is used internally to collect\n# the (key, value) pairs passed to attach_message() calls from the module_fn.\n# As above, it gets a non-string name to make it invisible within module_fn.\n_ATTACHMENT_COLLECTION_INTERNAL = (""__hub_module_attachments"",)\n# The ModuleAttachment protos are stored in SavedModel.meta_graphs (but never\n# in tf.Graphs) as CollectionDef.bytes_list under this key.\nATTACHMENT_COLLECTION_SAVED = ""hub_module_attachments""\n\n\ndef get_variables_path(export_dir):\n  """"""Returns the path for storing variables checkpoints.""""""\n  return os.path.join(\n      tf.compat.as_bytes(export_dir),\n      tf.compat.as_bytes(tf_v1.saved_model.constants.VARIABLES_DIRECTORY),\n      tf.compat.as_bytes(tf_v1.saved_model.constants.VARIABLES_FILENAME))\n\n\ndef _get_assets_dir(export_dir):\n  return os.path.join(\n      tf.compat.as_bytes(export_dir),\n      tf.compat.as_bytes(tf_v1.saved_model.constants.ASSETS_DIRECTORY))\n\n\ndef _get_asset_filename(export_dir, asset_filename):\n  assets_dir = _get_assets_dir(export_dir)\n  filename = os.path.join(\n      tf.compat.as_bytes(assets_dir),\n      tf.compat.as_bytes(asset_filename))\n  if not tf_utils.absolute_path(filename).startswith(\n      tf_utils.absolute_path(assets_dir)):\n    raise ValueError(\n        ""Asset filename (%s) points outside assets_dir"" % asset_filename)\n  return filename\n\n\ndef _get_saved_model_proto_path(export_dir):\n  return os.path.join(\n      tf.compat.as_bytes(export_dir),\n      tf.compat.as_bytes(tf_v1.saved_model.constants.SAVED_MODEL_FILENAME_PB))\n\n\ndef _get_node_name_from_tensor(tensor_name):\n  """"""tensor_name must have format node_name:output_number. Returns node_name.""""""\n  result = re.match(r""([^:]*):\\d+$"", tensor_name)\n  if not result:\n    raise ValueError(\n        ""Unexpected format for tensor name. Expected node_name:output_number. ""\n        ""Got %r"" % tensor_name)\n  return result.group(1)\n\n\ndef add_signature(key, inputs, outputs):\n  """"""Adds a signature to current graph.\n\n  Args:\n    key: Signature key as a string.\n    inputs: Signature inputs as a map from string to Tensor or SparseTensor.\n    outputs: Signature outputs as a map from string to Tensor or SparseTensor.\n      (Recall that a Variable is not a Tensor, but Variable.value() is.)\n\n  Raises:\n    TypeError: if the arguments have the wrong types.\n  """"""\n  _check_dict_maps_to_tensors_or_sparse_tensors(inputs)\n  _check_dict_maps_to_tensors_or_sparse_tensors(outputs)\n  input_info = {\n      input_name: tf_v1.saved_model.utils.build_tensor_info(tensor)\n      for input_name, tensor in inputs.items()\n  }\n  output_info = {\n      output_name: tf_v1.saved_model.utils.build_tensor_info(tensor)\n      for output_name, tensor in outputs.items()\n  }\n  signature = tf_v1.saved_model.signature_def_utils.build_signature_def(\n      input_info, output_info)\n  tf_v1.add_to_collection(_SIGNATURE_COLLECTION, (key, signature))\n\n\ndef _check_dict_maps_to_tensors_or_sparse_tensors(tensor_map):\n  for key, value in tensor_map.items():\n    if not isinstance(value, (tf.Tensor, tf.SparseTensor)):\n      raise TypeError(\n          ""Value for key \'%s\' should be a Tensor or SparseTensor object, found""\n          "" %s."" % (key, type(value)))\n\n\ndef _export_signatures(meta_graph):\n  """"""Exports signatures from current graph into a MetaGraphDef.""""""\n  named_signatures = tf_v1.get_collection(_SIGNATURE_COLLECTION)\n  if not named_signatures:\n    raise ValueError(""No signatures present. Please call hub.add_signature(...)""\n                     ""at least once in the module_fn."")\n  for key, signature in named_signatures:\n    meta_graph.signature_def[key].CopyFrom(signature)\n\n\ndef attach_bytes(key, the_bytes):\n  """"""Adds a ModuleAttachment to the current graph.\n\n  Args:\n    key: A string with the unique key of the attachment.\n    the_bytes: A bytes object with the serialized attachment.\n  """"""\n  tf_v1.add_to_collection(\n      _ATTACHMENT_COLLECTION_INTERNAL,\n      module_attachment_pb2.ModuleAttachment(key=key, value=the_bytes))\n\n\ndef _export_module_attachments(meta_graph):\n  """"""Exports ModuleAttachments from the current tf.Graph into `meta_graph`.""""""\n  added_attachments = tf_v1.get_collection(_ATTACHMENT_COLLECTION_INTERNAL)\n  if not added_attachments: return  # Don\'t touch `meta_graph`.\n  unique_attachments = collections.OrderedDict(  # Avoid indeterminism.\n      (attachment.key, attachment)\n      for attachment in added_attachments)\n  meta_graph.collection_def[ATTACHMENT_COLLECTION_SAVED].bytes_list.value[:] = [\n      attachment.SerializeToString()\n      for attachment in unique_attachments.values()]\n\n\ndef get_attached_bytes_map(meta_graph):\n  """"""Returns the dict of ModuleAttachments stored in `meta_graph`.\n\n  Args:\n    meta_graph: A MetaGraphDef, as built by SavedModelHandler.add_graph_copy()\n      from some graph.\n\n  Returns:\n    A dict, containing the `(key, bytes)` items passed to `attach_bytes()`\n    when the graph had been built.\n\n  Raises:\n    ValueError: if `meta-graph` is malformed.\n  """"""\n  result = {}\n  if ATTACHMENT_COLLECTION_SAVED not in meta_graph.collection_def:\n    return result\n  collection_def = meta_graph.collection_def[ATTACHMENT_COLLECTION_SAVED]\n  if collection_def.WhichOneof(""kind"") != ""bytes_list"":\n    raise ValueError(\n        ""Internal CollectionDef for attached messages has kind %s, ""\n        ""expected bytes_list"" % collection_def.WhichOneof(""kind""))\n  attachment = module_attachment_pb2.ModuleAttachment()\n  for value in collection_def.bytes_list.value:\n    attachment.ParseFromString(value)\n    result[attachment.key] = attachment.value  # Immutable; needs no copy.\n  return result\n\n\ndef _export_tags(meta_graph, tags):\n  """"""Exports tags into a MetaGraphDef.""""""\n  if tags is not None:\n    meta_graph.meta_info_def.tags.extend(tags)\n\n\ndef _check_asset_node_def(node_def):\n  """"""Raises TypeError if `node_def` does not match the expectations.""""""\n  if node_def.op != ""Const"":\n    raise TypeError(""Asset node must be of type constant."")\n  if tf.as_dtype(node_def.attr[""dtype""].type) != tf.string:\n    raise TypeError(""Asset node must be of dtype string."")\n  if len(node_def.attr[""value""].tensor.string_val) != 1:\n    raise TypeError(""Asset node must be a scalar."")\n\n\ndef _merge_assets_key_collection(saved_model_proto, path):\n  """"""Merges the ASSETS_KEY collection into the GraphDefs in saved_model_proto.\n\n  Removes the ASSETS_KEY collection from the GraphDefs in the SavedModel and\n  modifies nodes with the assets filenames to point to the assets in `path`.\n  After this transformation, the SavedModel GraphDefs can be used without\n  feeding asset tensors.\n\n  Args:\n    saved_model_proto: SavedModel proto to be modified.\n    path: path where the SavedModel is being loaded from.\n  """"""\n  for meta_graph in saved_model_proto.meta_graphs:\n    node_asset_map = {}\n    if tf_v1.saved_model.constants.ASSETS_KEY in meta_graph.collection_def:\n      assets_any_proto = meta_graph.collection_def[\n          tf_v1.saved_model.constants.ASSETS_KEY].any_list.value\n      for asset_any_proto in assets_any_proto:\n        asset_proto = meta_graph_pb2.AssetFileDef()\n        asset_any_proto.Unpack(asset_proto)\n        asset_filename = _get_asset_filename(path, asset_proto.filename)\n        node_asset_map[_get_node_name_from_tensor(\n            asset_proto.tensor_info.name)] = asset_filename\n      del meta_graph.collection_def[tf_v1.saved_model.constants.ASSETS_KEY]\n\n    for node in meta_graph.graph_def.node:\n      asset_filepath = node_asset_map.get(node.name)\n      if asset_filepath:\n        _check_asset_node_def(node)\n        node.attr[""value""].tensor.string_val[0] = asset_filepath\n\n\ndef _make_assets_key_collection(saved_model_proto, export_path):\n  """"""Creates an ASSETS_KEY collection in the GraphDefs in saved_model_proto.\n\n  Adds an ASSETS_KEY collection to the GraphDefs in the SavedModel and returns\n  a map from original asset filename to filename when exporting the SavedModel\n  to `export_path`.\n\n  This is roughly the inverse operation of `_merge_assets_key_collection`.\n\n  Args:\n    saved_model_proto: SavedModel proto to be modified.\n    export_path: string with path where the saved_model_proto will be exported.\n\n  Returns:\n    A map from original asset filename to asset filename when exporting the\n    SavedModel to path.\n\n  Raises:\n    ValueError: on unsuported/unexpected SavedModel.\n  """"""\n  asset_filenames = {}\n  used_asset_filenames = set()\n\n  def _make_asset_filename(original_filename):\n    """"""Returns the asset filename to use for the file.""""""\n    if original_filename in asset_filenames:\n      return asset_filenames[original_filename]\n\n    basename = os.path.basename(original_filename)\n    suggestion = basename\n    index = 0\n    while suggestion in used_asset_filenames:\n      suggestion = ""%s%d"" % (basename, index)\n      index += 1\n    asset_filenames[original_filename] = suggestion\n    used_asset_filenames.add(suggestion)\n    return suggestion\n\n  for meta_graph in saved_model_proto.meta_graphs:\n    collection_def = meta_graph.collection_def.get(\n        tf_v1.GraphKeys.ASSET_FILEPATHS)\n\n    if collection_def is None:\n      continue\n    if collection_def.WhichOneof(""kind"") != ""node_list"":\n      raise ValueError(\n          ""MetaGraph collection ASSET_FILEPATHS is not a list of tensors."")\n\n    for tensor in collection_def.node_list.value:\n      if not tensor.endswith("":0""):\n        raise ValueError(""Unexpected tensor in ASSET_FILEPATHS collection."")\n\n    asset_nodes = set([\n        _get_node_name_from_tensor(tensor)\n        for tensor in collection_def.node_list.value\n    ])\n\n    tensor_filename_map = {}\n    for node in meta_graph.graph_def.node:\n      if node.name in asset_nodes:\n        _check_asset_node_def(node)\n        filename = node.attr[""value""].tensor.string_val[0]\n        tensor_filename_map[node.name + "":0""] = filename\n        # Clear value to avoid leaking the original path.\n        node.attr[""value""].tensor.string_val[0] = (\n            tf.compat.as_bytes(""SAVEDMODEL-ASSET""))\n\n    if tensor_filename_map:\n      assets_key_collection = meta_graph.collection_def[\n          tf_v1.saved_model.constants.ASSETS_KEY]\n\n      for tensor, filename in sorted(tensor_filename_map.items()):\n        asset_proto = meta_graph_pb2.AssetFileDef()\n        asset_proto.filename = _make_asset_filename(filename)\n        asset_proto.tensor_info.name = tensor\n        assets_key_collection.any_list.value.add().Pack(asset_proto)\n\n  return {\n      original_filename: _get_asset_filename(export_path, asset_filename)\n      for original_filename, asset_filename in asset_filenames.items()\n  }\n\n\nclass SavedModelHandler(object):\n  """"""SavedModelHandler helps using SavedModel disk format.\n\n  Note: This is a lower level interface than most users need. See SavedModel\n  Builder/Loader API for an higher-level API centered around exporting and\n  loading Sessions.\n\n  A SavedModel disk format represents a collection of Graphs. To allow these\n  graphs to be easy to manipulate, SavedModel extends Graphs with tags and\n  signatures. Additionally it packages graphs, assets and variable checkpoints\n  into an hermetic directory that can be moved around.\n\n  This class hides the implementation details of SavedModels, in particular\n  related with assets and signatures.\n\n  SavedModelHandler deals with assets by:\n    - Only supporting asset files as constant ops added to ASSET_FILEPATHS\n      collection.\n    - Creating a ASSETS_KEY collection only when writing meta_graphs to disk so\n      they are never visible to user.\n    - Baking the ASSETS_KEY collection in the graphs when loading from disk as\n      to hide that the assets point to the packaged assets.\n\n  SavedModelHandler deals with signatures by:\n    - Providing `add_signature` API that allows to declare signatures directly\n      on a graph.\n    - That API is supported by a collection that is not serialized, but instead\n      is converted into the right fields of MetaGraphDef when writing and\n      loading a SavedModel from disk.\n  """"""\n\n  def __init__(self):\n    self._proto = saved_model_pb2.SavedModel()\n\n  def add_graph_copy(self, graph, tags=None):\n    """"""Adds a copy of Graph with the specified set of tags.""""""\n    with graph.as_default():\n      # Remove default attrs so that Modules created by a tensorflow version\n      # with ops that have new attrs that are left to their default values can\n      # still be loaded by older versions unware of those attributes.\n      meta_graph = tf_v1.train.export_meta_graph(strip_default_attrs=True)\n      _export_tags(meta_graph, tags)\n      _export_signatures(meta_graph)\n      _export_module_attachments(meta_graph)\n    self._proto.meta_graphs.extend([meta_graph])\n\n  def add_meta_graph_copy(self, meta_graph):\n    self._proto.meta_graphs.extend([meta_graph])\n\n  def get_meta_graph_copy(self, tags=None):\n    """"""Returns a copy of a MetaGraph with the identical set of tags.""""""\n    meta_graph = self.get_meta_graph(tags)\n    copy = tf_v1.MetaGraphDef()\n    copy.CopyFrom(meta_graph)\n    return copy\n\n  @property\n  def meta_graphs(self):\n    return iter(self._proto.meta_graphs)\n\n  def get_tags(self):\n    """"""Returns a list of set of tags.""""""\n    return sorted([frozenset(meta_graph.meta_info_def.tags)\n                   for meta_graph in self.meta_graphs])\n\n  def get_attached_bytes_map(self, tags=None):\n    return get_attached_bytes_map(self.get_meta_graph(tags))\n\n  def export(self, path, variables_saver=None):\n    """"""Exports to SavedModel directory.\n\n    Args:\n      path: path where to export the SavedModel to.\n      variables_saver: lambda that receives a directory path where to\n        export checkpoints of variables.\n    """"""\n    # Operate on a copy of self._proto since it needs to be modified.\n    proto = saved_model_pb2.SavedModel()\n    proto.CopyFrom(self._proto)\n    assets_map = _make_assets_key_collection(proto, path)\n\n    self._save_all_assets(path, assets_map)\n    self._save_variables(path, variables_saver)\n    self._save_proto(path, proto)\n\n  def get_meta_graph(self, tags=None):\n    """"""Returns the matching MetaGraphDef or raises KeyError.""""""\n    matches = [meta_graph\n               for meta_graph in self.meta_graphs\n               if set(meta_graph.meta_info_def.tags) == set(tags or [])]\n    if not matches:\n      raise KeyError(""SavedModelHandler has no graph with tags: %r"" % tags)\n    if len(matches) != 1:\n      raise KeyError(\n          ""SavedModelHandler has multiple graphs with tags %r"" % tags)\n    return matches[0]\n\n  def _save_all_assets(self, path, assets_map):\n    assets_dir = _get_assets_dir(path)\n    tf_v1.gfile.MakeDirs(assets_dir)\n    for source, destination in assets_map.items():\n      tf_v1.gfile.Copy(source, destination)\n\n  def _save_variables(self, path, variables_saver):\n    if variables_saver:\n      variables_path = get_variables_path(path)\n      variables_dir = os.path.dirname(variables_path)\n      tf_v1.gfile.MakeDirs(variables_dir)\n      variables_saver(variables_path)\n\n  def _save_proto(self, path, proto):\n    proto_path = _get_saved_model_proto_path(path)\n    tf_v1.gfile.MakeDirs(os.path.dirname(proto_path))\n    tf_utils.atomic_write_string_to_file(proto_path,\n                                         proto.SerializeToString(),\n                                         overwrite=True)\n\n\ndef _parse_saved_model(path):\n  """"""Reads the savedmodel.pb file containing `SavedModel`.""""""\n  # Based on tensorflow/python/saved_model/loader.py implementation.\n  path_to_pb = _get_saved_model_proto_path(path)\n  file_content = tf_v1.gfile.Open(path_to_pb, ""rb"").read()\n  saved_model = saved_model_pb2.SavedModel()\n  try:\n    saved_model.ParseFromString(file_content)\n  except message.DecodeError as e:\n    raise IOError(""Cannot parse file %s: %s."" % (path_to_pb, str(e)))\n  return saved_model\n\n\ndef load(path):\n  """"""Creates a SavedModelHandler from a SavedModel in `path`.""""""\n  proto = _parse_saved_model(path)\n  _merge_assets_key_collection(proto, path)\n  handler = SavedModelHandler()\n  handler._proto = proto  # pylint: disable=protected-access\n  return handler\n'"
tensorflow_hub/saved_model_lib_test.py,27,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.saved_model_lib.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport os\n\nimport tensorflow as tf\nfrom tensorflow_hub import saved_model_lib\nfrom tensorflow_hub import tf_v1\n\nfrom tensorflow.core.protobuf import meta_graph_pb2\n\n\ndef _instantiate_meta_graph(saved_model_handler, tags=None):\n  """"""Loads a MetaGraph from a SavedModelHandler into a new Graph.""""""\n  meta_graph = saved_model_handler.get_meta_graph(tags)\n  with tf.Graph().as_default() as graph:\n    tf_v1.train.import_meta_graph(meta_graph, import_scope="""")\n  return graph\n\n\ndef _write_string_to_file(path, contents):\n  with tf_v1.gfile.Open(path, ""w"") as f:\n    f.write(contents)\n\n\ndef _read_file_to_string(path):\n  with tf_v1.gfile.Open(path, ""r"") as f:\n    return f.read()\n\n\nclass SavedModelLibTest(tf.test.TestCase):\n\n  def testAssets(self):\n    original_asset_file = os.path.join(self.get_temp_dir(), ""hello.txt"")\n    _write_string_to_file(original_asset_file, ""hello world"")\n\n    with tf.Graph().as_default() as graph:\n      asset_tensor = tf.constant(original_asset_file, name=""file"")\n      graph.add_to_collection(tf_v1.GraphKeys.ASSET_FILEPATHS, asset_tensor)\n      saved_model_lib.add_signature(""default"", {}, {""default"": asset_tensor})\n\n    handler = saved_model_lib.SavedModelHandler()\n    handler.add_graph_copy(graph)\n\n    export_dir = os.path.join(self.get_temp_dir(), ""exported"")\n    handler.export(export_dir)\n\n    # Check that asset file got written to the expected place:\n    exported_asset_file = os.path.join(export_dir, ""assets"", ""hello.txt"")\n    self.assertTrue(tf_v1.gfile.Exists(exported_asset_file))\n\n    loaded_handler = saved_model_lib.load(export_dir)\n    with _instantiate_meta_graph(loaded_handler).as_default():\n      with tf_v1.Session() as sess:\n        self.assertEqual(sess.run(""file:0""),\n                         tf.compat.as_bytes(exported_asset_file))\n\n  def testWithMultipleAssetsWithSameBasename(self):\n    tmp_asset_dir = os.path.join(self.get_temp_dir(), ""asset"")\n    file_a = os.path.join(tmp_asset_dir, ""a"", ""hello.txt"")\n    file_b = os.path.join(tmp_asset_dir, ""b"", ""hello.txt"")\n    tf_v1.gfile.MakeDirs(os.path.dirname(file_a))\n    tf_v1.gfile.MakeDirs(os.path.dirname(file_b))\n    _write_string_to_file(file_a, ""hello A"")\n    _write_string_to_file(file_b, ""hello B"")\n    with tf.Graph().as_default() as graph:\n      asset_a = tf.constant(file_a, name=""file_a"")\n      asset_b = tf.constant(file_b, name=""file_b"")\n      graph.add_to_collection(tf_v1.GraphKeys.ASSET_FILEPATHS, asset_a)\n      graph.add_to_collection(tf_v1.GraphKeys.ASSET_FILEPATHS, asset_b)\n      saved_model_lib.add_signature(""default"", {}, {""default"": asset_a})\n\n    export_dir = os.path.join(self.get_temp_dir(), ""exported"")\n    handler = saved_model_lib.SavedModelHandler()\n    handler.add_graph_copy(graph)\n    handler.export(export_dir)\n    tf_v1.gfile.DeleteRecursively(tmp_asset_dir)\n\n    loaded_handler = saved_model_lib.load(export_dir)\n    with _instantiate_meta_graph(loaded_handler).as_default():\n      with tf_v1.Session() as sess:\n        self.assertEqual(_read_file_to_string(sess.run(""file_a:0"")), ""hello A"")\n        self.assertEqual(_read_file_to_string(sess.run(""file_b:0"")), ""hello B"")\n\n  def testCreationOfAssetsKeyCollectionIsDeterministic(self):\n    tmp_asset_dir = os.path.join(self.get_temp_dir(), ""assets"")\n    tf_v1.gfile.MakeDirs(tmp_asset_dir)\n    filenames = [\n        os.path.join(tmp_asset_dir, ""file%d.txt"" % n) for n in range(10)\n    ]\n    for filename in filenames:\n      _write_string_to_file(filename, ""I am file %s"" % filename)\n\n    with tf.Graph().as_default() as graph:\n      assets = [tf.constant(f, name=os.path.basename(f)) for f in filenames]\n      for asset in assets:\n        graph.add_to_collection(tf_v1.GraphKeys.ASSET_FILEPATHS, asset)\n      saved_model_lib.add_signature(""default"", {}, {""default"": assets[0]})\n\n    handler = saved_model_lib.SavedModelHandler()\n    handler.add_graph_copy(graph)\n    saved_model_proto = copy.deepcopy(handler._proto)\n    export_dir = os.path.join(self.get_temp_dir(), ""assets_key_test"")\n    saved_model_lib._make_assets_key_collection(saved_model_proto, export_dir)\n\n    meta_graph = list(saved_model_proto.meta_graphs)[0]\n    asset_tensor_names = []\n    for asset_any_proto in meta_graph.collection_def[\n        tf_v1.saved_model.constants.ASSETS_KEY].any_list.value:\n      asset_proto = meta_graph_pb2.AssetFileDef()\n      asset_any_proto.Unpack(asset_proto)\n      asset_tensor_names.append(asset_proto.tensor_info.name)\n    self.assertEqual(asset_tensor_names, sorted(asset_tensor_names))\n\n  def testSignatures(self):\n    with tf.Graph().as_default() as graph:\n      input_a = tf.constant(2)\n      input_b = tf.constant(3)\n      mul = input_a * input_b\n      saved_model_lib.add_signature(""six"", {}, {""out"": mul})\n      saved_model_lib.add_signature(""mul2"", {""in"": input_b}, {""out"": mul})\n\n    handler = saved_model_lib.SavedModelHandler()\n    handler.add_graph_copy(graph)\n\n    signatures = handler.get_meta_graph_copy().signature_def\n    self.assertEqual(set(signatures.keys()), set([""six"", ""mul2""]))\n    self.assertAllEqual(list(signatures[""six""].inputs.keys()), [])\n    self.assertAllEqual(list(signatures[""six""].outputs.keys()), [""out""])\n    self.assertAllEqual(list(signatures[""mul2""].inputs.keys()), [""in""])\n    self.assertAllEqual(list(signatures[""mul2""].outputs.keys()), [""out""])\n\n  def testSignatureImplementationIsInvisible(self):\n    with tf.Graph().as_default() as graph:\n      saved_model_lib.add_signature(""test"", {}, {})\n      self.assertEqual(graph.get_all_collection_keys(), [])\n\n    handler = saved_model_lib.SavedModelHandler()\n    handler.add_graph_copy(graph)\n    meta_graph, = handler.meta_graphs\n    self.assertEqual(len(meta_graph.collection_def), 0)\n    self.assertEqual(len(meta_graph.signature_def), 1)\n\n  def testTags(self):\n    with tf.Graph().as_default() as graph:\n      saved_model_lib.add_signature(""default"", {}, {""default"": tf.constant(1)})\n    handler = saved_model_lib.SavedModelHandler()\n    handler.add_graph_copy(graph, [""tag1""])\n    handler.add_graph_copy(graph, [""tag1"", ""tag2""])\n    self.assertAllEqual(sorted(handler.get_tags()),\n                        sorted([set([""tag1""]), set([""tag1"", ""tag2""])]))\n    self.assertTrue(handler.get_meta_graph_copy([""tag1""]) is not None)\n    self.assertTrue(handler.get_meta_graph_copy([""tag2"", ""tag1""]) is not None)\n    with self.assertRaises(KeyError):\n      handler.get_meta_graph_copy([""tag2""])\n\n  def testModuleAttachments(self):\n    meta_graph = meta_graph_pb2.MetaGraphDef()\n    with tf.Graph().as_default():\n      saved_model_lib.attach_bytes(""key1"", tf.compat.as_bytes(""oops""))\n      saved_model_lib.attach_bytes(""key2"", tf.compat.as_bytes(""value2""))\n      saved_model_lib.attach_bytes(""key1"", tf.compat.as_bytes(""value1""))\n      saved_model_lib._export_module_attachments(meta_graph)\n    actual = saved_model_lib.get_attached_bytes_map(meta_graph)\n    expected = {""key1"": tf.compat.as_bytes(""value1""),\n                ""key2"": tf.compat.as_bytes(""value2"")}\n    self.assertDictEqual(expected, actual)\n\n  def testNoModuleAttachments(self):\n    meta_graph = meta_graph_pb2.MetaGraphDef()\n    with tf.Graph().as_default():\n      # No calls to attach_bytes.\n      saved_model_lib._export_module_attachments(meta_graph)\n    actual = saved_model_lib.get_attached_bytes_map(meta_graph)\n    self.assertDictEqual({}, actual)\n    # Check there were no unwarranted subscript operations.\n    self.assertFalse(saved_model_lib.ATTACHMENT_COLLECTION_SAVED\n                     in meta_graph.collection_def)\n\n  def testEmptyCollectionsDoNotShowUpInMetaGraphDef(self):\n    with tf.Graph().as_default() as graph:\n      tf.Variable(""name"")\n      self.assertEqual(len(graph.get_all_collection_keys()), 2)\n      for collection_key in graph.get_all_collection_keys():\n        del graph.get_collection_ref(collection_key)[:]\n      saved_model_lib.add_signature(""default"", {}, {""default"": tf.constant(1)})\n\n    handler = saved_model_lib.SavedModelHandler()\n    handler.add_graph_copy(graph)\n    meta_graph, = handler.meta_graphs\n    self.assertEqual(len(meta_graph.collection_def), 0)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/saved_model_module.py,2,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of deprecated hub.Module that loads raw TF1 SavedModels.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow_hub import native_module\nfrom tensorflow_hub import saved_model_lib\nfrom tensorflow_hub import tf_v1\n\n\n_ALWAYS_DROPPED_COLLECTIONS = [\n    # SavedModels exported from estimator framework typically contain a\n    # collection with the variable that holds the global training step.\n    #\n    # This collection is ignored when loading it as a module. However the\n    # variable that contains the step would still be brought in if declared\n    # in the VARIABLES collection.\n    tf_v1.GraphKeys.GLOBAL_STEP,\n\n    # SavedModels exported for serving use cases contain collections which refer\n    # to ops in the graph that when run are responsible to initialize the\n    # session for subsequent signature executions.\n    #\n    # This generic initialization definition is impossible to support for many\n    # hub use cases and therefore the assumption here is that the SavedModel\n    # init op can be ignored in favor of initializing using the\n    # tf.train.MonitoredSession mechanisms + construction of a new tf.Saver()\n    # from the global variables collection.\n    tf_v1.saved_model.constants.LEGACY_INIT_OP_KEY,\n    tf_v1.saved_model.constants.MAIN_OP_KEY,\n]\n\n\ndef _drop_collections(saved_model_handler, collections):\n  for meta_graph in saved_model_handler.meta_graphs:\n    for collection in collections:\n      if collection in meta_graph.collection_def:\n        del meta_graph.collection_def[collection]\n\n\ndef create_module_spec_from_saved_model(saved_model_path,\n                                        drop_collections=None):\n  """"""Experimental: Create a ModuleSpec out of a SavedModel from TF1.\n\n  DEPRECATION NOTE: This belongs to the hub.Module API and file format for TF1.\n  For TF2, TensorFlow Hub ships plain SavedModels, removing the need for\n  conversions like this.\n\n  Define a ModuleSpec from a SavedModel. Note that this is not guaranteed to\n  work in all cases and it assumes the SavedModel has followed some conventions:\n\n  - The serialized SaverDef can be ignored and instead can be reconstructed.\n  - The init op and main op can be ignored and instead the module can be\n    initialized by using the conventions followed by\n    `tf.train.MonitoredSession`.\n\n  Note that the set of features supported can increase over time and have side\n  effects that were not previously visible. The pattern followed to avoid\n  surprises is forcing users to declare which features to ignore (even\n  if they are not supported).\n\n  Note that this function creates a ModuleSpec that when exported exports a\n  Module (based on a modified copy of the original SavedModel) and not a\n  SavedModel.\n\n  Args:\n    saved_model_path: Directory with the SavedModel to use.\n    drop_collections: Additionally list of collection to drop.\n\n  Returns:\n    A ModuleSpec.\n  """"""\n  saved_model_handler = saved_model_lib.load(saved_model_path)\n  checkpoint_filename = saved_model_lib.get_variables_path(saved_model_path)\n\n  drop_collections = (set(_ALWAYS_DROPPED_COLLECTIONS) |\n                      (set(drop_collections) if drop_collections else set()))\n  _drop_collections(saved_model_handler, drop_collections)\n\n  return native_module._ModuleSpec(saved_model_handler, checkpoint_filename)  # pylint: disable=protected-access\n'"
tensorflow_hub/saved_model_module_test.py,5,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.saved_model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow_hub import tf_v1\n\n\n_EXTRA_COLLECTION = ""exercise_drop_collection""\n\n\nclass SavedModelTest(tf.test.TestCase):\n\n  def createSavedModel(self):\n    model_dir = os.path.join(self.get_temp_dir(), ""saved_model"")\n    with tf.Graph().as_default():\n      x = tf_v1.placeholder(dtype=tf.float32, shape=[None, 3])\n      w = tf_v1.get_variable(""weights"", shape=[])\n      y = x*w\n      tf_v1.add_to_collection(_EXTRA_COLLECTION, y)\n\n      init_op = tf_v1.assign(w, 2)\n\n      with tf_v1.Session() as session:\n        session.run(init_op)\n        tf_v1.saved_model.simple_save(\n            session,\n            model_dir,\n            inputs={""x"": x},\n            outputs={""y"": y},\n        )\n    return model_dir\n\n  def testLoadSavedModel(self):\n    saved_model_path = self.createSavedModel()\n    spec = hub.create_module_spec_from_saved_model(\n        saved_model_path,\n        drop_collections=[_EXTRA_COLLECTION])\n    with tf.Graph().as_default():\n      m = hub.Module(spec, tags=[""serve""])\n      y = m([[2, 4, 5]], signature=""serving_default"", as_dict=True)[""y""]\n      with tf_v1.train.MonitoredSession() as session:\n        self.assertAllEqual(session.run(y), [[4, 8, 10]])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/tensor_info.py,6,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TensorFlow Hub internal utilities to handle information about tensors.\n\nThis file provides utilities to refer to properties of un-instantiated Tensors\nin a concise way. Note: Ideally TensorFlow would provide a way to do this.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow_hub import tf_v1\n\n\nclass ParsedTensorInfo(object):\n  """"""This is a tensor-looking object with information about a Tensor.\n\n  This class provides a subset of methods and attributes provided by real\n  instantiated Tensor/SparseTensors in a graph such that code designed to\n  handle instances of it would mostly work in real Tensors.\n  """"""\n\n  def __init__(self, dtype, shape, is_sparse):\n    self._dtype = dtype\n    self._shape = shape\n    self._is_sparse = is_sparse\n\n  @property\n  def dtype(self):\n    """"""The `DType` of elements in this tensor.""""""\n    return self._dtype\n\n  def get_shape(self):\n    """"""The `TensorShape` that represents the dense shape of this tensor.""""""\n    return self._shape\n\n  @property\n  def is_sparse(self):\n    """"""Whether it represents a sparse tensor.""""""\n    # This property is non-standard and does not exist in tf.Tensor or\n    # tf.SparseTensor instances.\n    return self._is_sparse\n\n  def __repr__(self):\n    return ""<hub.ParsedTensorInfo shape=%s dtype=%s is_sparse=%s>"" % (\n        self.get_shape(),\n        self.dtype.name,\n        self.is_sparse)\n\n\ndef _parse_tensor_info_proto(tensor_info):\n  """"""Returns a ParsedTensorInfo instance from a TensorInfo proto.""""""\n  encoding = tensor_info.WhichOneof(""encoding"")\n  dtype = tf.DType(tensor_info.dtype)\n  shape = tf.TensorShape(tensor_info.tensor_shape)\n  if encoding == ""name"":\n    return ParsedTensorInfo(dtype=dtype, shape=shape, is_sparse=False)\n  elif encoding == ""coo_sparse"":\n    return ParsedTensorInfo(dtype=dtype, shape=shape, is_sparse=True)\n  else:\n    raise ValueError(""Unsupported TensorInfo encoding %r"" % encoding)\n\n\ndef parse_tensor_info_map(protomap):\n  """"""Converts a proto map<string, TensorInfo> into a native Python dict.\n\n  The keys are preserved. The TensorInfo protos are parsed into objects\n  with dtype property and get_shape() method similar to Tensor and SparseTensor\n  objects and an additional `is_sparse` property.\n\n  Args:\n    protomap: A proto map<string, TensorInfo>.\n\n  Returns:\n    A map from the original keys to python objects.\n  """"""\n  return {\n      key: _parse_tensor_info_proto(value)\n      for key, value in protomap.items()\n  }\n\n\ndef _is_sparse(x):\n  """"""Returns whether x is a SparseTensor or a parsed sparse tensor info.""""""\n  return (\n      isinstance(x, (tf.SparseTensor, tf_v1.SparseTensorValue)) or\n      (hasattr(x, ""is_sparse"") and x.is_sparse))\n\n\ndef _convert_to_compatible_tensor(value, target, error_prefix):\n  """"""Converts `value` into a tensor that can be feed into `tensor_info`.\n\n  Args:\n    value: A value to convert into Tensor or SparseTensor.\n    target: An object returned by `parse_tensor_info_map`.\n    error_prefix: A string to prefix on raised TypeErrors.\n\n  Raises:\n    TypeError: If it fails to convert.\n\n  Returns:\n    A Tensor or SparseTensor compatible with tensor_info.\n  """"""\n  try:\n    tensor = tf_v1.convert_to_tensor_or_indexed_slices(value, target.dtype)\n  except TypeError as e:\n    raise TypeError(""%s: %s"" % (error_prefix, e))\n  if _is_sparse(tensor) != _is_sparse(target):\n    if _is_sparse(tensor):\n      raise TypeError(""%s: Is sparse. Expected dense."" % error_prefix)\n    else:\n      raise TypeError(""%s: Is dense. Expected sparse."" % error_prefix)\n  if not tensor.get_shape().is_compatible_with(target.get_shape()):\n    raise TypeError(""%s: Shape %r is incompatible with %r"" %\n                    (error_prefix, tensor.get_shape(), target.get_shape()))\n  return tensor\n\n\ndef convert_dict_to_compatible_tensor(values, targets):\n  """"""Converts dict `values` in tensors that are compatible with `targets`.\n\n  Args:\n    values: A dict to objects to convert with same keys as `targets`.\n    targets: A dict returned by `parse_tensor_info_map`.\n\n  Returns:\n    A map with the same keys as `values` but values converted into\n    Tensor/SparseTensors that can be fed into `protomap`.\n\n  Raises:\n    TypeError: If it fails to convert.\n  """"""\n  result = {}\n  for key, value in sorted(values.items()):\n    result[key] = _convert_to_compatible_tensor(\n        value, targets[key], error_prefix=""Can\'t convert %r"" % key)\n  return result\n\n\ndef build_input_map(protomap, inputs):\n  """"""Builds a map to feed tensors in `protomap` using `inputs`.\n\n  Args:\n    protomap: A proto map<string,TensorInfo>.\n    inputs: A map with same keys as `protomap` of Tensors and SparseTensors.\n\n  Returns:\n    A map from nodes refered by TensorInfo protos to corresponding input\n    tensors.\n\n  Raises:\n    ValueError: if a TensorInfo proto is malformed or map keys do not match.\n  """"""\n  if set(protomap.keys()) != set(inputs.keys()):\n    raise ValueError(""build_input_map: keys do not match."")\n  input_map = {}\n  for key, tensor_info in protomap.items():\n    arg = inputs[key]\n    encoding = tensor_info.WhichOneof(""encoding"")\n    if encoding == ""name"":\n      input_map[tensor_info.name] = arg\n    elif encoding == ""coo_sparse"":\n      coo_sparse = tensor_info.coo_sparse\n      input_map[coo_sparse.values_tensor_name] = arg.values\n      input_map[coo_sparse.indices_tensor_name] = arg.indices\n      input_map[coo_sparse.dense_shape_tensor_name] = arg.dense_shape\n    else:\n      raise ValueError(""Invalid TensorInfo.encoding: %s"" % encoding)\n  return input_map\n\n\ndef build_output_map(protomap, get_tensor_by_name):\n  """"""Builds a map of tensors from `protomap` using `get_tensor_by_name`.\n\n  Args:\n    protomap: A proto map<string,TensorInfo>.\n    get_tensor_by_name: A lambda that receives a tensor name and returns a\n      Tensor instance.\n\n  Returns:\n    A map from string to Tensor or SparseTensor instances built from `protomap`\n    and resolving tensors using `get_tensor_by_name()`.\n\n  Raises:\n    ValueError: if a TensorInfo proto is malformed.\n  """"""\n\n  def get_output_from_tensor_info(tensor_info):\n    encoding = tensor_info.WhichOneof(""encoding"")\n    if encoding == ""name"":\n      return get_tensor_by_name(tensor_info.name)\n    elif encoding == ""coo_sparse"":\n      return tf.SparseTensor(\n          get_tensor_by_name(tensor_info.coo_sparse.indices_tensor_name),\n          get_tensor_by_name(tensor_info.coo_sparse.values_tensor_name),\n          get_tensor_by_name(tensor_info.coo_sparse.dense_shape_tensor_name))\n    else:\n      raise ValueError(""Invalid TensorInfo.encoding: %s"" % encoding)\n\n  return {\n      key: get_output_from_tensor_info(tensor_info)\n      for key, tensor_info in protomap.items()\n  }\n\n\ndef _shape_match(a, b):\n  # TRICKY: as_list() can\'t be used if the number of dimensions is unknown.\n  # So we check those before.\n  if a.ndims != b.ndims:\n    return False\n  if a.ndims and a.as_list() != b.as_list():\n    return False\n  return True\n\n\ndef tensor_info_proto_maps_match(map_a, map_b):\n  """"""Whether two signature inputs/outputs match in dtype, shape and sparsity.\n\n  Args:\n    map_a: A proto map<string,TensorInfo>.\n    map_b: A proto map<string,TensorInfo>.\n\n  Returns:\n    A boolean whether `map_a` and `map_b` tensors have the same dtype, shape and\n    sparsity.\n  """"""\n  iter_a = sorted(parse_tensor_info_map(map_a).items())\n  iter_b = sorted(parse_tensor_info_map(map_b).items())\n  if len(iter_a) != len(iter_b):\n    return False  # Mismatch count.\n  for info_a, info_b in zip(iter_a, iter_b):\n    if info_a[0] != info_b[0]:\n      return False  # Mismatch keys.\n    if _is_sparse(info_a[1]) != _is_sparse(info_b[1]):\n      return False\n    if info_a[1].dtype != info_b[1].dtype:\n      return False\n    if not _shape_match(info_a[1].get_shape(), info_b[1].get_shape()):\n      return False\n  return True\n'"
tensorflow_hub/tensor_info_test.py,23,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.tensor_info.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow_hub import tensor_info\nfrom tensorflow_hub import tf_v1\n\n\ndef _make_signature(inputs, outputs, name=None):\n  input_info = {\n      input_name: tf_v1.saved_model.utils.build_tensor_info(tensor)\n      for input_name, tensor in inputs.items()\n  }\n  output_info = {\n      output_name: tf_v1.saved_model.utils.build_tensor_info(tensor)\n      for output_name, tensor in outputs.items()\n  }\n  return tf_v1.saved_model.signature_def_utils.build_signature_def(\n      input_info, output_info, name)\n\n\nclass TensorInfoTest(tf.test.TestCase):\n\n  def testParsingTensorInfoProtoMaps(self):\n    with tf_v1.Graph().as_default():\n      sig = _make_signature({\n          ""x"": tf_v1.placeholder(tf.string, [2]),\n      }, {\n          ""y"": tf_v1.placeholder(tf.int32, [2]),\n          ""z"": tf_v1.sparse_placeholder(tf.float32, [2, 10]),\n      })\n\n      inputs = tensor_info.parse_tensor_info_map(sig.inputs)\n      self.assertEqual(set(inputs.keys()), set([""x""]))\n      self.assertEqual(inputs[""x""].get_shape(), [2])\n      self.assertEqual(inputs[""x""].dtype, tf.string)\n      self.assertFalse(inputs[""x""].is_sparse)\n\n      outputs = tensor_info.parse_tensor_info_map(sig.outputs)\n      self.assertEqual(set(outputs.keys()), set([""y"", ""z""]))\n      self.assertEqual(outputs[""y""].get_shape(), [2])\n      self.assertEqual(outputs[""y""].dtype, tf.int32)\n      self.assertFalse(outputs[""y""].is_sparse)\n\n      self.assertEqual(outputs[""z""].get_shape(), [2, 10])\n      self.assertEqual(outputs[""z""].dtype, tf.float32)\n      self.assertTrue(outputs[""z""].is_sparse)\n\n  def testRepr(self):\n    with tf_v1.Graph().as_default():\n      sig = _make_signature({\n          ""x"": tf_v1.placeholder(tf.string, [2]),\n      }, {\n          ""y"": tf_v1.placeholder(tf.int32, [2]),\n          ""z"": tf_v1.sparse_placeholder(tf.float32, [2, 10]),\n      })\n\n      outputs = tensor_info.parse_tensor_info_map(sig.outputs)\n      self.assertEqual(\n          repr(outputs[""y""]),\n          ""<hub.ParsedTensorInfo shape=(2,) dtype=int32 is_sparse=False>"")\n      self.assertEqual(\n          repr(outputs[""z""]),\n          ""<hub.ParsedTensorInfo shape=(2, 10) dtype=float32 is_sparse=True>"")\n\n\n  def testMatchingTensorInfoProtoMaps(self):\n    with tf_v1.Graph().as_default():\n      sig1 = _make_signature({\n          ""x"": tf_v1.placeholder(tf.int32, [2]),\n      }, {\n          ""x"": tf_v1.placeholder(tf.int32, [2]),\n      })\n\n      sig2 = _make_signature({\n          ""x"": tf_v1.placeholder(tf.int32, [2]),\n      }, {\n          ""x"": tf_v1.sparse_placeholder(tf.int64, [2]),\n      })\n      self.assertTrue(\n          tensor_info.tensor_info_proto_maps_match(sig1.inputs, sig2.inputs))\n      self.assertFalse(\n          tensor_info.tensor_info_proto_maps_match(sig1.outputs, sig2.outputs))\n\n      sig3 = _make_signature({\n          ""x"": tf_v1.placeholder(tf.int32, [None]),\n      }, {\n          ""x"": tf_v1.placeholder(tf.int32, [2]),\n      })\n      self.assertFalse(\n          tensor_info.tensor_info_proto_maps_match(sig1.inputs, sig3.inputs))\n      self.assertTrue(\n          tensor_info.tensor_info_proto_maps_match(sig1.outputs, sig3.outputs))\n\n  def testBuildInputMap(self):\n    with tf_v1.Graph().as_default():\n      x = tf_v1.placeholder(tf.int32, [2])\n      y = tf_v1.sparse_placeholder(tf.string, [None])\n      sig = _make_signature({""x"": x, ""y"": y}, {})\n\n      input_map = tensor_info.build_input_map(sig.inputs, {""x"": x, ""y"": y})\n      self.assertEqual(len(input_map), 4)\n      self.assertEqual(input_map[x.name], x)\n      self.assertEqual(input_map[y.indices.name], y.indices)\n      self.assertEqual(input_map[y.values.name], y.values)\n      self.assertEqual(input_map[y.dense_shape.name], y.dense_shape)\n\n  def testBuildOutputMap(self):\n    with tf_v1.Graph().as_default():\n      x = tf_v1.placeholder(tf.int32, [2])\n      y = tf_v1.sparse_placeholder(tf.string, [None])\n      sig = _make_signature({}, {""x"": x, ""y"": y})\n\n      def _get_tensor(name):\n        return tf_v1.get_default_graph().get_tensor_by_name(name)\n\n      output_map = tensor_info.build_output_map(sig.outputs, _get_tensor)\n      self.assertEqual(len(output_map), 2)\n      self.assertEqual(output_map[""x""], x)\n      self.assertEqual(output_map[""y""].indices, y.indices)\n      self.assertEqual(output_map[""y""].values, y.values)\n      self.assertEqual(output_map[""y""].dense_shape, y.dense_shape)\n\n  def testConvertTensors(self):\n    with tf_v1.Graph().as_default():\n      a = tf_v1.placeholder(tf.int32, [None])\n      protomap = _make_signature({""a"": a}, {}).inputs\n      targets = tensor_info.parse_tensor_info_map(protomap)\n\n      # convert constant\n      in0 = [1, 2, 3]\n      output = tensor_info.convert_dict_to_compatible_tensor({""a"": in0},\n                                                             targets)\n      self.assertEqual(output[""a""].dtype, a.dtype)\n\n      # check sparsity\n      in1 = tf_v1.sparse_placeholder(tf.int32, [])\n      with self.assertRaisesRegexp(TypeError, ""dense""):\n        tensor_info.convert_dict_to_compatible_tensor({""a"": in1}, targets)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_hub/test_utils.py,1,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common testing functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport socket\nimport sys\nimport threading\n\nfrom absl import flags\nimport tensorflow as tf\n\n\ndef _do_redirect(handler, location):\n  handler.send_response(301)\n  handler.send_header(""Location"", location)\n  handler.end_headers()\n\n\ndef _do_documentation(handler):\n  handler.send_response(200)\n  handler.end_headers()\n  handler.wfile.write(b""Here is some documentation."")\n\n\ndef start_smart_module_server(download_url):\n  """"""Serve documentation and module requests at the same URL.""""""\n  # pylint:disable=g-import-not-at-top\n  if sys.version_info[0] == 2:\n    import BaseHTTPServer\n    import SimpleHTTPServer\n    import urlparse\n\n    class HTTPServerV6(BaseHTTPServer.HTTPServer):\n\n      address_family = socket.AF_INET6\n\n    class RequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n\n      def do_GET(self):\n        parsed_url = urlparse.urlparse(self.path)\n        qs = urlparse.parse_qs(parsed_url.query)\n        if qs[""tf-hub-format""][0] == ""compressed"":\n          _do_redirect(self, download_url)\n        else:\n          _do_documentation(self)\n\n    server = HTTPServerV6(("""", 0), RequestHandler)\n    server_port = server.server_port\n  else:\n    import http.server\n    import socketserver\n    import urllib\n\n    class TCPServerV6(socketserver.TCPServer):\n\n      address_family = socket.AF_INET6\n\n    class RequestHandler(http.server.SimpleHTTPRequestHandler):\n\n      def do_GET(self):\n        parsed_url = urllib.parse.urlparse(self.path)\n        qs = urllib.parse.parse_qs(parsed_url.query)\n        if qs[""tf-hub-format""][0] == ""compressed"":\n          _do_redirect(self, download_url)\n        else:\n          _do_documentation(self)\n\n    server = TCPServerV6(("""", 0), RequestHandler)\n    _, server_port, _, _ = server.server_address\n  # pylint:disable=g-import-not-at-top\n\n  thread = threading.Thread(target=server.serve_forever)\n  thread.daemon = True\n  thread.start()\n\n  return server_port\n\n\ndef start_http_server(redirect=None):\n  """"""Returns the port of the newly started HTTP server.""""""\n\n  # Start HTTP server to serve TAR files.\n  # pylint:disable=g-import-not-at-top\n  if sys.version_info[0] == 2:\n    import BaseHTTPServer\n    import SimpleHTTPServer\n\n    class HTTPServerV6(BaseHTTPServer.HTTPServer):\n\n      address_family = socket.AF_INET6\n\n    class RedirectHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n\n      def do_GET(self):\n        _do_redirect(self, redirect)\n\n    server = HTTPServerV6(("""", 0), RedirectHandler if redirect else\n                          SimpleHTTPServer.SimpleHTTPRequestHandler)\n    server_port = server.server_port\n  else:\n    import http.server\n    import socketserver\n\n    class TCPServerV6(socketserver.TCPServer):\n\n      address_family = socket.AF_INET6\n\n    class RedirectHandler(http.server.SimpleHTTPRequestHandler):\n\n      def do_GET(self):\n        _do_redirect(self, redirect)\n\n    server = TCPServerV6(("""", 0), RedirectHandler if redirect else\n                         http.server.SimpleHTTPRequestHandler)\n    _, server_port, _, _ = server.server_address\n  # pylint:disable=g-import-not-at-top\n\n  thread = threading.Thread(target=server.serve_forever)\n  thread.daemon = True\n  thread.start()\n\n  return server_port\n\n\ndef test_srcdir():\n  """"""Returns the path where to look for test data files.""""""\n  if ""test_srcdir"" in flags.FLAGS:\n    return flags.FLAGS[""test_srcdir""].value\n  elif ""TEST_SRCDIR"" in os.environ:\n    return os.environ[""TEST_SRCDIR""]\n  else:\n    raise RuntimeError(""Missing TEST_SRCDIR environment."")\n\n\ndef get_test_data_path(file_or_dirname):\n  """"""Return full test data path.""""""\n  for directory, subdirs, files in tf.io.gfile.walk(test_srcdir()):\n    for f in subdirs + files:\n      if f.endswith(file_or_dirname):\n        return os.path.join(directory, f)\n  raise ValueError(""No %s in test directory"" % file_or_dirname)\n'"
tensorflow_hub/tf_utils.py,11,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Replicates TensorFlow utilities which are not part of the public API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nimport uuid\n\nfrom absl import logging\nimport tensorflow as tf\n\nfrom tensorflow_hub import tf_v1\n\n# TODO(b/73987364): It is not possible to extend feature columns without\n# depending on TensorFlow internal implementation details.\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.feature_column import feature_column_v2\n# pylint: enable=g-direct-tensorflow-import\n\n\ndef read_file_to_string(filename):\n  """"""Returns the entire contents of a file to a string.\n\n  Args:\n    filename: string, path to a file\n  """"""\n  return tf_v1.gfile.GFile(filename, mode=""r"").read()\n\n\ndef atomic_write_string_to_file(filename, contents, overwrite):\n  """"""Writes to `filename` atomically.\n\n  This means that when `filename` appears in the filesystem, it will contain\n  all of `contents`. With write_string_to_file, it is possible for the file\n  to appear in the filesystem with `contents` only partially written.\n\n  Accomplished by writing to a temp file and then renaming it.\n\n  Args:\n    filename: string, pathname for a file\n    contents: string, contents that need to be written to the file\n    overwrite: boolean, if false it\'s an error for `filename` to be occupied by\n      an existing file.\n  """"""\n  temp_pathname = (tf.compat.as_bytes(filename) +\n                   tf.compat.as_bytes("".tmp"") +\n                   tf.compat.as_bytes(uuid.uuid4().hex))\n  with tf_v1.gfile.GFile(temp_pathname, mode=""w"") as f:\n    f.write(contents)\n  try:\n    tf_v1.gfile.Rename(temp_pathname, filename, overwrite)\n  except tf.errors.OpError:\n    tf_v1.gfile.Remove(temp_pathname)\n    raise\n\n\n# When we create a timestamped directory, there is a small chance that the\n# directory already exists because another worker is also writing exports.\n# In this case we just wait one second to get a new timestamp and try again.\n# If this fails several times in a row, then something is seriously wrong.\nMAX_DIRECTORY_CREATION_ATTEMPTS = 10\n\n\ndef get_timestamped_export_dir(export_dir_base):\n  """"""Builds a path to a new subdirectory within the base directory.\n\n  Each export is written into a new subdirectory named using the\n  current time.  This guarantees monotonically increasing version\n  numbers even across multiple runs of the pipeline.\n  The timestamp used is the number of seconds since epoch UTC.\n\n  Args:\n    export_dir_base: A string containing a directory to write the exported\n        graph and checkpoints.\n  Returns:\n    The full path of the new subdirectory (which is not actually created yet).\n\n  Raises:\n    RuntimeError: if repeated attempts fail to obtain a unique timestamped\n      directory name.\n  """"""\n  attempts = 0\n  while attempts < MAX_DIRECTORY_CREATION_ATTEMPTS:\n    export_timestamp = int(time.time())\n\n    export_dir = os.path.join(\n        tf.compat.as_bytes(export_dir_base),\n        tf.compat.as_bytes(str(export_timestamp)))\n    if not tf_v1.gfile.Exists(export_dir):\n      # Collisions are still possible (though extremely unlikely): this\n      # directory is not actually created yet, but it will be almost\n      # instantly on return from this function.\n      return export_dir\n    time.sleep(1)\n    attempts += 1\n    logging.warn(\n        ""Export directory %s already exists; retrying (attempt %d/%d)"",\n        export_dir, attempts, MAX_DIRECTORY_CREATION_ATTEMPTS)\n  raise RuntimeError(""Failed to obtain a unique export directory name after ""\n                     ""%d attempts."".MAX_DIRECTORY_CREATION_ATTEMPTS)\n\n\ndef get_temp_export_dir(timestamped_export_dir):\n  """"""Builds a directory name based on the argument but starting with \'temp-\'.\n\n  This relies on the fact that TensorFlow Serving ignores subdirectories of\n  the base directory that can\'t be parsed as integers.\n\n  Args:\n    timestamped_export_dir: the name of the eventual export directory, e.g.\n      /foo/bar/<timestamp>\n\n  Returns:\n    A sister directory prefixed with \'temp-\', e.g. /foo/bar/temp-<timestamp>.\n  """"""\n  dirname, basename = os.path.split(tf.compat.as_bytes(timestamped_export_dir))\n  return os.path.join(dirname, b""temp-"" + basename)\n\n\n# Note: This is written from scratch to mimic the pattern in:\n# `tf_v1.estimator.LatestExporter._garbage_collect_exports()`.\ndef garbage_collect_exports(export_dir_base, exports_to_keep):\n  """"""Deletes older exports, retaining only a given number of the most recent.\n\n  Export subdirectories are assumed to be named with monotonically increasing\n  integers; the most recent are taken to be those with the largest values.\n\n  Args:\n    export_dir_base: the base directory under which each export is in a\n      versioned subdirectory.\n    exports_to_keep: Number of exports to keep. Older exports will be garbage\n      collected. Set to None to disable.\n  """"""\n  if exports_to_keep is None:\n    return\n  version_paths = []  # List of tuples (version, path)\n  for filename in tf_v1.gfile.ListDirectory(export_dir_base):\n    path = os.path.join(\n        tf.compat.as_bytes(export_dir_base),\n        tf.compat.as_bytes(filename))\n    if len(filename) == 10 and filename.isdigit():\n      version_paths.append((int(filename), path))\n\n  oldest_version_path = sorted(version_paths)[:-exports_to_keep]\n  for _, path in oldest_version_path:\n    try:\n      tf_v1.gfile.DeleteRecursively(path)\n    except tf.errors.NotFoundError as e:\n      logging.warn(""Can not delete %s recursively: %s"", path, e)\n\n\ndef bytes_to_readable_str(num_bytes, include_b=False):\n  """"""Generate a human-readable string representing number of bytes.\n\n  The units B, kB, MB and GB are used.\n\n  Args:\n    num_bytes: (`int` or None) Number of bytes.\n    include_b: (`bool`) Include the letter B at the end of the unit.\n\n  Returns:\n    (`str`) A string representing the number of bytes in a human-readable way,\n      including a unit at the end.\n  """"""\n\n  if num_bytes is None:\n    return str(num_bytes)\n  if num_bytes < 1024:\n    result = ""%d"" % num_bytes\n  elif num_bytes < 1048576:\n    result = ""%.2fk"" % (num_bytes / float(1 << 10))\n  elif num_bytes < 1073741824:\n    result = ""%.2fM"" % (num_bytes / float(1 << 20))\n  else:\n    result = ""%.2fG"" % (num_bytes / float(1 << 30))\n\n  if include_b:\n    result += ""B""\n  return result\n\n\ndef absolute_path(path):\n  """"""Returns absolute path.\n\n  Args:\n    path: Path to compute absolute path from.\n\n  This implementation avoids calling os.path.abspath(path) if \'path\' already\n  represents an absolute Tensorflow filesystem location (e.g. <fs type>://).\n  """"""\n  return path if b""://"" in tf.compat.as_bytes(path) else os.path.abspath(path)\n\n\ndef fc2_implements_resources():\n  """"""Returns true if imported TF version implements resources for FCv2.""""""\n  if not hasattr(feature_column_v2, ""DenseColumn""):\n    return False\n  if not hasattr(feature_column_v2, ""_StateManagerImpl""):\n    return False\n  state_manager = feature_column_v2._StateManagerImpl(  # pylint: disable=protected-access\n      layer=None, trainable=False)\n  try:\n    state_manager.add_resource(""COLUMN_DUMMY"", ""RESOURCE_DUMMY"", True)\n  except NotImplementedError:\n    return False\n  return True\n'"
tensorflow_hub/tf_v1.py,1,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Util to allow tensorflow_hub to be used both in 1.x and 2.x TensorFlow.\n\nNote: this should not be needed once TF 1.13 is the lowest version to support as\nthat contains the tf.compat.v1 symbol.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n# pylint: disable=g-import-not-at-top\n# pylint: disable=unused-import\ntry:\n  from tensorflow.compat.v1 import *  # pylint: disable=wildcard-import\n  # The previous line also gets us tensorflow.compat.v1.estimator.\n  # Be sure not to import from tensorflow_estimator without version selection.\nexcept ImportError:\n  from tensorflow import add_to_collection\n  from tensorflow import app\n  from tensorflow import assign\n  from tensorflow import assign_add\n  from tensorflow import AttrValue\n  from tensorflow import colocate_with\n  from tensorflow import constant_initializer\n  from tensorflow import convert_to_tensor_or_indexed_slices\n  from tensorflow import estimator\n  from tensorflow import feature_column\n  from tensorflow import FixedLenFeature\n  from tensorflow import fixed_size_partitioner\n  from tensorflow import gather\n  from tensorflow import get_collection\n  from tensorflow import get_collection_ref\n  from tensorflow import get_default_graph\n  from tensorflow import get_variable\n  from tensorflow import get_variable_scope\n  from tensorflow import gfile\n  from tensorflow import global_variables\n  from tensorflow import global_variables_initializer\n  from tensorflow import initializers\n  from tensorflow import Graph\n  from tensorflow import GraphKeys\n  from tensorflow import layers\n  from tensorflow import losses\n  from tensorflow import MetaGraphDef\n  from tensorflow import name_scope\n  from tensorflow import nn\n  from tensorflow import placeholder\n  from tensorflow import regex_replace\n  from tensorflow import reset_default_graph\n  from tensorflow import saved_model\n  from tensorflow import Session\n  from tensorflow import set_random_seed\n  from tensorflow import SparseTensor\n  from tensorflow import SparseTensorValue\n  from tensorflow import sparse_fill_empty_rows\n  from tensorflow import sparse_placeholder\n  from tensorflow import sparse_reset_shape\n  from tensorflow import sparse_split\n  from tensorflow import sparse_tensor_to_dense\n  from tensorflow import string_to_hash_bucket_fast\n  from tensorflow import train\n  from tensorflow import trainable_variables\n  from tensorflow import tables_initializer\n  from tensorflow import variable_scope\n  from tensorflow import zeros_initializer\n# pylint: enable=g-import-not-at-top\n# pylint: enable=unused-import\n'"
tensorflow_hub/version.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the version string.""""""\n\n__version__ = ""0.9.0.dev""\n'"
examples/half_plus_two/export.py,10,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Creates a simple TF-Hub Module.\n\nThe module has a single default signature that computes a*x+b. Where \'a\' and \'b\'\nare variables in the graph. Before export, the Module is ""trained"" by explicitly\nsetting those variables to the magic numbers that make it compute:\n\n  0.5 * x + 2  # Half plus two.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow_hub as hub\n\n\ndef half_plus_two():\n  a = tf.get_variable(""a"", shape=[])\n  b = tf.get_variable(""b"", shape=[])\n  x = tf.placeholder(tf.float32)\n  y = a*x + b\n  hub.add_signature(inputs=x, outputs=y)\n\n\ndef export_module(path):\n  spec = hub.create_module_spec(half_plus_two)\n\n  with tf.Graph().as_default():\n    module = hub.Module(spec)\n\n    init_a = tf.assign(module.variable_map[""a""], 0.5)\n    init_b = tf.assign(module.variable_map[""b""], 2.0)\n    init_vars = tf.group([init_a, init_b])\n\n    with tf.Session() as session:\n      session.run(init_vars)\n      module.export(path, session)\n\n\ndef main(argv):\n  try:\n    _, export_path, = argv\n  except ValueError:\n    raise ValueError(""Usage: %s <export-path>"" % argv[0])\n\n  if tf.gfile.Exists(export_path):\n    raise RuntimeError(""Path %s already exists."" % export_path)\n\n  export_module(export_path)\n\n\nif __name__ == ""__main__"":\n  tf.app.run(main)\n'"
examples/half_plus_two/half_plus_two_test.py,8,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for half plus two TF-Hub example.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport subprocess\nfrom distutils.version import LooseVersion\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow_hub import test_utils\n\nEXPORT_TOOL_PATH = ""org_tensorflow_hub/examples/half_plus_two/export""\n\n\nclass HalfPlusTwoTest(tf.test.TestCase):\n\n  def testExportTool(self):\n    # Use the export tool to create the Module.\n    module_path = os.path.join(self.get_temp_dir(), ""half-plus-two-module"")\n\n    export_tool_path = os.path.join(test_utils.test_srcdir(), EXPORT_TOOL_PATH)\n    self.assertEquals(0, subprocess.call([export_tool_path, module_path]))\n\n    # Test the Module computes (0.5*input + 2).\n    with tf.Graph().as_default():\n      m = hub.Module(module_path)\n      output = m([10, 3, 4])\n      with tf.Session() as session:\n        session.run(tf.initializers.global_variables())\n        self.assertAllEqual(session.run(output), [7, 3.5, 4])\n\n\nif __name__ == ""__main__"":\n  # This test is only supported in TF 1.x.\n  if (LooseVersion(tf.__version__) <\n      LooseVersion(""2.0.0"")):\n    logging.info(""Using TF version: %s"", tf.__version__)\n    tf.test.main()\n  else:\n    logging.warning(""Skipping running tests for TF Version: %s"",\n                    tf.__version__)\n'"
examples/image_retraining/retrain.py,96,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# NOTICE: This work was derived from tensorflow/examples/image_retraining\n# and modified to use TensorFlow Hub modules.\n\n# pylint: disable=line-too-long\nr""""""Simple transfer learning with image modules from TensorFlow Hub.\n\nWARNING: This code is deprecated in favor of\nhttps://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier\n\nThis example shows how to train an image classifier based on any\nTensorFlow Hub module that computes image feature vectors. By default,\nit uses the feature vectors computed by Inception V3 trained on ImageNet.\nFor more options, search https://tfhub.dev for image feature vector modules.\n\nThe top layer receives as input a 2048-dimensional vector (assuming\nInception V3) for each image. We train a softmax layer on top of this\nrepresentation. If the softmax layer contains N labels, this corresponds\nto learning N + 2048*N model parameters for the biases and weights.\n\nHere\'s an example, which assumes you have a folder containing class-named\nsubfolders, each full of images for each label. The example folder flower_photos\nshould have a structure like this:\n\n~/flower_photos/daisy/photo1.jpg\n~/flower_photos/daisy/photo2.jpg\n...\n~/flower_photos/rose/anotherphoto77.jpg\n...\n~/flower_photos/sunflower/somepicture.jpg\n\nThe subfolder names are important, since they define what label is applied to\neach image, but the filenames themselves don\'t matter. (For a working example,\ndownload http://download.tensorflow.org/example_images/flower_photos.tgz\nand run  tar xzf flower_photos.tgz  to unpack it.)\n\nOnce your images are prepared, and you have pip-installed tensorflow-hub and\na sufficiently recent version of tensorflow, you can run the training with a\ncommand like this:\n\n```bash\npython retrain.py --image_dir ~/flower_photos\n```\n\nYou can replace the image_dir argument with any folder containing subfolders of\nimages. The label for each image is taken from the name of the subfolder it\'s\nin.\n\nThis produces a new model file that can be loaded and run by any TensorFlow\nprogram, for example the tensorflow/examples/label_image sample code.\n\nBy default this script will use the highly accurate, but comparatively large and\nslow Inception V3 model architecture. It\'s recommended that you start with this\nto validate that you have gathered good training data, but if you want to deploy\non resource-limited platforms, you can try the `--tfhub_module` flag with a\nMobilenet model. For more information on Mobilenet, see\nhttps://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\n\nFor example:\n\nRun floating-point version of Mobilenet:\n\n```bash\npython retrain.py --image_dir ~/flower_photos \\\n    --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/3\n```\n\nRun Mobilenet, instrumented for quantization:\n\n```bash\npython retrain.py --image_dir ~/flower_photos/ \\\n    --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/quantops/feature_vector/3\n```\n\nThese instrumented models can be converted to fully quantized mobile models via\nTensorFlow Lite.\n\nThere are different Mobilenet models to choose from, with a variety of file\nsize and latency options.\n  - The first number can be \'100\', \'075\', \'050\', or \'025\' to control the number\n    of neurons (activations of hidden layers); the number of weights (and hence\n    to some extent the file size and speed) shrinks with the square of that\n    fraction.\n  - The second number is the input image size. You can choose \'224\', \'192\',\n    \'160\', or \'128\', with smaller sizes giving faster speeds.\n\nTo use with TensorBoard:\n\nBy default, this script will log summaries to /tmp/retrain_logs directory\n\nVisualize the summaries with this command:\n\ntensorboard --logdir /tmp/retrain_logs\n\nTo use with Tensorflow Serving, run this tool with --saved_model_dir set\nto some increasingly numbered export location under the model base path, e.g.:\n\n```bash\npython retrain.py (... other args as before ...) \\\n    --saved_model_dir=/tmp/saved_models/$(date +%s)/\ntensorflow_model_server --port=9000 --model_name=my_image_classifier \\\n    --model_base_path=/tmp/saved_models/\n```\n""""""\n# pylint: enable=line-too-long\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\n\nimport argparse\nimport collections\nfrom datetime import datetime\nimport hashlib\nimport os.path\nimport random\nimport re\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.contrib import quantize as contrib_quantize\n\nFLAGS = None\n\nMAX_NUM_IMAGES_PER_CLASS = 2 ** 27 - 1  # ~134M\n\n# A module is understood as instrumented for quantization with TF-Lite\n# if it contains any of these ops.\nFAKE_QUANT_OPS = (\'FakeQuantWithMinMaxVars\',\n                  \'FakeQuantWithMinMaxVarsPerChannel\')\n\n\ndef create_image_lists(image_dir, testing_percentage, validation_percentage):\n  """"""Builds a list of training images from the file system.\n\n  Analyzes the sub folders in the image directory, splits them into stable\n  training, testing, and validation sets, and returns a data structure\n  describing the lists of images for each label and their paths.\n\n  Args:\n    image_dir: String path to a folder containing subfolders of images.\n    testing_percentage: Integer percentage of the images to reserve for tests.\n    validation_percentage: Integer percentage of images reserved for validation.\n\n  Returns:\n    An OrderedDict containing an entry for each label subfolder, with images\n    split into training, testing, and validation sets within each label.\n    The order of items defines the class indices.\n  """"""\n  if not tf.gfile.Exists(image_dir):\n    logging.error(""Image directory \'"" + image_dir + ""\' not found."")\n    return None\n  result = collections.OrderedDict()\n  sub_dirs = sorted(x[0] for x in tf.gfile.Walk(image_dir))\n  # The root directory comes first, so skip it.\n  is_root_dir = True\n  for sub_dir in sub_dirs:\n    if is_root_dir:\n      is_root_dir = False\n      continue\n    extensions = sorted(set(os.path.normcase(ext)  # Smash case on Windows.\n                            for ext in [\'JPEG\', \'JPG\', \'jpeg\', \'jpg\', \'png\']))\n    file_list = []\n    dir_name = os.path.basename(\n        # tf.gfile.Walk() returns sub-directory with trailing \'/\' when it is in\n        # Google Cloud Storage, which confuses os.path.basename().\n        sub_dir[:-1] if sub_dir.endswith(\'/\') else sub_dir)\n\n    if dir_name == image_dir:\n      continue\n    logging.info(""Looking for images in \'%s\'"",  dir_name)\n    for extension in extensions:\n      file_glob = os.path.join(image_dir, dir_name, \'*.\' + extension)\n      file_list.extend(tf.gfile.Glob(file_glob))\n    if not file_list:\n      logging.warning(\'No files found\')\n      continue\n    if len(file_list) < 20:\n      logging.warning(\n          \'WARNING: Folder has less than 20 images, which may cause issues.\')\n    elif len(file_list) > MAX_NUM_IMAGES_PER_CLASS:\n      logging.warning(\n          \'WARNING: Folder %s has more than %s images. Some images will \'\n          \'never be selected.\', dir_name, MAX_NUM_IMAGES_PER_CLASS)\n    label_name = re.sub(r\'[^a-z0-9]+\', \' \', dir_name.lower())\n    training_images = []\n    testing_images = []\n    validation_images = []\n    for file_name in file_list:\n      base_name = os.path.basename(file_name)\n      # We want to ignore anything after \'_nohash_\' in the file name when\n      # deciding which set to put an image in, the data set creator has a way of\n      # grouping photos that are close variations of each other. For example\n      # this is used in the plant disease data set to group multiple pictures of\n      # the same leaf.\n      hash_name = re.sub(r\'_nohash_.*$\', \'\', file_name)\n      # This looks a bit magical, but we need to decide whether this file should\n      # go into the training, testing, or validation sets, and we want to keep\n      # existing files in the same set even if more files are subsequently\n      # added.\n      # To do that, we need a stable way of deciding based on just the file name\n      # itself, so we do a hash of that and then use that to generate a\n      # probability value that we use to assign it.\n      hash_name_hashed = hashlib.sha1(tf.compat.as_bytes(hash_name)).hexdigest()\n      percentage_hash = ((int(hash_name_hashed, 16) %\n                          (MAX_NUM_IMAGES_PER_CLASS + 1)) *\n                         (100.0 / MAX_NUM_IMAGES_PER_CLASS))\n      if percentage_hash < validation_percentage:\n        validation_images.append(base_name)\n      elif percentage_hash < (testing_percentage + validation_percentage):\n        testing_images.append(base_name)\n      else:\n        training_images.append(base_name)\n    result[label_name] = {\n        \'dir\': dir_name,\n        \'training\': training_images,\n        \'testing\': testing_images,\n        \'validation\': validation_images,\n    }\n  return result\n\n\ndef get_image_path(image_lists, label_name, index, image_dir, category):\n  """"""Returns a path to an image for a label at the given index.\n\n  Args:\n    image_lists: OrderedDict of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Int offset of the image we want. This will be moduloed by the\n    available number of images for the label, so it can be arbitrarily large.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    category: Name string of set to pull images from - training, testing, or\n    validation.\n\n  Returns:\n    File system path string to an image that meets the requested parameters.\n\n  """"""\n  if label_name not in image_lists:\n    logging.fatal(\'Label does not exist %s.\', label_name)\n  label_lists = image_lists[label_name]\n  if category not in label_lists:\n    logging.fatal(\'Category does not exist %s.\', category)\n  category_list = label_lists[category]\n  if not category_list:\n    logging.fatal(\'Label %s has no images in the category %s.\',\n                  label_name, category)\n  mod_index = index % len(category_list)\n  base_name = category_list[mod_index]\n  sub_dir = label_lists[\'dir\']\n  full_path = os.path.join(image_dir, sub_dir, base_name)\n  return full_path\n\n\ndef get_bottleneck_path(image_lists, label_name, index, bottleneck_dir,\n                        category, module_name):\n  """"""Returns a path to a bottleneck file for a label at the given index.\n\n  Args:\n    image_lists: OrderedDict of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Integer offset of the image we want. This will be moduloed by the\n    available number of images for the label, so it can be arbitrarily large.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    category: Name string of set to pull images from - training, testing, or\n    validation.\n    module_name: The name of the image module being used.\n\n  Returns:\n    File system path string to an image that meets the requested parameters.\n  """"""\n  module_name = (module_name.replace(\'://\', \'~\')  # URL scheme.\n                 .replace(\'/\', \'~\')  # URL and Unix paths.\n                 .replace(\':\', \'~\').replace(\'\\\\\', \'~\'))  # Windows paths.\n  return get_image_path(image_lists, label_name, index, bottleneck_dir,\n                        category) + \'_\' + module_name + \'.txt\'\n\n\ndef create_module_graph(module_spec):\n  """"""Creates a graph and loads Hub Module into it.\n\n  Args:\n    module_spec: the hub.ModuleSpec for the image module being used.\n\n  Returns:\n    graph: the tf.Graph that was created.\n    bottleneck_tensor: the bottleneck values output by the module.\n    resized_input_tensor: the input images, resized as expected by the module.\n    wants_quantization: a boolean, whether the module has been instrumented\n      with fake quantization ops.\n  """"""\n  height, width = hub.get_expected_image_size(module_spec)\n  with tf.Graph().as_default() as graph:\n    resized_input_tensor = tf.placeholder(tf.float32, [None, height, width, 3])\n    m = hub.Module(module_spec)\n    bottleneck_tensor = m(resized_input_tensor)\n    wants_quantization = any(node.op in FAKE_QUANT_OPS\n                             for node in graph.as_graph_def().node)\n  return graph, bottleneck_tensor, resized_input_tensor, wants_quantization\n\n\ndef run_bottleneck_on_image(sess, image_data, image_data_tensor,\n                            decoded_image_tensor, resized_input_tensor,\n                            bottleneck_tensor):\n  """"""Runs inference on an image to extract the \'bottleneck\' summary layer.\n\n  Args:\n    sess: Current active TensorFlow Session.\n    image_data: String of raw JPEG data.\n    image_data_tensor: Input data layer in the graph.\n    decoded_image_tensor: Output of initial image resizing and preprocessing.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: Layer before the final softmax.\n\n  Returns:\n    Numpy array of bottleneck values.\n  """"""\n  # First decode the JPEG image, resize it, and rescale the pixel values.\n  resized_input_values = sess.run(decoded_image_tensor,\n                                  {image_data_tensor: image_data})\n  # Then run it through the recognition network.\n  bottleneck_values = sess.run(bottleneck_tensor,\n                               {resized_input_tensor: resized_input_values})\n  bottleneck_values = np.squeeze(bottleneck_values)\n  return bottleneck_values\n\n\ndef ensure_dir_exists(dir_name):\n  """"""Makes sure the folder exists on disk.\n\n  Args:\n    dir_name: Path string to the folder we want to create.\n  """"""\n  if not os.path.exists(dir_name):\n    os.makedirs(dir_name)\n\n\ndef create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n                           image_dir, category, sess, jpeg_data_tensor,\n                           decoded_image_tensor, resized_input_tensor,\n                           bottleneck_tensor):\n  """"""Create a single bottleneck file.""""""\n  logging.debug(\'Creating bottleneck at %s\', bottleneck_path)\n  image_path = get_image_path(image_lists, label_name, index,\n                              image_dir, category)\n  if not tf.gfile.Exists(image_path):\n    logging.fatal(\'File does not exist %s\', image_path)\n  image_data = tf.gfile.GFile(image_path, \'rb\').read()\n  try:\n    bottleneck_values = run_bottleneck_on_image(\n        sess, image_data, jpeg_data_tensor, decoded_image_tensor,\n        resized_input_tensor, bottleneck_tensor)\n  except Exception as e:\n    raise RuntimeError(\'Error during processing file %s (%s)\' % (image_path,\n                                                                 str(e)))\n  bottleneck_string = \',\'.join(str(x) for x in bottleneck_values)\n  with tf.gfile.GFile(bottleneck_path, \'w\') as bottleneck_file:\n    bottleneck_file.write(bottleneck_string)\n\n\ndef get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir,\n                             category, bottleneck_dir, jpeg_data_tensor,\n                             decoded_image_tensor, resized_input_tensor,\n                             bottleneck_tensor, module_name):\n  """"""Retrieves or calculates bottleneck values for an image.\n\n  If a cached version of the bottleneck data exists on-disk, return that,\n  otherwise calculate the data and save it to disk for future use.\n\n  Args:\n    sess: The current active TensorFlow Session.\n    image_lists: OrderedDict of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Integer offset of the image we want. This will be modulo-ed by the\n    available number of images for the label, so it can be arbitrarily large.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    category: Name string of which set to pull images from - training, testing,\n    or validation.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    jpeg_data_tensor: The tensor to feed loaded jpeg data into.\n    decoded_image_tensor: The output of decoding and resizing the image.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The output tensor for the bottleneck values.\n    module_name: The name of the image module being used.\n\n  Returns:\n    Numpy array of values produced by the bottleneck layer for the image.\n  """"""\n  label_lists = image_lists[label_name]\n  sub_dir = label_lists[\'dir\']\n  sub_dir_path = os.path.join(bottleneck_dir, sub_dir)\n  ensure_dir_exists(sub_dir_path)\n  bottleneck_path = get_bottleneck_path(image_lists, label_name, index,\n                                        bottleneck_dir, category, module_name)\n  if not os.path.exists(bottleneck_path):\n    create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n                           image_dir, category, sess, jpeg_data_tensor,\n                           decoded_image_tensor, resized_input_tensor,\n                           bottleneck_tensor)\n  with tf.gfile.GFile(bottleneck_path, \'r\') as bottleneck_file:\n    bottleneck_string = bottleneck_file.read()\n  did_hit_error = False\n  try:\n    bottleneck_values = [float(x) for x in bottleneck_string.split(\',\')]\n  except ValueError:\n    logging.warning(\'Invalid float found, recreating bottleneck\')\n    did_hit_error = True\n  if did_hit_error:\n    create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n                           image_dir, category, sess, jpeg_data_tensor,\n                           decoded_image_tensor, resized_input_tensor,\n                           bottleneck_tensor)\n    with tf.gfile.GFile(bottleneck_path, \'r\') as bottleneck_file:\n      bottleneck_string = bottleneck_file.read()\n    # Allow exceptions to propagate here, since they shouldn\'t happen after a\n    # fresh creation\n    bottleneck_values = [float(x) for x in bottleneck_string.split(\',\')]\n  return bottleneck_values\n\n\ndef cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir,\n                      jpeg_data_tensor, decoded_image_tensor,\n                      resized_input_tensor, bottleneck_tensor, module_name):\n  """"""Ensures all the training, testing, and validation bottlenecks are cached.\n\n  Because we\'re likely to read the same image multiple times (if there are no\n  distortions applied during training) it can speed things up a lot if we\n  calculate the bottleneck layer values once for each image during\n  preprocessing, and then just read those cached values repeatedly during\n  training. Here we go through all the images we\'ve found, calculate those\n  values, and save them off.\n\n  Args:\n    sess: The current active TensorFlow Session.\n    image_lists: OrderedDict of training images for each label.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    jpeg_data_tensor: Input tensor for jpeg data from file.\n    decoded_image_tensor: The output of decoding and resizing the image.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The penultimate output layer of the graph.\n    module_name: The name of the image module being used.\n\n  Returns:\n    Nothing.\n  """"""\n  how_many_bottlenecks = 0\n  ensure_dir_exists(bottleneck_dir)\n  for label_name, label_lists in image_lists.items():\n    for category in [\'training\', \'testing\', \'validation\']:\n      category_list = label_lists[category]\n      for index, unused_base_name in enumerate(category_list):\n        get_or_create_bottleneck(\n            sess, image_lists, label_name, index, image_dir, category,\n            bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n            resized_input_tensor, bottleneck_tensor, module_name)\n\n        how_many_bottlenecks += 1\n        if how_many_bottlenecks % 100 == 0:\n          logging.info(\'%s bottleneck files created.\', how_many_bottlenecks)\n\n\ndef get_random_cached_bottlenecks(sess, image_lists, how_many, category,\n                                  bottleneck_dir, image_dir, jpeg_data_tensor,\n                                  decoded_image_tensor, resized_input_tensor,\n                                  bottleneck_tensor, module_name):\n  """"""Retrieves bottleneck values for cached images.\n\n  If no distortions are being applied, this function can retrieve the cached\n  bottleneck values directly from disk for images. It picks a random set of\n  images from the specified category.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: OrderedDict of training images for each label.\n    how_many: If positive, a random sample of this size will be chosen.\n    If negative, all bottlenecks will be retrieved.\n    category: Name string of which set to pull from - training, testing, or\n    validation.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    jpeg_data_tensor: The layer to feed jpeg image data into.\n    decoded_image_tensor: The output of decoding and resizing the image.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n    module_name: The name of the image module being used.\n\n  Returns:\n    List of bottleneck arrays, their corresponding ground truths, and the\n    relevant filenames.\n  """"""\n  class_count = len(image_lists.keys())\n  bottlenecks = []\n  ground_truths = []\n  filenames = []\n  if how_many >= 0:\n    # Retrieve a random sample of bottlenecks.\n    for unused_i in range(how_many):\n      label_index = random.randrange(class_count)\n      label_name = list(image_lists.keys())[label_index]\n      image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n      image_name = get_image_path(image_lists, label_name, image_index,\n                                  image_dir, category)\n      bottleneck = get_or_create_bottleneck(\n          sess, image_lists, label_name, image_index, image_dir, category,\n          bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n          resized_input_tensor, bottleneck_tensor, module_name)\n      bottlenecks.append(bottleneck)\n      ground_truths.append(label_index)\n      filenames.append(image_name)\n  else:\n    # Retrieve all bottlenecks.\n    for label_index, label_name in enumerate(image_lists.keys()):\n      for image_index, image_name in enumerate(\n          image_lists[label_name][category]):\n        image_name = get_image_path(image_lists, label_name, image_index,\n                                    image_dir, category)\n        bottleneck = get_or_create_bottleneck(\n            sess, image_lists, label_name, image_index, image_dir, category,\n            bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n            resized_input_tensor, bottleneck_tensor, module_name)\n        bottlenecks.append(bottleneck)\n        ground_truths.append(label_index)\n        filenames.append(image_name)\n  return bottlenecks, ground_truths, filenames\n\n\ndef get_random_distorted_bottlenecks(\n    sess, image_lists, how_many, category, image_dir, input_jpeg_tensor,\n    distorted_image, resized_input_tensor, bottleneck_tensor):\n  """"""Retrieves bottleneck values for training images, after distortions.\n\n  If we\'re training with distortions like crops, scales, or flips, we have to\n  recalculate the full model for every image, and so we can\'t use cached\n  bottleneck values. Instead we find random images for the requested category,\n  run them through the distortion graph, and then the full graph to get the\n  bottleneck results for each.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: OrderedDict of training images for each label.\n    how_many: The integer number of bottleneck values to return.\n    category: Name string of which set of images to fetch - training, testing,\n    or validation.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    input_jpeg_tensor: The input layer we feed the image data to.\n    distorted_image: The output node of the distortion graph.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n\n  Returns:\n    List of bottleneck arrays and their corresponding ground truths.\n  """"""\n  class_count = len(image_lists.keys())\n  bottlenecks = []\n  ground_truths = []\n  for unused_i in range(how_many):\n    label_index = random.randrange(class_count)\n    label_name = list(image_lists.keys())[label_index]\n    image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n    image_path = get_image_path(image_lists, label_name, image_index, image_dir,\n                                category)\n    if not tf.gfile.Exists(image_path):\n      logging.fatal(\'File does not exist %s\', image_path)\n    jpeg_data = tf.gfile.GFile(image_path, \'rb\').read()\n    # Note that we materialize the distorted_image_data as a numpy array before\n    # sending running inference on the image. This involves 2 memory copies and\n    # might be optimized in other implementations.\n    distorted_image_data = sess.run(distorted_image,\n                                    {input_jpeg_tensor: jpeg_data})\n    bottleneck_values = sess.run(bottleneck_tensor,\n                                 {resized_input_tensor: distorted_image_data})\n    bottleneck_values = np.squeeze(bottleneck_values)\n    bottlenecks.append(bottleneck_values)\n    ground_truths.append(label_index)\n  return bottlenecks, ground_truths\n\n\ndef should_distort_images(flip_left_right, random_crop, random_scale,\n                          random_brightness):\n  """"""Whether any distortions are enabled, from the input flags.\n\n  Args:\n    flip_left_right: Boolean whether to randomly mirror images horizontally.\n    random_crop: Integer percentage setting the total margin used around the\n    crop box.\n    random_scale: Integer percentage of how much to vary the scale by.\n    random_brightness: Integer range to randomly multiply the pixel values by.\n\n  Returns:\n    Boolean value indicating whether any distortions should be applied.\n  """"""\n  return (flip_left_right or (random_crop != 0) or (random_scale != 0) or\n          (random_brightness != 0))\n\n\ndef add_input_distortions(flip_left_right, random_crop, random_scale,\n                          random_brightness, module_spec):\n  """"""Creates the operations to apply the specified distortions.\n\n  During training it can help to improve the results if we run the images\n  through simple distortions like crops, scales, and flips. These reflect the\n  kind of variations we expect in the real world, and so can help train the\n  model to cope with natural data more effectively. Here we take the supplied\n  parameters and construct a network of operations to apply them to an image.\n\n  Cropping\n  ~~~~~~~~\n\n  Cropping is done by placing a bounding box at a random position in the full\n  image. The cropping parameter controls the size of that box relative to the\n  input image. If it\'s zero, then the box is the same size as the input and no\n  cropping is performed. If the value is 50%, then the crop box will be half the\n  width and height of the input. In a diagram it looks like this:\n\n  <       width         >\n  +---------------------+\n  |                     |\n  |   width - crop%     |\n  |    <      >         |\n  |    +------+         |\n  |    |      |         |\n  |    |      |         |\n  |    |      |         |\n  |    +------+         |\n  |                     |\n  |                     |\n  +---------------------+\n\n  Scaling\n  ~~~~~~~\n\n  Scaling is a lot like cropping, except that the bounding box is always\n  centered and its size varies randomly within the given range. For example if\n  the scale percentage is zero, then the bounding box is the same size as the\n  input and no scaling is applied. If it\'s 50%, then the bounding box will be in\n  a random range between half the width and height and full size.\n\n  Args:\n    flip_left_right: Boolean whether to randomly mirror images horizontally.\n    random_crop: Integer percentage setting the total margin used around the\n    crop box.\n    random_scale: Integer percentage of how much to vary the scale by.\n    random_brightness: Integer range to randomly multiply the pixel values by.\n    graph.\n    module_spec: The hub.ModuleSpec for the image module being used.\n\n  Returns:\n    The jpeg input layer and the distorted result tensor.\n  """"""\n  input_height, input_width = hub.get_expected_image_size(module_spec)\n  input_depth = hub.get_num_image_channels(module_spec)\n  jpeg_data = tf.placeholder(tf.string, name=\'DistortJPGInput\')\n  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n  # Convert from full range of uint8 to range [0,1] of float32.\n  decoded_image_as_float = tf.image.convert_image_dtype(decoded_image,\n                                                        tf.float32)\n  decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n  margin_scale = 1.0 + (random_crop / 100.0)\n  resize_scale = 1.0 + (random_scale / 100.0)\n  margin_scale_value = tf.constant(margin_scale)\n  resize_scale_value = tf.random_uniform(shape=[],\n                                         minval=1.0,\n                                         maxval=resize_scale)\n  scale_value = tf.multiply(margin_scale_value, resize_scale_value)\n  precrop_width = tf.multiply(scale_value, input_width)\n  precrop_height = tf.multiply(scale_value, input_height)\n  precrop_shape = tf.stack([precrop_height, precrop_width])\n  precrop_shape_as_int = tf.cast(precrop_shape, dtype=tf.int32)\n  precropped_image = tf.image.resize_bilinear(decoded_image_4d,\n                                              precrop_shape_as_int)\n  precropped_image_3d = tf.squeeze(precropped_image, axis=[0])\n  cropped_image = tf.random_crop(precropped_image_3d,\n                                 [input_height, input_width, input_depth])\n  if flip_left_right:\n    flipped_image = tf.image.random_flip_left_right(cropped_image)\n  else:\n    flipped_image = cropped_image\n  brightness_min = 1.0 - (random_brightness / 100.0)\n  brightness_max = 1.0 + (random_brightness / 100.0)\n  brightness_value = tf.random_uniform(shape=[],\n                                       minval=brightness_min,\n                                       maxval=brightness_max)\n  brightened_image = tf.multiply(flipped_image, brightness_value)\n  distort_result = tf.expand_dims(brightened_image, 0, name=\'DistortResult\')\n  return jpeg_data, distort_result\n\n\ndef variable_summaries(var):\n  """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n  with tf.name_scope(\'summaries\'):\n    mean = tf.reduce_mean(var)\n    tf.summary.scalar(\'mean\', mean)\n    with tf.name_scope(\'stddev\'):\n      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n    tf.summary.scalar(\'stddev\', stddev)\n    tf.summary.scalar(\'max\', tf.reduce_max(var))\n    tf.summary.scalar(\'min\', tf.reduce_min(var))\n    tf.summary.histogram(\'histogram\', var)\n\n\ndef add_final_retrain_ops(class_count, final_tensor_name, bottleneck_tensor,\n                          quantize_layer, is_training):\n  """"""Adds a new softmax and fully-connected layer for training and eval.\n\n  We need to retrain the top layer to identify our new classes, so this function\n  adds the right operations to the graph, along with some variables to hold the\n  weights, and then sets up all the gradients for the backward pass.\n\n  The set up for the softmax and fully-connected layers is based on:\n  https://www.tensorflow.org/tutorials/mnist/beginners/index.html\n\n  Args:\n    class_count: Integer of how many categories of things we\'re trying to\n        recognize.\n    final_tensor_name: Name string for the new final node that produces results.\n    bottleneck_tensor: The output of the main CNN graph.\n    quantize_layer: Boolean, specifying whether the newly added layer should be\n        instrumented for quantization with TF-Lite.\n    is_training: Boolean, specifying whether the newly add layer is for training\n        or eval.\n\n  Returns:\n    The tensors for the training and cross entropy results, and tensors for the\n    bottleneck input and ground truth input.\n  """"""\n  batch_size, bottleneck_tensor_size = bottleneck_tensor.get_shape().as_list()\n  assert batch_size is None, \'We want to work with arbitrary batch size.\'\n  with tf.name_scope(\'input\'):\n    bottleneck_input = tf.placeholder_with_default(\n        bottleneck_tensor,\n        shape=[batch_size, bottleneck_tensor_size],\n        name=\'BottleneckInputPlaceholder\')\n\n    ground_truth_input = tf.placeholder(\n        tf.int64, [batch_size], name=\'GroundTruthInput\')\n\n  # Organizing the following ops so they are easier to see in TensorBoard.\n  layer_name = \'final_retrain_ops\'\n  with tf.name_scope(layer_name):\n    with tf.name_scope(\'weights\'):\n      initial_value = tf.truncated_normal(\n          [bottleneck_tensor_size, class_count], stddev=0.001)\n      layer_weights = tf.Variable(initial_value, name=\'final_weights\')\n      variable_summaries(layer_weights)\n\n    with tf.name_scope(\'biases\'):\n      layer_biases = tf.Variable(tf.zeros([class_count]), name=\'final_biases\')\n      variable_summaries(layer_biases)\n\n    with tf.name_scope(\'Wx_plus_b\'):\n      logits = tf.matmul(bottleneck_input, layer_weights) + layer_biases\n      tf.summary.histogram(\'pre_activations\', logits)\n\n  final_tensor = tf.nn.softmax(logits, name=final_tensor_name)\n\n  # The tf.contrib.quantize functions rewrite the graph in place for\n  # quantization. The imported model graph has already been rewritten, so upon\n  # calling these rewrites, only the newly added final layer will be\n  # transformed.\n  if quantize_layer:\n    if is_training:\n      contrib_quantize.create_training_graph()\n    else:\n      contrib_quantize.create_eval_graph()\n\n  tf.summary.histogram(\'activations\', final_tensor)\n\n  # If this is an eval graph, we don\'t need to add loss ops or an optimizer.\n  if not is_training:\n    return None, None, bottleneck_input, ground_truth_input, final_tensor\n\n  with tf.name_scope(\'cross_entropy\'):\n    cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(\n        labels=ground_truth_input, logits=logits)\n\n  tf.summary.scalar(\'cross_entropy\', cross_entropy_mean)\n\n  with tf.name_scope(\'train\'):\n    optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n    train_step = optimizer.minimize(cross_entropy_mean)\n\n  return (train_step, cross_entropy_mean, bottleneck_input, ground_truth_input,\n          final_tensor)\n\n\ndef add_evaluation_step(result_tensor, ground_truth_tensor):\n  """"""Inserts the operations we need to evaluate the accuracy of our results.\n\n  Args:\n    result_tensor: The new final node that produces results.\n    ground_truth_tensor: The node we feed ground truth data\n    into.\n\n  Returns:\n    Tuple of (evaluation step, prediction).\n  """"""\n  with tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n      prediction = tf.argmax(result_tensor, 1)\n      correct_prediction = tf.equal(prediction, ground_truth_tensor)\n    with tf.name_scope(\'accuracy\'):\n      evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  tf.summary.scalar(\'accuracy\', evaluation_step)\n  return evaluation_step, prediction\n\n\ndef run_final_eval(train_session, module_spec, class_count, image_lists,\n                   jpeg_data_tensor, decoded_image_tensor,\n                   resized_image_tensor, bottleneck_tensor):\n  """"""Runs a final evaluation on an eval graph using the test data set.\n\n  Args:\n    train_session: Session for the train graph with the tensors below.\n    module_spec: The hub.ModuleSpec for the image module being used.\n    class_count: Number of classes\n    image_lists: OrderedDict of training images for each label.\n    jpeg_data_tensor: The layer to feed jpeg image data into.\n    decoded_image_tensor: The output of decoding and resizing the image.\n    resized_image_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n  """"""\n  test_bottlenecks, test_ground_truth, test_filenames = (\n      get_random_cached_bottlenecks(train_session, image_lists,\n                                    FLAGS.test_batch_size,\n                                    \'testing\', FLAGS.bottleneck_dir,\n                                    FLAGS.image_dir, jpeg_data_tensor,\n                                    decoded_image_tensor, resized_image_tensor,\n                                    bottleneck_tensor, FLAGS.tfhub_module))\n\n  (eval_session, _, bottleneck_input, ground_truth_input, evaluation_step,\n   prediction) = build_eval_session(module_spec, class_count)\n  test_accuracy, predictions = eval_session.run(\n      [evaluation_step, prediction],\n      feed_dict={\n          bottleneck_input: test_bottlenecks,\n          ground_truth_input: test_ground_truth\n      })\n  logging.info(\'Final test accuracy = %.1f%% (N=%d)\',\n               test_accuracy * 100, len(test_bottlenecks))\n\n  if FLAGS.print_misclassified_test_images:\n    logging.info(\'=== MISCLASSIFIED TEST IMAGES ===\')\n    for i, test_filename in enumerate(test_filenames):\n      if predictions[i] != test_ground_truth[i]:\n        logging.info(\'%70s  %s\', test_filename,\n                     list(image_lists.keys())[predictions[i]])\n\n\ndef build_eval_session(module_spec, class_count):\n  """"""Builds an restored eval session without train operations for exporting.\n\n  Args:\n    module_spec: The hub.ModuleSpec for the image module being used.\n    class_count: Number of classes\n\n  Returns:\n    Eval session containing the restored eval graph.\n    The bottleneck input, ground truth, eval step, and prediction tensors.\n  """"""\n  # If quantized, we need to create the correct eval graph for exporting.\n  eval_graph, bottleneck_tensor, resized_input_tensor, wants_quantization = (\n      create_module_graph(module_spec))\n\n  eval_sess = tf.Session(graph=eval_graph)\n  with eval_graph.as_default():\n    # Add the new layer for exporting.\n    (_, _, bottleneck_input,\n     ground_truth_input, final_tensor) = add_final_retrain_ops(\n         class_count, FLAGS.final_tensor_name, bottleneck_tensor,\n         wants_quantization, is_training=False)\n\n    # Now we need to restore the values from the training graph to the eval\n    # graph.\n    tf.train.Saver().restore(eval_sess, FLAGS.checkpoint_path)\n\n    evaluation_step, prediction = add_evaluation_step(final_tensor,\n                                                      ground_truth_input)\n\n  return (eval_sess, resized_input_tensor, bottleneck_input, ground_truth_input,\n          evaluation_step, prediction)\n\n\ndef save_graph_to_file(graph_file_name, module_spec, class_count):\n  """"""Saves an graph to file, creating a valid quantized one if necessary.""""""\n  sess, _, _, _, _, _ = build_eval_session(module_spec, class_count)\n  graph = sess.graph\n\n  output_graph_def = tf.graph_util.convert_variables_to_constants(\n      sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n\n  with tf.gfile.GFile(graph_file_name, \'wb\') as f:\n    f.write(output_graph_def.SerializeToString())\n\n\ndef prepare_file_system():\n  # Set up the directory we\'ll write summaries to for TensorBoard\n  if tf.gfile.Exists(FLAGS.summaries_dir):\n    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n  tf.gfile.MakeDirs(FLAGS.summaries_dir)\n  if FLAGS.intermediate_store_frequency > 0:\n    ensure_dir_exists(FLAGS.intermediate_output_graphs_dir)\n  return\n\n\ndef add_jpeg_decoding(module_spec):\n  """"""Adds operations that perform JPEG decoding and resizing to the graph..\n\n  Args:\n    module_spec: The hub.ModuleSpec for the image module being used.\n\n  Returns:\n    Tensors for the node to feed JPEG data into, and the output of the\n      preprocessing steps.\n  """"""\n  input_height, input_width = hub.get_expected_image_size(module_spec)\n  input_depth = hub.get_num_image_channels(module_spec)\n  jpeg_data = tf.placeholder(tf.string, name=\'DecodeJPGInput\')\n  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n  # Convert from full range of uint8 to range [0,1] of float32.\n  decoded_image_as_float = tf.image.convert_image_dtype(decoded_image,\n                                                        tf.float32)\n  decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n  resize_shape = tf.stack([input_height, input_width])\n  resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)\n  resized_image = tf.image.resize_bilinear(decoded_image_4d,\n                                           resize_shape_as_int)\n  return jpeg_data, resized_image\n\n\ndef export_model(module_spec, class_count, saved_model_dir):\n  """"""Exports model for serving.\n\n  Args:\n    module_spec: The hub.ModuleSpec for the image module being used.\n    class_count: The number of classes.\n    saved_model_dir: Directory in which to save exported model and variables.\n  """"""\n  # The SavedModel should hold the eval graph.\n  sess, in_image, _, _, _, _ = build_eval_session(module_spec, class_count)\n  with sess.graph.as_default() as graph:\n    tf.saved_model.simple_save(\n        sess,\n        saved_model_dir,\n        inputs={\'image\': in_image},\n        outputs={\'prediction\': graph.get_tensor_by_name(\'final_result:0\')},\n        legacy_init_op=tf.group(tf.tables_initializer(), name=\'legacy_init_op\')\n    )\n\n\ndef logging_level_verbosity(logging_verbosity):\n  """"""Converts logging_level into TensorFlow logging verbosity value.\n\n  Args:\n    logging_verbosity: String value representing logging level: \'DEBUG\', \'INFO\',\n    \'WARN\', \'ERROR\', \'FATAL\'\n  """"""\n  name_to_level = {\n      \'FATAL\': logging.FATAL,\n      \'ERROR\': logging.ERROR,\n      \'WARN\': logging.WARN,\n      \'INFO\': logging.INFO,\n      \'DEBUG\': logging.DEBUG\n  }\n\n  try:\n    return name_to_level[logging_verbosity]\n  except Exception as e:\n    raise RuntimeError(\'Not supported logs verbosity (%s). Use one of %s.\' %\n                       (str(e), list(name_to_level)))\n\n\ndef main(_):\n  # Needed to make sure the logging output is visible.\n  # See https://github.com/tensorflow/tensorflow/issues/3047\n  logging_verbosity = logging_level_verbosity(FLAGS.logging_verbosity)\n  logging.set_verbosity(logging_verbosity)\n\n  logging.error(\'WARNING: This tool is deprecated in favor of \'\n                \'https://github.com/tensorflow/hub/tree/master/\'\n                \'tensorflow_hub/tools/make_image_classifier\')\n\n  if not FLAGS.image_dir:\n    logging.error(\'Must set flag --image_dir.\')\n    return -1\n\n  # Prepare necessary directories that can be used during training\n  prepare_file_system()\n\n  # Look at the folder structure, and create lists of all the images.\n  image_lists = create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,\n                                   FLAGS.validation_percentage)\n  class_count = len(image_lists.keys())\n  if class_count == 0:\n    logging.error(\'No valid folders of images found at %s\', FLAGS.image_dir)\n    return -1\n  if class_count == 1:\n    logging.error(\'Only one valid folder of images found at %s \'\n                  \' - multiple classes are needed for classification.\',\n                  FLAGS.image_dir)\n    return -1\n\n  # See if the command-line flags mean we\'re applying any distortions.\n  do_distort_images = should_distort_images(\n      FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n      FLAGS.random_brightness)\n\n  # Set up the pre-trained graph.\n  module_spec = hub.load_module_spec(FLAGS.tfhub_module)\n  graph, bottleneck_tensor, resized_image_tensor, wants_quantization = (\n      create_module_graph(module_spec))\n\n  # Add the new layer that we\'ll be training.\n  with graph.as_default():\n    (train_step, cross_entropy, bottleneck_input,\n     ground_truth_input, final_tensor) = add_final_retrain_ops(\n         class_count, FLAGS.final_tensor_name, bottleneck_tensor,\n         wants_quantization, is_training=True)\n\n  with tf.Session(graph=graph) as sess:\n    # Initialize all weights: for the module to their pretrained values,\n    # and for the newly added retraining layer to random initial values.\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    # Set up the image decoding sub-graph.\n    jpeg_data_tensor, decoded_image_tensor = add_jpeg_decoding(module_spec)\n\n    if do_distort_images:\n      # We will be applying distortions, so set up the operations we\'ll need.\n      (distorted_jpeg_data_tensor,\n       distorted_image_tensor) = add_input_distortions(\n           FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n           FLAGS.random_brightness, module_spec)\n    else:\n      # We\'ll make sure we\'ve calculated the \'bottleneck\' image summaries and\n      # cached them on disk.\n      cache_bottlenecks(sess, image_lists, FLAGS.image_dir,\n                        FLAGS.bottleneck_dir, jpeg_data_tensor,\n                        decoded_image_tensor, resized_image_tensor,\n                        bottleneck_tensor, FLAGS.tfhub_module)\n\n    # Create the operations we need to evaluate the accuracy of our new layer.\n    evaluation_step, _ = add_evaluation_step(final_tensor, ground_truth_input)\n\n    # Merge all the summaries and write them out to the summaries_dir\n    merged = tf.summary.merge_all()\n    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + \'/train\',\n                                         sess.graph)\n\n    validation_writer = tf.summary.FileWriter(\n        FLAGS.summaries_dir + \'/validation\')\n\n    # Create a train saver that is used to restore values into an eval graph\n    # when exporting models.\n    train_saver = tf.train.Saver()\n\n    # Run the training for as many cycles as requested on the command line.\n    for i in range(FLAGS.how_many_training_steps):\n      # Get a batch of input bottleneck values, either calculated fresh every\n      # time with distortions applied, or from the cache stored on disk.\n      if do_distort_images:\n        (train_bottlenecks,\n         train_ground_truth) = get_random_distorted_bottlenecks(\n             sess, image_lists, FLAGS.train_batch_size, \'training\',\n             FLAGS.image_dir, distorted_jpeg_data_tensor,\n             distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\n      else:\n        (train_bottlenecks,\n         train_ground_truth, _) = get_random_cached_bottlenecks(\n             sess, image_lists, FLAGS.train_batch_size, \'training\',\n             FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n             decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n             FLAGS.tfhub_module)\n      # Feed the bottlenecks and ground truth into the graph, and run a training\n      # step. Capture training summaries for TensorBoard with the `merged` op.\n      train_summary, _ = sess.run(\n          [merged, train_step],\n          feed_dict={bottleneck_input: train_bottlenecks,\n                     ground_truth_input: train_ground_truth})\n      train_writer.add_summary(train_summary, i)\n\n      # Every so often, print out how well the graph is training.\n      is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n      if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n        train_accuracy, cross_entropy_value = sess.run(\n            [evaluation_step, cross_entropy],\n            feed_dict={bottleneck_input: train_bottlenecks,\n                       ground_truth_input: train_ground_truth})\n        logging.info(\'%s: Step %d: Train accuracy = %.1f%%\',\n                     datetime.now(), i, train_accuracy * 100)\n        logging.info(\'%s: Step %d: Cross entropy = %f\',\n                     datetime.now(), i, cross_entropy_value)\n        # TODO: Make this use an eval graph, to avoid quantization\n        # moving averages being updated by the validation set, though in\n        # practice this makes a negligable difference.\n        validation_bottlenecks, validation_ground_truth, _ = (\n            get_random_cached_bottlenecks(\n                sess, image_lists, FLAGS.validation_batch_size, \'validation\',\n                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n                decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n                FLAGS.tfhub_module))\n        # Run a validation step and capture training summaries for TensorBoard\n        # with the `merged` op.\n        validation_summary, validation_accuracy = sess.run(\n            [merged, evaluation_step],\n            feed_dict={bottleneck_input: validation_bottlenecks,\n                       ground_truth_input: validation_ground_truth})\n        validation_writer.add_summary(validation_summary, i)\n        logging.info(\'%s: Step %d: Validation accuracy = %.1f%% (N=%d)\',\n                     datetime.now(), i, validation_accuracy * 100,\n                     len(validation_bottlenecks))\n\n      # Store intermediate results\n      intermediate_frequency = FLAGS.intermediate_store_frequency\n\n      if (intermediate_frequency > 0 and (i % intermediate_frequency == 0)\n          and i > 0):\n        # If we want to do an intermediate save, save a checkpoint of the train\n        # graph, to restore into the eval graph.\n        train_saver.save(sess, FLAGS.checkpoint_path)\n        intermediate_file_name = (FLAGS.intermediate_output_graphs_dir +\n                                  \'intermediate_\' + str(i) + \'.pb\')\n        logging.info(\'Save intermediate result to : %s\', intermediate_file_name)\n        save_graph_to_file(intermediate_file_name, module_spec,\n                           class_count)\n\n    # After training is complete, force one last save of the train checkpoint.\n    train_saver.save(sess, FLAGS.checkpoint_path)\n\n    # We\'ve completed all our training, so run a final test evaluation on\n    # some new images we haven\'t used before.\n    run_final_eval(sess, module_spec, class_count, image_lists,\n                   jpeg_data_tensor, decoded_image_tensor, resized_image_tensor,\n                   bottleneck_tensor)\n\n    # Write out the trained graph and labels with the weights stored as\n    # constants.\n    logging.info(\'Save final result to : %s\', FLAGS.output_graph)\n    if wants_quantization:\n      logging.info(\'The model is instrumented for quantization with TF-Lite\')\n    save_graph_to_file(FLAGS.output_graph, module_spec, class_count)\n    with tf.gfile.GFile(FLAGS.output_labels, \'w\') as f:\n      f.write(\'\\n\'.join(image_lists.keys()) + \'\\n\')\n\n    if FLAGS.saved_model_dir:\n      export_model(module_spec, class_count, FLAGS.saved_model_dir)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--image_dir\',\n      type=str,\n      default=\'\',\n      help=\'Path to folders of labeled images.\'\n  )\n  parser.add_argument(\n      \'--output_graph\',\n      type=str,\n      default=\'/tmp/output_graph.pb\',\n      help=\'Where to save the trained graph.\'\n  )\n  parser.add_argument(\n      \'--intermediate_output_graphs_dir\',\n      type=str,\n      default=\'/tmp/intermediate_graph/\',\n      help=\'Where to save the intermediate graphs.\'\n  )\n  parser.add_argument(\n      \'--intermediate_store_frequency\',\n      type=int,\n      default=0,\n      help=""""""\\\n         How many steps to store intermediate graph. If ""0"" then will not\n         store.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--output_labels\',\n      type=str,\n      default=\'/tmp/output_labels.txt\',\n      help=\'Where to save the trained graph\\\'s labels.\'\n  )\n  parser.add_argument(\n      \'--summaries_dir\',\n      type=str,\n      default=\'/tmp/retrain_logs\',\n      help=\'Where to save summary logs for TensorBoard.\'\n  )\n  parser.add_argument(\n      \'--how_many_training_steps\',\n      type=int,\n      default=4000,\n      help=\'How many training steps to run before ending.\'\n  )\n  parser.add_argument(\n      \'--learning_rate\',\n      type=float,\n      default=0.01,\n      help=\'How large a learning rate to use when training.\'\n  )\n  parser.add_argument(\n      \'--testing_percentage\',\n      type=int,\n      default=10,\n      help=\'What percentage of images to use as a test set.\'\n  )\n  parser.add_argument(\n      \'--validation_percentage\',\n      type=int,\n      default=10,\n      help=\'What percentage of images to use as a validation set.\'\n  )\n  parser.add_argument(\n      \'--eval_step_interval\',\n      type=int,\n      default=10,\n      help=\'How often to evaluate the training results.\'\n  )\n  parser.add_argument(\n      \'--train_batch_size\',\n      type=int,\n      default=100,\n      help=\'How many images to train on at a time.\'\n  )\n  parser.add_argument(\n      \'--test_batch_size\',\n      type=int,\n      default=-1,\n      help=""""""\\\n      How many images to test on. This test set is only used once, to evaluate\n      the final accuracy of the model after training completes.\n      A value of -1 causes the entire test set to be used, which leads to more\n      stable results across runs.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--validation_batch_size\',\n      type=int,\n      default=100,\n      help=""""""\\\n      How many images to use in an evaluation batch. This validation set is\n      used much more often than the test set, and is an early indicator of how\n      accurate the model is during training.\n      A value of -1 causes the entire validation set to be used, which leads to\n      more stable results across training iterations, but may be slower on large\n      training sets.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--print_misclassified_test_images\',\n      default=False,\n      help=""""""\\\n      Whether to print out a list of all misclassified test images.\\\n      """""",\n      action=\'store_true\'\n  )\n  parser.add_argument(\n      \'--bottleneck_dir\',\n      type=str,\n      default=\'/tmp/bottleneck\',\n      help=\'Path to cache bottleneck layer values as files.\'\n  )\n  parser.add_argument(\n      \'--final_tensor_name\',\n      type=str,\n      default=\'final_result\',\n      help=""""""\\\n      The name of the output classification layer in the retrained graph.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--flip_left_right\',\n      default=False,\n      help=""""""\\\n      Whether to randomly flip half of the training images horizontally.\\\n      """""",\n      action=\'store_true\'\n  )\n  parser.add_argument(\n      \'--random_crop\',\n      type=int,\n      default=0,\n      help=""""""\\\n      A percentage determining how much of a margin to randomly crop off the\n      training images.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--random_scale\',\n      type=int,\n      default=0,\n      help=""""""\\\n      A percentage determining how much to randomly scale up the size of the\n      training images by.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--random_brightness\',\n      type=int,\n      default=0,\n      help=""""""\\\n      A percentage determining how much to randomly multiply the training image\n      input pixels up or down by.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--tfhub_module\',\n      type=str,\n      default=(\n          \'https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3\'),\n      help=""""""\\\n      Which TensorFlow Hub module to use. For more options,\n      search https://tfhub.dev for image feature vector modules.\\\n      """""")\n  parser.add_argument(\n      \'--saved_model_dir\',\n      type=str,\n      default=\'\',\n      help=\'Where to save the exported graph.\')\n  parser.add_argument(\n      \'--logging_verbosity\',\n      type=str,\n      default=\'INFO\',\n      choices=[\'DEBUG\', \'INFO\', \'WARN\', \'ERROR\', \'FATAL\'],\n      help=\'How much logging output should be produced.\')\n  parser.add_argument(\n      \'--checkpoint_path\',\n      type=str,\n      default=\'/tmp/_retrain_checkpoint\',\n      help=\'Where to save checkpoint files.\'\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
examples/mnist_export_v2/export.py,15,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Example for exporting a MNIST classifier in SavedModel v2.0 format.\n\nThe module has as a single signature, accepting a batch of images with shape\n[None, 28, 28, 1] and returning a prediction vector.\nIn this example, we are loading the MNIST Dataset from TFDS and training a\nsimple digit classifier.\n\nFor a more realistic exporting example, see:\ntensorflow/examples/saved_model/integration_tests/export_mnist_cnn.py.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nimport sys\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_datasets as tfds\n\nFLAGS = None\n\n\nclass MNIST(tf.keras.models.Model):\n  """"""Model representing a MNIST classifier.""""""\n\n  def __init__(self, output_activation=""softmax""):\n    super(MNIST, self).__init__()\n    self.layer_1 = tf.keras.layers.Dense(64)\n    self.layer_2 = tf.keras.layers.Dense(10, activation=output_activation)\n\n  def call(self, inputs):\n    casted = tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32))(inputs)\n    flatten = tf.keras.layers.Flatten()(casted)\n\n    def normalize_fn(x):\n      return x / tf.reduce_max(tf.gather(x, 0))\n\n    normalize = tf.keras.layers.Lambda(normalize_fn)(flatten)\n    x = self.layer_1(normalize)\n    output = self.layer_2(x)\n    return output\n\n\ndef train_step(model, loss_fn, optimizer_fn, metric, image, label):\n  """"""Perform one training step for the model.\n\n  Args:\n    model: Keras model to train.\n    loss_fn: Loss function to use.\n    optimizer_fn: Optimizer function to use.\n    metric: keras.metric to use.\n    image: Tensor of training images of shape [batch_size, 28, 28, 1].\n    label: Tensor of class labels of shape [batch_size].\n  """"""\n  with tf.GradientTape() as tape:\n    preds = model(image)\n    label_onehot = tf.one_hot(label, 10)\n    loss_ = loss_fn(label_onehot, preds)\n  grads = tape.gradient(loss_, model.trainable_variables)\n  optimizer_fn.apply_gradients(zip(grads, model.trainable_variables))\n  metric(loss_)\n\n\ndef train_and_export(export_path,\n                     buffer_size=1000,\n                     batch_size=32,\n                     learning_rate=1e-3,\n                     epoch=10,\n                     dataset=None):\n  """"""Trains and export the model as SavedModel 2.0.\n\n  Args:\n    export_path: (str) Path to export the trained model.\n    buffer_size: (int) Size of buffer to use while shuffling.\n    batch_size: (int) Size of each training batch.\n    learning_rate: (float) Learning rate to use for the optimizer.\n    epoch: (int) Number of Epochs to train for.\n    dataset: (tf.data.Dataset) Dataset object. Defaults to a MNIST dataset.\n  """"""\n  model = MNIST()\n  if not dataset:\n    dataset = tfds.load(\n        ""mnist"", split=""train"", batch_size=batch_size,\n        shuffle_files=True).shuffle(\n            buffer_size, reshuffle_each_iteration=True)\n\n  optimizer_fn = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n  loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n  metric = tf.keras.metrics.Mean()\n  model.compile(optimizer_fn, loss=loss_fn)\n\n  # Training loop.\n  for epoch in range(epoch):\n    for step, data in enumerate(dataset):\n      train_step(model, loss_fn, optimizer_fn, metric, data[""image""],\n                 data[""label""])\n      print(""\\rEpoch: #{}\\tStep: #{}\\tLoss: {}\\n"".format(\n          epoch, step,\n          metric.result().numpy()))\n\n  # We have to call either predict or fit to make it possible to export with\n  # tf.saved_model.save.\n  model.predict(next(iter(dataset))[""image""])\n  # Export the model as SavedModel 2.0.\n  tf.saved_model.save(model, export_path)\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--export_path"",\n      type=str,\n      default=None,\n      help=""Path to export the module"")\n  parser.add_argument(\n      ""--buffer_size"",\n      type=int,\n      default=1000,\n      help=""Buffer Size to use while shuffling the dataset"")\n  parser.add_argument(\n      ""--batch_size"", type=int, default=32, help=""Size of each batch"")\n  parser.add_argument(\n      ""--learning_rate"", type=float, default=1e-3, help=""learning rate"")\n  parser.add_argument(\n      ""--epoch"", type=int, default=10, help=""Number of iterations"")\n  FLAGS, unparsed = parser.parse_known_args()\n\n  if not FLAGS.export_path:\n    logging.error(""Must set flag --export_path."")\n    sys.exit(1)\n\n  train_and_export(**vars(FLAGS))\n'"
examples/mnist_export_v2/export_test.py,9,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Tests for MNIST exporter.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom examples.mnist_export_v2 import export\n\n\nclass ExportTest(tf.test.TestCase):\n  """"""Test for MNIST model exporter.""""""\n\n  def setUp(self):\n    super(ExportTest, self).setUp()\n    def create_image_and_label(index):\n      image = tf.image.convert_image_dtype(\n          255 * tf.random.normal([1, 28, 28, 1]), dtype=tf.uint8, saturate=True)\n      return dict(image=image, label=[index])\n    self.mock_dataset = tf.data.Dataset.range(5).map(create_image_and_label)\n\n  def test_model_exporting(self):\n    export.train_and_export(\n        epoch=1,\n        dataset=self.mock_dataset,\n        export_path=""%s/model/1"" % self.get_temp_dir())\n    self.assertTrue(os.listdir(self.get_temp_dir()))\n\n  def test_empty_input(self):\n    export.train_and_export(\n        epoch=1,\n        dataset=self.mock_dataset,\n        export_path=""%s/model/1"" % self.get_temp_dir())\n    model = hub.load(""%s/model/1"" % self.get_temp_dir())\n    output_ = model(tf.zeros([1, 28, 28, 1], dtype=tf.uint8).numpy())\n    self.assertEqual(output_.shape, [1, 10])\n\n\nif __name__ == ""__main__"":\n  # This test is only supported in TF 2.0.\n  if tf.executing_eagerly():\n    logging.info(""Using TF version: %s"", tf.__version__)\n    tf.test.main()\n  else:\n    logging.warning(""Skipping running tests for TF Version: %s"", tf.__version__)\n'"
examples/text_embeddings/export.py,31,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Exporter tool for TF-Hub text embedding modules.\n\nThis tool creates TF-Hub Modules from embeddings text files in the following\nformat:\ntoken1 1.0 2.0 3.0 4.0 5.0\ntoken2 2.0 3.0 4.0 5.0 6.0\n...\n\nExample use:\n\npython export.py --embedding_file=/tmp/embedding.txt --export_path=/tmp/module\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport shutil\nimport sys\nimport tempfile\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nimport tensorflow_hub as hub\n\nFLAGS = None\n\nEMBEDDINGS_VAR_NAME = ""embeddings""\n\n\ndef parse_line(line):\n  """"""Parses a line of a text embedding file.\n\n  Args:\n    line: (str) One line of the text embedding file.\n\n  Returns:\n    A token string and its embedding vector in floats.\n  """"""\n  columns = line.split()\n  token = columns.pop(0)\n  values = [float(column) for column in columns]\n  return token, values\n\n\ndef load(file_path, parse_line_fn):\n  """"""Loads a text embedding into memory as a numpy matrix.\n\n  Args:\n    file_path: Path to the text embedding file.\n    parse_line_fn: callback function to parse each file line.\n\n  Returns:\n    A tuple of (list of vocabulary tokens, numpy matrix of embedding vectors).\n\n  Raises:\n    ValueError: if the data in the sstable is inconsistent.\n  """"""\n  vocabulary = []\n  embeddings = []\n  embeddings_dim = None\n  for line in tf.gfile.GFile(file_path):\n    token, embedding = parse_line_fn(line)\n    if not embeddings_dim:\n      embeddings_dim = len(embedding)\n    elif embeddings_dim != len(embedding):\n      raise ValueError(\n          (""Inconsistent embedding dimension detected, ""\n           ""%d != %d for token %s"") % (embeddings_dim, len(embedding), token))\n\n    vocabulary.append(token)\n    embeddings.append(embedding)\n\n  return vocabulary, np.array(embeddings)\n\n\ndef make_module_spec(vocabulary_file, vocab_size, embeddings_dim,\n                     num_oov_buckets, preprocess_text):\n  """"""Makes a module spec to simply perform token to embedding lookups.\n\n  Input of this module is a 1-D list of string tokens. For T tokens input and\n  an M dimensional embedding table, the lookup result is a [T, M] shaped Tensor.\n\n  Args:\n    vocabulary_file: Text file where each line is a key in the vocabulary.\n    vocab_size: The number of tokens contained in the vocabulary.\n    embeddings_dim: The embedding dimension.\n    num_oov_buckets: The number of out-of-vocabulary buckets.\n    preprocess_text: Whether to preprocess the input tensor by removing\n      punctuation and splitting on spaces.\n\n  Returns:\n    A module spec object used for constructing a TF-Hub module.\n  """"""\n\n  def module_fn():\n    """"""Spec function for a token embedding module.""""""\n    tokens = tf.placeholder(shape=[None], dtype=tf.string, name=""tokens"")\n\n    embeddings_var = tf.get_variable(\n        initializer=tf.zeros([vocab_size + num_oov_buckets, embeddings_dim]),\n        name=EMBEDDINGS_VAR_NAME,\n        dtype=tf.float32)\n\n    table_initializer = tf.lookup.TextFileInitializer(\n        vocabulary_file,\n        tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,\n        tf.int64, tf.lookup.TextFileIndex.LINE_NUMBER)\n    lookup_table = tf.lookup.StaticVocabularyTable(\n        table_initializer, num_oov_buckets=num_oov_buckets)\n    ids = lookup_table.lookup(tokens)\n    combined_embedding = tf.nn.embedding_lookup(params=embeddings_var, ids=ids)\n    hub.add_signature(""default"", {""tokens"": tokens},\n                      {""default"": combined_embedding})\n\n  def module_fn_with_preprocessing():\n    """"""Spec function for a full-text embedding module with preprocessing.""""""\n    sentences = tf.placeholder(shape=[None], dtype=tf.string, name=""sentences"")\n    # Perform a minimalistic text preprocessing by removing punctuation and\n    # splitting on spaces.\n    normalized_sentences = tf.regex_replace(\n        input=sentences, pattern=r""\\pP"", rewrite="""")\n    tokens = tf.string_split(normalized_sentences, "" "")\n\n    embeddings_var = tf.get_variable(\n        initializer=tf.zeros([vocab_size + num_oov_buckets, embeddings_dim]),\n        name=EMBEDDINGS_VAR_NAME,\n        dtype=tf.float32)\n    table_initializer = tf.lookup.TextFileInitializer(\n        vocabulary_file,\n        tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,\n        tf.int64, tf.lookup.TextFileIndex.LINE_NUMBER)\n    lookup_table = tf.lookup.StaticVocabularyTable(\n        table_initializer, num_oov_buckets=num_oov_buckets)\n    sparse_ids = tf.SparseTensor(\n        indices=tokens.indices,\n        values=lookup_table.lookup(tokens.values),\n        dense_shape=tokens.dense_shape)\n\n    # In case some of the input sentences are empty before or after\n    # normalization, we will end up with empty rows. We do however want to\n    # return embedding for every row, so we have to fill in the empty rows with\n    # a default.\n    sparse_ids, _ = tf.sparse_fill_empty_rows(\n        sparse_ids, lookup_table.lookup(tf.constant("""")))\n    # In case all of the input sentences are empty before or after\n    # normalization, we will end up with a SparseTensor with shape [?, 0]. After\n    # filling in the empty rows we must ensure the shape is set properly to\n    # [?, 1]. At this point, there are no empty rows, so the new shape will be\n    # [sparse_ids.dense_shape[0], max(1, sparse_ids.dense_shape[1])].\n    sparse_ids = tf.sparse_reset_shape(sparse_ids)\n\n    combined_embedding = tf.nn.embedding_lookup_sparse(\n        params=embeddings_var,\n        sp_ids=sparse_ids,\n        sp_weights=None,\n        combiner=""sqrtn"")\n\n    hub.add_signature(""default"", {""sentences"": sentences},\n                      {""default"": combined_embedding})\n\n  if preprocess_text:\n    return hub.create_module_spec(module_fn_with_preprocessing)\n  else:\n    return hub.create_module_spec(module_fn)\n\n\ndef export(export_path, vocabulary, embeddings, num_oov_buckets,\n           preprocess_text):\n  """"""Exports a TF-Hub module that performs embedding lookups.\n\n  Args:\n    export_path: Location to export the module.\n    vocabulary: List of the N tokens in the vocabulary.\n    embeddings: Numpy array of shape [N+K,M] the first N rows are the\n      M dimensional embeddings for the respective tokens and the next K\n      rows are for the K out-of-vocabulary buckets.\n    num_oov_buckets: How many out-of-vocabulary buckets to add.\n    preprocess_text: Whether to preprocess the input tensor by removing\n      punctuation and splitting on spaces.\n  """"""\n  # Write temporary vocab file for module construction.\n  tmpdir = tempfile.mkdtemp()\n  vocabulary_file = os.path.join(tmpdir, ""tokens.txt"")\n  with tf.gfile.GFile(vocabulary_file, ""w"") as f:\n    f.write(""\\n"".join(vocabulary))\n  vocab_size = len(vocabulary)\n  embeddings_dim = embeddings.shape[1]\n  spec = make_module_spec(vocabulary_file, vocab_size, embeddings_dim,\n                          num_oov_buckets, preprocess_text)\n\n  try:\n    with tf.Graph().as_default():\n      m = hub.Module(spec)\n      # The embeddings may be very large (e.g., larger than the 2GB serialized\n      # Tensor limit).  To avoid having them frozen as constant Tensors in the\n      # graph we instead assign them through the placeholders and feed_dict\n      # mechanism.\n      p_embeddings = tf.placeholder(tf.float32)\n      load_embeddings = tf.assign(m.variable_map[EMBEDDINGS_VAR_NAME],\n                                  p_embeddings)\n\n      with tf.Session() as sess:\n        sess.run([load_embeddings], feed_dict={p_embeddings: embeddings})\n        m.export(export_path, sess)\n  finally:\n    shutil.rmtree(tmpdir)\n\n\ndef maybe_append_oov_vectors(embeddings, num_oov_buckets):\n  """"""Adds zero vectors for oov buckets if num_oov_buckets > 0.\n\n  Since we are assigning zero vectors, adding more that one oov bucket is only\n  meaningful if we perform fine-tuning.\n\n  Args:\n    embeddings: Embeddings to extend.\n    num_oov_buckets: Number of OOV buckets in the extended embedding.\n  """"""\n  num_embeddings = np.shape(embeddings)[0]\n  embedding_dim = np.shape(embeddings)[1]\n  embeddings.resize(\n      [num_embeddings + num_oov_buckets, embedding_dim], refcheck=False)\n\n\ndef export_module_from_file(embedding_file, export_path, parse_line_fn,\n                            num_oov_buckets, preprocess_text):\n  # Load pretrained embeddings into memory.\n  vocabulary, embeddings = load(embedding_file, parse_line_fn)\n\n  # Add OOV buckets if num_oov_buckets > 0.\n  maybe_append_oov_vectors(embeddings, num_oov_buckets)\n\n  # Export the embedding vectors into a TF-Hub module.\n  export(export_path, vocabulary, embeddings, num_oov_buckets, preprocess_text)\n\n\ndef main(_):\n  export_module_from_file(FLAGS.embedding_file, FLAGS.export_path, parse_line,\n                          FLAGS.num_oov_buckets, FLAGS.preprocess_text)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--embedding_file"",\n      type=str,\n      default=None,\n      help=""Path to file with embeddings."")\n  parser.add_argument(\n      ""--export_path"",\n      type=str,\n      default=None,\n      help=""Where to export the module."")\n  parser.add_argument(\n      ""--preprocess_text"",\n      type=bool,\n      default=True,\n      help=""Whether to preprocess the input tensor by removing punctuation and ""\n      ""splitting on spaces. Use this if input is a dense tensor of untokenized ""\n      ""sentences."")\n  parser.add_argument(\n      ""--num_oov_buckets"",\n      type=int,\n      default=""1"",\n      help=""How many OOV buckets to add."")\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
examples/text_embeddings/export_test.py,25,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for text embedding exporting tool.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nfrom distutils.version import LooseVersion\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nimport tensorflow_hub as hub\n\nfrom examples.text_embeddings import export\n\n_MOCK_EMBEDDING = ""\\n"".join(\n    [""cat 1.11 2.56 3.45"", ""dog 1 2 3"", ""mouse 0.5 0.1 0.6""])\n\n\nclass ExportTokenEmbeddingTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._embedding_file_path = os.path.join(self.get_temp_dir(),\n                                             ""mock_embedding_file.txt"")\n    with tf.gfile.GFile(self._embedding_file_path, mode=""w"") as f:\n      f.write(_MOCK_EMBEDDING)\n\n  def testEmbeddingLoaded(self):\n    vocabulary, embeddings = export.load(self._embedding_file_path,\n                                         export.parse_line)\n    self.assertEqual((3,), np.shape(vocabulary))\n    self.assertEqual((3, 3), np.shape(embeddings))\n\n  def testExportTokenEmbeddingModule(self):\n    export.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        parse_line_fn=export.parse_line,\n        num_oov_buckets=1,\n        preprocess_text=False)\n    with tf.Graph().as_default():\n      hub_module = hub.Module(self.get_temp_dir())\n      tokens = tf.constant([""cat"", ""lizard"", ""dog""])\n      embeddings = hub_module(tokens)\n      with tf.Session() as session:\n        session.run(tf.tables_initializer())\n        session.run(tf.global_variables_initializer())\n        self.assertAllClose(\n            session.run(embeddings),\n            [[1.11, 2.56, 3.45], [0.0, 0.0, 0.0], [1.0, 2.0, 3.0]])\n\n  def testExportFulltextEmbeddingModule(self):\n    export.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        parse_line_fn=export.parse_line,\n        num_oov_buckets=1,\n        preprocess_text=True)\n    with tf.Graph().as_default():\n      hub_module = hub.Module(self.get_temp_dir())\n      tokens = tf.constant([""cat"", ""cat cat"", ""lizard. dog"", ""cat? dog"", """"])\n      embeddings = hub_module(tokens)\n      with tf.Session() as session:\n        session.run(tf.tables_initializer())\n        session.run(tf.global_variables_initializer())\n        self.assertAllClose(\n            session.run(embeddings),\n            [[1.11, 2.56, 3.45], [1.57, 3.62, 4.88], [0.70, 1.41, 2.12],\n             [1.49, 3.22, 4.56], [0.0, 0.0, 0.0]],\n            rtol=0.02)\n\n  def testEmptyInput(self):\n    export.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        parse_line_fn=export.parse_line,\n        num_oov_buckets=1,\n        preprocess_text=True)\n    with tf.Graph().as_default():\n      hub_module = hub.Module(self.get_temp_dir())\n      tokens = tf.constant(["""", """", """"])\n      embeddings = hub_module(tokens)\n      with tf.Session() as session:\n        session.run(tf.tables_initializer())\n        session.run(tf.global_variables_initializer())\n        self.assertAllClose(\n            session.run(embeddings),\n            [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n            rtol=0.02)\n\n  def testEmptyLeading(self):\n    export.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        parse_line_fn=export.parse_line,\n        num_oov_buckets=1,\n        preprocess_text=True)\n    with tf.Graph().as_default():\n      hub_module = hub.Module(self.get_temp_dir())\n      tokens = tf.constant(["""", ""cat dog""])\n      embeddings = hub_module(tokens)\n      with tf.Session() as session:\n        session.run(tf.tables_initializer())\n        session.run(tf.global_variables_initializer())\n        self.assertAllClose(\n            session.run(embeddings),\n            [[0.0, 0.0, 0.0], [1.49, 3.22, 4.56]],\n            rtol=0.02)\n\n\nif __name__ == ""__main__"":\n  # This test is only supported in graph mode.\n  if tf.executing_eagerly():\n    logging.warning(""Skipping running tests for TF Version: %s running eagerly."",\n                    tf.__version__)\n  else:\n    tf.test.main()\n'"
examples/text_embeddings_v2/export_test_v2.py,11,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for text embedding exporting tool v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom examples.text_embeddings_v2 import export_v2\n\n_MOCK_EMBEDDING = ""\\n"".join(\n    [""cat 1.11 2.56 3.45"", ""dog 1 2 3"", ""mouse 0.5 0.1 0.6""])\n\n\nclass ExportTokenEmbeddingTest(tf.test.TestCase):\n  """"""Test for text embedding exporter.""""""\n\n  def setUp(self):\n    self._embedding_file_path = os.path.join(self.get_temp_dir(),\n                                             ""mock_embedding_file.txt"")\n    with tf.io.gfile.GFile(self._embedding_file_path, mode=""w"") as f:\n      f.write(_MOCK_EMBEDDING)\n\n  def testEmbeddingLoaded(self):\n    vocabulary, embeddings = export_v2.load(self._embedding_file_path,\n                                            export_v2.parse_line,\n                                            num_lines_to_ignore=0,\n                                            num_lines_to_use=None)\n    self.assertEqual((3,), np.shape(vocabulary))\n    self.assertEqual((3, 3), np.shape(embeddings))\n\n  def testExportTextEmbeddingModule(self):\n    export_v2.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        num_oov_buckets=1,\n        num_lines_to_ignore=0,\n        num_lines_to_use=None)\n    hub_module = hub.load(self.get_temp_dir())\n    tokens = tf.constant([""cat"", ""cat cat"", ""lizard. dog"", ""cat? dog"", """"])\n    embeddings = hub_module(tokens)\n    self.assertAllClose(\n        embeddings.numpy(),\n        [[1.11, 2.56, 3.45], [1.57, 3.62, 4.88], [0.70, 1.41, 2.12],\n         [1.49, 3.22, 4.56], [0.0, 0.0, 0.0]],\n        rtol=0.02)\n\n  def testEmptyInput(self):\n    export_v2.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        num_oov_buckets=1,\n        num_lines_to_ignore=0,\n        num_lines_to_use=None)\n    hub_module = hub.load(self.get_temp_dir())\n    tokens = tf.constant(["""", """", """"])\n    embeddings = hub_module(tokens)\n    self.assertAllClose(\n        embeddings.numpy(), [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n        rtol=0.02)\n\n  def testEmptyLeading(self):\n    export_v2.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        num_oov_buckets=1,\n        num_lines_to_ignore=0,\n        num_lines_to_use=None)\n    hub_module = hub.load(self.get_temp_dir())\n    tokens = tf.constant(["""", ""cat dog""])\n    embeddings = hub_module(tokens)\n    self.assertAllClose(\n        embeddings.numpy(), [[0.0, 0.0, 0.0], [1.49, 3.22, 4.56]], rtol=0.02)\n\n  def testNumLinesIgnore(self):\n    export_v2.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        num_oov_buckets=1,\n        num_lines_to_ignore=1,\n        num_lines_to_use=None)\n    hub_module = hub.load(self.get_temp_dir())\n    tokens = tf.constant([""cat"", ""dog"", ""mouse""])\n    embeddings = hub_module(tokens)\n    self.assertAllClose(\n        embeddings.numpy(), [[0.0, 0.0, 0.0], [1, 2, 3], [0.5, 0.1, 0.6]],\n        rtol=0.02)\n\n  def testNumLinesUse(self):\n    export_v2.export_module_from_file(\n        embedding_file=self._embedding_file_path,\n        export_path=self.get_temp_dir(),\n        num_oov_buckets=1,\n        num_lines_to_ignore=0,\n        num_lines_to_use=2)\n    hub_module = hub.load(self.get_temp_dir())\n    tokens = tf.constant([""cat"", ""dog"", ""mouse""])\n    embeddings = hub_module(tokens)\n    self.assertAllClose(\n        embeddings.numpy(), [[1.1, 2.56, 3.45], [1, 2, 3], [0, 0, 0]],\n        rtol=0.02)\n\n\nif __name__ == ""__main__"":\n  # This test is only supported in TF 2.0+.\n  if tf.executing_eagerly():\n    logging.info(""Using TF version: %s"", tf.__version__)\n    tf.test.main()\n  else:\n    logging.warning(""Skipping running tests for TF Version: %s"", tf.__version__)\n'"
examples/text_embeddings_v2/export_v2.py,18,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Exporter tool for TF-Hub text embedding modules.\n\nThis tool creates TF-Hub modules (equivalent to TensorFlow 2.X SavedModel) from\nembeddings text files in the following format:\ntoken1 1.0 2.0 3.0 4.0 5.0\ntoken2 2.0 3.0 4.0 5.0 6.0\n...\n\nExample use:\n\npython export.py --embedding_file=/tmp/embedding.txt --export_path=/tmp/module\n\nThis currently depends on TF 2.0.0-beta0.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\nimport tempfile\nfrom absl import app\n\nimport numpy as np\nimport tensorflow as tf\n\nFLAGS = None\n\n\ndef parse_line(line):\n  """"""Parses a line of a text embedding file.\n\n  Args:\n    line: (str) One line of the text embedding file.\n\n  Returns:\n    A token string and its embedding vector in floats.\n  """"""\n  columns = line.split()\n  token = columns.pop(0)\n  values = [float(column) for column in columns]\n  return token, values\n\n\ndef load(file_path, parse_line_fn, num_lines_to_ignore,\n         num_lines_to_use):\n  """"""Loads a text embedding into memory as a numpy matrix.\n\n  Args:\n    file_path: Path to the text embedding file.\n    parse_line_fn: callback function to parse each file line.\n    num_lines_to_ignore: number of lines to ignore.\n    num_lines_to_use : number of lines to use. Offset by num_lines_to_ignore if\n      used together.\n\n  Returns:\n    A tuple of (list of vocabulary tokens, numpy matrix of embedding vectors).\n\n  Raises:\n    ValueError: if the data in the sstable is inconsistent.\n  """"""\n  vocabulary = []\n  embeddings = []\n  embeddings_dim = None\n  with tf.io.gfile.GFile(file_path) as f:\n    for index, line in enumerate(f):\n      if index >= num_lines_to_ignore:\n        token, embedding = parse_line_fn(line)\n        if not embeddings_dim:\n          embeddings_dim = len(embedding)\n        elif embeddings_dim != len(embedding):\n          raise ValueError(\n              ""Inconsistent embedding dimension detected, %d != %d for ""\n              ""token %s"" % (embeddings_dim, len(embedding), token))\n        vocabulary.append(token)\n        embeddings.append(embedding)\n        if (num_lines_to_use and\n            index >= num_lines_to_ignore + num_lines_to_use - 1):\n          break\n  return vocabulary, np.array(embeddings)\n\n\ndef write_vocabulary_file(vocabulary):\n  """"""Write temporary vocab file for module construction.""""""\n  tmpdir = tempfile.mkdtemp()\n  vocabulary_file = os.path.join(tmpdir, ""tokens.txt"")\n  with tf.io.gfile.GFile(vocabulary_file, ""w"") as f:\n    for entry in vocabulary:\n      f.write(entry + ""\\n"")\n  return vocabulary_file\n\n\nclass TextEmbeddingModel(tf.train.Checkpoint):\n  """"""Text embedding model.\n\n  A text embeddings model that takes a sentences on input and outputs the\n  sentence embedding.\n  """"""\n\n  def __init__(self,\n               vocab_file_path,\n               oov_buckets,\n               num_lines_to_ignore=0,\n               num_lines_to_use=None):\n    super(TextEmbeddingModel, self).__init__()\n    self._vocabulary, self._pretrained_vectors = load(vocab_file_path,\n                                                      parse_line,\n                                                      num_lines_to_ignore,\n                                                      num_lines_to_use)\n    self._oov_buckets = oov_buckets\n    # Assign the table initializer to this instance to ensure the asset\n    # it depends on is saved with the SavedModel.\n    self._table_initializer = tf.lookup.TextFileInitializer(\n        write_vocabulary_file(self._vocabulary),\n        tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,\n        tf.int64, tf.lookup.TextFileIndex.LINE_NUMBER)\n    self._table = tf.lookup.StaticVocabularyTable(\n        self._table_initializer, num_oov_buckets=oov_buckets)\n    oovs = np.zeros([oov_buckets, self._pretrained_vectors.shape[1]])\n    self._pretrained_vectors.resize([\n        self._pretrained_vectors.shape[0] + oov_buckets,\n        self._pretrained_vectors.shape[1]\n    ])\n    self._pretrained_vectors[self._pretrained_vectors.shape[0] -\n                             oov_buckets:, :] = oovs\n    self.embeddings = tf.Variable(self._pretrained_vectors)\n    self.variables = [self.embeddings]\n    self.trainable_variables = self.variables\n\n  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])\n  def _tokenize(self, sentences):\n    # Perform a minimalistic text preprocessing by removing punctuation and\n    # splitting on spaces.\n    normalized_sentences = tf.strings.regex_replace(\n        input=sentences, pattern=r""\\pP"", rewrite="""")\n    normalized_sentences = tf.reshape(normalized_sentences, [-1])\n    sparse_tokens = tf.strings.split(normalized_sentences, "" "").to_sparse()\n\n    # Deal with a corner case: there is one empty sentence.\n    sparse_tokens, _ = tf.sparse.fill_empty_rows(sparse_tokens, tf.constant(""""))\n    # Deal with a corner case: all sentences are empty.\n    sparse_tokens = tf.sparse.reset_shape(sparse_tokens)\n    sparse_token_ids = self._table.lookup(sparse_tokens.values)\n\n    return (sparse_tokens.indices, sparse_token_ids, sparse_tokens.dense_shape)\n\n  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])\n  def __call__(self, sentences):\n    token_ids, token_values, token_dense_shape = self._tokenize(sentences)\n\n    return tf.nn.safe_embedding_lookup_sparse(\n        embedding_weights=self.embeddings,\n        sparse_ids=tf.SparseTensor(token_ids, token_values, token_dense_shape),\n        sparse_weights=None,\n        combiner=""sqrtn"")\n\n\ndef export_module_from_file(embedding_file,\n                            num_oov_buckets,\n                            export_path,\n                            num_lines_to_ignore,\n                            num_lines_to_use):\n  module = TextEmbeddingModel(embedding_file, num_oov_buckets,\n                              num_lines_to_ignore, num_lines_to_use)\n  tf.saved_model.save(module, export_path)\n\n\ndef main(_):\n  export_module_from_file(FLAGS.embedding_file, FLAGS.num_oov_buckets,\n                          FLAGS.export_path, FLAGS.num_lines_to_ignore,\n                          FLAGS.num_lines_to_use)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--embedding_file"",\n      type=str,\n      default=None,\n      help=""Path to file with embeddings."")\n  parser.add_argument(\n      ""--export_path"",\n      type=str,\n      default=None,\n      help=""Where to export the module."")\n  parser.add_argument(\n      ""--num_oov_buckets"",\n      type=int,\n      default=""1"",\n      help=""How many OOV buckets to add."")\n  parser.add_argument(\n      ""--num_lines_to_ignore"",\n      type=int,\n      default=""0"",\n      help=""How many lines to ignore."")\n  parser.add_argument(\n      ""--num_lines_to_use"",\n      type=int,\n      default=None,\n      help=""How many lines to use."")\n  FLAGS, unparsed = parser.parse_known_args()\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
tensorflow_hub/pip_package/setup.py,0,"b'# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Setup for pip package.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nimport sys\n\n# Can\'t import the module during setup.py.\n# Use execfile to find __version__.\nwith open(\'tensorflow_hub/version.py\') as in_file:\n  exec(in_file.read())\n\nREQUIRED_PACKAGES = [\n    \'numpy >= 1.12.0\',\n    \'six >= 1.12.0\',\n    \'protobuf >= 3.8.0\',  # No less than what ../WORKSPACE uses.\n]\n\nproject_name = \'tensorflow-hub\'\nif \'--project_name\' in sys.argv:\n  project_name_idx = sys.argv.index(\'--project_name\')\n  project_name = sys.argv[project_name_idx + 1]\n  sys.argv.remove(\'--project_name\')\n  sys.argv.pop(project_name_idx)\n\n# If we\'re dealing with a nightly build we need to make sure that the\n# version changes for every release.\nversion = __version__\nif project_name == \'tf-hub-nightly\':\n  version += datetime.now().strftime(\'%Y%m%d%H%M\')\n\nsetup(\n    name=project_name,  # Automatic: tensorflow_hub, etc. Case insensitive.\n    version=version.replace(\'-\', \'\'),\n    description=(\'TensorFlow Hub is a library to foster the publication, \'\n                 \'discovery, and consumption of reusable parts of machine \'\n                 \'learning models.\'),\n    long_description=\'\',\n    url=\'https://github.com/tensorflow/hub\',\n    author=\'Google LLC\',\n    author_email=\'packages@tensorflow.org\',\n    packages=find_packages(),\n    install_requires=REQUIRED_PACKAGES,\n    extras_require={\n        \'make_image_classifier\': [\'keras_preprocessing[image]\'],\n        \'make_nearest_neighbour_index\': [\n            \'apache_beam\',\n            \'annoy\',\n        ],\n    },\n    entry_points={\n        \'console_scripts\': [\n            (\'make_image_classifier = \'\n             \'tensorflow_hub.tools.make_image_classifier.\'\n             \'make_image_classifier:run_main [make_image_classifier]\'),\n            (\'make_nearest_neighbour_index = tensorflow_hub.tools.\'\n             \'make_nearest_neighbour_index.make_nearest_neighbour_index:main \'\n             \'[make_nearest_neighbour_index]\'),\n        ],\n    },\n    # PyPI package information.\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n    license=\'Apache 2.0\',\n    keywords=(\'tensorflow machine learning share module subgraph component hub \'\n              \'embedding retraining transfer\'),\n)\n'"
tensorflow_hub/tools/__init__.py,0,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tfhub_dev/tools/validator.py,3,"b'# Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Markdown documentation validator for published models.\n\n1) To validate selected files, run from the project root path:\n$ python tfhub_dev/tools/validator.py vtab/models/wae-ukl/1.md [other_files]\n\nThis will download and smoke test the model specified on asset-path metadata.\n\n2) To validate all documentation files, run from the project root path:\n$ python tfhub_dev/tools/validator.py\n\nThis does not download and smoke test the model.\n\n3) To validate files from outside the project root path, use the --root_dir\nflag:\n$ python tfhub_dev/tools/validator.py --root_dir=path_to_project_root\n""""""\n\nimport abc\nimport argparse\nimport os\nimport re\nimport sys\nfrom absl import app\nfrom absl import logging\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.saved_model import loader_impl\n# pylint: enable=g-direct-tensorflow-import\n\nFLAGS = None\n\n# Regex pattern for the first line of the documentation of models.\n# Example: ""Module google/universal-sentence-encoder/1""\nMODEL_HANDLE_PATTERN = (\n    r""# Module (?P<publisher>[\\w-]+)/(?P<name>([\\w\\.-]+(/[\\w\\.-]+)*))/(?P<vers>\\d+)"")  # pylint: disable=line-too-long\n# Regex pattern for the first line of the documentation of publishers.\n# Example: ""Publisher google""\nPUBLISHER_HANDLE_PATTERN = r""# Publisher (?P<publisher>[\\w-]+)""\n# Regex pattern for the first line of the documentation of collections.\n# Example: ""Collection google/universal-sentence-encoders/1""\nCOLLECTION_HANDLE_PATTERN = (\n    r""# Collection (?P<publisher>[\\w-]+)/(?P<name>(\\w|-|/|&|;|\\.)+)/(\\d+)"")\n# Regex pattern for the line of the documentation describing model metadata.\n# Example: ""<!-- finetunable: true -->""\n# Note: Both key and value consumes free space characters, but later on these\n# are stripped.\nMETADATA_LINE_PATTERN = r""^<!--(?P<key>(\\w|\\s|-)+):(?P<value>.+)-->$""\n\n\nclass Filesystem(object):\n  """"""Convenient (and mockable) file system access.""""""\n\n  def get_contents(self, filename):\n    """"""Returns file contents as a string.""""""\n    with tf.io.gfile.GFile(filename, ""r"") as f:\n      return f.read()\n\n  def file_exists(self, filename):\n    """"""Returns whether file exists.""""""\n    return tf.io.gfile.exists(filename)\n\n  def recursive_list_dir(self, root_dir):\n    """"""Yields all files of a root directory tree.""""""\n    for dirname, _, filenames in tf.io.gfile.walk(root_dir):\n      for filename in filenames:\n        yield os.path.join(dirname, filename)\n\n\nclass MarkdownDocumentationError(Exception):\n  """"""Problem with markdown syntax parsing.""""""\n\n\ndef smoke_test_model(model_path):\n  try:\n    resolved_model = hub.resolve(model_path)\n    loader_impl.parse_saved_model(resolved_model)\n  except Exception as e:  # pylint: disable=broad-except\n    return False, e\n  return True, None\n\n\nclass ParsingPolicy(object):\n  """"""The base class for type specific parsing policy.""""""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, publisher, model_name, model_version):\n    self._publisher = publisher\n    self._model_name = model_name\n    self._model_version = model_version\n\n  @property\n  @abc.abstractmethod\n  def type_name(self):\n    """"""Return readable name of the parsed type.""""""\n\n  @property\n  def handle(self):\n    return ""%s/%s/%s"" % (self._publisher, self._model_name, self._model_version)\n\n  @property\n  def publisher(self):\n    return self._publisher\n\n  def get_expected_location(self, root_dir):\n    """"""Returns the expected path of a documentation file.""""""\n    del root_dir\n    return\n\n  def get_top_level_dir(self, root_dir):\n    """"""Returns the top level publisher directory.""""""\n    return os.path.join(root_dir, self._publisher)\n\n  def get_required_metadata(self):\n    """"""Return a list of required metadata for this type.""""""\n    return list()\n\n  def get_single_valued_metadata(self):\n    """"""Return a list of metadata attaining only one value for this type.""""""\n    return list()\n\n  def asset_tester(self):\n    """"""Return a function that smoke tests an asset.\n\n    This function takes asset path on input and returns a tuple\n    (passed, reason), where passed==True iff the asset passes a smoke test and\n    reason is None for passed==True, or reason for failing if passed==False.\n\n    Returns:\n      A function that smoke tests an asset.\n    """"""\n    return lambda _: True, None\n\n\nclass ModelParsingPolicy(ParsingPolicy):\n  """"""ParsingPolicy for model documentation.""""""\n\n  def type_name(self):\n    return ""Module""\n\n  def get_required_metadata(self):\n    return [""asset-path"", ""format"", ""module-type"", ""fine-tunable""]\n\n  def get_single_valued_metadata(self):\n    return [""asset-path"", ""format"", ""module-type"", ""fine-tunable""]\n\n  def asset_tester(self):\n    return smoke_test_model\n\n\nclass PublisherParsingPolicy(ParsingPolicy):\n  """"""ParsingPolicy for publisher documentation.""""""\n\n  def type_name(self):\n    return ""Publisher""\n\n  @property\n  def handle(self):\n    return self._publisher\n\n  def get_expected_location(self, root_dir):\n    return os.path.join(root_dir, self._publisher, self._publisher + "".md"")\n\n\nclass CollectionParsingPolicy(ParsingPolicy):\n  """"""ParsingPolicy for collection documentation.""""""\n\n  def type_name(self):\n    return ""Collection""\n\n  def get_required_metadata(self):\n    return [""module-type""]\n\n\nclass DocumentationParser(object):\n  """"""Class used for parsing model documentation strings.""""""\n\n  def __init__(self, documentation_dir, filesystem):\n    self._documentation_dir = documentation_dir\n    self._filesystem = filesystem\n    self._parsed_metadata = dict()\n    self._parsed_description = """"\n\n  @property\n  def parsed_description(self):\n    return self._parsed_description\n\n  @property\n  def parsed_metadata(self):\n    return self._parsed_metadata\n\n  def raise_error(self, message):\n    message_with_file = ""Error at file %s: %s"" % (self._file_path, message)\n    raise MarkdownDocumentationError(message_with_file)\n\n  def consume_first_line(self):\n    """"""Consume first line describing the model type and handle.""""""\n    first_line = self._lines[0].replace(""&zwnj;"", """")\n    patterns_and_policies = [\n        (MODEL_HANDLE_PATTERN, ModelParsingPolicy),\n        (PUBLISHER_HANDLE_PATTERN, PublisherParsingPolicy),\n        (COLLECTION_HANDLE_PATTERN, CollectionParsingPolicy),\n    ]\n    for pattern, policy in patterns_and_policies:\n      match = re.match(pattern, first_line)\n      if not match:\n        continue\n      groups = match.groupdict()\n      self._parsing_policy = policy(\n          groups.get(""publisher""), groups.get(""name""), groups.get(""vers""))\n      return\n    self.raise_error(\n        ""First line of the documentation file must describe either the model ""\n        ""handle in format \\""%s\\"", or a publisher handle in format \\""%s\\"", or ""\n        ""a collection handle in format \\""%s\\"". For example ""\n        ""\\""# Module google/text-embedding-model/1\\"". Instead the first line ""\n        ""is \\""%s\\""."" % (MODEL_HANDLE_PATTERN, PUBLISHER_HANDLE_PATTERN,\n                        COLLECTION_HANDLE_PATTERN, first_line))\n\n  def assert_publisher_page_exists(self):\n    """"""Assert that publisher page exists for the publisher of this model.""""""\n    # Use a publisher policy to get the expected documentation page path.\n    publisher_policy = PublisherParsingPolicy(self._parsing_policy.publisher,\n                                              self._parsing_policy.publisher,\n                                              None)\n    expected_publisher_doc_location = publisher_policy.get_expected_location(\n        self._documentation_dir)\n    if not self._filesystem.file_exists(expected_publisher_doc_location):\n      self.raise_error(\n          ""Publisher documentation does not exist. It should be added to %s."" %\n          expected_publisher_doc_location)\n\n  def assert_correct_location(self):\n    """"""Assert that documentation file is submitted to a correct location.""""""\n    expected_file_path = self._parsing_policy.get_expected_location(\n        self._documentation_dir)\n    # Exact location must be enforced for some types (publishers).\n    if expected_file_path and self._file_path != expected_file_path:\n      self.raise_error(\n          ""Documentation file is not on a correct path. Documentation for a ""\n          ""%s with handle \\""%s\\"" should be submitted to \\""%s\\"" "" %\n          (self._parsing_policy.type_name, self._parsing_policy.handle,\n           expected_file_path))\n\n    publisher_dir = self._parsing_policy.get_top_level_dir(\n        self._documentation_dir)\n    if not self._file_path.startswith(publisher_dir + ""/""):\n      self.raise_error(\n          ""Documentation file is not on a correct path. Documentation for a ""\n          ""%s with handle \\""%s\\"" should be placed in the publisher ""\n          ""directory: \\""%s\\"""" % (self._parsing_policy.type_name,\n                                 self._parsing_policy.handle, publisher_dir))\n\n    if not self._file_path.endswith("".md""):\n      self.raise_error(""Documentation file does not end with \\"".md\\"": %s"" %\n                       self._file_path)\n\n  def consume_description(self):\n    """"""Consume second line with a short model description.""""""\n    first_description_line = self._lines[1]\n    if not first_description_line:\n      self.raise_error(\n          ""Second line of the documentation file has to contain a short ""\n          ""description. For example \\""Word2vec text embedding model.\\""."")\n    self._parsed_description = self._lines[1]\n    self._current_index = 2\n    while self._lines[self._current_index] and not self._lines[\n        self._current_index].startswith(""<!--""):\n      self._parsed_description += "" "" + self._lines[self._current_index]\n      self._current_index += 1\n\n  def consume_metadata(self):\n    """"""Consume all metadata.""""""\n    while not self._lines[self._current_index].startswith(""#""):\n      if not self._lines[self._current_index]:\n        # Empty line is ok.\n        self._current_index += 1\n        continue\n      match = re.match(METADATA_LINE_PATTERN, self._lines[self._current_index])\n      if match:\n        # Add found metadata.\n        groups = match.groupdict()\n        key = groups.get(""key"").strip()\n        value = groups.get(""value"").strip()\n        if key not in self._parsed_metadata:\n          self._parsed_metadata[key] = set()\n        self._parsed_metadata[key].add(value)\n        self._current_index += 1\n        continue\n      if self._lines[self._current_index].startswith(""[![Icon URL]]""):\n        # Icon for publishers.\n        self._current_index += 1\n        continue\n      if self._lines[self._current_index].startswith(\n          ""[![Open Colab notebook]]""):\n        # Colab.\n        self._current_index += 1\n        continue\n      # Not an empty line and not expected metadata.\n      self.raise_error(\n          ""Unexpected line found: \\""%s\\"". Please refer to [README.md]""\n          ""(https://github.com/tensorflow/hub/blob/master/tensorflow_hub""\n          ""/tfhub_dev/README.md) for information about markdown format."" %\n          self._lines[self._current_index])\n\n  def assert_correct_metadata(self):\n    """"""Assert that all required metadata is present.""""""\n    required_metadata = set(self._parsing_policy.get_required_metadata())\n    provided_metadata = set(self._parsed_metadata.keys())\n    if not provided_metadata.issuperset(required_metadata):\n      self.raise_error(\n          ""There are missing required metadata lines. Please refer to ""\n          ""README.md for information about markdown format. In particular the ""\n          ""missing metadata are: %s"" %\n          sorted(required_metadata.difference(provided_metadata)))\n\n    duplicate_metadata = list()\n    for key, values in self._parsed_metadata.items():\n      if key in self._parsing_policy.get_single_valued_metadata(\n      ) and len(values) > 1:\n        duplicate_metadata.append(key)\n    if duplicate_metadata:\n      self.raise_error(\n          ""There are duplicate metadata values. Please refer to ""\n          ""README.md for information about markdown format. In particular the ""\n          ""duplicated metadata are: %s"" % sorted(duplicate_metadata))\n\n    if ""module-type"" in self._parsed_metadata:\n      allowed_prefixes = [""image-"", ""text-"", ""audio-"", ""video-""]\n      for value in self._parsed_metadata[""module-type""]:\n        if all([not value.startswith(prefix) for prefix in allowed_prefixes]):\n          self.raise_error(\n              ""The \\""module-type\\"" metadata has to start with any of \\""image-\\""""\n              "", \\""text\\"", \\""audio-\\"", \\""video-\\"", but is: \\""%s\\"""" % value)\n\n  def assert_allowed_license(self):\n    """"""Validate provided license.""""""\n    if ""license"" in self._parsed_metadata:\n      license_id = list(self._parsed_metadata[""license""])[0]\n      allowed_license_ids = [\n          ""Apache-2.0"", ""BSD-3-Clause"", ""BSD-2-Clause"", ""GPL-2.0"", ""GPL-3.0"",\n          ""LGPL-2.0"", ""LGPL-2.1"", ""LGPL-3.0"", ""MIT"", ""MPL-2.0"", ""CDDL-1.0"",\n          ""EPL-2.0"", ""custom""\n      ]\n      if license_id not in allowed_license_ids:\n        self.raise_error(\n            ""The license %s provided in metadata is not allowed. Please ""\n            ""specify a license id from list of allowed ids: [%s]. Example: ""\n            ""<!-- license: Apache-2.0 -->"" % (license_id, allowed_license_ids))\n\n  def smoke_test_asset(self):\n    """"""Smoke test asset provided on asset-path metadata.""""""\n    if ""asset-path"" in self._parsed_metadata:\n      asset_path = list(self._parsed_metadata[""asset-path""])[0]\n      asset_tester = self._parsing_policy.asset_tester()\n      passed, reason = asset_tester(asset_path)\n      if not passed:\n        self.raise_error(\n            ""The model on path %s failed to parse. Please make sure that the ""\n            ""asset-path metadata points to a valid TF2 SavedModel or a TF1 Hub ""\n            ""module, compressed as described in section \\""Model\\"" of ""\n            ""README.md. Underlying reason for failure: %s."" %\n            (asset_path, reason))\n\n  def validate(self, file_path, do_smoke_test):\n    """"""Validate one documentation markdown file.""""""\n    self._raw_content = self._filesystem.get_contents(file_path)\n    self._lines = self._raw_content.split(""\\n"")\n    self._file_path = file_path\n    self.consume_first_line()\n    self.assert_correct_location()\n    self.consume_description()\n    self.consume_metadata()\n    self.assert_correct_metadata()\n    self.assert_allowed_license()\n    self.assert_publisher_page_exists()\n    if do_smoke_test:\n      self.smoke_test_asset()\n\n\ndef validate_documentation_files(documentation_dir,\n                                 files_to_validate=None,\n                                 filesystem=Filesystem()):\n  """"""Validate documentation files in a directory.""""""\n  file_paths = list(filesystem.recursive_list_dir(documentation_dir))\n  do_smoke_test = bool(files_to_validate)\n  validated = 0\n  for file_path in file_paths:\n    if files_to_validate and file_path[len(documentation_dir) +\n                                       1:] not in files_to_validate:\n      continue\n    logging.info(""Validating %s."", file_path)\n    documentation_parser = DocumentationParser(documentation_dir, filesystem)\n    documentation_parser.validate(file_path, do_smoke_test)\n    validated += 1\n  logging.info(""Found %d matching files - all validated successfully."",\n               validated)\n  if not do_smoke_test:\n    logging.info(\n        ""No models were smoke tested. To download and smoke test a specific ""\n        ""model, specify files directly in the command line, for example: ""\n        ""\\""python tfhub_dev/tools/validator.py vtab/models/wae-ukl/1.md\\"""")\n  return validated\n\n\ndef main(_):\n  root_dir = FLAGS.root_dir or os.getcwd()\n  documentation_dir = os.path.join(root_dir, ""tfhub_dev"", ""assets"")\n  logging.info(""Using %s for documentation directory."", documentation_dir)\n\n  files_to_validate = None\n  if FLAGS.file:\n    files_to_validate = FLAGS.file\n    logging.info(""Going to validate files %s in documentation directory %s."",\n                 files_to_validate, documentation_dir)\n  else:\n    logging.info(""Going to validate all files in documentation directory %s."",\n                 documentation_dir)\n\n  validate_documentation_files(\n      documentation_dir=documentation_dir, files_to_validate=files_to_validate)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""file"",\n      type=str,\n      default=None,\n      help=(""Path to files to validate. Path is relative to `--root_dir`. ""\n            ""The model will be smoke tested only for files specified by this ""\n            ""flag.""),\n      nargs=""*"")\n  parser.add_argument(\n      ""--root_dir"",\n      type=str,\n      default=None,\n      help=(""Root directory that contains documentation files under ""\n            ""./tfhub_dev/assets. Defaults to current directory.""))\n  FLAGS, unparsed = parser.parse_known_args()\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
tfhub_dev/tools/validator_test.py,10,"b'# Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_hub.tfhub_dev.tools.validator.""""""\n\nimport os\nimport shutil\nimport tempfile\nimport tensorflow as tf\n\nfrom tfhub_dev.tools import validator\n\n\nclass MockFilesystem(validator.Filesystem):\n  """"""Returns a Mock Filesystem storing files in a dictionary.""""""\n\n  def __init__(self):\n    self._files = dict()\n\n  def get_contents(self, filename):\n    return self._files[filename]\n\n  def file_exists(self, filename):\n    """"""Returns whether file exists.""""""\n    return filename in self._files\n\n  def set_contents(self, filename, contents):\n    self._files[filename] = contents\n\n  def recursive_list_dir(self, root_dir):\n    return [\n        f for f in self._files.keys() if f.startswith(root_dir + os.path.sep)\n    ]\n\n\nMINIMAL_MARKDOWN_TEMPLATE = """"""# Module google/text-embedding-model/1\nSimple description spanning\nmultiple lines.\n\n<!-- asset-path: %s -->\n<!-- module-type:   text-embedding   -->\n<!-- fine-tunable:true -->\n<!-- format: saved_model_2 -->\n\n## Overview\n""""""\n\nMINIMAL_MARKDOWN_WITH_UNKNOWN_PUBLISHER = """"""# Module publisher-without-page/text-embedding-model/1\nSimple description spanning\nmultiple lines.\n\n<!-- asset-path: /path/to/model -->\n<!-- module-type:   text-embedding   -->\n<!-- fine-tunable:true -->\n<!-- format: saved_model_2 -->\n\n## Overview\n""""""\n\nMINIMAL_MARKDOWN_WITH_ALLOWED_LICENSE = """"""# Module google/model/1\nSimple description.\n\n<!-- asset-path: /path/to/model -->\n<!-- module-type: text-embedding -->\n<!-- fine-tunable: true -->\n<!-- format: saved_model_2 -->\n<!-- license: BSD-3-Clause -->\n\n## Overview\n""""""\n\nMINIMAL_MARKDOWN_WITH_UNKNOWN_LICENSE = """"""# Module google/model/1\nSimple description.\n\n<!-- asset-path: /path/to/model -->\n<!-- module-type: text-embedding -->\n<!-- fine-tunable: true -->\n<!-- format: saved_model_2 -->\n<!-- license: my_license -->\n\n## Overview\n""""""\n\nMINIMAL_MARKDOWN_WITH_BAD_MODULE_TYPE = """"""# Module google/model/1\nSimple description.\n\n<!-- asset-path: /path/to/model -->\n<!-- module-type: something-embedding -->\n<!-- fine-tunable: true -->\n<!-- format: saved_model_2 -->\n<!-- license: my_license -->\n\n## Overview\n""""""\n\nMARKDOWN_WITH_DOUBLE_SLASH_IN_HANDLE = """"""# Module google/model//1\nSimple description.\n""""""\n\nMARKDOWN_WITH_BAD_CHARS_IN_HANDLE = """"""# Module google/text-embedding&nbsp;/1\nSimple description.\n""""""\n\nMARKDOWN_WITH_MISSING_MODEL_IN_HANDLE = """"""# Module google/1\nSimple description.\n""""""\n\nMARKDOWN_WITHOUT_DESCRIPTION = """"""# Module google/text-embedding-model/1\n\n<!-- asset-path: https://path/to/text-embedding-model/model.tar.gz -->\n<!-- format: saved_model_2 -->\n\n## Overview\n""""""\n\nMARKDOWN_WITH_MISSING_METADATA = """"""# Module google/text-embedding-model/1\nOne line description.\n<!-- asset-path: https://path/to/text-embedding-model/model.tar.gz -->\n<!-- format: saved_model_2 -->\n\n## Overview\n""""""\n\nMARKDOWN_WITH_DUPLICATE_METADATA = """"""# Module google/text-embedding-model/1\nOne line description.\n<!-- asset-path: https://path/to/text-embedding-model/model.tar.gz -->\n<!-- asset-path: https://path/to/text-embedding-model/model2.tar.gz -->\n<!-- module-type: text-embedding -->\n<!-- fine-tunable: true -->\n<!-- format: saved_model_2 -->\n\n## Overview\n""""""\n\nMARKDOWN_WITH_UNEXPECTED_LINES = """"""# Module google/text-embedding-model/1\nOne line description.\n<!-- module-type: text-embedding -->\n\nThis should not be here.\n<!-- format: saved_model_2 -->\n\n## Overview\n""""""\n\nMINIMAL_COLLECTION_MARKDOWN = """"""# Collection google/text-embedding-collection/1\nSimple description spanning\nmultiple lines.\n\n<!-- module-type: text-embedding -->\n\n## Overview\n""""""\n\nMINIMAL_PUBLISHER_MARKDOWN = """"""# Publisher %s\nSimple description spanning one line.\n\n[![Icon URL]](https://path/to/icon.png)\n\n## Overview\n""""""\n\n\nclass ValidatorTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(tf.test.TestCase, self).setUp()\n    self.tmp_dir = tempfile.mkdtemp()\n    self.model_path = os.path.join(self.tmp_dir, ""model_1"")\n    self.not_a_model_path = os.path.join(self.tmp_dir, ""not_a_model"")\n    self.save_dummy_model(self.model_path)\n    self.minimal_markdown = MINIMAL_MARKDOWN_TEMPLATE % self.model_path\n    self.minimal_markdown_with_bad_model = (\n        MINIMAL_MARKDOWN_TEMPLATE % self.not_a_model_path)\n\n  def tearDown(self):\n    super(tf.test.TestCase, self).tearDown()\n    shutil.rmtree(self.tmp_dir)\n\n  def set_up_publisher_page(self, filesystem, publisher):\n    filesystem.set_contents(""root/%s/%s.md"" % (publisher, publisher),\n                            MINIMAL_PUBLISHER_MARKDOWN % publisher)\n\n  def save_dummy_model(self, path):\n\n    class MultiplyTimesTwoModel(tf.train.Checkpoint):\n      """"""Callable model that multiplies by two.""""""\n\n      @tf.function(\n          input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n      def __call__(self, x):\n        return x * 2\n\n    model = MultiplyTimesTwoModel()\n    tf.saved_model.save(model, path)\n\n  def test_filesystem(self):\n    tmp_dir = self.get_temp_dir()\n    tmp_file_path = tempfile.mktemp(dir=tmp_dir)\n    file_contents = ""CONTENTS""\n    with tf.io.gfile.GFile(tmp_file_path, ""w"") as output_file:\n      output_file.write(file_contents)\n    filesystem = validator.Filesystem()\n    self.assertEqual(file_contents, filesystem.get_contents(tmp_file_path))\n    self.assertAllEqual([tmp_file_path],\n                        list(filesystem.recursive_list_dir(tmp_dir)))\n\n  def test_minimal_markdown_parsed(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/text-embedding-model/1.md"",\n                            self.minimal_markdown)\n    self.set_up_publisher_page(filesystem, ""google"")\n    validator.validate_documentation_files(\n        documentation_dir=""root"", filesystem=filesystem)\n\n  def test_minimal_markdown_parsed_with_selected_files(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/text-embedding-model/1.md"",\n                            self.minimal_markdown)\n    self.set_up_publisher_page(filesystem, ""google"")\n    num_validated = validator.validate_documentation_files(\n        documentation_dir=""root"",\n        files_to_validate=[""google/models/text-embedding-model/1.md""],\n        filesystem=filesystem)\n    self.assertEqual(1, num_validated)\n\n  def test_minimal_collection_markdown_parsed(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(\n        ""root/google/collections/text-embedding-collection/1.md"",\n        MINIMAL_COLLECTION_MARKDOWN)\n    self.set_up_publisher_page(filesystem, ""google"")\n    validator.validate_documentation_files(\n        documentation_dir=""root"", filesystem=filesystem)\n\n  def test_minimal_publisher_markdown_parsed(self):\n    filesystem = MockFilesystem()\n    self.set_up_publisher_page(filesystem, ""some-publisher"")\n    validator.validate_documentation_files(\n        documentation_dir=""root"", filesystem=filesystem)\n\n  def test_invalid_markdown_fails(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/publisher/model/1.md"", ""INVALID MARKDOWN"")\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*First line.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_minimal_markdown_not_in_publisher_dir(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/gooogle/models/wrong-location/1.md"",\n                            self.minimal_markdown)\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*placed in the publisher directory.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_fails_if_publisher_page_does_not_exist(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(\n        ""root/publisher-without-page/models/text-embedding-model/1.md"",\n        MINIMAL_MARKDOWN_WITH_UNKNOWN_PUBLISHER)\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*Publisher documentation does not.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_minimal_markdown_does_not_end_with_md_fails(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/wrong-extension/1.mdz"",\n                            self.minimal_markdown)\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 r"".*end with \\""\\.md.\\""*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_publisher_markdown_at_incorrect_location_fails(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/publisher.md"",\n                            MINIMAL_PUBLISHER_MARKDOWN % ""some-publisher"")\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 r"".*some-publisher\\.md.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_publisher_markdown_at_correct_location(self):\n    filesystem = MockFilesystem()\n    self.set_up_publisher_page(filesystem, ""some-publisher"")\n    validator.validate_documentation_files(\n        documentation_dir=""root"", filesystem=filesystem)\n\n  def test_markdown_with_bad_handle(self):\n    for markdown in [\n        MARKDOWN_WITH_DOUBLE_SLASH_IN_HANDLE, MARKDOWN_WITH_BAD_CHARS_IN_HANDLE,\n        MARKDOWN_WITH_MISSING_MODEL_IN_HANDLE\n    ]:\n      filesystem = MockFilesystem()\n      filesystem.set_contents(""root/google/models/model/1.md"", markdown)\n      with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                   "".*First line of the documentation*""):\n        validator.validate_documentation_files(\n            documentation_dir=""root"", filesystem=filesystem)\n\n  def test_markdown_without_description(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/text-embedding-model/1.md"",\n                            MARKDOWN_WITHOUT_DESCRIPTION)\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*has to contain a short description.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_markdown_with_missing_metadata(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/text-embedding-model/1.md"",\n                            MARKDOWN_WITH_MISSING_METADATA)\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*missing.*fine-tunable.*module-type.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_markdown_with_duplicate_metadata(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/text-embedding-model/1.md"",\n                            MARKDOWN_WITH_DUPLICATE_METADATA)\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*duplicate.*asset-path.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_markdown_with_unexpected_lines(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/text-embedding-model/1.md"",\n                            MARKDOWN_WITH_UNEXPECTED_LINES)\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*Unexpected line.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_minimal_markdown_parsed_full(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/text-embedding-model/1.md"",\n                            self.minimal_markdown)\n    self.set_up_publisher_page(filesystem, ""google"")\n    documentation_parser = validator.DocumentationParser(""root"", filesystem)\n    documentation_parser.validate(\n        file_path=""root/google/models/text-embedding-model/1.md"",\n        do_smoke_test=True)\n    self.assertEqual(""Simple description spanning multiple lines."",\n                     documentation_parser.parsed_description)\n    expected_metadata = {\n        ""asset-path"": {self.model_path},\n        ""module-type"": {""text-embedding""},\n        ""fine-tunable"": {""true""},\n        ""format"": {""saved_model_2""},\n    }\n    self.assertAllEqual(expected_metadata, documentation_parser.parsed_metadata)\n\n  def test_bad_model_does_not_pass_smoke_test(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/text-embedding-model/1.md"",\n                            self.minimal_markdown_with_bad_model)\n    self.set_up_publisher_page(filesystem, ""google"")\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*failed to parse.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"",\n          files_to_validate=[""google/models/text-embedding-model/1.md""],\n          filesystem=filesystem)\n\n  def test_markdown_with_allowed_license(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/model/1.md"",\n                            MINIMAL_MARKDOWN_WITH_ALLOWED_LICENSE)\n    self.set_up_publisher_page(filesystem, ""google"")\n    validator.validate_documentation_files(\n        documentation_dir=""root"", filesystem=filesystem)\n\n  def test_markdown_with_unknown_license(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/model/1.md"",\n                            MINIMAL_MARKDOWN_WITH_UNKNOWN_LICENSE)\n    self.set_up_publisher_page(filesystem, ""google"")\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*specify a license id from list.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n  def test_markdown_with_bad_module_type(self):\n    filesystem = MockFilesystem()\n    filesystem.set_contents(""root/google/models/model/1.md"",\n                            MINIMAL_MARKDOWN_WITH_BAD_MODULE_TYPE)\n    self.set_up_publisher_page(filesystem, ""google"")\n    with self.assertRaisesRegexp(validator.MarkdownDocumentationError,\n                                 "".*metadata has to start with.*""):\n      validator.validate_documentation_files(\n          documentation_dir=""root"", filesystem=filesystem)\n\n\nif __name__ == ""__main__"":\n  tf.compat.v1.enable_v2_behavior()\n  tf.test.main()\n'"
tensorflow_hub/tools/make_image_classifier/__init__.py,0,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_hub/tools/make_image_classifier/make_image_classifier.py,10,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Trains a TensorFlow model based on directories of images.\n\nThis program builds, trains and exports a TensorFlow 2.x model that classifies\nnatural images (photos) into a fixed set of classes. The classes are learned\nfrom a user-supplied dataset of images, stored as a directory of subdirectories\nof JPEG images, each subdirectory representing one class.\n\nThe model is built from a pre-trained image feature vector module from\nTensorFlow Hub (in its TF2/SavedModel format, not the older hub.Module format)\nfollowed by a linear classifier. The linear classifier, and optionally also\nthe TF Hub module, are trained on the new dataset. TF Hub offers a variety of\nsuitable modules with various size/accuracy tradeoffs.\n\nThe resulting model can be exported in TensorFlow\'s standard SavedModel format\nand as a .tflite file for deployment to mobile devices with TensorFlow Lite.\nTODO(b/139467904): Add support for post-training model optimization.\n\nFor more information, please see the README file next to the source code,\nhttps://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/README.md\n""""""\n\n# NOTE: This is an expanded, command-line version of\n# https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb\n# PLEASE KEEP THEM IN SYNC, such that running tests for this program\n# provides assurance that the code in the colab notebook works.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tempfile\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport six\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow_hub.tools.make_image_classifier import make_image_classifier_lib as lib\n\n_DEFAULT_HPARAMS = lib.get_default_hparams()\n\nflags.DEFINE_string(\n    ""image_dir"", None,\n    ""A directory with subdirectories of images, one per class. ""\n    ""If unset, the TensorFlow Flowers example dataset will be used. ""\n    ""Internally, the dataset is split into training and validation pieces."")\nflags.DEFINE_string(\n    ""tfhub_module"",\n    ""https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"",\n    ""Which TF Hub module to use. Must be a module in TF2/SavedModel format ""\n    ""for computing image feature vectors."")\nflags.DEFINE_integer(\n    ""image_size"", None,\n    ""The height and width of images to feed into --tfhub_module. ""\n    ""(For now, must be set manually for modules with variable input size.)"")\nflags.DEFINE_string(\n    ""saved_model_dir"", None,\n    ""The final model is exported as a SavedModel directory with this name."")\nflags.DEFINE_string(\n    ""tflite_output_file"", None,\n    ""The final model is exported as a .tflite flatbuffers file with this name."")\nflags.DEFINE_string(\n    ""labels_output_file"", None,\n    ""Where to save the labels (that is, names of image subdirectories). ""\n    ""The lines in this file appear in the same order as the predictions ""\n    ""of the model."")\nflags.DEFINE_string(\n    ""summaries_dir"", None,\n    ""Where to save summary logs for TensorBoard."")\nflags.DEFINE_float(\n    ""assert_accuracy_at_least"", None,\n    ""If set, the program fails if the validation accuracy at the end of ""\n    ""training is less than this number (between 0 and 1), and no export of ""\n    ""the trained model happens."")\nflags.DEFINE_integer(\n    ""train_epochs"", _DEFAULT_HPARAMS.train_epochs,\n    ""Training will do this many iterations over the dataset."")\nflags.DEFINE_bool(\n    ""do_fine_tuning"", _DEFAULT_HPARAMS.do_fine_tuning,\n    ""If set, the --tfhub_module is trained together with the rest of ""\n    ""the model being built."")\nflags.DEFINE_integer(\n    ""batch_size"", _DEFAULT_HPARAMS.batch_size,\n    ""Each training step samples a batch of this many images ""\n    ""from the training data. (You may need to shrink this when using a GPU ""\n    ""and getting out-of-memory errors. Avoid values below 8 when re-training ""\n    ""modules that use batch normalization.)"")\nflags.DEFINE_float(\n    ""learning_rate"", _DEFAULT_HPARAMS.learning_rate,\n    ""The learning rate to use for gradient descent training."")\nflags.DEFINE_float(\n    ""momentum"", _DEFAULT_HPARAMS.momentum,\n    ""The momentum parameter to use for gradient descent training."")\nflags.DEFINE_float(\n    ""dropout_rate"", _DEFAULT_HPARAMS.dropout_rate,\n    ""The fraction of the input units to drop, used in dropout layer."")\nflags.DEFINE_bool(\n    ""set_memory_growth"", False,\n    ""If flag is set, memory growth functionality flag will be set as true for ""\n    ""all GPUs prior to training. ""\n    ""More details: ""\n    ""https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth""\n)\nFLAGS = flags.FLAGS\n\n\ndef _get_hparams_from_flags():\n  """"""Creates dict of hyperparameters from flags.""""""\n  return lib.HParams(\n      train_epochs=FLAGS.train_epochs,\n      do_fine_tuning=FLAGS.do_fine_tuning,\n      batch_size=FLAGS.batch_size,\n      learning_rate=FLAGS.learning_rate,\n      momentum=FLAGS.momentum,\n      dropout_rate=FLAGS.dropout_rate)\n\n\ndef _check_keras_dependencies():\n  """"""Checks dependencies of tf.keras.preprocessing.image are present.\n\n  This function may come to depend on flag values that determine the kind\n  of preprocessing being done.\n\n  Raises:\n    ImportError: If dependencies are missing.\n  """"""\n  try:\n    tf.keras.preprocessing.image.load_img(six.BytesIO())\n  except ImportError:\n    print(""\\n*** Unsatisfied dependencies of keras_preprocessing.image. ***\\n""\n          ""To install them, use your system\'s equivalent of\\n""\n          ""pip install tensorflow_hub[make_image_classifier]\\n"")\n    raise\n  except Exception as e:  # pylint: disable=broad-except\n    # Loading from dummy content as above is expected to fail in other ways.\n    pass\n\n\ndef _assert_accuracy(train_result, assert_accuracy_at_least):\n  # Fun fact: With TF1 behavior, the key was called ""val_acc"".\n  val_accuracy = train_result.history[""val_accuracy""][-1]\n  accuracy_message = ""found {:f}, expected at least {:f}"".format(\n      val_accuracy, assert_accuracy_at_least)\n  if val_accuracy >= assert_accuracy_at_least:\n    print(""ACCURACY PASSED:"", accuracy_message)\n  else:\n    raise AssertionError(""ACCURACY FAILED:"", accuracy_message)\n\n\ndef _set_gpu_memory_growth():\n  # Original code reference found here:\n  # https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n  gpus = tf.config.experimental.list_physical_devices(""GPU"")\n  if gpus:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    print(""All GPUs will scale memory steadily"")\n  else:\n    print(""No GPUs found for set_memory_growth"")\n\n\ndef main(args):\n  """"""Main function to be called by absl.app.run() after flag parsing.""""""\n  del args\n  _check_keras_dependencies()\n  hparams = _get_hparams_from_flags()\n\n  image_dir = FLAGS.image_dir or lib.get_default_image_dir()\n\n  if FLAGS.set_memory_growth:\n    _set_gpu_memory_growth()\n\n  model, labels, train_result = lib.make_image_classifier(\n      FLAGS.tfhub_module, image_dir, hparams, FLAGS.image_size,\n      FLAGS.summaries_dir)\n  if FLAGS.assert_accuracy_at_least:\n    _assert_accuracy(train_result, FLAGS.assert_accuracy_at_least)\n  print(""Done with training."")\n\n  if FLAGS.labels_output_file:\n    with tf.io.gfile.GFile(FLAGS.labels_output_file, ""w"") as f:\n      f.write(""\\n"".join(labels + ("""",)))\n    print(""Labels written to"", FLAGS.labels_output_file)\n\n  saved_model_dir = FLAGS.saved_model_dir\n  if FLAGS.tflite_output_file and not saved_model_dir:\n    # We need a SavedModel for conversion, even if the user did not request it.\n    saved_model_dir = tempfile.mkdtemp()\n  if saved_model_dir:\n    tf.saved_model.save(model, saved_model_dir)\n    print(""SavedModel model exported to"", saved_model_dir)\n\n  if FLAGS.tflite_output_file:\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    lite_model_content = converter.convert()\n    with tf.io.gfile.GFile(FLAGS.tflite_output_file, ""wb"") as f:\n      f.write(lite_model_content)\n    print(""TFLite model exported to"", FLAGS.tflite_output_file)\n\n\ndef _ensure_tf2():\n  """"""Ensure running with TensorFlow 2 behavior.\n\n  This function is safe to call even before flags have been parsed.\n\n  Raises:\n    ImportError: If tensorflow is too old for proper TF2 behavior.\n  """"""\n  logging.info(""Running with tensorflow %s and hub %s"",\n               tf.__version__, hub.__version__)\n  if not tf.executing_eagerly():\n    raise ImportError(""Sorry, this program needs TensorFlow 2."")\n\n\ndef run_main():\n  """"""Entry point equivalent to executing this file.""""""\n  _ensure_tf2()\n  app.run(main)\n\n\nif __name__ == ""__main__"":\n  run_main()\n'"
tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py,16,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Trains a TensorFlow model based on directories of images.\n\nThis library provides the major pieces for make_image_classifier (see there).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n_DEFAULT_IMAGE_URL = ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz""\n\n\ndef get_default_image_dir():\n  """"""Returns the path to a default image dataset, downloading it if needed.""""""\n  return tf.keras.utils.get_file(""flower_photos"",\n                                 _DEFAULT_IMAGE_URL, untar=True)\n\n\nclass HParams(\n    collections.namedtuple(""HParams"", [\n        ""train_epochs"", ""do_fine_tuning"", ""batch_size"", ""learning_rate"",\n        ""momentum"", ""dropout_rate""\n    ])):\n  """"""The hyperparameters for make_image_classifier.\n\n  train_epochs: Training will do this many iterations over the dataset.\n  do_fine_tuning: If true, the Hub module is trained together with the\n    classification layer on top.\n  batch_size: Each training step samples a batch of this many images.\n  learning_rate: The learning rate to use for gradient descent training.\n  momentum: The momentum parameter to use for gradient descent training.\n  dropout_rate: The fraction of the input units to drop, used in dropout layer.\n""""""\n\n\ndef get_default_hparams():\n  """"""Returns a fresh HParams object initialized to default values.""""""\n  return HParams(\n      train_epochs=5,\n      do_fine_tuning=False,\n      batch_size=32,\n      learning_rate=0.005,\n      momentum=0.9,\n      dropout_rate=0.2)\n\n\ndef _get_data_with_keras(image_dir, image_size, batch_size,\n                         do_data_augmentation=False):\n  """"""Gets training and validation data via keras_preprocessing.\n\n  Args:\n    image_dir: A Python string with the name of a directory that contains\n      subdirectories of images, one per class.\n    image_size: A list or tuple with 2 Python integers specifying\n      the fixed height and width to which input images are resized.\n    batch_size: A Python integer with the number of images per batch of\n      training and validation data.\n    do_data_augmentation: An optional boolean, controlling whether the\n      training dataset is augmented by randomly distorting input images.\n\n  Returns:\n    A nested tuple ((train_data, train_size),\n                    (valid_data, valid_size), labels) where:\n    train_data, valid_data: Generators for use with Model.fit_generator,\n      each yielding tuples (images, labels) where\n        images is a float32 Tensor of shape [batch_size, height, width, 3]\n          with pixel values in range [0,1],\n        labels is a float32 Tensor of shape [batch_size, num_classes]\n          with one-hot encoded classes.\n    train_size, valid_size: Python integers with the numbers of training\n      and validation examples, respectively.\n    labels: A tuple of strings with the class labels (subdirectory names).\n      The index of a label in this tuple is the numeric class id.\n  """"""\n  datagen_kwargs = dict(rescale=1./255,\n                        # TODO(b/139467904): Expose this as a flag.\n                        validation_split=.20)\n  dataflow_kwargs = dict(target_size=image_size, batch_size=batch_size,\n                         interpolation=""bilinear"")\n\n  valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n      **datagen_kwargs)\n  valid_generator = valid_datagen.flow_from_directory(\n      image_dir, subset=""validation"", shuffle=False, **dataflow_kwargs)\n\n  if do_data_augmentation:\n    # TODO(b/139467904): Expose the following constants as flags.\n    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        rotation_range=40, horizontal_flip=True, width_shift_range=0.2,\n        height_shift_range=0.2, shear_range=0.2, zoom_range=0.2,\n        **datagen_kwargs)\n  else:\n    train_datagen = valid_datagen\n  train_generator = train_datagen.flow_from_directory(\n      image_dir, subset=""training"", shuffle=True, **dataflow_kwargs)\n\n  indexed_labels = [(index, label)\n                    for label, index in train_generator.class_indices.items()]\n  sorted_indices, sorted_labels = zip(*sorted(indexed_labels))\n  assert sorted_indices == tuple(range(len(sorted_labels)))\n  return ((train_generator, train_generator.samples),\n          (valid_generator, valid_generator.samples),\n          sorted_labels)\n\n\ndef _image_size_for_module(module_layer, requested_image_size=None):\n  """"""Returns the input image size to use with the given module.\n\n  Args:\n    module_layer: A hub.KerasLayer initialized from a Hub module expecting\n      image input.\n    requested_image_size: An optional Python integer with the user-requested\n      height and width of the input image; or None.\n\n  Returns:\n    A tuple (height, width) of Python integers that can be used as input\n    image size for the given module_layer.\n\n  Raises:\n    ValueError: If requested_image_size is set but incompatible with the module.\n    ValueError: If the module does not specify a particular input size and\n       requested_image_size is not set.\n  """"""\n  # TODO(b/139530454): Use a library helper function once available.\n  # The stop-gap code below assumes any concrete function backing the\n  # module call will accept a batch of images with the one accepted size.\n  module_image_size = tuple(\n      module_layer._func.__call__  # pylint:disable=protected-access\n      .concrete_functions[0].structured_input_signature[0][0].shape[1:3])\n  if requested_image_size is None:\n    if None in module_image_size:\n      raise ValueError(""Must specify an image size because ""\n                       ""the selected TF Hub module specifies none."")\n    else:\n      return module_image_size\n  else:\n    requested_image_size = tf.TensorShape([requested_image_size, requested_image_size])\n    assert requested_image_size.is_fully_defined()\n    if requested_image_size.is_compatible_with(module_image_size):\n      return tuple(requested_image_size.as_list())\n    else:\n      raise ValueError(""The selected TF Hub module expects image size {}, ""\n                       ""but size {} is requested"".format(\n                           module_image_size,\n                           tuple(requested_image_size.as_list())))\n\n\ndef build_model(module_layer, hparams, image_size, num_classes):\n  """"""Builds the full classifier model from the given module_layer.\n\n  Args:\n    module_layer: Pre-trained tfhub model layer.\n    hparams: A namedtuple of hyperparameters. This function expects\n      .dropout_rate: The fraction of the input units to drop, used in dropout\n        layer.\n    image_size: The input image size to use with the given module layer.\n    num_classes: Number of the classes to be predicted.\n\n  Returns:\n    The full classifier model.\n  """"""\n  # TODO(b/139467904): Expose the hyperparameters below as flags.\n  model = tf.keras.Sequential([\n      tf.keras.Input(shape=(image_size[0], image_size[1], 3)), module_layer,\n      tf.keras.layers.Dropout(rate=hparams.dropout_rate),\n      tf.keras.layers.Dense(\n          num_classes,\n          activation=""softmax"",\n          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n  ])\n  print(model.summary())\n  return model\n\n\ndef train_model(model, hparams, train_data_and_size, valid_data_and_size,\n                log_dir=None):\n  """"""Trains model with the given data and hyperparameters.\n\n  Args:\n    model: The tf.keras.Model from _build_model().\n    hparams: A namedtuple of hyperparameters. This function expects\n      .train_epochs: a Python integer with the number of passes over the\n        training dataset;\n      .learning_rate: a Python float forwarded to the optimizer;\n      .momentum: a Python float forwarded to the optimizer;\n      .batch_size: a Python integer, the number of examples returned by each\n        call to the generators.\n    train_data_and_size: A (data, size) tuple in which data is training data to\n      be fed in tf.keras.Model.fit(), size is a Python integer with the\n      numbers of training.\n    valid_data_and_size: A (data, size) tuple in which data is validation data\n      to be fed in tf.keras.Model.fit(), size is a Python integer with the\n      numbers of validation.\n    log_dir: A directory to write logs for TensorBoard into (defaults to None,\n      no logs will then be written).\n\n  Returns:\n    The tf.keras.callbacks.History object returned by tf.keras.Model.fit().\n  """"""\n  train_data, train_size = train_data_and_size\n  valid_data, valid_size = valid_data_and_size\n  # TODO(b/139467904): Expose this hyperparameter as a flag.\n  loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n  model.compile(\n      optimizer=tf.keras.optimizers.SGD(\n          lr=hparams.learning_rate, momentum=hparams.momentum),\n      loss=loss,\n      metrics=[""accuracy""])\n  steps_per_epoch = train_size // hparams.batch_size\n  validation_steps = valid_size // hparams.batch_size\n  callbacks = []\n  if log_dir != None:\n    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n                                                    histogram_freq=1))\n  return model.fit(\n      train_data,\n      epochs=hparams.train_epochs,\n      steps_per_epoch=steps_per_epoch,\n      validation_data=valid_data,\n      validation_steps=validation_steps,\n      callbacks=callbacks)\n\n\ndef make_image_classifier(tfhub_module, image_dir, hparams,\n                          requested_image_size=None,\n                          log_dir=None):\n  """"""Builds and trains a TensorFLow model for image classification.\n\n  Args:\n    tfhub_module: A Python string with the handle of the Hub module.\n    image_dir: A Python string naming a directory with subdirectories of images,\n      one per class.\n    hparams: A HParams object with hyperparameters controlling the training.\n    requested_image_size: A Python integer controlling the size of images to\n      feed into the Hub module. If the module has a fixed input size, this\n      must be omitted or set to that same value.\n    log_dir: A directory to write logs for TensorBoard into (defaults to None,\n      no logs will then be written).\n  """"""\n  module_layer = hub.KerasLayer(tfhub_module,\n                                trainable=hparams.do_fine_tuning)\n  image_size = _image_size_for_module(module_layer, requested_image_size)\n  print(""Using module {} with image size {}"".format(\n      tfhub_module, image_size))\n  train_data_and_size, valid_data_and_size, labels = _get_data_with_keras(\n      image_dir, image_size, hparams.batch_size)\n  print(""Found"", len(labels), ""classes:"", "", "".join(labels))\n\n  model = build_model(module_layer, hparams, image_size, len(labels))\n  train_result = train_model(model, hparams, train_data_and_size,\n                             valid_data_and_size, log_dir)\n  return model, labels, train_result\n'"
tensorflow_hub/tools/make_image_classifier/make_image_classifier_test.py,9,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Unit tests for make_image_classifier_lib.py and make_image_classifier.py.\n\nFor now, we keep a single unit test for the library and its command-line\ndriver, because the latter is the best way to achieve end-to-end testing.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport random\nimport sys\n\nfrom absl import logging\nfrom absl.testing import flagsaver\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow_hub.tools.make_image_classifier import make_image_classifier\nfrom tensorflow_hub.tools.make_image_classifier import make_image_classifier_lib\n\n\ndef _fill_image(rgb, image_size):\n  r, g, b = rgb\n  return np.broadcast_to(np.array([[[r, g, b]]], dtype=np.uint8),\n                         shape=(image_size, image_size, 3))\n\n\ndef _write_filled_jpeg_file(path, rgb, image_size):\n  tf.keras.preprocessing.image.save_img(path, _fill_image(rgb, image_size),\n                                        ""channels_last"", ""jpeg"")\n\n\nclass MakeImageClassifierTest(tf.test.TestCase):\n  IMAGE_SIZE = 24\n  IMAGES_PER_CLASS = 20\n  CMY_NAMES_AND_RGB_VALUES = ((""cyan"", (0, 255, 255)),\n                              (""magenta"", (255, 0, 255)),\n                              (""yellow"", (255, 255, 0)))\n  DEFAULT_FLAGS = dict(image_size=IMAGE_SIZE, train_epochs=10,\n                       batch_size=8, learning_rate=0.1, momentum=0.0)\n\n  def _write_cmy_dataset(self):\n    path = os.path.join(self.get_temp_dir(), ""cmy_image_dir"")\n    os.mkdir(path)  # Fails if exists.\n    for class_name, rgb in self.CMY_NAMES_AND_RGB_VALUES:\n      class_subdir = os.path.join(path, class_name)\n      os.mkdir(class_subdir)\n      for i in range(self.IMAGES_PER_CLASS):\n        _write_filled_jpeg_file(\n            os.path.join(class_subdir, ""img_%s_%03d.jpeg"" % (class_name, i)),\n            rgb, self.IMAGE_SIZE)\n    return path\n\n  def _write_random_dataset(self):\n    path = os.path.join(self.get_temp_dir(), ""random_image_dir"")\n    os.mkdir(path)  # Fails if exists.\n    for class_name in (""ami"", ""baz"", ""zrh""):\n      class_subdir = os.path.join(path, class_name)\n      os.mkdir(class_subdir)\n      for i in range(self.IMAGES_PER_CLASS):\n        _write_filled_jpeg_file(\n            os.path.join(class_subdir, ""img_%s_%03d.jpeg"" % (class_name, i)),\n            [random.uniform(0, 255) for _ in range(3)],\n            self.IMAGE_SIZE)\n    return path\n\n  def _export_global_average_model(self, has_fixed_input_size=True):\n    if has_fixed_input_size:\n      input_size = (self.IMAGE_SIZE, self.IMAGE_SIZE, 3)\n      dirname = ""global_average_fixed_size""\n    else:\n      input_size = (None, None, 3)\n      dirname = ""global_average_variable_size""\n    path = os.path.join(self.get_temp_dir(), dirname)\n    inputs = tf.keras.Input(input_size)\n    outputs = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n    model = tf.keras.Model(inputs, outputs)\n    model.build((None,) + input_size)\n    model.save(path, save_format=""tf"")\n    return path\n\n  def _load_labels(self, filename):\n    with tf.io.gfile.GFile(filename, ""r"") as f:\n      return [label.strip(""\\n"") for label in f]\n\n  def _load_lite_model(self, filename):\n    """"""Returns a numpy-to-numpy wrapper for the model in a .tflite file.""""""\n    # TODO(b/149905925): Enable when fixed.\n    self.skipTest(""b/149905925 broke this."")\n    self.assertTrue(os.path.isfile(filename))\n    with tf.io.gfile.GFile(filename, ""rb"") as f:\n      model_content = f.read()\n    interpreter = tf.lite.Interpreter(model_content=model_content)\n    def lite_model(images):\n      interpreter.allocate_tensors()\n      input_index = interpreter.get_input_details()[0][\'index\']\n      interpreter.set_tensor(input_index, images)\n      interpreter.invoke()\n      output_index = interpreter.get_output_details()[0][\'index\']\n      return interpreter.get_tensor(output_index)\n    return lite_model\n\n  def testEndToEndSuccess(self):\n    logging.info(""Using testdata in %s"", self.get_temp_dir())\n    avg_model_dir = self._export_global_average_model()\n    image_dir = self._write_cmy_dataset()\n    saved_model_dir = os.path.join(self.get_temp_dir(), ""final_saved_model"")\n    saved_model_expected_file = os.path.join(saved_model_dir, ""saved_model.pb"")\n    tflite_output_file = os.path.join(self.get_temp_dir(), ""final_model.tflite"")\n    labels_output_file = os.path.join(self.get_temp_dir(), ""labels.txt"")\n    # Make sure we don\'t test for pre-existing files.\n    self.assertFalse(os.path.isfile(saved_model_expected_file))\n    self.assertFalse(os.path.isfile(tflite_output_file))\n    self.assertFalse(os.path.isfile(labels_output_file))\n\n    with flagsaver.flagsaver(\n        image_dir=image_dir, tfhub_module=avg_model_dir,\n        # This dataset is expected to be fit perfectly.\n        assert_accuracy_at_least=0.9,\n        saved_model_dir=saved_model_dir,\n        tflite_output_file=tflite_output_file,\n        labels_output_file=labels_output_file,\n        **self.DEFAULT_FLAGS):\n      make_image_classifier.main([])\n\n    # Test that the SavedModel was written.\n    self.assertTrue(os.path.isfile(saved_model_expected_file))\n\n    # Test that the TFLite model works.\n    labels = self._load_labels(labels_output_file)\n    lite_model = self._load_lite_model(tflite_output_file)\n    for class_name, rgb in self.CMY_NAMES_AND_RGB_VALUES:\n      input_batch = (_fill_image(rgb, self.IMAGE_SIZE)[None, ...]\n                     / np.array(255., dtype=np.float32))\n      output_batch = lite_model(input_batch)\n      prediction = labels[np.argmax(output_batch[0])]\n      self.assertEqual(class_name, prediction)\n\n  def testEndToEndAccuracyFailure(self):\n    logging.info(""Using testdata in %s"", self.get_temp_dir())\n    avg_model_dir = self._export_global_average_model()\n    image_dir = self._write_random_dataset()\n\n    with flagsaver.flagsaver(\n        image_dir=image_dir, tfhub_module=avg_model_dir,\n        # This is expeced to fail for this random dataset.\n        assert_accuracy_at_least=0.8, **self.DEFAULT_FLAGS):\n      with self.assertRaisesRegex(AssertionError, ""ACCURACY FAILED""):\n        make_image_classifier.main([])\n\n  def testImageSizeForModuleWithFixedInputSize(self):\n    model_dir = self._export_global_average_model(has_fixed_input_size=True)\n    module_layer = hub.KerasLayer(model_dir)\n    self.assertTupleEqual(\n        (self.IMAGE_SIZE, self.IMAGE_SIZE),\n        make_image_classifier_lib._image_size_for_module(module_layer, None))\n    self.assertTupleEqual(\n        (self.IMAGE_SIZE, self.IMAGE_SIZE),\n        make_image_classifier_lib._image_size_for_module(module_layer,\n                                                         self.IMAGE_SIZE))\n    with self.assertRaisesRegex(ValueError, ""image size""):\n      make_image_classifier_lib._image_size_for_module(\n          module_layer, self.IMAGE_SIZE + 1)\n\n  def testImageSizeForModuleWithVariableInputSize(self):\n    model_dir = self._export_global_average_model(has_fixed_input_size=False)\n    module_layer = hub.KerasLayer(model_dir)\n    self.assertTupleEqual(\n        (self.IMAGE_SIZE, self.IMAGE_SIZE),\n        make_image_classifier_lib._image_size_for_module(module_layer,\n                                                         self.IMAGE_SIZE))\n    self.assertTupleEqual(\n        (2 * self.IMAGE_SIZE, 2 * self.IMAGE_SIZE),\n        make_image_classifier_lib._image_size_for_module(module_layer,\n                                                         2 * self.IMAGE_SIZE))\n    with self.assertRaisesRegex(ValueError, ""none""):\n      make_image_classifier_lib._image_size_for_module(module_layer, None)\n\n\nif __name__ == ""__main__"":\n  try:\n    make_image_classifier._ensure_tf2()\n  except ImportError as e:\n    print(""Skipping tests:"", str(e))\n    sys.exit(0)\n  tf.test.main()\n'"
tensorflow_hub/tools/make_nearest_neighbour_index/__init__.py,0,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_hub/tools/make_nearest_neighbour_index/embedding_generator.py,8,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Generates embedding using a TF-Hub module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport pickle\n\nimport apache_beam as beam\nfrom apache_beam.transforms import util\nfrom sklearn.random_projection import gaussian_random_matrix\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n_RUNNER = \'DirectRunner\'\n_RANDOM_PROJECTION_FILENAME = \'random_projection.matrix\'\n_BATCH_SIZE = 1028\n\nembed_fn = None\n\n\ndef generate_embeddings(items, module_url, random_projection_matrix=None):\n  """"""Generates embeddings using a TF-Hub module.\n\n  Args:\n    items: The items to generate embedding for.\n    module_url: The TF-Hub module url.\n    random_projection_matrix: A numpy array of the random projection weights.\n\n  Returns:\n    item, embedding tuple.\n  """"""\n\n  global embed_fn\n  if embed_fn is None:\n    embed_fn = hub.load(module_url)\n  embeddings = embed_fn(items).numpy()\n  if random_projection_matrix is not None:\n    embeddings = embeddings.dot(random_projection_matrix)\n  return items, embeddings\n\n\ndef to_tf_example(entries):\n  """"""Convert to tf example.""""""\n\n  examples = []\n\n  item_list, embedding_list = entries\n  for i in range(len(item_list)):\n    item = item_list[i]\n    embedding = embedding_list[i]\n\n    features = {\n        \'item\':\n            tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[item.encode(\'utf-8\')])),\n        \'embedding\':\n            tf.train.Feature(\n                float_list=tf.train.FloatList(value=embedding.tolist()))\n    }\n\n    example = tf.train.Example(features=tf.train.Features(\n        feature=features)).SerializeToString(deterministic=True)\n\n    examples.append(example)\n\n  return examples\n\n\ndef generate_random_projection_weights(original_dim, projected_dim, output_dir):\n  """"""Generates a Gaussian random projection weights matrix.""""""\n\n  random_projection_matrix = None\n  if projected_dim and original_dim > projected_dim:\n    random_projection_matrix = gaussian_random_matrix(\n        n_components=projected_dim, n_features=original_dim).T\n    print(\'A Gaussian random weight matrix was creates with shape of {}\'.format(\n        random_projection_matrix.shape))\n    print(\'Storing random projection matrix to disk...\')\n    output_file_path = os.path.join(output_dir, _RANDOM_PROJECTION_FILENAME)\n    with open(output_file_path, \'wb\') as handle:\n      pickle.dump(\n          random_projection_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    print(\'Random projection matrix saved to disk.\')\n\n  return random_projection_matrix\n\n\ndef run(args):\n  """"""Runs the embedding generation Beam pipeline.""""""\n\n  if tf.io.gfile.exists(args.embed_output_dir):\n    print(\'Removing embedding output directory...\')\n    tf.io.gfile.rmtree(args.embed_output_dir)\n  print(\'Creating empty output directory...\')\n  tf.io.gfile.makedirs(args.embed_output_dir)\n\n  options = beam.options.pipeline_options.PipelineOptions(**vars(args))\n\n  original_dim = hub.load(args.module_url)([\'\']).shape[1]\n\n  random_projection_matrix = generate_random_projection_weights(\n      original_dim, args.projected_dim, args.embed_output_dir)\n\n  print(\'Starting the Beam pipeline...\')\n  with beam.Pipeline(runner=_RUNNER, options=options) as pipeline:\n    _ = (\n        pipeline\n        | \'Read sentences from files\' >>\n        beam.io.ReadFromText(file_pattern=args.data_file_pattern)\n        | \'Batch elements\' >> util.BatchElements(\n            min_batch_size=_BATCH_SIZE / 2, max_batch_size=_BATCH_SIZE)\n        | \'Generate embeddings\' >> beam.Map(\n            generate_embeddings, args.module_url, random_projection_matrix)\n        | \'Encode to tf example\' >> beam.FlatMap(to_tf_example)\n        | \'Write to TFRecords files\' >> beam.io.WriteToTFRecord(\n            file_path_prefix=\'{}/emb\'.format(args.embed_output_dir),\n            file_name_suffix=\'.tfrecords\')\n    )\n\n  print(\'Beam pipeline completed.\')\n'"
tensorflow_hub/tools/make_nearest_neighbour_index/embedding_generator_test.py,4,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests tensorflow_hub.tools.make_nearest_neighbour_index.embedding_generator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nfrom absl import flags\nimport tensorflow as tf\nfrom tensorflow_hub.tools.make_nearest_neighbour_index import embedding_generator\n# resources dependency\n\nMNNI_FOLDER = (""org_tensorflow_hub/tools/""\n               ""make_nearest_neighbour_index/"")\n\nflags.DEFINE_string(""data_file_pattern"", None, """")\nflags.DEFINE_string(""module_url"", None, """")\nflags.DEFINE_integer(""projected_dim"", None, """")\nflags.DEFINE_string(""embed_output_dir"", None, """")\n\nFLAGS = flags.FLAGS\n\n\ndef _get_resource(dirname, filename):\n  return os.path.join(os.path.dirname(__file__), filename)\n\n\nclass EmbeddingGeneratorTest(tf.test.TestCase):\n\n  def setUp(self):  # pylint: disable=g-missing-super-call\n    # create run parameters\n    # FLAGS.data_file_pattern = _get_resource(\n    # MNNI_FOLDER, ""test_data/data/titles.txt"")\n    FLAGS.data_file_pattern = _get_resource(MNNI_FOLDER,\n                                            ""test_data/data/titles.txt"")\n\n    FLAGS.module_url = ""https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1""\n    FLAGS.embed_output_dir = os.path.join(self.get_temp_dir(), ""embeds"")\n\n  def test_run(self):\n    FLAGS.projected_dim = None\n\n    # Make sure we don\'t test for pre-existing files.\n    self.assertFalse(os.path.isfile(FLAGS.embed_output_dir))\n\n    # Run embedding_generator\n    embedding_generator.run(FLAGS)\n\n    # Make sure that the embedding directory is created.\n    self.assertTrue(os.path.exists(FLAGS.embed_output_dir))\n    # Make sure that the embedding file is created.\n    expected_embedding_file = os.path.join(FLAGS.embed_output_dir,\n                                           ""emb-00000-of-00001.tfrecords"")\n    self.assertTrue(os.path.isfile(expected_embedding_file))\n\n  def test_run_with_projection(self):\n    FLAGS.projected_dim = 64\n\n    # Make sure we don\'t test for pre-existing files.\n    self.assertFalse(os.path.isfile(FLAGS.embed_output_dir))\n\n    # Run embedding_generator\n    embedding_generator.run(FLAGS)\n\n    # Make sure that the embedding directory is created.\n    self.assertTrue(os.path.exists(FLAGS.embed_output_dir))\n    # Make sure that the embedding file is created.\n    expected_embedding_file = os.path.join(FLAGS.embed_output_dir,\n                                           ""emb-00000-of-00001.tfrecords"")\n    self.assertTrue(os.path.isfile(expected_embedding_file))\n    # Make sure that the random prjection file is created.\n    expected_projection_matrix_file = os.path.join(FLAGS.embed_output_dir,\n                                                   ""random_projection.matrix"")\n    self.assertTrue(os.path.isfile(expected_projection_matrix_file))\n\n\ndef _ensure_tf2():\n  """"""Ensure running with TensorFlow 2 behavior.\n\n  This function is safe to call even before flags have been parsed.\n\n  Raises:\n    ImportError: If tensorflow is too old for proper TF2 behavior.\n  """"""\n  print(""Running with tensorflow %s"", tf.__version__)\n  if not tf.executing_eagerly():\n    raise ImportError(""Sorry, this program needs TensorFlow 2."")\n\n\nif __name__ == ""__main__"":\n  try:\n    _ensure_tf2()\n  except ImportError as e:\n    print(""Skipping tests:"", str(e))\n    sys.exit(0)\n  tf.test.main()\n'"
tensorflow_hub/tools/make_nearest_neighbour_index/index_builder.py,10,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Builds approximate nearest neighbor index for embeddings.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport pickle\nimport shutil\n\nimport annoy\nimport tensorflow as tf\n\n_INDEX_FILENAME = \'ann.index\'\n_MAPPING_FILENAME = \'ann.index.mapping\'\n_RANDOM_PROJECTION_FILENAME = \'random_projection.matrix\'\n_METRIC = \'angular\'\n\n\ndef _parse_example(example):\n  """"""Parse TF Example.""""""\n\n  # Create a description of the features in the tfrecords.\n  feature_description = {\n      \'item\': tf.io.FixedLenFeature([], tf.string),\n      \'embedding\': tf.io.VarLenFeature(tf.float32)\n  }\n  # Parse the input `tf.Example` proto using the dictionary above.\n  return tf.io.parse_single_example(example, feature_description)\n\n\ndef _infer_dimensions(embed_file):\n  """"""Infers the embedding vector size.""""""\n\n  dimensions = None\n  for record in tf.data.TFRecordDataset(embed_file).map(_parse_example):\n    dimensions = record[\'embedding\'].shape[0]\n    break\n  return dimensions\n\n\ndef run(args):\n  """"""Runs the index building process.""""""\n\n  embed_output_dir = args.embed_output_dir\n  output_dir = args.index_output_dir\n  num_trees = args.num_trees\n  index_file_path = os.path.join(output_dir, _INDEX_FILENAME)\n  mapping_file_path = os.path.join(output_dir, _MAPPING_FILENAME)\n\n  if tf.io.gfile.exists(output_dir):\n    print(\'Index output directory...\')\n    tf.io.gfile.rmtree(output_dir)\n  print(\'Creating empty output directory...\')\n  tf.io.gfile.makedirs(output_dir)\n\n  embed_files = tf.io.gfile.glob(os.path.join(embed_output_dir, \'*.tfrecords\'))\n  num_files = len(embed_files)\n  print(\'Found {} embedding file(s).\'.format(num_files))\n\n  dimensions = _infer_dimensions(embed_files[0])\n  print(\'Embedding size: {}\'.format(dimensions))\n\n  annoy_index = annoy.AnnoyIndex(dimensions, metric=_METRIC)\n\n  # Mapping between the item and its identifier in the index\n  mapping = {}\n\n  item_counter = 0\n  for i, embed_file in enumerate(embed_files):\n    print(\'Loading embeddings in file {} of {}...\'.format(\n        i + 1, num_files))\n    dataset = tf.data.TFRecordDataset(embed_file)\n    for record in dataset.map(_parse_example):\n      item = record[\'item\'].numpy().decode(\'utf-8\')\n      embedding = record[\'embedding\'].values.numpy()\n      mapping[item_counter] = item\n      annoy_index.add_item(item_counter, embedding)\n      item_counter += 1\n      if item_counter % 200000 == 0:\n        print(\'{} items loaded to the index\'.format(item_counter))\n\n  print(\'A total of {} items added to the index\'.format(item_counter))\n\n  print(\'Building the index with {} trees...\'.format(num_trees))\n  annoy_index.build(n_trees=num_trees)\n  print(\'Index is successfully built.\')\n\n  print(\'Saving index to disk...\')\n  annoy_index.save(index_file_path)\n  print(\'Index is saved to disk. File size: {} GB\'.format(\n      round(os.path.getsize(index_file_path) / float(1024**3), 2)))\n  annoy_index.unload()\n\n  print(\'Saving mapping to disk...\')\n  with open(mapping_file_path, \'wb\') as handle:\n    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n  print(\'Mapping is saved to disk. File size: {} MB\'.format(\n      round(os.path.getsize(mapping_file_path) / float(1024**2), 2)))\n\n  random_projection_file_path = os.path.join(\n      args.embed_output_dir, _RANDOM_PROJECTION_FILENAME)\n  if os.path.exists(random_projection_file_path):\n    shutil.copy(\n        random_projection_file_path, os.path.join(\n            args.index_output_dir, _RANDOM_PROJECTION_FILENAME))\n    print(\'Random projection matrix file copies to index output directory.\')\n'"
tensorflow_hub/tools/make_nearest_neighbour_index/index_builder_test.py,4,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests tensorflow_hub.tools.make_nearest_neighbour_index.index_builder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nfrom absl import flags\nimport tensorflow as tf\nfrom tensorflow_hub.tools.make_nearest_neighbour_index import index_builder\n# resources dependency\n\nMNNI_FOLDER = (""org_tensorflow_hub/tools/""\n               ""make_nearest_neighbour_index/"")\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_integer(""embed_output_dir"", None, """")\nflags.DEFINE_integer(""num_trees"", 10, """")\nflags.DEFINE_string(""index_output_dir"", None, """")\n\n\ndef _get_resource(dirname, filename):\n  return os.path.join(os.path.dirname(__file__), filename)\n\n\nclass IndexBuilderTest(tf.test.TestCase):\n\n  def setUp(self):  # pylint: disable=g-missing-super-call\n    # Create run parameters\n    FLAGS.embed_output_dir = _get_resource(MNNI_FOLDER, ""test_data/embeds/"")\n    FLAGS.index_output_dir = os.path.join(self.get_temp_dir(), ""index"")\n\n  def test_run(self):\n    # Make sure we don\'t test for pre-existing files.\n    self.assertFalse(os.path.isfile(FLAGS.index_output_dir))\n\n    # Run index_builder\n    index_builder.run(FLAGS)\n\n    # Make sure that the index directory is created.\n    self.assertTrue(os.path.exists(FLAGS.index_output_dir))\n    # Make sure that the index file is created.\n    expected_index = os.path.join(FLAGS.index_output_dir, ""ann.index"")\n    self.assertTrue(os.path.isfile(expected_index))\n    # Make sure that the mapping file is created.\n    expected_mapping_file = os.path.join(FLAGS.index_output_dir,\n                                         ""ann.index.mapping"")\n    self.assertTrue(os.path.isfile(expected_mapping_file))\n    # Make sure that the random prjection file is created.\n    expected_projection_matrix_file = os.path.join(FLAGS.index_output_dir,\n                                                   ""random_projection.matrix"")\n    self.assertTrue(os.path.isfile(expected_projection_matrix_file))\n\n\ndef _ensure_tf2():\n  """"""Ensure running with TensorFlow 2 behavior.\n\n  This function is safe to call even before flags have been parsed.\n\n  Raises:\n    ImportError: If tensorflow is too old for proper TF2 behavior.\n  """"""\n  print(""Running with tensorflow %s"", tf.__version__)\n  if not tf.executing_eagerly():\n    raise ImportError(""Sorry, this program needs TensorFlow 2."")\n\n\nif __name__ == ""__main__"":\n  try:\n    _ensure_tf2()\n  except ImportError as e:\n    print(""Skipping tests:"", str(e))\n    sys.exit(0)\n  tf.test.main()\n'"
tensorflow_hub/tools/make_nearest_neighbour_index/make_nearest_neighbour_index.py,4,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Entry point to run the hub2ann tool.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nimport tensorflow as tf\n\nfrom tensorflow_hub.tools.make_nearest_neighbour_index import embedding_generator as generator\nfrom tensorflow_hub.tools.make_nearest_neighbour_index import index_builder as builder\nfrom tensorflow_hub.tools.make_nearest_neighbour_index import similarity_finder as finder\n\n# Embedding generator flags\nflags.DEFINE_string(\n    ""data_file_pattern"", None,\n    ""Path to data file(s) to generate embeddings for."")\nflags.DEFINE_string(\n    ""module_url"", None, ""TF-Hub module to use. ""\n    ""For more options, search https://tfhub.dev."")\nflags.DEFINE_integer(\n    ""projected_dim"", None,\n    ""The desired target dimension to project the embedding to. ""\n    ""If specified, random projection will be uses."")\nflags.DEFINE_string(\n    ""embed_output_dir"", None,\n    ""The directory to store the generated embedding files to. ""\n    ""This can  be a local or a GCS location."")\n\n# index builder parameters\nflags.DEFINE_integer(\n    ""num_trees"", 100,\n    ""The number of trees to build the ANN index. Default is 100. ""\n    ""For more details, refer to https://github.com/spotify/annoy."")\nflags.DEFINE_string(\n    ""index_output_dir"", None,\n    ""The directory to store the created index and mapping files. ""\n    ""This can be a local or GCS location."")\n\n# similarity matching parameters\nflags.DEFINE_integer(\n    ""num_matches"", 10,\n    ""The number of similar matches to retrieve from the ANN index. ""\n    ""Default is 10."")\n\nFLAGS = flags.FLAGS\n\n\ndef validate_args(args):\n  """"""Validates the command line arguments specified by the user.""""""\n\n  if len(args) < 2 or args[1] not in [""generate"", ""build"", ""e2e"", ""query""]:\n    raise ValueError(""You need to specify one of four operations: ""\n                     ""generate | build | e2e | query"")\n\n  def _validate_generate_args():\n    """"""Validates generate operation args.""""""\n    if not FLAGS.data_file_pattern:\n      raise ValueError(\n          ""You must provide --data_file_pattern to generate embeddings for."")\n    if not FLAGS.module_url:\n      raise ValueError(\n          ""You must provide --module_url to use for embeddings generation."")\n    if not FLAGS.embed_output_dir:\n      raise ValueError(\n          ""You must provide --embed_output_dir to store the embedding files."")\n    if FLAGS.projected_dim and FLAGS.projected_dim < 1:\n      raise ValueError(""--projected_dim must be a positive integer value."")\n\n  def _validate_build_args(e2e=False):\n    """"""Validates build operation args.""""""\n    if not FLAGS.embed_output_dir and not e2e:\n      raise ValueError(\n          ""You must provide --embed_output_dir of the embeddings""\n          ""to build the ANN index for."")\n    if not FLAGS.index_output_dir:\n      raise ValueError(\n          ""You must provide --index_output_dir to store the index files."")\n    if not FLAGS.num_trees or FLAGS.num_trees < 1:\n      raise ValueError(\n          ""You must provide --num_trees as a positive integer value."")\n\n  def _validate_query_args():\n    if not FLAGS.module_url:\n      raise ValueError(""You must provide --module_url to use for query."")\n    if not FLAGS.index_output_dir:\n      raise ValueError(""You must provide --index_output_dir to use for query."")\n\n  operation = args[1]\n  if operation == ""generate"":\n    _validate_generate_args()\n  elif operation == ""build"":\n    _validate_build_args()\n  elif operation == ""e2e"":\n    _validate_generate_args()\n    _validate_build_args(True)\n  else:\n    _validate_query_args()\n\n  return operation\n\n\ndef _ensure_tf2():\n  """"""Ensure running with TensorFlow 2 behavior.\n\n  This function is safe to call even before flags have been parsed.\n\n  Raises:\n    ImportError: If tensorflow is too old for proper TF2 behavior.\n  """"""\n  print(""Running with tensorflow %s (git version %s)"",\n        tf.__version__, tf.__git_version__)\n  if tf.__version__.startswith(""1.""):\n    if tf.__git_version__ == ""unknown"":  # For internal testing use.\n      try:\n        tf.compat.v1.enable_v2_behavior()\n        return\n      except AttributeError:\n        pass  # Fail below for missing enabler function.\n    raise ImportError(""Sorry, this program needs TensorFlow 2."")\n\n\ndef main(args):\n  """"""Entry point main function.""""""\n\n  operation = validate_args(args)\n  print(""Selected operation: {}"".format(operation))\n\n  if operation == ""generate"":\n    print(""Generating embeddings..."")\n    generator.run(FLAGS)\n    print(""Embedding generation completed."")\n\n  elif operation == ""build"":\n    print(""Building ANN index..."")\n    builder.run(FLAGS)\n    print(""Building ANN index completed."")\n\n  elif operation == ""e2e"":\n    print(""Generating embeddings and building ANN index..."")\n    generator.run(FLAGS)\n    print(""Embedding generation completed."")\n    if FLAGS.projected_dim:\n      FLAGS.dimensions = FLAGS.projected_dim\n\n    builder.run(FLAGS)\n    print(""Building ANN index completed."")\n\n  else:\n    print(""Querying the ANN index..."")\n    similarity_finder = finder.load(FLAGS)\n    num_matches = FLAGS.num_matches\n    while True:\n      print(""Enter your query: "", end="""")\n      query = str(input())\n      similar_items = similarity_finder.find_similar_items(query, num_matches)\n      print(""Results:"")\n      print(""========="")\n      for item in similar_items:\n        print(item)\n\n\nif __name__ == ""__main__"":\n  _ensure_tf2()\n  app.run(main)\n'"
tensorflow_hub/tools/make_nearest_neighbour_index/similarity_finder.py,1,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Find similar items for a given query in the ANN index.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport pickle\n\nimport annoy\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n_INDEX_FILENAME = \'ann.index\'\n_MAPPING_FILENAME = \'ann.index.mapping\'\n_RANDOM_PROJECTION_FILENAME = \'random_projection.matrix\'\n_METRIC = \'angular\'\n\n\nclass SimilarityFinder(object):\n  """"""Similarity finder class.""""""\n\n  def __init__(\n      self,\n      module_url,\n      index_file_path,\n      mapping_file_path,\n      dimensions,\n      random_projection_matrix_file,\n  ):\n\n    # Load the TF-Hub module\n    print(\'Loading the TF-Hub module...\')\n    self.embed_fn = hub.load(module_url)\n    print(\'TF-hub module is loaded.\')\n\n    dimensions = self.embed_fn([\'\']).shape[1]\n\n    self.random_projection_matrix = None\n    if tf.io.gfile.exists(random_projection_matrix_file):\n      with open(random_projection_matrix_file, \'rb\') as handle:\n        self.random_projection_matrix = pickle.load(handle)\n      dimensions = self.random_projection_matrix.shape[1]\n\n    self.index = annoy.AnnoyIndex(dimensions, metric=_METRIC)\n    self.index.load(index_file_path, prefault=True)\n    print(\'Annoy index is loaded.\')\n    with open(mapping_file_path, \'rb\') as handle:\n      self.mapping = pickle.load(handle)\n    print(\'Mapping file is loaded.\')\n\n  def find_similar_items(self, query, num_matches=5):\n    """"""Finds similar items to a given quey in the ANN index.\n\n    Args:\n      query: The query string\n      num_matches: The number of similar items to retrieve.\n\n    Returns:\n      List of items.\n    """"""\n\n    query_embedding = self.embed_fn([query])[0].numpy()\n    if self.random_projection_matrix is not None:\n      query_embedding = query_embedding.dot(self.random_projection_matrix)\n    ids = self.index.get_nns_by_vector(\n        query_embedding, num_matches, search_k=-1, include_distances=False)\n    items = [self.mapping[i] for i in ids]\n    return items\n\n\ndef load(args):\n\n  module_url = args.module_url\n  index_file_path = os.path.join(args.index_output_dir, _INDEX_FILENAME)\n  mapping_file_path = os.path.join(args.index_output_dir, _MAPPING_FILENAME)\n  dimensions = args.dimensions\n  random_projection_matrix_file = os.path.join(\n      args.index_output_dir, _RANDOM_PROJECTION_FILENAME)\n\n  return SimilarityFinder(\n      module_url,\n      index_file_path,\n      mapping_file_path,\n      dimensions,\n      random_projection_matrix_file,\n  )\n'"
tensorflow_hub/tools/module_search/search.py,2,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tool to rank modules to use in a downstream classification task.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nimport numpy as np\nimport pandas as pd\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_hub.tools.module_search import utils\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""dataset"", None,\n                    ""Specification of a dataset. E.g. use `cifar10#1000` to ""\n                    ""perform search using 1000 examples from tfds `cifar10` ""\n                    ""dataset."")\n\nflags.DEFINE_multi_string(""module"", None, ""Module to consider in the search"")\n\nflags.DEFINE_string(""module_list"", None,\n    ""Path to text file with a module per line to be considered in the search.""\n    ""Empty lines and lines starting with # are ignored"")\n\n\ndef load_data(data_spec):\n  return utils.load_data(**data_spec)\n\n\ndef load_raw_features(data_spec):\n  data = load_data(data_spec=data_spec)\n  return data.map(lambda x: tf.image.resize(x[""image""], (224, 224)))\n\n\ndef load_labels(data_spec):\n  data = load_data(data_spec=data_spec)\n  return np.array([x for x in data.map(lambda x: x[""label""])])\n\n\ndef compute_embeddings(module_spec, data_spec):\n  raw_features = load_raw_features(data_spec=data_spec)\n  embedding_fn = utils.load_embedding_fn(\n      module=module_spec)\n  outputs = []\n  for batch in raw_features.batch(10):\n    outputs.extend(embedding_fn(batch))\n  return np.array(outputs)\n\n\ndef compute_score(module_spec, data_spec):\n  embeddings = compute_embeddings(module_spec=module_spec,\n                                  data_spec=data_spec)\n  distances = utils.compute_distance_matrix_loo(embeddings)\n  labels = load_labels(data_spec=data_spec)\n  error_rate = utils.knn_errorrate_loo(distances, labels, k=1)\n  return np.array(error_rate)\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError(""Too many command-line arguments."")\n\n  if not FLAGS.dataset:\n    raise app.UsageError(""--dataset is a required argument."")\n\n  module_list = []\n  if FLAGS.module:\n    module_list.extend(FLAGS.module)\n\n  if FLAGS.module_list:\n    with tf.io.gfile.GFile(FLAGS.module_list) as f:\n      lines = f.read().split(""\\n"")\n      module_list.extend([l for l in lines if l and not l.startswith(""#"")])\n\n  if not module_list:\n    raise app.UsageError(\n        ""Use --module or --module_list to define which modules to search."")\n\n  ds_sections = FLAGS.dataset.split(""#"")\n  dataset = ds_sections[0]\n  train_examples = int(ds_sections[1]) if len(ds_sections) != 1 else None\n  data_spec = {\n    ""dataset"": dataset,\n    ""split"": ""train"",\n    ""num_examples"": train_examples,\n  }\n\n  results = []\n  for module in module_list:\n    results.append((\n        module, data_spec,\n        compute_score(module_spec=module, data_spec=data_spec)))\n\n  df = pd.DataFrame(results, columns=[""module"", ""data"", ""1nn""])\n  df = df.filter([""module"", ""1nn""])\n  df.sort_values([""1nn""])\n  df.reset_index(drop=True)\n  df.set_index(""module"")\n\n  with pd.option_context(\n      ""display.max_rows"", None,\n      ""display.max_columns"", None,\n      ""display.precision"", 3,\n      ""max_colwidth"", -1,  # Don\'t truncate columns (e.g. module name).\n      ""display.expand_frame_repr"", False,  # Don\'t wrap output.\n  ):\n    print(""# Module ranking for %s"" % data_spec)\n    print(df)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
tensorflow_hub/tools/module_search/search_test.py,11,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for module search utility.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\ntry:\n  import mock\nexcept ImportError:\n  from unittest import mock\n# pylint:disable=g-import-not-at-top,g-statement-before-imports\n\nimport os\nimport numpy as np\n\nfrom absl.testing import flagsaver\nimport tensorflow.compat.v2 as tf\nimport tensorflow_datasets as tfds\n\nfrom tensorflow_hub.tools.module_search import search\n\n\nclass ImageChannelMeanModel(tf.train.Checkpoint):\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n  ])\n  def __call__(self, images):\n    return tf.math.reduce_mean(images, [1, 2])\n\n\ndef fake_image_dataset(*args, **kwargs):\n  num_examples = 30\n  return tf.data.Dataset.from_generator(\n      lambda: ({\n          ""image"": np.ones(shape=(32, 32, 3), dtype=np.uint8),\n          ""label"": i % 10,\n      } for i in range(num_examples)),\n      output_types={""image"": tf.uint8, ""label"": tf.int64},\n      output_shapes={""image"": (32, 32, 3), ""label"": ()},\n  )\n\n\nclass SearchTest(tf.test.TestCase):\n\n  def _create_image_models(self):\n    path1 = os.path.join(self.get_temp_dir(), ""model1"")\n    path2 = os.path.join(self.get_temp_dir(), ""model2"")\n    tf.saved_model.save(ImageChannelMeanModel(), path1)\n    tf.saved_model.save(ImageChannelMeanModel(), path2)\n    return [path1, path2]\n\n  @mock.patch.object(search.utils.tfds, ""load"", side_effect=fake_image_dataset)\n  def test_run_e2e(self, mock_tfds_load):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    modules = self._create_image_models()\n    #tfds.load = fake_image_dataset\n    with flagsaver.flagsaver(\n        dataset=""cifar100"",\n        module=modules,\n    ):\n      search.main([])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tensorflow_hub/tools/module_search/utils.py,17,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utils for module search functionality.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\n\ndef compute_distance_matrix(x_train, x_test, measure=""squared_l2""):\n  """"""Calculates the distance matrix between test and train.\n\n  Args:\n    x_train: Matrix (NxD) where each row represents a training sample\n    x_test: Matrix (MxD) where each row represents a test sample\n    measure: Distance measure (not necessarly metric) to use\n\n  Raises:\n    NotImplementedError: When the measure is not implemented\n\n  Returns:\n    Matrix (MxN) where elemnt i,j is the distance between\n    x_test_i and x_train_j.\n  """"""\n\n  if tf.test.is_gpu_available():\n    x_train = tf.convert_to_tensor(x_train, tf.float32)\n    x_test = tf.convert_to_tensor(x_test, tf.float32)\n  else:\n    if x_train.dtype != np.float32:\n      x_train = np.float32(x_train)\n    if x_test.dtype != np.float32:\n      x_test = np.float32(x_test)\n\n  if measure == ""squared_l2"":\n    if tf.test.is_gpu_available():\n      x_xt = tf.matmul(x_test, tf.transpose(x_train)).numpy()\n\n      x_train_2 = tf.reduce_sum(tf.math.square(x_train), 1).numpy()\n      x_test_2 = tf.reduce_sum(tf.math.square(x_test), 1).numpy()\n    else:\n      x_xt = np.matmul(x_test, np.transpose(x_train))\n\n      x_train_2 = np.sum(np.square(x_train), axis=1)\n      x_test_2 = np.sum(np.square(x_test), axis=1)\n\n    for i in range(np.shape(x_xt)[0]):\n      x_xt[i, :] = np.multiply(x_xt[i, :], -2)\n      x_xt[i, :] = np.add(x_xt[i, :], x_test_2[i])\n      x_xt[i, :] = np.add(x_xt[i, :], x_train_2)\n\n  elif measure == ""cosine"":\n    if tf.test.is_gpu_available():\n      x_xt = tf.matmul(x_test, tf.transpose(x_train)).numpy()\n\n      x_train_2 = tf.linalg.norm(x_train, axis=1).numpy()\n      x_test_2 = tf.linalg.norm(x_test, axis=1).numpy()\n    else:\n      x_xt = np.matmul(x_test, np.transpose(x_train))\n\n      x_train_2 = np.linalg.norm(x_train, axis=1)\n      x_test_2 = np.linalg.norm(x_test, axis=1)\n\n    outer = np.outer(x_test_2, x_train_2)\n    x_xt = np.ones(np.shape(x_xt)) - np.divide(x_xt, outer)\n\n  else:\n    raise NotImplementedError(""Method \'{}\' is not implemented"".format(measure))\n\n  return x_xt\n\n\ndef compute_distance_matrix_loo(x, measure=""squared_l2""):\n  """"""Calculates the distance matrix for leave-one-out strategy.\n\n  Args:\n    x: Matrix (NxD) where each row represents a sample\n    measure: Distance measure (not necessarly metric) to use\n\n  Raises:\n    NotImplementedError: When the measure is not implemented\n\n  Returns:\n    Matrix (NxN) where elemnt i,j is the distance between x_i and x_j.\n    The diagonal is set to infinity\n  """"""\n\n  if tf.test.is_gpu_available():\n    x = tf.convert_to_tensor(x, tf.float64)\n  else:\n    if x.dtype != np.float32:\n      x = np.float32(x)\n\n  if measure == ""squared_l2"":\n    if tf.test.is_gpu_available():\n      x_xt = tf.matmul(x, tf.transpose(x)).numpy()\n    else:\n      x_xt = np.matmul(x, np.transpose(x))\n    diag = np.diag(x_xt)\n    d = np.copy(x_xt)\n\n    for i in range(np.shape(d)[0]):\n      d[i, :] = np.multiply(d[i, :], -2)\n      d[i, :] = np.add(d[i, :], x_xt[i, i])\n      d[i, :] = np.add(d[i, :], diag)\n      d[i, i] = float(""inf"")\n\n  elif measure == ""cosine"":\n    if tf.test.is_gpu_available():\n      d = tf.matmul(x, tf.transpose(x)).numpy()\n    else:\n      d = np.matmul(x, np.transpose(x))\n    diag_sqrt = np.sqrt(np.diag(d))\n    outer = np.outer(diag_sqrt, diag_sqrt)\n    d = np.ones(np.shape(d)) - np.divide(d, outer)\n    np.fill_diagonal(d, float(""inf""))\n\n  else:\n    raise NotImplementedError(""Method \'{}\' is not implemented"".format(measure))\n\n  return d\n\n\ndef knn_errorrate(d, y_train, y_test, k=1):\n  """"""Calculate the knn error rate based on the distance matrix d.\n\n  Args:\n    d: distance matrix\n    y_train: label vector for the training samples / or matrix per entry in d\n    y_test: label vector for the test samples\n    k: number of direct neighbors for knn or list of multiple k\'s to evaluate\n\n  Returns:\n    knn error rate (1 - accuracy) for every k provided in descending order\n  """"""\n\n  return_list = True\n  if not isinstance(k, list):\n    return_list = False\n    k = [k]\n\n  k = sorted(set(k), reverse=True)\n  num_elements = np.shape(d)[0]\n\n  val_k = k[0]\n  if val_k == 1:\n    if len(k) > 1:\n        raise ValueError(""No smaller value than \'1\' allowed for k"")\n\n    indices = np.argmin(d, axis=1)\n\n    cnt = 0\n    for idx, val in enumerate(indices):\n\n      if len(np.shape(y_train)) == 1:\n        if y_test[idx] != y_train[val]:\n          cnt += 1\n      else:\n        if y_test[idx] != y_train[idx, val]:\n          cnt += 1\n\n    res = float(cnt) / num_elements\n\n  else:\n    indices = np.argpartition(d, val_k - 1, axis=1)\n    cnt = 0\n    for i in range(num_elements):\n\n      # Get max vote\n      if len(np.shape(y_train)) == 1:\n        labels = y_train[indices[i, :val_k]]\n      else:\n        labels = y_train[i, indices[i, :val_k]]\n      keys, counts = np.unique(labels, return_counts=True)\n\n      # alternative if multiple max voting neighbors:\n      # maxcnts = np.where(counts == counts.max())[0]\n      # found = False\n      # for idx in maxcnts:\n      #   if y_test[i] == keys[idx]:\n      #     found = True\n      #     break\n      # if found:\n      #   cnt += (len(maxcnts) - 1.0)/float(len(maxcnts))\n      # else:\n      #   cnt += 1\n\n      maxkey = keys[np.argmax(counts)]\n      if y_test[i] != maxkey:\n        cnt += 1\n\n    res = float(cnt) / num_elements\n\n    if len(k) > 1:\n      # update sub_d and y_train_new if needed\n      num_rows = indices[:, :val_k].shape[0]\n      num_cols = indices[:, :val_k].shape[1]\n      rows = [x for x in range(num_rows) for _ in range(num_cols)]\n      cols = indices[:, :val_k].reshape(-1)\n      sub_d = d[rows, cols].reshape(num_rows, -1)\n\n      y_train_new = indices[:, :val_k]\n      for i in range(num_elements):\n        if len(np.shape(y_train)) == 1:\n          y_train_new[i, :] = y_train[y_train_new[i, :]]\n        else:\n          y_train_new[i, :] = y_train[i, y_train_new[i, :]]\n\n  if not return_list:\n    return res\n\n  res = [res]\n  if len(k) > 1:\n      res.extend(knn_errorrate(sub_d, y_train_new, y_test, k[1:]))\n\n  return res\n\n\ndef knn_errorrate_loo(d, y, k=1):\n  """"""Calculate the leave-one-out expected knn error rate based\n  on the distance matrix d.\n\n  Args:\n    d: distance matrix, the diagonal should be infinity\n    y: label matrix\n    k: number of direct neighbors for knn or list of multiple k\'s to evaluate\n\n  Returns:\n    Expected leave-one-out knn error rate (1 - accuracy) for every k provided\n    in descending order\n  """"""\n\n  return knn_errorrate(d, y, y, k)\n\n\ndef load_data(dataset, split, num_examples=None):\n  ds = tfds.load(dataset, split=split, shuffle_files=False)\n  if num_examples:\n    ds = ds.take(num_examples)\n  return ds\n\n\ndef load_embedding_fn(module):\n  return hub.KerasLayer(module)\n'"
tensorflow_hub/tools/module_search/utils_test.py,13,"b'# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for module search utility.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom scipy.spatial import distance as spdist\n\nfrom tensorflow_hub.tools.module_search import utils\n\n\n# pylint: disable=g-import-not-at-top\nif tf.executing_eagerly():\n  # Note: presubmit and continuous integrations jobs are failing on these\n  # imports when running with TF-1 which anyhow is not supported by this\n  # test.\n  from sklearn.neighbors import KNeighborsClassifier\n  from sklearn import metrics\n# pylint: enable=g-import-not-at-top\n\n\nclass TestUtils(tf.test.TestCase):\n\n  train_samples = 450\n  test_samples = 50\n  dim = 10\n  classes = 7\n  random_seed = 127\n\n  def test_compute_distance_matrix(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    np.random.seed(seed=self.random_seed)\n    x_train = np.random.rand(self.train_samples, self.dim)\n    x_test = np.random.rand(self.test_samples, self.dim)\n\n    d = utils.compute_distance_matrix(x_train, x_test)\n    self.assertEqual(d.shape, (self.test_samples, self.train_samples))\n\n    for i in range(self.test_samples):\n      for j in range(self.train_samples):\n        d_ij = np.linalg.norm(x_train[j, :] - x_test[i, :])**2\n        self.assertAlmostEqual(d_ij, d[i, j], places=5)\n\n  def test_compute_distance_matrix_cosine(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    np.random.seed(seed=self.random_seed)\n    x_train = np.random.rand(self.train_samples, self.dim)\n    x_test = np.random.rand(self.test_samples, self.dim)\n\n    d = utils.compute_distance_matrix(x_train, x_test, measure=""cosine"")\n    self.assertEqual(d.shape, (self.test_samples, self.train_samples))\n\n    for i in range(self.test_samples):\n      for j in range(self.train_samples):\n        d_ij = spdist.cosine(x_test[i, :], x_train[j, :])\n        self.assertAlmostEqual(d_ij, d[i, j], places=5)\n\n  def test_compute_distance_matrix_loo(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    np.random.seed(seed=self.random_seed)\n    x_train = np.random.rand(self.train_samples, self.dim)\n\n    d = utils.compute_distance_matrix_loo(x_train)\n    self.assertEqual(d.shape, (self.train_samples, self.train_samples))\n\n    for i in range(self.train_samples):\n      for j in range(self.train_samples):\n        if i == j:\n          self.assertEqual(float(""inf""), d[i, j])\n        else:\n          d_ij = np.linalg.norm(x_train[j, :] - x_train[i, :])**2\n          self.assertAlmostEqual(d_ij, d[i, j], places=5)\n\n  def test_compute_distance_matrix_loo_cosine(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    np.random.seed(seed=self.random_seed)\n    x_train = np.random.rand(self.train_samples, self.dim)\n\n    d = utils.compute_distance_matrix_loo(x_train, measure=""cosine"")\n    self.assertEqual(d.shape, (self.train_samples, self.train_samples))\n\n    for i in range(self.train_samples):\n      for j in range(self.train_samples):\n        if i == j:\n          self.assertEqual(float(""inf""), d[i, j])\n        else:\n          d_ij = spdist.cosine(x_train[i, :], x_train[j, :])\n          self.assertAlmostEqual(d_ij, d[i, j], places=5)\n\n  def knn_errorrate(self, k):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    x_train = np.random.rand(self.train_samples, self.dim)\n    x_test = np.random.rand(self.test_samples, self.dim)\n\n    d = utils.compute_distance_matrix(x_train, x_test)\n\n    y_test = np.random.randint(self.classes, size=self.test_samples)\n    y_train = np.random.randint(self.classes, size=self.train_samples)\n\n    err = utils.knn_errorrate(d, y_train, y_test, k=k)\n\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train, y_train)\n    y_pred = knn.predict(x_test)\n    acc = metrics.accuracy_score(y_test, y_pred)\n\n    self.assertAlmostEqual(1.0 - err, acc, places=5)\n\n  def test_knn_errorrate(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    np.random.seed(seed=self.random_seed)\n    ks = [1, 3, 5]\n    for idx, val in enumerate(ks):\n      with self.subTest(i=idx):\n        self.knn_errorrate(val)\n\n  def test_knn_errorrate_multik(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    np.random.seed(seed=self.random_seed)\n    x_train = np.random.rand(self.train_samples, self.dim)\n    x_test = np.random.rand(self.test_samples, self.dim)\n\n    d = utils.compute_distance_matrix(x_train, x_test)\n\n    y_test = np.random.randint(self.classes, size=self.test_samples)\n    y_train = np.random.randint(self.classes, size=self.train_samples)\n\n    ks_input = [5, 1, 5, 3]\n    ks = [5,3,1]\n    vals = []\n    for val in ks:\n        err = utils.knn_errorrate(d, y_train, y_test, k=val)\n        vals.append(err)\n\n    comp = utils.knn_errorrate(d, y_train, y_test, k=ks_input)\n\n    self.assertEqual(len(vals), len(comp))\n    for k, v in enumerate(comp):\n        self.assertAlmostEqual(v, vals[k], places=5)\n\n  def knn_errorrate_loo(self, k):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    x_train = np.random.rand(self.train_samples, self.dim)\n\n    d = utils.compute_distance_matrix_loo(x_train)\n\n    y_train = np.random.randint(self.classes, size=self.train_samples)\n\n    err = utils.knn_errorrate_loo(d, y_train, k=k)\n\n    cnt = 0.0\n    for i in range(self.train_samples):\n      knn = KNeighborsClassifier(n_neighbors=k)\n      mask = [True]*self.train_samples\n      mask[i] = False\n      knn.fit(x_train[mask], y_train[mask])\n      y_pred = knn.predict(x_train[i].reshape(-1, self.dim))\n      if y_pred != y_train[i]:\n        cnt += 1\n\n    self.assertAlmostEqual(err, cnt / self.train_samples, places=5)\n\n  def test_knn_errorrate_loo(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    np.random.seed(seed=self.random_seed)\n    ks = [1, 3, 5]\n    for idx, val in enumerate(ks):\n      with self.subTest(i=idx):\n        self.knn_errorrate_loo(val)\n\n  def test_knn_errorrate_loo_multik(self):\n    if not tf.executing_eagerly():\n      self.skipTest(""Test requires eager mode."")\n    np.random.seed(seed=self.random_seed)\n    x_train = np.random.rand(self.train_samples, self.dim)\n\n    d = utils.compute_distance_matrix_loo(x_train)\n\n    y_train = np.random.randint(self.classes, size=self.train_samples)\n\n    ks_input = [5, 1, 5, 3]\n    ks = [5,3,1]\n    vals = []\n    for val in ks:\n        err = utils.knn_errorrate_loo(d, y_train, k=val)\n        vals.append(err)\n\n    comp = utils.knn_errorrate_loo(d, y_train, k=ks_input)\n\n    self.assertEqual(len(vals), len(comp))\n    for k, v in enumerate(comp):\n        self.assertAlmostEqual(v, vals[k], places=5)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
