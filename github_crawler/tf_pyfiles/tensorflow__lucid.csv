file_path,api_count,code
setup.py,0,"b'# Copyright 2017 The TensorFlow Lucid Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Setup script for TensorFlow Lucid.""""""\n\nimport lucid\nfrom setuptools import setup, find_packages\n\nversion = lucid.__version__\n\ntest_deps = [""future"", ""twine"", ""pytest"", ""pytest-mock"", ""python-coveralls""]\n\nextras = {\n    ""test"": test_deps,\n    ""tf"": [""tensorflow>=1.6.0""],\n    ""tf_gpu"": [""tensorflow-gpu>=1.6.0""],\n}\n\nsetup(\n    name=""lucid"",\n    packages=find_packages(exclude=[]),\n    version=version,\n    description=(\n        ""Collection of infrastructure and tools for research in ""\n        ""neural network interpretability.""\n    ),\n    author=""The Lucid Authors"",\n    author_email=""deepviz@google.com"",\n    url=""https://github.com/tensorflow/lucid"",\n    download_url=(\n        ""https://github.com/tensorflow/lucid"" ""/archive/v{}.tar.gz"".format(version)\n    ),\n    license=""Apache License 2.0"",\n    keywords=[\n        ""tensorflow"",\n        ""tensor"",\n        ""machine learning"",\n        ""neural networks"",\n        ""convolutional neural networks"",\n        ""feature visualization"",\n        ""optimization"",\n    ],\n    install_requires=[\n        ""numpy"",\n        ""scipy"",\n        ""scikit-learn"",\n        ""umap-learn"",\n        ""nltk"",\n        ""ipython"",\n        ""pillow"",\n        ""future"",\n        ""decorator"",\n        ""pyopengl"",\n        ""filelock"",\n        ""cachetools"",\n        ""more-itertools"",\n    ],\n    setup_requires=[""pytest-runner""],\n    tests_require=test_deps,\n    extras_require=extras,\n    classifiers=[\n        ""Development Status :: 3 - Alpha"",\n        ""Intended Audience :: Science/Research"",\n        ""Intended Audience :: Education"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Natural Language :: English"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Topic :: Scientific/Engineering"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Scientific/Engineering :: Mathematics"",\n        ""Topic :: Scientific/Engineering :: Visualization"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n    ],\n)\n'"
lucid/__init__.py,0,"b'"""""" Logging in lucid\n\nLucid uses Python\'s default logging system.\nEach module has it\'s own logger, so you can control the verbosity of each\nmodule to your needs. To change the overall log level, you can set log levels\nat each part of the module hierarchy, including simply at the root, `lucid`:\n\n```python\nimport logging\nlogging.getLogger(\'lucid\').setLevel(logging.INFO)\n```\n\nOur log levels also confirm to Python\'s defaults:\n\nDEBUG\tDetailed information typically of interest only when diagnosing problems.\nINFO\tConfirmation that things are working as expected.\nWARN\tAn indication that something unexpected happened.\nERROR\tThe software has not been able to perform some function.\nCRITICAL\tThe program itself may be unable to continue running.\n\n""""""\n\nimport logging\nimport warnings\n\nlogging.basicConfig(level=logging.WARN)\ndel logging\n\n# silence unnecessarily loud TF warnings that we can\'t do anything about\nwarnings.filterwarnings(""ignore"", category=DeprecationWarning, module=""tensorflow"")\nwarnings.filterwarnings(""ignore"", category=DeprecationWarning, module=""google"")\nwarnings.filterwarnings(""ignore"", module=""tensorflow.core.platform.cpu_feature_guard"")\n\n# Lucid uses a fixed random seed for reproducability. Use to seed sources of randomness.\nseed = 0\n\n# Set the lucid version - setup.py imports this value!\n__version__ = ""0.3.9""\n'"
tests/conftest.py,5,"b'import pytest\nimport tensorflow as tf\n\n\n@pytest.fixture\ndef minimodel():\n  def inner(input=None, shape=(16,16,3)):\n    """"""Constructs a tiny graph containing one each of a typical input\n    (tf.placegholder), variable and typical output (softmax) nodes.""""""\n    if input is None:\n      input = tf.placeholder(tf.float32, shape=shape, name=""input"")\n    w = tf.Variable(0.1, name=""variable"")\n    logits = tf.reduce_mean(w*input, name=""output"", axis=(0,1))\n    return tf.nn.softmax(logits)\n  return inner\n\n# Add support for a slow tests marker:\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        ""--run-slow"", action=""store_true"", default=False, help=""run slow tests""\n    )\n\n\ndef pytest_collection_modifyitems(config, items):\n    if config.getoption(""--run-slow""):\n        return\n    else:\n        skip_slow = pytest.mark.skip(reason=""need --runslow option to run"")\n        for item in items:\n            if ""slow"" in item.keywords:\n                item.add_marker(skip_slow)\n'"
tests/test_gl.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport os\nimport numpy as np\n\nHAVE_COLAB_NVIDIA = (os.path.exists(\'/usr/lib64-nvidia/\') and\n                     os.path.exists(\'/opt/bin/nvidia-smi\'))\n\n\nWIDTH, HEIGHT = 200, 100\n\nif HAVE_COLAB_NVIDIA:\n  from lucid.misc.gl import glcontext  # must be imported before OpenGL.GL\n  import OpenGL.GL as gl\n  from lucid.misc.gl import glrenderer\n  \n  glcontext.create_opengl_context((WIDTH, HEIGHT))\n\n\n@pytest.mark.skipif(not HAVE_COLAB_NVIDIA, reason=""GPU Colab kernel only"")\ndef test_gl_context():\n  # Render triangle\n  gl.glClear(gl.GL_COLOR_BUFFER_BIT)\n  gl.glBegin(gl.GL_TRIANGLES)\n  gl.glColor3f(1, 0, 0)\n  gl.glVertex2f(0,  1)\n  \n  gl.glColor3f(0, 1, 0)\n  gl.glVertex2f(-1, -1)\n  \n  gl.glColor3f(0, 0, 1)\n  gl.glVertex2f(1, -1)\n  gl.glEnd()\n\n  # Read result\n  img_buf = gl.glReadPixels(0, 0, WIDTH, HEIGHT, gl.GL_RGB, gl.GL_UNSIGNED_BYTE)    \n  img = np.frombuffer(img_buf, np.uint8).reshape(HEIGHT, WIDTH, 3)[::-1]\n\n  assert all(img[0, 0] == 0)  # black corner\n  assert all(img[0,-1] == 0)  # black corner\n  assert img[10, WIDTH//2].argmax() == 0  # red corner\n  assert img[-1,  10].argmax() == 1  # green corner    \n  assert img[-1, -10].argmax() == 2  # blue corner\n  \n\n@pytest.mark.skipif(not HAVE_COLAB_NVIDIA, reason=""GPU Colab kernel only"")\ndef test_glrenderer():\n  w, h = 400, 200\n  renderer = glrenderer.MeshRenderer((w, h))\n  renderer.fovy = 90\n  position = [[0, 1, -1], [-2, -1,-1], [2, -1, -1]]\n  color = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n  img = renderer.render_mesh(position, color)\n  img, alpha = img[..., :3], img[..., 3]\n  \n  assert all(img[0, 0] == 0)  # black corner\n  assert all(img[0,-1] == 0)  # black corner\n  assert img[10, w//2].argmax() == 0  # red corner\n  assert img[-1,  10].argmax() == 1  # green corner    \n  assert img[-1, -10].argmax() == 2  # blue corner\n  assert np.abs(img.sum(-1)-alpha).max() < 1e-5'"
tests/test_meshutil.py,0,"b""import pytest\n\nimport io\nimport numpy as np\nfrom lucid.misc.gl import meshutil\n\ntest_obj = u'''\n# comment\n#\nv 0 0 0\nv 0 1 0\nv 1 1 0\nv 1 0 0\nvt 0 0\nvt 0 1\nvt 1 1\nvt 1 0\nvn 1 0 1\nvn 0 1 1\nvn 1 1 1\nvn 1 0 1\nf 1 2 3\nf 2/2/2 3/3/3 4/4/4\nf 1//1 2//2 3//3 4//4\n'''\n\n\ndef test_load_obj():\n  f = io.StringIO(test_obj)\n  mesh = meshutil.load_obj(f)\n  vert_n = len(mesh['position'])\n  assert vert_n == 10\n  assert mesh['position'].shape == (vert_n, 3)\n  assert mesh['normal'].shape == (vert_n, 3)\n  assert mesh['uv'].shape == (vert_n, 2)\n  assert mesh['face'].shape == ((1+1+2)*3,)\n  \n  \ndef test_lookat():\n  eye = [1, 2, 3]\n  target = [-2, 1, 0]\n  up = [0, 1, 0]\n  M = meshutil.lookat(eye, target, up)\n  assert all(M[-1] == [0, 0, 0, 1])\n  eps = 1e-6\n  assert np.abs(meshutil.homotrans(M, eye)).max() < eps\n  \n  dist = meshutil.anorm(np.float32(eye)-target)\n  assert np.abs(meshutil.homotrans(M, target) - [0, 0, -dist]).max() < eps"""
lucid/misc/__init__.py,0,b''
lucid/misc/channel_reducer.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helper for using sklearn.decomposition on high-dimensional tensors.\n\nProvides ChannelReducer, a wrapper around sklearn.decomposition to help them\napply to arbitrary rank tensors. It saves lots of annoying reshaping.\n""""""\n\nimport numpy as np\nimport sklearn.decomposition\n\ntry:\n    from sklearn.decomposition.base import BaseEstimator\nexcept AttributeError:\n    from sklearn.base import BaseEstimator\n\n\nclass ChannelReducer(object):\n  """"""Helper for dimensionality reduction to the innermost dimension of a tensor.\n\n  This class wraps sklearn.decomposition classes to help them apply to arbitrary\n  rank tensors. It saves lots of annoying reshaping.\n\n  See the original sklearn.decomposition documentation:\n  http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n  """"""\n\n  def __init__(self, n_components=3, reduction_alg=""NMF"", **kwargs):\n    """"""Constructor for ChannelReducer.\n\n    Inputs:\n      n_components: Numer of dimensions to reduce inner most dimension to.\n      reduction_alg: A string or sklearn.decomposition class. Defaults to\n        ""NMF"" (non-negative matrix facotrization). Other options include:\n        ""PCA"", ""FastICA"", and ""MiniBatchDictionaryLearning"". The name of any of\n        the sklearn.decomposition classes will work, though.\n      kwargs: Additional kwargs to be passed on to the reducer.\n    """"""\n\n    if not isinstance(n_components, int):\n      raise ValueError(""n_components must be an int, not \'%s\'."" % n_components)\n\n    # Defensively look up reduction_alg if it is a string and give useful errors.\n    algorithm_map = {}\n    for name in dir(sklearn.decomposition):\n      obj = sklearn.decomposition.__getattribute__(name)\n      if isinstance(obj, type) and issubclass(obj, BaseEstimator):\n        algorithm_map[name] = obj\n    if isinstance(reduction_alg, str):\n      if reduction_alg in algorithm_map:\n        reduction_alg = algorithm_map[reduction_alg]\n      else:\n        raise ValueError(""Unknown dimensionality reduction method \'%s\'."" % reduction_alg)\n\n\n    self.n_components = n_components\n    self._reducer = reduction_alg(n_components=n_components, **kwargs)\n    self._is_fit = False\n\n  @classmethod\n  def _apply_flat(cls, f, acts):\n    """"""Utility for applying f to inner dimension of acts.\n\n    Flattens acts into a 2D tensor, applies f, then unflattens so that all\n    dimesnions except innermost are unchanged.\n    """"""\n    orig_shape = acts.shape\n    acts_flat = acts.reshape([-1, acts.shape[-1]])\n    new_flat = f(acts_flat)\n    if not isinstance(new_flat, np.ndarray):\n      return new_flat\n    shape = list(orig_shape[:-1]) + [-1]\n    return new_flat.reshape(shape)\n\n  def fit(self, acts):\n    self._is_fit = True\n    return ChannelReducer._apply_flat(self._reducer.fit, acts)\n\n  def fit_transform(self, acts):\n    self._is_fit = True\n    return ChannelReducer._apply_flat(self._reducer.fit_transform, acts)\n\n  def transform(self, acts):\n    return ChannelReducer._apply_flat(self._reducer.transform, acts)\n\n  def __call__(self, acts):\n    if self._is_fit:\n      return self.transform(acts)\n    else:\n      return self.fit_transform(acts)\n\n  def __getattr__(self, name):\n    if name in self.__dict__:\n      return self.__dict__[name]\n    elif name + ""_"" in self._reducer.__dict__:\n      return self._reducer.__dict__[name+""_""]\n\n  def __dir__(self):\n    dynamic_attrs = [name[:-1]\n                     for name in dir(self._reducer)\n                     if name[-1] == ""_"" and name[0] != ""_""\n                    ]\n\n    return list(ChannelReducer.__dict__.keys()) + list(self.__dict__.keys()) + dynamic_attrs\n'"
lucid/misc/convert_matplotlib.py,0,"b'import io\nimport numpy as np\nfrom PIL import Image\n\ndef matplotlib_to_numpy(plt):\n  """"""Convert a matplotlib plot to a numpy array represent it as an image.\n\n  Inputs:\n    plot - matplotlib plot\n\n  Returns:\n    A numpy array with shape [W, H, 3], representing RGB values between 0 and 1.\n  """"""\n  \n  f = io.BytesIO()\n  plt.savefig(f, format=""png"")\n  f.seek(0)\n  arr = np.array(Image.open(f)).copy()\n  f.close()\n  return arr/255.\n'"
lucid/misc/environment.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Ensuring compatibilty across environments, e.g. Jupyter/Colab/Shell.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\n\ndef is_notebook_environment():\n  try:\n    shell = get_ipython().__class__.__name__\n    if shell == \'ZMQInteractiveShell\':\n      return True   # IPython Notebook\n    elif shell == \'Shell\':\n      return True   # Colaboratory Notebook\n    elif shell == \'TerminalInteractiveShell\':\n      return False  # Terminal running IPython\n    else:\n      return False  # Other unknown type (?)\n  except NameError:\n    return False    # Probably standard Python interpreter\n'"
lucid/misc/gradient_override.py,5,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Easily declare tensorflow functions with custom gradients.\n\nNormally, overriding gradients in TensorFlow requires you to register the\ngradient function to a string and then use a with block with\ngraph.gradient_override_map().\n\nThere\'s a bunch of things about this that are annoying:\n\n(1) You have to pick a string\n(2) You can only use a string once, so if you\'re prototyping you need to\n    generate a new name every time you modify the function.\n(3) You have to make annoying with blocks all the time and it doesn\'t feel\n    very functional.\n\nThis abstraction solves those issues. You no longer need to think about strings\nor with blocks. Just use a single decorator and everything else will be\nhandled for you.\n\nIf you don\'t need to serialize your graph and the gradient override isn\'t\nperformance critical, you can use the high level `use_gradient()` decorator:\n\n  @use_gradient(_foo_grad)\n  def foo(x): ...\n\nOtherwise, you can use use the lower level `gradient_override_map()`, a\nconvenience wrapper for `graph.gradient_override_map()`.\n""""""\n\nfrom contextlib import contextmanager\nimport numpy as np\nimport tensorflow as tf\nimport uuid\n\n\ndef register_to_random_name(grad_f):\n  """"""Register a gradient function to a random string.\n\n  In order to use a custom gradient in TensorFlow, it must be registered to a\n  string. This is both a hassle, and -- because only one function can every be\n  registered to a string -- annoying to iterate on in an interactive\n  environemnt.\n\n  This function registers a function to a unique random string of the form:\n\n    {FUNCTION_NAME}_{RANDOM_SALT}\n\n  And then returns the random string. This is a helper in creating more\n  convenient gradient overrides.\n\n  Args:\n    grad_f: gradient function to register. Should map (op, grad) -> grad(s)\n\n  Returns:\n    String that gradient function was registered to.\n  """"""\n  grad_f_name = grad_f.__name__ + ""_"" + str(uuid.uuid4())\n  tf.RegisterGradient(grad_f_name)(grad_f)\n  return grad_f_name\n\n\n@contextmanager\ndef gradient_override_map(override_dict):\n  """"""Convenience wrapper for graph.gradient_override_map().\n\n  This functions provides two conveniences over normal tensorflow gradient\n  overrides: it auomatically uses the default graph instead of you needing to\n  find the graph, and it automatically\n\n  Example:\n\n    def _foo_grad_alt(op, grad): ...\n\n    with gradient_override({""Foo"": _foo_grad_alt}):\n\n  Args:\n    override_dict: A dictionary describing how to override the gradient.\n      keys: strings correponding to the op type that should have their gradient\n        overriden.\n      values: functions or strings registered to gradient functions\n\n  """"""\n  override_dict_by_name = {}\n  for (op_name, grad_f) in override_dict.items():\n    if isinstance(grad_f, str):\n       override_dict_by_name[op_name] = grad_f\n    else:\n      override_dict_by_name[op_name] = register_to_random_name(grad_f)\n  with tf.get_default_graph().gradient_override_map(override_dict_by_name):\n    yield\n\n\ndef use_gradient(grad_f):\n  """"""Decorator for easily setting custom gradients for TensorFlow functions.\n\n  * DO NOT use this function if you need to serialize your graph.\n  * This function will cause the decorated function to run slower.\n\n  Example:\n\n    def _foo_grad(op, grad): ...\n\n    @use_gradient(_foo_grad)\n    def foo(x1, x2, x3): ...\n\n  Args:\n    grad_f: function to use as gradient.\n\n  Returns:\n    A decorator to apply to the function you wish to override the gradient of.\n\n  """"""\n  grad_f_name = register_to_random_name(grad_f)\n\n  def function_wrapper(f):\n    def inner(*inputs):\n\n      # TensorFlow only supports (as of writing) overriding the gradient of\n      # individual ops. In order to override the gardient of `f`, we need to\n      # somehow make it appear to be an individual TensorFlow op.\n      #\n      # Our solution is to create a PyFunc that mimics `f`.\n      #\n      # In particular, we construct a graph for `f` and run it, then use a\n      # stateful PyFunc to stash it\'s results in Python. Then we have another\n      # PyFunc mimic it by taking all the same inputs and returning the stashed\n      # output.\n      #\n      # I wish we could do this without PyFunc, but I don\'t see a way to have\n      # it be fully general.\n\n      state = {""out_value"": None}\n\n      # First, we need to run `f` and store it\'s output.\n\n      out = f(*inputs)\n\n      def store_out(out_value):\n        """"""Store the value of out to a python variable.""""""\n        state[""out_value""] = out_value\n\n      store_name = ""store_"" + f.__name__\n      store = tf.py_func(store_out, [out], (), stateful=True, name=store_name)\n\n      # Next, we create the mock function, with an overriden gradient.\n      # Note that we need to make sure store gets evaluated before the mock\n      # runs.\n\n      def mock_f(*inputs):\n        """"""Mimic f by retrieving the stored value of out.""""""\n        return state[""out_value""]\n\n      with tf.control_dependencies([store]):\n        with gradient_override_map({""PyFunc"": grad_f_name}):\n          mock_name = ""mock_"" + f.__name__\n          mock_out = tf.py_func(mock_f, inputs, out.dtype, stateful=True,\n                                name=mock_name)\n          mock_out.set_shape(out.get_shape())\n\n      # Finally, we can return the mock.\n\n      return mock_out\n    return inner\n  return function_wrapper\n'"
lucid/misc/iter_nd_utils.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helpers for doing gnerator/iterable style workflows in n-dimensions.""""""\n\nimport itertools\nimport types\nfrom collections.abc import Iterable\nimport numpy as np\n\n\ndef recursive_enumerate_nd(it, stop_iter=None, prefix=()):\n  """"""Recursively enumerate nested iterables with tuples n-dimenional indices.\n\n  Arguments:\n    it: object to be enumerated\n    stop_iter: User defined funciton which can conditionally block further\n      iteration. Defaults to allowing iteration.\n    prefix: index prefix (not intended for end users)\n\n  Yields:\n    (tuple representing n-dimensional index, original iterator value)\n\n  Example use:\n    it = ((x+y for y in range(10) )\n               for x in range(10) )\n    recursive_enumerate_nd(it) # yields things like ((9,9), 18)\n\n  Example stop_iter:\n    stop_iter = lambda x: isinstance(x, np.ndarray) and len(x.shape) <= 3\n    # this prevents iteration into the last three levels (eg. x,y,channels) of\n    # a numpy ndarray\n\n  """"""\n  if stop_iter is None:\n    stop_iter = lambda x: False\n\n  for n, x in enumerate(it):\n    n_ = prefix + (n,)\n    if isinstance(x, Iterable) and (not stop_iter(x)):\n      yield from recursive_enumerate_nd(x, stop_iter=stop_iter, prefix=n_)\n    else:\n      yield (n_, x)\n\n\ndef dict_to_ndarray(d):\n  """"""Convert a dictionary representation of an array (keys as indices) into a ndarray.\n\n  Args:\n    d: dict to be converted.\n\n  Converts a dictionary representation of a sparse array into a ndarray. Array\n  shape infered from maximum indices. Entries default to zero if unfilled.\n\n  Example:\n    >>> dict_to_ndarray({(0,0) : 3, (1,1) : 7})\n    [[3, 0],\n     [0, 7]]\n\n  """"""\n  assert len(d), ""Dictionary passed to dict_to_ndarray() must not be empty.""\n  inds = list(d.keys())\n  ind_dims = len(inds[0])\n  assert all(len(ind) == ind_dims for ind in inds)\n  ind_shape = [max(ind[i]+1 for ind in inds) for i in range(ind_dims)]\n\n  val0  = d[inds[0]]\n  if isinstance(val0, np.ndarray):\n    arr = np.zeros(ind_shape + list(val0.shape), dtype=val0.dtype)\n  else:\n    arr = np.zeros(ind_shape, dtype=""float32"")\n\n  for ind, val in d.items():\n    arr[ind] = val\n  return arr\n\n\ndef batch_iter(it, batch_size=64):\n  """"""Iterate through an iterable object in batches.""""""\n  while True:\n    batch = list(itertools.islice(it, batch_size))\n    if not batch: break\n    yield batch\n'"
lucid/misc/ndimage_utils.py,0,"b'# Copyright 2019 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\nfrom scipy import ndimage\n\n\ndef resize(image, target_size=None, ratios=None, **kwargs):\n    """"""Resize an ndarray image of rank 3 or 4.\n    target_size can be a tuple `(width, height)` or scalar `width`.\n    Alternatively you can directly specify the ratios by which each\n    dimension should be scaled, or a single ratio""""""\n\n    # input validation\n    if target_size is None:\n      assert ratios is not None\n    else:\n      if isinstance(target_size, int):\n          target_size = (target_size, target_size)\n\n      if not isinstance(target_size, (list, tuple, np.ndarray)):\n          message = (\n              ""`target_size` should be a single number (width) or a list""\n              ""/tuple/ndarray (width, height), not {}."".format(type(target_size))\n          )\n          raise ValueError(message)\n\n    rank = len(image.shape)\n    assert 3 <= rank <= 4\n\n    original_size = image.shape[-3:-1]\n\n\n    ratios_are_noop = all(ratio == 1 for ratio in ratios) if ratios is not None else False\n    target_size_is_noop = target_size == original_size if target_size is not None else False\n    if ratios_are_noop or target_size_is_noop:\n        return image  # noop return because ndimage.zoom doesn\'t check itself\n\n    # TODO: maybe allow -1 in target_size to signify aspect-ratio preserving resize?\n    ratios = ratios or [t / o for t, o in zip(target_size, original_size)]\n    zoom = [1] * rank\n    zoom[-3:-1] = ratios\n\n    roughly_resized = ndimage.zoom(image, zoom, **kwargs)\n    if target_size is not None:\n      return roughly_resized[..., : target_size[0], : target_size[1], :]\n    else:\n      return roughly_resized\n\n\ndef composite(\n    background_image,\n    foreground_image,\n    foreground_width_ratio=0.25,\n    foreground_position=(0.0, 0.0),\n):\n    """"""Takes two images and composites them.""""""\n\n    if foreground_width_ratio <= 0:\n        return background_image\n\n    composite = background_image.copy()\n    width = int(foreground_width_ratio * background_image.shape[1])\n    foreground_resized = resize(foreground_image, width)\n    size = foreground_resized.shape\n\n    x = int(foreground_position[1] * (background_image.shape[1] - size[1]))\n    y = int(foreground_position[0] * (background_image.shape[0] - size[0]))\n\n    # TODO: warn if resulting coordinates are out of bounds?\n    composite[y : y + size[0], x : x + size[1]] = foreground_resized\n\n    return composite\n\n\ndef soft_alpha_blend(image_with_alpha, amount_background=.333, gamma=2.2):\n    assert image_with_alpha.shape[-1] == 4\n\n    alpha = image_with_alpha[..., -1][..., np.newaxis]\n    rgb = image_with_alpha[..., :3]\n    white = np.ones_like(rgb)\n    background = amount_background * rgb + (1 - amount_background) * white\n    blended = np.power(\n        (alpha * np.power(rgb, gamma) + (1 - alpha) * np.power(background, gamma)),\n        1 / gamma,\n    )\n    return blended\n\n'"
lucid/misc/redirected_relu_grad.py,30,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Redirected ReLu Gradient Overrides\n\nWhen we visualize ReLU networks, the initial random input we give the model may\nnot cause the neuron we\'re visualizing to fire at all. For a ReLU neuron, this\nmeans that no gradient flow backwards and the visualization never takes off.\nOne solution would be to find the pre-ReLU tensor, but that can be tedious.\n\nThese functions provide a more convenient solution: temporarily override the\ngradient of ReLUs to allow gradient to flow back through the ReLU -- even if it\ndidn\'t activate and had a derivative of zero -- allowing the visualization\nprocess to get started. These functions override the gradient for at most 16\nsteps. Thus, you need to initialize `global_step` before using these functions.\n\nUsage:\n```python\nfrom lucid.misc.gradient_override import gradient_override_map\nfrom lucid.misc.redirected_relu_grad import redirected_relu_grad\n\n...\nglobal_step_t = tf.train.get_or_create_global_step()\ninit_global_step_op = tf.variables_initializer([global_step_t])\ninit_global_step_op.run()\n...\n\nwith gradient_override_map({\'Relu\': redirected_relu_grad}):\n  model.import_graph(...)\n```\n\nDiscussion:\nReLus block the flow of the gradient during backpropagation when their input is\nnegative. ReLu6s also do so when the input is larger than 6. These overrides\nchange this behavior to allow gradient pushing the input into a desired regime\nbetween these points.\n\n(This override first checks if the entire gradient would be blocked, and only\nchanges it in that case. It does this check independently for each batch entry.)\n\nIn effect, this replaces the relu gradient with the following:\n\nRegime       | Effect\n============================================================\n 0 <= x <= 6 | pass through gradient\n x < 0       | pass through gradient pushing the input up\n x > 6       | pass through gradient pushing the input down\n\nOr visually:\n\n  ReLu:                     |   |____________\n                            |  /|\n                            | / |\n                ____________|/  |\n                            0   6\n\n  Override:     ------------|   |------------\n                  allow  ->       <-  allow\n\nOur implementations contains one extra complication:\ntf.train.Optimizer performs gradient _descent_, so in the update step the\noptimizer changes values in the opposite direction of the gradient. Thus, the\nsign of the gradient in our overrides has the opposite of the intuitive effect:\nnegative gradient pushes the input up, positive pushes it down.\nThus, the code below only allows _negative_ gradient when the input is already\nnegative, and allows _positive_ gradient when the input is already above 6.\n\n\n[0] That is because many model architectures don\'t provide easy access\nto pre-relu tensors. For example, GoogLeNet\'s mixed__ layers are passed through\nan activation function before being concatenated. We are still interested in the\nentire concatenated layer, we would just like to skip the activation function.\n""""""\n\nimport tensorflow as tf\n\n\ndef redirected_relu_grad(op, grad):\n  assert op.type == ""Relu""\n  x = op.inputs[0]\n\n  # Compute ReLu gradient\n  relu_grad = tf.where(x < 0., tf.zeros_like(grad), grad)\n\n  # Compute redirected gradient: where do we need to zero out incoming gradient\n  # to prevent input going lower if its already negative\n  neg_pushing_lower = tf.logical_and(x < 0., grad > 0.)\n  redirected_grad = tf.where(neg_pushing_lower, tf.zeros_like(grad), grad)\n\n  # Ensure we have at least a rank 2 tensor, as we expect a batch dimension\n  assert_op = tf.Assert(tf.greater(tf.rank(relu_grad), 1), [tf.rank(relu_grad)])\n  with tf.control_dependencies([assert_op]):\n    # only use redirected gradient where nothing got through original gradient\n    batch = tf.shape(relu_grad)[0]\n    reshaped_relu_grad = tf.reshape(relu_grad, [batch, -1])\n    relu_grad_mag = tf.norm(reshaped_relu_grad, axis=1)\n  result_grad = tf.where(relu_grad_mag > 0., relu_grad, redirected_grad)\n\n  global_step_t =tf.train.get_or_create_global_step()\n  return_relu_grad = tf.greater(global_step_t, tf.constant(16, tf.int64))\n\n  return tf.where(return_relu_grad, relu_grad, result_grad)\n\n\ndef redirected_relu6_grad(op, grad):\n  assert op.type == ""Relu6""\n  x = op.inputs[0]\n\n  # Compute ReLu gradient\n  relu6_cond = tf.logical_or(x < 0., x > 6.)\n  relu_grad = tf.where(relu6_cond, tf.zeros_like(grad), grad)\n\n  # Compute redirected gradient: where do we need to zero out incoming gradient\n  # to prevent input going lower if its already negative, or going higher if\n  # already bigger than 6?\n  neg_pushing_lower = tf.logical_and(x < 0., grad > 0.)\n  pos_pushing_higher = tf.logical_and(x > 6., grad < 0.)\n  dir_filter = tf.logical_or(neg_pushing_lower, pos_pushing_higher)\n  redirected_grad = tf.where(dir_filter, tf.zeros_like(grad), grad)\n\n  # Ensure we have at least a rank 2 tensor, as we expect a batch dimension\n  assert_op = tf.Assert(tf.greater(tf.rank(relu_grad), 1), [tf.rank(relu_grad)])\n  with tf.control_dependencies([assert_op]):\n    # only use redirected gradient where nothing got through original gradient\n    batch = tf.shape(relu_grad)[0]\n    reshaped_relu_grad = tf.reshape(relu_grad, [batch, -1])\n    relu_grad_mag = tf.norm(reshaped_relu_grad, axis=1)\n  result_grad =  tf.where(relu_grad_mag > 0., relu_grad, redirected_grad)\n\n  global_step_t = tf.train.get_or_create_global_step()\n  return_relu_grad = tf.greater(global_step_t, tf.constant(16, tf.int64))\n\n  return tf.where(return_relu_grad, relu_grad, result_grad)\n'"
lucid/misc/stimuli.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helpers for generating synthetic stimuli for probing network behavior.""""""\n\nimport numpy as np\n\n\ndef sample_binary_image(size, alias_factor=10, color_a=(1,1,1), color_b=(0,0,0),\n                        boundary_line=False, boundary_width=1,\n                        blur_beyond_radius=None, fade_beyond_radius=None,\n                        fade_over_distance=10, fade_color=(.5, .5, .5), **kwds):\n  """"""Highly flexible tool for sampling binary images.\n\n  Many stimuli of interest are ""binary"" in that they have two regions. For\n  example, a curve stimuli has an interio and exterior region. Ideally, such a\n  stimulus should be rendered with antialiasing. Additionlly, there are many\n  styling options that effect how one might wish to render the image: selecting\n  the color for interior and exterior, showing the boundary between the regions\n  instead of interior vs exterior, and much more.\n\n  This function provides a flexible rendering tool that supports many options.\n  We assume the image is reprented in the ""f-rep"" or implicit funciton\n  convention: the image is represented by a function which maps (x,y) values\n  to a sclar, with negative representing the object interior and positive\n  representing the exterior.\n\n  The general usage would look smething like:\n\n      @sample_binary_image(size, more_options)\n      def img(x,y):\n        return (negative if interior, positive if exterior)\n\n  Or alternatively:\n\n      sampler = sample_binary_image(size, more_options)\n      def img_f(x,y):\n        return (negative if interior, positive if exterior)\n      img = sampler(img_f)\n\n  Arguments:\n    size: Size of image to be rendered in pixels.\n    alias_factor: Number of samples to use in aliasing.\n    color_a: Color of exterior. A 3-tuple of floats between 0 and 1. Defaults\n      to white (1,1,1).\n    color_b: Color of interior or boundary. A 3-tuple of floats between 0 and 1.\n      Defaults to black (0,0,0).\n    boundary_line: Draw boundary instead of interior vs exterior.\n    boundary_width: If drawing boundary, number of pixels wide boundary line\n      should be. Defaults to 1 pixel.\n    blur_beyond_radius: If not None, blur the image outside a given radius.\n      Defaults to None.\n    fade_beyond_radius: If not None, fade the image to fade_color outside a\n      given radius. Defaults to None.\n    fade_over_distance: Controls rate of fading.\n    fade_color: Color to fade to, if fade_beyond_radius is set. Defaults to\n      (.5, .5, .5).\n\n  Returns:\n    A function which takes a function mapping (x,y) -> float and returns a\n    numpy array of shape [size, size, 3].\n  """"""\n\n\n  # Initial setup\n  color_a, color_b = np.asarray(color_a).reshape([1,1,3]), np.asarray(color_b).reshape([1,1,3])\n  fade_color = np.asarray(fade_color).reshape([1,1,3])\n  X = (np.arange(size) - size//2)\n  X, Y = X[None, :], X[:, None]\n  alias_offsets = [ tuple(np.random.uniform(-.5, .5, size=2)) for n in range(alias_factor) ]\n  boundary_offsets = [ (boundary_width*np.cos(2*np.pi*n/16.), boundary_width*np.sin(2*np.pi*n/16.)) for n in range(16) ]\n\n  # Setup for blur / fade stuff\n  radius = np.sqrt(X**2+Y**2)\n  offset_scale = 1\n  fade_coef = 0\n  if blur_beyond_radius is not None:\n    offset_scale += np.maximum(0, radius-blur_beyond_radius)\n  if fade_beyond_radius is not None:\n    fade_coef = np.maximum(0, radius-fade_beyond_radius)\n    fade_coef /= float(fade_over_distance)\n    fade_coef = np.clip(fade_coef, 0, 1)[..., None]\n\n  # The function we\'ll return.\n  # E is an ""energy function"" mapping (x,y) -> float\n  # (and vectorized for numpy support)\n  # such that it is negative on interior reigions and positive on exterior ones.\n  def sampler(E):\n\n    # Naively smaple an image\n    def sample(x_off, y_off):\n      # note: offset_scale controls blurring\n      vals = E(X + offset_scale * x_off, Y + offset_scale * y_off)\n      return np.greater_equal(vals, 0).astype(""float32"")\n\n    def boundary_sample(x_off, y_off):\n      imgs = [sample(x_off + bd_off_x, y_off + bd_off_y)\n              for bd_off_x, bd_off_y in boundary_offsets]\n      # If we are on the boundary, some smaples will be zero and others one.\n      # as a result, the mean will be in the middle.\n      vals = np.mean(imgs, axis=0)\n      vals = 2*np.abs(vals-0.5)\n      return np.greater_equal(vals, 0.99).astype(""float32"")\n\n    # Sample anti-aliased image\n    sampler = boundary_sample if boundary_line else sample\n    img = np.mean([sampler(*offset) for offset in alias_offsets], axis=0)\n    img = np.clip(img, 0, 1)[..., None]\n\n    # final transformations to colorize and fade\n    img = img*color_a + (1-img)*color_b\n    img = (1-fade_coef)*img + fade_coef*fade_color\n    return img\n  return sampler\n\n\ndef rmin(X, Y, r):\n  """"""A  ""rounded minimum"" function.\n\n  Useful for creating bevelled intersections with implicit geometry image\n  representations.\n\n  See Olah (2011), ""Manipulation of Implicit Functions (With an Eye on CAD)"".\n  https://christopherolah.wordpress.com/2011/11/06/manipulation-of-implicit-functions-with-an-eye-on-cad/\n  """"""\n  r_ = np.maximum(r, 1e-6)\n  v1 = Y + r*np.sin(np.pi/4 + np.arcsin(np.clip((X-Y)/r_/np.sqrt(2), -1, 1))) - r\n  v2 = np.minimum(X,Y)\n  cond = np.less_equal(np.abs(X-Y), r)\n  return np.where(cond, v1, v2)\n\n\ndef rounded_corner(orientation, r, angular_width=90, size=224, **kwds):\n  """"""Generate a ""rounded corner"" stimulus.\n\n  This function is a flexible generator of ""rounded corner"" stimuli. It returns\n  an image, represented as a numpy array of shape [size, size, 3].\n\n  Arguments:\n    orientation: The orientation of the curve, in degrees.\n    r: radius of the curve\n    angular_width: when r=0 and we have sharp corner, this controls the angle\n      of the corner. For other radius values, it controls the sharpness of the\n      corner and eventual divergence of the outer lines. Specified in degrees,\n      defaults to 90 to give a traditional curve.\n    size: Size of image.\n\n    Note: Also inherits many arguments from sample_binary_image().\n\n  Returns:\n    An image, represented as a numpy array of shape [size, size, 3].\n  """"""\n  orientation *= 2 * np.pi / 360.\n  orientation += np.pi/2.\n  angular_width *= 2 * np.pi / 360.\n  ang1, ang2 = orientation - angular_width / 2., orientation + angular_width / 2. + np.pi\n  @sample_binary_image(size, **kwds)\n  def img(X, Y):\n    X_, Y_ = np.cos(ang1)*X + np.sin(ang1)*Y,  np.cos(ang2)*X + np.sin(ang2)*Y\n    X_, Y_ = X_ + r*(1-1/np.sqrt(2.)), Y_ + r*(1-1/np.sqrt(2.))\n    E = rmin(X_,Y_,r)\n    return 10*E\n  return img\n'"
lucid/misc/tfutil.py,3,"b'# Copyright 2019 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\n\ndef create_session(target=\'\', timeout_sec=10):\n  \'\'\'Create an intractive TensorFlow session.\n\n  Helper function that creates TF session that uses growing GPU memory\n  allocation and opration timeout. \'allow_growth\' flag prevents TF\n  from allocating the whole GPU memory an once, which is useful\n  when having multiple python sessions sharing the same GPU.\n  \'\'\'\n  graph = tf.Graph()\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  config.operation_timeout_in_ms = int(timeout_sec*1000)\n  return tf.InteractiveSession(target=target, graph=graph, config=config)\n'"
lucid/modelzoo/__init__.py,0,b''
lucid/modelzoo/aligned_activations.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helper code for downloading and using previously collected aligned activations.\nSee recipes > collection for code that collected them in teh first place.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom lucid.misc.io.sanitizing import sanitize\nfrom lucid.misc.io import load\n\nimport numpy as np\nfrom cachetools.func import lru_cache\n\nPATH_TEMPLATE = ""gs://modelzoo/aligned-activations/{}/{}-{:05d}-of-01000.npy""\nPAGE_SIZE = 10000\nNUMBER_OF_AVAILABLE_SAMPLES = 100000\nassert NUMBER_OF_AVAILABLE_SAMPLES % PAGE_SIZE == 0\nNUMBER_OF_PAGES = NUMBER_OF_AVAILABLE_SAMPLES // PAGE_SIZE\n\n\n@lru_cache()\ndef get_aligned_activations(layer) -> np.ndarray:\n    """"""Downloads 100k activations of the specified layer sampled from iterating over\n    ImageNet. Activations of all layers where sampled at the same spatial positions for\n    each image, allowing the calculation of correlations.""""""\n    activation_paths = [\n        PATH_TEMPLATE.format(\n            sanitize(layer.model_name), sanitize(layer.name), page\n        )\n        for page in range(NUMBER_OF_PAGES)\n    ]\n    activations = np.vstack([load(path) for path in activation_paths])\n    assert np.all(np.isfinite(activations))\n    return activations\n\n\n@lru_cache()\ndef layer_covariance(layer1, layer2=None):\n    """"""Computes the covariance matrix between the neurons of two layers. If only one\n    layer is passed, computes the symmetric covariance matrix of that layer.""""""\n    layer2 = layer2 or layer1\n    act1, act2 = layer1.activations, layer2.activations\n    num_datapoints = act1.shape[0]  # cast to avoid numpy type promotion during division\n    return np.matmul(act1.T, act2) / float(num_datapoints)\n\n\n@lru_cache()\ndef layer_inverse_covariance(layer):\n    """"""Inverse of a layer\'s correlation matrix. Function exists mostly for caching.""""""\n    return np.linalg.inv(layer_covariance(layer))\n\n\ndef push_activations(activations, from_layer, to_layer):\n    """"""Push activations from one model to another using prerecorded correlations""""""\n    inverse_covariance_matrix = layer_inverse_covariance(from_layer)\n    activations_decorrelated = np.dot(inverse_covariance_matrix, activations.T).T\n    covariance_matrix = layer_covariance(from_layer, to_layer)\n    activation_recorrelated = np.dot(activations_decorrelated, covariance_matrix)\n    return activation_recorrelated\n'"
lucid/modelzoo/get_activations.py,2,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helpers for getting responses from models over large collections.""""""\n\n\nimport itertools\nfrom collections import defaultdict\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom lucid.misc.iter_nd_utils import recursive_enumerate_nd, dict_to_ndarray, batch_iter\n\n\ndef get_activations_iter(model, layer, generator, reducer=""mean"", batch_size=64,\n                         dtype=None, ind_shape=None, center_only=False):\n  """"""Collect center activtions of a layer over many images from an iterable obj.\n\n  Note: this is mostly intended for large synthetic families of images, where\n    you can cheaply generate them in Python. For collecting activations over,\n    say, ImageNet, there will be better workflows based on various dataset APIs\n    in TensorFlow.\n\n  Args:\n    model: model for activations to be collected from.\n    layer: layer (in model) for activtions to be collected from.\n    generator: An iterable object (intended to be a generator) which produces\n      tuples of the form (index, image). See details below.\n    reducer: How to combine activations if multiple images map to the same index.\n      Supports ""mean"", ""rms"", and ""max"".\n    batch_size: How many images from the generator should be processes at once?\n    dtype: determines dtype of returned data (defaults to model activation\n      dtype). Can be used to make function memory efficient.\n    ind_shape: Shape that indices can span. Optional, but makes function orders\n      of magnitiude more memory efficient.\n\n  Memory efficeincy:\n    Using ind_shape is the main tool for make this function memory efficient.\n    dtype=""float16"" can further help.\n\n  Returns:\n    A numpy array of shape [ind1, ind2, ..., layer_channels]\n  """"""\n\n\n  assert reducer in [""mean"", ""max"", ""rms""]\n  combiner, normalizer = {\n      ""mean"" : (lambda a,b: a+b,             lambda a,n: a/n         ),\n      ""rms""  : (lambda a,b: a+b**2,          lambda a,n: np.sqrt(a/n)),\n      ""max""  : (lambda a,b: np.maximum(a,b), lambda a,n: a           ),\n  }[reducer]\n\n  with tf.Graph().as_default(), tf.Session() as sess:\n    t_img = tf.placeholder(""float32"", [None, None, None, 3])\n    T = model.import_graph(t_img)\n    t_layer = T(layer)\n\n    responses = None\n    count = None\n\n    # # If we know the total length, let\'s give a progress bar\n    # if ind_shape is not None:\n    #   total = int(np.prod(ind_shape))\n    #   generator = tqdm(generator, total=total)\n\n    for batch in batch_iter(generator, batch_size=batch_size):\n\n      inds, imgs = [x[0] for x in batch], [x[1] for x in batch]\n\n      # Get activations (middle of image)\n      acts = t_layer.eval({t_img: imgs})\n      if center_only:\n        acts = acts[:, acts.shape[1]//2, acts.shape[2]//2]\n      if dtype is not None:\n        acts = acts.astype(dtype)\n\n      # On the first iteration of the loop, create objects to hold responses\n      # (we wanted to know acts.shape[-1] before creating it in the numpy case)\n      if responses is None:\n        # If we don\'t know what the extent of the indices will be in advance\n        # we need to use a dictionary to support dynamic range\n        if ind_shape is None:\n          responses = {}\n          count = defaultdict(lambda: 0)\n        # But if we do, we can use a much more efficient numpy array\n        else:\n          responses = np.zeros(list(ind_shape) + list(acts.shape[1:]),\n                               dtype=acts.dtype)\n          count = np.zeros(ind_shape, dtype=acts.dtype)\n\n\n      # Store each batch item in appropriate index, performing reduction\n      for ind, act in zip(inds, acts):\n        count[ind] += 1\n        if ind in responses:\n          responses[ind] = combiner(responses[ind], act)\n        else:\n          responses[ind] = act\n\n  # complete reduction as necessary, then return\n  # First the case where everything is in numpy\n  if isinstance(responses, np.ndarray):\n    count = np.maximum(count, 1e-6)[..., None]\n    return normalizer(responses, count)\n  # Then the dynamic shape dictionary case\n  else:\n    for k in responses:\n      count_ = np.maximum(count[k], 1e-6)[None].astype(acts.dtype)\n      responses[k] = normalizer(responses[k], count_)\n    return dict_to_ndarray(responses)\n\n\ndef get_activations(model, layer, examples, batch_size=64,\n                       dtype=None, ind_shape=None, center_only=False):\n  """"""Collect center activtions of a layer over an n-dimensional array of images.\n\n  Note: this is mostly intended for large synthetic families of images, where\n    you can cheaply generate them in Python. For collecting activations over,\n    say, ImageNet, there will be better workflows based on various dataset APIs\n    in TensorFlow.\n\n  Args:\n    model: model for activations to be collected from.\n    layer: layer (in model) for activtions to be collected from.\n    examples: A (potentially n-dimensional) array of images. Can be any nested\n      iterable object, including a generator, as long as the inner most objects\n      are a numpy array with at least 3 dimensions (image X, Y, channels=3).\n    batch_size: How many images should be processed at once?\n    dtype: determines dtype of returned data (defaults to model activation\n      dtype). Can be used to make function memory efficient.\n    ind_shape: Shape that the index (non-image) dimensions of examples. Makes\n      code much more memory efficient if examples is not a numpy array.\n\n  Memory efficeincy:\n    Have examples be a generator rather than an array of images; this allows\n    them to be lazily generated and not all stored in memory at once. Also\n    use ind_shape so that activations can be stored in an efficient data\n    structure. If you still have memory problems, dtype=""float16"" can probably\n    get you another 2x.\n\n  Returns:\n    A numpy array of shape [ind1, ind2, ..., layer_channels]\n  """"""\n\n\n  if ind_shape is None and isinstance(examples, np.ndarray):\n    ind_shape = examples.shape[:-3]\n\n  # Create a generator which recursive enumerates examples, stoppping at\n  # the third last dimesnion (ie. an individual iamge) if numpy arrays.\n  examples_enumerated = recursive_enumerate_nd(examples,\n    stop_iter = lambda x: isinstance(x, np.ndarray) and len(x.shape) <= 3)\n\n  # Get responses\n  return get_activations_iter(model, layer, examples_enumerated,\n                            batch_size=batch_size, dtype=dtype,\n                            ind_shape=ind_shape, center_only=center_only)\n'"
lucid/modelzoo/nets_factory.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains a factory function for accessing models.\n\nYou can either use the provided `get_model` function, or directly access the\n`models_map` variable containing a dictionary from a model name to its class.\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport inspect\nimport tensorflow as tf\n\nfrom lucid.modelzoo import vision_models\nfrom lucid.modelzoo import vision_base\n\n\ndef _generate_models_map():\n    base_classes = inspect.getmembers(vision_base, inspect.isclass)\n\n    list_all_models = []\n    list_all_models += inspect.getmembers(vision_models, inspect.isclass)\n\n    list_filtered = filter(lambda c: c not in base_classes, list_all_models)\n    return dict(list_filtered)\n\n\nmodels_map = _generate_models_map()\n\n\ndef get_model(name):\n    """"""Returns a model instance such as `model = vision_models.InceptionV1()`.\n    In the future may be expanded to filter by additional criteria, such as\n    architecture, dataset, and task the model was trained on.\n    Args:\n      name: The name of the model, as given by the class name in vision_models.\n    Returns:\n      An instantiated Model class with the requested model. Users still need to\n      manually `load_graphdef` on the return value, and manually import this\n      model\'s graph into their current graph.\n    Raises:\n      ValueError: If network `name` is not recognized.\n    """"""\n    if name not in models_map:\n        candidates = filter(lambda key: name in key, models_map.keys())\n        candidates_string = "", "".join(candidates)\n        raise ValueError(\n            ""No network named {}. Did you mean one of {}?"".format(\n                name, candidates_string\n            )\n        )\n\n    model_class = models_map[name]\n    model = model_class()\n    return model\n'"
lucid/modelzoo/util.py,9,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utility functions for modelzoo models.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nimport json\nfrom google.protobuf.message import DecodeError\nimport logging\nimport warnings\nfrom collections import defaultdict\nfrom itertools import chain\n\n# create logger with module name, e.g. lucid.misc.io.reading\nlog = logging.getLogger(__name__)\n\nfrom lucid.misc.io import load\nfrom lucid.misc.io.saving import ClarityJSONEncoder\n\n\ndef load_text_labels(labels_path):\n  return load(labels_path).splitlines()\n\n\ndef load_graphdef(model_url, reset_device=True):\n  """"""Load GraphDef from a binary proto file.""""""\n  graph_def = load(model_url)\n\n  if reset_device:\n    for n in graph_def.node:\n      n.device = """"\n\n  return graph_def\n\n\ndef forget_xy(t):\n  """"""Ignore sizes of dimensions (1, 2) of a 4d tensor in shape inference.\n\n  This allows using smaller input sizes, which create an invalid graph at higher\n  layers (for example because a spatial dimension becomes smaller than a conv\n  filter) when we only use early parts of it.\n  """"""\n  shape = (t.shape[0], None, None, t.shape[3])\n  return tf.placeholder_with_default(t, shape)\n\n\ndef frozen_default_graph_def(input_node_names, output_node_names):\n  """"""Return frozen and simplified graph_def of default graph.""""""\n\n  sess = tf.get_default_session()\n  if sess is None:\n    raise RuntimeError(""Default session not registered."")\n\n  input_graph_def = tf.get_default_graph().as_graph_def()\n  if len(input_graph_def.node) == 0:\n    raise RuntimeError(""Default graph is empty. Is it possible your model wasn\'t constructed or is in a different graph?"")\n\n  pruned_graph = tf.graph_util.remove_training_nodes(\n      input_graph_def, protected_nodes=(output_node_names + input_node_names)\n  )\n  pruned_graph = tf.graph_util.extract_sub_graph(pruned_graph, output_node_names)\n\n  # remove explicit device assignments\n  for node in pruned_graph.node:\n      node.device = """"\n\n  all_variable_names = [v.op.name for v in tf.global_variables()]\n  output_graph_def = tf.graph_util.convert_variables_to_constants(\n      sess=sess,\n      input_graph_def=pruned_graph,\n      output_node_names=output_node_names,\n      variable_names_whitelist=all_variable_names,\n  )\n\n  return output_graph_def\n\n\nmetadata_node_name = ""lucid_metadata_json""\n\ndef infuse_metadata(graph_def, info):\n  """"""Embed meta data as a string constant in a TF graph.\n\n  This function takes info, converts it into json, and embeds\n  it in graph_def as a constant op called `__lucid_metadata_json`.\n  """"""\n  temp_graph = tf.Graph()\n  with temp_graph.as_default():\n    tf.constant(json.dumps(info, cls=ClarityJSONEncoder), name=metadata_node_name)\n  meta_node = temp_graph.as_graph_def().node[0]\n  graph_def.node.extend([meta_node])\n\n\ndef extract_metadata(graph_def):\n  """"""Attempt to extract meta data hidden in graph_def.\n\n  Looks for a `__lucid_metadata_json` constant string op.\n  If present, extract it\'s content and convert it from json to python.\n  If not, returns None.\n  """"""\n  meta_matches = [n for n in graph_def.node if n.name==metadata_node_name]\n  if meta_matches:\n    assert len(meta_matches) == 1, ""found more than 1 lucid metadata node!""\n    meta_tensor = meta_matches[0].attr[\'value\'].tensor\n    return json.loads(meta_tensor.string_val[0])\n  else:\n    return None\n\n\n# TODO: merge with pretty_graph\'s Graph class. Until then, only use this internally\nclass GraphDefHelper(object):\n  """"""Allows constant time lookups of graphdef nodes by common properties.""""""\n\n  def __init__(self, graph_def):\n    self.graph_def = graph_def\n    self.by_op = defaultdict(list)\n    self.by_name = dict()\n    self.by_input = defaultdict(list)\n    for node in graph_def.node:\n      self.by_op[node.op].append(node)\n      assert node.name not in self.by_name  # names should be unique I guess?\n      self.by_name[node.name] = node\n      for input_name in node.input:\n        self.by_input[input_name].append(node)\n\n\n  def neighborhood(self, node, degree=4):\n    """"""Am I really handcoding graph traversal please no""""""\n    assert self.by_name[node.name] == node\n    already_visited = frontier = set([node.name])\n    for _ in range(degree):\n      neighbor_names = set()\n      for node_name in frontier:\n        outgoing = set(n.name for n in self.by_input[node_name])\n        incoming = set(self.by_name[node_name].input)\n        neighbor_names |= incoming | outgoing\n      frontier = neighbor_names - already_visited\n      already_visited |= neighbor_names\n    return [self.by_name[name] for name in already_visited]\n'"
lucid/modelzoo/vision_base.py,7,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom os import path\nimport warnings\nimport logging\nfrom itertools import chain\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom lucid.modelzoo import util as model_util\nfrom lucid.modelzoo.aligned_activations import get_aligned_activations as _get_aligned_activations\nfrom lucid.modelzoo.get_activations import get_activations as _get_activations\nfrom lucid.misc.io import load, save\nimport lucid.misc.io.showing as showing\n\n\nIMAGENET_MEAN = np.array([123.68, 116.779, 103.939])\nIMAGENET_MEAN_BGR = np.flip(IMAGENET_MEAN, 0)\n\n\nlog = logging.getLogger(__name__)\n\n\nclass Layer(object):\n  """"""Layer provides information on a model\'s layers.""""""\n\n  width = None  # reserved for future use\n  height = None  # reserved for future use\n  shape = None  # reserved for future use\n\n  def __init__(self, model_instance: \'Model\', name, depth, tags):\n    self._activations = None\n    self.model_class = model_instance.__class__\n    self.model_name = model_instance.name\n    self.name = name\n    self.depth = depth\n    self.tags = set(tags)\n\n  def __getitem__(self, name):\n    if name == \'type\':\n      warnings.warn(""Property \'type\' is deprecated on model layers. Please check if \'tags\' contains the type you are looking for in the future! We\'re simply a tag for now."", DeprecationWarning)\n      return list(self.tags)[0]\n    if name not in self.__dict__:\n      error_message = ""\'Layer\' object has no attribute \'{}\'"".format(name)\n      raise AttributeError(error_message)\n    return self.__dict__[name]\n\n  @property\n  def size(self):\n    warnings.warn(""Property \'size\' is deprecated on model layers because it may be confused with the spatial \'size\' of a layer. Please use \'depth\' in the future!"", DeprecationWarning)\n    return self.depth\n\n  @property\n  def activations(self):\n    """"""Loads sampled activations, which requires network access.""""""\n    if self._activations is None:\n      self._activations = _get_aligned_activations(self)\n    return self._activations\n\n  def __repr__(self):\n    return f""Layer (belonging to {self.model_name}) <{self.name}: {self.depth}> ([{self.tags}])""\n\n  def to_json(self):\n    return self.name  # TODO\n\n\ndef _layers_from_list_of_dicts(model_instance: \'Model\', list_of_dicts):\n  layers = []\n  for layer_info in list_of_dicts:\n    name, depth, tags = layer_info[\'name\'], layer_info[\'depth\'], layer_info[\'tags\']\n    layer = Layer(model_instance, name, depth, tags)\n    layers.append(layer)\n  return tuple(layers)\n\n\nclass Model():\n  """"""Model allows using pre-trained models.""""""\n\n  model_path = None\n  labels_path = None\n  image_value_range = (-1, 1)\n  image_shape = (None, None, 3)\n  layers = ()\n  model_name = None\n\n  _labels = None\n  _synset_ids = None\n  _synsets = None\n  _graph_def = None\n\n  # Avoid pickling the in-memory graph_def.\n  _blacklist = [\'_graph_def\']\n  def __getstate__(self):\n      return {k: v for k, v in self.__dict__.items() if k not in self._blacklist}\n\n  def __setstate__(self, state):\n      self.__dict__.update(state)\n\n  def __eq__(self, other):\n    if isinstance(other, Model):\n        return self.model_path == other.model_path\n    return False\n\n  def __hash__(self):\n    return hash(self.model_path)\n\n  @property\n  def labels(self):\n    if not hasattr(self, \'labels_path\') or self.labels_path is None:\n      raise RuntimeError(""This model does not have a labels_path specified!"")\n    if not self._labels:\n      self._labels = load(self.labels_path, split=True)\n    return self._labels\n\n  @property\n  def synset_ids(self):\n    if not hasattr(self, \'synsets_path\') or self.synsets_path is None:\n      raise RuntimeError(""This model does not have a synset_path specified!"")\n    if not self._synset_ids:\n      self._synset_ids = load(self.synsets_path, split=True)\n    return self._synset_ids\n\n  @property\n  def synsets(self):\n    from lucid.modelzoo.wordnet import synset_from_id\n\n    if not self._synsets:\n      self._synsets = [synset_from_id(s_id) for s_id in self.synset_ids]\n    return self._synsets\n\n  @property\n  def name(self):\n    if self.model_name == None:\n      return self.__class__.__name__\n    else:\n      return self.model_name\n\n  def __str__(self):\n    return self.name\n\n  def to_json(self):\n    return self.name  # TODO\n\n  @property\n  def graph_def(self):\n    if not self._graph_def:\n      self._graph_def = model_util.load_graphdef(self.model_path)\n    return self._graph_def\n\n  def load_graphdef(self):\n    warnings.warn(\n        ""Calling `load_graphdef` is no longer necessary and now a noop. Graphs are loaded lazily when a models graph_def property is accessed."",\n        DeprecationWarning,\n    )\n    pass\n\n  def post_import(self, scope):\n    pass\n\n  def create_input(self, t_input=None, forget_xy_shape=True):\n    """"""Create input tensor.""""""\n    if t_input is None:\n      t_input = tf.placeholder(tf.float32, self.image_shape)\n    t_prep_input = t_input\n    if len(t_prep_input.shape) == 3:\n      t_prep_input = tf.expand_dims(t_prep_input, 0)\n    if forget_xy_shape:\n      t_prep_input = model_util.forget_xy(t_prep_input)\n    if hasattr(self, ""is_BGR"") and self.is_BGR is True:\n      t_prep_input = tf.reverse(t_prep_input, [-1])\n    lo, hi = self.image_value_range\n    t_prep_input = lo + t_prep_input * (hi - lo)\n    return t_input, t_prep_input\n\n  def import_graph(self, t_input=None, scope=\'import\', forget_xy_shape=True, input_map=None):\n    """"""Import model GraphDef into the current graph.""""""\n    graph = tf.get_default_graph()\n    assert graph.unique_name(scope, False) == scope, (\n        \'Scope ""%s"" already exists. Provide explicit scope names when \'\n        \'importing multiple instances of the model.\') % scope\n    t_input, t_prep_input = self.create_input(t_input, forget_xy_shape)\n    final_input_map = {self.input_name: t_prep_input}\n    if input_map is not None:\n      final_input_map.update(input_map)\n    tf.import_graph_def(\n        self.graph_def, final_input_map, name=scope)\n    self.post_import(scope)\n\n    def T(layer):\n      if "":"" in layer:\n          return graph.get_tensor_by_name(""%s/%s"" % (scope,layer))\n      else:\n          return graph.get_tensor_by_name(""%s/%s:0"" % (scope,layer))\n\n    return T\n\n  def show_graph(self):\n    if self.graph_def is None:\n      raise Exception(""Model.show_graph(): Must load graph def before showing it."")\n    showing.graph(self.graph_def)\n\n  def get_layer(self, name):\n    # Search by exact match\n    for layer in self.layers:\n      if layer.name == name:\n        return layer\n    # if not found by exact match, search fuzzy and warn user:\n    for layer in self.layers:\n      if name.lower() in layer.name.lower():\n        log.warning(""Found layer by fuzzy matching, please use \'%s\' in the future!"", layer.name)\n        return layer\n    key_error_message = ""Could not find layer with name \'{}\'! Existing layer names are: {}""\n    layer_names = str([l.name for l in self.layers])\n    raise KeyError(key_error_message.format(name, layer_names))\n\n  @staticmethod\n  def suggest_save_args(graph_def=None):\n    if graph_def is None:\n      graph_def = tf.get_default_graph().as_graph_def()\n    gdhelper = model_util.GraphDefHelper(graph_def)\n    inferred_info = dict.fromkeys((""input_name"", ""image_shape"", ""output_names"", ""image_value_range""))\n    node_shape = lambda n: [dim.size for dim in n.attr[\'shape\'].shape.dim]\n    potential_input_nodes = gdhelper.by_op[""Placeholder""]\n    output_nodes = [node.name for node in gdhelper.by_op[""Softmax""]]\n\n    if len(potential_input_nodes) == 1:\n      input_node = potential_input_nodes[0]\n      input_dtype = tf.dtypes.as_dtype(input_node.attr[\'dtype\'].type)\n      if input_dtype.is_floating:\n        input_name = input_node.name\n        print(""Inferred: input_name = {} (because it was the only Placeholder in the graph_def)"".format(input_name))\n        inferred_info[""input_name""] = input_name\n      else:\n        print(""Warning: found a single Placeholder, but its dtype is {}. Lucid\'s parameterizations can only replace float dtypes. We\'re now scanning to see if you maybe divide this placeholder by 255 to get a float later in the graph..."".format(str(input_node.attr[\'dtype\']).strip()))\n        neighborhood = gdhelper.neighborhood(input_node, degree=5)\n        divs = [n for n in neighborhood if n.op == ""RealDiv""]\n        consts = [n for n in neighborhood if n.op == ""Const""]\n        magic_number_present = any(255 in c.attr[\'value\'].tensor.int_val for c in consts)\n        if divs and magic_number_present:\n          if len(divs) == 1:\n            input_name = divs[0].name\n            print(""Guessed: input_name = {} (because it\'s the only division by 255 near the only placeholder)"".format(input_name))\n            inferred_info[""input_name""] = input_name\n            image_value_range = (0,1)\n            print(""Guessed: image_value_range = {} (because you\'re dividing by 255 near the only placeholder)"".format(image_value_range))\n            inferred_info[""image_value_range""] = (0,1)\n          else:\n            warnings.warn(""Could not infer input_name because there were multiple division ops near your the only placeholder. Candidates include: {}"".format([n.name for n in divs]))\n    else:\n      warnings.warn(""Could not infer input_name because there were multiple or no Placeholders."")\n\n    if inferred_info[""input_name""] is not None:\n      input_node = gdhelper.by_name[inferred_info[""input_name""]]\n      shape = node_shape(input_node)\n      if len(shape) in (3,4):\n        if len(shape) == 4:\n          shape = shape[1:]\n        if -1 not in shape:\n          print(""Inferred: image_shape = {}"".format(shape))\n          inferred_info[""image_shape""] = shape\n      if inferred_info[""image_shape""] is None:\n        warnings.warn(""Could not infer image_shape."")\n\n    if output_nodes:\n      print(""Inferred: output_names = {}  (because those are all the Softmax ops)"".format(output_nodes))\n      inferred_info[""output_names""] = output_nodes\n    else:\n      warnings.warn(""Could not infer output_names."")\n\n    report = []\n    report.append(""# Please sanity check all inferred values before using this code."")\n    report.append(""# Incorrect `image_value_range` is the most common cause of feature visualization bugs! Most methods will fail silently with incorrect visualizations!"")\n    report.append(""Model.save("")\n\n    suggestions = {\n        ""input_name"" : \'input\',\n        ""image_shape"" : [224, 224, 3],\n        ""output_names"": [\'logits\'],\n        ""image_value_range"": ""[-1, 1], [0, 1], [0, 255], or [-117, 138]""\n    }\n    for key, value in inferred_info.items():\n      if value is not None:\n        report.append(""    {}={!r},"".format(key, value))\n      else:\n        report.append(""    {}=_,                   # TODO (eg. {!r})"".format(key, suggestions[key]))\n    report.append(""  )"")\n\n    print(""\\n"".join(report))\n    return inferred_info\n\n\n  @staticmethod\n  def save(save_url, input_name, output_names, image_shape, image_value_range):\n    if "":"" in input_name:\n      raise ValueError(""input_name appears to be a tensor (name contains \':\') but must be an op."")\n    if any(["":"" in name for name in output_names]):\n      raise ValueError(""output_namess appears to be contain tensor (name contains \':\') but must be ops."")\n\n    metadata = {\n      ""input_name"" : input_name,\n      ""image_shape"" : image_shape,\n      ""image_value_range"": image_value_range,\n    }\n\n    graph_def = model_util.frozen_default_graph_def([input_name], output_names)\n    model_util.infuse_metadata(graph_def, metadata)\n    save(graph_def, save_url)\n\n  @staticmethod\n  def load(url):\n    if url.endswith("".pb""):\n      return Model.load_from_graphdef(url)\n    elif url.endswith("".json""):\n      return Model.load_from_manifest(url)\n\n  @staticmethod\n  def load_from_metadata(model_url, metadata):\n    class DynamicModel(Model):\n      model_path = model_url\n      input_name = metadata[""input_name""]\n      image_shape = metadata[""image_shape""]\n      image_value_range = metadata[""image_value_range""]\n    return DynamicModel()\n\n  @staticmethod\n  def load_from_graphdef(graphdef_url):\n    graph_def = load(graphdef_url)\n    metadata = model_util.extract_metadata(graph_def)\n    if metadata:\n      return Model.load_from_metadata(graphdef_url, metadata)\n    else:\n      raise ValueError(""Model.load_from_graphdef was called on a GraphDef ({}) that does not contain Lucid\'s metadata node. Model.load only works for models saved via Model.save. For the graphdef you\'re trying to load, you will need to provide custom metadata; see Model.load_from_metadata()"".format(graphdef_url))\n\n  @staticmethod\n  def load_from_manifest(manifest_url):\n    try:\n      manifest = load(manifest_url)\n    except Exception as e:\n      raise ValueError(""Could not find manifest.json file in dir {}. Error: {}"".format(manifest_url, e))\n\n    if manifest.get(\'type\', \'frozen\') == \'frozen\':\n      manifest_folder = path.dirname(manifest_url)\n      return FrozenGraphModel(manifest_folder, manifest)\n    else:\n      raise NotImplementedError(""SerializedModel Manifest type \'{}\' has not been implemented!"".format(manifest.get(\'type\')))\n\n  def get_activations(self, layer, examples, batch_size=64,\n                         dtype=None, ind_shape=None, center_only=False):\n    """"""Collect center activtions of a layer over an n-dimensional array of images.\n\n    Note: this is mostly intended for large synthetic families of images, where\n      you can cheaply generate them in Python. For collecting activations over,\n      say, ImageNet, there will be better workflows based on various dataset APIs\n      in TensorFlow.\n\n    Args:\n      layer: layer (in model) for activtions to be collected from.\n      examples: A (potentially n-dimensional) array of images. Can be any nested\n        iterable object, including a generator, as long as the inner most objects\n        are a numpy array with at least 3 dimensions (image X, Y, channels=3).\n      batch_size: How many images should be processed at once?\n      dtype: determines dtype of returned data (defaults to model activation\n        dtype). Can be used to make function memory efficient.\n      ind_shape: Shape that the index (non-image) dimensions of examples. Makes\n        code much more memory efficient if examples is not a numpy array.\n\n    Memory efficeincy:\n      Have examples be a generator rather than an array of images; this allows\n      them to be lazily generated and not all stored in memory at once. Also\n      use ind_shape so that activations can be stored in an efficient data\n      structure. If you still have memory problems, dtype=""float16"" can probably\n      get you another 2x.\n\n    Returns:\n      A numpy array of shape [ind1, ind2, ..., layer_channels]\n    """"""\n\n    return _get_activations(self, layer, examples, batch_size=batch_size,\n                            dtype=dtype, ind_shape=ind_shape,\n                            center_only=center_only)\n\n\nclass SerializedModel(Model):\n\n  @classmethod\n  def from_directory(cls, model_path, manifest_path=None):\n    warnings.warn(""SerializedModel is deprecated. Please use Model.load_from_manifest instead."", DeprecationWarning)\n    if manifest_path is None:\n      manifest_path = path.join(model_path, \'manifest.json\')\n    return Model.load_from_manifest(manifest_path)\n\n\nclass FrozenGraphModel(SerializedModel):\n\n  _mandatory_properties = [\'model_path\', \'image_value_range\', \'input_name\', \'image_shape\']\n\n  def __init__(self, model_directory, manifest):\n    self.manifest = manifest\n\n    for mandatory_key in self._mandatory_properties:\n      # TODO: consider if we can tell you the path of the faulty manifest here\n      assert mandatory_key in manifest.keys(), ""Mandatory property \'{}\' was not defined in json manifest."".format(mandatory_key)\n    for key, value in manifest.items():\n      setattr(self, key, value)\n\n    model_path = manifest.get(\'model_path\', \'graph.pb\')\n    if model_path.startswith(""./""): # TODO: can we be less specific here?\n      self.model_path = path.join(model_directory, model_path[2:])\n    else:\n      self.model_path = model_path\n\n    layers_or_layer_names = manifest.get(\'layers\')\n    if len(layers_or_layer_names) > 0:\n      if isinstance(layers_or_layer_names[0], str):\n        self.layer_names = layers_or_layer_names\n      elif isinstance(layers_or_layer_names[0], dict):\n        self.layers = _layers_from_list_of_dicts(self, layers_or_layer_names)\n        self.layer_names = [layer.name for layer in self.layers]\n\n    super().__init__()\n'"
lucid/modelzoo/vision_models.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Clean export of vision_models.\n\nWe manually remove the following symbols from this module to keep tab\ncompletion as clean as possible--even when it doesn\'t respect `__all__`.\nClean namespaces for those lucid.modelzoo modules that contain models are\nenforced by tests in test/modelzoo/test_vision_models.\n""""""\n\nfrom lucid.modelzoo.vision_base import Model, Layer\n\nfrom lucid.modelzoo.caffe_models import *\nfrom lucid.modelzoo.slim_models import *\nfrom lucid.modelzoo.other_models import *\n\n\n__all__ = [_name for _name, _obj in list(globals().items())\n           if isinstance(_obj, type) and issubclass(_obj, Model)]\n\n# in Python 2 only, list comprehensions leak bound vars to a broader scope\ntry:\n  del _obj\n  del _name\nexcept:\n  pass\n'"
lucid/modelzoo/wordnet.py,0,"b'# Copyright 2019 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helpers for using WordNet Synsets.\n\nWhen comparing different models, be aware that they may encode their predictions in\ndifferent orders. Do not compare outputs of models without ensuring their outputs are\nin the same order! We recommend relying on WordNet\'s synsets to uniquely identify a\nlabel. Let\'s clarify these terms:\n\n## Labels (""Labrador Retriever"")\n\nLabel are totally informal and vary between implementations. We aim to provide a list of\nmodel labels in the `.labels` property. These may include differen labels in different\norders for each model.\n\nFor translating between textual labels and synsets, plase use the labels and synsets\ncollections on models. There\'s no other foolproof way of goinfg from a descriptive text\nlabel to a precise synset definition.\n\n\n## Synset IDs (""n02099712"")\n\nSynset IDs are identifiers used by the ILSVRC2012 ImageNet classification contest.\nWe provide `id_from_synset()` to format them correctly.\n\n\n## Synsets Names (\'labrador_retriever.n.01\')\n\nSynset names are a wordnet internal concept. When youw ant to create a synset but don\'t\nknow its precise name, we offer `imagenet_synset_from_description()` to search for a\nsynset containing the description in its name that is also one of the synsets used for\nthe ILSVRC2012.\n\n\n## Label indexes (logits[i])\n\nWhen obtaining predictions from a model, they will often be provided in the form of a\nBATCH by NUM_CLASSES multidimensional array. In order to map those to human readable\nstrings, please use a model\'s `.labels` or `.synsets` or `.synset_ids` property. We aim\nto provide these in the same ordering as the model was trained on. Unfortunately these\nmay be subtly different between models.\n\n""""""\n\nfrom cachetools.func import lru_cache\n\nimport nltk\nnltk.download(\'wordnet\', quiet=True)\nfrom nltk.corpus import wordnet as wn\n\nfrom lucid.misc.io import load\n\n\nIMAGENET_SYNSETS_PATH = ""gs://modelzoo/labels/ImageNet_standard_synsets.txt""\n\n\ndef id_from_synset(synset):\n    return ""{}{:08}"".format(synset.pos(), synset.offset())\n\n\ndef synset_from_id(id_str):\n    assert len(id_str) == 1 + 8\n    pos, offset = id_str[0], int(id_str[1:])\n    return wn.synset_from_pos_and_offset(pos, offset)\n\n\n@lru_cache(maxsize=1)\ndef imagenet_synset_ids():\n    return load(IMAGENET_SYNSETS_PATH, split=True)\n\n\n@lru_cache(maxsize=1)\ndef imagenet_synsets():\n    return [synset_from_id(id) for id in imagenet_synset_ids()]\n\n\n@lru_cache()\ndef imagenet_synset_from_description(search_term):\n    names_and_synsets = [(synset.name(), synset) for synset in imagenet_synsets()]\n    candidates = [\n        synset for (name, synset) in names_and_synsets if search_term.lower().replace(\' \', \'_\') in name\n    ]\n    hits = len(candidates)\n    if hits == 1:\n        return candidates[0]\n    if hits == 0:\n        message = ""Could not find any imagenet synset with search term {}.""\n        raise ValueError(message.format(search_term))\n    else:\n      message = ""Found {} imagenet synsets with search term {}: {}.""\n      names = [synset.name() for synset in candidates]\n      raise ValueError(message.format(hits, search_term, "", "".join(names)))\n'"
lucid/optvis/__init__.py,0,b''
lucid/optvis/objectives.py,23,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Objective functions for visualizing neural networks.\n\nWe represent objectives with a class `Objective` enclosing functions of the\nform:\n\n  (T) => TensorFlow Scalar\n\nWhere `T` is a function that allows one to access the activations of different\nlayers of the network. For example `T(""mixed4a"")` gives the activations for\nthe layer mixed4a.\n\nThis allows objectives to be declared outside the rendering function, but then\nactually constructed within its graph/session.\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom decorator import decorator\nimport numpy as np\nimport tensorflow as tf\n\n\nfrom lucid.optvis.objectives_util import _dot, _dot_cossim, _extract_act_pos, _make_arg_str, _T_force_NHWC, _T_handle_batch\n\n# We use T as a variable name to access all kinds of tensors\n# pylint: disable=invalid-name\n\n\nclass Objective(object):\n  """"""""A wrapper to make objective functions easy to combine.\n\n  For example, suppose you want to optimize 20% for mixed4a:20 and 80% for\n  mixed4a:21. Then you could use:\n\n    objective = 0.2 * channel(""mixed4a"", 20) + 0.8 * channel(""mixed4a"", 21)\n\n  Under the hood, we think of objectives as functions of the form:\n\n    T => tensorflow scalar for loss\n\n  where T is a function allowing you to index layers in the network -- that is,\n  if there\'s a layer ""mixed4a"" then T(""mixed4a"") would give you its\n  activations).\n\n  This allows objectives to be declared outside the rendering function, but then\n  actually constructed within its graph/session.\n  """"""\n\n  def __init__(self, objective_func, name="""", description=""""):\n    self.objective_func = objective_func\n    self.name = name\n    self.description = description\n\n  def __add__(self, other):\n    if isinstance(other, (int, float)):\n      objective_func = lambda T: other + self(T)\n      name = self.name\n      description = self.description\n    else:\n      objective_func = lambda T: self(T) + other(T)\n      name = "", "".join([self.name, other.name])\n      description = ""Sum("" + "" +\\n"".join([self.description, other.description]) + "")""\n    return Objective(objective_func, name=name, description=description)\n\n  def __neg__(self):\n    return -1 * self\n\n  def __sub__(self, other):\n    return self + (-1 * other)\n\n  @staticmethod\n  def sum(objs):\n    objective_func = lambda T: sum([obj(T) for obj in objs])\n    descriptions = [obj.description for obj in objs]\n    description = ""Sum("" + "" +\\n"".join(descriptions) + "")""\n    names = [obj.name for obj in objs]\n    name = "", "".join(names)\n    return Objective(objective_func, name=name, description=description)\n\n  def __mul__(self, other):\n    if isinstance(other, (int, float)):\n      objective_func = lambda T: other * self(T)\n    else:\n      objective_func = lambda T: self(T) * other(T)\n    return Objective(objective_func, name=self.name, description=self.description)\n\n  def __rmul__(self, other):\n    return self.__mul__(other)\n\n  def __radd__(self, other):\n    return self.__add__(other)\n\n  def __call__(self, T):\n    return self.objective_func(T)\n\n\n\n\ndef wrap_objective(require_format=None, handle_batch=False):\n  """"""Decorator for creating Objective factories.\n\n  Changes f from the closure: (args) => () => TF Tensor\n  into an Objective factory: (args) => Objective\n\n  while preserving function name, arg info, docs... for interactive python.\n  """"""\n\n  @decorator\n  def inner(f, *args, **kwds):\n    objective_func = f(*args, **kwds)\n    objective_name = f.__name__\n    args_str = "" ["" + "", "".join([_make_arg_str(arg) for arg in args]) + ""]""\n    description = objective_name.title() + args_str\n\n    def process_T(T):\n      if require_format == ""NHWC"":\n        T = _T_force_NHWC(T)\n      return T\n\n    return Objective(lambda T: objective_func(process_T(T)),\n                     objective_name, description)\n  return inner\n\n\ndef handle_batch(batch=None):\n  return lambda f: lambda T: f(_T_handle_batch(T, batch=batch))\n\n\n@wrap_objective(require_format=\'NHWC\')\ndef neuron(layer_name, channel_n, x=None, y=None, batch=None):\n  """"""Visualize a single neuron of a single channel.\n\n  Defaults to the center neuron. When width and height are even numbers, we\n  choose the neuron in the bottom right of the center 2x2 neurons.\n\n  Odd width & height:               Even width & height:\n\n  +---+---+---+                     +---+---+---+---+\n  |   |   |   |                     |   |   |   |   |\n  +---+---+---+                     +---+---+---+---+\n  |   | X |   |                     |   |   |   |   |\n  +---+---+---+                     +---+---+---+---+\n  |   |   |   |                     |   |   | X |   |\n  +---+---+---+                     +---+---+---+---+\n                                    |   |   |   |   |\n                                    +---+---+---+---+\n  """"""\n\n  @handle_batch(batch)\n  def inner(T):\n    layer = T(layer_name)\n    layer = _extract_act_pos(layer, x, y)\n    return tf.reduce_mean(layer[..., channel_n])\n  return inner\n\n\n@wrap_objective(require_format=\'NHWC\')\ndef channel(layer, n_channel, batch=None):\n  """"""Visualize a single channel""""""\n\n  @handle_batch(batch)\n  def inner(T):\n    return tf.reduce_mean(T(layer)[..., n_channel])\n  return inner\n\n\n@wrap_objective(require_format=\'NHWC\')\ndef direction(layer, vec, cossim_pow=0, batch=None):\n  """"""Visualize a direction""""""\n  vec = vec[None, None, None]\n  vec = vec.astype(""float32"")\n\n  @handle_batch(batch)\n  def inner(T):\n    return _dot_cossim(T(layer), vec, cossim_pow=cossim_pow)\n  return inner\n\ndirection_cossim = direction\n\n@wrap_objective(require_format=\'NHWC\')\ndef direction_neuron(layer_name, vec, x=None, y=None, cossim_pow=0, batch=None):\n  """"""Visualize a single (x, y) position along the given direction""""""\n  vec = vec.astype(""float32"")\n  @handle_batch(batch)\n  def inner(T):\n    layer = T(layer_name)\n    layer = _extract_act_pos(layer, x, y)\n    return _dot_cossim(layer, vec[None, None, None], cossim_pow=cossim_pow)\n  return inner\n\n\n@wrap_objective(require_format=\'NHWC\')\ndef tensor_direction(layer, vec, cossim_pow=0, batch=None):\n  """"""Visualize a tensor.""""""\n  assert len(vec.shape) in [3,4]\n  vec = vec.astype(""float32"")\n  if len(vec.shape) == 3:\n    vec = vec[None]\n  @handle_batch(batch)\n  def inner(T):\n    t_acts = T(layer)\n    t_shp = tf.shape(t_acts)\n    v_shp = vec.shape\n    M1 = (t_shp[1] - v_shp[1]) // 2\n    M2 = (t_shp[2] - v_shp[2]) // 2\n    t_acts_ = t_acts[:,\n                     M1 : M1+v_shp[1],\n                     M2 : M2+v_shp[2],\n                     :]\n    return _dot_cossim(t_acts_, vec, cossim_pow=cossim_pow)\n  return inner\n\n\n@wrap_objective(handle_batch=True)\ndef deepdream(layer):\n  """"""Maximize \'interestingness\' at some layer.\n\n  See Mordvintsev et al., 2015.\n  """"""\n  return lambda T: tf.reduce_mean(T(layer)**2)\n\n\n@wrap_objective(handle_batch=True)\ndef total_variation(layer=""input""):\n  """"""Total variation of image (or activations at some layer).\n\n  This operation is most often used as a penalty to reduce noise.\n  See Simonyan, et al., 2014.\n  """"""\n  return lambda T: tf.image.total_variation(T(layer))\n\n\n@wrap_objective(handle_batch=True)\ndef L1(layer=""input"", constant=0):\n  """"""L1 norm of layer. Generally used as penalty.""""""\n  return lambda T: tf.reduce_sum(tf.abs(T(layer) - constant))\n\n\n@wrap_objective(handle_batch=True)\ndef L2(layer=""input"", constant=0, epsilon=1e-6):\n  """"""L2 norm of layer. Generally used as penalty.""""""\n  return lambda T: tf.sqrt(epsilon + tf.reduce_sum((T(layer) - constant) ** 2))\n\n\ndef _tf_blur(x, w=3):\n  depth = x.shape[-1]\n  k = np.zeros([w, w, depth, depth])\n  for ch in range(depth):\n    k_ch = k[:, :, ch, ch]\n    k_ch[ :,    :  ] = 0.5\n    k_ch[1:-1, 1:-1] = 1.0\n\n  conv_k = lambda t: tf.nn.conv2d(t, k, [1, 1, 1, 1], ""SAME"")\n  return conv_k(x) / conv_k(tf.ones_like(x))\n\n\n@wrap_objective()\ndef blur_input_each_step():\n  """"""Minimizing this objective is equivelant to blurring input each step.\n\n  Optimizing (-k)*blur_input_each_step() is equivelant to:\n\n    input <- (1-k)*input + k*blur(input)\n\n  An operation that was used in early feature visualization work.\n  See Nguyen, et al., 2015.\n  """"""\n  def inner(T):\n    t_input = T(""input"")\n    t_input_blurred = tf.stop_gradient(_tf_blur(t_input))\n    return 0.5*tf.reduce_sum((t_input - t_input_blurred)**2)\n  return inner\n\n@wrap_objective()\ndef blur_alpha_each_step():\n  def inner(T):\n    t_input = T(""input"")[..., 3:4]\n    t_input_blurred = tf.stop_gradient(_tf_blur(t_input))\n    return 0.5*tf.reduce_sum((t_input - t_input_blurred)**2)\n  return inner\n\n\n@wrap_objective()\ndef channel_interpolate(layer1, n_channel1, layer2, n_channel2):\n  """"""Interpolate between layer1, n_channel1 and layer2, n_channel2.\n\n  Optimize for a convex combination of layer1, n_channel1 and\n  layer2, n_channel2, transitioning across the batch.\n\n  Args:\n    layer1: layer to optimize 100% at batch=0.\n    n_channel1: neuron index to optimize 100% at batch=0.\n    layer2: layer to optimize 100% at batch=N.\n    n_channel2: neuron index to optimize 100% at batch=N.\n\n  Returns:\n    Objective\n  """"""\n  def inner(T):\n    batch_n = T(layer1).get_shape().as_list()[0]\n    arr1 = T(layer1)[..., n_channel1]\n    arr2 = T(layer2)[..., n_channel2]\n    weights = (np.arange(batch_n)/float(batch_n-1))\n    S = 0\n    for n in range(batch_n):\n      S += (1-weights[n]) * tf.reduce_mean(arr1[n])\n      S += weights[n] * tf.reduce_mean(arr2[n])\n    return S\n  return inner\n\n\n@wrap_objective()\ndef penalize_boundary_complexity(shp, w=20, mask=None, C=0.5):\n  """"""Encourage the boundaries of an image to have less variation and of color C.\n\n  Args:\n    shp: shape of T(""input"") because this may not be known.\n    w: width of boundary to penalize. Ignored if mask is set.\n    mask: mask describing what area should be penalized.\n\n  Returns:\n    Objective.\n  """"""\n  def inner(T):\n    arr = T(""input"")\n\n    # print shp\n    if mask is None:\n      mask_ = np.ones(shp)\n      mask_[:, w:-w, w:-w] = 0\n    else:\n      mask_ = mask\n\n    blur = _tf_blur(arr, w=5)\n    diffs = (blur-arr)**2\n    diffs += 0.8*(arr-C)**2\n\n    return -tf.reduce_sum(diffs*mask_)\n  return inner\n\n\n@wrap_objective()\ndef alignment(layer, decay_ratio=2):\n  """"""Encourage neighboring images to be similar.\n\n  When visualizing the interpolation between two objectives, it\'s often\n  desirable to encourage analogous objects to be drawn in the same position,\n  to make them more comparable.\n\n  This term penalizes L2 distance between neighboring images, as evaluated at\n  layer.\n\n  In general, we find this most effective if used with a parameterization that\n  shares across the batch. (In fact, that works quite well by itself, so this\n  function may just be obsolete.)\n\n  Args:\n    layer: layer to penalize at.\n    decay_ratio: how much to decay penalty as images move apart in batch.\n\n  Returns:\n    Objective.\n  """"""\n  def inner(T):\n    batch_n = T(layer).get_shape().as_list()[0]\n    arr = T(layer)\n    accum = 0\n    for d in [1, 2, 3, 4]:\n      for i in range(batch_n - d):\n        a, b = i, i+d\n        arr1, arr2 = arr[a], arr[b]\n        accum += tf.reduce_mean((arr1-arr2)**2) / decay_ratio**float(d)\n    return -accum\n  return inner\n\n@wrap_objective()\ndef diversity(layer):\n  """"""Encourage diversity between each batch element.\n\n  A neural net feature often responds to multiple things, but naive feature\n  visualization often only shows us one. If you optimize a batch of images,\n  this objective will encourage them all to be different.\n\n  In particular, it calculates the correlation matrix of activations at layer\n  for each image, and then penalizes cossine similarity between them. This is\n  very similar to ideas in style transfer, except we\'re *penalizing* style\n  similarity instead of encouraging it.\n\n  Args:\n    layer: layer to evaluate activation correlations on.\n\n  Returns:\n    Objective.\n  """"""\n  def inner(T):\n    layer_t = T(layer)\n    batch_n, _, _, channels = layer_t.get_shape().as_list()\n\n    flattened = tf.reshape(layer_t, [batch_n, -1, channels])\n    grams = tf.matmul(flattened, flattened, transpose_a=True)\n    grams = tf.nn.l2_normalize(grams, axis=[1,2], epsilon=1e-10)\n\n    return sum([ sum([ tf.reduce_sum(grams[i]*grams[j])\n                      for j in range(batch_n) if j != i])\n                for i in range(batch_n)]) / batch_n\n  return inner\n\n\n@wrap_objective()\ndef input_diff(orig_img):\n  """"""Average L2 difference between optimized image and orig_img.\n\n  This objective is usually mutliplied by a negative number and used as a\n  penalty in making advarsarial counterexamples.\n  """"""\n  def inner(T):\n    diff = T(""input"") - orig_img\n    return tf.sqrt(tf.reduce_mean(diff**2))\n  return inner\n\n\n@wrap_objective()\ndef class_logit(layer, label, batch=None):\n  """"""Like channel, but for softmax layers.\n\n  Args:\n    layer: A layer name string.\n    label: Either a string (refering to a label in model.labels) or an int\n      label position.\n\n  Returns:\n    Objective maximizing a logit.\n  """"""\n  @handle_batch(batch)\n  def inner(T):\n    if isinstance(label, int):\n      class_n = label\n    else:\n      class_n = T(""labels"").index(label)\n    logits = T(layer)\n    logit = tf.reduce_sum(logits[:, class_n])\n    return logit\n  return inner\n\n\ndef as_objective(obj):\n  """"""Convert obj into Objective class.\n\n  Strings of the form ""layer:n"" become the Objective channel(layer, n).\n  Objectives are returned unchanged.\n\n  Args:\n    obj: string or Objective.\n\n  Returns:\n    Objective\n  """"""\n  if isinstance(obj, Objective):\n    return obj\n  elif callable(obj):\n    return obj\n  elif isinstance(obj, str):\n    layer, n = obj.split("":"")\n    layer, n = layer.strip(), int(n)\n    return channel(layer, n)\n'"
lucid/optvis/objectives_util.py,8,"b'\nfrom __future__ import absolute_import, division, print_function\n\nfrom decorator import decorator\nimport numpy as np\nimport tensorflow as tf\n\nimport lucid.misc.graph_analysis.property_inference as property_inference\n\n\ndef _dot(x, y):\n  return tf.reduce_sum(x * y, -1)\n\n\ndef _dot_cossim(x, y, cossim_pow=0):\n  eps = 1e-4\n  xy_dot = _dot(x, y)\n  if cossim_pow == 0: return tf.reduce_mean(xy_dot)\n  x_mags = tf.sqrt(_dot(x,x))\n  y_mags = tf.sqrt(_dot(y,y))\n  cossims = xy_dot / (eps + x_mags ) / (eps + y_mags)\n  floored_cossims = tf.maximum(0.1, cossims)\n  return tf.reduce_mean(xy_dot * floored_cossims**cossim_pow)\n\n\ndef _extract_act_pos(acts, x=None, y=None):\n  shape = tf.shape(acts)\n  x_ = shape[1] // 2 if x is None else x\n  y_ = shape[2] // 2 if y is None else y\n  return acts[:, x_:x_+1, y_:y_+1]\n\n\ndef _make_arg_str(arg):\n  arg = str(arg)\n  too_big = len(arg) > 15 or ""\\n"" in arg\n  return ""..."" if too_big else arg\n\n\ndef _T_force_NHWC(T):\n  """"""Modify T to accomdate different data types\n\n    [N, C, H, W]  ->  [N, H, W, C]\n    [N, C]        ->  [N, 1, 1, C]\n\n  """"""\n  def T2(name):\n    t = T(name)\n    shape = t.shape\n    if str(shape) == ""<unknown>"":\n      return t\n    if len(shape) == 2:\n      return t[:, None, None, :]\n    elif len(shape) == 4:\n      fmt = property_inference.infer_data_format(t)\n      if fmt == ""NCHW"":\n        return tf.transpose(t, [0, 2, 3, 1])\n    return t\n  return T2\n\n\ndef _T_handle_batch(T, batch=None):\n  def T2(name):\n    t = T(name)\n    if isinstance(batch, int):\n      return t[batch:batch+1]\n    else:\n      return t\n  return T2\n'"
lucid/optvis/render.py,15,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Provides render_vis() for actually rendering visualizations.\n\nThis module primarily provides render_vis() for rendering visualizations.\nIt also provides some utilities in case you need to create your own rendering\nfunction.\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom future.standard_library import install_aliases\ninstall_aliases()\nfrom builtins import range\n\nimport numpy as np\nimport tensorflow as tf\nimport logging\n\nfrom lucid.optvis import objectives, param, transform\nfrom lucid.misc.io import show\nfrom lucid.misc.redirected_relu_grad import redirected_relu_grad, redirected_relu6_grad\nfrom lucid.misc.gradient_override import gradient_override_map\n\n# pylint: disable=invalid-name\n\n\n# create logger with module name, e.g. lucid.misc.io.reading\nlog = logging.getLogger(__name__)\n\n\ndef render_vis(model, objective_f, param_f=None, optimizer=None,\n               transforms=None, thresholds=(512,), print_objectives=None,\n               verbose=True, relu_gradient_override=True, use_fixed_seed=False):\n  """"""Flexible optimization-based feature vis.\n\n  There\'s a lot of ways one might wish to customize optimization-based\n  feature visualization. It\'s hard to create an abstraction that stands up\n  to all the things one might wish to try.\n\n  This function probably can\'t do *everything* you want, but it\'s much more\n  flexible than a naive attempt. The basic abstraction is to split the problem\n  into several parts. Consider the arguments:\n\n  Args:\n    model: The model to be visualized, from Alex\' modelzoo.\n    objective_f: The objective our visualization maximizes.\n      See the objectives module for more details.\n    param_f: Paramaterization of the image we\'re optimizing.\n      See the paramaterization module for more details.\n      Defaults to a naively paramaterized [1, 128, 128, 3] image.\n    optimizer: Optimizer to optimize with. Either tf.train.Optimizer instance,\n      or a function from (graph, sess) to such an instance.\n      Defaults to Adam with lr .05.\n    transforms: A list of stochastic transformations that get composed,\n      which our visualization should robustly activate the network against.\n      See the transform module for more details.\n      Defaults to [transform.jitter(8)].\n    thresholds: A list of numbers of optimization steps, at which we should\n      save (and display if verbose=True) the visualization.\n    print_objectives: A list of objectives separate from those being optimized,\n      whose values get logged during the optimization.\n    verbose: Should we display the visualization when we hit a threshold?\n      This should only be used in IPython.\n    relu_gradient_override: Whether to use the gradient override scheme\n      described in lucid/misc/redirected_relu_grad.py. On by default!\n    use_fixed_seed: Seed the RNG with a fixed value so results are reproducible.\n      Off by default. As of tf 1.8 this does not work as intended, see:\n      https://github.com/tensorflow/tensorflow/issues/9171\n\n  Returns:\n    2D array of optimization results containing of evaluations of supplied\n    param_f snapshotted at specified thresholds. Usually that will mean one or\n    multiple channel visualizations stacked on top of each other.\n  """"""\n\n  with tf.Graph().as_default() as graph, tf.Session() as sess:\n\n    if use_fixed_seed:  # does not mean results are reproducible, see Args doc\n      tf.set_random_seed(0)\n\n    T = make_vis_T(model, objective_f, param_f, optimizer, transforms,\n                   relu_gradient_override)\n    print_objective_func = make_print_objective_func(print_objectives, T)\n    loss, vis_op, t_image = T(""loss""), T(""vis_op""), T(""input"")\n    tf.global_variables_initializer().run()\n\n    images = []\n    try:\n      for i in range(max(thresholds)+1):\n        loss_, _ = sess.run([loss, vis_op])\n        if i in thresholds:\n          vis = t_image.eval()\n          images.append(vis)\n          if verbose:\n            print(i, loss_)\n            print_objective_func(sess)\n            show(np.hstack(vis))\n    except KeyboardInterrupt:\n      log.warning(""Interrupted optimization at step {:d}."".format(i+1))\n      vis = t_image.eval()\n      show(np.hstack(vis))\n\n    return images\n\n\ndef make_vis_T(model, objective_f, param_f=None, optimizer=None,\n               transforms=None, relu_gradient_override=False):\n  """"""Even more flexible optimization-base feature vis.\n\n  This function is the inner core of render_vis(), and can be used\n  when render_vis() isn\'t flexible enough. Unfortunately, it\'s a bit more\n  tedious to use:\n\n  >  with tf.Graph().as_default() as graph, tf.Session() as sess:\n  >\n  >    T = make_vis_T(model, ""mixed4a_pre_relu:0"")\n  >    tf.initialize_all_variables().run()\n  >\n  >    for i in range(10):\n  >      T(""vis_op"").run()\n  >      showarray(T(""input"").eval()[0])\n\n  This approach allows more control over how the visualizaiton is displayed\n  as it renders. It also allows a lot more flexibility in constructing\n  objectives / params because the session is already in scope.\n\n\n  Args:\n    model: The model to be visualized, from Alex\' modelzoo.\n    objective_f: The objective our visualization maximizes.\n      See the objectives module for more details.\n    param_f: Paramaterization of the image we\'re optimizing.\n      See the paramaterization module for more details.\n      Defaults to a naively paramaterized [1, 128, 128, 3] image.\n    optimizer: Optimizer to optimize with. Either tf.train.Optimizer instance,\n      or a function from (graph, sess) to such an instance.\n      Defaults to Adam with lr .05.\n    transforms: A list of stochastic transformations that get composed,\n      which our visualization should robustly activate the network against.\n      See the transform module for more details.\n      Defaults to [transform.jitter(8)].\n\n  Returns:\n    A function T, which allows access to:\n      * T(""vis_op"") -- the operation for to optimize the visualization\n      * T(""input"") -- the visualization itself\n      * T(""loss"") -- the loss for the visualization\n      * T(layer) -- any layer inside the network\n  """"""\n\n  # pylint: disable=unused-variable\n  t_image = make_t_image(param_f)\n  objective_f = objectives.as_objective(objective_f)\n  transform_f = make_transform_f(transforms)\n  optimizer = make_optimizer(optimizer, [])\n\n  global_step = tf.train.get_or_create_global_step()\n  init_global_step = tf.variables_initializer([global_step])\n  init_global_step.run()\n\n  if relu_gradient_override:\n    with gradient_override_map({\'Relu\': redirected_relu_grad,\n                                \'Relu6\': redirected_relu6_grad}):\n      T = import_model(model, transform_f(t_image), t_image)\n  else:\n    T = import_model(model, transform_f(t_image), t_image)\n  loss = objective_f(T)\n\n\n  vis_op = optimizer.minimize(-loss, global_step=global_step)\n\n  local_vars = locals()\n  # pylint: enable=unused-variable\n\n  def T2(name):\n    if name in local_vars:\n      return local_vars[name]\n    else: return T(name)\n\n  return T2\n\n\ndef make_print_objective_func(print_objectives, T):\n  print_objectives = print_objectives or []\n  po_descriptions = [obj.description for obj in print_objectives]\n  pos = [obj(T) for obj in print_objectives]\n\n  def print_objective_func(sess):\n    pos_results = sess.run(pos)\n    for k, v, i in zip(po_descriptions, pos_results, range(len(pos_results))):\n      print(""{:02d}: {}: {:7.2f}"".format(i+1, k, v))\n\n  return print_objective_func\n\n# pylint: enable=invalid-name\n\n\ndef make_t_image(param_f):\n  if param_f is None:\n    t_image = param.image(128)\n  elif callable(param_f):\n    t_image = param_f()\n  elif isinstance(param_f, tf.Tensor):\n    t_image = param_f\n  else:\n    raise TypeError(""Incompatible type for param_f, "" + str(type(param_f)) )\n\n  if not isinstance(t_image, tf.Tensor):\n    raise TypeError(""param_f should produce a Tensor, but instead created a ""\n                   + str(type(t_image)) )\n  elif t_image.graph != tf.get_default_graph():\n    raise TypeError(""""""param_f produced a t_image tensor belonging to a graph\n                     that isn\'t the default graph for rendering. Did you\n                     accidentally use render_vis when you meant to use\n                     make_vis_T?"""""")\n  else:\n    return t_image\n\n\ndef make_transform_f(transforms):\n  if type(transforms) is not list:\n    transforms = transform.standard_transforms\n  transform_f = transform.compose(transforms)\n  return transform_f\n\n\ndef make_optimizer(optimizer, args):\n  if optimizer is None:\n    return tf.train.AdamOptimizer(0.05)\n  elif callable(optimizer):\n    return optimizer(*args)\n  elif isinstance(optimizer, tf.train.Optimizer):\n    return optimizer\n  else:\n    raise (""Could not convert optimizer argument to usable optimizer. ""\n           ""Needs to be one of None, function from (graph, sess) to ""\n           ""optimizer, or tf.train.Optimizer instance."")\n\n\ndef import_model(model, t_image, t_image_raw=None, scope=""import"", input_map=None):\n  if t_image_raw is None:\n    t_image_raw = t_image\n\n  T_ = model.import_graph(t_image, scope=scope, forget_xy_shape=True, input_map=input_map)\n\n  def T(layer):\n    if layer == ""input"": return t_image_raw\n    if layer == ""labels"": return model.labels\n    return T_(layer)\n\n  return T\n'"
lucid/optvis/style.py,16,"b'""""""Neural Image Style Transfer utils.""""""\n\nimport tensorflow as tf\n\n\ndef gram_style(a):\n  with tf.name_scope(\'gram\'):\n    chn = int(a.shape[-1])\n    a = tf.reshape(a, [-1, chn])\n    n = tf.shape(a)[0]\n    gram = tf.matmul(a, a, transpose_a=True)\n    return gram / tf.cast(n, tf.float32)\n\n\ndef mean_l1_loss(g1, g2):\n  with tf.name_scope(\'mean_l1_loss\'):\n    return tf.reduce_mean(tf.abs(g1-g2))\n\n\ndef mean_l2_loss(g1, g2):\n  with tf.name_scope(\'mean_l2_loss\'):\n    return tf.sqrt(tf.reduce_mean(tf.square(g1 - g2)))\n\n\nclass StyleLoss(object):\n  """"""Image Style Loss.\n  \n  A variant of style component of Artistic Image Style Transfer loss,\n  mainly inspired by [1].\n\n  [1] Leon A. Gatys et al. ""A Neural Algorithm of Artistic Style""\n      https://arxiv.org/abs/1508.06576\n  """"""\n\n  def __init__(self, style_layers, ema_decay=None,\n               style_func=gram_style,\n               loss_func=mean_l1_loss):\n    """"""Initilize style loss.\n    \n    Args:\n      style_layers: List of tensors that are used to compute statistics that\n        define a style.\n      ema_decay: Number in range [0.0 .. 1.0] or None. Loss function is computed against\n        moving averaged versions of style statistics if ema_decay is not None.\n        This is useful when each optimisation step only covers some part of\n        the full output image.\n      style_func: Function that is used to compute layer statistics.\n      loss_func: Function that is used to compute difference between two\n        outputs of \'style_func\'.\n    """"""\n    self.input_grams = [style_func(s) for s in style_layers]\n    self.ema = None\n    \n    if ema_decay is not None:\n      # creating moving average versions of input Gram matrices\n      self.ema = tf.train.ExponentialMovingAverage(decay=ema_decay)\n      update_ema_op = self.ema.apply(self.input_grams)\n      # averages get updated before evaluation of effective_grams\n      with tf.control_dependencies([update_ema_op]):\n        # Using stop_gradient trick to substitute each Gram matrix with its\n        # moving avarage before style loss computation, but still propagate\n        # loss gradients to the input.\n        self.effective_grams = [g + tf.stop_gradient(self.ema.average(g)-g)\n                                for g in self.input_grams]\n    else:\n      self.effective_grams = self.input_grams\n    \n    self.target_vars = [tf.Variable(tf.zeros_like(g), trainable=False)\n                        for g in self.input_grams]\n    self.style_losses = [loss_func(g, gv)\n                         for g, gv in zip(self.effective_grams, self.target_vars)]\n    self.style_loss = tf.add_n(self.style_losses)\n\n  def set_style(self, input_feeds):\n    """"""Set target style variables.\n    \n    Expected usage: \n      style_loss = StyleLoss(style_layers)\n      ...\n      init_op = tf.global_variables_initializer()\n      init_op.run()\n      \n      feeds = {... session.run() \'feeds\' argument that will make \'style_layers\'\n               tensors evaluate to activation values of style image...}\n      style_loss.set_style(feeds)  # this must be called after \'init_op.run()\'\n    """"""\n    sess = tf.get_default_session()\n    computed = sess.run(self.input_grams, input_feeds)\n    for v, g in zip(self.target_vars, computed):\n      v.load(g)\n'"
lucid/optvis/transform.py,23,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Tranformations you might want neural net visualizations to be robust to.\n\nThis module provides a variety of functions which stochastically transform a\ntensorflow tensor. The functions are of the form:\n\n  (config) => (tensor) => (stochastic transformed tensor)\n\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport uuid\n\nfrom lucid.optvis import param\n\n\ndef jitter(d, seed=None):\n    def inner(t_image):\n        t_image = tf.convert_to_tensor(t_image, preferred_dtype=tf.float32)\n        t_shp = tf.shape(t_image)\n        crop_shape = tf.concat([t_shp[:-3], t_shp[-3:-1] - d, t_shp[-1:]], 0)\n        crop = tf.random_crop(t_image, crop_shape, seed=seed)\n        shp = t_image.get_shape().as_list()\n        mid_shp_changed = [\n            shp[-3] - d if shp[-3] is not None else None,\n            shp[-2] - d if shp[-3] is not None else None,\n        ]\n        crop.set_shape(shp[:-3] + mid_shp_changed + shp[-1:])\n        return crop\n\n    return inner\n\n\ndef pad(w, mode=""REFLECT"", constant_value=0.5):\n    def inner(t_image):\n        if constant_value == ""uniform"":\n            constant_value_ = tf.random_uniform([], 0, 1)\n        else:\n            constant_value_ = constant_value\n        return tf.pad(\n            t_image,\n            [(0, 0), (w, w), (w, w), (0, 0)],\n            mode=mode,\n            constant_values=constant_value_,\n        )\n\n    return inner\n\n\n# def random_scale(scales, seed=None):\n#   def inner(t):\n#     t = tf.convert_to_tensor(t, preferred_dtype=""float32"")\n#     scale = _rand_select(scales, seed=seed)\n#     shp = tf.shape(t)\n#     scale_shape = tf.concat(\n#         [shp[:-3], tf.cast(scale * tf.cast(shp[-3:-1], ""float32""), ""int32""), shp[-1:]], 0)\n#     return resize_bilinear_nd(t, scale_shape)\n#   return inner\n\n# 2D only version\ndef random_scale(scales, seed=None):\n    def inner(t):\n        t = tf.convert_to_tensor(t, preferred_dtype=tf.float32)\n        scale = _rand_select(scales, seed=seed)\n        shp = tf.shape(t)\n        scale_shape = tf.cast(scale * tf.cast(shp[-3:-1], ""float32""), ""int32"")\n        return tf.image.resize_bilinear(t, scale_shape)\n\n    return inner\n\n\ndef random_rotate(angles, units=""degrees"", seed=None):\n    def inner(t):\n        t = tf.convert_to_tensor(t, preferred_dtype=tf.float32)\n        angle = _rand_select(angles, seed=seed)\n        angle = _angle2rads(angle, units)\n        return tf.contrib.image.rotate(t, angle)\n\n    return inner\n\n\ndef normalize_gradient(grad_scales=None):\n\n    if grad_scales is not None:\n        grad_scales = np.float32(grad_scales)\n\n    op_name = ""NormalizeGrad_"" + str(uuid.uuid4())\n\n    @tf.RegisterGradient(op_name)\n    def _NormalizeGrad(op, grad):\n        grad_norm = tf.sqrt(tf.reduce_sum(grad ** 2, [1, 2, 3], keepdims=True))\n        if grad_scales is not None:\n            grad *= grad_scales[:, None, None, None]\n        return grad / grad_norm\n\n    def inner(x):\n        with x.graph.gradient_override_map({""Identity"": op_name}):\n            x = tf.identity(x)\n        return x\n\n    return inner\n\n\ndef compose(transforms):\n    def inner(x):\n        for transform in transforms:\n            x = transform(x)\n        return x\n\n    return inner\n\n\ndef collapse_alpha_random(sd=0.5):\n    def inner(t_image):\n        rgb, a = t_image[..., :3], t_image[..., 3:4]\n        rgb_shape = rgb.get_shape().as_list()\n        rand_img = param.random.image_sample(rgb_shape, sd=sd)\n        return a * rgb + (1 - a) * rand_img\n\n    return inner\n\n\ndef _rand_select(xs, seed=None):\n    xs_list = list(xs)\n    rand_n = tf.random_uniform((), 0, len(xs_list), ""int32"", seed=seed)\n    return tf.constant(xs_list)[rand_n]\n\n\ndef _angle2rads(angle, units):\n    angle = tf.cast(angle, ""float32"")\n    if units.lower() == ""degrees"":\n        angle = 3.14 * angle / 180.\n    elif units.lower() in [""radians"", ""rads"", ""rad""]:\n        angle = angle\n    return angle\n\n\ndef crop_or_pad_to(height, width):\n    """"""Ensures the specified spatial shape by either padding or cropping.\n    Meant to be used as a last transform for architectures insisting on a specific\n    spatial shape of their inputs.\n    """"""\n    def inner(t_image):\n        return tf.image.resize_image_with_crop_or_pad(t_image, height, width)\n    return inner\n\n\nstandard_transforms = [\n    pad(12, mode=""constant"", constant_value=.5),\n    jitter(8),\n    random_scale([1 + (i - 5) / 50. for i in range(11)]),\n    random_rotate(list(range(-10, 11)) + 5 * [0]),\n    jitter(4),\n]\n'"
lucid/recipes/__init__.py,0,b''
lucid/recipes/caricature.py,7,"b'from os.path import join\nimport json\n\nimport numpy as np\nimport tensorflow as tf\nimport scipy.ndimage as nd\n\nimport lucid.modelzoo.vision_models as models\nimport lucid.optvis.objectives as objectives\nimport lucid.optvis.param as param\nimport lucid.optvis.render as render\nimport lucid.optvis.transform as transform\nfrom lucid.misc.io import show, load, save\nfrom lucid.misc.io.reading import read\nfrom lucid.misc.ndimage_utils import resize\nimport lucid.misc.io.showing\nfrom lucid.modelzoo.vision_base import SerializedModel\n\n\n@objectives.wrap_objective()\ndef dot_compare(layer, batch=1, cossim_pow=0):\n  def inner(T):\n    acts1 = T(layer)[0]\n    acts2 = T(layer)[batch]\n    dot = tf.reduce_sum(acts1 * acts2)\n    mag = tf.sqrt(tf.reduce_sum(acts2**2))\n    cossim = dot / (1e-6 + mag)\n    cossim = tf.maximum(0.1, cossim)\n    return dot * cossim ** cossim_pow\n  return inner\n\n\ndef feature_inversion(*args, **kwargs):\n  return caricature(*args, **kwargs)\n\n\ndef caricature(img, model, layer, n_steps=512, cossim_pow=0.0, verbose=True):\n  if isinstance(layer, str):\n    layers = [layer]\n  elif isinstance(layer, (tuple, list)):\n    layers = layer\n  else:\n    raise TypeError(""layer must be str, tuple or list"")\n\n  with tf.Graph().as_default(), tf.Session() as sess:\n    img = resize(img, model.image_shape[:2])\n\n    objective = objectives.Objective.sum([\n        1.0 * dot_compare(layer, cossim_pow=cossim_pow, batch=i+1)\n        for i, layer in enumerate(layers)\n    ])\n\n    t_input = tf.placeholder(tf.float32, img.shape)\n    param_f = param.image(img.shape[0], decorrelate=True, fft=True, alpha=False, batch=len(layers))\n    param_f = tf.concat([t_input[None], param_f], 0)\n\n    transforms = transform.standard_transforms + [transform.crop_or_pad_to(*model.image_shape[:2])]\n\n    T = render.make_vis_T(model, objective, param_f, transforms=transforms)\n    loss, vis_op, t_image = T(""loss""), T(""vis_op""), T(""input"")\n\n    tf.global_variables_initializer().run()\n    for i in range(n_steps): _ = sess.run([vis_op], {t_input: img})\n\n    result = t_image.eval(feed_dict={t_input: img})\n\n  if verbose:\n    lucid.misc.io.showing.images(result[1:], layers)\n  return result\n\n\ndef make_caricature(image_url, saved_model_folder_url, to, *args, **kwargs):\n  image = load(image_url)\n  model = SerializedModel.from_directory(saved_model_folder_url)\n  layers = model.layer_names\n  caricatures = caricature(image, model, layers, *args, verbose=False, **kwargs)\n\n  results = {""type"": ""caricature""}\n\n  save_input_url = join(to, ""input.jpg"")\n  save(caricatures[0], save_input_url)\n  results[""input_image""] = save_input_url\n\n  values_list = []\n  for single_caricature, layer_name in zip(caricatures[1:], model.layer_names):\n    save_caricature_url = join(to, layer_name + "".jpg"")\n    save(single_caricature, save_caricature_url)\n    values_list.append({""type"": ""image"", ""url"": save_caricature_url, ""shape"": single_caricature.shape})\n  results[""values""] = values_list\n\n  save(results, join(to, ""results.json""))\n\n  return results\n'"
lucid/recipes/feature_visualization.py,5,"b'from timeit import default_timer as timer\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom lucid.optvis import render\nfrom lucid.optvis import param\nfrom lucid.optvis import transform\nfrom lucid.optvis import objectives\nfrom lucid.optvis import overrides\nfrom lucid.misc.io import show, load\nfrom lucid.modelzoo.vision_base import Layer\n\n\ndef neuron(*args, **kwargs):\n    return _main(objectives.neuron, *args, **kwargs)\n\n\ndef channel(*args, **kwargs):\n    return _main(objectives.channel, *args, **kwargs)\n\n\ndef _main(\n    objective_f,\n    model,\n    layer,\n    channel_indices,\n    alpha=False,\n    negative=False,\n    decorrelation_matrix=None,\n    n_steps=128,\n    parallel_transforms=16,\n    lr=0.05,\n    override_gradients=True,\n):\n    if not isinstance(channel_indices, (tuple, list)):\n        channel_indices = [channel_indices]\n\n    if isinstance(layer, Layer):\n      layer_name = layer.name\n    elif isinstance(layer, str):\n      layer_name = layer\n    else:\n      raise ValueError(""layer argument can be either a Layer object or str"")\n\n    sign = -1 if negative else 1\n    batch = len(channel_indices)\n    w, h, _ = model.image_shape\n\n    with tf.Graph().as_default(), tf.Session() as sess:\n\n        # Transformation Robustness\n        transforms = transform.standard_transforms + [transform.crop_or_pad_to(w, h)]\n        if alpha:\n            transforms += [transform.collapse_alpha_random()]\n        transform_f = render.make_transform_f(transforms)\n\n        # Parameterization\n        image_t = param.image(\n            w, h, fft=True, decorrelate=True, alpha=alpha, batch=batch\n        )\n        param_t = transform_f(image_t)\n\n        # Gradient Overrides\n        if override_gradients:\n            with overrides.relu_overrides():\n                T = render.import_model(model, param_t, image_t)\n        else:\n            T = render.import_model(model, param_t, image_t)\n\n        # Objective\n        if decorrelation_matrix is None:\n            objs = [\n                sign * objective_f(layer_name, i, batch=b)\n                for b, i in enumerate(channel_indices)\n            ]\n        else:\n            raise NotImplementedError\n\n        reported_obj = tf.stack([o(T) for o in objs])\n\n        obj = sum(objs)(T)\n\n        if alpha:\n            obj *= 1.0 - tf.reduce_mean(image_t[..., -1])\n            obj -= 0.1 * objectives.blur_alpha_each_step()(T)\n\n        # Optimization\n\n        optimization = tf.train.AdamOptimizer(lr).minimize(-obj)\n        tf.global_variables_initializer().run()\n        losses = np.zeros((batch, n_steps))\n        for step in range(n_steps):\n            _, loss = sess.run([optimization, reported_obj])\n            losses[:, step] = loss\n\n        # Evaluation\n        visualization = image_t.eval()\n\n    if batch == 1:\n      return (visualization, losses)\n    else:\n      return list(zip(visualization, losses))\n\n\n# def manifest_neuron(model, layer_name, channel_index, **kwargs):\n#     start = timer()\n#     visualization, losses = neuron(model, layer_name, channel_index, **kwargs)\n#     end = timer()\n#     elapsed = end - start\n\n#     results = {\n#         ""type"": ""feature-visualization-neuron"",\n#         ""took"": elapsed,\n#         ""layer_name"": layer_name,\n#         ""channel_index"": channel_index,\n#         ""objective_values"": losses,\n#     }\n\n#     results[""values""] = [\n#         {""type"": ""image"", ""value"": visualization, ""shape"": visualization.shape}\n#     ]\n\n#     return results\n'"
lucid/recipes/image_activations.py,2,"b'from timeit import default_timer as timer\n\nimport tensorflow as tf\n\nfrom lucid.optvis import render\nfrom lucid.optvis import param\nfrom lucid.optvis import transform\nfrom lucid.optvis import objectives\nfrom lucid.optvis import overrides\nfrom lucid.misc.io import show, load\nfrom lucid.misc.ndimage_utils import resize\n\n\ndef image_activations(model, image, layer_names=None):\n    if layer_names is None:\n        layer_names = [layer[""name""] for layer in model.layers if ""conv"" in layer.tags]\n\n    resized_image = resize(image, model.image_shape[:2])\n\n    with tf.Graph().as_default() as graph, tf.Session() as sess:\n        image_t = tf.placeholder_with_default(resized_image, shape=model.image_shape)\n        model.import_graph(image_t, scope=""import"")\n        layer_ts = {}\n        for layer_name in layer_names:\n          name = layer_name if layer_name.endswith("":0"") else layer_name + "":0""\n          layer_t = graph.get_tensor_by_name(""import/{}"".format(name))[0]\n          layer_ts[layer_name] = layer_t\n        activations = sess.run(layer_ts)\n\n    return activations\n\n\ndef manifest_image_activations(model, image, **kwargs):\n    start = timer()\n    activations_dict = image_activations(model, image, **kwargs)\n    end = timer()\n    elapsed = end - start\n\n    results = {""type"": ""image-activations"", ""took"": elapsed}\n\n    results[""values""] = [\n        {\n            ""type"": ""activations"",\n            ""value"": value,\n            ""shape"": value.shape,\n            ""layer_name"": layer_name,\n        }\n        for layer_name, value in activations_dict.items()\n    ]\n\n    return results\n'"
lucid/recipes/image_interpolation_params.py,5,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom lucid.optvis.param import lowres_tensor\n\n\ndef multi_interpolation_basis(n_objectives=6, n_interp_steps=5, width=128,\n                              channels=3):\n  """"""A paramaterization for interpolating between each pair of N objectives.\n\n  Sometimes you want to interpolate between optimizing a bunch of objectives,\n  in a paramaterization that encourages images to align.\n\n  Args:\n    n_objectives: number of objectives you want interpolate between\n    n_interp_steps: number of interpolation steps\n    width: width of intepolated images\n    channel\n\n  Returns:\n    A [n_objectives, n_objectives, n_interp_steps, width, width, channel]\n    shaped tensor, t, where the final [width, width, channel] should be\n    seen as images, such that the following properties hold:\n\n     t[a, b]    = t[b, a, ::-1]\n     t[a, i, 0] = t[a, j, 0] for all i, j\n     t[a, a, i] = t[a, a, j] for all i, j\n     t[a, b, i] = t[b, a, -i] for all i\n\n  """"""\n  N, M, W, Ch = n_objectives, n_interp_steps, width, channels\n\n  const_term = sum([lowres_tensor([W, W, Ch], [W//k, W//k, Ch])\n                    for k in [1, 2, 4, 8]])\n  const_term = tf.reshape(const_term, [1, 1, 1, W, W, Ch])\n\n  example_interps = [\n      sum([lowres_tensor([M, W, W, Ch], [2, W//k, W//k, Ch])\n           for k in [1, 2, 4, 8]])\n      for _ in range(N)]\n\n  example_basis = []\n  for n in range(N):\n    col = []\n    for m in range(N):\n      interp = example_interps[n] + example_interps[m][::-1]\n      col.append(interp)\n    example_basis.append(col)\n\n  interp_basis = []\n  for n in range(N):\n    col = [interp_basis[m][N-n][::-1] for m in range(n)]\n    col.append(tf.zeros([M, W, W, 3]))\n    for m in range(n+1, N):\n      interp = sum([lowres_tensor([M, W, W, Ch], [M, W//k, W//k, Ch])\n                    for k in [1, 2]])\n      col.append(interp)\n    interp_basis.append(col)\n\n  basis = []\n  for n in range(N):\n    col_ex = tf.stack(example_basis[n])\n    col_in = tf.stack(interp_basis[n])\n    basis.append(col_ex + col_in)\n  basis = tf.stack(basis)\n\n  return basis + const_term\n'"
lucid/scratch/__init__.py,0,b''
lucid/scratch/parameter_editor.py,3,"b'import numpy as np\nimport tensorflow as tf\n\n\nclass ParameterEditor():\n  """"""Conveniently edit the parameters of a lucid model.\n\n  Example usage:\n\n    model = models.InceptionV1()\n    param = ParameterEditor(model.graph_def)\n    # Flip weights of first channel of conv2d0\n    param[""conv2d0_w"", :, :, :, 0] *= -1\n\n  """"""\n\n  def __init__(self, graph_def):\n    self.nodes = {}\n    for node in graph_def.node:\n      if ""value"" in node.attr:\n        self.nodes[str(node.name)] = node\n    # Set a flag to mark the fact that this is an edited model\n    if not ""lucid_is_edited"" in self.nodes:\n      with tf.Graph().as_default() as temp_graph:\n        const = tf.constant(True, name=""lucid_is_edited"")\n        const_node = temp_graph.as_graph_def().node[0]\n        graph_def.node.extend([const_node])\n\n\n  def __getitem__(self, key):\n    name = key[0] if isinstance(key, tuple) else key\n    tensor = self.nodes[name].attr[""value""].tensor\n    shape = [int(d.size) for d in tensor.tensor_shape.dim]\n    array = np.frombuffer(tensor.tensor_content, dtype=""float32"").reshape(shape).copy()\n    return array[key[1:]] if isinstance(key, tuple) else array\n\n  def __setitem__(self, key, new_value):\n    name = key[0] if isinstance(key, tuple) else key\n    tensor = self.nodes[name].attr[""value""].tensor\n    node_shape = tuple([int(d.size) for d in tensor.tensor_shape.dim])\n    if isinstance(key, tuple):\n      array = np.frombuffer(tensor.tensor_content, dtype=""float32"")\n      array = array.reshape(node_shape).copy()\n      array[key[1:]] = new_value\n      tensor.tensor_content = array.tostring()\n    else:\n      assert new_value.shape == node_shape\n      dtype = tf.DType(tensor.dtype).as_numpy_dtype\n      tensor.tensor_content = new_value.astype(dtype).tostring()\n'"
tests/misc/test_channel_reducer.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport numpy as np\nfrom lucid.misc.channel_reducer import ChannelReducer\n\n\ndef test_channel_reducer_trivial():\n    array = np.zeros((10, 10, 10))\n    for d in range(array.shape[-1]):\n        array[:, :, d] = np.eye(10, 10)\n\n    channel_reducer = ChannelReducer(n_components=2)\n    channel_reducer.fit(array)\n    reduced = channel_reducer.transform(array)\n\n    assert reduced.shape == (10, 10, 2)\n    # the hope here is that this was reduced to use only the first channel\n    assert np.sum(reduced[:, :, 1]) == 0\n'"
tests/misc/test_environment.py,0,"b""from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nfrom lucid.misc.environment import is_notebook_environment\n\ndef test_is_notebook_environment():\n  is_notebook = is_notebook_environment()\n  assert not is_notebook  # tests aren't usually run in notebook env\n"""
tests/misc/test_gradient_override.py,31,"b'import pytest\n\nimport tensorflow as tf\nfrom lucid.misc.gradient_override import use_gradient\n\ndef test_use_gradient():\n  def foo_grad(op, grad):\n    return tf.constant(42), tf.constant(43)\n\n  @use_gradient(foo_grad)\n  def foo(x, y):\n    return x + y\n\n  with tf.Session().as_default() as sess:\n    x = tf.constant(1.)\n    y = tf.constant(2.)\n    z = foo(x, y)\n    grad_wrt_x = tf.gradients(z, x, [1.])[0]\n    grad_wrt_y = tf.gradients(z, y, [1.])[0]\n    assert grad_wrt_x.eval() == 42\n    assert grad_wrt_y.eval() == 43\n\n\nfrom lucid.misc.gradient_override import gradient_override_map\n\ndef test_gradient_override_map():\n\n  def gradient_override(op, grad):\n    return tf.constant(42)\n\n  with tf.Session().as_default() as sess:\n    global_step = tf.train.get_or_create_global_step()\n    init_global_step = tf.variables_initializer([global_step])\n    init_global_step.run()\n\n    a = tf.constant(1.)\n    standard_relu = tf.nn.relu(a)\n    grad_wrt_a = tf.gradients(standard_relu, a, [1.])[0]\n    with gradient_override_map({""Relu"": gradient_override}):\n      overriden_relu = tf.nn.relu(a)\n      overriden_grad_wrt_a = tf.gradients(overriden_relu, a, [1.])[0]\n    assert grad_wrt_a.eval() != overriden_grad_wrt_a.eval()\n    assert overriden_grad_wrt_a.eval() == 42\n\n\nfrom lucid.misc.redirected_relu_grad import redirected_relu_grad, redirected_relu6_grad\n\nrelu_examples = [\n    (1., -1., 0.), (-1., -1., -1.),\n    (1.,  1., 1.), (-1.,  1., -1.),\n]\nrelu6_examples = relu_examples + [\n    (1.,  7., 1.), (-1.,  7.,  0.),\n]\nnonls = [(""Relu"", tf.nn.relu, redirected_relu_grad, relu_examples),\n         (""Relu6"", tf.nn.relu6, redirected_relu6_grad, relu6_examples)]\n\n@pytest.mark.parametrize(""nonl_name,nonl,nonl_grad_override, examples"", nonls)\ndef test_gradient_override_relu6_directionality(nonl_name, nonl,\n    nonl_grad_override, examples):\n  for incoming_grad, input, grad in examples:\n    with tf.Session().as_default() as sess:\n      global_step = tf.train.get_or_create_global_step()\n      init_global_step = tf.variables_initializer([global_step])\n      init_global_step.run()\n\n      batched_shape = [1,1]\n      incoming_grad_t = tf.constant(incoming_grad, shape=batched_shape)\n      input_t = tf.constant(input, shape=batched_shape)\n      with gradient_override_map({nonl_name: nonl_grad_override}):\n        nonl_t = nonl(input_t)\n        grad_wrt_input = tf.gradients(nonl_t, input_t, [incoming_grad_t])[0]\n      assert (grad_wrt_input.eval() == grad).all()\n\n@pytest.mark.parametrize(""nonl_name,nonl,nonl_grad_override, examples"", nonls)\ndef test_gradient_override_shutoff(nonl_name, nonl,\n    nonl_grad_override, examples):\n  for incoming_grad, input, grad in examples:\n    with tf.Session().as_default() as sess:\n      global_step_t = tf.train.get_or_create_global_step()\n      global_step_init_op = tf.variables_initializer([global_step_t])\n      global_step_init_op.run()\n      global_step_assign_t = tf.assign(global_step_t, 17)\n      sess.run(global_step_assign_t)\n\n      # similar setup to test_gradient_override_relu6_directionality,\n      # but we test that the gradient is *not* what we\'re expecting as after 16\n      # steps the override is shut off\n      batched_shape = [1,1]\n      incoming_grad_t = tf.constant(incoming_grad, shape=batched_shape)\n      input_t = tf.constant(input, shape=batched_shape)\n      with gradient_override_map({nonl_name: nonl_grad_override}):\n        nonl_t = nonl(input_t)\n        grad_wrt_input = tf.gradients(nonl_t, input_t, [incoming_grad_t])[0]\n      nonl_t_no_override = nonl(input_t)\n      grad_wrt_input_no_override = tf.gradients(nonl_t_no_override, input_t, [incoming_grad_t])[0]\n      assert (grad_wrt_input.eval() == grad_wrt_input_no_override.eval()).all()\n'"
tests/misc/test_ndimage_utils.py,0,"b'import pytest\nimport numpy as np\n\nfrom lucid.misc.io import load, save\n\nfrom lucid.misc.ndimage_utils import resize, composite\n\n\n@pytest.fixture()\ndef image():\n    return load(""./tests/fixtures/rgbeye.png"")\n\n\ndef test_resize(image):\n    size = (3, 3)\n    resized = resize(image, size)\n    assert resized.shape[-3:-1] == size\n\n\ndef test_resize_noop(image):\n    """"""FYI: this essentially tests the `if original_size == target_size: return image`\n    part of `resize()`\'s implementation. Without it, machine precision makes the\n    following assert no longer true, but I thought it was an imoprtant property to have.\n    """"""\n    resized = resize(image, image.shape[-3:-1])\n    assert np.all(resized == image)\n\n\n@pytest.mark.parametrize(""corner"", [(0, 0), (0, 1), (1, 0), (1, 1)])\ndef test_composite(corner):\n    background_shape = (10, 10, 1)\n    foreground_shape = (2, 2, 1)\n    black = np.zeros(background_shape)\n    white = np.ones(foreground_shape)\n\n    comp = composite(\n        black, white, foreground_position=corner, foreground_width_ratio=0.2\n    )\n\n    wh = comp.shape[-3:-1]\n    xy = [min(d * c, d - 1) for d, c in zip(wh, corner)]\n    print(wh, corner, xy)\n    assert comp[xy[0], xy[1], 0] > 0.99  # because interpolation!\n    assert comp[5, 5] == 0\n'"
tests/modelzoo/test_InceptionV1.py,1,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport tensorflow as tf\nfrom lucid.modelzoo.vision_models import InceptionV1\nfrom lucid.modelzoo.aligned_activations import get_aligned_activations\n\nimportant_layer_names = [\n    ""mixed3a"",\n    ""mixed3b"",\n    ""mixed4a"",\n    ""mixed4b"",\n    ""mixed4c"",\n    ""mixed4d"",\n    ""mixed4e"",\n    ""mixed5a"",\n    ""mixed5b"",\n]\n\n\n@pytest.mark.slow\ndef test_InceptionV1_model_download():\n    model = InceptionV1()\n    assert model.graph_def is not None\n\n\n@pytest.mark.slow\ndef test_InceptionV1_graph_import():\n    model = InceptionV1()\n    model.import_graph()\n    nodes = tf.get_default_graph().as_graph_def().node\n    node_names = set(node.name for node in nodes)\n    for layer_name in important_layer_names:\n        assert ""import/"" + layer_name + ""_pre_relu"" in node_names\n\n\ndef test_InceptionV1_labels():\n    model = InceptionV1()\n    assert model.labels is not None\n    assert model.labels[0] == ""dummy""\n\n\n@pytest.mark.slow\ndef test_InceptionV1_aligned_activations():\n    model = InceptionV1()\n    activations = get_aligned_activations(model.layers[0])\n    assert activations.shape == (100000, 64)\n\n\n'"
tests/modelzoo/test_nets_factory.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pytest\n\nfrom lucid.modelzoo.nets_factory import get_model, models_map\nfrom lucid.modelzoo.vision_models import InceptionV1\nfrom lucid.modelzoo.vision_base import Model, SerializedModel, FrozenGraphModel\n\ndef test_models_map():\n  assert len(models_map) > 1\n  assert Model.__name__ not in models_map\n  assert SerializedModel.__name__ not in models_map\n  assert FrozenGraphModel.__name__ not in models_map\n  assert InceptionV1.__name__ in models_map\n\ndef test_get_model():\n  model = get_model(""InceptionV1"")\n  assert model is not None\n  assert type(model) == InceptionV1\n\ndef test_get_model_fuzzy_feedback():\n  with pytest.raises(ValueError) as excinfo:\n    _ = get_model(""InceptionV2"")\n  assert ""InceptionV2_slim"" in str(excinfo.value)\n'"
tests/modelzoo/test_saveload.py,2,"b'import pytest\nimport tensorflow as tf\n\nfrom lucid.modelzoo.vision_base import Model\nfrom lucid.modelzoo.vision_models import AlexNet\n\n\nshape = (16,16,3)\n\ndef test_Model_save(minimodel):\n  with tf.Session().as_default() as sess:\n    _ = minimodel()\n    sess.run(tf.global_variables_initializer())\n    path = ""./tests/fixtures/minigraph.pb""\n    Model.save(path, ""input"", [""output""], shape, [0,1])\n\ndef test_Model_load():\n  path = ""./tests/fixtures/minigraph.pb""\n  model = Model.load(path)\n  assert all(str(shape[i]) in repr(model.graph_def) for i in range(len(shape)))\n'"
tests/modelzoo/test_vision_base.py,6,"b'import pytest\nimport tensorflow as tf\n\nfrom lucid.modelzoo.vision_base import Model\nfrom lucid.modelzoo.vision_models import AlexNet, InceptionV1, InceptionV3_slim, ResnetV1_50_slim\n\n\ndef test_suggest_save_args_happy_path(capsys, minimodel):\n  path = ""./tests/fixtures/minigraph.pb""\n\n  with tf.Graph().as_default() as graph, tf.Session() as sess:\n    _ = minimodel()\n    sess.run(tf.global_variables_initializer())\n\n    # ask for suggested arguments\n    inferred = Model.suggest_save_args()\n    # they should be both printed...\n    captured = capsys.readouterr().out  # captures stdout\n    names = [""input_name"", ""image_shape"", ""output_names""]\n    assert all(name in captured for name in names)\n    #...and returned\n\n    # check that these inferred values work\n    inferred.update(image_value_range=(0,1))\n    Model.save(path, **inferred)\n    loaded_model = Model.load(path)\n    assert ""0.100"" in repr(loaded_model.graph_def)\n\n\ndef test_suggest_save_args_int_input(capsys, minimodel):\n  with tf.Graph().as_default() as graph, tf.Session() as sess:\n    image_t = tf.placeholder(tf.uint8, shape=(32, 32, 3), name=""input"")\n    input_t = tf.math.divide(image_t, tf.constant(255, dtype=tf.uint8), name=""divide"")\n    _ = minimodel(input_t)\n    sess.run(tf.global_variables_initializer())\n\n    # ask for suggested arguments\n    inferred = Model.suggest_save_args()\n    captured = capsys.readouterr().out  # captures stdout\n    assert ""DT_UINT8"" in captured\n    assert inferred[""input_name""] == ""divide""\n\n\n@pytest.mark.parametrize(""model_class"", [AlexNet, InceptionV1, InceptionV3_slim, ResnetV1_50_slim])\ndef test_suggest_save_args_existing_graphs(capsys, model_class):\n  graph_def = model_class().graph_def\n\n  if model_class == InceptionV1:  # has flexible input shape, can\'t be inferred\n    with pytest.warns(UserWarning):\n      inferred = Model.suggest_save_args(graph_def)\n  else:\n    inferred = Model.suggest_save_args(graph_def)\n\n  assert model_class.input_name == inferred[""input_name""]\n\n  if model_class != InceptionV1:\n    assert model_class.image_shape == inferred[""image_shape""]\n\n  layer_names = [layer.name for layer in model_class.layers]\n  for output_name in list(inferred[""output_names""]):\n    assert output_name in layer_names\n'"
tests/modelzoo/test_vision_models.py,1,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\nimport inspect\nimport tensorflow as tf\n\nfrom lucid.modelzoo.nets_factory import models_map, get_model\nfrom lucid.modelzoo import caffe_models, other_models, slim_models, vision_models\nfrom lucid.modelzoo.vision_base import Layer\n\n\nclean_modules = [\n    caffe_models,\n    other_models,\n    slim_models,\n    vision_models\n]\n\nforbidden_names = [\n    ""obj"",\n    ""name"",\n    ""_obj"",\n    ""_name"",\n    ""absolute_import"",\n    ""division"",\n    ""print_function"",\n    ""IMAGENET_MEAN"",\n    ""IMAGENET_MEAN_BGR"",\n    ""_layers_from_list_of_dicts""\n]\n\n\n@pytest.mark.parametrize(""module"", clean_modules, ids=lambda m: m.__name__.split(\'.\')[-1])\ndef test_clean_namespace(module):\n    names = dir(module)\n    for forbidden in forbidden_names:\n      assert forbidden not in names\n\n\ndef test_consistent_namespaces():\n    model_names = set(models_map.keys())\n    exported_model_names = set(dir(vision_models))\n    diffs = model_names.symmetric_difference(exported_model_names)\n    for difference in diffs:\n        assert difference in (\'Model\', \'Layer\') or difference.startswith(""__"")\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize(""name,model_class"", models_map.items())\ndef test_model_properties(name, model_class):\n    assert hasattr(model_class, ""model_path"")\n    assert model_class.model_path.endswith("".pb"")\n    assert hasattr(model_class, ""labels_path"")\n    assert model_class.labels_path.endswith("".txt"")\n    assert hasattr(model_class, ""dataset"")\n    assert hasattr(model_class, ""image_shape"")\n    assert len(model_class.image_shape) == 3\n    assert hasattr(model_class, ""image_value_range"")\n    assert hasattr(model_class, ""input_name"")\n    assert hasattr(model_class, ""layers"")\n    assert len(model_class.layers) > 0\n    last_layer = model_class.layers[-1]\n    assert \'dense\' in last_layer.tags\n    assert type(last_layer) == Layer\n    assert last_layer.model_class == model_class\n    model_instance = model_class()\n    assert model_instance.name == model_class.__name__\n    assert last_layer.model_name == model_instance.name\n\n@pytest.mark.slow\n@pytest.mark.parametrize(""model_class"", models_map.values())\ndef test_model_layers_shapes(model_class):\n    name = model_class.__name__\n    scope = ""TestLucidModelzoo""\n    model = model_class()\n    with tf.Graph().as_default() as graph:\n        model.import_graph(scope=scope)\n        for layer in model.layers:\n            name, declared_size = layer.name, layer.depth\n            imported_name = ""{}/{}:0"".format(scope, name)\n            tensor = graph.get_tensor_by_name(imported_name)\n            actual_size = tensor.shape[-1]\n            assert int(actual_size) == int(declared_size)\n'"
tests/modelzoo/test_wordnet.py,0,"b'import pytest\n\nimport nltk\n\nnltk.download(""wordnet"")\nfrom nltk.corpus import wordnet as wn\n\nfrom lucid.modelzoo.wordnet import (\n    id_from_synset,\n    synset_from_id,\n    imagenet_synset_ids,\n    imagenet_synsets,\n    imagenet_synset_from_description,\n)\n\n\n@pytest.fixture()\ndef synset():\n    return wn.synset(""great_white_shark.n.01"")\n\n\n@pytest.fixture()\ndef synset_id():\n    return ""n01484850""\n\n\ndef test_id_from_synset(synset, synset_id):\n    result = id_from_synset(synset)\n    assert result == synset_id\n\n\ndef test_synset_from_id(synset_id, synset):\n    result = synset_from_id(synset_id)\n    assert result == synset\n\n\ndef test_imagenet_synset_ids(synset_id):\n    synset_ids = imagenet_synset_ids()\n    assert len(synset_ids) == 1000\n    assert synset_id in synset_ids\n\n\ndef test_imagenet_synsets(synset):\n    synsets = imagenet_synsets()\n    assert len(synsets) == 1000\n    assert synset in synsets\n\n\ndef test_imagenet_synset_from_description(synset):\n    synset_from_description = imagenet_synset_from_description(""white shark"")\n    assert synset == synset_from_description\n\n\ndef test_imagenet_synset_from_description_raises(synset):\n    with pytest.raises(ValueError, match=r\'.*great_white_shark.*tiger_shark.*\'):\n        imagenet_synset_from_description(""shark"")\n'"
tests/optvis/test_integration.py,4,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport tensorflow as tf\nfrom lucid.optvis import objectives, param, render, transform\nfrom lucid.modelzoo.vision_models import InceptionV1\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize(""decorrelate"", [True, False])\n@pytest.mark.parametrize(""fft"", [True, False])\ndef test_integration(decorrelate, fft):\n    inceptionv1 = InceptionV1()\n    obj = objectives.neuron(""mixed3a_pre_relu"", 0)\n    param_f = lambda: param.image(16, decorrelate=decorrelate, fft=fft)\n    rendering = render.render_vis(\n        inceptionv1,\n        obj,\n        param_f=param_f,\n        thresholds=(1, 2),\n        verbose=False,\n        transforms=[],\n    )\n    start_image = rendering[0]\n    end_image = rendering[-1]\n    objective_f = objectives.neuron(""mixed3a"", 177)\n    param_f = lambda: param.image(64, decorrelate=decorrelate, fft=fft)\n    rendering = render.render_vis(\n        inceptionv1,\n        objective_f,\n        param_f,\n        verbose=False,\n        thresholds=(0, 64),\n        use_fixed_seed=True,\n    )\n    start_image, end_image = rendering\n\n    assert (start_image != end_image).any()\n\ndef arbitrary_channels_to_rgb(*args, **kwargs):\n        """"""Arbitrary parametrization for testing""""""\n        channels = kwargs.pop(\'channels\', None) or 10\n        full_im = param.image(*args, channels=channels, **kwargs)\n        r = tf.reduce_mean(full_im[...,:channels//3]**2, axis=-1)\n        g = tf.reduce_mean(full_im[...,channels//3:2*channels//3]**2, axis=-1)\n        b = tf.reduce_mean(full_im[...,2*channels//3:]**2, axis=-1)\n        return tf.stack([r,g,b], axis=-1)\n\n@pytest.mark.slow\ndef test_integration_any_channels():\n    inceptionv1 = InceptionV1()\n    objectives_f = [objectives.deepdream(""mixed4a_pre_relu""), \n                objectives.channel(""mixed4a_pre_relu"", 360), \n                objectives.neuron(""mixed3a"", 177)]\n    params_f = [lambda: param.grayscale_image_rgb(128),\n                lambda: arbitrary_channels_to_rgb(128, channels=10)]\n    for objective_f in objectives_f:\n        for param_f in params_f:\n            rendering = render.render_vis(\n                inceptionv1,\n                objective_f,\n                param_f,\n                verbose=False,\n                thresholds=(0, 64),\n                use_fixed_seed=True,\n            )\n            start_image, end_image = rendering\n\n            assert (start_image != end_image).any()'"
tests/optvis/test_objectives.py,3,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport tensorflow as tf\nimport numpy as np\nfrom lucid.optvis import objectives, param, render, transform\nfrom lucid.modelzoo.vision_models import InceptionV1\n\nnp.random.seed(42)\n\nNUM_STEPS = 3\n\n\n\n@pytest.fixture\ndef inceptionv1():\n    return InceptionV1()\n\n\n\ndef assert_gradient_ascent(objective, model, batch=None, alpha=False, shape=None):\n    with tf.Graph().as_default() as graph, tf.Session() as sess:\n        shape = shape or [1, 32, 32, 3]\n        t_input = param.image(shape[1], h=shape[2], batch=batch, alpha=alpha)\n        if alpha:\n            t_input = transform.collapse_alpha_random()(t_input)\n        model.import_graph(t_input, scope=""import"", forget_xy_shape=True)\n\n        def T(layer):\n            if layer == ""input"":\n                return t_input\n            if layer == ""labels"":\n                return model.labels\n            return graph.get_tensor_by_name(""import/%s:0"" % layer)\n\n        loss_t = objective(T)\n        opt_op = tf.train.AdamOptimizer(0.1).minimize(-loss_t)\n        tf.global_variables_initializer().run()\n        start_value = sess.run([loss_t])\n        for _ in range(NUM_STEPS):\n            _ = sess.run([opt_op])\n        end_value, = sess.run([loss_t])\n        print(start_value, end_value)\n        assert start_value < end_value\n\n\ndef test_neuron(inceptionv1):\n    objective = objectives.neuron(""mixed4a_pre_relu"", 42)\n    assert_gradient_ascent(objective, inceptionv1)\n\n\ndef test_channel(inceptionv1):\n    objective = objectives.channel(""mixed4a_pre_relu"", 42)\n    assert_gradient_ascent(objective, inceptionv1)\n\n\n@pytest.mark.parametrize(""cossim_pow"", [0, 1, 2])\ndef test_direction(cossim_pow, inceptionv1):\n    mixed_4a_depth = 508\n    random_direction = np.random.random((mixed_4a_depth))\n    objective = objectives.direction(\n        ""mixed4a_pre_relu"", random_direction, cossim_pow=cossim_pow\n    )\n    assert_gradient_ascent(objective, inceptionv1)\n\n\ndef test_direction_neuron(inceptionv1):\n    mixed_4a_depth = 508\n    random_direction = np.random.random([mixed_4a_depth])\n    objective = objectives.direction_neuron(""mixed4a_pre_relu"", random_direction)\n    assert_gradient_ascent(objective, inceptionv1)\n\n\ndef test_direction_cossim(inceptionv1):\n    mixed_4a_depth = 508\n    random_direction = np.random.random([mixed_4a_depth]).astype(np.float32)\n    objective = objectives.direction_cossim(""mixed4a_pre_relu"", random_direction)\n    assert_gradient_ascent(objective, inceptionv1)\n\ndef test_tensor_neuron(inceptionv1):\n    mixed_4a_depth = 508\n    random_direction = np.random.random([1,3,3,mixed_4a_depth])\n    objective = objectives.tensor_direction(""mixed4a_pre_relu"", random_direction)\n    assert_gradient_ascent(objective, inceptionv1)\n\n\ndef test_deepdream(inceptionv1):\n    objective = objectives.deepdream(""mixed4a_pre_relu"")\n    assert_gradient_ascent(objective, inceptionv1)\n\n\ndef test_tv(inceptionv1):\n    objective = objectives.total_variation(""mixed4a_pre_relu"")\n    assert_gradient_ascent(objective, inceptionv1)\n\n\ndef test_L1(inceptionv1):\n    objective = objectives.L1()  # on input by default\n    assert_gradient_ascent(objective, inceptionv1)\n\n\ndef test_L2(inceptionv1):\n    objective = objectives.L2()  # on input by default\n    assert_gradient_ascent(objective, inceptionv1)\n\n\ndef test_blur_input_each_step(inceptionv1):\n    objective = objectives.blur_input_each_step()\n    assert_gradient_ascent(objective, inceptionv1)\n\n\n# TODO: add test_blur_alpha_each_step\n# def test_blur_alpha_each_step(inceptionv1):\n#     objective = objectives.blur_alpha_each_step()\n#     assert_gradient_ascent(objective, inceptionv1, alpha=True)\n\n\ndef test_channel_interpolate(inceptionv1):\n    # TODO: should channel_interpolate fail early if batch is available?\n    objective = objectives.channel_interpolate(\n        ""mixed4a_pre_relu"", 0, ""mixed4a_pre_relu"", 42\n    )\n    assert_gradient_ascent(objective, inceptionv1, batch=5)\n\n\ndef test_penalize_boundary_complexity(inceptionv1):\n    # TODO: is input shape really unknown at evaluation time?\n    # TODO: is the sign correctly defined on this objective? It seems I need to invert it.\n    objective = objectives.penalize_boundary_complexity([1, 32, 32, 3])\n    assert_gradient_ascent(-1 * objective, inceptionv1)\n\n\ndef test_alignment(inceptionv1):\n    # TODO: is the sign correctly defined on this objective? It seems I need to invert it.\n    objective = objectives.alignment(""mixed4a_pre_relu"")\n    assert_gradient_ascent(-1 * objective, inceptionv1, batch=2)\n\n\ndef test_diversity(inceptionv1):\n    # TODO: is the sign correctly defined on this objective? It seems I need to invert it.\n    objective = objectives.diversity(""mixed4a_pre_relu"")\n    assert_gradient_ascent(-1 * objective, inceptionv1, batch=2)\n\n\ndef test_input_diff(inceptionv1):\n    random_image = np.random.random([1, 32, 32, 3])\n    objective = objectives.input_diff(random_image)\n    assert_gradient_ascent(-1 * objective, inceptionv1, batch=2)\n\n@pytest.mark.xfail(reason=""Unknown cause of failures; seems find in colab."")\ndef test_class_logit(inceptionv1):\n    objective = objectives.class_logit(""softmax1"", ""kit fox"")\n    assert_gradient_ascent(objective, inceptionv1, shape=[1, 224, 224, 3])\n'"
tests/recipes/activation_atlas.py,0,"b'import pytest\n\nfrom lucid.modelzoo.aligned_activations import NUMBER_OF_AVAILABLE_SAMPLES\nfrom lucid.modelzoo.vision_models import AlexNet, InceptionV1\nfrom lucid.recipes.activation_atlas import activation_atlas, aligned_activation_atlas\nfrom lucid.misc.io import save\n\n# Run test with just 1/10th of available samples\nsubset = NUMBER_OF_AVAILABLE_SAMPLES // 10\n\n\n@pytest.mark.skip(reason=""takes too long to complete on CI"")\ndef test_activation_atlas():\n    model = AlexNet()\n    layer = model.layers[1]\n    atlas = activation_atlas(model, layer, number_activations=subset)\n    save(atlas, ""tests/recipes/results/activation_atlas/atlas.jpg"")\n\n\n@pytest.mark.skip(reason=""takes too long to complete on CI"")\ndef test_aligned_activation_atlas():\n    model1 = AlexNet()\n    layer1 = model1.layers[1]\n\n    model2 = InceptionV1()\n    layer2 = model2.layers[8]  # mixed4d\n\n    atlasses = aligned_activation_atlas(\n        model1, layer1, model2, layer2, number_activations=subset\n    )\n    path = ""tests/recipes/results/activation_atlas/aligned_atlas-{}-of-{}.jpg"".format(index, len(atlasses))\n    for index, atlas in enumerate(atlasses):\n        save(atlas, path)\n'"
tests/recipes/test_multi_interpolation_basis.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport numpy as np\nfrom lucid.recipes.image_interpolation_params import multi_interpolation_basis\n\n\ndef test_multi_interpolation_basis():\n  basis = multi_interpolation_basis()\n  assert basis is not None\n\n# TODO: build a larger test that actually checks that when you put the resulting\n# visualizations into the objectives that their values actually increase and\n# respectively decrease over the 6 or so images that this is interpolating over.\n'"
lucid/misc/gl/__init__.py,0,b''
lucid/misc/gl/glcontext.py,0,"b'""""""Headless GPU-accelerated OpenGL context creation on Google Colaboratory.\n\nTypical usage:\n\n    # Optional PyOpenGL configuratiopn can be done here.\n    # import OpenGL\n    # OpenGL.ERROR_CHECKING = True\n\n    # \'glcontext\' must be imported before any OpenGL.* API.\n    from lucid.misc.gl.glcontext import create_opengl_context\n\n    # Now it\'s safe to import OpenGL and EGL functions\n    import OpenGL.GL as gl\n\n    # create_opengl_context() creates a GL context that is attached to an\n    # offscreen surface of the specified size. Note that rendering to buffers\n    # of other sizes and formats is still possible with OpenGL Framebuffers.\n    #\n    # Users are expected to directly use the EGL API in case more advanced\n    # context management is required.\n    width, height = 640, 480\n    create_opengl_context((width, height))\n\n    # OpenGL context is available here.\n\n""""""\n\nfrom __future__ import print_function\n\n# pylint: disable=unused-import,g-import-not-at-top,g-statement-before-imports\n\ntry:\n  import OpenGL\nexcept:\n  print(\'This module depends on PyOpenGL.\')\n  print(\'Please run ""\\033[1m!pip install -q pyopengl\\033[0m"" \'\n        \'prior importing this module.\')\n  raise\n\nimport ctypes\nfrom ctypes import pointer\nimport os\n\nos.environ[\'PYOPENGL_PLATFORM\'] = \'egl\'\n\n# OpenGL loading workaround.\n#\n# * PyOpenGL tries to load libGL, but we need libOpenGL, see [1,2].\n#   This could have been solved by a symlink libGL->libOpenGL, but:\n#\n# * Python 2.7 can\'t find libGL and linEGL due to a bug (see [3])\n#   in ctypes.util, that was only wixed in Python 3.6.\n#\n# So, the only solution I\'ve found is to monkeypatch ctypes.util\n# [1] https://devblogs.nvidia.com/egl-eye-opengl-visualization-without-x-server/\n# [2] https://devblogs.nvidia.com/linking-opengl-server-side-rendering/\n# [3] https://bugs.python.org/issue9998\n_find_library_old = ctypes.util.find_library\ntry:\n\n  def _find_library_new(name):\n    return {\n        \'GL\': \'libOpenGL.so\',\n        \'EGL\': \'libEGL.so\',\n    }.get(name, _find_library_old(name))\n  ctypes.util.find_library = _find_library_new\n  import OpenGL.GL as gl\n  import OpenGL.EGL as egl\nexcept:\n  print(\'Unable to load OpenGL libraries. \'\n        \'Make sure you use GPU-enabled backend.\')\n  print(\'Press ""Runtime->Change runtime type"" and set \'\n        \'""Hardware accelerator"" to GPU.\')\n  raise\nfinally:\n  ctypes.util.find_library = _find_library_old\n\n\ndef create_opengl_context(surface_size=(640, 480)):\n  """"""Create offscreen OpenGL context and make it current.\n\n  Users are expected to directly use EGL API in case more advanced\n  context management is required.\n\n  Args:\n    surface_size: (width, height), size of the offscreen rendering surface.\n  """"""\n  egl_display = egl.eglGetDisplay(egl.EGL_DEFAULT_DISPLAY)\n\n  major, minor = egl.EGLint(), egl.EGLint()\n  egl.eglInitialize(egl_display, pointer(major), pointer(minor))\n\n  config_attribs = [\n      egl.EGL_SURFACE_TYPE, egl.EGL_PBUFFER_BIT, egl.EGL_BLUE_SIZE, 8,\n      egl.EGL_GREEN_SIZE, 8, egl.EGL_RED_SIZE, 8, egl.EGL_DEPTH_SIZE, 24,\n      egl.EGL_RENDERABLE_TYPE, egl.EGL_OPENGL_BIT, egl.EGL_NONE\n  ]\n  config_attribs = (egl.EGLint * len(config_attribs))(*config_attribs)\n\n  num_configs = egl.EGLint()\n  egl_cfg = egl.EGLConfig()\n  egl.eglChooseConfig(egl_display, config_attribs, pointer(egl_cfg), 1,\n                      pointer(num_configs))\n\n  width, height = surface_size\n  pbuffer_attribs = [\n      egl.EGL_WIDTH,\n      width,\n      egl.EGL_HEIGHT,\n      height,\n      egl.EGL_NONE,\n  ]\n  pbuffer_attribs = (egl.EGLint * len(pbuffer_attribs))(*pbuffer_attribs)\n  egl_surf = egl.eglCreatePbufferSurface(egl_display, egl_cfg, pbuffer_attribs)\n\n  egl.eglBindAPI(egl.EGL_OPENGL_API)\n\n  egl_context = egl.eglCreateContext(egl_display, egl_cfg, egl.EGL_NO_CONTEXT,\n                                     None)\n  egl.eglMakeCurrent(egl_display, egl_surf, egl_surf, egl_context)'"
lucid/misc/gl/glrenderer.py,0,"b'""""""OpenGL Mesh rendering utils.""""""\n\nfrom contextlib import contextmanager\nimport numpy as np\n\nimport OpenGL.GL as gl\n\nfrom .meshutil import perspective\n\n\nclass GLObject(object):\n  def __del__(self):\n    self.release()\n  def __enter__(self):\n    bind_func, const = self._bind\n    bind_func(const, self)\n  def __exit__(self, *args):\n    bind_func, const = self._bind\n    bind_func(const, 0)\n    \nclass FBO(GLObject):\n  _bind = gl.glBindFramebuffer, gl.GL_FRAMEBUFFER\n  def __init__(self):\n    self._as_parameter_ = gl.glGenFramebuffers(1)\n  def release(self):\n    gl.glDeleteFramebuffers(1, [self._as_parameter_])\n    \n\nclass Texture(GLObject):\n  _bind = gl.glBindTexture, gl.GL_TEXTURE_2D\n  def __init__(self):\n    self._as_parameter_ = gl.glGenTextures(1)\n  def release(self):\n    gl.glDeleteTextures([self._as_parameter_])\n    \n\nclass Shader(GLObject):\n\n  def __init__(self, vp_code, fp_code):\n    # Importing here, when gl context is already present.\n    # Otherwise get expection on Python3 because of PyOpenGL bug.\n    from OpenGL.GL import shaders\n    self._as_parameter_ = self._shader = shaders.compileProgram(\n        shaders.compileShader( vp_code, gl.GL_VERTEX_SHADER ),\n        shaders.compileShader( fp_code, gl.GL_FRAGMENT_SHADER )\n    )\n    self._uniforms = {}\n    \n  def release(self):\n    gl.glDeleteProgram(self._as_parameter_)\n  \n  def __getitem__(self, uniform_name):\n    if uniform_name not in self._uniforms:\n      self._uniforms[uniform_name] = gl.glGetUniformLocation(self, uniform_name)\n    return self._uniforms[uniform_name]\n\n  def __enter__(self):\n    return self._shader.__enter__()\n  def __exit__(self, *args):\n    return self._shader.__exit__(*args)\n      \n    \nclass MeshRenderer(object):\n  def __init__(self, size):\n    self.size = size\n    self.fbo = FBO()\n    self.color_tex = Texture()\n    self.depth_tex = Texture()\n    w, h = size\n    \n    with self.color_tex:\n      gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_RGBA32F, w, h, 0,\n                      gl.GL_RGBA, gl.GL_FLOAT, None)\n      \n    with self.depth_tex:\n      gl.glTexImage2D.wrappedOperation(\n          gl.GL_TEXTURE_2D, 0,gl.GL_DEPTH24_STENCIL8, w, h, 0,\n          gl.GL_DEPTH_STENCIL, gl.GL_UNSIGNED_INT_24_8, None)\n    \n    with self.fbo:\n      gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_COLOR_ATTACHMENT0,\n                             gl.GL_TEXTURE_2D, self.color_tex, 0)\n      gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_DEPTH_STENCIL_ATTACHMENT,\n                             gl.GL_TEXTURE_2D, self.depth_tex, 0)\n      gl.glViewport(0, 0, w, h)\n      assert gl.glCheckFramebufferStatus(gl.GL_FRAMEBUFFER) == gl.GL_FRAMEBUFFER_COMPLETE\n        \n    self.shader = Shader(vp_code=\'\'\'\n      #version 130\n      uniform mat4 MVP;\n      in vec4 data;\n      out vec4 aData;\n\n      void main() {\n        aData = data;\n        gl_Position = MVP * gl_Vertex;\n      }\n    \'\'\', \n    fp_code=\'\'\'\n      #version 130\n      in vec4 aData;\n      out vec4 fragColor;\n      void main() {\n        fragColor = aData;\n      }\n    \'\'\')\n    \n    self.fovy = 10.0\n    self.aspect = 1.0*w/h\n    self.znear, self.zfar = 0.01, 100.0\n    \n  @contextmanager\n  def _bind_attrib(self, i, arr):\n    if arr is None:\n      yield\n      return\n    arr = np.ascontiguousarray(arr, np.float32)\n    coord_n = arr.shape[-1]\n    gl.glEnableVertexAttribArray(i)\n    gl.glVertexAttribPointer(i, coord_n, gl.GL_FLOAT, gl.GL_FALSE, 0, arr)\n    yield\n    gl.glDisableVertexAttribArray(i)\n    \n  def proj_matrix(self):\n    return perspective(self.fovy, self.aspect, self.znear, self.zfar)\n    \n  def render_mesh(self, position, uv, face=None,\n                  clear_color=[0, 0, 0, 0],\n                  modelview=np.eye(4)):\n    MVP = modelview.T.dot(self.proj_matrix())\n    MVP = np.ascontiguousarray(MVP, np.float32)\n    position = np.ascontiguousarray(position, np.float32)\n    with self.fbo:\n      gl.glClearColor(*clear_color)\n      gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n      \n      with self.shader, self._bind_attrib(0, position), self._bind_attrib(1, uv):\n        gl.glUniformMatrix4fv(self.shader[\'MVP\'], 1, gl.GL_FALSE, MVP)\n        gl.glEnable(gl.GL_DEPTH_TEST)\n        if face is not None:\n          face = np.ascontiguousarray(face, np.uint32)\n          gl.glDrawElements(gl.GL_TRIANGLES, face.size, gl.GL_UNSIGNED_INT, face)\n        else:\n          vert_n = position.size//position.shape[-1]\n          gl.glDrawArrays(gl.GL_TRIANGLES, 0, vert_n)\n        gl.glDisable(gl.GL_DEPTH_TEST)\n      \n      w, h = self.size\n      frame = gl.glReadPixels(0, 0, w, h, gl.GL_RGBA, gl.GL_FLOAT)\n      frame = frame.reshape(h, w, 4)  # fix PyOpenGL bug\n      frame = frame[::-1]  # verical flip to match GL convention\n      return frame'"
lucid/misc/gl/meshutil.py,0,"b'""""""3D mesh manipulation utilities.""""""\n\nfrom builtins import str\nfrom collections import OrderedDict\nimport numpy as np\n\n\ndef frustum(left, right, bottom, top, znear, zfar):\n  """"""Create view frustum matrix.""""""\n  assert right != left\n  assert bottom != top\n  assert znear != zfar\n\n  M = np.zeros((4, 4), dtype=np.float32)\n  M[0, 0] = +2.0 * znear / (right - left)\n  M[2, 0] = (right + left) / (right - left)\n  M[1, 1] = +2.0 * znear / (top - bottom)\n  M[3, 1] = (top + bottom) / (top - bottom)\n  M[2, 2] = -(zfar + znear) / (zfar - znear)\n  M[3, 2] = -2.0 * znear * zfar / (zfar - znear)\n  M[2, 3] = -1.0\n  return M\n\n\ndef perspective(fovy, aspect, znear, zfar):\n  """"""Create perspective projection matrix.""""""\n  assert znear != zfar\n  h = np.tan(fovy / 360.0 * np.pi) * znear\n  w = h * aspect\n  return frustum(-w, w, -h, h, znear, zfar)\n\n\ndef anorm(x, axis=None, keepdims=False):\n  """"""Compute L2 norms alogn specified axes.""""""\n  return np.sqrt((x*x).sum(axis=axis, keepdims=keepdims))\n\n\ndef normalize(v, axis=None, eps=1e-10):\n  """"""L2 Normalize along specified axes.""""""\n  return v / max(anorm(v, axis=axis, keepdims=True), eps)\n\n\ndef lookat(eye, target=[0, 0, 0], up=[0, 1, 0]):\n  """"""Generate LookAt modelview matrix.""""""\n  eye = np.float32(eye)\n  forward = normalize(target - eye)\n  side = normalize(np.cross(forward, up))\n  up = np.cross(side, forward)\n  M = np.eye(4, dtype=np.float32)\n  R = M[:3, :3]\n  R[:] = [side, up, -forward]\n  M[:3, 3] = -R.dot(eye)\n  return M\n\n\ndef sample_view(min_dist, max_dist=None):\n  \'\'\'Sample random camera position.\n  \n  Sample origin directed camera position in given distance\n  range from the origin. ModelView matrix is returned.\n  \'\'\'\n  if max_dist is None:\n    max_dist = min_dist\n  dist = np.random.uniform(min_dist, max_dist)\n  eye = np.random.normal(size=3)\n  eye = normalize(eye)*dist\n  return lookat(eye)\n\n\ndef homotrans(M, p):\n  p = np.asarray(p)\n  if p.shape[-1] == M.shape[1]-1:\n    p = np.append(p, np.ones_like(p[...,:1]), -1)\n  p = np.dot(p, M.T)\n  return p[...,:-1] / p[...,-1:]\n\n\ndef _parse_vertex_tuple(s):\n  """"""Parse vertex indices in \'/\' separated form (like \'i/j/k\', \'i//k\' ...).""""""\n  vt = [0, 0, 0]\n  for i, c in enumerate(s.split(\'/\')):\n    if c:\n      vt[i] = int(c)\n  return tuple(vt)\n\n\ndef _unify_rows(a):\n  """"""Unify lengths of each row of a.""""""\n  lens = np.fromiter(map(len, a), np.int32)\n  if not (lens[0] == lens).all():\n    out = np.zeros((len(a), lens.max()), np.float32)\n    for i, row in enumerate(a):\n      out[i, :lens[i]] = row\n  else:\n    out = np.float32(a)\n  return out\n\n\ndef load_obj(fn):\n  """"""Load 3d mesh form .obj\' file.\n  \n  Args:\n    fn: Input file name or file-like object.\n    \n  Returns:\n    dictionary with the following keys (some of which may be missing):\n      position: np.float32, (n, 3) array, vertex positions\n      uv: np.float32, (n, 2) array, vertex uv coordinates\n      normal: np.float32, (n, 3) array, vertex uv normals\n      face: np.int32, (k*3,) traingular face indices\n  """"""\n  position = [np.zeros(3, dtype=np.float32)]\n  normal = [np.zeros(3, dtype=np.float32)]\n  uv = [np.zeros(2, dtype=np.float32)]\n  \n  tuple2idx = OrderedDict()\n  trinagle_indices = []\n  \n  input_file = open(fn) if isinstance(fn, str) else fn\n  for line in input_file:\n    line = line.strip()\n    if not line or line[0] == \'#\':\n      continue\n    line = line.split(\' \', 1)\n    tag = line[0]\n    if len(line) > 1:\n      line = line[1]\n    else:\n      line = \'\'\n    if tag == \'v\':\n      position.append(np.fromstring(line, sep=\' \'))\n    elif tag == \'vt\':\n      uv.append(np.fromstring(line, sep=\' \'))\n    elif tag == \'vn\':\n      normal.append(np.fromstring(line, sep=\' \'))\n    elif tag == \'f\':\n      output_face_indices = []\n      for chunk in line.split():\n        # tuple order: pos_idx, uv_idx, normal_idx\n        vt = _parse_vertex_tuple(chunk)\n        if vt not in tuple2idx:  # create a new output vertex?\n          tuple2idx[vt] = len(tuple2idx)\n        output_face_indices.append(tuple2idx[vt])\n      # generate face triangles\n      for i in range(1, len(output_face_indices)-1):\n        for vi in [0, i, i+1]:\n          trinagle_indices.append(output_face_indices[vi])\n  \n  outputs = {}\n  outputs[\'face\'] = np.int32(trinagle_indices)\n  pos_idx, uv_idx, normal_idx = np.int32(list(tuple2idx)).T\n  if np.any(pos_idx):\n    outputs[\'position\'] = _unify_rows(position)[pos_idx]\n  if np.any(uv_idx):\n    outputs[\'uv\'] = _unify_rows(uv)[uv_idx]\n  if np.any(normal_idx):\n    outputs[\'normal\'] = _unify_rows(normal)[normal_idx]\n  return outputs\n\n\ndef normalize_mesh(mesh):\n  \'\'\'Scale mesh to fit into -1..1 cube\'\'\'\n  mesh = dict(mesh)\n  pos = mesh[\'position\'][:,:3].copy()\n  pos -= (pos.max(0)+pos.min(0)) / 2.0\n  pos /= np.abs(pos).max()\n  mesh[\'position\'] = pos\n  return mesh\n'"
lucid/misc/graph_analysis/__init__.py,0,b''
lucid/misc/graph_analysis/filter_overlay.py,0,"b'"""""" Simplify `OverlayGraph`s.\n\nOften, we want to extract a simplified version of a TensorFlow graph for\nvisualization and analysis. These simplified graphs can be represented as\nan `OverlayGraph`, but we need a way to simplify and prune it down. That is\nwhat this module provides.\n\nExample use:\n\n```\n  overlay = OverlayGraph(graph)\n\n  # Get rid of most random nodes we aren\'t intersted in.\n  overlay = filter_overlay.ops_whitelist(overlay)\n\n  # Get rid of nodes that aren\'t effected by the input.\n  overlay = filter_overlay.is_dynamic(overlay)\n\n  # Collapse sequences of nodes taht we want to think of as a single layer.\n  overlay = filter_overlay.collapse_sequence(overlay, [""Conv2D"", ""Relu""])\n  overlay = filter_overlay.collapse_sequence(overlay, [""MatMul"", ""Relu""])\n  overlay = filter_overlay.collapse_sequence(overlay, [""MatMul"", ""Softmax""])\n```\n\n""""""\n\n\nstandard_include_ops = [""Placeholder"", ""Relu"", ""Relu6"", ""Add"", ""Split"", ""Softmax"", ""Concat"", ""ConcatV2"", ""Conv2D"", ""MaxPool"", ""AvgPool"", ""MatMul""] # Conv2D\n\ndef ops_whitelist(graph, include_ops=standard_include_ops):\n  keep_nodes = [node.name for node in graph.nodes if node.op in include_ops]\n  return graph.filter(keep_nodes)\n\n\ndef cut_shapes(graph):\n  keep_nodes = [node.name for node in graph.nodes if node.op != ""Shape""]\n  return graph.filter(keep_nodes, pass_through=False)\n\n\ndef is_dynamic(graph):\n\n  dynamic_nodes = []\n\n  def recursive_walk_forward(node):\n    if node.name in dynamic_nodes: return\n    dynamic_nodes.append(node.name)\n    for next in node.consumers:\n      recursive_walk_forward(next)\n\n  recursive_walk_forward(graph.nodes[0])\n  return graph.filter(dynamic_nodes)\n\n\ndef collapse_sequence(graph, sequence):\n  exclude_nodes = []\n\n  for node in graph.nodes:\n    remainder = sequence[:]\n    matches = []\n    while remainder:\n      if node.op == remainder[0]:\n        matches.append(node.name)\n        remainder = remainder[1:]\n      else:\n        break\n\n      if len(remainder):\n        if len(node.consumers) != 1:\n          break\n        node = list(node.consumers)[0]\n\n    if len(remainder) == 0:\n      exclude_nodes += matches[:-1]\n\n  include_nodes = [node.name for node in graph.nodes\n                   if node.name not in exclude_nodes]\n\n  return graph.filter(include_nodes)\n'"
lucid/misc/graph_analysis/overlay_graph.py,3,"b'"""""" Simplified ""overlays"" on top of TensorFlow graphs.\n\nTensorFlow graphs are often too low-level to represent our conceptual\nunderstanding of a model. This module provides abstractions to represent\nsimplified graphs on top of a TensorFlow graph:\n\n`OverlayGraph` - A subgraph of a TensorFlow computational graph. Each node\n    corresponds to a node in the original TensorFlow graph, and edges\n    correspond to paths through the original graph.\n\n`OverlayNode` - A node in an OverlayGraph. Corresponds to a node in a\n    TensorFlow graph.\n\n# Example usage:\n\n```\nwith tf.Graph().as_default() as graph:\n  model = models.InceptionV1()\n  tf.import_graph_def(model.graph_def)\n  overlay = OverlayGraph(graph)\n```\n\n""""""\nfrom collections import defaultdict\nimport numpy as np\nimport tensorflow as tf\n\n\nclass OverlayNode():\n  """"""A node in an OverlayGraph. Corresponds to a TensorFlow Tensor.\n  """"""\n  def __init__(self, name, overlay_graph):\n    self.name = name\n    self.overlay_graph = overlay_graph\n    self.tf_graph = overlay_graph.tf_graph\n    try:\n      self.tf_node = self.tf_graph.get_tensor_by_name(name)\n    except:\n      self.tf_node = None\n    self.sub_structure = None\n\n  @staticmethod\n  def as_name(node):\n    if isinstance(node, str):\n      return node\n    elif isinstance(node, (OverlayNode, tf.Tensor)):\n      return node.name\n    else:\n        raise NotImplementedError\n\n  def __repr__(self):\n    return ""<%s: %s>"" % (self.name, self.op)\n\n  @property\n  def op(self):\n    return self.tf_node.op.type\n\n  @property\n  def inputs(self):\n    return self.overlay_graph.node_to_inputs[self]\n\n  @property\n  def consumers(self):\n    return self.overlay_graph.node_to_consumers[self]\n\n  @property\n  def extended_inputs(self):\n    return self.overlay_graph.node_to_extended_inputs[self]\n\n  @property\n  def extended_consumers(self):\n    return self.overlay_graph.node_to_extended_consumers[self]\n\n  @property\n  def gcd(self):\n    return self.overlay_graph.gcd(self.inputs)\n\n  @property\n  def lcm(self):\n    return self.overlay_graph.lcm(self.consumers)\n\n\nclass OverlayStructure():\n  """"""Represents a sub-structure of a OverlayGraph.\n\n  Often, we want to find structures within a graph, such as branches and\n  sequences, to assist with graph layout for users.\n\n  An OverlayStructure represents such a structure. It is typically used\n  in conjunction with OverlayGraph.collapse_structures() to parse a graph.\n  """"""\n\n  def __init__(self, structure_type, structure):\n    self.structure_type = structure_type\n    self.structure = structure # A dictionary\n    self.children = sum([component if isinstance(component, (list, tuple)) else [component]\n                       for component in structure.values()], [])\n\n  def __contains__(self, item):\n    return OverlayNode.as_name(item) in [n.name for n in self.children]\n\n\nclass OverlayGraph():\n  """"""A subgraph of a TensorFlow computational graph.\n\n  TensorFlow graphs are often too low-level to represent our conceptual\n  understanding of a model\n\n  OverlayGraph can be used to represent a simplified version of a TensorFlow\n  graph. Each node corresponds to a node in the original TensorFlow graph, and\n  edges correspond to paths through the original graph.\n  """"""\n\n  def __init__(self, tf_graph, nodes=None, no_pass_through=None, prev_overlay=None):\n    self.tf_graph = tf_graph\n\n    if nodes is None:\n      nodes = []\n      for op in tf_graph.get_operations():\n        nodes.extend([out.name for out in op.outputs])\n\n    self.name_map = {name: OverlayNode(name, self) for name in nodes}\n    self.nodes = [self.name_map[name] for name in nodes]\n    self.no_pass_through = [] if no_pass_through is None else no_pass_through\n    self.node_to_consumers = defaultdict(set)\n    self.node_to_inputs = defaultdict(set)\n    self.prev_overlay = prev_overlay\n\n    for node in self.nodes:\n      for inp in self._get_overlay_inputs(node):\n        self.node_to_inputs[node].add(inp)\n        self.node_to_consumers[inp].add(node)\n\n    self.node_to_extended_consumers = defaultdict(set)\n    self.node_to_extended_inputs = defaultdict(set)\n\n    for node in self.nodes:\n      for inp in self.node_to_inputs[node]:\n        self.node_to_extended_inputs[node].add(inp)\n        self.node_to_extended_inputs[node].update(self.node_to_extended_inputs[inp])\n\n    for node in self.nodes[::-1]:\n      for out in self.node_to_consumers[node]:\n        self.node_to_extended_consumers[node].add(out)\n        self.node_to_extended_consumers[node].update(self.node_to_extended_consumers[out])\n\n  def __getitem__(self, index):\n    return self.name_map[OverlayNode.as_name(index)]\n\n  def __contains__(self, item):\n    return OverlayNode.as_name(item) in self.name_map\n\n  def get_tf_node(self, node):\n    name = OverlayNode.as_name(node)\n    return self.tf_graph.get_tensor_by_name(name)\n\n  def _get_overlay_inputs(self, node):\n    if self.prev_overlay:\n      raw_inps = self.prev_overlay[node].inputs\n    else:\n      raw_inps = self.get_tf_node(node).op.inputs\n\n    overlay_inps = []\n    for inp in raw_inps:\n      if inp.name.startswith(\'^\'):  # skip control inputs\n        continue\n      if inp in self:\n        overlay_inps.append(self[inp])\n      elif node.name not in self.no_pass_through:\n        overlay_inps.extend(self._get_overlay_inputs(inp))\n    return overlay_inps\n\n\n  def graphviz(self, groups=None):\n    """"""Print graphviz graph.""""""\n\n    print(""digraph G {"")\n    if groups is not None:\n        for root, group in groups.items():\n          print("""")\n          print((""  subgraph"", ""cluster_%s"" % root.name.replace(""/"", ""_""), ""{""))\n          print((""  label = \\""%s\\"""") % (root.name))\n          for node in group:\n            print((""    \\""%s\\"""") % node.name)\n          print(""  }"")\n    for node in self.nodes:\n      for inp in node.inputs:\n        print(""  "", \'""\' + inp.name + \'""\', "" -> "", \'""\' + (node.name) + \'""\')\n    print(""}"")\n\n  def filter(self, keep_nodes, pass_through=True):\n    keep_nodes = [self[n].name for n in keep_nodes]\n    old_nodes = set(self.name_map.keys())\n    new_nodes = set(keep_nodes)\n    no_pass_through = set(self.no_pass_through)\n\n    if not pass_through:\n      no_pass_through += old_nodes - new_nodes\n\n    keep_nodes = [node for node in self.name_map if node in keep_nodes]\n    new_overlay = OverlayGraph(self.tf_graph, keep_nodes, no_pass_through, prev_overlay=self)\n    for node in new_overlay.nodes:\n      node.sub_structure = self[node].sub_structure\n    return new_overlay\n\n  def gcd(self, branches):\n    """"""Greatest common divisor (ie. input) of several nodes.""""""\n    branches = [self[node] for node in branches]\n    branch_nodes  = [set([node]) | node.extended_inputs for node in branches]\n    branch_shared =  set.intersection(*branch_nodes)\n    return max(branch_shared, key=lambda n: self.nodes.index(n))\n\n  def lcm(self, branches):\n    """"""Lowest common multiplie (ie. consumer) of several nodes.""""""\n    branches = [self[node] for node in branches]\n    branch_nodes  = [set([node]) | node.extended_consumers for node in branches]\n    branch_shared =  set.intersection(*branch_nodes)\n    return min(branch_shared, key=lambda n: self.nodes.index(n))\n\n  def sorted(self, items):\n    return sorted(items, key=lambda n: self.nodes.index(self[n]))\n\n  def collapse_structures(self, structure_map):\n\n    keep_nodes = [node.name for node in self.nodes\n                  if not any(node in structure.children for structure in structure_map.values())\n                    or node in structure_map]\n\n    new_overlay = self.filter(keep_nodes)\n\n    for node in structure_map:\n      new_overlay[node].sub_structure = structure_map[node]\n\n    return new_overlay\n'"
lucid/misc/graph_analysis/parse_overlay.py,0,"b'from lucid.misc.graph_analysis.overlay_graph import OverlayGraph, OverlayNode, OverlayStructure\n\ndef collapse_sequences(overlay):\n  """"""Detect and collapse sequences of nodes in an overlay.""""""\n  sequences = []\n  for node in overlay.nodes:\n    if any([node in seq for seq in sequences]): continue\n    seq = [node]\n    while len(node.consumers) == 1 and len(list(node.consumers)[0].inputs) == 1:\n      node = list(node.consumers)[0]\n      seq.append(node)\n    if len(seq) > 1:\n      sequences.append(seq)\n\n  structure_map = {}\n  for seq in sequences:\n    structure_map[seq[-1]] = OverlayStructure(""Sequence"", {""sequence"": seq})\n\n  return overlay.collapse_structures(structure_map)\n\n\ndef collapse_branches(overlay):\n  """"""Detect and collapse brances of nodes in an overlay.""""""\n  structure_map = {}\n\n  for node in overlay.nodes:\n    if len(node.inputs) <= 1: continue\n    gcd = node.gcd\n    if all(inp == gcd or inp.inputs == set([gcd]) for inp in node.inputs):\n      branches = [inp if inp != gcd else None\n                  for inp in overlay.sorted(node.inputs)]\n      structure_map[node] = OverlayStructure(""HeadBranch"", {""branches"" : branches, ""head"": node})\n\n  for node in overlay.nodes:\n    if len(node.consumers) <= 1: continue\n    if all(len(out.consumers) == 0 for out in node.consumers):\n      branches = overlay.sorted(node.consumers)\n      max_node = overlay.sorted(branches)[-1]\n      structure_map[max_node] = OverlayStructure(""TailBranch"", {""branches"" : branches, ""tail"": node})\n\n  return overlay.collapse_structures(structure_map)\n\n\ndef parse_structure(node):\n  """"""Turn a collapsed node in an OverlayGraph into a heirchaical grpah structure.""""""\n  if node is None:\n    return None\n\n  structure = node.sub_structure\n\n  if structure is None:\n    return node.name\n  elif structure.structure_type == ""Sequence"":\n    return {""Sequence"" : [parse_structure(n) for n in structure.structure[""sequence""]]}\n  elif structure.structure_type == ""HeadBranch"":\n    return {""Sequence"" : [\n        {""Branch"" : [parse_structure(n) for n in structure.structure[""branches""]] },\n        parse_structure(structure.structure[""head""])\n    ]}\n  elif structure.structure_type == ""TailBranch"":\n    return {""Sequence"" : [\n        parse_structure(structure.structure[""tail""]),\n        {""Branch"" : [parse_structure(n) for n in structure.structure[""branches""]] },\n    ]}\n  else:\n    data = {}\n    for k in structure.structure:\n      if isinstance(structure.structure[k], list):\n        data[k] = [parse_structure(n) for n in structure.structure[k]]\n      else:\n        data[k] = parse_structure(structure.structure[k])\n\n    return {structure.structure_type : data}\n\n\ndef flatten_sequences(structure):\n  """"""Flatten nested sequences into a single sequence.""""""\n  if isinstance(structure, str) or structure is None:\n    return structure\n  else:\n    structure = structure.copy()\n    for k in structure:\n      structure[k] = [flatten_sequences(sub) for sub in structure[k]]\n\n  if ""Sequence"" in structure:\n    new_seq = []\n    for sub in structure[""Sequence""]:\n      if isinstance(sub, dict) and ""Sequence"" in sub:\n        new_seq += sub[""Sequence""]\n      else:\n        new_seq.append(sub)\n    structure[""Sequence""] = new_seq\n  return structure\n\n\ndef parse_overlay(overlay):\n  new_overlay = overlay\n  prev_len = len(overlay.nodes)\n\n  collapsers = [collapse_sequences, collapse_branches]\n\n  while True:\n    new_overlay = collapse_branches(collapse_sequences(new_overlay))\n    if not len(new_overlay.nodes) < prev_len:\n      break\n    prev_len = len(new_overlay.nodes)\n\n  if len(new_overlay.nodes) != 1: return None\n\n\n  return flatten_sequences(parse_structure(new_overlay.nodes[0]))\n\n\ndef _namify(arr):\n  return [x.name for x in arr]\n\ndef toplevel_group_data(overlay):\n  pres = overlay.nodes[-1]\n  tops = [pres]\n  while pres.inputs:\n    pres = pres.gcd\n    tops.append(pres)\n  tops = tops[::-1]\n\n  groups = {}\n\n  for top in tops:\n    if top.op in [""Concat"", ""ConcatV2""]:\n      groups[top.name] = {\n          ""immediate"" : _namify(overlay.sorted(top.inputs)),\n          ""all"" : _namify(overlay.sorted(top.extended_inputs - top.gcd.extended_inputs - set([top.gcd]) | set([top]))),\n          ""direction"" : ""backward""\n      }\n    if len(top.consumers) > 1 and all(out.op == ""Split"" for out in top.consumers):\n      groups[top.name] = {\n          ""immediate"" : _namify(overlay.sorted(top.consumers)),\n          ""all"" : _namify(overlay.sorted(top.extended_consumers - top.lcm.extended_consumers - set([top.lcm]) | set([top]))),\n          ""direction"" : ""forward""\n      }\n\n  return groups\n'"
lucid/misc/graph_analysis/property_inference.py,3,"b'""""""Infer properties of TensorFlow nodes.\n""""""\n\nfrom lucid.misc.graph_analysis.overlay_graph import OverlayNode, OverlayGraph\nimport tensorflow as tf\n\ndef as_tensor(t):\n  if isinstance(t, OverlayNode):\n    return t.tf_node\n  elif isinstance(t, tf.Operation):\n    return t.outputs[0]\n  elif isinstance(t, tf.Tensor):\n    return t\n\n\ndef infer_data_format(t, max_depth=20):\n  """"""Infer data_format of a conv net activation.\n\n  Inputs:\n    t: a tf.Tensor, tf.Op, or OverlayNode\n\n  Returns: ""NHWC"", ""NCHW"", or None\n  """"""\n  if str(t.shape) == ""<unknown>"" or len(t.shape) != 4:\n    return None\n\n  next_candidates = [as_tensor(t)]\n\n  for n in range(max_depth): # 5 is random sanity limit on recursion\n    inps = []\n    for t in next_candidates:\n      # Easiest way to find out if a tensor has an attribute seems to be trying\n      try:\n        return t.op.get_attr(""data_format"").decode(""ascii"")\n      except:\n        inps.extend(t.op.inputs)\n    next_candidates = inps\n  return None\n'"
lucid/misc/io/__init__.py,0,"b'from lucid.misc.io.showing import show\nfrom lucid.misc.io.loading import load\nfrom lucid.misc.io.saving import save, CaptureSaveContext, batch_save\nfrom lucid.misc.io.scoping import io_scope, scope_url\n'"
lucid/misc/io/collapse_channels.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Convert an ""image"" wtih n channels into 3 RGB channels.""""""\n\n\nimport math\nimport numpy as np\n\ndef hue_to_rgb(ang, warp=True):\n  """"""Produce an RGB unit vector corresponding to a hue of a given angle.""""""\n  ang = ang - 360*(ang//360)\n  colors = np.asarray([\n      [1,0,0],\n      [1,1,0],\n      [0,1,0],\n      [0,1,1],\n      [0,0,1],\n      [1,0,1],\n  ])\n  colors = colors / np.linalg.norm(colors, axis=1, keepdims=True)\n  R = 360 / len(colors)\n  n = math.floor(ang / R)\n  D = (ang - n*R) / R\n\n  if warp:\n    # warping the angle away from the primary colors (RGB)\n    # helps make equally-spaced angles more visually distinguishable\n    adj = lambda x: math.sin(x * math.pi / 2)\n    if n % 2 == 0:\n      D = adj(D)\n    else:\n      D = 1 - adj(1 - D)\n\n  v = (1-D) * colors[n] + D * colors[(n+1) % len(colors)]\n  return v / np.linalg.norm(v)\n\n\ndef sparse_channels_to_rgb(X):\n  assert (X >= 0).all()\n\n  K = X.shape[-1]\n\n  rgb = 0\n  for i in range(K):\n    ang = 360 * i / K\n    color = hue_to_rgb(ang)\n    color = color[tuple(None for _ in range(len(X.shape)-1))]\n    rgb += X[..., i, None] * color\n\n  rgb += np.ones(X.shape[:-1])[..., None] * (X.sum(-1) - X.max(-1))[..., None]\n  rgb /= 1e-4 + np.linalg.norm(rgb, axis=-1, keepdims=True)\n  rgb *= np.linalg.norm(X, axis=-1, keepdims=True)\n\n  return rgb\n\n\ndef collapse_channels(X):\n\n  if (X < 0).any():\n    X = np.concatenate([np.maximum(0, X), np.maximum(0, -X)], axis=-1)\n  return sparse_channels_to_rgb(X)\n'"
lucid/misc/io/loading.py,1,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Methods for loading arbitrary data from arbitrary sources.\n\nThis module takes a URL, infers its underlying data type and how to locate it,\nloads the data into memory and returns a convenient representation.\n\nThis should support for example PNG images, JSON files, npy files, etc.\n""""""\nimport io\nimport lzma\nimport os\nimport json\nimport logging\nimport pickle\n\nimport numpy as np\nimport PIL.Image\nimport tensorflow as tf\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom google.protobuf.message import DecodeError\n\nfrom lucid.misc.io.reading import read_handle\nfrom lucid.misc.io.scoping import current_io_scopes, set_io_scopes\nfrom lucid.misc.io.saving import nullcontext\n\n# from lucid import modelzoo\n\n\n# create logger with module name, e.g. lucid.misc.io.reading\nlog = logging.getLogger(__name__)\n\n\ndef _load_urls(urls, cache=None, **kwargs):\n    if not urls:\n        return []\n    pages = {}\n    caller_io_scopes = current_io_scopes()\n\n    def _do_load(url):\n        set_io_scopes(caller_io_scopes)\n        return load(url, cache=cache, **kwargs)\n\n    with ThreadPoolExecutor(max_workers=8) as executor:\n        future_to_urls = {\n            executor.submit(_do_load, url): url for url in urls\n        }\n        for future in as_completed(future_to_urls):\n            url = future_to_urls[future]\n            try:\n                pages[url] = future.result()\n            except Exception as exc:\n                pages[url] = exc\n                log.error(""Loading {} generated an exception: {}"".format(url, exc))\n    ordered = [pages[url] for url in urls]\n    return ordered\n\n\ndef _load_npy(handle, **kwargs):\n    """"""Load npy file as numpy array.""""""\n    return np.load(handle, **kwargs)\n\n\ndef _load_img(handle, target_dtype=np.float32, size=None, **kwargs):\n    """"""Load image file as numpy array.""""""\n\n    image_pil = PIL.Image.open(handle, **kwargs)\n\n    # resize the image to the requested size, if one was specified\n    if size is not None:\n        if len(size) > 2:\n            size = size[:2]\n            log.warning(\n                ""`_load_img()` received size: {}, trimming to first two dims!"".format(\n                    size\n                )\n            )\n        image_pil = image_pil.resize(size, resample=PIL.Image.LANCZOS)\n\n    image_array = np.asarray(image_pil)\n\n    # remove alpha channel if it contains no information\n    # if image_array.shape[-1] > 3 and \'A\' not in image_pil.mode:\n    # image_array = image_array[..., :-1]\n\n    image_dtype = image_array.dtype\n    image_max_value = np.iinfo(image_dtype).max  # ...for uint8 that\'s 255, etc.\n\n    # using np.divide should avoid an extra copy compared to doing division first\n    ndimage = np.divide(image_array, image_max_value, dtype=target_dtype)\n\n    rank = len(ndimage.shape)\n    if rank == 3:\n        return ndimage\n    elif rank == 2:\n        return np.repeat(np.expand_dims(ndimage, axis=2), 3, axis=2)\n    else:\n        message = ""Loaded image has more dimensions than expected: {}"".format(rank)\n        raise NotImplementedError(message)\n\n\ndef _load_json(handle, **kwargs):\n    """"""Load json file as python object.""""""\n    return json.load(handle, **kwargs)\n\n\ndef _load_text(handle, split=False, encoding=""utf-8""):\n    """"""Load and decode a string.""""""\n    string = handle.read().decode(encoding)\n    return string.splitlines() if split else string\n\n\ndef _load_graphdef_protobuf(handle, **kwargs):\n    """"""Load GraphDef from a binary proto file.""""""\n    # as_graph_def\n    graph_def = tf.GraphDef.FromString(handle.read())\n\n    # check if this is a lucid-saved model\n    # metadata = modelzoo.util.extract_metadata(graph_def)\n    # if metadata is not None:\n    #   url = handle.name\n    #   return modelzoo.vision_base.Model.load_from_metadata(url, metadata)\n\n    # else return a normal graph_def\n    return graph_def\n\n\ndef _load_pickle(handle, **kwargs):\n  """"""Load a pickled python object.""""""\n  return pickle.load(handle, **kwargs)\n\n\ndef _decompress_xz(handle, **kwargs):\n    if not hasattr(handle, \'seekable\') or not handle.seekable():\n        # this handle is not seekable (gfile currently isn\'t), must load it all into memory to help lzma seek through it\n        handle = io.BytesIO(handle.read())\n    return lzma.LZMAFile(handle, mode=""rb"", format=lzma.FORMAT_XZ)\n\n\nloaders = {\n    "".png"": _load_img,\n    "".jpg"": _load_img,\n    "".jpeg"": _load_img,\n    "".npy"": _load_npy,\n    "".npz"": _load_npy,\n    "".json"": _load_json,\n    "".txt"": _load_text,\n    "".md"": _load_text,\n    "".pb"": _load_graphdef_protobuf,\n}\n\n\nunsafe_loaders = {\n    "".pickle"": _load_pickle,\n    "".pkl"": _load_pickle,\n}\n\n\ndecompressors = {\n    "".xz"": _decompress_xz,\n}\n\n\ndef load(url_or_handle, allow_unsafe_formats=False, cache=None, **kwargs):\n    """"""Load a file.\n\n    File format is inferred from url. File retrieval strategy is inferred from\n    URL. Returned object type is inferred from url extension.\n\n    Args:\n      url_or_handle: a (reachable) URL, or an already open file handle\n      allow_unsafe_formats: set to True to allow saving unsafe formats (eg. pickles)\n\n    Raises:\n      RuntimeError: If file extension or URL is not supported.\n    """"""\n\n    # handle lists of URLs in a performant manner\n    if isinstance(url_or_handle, (list, tuple)):\n        return _load_urls(url_or_handle, cache=cache, **kwargs)\n\n    ext, decompressor_ext = _get_extension(url_or_handle)\n    try:\n        ext = ext.lower()\n        if ext in loaders:\n            loader = loaders[ext]\n        elif ext in unsafe_loaders:\n            if not allow_unsafe_formats:\n                raise ValueError(f""{ext} is considered unsafe, you must explicitly allow its use by passing allow_unsafe_formats=True"")\n            loader = unsafe_loaders[ext]\n        else:\n            raise KeyError(f\'no loader found for {ext}\')\n        decompressor = decompressors[decompressor_ext] if decompressor_ext is not None else nullcontext\n        message = ""Using inferred loader \'%s\' due to passed file extension \'%s\'.""\n        log.debug(message, loader.__name__[6:], ext)\n        return load_using_loader(url_or_handle, decompressor, loader, cache, **kwargs)\n    except KeyError:\n        log.warning(""Unknown extension \'%s\', attempting to load as image."", ext)\n        try:\n            with read_handle(url_or_handle, cache=cache) as handle:\n                result = _load_img(handle)\n        except Exception as e:\n            message = ""Could not load resource %s as image. Supported extensions: %s""\n            log.error(message, url_or_handle, list(loaders))\n            raise RuntimeError(message.format(url_or_handle, list(loaders)))\n        else:\n            log.info(""Unknown extension \'%s\' successfully loaded as image."", ext)\n            return result\n\n\n# Helpers\n\n\ndef load_using_loader(url_or_handle, decompressor, loader, cache, **kwargs):\n    if is_handle(url_or_handle):\n        with decompressor(url_or_handle) as decompressor_handle:\n            result = loader(decompressor_handle, **kwargs)\n    else:\n        url = url_or_handle\n        try:\n            with read_handle(url, cache=cache) as handle:\n                with decompressor(handle) as decompressor_handle:\n                    result = loader(decompressor_handle, **kwargs)\n        except (DecodeError, ValueError):\n            log.warning(\n                ""While loading \'%s\' an error occurred. Purging cache once and trying again; if this fails we will raise an Exception! Current io scopes: %r"",\n                url,\n                current_io_scopes(),\n            )\n            # since this may have been cached, it\'s our responsibility to try again once\n            # since we use a handle here, the next DecodeError should propagate upwards\n            with read_handle(url, cache=""purge"") as handle:\n                result = load_using_loader(handle, decompressor, loader, cache, **kwargs)\n    return result\n\n\ndef is_handle(url_or_handle):\n    return hasattr(url_or_handle, ""read"") and hasattr(url_or_handle, ""name"")\n\n\ndef _get_extension(url_or_handle):\n    compression_ext = None\n    if is_handle(url_or_handle):\n        path_without_ext, ext = os.path.splitext(url_or_handle.name)\n    else:\n        path_without_ext, ext = os.path.splitext(url_or_handle)\n\n    if ext in decompressors:\n        decompressor_ext = ext\n        _, ext = os.path.splitext(path_without_ext)\n    else:\n        decompressor_ext = None\n    if not ext:\n        raise RuntimeError(""No extension in URL: "" + url_or_handle)\n    return ext, decompressor_ext\n\n'"
lucid/misc/io/reading.py,2,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Methods for read_handle bytes from arbitrary sources.\n\nThis module takes a URL, infers how to locate it,\nloads the data into memory and returns it.\n""""""\nfrom contextlib import contextmanager\nimport hashlib\nimport os\nimport re\nimport logging\nfrom urllib.parse import urlparse\nfrom urllib import request\nfrom tensorflow import gfile\nimport tensorflow as tf\nfrom tempfile import gettempdir\nimport gc\nfrom filelock import FileLock\n\nfrom lucid.misc.io.writing import write_handle\nfrom lucid.misc.io.scoping import scope_url\n\n\n# create logger with module name, e.g. lucid.misc.io.reading\nlog = logging.getLogger(__name__)\n\n\n# Public functions\n\n\ndef read(url, encoding=None, cache=None, mode=""rb""):\n    """"""Read from any URL.\n\n    Internally differentiates between URLs supported by tf.gfile, such as URLs\n    with the Google Cloud Storage scheme (\'gs://...\') or local paths, and HTTP\n    URLs. This way users don\'t need to know about the underlying fetch mechanism.\n\n    Args:\n        url: a URL including scheme or a local path\n        mode: mode in which to open the file. defaults to binary (\'rb\')\n        encoding: if specified, encoding that should be used to decode read data\n          if mode is specified to be text (\'r\'), this defaults to \'utf-8\'.\n        cache: whether to attempt caching the resource. Defaults to True only if\n          the given URL specifies a remote resource.\n    Returns:\n        All bytes form the specified resource, or a decoded string of those.\n    """"""\n    with read_handle(url, cache, mode=mode) as handle:\n        data = handle.read()\n\n    if encoding:\n        data = data.decode(encoding)\n\n    return data\n\n\n@contextmanager\ndef read_handle(url, cache=None, mode=""rb""):\n    """"""Read from any URL with a file handle.\n\n    Use this to get a handle to a file rather than eagerly load the data:\n\n    ```\n    with read_handle(url) as handle:\n    result = something.load(handle)\n\n    result.do_something()\n\n    ```\n\n    When program execution leaves this `with` block, the handle will be closed\n    automatically.\n\n    Args:\n        url: a URL including scheme or a local path\n    Returns:\n        A file handle to the specified resource if it could be reached.\n        The handle will be closed automatically once execution leaves this context.\n    """"""\n    url = scope_url(url)\n\n    scheme = urlparse(url).scheme\n\n    if cache == ""purge"":\n        _purge_cached(url)\n        cache = None\n\n    if _is_remote(scheme) and cache is None:\n        cache = True\n        log.debug(""Cache not specified, enabling because resource is remote."")\n\n    if cache:\n        handle = _read_and_cache(url, mode=mode)\n    else:\n        if scheme in (""http"", ""https""):\n            handle = _handle_web_url(url, mode=mode)\n        elif scheme in (""gs""):\n            handle = _handle_gfile(url, mode=mode)\n        else:\n            handle = open(url, mode=mode)\n\n    yield handle\n    handle.close()\n\n\n# Handlers\n\n\ndef _handle_gfile(url, mode=""rb""):\n    return gfile.Open(url, mode)\n\n\ndef _handle_web_url(url, mode=""r""):\n    return request.urlopen(url)\n\n\n# Helper Functions\n\n\ndef _is_remote(scheme):\n    return scheme in (""http"", ""https"", ""gs"")\n\n\nRESERVED_PATH_CHARS = re.compile(""[^a-zA-Z0-9]"")\nLUCID_CACHE_DIR_NAME = \'lucid_cache\'\nMAX_FILENAME_LENGTH = 200\n_LUCID_CACHE_DIR = None  # filled on first use\n\n\ndef local_cache_path(remote_url):\n    global _LUCID_CACHE_DIR\n    """"""Returns the path that remote_url would be cached at locally.""""""\n    local_name = RESERVED_PATH_CHARS.sub(""_"", remote_url)\n    if len(local_name) > MAX_FILENAME_LENGTH:\n        filename_hash = hashlib.sha256(local_name.encode(\'utf-8\')).hexdigest()\n        truncated_name = local_name[:(MAX_FILENAME_LENGTH-(len(filename_hash)) - 1)] + \'-\' + filename_hash\n        log.debug(f\'truncated long cache filename to {truncated_name} (original {len(local_name)} char name: {local_name}\')\n        local_name = truncated_name\n    if _LUCID_CACHE_DIR is None:\n        _LUCID_CACHE_DIR = os.path.join(gettempdir(), LUCID_CACHE_DIR_NAME)\n        if not os.path.exists(_LUCID_CACHE_DIR):\n            # folder might exist if another thread/process creates it concurrently, this would be ok\n            os.makedirs(_LUCID_CACHE_DIR, exist_ok=True)\n            log.info(f\'created lucid cache dir at {_LUCID_CACHE_DIR}\')\n    return os.path.join(_LUCID_CACHE_DIR, local_name)\n\n\ndef _purge_cached(url):\n    local_path = local_cache_path(url)\n    if not os.path.exists(local_path):\n        return  # avoids obtaining lock if no work to do anyway\n    lock = FileLock(local_path + "".lockfile"")\n    with lock:\n        try:\n            os.remove(local_path)\n        except OSError:\n            pass\n\n\ndef _read_and_cache(url, mode=""rb""):\n    local_path = local_cache_path(url)\n    lock = FileLock(local_path + "".lockfile"")\n    with lock:\n        if os.path.exists(local_path):\n            log.debug(""Found cached file \'%s\'."", local_path)\n            return _handle_gfile(local_path)\n        log.debug(""Caching URL \'%s\' locally at \'%s\'."", url, local_path)\n        try:\n            with write_handle(local_path, ""wb"") as output_handle, read_handle(\n                url, cache=False, mode=""rb""\n            ) as input_handle:\n                for chunk in _file_chunk_iterator(input_handle):\n                    output_handle.write(chunk)\n            gc.collect()\n            return _handle_gfile(local_path, mode=mode)\n        except tf.errors.NotFoundError:\n            raise\n        except Exception as e:  # bare except to catch things like SystemExit or KeyboardInterrupt\n            log.warning(""Caching (%s -> %s) failed: %s"", url, local_path, e)\n            try:\n                os.remove(local_path)\n            except OSError:\n                pass\n            raise\n\n\nfrom functools import partial\n_READ_BUFFER_SIZE = 1048576     # setting a larger value here to help read bigger chunks of files over the network (eg from GCS)\n\n\ndef _file_chunk_iterator(file_handle):\n    reader = partial(file_handle.read, _READ_BUFFER_SIZE)\n    file_iterator = iter(reader, bytes())\n    # TODO: once dropping Python <3.3 compat, update to `yield from ...`\n    for chunk in file_iterator:\n        yield chunk\n'"
lucid/misc/io/sanitizing.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Sanitize strings that you want to use in filenames.\nReplaces, for example, ""/"" with underscores.\n""""""\n\ndef sanitize(string):\n    return string.replace(""/"", ""_"")\n'"
lucid/misc/io/saving.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Method for saving arbitrary data to arbitrary destinations.\n\nThis module takes an object and URL, infers how to serialize and how to write\nit out to the destination. The intention is to preserve your work under most\ncircumstances, so sometimes this will convert values by default and warn rather than\nerror out immediately. This sometimes means less predictable behavior.\n\nIf an object could have multiple serializations, this tries to infer the\nintended serializations from the URL\'s file extension.\n\nPossible extension: if not given a URL this could create one and return it?\n""""""\nimport contextlib\nimport logging\nimport lzma\nimport pickle\nimport subprocess\nimport warnings\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nimport os.path\nimport json\nfrom typing import Optional, List, Tuple\nimport numpy as np\nimport PIL.Image\n\nfrom lucid.misc.io.writing import write_handle\nfrom lucid.misc.io.serialize_array import _normalize_array\nfrom lucid.misc.io.scoping import current_io_scopes, set_io_scopes\n\n\n# create logger with module name, e.g. lucid.misc.io.saving\nlog = logging.getLogger(__name__)\n\n_module_thread_locals = threading.local()\n\n\n# backfill nullcontext for use before Python 3.7\nif hasattr(contextlib, \'nullcontext\') and False:\n    nullcontext = contextlib.nullcontext\nelse:\n    @contextlib.contextmanager\n    def nullcontext(enter_result=None):\n        yield enter_result\n\n\nclass CaptureSaveContext:\n    """"""Keeps captured save results.\n    Usage:\n    save_context = CaptureSaveContext()\n    with save_context:\n        ...\n    captured_results = save_context.captured_saves\n    """"""\n\n    def __init__(self):\n        self.captured_saves = []\n\n    def __enter__(self):\n        if getattr(_module_thread_locals, \'save_contexts\', None) is None:\n            _module_thread_locals.save_contexts = []\n        _module_thread_locals.save_contexts.append(self)\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        _module_thread_locals.save_contexts.pop()\n\n    def capture(self, save_result):\n        if save_result is not None:\n            self.captured_saves.append(save_result)\n\n    @classmethod\n    def current_save_context(cls) -> Optional[\'CaptureSaveContext\']:\n        contexts = getattr(_module_thread_locals, \'save_contexts\', None)\n        return contexts[-1] if contexts else None\n\n\nclass ClarityJSONEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, (tuple, set)):\n            return list(obj)\n        elif isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif hasattr(obj, ""to_json""):\n            return obj.to_json()\n        else:\n            return super(ClarityJSONEncoder, self).default(obj)\n\n\ndef save_json(object, handle, indent=2):\n    """"""Save object as json on CNS.""""""\n    obj_json = json.dumps(object, indent=indent, cls=ClarityJSONEncoder)\n    handle.write(obj_json)\n\n    return {""type"": ""json"", ""url"": handle.name}\n\n\ndef save_npy(object, handle):\n    """"""Save numpy array as npy file.""""""\n    np.save(handle, object)\n\n    return {""type"": ""npy"", ""shape"": object.shape, ""dtype"": str(object.dtype), ""url"": handle.name}\n\n\ndef save_npz(object, handle):\n    """"""Save dict of numpy array as npz file.""""""\n    # there is a bug where savez doesn\'t actually accept a file handle.\n    log.warning(""Saving npz files currently only works locally. :/"")\n    path = handle.name\n    handle.close()\n    if type(object) is dict:\n        np.savez(path, **object)\n    elif type(object) is list:\n        np.savez(path, *object)\n    else:\n        log.warning(""Saving non dict or list as npz file, did you maybe want npy?"")\n        np.savez(path, object)\n    return {""type"": ""npz"", ""url"": path}\n\n\ndef save_img(object, handle, domain=None, **kwargs):\n    """"""Save numpy array as image file on CNS.""""""\n\n    if isinstance(object, np.ndarray):\n        normalized = _normalize_array(object, domain=domain)\n        object = PIL.Image.fromarray(normalized)\n\n    if isinstance(object, PIL.Image.Image):\n        object.save(handle, **kwargs)  # will infer format from handle\'s url ext.\n    else:\n        raise ValueError(""Can only save_img for numpy arrays or PIL.Images!"")\n\n    return {\n        ""type"": ""image"",\n        ""shape"": object.size + (len(object.getbands()),),\n        ""url"": handle.name,\n    }\n\n\ndef save_txt(object, handle, **kwargs):\n    if isinstance(object, str):\n        handle.write(object)\n    elif isinstance(object, list):\n        for line in object:\n            if isinstance(line, str):\n                line = line.encode()\n            if not isinstance(line, bytes):\n                line_type = type(line)\n                line = repr(line).encode()\n                warnings.warn(\n                    ""`save_txt` found an object of type {}; using `repr` to convert it to string."".format(\n                        line_type\n                    )\n                )\n            if not line.endswith(b""\\n""):\n                line += b""\\n""\n            handle.write(line)\n\n    return {""type"": ""txt"", ""url"": handle.name}\n\n\ndef save_str(object, handle, **kwargs):\n    assert isinstance(object, str)\n    handle.write(object)\n    return {""type"": ""txt"", ""url"": handle.name}\n\n\ndef save_pb(object, handle, **kwargs):\n    try:\n        handle.write(object.SerializeToString())\n    except AttributeError:\n        warnings.warn(\n            ""`save_protobuf` failed for object {}. Re-raising original exception."".format(\n                object\n            )\n        )\n        raise\n    finally:\n        return {""type"": ""pb"", ""url"": handle.name}\n\n\ndef save_pickle(object, handle, **kwargs):\n  try:\n    pickle.dump(object, handle)\n  except AttributeError as e:\n    warnings.warn(""`save_pickle` failed for object {}. Re-raising original exception."".format(object))\n    raise e\n\n\ndef compress_xz(handle, **kwargs):\n    try:\n        ret = lzma.LZMAFile(handle, format=lzma.FORMAT_XZ, mode=""wb"")\n        ret.name = handle.name\n        return ret\n    except AttributeError as e:\n        warnings.warn(""`compress_xz` failed for handle {}. Re-raising original exception."".format(handle))\n        raise e\n\n\nsavers = {\n    "".png"": save_img,\n    "".jpg"": save_img,\n    "".jpeg"": save_img,\n    "".webp"": save_img,\n    "".npy"": save_npy,\n    "".npz"": save_npz,\n    "".json"": save_json,\n    "".txt"": save_txt,\n    "".pb"": save_pb,\n}\n\nunsafe_savers = {\n    "".pickle"": save_pickle,\n    "".pkl"": save_pickle,\n}\n\ncompressors = {\n    "".xz"": compress_xz,\n}\n\n\ndef save(thing, url_or_handle, allow_unsafe_formats=False, save_context: Optional[CaptureSaveContext] = None, **kwargs):\n    """"""Save object to file on CNS.\n\n    File format is inferred from path. Use save_img(), save_npy(), or save_json()\n    if you need to force a particular format.\n\n    Args:\n      obj: object to save.\n      path: CNS path.\n      allow_unsafe_formats: set to True to allow saving unsafe formats (eg. pickles)\n      save_context: a context into which to capture saves, otherwise will try to use global context\n\n    Raises:\n      RuntimeError: If file extension not supported.\n    """"""\n\n    # Determine context\n    # Is this a handle? What is the extension? Are we saving to GCS?\n    is_handle = hasattr(url_or_handle, ""write"") and hasattr(url_or_handle, ""name"")\n    if is_handle:\n        path = url_or_handle.name\n    else:\n        path = url_or_handle\n\n    path_without_ext, ext = os.path.splitext(path)\n    is_gcs = path.startswith(""gs://"")\n\n    if ext in compressors:\n        compressor = compressors[ext]\n        _, ext = os.path.splitext(path_without_ext)\n    else:\n        compressor = nullcontext\n\n    if not ext:\n        raise RuntimeError(""No extension in URL: "" + path)\n\n    # Determine which saver should be used\n    if ext in savers:\n        saver = savers[ext]\n    elif ext in unsafe_savers:\n        if not allow_unsafe_formats:\n            raise ValueError(f""{ext} is considered unsafe, you must explicitly allow its use by passing allow_unsafe_formats=True"")\n        saver = unsafe_savers[ext]\n    elif isinstance(thing, str):\n        saver = save_str\n    else:\n        message = ""Unknown extension \'{}\'. As a result, only strings can be saved, not {}. Supported extensions: {}""\n        raise ValueError(message.format(ext, type(thing).__name__, list(savers.keys())))\n\n    # Actually save\n    if is_handle:\n        handle_provider = nullcontext\n    else:\n        handle_provider = write_handle\n\n    with handle_provider(url_or_handle) as handle:\n        with compressor(handle) as compressed_handle:\n            result = saver(thing, compressed_handle, **kwargs)\n\n    # Set mime type on gcs if html -- usually, when one saves an html to GCS,\n    # they want it to be viewsable as a website.\n    if is_gcs and ext == "".html"":\n        subprocess.run(\n            [""gsutil"", ""setmeta"", ""-h"", ""Content-Type: text/html; charset=utf-8"", path]\n        )\n    if is_gcs and ext == "".json"":\n        subprocess.run(\n            [""gsutil"", ""setmeta"", ""-h"", ""Content-Type: application/json"", path]\n        )\n\n    # capture save if a save context is available\n    save_context = save_context if save_context is not None else CaptureSaveContext.current_save_context()\n    if save_context:\n        log.debug(\n            ""capturing save: resulted in {} -> {} in save_context {}"".format(\n                result, path, save_context\n            )\n        )\n        save_context.capture(result)\n\n    if result is not None and ""url"" in result and result[""url""].startswith(""gs://""):\n        result[""serve""] = ""https://storage.googleapis.com/{}"".format(result[""url""][5:])\n\n    return result\n\n\ndef batch_save(save_ops: List[Tuple], num_workers: int = 16):\n    caller_io_scopes = current_io_scopes()\n    current_save_context = CaptureSaveContext.current_save_context()\n\n    def _do_save(save_op_tuple: Tuple):\n        set_io_scopes(caller_io_scopes)\n        if len(save_op_tuple) == 2:\n            return save(save_op_tuple[0], save_op_tuple[1], save_context=current_save_context)\n        elif len(save_op_tuple) == 3:\n            return save(save_op_tuple[0], save_op_tuple[1], save_context=current_save_context, **(save_op_tuple[2]))\n        else:\n            raise ValueError(f\'unknown save tuple size: {len(save_op_tuple)}\')\n\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        save_op_futures = [executor.submit(_do_save, save_op_tuple) for save_op_tuple in save_ops]\n        return [future.result() for future in save_op_futures]\n'"
lucid/misc/io/scoping.py,0,"b'import os\nfrom copy import copy\nimport threading\nimport sys\nfrom contextlib import contextmanager\n\n_thread_local_scopes = threading.local()\n\n\ndef current_io_scopes():\n    ret = getattr(_thread_local_scopes, ""io_scopes"", None)\n    if ret is None:\n        ret = []\n        _thread_local_scopes.io_scopes = ret\n    return ret\n\n\ndef set_io_scopes(scopes):\n    _thread_local_scopes.io_scopes = scopes\n\n\n@contextmanager\ndef io_scope(path, replace_current_scopes=False):\n    current_scope = current_io_scopes()\n    before = copy(current_scope)\n    if replace_current_scopes:\n        set_io_scopes(path if isinstance(path, list) else [path])\n    else:\n        current_scope.append(path)\n    try:\n        yield\n    finally:\n        set_io_scopes(before)\n\n\ndef _normalize_url(url: str) -> str:\n    # os.path.normpath mangles url schemes: gs://etc -> gs:/etc\n    # urlparse.urljoin doesn\'t normalize paths\n    url_scheme, sep, url_path = url.partition(""://"")\n    normalized_path = os.path.normpath(url_path)\n    return url_scheme + sep + normalized_path\n\n\ndef scope_url(url, io_scopes=None):\n    io_scopes = io_scopes or current_io_scopes()\n    if ""//"" in url or url.startswith(""/""):\n        return url\n    paths = io_scopes + [url]\n    joined = os.path.join(*paths)\n    return _normalize_url(joined)\n'"
lucid/misc/io/serialize_array.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for normalizing arrays and converting them to images.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport base64\nimport logging\nimport numpy as np\nimport PIL.Image\nfrom io import BytesIO\n\n\n# create logger with module name, e.g. lucid.misc.io.array_to_image\nlog = logging.getLogger(__name__)\n\n\ndef _normalize_array(array, domain=(0, 1)):\n  """"""Given an arbitrary rank-3 NumPy array, produce one representing an image.\n\n  This ensures the resulting array has a dtype of uint8 and a domain of 0-255.\n\n  Args:\n    array: NumPy array representing the image\n    domain: expected range of values in array,\n      defaults to (0, 1), if explicitly set to None will use the array\'s\n      own range of values and normalize them.\n\n  Returns:\n    normalized PIL.Image\n  """"""\n  # first copy the input so we\'re never mutating the user\'s data\n  array = np.array(array)\n  # squeeze helps both with batch=1 and B/W and PIL\'s mode inference\n  array = np.squeeze(array)\n  assert len(array.shape) <= 3\n  assert np.issubdtype(array.dtype, np.number)\n  assert not np.isnan(array).any()\n\n  low, high = np.min(array), np.max(array)\n  if domain is None:\n    message = ""No domain specified, normalizing from measured (~%.2f, ~%.2f)""\n    log.debug(message, low, high)\n    domain = (low, high)\n\n  # clip values if domain was specified and array contains values outside of it\n  if low < domain[0] or high > domain[1]:\n    message = ""Clipping domain from (~{:.2f}, ~{:.2f}) to (~{:.2f}, ~{:.2f}).""\n    log.info(message.format(low, high, domain[0], domain[1]))\n    array = array.clip(*domain)\n\n  min_value, max_value = np.iinfo(np.uint8).min, np.iinfo(np.uint8).max  # 0, 255\n  # convert signed to unsigned if needed\n  if np.issubdtype(array.dtype, np.inexact):\n    offset = domain[0]\n    if offset != 0:\n      array -= offset\n      log.debug(""Converting inexact array by subtracting -%.2f."", offset)\n    if domain[0] != domain[1]:\n      scalar = max_value / (domain[1] - domain[0])\n      if scalar != 1:\n        array *= scalar\n        log.debug(""Converting inexact array by scaling by %.2f."", scalar)\n\n  return array.clip(min_value, max_value).astype(np.uint8)\n\n\ndef _serialize_normalized_array(array, fmt=\'png\', quality=70):\n  """"""Given a normalized array, returns byte representation of image encoding.\n\n  Args:\n    array: NumPy array of dtype uint8 and range 0 to 255\n    fmt: string describing desired file format, defaults to \'png\'\n    quality: specifies compression quality from 0 to 100 for lossy formats\n\n  Returns:\n    image data as BytesIO buffer\n  """"""\n  dtype = array.dtype\n  assert np.issubdtype(dtype, np.unsignedinteger)\n  assert np.max(array) <= np.iinfo(dtype).max\n  assert array.shape[-1] > 1  # array dims must have been squeezed\n\n  image = PIL.Image.fromarray(array)\n  image_bytes = BytesIO()\n  image.save(image_bytes, fmt, quality=quality)\n  # TODO: Python 3 could save a copy here by using `getbuffer()` instead.\n  image_data = image_bytes.getvalue()\n  return image_data\n\n\ndef serialize_array(array, domain=(0, 1), fmt=\'png\', quality=70):\n  """"""Given an arbitrary rank-3 NumPy array,\n  returns the byte representation of the encoded image.\n\n  Args:\n    array: NumPy array of dtype uint8 and range 0 to 255\n    domain: expected range of values in array, see `_normalize_array()`\n    fmt: string describing desired file format, defaults to \'png\'\n    quality: specifies compression quality from 0 to 100 for lossy formats\n\n  Returns:\n    image data as BytesIO buffer\n  """"""\n  normalized = _normalize_array(array, domain=domain)\n  return _serialize_normalized_array(normalized, fmt=fmt, quality=quality)\n\n\nJS_ARRAY_TYPES = {\n    \'int8\', \'int16\', \'int32\', \'uint8\', \'uint16\', \'uint32\', \'float32\', \'float64\'\n}\n\n\ndef array_to_jsbuffer(array):\n  """"""Serialize 1d NumPy array to JS TypedArray.\n\n  Data is serialized to base64-encoded string, which is much faster\n  and memory-efficient than json list serialization.\n\n  Args:\n    array: 1d NumPy array, dtype must be one of JS_ARRAY_TYPES.\n\n  Returns:\n    JS code that evaluates to a TypedArray as string.\n\n  Raises:\n    TypeError: if array dtype or shape not supported.\n  """"""\n  if array.ndim != 1:\n    raise TypeError(\'Only 1d arrays can be converted JS TypedArray.\')\n  if array.dtype.name not in JS_ARRAY_TYPES:\n    raise TypeError(\'Array dtype not supported by JS TypedArray.\')\n  js_type_name = array.dtype.name.capitalize() + \'Array\'\n  data_base64 = base64.b64encode(array.tobytes()).decode(\'ascii\')\n  code = """"""\n    (function() {\n      const data = atob(""%s"");\n      const buf = new Uint8Array(data.length);\n      for (var i=0; i<data.length; ++i) {\n        buf[i] = data.charCodeAt(i);\n      }\n      var array_type = %s;\n      if (array_type == Uint8Array) {\n        return buf;\n      }\n      return new array_type(buf.buffer);\n    })()\n  """""" % (data_base64, js_type_name)\n  return code\n'"
lucid/misc/io/showing.py,2,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Methods for displaying images from Numpy arrays.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom io import BytesIO\nimport base64\nimport logging\nimport numpy as np\nimport IPython.display\nfrom string import Template\nimport tensorflow as tf\n\nfrom lucid.misc.io.serialize_array import serialize_array, array_to_jsbuffer\nfrom lucid.misc.io.collapse_channels import collapse_channels\n\n\n# create logger with module name, e.g. lucid.misc.io.showing\nlog = logging.getLogger(__name__)\n\n\ndef _display_html(html_str):\n  IPython.display.display(IPython.display.HTML(html_str))\n\n\ndef _image_url(array, fmt=\'png\', mode=""data"", quality=90, domain=None):\n  """"""Create a data URL representing an image from a PIL.Image.\n\n  Args:\n    image: a numpy array\n    mode: presently only supports ""data"" for data URL\n\n  Returns:\n    URL representing image\n  """"""\n  supported_modes = (""data"")\n  if mode not in supported_modes:\n    message = ""Unsupported mode \'%s\', should be one of \'%s\'.""\n    raise ValueError(message, mode, supported_modes)\n\n  image_data = serialize_array(array, fmt=fmt, quality=quality, domain=domain)\n  base64_byte_string = base64.b64encode(image_data).decode(\'ascii\')\n  return ""data:image/"" + fmt.upper() + "";base64,"" + base64_byte_string\n\n\n# public functions\n\ndef _image_html(array, w=None, domain=None, fmt=\'png\'):\n  url = _image_url(array, domain=domain, fmt=fmt)\n  style = ""image-rendering: pixelated;""\n  if w is not None:\n    style += ""width: {w}px;"".format(w=w)\n  return """"""<img src=""{url}"" style=""{style}"">"""""".format(**locals())\n\ndef image(array, domain=None, w=None, format=\'png\', **kwargs):\n  """"""Display an image.\n\n  Args:\n    array: NumPy array representing the image\n    fmt: Image format e.g. png, jpeg\n    domain: Domain of pixel values, inferred from min & max values if None\n    w: width of output image, scaled using nearest neighbor interpolation.\n      size unchanged if None\n  """"""\n\n  _display_html(\n    _image_html(array, w=w, domain=domain, fmt=format)\n  )\n\n\ndef images(arrays, labels=None, domain=None, w=None):\n  """"""Display a list of images with optional labels.\n\n  Args:\n    arrays: A list of NumPy arrays representing images\n    labels: A list of strings to label each image.\n      Defaults to show index if None\n    domain: Domain of pixel values, inferred from min & max values if None\n    w: width of output image, scaled using nearest neighbor interpolation.\n      size unchanged if None\n  """"""\n\n  s = \'<div style=""display: flex; flex-direction: row;"">\'\n  for i, array in enumerate(arrays):\n    label = labels[i] if labels is not None else i\n    img_html = _image_html(array, w=w, domain=domain)\n    s += """"""<div style=""margin-right:10px; margin-top: 4px;"">\n              {label} <br/>\n              {img_html}\n            </div>"""""".format(**locals())\n  s += ""</div>""\n  _display_html(s)\n\n\ndef show(thing, domain=(0, 1), **kwargs):\n  """"""Display a numpy array without having to specify what it represents.\n\n  This module will attempt to infer how to display your tensor based on its\n  rank, shape and dtype. rank 4 tensors will be displayed as image grids, rank\n  2 and 3 tensors as images.\n\n  For tensors of rank 3 or 4, the innermost dimension is interpreted as channel.\n  Depending on the size of that dimension, different types of images will be\n  generated:\n\n    shp[-1]\n      = 1  --  Black and white image.\n      = 2  --  See >4\n      = 3  --  RGB image.\n      = 4  --  RGBA image.\n      > 4  --  Collapse into an RGB image.\n               If all positive: each dimension gets an evenly spaced hue.\n               If pos and neg: each dimension gets two hues\n                  (180 degrees apart) for positive and negative.\n\n  Common optional arguments:\n\n    domain: range values can be between, for displaying normal images\n      None  = infer domain with heuristics\n      (a,b) = clip values to be between a (min) and b (max).\n\n    w: width of displayed images\n      None  = display 1 pixel per value\n      int   = display n pixels per value (often used for small images)\n\n    labels: if displaying multiple objects, label for each object.\n      None  = label with index\n      []    = no labels\n      [...] = label with corresponding list item\n\n  """"""\n  def collapse_if_needed(arr):\n    K = arr.shape[-1]\n    if K not in [1,3,4]:\n      log.debug(""Collapsing %s channels into 3 RGB channels."" % K)\n      return collapse_channels(arr)\n    else:\n      return arr\n\n\n  if isinstance(thing, np.ndarray):\n    rank = len(thing.shape)\n\n    if rank in [3,4]:\n      thing = collapse_if_needed(thing)\n\n    if rank == 4:\n      log.debug(""Show is assuming rank 4 tensor to be a list of images."")\n      images(thing, domain=domain, **kwargs)\n    elif rank in (2, 3):\n      log.debug(""Show is assuming rank 2 or 3 tensor to be an image."")\n      image(thing, domain=domain, **kwargs)\n    else:\n      log.warning(""Show only supports numpy arrays of rank 2-4. Using repr()."")\n      print(repr(thing))\n  elif isinstance(thing, (list, tuple)):\n    log.debug(""Show is assuming list or tuple to be a collection of images."")\n\n    if isinstance(thing[0], np.ndarray) and len(thing[0].shape) == 3:\n      thing = [collapse_if_needed(t) for t in thing]\n\n    images(thing, domain=domain, **kwargs)\n  else:\n    log.warning(""Show only supports numpy arrays so far. Using repr()."")\n    print(repr(thing))\n\n\ndef textured_mesh(mesh, texture, background=\'0xffffff\'):\n  texture_data_url = _image_url(texture, fmt=\'jpeg\', quality=90)\n\n  code = Template(\'\'\'\n  <input id=""unfoldBox"" type=""checkbox"" class=""control"">Unfold</input>\n  <input id=""shadeBox"" type=""checkbox"" class=""control"">Shade</input>\n\n  <script src=""https://cdn.rawgit.com/mrdoob/three.js/r89/build/three.min.js""></script>\n  <script src=""https://cdn.rawgit.com/mrdoob/three.js/r89/examples/js/controls/OrbitControls.js""></script>\n\n  <script type=""x-shader/x-vertex"" id=""vertexShader"">\n    uniform float viewAspect;\n    uniform float unfolding_perc;\n    uniform float shadeFlag;\n    varying vec2 text_coord;\n    varying float shading;\n    void main () {\n      gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);\n      vec4 plane_position = vec4((uv.x*2.0-1.0)/viewAspect, (uv.y*2.0-1.0), 0, 1);\n      gl_Position = mix(gl_Position, plane_position, unfolding_perc);\n\n      //not normalized on purpose to simulate the rotation\n      shading = 1.0;\n      if (shadeFlag > 0.5) {\n        vec3 light_vector = mix(normalize(cameraPosition-position), normal, unfolding_perc);\n        shading = dot(normal, light_vector);\n      }\n\n      text_coord = uv;\n    }\n  </script>\n\n  <script type=""x-shader/x-fragment"" id=""fragmentShader"">\n    uniform float unfolding_perc;\n    varying vec2  text_coord;\n    varying float shading;\n    uniform sampler2D texture;\n\n    void main() {\n      gl_FragColor = texture2D(texture, text_coord);\n      gl_FragColor.rgb *= shading;\n    }\n  </script>\n\n  <script>\n  ""use strict"";\n\n  const el = id => document.getElementById(id);\n\n  const unfoldDuration = 1000.0;\n  var camera, scene, renderer, controls, material;\n  var unfolded = false;\n  var unfoldStart = -unfoldDuration*10.0;\n\n  init();\n  animate(0.0);\n\n  function init() {\n    var width = 800, height = 600;\n\n    scene = new THREE.Scene();\n\n    camera = new THREE.PerspectiveCamera(42, width / height, 0.1, 100);\n    camera.position.z = 3.3;\n    scene.add(camera);\n\n    controls = new THREE.OrbitControls( camera );\n\n    var geometry = new THREE.BufferGeometry();\n    geometry.addAttribute( \'position\', new THREE.BufferAttribute($verts, 3 ) );\n    geometry.addAttribute( \'uv\', new THREE.BufferAttribute($uvs, 2) );\n    geometry.setIndex(new THREE.BufferAttribute($faces, 1 ));\n    geometry.computeVertexNormals();\n\n    var texture = new THREE.TextureLoader().load(\'$tex_data_url\', update);\n    material = new THREE.ShaderMaterial( {\n      uniforms: {\n        viewAspect: {value: width/height},\n        unfolding_perc: { value: 0.0 },\n        shadeFlag: { value: 0.0 },\n        texture: { type: \'t\', value: texture },\n      },\n      side: THREE.DoubleSide,\n      vertexShader: el( \'vertexShader\' ).textContent,\n      fragmentShader: el( \'fragmentShader\' ).textContent\n    });\n\n    var mesh = new THREE.Mesh(geometry, material);\n    scene.add(mesh);\n    scene.background = new THREE.Color( $background );\n\n    renderer = new THREE.WebGLRenderer({antialias: true});\n    renderer.setSize(width, height);\n    document.body.appendChild(renderer.domElement);\n\n    // render on change only\n    controls.addEventListener(\'change\', function() {\n      // fold mesh back if user wants to interact\n      el(\'unfoldBox\').checked = false;\n      update();\n    });\n    document.querySelectorAll(\'.control\').forEach(e=>{\n      e.addEventListener(\'change\', update);\n    });\n  }\n\n  function update() {\n    requestAnimationFrame(animate);\n  }\n\n  function ease(x) {\n    x = Math.min(Math.max(x, 0.0), 1.0);\n    return x*x*(3.0 - 2.0*x);\n  }\n\n  function animate(time) {\n    var unfoldFlag = el(\'unfoldBox\').checked;\n    if (unfolded != unfoldFlag) {\n      unfolded = unfoldFlag;\n      unfoldStart = time - Math.max(unfoldStart+unfoldDuration-time, 0.0);\n    }\n    var unfoldTime = (time-unfoldStart) / unfoldDuration;\n    if (unfoldTime < 1.0) {\n      update();\n    }\n    var unfoldVal = ease(unfoldTime);\n    unfoldVal = unfolded ? unfoldVal : 1.0 - unfoldVal;\n    material.uniforms.unfolding_perc.value = unfoldVal;\n\n    material.uniforms.shadeFlag.value = el(\'shadeBox\').checked ? 1.0 : 0.0;\n    controls.update();\n    renderer.render(scene, camera);\n  }\n  </script>\n  \'\'\').substitute(\n      verts = array_to_jsbuffer(mesh[\'position\'].ravel()),\n      uvs = array_to_jsbuffer(mesh[\'uv\'].ravel()),\n      faces = array_to_jsbuffer(np.uint32(mesh[\'face\'].ravel())),\n      tex_data_url = texture_data_url,\n      background = background,\n  )\n  _display_html(code)\n\n\ndef _strip_consts(graph_def, max_const_size=32):\n    """"""Strip large constant values from graph_def.\n\n    This is mostly a utility function for graph(), and also originates here:\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\n    """"""\n    strip_def = tf.GraphDef()\n    for n0 in graph_def.node:\n        n = strip_def.node.add()\n        n.MergeFrom(n0)\n        if n.op == \'Const\':\n            tensor = n.attr[\'value\'].tensor\n            size = len(tensor.tensor_content)\n            if size > max_const_size:\n                tensor.tensor_content = tf.compat.as_bytes(""<stripped %d bytes>""%size)\n    return strip_def\n\n\ndef graph(graph_def, max_const_size=32):\n    """"""Visualize a TensorFlow graph.\n\n    This function was originally found in this notebook (also Apache licensed):\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\n    """"""\n    if hasattr(graph_def, \'as_graph_def\'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = _strip_consts(graph_def, max_const_size=max_const_size)\n    code = """"""\n        <script>\n          function load() {{\n            document.getElementById(""{id}"").pbtxt = {data};\n          }}\n        </script>\n        <link rel=""import"" href=""https://tensorboard.appspot.com/tf-graph-basic.build.html"" onload=load()>\n        <div style=""height:600px"">\n          <tf-graph-basic id=""{id}""></tf-graph-basic>\n        </div>\n    """""".format(data=repr(str(strip_def)), id=\'graph\'+str(np.random.rand()))\n\n    iframe = """"""\n        <iframe seamless style=""width:100%; height:620px; border: none;"" srcdoc=""{}""></iframe>\n    """""".format(code.replace(\'""\', \'&quot;\'))\n    _display_html(iframe)\n'"
lucid/misc/io/writing.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Methods for writing bytes to arbitrary destinations.\n\nThis module takes data and a URL, and attempts to save that data at that URL.\n\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport logging\nfrom contextlib import contextmanager\nfrom urllib.parse import urlparse\nfrom tensorflow import gfile\n\nfrom lucid.misc.io.scoping import scope_url\n\nlog = logging.getLogger(__name__)\n\n\ndef _supports_make_dirs(path):\n    """"""Whether this path implies a storage system that supports and requires\n    intermediate directories to be created explicitly.""""""\n    prefixes = [""/bigstore"", ""gs://""]\n    return not any(path.startswith(prefix) for prefix in prefixes)\n\n\ndef _supports_binary_writing(path):\n    """"""Whether this path implies a storage system that supports and requires\n    intermediate directories to be created explicitly.""""""\n    return not path.startswith(""/bigstore"")\n\n\ndef _write_to_path(data, path, mode=""wb""):\n    with write_handle(path, mode) as handle:\n        handle.write(data)\n\n\n# Public functions\n\n\ndef write(data, url, mode=""wb""):\n    if urlparse(url).scheme in (""http"", ""https""):\n        message = ""Writing to remote URL (%s) is not yet supported.""\n        raise ValueError(message, url)\n\n    _write_to_path(data, url, mode=mode)\n\n\n@contextmanager\ndef write_handle(path, mode=None):\n    path = scope_url(path)\n\n    if _supports_make_dirs(path):\n        gfile.MakeDirs(os.path.dirname(path))\n\n    if mode is None:\n        if _supports_binary_writing(path):\n            mode = ""wb""\n        else:\n            mode = ""w""\n\n    handle = gfile.Open(path, mode)\n    yield handle\n    handle.close()\n'"
lucid/modelzoo/caffe_models/AlexNet.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts, IMAGENET_MEAN_BGR\n\n\nclass AlexNet_caffe_Places365(Model):\n  """"""AlexNet re-implementation trained on Places365.\n\n  This model is a reimplementation of AlexNet\n  https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n  trained on the MIT Places365 dataset, retrieved here:\n  https://github.com/CSAILVision/places365\n  and then ported to TensorFlow using caffe-tensorflow.\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/caffe_models/AlexNet_places365.pb\'\n  labels_path = \'gs://modelzoo/labels/Places365.txt\'\n  dataset = \'Places365\'\n  image_shape = [227, 227, 3]\n  is_BGR = True\n  image_value_range = (-IMAGENET_MEAN_BGR, 255-IMAGENET_MEAN_BGR)\n  input_name = \'input\'\n\n# TODO - Sanity check this graph and layers\nAlexNet_caffe_Places365.layers = _layers_from_list_of_dicts(AlexNet_caffe_Places365(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv5/concat\', \'depth\': 256} ,\n  {\'tags\': [\'conv\'], \'name\': \'conv5/conv5\', \'depth\': 256} ,\n  {\'tags\': [\'dense\'], \'name\': \'fc6/fc6\', \'depth\': 4096} ,\n  {\'tags\': [\'dense\'], \'name\': \'fc7/fc7\', \'depth\': 4096} ,\n  {\'tags\': [\'dense\'], \'name\': \'prob\', \'depth\': 365} ,\n])\n'"
lucid/modelzoo/caffe_models/InceptionV1.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts, IMAGENET_MEAN_BGR\n\n\nclass InceptionV1_caffe(Model):\n  """"""InceptionV1 (or \'GoogLeNet\') as reimplemented in caffe.\n\n  This model is a reimplementation of GoogLeNet:\n  https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n  reimplemented in caffe by BVLC / Sergio Guadarrama:\n  https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet\n  and then ported to TensorFlow using caffe-tensorflow.\n  """"""\n  model_path = \'gs://modelzoo/vision/caffe_models/InceptionV1.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  is_BGR = True\n  image_value_range = (-IMAGENET_MEAN_BGR, 255-IMAGENET_MEAN_BGR)\n  input_name = \'data\'\n\nInceptionV1_caffe.layers = _layers_from_list_of_dicts(InceptionV1_caffe(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv1_7x7_s2/conv1_7x7_s2\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_3x3_reduce/conv2_3x3_reduce\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_3x3/conv2_3x3\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'inception_3a_output\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'inception_3b_output\', \'depth\': 480},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4a_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4b_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4c_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4d_output\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4e_output\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'inception_5a_output\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'inception_5b_output\', \'depth\': 1024},\n  {\'tags\': [\'dense\'], \'name\': \'prob\', \'depth\': 1000},\n])\n\n\nclass InceptionV1_caffe_Places205(Model):\n  """"""InceptionV1 (or \'GoogLeNet\') trained on Places205.\n\n  This model is a caffe reimplementation of GoogLeNet:\n  https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n  trained on the MIT Places205 dataset, retrieved here:\n  http://places.csail.mit.edu/downloadCNN.html\n  and then ported to TensorFlow using caffe-tensorflow.\n  """"""\n  model_path = \'gs://modelzoo/vision/caffe_models/InceptionV1_places205.pb\'\n  labels_path = \'gs://modelzoo/labels/Places205.txt\'\n  dataset = \'Places205\'\n  image_shape = [224, 224, 3]\n  is_BGR = True\n  image_value_range = (-IMAGENET_MEAN_BGR, 255-IMAGENET_MEAN_BGR)\n  input_name = \'data\'\n\nInceptionV1_caffe_Places205.layers = _layers_from_list_of_dicts(InceptionV1_caffe_Places205(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv1_7x7_s2/conv1_7x7_s2\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_3x3_reduce/conv2_3x3_reduce\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_3x3/conv2_3x3\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'inception_3a_output\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'inception_3b_output\', \'depth\': 480},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4a_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4b_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4c_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4d_output\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4e_output\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'inception_5a_output\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'inception_5b_output\', \'depth\': 1024},\n  {\'tags\': [\'dense\'], \'name\': \'prob\', \'depth\': 205},\n])\n\n\nclass InceptionV1_caffe_Places365(Model):\n  """"""InceptionV1 (or \'GoogLeNet\') trained on Places365.\n\n  This model is a caffe reimplementation of GoogLeNet:\n  https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n  trained on the MIT Places365 dataset, retrieved here:\n  https://github.com/CSAILVision/places365\n  and then ported to TensorFlow using caffe-tensorflow.\n  """"""\n  model_path = \'gs://modelzoo/vision/caffe_models/InceptionV1_places365.pb\'\n  labels_path = \'gs://modelzoo/labels/Places365.txt\'\n  dataset = \'Places365\'\n  image_shape = [224, 224, 3]\n  # What is the correct input range???\n  is_BGR = True\n  image_value_range = (-IMAGENET_MEAN_BGR, 255-IMAGENET_MEAN_BGR)\n  input_name = \'data\'\n\nInceptionV1_caffe_Places365.layers = _layers_from_list_of_dicts(InceptionV1_caffe_Places365(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv1_7x7_s2/conv1_7x7_s2\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_3x3_reduce/conv2_3x3_reduce\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_3x3/conv2_3x3\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'inception_3a_output\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'inception_3b_output\', \'depth\': 480},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4a_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4b_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4c_output\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4d_output\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'inception_4e_output\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'inception_5a_output\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'inception_5b_output\', \'depth\': 1024},\n  {\'tags\': [\'dense\'], \'name\': \'prob\', \'depth\': 365},\n])\n'"
lucid/modelzoo/caffe_models/__init__.py,0,"b'""""""Clean export of caffe_models.\n\nWe manually remove the following symbols from this module to keep tab\ncompletion as clean as possible--even when it doesn\'t respect `__all__`.\nClean namespaces for those lucid.modelzoo modules that contain models are\nenforced by tests in test/modelzoo/test_vision_models.\n""""""\n\nfrom lucid.modelzoo.vision_base import Model as _Model\n\nfrom lucid.modelzoo.caffe_models.AlexNet import *\nfrom lucid.modelzoo.caffe_models.InceptionV1 import *\nfrom lucid.modelzoo.caffe_models.others import *\n\n__all__ = [_name for _name, _obj in list(globals().items())\n           if isinstance(_obj, type) and issubclass(_obj, _Model)\n           and _obj is not _Model]\n\n\ndel absolute_import\ndel division\ndel print_function\n\ndel IMAGENET_MEAN_BGR\n\n# in Python 2 only, list comprehensions leak bound vars to a broader scope\ntry:\n  del _obj\n  del _name\nexcept:\n  pass\n'"
lucid/modelzoo/caffe_models/others.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts, IMAGENET_MEAN_BGR\n\nclass CaffeNet_caffe(Model):\n  """"""CaffeNet (AlexNet variant included in Caffe)\n\n  CaffeNet is a slight variant on AlexNet, described here:\n  https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/caffe_models/CaffeNet.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [227, 227, 3]\n  is_BGR = True\n  image_value_range = (-IMAGENET_MEAN_BGR, 255-IMAGENET_MEAN_BGR)\n  input_name = \'data\'\n\nCaffeNet_caffe.layers = _layers_from_list_of_dicts(CaffeNet_caffe(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv5/concat\', \'depth\': 256} ,\n  {\'tags\': [\'conv\'], \'name\': \'conv5/conv5\', \'depth\': 256} ,\n  {\'tags\': [\'dense\'], \'name\': \'fc6/fc6\', \'depth\': 4096} ,\n  {\'tags\': [\'dense\'], \'name\': \'fc7/fc7\', \'depth\': 4096} ,\n  {\'tags\': [\'dense\'], \'name\': \'prob\', \'depth\': 1000} ,\n])\n\n\nclass VGG16_caffe(Model):\n  """"""VGG16 model used in ImageNet ILSVRC-2014, ported from caffe.\n\n  VGG16 was introduced by Simonyan & Zisserman (2014):\n  https://arxiv.org/pdf/1409.1556.pdf\n  http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n  as the Oxford Visual Geometry Group\'s submission for the ImageNet ILSVRC-2014\n  contest. We download their caffe trained model from\n  https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n  and convert it with caffe-tensorflow.\n  """"""\n  model_path = \'gs://modelzoo/vision/caffe_models/VGG16.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  is_BGR = True\n  image_value_range = (-IMAGENET_MEAN_BGR, 255-IMAGENET_MEAN_BGR)\n  input_name = \'input\'\n\nVGG16_caffe.layers = _layers_from_list_of_dicts(VGG16_caffe(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv1_1/conv1_1\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv1_2/conv1_2\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_1/conv2_1\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_2/conv2_2\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'conv3_1/conv3_1\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'conv3_2/conv3_2\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'conv3_3/conv3_3\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'conv4_1/conv4_1\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv4_2/conv4_2\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv4_3/conv4_3\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv5_1/conv5_1\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv5_2/conv5_2\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv5_3/conv5_3\', \'depth\': 512},\n  {\'tags\': [\'dense\'], \'name\': \'fc6/fc6\', \'depth\': 4096},\n  {\'tags\': [\'dense\'], \'name\': \'fc7/fc7\', \'depth\': 4096},\n  {\'tags\': [\'dense\'], \'name\': \'prob\', \'depth\': 1000},\n])\n\n\nclass VGG19_caffe(Model):\n  """"""VGG16 model used in ImageNet ILSVRC-2014, ported from caffe.\n\n  VGG19 was introduced by Simonyan & Zisserman (2014):\n  https://arxiv.org/pdf/1409.1556.pdf\n  http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n  as the Oxford Visual Geometry Group\'s submission for the ImageNet ILSVRC-2014\n  contest. We download their caffe trained model from\n  https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md\n  and convert it with caffe-tensorflow.\n  """"""\n  model_path = \'gs://modelzoo/vision/caffe_models/VGG19.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  is_BGR = True\n  image_value_range = (-IMAGENET_MEAN_BGR, 255-IMAGENET_MEAN_BGR)\n  input_name = \'input\'\n\nVGG19_caffe.layers = _layers_from_list_of_dicts(VGG19_caffe(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv1_1/conv1_1\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv1_2/conv1_2\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_1/conv2_1\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'conv2_2/conv2_2\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'conv3_1/conv3_1\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'conv3_2/conv3_2\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'conv3_3/conv3_3\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'conv3_4/conv3_4\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'conv4_1/conv4_1\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv4_2/conv4_2\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv4_3/conv4_3\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv4_4/conv4_4\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv5_1/conv5_1\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv5_2/conv5_2\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv5_3/conv5_3\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'conv5_4/conv5_4\', \'depth\': 512},\n  {\'tags\': [\'dense\'], \'name\': \'fc6/fc6\', \'depth\': 4096},\n  {\'tags\': [\'dense\'], \'name\': \'fc7/fc7\', \'depth\': 4096},\n  {\'tags\': [\'dense\'], \'name\': \'prob\', \'depth\': 1000},\n])\n'"
lucid/modelzoo/other_models/AlexNet.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts, IMAGENET_MEAN_BGR\n\n\nclass AlexNet(Model):\n  """"""Original AlexNet weights ported to TF.\n\n  AlexNet is the breakthrough vision model from Krizhevsky, et al (2012):\n  https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n  This implementation is a caffe re-implementation:\n  http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n  It was converted to TensorFlow by this GitHub project:\n  https://github.com/huanzhang12/tensorflow-alexnet-model\n  It appears the parameters are the actual original parameters.\n  """"""\n\n  # The authors of code to convert AlexNet to TF host weights at\n  # http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/alexnet_frozen.pb\n  # but it seems more polite and reliable to host our own.\n  model_path  = \'gs://modelzoo/vision/other_models/AlexNet.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [227, 227, 3]\n  is_BGR = True\n  image_value_range = (-IMAGENET_MEAN_BGR, 255-IMAGENET_MEAN_BGR)\n  input_name = \'Placeholder\'\n\nAlexNet.layers = _layers_from_list_of_dicts(AlexNet(), [\n  {\'tags\': [\'pre_relu\', \'conv\'], \'name\': \'Conv2D\', \'depth\': 96},\n  {\'tags\': [\'pre_relu\', \'conv\'], \'name\': \'Conv2D_1\', \'depth\': 128},\n  {\'tags\': [\'pre_relu\', \'conv\'], \'name\': \'Conv2D_2\', \'depth\': 128},\n  {\'tags\': [\'pre_relu\', \'conv\'], \'name\': \'Conv2D_3\', \'depth\': 384},\n  {\'tags\': [\'pre_relu\', \'conv\'], \'name\': \'Conv2D_4\', \'depth\': 192},\n  {\'tags\': [\'pre_relu\', \'conv\'], \'name\': \'Conv2D_5\', \'depth\': 192},\n  {\'tags\': [\'pre_relu\', \'conv\'], \'name\': \'Conv2D_6\', \'depth\': 128},\n  {\'tags\': [\'pre_relu\', \'conv\'], \'name\': \'Conv2D_7\', \'depth\': 128},\n  {\'tags\': [\'dense\'], \'name\': \'Relu\', \'depth\': 4096},\n  {\'tags\': [\'dense\'], \'name\': \'Relu_1\', \'depth\': 4096},\n  {\'tags\': [\'dense\'], \'name\': \'Softmax\', \'depth\': 1000},\n])\n'"
lucid/modelzoo/other_models/InceptionV1.py,2,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts\n\n\ndef _populate_inception_bottlenecks(scope):\n  """"""Add Inception bottlenecks and their pre-Relu versions to the graph.""""""\n  graph = tf.get_default_graph()\n  for op in graph.get_operations():\n    if op.name.startswith(scope+\'/\') and \'Concat\' in op.type:\n      name = op.name.split(\'/\')[1]\n      pre_relus = []\n      for tower in op.inputs[1:]:\n        if tower.op.type == \'Relu\':\n          tower = tower.op.inputs[0]\n        pre_relus.append(tower)\n      concat_name = scope + \'/\' + name + \'_pre_relu\'\n      _ = tf.concat(pre_relus, -1, name=concat_name)\n\n\nclass InceptionV1(Model):\n  """"""InceptionV1 (or \'GoogLeNet\')\n\n  This is a (re?)implementation of InceptionV1\n  https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n  The weights were trained at Google and released in an early TensorFlow\n  tutorial. It is possible the parameters are the original weights\n  (trained in TensorFlow\'s predecessor), but we haven\'t been able to\n  confirm this.\n\n  As far as we can tell, it is exactly the same as the model described in\n  the original paper, where as the slim and caffe implementations have\n  minor implementation differences (such as eliding the heads).\n  """"""\n  model_path = \'gs://modelzoo/vision/other_models/InceptionV1.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_alternate.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_alternate_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  image_value_range = (-117, 255-117)\n  input_name = \'input\'\n\n  def post_import(self, scope):\n    _populate_inception_bottlenecks(scope)\n\n\nInceptionV1.layers = _layers_from_list_of_dicts(InceptionV1(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv2d0\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2d1\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'conv2d2\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'mixed3a\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'mixed3b\', \'depth\': 480},\n  {\'tags\': [\'conv\'], \'name\': \'mixed4a\', \'depth\': 508},\n  {\'tags\': [\'conv\'], \'name\': \'mixed4b\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'mixed4c\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'mixed4d\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'mixed4e\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'mixed5a\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'mixed5b\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'head0_bottleneck\', \'depth\': 128},\n  {\'tags\': [\'dense\'], \'name\': \'nn0\', \'depth\': 1024},\n  {\'tags\': [\'dense\'], \'name\': \'softmax0\', \'depth\': 1008},\n  {\'tags\': [\'conv\'], \'name\': \'head1_bottleneck\', \'depth\': 128},\n  {\'tags\': [\'dense\'], \'name\': \'nn1\', \'depth\': 1024},\n  {\'tags\': [\'dense\'], \'name\': \'softmax1\', \'depth\': 1008},\n  {\'tags\': [\'dense\'], \'name\': \'softmax2\', \'depth\': 1008},\n])\n'"
lucid/modelzoo/other_models/__init__.py,0,"b'""""""Clean export of other_models.\n\nWe manually remove the following symbols from this module to keep tab\ncompletion as clean as possible--even when it doesn\'t respect `__all__`.\nClean namespaces for those lucid.modelzoo modules that contain models are\nenforced by tests in test/modelzoo/test_vision_models.\n""""""\n\n\nfrom lucid.modelzoo.vision_base import Model as _Model\n\nfrom lucid.modelzoo.other_models.AlexNet import AlexNet\nfrom lucid.modelzoo.other_models.InceptionV1 import InceptionV1\n\n\n__all__ = [_name for _name, _obj in list(globals().items())\n           if isinstance(_obj, type) and issubclass(_obj, _Model)\n           and _obj is not _Model]\n\n# in Python 2 only, list comprehensions leak bound vars to a broader scope\ntry:\n  del _obj\n  del _name\nexcept:\n  pass\n'"
lucid/modelzoo/slim_models/Inception.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts, IMAGENET_MEAN\n\n\nclass InceptionV1_slim(Model):\n  """"""InceptionV1 as implemented by the TensorFlow slim framework.\n\n  InceptionV1 was introduced by Szegedy, et al (2014):\n  https://arxiv.org/pdf/1409.4842v1.pdf\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  corresponding to the name ""inception_v1"".\n  """"""\n  layers = None\n  model_path  = \'gs://modelzoo/vision/slim_models/InceptionV1.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nInceptionV1_slim.layers = _layers_from_list_of_dicts(InceptionV1_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Conv2d_1a_7x7/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Conv2d_2b_1x1/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Conv2d_2c_3x3/Relu\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_3b/concat\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_3c/concat\', \'depth\': 480},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_4b/concat\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_4c/concat\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_4d/concat\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_4e/concat\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_4f/concat\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_5b/concat\', \'depth\': 832},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV1/InceptionV1/Mixed_5c/concat\', \'depth\': 1024},\n  {\'tags\': [\'dense\'], \'name\': \'InceptionV1/Logits/Predictions/Softmax\', \'depth\': 1001},\n])\n\nclass InceptionV2_slim(Model):\n  """"""InceptionV2 as implemented by the TensorFlow slim framework.\n\n  InceptionV2 was introduced by Ioffe & Szegedy (2015):\n  https://arxiv.org/pdf/1502.03167.pdf\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  corresponding to the name ""inception_v2"".\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/InceptionV2.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nInceptionV2_slim.layers = _layers_from_list_of_dicts(InceptionV2_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Conv2d_1a_7x7/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Conv2d_2c_3x3/Relu\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_3b/concat\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_3c/concat\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_4a/concat\', \'depth\': 576},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_4b/concat\', \'depth\': 576},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_4c/concat\', \'depth\': 576},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_4d/concat\', \'depth\': 576},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_4e/concat\', \'depth\': 576},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_5a/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_5b/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV2/InceptionV2/Mixed_5c/concat\', \'depth\': 1024},\n  {\'tags\': [\'dense\'], \'name\': \'InceptionV2/Predictions/Softmax\', \'depth\': 1001},\n])\n\n\nclass InceptionV3_slim(Model):\n  """"""InceptionV3 as implemented by the TensorFlow slim framework.\n\n  InceptionV3 was introduced by Szegedy, et al (2015)\n  https://arxiv.org/pdf/1512.00567.pdf\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  corresponding to the name ""inception_v3"".\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/InceptionV3.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [299, 299, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nInceptionV3_slim.layers = _layers_from_list_of_dicts(InceptionV3_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Conv2d_1a_3x3/Relu\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Conv2d_2a_3x3/Relu\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Conv2d_2b_3x3/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Conv2d_3b_1x1/Relu\', \'depth\': 80},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Conv2d_4a_3x3/Relu\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_5b/concat\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_5c/concat\', \'depth\': 288},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_5d/concat\', \'depth\': 288},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_6a/concat\', \'depth\': 768},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_6b/concat\', \'depth\': 768},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_6c/concat\', \'depth\': 768},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_6d/concat\', \'depth\': 768},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_6e/concat\', \'depth\': 768},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_7a/concat\', \'depth\': 1280},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_7b/concat\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV3/InceptionV3/Mixed_7c/concat\', \'depth\': 2048},\n  {\'tags\': [\'dense\'], \'name\': \'InceptionV3/Predictions/Softmax\', \'depth\': 1001},\n])\n\n\nclass InceptionV4_slim(Model):\n  """"""InceptionV4 as implemented by the TensorFlow slim framework.\n\n  InceptionV4 was introduced by Szegedy, et al (2016):\n  https://arxiv.org/pdf/1602.07261.pdf\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  corresponding to the name ""inception_v4"".\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/InceptionV4.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [299, 299, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nInceptionV4_slim.layers = _layers_from_list_of_dicts(InceptionV4_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Conv2d_1a_3x3/Relu\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Conv2d_2a_3x3/Relu\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Conv2d_2b_3x3/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_3a/concat\', \'depth\': 160},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_4a/concat\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_5a/concat\', \'depth\': 384},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_5b/concat\', \'depth\': 384},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_5c/concat\', \'depth\': 384},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_5d/concat\', \'depth\': 384},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_5e/concat\', \'depth\': 384},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_6a/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_6b/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_6c/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_6d/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_6e/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_6f/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_6g/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_6h/concat\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_7a/concat\', \'depth\': 1536},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_7b/concat\', \'depth\': 1536},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_7c/concat\', \'depth\': 1536},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionV4/InceptionV4/Mixed_7d/concat\', \'depth\': 1536},\n  {\'tags\': [\'dense\'], \'name\': \'InceptionV4/Logits/Predictions\', \'depth\': 1001},\n])\n\n\nclass InceptionResnetV2_slim(Model):\n  """"""InceptionResnetV2 as implemented by the TensorFlow slim framework.\n\n  InceptionResnetV2 was introduced in this paper by Szegedy, et al (2016):\n  https://arxiv.org/pdf/1602.07261.pdf\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  corresponding to the name ""inception_resnet_v2"".\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/InceptionResnetV2.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [299, 299, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\n# TODO: understand this graph, see if we can delete some add or relu nodes from layers\nInceptionResnetV2_slim.layers = _layers_from_list_of_dicts(InceptionResnetV2_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Conv2d_1a_3x3/Relu\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Conv2d_2a_3x3/Relu\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Conv2d_2b_3x3/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Conv2d_3b_1x1/Relu\', \'depth\': 80},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Conv2d_4a_3x3/Relu\', \'depth\': 192},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Mixed_5b/concat\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_1/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_1/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_2/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_2/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_3/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_3/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_4/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_4/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_5/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_5/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_6/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_6/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_7/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_7/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_8/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_8/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_9/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_9/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_10/add\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat/block35_10/Relu\', \'depth\': 320},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Mixed_6a/concat\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_1/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_1/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_2/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_2/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_3/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_3/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_4/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_4/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_5/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_5/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_6/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_6/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_7/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_7/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_8/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_8/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_9/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_9/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_10/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_10/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_11/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_11/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_12/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_12/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_13/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_13/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_14/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_14/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_15/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_15/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_16/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_16/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_17/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_17/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_18/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_18/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_19/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_19/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_20/add\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_20/Relu\', \'depth\': 1088},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Mixed_7a/concat\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_1/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_1/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_2/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_2/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_3/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_3/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_4/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_4/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_5/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_5/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_6/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_6/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_7/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_7/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_8/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_8/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_9/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Repeat_2/block8_9/Relu\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Block8/add\', \'depth\': 2080},\n  {\'tags\': [\'conv\'], \'name\': \'InceptionResnetV2/InceptionResnetV2/Conv2d_7b_1x1/Relu\', \'depth\': 1536},\n  {\'tags\': [\'dense\'], \'name\': \'InceptionResnetV2/Logits/Predictions\', \'depth\': 1001},\n])\n'"
lucid/modelzoo/slim_models/MobilenetV1.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts\n\n\nclass MobilenetV1_slim(Model):\n  """"""MobilenetV1 as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/MobilenetV1.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nMobilenetV1_slim.layers = _layers_from_list_of_dicts(MobilenetV1_slim(), [\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_0/Relu6\', \'depth\': 32, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6\', \'depth\': 64, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6\', \'depth\': 128, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6\', \'depth\': 128, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6\', \'depth\': 256, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6\', \'depth\': 256, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6\', \'depth\': 512, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6\', \'depth\': 512, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6\', \'depth\': 512, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6\', \'depth\': 512, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6\', \'depth\': 512, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6\', \'depth\': 512, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6\', \'depth\': 1024, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6\', \'depth\': 1024, \'tags\': [\'conv\']},\n  # {\'name\': \'MobilenetV1/Logits/AvgPool_1a/AvgPool\', \'depth\': 1024, \'type\': \'avgpool\'},\n  # {\'name\': \'MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D\', \'depth\': 1001, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/Predictions/Softmax\', \'depth\': 1001, \'tags\': [\'dense\']},\n])\n\n\nclass MobilenetV1_050_slim(Model):\n  """"""MobilenetV1050 as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/MobilenetV1050.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nMobilenetV1_050_slim.layers = _layers_from_list_of_dicts(MobilenetV1_050_slim(), [\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_0/Relu6\', \'depth\': 16, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6\', \'depth\': 32, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6\', \'depth\': 64, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6\', \'depth\': 64, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6\', \'depth\': 128, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6\', \'depth\': 128, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6\', \'depth\': 256, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6\', \'depth\': 256, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6\', \'depth\': 256, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6\', \'depth\': 256, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6\', \'depth\': 256, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6\', \'depth\': 256, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6\', \'depth\': 512, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6\', \'depth\': 512, \'tags\': [\'conv\']},\n  # {\'name\': \'MobilenetV1/Logits/AvgPool_1a/AvgPool\', \'depth\': 512, \'type\': \'avgpool\'},\n  # {\'name\': \'MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D\', \'depth\': 1001, \'tags\': [\'conv\']},\n  {\'name\': \'MobilenetV1/Predictions/Softmax\', \'depth\': 1001, \'tags\': [\'dense\']},\n])\n\n\nclass MobilenetV1_025_slim(Model):\n  """"""MobilenetV1025 as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/MobilenetV1025.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nMobilenetV1_025_slim.layers = _layers_from_list_of_dicts(MobilenetV1_025_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_0/Relu6\', \'depth\': 8},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6\', \'depth\': 16},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6\', \'depth\': 128},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6\', \'depth\': 256},\n  # {\'tags\': \'avgpool\', \'name\': \'MobilenetV1/Logits/AvgPool_1a/AvgPool\', \'depth\': 256},\n  # {\'tags\': [\'conv\'], \'name\': \'MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D\', \'depth\': 1001},\n  {\'tags\': [\'dense\'], \'name\': \'MobilenetV1/Predictions/Softmax\', \'depth\': 1001},\n])\n'"
lucid/modelzoo/slim_models/MobilenetV2.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts\n\n\nclass MobilenetV2_10_slim(Model):\n  """"""MobilenetV2 1.0 as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/MobilenetV2_10.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nMobilenetV2_10_slim.layers = _layers_from_list_of_dicts(MobilenetV2_10_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_2/add\', \'depth\': 24},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_4/add\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_5/add\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_7/add\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_8/add\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_9/add\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_11/add\', \'depth\': 96},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_12/add\', \'depth\': 96},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_14/add\', \'depth\': 160},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_15/add\', \'depth\': 160},\n  {\'tags\': [\'dense\'], \'name\': \'MobilenetV2/Predictions/Softmax\', \'depth\': 1001},\n])\n\n\nclass MobilenetV2_14_slim(Model):\n  """"""MobilenetV2 1.4 as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/MobilenetV2_14.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nMobilenetV2_14_slim.layers = _layers_from_list_of_dicts(MobilenetV2_14_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_2/add\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_4/add\', \'depth\': 48},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_5/add\', \'depth\': 48},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_7/add\', \'depth\': 88},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_8/add\', \'depth\': 88},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_9/add\', \'depth\': 88},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_11/add\', \'depth\': 136},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_12/add\', \'depth\': 136},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_14/add\', \'depth\': 224},\n  {\'tags\': [\'conv\'], \'name\': \'MobilenetV2/expanded_conv_15/add\', \'depth\': 224},\n  {\'tags\': [\'dense\'], \'name\': \'MobilenetV2/Predictions/Softmax\', \'depth\': 1001},\n])\n'"
lucid/modelzoo/slim_models/ResNetV1.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts, IMAGENET_MEAN\n\n\nclass ResnetV1_50_slim(Model):\n  """"""ResnetV150 as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/ResnetV1_50.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n\n\n  image_value_range = (-117, 255-117) # Inferred by testing, may not be exactly right\n  input_name = \'input\'\n\n# In ResNetV1, each add (joining the residual branch) is followed by a Relu\n# this seems to be the natural ""layer"" position\nResnetV1_50_slim.layers = _layers_from_list_of_dicts(ResnetV1_50_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/conv1/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block1/unit_1/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block1/unit_2/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block1/unit_3/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block2/unit_1/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block2/unit_2/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block2/unit_3/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block2/unit_4/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block3/unit_1/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block3/unit_2/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block3/unit_3/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block3/unit_4/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block3/unit_5/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block3/unit_6/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block4/unit_1/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block4/unit_2/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_50/block4/unit_3/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'dense\'], \'name\': \'resnet_v1_50/predictions/Softmax\', \'depth\': 1000},\n])\n\n\nclass ResnetV1_101_slim(Model):\n  """"""ResnetV1101 as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/ResnetV1_101.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  image_value_range = (-117, 255-117) # Inferred by testing, may not be exactly right\n  input_name = \'input\'\n\n# In ResNetV1, each add (joining the residual branch) is followed by a Relu\n# this seems to be the natural ""layer"" position\nResnetV1_101_slim.layers = _layers_from_list_of_dicts(ResnetV1_101_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/conv1/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block1/unit_1/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block1/unit_2/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block1/unit_3/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block2/unit_1/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block2/unit_2/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block2/unit_3/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block2/unit_4/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_1/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_2/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_3/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_4/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_5/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_6/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_7/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_8/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_9/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_10/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_11/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_12/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_13/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_14/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_15/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_16/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_17/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_18/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_19/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_20/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_21/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_22/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block3/unit_23/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block4/unit_1/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block4/unit_2/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_101/block4/unit_3/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'dense\'], \'name\': \'resnet_v1_101/predictions/Softmax\', \'depth\': 1000},\n])\n\n\n\nclass ResnetV1_152_slim(Model):\n  """"""ResnetV1152 as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/ResnetV1_152.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  image_value_range = (-117, 255-117) # Inferred by testing, may not be exactly right\n  input_name = \'input\'\n\n# In ResNetV1, each add (joining the residual branch) is followed by a Relu\n# this seems to be the natural ""layer"" position\nResnetV1_152_slim.layers = _layers_from_list_of_dicts(ResnetV1_152_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/conv1/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block1/unit_1/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block1/unit_2/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block1/unit_3/bottleneck_v1/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block2/unit_1/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block2/unit_2/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block2/unit_3/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block2/unit_4/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block2/unit_5/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block2/unit_6/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block2/unit_7/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block2/unit_8/bottleneck_v1/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_1/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_2/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_3/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_4/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_5/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_6/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_7/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_8/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_9/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_10/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_11/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_12/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_13/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_14/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_15/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_16/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_17/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_18/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_19/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_20/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_21/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_22/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_23/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_24/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_25/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_26/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_27/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_28/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_29/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_30/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_31/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_32/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_33/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_34/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_35/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block3/unit_36/bottleneck_v1/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block4/unit_1/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block4/unit_2/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v1_152/block4/unit_3/bottleneck_v1/Relu\', \'depth\': 2048},\n  {\'tags\': [\'dense\'], \'name\': \'resnet_v1_152/predictions/Softmax\', \'depth\': 1000},\n])\n'"
lucid/modelzoo/slim_models/ResNetV2.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts, IMAGENET_MEAN\n\n\nclass ResnetV2_50_slim(Model):\n  """"""ResnetV2_50 as implemented by the TensorFlow slim framework.\n\n  ResnetV2_50 was introduced by He, et al (2016):\n  https://arxiv.org/pdf/1603.05027.pdf\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  corresponding to the name ""resnet_v2_50"".\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/ResnetV2_50.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nResnetV2_50_slim.layers = _layers_from_list_of_dicts(ResnetV2_50_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block1/unit_1/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block1/unit_2/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block1/unit_3/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block2/unit_1/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block2/unit_2/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block2/unit_3/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block2/unit_4/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block3/unit_1/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block3/unit_2/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block3/unit_3/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block3/unit_4/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block3/unit_5/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block3/unit_6/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block4/unit_1/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block4/unit_2/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/block4/unit_3/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_50/postnorm/Relu\', \'depth\': 2048},\n  {\'tags\': [\'dense\'], \'name\': \'resnet_v2_50/predictions/Softmax\', \'depth\': 1001},\n])\n\n\nclass ResnetV2_101_slim(Model):\n  """"""ResnetV2_101 as implemented by the TensorFlow slim framework.\n\n  ResnetV2_101 was introduced by He, et al (2016):\n  https://arxiv.org/pdf/1603.05027.pdf\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  corresponding to the name ""resnet_v2_101"".\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/ResnetV2_101.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nResnetV2_101_slim.layers = _layers_from_list_of_dicts(ResnetV2_101_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block1/unit_1/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block1/unit_2/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block1/unit_3/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block2/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block2/unit_1/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block2/unit_2/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block2/unit_3/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block2/unit_4/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_1/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_2/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_3/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_4/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_5/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_6/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_7/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_8/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_9/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_10/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_11/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_12/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_13/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_14/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_15/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_16/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_17/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_18/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_19/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_20/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_21/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_22/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block3/unit_23/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block4/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block4/unit_1/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block4/unit_2/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/block4/unit_3/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_101/postnorm/Relu\', \'depth\': 2048},\n  {\'tags\': [\'dense\'], \'name\': \'resnet_v2_101/predictions/Softmax\', \'depth\': 1001},\n])\n\n\nclass ResnetV2_152_slim(Model):\n  """"""ResnetV2_152 as implemented by the TensorFlow slim framework.\n\n  ResnetV2_152 was introduced by He, et al (2016):\n  https://arxiv.org/pdf/1603.05027.pdf\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  corresponding to the name ""resnet_v2_152"".\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/ResnetV2_152.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\'\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nResnetV2_152_slim.layers = _layers_from_list_of_dicts(ResnetV2_152_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block1/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 64},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block1/unit_1/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block1/unit_2/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block1/unit_3/bottleneck_v2/add\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 256},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_1/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_2/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_3/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_4/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_5/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_6/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_7/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block2/unit_8/bottleneck_v2/add\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 512},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_1/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_2/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_3/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_4/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_5/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_6/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_7/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_8/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_9/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_10/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_11/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_12/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_13/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_14/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_15/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_16/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_17/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_18/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_19/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_20/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_21/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_22/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_23/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_24/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_25/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_26/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_27/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_28/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_29/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_30/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_31/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_32/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_33/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_34/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_35/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block3/unit_36/bottleneck_v2/add\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block4/unit_1/bottleneck_v2/preact/Relu\', \'depth\': 1024},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block4/unit_1/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block4/unit_2/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/block4/unit_3/bottleneck_v2/add\', \'depth\': 2048},\n  {\'tags\': [\'conv\'], \'name\': \'resnet_v2_152/postnorm/Relu\', \'depth\': 2048},\n  {\'tags\': [\'dense\'], \'name\': \'resnet_v2_152/predictions/Softmax\', \'depth\': 1001},\n])\n'"
lucid/modelzoo/slim_models/__init__.py,0,"b'""""""Clean export of slim_models.\n\nWe manually remove the following symbols from this module to keep tab\ncompletion as clean as possible--even when it doesn\'t respect `__all__`.\nClean namespaces for those lucid.modelzoo modules that contain models are\nenforced by tests in test/modelzoo/test_vision_models.\n""""""\n\n\nfrom lucid.modelzoo.vision_base import Model as _Model\n\nfrom lucid.modelzoo.slim_models.Inception import *\nfrom lucid.modelzoo.slim_models.ResNetV1 import *\nfrom lucid.modelzoo.slim_models.ResNetV2 import *\nfrom lucid.modelzoo.slim_models.MobilenetV1 import *\nfrom lucid.modelzoo.slim_models.MobilenetV2 import *\nfrom lucid.modelzoo.slim_models.others import *\n\n__all__ = [\n    _name\n    for _name, _obj in list(globals().items())\n    if isinstance(_obj, type) and issubclass(_obj, _Model) and _obj is not _Model\n]\n\ndel absolute_import\ndel division\ndel print_function\n\ndel IMAGENET_MEAN\n\n# in Python 2 only, list comprehensions leak bound vars to a broader scope\ntry:\n    del _obj\n    del _name\n    del _layers_from_list_of_dicts\nexcept NameError:\n    pass\n'"
lucid/modelzoo/slim_models/others.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\nfrom lucid.modelzoo.vision_base import Model, _layers_from_list_of_dicts, IMAGENET_MEAN\n\n\n## Commenting out slim implementations of VGG16 and VGG19 because:\n##\n##  * They are supposed to be ports of the caffe implementation,\n##    which we also provide.\n##  * We can\'t determine the correct input range or whether they are BGR.\n##  * They do no provide a softmax layer, leaving us no clear way to test them.\n##\n\n# class VGG16_slim(Model):\n#   """"""VGG16 as implemented by the TensorFlow slim framework.\n#\n#   VGG16 was introduced by Simonyan & Zisserman (2014):\n#   https://arxiv.org/pdf/1409.1556.pdf\n#   This function provides the pre-trained reimplementation from TF slim:\n#   https://github.com/tensorflow/models/tree/master/research/slim\n#   We believe the weights were actually trained in caffe and ported.\n#   """"""\n#\n#   model_path  = \'gs://modelzoo/VGG16.pb\'\n#   labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n#   synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n#   dataset = \'ImageNet\'\n#   image_shape = [224, 224, 3]\n#   image_value_range = (-1, 1)\n#   input_name = \'input\'\n#\n#   layers = _layers_from_list_of_dicts([\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv1/conv1_1/Relu\', \'depth\': 64} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv1/conv1_2/Relu\', \'depth\': 64} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv2/conv2_1/Relu\', \'depth\': 128} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv2/conv2_2/Relu\', \'depth\': 128} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv3/conv3_1/Relu\', \'depth\': 256} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv3/conv3_2/Relu\', \'depth\': 256} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv3/conv3_3/Relu\', \'depth\': 256} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv4/conv4_1/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv4/conv4_2/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv4/conv4_3/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv5/conv5_1/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv5/conv5_2/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/conv5/conv5_3/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/fc6/Relu\', \'depth\': 4096} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_16/fc7/Relu\', \'depth\': 4096} ,\n#    ]\n\n\n# class VGG19_slim(Model):\n#   """"""VGG19 as implemented by the TensorFlow slim framework.\n#\n#   VGG19 was introduced by Simonyan & Zisserman (2014):\n#   https://arxiv.org/pdf/1409.1556.pdf\n#   This function provides the pre-trained reimplementation from TF slim:\n#   https://github.com/tensorflow/models/tree/master/research/slim\n#   We believe the weights were actually trained in caffe and ported.\n#   """"""\n#\n#   model_path  = \'gs://modelzoo/VGG19.pb\'\n#   labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n#   synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n#   dataset = \'ImageNet\'\n#   image_shape = [224, 224, 3]\n#   image_value_range = (-1, 1)\n#   input_name = \'input\'\n#\n#   layers = _layers_from_list_of_dicts([\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv1/conv1_1/Relu\', \'depth\': 64} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv1/conv1_2/Relu\', \'depth\': 64} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv2/conv2_1/Relu\', \'depth\': 128} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv2/conv2_2/Relu\', \'depth\': 128} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv3/conv3_1/Relu\', \'depth\': 256} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv3/conv3_2/Relu\', \'depth\': 256} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv3/conv3_3/Relu\', \'depth\': 256} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv3/conv3_4/Relu\', \'depth\': 256} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv4/conv4_1/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv4/conv4_2/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv4/conv4_3/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv4/conv4_4/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv5/conv5_1/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv5/conv5_2/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv5/conv5_3/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/conv5/conv5_4/Relu\', \'depth\': 512} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/fc6/Relu\', \'depth\': 4096} ,\n#      {\'tags\': [\'conv\'], \'name\': \'vgg_19/fc7/Relu\', \'depth\': 4096} ,\n#    ]\n\n\nclass NasnetMobile_slim(Model):\n  """"""NasnetMobile as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/NasnetMobile.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nNasnetMobile_slim.layers = _layers_from_list_of_dicts(NasnetMobile_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv0/Conv2D\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'cell_stem_0/cell_output/concat\', \'depth\': 44},\n  {\'tags\': [\'conv\'], \'name\': \'cell_stem_1/cell_output/concat\', \'depth\': 88},\n  {\'tags\': [\'conv\'], \'name\': \'cell_0/cell_output/concat\', \'depth\': 264},\n  {\'tags\': [\'conv\'], \'name\': \'cell_1/cell_output/concat\', \'depth\': 264},\n  {\'tags\': [\'conv\'], \'name\': \'cell_2/cell_output/concat\', \'depth\': 264},\n  {\'tags\': [\'conv\'], \'name\': \'cell_3/cell_output/concat\', \'depth\': 264},\n  {\'tags\': [\'conv\'], \'name\': \'reduction_cell_0/cell_output/concat\', \'depth\': 352},\n  {\'tags\': [\'conv\'], \'name\': \'cell_4/cell_output/concat\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'cell_5/cell_output/concat\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'cell_6/cell_output/concat\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'cell_7/cell_output/concat\', \'depth\': 528},\n  {\'tags\': [\'conv\'], \'name\': \'reduction_cell_1/cell_output/concat\', \'depth\': 704},\n  {\'tags\': [\'conv\'], \'name\': \'cell_8/cell_output/concat\', \'depth\': 1056},\n  {\'tags\': [\'conv\'], \'name\': \'cell_9/cell_output/concat\', \'depth\': 1056},\n  {\'tags\': [\'conv\'], \'name\': \'cell_10/cell_output/concat\', \'depth\': 1056},\n  {\'tags\': [\'conv\'], \'name\': \'cell_11/cell_output/concat\', \'depth\': 1056},\n  # {\'tags\': [\'dense\'], \'name\': \'final_layer/FC/BiasAdd\', \'depth\': 1001},\n  {\'tags\': [\'dense\'], \'name\': \'final_layer/predictions\', \'depth\': 1001},\n])\n\n\nclass NasnetLarge_slim(Model):\n  """"""NasnetLarge as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/NasnetLarge.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n  dataset = \'ImageNet\'\n  image_shape = [331, 331, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nNasnetLarge_slim.layers = _layers_from_list_of_dicts(NasnetLarge_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv0/Conv2D\', \'depth\': 96},\n  {\'tags\': [\'conv\'], \'name\': \'cell_stem_0/cell_output/concat\', \'depth\': 168},\n  {\'tags\': [\'conv\'], \'name\': \'cell_stem_1/cell_output/concat\', \'depth\': 336},\n  {\'tags\': [\'conv\'], \'name\': \'cell_0/cell_output/concat\', \'depth\': 1008},\n  {\'tags\': [\'conv\'], \'name\': \'cell_1/cell_output/concat\', \'depth\': 1008},\n  {\'tags\': [\'conv\'], \'name\': \'cell_2/cell_output/concat\', \'depth\': 1008},\n  {\'tags\': [\'conv\'], \'name\': \'cell_3/cell_output/concat\', \'depth\': 1008},\n  {\'tags\': [\'conv\'], \'name\': \'cell_4/cell_output/concat\', \'depth\': 1008},\n  {\'tags\': [\'conv\'], \'name\': \'cell_5/cell_output/concat\', \'depth\': 1008},\n  {\'tags\': [\'conv\'], \'name\': \'reduction_cell_0/cell_output/concat\', \'depth\': 1344},\n  {\'tags\': [\'conv\'], \'name\': \'cell_6/cell_output/concat\', \'depth\': 2016},\n  {\'tags\': [\'conv\'], \'name\': \'cell_7/cell_output/concat\', \'depth\': 2016},\n  {\'tags\': [\'conv\'], \'name\': \'cell_8/cell_output/concat\', \'depth\': 2016},\n  {\'tags\': [\'conv\'], \'name\': \'cell_9/cell_output/concat\', \'depth\': 2016},\n  {\'tags\': [\'conv\'], \'name\': \'cell_10/cell_output/concat\', \'depth\': 2016},\n  {\'tags\': [\'conv\'], \'name\': \'cell_11/cell_output/concat\', \'depth\': 2016},\n  {\'tags\': [\'conv\'], \'name\': \'reduction_cell_1/cell_output/concat\', \'depth\': 2688},\n  {\'tags\': [\'conv\'], \'name\': \'cell_12/cell_output/concat\', \'depth\': 4032},\n  {\'tags\': [\'conv\'], \'name\': \'cell_13/cell_output/concat\', \'depth\': 4032},\n  {\'tags\': [\'conv\'], \'name\': \'cell_14/cell_output/concat\', \'depth\': 4032},\n  {\'tags\': [\'conv\'], \'name\': \'cell_15/cell_output/concat\', \'depth\': 4032},\n  {\'tags\': [\'conv\'], \'name\': \'cell_16/cell_output/concat\', \'depth\': 4032},\n  {\'tags\': [\'conv\'], \'name\': \'cell_17/cell_output/concat\', \'depth\': 4032},\n  # {\'tags\': [\'conv\'], \'name\': \'final_layer/FC/BiasAdd\', \'depth\': 1001},\n  {\'tags\': [\'dense\'], \'name\': \'final_layer/predictions\', \'depth\': 1001},\n])\n\n\nclass PnasnetMobile_slim(Model):\n  """"""PnasnetMobile as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/PnasnetMobile.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [224, 224, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nPnasnetMobile_slim.layers = _layers_from_list_of_dicts(PnasnetMobile_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv0/Conv2D\', \'depth\': 32},\n  {\'tags\': [\'conv\'], \'name\': \'cell_stem_0/cell_output/concat\', \'depth\': 65},\n  {\'tags\': [\'conv\'], \'name\': \'cell_stem_1/cell_output/concat\', \'depth\': 135},\n  {\'tags\': [\'conv\'], \'name\': \'cell_0/cell_output/concat\', \'depth\': 270},\n  {\'tags\': [\'conv\'], \'name\': \'cell_1/cell_output/concat\', \'depth\': 270},\n  {\'tags\': [\'conv\'], \'name\': \'cell_2/cell_output/concat\', \'depth\': 270},\n  {\'tags\': [\'conv\'], \'name\': \'cell_3/cell_output/concat\', \'depth\': 540},\n  {\'tags\': [\'conv\'], \'name\': \'cell_4/cell_output/concat\', \'depth\': 540},\n  {\'tags\': [\'conv\'], \'name\': \'cell_5/cell_output/concat\', \'depth\': 540},\n  {\'tags\': [\'conv\'], \'name\': \'cell_6/cell_output/concat\', \'depth\': 1080},\n  {\'tags\': [\'conv\'], \'name\': \'cell_7/cell_output/concat\', \'depth\': 1080},\n  {\'tags\': [\'conv\'], \'name\': \'cell_8/cell_output/concat\', \'depth\': 1080},\n  {\'tags\': [\'dense\'], \'name\': \'final_layer/predictions\', \'depth\': 1001}\n])\n\n\nclass PnasnetLarge_slim(Model):\n  """"""PnasnetLarge as implemented by the TensorFlow slim framework.\n\n  This function provides the pre-trained reimplementation from TF slim:\n  https://github.com/tensorflow/models/tree/master/research/slim\n  """"""\n\n  model_path  = \'gs://modelzoo/vision/slim_models/PnasnetLarge.pb\'\n  labels_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy.txt\' #TODO\n  synsets_path = \'gs://modelzoo/labels/ImageNet_standard_with_dummy_synsets.txt\'\n  dataset = \'ImageNet\'\n  image_shape = [331, 331, 3]\n  # inpute range taken from:\n  # https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L280\n  image_value_range = (-1, 1)\n  input_name = \'input\'\n\nPnasnetLarge_slim.layers = _layers_from_list_of_dicts(PnasnetLarge_slim(), [\n  {\'tags\': [\'conv\'], \'name\': \'conv0/Conv2D\', \'depth\': 96},\n  {\'tags\': [\'conv\'], \'name\': \'cell_stem_0/cell_output/concat\', \'depth\': 270},\n  {\'tags\': [\'conv\'], \'name\': \'cell_stem_1/cell_output/concat\', \'depth\': 540},\n  {\'tags\': [\'conv\'], \'name\': \'cell_0/cell_output/concat\', \'depth\': 1080},\n  {\'tags\': [\'conv\'], \'name\': \'cell_1/cell_output/concat\', \'depth\': 1080},\n  {\'tags\': [\'conv\'], \'name\': \'cell_2/cell_output/concat\', \'depth\': 1080},\n  {\'tags\': [\'conv\'], \'name\': \'cell_3/cell_output/concat\', \'depth\': 1080},\n  {\'tags\': [\'conv\'], \'name\': \'cell_4/cell_output/concat\', \'depth\': 2160},\n  {\'tags\': [\'conv\'], \'name\': \'cell_5/cell_output/concat\', \'depth\': 2160},\n  {\'tags\': [\'conv\'], \'name\': \'cell_6/cell_output/concat\', \'depth\': 2160},\n  {\'tags\': [\'conv\'], \'name\': \'cell_7/cell_output/concat\', \'depth\': 2160},\n  {\'tags\': [\'conv\'], \'name\': \'cell_8/cell_output/concat\', \'depth\': 4320},\n  {\'tags\': [\'conv\'], \'name\': \'cell_9/cell_output/concat\', \'depth\': 4320},\n  {\'tags\': [\'conv\'], \'name\': \'cell_10/cell_output/concat\', \'depth\': 4320},\n  {\'tags\': [\'conv\'], \'name\': \'cell_11/cell_output/concat\', \'depth\': 4320},\n  {\'tags\': [\'dense\'], \'name\': \'final_layer/predictions\', \'depth\': 1001}\n])\n'"
lucid/optvis/overrides/__init__.py,0,"b'""""""Override gradient implementations easily.\n\nExample usages:\n\nFully default overrides\n```\n  from lucid.optvis import overrides\n  #...\n  with overrides.standard_overrides():\n      T = render.import_model(model, param_t, param_t)\n  #...use imported graph\n```\n\nSpecific subsets of overrides\n```\n  from lucid.optvis import overrides\n  with overrides.gradient_override_map(overrides.pooling_overrides_map):\n    T = render.import_model(model, param_t, param_t)\n  #...use imported graph\n```\n\nCustom overrides\n```\n  from lucid.optvis import overrides\n  with overrides.gradient_override_map({""MaxPool"": overrides.avg_smoothed_maxpool_grad}):\n    T = render.import_model(model, param_t, param_t)\n  #...use imported graph\n```\n\n""""""\nfrom lucid.optvis.overrides.gradient_override import gradient_override_map, use_gradient\nfrom lucid.optvis.overrides.identity_grad import identity_grad\nfrom lucid.optvis.overrides.redirected_relu_grad import (\n    redirected_relu_grad,\n    redirected_relu6_grad,\n)\nfrom lucid.optvis.overrides.smoothed_maxpool_grad import avg_smoothed_maxpool_grad\n\npooling_overrides_map = {""MaxPool"": avg_smoothed_maxpool_grad}\n\nrelu_overrides_map = {""Relu"": redirected_relu_grad, ""Relu6"": redirected_relu6_grad}\n\ndefault_overrides_map = {**pooling_overrides_map, **relu_overrides_map}\n\n\ndef relu_overrides():\n    return gradient_override_map(relu_overrides_map)\n\n\ndef pooling_overrides():\n    return gradient_override_map(pooling_overrides_map)\n\n\ndef default_overrides():\n    return gradient_override_map(default_overrides_map)\n\n\ndef linearization_overrides():\n    return gradient_override_map({""Relu"": identity_grad, **pooling_overrides_map})\n\n'"
lucid/optvis/overrides/gradient_override.py,5,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Easily declare tensorflow functions with custom gradients.\n\nNormally, overriding gradients in TensorFlow requires you to register the\ngradient function to a string and then use a with block with\ngraph.gradient_override_map().\n\nThere\'s a bunch of things about this that are annoying:\n\n(1) You have to pick a string\n(2) You can only use a string once, so if you\'re prototyping you need to\n    generate a new name every time you modify the function.\n(3) You have to make annoying with blocks all the time and it doesn\'t feel\n    very functional.\n\nThis abstraction solves those issues. You no longer need to think about strings\nor with blocks. Just use a single decorator and everything else will be\nhandled for you.\n\nIf you don\'t need to serialize your graph and the gradient override isn\'t\nperformance critical, you can use the high level `use_gradient()` decorator:\n\n  @use_gradient(_foo_grad)\n  def foo(x): ...\n\nOtherwise, you can use use the lower level `gradient_override_map()`, a\nconvenience wrapper for `graph.gradient_override_map()`.\n""""""\n\nfrom contextlib import contextmanager\nimport numpy as np\nimport tensorflow as tf\nimport uuid\n\n\ndef register_to_random_name(grad_f):\n  """"""Register a gradient function to a random string.\n\n  In order to use a custom gradient in TensorFlow, it must be registered to a\n  string. This is both a hassle, and -- because only one function can every be\n  registered to a string -- annoying to iterate on in an interactive\n  environemnt.\n\n  This function registers a function to a unique random string of the form:\n\n    {FUNCTION_NAME}_{RANDOM_SALT}\n\n  And then returns the random string. This is a helper in creating more\n  convenient gradient overrides.\n\n  Args:\n    grad_f: gradient function to register. Should map (op, grad) -> grad(s)\n\n  Returns:\n    String that gradient function was registered to.\n  """"""\n  grad_f_name = grad_f.__name__ + ""_"" + str(uuid.uuid4())\n  tf.RegisterGradient(grad_f_name)(grad_f)\n  return grad_f_name\n\n\n@contextmanager\ndef gradient_override_map(override_dict):\n  """"""Convenience wrapper for graph.gradient_override_map().\n\n  This functions provides two conveniences over normal tensorflow gradient\n  overrides: it auomatically uses the default graph instead of you needing to\n  find the graph, and it automatically\n\n  Example:\n\n    def _foo_grad_alt(op, grad): ...\n\n    with gradient_override({""Foo"": _foo_grad_alt}):\n\n  Args:\n    override_dict: A dictionary describing how to override the gradient.\n      keys: strings correponding to the op type that should have their gradient\n        overriden.\n      values: functions or strings registered to gradient functions\n\n  """"""\n  override_dict_by_name = {}\n  for (op_name, grad_f) in override_dict.items():\n    if isinstance(grad_f, str):\n       override_dict_by_name[op_name] = grad_f\n    else:\n      override_dict_by_name[op_name] = register_to_random_name(grad_f)\n  with tf.get_default_graph().gradient_override_map(override_dict_by_name):\n    yield\n\n\ndef use_gradient(grad_f):\n  """"""Decorator for easily setting custom gradients for TensorFlow functions.\n\n  * DO NOT use this function if you need to serialize your graph.\n  * This function will cause the decorated function to run slower.\n\n  Example:\n\n    def _foo_grad(op, grad): ...\n\n    @use_gradient(_foo_grad)\n    def foo(x1, x2, x3): ...\n\n  Args:\n    grad_f: function to use as gradient.\n\n  Returns:\n    A decorator to apply to the function you wish to override the gradient of.\n\n  """"""\n  grad_f_name = register_to_random_name(grad_f)\n\n  def function_wrapper(f):\n    def inner(*inputs):\n\n      # TensorFlow only supports (as of writing) overriding the gradient of\n      # individual ops. In order to override the gardient of `f`, we need to\n      # somehow make it appear to be an individual TensorFlow op.\n      #\n      # Our solution is to create a PyFunc that mimics `f`.\n      #\n      # In particular, we construct a graph for `f` and run it, then use a\n      # stateful PyFunc to stash it\'s results in Python. Then we have another\n      # PyFunc mimic it by taking all the same inputs and returning the stashed\n      # output.\n      #\n      # I wish we could do this without PyFunc, but I don\'t see a way to have\n      # it be fully general.\n\n      state = {""out_value"": None}\n\n      # First, we need to run `f` and store it\'s output.\n\n      out = f(*inputs)\n\n      def store_out(out_value):\n        """"""Store the value of out to a python variable.""""""\n        state[""out_value""] = out_value\n\n      store_name = ""store_"" + f.__name__\n      store = tf.py_func(store_out, [out], (), stateful=True, name=store_name)\n\n      # Next, we create the mock function, with an overriden gradient.\n      # Note that we need to make sure store gets evaluated before the mock\n      # runs.\n\n      def mock_f(*inputs):\n        """"""Mimic f by retrieving the stored value of out.""""""\n        return state[""out_value""]\n\n      with tf.control_dependencies([store]):\n        with gradient_override_map({""PyFunc"": grad_f_name}):\n          mock_name = ""mock_"" + f.__name__\n          mock_out = tf.py_func(mock_f, inputs, out.dtype, stateful=True,\n                                name=mock_name)\n          mock_out.set_shape(out.get_shape())\n\n      # Finally, we can return the mock.\n\n      return mock_out\n    return inner\n  return function_wrapper\n'"
lucid/optvis/overrides/identity_grad.py,0,"b'import tensorflow as tf\n\n\ndef identity_grad(_op, grad):\n    """"""Returns the incoming gradient unmodified.""""""\n    return grad\n'"
lucid/optvis/overrides/redirected_relu_grad.py,30,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Redirected ReLu Gradient Overrides\n\nWhen we visualize ReLU networks, the initial random input we give the model may\nnot cause the neuron we\'re visualizing to fire at all. For a ReLU neuron, this\nmeans that no gradient flow backwards and the visualization never takes off.\nOne solution would be to find the pre-ReLU tensor, but that can be tedious.\n\nThese functions provide a more convenient solution: temporarily override the\ngradient of ReLUs to allow gradient to flow back through the ReLU -- even if it\ndidn\'t activate and had a derivative of zero -- allowing the visualization\nprocess to get started. These functions override the gradient for at most 16\nsteps. Thus, you need to initialize `global_step` before using these functions.\n\nUsage:\n```python\nfrom lucid.misc.gradient_override import gradient_override_map\nfrom lucid.misc.redirected_relu_grad import redirected_relu_grad\n\n...\nglobal_step_t = tf.train.get_or_create_global_step()\ninit_global_step_op = tf.variables_initializer([global_step_t])\ninit_global_step_op.run()\n...\n\nwith gradient_override_map({\'Relu\': redirected_relu_grad}):\n  model.import_graph(...)\n```\n\nDiscussion:\nReLus block the flow of the gradient during backpropagation when their input is\nnegative. ReLu6s also do so when the input is larger than 6. These overrides\nchange this behavior to allow gradient pushing the input into a desired regime\nbetween these points.\n\n(This override first checks if the entire gradient would be blocked, and only\nchanges it in that case. It does this check independently for each batch entry.)\n\nIn effect, this replaces the relu gradient with the following:\n\nRegime       | Effect\n============================================================\n 0 <= x <= 6 | pass through gradient\n x < 0       | pass through gradient pushing the input up\n x > 6       | pass through gradient pushing the input down\n\nOr visually:\n\n  ReLu:                     |   |____________\n                            |  /|\n                            | / |\n                ____________|/  |\n                            0   6\n\n  Override:     ------------|   |------------\n                  allow  ->       <-  allow\n\nOur implementations contains one extra complication:\ntf.train.Optimizer performs gradient _descent_, so in the update step the\noptimizer changes values in the opposite direction of the gradient. Thus, the\nsign of the gradient in our overrides has the opposite of the intuitive effect:\nnegative gradient pushes the input up, positive pushes it down.\nThus, the code below only allows _negative_ gradient when the input is already\nnegative, and allows _positive_ gradient when the input is already above 6.\n\n\n[0] That is because many model architectures don\'t provide easy access\nto pre-relu tensors. For example, GoogLeNet\'s mixed__ layers are passed through\nan activation function before being concatenated. We are still interested in the\nentire concatenated layer, we would just like to skip the activation function.\n""""""\n\nimport tensorflow as tf\n\n\ndef redirected_relu_grad(op, grad):\n  assert op.type == ""Relu""\n  x = op.inputs[0]\n\n  # Compute ReLu gradient\n  relu_grad = tf.where(x < 0., tf.zeros_like(grad), grad)\n\n  # Compute redirected gradient: where do we need to zero out incoming gradient\n  # to prevent input going lower if its already negative\n  neg_pushing_lower = tf.logical_and(x < 0., grad > 0.)\n  redirected_grad = tf.where(neg_pushing_lower, tf.zeros_like(grad), grad)\n\n  # Ensure we have at least a rank 2 tensor, as we expect a batch dimension\n  assert_op = tf.Assert(tf.greater(tf.rank(relu_grad), 1), [tf.rank(relu_grad)])\n  with tf.control_dependencies([assert_op]):\n    # only use redirected gradient where nothing got through original gradient\n    batch = tf.shape(relu_grad)[0]\n    reshaped_relu_grad = tf.reshape(relu_grad, [batch, -1])\n    relu_grad_mag = tf.norm(reshaped_relu_grad, axis=1)\n  result_grad = tf.where(relu_grad_mag > 0., relu_grad, redirected_grad)\n\n  global_step_t =tf.train.get_or_create_global_step()\n  return_relu_grad = tf.greater(global_step_t, tf.constant(16, tf.int64))\n\n  return tf.where(return_relu_grad, relu_grad, result_grad)\n\n\ndef redirected_relu6_grad(op, grad):\n  assert op.type == ""Relu6""\n  x = op.inputs[0]\n\n  # Compute ReLu gradient\n  relu6_cond = tf.logical_or(x < 0., x > 6.)\n  relu_grad = tf.where(relu6_cond, tf.zeros_like(grad), grad)\n\n  # Compute redirected gradient: where do we need to zero out incoming gradient\n  # to prevent input going lower if its already negative, or going higher if\n  # already bigger than 6?\n  neg_pushing_lower = tf.logical_and(x < 0., grad > 0.)\n  pos_pushing_higher = tf.logical_and(x > 6., grad < 0.)\n  dir_filter = tf.logical_or(neg_pushing_lower, pos_pushing_higher)\n  redirected_grad = tf.where(dir_filter, tf.zeros_like(grad), grad)\n\n  # Ensure we have at least a rank 2 tensor, as we expect a batch dimension\n  assert_op = tf.Assert(tf.greater(tf.rank(relu_grad), 1), [tf.rank(relu_grad)])\n  with tf.control_dependencies([assert_op]):\n    # only use redirected gradient where nothing got through original gradient\n    batch = tf.shape(relu_grad)[0]\n    reshaped_relu_grad = tf.reshape(relu_grad, [batch, -1])\n    relu_grad_mag = tf.norm(reshaped_relu_grad, axis=1)\n  result_grad =  tf.where(relu_grad_mag > 0., relu_grad, redirected_grad)\n\n  global_step_t = tf.train.get_or_create_global_step()\n  return_relu_grad = tf.greater(global_step_t, tf.constant(16, tf.int64))\n\n  return tf.where(return_relu_grad, relu_grad, result_grad)\n'"
lucid/optvis/overrides/smoothed_maxpool_grad.py,4,"b'import tensorflow as tf\nimport logging\n\nlog = logging.getLogger(__name__)\n\ndef make_smoothed_maxpool_grad(smooth_type=""avg"", epsilon=1e-2):\n\n  def MaxPoolGrad(op, grad):\n    inp = op.inputs[0]\n\n    op_args = [op.get_attr(""ksize""), op.get_attr(""strides""), op.get_attr(""padding"")]\n    op_kwargs = {}#dict(data_format=op.get_attr(\'data_format\'))\n\n    if smooth_type == ""L2"":\n      smooth_out = tf.nn.avg_pool(inp**2, *op_args, **op_kwargs)\n      smooth_out /= epsilon + tf.nn.avg_pool(tf.abs(inp), *op_args, **op_kwargs)\n    elif smooth_type == ""avg"":\n      smooth_out = tf.nn.avg_pool(inp, *op_args)\n    else:\n      raise RuntimeError(""Invalid smooth_type"")\n    inp_smooth_grad = tf.gradients(smooth_out, [inp], grad)[0]\n\n    return inp_smooth_grad\n  return MaxPoolGrad\n\n\nl2_smoothed_maxpool_grad = make_smoothed_maxpool_grad(smooth_type=""L2"")\navg_smoothed_maxpool_grad = make_smoothed_maxpool_grad(smooth_type=""avg"")\n'"
lucid/optvis/param/__init__.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom lucid.optvis.param.images import image, grayscale_image_rgb\nfrom lucid.optvis.param.lowres import lowres_tensor\nfrom lucid.optvis.param.color import to_valid_rgb\nfrom lucid.optvis.param.spatial import naive, fft_image, laplacian_pyramid\nfrom lucid.optvis.param.random import image_sample\nfrom lucid.optvis.param.cppn import cppn\n'"
lucid/optvis/param/color.py,4,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for transforming and constraining color channels.""""""\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom lucid.optvis.param.unit_balls import constrain_L_inf\n\ncolor_correlation_svd_sqrt = np.asarray([[0.26, 0.09, 0.02],\n                                         [0.27, 0.00, -0.05],\n                                         [0.27, -0.09, 0.03]]).astype(""float32"")\nmax_norm_svd_sqrt = np.max(np.linalg.norm(color_correlation_svd_sqrt, axis=0))\n\ncolor_mean = [0.48, 0.46, 0.41]\n\n\ndef _linear_decorelate_color(t):\n  """"""Multiply input by sqrt of empirical (ImageNet) color correlation matrix.\n\n  If you interpret t\'s innermost dimension as describing colors in a\n  decorrelated version of the color space (which is a very natural way to\n  describe colors -- see discussion in Feature Visualization article) the way\n  to map back to normal colors is multiply the square root of your color\n  correlations.\n  """"""\n  # check that inner dimension is 3?\n  t_flat = tf.reshape(t, [-1, 3])\n  color_correlation_normalized = color_correlation_svd_sqrt / max_norm_svd_sqrt\n  t_flat = tf.matmul(t_flat, color_correlation_normalized.T)\n  t = tf.reshape(t_flat, tf.shape(t))\n  return t\n\n\ndef to_valid_rgb(t, decorrelate=False, sigmoid=True):\n  """"""Transform inner dimension of t to valid rgb colors.\n\n  In practice this consists of two parts:\n  (1) If requested, transform the colors from a decorrelated color space to RGB.\n  (2) Constrain the color channels to be in [0,1], either using a sigmoid\n      function or clipping.\n\n  Args:\n    t: input tensor, innermost dimension will be interpreted as colors\n      and transformed/constrained.\n    decorrelate: should the input tensor\'s colors be interpreted as coming from\n      a whitened space or not?\n    sigmoid: should the colors be constrained using sigmoid (if True) or\n      clipping (if False).\n\n  Returns:\n    t with the innermost dimension transformed.\n  """"""\n  if decorrelate:\n    t = _linear_decorelate_color(t)\n  if decorrelate and not sigmoid:\n    t += color_mean\n  if sigmoid:\n    return tf.nn.sigmoid(t)\n  else:\n    return constrain_L_inf(2*t-1)/2 + 0.5\n'"
lucid/optvis/param/cppn.py,10,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Compositional Pattern Producing Networks for use as image parameterizations.""""""\n\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\n\ndef _composite_activation(x, biased=True):\n    x = tf.atan(x)\n    if biased:\n        # Biased Coefficients computed by:\n        #   def rms(x):\n        #     return np.sqrt((x*x).mean())\n        #   a = np.arctan(np.random.normal(0.0, 1.0, 10**6))\n        #   print(rms(a), rms(a*a))\n        coeffs = (.67, .6)\n        means = (.0, .0)\n    else:\n        # Unbiased Coefficients computed by:\n        #   a = np.arctan(np.random.normal(0.0, 1.0, 10**6))\n        #   aa = a*a\n        #   print(a.std(), aa.mean(), aa.std())\n        coeffs = (.67, .396)\n        means = (.0, .45)\n    composite = [(x - means[0]) / coeffs[0], (x * x - means[1]) / coeffs[1]]\n    return tf.concat(composite, -1)\n\n\ndef _relu_normalized_activation(x):\n    x = tf.nn.relu(x)\n    # Coefficients computed by:\n    #   a = np.random.normal(0.0, 1.0, 10**6)\n    #   a = np.maximum(a, 0.0)\n    #   print(a.mean(), a.std())\n    return (x - 0.40) / 0.58\n\n\ndef cppn(\n    width,\n    batch=1,\n    num_output_channels=3,\n    num_hidden_channels=24,\n    num_layers=8,\n    activation_func=_composite_activation,\n    normalize=False,\n):\n    """"""Compositional Pattern Producing Network\n\n    Args:\n      width: width of resulting image, equals height\n      batch: batch dimension of output, note that all params share the same weights!\n      num_output_channels:\n      num_hidden_channels:\n      num_layers:\n      activation_func:\n      normalize:\n\n    Returns:\n      The collapsed shape, represented as a list.\n    """"""\n    r = 3.0 ** 0.5  # std(coord_range) == 1.0\n    coord_range = tf.linspace(-r, r, width)\n    y, x = tf.meshgrid(coord_range, coord_range, indexing=""ij"")\n    net = tf.stack([tf.stack([x, y], -1)] * batch, 0)\n\n    with slim.arg_scope(\n        [slim.conv2d],\n        kernel_size=[1, 1],\n        activation_fn=None,\n        weights_initializer=tf.initializers.variance_scaling(),\n        biases_initializer=tf.initializers.random_normal(0.0, 0.1),\n    ):\n        for i in range(num_layers):\n            x = slim.conv2d(net, num_hidden_channels)\n            if normalize:\n                x = slim.instance_norm(x)\n            net = activation_func(x)\n        rgb = slim.conv2d(\n            net,\n            num_output_channels,\n            activation_fn=tf.nn.sigmoid,\n            weights_initializer=tf.zeros_initializer(),\n        )\n    return rgb\n'"
lucid/optvis/param/images.py,4,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""High-level wrapper for paramaterizing images.""""""\n\n\nimport tensorflow as tf\n\nfrom lucid.optvis.param.color import to_valid_rgb\nfrom lucid.optvis.param.spatial import pixel_image, fft_image\n\n\ndef image(\n    w,\n    h=None,\n    batch=None,\n    sd=None,\n    decorrelate=True,\n    fft=True,\n    alpha=False,\n    channels=None,\n):\n    h = h or w\n    batch = batch or 1\n    ch = channels or (4 if alpha else 3)\n    shape = [batch, h, w, ch]\n    param_f = fft_image if fft else pixel_image\n    t = param_f(shape, sd=sd)\n    if channels:\n        output = tf.nn.sigmoid(t)\n    else:\n        output = to_valid_rgb(t[..., :3], decorrelate=decorrelate, sigmoid=True)\n        if alpha:\n            a = tf.nn.sigmoid(t[..., 3:])\n            output = tf.concat([output, a], -1)\n    return output\n\n\ndef grayscale_image_rgb(*args, **kwargs):\n    """"""Takes same arguments as image""""""\n    output = image(*args, channels=1, **kwargs)\n    return tf.tile(output, (1, 1, 1, 3))\n'"
lucid/optvis/param/lowres.py,3,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Provides lowres_tensor().""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom lucid.optvis.param.resize_bilinear_nd import resize_bilinear_nd\n\n\ndef lowres_tensor(shape, underlying_shape, offset=None, sd=None):\n    """"""Produces a tensor paramaterized by a interpolated lower resolution tensor.\n\n  This is like what is done in a laplacian pyramid, but a bit more general. It\n  can be a powerful way to describe images.\n\n  Args:\n    shape: desired shape of resulting tensor\n    underlying_shape: shape of the tensor being resized into final tensor\n    offset: Describes how to offset the interpolated vector (like phase in a\n      Fourier transform). If None, apply no offset. If a scalar, apply the same\n      offset to each dimension; if a list use each entry for each dimension.\n      If a int, offset by that much. If False, do not offset. If True, offset by\n      half the ratio between shape and underlying shape (analogous to 90\n      degrees).\n    sd: Standard deviation of initial tensor variable.\n\n  Returns:\n    A tensor paramaterized by a lower resolution tensorflow variable.\n  """"""\n    sd = sd or 0.01\n    init_val = sd * np.random.randn(*underlying_shape).astype(""float32"")\n    underlying_t = tf.Variable(init_val)\n    t = resize_bilinear_nd(underlying_t, shape)\n    if offset is not None:\n        # Deal with non-list offset\n        if not isinstance(offset, list):\n            offset = len(shape) * [offset]\n\n        # Deal with the non-int offset entries\n        for n in range(len(offset)):\n            if offset[n] is True:\n                offset[n] = shape[n] / underlying_shape[n] / 2\n            if offset[n] is False:\n                offset[n] = 0\n            offset[n] = int(offset[n])\n\n        # Actually apply offset by padding and then croping off the excess.\n        padding = [(pad, 0) for pad in offset]\n        t = tf.pad(t, padding, ""SYMMETRIC"")\n        begin = len(shape) * [0]\n        t = tf.slice(t, begin, shape)\n    return t\n'"
lucid/optvis/param/random.py,5,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom lucid.optvis.param.color import to_valid_rgb\nfrom lucid.optvis.param.spatial import rfft2d_freqs\n\n\ndef image_sample(shape, decorrelate=True, sd=None, decay_power=1):\n    raw_spatial = rand_fft_image(shape, sd=sd, decay_power=decay_power)\n    return to_valid_rgb(raw_spatial, decorrelate=decorrelate)\n\n\n# TODO: DRY with regard to fft_image from lucid.optvis.param.spatial\ndef rand_fft_image(shape, sd=None, decay_power=1):\n    b, h, w, ch = shape\n    sd = 0.01 if sd is None else sd\n\n    imgs = []\n    for _ in range(b):\n        freqs = rfft2d_freqs(h, w)\n        fh, fw = freqs.shape\n        spectrum_var = sd * tf.random_normal([2, ch, fh, fw], dtype=""float32"")\n        spectrum = tf.complex(spectrum_var[0], spectrum_var[1])\n        spertum_scale = 1.0 / np.maximum(freqs, 1.0 / max(h, w)) ** decay_power\n        # Scale the spectrum by the square-root of the number of pixels\n        # to get a unitary transformation. This allows to use similar\n        # learning rates to pixel-wise optimisation.\n        spertum_scale *= np.sqrt(w * h)\n        scaled_spectrum = spectrum * spertum_scale\n        img = tf.spectral.irfft2d(scaled_spectrum)\n        # in case of odd input dimension we cut off the additional pixel\n        # we get from irfft2d length computation\n        img = img[:ch, :h, :w]\n        img = tf.transpose(img, [1, 2, 0])\n        imgs.append(img)\n    return tf.stack(imgs) / 4.0\n'"
lucid/optvis/param/resize_bilinear_nd.py,6,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Provides resize_bilinear_nd.\n\nThis module provides resize_bilinear_nd, a function for resizing a tensor\nwith bilinear interpolation in n dimesions. It iteratively\napplies tf.image.resize_bilinear (which can only resize 2 dimensions).\n""""""\n\nimport tensorflow as tf\n\n\ndef product(l):\n  """"""Multiply together the elements of a list.""""""\n  prod = 1\n  for x in l:\n    prod *= x\n  return prod\n\n\ndef collapse_shape(shape, a, b):\n  """"""Collapse `shape` outside the interval (`a`,`b`).\n\n  This function collapses `shape` outside the interval (`a`,`b`) by\n  multiplying the dimensions before `a` into a single dimension,\n  and mutliplying the dimensions after `b` into a single dimension.\n\n  Args:\n    shape: a tensor shape\n    a: integer, position in shape\n    b: integer, position in shape\n\n  Returns:\n    The collapsed shape, represented as a list.\n\n  Examples:\n    [1, 2, 3, 4, 5], (a=0, b=2) => [1, 1, 2, 60]\n    [1, 2, 3, 4, 5], (a=1, b=3) => [1, 2, 3, 20]\n    [1, 2, 3, 4, 5], (a=2, b=4) => [2, 3, 4, 5 ]\n    [1, 2, 3, 4, 5], (a=3, b=5) => [6, 4, 5, 1 ]\n  """"""\n  shape = list(shape)\n  if a < 0:\n    n_pad = -a\n    pad = n_pad * [1]\n    return collapse_shape(pad + shape, a + n_pad, b + n_pad)\n  if b > len(shape):\n    n_pad = b - len(shape)\n    pad = n_pad * [1]\n    return collapse_shape(shape + pad, a, b)\n  return [product(shape[:a])] + shape[a:b] + [product(shape[b:])]\n\n\ndef resize_bilinear_nd(t, target_shape):\n  """"""Bilinear resizes a tensor t to have shape target_shape.\n\n  This function bilinearly resizes a n-dimensional tensor by iteratively\n  applying tf.image.resize_bilinear (which can only resize 2 dimensions).\n  For bilinear interpolation, the order in which it is applied does not matter.\n\n  Args:\n    t: tensor to be resized\n    target_shape: the desired shape of the new tensor.\n\n  Returns:\n   The resized tensor\n  """"""\n  shape = t.get_shape().as_list()\n  target_shape = list(target_shape)\n  assert len(shape) == len(target_shape)\n\n  # We progressively move through the shape, resizing dimensions...\n  d = 0\n  while d < len(shape):\n\n    # If we don\'t need to deal with the next dimesnion, step over it\n    if shape[d] == target_shape[d]:\n      d += 1\n      continue\n\n    # Otherwise, we\'ll resize the next two dimensions...\n    # If d+2 doesn\'t need to be resized, this will just be a null op for it\n    new_shape = shape[:]\n    new_shape[d : d+2] = target_shape[d : d+2]\n\n    # The helper collapse_shape() makes our shapes 4-dimensional with\n    # the two dimesnions we want to deal with in the middle.\n    shape_ = collapse_shape(shape, d, d+2)\n    new_shape_ = collapse_shape(new_shape, d, d+2)\n\n    # We can then reshape and use the 2d tf.image.resize_bilinear() on the\n    # inner two dimesions.\n    t_ = tf.reshape(t, shape_)\n    t_ = tf.image.resize_bilinear(t_, new_shape_[1:3])\n\n    # And then reshape back to our uncollapsed version, having finished resizing\n    # two more dimensions in our shape.\n    t = tf.reshape(t_, new_shape)\n    shape = new_shape\n    d += 2\n\n  return t\n'"
lucid/optvis/param/spatial.py,12,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport warnings\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom lucid.optvis.param.lowres import lowres_tensor\n\n\ndef pixel_image(shape, sd=None, init_val=None):\n    """"""A naive, pixel-based image parameterization.\n    Defaults to a random initialization, but can take a supplied init_val argument\n    instead.\n\n    Args:\n      shape: shape of resulting image, [batch, width, height, channels].\n      sd: standard deviation of param initialization noise.\n      init_val: an initial value to use instead of a random initialization. Needs\n        to have the same shape as the supplied shape argument.\n\n    Returns:\n      tensor with shape from first argument.\n    """"""\n    if sd is not None and init_val is not None:\n        warnings.warn(\n            ""`pixel_image` received both an initial value and a sd argument. Ignoring sd in favor of the supplied initial value.""\n        )\n\n    sd = sd or 0.01\n    init_val = init_val or np.random.normal(size=shape, scale=sd).astype(np.float32)\n    return tf.Variable(init_val)\n\n\ndef rfft2d_freqs(h, w):\n    """"""Computes 2D spectrum frequencies.""""""\n\n    fy = np.fft.fftfreq(h)[:, None]\n    # when we have an odd input dimension we need to keep one additional\n    # frequency and later cut off 1 pixel\n    if w % 2 == 1:\n        fx = np.fft.fftfreq(w)[: w // 2 + 2]\n    else:\n        fx = np.fft.fftfreq(w)[: w // 2 + 1]\n    return np.sqrt(fx * fx + fy * fy)\n\n\ndef fft_image(shape, sd=None, decay_power=1):\n    """"""An image paramaterization using 2D Fourier coefficients.""""""\n\n    sd = sd or 0.01\n    batch, h, w, ch = shape\n    freqs = rfft2d_freqs(h, w)\n    init_val_size = (2, batch, ch) + freqs.shape\n\n    init_val = np.random.normal(size=init_val_size, scale=sd).astype(np.float32)\n    spectrum_real_imag_t = tf.Variable(init_val)\n    spectrum_t = tf.complex(spectrum_real_imag_t[0], spectrum_real_imag_t[1])\n\n    # Scale the spectrum. First normalize energy, then scale by the square-root\n    # of the number of pixels to get a unitary transformation.\n    # This allows to use similar leanring rates to pixel-wise optimisation.\n    scale = 1.0 / np.maximum(freqs, 1.0 / max(w, h)) ** decay_power\n    scale *= np.sqrt(w * h)\n    scaled_spectrum_t = scale * spectrum_t\n\n    # convert complex scaled spectrum to shape (h, w, ch) image tensor\n    # needs to transpose because irfft2d returns channels first\n    image_t = tf.transpose(tf.spectral.irfft2d(scaled_spectrum_t), (0, 2, 3, 1))\n\n    # in case of odd spatial input dimensions we need to crop\n    image_t = image_t[:batch, :h, :w, :ch]\n    image_t = image_t / 4.0  # TODO: is that a magic constant?\n    return image_t\n\n\ndef laplacian_pyramid_image(shape, n_levels=4, sd=None):\n    """"""Simple laplacian pyramid paramaterization of an image.\n\n    For more flexibility, use a sum of lowres_tensor()s.\n\n    Args:\n      shape: shape of resulting image, [batch, width, height, channels].\n      n_levels: number of levels of laplacian pyarmid.\n      sd: standard deviation of param initialization.\n\n    Returns:\n      tensor with shape from first argument.\n    """"""\n    batch_dims = shape[:-3]\n    w, h, ch = shape[-3:]\n    pyramid = 0\n    for n in range(n_levels):\n        k = 2 ** n\n        pyramid += lowres_tensor(shape, batch_dims + (w // k, h // k, ch), sd=sd)\n    return pyramid\n\n\ndef bilinearly_sampled_image(texture, uv):\n    """"""Build bilinear texture sampling graph.\n\n    Coordinate transformation rules match OpenGL GL_REPEAT wrapping and GL_LINEAR\n    interpolation modes.\n\n    Args:\n      texture: [tex_h, tex_w, channel_n] tensor.\n      uv: [frame_h, frame_h, 2] tensor with per-pixel UV coordinates in range [0..1]\n\n    Returns:\n      [frame_h, frame_h, channel_n] tensor with per-pixel sampled values.\n    """"""\n    h, w = tf.unstack(tf.shape(texture)[:2])\n    u, v = tf.split(uv, 2, axis=-1)\n    v = 1.0 - v  # vertical flip to match GL convention\n    u, v = u * tf.to_float(w) - 0.5, v * tf.to_float(h) - 0.5\n    u0, u1 = tf.floor(u), tf.ceil(u)\n    v0, v1 = tf.floor(v), tf.ceil(v)\n    uf, vf = u - u0, v - v0\n    u0, u1, v0, v1 = map(tf.to_int32, [u0, u1, v0, v1])\n\n    def sample(u, v):\n        vu = tf.concat([v % h, u % w], axis=-1)\n        return tf.gather_nd(texture, vu)\n\n    s00, s01 = sample(u0, v0), sample(u0, v1)\n    s10, s11 = sample(u1, v0), sample(u1, v1)\n    s0 = s00 * (1.0 - vf) + s01 * vf\n    s1 = s10 * (1.0 - vf) + s11 * vf\n    s = s0 * (1.0 - uf) + s1 * uf\n    return s\n\n\n# Deprecations\n\n\ndef naive(shape, sd=None):\n    warnings.warn(\n        ""`naive` has been renamed `pixel_image` for clarity."", DeprecationWarning\n    )\n    return pixel_image(shape, sd)\n\n\ndef laplacian_pyramid(shape, n_levels=4, sd=None):\n    warnings.warn(\n        ""`laplacian_pyramid` has been renamed `laplacian_pyramid_image` for clarity."",\n        DeprecationWarning,\n    )\n    return laplacian_pyramid_image(shape, n_levels=n_levels, sd=sd)\n\n\ndef sample_bilinear(texture, uv):\n    warnings.warn(\n        ""`sample_bilinear` has been renamed `bilinearly_sampled_image` for clarity."",\n        DeprecationWarning,\n    )\n    return bilinearly_sampled_image(texture, uv)\n'"
lucid/optvis/param/unit_balls.py,18,"b'""""""Optimize within unit balls in TensorFlow.\n\nIn adverserial examples, one often wants to optize within a constrained ball.\nThis module makes this easy through functions like unit_ball_L2(), which\ncreates a tensorflow variable constrained within a L2 unit ball.\n\nEXPERIMENTAL: Do not use for adverserial examples if you need to be confident\nthey are strong attacks. We are not yet confident in this code.\n""""""\n\nimport tensorflow as tf\n\nfrom lucid.misc.gradient_override import use_gradient\n\n\ndef dot(a, b):\n  return tf.reduce_sum(a * b)\n\n\ndef _constrain_L2_grad(op, grad):\n  """"""Gradient for constrained optimization on an L2 unit ball.\n\n  This function projects the gradient onto the ball if you are on the boundary\n  (or outside!), but leaves it untouched if you are inside the ball.\n\n  Args:\n    op: the tensorflow op we\'re computing the gradient for.\n    grad: gradient we need to backprop\n\n  Returns:\n    (projected if necessary) gradient.\n  """"""\n  inp = op.inputs[0]\n  inp_norm = tf.norm(inp)\n  unit_inp = inp / inp_norm\n\n  grad_projection = dot(unit_inp, grad)\n  parallel_grad = unit_inp * grad_projection\n\n  is_in_ball = tf.less_equal(inp_norm, 1)\n  is_pointed_inward = tf.less(grad_projection, 0)\n  allow_grad = tf.logical_or(is_in_ball, is_pointed_inward)\n  clip_grad = tf.logical_not(allow_grad)\n\n  clipped_grad = tf.cond(clip_grad, lambda: grad - parallel_grad, lambda: grad)\n\n  return clipped_grad\n\n\n@use_gradient(_constrain_L2_grad)\ndef constrain_L2(x):\n  return x / tf.maximum(1.0, tf.norm(x))\n\n\ndef unit_ball_L2(shape):\n  """"""A tensorflow variable tranfomed to be constrained in a L2 unit ball.\n\n  EXPERIMENTAL: Do not use for adverserial examples if you need to be confident\n  they are strong attacks. We are not yet confident in this code.\n  """"""\n  x = tf.Variable(tf.zeros(shape))\n  return constrain_L2(x)\n\n\ndef _constrain_L_inf_grad(precondition=True):\n\n  def grad_f(op, grad):\n    """"""Gradient for constrained preconditioned optimization on an L_inf unit\n    ball.\n\n    This function projects the gradient onto the ball if you are on the\n    boundary (or outside!). It always preconditions the gradient so it is the\n    direction of steepest descent under L_inf.\n\n    Args:\n      op: the tensorflow op we\'re computing the gradient for.\n      grad: gradient we need to backprop\n\n    Returns:\n      (projected if necessary) preconditioned gradient.\n    """"""\n    inp = op.inputs[0]\n    dim_at_edge = tf.greater_equal(tf.abs(inp), 1.0)\n    dim_outward = tf.greater(inp * grad, 0.0)\n    if precondition:\n      grad = tf.sign(grad)\n\n    return tf.where(\n        tf.logical_and(dim_at_edge, dim_outward),\n        tf.zeros(grad.shape),\n        grad\n      )\n  return grad_f\n\n\n@use_gradient(_constrain_L_inf_grad(precondition=True))\ndef constrain_L_inf_precondition(x):\n  return x / tf.maximum(1.0, tf.abs(x))\n\n\n@use_gradient(_constrain_L_inf_grad(precondition=False))\ndef constrain_L_inf(x):\n  return x / tf.maximum(1.0, tf.abs(x))\n\n\ndef unit_ball_L_inf(shape, precondition=True):\n  """"""A tensorflow variable tranfomed to be constrained in a L_inf unit ball.\n\n  Note that this code also preconditions the gradient to go in the L_inf\n  direction of steepest descent.\n\n  EXPERIMENTAL: Do not use for adverserial examples if you need to be confident\n  they are strong attacks. We are not yet confident in this code.\n  """"""\n  x = tf.Variable(tf.zeros(shape))\n  if precondition:\n    return constrain_L_inf_precondition(x)\n  else:\n    return constrain_L_inf(x)\n'"
lucid/recipes/activation_atlas/__init__.py,0,"b'from lucid.recipes.activation_atlas.main import (\n    activation_atlas,\n    aligned_activation_atlas,\n)\n'"
lucid/recipes/activation_atlas/layout.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport logging\nfrom umap import UMAP\n\nlog = logging.getLogger(__name__)\n\n\ndef normalize_layout(layout, min_percentile=1, max_percentile=99, relative_margin=0.1):\n    """"""Removes outliers and scales layout to between [0,1].""""""\n\n    # compute percentiles\n    mins = np.percentile(layout, min_percentile, axis=(0))\n    maxs = np.percentile(layout, max_percentile, axis=(0))\n\n    # add margins\n    mins -= relative_margin * (maxs - mins)\n    maxs += relative_margin * (maxs - mins)\n\n    # `clip` broadcasts, `[None]`s added only for readability\n    clipped = np.clip(layout, mins, maxs)\n\n    # embed within [0,1] along both axes\n    clipped -= clipped.min(axis=0)\n    clipped /= clipped.max(axis=0)\n\n    return clipped\n\n\ndef aligned_umap(activations, umap_options={}, normalize=True, verbose=False):\n    """"""`activations` can be a list of ndarrays. In that case a list of layouts is returned.""""""\n\n    umap_defaults = dict(\n        n_components=2, n_neighbors=50, min_dist=0.05, verbose=verbose, metric=""cosine""\n    )\n    umap_defaults.update(umap_options)\n\n    # if passed a list of activations, we combine them and later split the layouts\n    if type(activations) is list or type(activations) is tuple:\n        num_activation_groups = len(activations)\n        combined_activations = np.concatenate(activations)\n    else:\n        num_activation_groups = 1\n        combined_activations = activations\n    try:\n        layout = UMAP(**umap_defaults).fit_transform(combined_activations)\n    except (RecursionError, SystemError) as exception:\n        log.error(""UMAP failed to fit these activations. We\'re not yet sure why this sometimes occurs."")\n        raise ValueError(""UMAP failed to fit activations: %s"", exception)\n\n    if normalize:\n        layout = normalize_layout(layout)\n\n    if num_activation_groups > 1:\n        layouts = np.split(layout, num_activation_groups, axis=0)\n        return layouts\n    else:\n        return layout\n'"
lucid/recipes/activation_atlas/main.py,0,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom enum import Enum, auto\n\nimport numpy as np\n\nfrom lucid.modelzoo.aligned_activations import (\n    push_activations,\n    NUMBER_OF_AVAILABLE_SAMPLES,\n    layer_inverse_covariance,\n)\nfrom lucid.recipes.activation_atlas.layout import aligned_umap\nfrom lucid.recipes.activation_atlas.render import render_icons\nfrom more_itertools import chunked\n\n\ndef activation_atlas(\n    model,\n    layer,\n    grid_size=10,\n    icon_size=96,\n    number_activations=NUMBER_OF_AVAILABLE_SAMPLES,\n    icon_batch_size=32,\n    verbose=False,\n):\n    """"""Renders an Activation Atlas of the given model\'s layer.""""""\n\n    activations = layer.activations[:number_activations, ...]\n    layout, = aligned_umap(activations, verbose=verbose)\n    directions, coordinates, _ = bin_laid_out_activations(\n        layout, activations, grid_size\n    )\n    icons = []\n    for directions_batch in chunked(directions, icon_batch_size):\n        icon_batch, losses = render_icons(\n            directions_batch, model, layer=layer.name, size=icon_size, num_attempts=1\n        )\n        icons += icon_batch\n    canvas = make_canvas(icons, coordinates, grid_size)\n\n    return canvas\n\n\ndef aligned_activation_atlas(\n    model1,\n    layer1,\n    model2,\n    layer2,\n    grid_size=10,\n    icon_size=80,\n    num_steps=1024,\n    whiten_layers=True,\n    number_activations=NUMBER_OF_AVAILABLE_SAMPLES,\n    icon_batch_size=32,\n    verbose=False,\n):\n    """"""Renders two aligned Activation Atlases of the given models\' layers.\n\n    Returns a generator of the two atlasses, and a nested generator for intermediate\n    atlasses while they\'re being rendered.\n    """"""\n    combined_activations = _combine_activations(\n        layer1, layer2, number_activations=number_activations\n    )\n    layouts = aligned_umap(combined_activations, verbose=verbose)\n\n    for model, layer, layout in zip((model1, model2), (layer1, layer2), layouts):\n        directions, coordinates, densities = bin_laid_out_activations(\n            layout, layer.activations[:number_activations, ...], grid_size, threshold=10\n        )\n\n        def _progressive_canvas_iterator():\n            icons = []\n            for directions_batch in chunked(directions, icon_batch_size):\n                icon_batch, losses = render_icons(\n                    directions_batch,\n                    model,\n                    alpha=False,\n                    layer=layer.name,\n                    size=icon_size,\n                    n_steps=num_steps,\n                    S=layer_inverse_covariance(layer) if whiten_layers else None,\n                )\n                icons += icon_batch\n                yield make_canvas(icons, coordinates, grid_size)\n\n        yield _progressive_canvas_iterator()\n\n\n# Helpers\n\n\nclass ActivationTranslation(Enum):\n    ONE_TO_TWO = auto()\n    BIDIRECTIONAL = auto()\n\n\ndef _combine_activations(\n    layer1,\n    layer2,\n    activations1=None,\n    activations2=None,\n    mode=ActivationTranslation.BIDIRECTIONAL,\n    number_activations=NUMBER_OF_AVAILABLE_SAMPLES,\n):\n    """"""Given two layers, combines their activations according to mode.\n\n    ActivationTranslation.ONE_TO_TWO:\n      Translate activations of layer1 into the space of layer2, and return a tuple of\n      the translated activations and the original layer2 activations.\n\n    ActivationTranslation.BIDIRECTIONAL:\n      Translate activations of layer1 into the space of layer2, activations of layer2\n      into the space of layer 1, concatenate them along their channels, and returns a\n      tuple of the concatenated activations for each layer.\n    """"""\n    activations1 = activations1 or layer1.activations[:number_activations, ...]\n    activations2 = activations2 or layer2.activations[:number_activations, ...]\n\n    if mode is ActivationTranslation.ONE_TO_TWO:\n\n        acts_1_to_2 = push_activations(activations1, layer1, layer2)\n        return acts_1_to_2, activations2\n\n    elif mode is ActivationTranslation.BIDIRECTIONAL:\n\n        acts_1_to_2 = push_activations(activations1, layer1, layer2)\n        acts_2_to_1 = push_activations(activations2, layer2, layer1)\n\n        activations_model1 = np.concatenate((activations1, acts_1_to_2), axis=1)\n        activations_model2 = np.concatenate((acts_2_to_1, activations2), axis=1)\n\n        return activations_model1, activations_model2\n\n\ndef bin_laid_out_activations(layout, activations, grid_size, threshold=5):\n    """"""Given a layout and activations, overlays a grid on the layout and returns\n    averaged activations for each grid cell. If a cell contains less than `threshold`\n    activations it will be discarded, so the number of returned data is variable.""""""\n\n    assert layout.shape[0] == activations.shape[0]\n\n    # calculate which grid cells each activation\'s layout position falls into\n    # first bin stays empty because nothing should be < 0, so we add an extra bin\n    bins = np.linspace(0, 1, num=grid_size + 1)\n    bins[-1] = np.inf  # last bin should include all higher values\n    indices = np.digitize(layout, bins) - 1  # subtract 1 to account for empty first bin\n\n    # because of thresholding we may need to return a variable number of means\n    means, coordinates, counts = [], [], []\n\n    # iterate over all grid cell coordinates to compute their average directions\n    grid_coordinates = np.indices((grid_size, grid_size)).transpose().reshape(-1, 2)\n    for xy_coordinates in grid_coordinates:\n        mask = np.equal(xy_coordinates, indices).all(axis=1)\n        count = np.count_nonzero(mask)\n        if count > threshold:\n            counts.append(count)\n            coordinates.append(xy_coordinates)\n            mean = np.average(activations[mask], axis=0)\n            means.append(mean)\n\n    assert len(means) == len(coordinates) == len(counts)\n    if len(coordinates) == 0:\n        raise RuntimeError(""Binning activations led to 0 cells containing activations!"")\n\n    return means, coordinates, counts\n\n\ndef make_canvas(icon_batch, coordinates, grid_size):\n    """"""Given a list of images and their coordinates, places them on a white canvas.""""""\n\n    grid_shape = (grid_size, grid_size)\n    icon_shape = icon_batch[0].shape\n    canvas = np.ones((*grid_shape, *icon_shape))\n\n    for icon, (x, y) in zip(icon_batch, coordinates):\n        canvas[x, y] = icon\n\n    return np.hstack(np.hstack(canvas))\n\n\nif __name__ == ""__main__"":\n    activation_atlas()\n'"
lucid/recipes/activation_atlas/render.py,12,"b'# Copyright 2018 The Lucid Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nimport numpy as np\nfrom itertools import chain\n\n\n# TODO(schubert@): simplify, cleanup, dedupe objectives\n\nimport lucid.optvis.objectives as objectives\nimport lucid.optvis.param as param\nimport lucid.optvis.render as render\nimport lucid.optvis.transform as transform\n\n\n@objectives.wrap_objective()\ndef direction_neuron_S(layer_name, vec, batch=None, x=None, y=None, S=None):\n\n    def inner(T):\n        layer = T(layer_name)\n        shape = tf.shape(layer)\n        x_ = shape[1] // 2 if x is None else x\n        y_ = shape[2] // 2 if y is None else y\n        if batch is None:\n            raise RuntimeError(""requires batch"")\n\n        acts = layer[batch, x_, y_]\n        vec_ = vec\n        if S is not None:\n            vec_ = tf.matmul(vec_[None], S)[0]\n        # mag = tf.sqrt(tf.reduce_sum(acts**2))\n        dot = tf.reduce_mean(acts * vec_)\n        # cossim = dot/(1e-4 + mag)\n        return dot\n\n    return inner\n\n\n@objectives.wrap_objective()\ndef direction_neuron_cossim_S(\n    layer_name, vec, batch=None, x=None, y=None, cossim_pow=2, S=None\n):\n\n    def inner(T):\n        layer = T(layer_name)\n        shape = tf.shape(layer)\n        x_ = shape[1] // 2 if x is None else x\n        y_ = shape[2] // 2 if y is None else y\n        if batch is None:\n            raise RuntimeError(""requires batch"")\n            \n        acts = layer[batch, x_, y_]\n        vec_ = vec\n        if S is not None:\n            vec_ = tf.matmul(vec_[None], S)[0]\n        mag = tf.sqrt(tf.reduce_sum(acts ** 2))\n        dot = tf.reduce_mean(acts * vec_)\n        cossim = dot / (1e-4 + mag)\n        cossim = tf.maximum(0.1, cossim)\n        return dot * cossim ** cossim_pow\n\n    return inner\n\n\ndef render_icons(\n    directions,\n    model,\n    layer,\n    size=80,\n    n_steps=128,\n    verbose=False,\n    S=None,\n    num_attempts=3,\n    cossim=True,\n    alpha=False,\n):\n\n    model.load_graphdef()\n\n    image_attempts = []\n    loss_attempts = []\n\n    depth = 4 if alpha else 3\n    batch = len(directions)\n    input_shape = (batch, size, size, depth)\n\n    # Render two attempts, and pull the one with the lowest loss score.\n    for attempt in range(num_attempts):\n\n        # Render an image for each activation vector\n        param_f = lambda: param.image(\n            size, batch=len(directions), fft=True, decorrelate=True, alpha=alpha\n        )\n\n        if cossim is True:\n            obj_list = [\n                direction_neuron_cossim_S(layer, v, batch=n, S=S)\n                for n, v in enumerate(directions)\n            ]\n        else:\n            obj_list = [\n                direction_neuron_S(layer, v, batch=n, S=S)\n                for n, v in enumerate(directions)\n            ]\n\n        obj_list += [\n          objectives.penalize_boundary_complexity(input_shape, w=5)\n        ]\n\n        obj = objectives.Objective.sum(obj_list)\n\n        # holy mother of transforms\n        transforms = [\n           transform.pad(16, mode=\'constant\'),\n           transform.jitter(4),\n           transform.jitter(4),\n           transform.jitter(8),\n           transform.jitter(8),\n           transform.jitter(8),\n           transform.random_scale(0.998**n for n in range(20,40)),\n           transform.random_rotate(chain(range(-20,20), range(-10,10), range(-5,5), 5*[0])),\n           transform.jitter(2),\n           transform.crop_or_pad_to(size, size)\n        ]\n        if alpha:\n            transforms.append(transform.collapse_alpha_random())\n\n        # This is the tensorflow optimization process\n\n        # print(""attempt: "", attempt)\n        with tf.Graph().as_default(), tf.Session() as sess:\n            learning_rate = 0.05\n            losses = []\n            trainer = tf.train.AdamOptimizer(learning_rate)\n            T = render.make_vis_T(model, obj, param_f, trainer, transforms)\n            vis_op, t_image = T(""vis_op""), T(""input"")\n            losses_ = [obj_part(T) for obj_part in obj_list]\n            tf.global_variables_initializer().run()\n            for i in range(n_steps):\n                loss, _ = sess.run([losses_, vis_op])\n                losses.append(loss)\n                # if i % 100 == 0:\n                    # print(i)\n\n            img = t_image.eval()\n            img_rgb = img[:, :, :, :3]\n            if alpha:\n                # print(""alpha true"")\n                k = 0.8\n                bg_color = 0.0\n                img_a = img[:, :, :, 3:]\n                img_merged = img_rgb * ((1 - k) + k * img_a) + bg_color * k * (\n                    1 - img_a\n                )\n                image_attempts.append(img_merged)\n            else:\n                # print(""alpha false"")\n                image_attempts.append(img_rgb)\n\n            loss_attempts.append(losses[-1])\n\n    # Use only the icons with the lowest loss\n    loss_attempts = np.asarray(loss_attempts)\n    loss_final = []\n    image_final = []\n    # print(""merging best scores from attempts..."")\n    for i, d in enumerate(directions):\n        # note, this should be max, it is not a traditional loss\n        mi = np.argmax(loss_attempts[:, i])\n        image_final.append(image_attempts[mi][i])\n\n    return (image_final, loss_final)\n'"
lucid/scratch/atlas_pipeline/__init__.py,0,b''
lucid/scratch/atlas_pipeline/grid.py,0,"b'""""""\nInternal pipeline functions that takes in a layout and produces grid cells, organized into tiles\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\nimport pandas as pd\n\ndef grid(metadata, layout, params):\n  """"""\n  layout: numpy arrays x, y\n  metadata: user-defined numpy arrays with metadata\n  n_layer: number of cells in the layer (squared)\n  n_tile: number of cells in the tile (squared)\n  """"""\n  x = layout[""x""]\n  y = layout[""y""]\n  x_min = np.min(x)\n  x_max = np.max(x)\n  y_min = np.min(y)\n  y_max = np.max(y)\n\n  # this creates the grid\n  bins = np.linspace(x_min, x_max, params[""n_layer""] - 1)\n  xd = np.digitize(x, bins)\n  bins = np.linspace(y_min, y_max, params[""n_layer""] - 1)\n  yd = np.digitize(y, bins)\n\n  # the number of tiles is the number of cells divided by the number of cells in each tile\n  num_tiles = int(params[""n_layer""]/params[""n_tile""])\n  print(""num tiles"", num_tiles)\n  # we will save the tiles in an array indexed by the tile coordinates\n  tiles = {}\n  for ti in range(num_tiles):\n    for tj in range(num_tiles):\n      tiles[(ti,tj)] = {\n        ""x"": [],\n        ""y"": [],\n        ""ci"": [], # cell-space x coordinate\n        ""cj"": [], # cell-space y coordinate\n        ""gi"": [], # global index\n      }\n\n  for i,xi in enumerate(x):\n    if(i % 1000 == 0 or i+1 == len(x)):\n      print(""point"", i+1, ""/"", len(x), end=""\\r"")\n    # layout-space coordinates\n    yi = y[i]\n    # grid-space cell coordinates\n    ci = xd[i]\n    cj = yd[i]\n    # tile coordinate\n    ti = math.floor(ci / params[""n_tile""])\n    tj = math.floor(cj / params[""n_tile""])\n\n    # TODO: don\'t append a point if it doesn\'t match a filter function provided in params\n    filter = params.get(""filter"", lambda i,metadata: True)\n    if(filter(i, metadata=metadata)):\n      tiles[(ti,tj)][""x""].append(xi)\n      tiles[(ti,tj)][""y""].append(yi)\n      tiles[(ti,tj)][""ci""].append(ci)\n      tiles[(ti,tj)][""cj""].append(cj)\n      tiles[(ti,tj)][""gi""].append(i)\n    \n  return tiles\n\ndef write_grid_local(tiles, params):\n  """"""\n  Write a file for each tile\n  """"""\n  # TODO: this isn\'t being used right now, will need to be\n  # ported to gfile if we want to keep it\n  for ti,tj,tile in enumerate_tiles(tiles):\n    filename = ""{directory}/{name}/tile_{n_layer}_{n_tile}_{ti}_{tj}"".format(ti=ti, tj=tj, **params) #directory=directory, name=name, n_layer=n_layer, n_tile=n_tile, \n    # write out the tile as a npz\n    print(""saving"", filename + "".npz"")\n    np.savez_compressed(filename + "".npz"", **tile)\n    # write out the tile as a csv\n    print(""saving"", filename + "".csv"")\n    df = pd.DataFrame(tile)\n    df.to_csv(filename + "".csv"", index=False)\n\ndef enumerate_tiles(tiles):\n  """"""\n  Convenience\n  """"""\n  enumerated = []\n  for key in tiles.keys():\n    enumerated.append((key[0], key[1], tiles[key]))\n  return enumerated\n\n\n# TODO: use named parameters?\ndef tile_cells(tile):\n  # we need to collect all the data points for a given set of cell indices\n  # and then pass that to the render function\n  cells = {}\n  for i,gi in enumerate(tile[""gi""]):\n    ci = tile[""ci""][i]\n    cj = tile[""cj""][i]\n    c = cells.get((ci,cj), {""gi"": [], ""i"": ci, ""j"": cj})\n    c[""gi""].append(gi)\n    cells[(ci,cj)] = c\n\n  return cells'"
lucid/scratch/atlas_pipeline/pipeline.py,0,"b'""""""\nTake user input data and run it through the grid and tile rendering pipeline\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport numpy as np\nimport lucid.scratch.atlas_pipeline.render_tile as render_tile\nimport lucid.scratch.atlas_pipeline.grid as grid\nfrom tensorflow import gfile\n\n# TODO: this is where we would distribute the tile processing\ndef run(render, aggregate, params, metadata, layout):\n  \n  gfile.MakeDirs(""{directory}"".format(**params))\n\n  layers = params[""n_cells""]\n  for n_layer in layers:\n    params[""n_layer""] = n_layer\n    tiles = grid.grid(metadata, layout, params)\n\n    # TODO: write out summary? might be useful for the client-side renderer\n    summary = summarize(tiles, params, layout, metadata)\n    print(""summary"", summary)\n    etiles = grid.enumerate_tiles(tiles)\n    for i,t in enumerate(etiles):\n      ti = t[0]\n      tj = t[1]\n      tile = t[2] \n      cells = grid.tile_cells(tile)\n\n      print(""aggregate tile"", i+1, ""/"", len(etiles))\n      tile_json = render_tile.aggregate_tile(cells, ti, tj, aggregate, params, metadata, layout, summary)\n      filename = ""{directory}/{name}-tile_{n_layer}_{n_tile}_{ti}_{tj}.json"".format(ti=ti, tj=tj, **params)\n      print(""saving"", filename)\n      with gfile.Open(filename, \'w\') as f:\n        json.dump(tile_json, f)\n\n      print(""render tile"", i+1, ""/"", len(etiles))\n      tile_img = render_tile.render_tile(cells, ti, tj, render, params, metadata, layout, summary)\n      filename = ""{directory}/{name}-tile_{n_layer}_{n_tile}_{ti}_{tj}.png"".format(ti=ti, tj=tj, **params)\n      print(""saving"", filename)\n      with gfile.Open(filename, \'w\') as f:\n        tile_img.save(f)\n\ndef summarize(tiles, params, layout, metadata):\n  # calculate summary statistics that may be useful for rendering\n  # e.g. max point density, bounds of x and y\n  # we can also track the parameters for convenience\n  summary = {\n  }\n  x = layout[""x""]\n  y = layout[""y""]\n  summary[""x_min""] = np.min(x)\n  summary[""x_max""] = np.max(x)\n  summary[""y_min""] = np.min(y)\n  summary[""y_max""] = np.max(y)\n  # the size of a cell in layout coordinate space\n  summary[""x_bin""] = abs(summary[""x_max""] - summary[""x_min""])/params[""n_layer""]\n  summary[""y_bin""] = abs(summary[""y_max""] - summary[""y_min""])/params[""n_layer""]\n\n  # maximum density of a cell\n  max_density = 0\n  min_density = float(""inf"")\n  user_max_density = 0\n  user_min_density = float(""inf"")\n\n  num_cells = 0\n  total_count = 0\n  user_total_count = 0\n  def density(cell, metadata):\n    return len(cell[""gi""])\n  # if the user doesn\'t provide a scale function its just the density\n  user_density = params.get(""density_function"", density)\n\n  for t in grid.enumerate_tiles(tiles):\n    tile = t[2] \n    cells = grid.tile_cells(tile) \n    keys = cells.keys()\n    for i,key in enumerate(keys):\n      num_cells += 1\n      num_points = density(cells[key], metadata)\n      user_num_points = user_density(cells[key], metadata)\n      \n      total_count += num_points\n      user_total_count += user_num_points\n      if num_points > max_density:\n        max_density = num_points\n      if num_points < min_density:\n        min_density = num_points\n      if user_num_points > user_max_density:\n        user_max_density = user_num_points\n      if user_num_points < user_min_density:\n        user_min_density = user_num_points\n  summary[""max_density""] = max_density\n  summary[""min_density""] = min_density\n  summary[""total_count""] = total_count\n  summary[""user_max_density""] = user_max_density\n  summary[""user_min_density""] = user_min_density\n  summary[""user_total_count""] = user_total_count\n  summary[""num_cells""] = num_cells\n  # TODO: enable summarizing of meta attributes?\n  return summary\n'"
lucid/scratch/atlas_pipeline/render_tile.py,0,"b'""""""\nController that runs the user defined render function on each cell in a tile\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom PIL import Image\nimport math\n\ndef render_tile(cells, ti, tj, render, params, metadata, layout, summary):\n  """"""\n    Render each cell in the tile and stitch it into a single image\n  """"""\n  image_size = params[""cell_size""] * params[""n_tile""]\n  tile = Image.new(""RGB"", (image_size, image_size), (255,255,255))\n  keys = cells.keys()\n\n  def density(cell, metadata):\n    return len(cell[""gi""])\n  # if the user doesn\'t provide a scale function its just the density\n  user_density = params.get(""density_function"", density)\n\n  for i,key in enumerate(keys):\n    print(""cell"", i+1, ""/"", len(keys), end=\'\\r\')\n    cell_image = render(cells[key], params, metadata, layout, summary)\n    # stitch this rendering into the tile image\n    ci = key[0] % params[""n_tile""]\n    cj = key[1] % params[""n_tile""]\n    xmin = ci*params[""cell_size""]\n    ymin = cj*params[""cell_size""]\n    xmax = (ci+1)*params[""cell_size""]\n    ymax = (cj+1)*params[""cell_size""]\n\n    if params.get(""scale_density"", False):\n      cell_density = user_density(cells[key], metadata)\n      # scale = density/summary[""max_density""]\n      # for now, user_max_density will be the same as max_density if the user didn\'t supply a fn\n      scale = math.log(cell_density)/(math.log(summary[""user_max_density""]) or 1)\n      owidth = xmax - xmin\n      width = int(round(owidth * scale))\n      if(width < 1):\n        width = 1\n      offsetL = int(round((owidth - width)/2))\n      offsetR = owidth - width - offsetL # handle odd numbers\n      # print(""\\n"")\n      # print(""width"", width, offsetL, offsetR)\n      box = [xmin + offsetL, ymin + offsetL, xmax - offsetR, ymax - offsetR]\n      resample = params.get(""scale_type"", Image.NEAREST)\n      cell_image = cell_image.resize(size=(width,width), resample=resample)\n      # print(cell_image)\n    else:\n      box = [xmin, ymin, xmax, ymax]\n\n    # print(""box"", box)\n    tile.paste(cell_image, box)\n  print(""\\n"")\n  return tile\n\n\ndef aggregate_tile(cells, ti, tj, aggregate, params, metadata, layout, summary):\n  """"""\n    Call the user defined aggregation function on each cell and combine into a single json object\n  """"""\n  tile = []\n  keys = cells.keys()\n  for i,key in enumerate(keys):\n    print(""cell"", i+1, ""/"", len(keys), end=\'\\r\')\n    cell_json = aggregate(cells[key], params, metadata, layout, summary)\n    tile.append({""aggregate"":cell_json, ""i"":int(key[0]), ""j"":int(key[1])})\n  return tile\n '"
lucid/scratch/pretty_graphs/__init__.py,0,b''
lucid/scratch/pretty_graphs/format_graph.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nimport lucid.modelzoo.vision_models as models\nimport lucid.optvis.objectives as objectives\nimport lucid.optvis.param as param\nimport lucid.optvis.render as render\nfrom lucid.misc.io import show, load\nfrom lucid.misc.io.showing import _image_url, _display_html\nfrom collections import defaultdict\n\nfrom lucid.scratch.pretty_graphs.graph import *\n\nclass Fragment(object):\n\n  def __init__(self, svg, shape, node, offset=None ):\n    self.svg    = svg\n    self.shape = shape\n    self.node = node\n    self.offset = [0,0] if offset is None else offset\n\n  def render(self):\n    return """"""<g transform=""translate(%s,%s)"">%s</g>"""""" % (self.offset[0], self.offset[1], self.svg)\n\n  def shift(self, delta):\n    self.offset[0] += delta[0]\n    self.offset[1] += delta[1]\n\n  @property\n  def fragments(self):\n    return [self]\n\n  @property\n  def box(self):\n    x = self.offset[0]\n    y = self.offset[1]\n    return [[x, x+self.shape[0]], [y, y+self.shape[1]]]\n\n\nclass FragmentContainer(object):\n\n  def __init__(self, children, shape):\n    self.children = children\n    self.shape = shape\n\n\n  def shift(self, delta):\n    for child in self.children:\n      child.shift(delta)\n\n  def pad(self, pad):\n    for child in self.children:\n      child.shift([pad[0][0], pad[1][0]])\n\n    self.shape[0] += pad[0][0] + pad[0][1]\n    self.shape[0] += pad[1][0] + pad[1][1]\n\n  @property\n  def fragments(self):\n    frags = []\n    for child in self.children:\n      frags += child.fragments\n    return frags\n\n  @property\n  def box(self):\n    boxes = [child.box for child in self.children]\n    x_min = min(box[0][0] for box in boxes)\n    x_max = max(box[0][1] for box in boxes)\n    y_min = min(box[1][0] for box in boxes)\n    y_max = max(box[1][1] for box in boxes)\n\n    return [[x_min, x_max], [y_min, y_max]]\n\n  def show(self, show_bounds=False):\n\n    shape = self.shape\n    container = [shape[0] + 20, shape[1] + 20]\n    svg = ""\\n  "".join(node.render() for node in self.fragments)\n\n    if show_bounds:\n      bound_rect = """"""<rect x=%s y=%s width=%s height=%s fill=""#E55"" style=""opacity: 0.3""> </rect>"""""" % (0, 0, shape[0], shape[1])\n    else:\n      bound_rect = """"\n\n    _display_html(""""""\n      <style>.node, .background {fill: #B8D8FF; } .node:hover, .background:hover {fill: rgb(117, 172, 240);} .background {opacity: 0.7;}</style>\n      <svg width=%s height=%s>\n        %s\n        %s\n      </svg>\n    """""" % (container[0], container[1], bound_rect, svg))\n\n\ndef one_fragement(svg, shape, node=None, inner_shape=None):\n  fragment = Fragment(svg, inner_shape or shape, node)\n  return FragmentContainer([fragment], shape)\n\n\ndef alignment_func(name):\n  if name == ""min"":\n    return lambda max_size, size: 0\n  elif name == ""max"":\n    return lambda max_size, size: max_size - size\n  elif name == ""mid"":\n    return lambda max_size, size: (max_size - size) / 2.0\n\n\nclass LayoutComponent(object):\n\n  def __init__(self):\n    self.pad = [[0,0], [0,0]]\n    self.inner_spacing = 4\n\n  def render(self):\n    return """"\n\n\nclass LayoutNode(LayoutComponent):\n\n  def __init__(self, node):\n    self.pad = [[0,0], [0,0]]\n    self.inner_spacing = 4\n    self.node = node\n\n  @property\n  def contained_nodes(self):\n    return [self.node]\n\n  def render(self):\n    if self.node == None:\n      inner = """"\n      shape = [10, 20]\n    elif self.node.op in [""Placeholder"", ""Softmax""]:\n      inner = ""<polygon points=\\""0,10 5,20 15,20 20,10 15,0 5,0 0,10\\"" %s></polygon>""\n      shape = [20,20]\n#     elif self.node.op in [""Concat"", ""ConcatV2""]:\n#       inner = """"\n#       shape = [3, 20]\n\n    elif self.node.op in [""Concat"", ""ConcatV2""]:\n      inner = ""<rect width=10 height=20 rx=2 ry=2 x=4 %s></rect>""\n      shape = [14, 20]\n    elif self.node.op in [""MaxPool"", ""AvgPool""]:\n      inner = ""<polygon points=\\"" 0,0 0,20 3,20 10,12 10,7 3,0 0,0 \\"" %s></polygon>""\n      shape = [10,20]\n    elif self.node.op in [""Add""]:\n      inner = ""<circle r=6 cx=5 cy=10 %s></circle>""\n      shape = [12,20]\n    else:\n      inner = ""<rect width=10 height=20 rx=2 ry=2 %s></rect>""\n      shape = [10, 20]\n\n    if ""%s"" in inner:\n      info = """"""class=""node"" data-tf-name=""%s"" data-tf-op=""%s"" """""" % (self.node.name, self.node.op)\n      inner = inner % info\n\n    inner = ""<g transform=\\""translate(%s, %s)\\"">%s</g>"" % (self.pad[0][0], self.pad[1][0], inner)\n    orig_shape = shape[:]\n    shape[0] += self.pad[0][0] + self.pad[0][1]\n    shape[1] += self.pad[1][0] + self.pad[1][1]\n    return one_fragement(inner, shape, node=self.node, inner_shape=orig_shape)\n\n\nclass LayoutBranch(LayoutComponent):\n\n  def __init__(self, branches):\n    self.pad = [[0,0], [0,0]]\n    self.inner_spacing = 4\n    self.branches = branches\n    self.alignment = ""max""\n\n  @property\n  def contained_nodes(self):\n    return sum([branch.contained_nodes for branch in self.branches], [])\n\n  def render(self):\n    rendered_nodes = [node.render() for node in self.branches]\n\n    align_func = alignment_func(self.alignment)\n\n    svg   = """"\n    max_x = max(node.shape[0] for node in rendered_nodes)\n    delta_y = 0\n\n    for rendered_node in rendered_nodes:\n      delta_x = align_func(max_x, rendered_node.shape[0])\n      rendered_node.shift([delta_x, delta_y])\n      delta_y += rendered_node.shape[1] + self.inner_spacing\n\n    if len(rendered_nodes):\n      delta_y -= self.inner_spacing\n\n    new_container = FragmentContainer(rendered_nodes, [max_x, delta_y])\n    new_container.pad(self.pad)\n\n    return new_container\n\n\nclass LayoutSeq(LayoutComponent):\n\n  def __init__(self, nodes):\n    self.pad = [[0,0], [0,0]]\n    self.inner_spacing = 4\n    self.nodes = nodes\n    self.alignment = ""mid""\n\n  @property\n  def contained_nodes(self):\n    return sum([branch.contained_nodes for branch in self.nodes], [])\n\n  def render(self):\n    rendered_nodes = [node.render() for node in self.nodes]\n\n    align_func = alignment_func(self.alignment)\n\n    svg   = """"\n    max_y = max(node.shape[1] for node in rendered_nodes)\n    delta_x = 0\n\n    for rendered_node in rendered_nodes:\n      delta_y = align_func(max_y, rendered_node.shape[1])\n      rendered_node.shift([delta_x, delta_y])\n      delta_x += rendered_node.shape[0] + self.inner_spacing\n\n    if len(rendered_nodes):\n      delta_x -= self.inner_spacing\n\n    new_container = FragmentContainer(rendered_nodes, [delta_x, max_y])\n    new_container.pad(self.pad)\n\n    return new_container\n\ndef parse_graph(graph):\n\n  node_prevs = {}\n  for node in graph.nodes:\n    node_prevs[node.name] = set(node.inputs)\n    for inp in node.inputs:\n      node_prevs[node.name] |= node_prevs[inp.name]\n\n  node_posts = {}\n  for node in reversed(graph.nodes):\n    node_posts[node.name] = set(node.consumers)\n    for inp in node.consumers:\n      node_posts[node.name] |= node_posts[inp.name]\n\n\n  def GCA(node):\n    branches = node.inputs\n    branch_nodes  = [set([node]) | node_prevs[node.name] for node in branches]\n    branch_shared =  set.intersection(*branch_nodes)\n    return max(branch_shared, key=lambda n: graph.nodes.index(n))\n\n  def MCP(node):\n    branches = node.consumers\n    branch_nodes  = [set([node]) | node_posts[node.name] for node in branches]\n    branch_shared =  set.intersection(*branch_nodes)\n    return min(branch_shared, key=lambda n: graph.nodes.index(n))\n\n  def sorted_nodes(nodes):\n    return sorted(nodes, key = lambda n: graph.nodes.index(n))\n\n  def nodes_between(a, b):\n    return (node_prevs[b.name] - node_prevs[a.name]) | set([a,b])\n\n  def parse_node(node):\n    return LayoutNode(node)\n\n  def fake_node():\n    return LayoutNode(None)\n\n  def parse_list(a, b):\n    seq = []\n    pres = b\n    while pres is not a:\n      prev = GCA(pres)\n      seq.append(parse_node(pres))\n      if prev not in pres.inputs or len(pres.inputs) > 1:\n        seq.append(parse_branch(prev, pres))\n      pres = prev\n    seq += [parse_node(a)]\n    return LayoutSeq(list(reversed(seq)))\n\n\n  def parse_branch(start, stop):\n    assert GCA(stop) == start\n\n    branches = stop.inputs\n    branch_nodes  = [ (set([node]) | node_prevs[node.name])\n                     - (set([start]) | node_prevs[start.name])\n                     for node in branches]\n\n\n    assert all([len(b1 & b2) == 0\n                for b1 in branch_nodes\n                for b2 in branch_nodes if b1 != b2])\n\n    ret = []\n    for nodes in branch_nodes:\n      nodes_seq = sorted_nodes(nodes)\n      if len(nodes) > 1:\n        ret.append(parse_list(nodes_seq[0], nodes_seq[-1]))\n      elif len(nodes) == 1:\n        ret.append(parse_node(nodes_seq[0]))\n      else:\n        ret.append(fake_node())\n\n    return LayoutBranch(ret)\n\n  return parse_list(graph.nodes[0], graph.nodes[-1])\n\n\n\ndef render_with_groups(seq, groups, bg_pad=6, pad_diff=8, pad_none=2):\n\n  for child in seq.nodes:\n    matched_groups = [root.name for root, group_set in list(groups.items())\n                      if any(grandchild in group_set for grandchild in child.contained_nodes)]\n    match_group = matched_groups[0] if matched_groups else None\n    child.group = match_group\n\n\n  for child1, child2 in zip(seq.nodes[:-1], seq.nodes[1:]):\n    if child1.group != child2.group:\n      child1.pad[0][1] += pad_diff if child1.group != None else pad_none\n      child2.pad[0][0] += pad_diff if child2.group != None else pad_none\n\n    elif child1.group == None:\n      child1.pad[0][1] += pad_none\n      child2.pad[0][0] += pad_none\n\n  fragment_container = seq.render()\n  fragments = fragment_container.fragments\n\n  for frag in fragments:\n    frag.shift([bg_pad, bg_pad])\n\n\n  # Generate groups\n  used = []\n  group_joined_frags = []\n  for root, group_nodes in reversed(list(groups.items())):\n    group_frags = [frag for frag in fragments if frag.node in group_nodes]\n    used += group_frags\n    box = FragmentContainer(group_frags, []).box\n    bg_info = ""class=\\""background\\"" data-group-tf-name=\\""%s\\"" "" % root.name\n    bg_svg = """"""\n      <g transform=""translate(%s, %s)"">\n        <rect width=%s height=%s rx=2 ry=2 %s></rect>\n      </g>"""""" % (box[0][0] - bg_pad, box[1][0] - bg_pad, box[0][1] - box[0][0] + 2*bg_pad, box[1][1] - box[1][0] + 2*bg_pad, bg_info)\n    svgs = [frag.render() for frag in group_frags] + [bg_svg]\n    svg = """"""<g>%s</g>"""""" % ""\\n  "".join(svgs)\n    group_joined_frags.append(Fragment(svg, [], None))\n\n  unused = [frag for frag in fragments if frag not in used]\n\n  all_final_node_frags = unused+group_joined_frags\n\n\n  # Make edges\n  node_boxes = {}\n  nodes = []\n  for frag in fragments:\n    if frag.node:\n      node_boxes[frag.node] = frag.box\n      nodes.append(frag.node)\n\n  edges = []\n  def p(point, dx=0):\n    return ""%s,%s"" % (point[0]+dx, point[1])\n\n  for node in nodes:\n    for inp in node.inputs:\n      box1 = node_boxes[inp]\n      mid1 = [sum(box1[0] + box1[0][1:])/3., sum(box1[1])/2.]\n      box2 = node_boxes[node]\n      end = [sum(box2[0] + box2[0][:1])/3., sum(box2[1])/2.]\n\n\n      if end[0] - mid1[0] > 50:\n        mid2 = [mid1[0] + 30, end[1]]\n        dx2  = -15\n      if end[0] - mid1[0] > 20:\n        mid2 = [mid1[0] + 20, end[1]]\n        dx2  = -8\n      else:\n        mid2 = end[:]\n        dx2 = -6\n\n      d = ""M %s C %s %s %s L %s"" % (p(mid1), p(mid1, dx=6), p(mid2, dx=dx2), p(mid2), p(end))\n      info = ""data-source-tf-name=\\""%s\\"" data-dest-tf-name=\\""%s\\"" class=\\""edge\\"" "" % (inp.name, node.name)\n      path = ""<path d=\\""%s\\"" style=\\""stroke: #DDD; stroke-width: 1.5px; fill: none;\\"" %s ></path>"" % (d, info)\n      frag = Fragment(path, [], None)\n      edges.append(frag)\n\n\n  #new_container = FragmentContainer(edges + all_final_node_grags, fragment_container.shape)\n  final_fragments = edges + all_final_node_frags\n  inners = [frag.render() for frag in final_fragments]\n  return {""svg_inner"" : ""\\n"".join(inners), ""shape"" : fragment_container.shape, ""node_boxes"" : node_boxes }\n\n\ndef complete_render_model_graph(model, custom_ops=None):\n  graph = Graph.from_graphdef(model.graph_def)\n  include_ops = standard_include_ops\n  if custom_ops is not None:\n    include_ops += custom_ops\n  graph = filter_graph_ops(graph, include_ops=include_ops)\n  graph = filter_graph_dynamic(graph)\n  graph = filter_graph_collapse_sequence(graph, [""Conv2D"", ""Relu""])\n  parsed_graph = parse_graph(graph)\n  #parsed_graph.render().show()\n  if ""Resnet"" in model.model_path:\n    print((parsed_graph.alignment))\n    parsed_graph.alignment = ""min""\n\n  groups = find_groups(graph)\n\n  return render_with_groups(parsed_graph, groups)\n\n'"
lucid/scratch/pretty_graphs/graph.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nimport lucid.modelzoo.vision_models as models\nimport lucid.optvis.objectives as objectives\nimport lucid.optvis.param as param\nimport lucid.optvis.render as render\nfrom lucid.misc.io import show, load\nfrom lucid.misc.io.showing import _image_url, _display_html\nfrom collections import defaultdict\n\n\nclass Node(object):\n\n  def __init__(self, name, op, graph, pretty_name=None):\n    self.name = name\n    self.op = op\n    self.graph = graph\n    self.pretty_name = pretty_name\n\n  def __repr__(self):\n    return ""<%s: %s>"" % (self.name, self.op)\n\n  @property\n  def inputs(self):\n    return self.graph.node_to_inputs[self.name]\n\n  @property\n  def consumers(self):\n    return self.graph.node_to_consumers[self.name]\n\n  def copy(self):\n    return Node(self.name, self.op, self.graph)\n\n\n\nclass Graph(object):\n\n  def __init__(self):\n    self.nodes = []\n    self.name_map = {}\n    self.node_to_consumers = defaultdict(lambda: [])\n    self.node_to_inputs = defaultdict(lambda: [])\n\n  def add_node(self, node):\n    self.nodes.append(node)\n    self.name_map[node.name] = node\n\n  def add_edge(self, node1, node2):\n    node1, node2 = self[node1], self[node2]\n    self.node_to_consumers[node1.name].append(node2)\n    self.node_to_inputs[node2.name].append(node1)\n\n  def __getitem__(self, index):\n    if isinstance(index, str):\n      return self.name_map[index]\n    elif isinstance(index, Node):\n      return self.name_map[index.name]\n    else:\n      raise Exception(""Unsupported index for Graph"", type(index) )\n\n  def graphviz(self, groups=None):\n    print(""digraph G {"")\n    if groups is not None:\n        for root, group in groups.items():\n          print("""")\n          print((""  subgraph"", ""cluster_%s"" % root.name.replace(""/"", ""_""), ""{""))\n          print((""  label = \\""%s\\"""") % (root.pretty_name or root.name))\n          for node in group:\n            print((""    \\""%s\\"""") % (node.pretty_name or node.name))\n          print(""  }"")\n    for node in self.nodes:\n      for inp in node.inputs:\n        print((""  "", \'""\' + (inp.pretty_name or inp.name) + \'""\', "" -> "", \'""\' + (node.pretty_name or node.name) + \'""\'))\n    print(""}"")\n\n  @staticmethod\n  def from_graphdef(graphdef):\n\n    graph = Graph()\n\n    for raw_node in graphdef.node:\n      graph.add_node(Node(raw_node.name, raw_node.op, graph))\n\n    for raw_node in graphdef.node:\n      for raw_inp in raw_node.input:\n        if raw_inp.startswith(\'^\'):  # skip control inputs\n          continue\n        raw_inp_name = raw_inp.split("":"")[0]\n        graph.add_edge(raw_inp_name, raw_node.name)\n\n    return graph\n\n\ndef filter_graph(graph, keep_nodes, pass_through=True):\n\n  new_graph = Graph()\n\n  for node in graph.nodes:\n    if node.name in keep_nodes:\n      new_node = node.copy()\n      new_node.graph = new_graph\n      new_node.subsumed = []\n      new_graph.add_node(new_node)\n\n  def kept_inputs(node):\n    ret = []\n    visited = []\n\n    def walk(inp):\n      if inp in visited: return\n      visited.append(inp)\n      if inp.name in keep_nodes:\n        ret.append(inp)\n      else:\n        if pass_through:\n          new_graph[node].subsumed.append(inp.name)\n          for inp2 in inp.inputs:\n            walk(inp2)\n\n    for inp in node.inputs:\n      walk(inp)\n\n    return ret\n\n  for node in graph.nodes:\n    if node.name in keep_nodes:\n      for inp in kept_inputs(node):\n        new_graph.add_edge(inp, node)\n\n  return new_graph\n\n\nstandard_include_ops = [""Placeholder"", ""Relu"", ""Relu6"", ""Add"", ""Split"", ""Softmax"", ""Concat"", ""ConcatV2"", ""Conv2D"", ""MaxPool"", ""AvgPool"", ""MatMul""] # Conv2D\n\n\ndef filter_graph_ops(graph, include_ops=standard_include_ops):\n  keep_nodes = [node.name for node in graph.nodes if node.op in include_ops]\n  return filter_graph(graph, keep_nodes)\n\n\ndef filter_graph_cut_shapes(graph):\n  keep_nodes = [node.name for node in graph.nodes if node.op != ""Shape""]\n  return filter_graph(graph, keep_nodes, pass_through=False)\n\n\ndef filter_graph_dynamic(graph):\n\n  dynamic_nodes = []\n\n  def recursive_walk_forward(node):\n    if node.name in dynamic_nodes: return\n    dynamic_nodes.append(node.name)\n    for next in node.consumers:\n      recursive_walk_forward(next)\n\n  recursive_walk_forward(graph.nodes[0])\n  return filter_graph(graph, dynamic_nodes)\n\n\ndef filter_graph_collapse_sequence(graph, sequence):\n  exclude_nodes = []\n\n  for node in graph.nodes:\n    remainder = sequence[:]\n    matches = []\n    while remainder:\n      if len(node.consumers) > 1 and len(remainder) > 1:\n        break\n      if node.op == remainder[0]:\n        matches.append(node.name)\n        node = node.consumers[0]\n        remainder = remainder[1:]\n      else:\n        break\n    if len(remainder) == 0:\n      exclude_nodes += matches[:-1]\n\n  include_nodes = [node.name for node in graph.nodes\n                   if node.name not in exclude_nodes]\n\n  return filter_graph(graph, include_nodes)\n\n\ndef clip_node_names(graph, prefix):\n\n  new_graph = Graph()\n  for node in graph.nodes:\n    new_node = node.copy()\n    new_node.graph = new_graph\n    new_node.subsumed = []\n    new_graph.add_node(new_node)\n    for inp in node.inputs:\n      new_graph.add_edge(inp, new_node)\n\n  for node in new_graph.nodes:\n    if node.name.startswith(prefix):\n      node.pretty_name = node.name[len(prefix):]\n\n  return new_graph\n\n\ndef find_groups(graph):\n\n  node_successors = {}\n  for node in graph.nodes:\n    node_successors[node.name] = set(node.inputs)\n    for inp in node.inputs:\n      node_successors[node.name] |= node_successors[inp.name]\n\n  concat_nodes = [node for node in graph.nodes\n                  if node.op in [""Concat"", ""ConcatV2"", ""Add""] and len(node.inputs) > 1]\n\n  groups = {}\n  group_children = set()\n  for root_node in concat_nodes:\n    branch_heads = root_node.inputs\n    branch_nodes = [set([node]) | node_successors[node.name] for node in branch_heads]\n    branch_shared = set.intersection(*branch_nodes)\n    branch_uniq = set.union(*branch_nodes) - branch_shared\n    groups[root_node] = set([root_node]) | branch_uniq\n    group_children |= branch_uniq\n\n  for root in list(groups.keys()):\n    if root in group_children:\n      del groups[root]\n\n  return groups\n'"
lucid/scratch/pretty_graphs/visualizations.py,0,"b'import numpy as np\nimport tensorflow as tf\nimport json\n\nimport lucid.modelzoo.vision_models as models\nfrom lucid.misc.io.showing import _image_url, _display_html\nfrom lucid.scratch.pretty_graphs.format_graph import complete_render_model_graph\nfrom lucid.misc.io import show, load\n\n\ndef display_model(model, tooltips=None):\n  tooltips = tooltips or {};\n  render = complete_render_model_graph(model)\n\n  svg = """"""\n    <script src=""https://d3js.org/d3.v5.min.js""></script>\n    <style>\n      .node, .background {\n        fill: hsla(213, 80%%, 85%%,1);\n      }\n      .node:hover, .background:hover {\n        fill: hsla(213, 60%%, 70%%, 1);\n      }\n      .background {\n        opacity: 0.7;\n      }\n      .tooltip {\n        position: absolute;\n        background: #F8F8FA;\n        padding: 6px;\n        border-radius: 4px;\n        transition: opacity 0.2s, left 0.2s, top 0.2s;\n      }\n    </style>\n\n    <div style=""width: %spx; height: %spx; position: relative"" id=""container"">\n    <svg width=""100%%"" height=""100%%"" style=""position: absolute; top: 0px; left:0px;"">\n\n    <g transform=""translate(0, 0)"" class=""model"">\n      %s\n    </g>\n    </svg>\n    </div>\n    """""" % (render[""shape""][0] + 200, render[""shape""][1] + 40,  render[""svg_inner""])\n  _display_html(svg)\n\n\ndef display_tooltips_name(node_selector, tooltip_key_attr, container_selector=""#output-body""):\n  html = """"""\n  <script>\n    var nodes = d3.selectAll(""{node_selector}"");\n    var container = d3.select(""{container_selector}"");\n    var tooltip_div = container.append(""div"");\n    tooltip_div.classed(""tooltip"", true).style(""opacity"", ""0.0"");\n\n    function show_node() {{\n      var node = d3.select(this);\n      var svg_box = container.node().getBoundingClientRect();\n      var box = node.node().getBoundingClientRect();\n      var name = node.attr(""{tooltip_key_attr}"");\n\n\n      tooltip_div.html(name);\n      tooltip_div.style(""opacity"", ""1.0"")\n        .style(""left"", (box.x + 4 - svg_box.x)+""px"")\n        .style(""top"", (box.y + box.height + 12 - svg_box.y)+""px"");\n\n    }}\n\n    d3.selectAll(""{node_selector}"")\n      .on(""mouseover"", show_node)\n      .on(""mouseout"", () => tooltip_div.style(""opacity"", ""0.0"") );\n    </script>\n    """""".format(**locals())\n  #print(html)\n  _display_html(html)\n\n\ndef display_tooltips(node_selector, tooltip_key_attr, tooltips, container_selector=""#output-body""):\n  tooltips_json = json.dumps(tooltips)\n  html = """"""\n  <script>\n    var nodes = d3.selectAll(""{node_selector}"");\n    var container = d3.select(""{container_selector}"");\n    var tooltip_div = container.append(""div"");\n    tooltip_div.classed(""tooltip"", true).style(""opacity"", ""0.0"");\n\n    var tooltips = {tooltips_json};\n\n    function show_node() {{\n      var node = d3.select(this);\n      var svg_box = container.node().getBoundingClientRect();\n      var box = node.node().getBoundingClientRect();\n      var name = node.attr(""{tooltip_key_attr}"");\n\n      if (name in tooltips){{\n        tooltip_div.html(tooltips[name]);\n        tooltip_div.style(""opacity"", ""1.0"")\n          .style(""left"", (box.x + 4 - svg_box.x)+""px"")\n          .style(""top"", (box.y + box.height + 12 - svg_box.y)+""px"");\n      }}\n    }}\n\n    d3.selectAll(""{node_selector}"")\n      .on(""mouseover"", show_node)\n      .on(""mouseout"", () => tooltip_div.style(""opacity"", ""0.0"") );\n    </script>\n    """""".format(**locals())\n  #print(html)\n  _display_html(html)\n\n\ndef get_box(render, name):\n  box_map = render[""node_boxes""]\n  matches = [node for node in list(box_map.keys()) if node.name == name]\n  match = matches[0]\n  box = box_map[match]\n  return box\n\n\ndef p(point, dy=0):\n  return ""%s,%s"" % (point[0], point[1] + dy)\n\n\ndef model_align_display(model1, model2, lines, middle_sep=150):\n\n\n  render1 = complete_render_model_graph(model1)\n  render2 = complete_render_model_graph(model2)\n  shape1, shape2 = render1[""shape""], render2[""shape""]\n  inner1, inner2 = render1[""svg_inner""], render2[""svg_inner""]\n  W = max(shape1[0], shape2[0]) + 20\n  H = shape1[1] + middle_sep + shape2[1] + 20\n\n  paths = []\n  for (name1, name2), weight in list(lines.items()):\n    box1, box2 = get_box(render1, name1), get_box(render2, name2)\n    start = [(box1[0][0]+box1[0][1])/2., shape1[1] + 15 + 10]\n    end = [(box2[0][0]+box2[0][1])/2., shape1[1] + middle_sep - 10]\n    d = ""M %s C %s %s %s"" % (p(start), p(start, dy=50), p(end, dy=-50), p(end))\n    style = ""stroke: hsla(20, %s%%, 70%%, %s%%); stroke-width: %spx; "" % (100*(0.5 + weight/2.0), 100*weight, 3*weight)\n    info = ""data-tf-src=\\""%s\\"" data-tf-dest=\\""%s\\"" class=\\""comparison-edge\\"" "" % (name1, name2)\n    path = ""<path d=\\""%s\\"" style=\\""%s\\"" %s ></path>"" % (d, style, info)\n    paths.append(path)\n\n  svg = """"""\n    <script src=""https://d3js.org/d3.v5.min.js""></script>\n    <style>\n      .node, .background {\n        fill: hsla(213, 80%%, 85%%,1);\n      }\n      .node:hover, .background:hover {\n        fill: hsla(213, 60%%, 70%%, 1);\n      }\n      .background {\n        opacity: 0.7;\n      }\n      .comparison-edge {\n        fill: none;\n        stroke-linecap: round;\n        transition: opacity 0.2s;\n      }\n      .tooltip {\n        position: absolute;\n        background: #F8F8FA;\n        padding: 6px;\n        border-radius: 4px;\n        transition: opacity 0.2s, left 0.2s, top 0.2s;\n      }\n    </style>\n\n    <svg width=%s height=%s>\n\n    <g transform=""translate(0, 0)"" id=""compare-top"" class=""model"">\n      %s\n    </g>\n    <g>\n      %s\n    </g>\n    <g transform=""translate(0, %s)"" id=""compare-bottom"" class=""model"">\n      %s\n    </g>\n    </svg>\n\n    <script>\n    var edges = d3.selectAll("".comparison-edge"");\n\n    function hide_edges_top() {{\n      console.log(""test"")\n      var node = d3.select(this);\n      var name = node.attr(""data-tf-name"") || node.attr(""data-group-tf-name"");\n      edges.each(function(d) {\n        var pres = d3.select(this);\n        var src = pres.attr(""data-tf-src"");\n        pres.style(""opacity"", (src == name)? 1.0 : 0.0)\n      })\n    }}\n\n    function hide_edges_bottom() {{\n      var node = d3.select(this);\n      var name = node.attr(""data-tf-name"") || node.attr(""data-group-tf-name"");\n      edges.each(function(d) {\n        var pres = d3.select(this);\n        var src = pres.attr(""data-tf-dest"");\n        pres.style(""opacity"", (src == name)? 1.0 : 0.0)\n      })\n    }}\n\n    d3.selectAll(""#compare-top .node, #compare-top .background"")\n      .on(""mouseover.edges"", hide_edges_top)\n      .on(""mouseout.edges"", () => edges.style(""opacity"", ""1.0"") );\n\n    d3.selectAll(""#compare-bottom .node, #compare-bottom .background"")\n      .on(""mouseover.edges"", hide_edges_bottom)\n      .on(""mouseout.edges"", () => edges.style(""opacity"", ""1.0"") );\n    </script>\n    """""" % (W, H, inner1, ""\\n"".join(paths), shape1[1] + middle_sep, inner2)\n\n  _display_html(svg)\n\n\ndef vpad(space):\n  _display_html(""""""<div style=""width:20px; height: %spx""></div>"""""" % space)\n'"
lucid/scratch/rl_util/__init__.py,0,"b'import numpy as np\nimport tensorflow as tf\nimport sys\nimport importlib\nfrom lucid.modelzoo.vision_base import Model\nfrom lucid.misc.channel_reducer import ChannelReducer\nimport lucid.optvis.param as param\nimport lucid.optvis.objectives as objectives\nimport lucid.optvis.render as render\nimport lucid.optvis.transform as transform\nfrom lucid.misc.io import show, save\nfrom lucid.misc.io.showing import _image_url, _display_html\n\ntry:\n    import lucid.scratch.web.svelte as lucid_svelte\nexcept NameError:\n    lucid_svelte = None\nfrom .joblib_wrapper import load_joblib, save_joblib\nfrom .util import (\n    zoom_to,\n    get_var,\n    get_shape,\n    concatenate_horizontally,\n    hue_to_rgb,\n    channels_to_rgb,\n    conv2d,\n    norm_filter,\n    brightness_to_opacity,\n)\nfrom .attribution import (\n    gradient_override_map,\n    maxpool_override,\n    get_acts,\n    get_grad_or_attr,\n    get_attr,\n    get_grad,\n    get_paths,\n    get_multi_path_attr,\n)\nfrom .nmf import argmax_nd, LayerNMF, rescale_opacity\n\n\ndef all_():\n    return __all__\n\n\ndef reload(globals_dict):\n    m = importlib.reload(sys.modules[__name__])\n    for f in m.__all__:\n        globals_dict.update({f: getattr(m, f)})\n\n\n__all__ = [\n    ""np"",\n    ""tf"",\n    ""Model"",\n    ""ChannelReducer"",\n    ""param"",\n    ""objectives"",\n    ""render"",\n    ""transform"",\n    ""show"",\n    ""save"",\n    ""_image_url"",\n    ""_display_html"",\n    ""lucid_svelte"",\n    ""load_joblib"",\n    ""save_joblib"",\n    ""zoom_to"",\n    ""get_var"",\n    ""get_shape"",\n    ""concatenate_horizontally"",\n    ""hue_to_rgb"",\n    ""channels_to_rgb"",\n    ""conv2d"",\n    ""norm_filter"",\n    ""brightness_to_opacity"",\n    ""gradient_override_map"",\n    ""maxpool_override"",\n    ""get_acts"",\n    ""get_grad_or_attr"",\n    ""get_attr"",\n    ""get_grad"",\n    ""get_paths"",\n    ""get_multi_path_attr"",\n    ""argmax_nd"",\n    ""LayerNMF"",\n    ""rescale_opacity"",\n    ""all_"",\n    ""reload"",\n]\n'"
lucid/scratch/rl_util/arch.py,14,"b'import tensorflow as tf\n\n\ndef clear_cnn(unscaled_images, batch_norm=False):\n    """"""A simple convolutional architecture designed with interpretability in\n    mind:\n\n    - Later convolutional layers have been replaced with dense layers, to allow\n        for non-visual processing\n    - There are no residual connections, so that the flow of information passes\n        through every layer\n    - A pool size equal to the stride has been used, to avoid gradient gridding\n    - L2 pooling has been used instead of max pooling, for more continuous\n        gradients\n\n    Batch norm has been optionally included to help with optimization.\n    """"""\n\n    def conv_layer(out, filters, kernel_size):\n        out = tf.layers.conv2d(\n            out, filters, kernel_size, padding=""same"", activation=None\n        )\n        if batch_norm:\n            out = tf.layers.batch_normalization(out)\n        out = tf.nn.relu(out)\n        return out\n\n    def pool_l2(out, pool_size):\n        return tf.sqrt(\n            tf.layers.average_pooling2d(\n                out ** 2, pool_size=pool_size, strides=pool_size, padding=""same""\n            )\n            + 1e-8\n        )\n\n    out = tf.cast(unscaled_images, tf.float32) / 255.0\n    with tf.variable_scope(""1a""):\n        out = conv_layer(out, 16, 7)\n        out = pool_l2(out, 2)\n    with tf.variable_scope(""2a""):\n        out = conv_layer(out, 32, 5)\n    with tf.variable_scope(""2b""):\n        out = conv_layer(out, 32, 5)\n        out = pool_l2(out, 2)\n    with tf.variable_scope(""3a""):\n        out = conv_layer(out, 32, 5)\n        out = pool_l2(out, 2)\n    with tf.variable_scope(""4a""):\n        out = conv_layer(out, 32, 5)\n        out = pool_l2(out, 2)\n    out = tf.layers.flatten(out)\n    out = tf.layers.dense(out, 256, activation=tf.nn.relu)\n    out = tf.layers.dense(out, 512, activation=tf.nn.relu)\n    return out\n'"
lucid/scratch/rl_util/attribution.py,17,"b'import numpy as np\nimport tensorflow as tf\nimport lucid.optvis.render as render\nimport itertools\nfrom lucid.misc.gradient_override import gradient_override_map\n\n\ndef maxpool_override():\n    def MaxPoolGrad(op, grad):\n        inp = op.inputs[0]\n        op_args = [\n            op.get_attr(""ksize""),\n            op.get_attr(""strides""),\n            op.get_attr(""padding""),\n        ]\n        smooth_out = tf.nn.avg_pool(inp ** 2, *op_args) / (\n            1e-2 + tf.nn.avg_pool(tf.abs(inp), *op_args)\n        )\n        inp_smooth_grad = tf.gradients(smooth_out, [inp], grad)[0]\n        return inp_smooth_grad\n\n    return {""MaxPool"": MaxPoolGrad}\n\n\ndef get_acts(model, layer_name, obses):\n    with tf.Graph().as_default(), tf.Session():\n        t_obses = tf.placeholder_with_default(\n            obses.astype(np.float32), (None, None, None, None)\n        )\n        T = render.import_model(model, t_obses, t_obses)\n        t_acts = T(layer_name)\n        return t_acts.eval()\n\n\ndef get_grad_or_attr(\n    model,\n    layer_name,\n    prev_layer_name,\n    obses,\n    *,\n    act_dir=None,\n    act_poses=None,\n    score_fn=tf.reduce_sum,\n    grad_or_attr,\n    override=None,\n    integrate_steps=1\n):\n    with tf.Graph().as_default(), tf.Session(), gradient_override_map(override or {}):\n        t_obses = tf.placeholder_with_default(\n            obses.astype(np.float32), (None, None, None, None)\n        )\n        T = render.import_model(model, t_obses, t_obses)\n        t_acts = T(layer_name)\n        if prev_layer_name is None:\n            t_acts_prev = t_obses\n        else:\n            t_acts_prev = T(prev_layer_name)\n        if act_dir is not None:\n            t_acts = act_dir[None, None, None] * t_acts\n        if act_poses is not None:\n            t_acts = tf.gather_nd(\n                t_acts,\n                tf.concat([tf.range(obses.shape[0])[..., None], act_poses], axis=-1),\n            )\n        t_score = score_fn(t_acts)\n        t_grad = tf.gradients(t_score, [t_acts_prev])[0]\n        if integrate_steps > 1:\n            acts_prev = t_acts_prev.eval()\n            grad = (\n                sum(\n                    [\n                        t_grad.eval(feed_dict={t_acts_prev: acts_prev * alpha})\n                        for alpha in np.linspace(0, 1, integrate_steps + 1)[1:]\n                    ]\n                )\n                / integrate_steps\n            )\n        else:\n            acts_prev = None\n            grad = t_grad.eval()\n        if grad_or_attr == ""grad"":\n            return grad\n        elif grad_or_attr == ""attr"":\n            if acts_prev is None:\n                acts_prev = t_acts_prev.eval()\n            return acts_prev * grad\n        else:\n            raise NotImplementedError\n\n\ndef get_attr(model, layer_name, prev_layer_name, obses, **kwargs):\n    kwargs[""grad_or_attr""] = ""attr""\n    return get_grad_or_attr(model, layer_name, prev_layer_name, obses, **kwargs)\n\n\ndef get_grad(model, layer_name, obses, **kwargs):\n    kwargs[""grad_or_attr""] = ""grad""\n    return get_grad_or_attr(model, layer_name, None, obses, **kwargs)\n\n\ndef get_paths(acts, nmf, *, max_paths, integrate_steps):\n    acts_reduced = nmf.transform(acts)\n    residual = acts - nmf.inverse_transform(acts_reduced)\n    combs = itertools.combinations(range(nmf.features), nmf.features // 2)\n    if nmf.features % 2 == 0:\n        combs = np.array([comb for comb in combs if 0 in comb])\n    else:\n        combs = np.array(list(combs))\n    if max_paths is None:\n        splits = combs\n    else:\n        num_splits = min((max_paths + 1) // 2, combs.shape[0])\n        splits = combs[\n            np.random.choice(combs.shape[0], size=num_splits, replace=False), :\n        ]\n    for i, split in enumerate(splits):\n        indices = np.zeros(nmf.features)\n        indices[split] = 1.0\n        indices = indices[tuple(None for _ in range(acts_reduced.ndim - 1))]\n        complements = [False, True]\n        if max_paths is not None and i * 2 + 1 == max_paths:\n            complements = [np.random.choice(complements)]\n        for complement in complements:\n            path = []\n            for alpha in np.linspace(0, 1, integrate_steps + 1)[1:]:\n                if complement:\n                    coordinates = (1.0 - indices) * alpha ** 2 + indices * (\n                        1.0 - (1.0 - alpha) ** 2\n                    )\n                else:\n                    coordinates = indices * alpha ** 2 + (1.0 - indices) * (\n                        1.0 - (1.0 - alpha) ** 2\n                    )\n                path.append(\n                    nmf.inverse_transform(acts_reduced * coordinates) + residual * alpha\n                )\n            yield path\n\n\ndef get_multi_path_attr(\n    model,\n    layer_name,\n    prev_layer_name,\n    obses,\n    prev_nmf,\n    *,\n    act_dir=None,\n    act_poses=None,\n    score_fn=tf.reduce_sum,\n    override=None,\n    max_paths=50,\n    integrate_steps=10\n):\n    with tf.Graph().as_default(), tf.Session(), gradient_override_map(override or {}):\n        t_obses = tf.placeholder_with_default(\n            obses.astype(np.float32), (None, None, None, None)\n        )\n        T = render.import_model(model, t_obses, t_obses)\n        t_acts = T(layer_name)\n        if prev_layer_name is None:\n            t_acts_prev = t_obses\n        else:\n            t_acts_prev = T(prev_layer_name)\n        if act_dir is not None:\n            t_acts = act_dir[None, None, None] * t_acts\n        if act_poses is not None:\n            t_acts = tf.gather_nd(\n                t_acts,\n                tf.concat([tf.range(obses.shape[0])[..., None], act_poses], axis=-1),\n            )\n        t_score = score_fn(t_acts)\n        t_grad = tf.gradients(t_score, [t_acts_prev])[0]\n        acts_prev = t_acts_prev.eval()\n        path_acts = get_paths(\n            acts_prev, prev_nmf, max_paths=max_paths, integrate_steps=integrate_steps\n        )\n        deltas_of_path = lambda path: np.array(\n            [b - a for a, b in zip([np.zeros_like(acts_prev)] + path[:-1], path)]\n        )\n        grads_of_path = lambda path: np.array(\n            [t_grad.eval(feed_dict={t_acts_prev: acts}) for acts in path]\n        )\n        path_attrs = map(\n            lambda path: (deltas_of_path(path) * grads_of_path(path)).sum(axis=0),\n            path_acts,\n        )\n        total_attr = 0\n        num_paths = 0\n        for attr in path_attrs:\n            total_attr += attr\n            num_paths += 1\n        return total_attr / num_paths\n'"
lucid/scratch/rl_util/joblib_wrapper.py,0,"b'from lucid.misc.io.saving import nullcontext\nfrom lucid.misc.io.loading import load_using_loader\nfrom lucid.misc.io.writing import write_handle\n\n\ndef load_joblib(url_or_handle, *, cache=None, **kwargs):\n    import joblib\n\n    return load_using_loader(\n        url_or_handle,\n        decompressor=nullcontext,\n        loader=joblib.load,\n        cache=cache,\n        **kwargs\n    )\n\n\ndef save_joblib(value, url_or_handle, **kwargs):\n    import joblib\n\n    if hasattr(url_or_handle, ""write"") and hasattr(url_or_handle, ""name""):\n        joblib.dump(value, url_or_handle, **kwargs)\n    else:\n        with write_handle(url_or_handle) as handle:\n            joblib.dump(value, handle, **kwargs)\n'"
lucid/scratch/rl_util/nmf.py,0,"b'import numpy as np\nfrom functools import reduce\nimport scipy.ndimage as nd\nfrom lucid.misc.channel_reducer import ChannelReducer\nimport lucid.optvis.param as param\nimport lucid.optvis.objectives as objectives\nimport lucid.optvis.render as render\nimport lucid.optvis.transform as transform\nfrom .attribution import get_acts, get_attr\n\n\ndef argmax_nd(x, axes, *, max_rep=np.inf, max_rep_strict=None):\n    assert max_rep > 0\n    assert np.isinf(max_rep) or max_rep_strict is not None\n    perm = list(range(len(x.shape)))\n    for axis in reversed(axes):\n        loc = perm.index(axis)\n        perm = [axis] + perm[:loc] + perm[loc + 1 :]\n    x = x.transpose(perm)\n    shape = x.shape\n    axes_size = reduce(lambda a, b: a * b, shape[: len(axes)], 1)\n    x = x.reshape([axes_size, -1])\n    indices = np.argsort(-x, axis=0)\n    result = indices[0].copy()\n    counts = np.zeros(len(indices), dtype=int)\n    unique_values, unique_counts = np.unique(result, return_counts=True)\n    counts[unique_values] = unique_counts\n    for i in range(1, len(indices) + (0 if max_rep_strict else 1)):\n        order = np.argsort(x[result, range(len(result))])\n        result_in_order = result[order]\n        current_counts = counts.copy()\n        changed = False\n        for j in range(len(order)):\n            value = result_in_order[j]\n            if current_counts[value] > max_rep:\n                pos = order[j]\n                new_value = indices[i % len(indices)][pos]\n                result[pos] = new_value\n                current_counts[value] -= 1\n                counts[value] -= 1\n                counts[new_value] += 1\n                changed = True\n        if not changed:\n            break\n    result = result.reshape(shape[len(axes) :])\n    return np.unravel_index(result, shape[: len(axes)])\n\n\nclass LayerNMF:\n    def __init__(\n        self,\n        model,\n        layer_name,\n        obses,\n        obses_full=None,\n        features=10,\n        *,\n        attr_layer_name=None,\n        attr_opts={""integrate_steps"": 10},\n    ):\n        self.model = model\n        self.layer_name = layer_name\n        self.obses = obses\n        self.obses_full = obses_full\n        if self.obses_full is None:\n            self.obses_full = obses\n        self.features = features\n        self.pad_h = 0\n        self.pad_w = 0\n        self.padded_obses = self.obses_full\n        if self.features is None:\n            self.reducer = None\n        else:\n            self.reducer = ChannelReducer(features)\n        acts = get_acts(model, layer_name, obses)\n        self.patch_h = self.obses_full.shape[1] / acts.shape[1]\n        self.patch_w = self.obses_full.shape[2] / acts.shape[2]\n        if self.reducer is None:\n            self.acts_reduced = acts\n            self.channel_dirs = np.eye(self.acts_reduced.shape[-1])\n            self.transform = lambda acts: acts.copy()\n            self.inverse_transform = lambda acts: acts.copy()\n        else:\n            if attr_layer_name is None:\n                self.acts_reduced = self.reducer.fit_transform(acts)\n            else:\n                attrs = get_attr(model, attr_layer_name, layer_name, obses, **attr_opts)\n                attrs_signed = np.concatenate(\n                    [np.maximum(0, attrs), np.maximum(0, -attrs)], axis=0\n                )\n                self.reducer.fit(attrs_signed)\n                self.acts_reduced = self.reducer.transform(acts)\n            self.channel_dirs = self.reducer._reducer.components_\n            self.transform = lambda acts: self.reducer.transform(acts)\n            self.inverse_transform = lambda acts_r: ChannelReducer._apply_flat(\n                self.reducer._reducer.inverse_transform, acts_r\n            )\n\n    def vis_traditional(\n        self,\n        feature_list=None,\n        *,\n        transforms=[transform.jitter(2)],\n        l2_coeff=0.0,\n        l2_layer_name=None,\n    ):\n        if feature_list is None:\n            feature_list = list(range(self.acts_reduced.shape[-1]))\n        try:\n            feature_list = list(feature_list)\n        except TypeError:\n            feature_list = [feature_list]\n\n        obj = sum(\n            [\n                objectives.direction_neuron(\n                    self.layer_name, self.channel_dirs[feature], batch=feature\n                )\n                for feature in feature_list\n            ]\n        )\n        if l2_coeff != 0.0:\n            assert (\n                l2_layer_name is not None\n            ), ""l2_layer_name must be specified if l2_coeff is non-zero""\n            obj -= objectives.L2(l2_layer_name) * l2_coeff\n        param_f = lambda: param.image(64, batch=len(feature_list))\n        return render.render_vis(\n            self.model, obj, param_f=param_f, transforms=transforms\n        )[-1]\n\n    def pad_obses(self, *, expand_mult=1):\n        pad_h = np.ceil(self.patch_h * expand_mult).astype(int)\n        pad_w = np.ceil(self.patch_w * expand_mult).astype(int)\n        if pad_h > self.pad_h or pad_w > self.pad_w:\n            self.pad_h = pad_h\n            self.pad_w = pad_w\n            self.padded_obses = (\n                np.indices(\n                    (\n                        self.obses_full.shape[1] + self.pad_h * 2,\n                        self.obses_full.shape[2] + self.pad_w * 2,\n                    )\n                ).sum(axis=0)\n                % 2\n            )\n            self.padded_obses = self.padded_obses * 0.25 + 0.75\n            self.padded_obses = self.padded_obses.astype(self.obses_full.dtype)\n            self.padded_obses = self.padded_obses[None, ..., None]\n            self.padded_obses = self.padded_obses.repeat(\n                self.obses_full.shape[0], axis=0\n            )\n            self.padded_obses = self.padded_obses.repeat(3, axis=-1)\n            self.padded_obses[\n                :, self.pad_h : -self.pad_h, self.pad_w : -self.pad_w, :\n            ] = self.obses_full\n\n    def get_patch(self, obs_index, pos_h, pos_w, *, expand_mult=1):\n        left_h = self.pad_h + (pos_h - 0.5 * expand_mult) * self.patch_h\n        right_h = self.pad_h + (pos_h + 0.5 * expand_mult) * self.patch_h\n        left_w = self.pad_w + (pos_w - 0.5 * expand_mult) * self.patch_w\n        right_w = self.pad_w + (pos_w + 0.5 * expand_mult) * self.patch_w\n        slice_h = slice(int(round(left_h)), int(round(right_h)))\n        slice_w = slice(int(round(left_w)), int(round(right_w)))\n        return self.padded_obses[obs_index, slice_h, slice_w]\n\n    def vis_dataset(self, feature, *, subdiv_mult=1, expand_mult=1, top_frac=0.1):\n        acts_h, acts_w = self.acts_reduced.shape[1:3]\n        zoom_h = subdiv_mult - (subdiv_mult - 1) / (acts_h + 2)\n        zoom_w = subdiv_mult - (subdiv_mult - 1) / (acts_w + 2)\n        acts_subdiv = self.acts_reduced[..., feature]\n        acts_subdiv = np.pad(acts_subdiv, [(0, 0), (1, 1), (1, 1)], mode=""edge"")\n        acts_subdiv = nd.zoom(acts_subdiv, [1, zoom_h, zoom_w], order=1, mode=""nearest"")\n        acts_subdiv = acts_subdiv[:, 1:-1, 1:-1]\n        if acts_subdiv.size == 0:\n            raise RuntimeError(\n                f""subdiv_mult of {subdiv_mult} too small for ""\n                f""{self.acts_reduced.shape[1]}x{self.acts_reduced.shape[2]} ""\n                ""activations""\n            )\n        poses = np.indices((acts_h + 2, acts_w + 2)).transpose((1, 2, 0))\n        poses = nd.zoom(\n            poses.astype(float), [zoom_h, zoom_w, 1], order=1, mode=""nearest""\n        )\n        poses = poses[1:-1, 1:-1, :] - 0.5\n        with np.errstate(divide=""ignore""):\n            max_rep = np.ceil(\n                np.divide(\n                    acts_subdiv.shape[1] * acts_subdiv.shape[2],\n                    acts_subdiv.shape[0] * top_frac,\n                )\n            )\n        obs_indices = argmax_nd(\n            acts_subdiv, axes=[0], max_rep=max_rep, max_rep_strict=False\n        )[0]\n        self.pad_obses(expand_mult=expand_mult)\n        patches = []\n        patch_acts = np.zeros(obs_indices.shape)\n        for i in range(obs_indices.shape[0]):\n            patches.append([])\n            for j in range(obs_indices.shape[1]):\n                obs_index = obs_indices[i, j]\n                pos_h, pos_w = poses[i, j]\n                patch = self.get_patch(obs_index, pos_h, pos_w, expand_mult=expand_mult)\n                patches[i].append(patch)\n                patch_acts[i, j] = acts_subdiv[obs_index, i, j]\n        patch_acts_max = patch_acts.max()\n        opacities = patch_acts / (1 if patch_acts_max == 0 else patch_acts_max)\n        for i in range(obs_indices.shape[0]):\n            for j in range(obs_indices.shape[1]):\n                opacity = opacities[i, j][None, None, None]\n                opacity = opacity.repeat(patches[i][j].shape[0], axis=0)\n                opacity = opacity.repeat(patches[i][j].shape[1], axis=1)\n                patches[i][j] = np.concatenate([patches[i][j], opacity], axis=-1)\n        return (\n            np.concatenate(\n                [np.concatenate(patches[i], axis=1) for i in range(len(patches))],\n                axis=0,\n            ),\n            obs_indices.tolist(),\n        )\n\n    def vis_dataset_thumbnail(\n        self, feature, *, num_mult=1, expand_mult=1, max_rep=None\n    ):\n        if max_rep is None:\n            max_rep = num_mult\n        if self.acts_reduced.shape[0] < num_mult ** 2:\n            raise RuntimeError(\n                f""At least {num_mult ** 2} observations are required to produce""\n                "" a thumbnail visualization.""\n            )\n        acts_feature = self.acts_reduced[..., feature]\n        pos_indices = argmax_nd(\n            acts_feature, axes=[1, 2], max_rep=max_rep, max_rep_strict=True\n        )\n        acts_single = acts_feature[\n            range(acts_feature.shape[0]), pos_indices[0], pos_indices[1]\n        ]\n        obs_indices = np.argsort(-acts_single, axis=0)[: num_mult ** 2]\n        coords = np.array(list(zip(*pos_indices)), dtype=[(""h"", int), (""w"", int)])[\n            obs_indices\n        ]\n        indices_order = np.argsort(coords, axis=0, order=(""h"", ""w""))\n        indices_order = indices_order.reshape((num_mult, num_mult))\n        for i in range(num_mult):\n            indices_order[i] = indices_order[i][\n                np.argsort(coords[indices_order[i]], axis=0, order=""w"")\n            ]\n        obs_indices = obs_indices[indices_order]\n        poses = np.array(pos_indices).transpose()[obs_indices] + 0.5\n        self.pad_obses(expand_mult=expand_mult)\n        patches = []\n        patch_acts = np.zeros((num_mult, num_mult))\n        patch_shapes = []\n        for i in range(num_mult):\n            patches.append([])\n            for j in range(num_mult):\n                obs_index = obs_indices[i, j]\n                pos_h, pos_w = poses[i, j]\n                patch = self.get_patch(obs_index, pos_h, pos_w, expand_mult=expand_mult)\n                patches[i].append(patch)\n                patch_acts[i, j] = acts_single[obs_index]\n                patch_shapes.append(patch.shape)\n        patch_acts_max = patch_acts.max()\n        opacities = patch_acts / (1 if patch_acts_max == 0 else patch_acts_max)\n        patch_min_h = np.array([s[0] for s in patch_shapes]).min()\n        patch_min_w = np.array([s[1] for s in patch_shapes]).min()\n        for i in range(num_mult):\n            for j in range(num_mult):\n                opacity = opacities[i, j][None, None, None]\n                opacity = opacity.repeat(patches[i][j].shape[0], axis=0)\n                opacity = opacity.repeat(patches[i][j].shape[1], axis=1)\n                patches[i][j] = np.concatenate([patches[i][j], opacity], axis=-1)\n                patches[i][j] = patches[i][j][:patch_min_h, :patch_min_w]\n        return (\n            np.concatenate(\n                [np.concatenate(patches[i], axis=1) for i in range(len(patches))],\n                axis=0,\n            ),\n            obs_indices.tolist(),\n        )\n\n\ndef rescale_opacity(\n    images, min_opacity=15 / 255, opaque_frac=0.1, max_scale=10, keep_zeros=False\n):\n    images_orig = images\n    images = images_orig.copy()\n    opacities_flat = images[..., 3].reshape(\n        images.shape[:-3] + (images.shape[-3] * images.shape[-2],)\n    )\n    opaque_threshold = np.percentile(opacities_flat, (1 - opaque_frac) * 100, axis=-1)\n    opaque_threshold = np.maximum(\n        opaque_threshold, np.amax(opacities_flat, axis=-1) / max_scale\n    )[..., None, None]\n    opaque_threshold[opaque_threshold == 0] = 1\n    images[..., 3] = images[..., 3] * (1 - min_opacity) / opaque_threshold\n    images[..., 3] = np.minimum(1, min_opacity + images[..., 3])\n    if keep_zeros:\n        images[..., 3][images_orig[..., 3] == 0] = 0\n    return images\n'"
lucid/scratch/rl_util/util.py,7,"b'import numpy as np\nimport tensorflow as tf\nimport scipy.ndimage as nd\nimport lucid.optvis.render as render\nfrom lucid.misc.io.collapse_channels import hue_to_rgb\n\n\ndef zoom_to(img, width):\n    n = width // img.shape[-2] + 1\n    img = img.repeat(n, axis=-3).repeat(n, axis=-2)\n    r = float(width) / img.shape[-2]\n    zoom = [1] * (img.ndim - 3) + [r, r, 1]\n    return nd.zoom(img, zoom, order=0, mode=""nearest"")\n\n\ndef get_var(model, var_name):\n    with tf.Graph().as_default(), tf.Session():\n        t_obses = tf.placeholder(dtype=tf.float32, shape=(None, None, None, None))\n        T = render.import_model(model, t_obses, t_obses)\n        return T(var_name).eval()\n\n\ndef get_shape(model, node_name):\n    with tf.Graph().as_default():\n        t_obses = tf.placeholder(dtype=tf.float32, shape=(None, None, None, None))\n        T = render.import_model(model, t_obses, t_obses)\n        return T(node_name).get_shape().as_list()\n\n\ndef concatenate_horizontally(images):\n    images = np.asarray(images)\n    return images.transpose((1, 0, 2, 3)).reshape(\n        (1, images.shape[1], images.shape[0] * images.shape[2], images.shape[3])\n    )\n\n\ndef channels_to_rgb(X, warp=True):\n    assert (X >= 0).all()\n\n    K = X.shape[-1]\n\n    rgb = 0\n    for i in range(K):\n        ang = 360 * i / K\n        color = hue_to_rgb(ang, warp=warp)\n        color = color[tuple(None for _ in range(len(X.shape) - 1))]\n        rgb += X[..., i, None] * color\n\n    return rgb\n\n\ndef conv2d(input_, filter_):\n    assert input_.ndim == 4, (\n        ""input_ must have 4 dimensions ""\n        ""corresponding to batch, height, width and channels""\n    )\n    assert (\n        filter_.ndim == 2\n    ), ""filter_ must have 2 dimensions and will be applied channelwise""\n    with tf.Graph().as_default(), tf.Session():\n        filter_ = tf.tensordot(filter_, np.eye(input_.shape[-1]), axes=[[], []])\n        return tf.nn.conv2d(\n            input_, filter=filter_, strides=[1, 1, 1, 1], padding=""SAME""\n        ).eval()\n\n\ndef norm_filter(length, norm_ord=2, norm_func=lambda n: np.exp(-n), clip=True):\n    arr = np.indices((length, length)) - ((length - 1) / 2)\n    func1d = lambda x: norm_func(np.linalg.norm(x, ord=norm_ord))\n    result = np.apply_along_axis(func1d, axis=0, arr=arr)\n    if clip:\n        bound = np.amax(np.amin(result, axis=0), axis=0)\n        result *= np.logical_or(result >= bound, np.isclose(result, bound, atol=0))\n    return result\n\n\ndef brightness_to_opacity(image):\n    assert image.shape[-1] == 3\n    brightness = np.apply_along_axis(\n        lambda x: np.linalg.norm(x, ord=2), axis=-1, arr=image\n    )[..., None]\n    brightness = np.minimum(1, brightness)\n    image = np.divide(\n        image, brightness, out=np.zeros_like(image), where=(brightness != 0)\n    )\n    return np.concatenate([image, brightness], axis=-1)\n'"
lucid/scratch/scripts/get_model_layers.py,3,"b'import lucid.optvis.render as render\n\n\ndef predecessors(graphdef, name):\n  node_map = {}\n  for n in graphdef.node:\n    node_map[n.name] = n\n    \n  seen = []\n  def get_predecessors(node_name):\n    if node_name not in node_map: return []\n    node = node_map[node_name]\n    preds = []\n    for inp in node.input:\n      if inp in seen: continue\n      seen.append(inp)\n      preds.append(inp)\n      inp_preds = get_predecessors(inp)\n      preds += inp_preds\n    return list(set(preds))\n  \n  return get_predecessors(name)\n\n\ndef get_branch_nodes(graphdef, concat_node):\n  branch_head_names = concat_node.input[:]\n  branch_head_preds = [predecessors(graphdef, name) + [name] for name in branch_head_names]\n  uniqs = []\n  for n in range(len(branch_head_preds)):\n    pres_preds = branch_head_preds[n]\n    other_preds = branch_head_preds[:n] + branch_head_preds[n+1:]\n    uniqs += [name for name in pres_preds\n             if not any(name in others for others in other_preds)]\n  return uniqs\n  \n  \ndef propose_layers(graphdef):\n  concats = [node for node in graphdef.node\n             if node.op in [""Concat"", ""ConcatV2"", ""Add""]]\n  branch_nodes = []\n  for node in concats:\n    branch_nodes += get_branch_nodes(graphdef, node)\n  \n  layer_proposals = [node for node in graphdef.node\n             if node.op in [""Relu"", ""Concat"", ""ConcatV2"", ""Softmax"", ""Add""] and node.name not in branch_nodes]\n\n  return layer_proposals\n  \n  \ndef propose_layers_with_shapes(model):\n  proposed_layers = propose_layers(model.graph_def)\n  with tf.Graph().as_default(), tf.Session() as sess:\n    t_input = tf.placeholder(tf.float32, [1] + model.image_shape)\n    T = render.import_model(model, t_input, t_input)\n    t_shapes = [tf.shape(T(node.name))[1:] for node in proposed_layers]\n    shapes = sess.run(t_shapes, {t_input: np.zeros([1] + model.image_shape)})\n  return zip(proposed_layers, shapes)\n  \n  \ndef print_model_layer_code(model):\n  print(""layers = ["")\n  for node, shape in propose_layers_with_shapes(model):\n      layer_type = ""conv"" if len(shape) == 3 else ""dense""\n      name = node.name.encode(""ascii"", ""replace"")\n      layer = {""name"": name, ""type"": layer_type, ""size"": shape[-1]}\n      print(""  "", layer, "","")\n  print("" ]"")\n'"
lucid/scratch/web/__init__.py,0,b''
lucid/scratch/web/observable.py,0,"b'import json\nfrom lucid.misc.io.showing import _display_html\n\ndef renderObservable(url, cells=None, data=None):\n  """"""Display observable notebook cells in iPython.\n\n  Args:\n    url: url fragment to observable notebook. ex: \'@observablehq/downloading-and-embedding-notebooks\'\n    cells: an array of strings for the names of cells you want to render. ex: [\'viewof stage\', \'viewof x\']\n    data: a dictionary of variables that you\'d like to overwrite. ex: {\'x\': 200, \'width\': 500}\n  """"""\n\n  head = """"""\n  <div id=""output""></div>\n  <div>\n    <a target=""_blank"" href=\'https://observablehq.com/{}\'>source</a>\n  </div>\n  <script type=""module"">\n  """""".format(url)\n  \n  runtimeImport = ""import {Runtime} from \'https://unpkg.com/@observablehq/notebook-runtime?module\';""\n  \n  notebookImport = ""import notebook from \'https://api.observablehq.com/{0}.js\';"".format(url)\n  \n  cellsSerialized = ""let cells = {};"".format(json.dumps(cells))\n  dataSerialized = ""let data = {};"".format(json.dumps(data))\n  \n  code = """"""\n  const outputEl = document.getElementById(""output"");\n  \n  // Converts data into a map\n  let dataMap = new Map();\n  if (data) {\n    Object.keys(data).forEach(key => {\n      dataMap.set(key, data[key]);\n    });\n  }\n  \n  // Converts cells into a map\n  let cellsMap = new Map();\n  if (cells) {\n    cells.forEach((key, i) => {\n      const element = document.createElement(""div"");\n      outputEl.appendChild(element)\n      cellsMap.set(key, element)\n    });\n  }\n  \n  function render(_node, value) {\n    if (!(value instanceof Element)) {\n      const el = document.createElement(""span"");\n      el.innerHTML = value;\n      value = el;\n    }\n    if (_node.firstChild !== value) {\n      if (_node.firstChild) {\n        while (_node.lastChild !== _node.firstChild) _node.removeChild(_node.lastChild);\n        _node.replaceChild(value, _node.firstChild);\n      } else {\n        _node.appendChild(value);\n      }\n    }\n  }\n  \n  Runtime.load(notebook, (variable) => {\n  \n    // Override a variable with a passed value\n    if (dataMap.has(variable.name)) {\n      variable.value = dataMap.get(variable.name)\n    }\n    \n    // Render the output to the corrent element\n    if (cellsMap.has(variable.name)) {\n      return { fulfilled: (value) => render(cellsMap.get(variable.name), value) }; \n    } else {\n      return true;\n    }\n    \n  });\n  """"""\n  \n  foot = ""</script>""\n  \n  _display_html(\n      head + runtimeImport + notebookImport + cellsSerialized + dataSerialized + code + foot\n  )\n'"
lucid/scratch/web/svelte.py,0,"b'import random\nimport json\nimport tempfile\nimport subprocess\nimport os.path as osp\nimport uuid\n\nfrom IPython.core.magic import register_cell_magic\n\nfrom lucid.misc.io.showing import _display_html\nfrom lucid.misc.io.reading import read\n\n_svelte_temp_dir = tempfile.mkdtemp(prefix=""svelte_"")\n\n_template = """"""\n  <div id=\'$id\'></div>\n  <script>\n  $js\n  </script>\n  <script>\n    var app = new $name({\n        target: document.querySelector(\'#$id\'),\n        data: $data,\n      });\n  </script>\n """"""\n\ndef js_id(name):\n  # name_str will become the name of a javascript variable, and can\'t contain dashes.\n  return name + ""_"" + str(uuid.uuid4()).replace(\'-\', \'_\')\n\ndef build_svelte(html_fname):\n  js_fname = html_fname.replace("".html"", "".js"")\n  cmd = ""svelte compile --format iife "" + html_fname + "" > "" + js_fname\n  print(cmd)\n  try:\n    print(subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT))\n  except subprocess.CalledProcessError as exception:\n    print(""Svelte build failed! Output:\\n{}"".format(exception.output.decode()))\n  return js_fname\n\n\ndef SvelteComponent(name, path):\n  """"""Display svelte components in iPython.\n\n  Args:\n    name: name of svelte component (must match component filename when built)\n    path: path to compile svelte .js file or source svelte .html file.\n      (If html file, we try to call svelte and build the file.)\n\n  Returns:\n    A function mapping data to a rendered svelte component in ipython.\n  """"""\n  if path[-3:] == "".js"":\n    js_path = path\n  elif path[-5:] == "".html"":\n    print(""Trying to build svelte component from html..."")\n    js_path = build_svelte(path)\n  js_content = read(js_path, mode=\'r\')\n  def inner(data):\n    id_str = js_id(name)\n    html = _template \\\n        .replace(""$js"", js_content) \\\n        .replace(""$name"", name) \\\n        .replace(""$data"", json.dumps(data)) \\\n        .replace(""$id"", id_str)\n    _display_html(html)\n  return inner\n\n\n@register_cell_magic\ndef html_define_svelte(line, cell):\n  base_name = line.split()[0]\n  id_str = js_id(base_name)\n  html_fname = osp.join(_svelte_temp_dir, id_str + "".html"")\n  with open(html_fname, ""w"") as f:\n    f.write(cell)\n  component = SvelteComponent(id_str, html_fname)\n  globals()[base_name] = component\n'"
tests/misc/io/test_loading.py,0,"b'# -*- coding: UTF-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport pytest\n\nimport numpy as np\nfrom lucid.misc.io.loading import load\nfrom lucid.misc.io.scoping import io_scope\nimport io\n\ntest_images = [\n    ""./tests/fixtures/rgbeye.png"",\n    ""./tests/fixtures/noise_uppercase.PNG"",\n    ""./tests/fixtures/rgbeye.jpg"",\n    ""./tests/fixtures/noise.jpeg"",\n    ""./tests/fixtures/image.xyz"",\n]\n\ndef test_load_json():\n  path = ""./tests/fixtures/dictionary.json""\n  dictionary = load(path)\n  assert ""key"" in dictionary\n\n\ndef test_load_text():\n  path = ""./tests/fixtures/string.txt""\n  string = load(path)\n  assert u""\xf0\x9f\x90\x95"" in string\n\n\ndef test_load_multiline_text_as_list():\n  path = ""./tests/fixtures/multiline.txt""\n  string_list = load(path, split=True)\n  assert isinstance(string_list, list)\n  assert all(isinstance(string, ("""".__class__, u"""".__class__)) for string in string_list)\n\n\ndef test_load_npy():\n  path = ""./tests/fixtures/array.npy""\n  array = load(path)\n  assert array.shape is not None\n\n\ndef test_load_npz():\n  path = ""./tests/fixtures/arrays.npz""\n  arrays = load(path)\n  assert isinstance(arrays, np.lib.npyio.NpzFile)\n\n\n@pytest.mark.parametrize(""path"", test_images)\ndef test_load_image(path):\n  image = load(path)\n  assert image.shape is not None\n  assert all(dimension > 2 for dimension in image.shape)\n\n\n@pytest.mark.parametrize(""path"", [\n  ""./tests/fixtures/rgbeye.png"",\n  ""./tests/fixtures/noise.jpeg"",\n])\ndef test_load_image_resized(path):\n  image = load(path)\n  assert image.shape is not None\n  assert all(dimension > 2 for dimension in image.shape)\n\n\ndef test_load_garbage_with_unknown_extension():\n  path = ""./tests/fixtures/string.XYZ""\n  with pytest.raises(RuntimeError):\n    load(path)\n\n\ndef test_load_json_with_file_handle():\n  path = ""./tests/fixtures/dictionary.json""\n  with io.open(path, \'r\') as handle:\n    dictionary = load(handle)\n  assert ""key"" in dictionary\n\n\ndef test_load_protobuf():\n  path = ""./tests/fixtures/graphdef.pb""\n  graphdef = load(path)\n  assert ""int_val: 42"" in repr(graphdef)\n\n\ndef test_batch_load():\n    image_names = [os.path.basename(image) for image in test_images]\n    with io_scope(\'./tests/fixtures\'):\n        images = load(image_names)\n    assert len(images) == len(test_images)\n    for i in range(len(test_images)):\n        assert np.allclose(load(test_images[i]), images[i])\n\n'"
tests/misc/io/test_reading.py,0,"b'# -*- coding: UTF-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport pytest\n\nfrom lucid.misc.io.reading import read, read_handle\nimport os.path\nimport io\n\npath = ""./tests/fixtures/string.txt""\nstring = u""The quick brown fox jumps over the lazy \xf0\x9f\x90\x95""\nio.open(path, \'w\', encoding=""utf-8"").write(string)\n\n\ndef test_read_txt_file():\n  content = read(path, encoding=\'utf-8\')\n  assert content == string\n\n\ndef test_read_handle_txt_file():\n  with read_handle(path) as handle:\n    content1 = handle.read().decode(\'utf-8\')\n  assert content1 == string\n\n\ndef test_read_handle_behaves_like_file():\n  with read_handle(path) as handle:\n    content1 = handle.read()\n    handle.seek(0)\n    content2 = handle.read()\n  assert content1 == content2\n\n\ndef test_read_binary_file():\n  path = ""./tests/fixtures/bytes""\n  content = read(path)\n  golden_content = io.open(path, \'rb\').read()\n  assert content == golden_content\n\n\ndef test_read_remote_url(mocker):\n  path = ""https://example.com/example.html""\n  golden = b""42""\n  mock_urlopen = mocker.patch(\'urllib.request.urlopen\',\n    return_value=io.BytesIO(golden))\n\n  content = read(path, cache=False)\n\n  mock_urlopen.assert_called_once_with(path)\n  assert content == golden\n'"
tests/misc/io/test_saving.py,2,"b'import time\nimport pytest\nimport numpy as np\nfrom lucid.misc.io.saving import save, CaptureSaveContext, batch_save\nfrom lucid.misc.io.loading import load\nfrom lucid.misc.io.scoping import io_scope, current_io_scopes\nfrom concurrent.futures import ThreadPoolExecutor\nimport os.path\nimport io\nimport tensorflow as tf\n\n\ndictionary = {""key"": ""value""}\ndictionary_json = """"""{\n  ""key"": ""value""\n}""""""\narray1 = np.eye(10, 10)\narray2 = np.dstack([np.eye(10, 10, k=i - 1) for i in range(3)])\n\n\ndef _remove(path):\n    try:\n        os.remove(path)\n    except OSError:\n        pass\n\n\ndef test_save_json():\n    path = ""./tests/fixtures/generated_outputs/dictionary.json""\n    _remove(path)\n    save(dictionary, path)\n    assert os.path.isfile(path)\n    content = io.open(path, ""rt"").read()\n    assert content == dictionary_json\n\n\ndef test_save_npy():\n    path = ""./tests/fixtures/generated_outputs/array.npy""\n    _remove(path)\n    save(array1, path)\n    assert os.path.isfile(path)\n    re_read_array = np.load(path)\n    assert np.array_equal(array1, re_read_array)\n\n\ndef test_save_npz_array():\n    path = ""./tests/fixtures/generated_outputs/arrays.npz""\n    _remove(path)\n    save([array1, array2], path)\n    assert os.path.isfile(path)\n    re_read_arrays = np.load(path)\n    assert all(arr in re_read_arrays for arr in (""arr_0"", ""arr_1""))\n    assert np.array_equal(array1, re_read_arrays[""arr_0""])\n    assert np.array_equal(array2, re_read_arrays[""arr_1""])\n\n\ndef test_save_npz_dict():\n    path = ""./tests/fixtures/generated_outputs/arrays.npz""\n    _remove(path)\n    arrays = {""array1"": array1, ""array2"": array2}\n    save(arrays, path)\n    assert os.path.isfile(path)\n    re_read_arrays = np.load(path)\n    assert all(arr in re_read_arrays for arr in list(arrays))\n    assert np.array_equal(arrays[""array1""], re_read_arrays[""array1""])\n\n\ndef test_save_image_png():\n    path = ""./tests/fixtures/generated_outputs/rgbeye.png""\n    _remove(path)\n    save(array2, path)\n    assert os.path.isfile(path)\n\n\ndef test_save_image_jpg():\n    path = ""./tests/fixtures/generated_outputs/rgbeye.jpg""\n    _remove(path)\n    save(array2, path)\n    assert os.path.isfile(path)\n\n\ndef test_save_array_txt():\n    path = ""./tests/fixtures/generated_outputs/multiline.txt""\n    _remove(path)\n    stringarray = [""Line {:d}"".format(i) for i in range(10)]\n    save(stringarray, path)\n    assert os.path.isfile(path)\n\n\ndef test_save_txt():\n    path = ""./tests/fixtures/generated_outputs/multiline.txt""\n    _remove(path)\n    string = """".join([""Line {:d}\\n"".format(i) for i in range(10)])\n    save(string, path)\n    assert os.path.isfile(path)\n\n\ndef test_save_named_handle():\n    path = ""./tests/fixtures/generated_outputs/rgbeye.jpg""\n    _remove(path)\n    with io.open(path, ""wb"") as handle:\n        save(array2, handle)\n    assert os.path.isfile(path)\n\n\ndef test_save_compressed_npy():\n    uncompressed_path = ""./tests/fixtures/generated_outputs/array.npy""\n    _remove(uncompressed_path)\n    save(array2, uncompressed_path)\n    compressed_path = ""./tests/fixtures/generated_outputs/array.npy.xz""\n    _remove(compressed_path)\n    save(array2, compressed_path)\n    assert os.path.isfile(uncompressed_path)\n    assert os.path.isfile(compressed_path)\n    re_read_array = load(compressed_path)\n    assert np.array_equal(array2, re_read_array)\n    uncompressed_size = os.path.getsize(uncompressed_path)\n    compressed_size = os.path.getsize(compressed_path)\n    assert compressed_size < uncompressed_size\n\n\ndef test_save_load_pickle():\n    path = ""./tests/fixtures/generated_outputs/some_data.pickle""\n    data = {\n        \'test\': [1, 2, 3, ""some string""],\n        \'numpy_values\': array2\n    }\n    _remove(path)\n    with io.open(path, ""wb"") as handle:\n        with pytest.raises(ValueError):\n            save(data, handle)\n        save(data, handle, allow_unsafe_formats=True)\n    assert os.path.isfile(path)\n    with pytest.raises(ValueError):\n        loaded_data = load(path)\n    loaded_data = load(path, allow_unsafe_formats=True)\n    assert loaded_data[\'test\'] == data[\'test\']\n    assert np.array_equal(loaded_data[\'numpy_values\'], data[\'numpy_values\'])\n\n\ndef test_unknown_extension():\n    with pytest.raises(ValueError):\n        save({}, ""test.unknown"")\n\n\ndef test_unknown_compressor():\n    with pytest.raises(ValueError):\n        save(array2, ""test.npy.gz"")  # .gz is not currently supported, only xy\n\n\ndef test_save_protobuf():\n    path = ""./tests/fixtures/generated_outputs/graphdef.pb""\n    _remove(path)\n    with tf.Graph().as_default():\n        a = tf.Variable(42)\n        graphdef = a.graph.as_graph_def()\n    save(graphdef, path)\n    assert os.path.isfile(path)\n\n\ndef test_write_scope_compatibility():\n    path = ""./tests/fixtures/generated_outputs/write_scope_compatibility.txt""\n    _remove(path)\n\n    with io_scope(""./tests/fixtures/generated_outputs""):\n        save(""test content"", \'write_scope_compatibility.txt\')\n\n    assert os.path.isfile(path)\n\n\ndef test_capturing_saves():\n    path = ""./tests/fixtures/generated_outputs/test_capturing_saves.txt""\n    _remove(path)\n    context = CaptureSaveContext()\n\n    with context, io_scope(""./tests/fixtures/generated_outputs""):\n        save(""test"", ""test_capturing_saves.txt"")\n\n    captured = context.captured_saves\n    assert len(captured) == 1\n    assert ""type"" in captured[0]\n    assert captured[0][""type""] == ""txt""\n\n\ndef test_threadlocal_io_scopes():\n    """""" This tests that scopes are thread local and they don\'t clobber each other when different threads are competing""""""\n    def _return_io_scope(io_scope_path):\n        with io_scope(io_scope_path):\n            time.sleep(np.random.uniform(0.05, 0.1))\n            return current_io_scopes()[-1]\n\n    n_tasks = 16\n    n_workers = 8\n    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n        futures = {executor.submit(_return_io_scope, f\'gs://test-{i}\'): f\'gs://test-{i}\' for i in range(n_tasks)}\n        results = [f.result() for f in futures]\n        assert results == list(futures.values())\n\n\ndef test_batch_saves():\n    save_ops = [(str(i), f""write_batch_{i}.txt"") for i in range(5)]\n    [_remove(f""./tests/fixtures/generated_outputs/write_batch_{i}.txt"") for i in range(5)]\n\n    context = CaptureSaveContext()\n    with context, io_scope(""./tests/fixtures/generated_outputs""):\n        results = batch_save(save_ops)\n        assert len(results) == 5\n\n    assert len(context.captured_saves) == 5\n    assert context.captured_saves[0][\'type\'] == \'txt\'\n    print(context.captured_saves)\n    assert \'write_batch_\' in context.captured_saves[0][\'url\']\n    assert all([os.path.isfile(f""./tests/fixtures/generated_outputs/write_batch_{i}.txt"") for i in range(5)])\n'"
tests/misc/io/test_showing.py,0,"b'""""""show() smoke tests\nshow relies heavily on a notebook environment, so we can only ahve some smoke\ntests in the test suite. There is also a notebook with tests that you can run\nto cover common scenarios.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport numpy as np\nimport lucid\nimport lucid.misc.io.showing as show\nimport IPython.display\n\n\ngolden_eye_html = (\'<img src=""data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAAAU\'\n  \'AAAAFCAAAAACoBHk5AAAAEklEQVR4nGP4z8DAwMDAgJ0CAErTBPw/r52mAAAAAElFTkSuQmCC"">\')\n\n\ndef test_show_image(mocker):\n  mock_display = mocker.patch(\'IPython.display.display\')\n  array = np.eye(5)\n  original = array.copy()\n\n  show.image(array)\n\n  mock_display.assert_called_once()\n  assert (original == array).all()\n\n\ndef test_show_images(mocker):\n  mock_html = mocker.patch(\'IPython.display.HTML\')\n  labels = [""one"", ""two"", ""three""]\n\n  show.images([np.eye(5)] * 3, labels=labels)\n\n  mock_html.assert_called_once()\n  args, _ = mock_html.call_args_list[0]\n  html_arg = args[0]\n  # check for img tag without closing bracket\n  assert golden_eye_html[:-1] in html_arg\n  # check that label strings are in output\n  assert all(label in html_arg for label in labels)\n\ndef test_show_textured_mesh(mocker):\n  mock_html = mocker.patch(\'IPython.display.HTML\')\n\n  texture = np.ones((16, 16, 3), np.float32)\n  old_texture = texture.copy()\n  mesh = dict(\n    position = np.float32([[0, 0, 0], [1, 0, 0], [0, 1, 0]]),\n    uv = np.float32([[0, 0], [1, 0], [0, 1]]),\n    face = np.int32([0, 1, 2])\n  )\n  show.textured_mesh(mesh, texture)\n\n  assert (texture==old_texture).all()  # check that we don\'t modify data\n  mock_html.assert_called_once()\n'"
tests/misc/io/test_writing.py,0,"b'# -*- coding: UTF-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport pytest\n\nfrom lucid.misc.io.writing import write, write_handle\nfrom lucid.misc.io.scoping import io_scope\nimport lucid.misc.io.scoping as scoping\nimport os\nimport io\n\n\nrandom_bytes = b""\\x7f\\x45\\x4c\\x46\\x01\\x01\\x01\\x00""\n\n\ndef test_write_text():\n    text = u""The quick brown fox jumps over the lazy \xf0\x9f\x90\x95""\n    path = ""./tests/fixtures/string.txt""\n\n    write(text, path, mode=""w"")\n    content = io.open(path, ""rt"").read()\n\n    assert os.path.isfile(path)\n    assert content == text\n\n\ndef test_write_bytes():\n    path = ""./tests/fixtures/bytes""\n\n    write(random_bytes, path)\n    content = io.open(path, ""rb"").read()\n\n    assert os.path.isfile(path)\n    assert content == random_bytes\n\n\ndef test_write_handle_text():\n    text = u""The quick brown \xf0\x9f\xa6\x8a jumps over the lazy dog""\n    path = ""./tests/fixtures/string2.txt""\n\n    with write_handle(path, mode=""w"") as handle:\n        handle.write(text)\n    content = io.open(path, ""rt"").read()\n\n    assert os.path.isfile(path)\n    assert content == text\n\n\ndef test_write_handle_binary():\n    path = ""./tests/fixtures/bytes""\n\n    with write_handle(path) as handle:\n        handle.write(random_bytes)\n    content = io.open(path, ""rb"").read()\n\n    assert os.path.isfile(path)\n    assert content == random_bytes\n\n\ndef test_write_scope():\n    target_path = ""./tests/fixtures/write_scope.txt""\n    if os.path.exists(target_path):\n        os.remove(target_path)\n\n    with io_scope(""./tests/fixtures""):\n        write(""test"", ""write_scope.txt"", mode=""w"")\n\n    assert os.path.isfile(target_path)\n\n\ndef test_write_scope_nested():\n    target_path = ""./tests/fixtures/write_scope4.txt""\n    if os.path.exists(target_path):\n        os.remove(target_path)\n\n    with io_scope(""./tests""):\n        with io_scope(""fixtures""):\n            write(""test"", ""write_scope4.txt"", mode=""w"")\n\n    assert os.path.isfile(target_path)\n\n\ndef test_set_write_scopes():\n    target_path = ""./tests/fixtures/write_scope5.txt""\n    if os.path.exists(target_path):\n        os.remove(target_path)\n\n    # fake write scopes, such as when called on a remote worker:\n    _old_scopes = scoping.current_io_scopes()\n    scoping.set_io_scopes([""./tests"", ""fixtures""])\n\n    write(""test"", ""write_scope5.txt"", mode=""w"")\n\n    assert os.path.isfile(target_path)\n\n    # restore write scopes for later tests\n    scoping.set_io_scopes(_old_scopes)\n'"
tests/optvis/param/test_cppn.py,7,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport logging\n\nfrom lucid.optvis.param.cppn import cppn\n\n\nlog = logging.getLogger(__name__)\n\n\n@pytest.mark.slow\ndef test_cppn_fits_xor():\n\n    with tf.Graph().as_default(), tf.Session() as sess:\n        cppn_param = cppn(16, num_output_channels=1)[0]\n\n        def xor_objective(a):\n            return -(\n                tf.square(a[0, 0])\n                + tf.square(a[-1, -1])\n                + tf.square(1.0 - a[-1, 0])\n                + tf.square(1.0 - a[0, -1])\n            )\n\n        loss_t = xor_objective(cppn_param)\n        optimizer = tf.train.AdamOptimizer(0.01)\n        objective = optimizer.minimize(loss_t)\n        for try_i in range(3):\n            tf.global_variables_initializer().run()\n            # loss = loss_t.eval()\n            for i in range(200):\n                _, vis = sess.run([objective, cppn_param])\n                close_enough = (\n                    vis[0, 0] > .99\n                    and vis[-1, -1] > .99\n                    and vis[-1, 0] < .01\n                    and vis[0, -1] < .01\n                )\n                if close_enough:\n                    return\n        assert False, ""fitting XOR took more than 200 steps, failing test""\n'"
tests/optvis/param/test_random.py,2,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom lucid.optvis.param.random import image_sample\n\n\ndef test_image_sample():\n    shape = (1, 32, 32, 3)\n    with tf.Session() as sess:\n        image_t = image_sample(shape)\n\n        tf.global_variables_initializer().run()\n        image1, image2 = [sess.run([image_t])[0] for _ in range(2)]\n    distance = np.mean(np.absolute(image1 - image2))\n    assert distance > 0\n'"
tests/optvis/param/test_spatial.py,6,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom lucid.misc.io import load\nfrom lucid.optvis.param.color import to_valid_rgb\nfrom lucid.optvis.param.spatial import (\n    pixel_image,\n    fft_image,\n    laplacian_pyramid_image,\n    bilinearly_sampled_image,\n)\n\n\n@pytest.fixture\ndef test_image():\n    return load(""./tests/fixtures/dog_cat_112.jpg"")\n\n\n@pytest.mark.parametrize(""param"", [pixel_image, fft_image, laplacian_pyramid_image])\ndef test_param_can_fit_image(param, test_image, maxsteps=1000):\n    shape = (1,) + test_image.shape\n    with tf.Session() as sess:\n        image_param_t = param(shape)\n        image_t = to_valid_rgb(image_param_t)\n        loss_t = tf.reduce_mean((image_t - test_image) ** 2)\n        dist_t = tf.reduce_mean(tf.abs(image_t - test_image))\n\n        optimizer = tf.train.AdamOptimizer(0.05)\n        optimize_op = optimizer.minimize(loss_t)\n\n        tf.global_variables_initializer().run()\n        for step in range(maxsteps):\n            mean_distance, _ = sess.run([dist_t, optimize_op])\n            if mean_distance < 0.01:\n                break\n    assert mean_distance < 0.01\n\n\ndef test_bilinearly_sampled_image():\n    h, w = 2, 3\n    img = np.float32(np.arange(6).reshape(h, w, 1))\n    img = img[::-1]  # flip y to match OpenGL\n    tests = [\n        [0, 0, 0],\n        [1, 1, 4],\n        [0.5, 0.5, 2],\n        [0.5, 0.0, 0.5],\n        [0.0, 0.5, 1.5],\n        [-1.0, -1.0, 5.0],\n        [w, 1, 3.0],\n        [w - 0.5, h - 0.5, 2.5],\n        [2 * w - 0.5, 2 * h - 0.5, 2.5],\n    ]\n    tests = np.float32(tests)\n    uv = np.float32((tests[:, :2] + 0.5) / [w, h])  # normalize UVs\n    expected = tests[:, 2:]\n\n    with tf.Session() as sess:\n        output, = sess.run([bilinearly_sampled_image(img, uv)])\n        assert np.abs(output - expected).max() < 1e-8\n'"
tests/optvis/param/test_unit_balls.py,8,"b'from __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport tensorflow as tf\nimport logging\nfrom lucid.optvis.param.unit_balls import unit_ball_L2, unit_ball_L_inf\n\n\nlearning_rate = .1\nnum_steps = 16\nlog = logging.getLogger(__name__)\n\n\n@pytest.mark.xfail(reason=""Unpredictably imprecise on Travis right now."")\n@pytest.mark.parametrize(""shape"", [(3), (2, 5, 5)])\ndef test_unit_ball_L2(shape, eps=1e-6):\n  """"""Tests that a L2 unit ball variable\'s norm stays roughly within 1.0.\n  Note: only holds down to eps ~= 5e-7.\n  """"""\n  with tf.Session() as sess:\n    unit_ball = unit_ball_L2(shape)\n    unit_ball_L2_norm = tf.norm(unit_ball)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    objective = optimizer.minimize(-unit_ball)\n    tf.global_variables_initializer().run()\n    norm_value = unit_ball_L2_norm.eval()\n    for i in range(num_steps):\n      _, new_norm_value, unit_ball_value = sess.run([objective, unit_ball_L2_norm, unit_ball])\n      log.info(""L2: %s, Value: %s"", new_norm_value, unit_ball_value)\n      assert new_norm_value >= norm_value - eps\n      assert new_norm_value <= 1.0 + eps\n      norm_value = new_norm_value\n\n\n@pytest.mark.xfail(reason=""Unpredictably imprecise on Travis right now."")\n@pytest.mark.parametrize(""shape"", [(3), (2, 5, 5)])\n@pytest.mark.parametrize(""precondition"", [True, False])\ndef test_unit_ball_L_inf(shape, precondition, eps=1e-6):\n  """"""Tests that a L infinity unit ball variables\' stay roughly within 1.0.\n  Note: only holds down to eps ~= 5e-7.\n  """"""\n  with tf.Session() as sess:\n    unit_ball = unit_ball_L_inf(shape, precondition=precondition)\n    unit_ball_max = tf.reduce_max(unit_ball)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    objective = optimizer.minimize(-unit_ball)\n    tf.global_variables_initializer().run()\n    for i in range(num_steps):\n      _, unit_ball_max_value, unit_ball_value = sess.run([objective, unit_ball_max, unit_ball])\n      log.info(""Linf: %s, Value: %s"", unit_ball_max_value, unit_ball_value)\n      assert unit_ball_max_value <= 1.0 + eps\n'"
