file_path,api_count,code
setup.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Setup file for the TensorFlow Neural Structured Learning pip package.""""""\n\nimport os\nimport sys\nimport textwrap\n\nimport setuptools\n\nINSTALL_REQUIRES = [\n    ""absl-py"",\n    ""attrs"",\n    ""scipy"",\n    ""six"",\n]\n\nNSL_OVERVIEW = textwrap.dedent(""""""\\\n    Neural Structured Learning (NSL) is a new learning paradigm to train neural\n    networks by leveraging structured signals in addition to feature inputs.\n    Structure can be explicit as represented by a graph or implicit as induced\n    by adversarial perturbation.\n\n    Structured signals are commonly used to represent relations or similarity\n    among samples that may be labeled or unlabeled. Leveraging these signals\n    during neural network training harnesses both labeled and unlabeled data,\n    which can improve model accuracy, particularly when the amount of labeled\n    data is relatively small. Additionally, models trained with samples that are\n    generated by adversarial perturbation have been shown to be robust against\n    malicious attacks, which are designed to mislead a model\'s prediction or\n    classification.\n\n    NSL generalizes to Neural Graph Learning as well as to Adversarial Learning.\n    The NSL framework in TensorFlow provides the following easy-to-use APIs and\n    tools for developers to train models with structured signals:\n\n    * Keras APIs to enable training with graphs (explicit structure) and\n      adversarial perturbations (implicit structure).\n\n    * TF ops and functions to enable training with structure when using\n      lower-level TensorFlow APIs.\n\n    * Tools to build graphs and construct graph inputs for training.\n\n    The NSL framework is designed to be flexible and can be used to train any\n    kind of neural network. For example, feed-forward, convolution, and\n    recurrent neural networks can all be trained using the NSL framework. In\n    addition to supervised and semi-supervised learning (a low amount of\n    supervision), NSL can in theory be generalized to unsupervised learning.\n    Incorporating structured signals is done only during training, so the\n    performance of the serving/inference workflow remains unchanged."""""")\n\n# Adds the path to sys.path so that we can import version.py.\nversion_path = os.path.join(os.path.dirname(__file__),\n                            ""neural_structured_learning"")\nsys.path.append(version_path)\nfrom version import __version__  # pylint: disable=g-import-not-at-top\n\nsetuptools.setup(\n    name=""neural-structured-learning"",\n    version=__version__,\n    author=""Google LLC"",\n    description=""Neural Structured Learning is an open-source TensorFlow ""\n    ""framework to train neural networks with structured signals"",\n    long_description=NSL_OVERVIEW,\n    long_description_content_type=""text/plain"",\n    url=""https://github.com/tensorflow/neural-structured-learning"",\n    packages=setuptools.find_packages(),\n    install_requires=INSTALL_REQUIRES,\n    classifiers=[\n        ""Programming Language :: Python :: 2"",\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: OS Independent"",\n    ],\n)\n\n# attrs\n'"
neural_structured_learning/__init__.py,0,"b'""""""Subpackages of Neural Structured Learning.""""""\n\nfrom neural_structured_learning import configs\nfrom neural_structured_learning import estimator\nfrom neural_structured_learning import keras\nfrom neural_structured_learning import lib\nfrom neural_structured_learning import tools\nfrom neural_structured_learning.version import __version__\n'"
neural_structured_learning/version.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Defines neural_structured_learning version information.""""""\n\n# We follow Semantic Versioning (https://semver.org/).\n_MAJOR_VERSION = \'1\'\n_MINOR_VERSION = \'1\'\n_PATCH_VERSION = \'0\'\n\n_VERSION_SUFFIX = \'\'\n\n__version__ = \'.\'.join([_MAJOR_VERSION, _MINOR_VERSION, _PATCH_VERSION])\nif _VERSION_SUFFIX:\n  __version__ = \'{}-{}\'.format(__version__, _VERSION_SUFFIX)\n'"
neural_structured_learning/configs/__init__.py,0,"b'""""""Configuration classes and APIs for Neural Structured Learning.""""""\n\nfrom neural_structured_learning.configs.configs import AdvNeighborConfig\nfrom neural_structured_learning.configs.configs import AdvRegConfig\nfrom neural_structured_learning.configs.configs import AdvTargetConfig\nfrom neural_structured_learning.configs.configs import AdvTargetType\nfrom neural_structured_learning.configs.configs import DecayConfig\nfrom neural_structured_learning.configs.configs import DecayType\nfrom neural_structured_learning.configs.configs import DEFAULT_ADVERSARIAL_PARAMS\nfrom neural_structured_learning.configs.configs import DEFAULT_DISTANCE_PARAMS\nfrom neural_structured_learning.configs.configs import DistanceConfig\nfrom neural_structured_learning.configs.configs import DistanceType\nfrom neural_structured_learning.configs.configs import GraphNeighborConfig\nfrom neural_structured_learning.configs.configs import GraphRegConfig\nfrom neural_structured_learning.configs.configs import IntegrationConfig\nfrom neural_structured_learning.configs.configs import IntegrationType\nfrom neural_structured_learning.configs.configs import make_adv_reg_config\nfrom neural_structured_learning.configs.configs import make_graph_reg_config\nfrom neural_structured_learning.configs.configs import NormType\nfrom neural_structured_learning.configs.configs import TransformType\nfrom neural_structured_learning.configs.configs import VirtualAdvConfig\n\n__all__ = [\n    \'AdvNeighborConfig\',\n    \'AdvRegConfig\',\n    \'AdvTargetConfig\',\n    \'AdvTargetType\',\n    \'DecayConfig\',\n    \'DecayType\',\n    \'DEFAULT_ADVERSARIAL_PARAMS\',\n    \'DEFAULT_DISTANCE_PARAMS\',\n    \'DistanceConfig\',\n    \'DistanceType\',\n    \'GraphNeighborConfig\',\n    \'GraphRegConfig\',\n    \'IntegrationConfig\',\n    \'IntegrationType\',\n    \'make_adv_reg_config\',\n    \'make_graph_reg_config\',\n    \'NormType\',\n    \'TransformType\',\n    \'VirtualAdvConfig\',\n]\n'"
neural_structured_learning/configs/configs.py,6,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for configuring modules in Neural Structured Learning (NSL).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport attr\nimport enum\nimport tensorflow as tf\n\n\nclass NormType(enum.Enum):\n  """"""Types of norms.""""""\n  L1 = \'l1\'\n  L2 = \'l2\'\n  INFINITY = \'infinity\'\n\n  @classmethod\n  def all(cls):\n    return list(cls)\n\n\n@attr.s\nclass AdvNeighborConfig(object):\n  """"""Contains configuration for generating adversarial neighbors.\n\n  Attributes:\n    feature_mask: mask w/ values in `[0, 1]` applied on the gradient. Its shape\n      should be the same as (or broadcastable to) that of the input features.\n      If the input features are in a collection (e.g. list or dictionary), this\n      field should also be a collection of the same structure. Input features\n      corresponding to mask values of 0.0 are *not* perturbed. Setting this\n      field to `None` is equivalent to setting a mask value of 1.0 for all input\n      features.\n    adv_step_size: step size to find the adversarial sample. Default set to\n      0.001.\n    adv_grad_norm: type of tensor norm to normalize the gradient. Input will be\n      converted to `nsl.configs.NormType` when applicable (e.g., `\'l2\'` ->\n      `nls.configs.NormType.L2`). Default set to L2 norm.\n    clip_value_min: minimum value to clip the features after perturbation. The\n      shape should be the same as (or broadcastable to) input features. If the\n      input features are in a collection (e.g. list or dictionary), this field\n      should also be a collection with the same structure. An omitted or\n      `None`-valued entry in the collection indicates no constraint on the\n      corresponding feature.\n    clip_value_max: maximum value to clip the feature after perturbation. (See\n      `clip_value_min` for the structure and shape limitations.)\n    pgd_iterations: number of attack iterations for Projected Gradient Descent\n      (PGD) attack. Defaults to 1, which resembles the Fast Gradient Sign Method\n      (FGSM) attack.\n    pgd_epsilon: radius of the epsilon ball to project back to. Only used in\n      Projected Gradient Descent (PGD) attack.\n  """"""\n  feature_mask = attr.ib(default=None)\n  adv_step_size = attr.ib(default=0.001)\n  adv_grad_norm = attr.ib(converter=NormType, default=\'l2\')\n  clip_value_min = attr.ib(default=None)\n  clip_value_max = attr.ib(default=None)\n  pgd_iterations = attr.ib(default=1)  # 1 is the FGSM attack.\n  pgd_epsilon = attr.ib(default=None)\n\n\n@attr.s\nclass AdvRegConfig(object):\n  """"""Contains configuration for adversarial regularization.\n\n  Attributes:\n    multiplier: multiplier to adversarial regularization loss. Default set to\n      0.2.\n    adv_neighbor_config: an `nsl.configs.AdvNeighborConfig` object for\n      generating adversarial neighbor examples.\n  """"""\n  multiplier = attr.ib(default=0.2)\n  adv_neighbor_config = attr.ib(default=AdvNeighborConfig())\n\n\ndef make_adv_reg_config(\n    multiplier=attr.fields(AdvRegConfig).multiplier.default,\n    feature_mask=attr.fields(AdvNeighborConfig).feature_mask.default,\n    adv_step_size=attr.fields(AdvNeighborConfig).adv_step_size.default,\n    adv_grad_norm=attr.fields(AdvNeighborConfig).adv_grad_norm.default,\n    clip_value_min=attr.fields(AdvNeighborConfig).clip_value_min.default,\n    clip_value_max=attr.fields(AdvNeighborConfig).clip_value_max.default,\n    pgd_iterations=attr.fields(AdvNeighborConfig).pgd_iterations.default,\n    pgd_epsilon=attr.fields(AdvNeighborConfig).pgd_epsilon.default):\n  """"""Creates an `nsl.configs.AdvRegConfig` object.\n\n  Args:\n    multiplier: multiplier to adversarial regularization loss. Defaults to 0.2.\n    feature_mask: mask w/ values in `[0, 1]` applied on the gradient. Its shape\n      should be the same as (or broadcastable to) that of the input features.\n      If the input features are in a collection (e.g. list or dictionary), this\n      field should also be a collection of the same structure. Input features\n      corresponding to mask values of 0.0 are *not* perturbed. Setting this\n      field to `None` is equivalent to setting a mask value of 1.0 for all input\n      features.\n    adv_step_size: step size to find the adversarial sample. Defaults to 0.001.\n    adv_grad_norm: type of tensor norm to normalize the gradient. Input will be\n      converted to `NormType` when applicable (e.g., a value of \'l2\' will be\n      converted to `nsl.configs.NormType.L2`). Defaults to L2 norm.\n    clip_value_min: minimum value to clip the features after perturbation. The\n      shape should be the same as (or broadcastable to) input features. If the\n      input features are in a collection (e.g. list or dictionary), this field\n      should also be a collection with the same structure. An omitted or\n      `None`-valued entry in the collection indicates no constraint on the\n      corresponding feature.\n    clip_value_max: maximum value to clip the feature after perturbation. (See\n      `clip_value_min` for the structure and shape limitations.)\n    pgd_iterations: number of attack iterations for Projected Gradient Descent\n      (PGD) attack. Defaults to 1, which resembles the Fast Gradient Sign Method\n      (FGSM) attack.\n    pgd_epsilon: radius of the epsilon ball to project back to. Only used in\n      Projected Gradient Descent (PGD) attack.\n\n  Returns:\n    An `nsl.configs.AdvRegConfig` object.\n  """"""\n  return AdvRegConfig(\n      multiplier=multiplier,\n      adv_neighbor_config=AdvNeighborConfig(\n          feature_mask=feature_mask,\n          adv_step_size=adv_step_size,\n          adv_grad_norm=adv_grad_norm,\n          clip_value_min=clip_value_min,\n          clip_value_max=clip_value_max,\n          pgd_iterations=pgd_iterations,\n          pgd_epsilon=pgd_epsilon))\n\n\nclass AdvTargetType(enum.Enum):\n  """"""Types of adversarial targeting.""""""\n  SECOND = \'second\'\n  LEAST = \'least\'\n  RANDOM = \'random\'\n  GROUND_TRUTH = \'ground_truth\'\n\n  @classmethod\n  def all(cls):\n    return list(cls)\n\n\n@attr.s\nclass AdvTargetConfig(object):\n  """"""Contains configuration for selecting targets to be attacked.\n\n  Attributes:\n    target_method: type of adversarial targeting method. The value needs to be\n      one of the enums from `nsl.configs.AdvTargetType` (e.g.,\n      `nsl.configs.AdvTargetType.LEAST`).\n    random_seed: a Python integer as seed in \'random_uniform\' op.\n  """"""\n  target_method = attr.ib(default=AdvTargetType.GROUND_TRUTH)\n  random_seed = attr.ib(default=0.0)\n\n\nclass TransformType(enum.Enum):\n  """"""Types of nonlinear functions to be applied .""""""\n  SOFTMAX = \'softmax\'\n  NONE = \'none\'\n\n\nclass DistanceType(enum.Enum):\n  """"""Types of distance.""""""\n  L1 = \'l1\'\n  L2 = \'l2\'\n  COSINE = \'cosine\'\n  JENSEN_SHANNON_DIVERGENCE = \'jensen_shannon_divergence\'\n  KL_DIVERGENCE = \'kl_divergence\'\n\n  @classmethod\n  def all(cls):\n    return list(cls)\n\n\n@attr.s\nclass DistanceConfig(object):\n  """"""Contains configuration for computing distances between tensors.\n\n  Attributes:\n    distance_type: type of distance function. Input type will be converted to\n      the appropriate `nsl.configs.DistanceType` value (e.g., the value \'l2\' is\n      converted to `nsl.configs.DistanceType.L2`). Defaults to the L2 norm.\n    reduction: type of distance reduction. See `tf.compat.v1.losses.Reduction`\n      for details. Defaults to `tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS`.\n    sum_over_axis: the distance is the sum over the difference along the axis.\n      See `nsl.lib.pairwise_distance_wrapper` for how this field is used.\n      Defaults to `None`.\n    transform_fn: type of transform function to be applied on each side before\n      computing the pairwise distance. Input type will be converted to\n      `nsl.configs.TransformType` when applicable (e.g., the value \'softmax\'\n      maps to `nsl.configs.TransformType.SOFTMAX`). Defaults to \'none\'.\n  """"""\n  distance_type = attr.ib(converter=DistanceType, default=DistanceType.L2)\n  reduction = attr.ib(\n      default=tf.compat.v1.losses.Reduction.SUM_BY_NONZERO_WEIGHTS)\n  sum_over_axis = attr.ib(default=None)\n  transform_fn = attr.ib(converter=TransformType, default=\'none\')\n\n\nclass DecayType(enum.Enum):\n  """"""Types of decay.""""""\n  EXPONENTIAL_DECAY = \'exponential_decay\'\n  INVERSE_TIME_DECAY = \'inverse_time_decay\'\n  NATURAL_EXP_DECAY = \'natural_exp_decay\'\n\n  @classmethod\n  def all(cls):\n    return list(cls)\n\n\n@attr.s\nclass DecayConfig(object):\n  """"""Contains configuration for decaying a value during training.\n\n  Attributes:\n    decay_steps: A scalar `int32` or `int64` Tensor or a Python number that\n      specifies the decay frequency, specied in units of training steps. Must be\n      positive.\n    decay_rate: A scalar `float32` or `float64` Tensor or a Python number.\n      Defaults to 0.96.\n    min_value: minimal acceptable value after applying decay. Defaults to 0.0.\n    decay_type: Type of decay function to apply. Defaults to\n      `nsl.configs.DecayType.EXPONENTIAL_DECAY`.\n  """"""\n  decay_steps = attr.ib()\n  decay_rate = attr.ib(default=0.96)\n  min_value = attr.ib(default=0.0)\n  decay_type = attr.ib(default=DecayType.EXPONENTIAL_DECAY)\n\n\nclass IntegrationType(enum.Enum):\n  """"""Types of integration for multimodal fusion.""""""\n  ADD = \'additive\'\n  MUL = \'multiplicative\'\n  TUCKER_DECOMP = \'tucker_decomp\'\n\n  @classmethod\n  def all(cls):\n    return list(cls)\n\n\n@attr.s\nclass IntegrationConfig(object):\n  """"""Contains configuration for computing multimodal integration.\n\n  Attributes:\n    integration_type: Type of integration function to apply.\n    hidden_dims: Integer or a list of Integer, the number of hidden units in the\n      fully-connected layer(s) before the output layer.\n    activation_fn: Activation function to be applied to.\n  """"""\n  integration_type = attr.ib(converter=IntegrationType)\n  hidden_dims = attr.ib()\n  activation_fn = attr.ib(default=tf.nn.tanh)\n\n\n@attr.s\nclass VirtualAdvConfig(object):\n  """"""Contains configuration for virtual adversarial training.\n\n  Attributes:\n    adv_neighbor_config: an `nsl.configs.AdvNeighborConfig` object for\n      generating virtual adversarial examples. Defaults to\n      `nsl.configs.AdvNeighborConfig()`.\n    distance_config: a `nsl.configs.DistanceConfig` object for calculating\n      virtual adversarial loss. Defaults to `nsl.configs.DistanceConfig()`.\n    num_approx_steps: number of steps used to approximate the calculation of\n      Hessian matrix required for creating virtual adversarial examples.\n      Defaults to 1.\n    approx_difference: the finite difference to approximate the calculation of\n      the Hessian matrix required for creating virtual adversarial examples,\n      namely, the `xi` in Equation 12 in the paper:\n      https://arxiv.org/pdf/1704.03976.pdf. Defaults to 1e-6.\n  """"""\n  adv_neighbor_config = attr.ib(default=AdvNeighborConfig())\n  distance_config = attr.ib(default=DistanceConfig())\n  num_approx_steps = attr.ib(default=1)\n  approx_difference = attr.ib(default=1e-6)\n\n\n@attr.s\nclass GraphNeighborConfig(object):\n  """"""Specifies neighbor attributes for graph regularization.\n\n  Attributes:\n    prefix: The prefix in feature names that identifies neighbor-specific\n      features. Defaults to \'NL_nbr_\'.\n    weight_suffix: The suffix in feature names that identifies the neighbor\n      weight value. Defaults to \'_weight\'. Note that neighbor weight features\n      will have `prefix` as a prefix and `weight_suffix` as a suffix. For\n      example, based on the default values of `prefix` and `weight_suffix`, a\n      valid neighbor weight feature is \'NL_nbr_0_weight\', where 0 corresponds to\n      the first neighbor of the sample.\n    max_neighbors: The maximum number of neighbors to be used for graph\n      regularization. Defaults to 0, which disables graph regularization. Note\n      that this value has to be less than or equal to the actual number of\n      neighbors in each sample.\n  """"""\n  prefix = attr.ib(default=\'NL_nbr_\')\n  weight_suffix = attr.ib(default=\'_weight\')\n  max_neighbors = attr.ib(default=0)\n\n\n@attr.s\nclass GraphRegConfig(object):\n  """"""Contains the configuration for graph regularization.\n\n  Attributes:\n    neighbor_config: A `nsl.configs.GraphNeighborConfig` instance that describes\n      neighbor attributes for graph regularization. Defaults to\n      `nsl.configs.GraphNeighborConfig()`.\n    multiplier: The multiplier or weight factor applied on the graph\n      regularization loss term. This value has to be non-negative. Defaults to\n      0.01.\n    distance_config: An instance of `DistanceConfig` to calculate the graph\n      regularization loss term. Defaults to `nsl.configs.DistanceConfig()`.\n  """"""\n  neighbor_config = attr.ib(default=GraphNeighborConfig())\n  multiplier = attr.ib(default=0.01)\n  distance_config = attr.ib(default=DistanceConfig())\n\n\ndef make_graph_reg_config(\n    neighbor_prefix=attr.fields(GraphNeighborConfig).prefix.default,\n    neighbor_weight_suffix=attr.fields(\n        GraphNeighborConfig).weight_suffix.default,\n    max_neighbors=attr.fields(GraphNeighborConfig).max_neighbors.default,\n    multiplier=attr.fields(GraphRegConfig).multiplier.default,\n    distance_type=attr.fields(DistanceConfig).distance_type.default,\n    reduction=attr.fields(DistanceConfig).reduction.default,\n    sum_over_axis=attr.fields(DistanceConfig).sum_over_axis.default,\n    transform_fn=attr.fields(DistanceConfig).transform_fn.default):\n  """"""Creates an `nsl.configs.GraphRegConfig` object.\n\n  Args:\n    neighbor_prefix: The prefix in feature names that identifies\n      neighbor-specific features. Defaults to \'NL_nbr_\'.\n    neighbor_weight_suffix: The suffix in feature names that identifies the\n      neighbor weight value. Defaults to \'_weight\'. Note that neighbor weight\n      features will have `prefix` as a prefix and `weight_suffix` as a suffix.\n      For example, based on the default values of `prefix` and `weight_suffix`,\n      a valid neighbor weight feature is \'NL_nbr_0_weight\', where 0 corresponds\n      to the first neighbor of the sample.\n    max_neighbors: The maximum number of neighbors to be used for graph\n      regularization. Defaults to 0, which disables graph regularization. Note\n      that this value has to be less than or equal to the actual number of\n      neighbors in each sample.\n    multiplier: The multiplier or weight factor applied on the graph\n      regularization loss term. This value has to be non-negative. Defaults to\n      0.01.\n    distance_type: type of distance function. Input type will be converted to\n      the appropriate `nsl.configs.DistanceType` value (e.g., the value \'l2\' is\n      converted to `nsl.configs.DistanceType.L2`). Defaults to the L2 norm.\n    reduction: type of distance reduction. See `tf.compat.v1.losses.Reduction`\n      for details. Defaults to `tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS`.\n    sum_over_axis: the distance is the sum over the difference along the axis.\n      See `nsl.lib.pairwise_distance_wrapper` for how this field is used.\n      Defaults to `None`.\n    transform_fn: type of transform function to be applied on each side before\n      computing the pairwise distance. Input type will be converted to\n      `nsl.configs.TransformType` when applicable (e.g., the value \'softmax\'\n      maps to `nsl.configs.TransformType.SOFTMAX`). Defaults to \'none\'.\n\n  Returns:\n    An `nsl.configs.GraphRegConfig` object.\n  """"""\n  return GraphRegConfig(\n      neighbor_config=GraphNeighborConfig(\n          prefix=neighbor_prefix,\n          weight_suffix=neighbor_weight_suffix,\n          max_neighbors=max_neighbors),\n      multiplier=multiplier,\n      distance_config=DistanceConfig(\n          distance_type=distance_type,\n          reduction=reduction,\n          sum_over_axis=sum_over_axis,\n          transform_fn=transform_fn))\n\n\nDEFAULT_DISTANCE_PARAMS = attr.asdict(DistanceConfig())\nDEFAULT_ADVERSARIAL_PARAMS = attr.asdict(AdvNeighborConfig())\n'"
neural_structured_learning/estimator/__init__.py,0,"b'r""""""Estimator APIs for Neural Structured Learning.\n\nThe current NSL Estimator APIs may not be compatible with certain TensorFlow\nversions (such as 2.0 or above).\n""""""\n\nfrom neural_structured_learning.estimator.adversarial_regularization import add_adversarial_regularization\nfrom neural_structured_learning.estimator.graph_regularization import add_graph_regularization\n\n__all__ = [\'add_adversarial_regularization\', \'add_graph_regularization\']\n'"
neural_structured_learning/estimator/adversarial_regularization.py,17,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A wrapper function to enable adversarial regularization to an Estimator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport inspect\n\nimport neural_structured_learning.configs as nsl_configs\nimport neural_structured_learning.lib as nsl_lib\nimport tensorflow as tf\n\n\ndef add_adversarial_regularization(estimator,\n                                   optimizer_fn=None,\n                                   adv_config=None):\n  """"""Adds adversarial regularization to a `tf.estimator.Estimator`.\n\n  The returned estimator will include the adversarial loss as a regularization\n  term in its training objective, and will be trained using the optimizer\n  provided by `optimizer_fn`. `optimizer_fn` (along with the hyperparameters)\n  should be set to the same one used in the base `estimator`.\n\n  If `optimizer_fn` is not set, a default optimizer `tf.train.AdagradOptimizer`\n  with `learning_rate=0.05` will be used.\n\n  Args:\n    estimator: A `tf.estimator.Estimator` object, the base model.\n    optimizer_fn: A function that accepts no arguments and returns an instance\n      of `tf.train.Optimizer`. This optimizer (instead of the one used in\n      `estimator`) will be used to train the model. If not specified, default to\n      `tf.train.AdagradOptimizer` with `learning_rate=0.05`.\n    adv_config: An instance of `nsl.configs.AdvRegConfig` that specifies various\n      hyperparameters for adversarial regularization.\n\n  Returns:\n    A modified `tf.estimator.Estimator` object with adversarial regularization\n    incorporated into its loss.\n  """"""\n\n  if not adv_config:\n    adv_config = nsl_configs.AdvRegConfig()\n\n  base_model_fn = estimator._model_fn  # pylint: disable=protected-access\n  try:\n    base_model_fn_args = inspect.signature(base_model_fn).parameters.keys()\n  except AttributeError:  # For Python 2 compatibility\n    base_model_fn_args = inspect.getargspec(base_model_fn).args  # pylint: disable=deprecated-method\n\n  def adv_model_fn(features, labels, mode, params=None, config=None):\n    """"""The adversarial-regularized model_fn.\n\n    Args:\n      features: This is the first item returned from the `input_fn` passed to\n        `train`, `evaluate`, and `predict`. This should be a single `tf.Tensor`\n        or `dict` of same.\n      labels: This is the second item returned from the `input_fn` passed to\n        `train`, `evaluate`, and `predict`. This should be a single `tf.Tensor`\n        or dict of same (for multi-head models). If mode is\n        `tf.estimator.ModeKeys.PREDICT`, `labels=None` will be passed. If the\n        `model_fn`\'s signature does not accept `mode`, the `model_fn` must still\n        be able to handle `labels=None`.\n      mode: Optional. Specifies if this is training, evaluation, or prediction.\n        See `tf.estimator.ModeKeys`.\n      params: Optional `dict` of hyperparameters. Will receive what is passed to\n        Estimator in the `params` parameter. This allows users to configure\n        Estimators from hyper parameter tuning.\n      config: Optional `estimator.RunConfig` object. Will receive what is passed\n        to Estimator as its `config` parameter, or a default value. Allows\n        setting up things in the model_fn based on configuration such as\n        `num_ps_replicas`, or `model_dir`. Unused currently.\n\n    Returns:\n      A `tf.estimator.EstimatorSpec` with adversarial regularization.\n    """"""\n    # Parameters \'params\' and \'config\' are optional. If they are not passed,\n    # then it is possible for base_model_fn not to accept these arguments.\n    # See documentation for tf.estimator.Estimator for additional context.\n    kwargs = {\'mode\': mode}\n    if \'params\' in base_model_fn_args:\n      kwargs[\'params\'] = params\n    if \'config\' in base_model_fn_args:\n      kwargs[\'config\'] = config\n    base_fn = functools.partial(base_model_fn, **kwargs)\n\n    # Uses the same variable scope for calculating the original objective and\n    # adversarial regularization.\n    with tf.compat.v1.variable_scope(tf.compat.v1.get_variable_scope(),\n                                     reuse=tf.compat.v1.AUTO_REUSE,\n                                     auxiliary_name_scope=False):\n      original_spec = base_fn(features, labels)\n\n      # Adversarial regularization only happens in training.\n      if mode != tf.estimator.ModeKeys.TRAIN:\n        return original_spec\n\n      adv_neighbor, _ = nsl_lib.gen_adv_neighbor(\n          features,\n          original_spec.loss,\n          adv_config.adv_neighbor_config,\n          # The pgd_model_fn is a dummy identity function since loss is\n          # directly available from spec_fn.\n          pgd_model_fn=lambda features: features,\n          pgd_loss_fn=lambda labels, features: base_fn(features, labels).loss,\n          pgd_labels=labels)\n\n      # Runs the base model again to compute loss on adv_neighbor.\n      adv_spec = base_fn(adv_neighbor, labels)\n\n      final_loss = original_spec.loss + adv_config.multiplier * adv_spec.loss\n\n      if not optimizer_fn:\n        # Default to the Adagrad optimizer, the same as canned DNNEstimator.\n        optimizer = tf.train.AdagradOptimizer(learning_rate=0.05)\n      else:\n        optimizer = optimizer_fn()\n\n      final_train_op = optimizer.minimize(\n          loss=final_loss, global_step=tf.compat.v1.train.get_global_step())\n\n    return original_spec._replace(loss=final_loss, train_op=final_train_op)\n\n  # Replaces the model_fn while keeps other fields/methods in the estimator.\n  estimator._model_fn = adv_model_fn  # pylint: disable=protected-access\n  return estimator\n'"
neural_structured_learning/estimator/adversarial_regularization_test.py,15,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nsl.estimator.adversarial_regularization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport tempfile\n\nfrom absl.testing import parameterized\nimport neural_structured_learning.configs as nsl_configs\nimport neural_structured_learning.estimator as nsl_estimator\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\nFEATURE_NAME = \'x\'\nWEIGHT_VARIABLE = \'linear/linear_model/\' + FEATURE_NAME + \'/weights\'\nBIAS_VARIABLE = \'linear/linear_model/bias_weights\'\n\n\ndef single_batch_input_fn(features, labels=None):\n  def input_fn():\n    inputs = features if labels is None else (features, labels)\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n    return dataset.batch(len(features))\n  return input_fn\n\n\nclass AdversarialRegularizationTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(AdversarialRegularizationTest, self).setUp()\n    self.model_dir = tempfile.mkdtemp()\n\n  def tearDown(self):\n    if self.model_dir:\n      shutil.rmtree(self.model_dir)\n    super(AdversarialRegularizationTest, self).tearDown()\n\n  def build_linear_regressor(self, weight, bias):\n    with tf.Graph().as_default():\n      tf.Variable(weight, name=WEIGHT_VARIABLE)\n      tf.Variable(bias, name=BIAS_VARIABLE)\n      tf.Variable(100, name=tf.GraphKeys.GLOBAL_STEP, dtype=tf.int64)\n\n      with tf.Session() as sess:\n        sess.run([tf.global_variables_initializer()])\n        tf.train.Saver().save(sess, os.path.join(self.model_dir, \'model.ckpt\'))\n\n    fc = tf.feature_column.numeric_column(FEATURE_NAME,\n                                          shape=np.array(weight).shape)\n    return tf.estimator.LinearRegressor(\n        feature_columns=(fc,), model_dir=self.model_dir, optimizer=\'SGD\')\n\n  @test_util.run_v1_only(\'Requires tf.GraphKeys\')\n  def test_adversarial_wrapper_not_affecting_predictions(self):\n    # base model: y = x + 2\n    base_est = self.build_linear_regressor(weight=[[1.0]], bias=[2.0])\n    adv_est = nsl_estimator.add_adversarial_regularization(base_est)\n    input_fn = single_batch_input_fn({FEATURE_NAME: np.array([[1.0], [2.0]])})\n    predictions = adv_est.predict(input_fn=input_fn)\n    predicted_scores = [x[\'predictions\'] for x in predictions]\n    self.assertAllClose([[3.0], [4.0]], predicted_scores)\n\n  @parameterized.named_parameters([\n      (\'fgsm\', 0.1, 1, None),\n      (\'pgd\', 0.1, 3, 0.25),\n  ])\n  @test_util.run_v1_only(\'Requires tf.GraphKeys\')\n  def test_adversarial_wrapper_adds_regularization(self, adv_step_size,\n                                                   pgd_iterations, pgd_epsilon):\n    # base model: y = w*x+b = 4*x1 + 3*x2 + 2\n    weight = np.array([[4.0], [3.0]], dtype=np.float32)\n    bias = np.array([2.0], dtype=np.float32)\n    x0, y0 = np.array([[1.0, 1.0]]), np.array([8.0])\n    learning_rate = 0.01\n\n    base_est = self.build_linear_regressor(weight=weight, bias=bias)\n    adv_config = nsl_configs.make_adv_reg_config(\n        multiplier=1.0,  # equal weight on original and adv examples\n        adv_step_size=adv_step_size,\n        pgd_iterations=pgd_iterations,\n        pgd_epsilon=pgd_epsilon)\n    adv_est = nsl_estimator.add_adversarial_regularization(\n        base_est,\n        optimizer_fn=lambda: tf.train.GradientDescentOptimizer(learning_rate),\n        adv_config=adv_config)\n    input_fn = single_batch_input_fn({FEATURE_NAME: x0}, y0)\n    adv_est.train(input_fn=input_fn, steps=1)\n\n    # Computes the gradients on original and adversarial examples.\n    orig_pred = np.dot(x0, weight) + bias  # [9.0]\n    orig_grad_w = 2 * (orig_pred - y0) * x0.T  # [[2.0], [2.0]]\n    orig_grad_b = 2 * (orig_pred - y0).reshape((1,))  # [2.0]\n    grad_x = 2 * (orig_pred - y0) * weight.T  # [[8.0, 6.0]]\n    # Gradient direction is independent of x, so perturbing for multiple\n    # iterations is the same as scaling the perturbation.\n    perturbation_magnitude = pgd_iterations * adv_step_size\n    if pgd_epsilon is not None:\n      perturbation_magnitude = np.minimum(perturbation_magnitude, pgd_epsilon)\n    perturbation = perturbation_magnitude * grad_x / np.linalg.norm(grad_x)\n    x_adv = x0 + perturbation  # fgm: [[1.08, 1.06]]; pgd: [[1.20, 1.15]]\n    adv_pred = np.dot(x_adv, weight) + bias  # fgm: [9.5]; pgd: [10.25]\n    adv_grad_w = 2 * (adv_pred - y0) * x_adv.T  # fgm: [[3.24], [3.18]]\n    adv_grad_b = 2 * (adv_pred - y0).reshape((1,))  # fgm: [3.0]; pgd: [4.5]\n\n    new_bias = bias - learning_rate * (orig_grad_b + adv_grad_b)\n    new_weight = weight - learning_rate * (orig_grad_w + adv_grad_w)\n    self.assertAllClose(new_bias, adv_est.get_variable_value(BIAS_VARIABLE))\n    self.assertAllClose(new_weight, adv_est.get_variable_value(WEIGHT_VARIABLE))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/estimator/graph_regularization.py,17,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A wrapper function to enable graph-based regularization for an Estimator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect\n\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.lib import distances\nfrom neural_structured_learning.lib import utils\n\nimport tensorflow as tf\n\n\ndef add_graph_regularization(estimator,\n                             embedding_fn,\n                             optimizer_fn=None,\n                             graph_reg_config=None):\n  """"""Adds graph regularization to a `tf.estimator.Estimator`.\n\n  Args:\n    estimator: An object of type `tf.estimator.Estimator`.\n    embedding_fn: A function that accepts the input layer (dictionary of feature\n      names and corresponding batched tensor values) as its first argument and\n      an instance of `tf.estimator.ModeKeys` as its second argument to indicate\n      if the mode is training, evaluation, or prediction, and returns the\n      corresponding embeddings or logits to be used for graph regularization.\n    optimizer_fn: A function that accepts no arguments and returns an instance\n      of `tf.train.Optimizer`.\n    graph_reg_config: An instance of `nsl.configs.GraphRegConfig` that specifies\n      various hyperparameters for graph regularization.\n\n  Returns:\n    A modified `tf.estimator.Estimator` object with graph regularization\n    incorporated into its loss.\n  """"""\n\n  if not graph_reg_config:\n    graph_reg_config = configs.GraphRegConfig()\n\n  base_model_fn = estimator._model_fn  # pylint: disable=protected-access\n  try:\n    base_model_fn_args = inspect.signature(base_model_fn).parameters.keys()\n  except AttributeError:  # For Python 2 compatibility\n    base_model_fn_args = inspect.getargspec(base_model_fn).args  # pylint: disable=deprecated-method\n\n  def graph_reg_model_fn(features, labels, mode, params=None, config=None):\n    """"""The graph-regularized model function.\n\n    Args:\n      features: This is the first item returned from the `input_fn` passed to\n        `train`, `evaluate`, and `predict`. This should be a dictionary\n        containing sample features as well as corresponding neighbor features\n        and neighbor weights.\n      labels: This is the second item returned from the `input_fn` passed to\n        `train`, `evaluate`, and `predict`. This should be a single `Tensor` or\n        `dict` of same (for multi-head models). If mode is\n        `tf.estimator.ModeKeys.PREDICT`, `labels=None` will be passed. If the\n        `model_fn`\'s signature does not accept `mode`, the `model_fn` must still\n        be able to handle `labels=None`.\n      mode: Optional. Specifies if this is training, evaluation, or prediction.\n        See `tf.estimator.ModeKeys`.\n      params: Optional `dict` of hyperparameters. Will receive what is passed to\n        Estimator in the `params` parameter. This allows users to configure\n        Estimators from hyper parameter tuning.\n      config: Optional `tf.estimator.RunConfig` object. Will receive what is\n        passed to Estimator as its `config` parameter, or a default value.\n        Allows setting up things in the `model_fn` based on configuration such\n        as `num_ps_replicas`, or `model_dir`. Unused currently.\n\n    Returns:\n      A `tf.estimator.EstimatorSpec` with graph regularization.\n    """"""\n    # Parameters \'params\' and \'config\' are optional. If they are not passed,\n    # then it is possible for base_model_fn not to accept these arguments.\n    # See documentation for tf.estimator.Estimator for additional context.\n    kwargs = {\'mode\': mode}\n    if \'params\' in base_model_fn_args:\n      kwargs[\'params\'] = params\n    if \'config\' in base_model_fn_args:\n      kwargs[\'config\'] = config\n\n    # Uses the same variable scope for calculating the original objective and\n    # the graph regularization loss term.\n    with tf.compat.v1.variable_scope(\n        tf.compat.v1.get_variable_scope(),\n        reuse=tf.compat.v1.AUTO_REUSE,\n        auxiliary_name_scope=False):\n      nbr_features = dict()\n      nbr_weights = None\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        # Extract sample features, neighbor features, and neighbor weights if we\n        # are in training mode.\n        sample_features, nbr_features, nbr_weights = (\n            utils.unpack_neighbor_features(features,\n                                           graph_reg_config.neighbor_config))\n      else:\n        # Otherwise, we strip out all neighbor features and use just the\n        # sample\'s features.\n        sample_features = utils.strip_neighbor_features(\n            features, graph_reg_config.neighbor_config)\n\n      base_spec = base_model_fn(sample_features, labels, **kwargs)\n\n      has_nbr_inputs = nbr_weights is not None and nbr_features\n\n      # Graph regularization happens only if all the following conditions are\n      # satisfied:\n      # - the mode is training\n      # - neighbor inputs exist\n      # - the graph regularization multiplier is greater than zero.\n      # So, return early if any of these conditions is false.\n      if (not has_nbr_inputs or mode != tf.estimator.ModeKeys.TRAIN or\n          graph_reg_config.multiplier <= 0):\n        return base_spec\n\n      # Compute sample embeddings.\n      sample_embeddings = embedding_fn(sample_features, mode)\n\n      # Compute the embeddings of the neighbors.\n      nbr_embeddings = embedding_fn(nbr_features, mode)\n\n      replicated_sample_embeddings = utils.replicate_embeddings(\n          sample_embeddings, graph_reg_config.neighbor_config.max_neighbors)\n\n      # Compute the distance between the sample embeddings and each of their\n      # corresponding neighbor embeddings.\n      graph_loss = distances.pairwise_distance_wrapper(\n          replicated_sample_embeddings,\n          nbr_embeddings,\n          weights=nbr_weights,\n          distance_config=graph_reg_config.distance_config)\n      total_loss = base_spec.loss + graph_reg_config.multiplier * graph_loss\n\n      if not optimizer_fn:\n        # Default to Adagrad optimizer, the same as the canned DNNEstimator.\n        optimizer = tf.train.AdagradOptimizer(learning_rate=0.05)\n      else:\n        optimizer = optimizer_fn()\n      final_train_op = optimizer.minimize(\n          loss=total_loss, global_step=tf.compat.v1.train.get_global_step())\n\n    return base_spec._replace(loss=total_loss, train_op=final_train_op)\n\n  # Replaces the model_fn while keeping other fields/methods in the estimator.\n  estimator._model_fn = graph_reg_model_fn  # pylint: disable=protected-access\n  return estimator\n'"
neural_structured_learning/estimator/graph_regularization_test.py,51,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nsl.estimator.graph_regularization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport tempfile\n\nimport neural_structured_learning.configs as nsl_configs\nimport neural_structured_learning.estimator as nsl_estimator\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\nFEATURE_NAME = \'x\'\nLABEL_NAME = \'y\'\nNBR_FEATURE_PREFIX = \'NL_nbr_\'\nNBR_WEIGHT_SUFFIX = \'_weight\'\nWEIGHT_VARIABLE = \'linear/linear_model/\' + FEATURE_NAME + \'/weights\'\nBIAS_VARIABLE = \'linear/linear_model/bias_weights\'\nLEARNING_RATE = 0.01\n\n\ndef make_feature_spec(input_shape, max_neighbors):\n  """"""Returns a feature spec that can be used to parse tf.train.Examples.\n\n  Args:\n    input_shape: A list of integers representing the shape of the input feature\n      and corresponding neighbor features.\n    max_neighbors: The maximum neighbors per sample to be used for graph\n      regularization.\n  """"""\n  feature_spec = {\n      FEATURE_NAME:\n          tf.FixedLenFeature(input_shape, tf.float32),\n      LABEL_NAME:\n          tf.FixedLenFeature([1], tf.float32, default_value=tf.constant([0.0])),\n  }\n  for i in range(max_neighbors):\n    nbr_feature_key = \'{}{}_{}\'.format(NBR_FEATURE_PREFIX, i, FEATURE_NAME)\n    nbr_weight_key = \'{}{}{}\'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)\n    feature_spec[nbr_feature_key] = tf.FixedLenFeature(input_shape, tf.float32)\n    feature_spec[nbr_weight_key] = tf.FixedLenFeature([1],\n                                                      tf.float32,\n                                                      default_value=tf.constant(\n                                                          [0.0]))\n  return feature_spec\n\n\ndef single_example_input_fn(example_proto, input_shape, max_neighbors):\n  """"""Returns an input_fn.""""""\n\n  def make_parse_example_fn(feature_spec):\n    """"""Creates a function parse_example function.\n\n    Args:\n      feature_spec: A dictionary of features for parsing an input\n        tf.train.Example.\n\n    Returns:\n      A parse_example function.\n    """"""\n\n    def parse_example(serialized_example_proto):\n      """"""Extracts relevant fields from the example_proto.""""""\n      return tf.parse_single_example(serialized_example_proto, feature_spec)\n\n    return parse_example\n\n  def input_fn():\n    # Construct a tf.data.Dataset from the given Example.\n    example = text_format.Parse(example_proto, tf.train.Example())\n    serialized_example = example.SerializeToString()\n    dataset = tf.data.Dataset.from_tensors(\n        tf.convert_to_tensor(serialized_example))\n    dataset = dataset.map(\n        make_parse_example_fn(make_feature_spec(input_shape, max_neighbors)))\n    dataset = dataset.batch(1)\n    iterator = dataset.make_one_shot_iterator()\n    batch_features = iterator.get_next()\n    return batch_features, batch_features.pop(LABEL_NAME)\n\n  return input_fn\n\n\nclass GraphRegularizationTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(GraphRegularizationTest, self).setUp()\n    self.model_dir = tempfile.mkdtemp()\n\n  def tearDown(self):\n    if self.model_dir:\n      shutil.rmtree(self.model_dir)\n    super(GraphRegularizationTest, self).tearDown()\n\n  def build_linear_regressor(self, weight, weight_shape, bias, bias_shape):\n    with tf.Graph().as_default():\n      # Use a partitioner that is known a priori because canned Estimators\n      # default to using one otherwise. This allows tests to access variables\n      # used in the underlying Estimator.\n      tf.get_variable(\n          name=WEIGHT_VARIABLE,\n          shape=weight_shape,\n          initializer=weight,\n          partitioner=tf.fixed_size_partitioner(1))\n      tf.get_variable(\n          name=BIAS_VARIABLE,\n          shape=bias_shape,\n          initializer=bias,\n          partitioner=tf.fixed_size_partitioner(1))\n      tf.Variable(100, name=tf.GraphKeys.GLOBAL_STEP, dtype=tf.int64)\n\n      with tf.Session() as sess:\n        sess.run([tf.global_variables_initializer()])\n        tf.train.Saver().save(sess, os.path.join(self.model_dir, \'model.ckpt\'))\n\n    fc = tf.feature_column.numeric_column(\n        FEATURE_NAME, shape=np.array(weight).shape)\n    return tf.estimator.LinearRegressor(\n        feature_columns=(fc,), model_dir=self.model_dir, optimizer=\'SGD\')\n\n  @test_util.run_v1_only(\'Requires tf.get_variable\')\n  def test_graph_reg_wrapper_no_training(self):\n    """"""Test that predictions are unaffected when there is no training.""""""\n    # Base model: y = x + 2\n    base_est = self.build_linear_regressor(\n        weight=[[1.0]], weight_shape=[1, 1], bias=[2.0], bias_shape=[1])\n\n    def embedding_fn(features, unused_mode):\n      # Apply the same model, i.e, y = x + 2.\n      # Use broadcasting to do element-wise addition.\n      return tf.math.add(features[FEATURE_NAME], [2.0])\n\n    graph_reg_config = nsl_configs.make_graph_reg_config(max_neighbors=1)\n    graph_reg_est = nsl_estimator.add_graph_regularization(\n        base_est, embedding_fn, graph_reg_config=graph_reg_config)\n\n    # Consider only one neighbor for the input sample.\n    example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 1.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_x""\n                    value: { float_list { value: [ 2.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n               }\n              """"""\n\n    input_fn = single_example_input_fn(\n        example, input_shape=[1], max_neighbors=0)\n    predictions = graph_reg_est.predict(input_fn=input_fn)\n    predicted_scores = [x[\'predictions\'] for x in predictions]\n    self.assertAllClose([[3.0]], predicted_scores)\n\n  def _train_and_check_params(self, example, max_neighbors, weight, bias,\n                              expected_grad_from_weight,\n                              expected_grad_from_bias):\n    """"""Runs training for one step and verifies gradient-based updates.""""""\n\n    def embedding_fn(features, unused_mode):\n      # Computes y = w*x\n      with tf.variable_scope(\n          tf.get_variable_scope(),\n          reuse=tf.AUTO_REUSE,\n          auxiliary_name_scope=False):\n        weight_tensor = tf.reshape(\n            tf.get_variable(\n                WEIGHT_VARIABLE,\n                shape=[2, 1],\n                partitioner=tf.fixed_size_partitioner(1)),\n            shape=[-1, 2])\n\n      x_tensor = tf.reshape(features[FEATURE_NAME], shape=[-1, 2])\n      return tf.reduce_sum(\n          tf.multiply(weight_tensor, x_tensor), 1, keep_dims=True)\n\n    def optimizer_fn():\n      return tf.train.GradientDescentOptimizer(LEARNING_RATE)\n\n    base_est = self.build_linear_regressor(\n        weight=weight, weight_shape=[2, 1], bias=bias, bias_shape=[1])\n\n    graph_reg_config = nsl_configs.make_graph_reg_config(\n        max_neighbors=max_neighbors, multiplier=1)\n    graph_reg_est = nsl_estimator.add_graph_regularization(\n        base_est, embedding_fn, optimizer_fn, graph_reg_config=graph_reg_config)\n\n    input_fn = single_example_input_fn(\n        example, input_shape=[2], max_neighbors=max_neighbors)\n    graph_reg_est.train(input_fn=input_fn, steps=1)\n\n    # Compute the new bias and weight values based on the gradients.\n    expected_bias = bias - LEARNING_RATE * (expected_grad_from_bias)\n    expected_weight = weight - LEARNING_RATE * (expected_grad_from_weight)\n\n    # Check that the parameters of the linear regressor have the correct values.\n    self.assertAllClose(expected_bias,\n                        graph_reg_est.get_variable_value(BIAS_VARIABLE))\n    self.assertAllClose(expected_weight,\n                        graph_reg_est.get_variable_value(WEIGHT_VARIABLE))\n\n  @test_util.run_v1_only(\'Requires tf.get_variable\')\n  def test_graph_reg_wrapper_one_neighbor_with_training(self):\n    """"""Tests that the loss during training includes graph regularization.""""""\n    # Base model: y = w*x+b = 4*x1 + 3*x2 + 2\n    weight = np.array([[4.0], [3.0]], dtype=np.float32)\n    bias = np.array([2.0], dtype=np.float32)\n    # Expected y value.\n    x0, y0 = np.array([[1.0, 1.0]]), np.array([8.0])\n    neighbor0 = np.array([[0.5, 1.5]])\n\n    example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 1.0, 1.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_x""\n                    value: { float_list { value: [ 0.5, 1.5 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""y""\n                    value: { float_list { value: 8.0 } }\n                  }\n                }\n              """"""\n\n    # Compute the gradients on the original example using the updated loss term,\n    # which includes the supervised loss as well as the graph loss.\n    orig_pred = np.dot(x0, weight) + bias  # [9.0]\n\n    # Based on the implementation of embedding_fn inside\n    # _train_and_check_params.\n    x0_embedding = np.dot(x0, weight)\n    neighbor0_embedding = np.dot(neighbor0, weight)\n\n    # The graph loss term is (x0_embedding - neighbor0_embedding)^2\n    orig_grad_w = 2 * (orig_pred - y0) * x0.T + 2 * (\n        x0_embedding - neighbor0_embedding) * (x0 -\n                                               neighbor0).T  # [[2.5], [1.5]]\n    orig_grad_b = 2 * (orig_pred - y0).reshape((1,))  # [2.0]\n\n    self._train_and_check_params(example, 1, weight, bias, orig_grad_w,\n                                 orig_grad_b)\n\n  @test_util.run_v1_only(\'Requires tf.get_variable\')\n  def test_graph_reg_wrapper_two_neighbors_with_training(self):\n    """"""Tests that the loss during training includes graph regularization.""""""\n    # Base model: y = w*x+b = 4*x1 + 3*x2 + 2\n    weight = np.array([[4.0], [3.0]], dtype=np.float32)\n    bias = np.array([2.0], dtype=np.float32)\n    # Expected y value.\n    x0, y0 = np.array([[1.0, 1.0]]), np.array([8.0])\n    neighbor0 = np.array([[0.5, 1.5]])\n    neighbor1 = np.array([[0.75, 1.25]])\n\n    example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 1.0, 1.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_x""\n                    value: { float_list { value: [ 0.5, 1.5 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""NL_nbr_1_x""\n                    value: { float_list { value: [ 0.75, 1.25 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_1_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""y""\n                    value: { float_list { value: 8.0 } }\n                  }\n                }\n              """"""\n\n    # Compute the gradients on the original example using the updated loss term,\n    # which includes the supervised loss as well as the graph loss.\n    orig_pred = np.dot(x0, weight) + bias  # [9.0]\n\n    # Based on the implementation of embedding_fn inside\n    # _train_and_check_params.\n    x0_embedding = np.dot(x0, weight)\n    neighbor0_embedding = np.dot(neighbor0, weight)\n    neighbor1_embedding = np.dot(neighbor1, weight)\n\n    # Vertify that the loss includes the supervised loss as well as the graph\n    # loss by computing the gradients of the loss.\n\n    grad_w_supervised_loss = 2 * (orig_pred - y0) * x0.T  # [[2.0], [2.0]]\n\n    # The distance metric for the graph loss is \'L2\'. So, the graph loss term is\n    # [(x0_embedding - neighbor0_embedding)^2 +\n    #  (x0_embedding - neighbor1_embedding)^2] / 2\n    grad_w_graph_loss = ((x0_embedding - neighbor0_embedding) *\n                         (x0 - neighbor0).T) + (\n                             (x0_embedding - neighbor1_embedding) *\n                             (x0 - neighbor1).T)  # [[0.3125], [-0.3125]]\n    orig_grad_w = grad_w_supervised_loss + grad_w_graph_loss\n    orig_grad_b = 2 * (orig_pred - y0).reshape((1,))  # [2.0]\n\n    self._train_and_check_params(example, 2, weight, bias, orig_grad_w,\n                                 orig_grad_b)\n\n  def _train_and_check_eval_results(self, train_example, test_example,\n                                    max_neighbors, weight, bias):\n    """"""Verifies evaluation results for the graph-regularized model.""""""\n\n    def embedding_fn(features, unused_mode):\n      # Computes y = w*x\n      with tf.variable_scope(\n          tf.get_variable_scope(),\n          reuse=tf.AUTO_REUSE,\n          auxiliary_name_scope=False):\n        weight_tensor = tf.reshape(\n            tf.get_variable(\n                WEIGHT_VARIABLE,\n                shape=[2, 1],\n                partitioner=tf.fixed_size_partitioner(1)),\n            shape=[-1, 2])\n\n      x_tensor = tf.reshape(features[FEATURE_NAME], shape=[-1, 2])\n      return tf.reduce_sum(\n          tf.multiply(weight_tensor, x_tensor), 1, keep_dims=True)\n\n    def optimizer_fn():\n      return tf.train.GradientDescentOptimizer(LEARNING_RATE)\n\n    base_est = self.build_linear_regressor(\n        weight=weight, weight_shape=[2, 1], bias=bias, bias_shape=[1])\n\n    graph_reg_config = nsl_configs.make_graph_reg_config(\n        max_neighbors=max_neighbors, multiplier=1)\n    graph_reg_est = nsl_estimator.add_graph_regularization(\n        base_est, embedding_fn, optimizer_fn, graph_reg_config=graph_reg_config)\n\n    train_input_fn = single_example_input_fn(\n        train_example, input_shape=[2], max_neighbors=max_neighbors)\n    graph_reg_est.train(input_fn=train_input_fn, steps=1)\n\n    # Evaluating the graph-regularized model should yield the same results\n    # as evaluating the base model because model paramters are shared.\n    eval_input_fn = single_example_input_fn(\n        test_example, input_shape=[2], max_neighbors=0)\n    graph_eval_results = graph_reg_est.evaluate(input_fn=eval_input_fn)\n    base_eval_results = base_est.evaluate(input_fn=eval_input_fn)\n    self.assertAllClose(base_eval_results, graph_eval_results)\n\n  @test_util.run_v1_only(\'Requires tf.get_variable\')\n  def test_graph_reg_model_evaluate(self):\n    weight = np.array([[4.0], [-3.0]])\n    bias = np.array([0.0], dtype=np.float32)\n\n    train_example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 2.0, 3.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_x""\n                    value: { float_list { value: [ 2.5, 3.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""NL_nbr_1_x""\n                    value: { float_list { value: [ 2.0, 2.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_1_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""y""\n                    value: { float_list { value: 0.0 } }\n                  }\n                }\n              """"""\n\n    test_example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 4.0, 2.0 ] } }\n                  }\n                  feature {\n                    key: ""y""\n                    value: { float_list { value: 4.0 } }\n                  }\n                }\n              """"""\n    self._train_and_check_eval_results(\n        train_example, test_example, max_neighbors=2, weight=weight, bias=bias)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/examples/adv_keras_cnn_mnist.py,17,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of adversarial Keras trainer on classifying MNIST images.\n\nUSAGE:\n  python adv_keras_cnn_mnist.py\n\nSee http://yann.lecun.com/exdb/mnist/ for the description of the MNIST dataset.\n\nThis example demonstrates how to train a Keras model with adversarial\nregularization. The base model demonstrated in this example is a convolutional\nneural network built with Keras functional APIs, and users are encouraged to\nmodify the `build_base_model()` function to try other types of models.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nimport attr\nimport neural_structured_learning as nsl\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_integer(\'epochs\', None, \'Number of epochs to train.\')\nflags.DEFINE_integer(\'steps_per_epoch\', None,\n                     \'Number of steps in each training epoch.\')\nflags.DEFINE_integer(\'eval_steps\', None, \'Number of steps to evaluate.\')\nflags.DEFINE_float(\'adv_step_size\', None,\n                   \'Step size for generating adversarial examples.\')\n\nFEATURE_INPUT_NAME = \'image\'\nLABEL_INPUT_NAME = \'label\'\n\n\n@attr.s\nclass HParams(object):\n  """"""Hyper-parameters for training the model.""""""\n  # model architecture parameters\n  input_shape = attr.ib(default=(28, 28, 1))\n  conv_filters = attr.ib(default=[32, 64, 64])\n  kernel_size = attr.ib(default=(3, 3))\n  pool_size = attr.ib(default=(2, 2))\n  dense_units = attr.ib(default=[64])\n  num_classes = attr.ib(default=10)\n  # adversarial parameters\n  adv_multiplier = attr.ib(default=0.2)\n  adv_step_size = attr.ib(default=0.2)\n  adv_grad_norm = attr.ib(default=\'infinity\')\n  # training parameters\n  batch_size = attr.ib(default=32)\n  buffer_size = attr.ib(default=10000)\n  epochs = attr.ib(default=5)\n  steps_per_epoch = attr.ib(default=None)\n  eval_steps = attr.ib(default=None)\n\n\ndef get_hparams():\n  """"""Returns the hyperparameters with defaults overwritten by flags.""""""\n  hparams = HParams()\n  if FLAGS.epochs:\n    hparams.epochs = FLAGS.epochs\n  if FLAGS.adv_step_size:\n    hparams.adv_step_size = FLAGS.adv_step_size\n  if FLAGS.steps_per_epoch:\n    hparams.steps_per_epoch = FLAGS.steps_per_epoch\n  if FLAGS.eval_steps:\n    hparams.eval_steps = FLAGS.eval_steps\n  return hparams\n\n\ndef prepare_datasets(hparams):\n  """"""Downloads the MNIST dataset and converts to `tf.data.Dataset` format.""""""\n  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n  def make_dataset(x, y, shuffle=False):\n    x = x.reshape((-1, 28, 28, 1)).astype(\'float32\') / 255.0\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    if shuffle:\n      dataset = dataset.shuffle(hparams.buffer_size)\n    return dataset.batch(hparams.batch_size)\n\n  return make_dataset(x_train, y_train, True), make_dataset(x_test, y_test)\n\n\ndef convert_to_adversarial_training_dataset(dataset):\n  def to_dict(x, y):\n    return {FEATURE_INPUT_NAME: x, LABEL_INPUT_NAME: y}\n\n  return dataset.map(to_dict)\n\n\ndef build_base_model(hparams):\n  """"""Builds a model according to the architecture defined in `hparams`.""""""\n  inputs = tf.keras.Input(\n      shape=hparams.input_shape, dtype=tf.float32, name=FEATURE_INPUT_NAME)\n\n  x = inputs\n  for filter_idx, num_filters in enumerate(hparams.conv_filters):\n    x = tf.keras.layers.Conv2D(\n        num_filters, hparams.kernel_size, activation=\'relu\')(\n            x)\n    if filter_idx < len(hparams.conv_filters) - 1:\n      # max pooling between convolutional layers\n      x = tf.keras.layers.MaxPooling2D(hparams.pool_size)(x)\n  x = tf.keras.layers.Flatten()(x)\n  for num_hidden_units in hparams.dense_units:\n    x = tf.keras.layers.Dense(num_hidden_units, activation=\'relu\')(x)\n  pred = tf.keras.layers.Dense(hparams.num_classes, activation=\'softmax\')(x)\n  model = tf.keras.Model(inputs=inputs, outputs=pred)\n  return model\n\n\ndef apply_adversarial_regularization(model, hparams):\n  adv_config = nsl.configs.make_adv_reg_config(\n      multiplier=hparams.adv_multiplier,\n      adv_step_size=hparams.adv_step_size,\n      adv_grad_norm=hparams.adv_grad_norm)\n  return nsl.keras.AdversarialRegularization(\n      model, label_keys=[LABEL_INPUT_NAME], adv_config=adv_config)\n\n\ndef build_adv_model(hparams):\n  """"""Builds an adversarial-regularized model from parameters in `hparams`.""""""\n  base_model = build_base_model(hparams)\n  return apply_adversarial_regularization(base_model, hparams)\n\n\ndef train_and_evaluate(model, hparams, train_dataset, test_dataset):\n  """"""Trains the model and returns the evaluation result.""""""\n  model.compile(\n      optimizer=\'adam\',\n      loss=\'sparse_categorical_crossentropy\',\n      metrics=[\'accuracy\'])\n  model.fit(\n      train_dataset,\n      epochs=hparams.epochs,\n      steps_per_epoch=hparams.steps_per_epoch)\n  eval_result = model.evaluate(test_dataset, steps=hparams.eval_steps)\n  return list(zip(model.metrics_names, eval_result))\n\n\ndef evaluate_robustness(model_to_attack, dataset, models, hparams):\n  """"""Evaluates the robustness of `models` with adversarially-perturbed input.\n\n  Args:\n    model_to_attack: `tf.keras.Model`. Perturbations will be generated based\n      on this model\'s weights.\n    dataset: Dataset to be perturbed.\n    models: Dictionary of model names and `tf.keras.Model` to be evaluated.\n    hparams: Hyper-parameters for generating adversarial examples.\n\n  Returns:\n    A dictionary of model names and accuracy of the model on adversarially\n    perturbed input, i.e. robustness.\n  """"""\n  if not isinstance(model_to_attack, nsl.keras.AdversarialRegularization):\n    # Enables AdversarialRegularization-specific API for the model_to_attack.\n    # This won\'t change the model\'s weights.\n    model_to_attack = apply_adversarial_regularization(model_to_attack, hparams)\n    model_to_attack.compile(\n        optimizer=\'adam\',\n        loss=\'sparse_categorical_crossentropy\',\n        metrics=[\'accuracy\'])\n\n  metrics = {\n      name: tf.keras.metrics.SparseCategoricalAccuracy()\n      for name in models.keys()\n  }\n\n  if hparams.eval_steps:\n    dataset = dataset.take(hparams.eval_steps)\n  # When running on accelerators, looping over the dataset inside a tf.function\n  # may be much faster.\n  for batch in dataset:\n    adv_batch = model_to_attack.perturb_on_batch(batch)\n    # Clips the perturbed values to 0~1, the same as normalized values after\n    # preprocessing.\n    adv_batch[FEATURE_INPUT_NAME] = tf.clip_by_value(\n        adv_batch[FEATURE_INPUT_NAME], 0.0, 1.0)\n    y_true = adv_batch.pop(LABEL_INPUT_NAME)\n    for name, model in models.items():\n      y_pred = model(adv_batch)\n      metrics[name](y_true, y_pred)\n\n  return {name: metric.result().numpy() for name, metric in metrics.items()}\n\n\ndef main(argv):\n  del argv  # Unused.\n\n  hparams = get_hparams()\n  train_dataset, test_dataset = prepare_datasets(hparams)\n\n  adv_model = build_adv_model(hparams)\n  adv_train_dataset = convert_to_adversarial_training_dataset(train_dataset)\n  adv_test_dataset = convert_to_adversarial_training_dataset(test_dataset)\n  adv_result = train_and_evaluate(\n      adv_model, hparams, adv_train_dataset, adv_test_dataset)\n\n  base_model = build_base_model(hparams)\n  base_result = train_and_evaluate(\n      base_model, hparams, train_dataset, test_dataset)\n\n  for metric_name, result in base_result:\n    print(\'Eval %s for base model: %s\' % (metric_name, result))\n  for metric_name, result in adv_result:\n    print(\'Eval %s for adversarial model: %s\' % (metric_name, result))\n\n  models = {\n      \'base\': base_model,\n      # Takes the base model from adv_model so that input format is the same.\n      \'adv-regularized\': adv_model.base_model,\n  }\n\n  adv_accuracy = evaluate_robustness(base_model, adv_test_dataset, models,\n                                     hparams)\n  print(\'----- Adversarial attack on base model -----\')\n  for name, accuracy in adv_accuracy.items():\n    print(\'%s model accuracy: %f\' % (name, accuracy))\n  adv_accuracy = evaluate_robustness(adv_model, adv_test_dataset, models,\n                                     hparams)\n  print(\'----- Adversarial attack on adv model -----\')\n  for name, accuracy in adv_accuracy.items():\n    print(\'%s model accuracy: %f\' % (name, accuracy))\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.enable_v2_behavior()\n  app.run(main)\n'"
neural_structured_learning/examples/graph_keras_mlp_cora.py,45,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""An example Keras trainer for the Cora data set using graph regularization.\n\nUSAGE:\n  python graph_keras_mlp_cora.py [flags] train.tfr test.tfr\n\nSee https://linqs.soe.ucsc.edu/data for a description of the Cora data set, and\nthe corresponding graph and training data set.\n\nThis example demonstrates the use of sequential, functional, and subclass models\nin Keras for graph regularization. Users may change \'base_models\' defined in\nmain() as necessary, to select a subset of the supported Keras base model types.\nIn all cases, the base model used is a multi-layer perceptron containing two\nhidden layers with drop out.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport attr\n\nimport neural_structured_learning as nsl\n\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\nFLAGS.showprefixforinfo = False\n\nflags.DEFINE_integer(\'train_epochs\', None, \'Number of epochs to train.\')\nflags.DEFINE_integer(\'eval_steps\', None, \'Number of steps to evaluate.\')\n\nNBR_FEATURE_PREFIX = \'NL_nbr_\'\nNBR_WEIGHT_SUFFIX = \'_weight\'\n\n\n@attr.s\nclass HParams(object):\n  """"""Hyper-parameters used for training.""""""\n  ### dataset parameters\n  num_classes = attr.ib(default=7)\n  max_seq_length = attr.ib(default=1433)\n  ### NGM parameters\n  distance_type = attr.ib(default=nsl.configs.DistanceType.L2)\n  graph_regularization_multiplier = attr.ib(default=0.1)\n  num_neighbors = attr.ib(default=1)\n  ### model architecture\n  num_fc_units = attr.ib(default=[50, 50])\n  ### training parameters\n  train_epochs = attr.ib(default=10)\n  batch_size = attr.ib(default=128)\n  dropout_rate = attr.ib(default=0.5)\n  ### eval parameters\n  eval_steps = attr.ib(default=None)  # Every test instance is evaluated.\n\n\ndef get_hyper_parameters():\n  """"""Returns the hyper-parameters used for training.""""""\n  hparams = HParams()\n  if FLAGS.train_epochs:\n    hparams.train_epochs = FLAGS.train_epochs\n  if FLAGS.eval_steps:\n    hparams.eval_steps = FLAGS.eval_steps\n  return hparams\n\n\ndef load_dataset(filename):\n  """"""Reads a file in the `.tfrecord` format.\n\n  Args:\n    filename: Name of the file containing `tf.train.Example` objects.\n\n  Returns:\n    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n    objects.\n  """"""\n  return tf.data.TFRecordDataset([filename])\n\n\ndef make_dataset(file_path, training, include_nbr_features, hparams):\n  """"""Returns a `tf.data.Dataset` instance based on data in `file_path`.""""""\n\n  def parse_example(example_proto):\n    """"""Extracts relevant fields from the `example_proto`.\n\n    Args:\n      example_proto: An instance of `tf.train.Example`.\n\n    Returns:\n      A pair whose first value is a dictionary containing relevant features\n      and whose second value contains the ground truth labels.\n    """"""\n    # The \'words\' feature is a multi-hot, bag-of-words representation of the\n    # original raw text. A default value is required for examples that don\'t\n    # have the feature.\n    feature_spec = {\n        \'words\':\n            tf.io.FixedLenFeature([hparams.max_seq_length],\n                                  tf.int64,\n                                  default_value=tf.constant(\n                                      0,\n                                      dtype=tf.int64,\n                                      shape=[hparams.max_seq_length])),\n        \'label\':\n            tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n    }\n    if include_nbr_features:\n      for i in range(hparams.num_neighbors):\n        nbr_feature_key = \'{}{}_{}\'.format(NBR_FEATURE_PREFIX, i, \'words\')\n        nbr_weight_key = \'{}{}{}\'.format(NBR_FEATURE_PREFIX, i,\n                                         NBR_WEIGHT_SUFFIX)\n        nbr_id_key = \'{}{}_{}\'.format(NBR_FEATURE_PREFIX, i, \'id\')\n        feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n            [hparams.max_seq_length],\n            tf.int64,\n            default_value=tf.constant(\n                0, dtype=tf.int64, shape=[hparams.max_seq_length]))\n        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n            [1], tf.float32, default_value=tf.constant([0.0]))\n        feature_spec[nbr_id_key] = tf.io.FixedLenFeature(\n            (), tf.string, default_value=\'\')\n\n    features = tf.io.parse_single_example(example_proto, feature_spec)\n\n    labels = features.pop(\'label\')\n    return features, labels\n\n  # If the dataset is sharded, the following code may be required:\n  # filenames = tf.data.Dataset.list_files(file_path, shuffle=True)\n  # dataset = filenames.interleave(load_dataset, cycle_length=1)\n  dataset = load_dataset(file_path)\n  if training:\n    dataset = dataset.shuffle(10000)\n  dataset = dataset.map(parse_example)\n  dataset = dataset.batch(hparams.batch_size)\n  return dataset\n\n\ndef make_mlp_sequential_model(hparams):\n  """"""Creates a sequential multi-layer perceptron model.""""""\n  model = tf.keras.Sequential()\n  model.add(\n      tf.keras.layers.InputLayer(\n          input_shape=(hparams.max_seq_length,), name=\'words\'))\n  # Input is already one-hot encoded in the integer format. We cast it to\n  # floating point format here.\n  model.add(\n      tf.keras.layers.Lambda(lambda x: tf.keras.backend.cast(x, tf.float32)))\n  for num_units in hparams.num_fc_units:\n    model.add(tf.keras.layers.Dense(num_units, activation=\'relu\'))\n    model.add(tf.keras.layers.Dropout(hparams.dropout_rate))\n  model.add(tf.keras.layers.Dense(hparams.num_classes, activation=\'softmax\'))\n  return model\n\n\ndef make_mlp_functional_model(hparams):\n  """"""Creates a functional API-based multi-layer perceptron model.""""""\n  inputs = tf.keras.Input(\n      shape=(hparams.max_seq_length,), dtype=\'int64\', name=\'words\')\n\n  # Input is already one-hot encoded in the integer format. We cast it to\n  # floating point format here.\n  cur_layer = tf.keras.layers.Lambda(\n      lambda x: tf.keras.backend.cast(x, tf.float32))(\n          inputs)\n\n  for num_units in hparams.num_fc_units:\n    cur_layer = tf.keras.layers.Dense(num_units, activation=\'relu\')(cur_layer)\n    # For functional models, by default, Keras ensures that the \'dropout\' layer\n    # is invoked only during training.\n    cur_layer = tf.keras.layers.Dropout(hparams.dropout_rate)(cur_layer)\n\n  outputs = tf.keras.layers.Dense(\n      hparams.num_classes, activation=\'softmax\')(\n          cur_layer)\n\n  model = tf.keras.Model(inputs, outputs=outputs)\n  return model\n\n\ndef make_mlp_subclass_model(hparams):\n  """"""Creates a multi-layer perceptron subclass model in Keras.""""""\n\n  class MLP(tf.keras.Model):\n    """"""Subclass model defining a multi-layer perceptron.""""""\n\n    def __init__(self):\n      super(MLP, self).__init__()\n      self.cast_to_float_layer = tf.keras.layers.Lambda(\n          lambda x: tf.keras.backend.cast(x, tf.float32))\n      self.dense_layers = [\n          tf.keras.layers.Dense(num_units, activation=\'relu\')\n          for num_units in hparams.num_fc_units\n      ]\n      self.dropout_layer = tf.keras.layers.Dropout(hparams.dropout_rate)\n      self.output_layer = tf.keras.layers.Dense(\n          hparams.num_classes, activation=\'softmax\')\n\n    def call(self, inputs, training=False):\n      cur_layer = self.cast_to_float_layer(inputs[\'words\'])\n      for dense_layer in self.dense_layers:\n        cur_layer = dense_layer(cur_layer)\n        cur_layer = self.dropout_layer(cur_layer, training=training)\n\n      outputs = self.output_layer(cur_layer)\n\n      return outputs\n\n  return MLP()\n\n\ndef log_metrics(model_desc, eval_metrics):\n  """"""Logs evaluation metrics at `logging.INFO` level.\n\n  Args:\n    model_desc: A description of the model.\n    eval_metrics: A dictionary mapping metric names to corresponding values. It\n      must contain the loss and accuracy metrics.\n  """"""\n  logging.info(\'\\n\')\n  logging.info(\'Eval accuracy for %s: %s\', model_desc, eval_metrics[\'accuracy\'])\n  logging.info(\'Eval loss for %s: %s\', model_desc, eval_metrics[\'loss\'])\n  if \'graph_loss\' in eval_metrics:\n    logging.info(\'Eval graph loss for %s: %s\', model_desc,\n                 eval_metrics[\'graph_loss\'])\n\n\ndef train_and_evaluate(model, model_desc, train_dataset, test_dataset, hparams):\n  """"""Compiles, trains, and evaluates a `Keras` model.\n\n  Args:\n    model: An instance of `tf.Keras.Model`.\n    model_desc: A description of the model.\n    train_dataset: An instance of `tf.data.Dataset` representing training data.\n    test_dataset: An instance of `tf.data.Dataset` representing test data.\n    hparams: An instance of `Hparams`.\n  """"""\n  model.compile(\n      optimizer=\'adam\',\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n      metrics=[\'accuracy\'])\n  model.fit(train_dataset, epochs=hparams.train_epochs, verbose=1)\n  eval_results = dict(\n      zip(model.metrics_names,\n          model.evaluate(test_dataset, steps=hparams.eval_steps)))\n  log_metrics(model_desc, eval_results)\n\n\ndef main(argv):\n  # Check that the correct number of arguments have been provided. The\n  # training and test data should contain \'tf.train.Example\' objects in the\n  # TFRecord format.\n  if len(argv) != 3:\n    raise app.UsageError(\'Invalid number of arguments; expected 2, got %d\' %\n                         (len(argv) - 1))\n\n  hparams = get_hyper_parameters()\n  train_data_path = argv[1]\n  test_data_path = argv[2]\n\n  # Graph regularization configuration.\n  graph_reg_config = nsl.configs.make_graph_reg_config(\n      max_neighbors=hparams.num_neighbors,\n      multiplier=hparams.graph_regularization_multiplier,\n      distance_type=hparams.distance_type,\n      sum_over_axis=-1)\n\n  # Create the base MLP models.\n  base_models = {\n      \'FUNCTIONAL\': make_mlp_functional_model(hparams),\n      \'SEQUENTIAL\': make_mlp_sequential_model(hparams),\n      \'SUBCLASS\': make_mlp_subclass_model(hparams)\n  }\n  for base_model_tag, base_model in base_models.items():\n    logging.info(\'\\n====== %s BASE MODEL TEST BEGIN ======\', base_model_tag)\n    train_dataset = make_dataset(train_data_path, True, False, hparams)\n    test_dataset = make_dataset(test_data_path, False, False, hparams)\n    train_and_evaluate(base_model, \'Base MLP model\', train_dataset,\n                       test_dataset, hparams)\n\n    logging.info(\'\\n====== TRAINING WITH GRAPH REGULARIZATION ======\\n\')\n\n    # Wrap the base MLP model with graph regularization.\n    graph_reg_model = nsl.keras.GraphRegularization(base_model,\n                                                    graph_reg_config)\n    train_dataset = make_dataset(train_data_path, True, True, hparams)\n    test_dataset = make_dataset(test_data_path, False, False, hparams)\n    train_and_evaluate(graph_reg_model, \'MLP + graph regularization\',\n                       train_dataset, test_dataset, hparams)\n\n    logging.info(\'\\n====== %s BASE MODEL TEST END ======\', base_model_tag)\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.enable_v2_behavior()\n  app.run(main)\n'"
neural_structured_learning/keras/__init__.py,0,"b'""""""Keras APIs for Neural Structured Learning.""""""\n\nfrom neural_structured_learning.keras import layers\nfrom neural_structured_learning.keras.adversarial_regularization import adversarial_loss\nfrom neural_structured_learning.keras.adversarial_regularization import AdversarialRegularization\nfrom neural_structured_learning.keras.graph_regularization import GraphRegularization\n\n__all__ = [\n    \'adversarial_loss\', \'AdversarialRegularization\', \'GraphRegularization\',\n    \'layers\'\n]\n'"
neural_structured_learning/keras/adversarial_regularization.py,68,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Incorporates adversarial regularization into a Keras model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\nimport types\n\nimport attr\nimport neural_structured_learning.configs as nsl_configs\nimport neural_structured_learning.lib as nsl_lib\nimport six\nimport tensorflow as tf\n\n\ndef adversarial_loss(features,\n                     labels,\n                     model,\n                     loss_fn,\n                     sample_weights=None,\n                     adv_config=None,\n                     predictions=None,\n                     labeled_loss=None,\n                     gradient_tape=None,\n                     model_kwargs=None):\n  """"""Computes the adversarial loss for `model` given `features` and `labels`.\n\n  This utility function adds adversarial perturbations to the input `features`,\n  runs the `model` on the perturbed features for predictions, and returns the\n  corresponding loss `loss_fn(labels, model(perturbed_features))`. This function\n  can be used in a Keras subclassed model and a custom training loop. This can\n  also be used freely as a helper function in eager execution mode.\n\n  The adversarial perturbation is based on the gradient of the labeled loss on\n  the original input features, i.e. `loss_fn(labels, model(features))`.\n  Therefore, this function needs to compute the model\'s predictions on the input\n  features as `model(features)`, and the labeled loss as `loss_fn(labels,\n  predictions)`. If predictions or labeled loss have already been computed, they\n  can be passed in via the `predictions` and `labeled_loss` arguments in order\n  to save computational resources. Note that in eager execution mode,\n  `gradient_tape` needs to be set accordingly when passing in `predictions` or\n  `labeled_loss`, so that the gradient can be computed correctly.\n\n  Example:\n  ```python\n  # A linear regression model (for demonstrating the usage only)\n  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(2,))])\n  loss_fn = tf.keras.losses.MeanSquaredError()\n  optimizer = tf.keras.optimizers.SGD()\n\n  # Custom training loop. (The actual training data is omitted for clarity.)\n  for x, y in train_dataset:\n    with tf.GradientTape() as tape_w:\n\n      # A separate GradientTape is needed for watching the input.\n      with tf.GradientTape() as tape_x:\n        tape_x.watch(x)\n\n        # Regular forward pass.\n        labeled_loss = loss_fn(y, model(x))\n\n      # Calculates the adversarial loss. This will reuse labeled_loss and will\n      # consume tape_x.\n      adv_loss = nsl.keras.adversarial_loss(\n          x, y, model, loss_fn, labeled_loss=labeled_loss, gradient_tape=tape_x)\n\n      # Combines both losses. This could also be a weighted combination.\n      total_loss = labeled_loss + adv_loss\n\n    # Regular backward pass.\n    gradients = tape_w.gradient(total_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n  ```\n\n  Arguments:\n    features: Input features, should be a `Tensor` or a collection of `Tensor`\n      objects. If it is a collection, the first dimension of all `Tensor`\n      objects inside should be the same (i.e. batch size).\n    labels: Target labels.\n    model: A callable that takes `features` as inputs and computes `predictions`\n      as outputs. An example would be a `tf.keras.Model` object.\n    loss_fn: A callable which calcualtes labeled loss from `labels`,\n      `predictions`, and `sample_weights`. An example would be a\n      `tf.keras.losses.Loss` object.\n    sample_weights: (optional) A 1-D `Tensor` of weights for the examples, with\n      the same length as the first dimension of `features`.\n    adv_config: (optional) An `nsl.configs.AdvRegConfig` object for adversarial\n      regularization hyperparameters. Use `nsl.configs.make_adv_reg_config` to\n      construct one.\n    predictions: (optional) Precomputed value of `model(features)`. If set, the\n      value will be reused when calculating adversarial regularization. In eager\n      mode, the `gradient_tape` has to be set as well.\n    labeled_loss: (optional) Precomputed value of `loss_fn(labels,\n      model(features))`. If set, the value will be reused when calculating\n      adversarial regularization. In eager mode, the `gradient_tape` has to be\n      set as well.\n    gradient_tape: (optional) A `tf.GradientTape` object watching `features`.\n    model_kwargs: (optional) A dictionary of additional keyword arguments to be\n      passed to the `model`.\n\n  Returns:\n    A `Tensor` for adversarial regularization loss, i.e. labeled loss on\n    adversarially perturbed features.\n  """"""\n\n  if adv_config is None:\n    adv_config = nsl_configs.AdvRegConfig()\n\n  if model_kwargs is not None:\n    model = functools.partial(model, **model_kwargs)\n\n  # Calculates labeled_loss if not provided.\n  if labeled_loss is None:\n    # Reuses the tape if provided; otherwise creates a new tape.\n    gradient_tape = gradient_tape or tf.GradientTape()\n    with gradient_tape:\n      gradient_tape.watch(tf.nest.flatten(features))\n      # Calculates prediction if not provided.\n      predictions = predictions if predictions is not None else model(features)\n      labeled_loss = loss_fn(labels, predictions, sample_weights)\n\n  adv_input, adv_sample_weights = nsl_lib.gen_adv_neighbor(\n      features,\n      labeled_loss,\n      config=adv_config.adv_neighbor_config,\n      gradient_tape=gradient_tape,\n      pgd_model_fn=model,\n      pgd_loss_fn=functools.partial(loss_fn, sample_weights=sample_weights),\n      pgd_labels=labels)\n  adv_output = model(adv_input)\n  if sample_weights is not None:\n    adv_sample_weights = tf.math.multiply(sample_weights, adv_sample_weights)\n  adv_loss = loss_fn(labels, adv_output, adv_sample_weights)\n  return adv_loss\n\n\nclass _LossWrapper(tf.keras.losses.Loss):\n  """"""Wrapper converting a loss function into a `Loss` object.\n\n  This is to reuse logic of sample-weighted loss computation in `Loss` base\n  class.\n\n  Attributes:\n    loss_fn: Underlying loss function.\n    weight: Weight of this loss term in total loss. Should be applied outside\n      of this class, e.g. `total_loss += loss.weight * loss(y_true, y_pred)`.\n    batch_size_reduction: Whether to perform `SUM_OVER_BATCH_SIZE` reduction.\n      This field is set in lieu of having `reduction=SUM_OVER_BATCH_SIZE`,\n      because the latter is not supported when using with\n      `tf.distribute.Strategy`.\n  """"""\n\n  def __init__(self, loss_fn, name, weight):\n    reduction = getattr(loss_fn, \'reduction\', None)\n    if reduction in (None, tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n                     tf.compat.v2.losses.Reduction.AUTO):\n      reduction = tf.losses.Reduction.NONE\n      self.batch_size_reduction = True\n    else:\n      self.batch_size_reduction = False\n    super(_LossWrapper, self).__init__(name=name, reduction=reduction)\n    self.weight = weight\n    if isinstance(loss_fn, tf.keras.losses.Loss) and self.batch_size_reduction:\n      self.loss_fn = loss_fn.__class__.from_config(loss_fn.get_config())\n      self.loss_fn.reduction = tf.losses.Reduction.NONE\n    else:\n      self.loss_fn = loss_fn\n\n  def call(self, y_true, y_pred):\n    return self.loss_fn(y_true, y_pred)\n\n  def __call__(self, *args, **kwargs):\n    if isinstance(self.loss_fn, tf.keras.losses.Loss):\n      loss_value = self.loss_fn(*args, **kwargs)\n    else:\n      loss_value = super(_LossWrapper, self).__call__(*args, **kwargs)\n    if self.batch_size_reduction:\n      size = tf.cast(tf.size(loss_value), dtype=loss_value.dtype)\n      loss_value = tf.math.divide_no_nan(tf.math.reduce_sum(loss_value), size)\n    return loss_value\n\n  def _is_sparse_categorical_loss(self):\n    return self.loss_fn == tf.keras.losses.sparse_categorical_crossentropy or (\n        isinstance(self.loss_fn, tf.keras.losses.SparseCategoricalCrossentropy))\n\n  def _is_binary_classification_loss(self):\n    return self.loss_fn in (\n        tf.keras.losses.binary_crossentropy,\n        tf.keras.losses.hinge, tf.keras.losses.squared_hinge) or isinstance(\n            self.loss_fn, (tf.keras.losses.BinaryCrossentropy,\n                           tf.keras.losses.Hinge, tf.keras.losses.SquaredHinge))\n\n  def resolve_metric(self, metric):\n    """"""Resolves potentially ambiguous metric name based on the loss function.""""""\n    # This method is intended for the scenario that a Keras model is compiled\n    # with a metric which meaning depends on the learning task. For example,\n    # `\'accuracy\'` may refer to `tf.keras.metrics.binary_accuracy` for binary\n    # classification tasks, to `tf.keras.metrics.categorical_accuracy` for\n    # multi-class classification tasks with one-hot labels, or to\n    # `tf.keras.metrics.sparse_categorical_accuracy` for mult-class\n    # classification tasks with index labels. In such scenario the loss\n    # function can help deduce the desired metric function since they share the\n    # same input `(y_true, y_pred)`.\n    # The list of such metrics is defined in `get_metric_function()` in\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py\n    if metric not in (\'accuracy\', \'acc\', \'crossentropy\', \'ce\'):\n      return metric\n    if self._is_binary_classification_loss():\n      prefix = \'binary_\'\n    elif self._is_sparse_categorical_loss():\n      prefix = \'sparse_categorical_\'\n    else:\n      prefix = \'categorical_\'\n    suffix = \'accuracy\' if metric in (\'accuracy\', \'acc\') else \'crossentropy\'\n    return prefix + suffix\n\n\ndef _prepare_loss_fns(loss, output_names):\n  """"""Converts `loss` to a list of per-output loss functions or objects.""""""\n  # losses for multiple outputs indexed by name\n  if isinstance(loss, collections.Mapping):\n    for name in output_names:\n      if name not in loss:\n        raise ValueError(\n            \'Loss for {} not found in `loss` dictionary.\'.format(name))\n    return [tf.keras.losses.get(loss[name]) for name in output_names]\n\n  # loss for single output, or shared loss fn for multiple outputs\n  if isinstance(loss, six.string_types):\n    return [tf.keras.losses.get(loss) for _ in output_names]\n\n  # losses for multiple outputs indexed by position\n  if isinstance(loss, collections.Sequence):\n    if len(loss) != len(output_names):\n      raise ValueError(\'`loss` should have the same number of elements as \'\n                       \'model output\')\n    return six.moves.map(tf.keras.losses.get, loss)\n\n  # loss for single output, or shared loss fn for multiple outputs\n  return [tf.keras.losses.get(loss) for _ in output_names]\n\n\ndef _prepare_loss_weights(loss_weights, output_names):\n  """"""Converts `loss_weights` to a list of float values.""""""\n  if loss_weights is None:\n    return [1.0] * len(output_names)\n\n  if isinstance(loss_weights, collections.Sequence):\n    if len(loss_weights) != len(output_names):\n      raise ValueError(\'`loss_weights` should have the same number of elements \'\n                       \'as model output\')\n    return list(map(float, loss_weights))\n\n  if isinstance(loss_weights, collections.Mapping):\n    for name in output_names:\n      if name not in loss_weights:\n        raise ValueError(\'Loss weight for {} not found in `loss_weights` \'\n                         \'dictionary.\'.format(name))\n    return [float(loss_weights[name]) for name in output_names]\n\n  raise TypeError(\'`loss_weights` must be a list or a dict, \'\n                  \'got {}\'.format(str(loss_weights)))\n\n\ndef _clone_metrics(metrics):\n  """"""Creates a copy of the maybe-nested metric specification.\n\n  Args:\n    metrics: A collection of metric specifications. Supports the same set of\n      formats as the `metrics` argument in `tf.keras.Model.compile`.\n\n  Returns:\n    The same format as the `metrics` argument, with all `tf.keras.metric.Metric`\n    objects replaced by their copies.\n  """"""\n\n  def clone(metric):\n    # A `Metric` object is stateful and can only be used in 1 model on 1 output.\n    # Cloning the object allows the same metric to be applied in both base and\n    # adversarial-regularized models, and also on multiple outputs in one model.\n    # The cloning logic is the same as the `clone_metric` function in\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/metrics.py\n    if not isinstance(metric, tf.keras.metrics.Metric):\n      return metric\n    with tf.init_scope():\n      return metric.__class__.from_config(metric.get_config())\n\n  return tf.nest.map_structure(clone, metrics)\n\n\ndef _prepare_metric_fns(metrics, output_names, loss_wrappers):\n  """"""Converts `metrics` into a list of per-output list of metrics.\n\n  Args:\n    metrics: List of metrics to be evaluated during training and testing. Each\n      metric can be specified using a string (e.g. `\'accuracy\'`) or a\n      `tf.keras.metrics.Metric` object. For multi-output model, this can also be\n      a dictionary like `{\'output1\': [\'metric\'], \'output2\': [\'metric2\']}`.\n      See the `metrics` argument in `tf.keras.Model.compile`.\n    output_names: List of names of the model\'s output. If `metrics` is a\n      dictionary, the names in this list will be taken as lookup keys.\n    loss_wrappers: List of `_LossWrapper` objects corresponding to each output.\n\n  Returns:\n    A list of the same length as `output_names`, where each element is a list of\n    callables representing the metrics to be evaluated on the corresponding\n    output.\n  """"""\n  if metrics is None:\n    return [[] for _ in output_names]\n\n  if not isinstance(metrics, (list, collections.Mapping)):\n    raise TypeError(\'`metrics` must be a list or a dict, got {}\'.format(\n        str(metrics)))\n\n  to_list = lambda x: x if isinstance(x, list) else [x]\n\n  if isinstance(metrics, collections.Mapping):\n    # Converts `metrics` from a dictionary to a list of lists using the order\n    # specified in `output_names`.\n    metrics = [to_list(metrics.get(name, [])) for name in output_names]\n\n  if not any(isinstance(m, list) for m in metrics):\n    # Replicates `metrics` to be a list of lists if it is a plain list of\n    # metrics, so that all metrics can be applied to each output.\n    metrics = [metrics] + [_clone_metrics(metrics) for _ in output_names[1:]]\n\n  # Here `metrics` is a list of lists, and each sub-list corresponds to metrics\n  # to be applied on an output.\n  if len(metrics) != len(output_names):\n    raise ValueError(\'The number of sub-lists in `metrics` should be the \'\n                     \'same as model output.\')\n\n  metric_fns = []\n  for per_output_metrics, loss_wrapper in zip(metrics, loss_wrappers):\n    metric_fns.append([\n        tf.keras.metrics.get(loss_wrapper.resolve_metric(metric))\n        for metric in to_list(per_output_metrics)\n    ])\n  return metric_fns\n\n\ndef _compute_loss_and_metrics(losses,\n                              metrics,\n                              labels,\n                              outputs,\n                              sample_weights=None):\n  """"""Computes total loss and (loss value, loss name) pairs for metrics.\n\n  Args:\n    losses: List of `_LossWrapper` objects to be evaluated on corresponding\n      outputs. Must have the same length as `labels` and `outputs`.\n    metrics: List of list of (metric fn, metric name) pairs, for additional\n      metrics to report for each output. Must have the same length as `outputs`.\n      If set to `None`, no additional metrics will be reported.\n    labels: List of `Tensor` objects of ground truth targets. Must have the same\n      length as `losses` and `outputs`.\n    outputs: List of `Tensor` objects of predicted targets. Must have the same\n      length as `losses` and `labels`.\n    sample_weights: (optional) `Tensor` of weight for the loss of each sample.\n\n  Returns:\n    total_loss: Weighted sum of losses on all outputs.\n    metrics: List of (value, aggregation, name) tuples for metric reporting.\n  """"""\n  outputs = tf.nest.flatten(outputs)\n  total_loss, output_metrics = [], []\n  if metrics is None:\n    metrics = [[]] * len(losses)\n  for (label, output, loss, per_output_metrics) in zip(labels, outputs, losses,\n                                                       metrics):\n    loss_value = loss(label, output, sample_weights)\n    total_loss.append(loss.weight * loss_value)\n    output_metrics.append((loss_value, \'mean\', loss.name))\n    for metric_fn, metric_name in per_output_metrics:\n      value = metric_fn(label, output)\n      # Metric objects always return an aggregated result, and shouldn\'t be\n      # aggregated again.\n      if isinstance(metric_fn, tf.keras.metrics.Metric):\n        aggregation = None\n      else:\n        aggregation = \'mean\'\n      output_metrics.append((value, aggregation, metric_name))\n  return tf.add_n(total_loss), output_metrics\n\n\nclass AdversarialRegularization(tf.keras.Model):\n  """"""Wrapper thats adds adversarial regularization to a given `tf.keras.Model`.\n\n  This model will reuse the layers and variables as the given `base_model`, so\n  training this model will also update the variables in the `base_model`. The\n  adversarial regularization can be configured by `adv_config`. (See\n  `nsl.configs.AdvRegConfig` for the hyperparameters.) The regularization term\n  will be added into training objective, and will be minimized during training\n  together with other losses specified in `compile()`.\n\n  This model expects its input to be a dictionary mapping feature names to\n  feature values. The dictionary should contain both input data (`x`) and target\n  data (`y`). The feature names of the target data should be passed to this\n  model\'s constructor in `label_keys`, so the model can distinguish between\n  input data and target data. If your samples are weighted, the sample weight\n  should also be a feature in the dictionary, and its name should be passed to\n  the constructor in `sample_weight_key`. When calling this model\'s `fit()` or\n  `evaluate()` method, the argument `y` should not be set because the target\n  data is already in the input dictionary. The dictionary format also implies\n  that the input has to be named, i.e. the `name` argument of `tf.keras.Input()`\n  should be set.\n\n  Example:\n\n  ```python\n  # A linear regression model (for demonstrating the usage only)\n  base_model = tf.keras.Sequential([\n      tf.keras.Input(shape=(2,), name=\'input\'),\n      tf.keras.layers.Dense(1),\n  ])\n\n  # Applies the wrapper, with 0.2 as regularization weight.\n  adv_config = nsl.configs.make_adv_reg_config(multiplier=0.2)\n  adv_model = nsl.keras.AdversarialRegularization(base_model,\n                                                  label_keys=[\'label\'],\n                                                  adv_config=adv_config)\n\n  # Compiles the model as usual.\n  adv_model.compile(optimizer=\'adam\', loss=\'mean_squared_error\')\n\n  # Trains the model. (The actual training data is omitted for clarity.)\n  # The model minimizes (mean_squared_error + 0.2 * adversarial_regularization).\n  adv_model.fit(x={\'input\': x_train, \'label\': y_train}, batch_size=32)\n  ```\n\n  It is recommended to use `tf.data.Dataset` to handle the dictionary format\n  requirement of the input, especially when using the `validation_data` argument\n  in `fit()`.\n\n  ```python\n  train_data = tf.data.Dataset.from_tensor_slices(\n      {\'input\': x_train, \'label\': y_train}).batch(batch_size)\n  val_data = tf.data.Dataset.from_tensor_slices(\n      {\'input\': x_val, \'label\': y_val}).batch(batch_size)\n  val_steps = x_val.shape[0] / batch_size\n  adv_model.fit(train_data, validation_data=val_data,\n                validation_steps=val_steps, epochs=2, verbose=1)\n  ```\n  """"""\n\n  def __init__(self,\n               base_model,\n               label_keys=(\'label\',),\n               sample_weight_key=None,\n               adv_config=None,\n               base_with_labels_in_features=False):\n    """"""Constructor of `AdversarialRegularization` class.\n\n    Args:\n      base_model: A `tf.Keras.Model` to which adversarial regularization will be\n        applied.\n      label_keys: A tuple of strings denoting which keys in the input features\n        (a `dict` mapping keys to tensors) represent labels. This list should be\n        1-to-1 corresponding to the output of the `base_model`.\n      sample_weight_key: A string denoting which key in the input feature (a\n        `dict` mapping keys to tensors) represents sample weight. If not set,\n        the weight is 1.0 for each input example.\n      adv_config: Instance of `nsl.configs.AdvRegConfig` for configuring\n        adversarial regularization.\n      base_with_labels_in_features: A Boolean value indicating whether the base\n        model expects label features as input. This option is effective only\n        when the base model is a subclassed Keras model. (For functional and\n        Sequential models, the expected inputs can be inferred from the model\n        itself.) If set to true, the base model will be called with an input\n        dictionary including label and sample-weight features. If set to false,\n        label and sample-weight features will not present in base model\'s input\n        dictionary.\n    """"""\n    super(AdversarialRegularization,\n          self).__init__(name=\'AdversarialRegularization\')\n    self.base_model = base_model\n    self.label_keys = label_keys\n    self.sample_weight_key = sample_weight_key\n    self.adv_config = adv_config or nsl_configs.AdvRegConfig()\n    self._base_with_labels_in_features = base_with_labels_in_features\n\n  def compile(self,\n              optimizer,\n              loss=None,\n              metrics=None,\n              loss_weights=None,\n              **kwargs):\n    if loss:\n      self._compile_arg_loss = loss\n      self._compile_arg_loss_weights = loss_weights\n      self._compile_arg_metrics = metrics\n      self._labeled_losses = None\n      self._labeled_metrics = None\n\n    # Compiles base model with saved losses and metrics.\n    self.base_model.compile(\n        optimizer,\n        loss=self._compile_arg_loss,\n        metrics=_clone_metrics(self._compile_arg_metrics),\n        loss_weights=self._compile_arg_loss_weights,\n        **kwargs)\n\n    if getattr(self.base_model, \'output_names\', None):\n      # Organizes losses after the base model is fully compiled. The output\n      # names from the base model is needed in case the loss (and/or\n      # loss_weights) is specified in a dict().\n      self._build_loss_and_metric_fns(self.base_model.output_names)\n\n    # Hides losses and metrics for parent class so the model won\'t expect\n    # separate label input (parameter `y`) in fit() and evaluate().\n    super(AdversarialRegularization, self).compile(optimizer, **kwargs)\n\n  def _make_metric_name(self, fn, label):\n    """"""Generates a unique name, and resolves conflicts by appending a number.""""""\n    if isinstance(fn, types.FunctionType):\n      base_name = fn.__name__\n    else:\n      base_name = getattr(fn, \'name\', fn.__class__.__name__)\n    if len(self.label_keys) > 1:\n      # If there are more than one output, disambigaute losses by corresponding\n      # label name.\n      base_name += \'_\' + label\n    if base_name not in self._metric_name_count:\n      self._metric_name_count[base_name] = 1\n      return base_name\n    else:\n      self._metric_name_count[base_name] += 1\n      return \'{}_{}\'.format(base_name, self._metric_name_count[base_name])\n\n  def _build_loss_and_metric_fns(self, output_names):\n    self._metric_name_count = collections.Counter()\n    self._build_labeled_losses(output_names)\n    self._build_labeled_metrics(output_names, self._labeled_losses)\n    del self._metric_name_count  # no longer needed\n\n  def _build_labeled_losses(self, output_names):\n    if self._labeled_losses:\n      return  # Losses are already populated.\n\n    if len(output_names) != len(self.label_keys):\n      raise ValueError(\'The model has different number of outputs and labels. \'\n                       \'({} vs. {})\'.format(\n                           len(output_names), len(self.label_keys)))\n\n    loss_fns = _prepare_loss_fns(self._compile_arg_loss, output_names)\n    loss_weights = _prepare_loss_weights(self._compile_arg_loss_weights,\n                                         output_names)\n    self._labeled_losses = []\n    for loss_fn, loss_weight, label_key in zip(loss_fns, loss_weights,\n                                               self.label_keys):\n      loss_name = self._make_metric_name(loss_fn, label_key)\n      self._labeled_losses.append(_LossWrapper(loss_fn, loss_name, loss_weight))\n\n  def _build_labeled_metrics(self, output_names, labeled_losses):\n    if self._labeled_metrics:\n      return  # Metrics are already populated.\n\n    metric_fn_lists = _prepare_metric_fns(self._compile_arg_metrics,\n                                          output_names, labeled_losses)\n    self._labeled_metrics = []\n    for metric_fns, label_key in zip(metric_fn_lists, self.label_keys):\n      per_output_metrics = []\n      for metric_fn in metric_fns:\n        metric_name = self._make_metric_name(metric_fn, label_key)\n        if isinstance(metric_fn, tf.keras.metrics.Metric):\n          # Updates the name of the Metric object to make sure it is unique.\n          metric_fn._name = metric_name  # pylint: disable=protected-access\n        per_output_metrics.append((metric_fn, metric_name))\n      self._labeled_metrics.append(per_output_metrics)\n\n  def _get_or_create_base_output_names(self, outputs):\n    output_names = getattr(self.base_model, \'output_names\', None)\n    if not output_names:\n      num_output = len(tf.nest.flatten(outputs))\n      output_names = [\'output_%d\' % i for i in range(1, num_output + 1)]\n    return output_names\n\n  def _compute_total_loss(self, labels, outputs, sample_weights=None):\n    # `None` is passed instead of the actual metrics in order to skip computing\n    # metric values and updating metric states.\n    loss, _ = _compute_loss_and_metrics(self._labeled_losses, None, labels,\n                                        outputs, sample_weights)\n    return loss\n\n  def _extract_labels_and_weights(self, inputs):\n    sample_weights = inputs.get(self.sample_weight_key, None)\n    if sample_weights is not None:\n      sample_weights = tf.stop_gradient(sample_weights)\n    # Labels shouldn\'t be perturbed when generating adversarial examples.\n    labels = [\n        tf.stop_gradient(inputs[label_key]) for label_key in self.label_keys\n    ]\n    return labels, sample_weights\n\n  def _remove_labels_and_weights(self, inputs):\n    non_feature_keys = set(self.label_keys).union([self.sample_weight_key])\n    return {\n        key: value\n        for key, value in six.iteritems(inputs)\n        if key not in non_feature_keys\n    }\n\n  def _call_base_model(self, inputs, **kwargs):\n    base_input_names = getattr(self.base_model, \'input_names\', [])\n    if (isinstance(self.base_model, tf.keras.Sequential) and\n        not set(base_input_names) & set(inputs.keys())):\n      # In some cases, Sequential models are automatically compiled to graph\n      # networks with automatically generated input names. In this case, the\n      # user isn\'t expected to know those names, so we just flatten the inputs.\n      # But the input names are sometimes meaningful (e.g. DenseFeatures layer).\n      # We check if there is any intersection between the user-provided names\n      # and model\'s input names. If there is, we assume the names are meaningful\n      # and do name-based lookup in the next branch.\n      inputs = tf.nest.flatten(self._remove_labels_and_weights(inputs))\n    elif self.base_model._is_graph_network:  # pylint: disable=protected-access\n      if base_input_names:\n        # Converts input dictionary to a list so it conforms with the model\'s\n        # expected input.\n        inputs = [inputs[name] for name in base_input_names]\n    elif not self._base_with_labels_in_features:\n      # Removes labels and sample weights from the input dictionary, since they\n      # are only used in this class and base model does not need them as inputs.\n      inputs = self._remove_labels_and_weights(inputs)\n    return self.base_model(inputs, **kwargs)\n\n  def _forward_pass(self, inputs, labels, sample_weights, base_model_kwargs):\n    """"""Runs the usual forward pass to compute outputs, loss, and metrics.""""""\n    with tf.GradientTape() as tape:\n      tape.watch(tf.nest.flatten(inputs))\n      outputs = self._call_base_model(inputs, **base_model_kwargs)\n      # If the base_model is a subclassed model, its output_names are not\n      # available before its first call. If it is a dynamic subclassed model,\n      # its output_names are not available even after its first call, so we\n      # create names to match the number of outputs.\n      self._build_loss_and_metric_fns(\n          self._get_or_create_base_output_names(outputs))\n      labeled_loss, metrics = _compute_loss_and_metrics(self._labeled_losses,\n                                                        self._labeled_metrics,\n                                                        labels, outputs,\n                                                        sample_weights)\n    return outputs, labeled_loss, metrics, tape\n\n  def call(self, inputs, **kwargs):\n    if any(key not in inputs for key in self.label_keys):\n      # This is to prevent ""no loss to optimize"" error when the first call to\n      # the model is without label input.\n      raise ValueError(\'Labels are not in the input. For predicting examples \'\n                       \'without labels, please use the base model instead.\')\n\n    labels, sample_weights = self._extract_labels_and_weights(inputs)\n    outputs, labeled_loss, metrics, tape = self._forward_pass(\n        inputs, labels, sample_weights, kwargs)\n    self.add_loss(labeled_loss)\n    for value, aggregation, name in metrics:\n      self.add_metric(value, aggregation=aggregation, name=name)\n\n    # Adversarial loss.\n    adv_loss = adversarial_loss(\n        inputs,\n        labels,\n        self._call_base_model,\n        self._compute_total_loss,\n        sample_weights=sample_weights,\n        adv_config=self.adv_config,\n        labeled_loss=labeled_loss,\n        gradient_tape=tape,\n        model_kwargs=kwargs)\n    self.add_loss(self.adv_config.multiplier * adv_loss)\n    self.add_metric(adv_loss, name=\'adversarial_loss\', aggregation=\'mean\')\n    return outputs\n\n  def save(self, *args, **kwargs):\n    raise NotImplementedError(\n        \'Saving `AdversarialRegularization` models is currently not supported. \'\n        \'Consider using `save_weights` or saving the `base_model`.\')\n\n  def perturb_on_batch(self, x, **config_kwargs):\n    """"""Perturbs the given input to generates adversarial examples.\n\n    Args:\n      x: Input examples to be perturbed, in a dictionary of Numpy arrays,\n        `Tensor`, `SparseTensor`, or `RaggedTensor` objects. The first\n        dimension of all tensors or arrays should be the same (i.e. batch size).\n      **config_kwargs: (optional) hyperparameters for generating adversarial\n        preturbation. Any keyword argument here will overwrite the corresponding\n        field in `nsl.configs.AdvNeighborConfig` specified in `__init__`.\n        Acceptable keys: `feature_mask`, `adv_step_size`, `adv_grad_norm`,\n        `clip_value_min`, `clip_value_max`, `pgd_iterations`, and `pgd_epsilon`.\n\n    Returns:\n      A dictionary of NumPy arrays, `SparseTensor`, or `RaggedTensor` objects of\n      the generated adversarial examples.\n    """"""\n    inputs = tf.nest.map_structure(\n        tf.convert_to_tensor, x, expand_composites=True)\n    labels, sample_weights = self._extract_labels_and_weights(inputs)\n    _, labeled_loss, _, tape = self._forward_pass(inputs, labels,\n                                                  sample_weights,\n                                                  {\'training\': False})\n\n    config_kwargs = {k: v for k, v in config_kwargs.items() if v is not None}\n    config = attr.evolve(self.adv_config.adv_neighbor_config, **config_kwargs)\n    adv_inputs, _ = nsl_lib.gen_adv_neighbor(\n        inputs,\n        labeled_loss,\n        config=config,\n        gradient_tape=tape,\n        pgd_model_fn=self._call_base_model,\n        pgd_loss_fn=self._compute_total_loss,\n        pgd_labels=labels)\n\n    if tf.executing_eagerly():\n      # Converts `Tensor` objects to NumPy arrays and keeps other objects (e.g.\n      # `SparseTensor`) as-is.\n      adv_inputs = tf.nest.map_structure(\n          lambda x: x.numpy() if hasattr(x, \'numpy\') else x,\n          adv_inputs,\n          expand_composites=False)\n    else:\n      adv_inputs = tf.keras.backend.function([], adv_inputs)([])\n\n    # Inserts the labels and sample_weights back to the input dictionary, so\n    # the returned input has the same structure as the original input.\n    for label_key, label in zip(self.label_keys, labels):\n      adv_inputs[label_key] = label\n    if self.sample_weight_key is not None:\n      adv_inputs[self.sample_weight_key] = sample_weights\n\n    return adv_inputs\n'"
neural_structured_learning/keras/adversarial_regularization_multi_device_test.py,20,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.keras.adversarial_regularization.\n\nThe test cases here runs on multiple virtual devices.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.keras import adversarial_regularization\nimport numpy as np\nimport tensorflow as tf\n\nNUM_REPLICAS = 2\n\n\ndef _set_up_virtual_devices():\n  """"""Sets up virtual CPU or GPU devices.\n\n  This function has to be called before any TF ops has run, and can only be\n  called at such moment. The setting is effective for all test classes and\n  methods in this file.\n  """"""\n  if tf.test.is_gpu_available():\n    device_type = \'GPU\'\n    kwargs = {\'memory_limit\': 1000}  # 1G ram on each virtual GPU\n  else:\n    device_type = \'CPU\'\n    kwargs = {}\n  physical_devices = tf.config.experimental.list_physical_devices(device_type)\n  tf.config.experimental.set_virtual_device_configuration(\n      physical_devices[0], [\n          tf.config.experimental.VirtualDeviceConfiguration(**kwargs)\n          for _ in range(NUM_REPLICAS)\n      ])\n\n\ndef build_linear_keras_functional_model(input_shape, weights):\n  inputs = tf.keras.Input(shape=input_shape, name=\'feature\')\n  layer = tf.keras.layers.Dense(\n      1,\n      use_bias=False,\n      kernel_initializer=tf.keras.initializers.Constant(weights))\n  outputs = layer(inputs)\n  return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\nclass AdversarialRegularizationMultiDeviceTest(tf.test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(AdversarialRegularizationMultiDeviceTest, cls).setUpClass()\n    _set_up_virtual_devices()\n\n  def _set_up_linear_regression(self, sample_weight=1.0):\n    w = np.array([[4.0], [-3.0]])\n    x0 = np.tile(np.array([[2.0, 3.0]]), (NUM_REPLICAS, 1))\n    y0 = np.tile(np.array([[0.0]]), (NUM_REPLICAS, 1))\n    adv_multiplier = 0.2\n    adv_step_size = 0.01\n    learning_rate = 0.01\n    adv_config = configs.make_adv_reg_config(\n        multiplier=adv_multiplier,\n        adv_step_size=adv_step_size,\n        adv_grad_norm=\'infinity\')\n    y_hat = np.dot(x0, w)\n    loss = (y_hat - y0) / NUM_REPLICAS\n    x_adv = x0 + adv_step_size * np.sign((y_hat - y0) * w.T)\n    y_hat_adv = np.dot(x_adv, w)\n    loss_adv = (y_hat_adv - y0) / NUM_REPLICAS\n    grad_w_labeled_loss = sample_weight * 2. * np.matmul(x0.T, loss)\n    grad_w_adv_loss = adv_multiplier * sample_weight * 2. * np.matmul(\n        x_adv.T, loss_adv)\n    w_new = w - learning_rate * (grad_w_labeled_loss + grad_w_adv_loss)\n    return w, x0, y0, learning_rate, adv_config, w_new\n\n  def _get_mirrored_strategy(self):\n    device_type = \'GPU\' if tf.test.is_gpu_available() else \'CPU\'\n    devices = [\'{}:{}\'.format(device_type, i) for i in range(NUM_REPLICAS)]\n    return tf.distribute.MirroredStrategy(devices)\n\n  def test_train_with_distribution_strategy(self):\n    w, x0, y0, lr, adv_config, w_new = self._set_up_linear_regression()\n    inputs = tf.data.Dataset.from_tensor_slices({\n        \'feature\': x0,\n        \'label\': y0\n    }).batch(NUM_REPLICAS)\n\n    strategy = self._get_mirrored_strategy()\n    with strategy.scope():\n      # Makes sure we are running on multiple devices.\n      self.assertEqual(NUM_REPLICAS, strategy.num_replicas_in_sync)\n      model = build_linear_keras_functional_model(input_shape=(2,), weights=w)\n      adv_model = adversarial_regularization.AdversarialRegularization(\n          model, label_keys=[\'label\'], adv_config=adv_config)\n      adv_model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss=\'MSE\')\n\n    adv_model.fit(x=inputs)\n\n    # The updated weight should be the same regardless of the number of devices.\n    self.assertAllClose(w_new, tf.keras.backend.get_value(model.weights[0]))\n\n  def test_train_with_loss_object(self):\n    w, x0, y0, lr, adv_config, w_new = self._set_up_linear_regression()\n    inputs = tf.data.Dataset.from_tensor_slices({\n        \'feature\': x0,\n        \'label\': y0\n    }).batch(NUM_REPLICAS)\n\n    strategy = self._get_mirrored_strategy()\n    with strategy.scope():\n      model = build_linear_keras_functional_model(input_shape=(2,), weights=w)\n      adv_model = adversarial_regularization.AdversarialRegularization(\n          model, label_keys=[\'label\'], adv_config=adv_config)\n      adv_model.compile(\n          optimizer=tf.keras.optimizers.SGD(lr),\n          loss=tf.keras.losses.MeanSquaredError())\n    adv_model.fit(x=inputs)\n\n    self.assertAllClose(w_new, tf.keras.backend.get_value(model.weights[0]))\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.enable_v2_behavior()\n  tf.test.main()\n'"
neural_structured_learning/keras/adversarial_regularization_test.py,99,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.keras.adversarial_regularization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom absl.testing import parameterized\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.keras import adversarial_regularization\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\ndef build_linear_keras_sequential_model(input_shape, weights):\n  model = tf.keras.Sequential()\n  model.add(tf.keras.Input(shape=input_shape, name=\'feature\'))\n  model.add(\n      tf.keras.layers.Dense(\n          weights.shape[-1],\n          use_bias=False,\n          kernel_initializer=tf.keras.initializers.Constant(weights)))\n  return model\n\n\ndef build_linear_keras_sequential_model_no_input_layer(input_shape, weights):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(\n          weights.shape[-1],\n          use_bias=False,\n          input_shape=input_shape,\n          kernel_initializer=tf.keras.initializers.Constant(weights)),\n  ])\n\n\ndef build_linear_keras_functional_model(input_shape,\n                                        weights,\n                                        input_name=\'feature\'):\n  inputs = tf.keras.Input(shape=input_shape, name=input_name)\n  layer = tf.keras.layers.Dense(\n      weights.shape[-1],\n      use_bias=False,\n      kernel_initializer=tf.keras.initializers.Constant(weights))\n  outputs = layer(inputs)\n  return tf.keras.Model(inputs={input_name: inputs}, outputs=outputs)\n\n\ndef build_linear_keras_subclassed_model(input_shape, weights, dynamic=False):\n  del input_shape\n\n  class LinearModel(tf.keras.Model):\n\n    def __init__(self):\n      super(LinearModel, self).__init__(dynamic=dynamic)\n      self.dense = tf.keras.layers.Dense(\n          weights.shape[-1],\n          use_bias=False,\n          name=\'dense\',\n          kernel_initializer=tf.keras.initializers.Constant(weights))\n\n    def call(self, inputs):\n      return self.dense(inputs[\'feature\'])\n\n  return LinearModel()\n\n\ndef build_linear_keras_dynamic_model(input_shape, weights):\n  return build_linear_keras_subclassed_model(input_shape, weights, dynamic=True)\n\n\nclass AdversarialLossTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(AdversarialLossTest, self).setUp()\n    self.adv_step_size = 0.01\n    self.adv_config = configs.make_adv_reg_config(\n        adv_step_size=self.adv_step_size, adv_grad_norm=\'infinity\')\n\n  def _build_linear_regression_model_and_inputs(self, model_fn):\n    # The model and input are shared across test cases.\n    w = np.array([[4.0], [-3.0]])\n    x0 = np.array([[2.0, 3.0]])\n    y0 = np.array([[0.0]])\n    y_hat = np.dot(x0, w)\n    x_adv = x0 + self.adv_step_size * np.sign((y_hat - y0) * w.T)\n    y_hat_adv = np.dot(x_adv, w)\n    model = model_fn(input_shape=(2,), weights=w)\n    loss_fn = tf.keras.losses.MeanSquaredError()\n    inputs = {\'feature\': tf.constant(x0)}\n    labels = tf.constant(y0)\n    expected_adv_loss = np.reshape((y_hat_adv - y0)**2, ())\n\n    # Initializes the variables in TF 1.x. This is a no-op in TF 2.0.\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    return inputs, labels, model, loss_fn, expected_adv_loss\n\n  def evaluate(self, *args, **kwargs):\n    if hasattr(tf.keras.backend, \'get_session\'):\n      # Sets the Keras Session as default TF Session, so that the variable\n      # in Keras subclassed model can be initialized correctly. The variable\n      # is not created until the first call to the model, so the initialization\n      # is not captured in the global_variables_initializer above.\n      with tf.keras.backend.get_session().as_default():\n        return super(AdversarialLossTest, self).evaluate(*args, **kwargs)\n    else:\n      return super(AdversarialLossTest, self).evaluate(*args, **kwargs)\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_normal_case(self, model_fn):\n    inputs, labels, model, loss_fn, expected_adv_loss = (\n        self._build_linear_regression_model_and_inputs(model_fn))\n    adv_loss = adversarial_regularization.adversarial_loss(\n        inputs, labels, model, loss_fn, adv_config=self.adv_config)\n    self.assertAllClose(expected_adv_loss, self.evaluate(adv_loss))\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_given_predictions(self, model_fn):\n    inputs, labels, model, loss_fn, expected_adv_loss = (\n        self._build_linear_regression_model_and_inputs(model_fn))\n\n    with tf.GradientTape() as tape:\n      tape.watch(inputs[\'feature\'])\n      outputs = model(inputs)\n\n    # Wraps self.model to record the number of times it gets called. The counter\n    # cannot be a local variable because assignments to names always go into the\n    # innermost scope.\n    # https://docs.python.org/3/tutorial/classes.html#python-scopes-and-namespaces\n    call_count = collections.Counter()\n\n    def wrapped_model(inputs):\n      call_count[\'model\'] += 1\n      return model(inputs)\n\n    adv_loss = adversarial_regularization.adversarial_loss(\n        inputs,\n        labels,\n        wrapped_model,\n        loss_fn,\n        adv_config=self.adv_config,\n        predictions=outputs,\n        gradient_tape=tape)\n    self.assertAllClose(expected_adv_loss, self.evaluate(adv_loss))\n    # The model should be called only once, i.e. not re-calculating the\n    # predictions on original inputs.\n    self.assertEqual(1, call_count[\'model\'])\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_given_labeled_loss(self, model_fn):\n    inputs, labels, model, loss_fn, expected_adv_loss = (\n        self._build_linear_regression_model_and_inputs(model_fn))\n\n    with tf.GradientTape() as tape:\n      tape.watch(inputs[\'feature\'])\n      outputs = model(inputs)\n      labeled_loss = loss_fn(labels, outputs)\n\n    # Wraps self.model and self.loss_fn to record the number of times they get\n    # called.\n    call_count = collections.Counter()\n\n    def wrapped_model(inputs):\n      call_count[\'model\'] += 1\n      return model(inputs)\n\n    def wrapped_loss_fn(*args, **kwargs):\n      call_count[\'loss_fn\'] += 1\n      return loss_fn(*args, **kwargs)\n\n    adv_loss = adversarial_regularization.adversarial_loss(\n        inputs,\n        labels,\n        wrapped_model,\n        wrapped_loss_fn,\n        adv_config=self.adv_config,\n        labeled_loss=labeled_loss,\n        gradient_tape=tape)\n    self.assertAllClose(expected_adv_loss, self.evaluate(adv_loss))\n    # The model and loss_fn should be called only once, i.e. not re-calculating\n    # the predictions and/or loss on original inputs.\n    self.assertEqual(1, call_count[\'model\'])\n    self.assertEqual(1, call_count[\'loss_fn\'])\n\n  def test_with_model_kwargs(self):\n    w = np.array([[4.0], [-3.0]])\n    x0 = np.array([[2.0, 3.0]])\n    y0 = np.array([[0.0]])\n    model = build_linear_keras_sequential_model(input_shape=(2,), weights=w)\n    model.add(tf.keras.layers.BatchNormalization())\n\n    adv_loss = adversarial_regularization.adversarial_loss(\n        features={\'feature\': tf.constant(x0)},\n        labels=tf.constant(y0),\n        model=model,\n        loss_fn=tf.keras.losses.MeanSquaredError(),\n        adv_config=self.adv_config,\n        model_kwargs={\'training\': True})\n    # BatchNormalization returns 0 for signle-example batch when training=True.\n    self.assertAllClose(0.0, self.evaluate(adv_loss))\n\n\nclass AdversarialRegularizationTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_predict_by_adv_model_with_labels(self, model_fn):\n    model = model_fn(input_shape=(2,), weights=np.array([[1.0], [-1.0]]))\n    inputs = {\n        \'feature\': tf.constant([[5.0, 3.0]]),\n        \'label\': tf.constant([[1.0]])\n    }\n\n    adv_model = adversarial_regularization.AdversarialRegularization(model)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss=\'MSE\')\n\n    prediction = adv_model.predict(x=inputs, steps=1, batch_size=1)\n    self.assertAllEqual([[2.0]], prediction)\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_predict_by_base_model(self, model_fn):\n    model = model_fn(input_shape=(2,), weights=np.array([[1.0], [-1.0]]))\n    inputs = {\'feature\': tf.constant([[5.0, 3.0]])}\n\n    adv_model = adversarial_regularization.AdversarialRegularization(model)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss=\'MSE\')\n\n    prediction = model.predict(x=inputs, steps=1, batch_size=1)\n    self.assertAllEqual([[2.0]], prediction)\n\n  def _set_up_linear_regression(self, sample_weight=1.0):\n    w = np.array([[4.0], [-3.0]])\n    x0 = np.array([[2.0, 3.0]])\n    y0 = np.array([[0.0]])\n    adv_multiplier = 0.2\n    adv_step_size = 0.01\n    learning_rate = 0.01\n    adv_config = configs.make_adv_reg_config(\n        multiplier=adv_multiplier,\n        adv_step_size=adv_step_size,\n        adv_grad_norm=\'infinity\')\n    y_hat = np.dot(x0, w)\n    x_adv = x0 + adv_step_size * np.sign((y_hat - y0) * w.T)\n    y_hat_adv = np.dot(x_adv, w)\n    grad_w_labeled_loss = sample_weight * 2. * (y_hat - y0) * x0.T\n    grad_w_adv_loss = adv_multiplier * sample_weight * 2. * (y_hat_adv -\n                                                             y0) * x_adv.T\n    w_new = w - learning_rate * (grad_w_labeled_loss + grad_w_adv_loss)\n    return w, x0, y0, learning_rate, adv_config, w_new\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'sequential_no_input_layer\',\n       build_linear_keras_sequential_model_no_input_layer),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_train_fgsm(self, model_fn):\n    w, x0, y0, lr, adv_config, w_new = self._set_up_linear_regression()\n\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = model_fn(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss=\'MSE\')\n    adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    self.assertAllClose(w_new, tf.keras.backend.get_value(model.weights[0]))\n\n  def test_train_fgsm_functional_model_diff_feature_key(self):\n    # This test asserts that AdversarialRegularization works regardless of the\n    # alphabetical order of feature and label keys in the input dictionary. This\n    # is specifically for Keras Functional models because those models sort the\n    # inputs by key.\n    w, x0, y0, lr, adv_config, w_new = self._set_up_linear_regression()\n\n    inputs = {\'the_feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = build_linear_keras_functional_model(\n        input_shape=(2,), weights=w, input_name=\'the_feature\')\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss=\'MSE\')\n    adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    self.assertAllClose(w_new, tf.keras.backend.get_value(model.weights[0]))\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_train_fgsm_with_sample_weights(self, model_fn):\n    sample_weight = np.array([[2.0]])\n    w, x0, y0, lr, adv_config, w_new = self._set_up_linear_regression(\n        sample_weight)\n\n    inputs = {\n        \'feature\': tf.constant(x0),\n        \'label\': tf.constant(y0),\n        \'sample_weight\': tf.constant(sample_weight)\n    }\n    model = model_fn(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model,\n        label_keys=[\'label\'],\n        sample_weight_key=\'sample_weight\',\n        adv_config=adv_config)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss=\'MSE\')\n    adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    self.assertAllClose(w_new, tf.keras.backend.get_value(model.weights[0]))\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n      (\'dynamic\', build_linear_keras_dynamic_model),\n  ])\n  @test_util.run_v2_only\n  def test_train_with_distribution_strategy(self, model_fn):\n    w, x0, y0, lr, adv_config, w_new = self._set_up_linear_regression()\n    inputs = tf.data.Dataset.from_tensor_slices({\n        \'feature\': x0,\n        \'label\': y0\n    }).batch(1)\n\n    strategy = tf.distribute.MirroredStrategy()\n    with strategy.scope():\n      model = model_fn(input_shape=(2,), weights=w)\n      adv_model = adversarial_regularization.AdversarialRegularization(\n          model, label_keys=[\'label\'], adv_config=adv_config)\n      adv_model.compile(\n          optimizer=tf.keras.optimizers.SGD(lr), loss=\'MSE\', metrics=[\'mae\'])\n\n    adv_model.fit(x=inputs)\n\n    self.assertAllClose(w_new, tf.keras.backend.get_value(model.weights[0]))\n\n  def test_train_with_loss_object(self):\n    w, x0, y0, lr, adv_config, w_new = self._set_up_linear_regression()\n\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = build_linear_keras_functional_model(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(\n        optimizer=tf.keras.optimizers.SGD(lr),\n        loss=tf.keras.losses.MeanSquaredError())\n    adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    self.assertAllClose(w_new, tf.keras.backend.get_value(model.weights[0]))\n\n  def test_train_with_metrics(self):\n    w, x0, y0, lr, adv_config, _ = self._set_up_linear_regression()\n\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = build_linear_keras_functional_model(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(\n        optimizer=tf.keras.optimizers.SGD(lr), loss=\'MSE\', metrics=[\'mae\'])\n    history = adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    actual_labeled_loss = history.history[\'mean_squared_error\'][0]\n    actual_metric = history.history[\'mean_absolute_error\'][0]\n    expected_labeled_loss = np.power(y0 - np.dot(x0, w), 2).mean()\n    expected_metric = np.abs(y0 - np.dot(x0, w)).mean()\n    self.assertAllClose(expected_labeled_loss, actual_labeled_loss)\n    self.assertAllClose(expected_metric, actual_metric)\n\n  def test_train_with_duplicated_metrics(self):\n    w, x0, y0, lr, adv_config, _ = self._set_up_linear_regression()\n\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = build_linear_keras_functional_model(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(\n        optimizer=tf.keras.optimizers.SGD(lr), loss=[\'MSE\'], metrics=[[\'MSE\']])\n    history = adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    self.assertIn(\'mean_squared_error\', history.history)\n    self.assertIn(\'mean_squared_error_2\', history.history)\n    self.assertEqual(history.history[\'mean_squared_error\'],\n                     history.history[\'mean_squared_error_2\'])\n\n  def test_train_with_metric_object(self):\n    w, x0, y0, lr, adv_config, _ = self._set_up_linear_regression()\n\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = build_linear_keras_functional_model(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(\n        optimizer=tf.keras.optimizers.SGD(lr),\n        loss=\'MSE\',\n        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n    history = adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    actual_metric = history.history[\'mean_absolute_error\'][0]\n    expected_metric = np.abs(y0 - np.dot(x0, w)).mean()\n    self.assertAllClose(expected_metric, actual_metric)\n\n  def test_train_with_2_outputs(self):\n    w, x0, y0, lr, adv_config, _ = self._set_up_linear_regression()\n    inputs = {\n        \'feature\': tf.constant(x0),\n        \'label1\': tf.constant(y0),\n        \'label2\': tf.constant(-y0)\n    }\n\n    input_layer = tf.keras.Input(shape=(2,), name=\'feature\')\n    layer1 = tf.keras.layers.Dense(\n        w.shape[-1],\n        use_bias=False,\n        kernel_initializer=tf.keras.initializers.Constant(w))\n    layer2 = tf.keras.layers.Dense(\n        w.shape[-1],\n        use_bias=False,\n        kernel_initializer=tf.keras.initializers.Constant(-w))\n    model = tf.keras.Model(\n        inputs={\'feature\': input_layer},\n        outputs=[layer1(input_layer), layer2(input_layer)])\n\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label1\', \'label2\'], adv_config=adv_config)\n    adv_model.compile(\n        optimizer=tf.keras.optimizers.SGD(lr),\n        loss=\'MSE\',\n        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n    history = adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    expected_metric = np.abs(y0 - np.dot(x0, w)).mean()\n    self.assertAllClose(expected_metric,\n                        history.history[\'mean_absolute_error_label1\'][0])\n    self.assertAllClose(expected_metric,\n                        history.history[\'mean_absolute_error_label2\'][0])\n\n  @parameterized.named_parameters([\n      (\'order_1_2\', \'first\', \'second\'),\n      (\'order_2_1\', \'second\', \'first\'),\n  ])\n  def test_train_with_2_inputs(self, name1, name2):\n    x1, x2 = np.array([[1.]]), np.array([[4., 5.]])\n    w1, w2 = np.array([[2.]]), np.array([[3.], [6.]])\n    y = np.array([0.])\n    inputs = {name1: x1, name2: x2, \'label\': y}\n    lr, adv_step_size = 0.001, 0.1\n\n    input1 = tf.keras.Input(shape=(1,), name=name1)\n    input2 = tf.keras.Input(shape=(2,), name=name2)\n    dense1 = tf.keras.layers.Dense(\n        w1.shape[-1],\n        use_bias=False,\n        kernel_initializer=tf.keras.initializers.Constant(w1))\n    dense2 = tf.keras.layers.Dense(\n        w2.shape[-1],\n        use_bias=False,\n        kernel_initializer=tf.keras.initializers.Constant(w2))\n    output = tf.keras.layers.Add()([dense1(input1), dense2(input2)])\n    model = tf.keras.Model(inputs=[input1, input2], outputs=output)\n\n    adv_config = configs.make_adv_reg_config(\n        multiplier=1.0, adv_step_size=adv_step_size, adv_grad_norm=\'l2\')\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss=\'MAE\')\n    adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    # loss = |x1 * w1 + x2 * w2|, gradient(loss, [x1, x2]) = [w1, w2]\n    w_norm = np.sqrt((np.sum(w1 * w1) + np.sum(w2 * w2)))\n    x1_adv, x2_adv = x1 + adv_step_size * w1.T / w_norm, x2 + adv_step_size * w2.T / w_norm\n    # gradient(loss, [w1, w2]) = [x1, x2]\n    w1_new, w2_new = w1 - lr * (x1 + x1_adv).T, w2 - lr * (x2 + x2_adv).T\n    self.assertAllClose(w1_new, tf.keras.backend.get_value(dense1.weights[0]))\n    self.assertAllClose(w2_new, tf.keras.backend.get_value(dense2.weights[0]))\n\n  def test_train_subclassed_base_model_with_label_input(self):\n    w, x0, y0, lr, adv_config, _ = self._set_up_linear_regression()\n\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n\n    class BaseModel(tf.keras.Model):\n\n      def __init__(self):\n        super(BaseModel, self).__init__()\n        self.dense = tf.keras.layers.Dense(\n            w.shape[-1],\n            use_bias=False,\n            kernel_initializer=tf.keras.initializers.Constant(w))\n        self.seen_input_keys = set()\n\n      def call(self, inputs):\n        self.seen_input_keys |= set(inputs.keys())\n        return self.dense(inputs[\'feature\'])\n\n    model = BaseModel()\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model,\n        label_keys=[\'label\'],\n        adv_config=adv_config,\n        base_with_labels_in_features=True)\n    adv_model.compile(\n        optimizer=tf.keras.optimizers.SGD(lr), loss=\'MSE\', metrics=[\'mae\'])\n    adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    self.assertIn(\'label\', model.seen_input_keys)\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'sequential_no_input_layer\',\n       build_linear_keras_sequential_model_no_input_layer),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_train_pgd(self, model_fn):\n    w = np.array([[4.0], [-3.0]])\n    x0 = np.array([[2.0, 3.0]])\n    y0 = np.array([[0.0]])\n    adv_multiplier = 0.2\n    adv_step_size = 0.01\n    learning_rate = 0.01\n    pgd_iterations = 3\n    pgd_epsilon = 2.5 * adv_step_size\n    adv_config = configs.make_adv_reg_config(\n        multiplier=adv_multiplier,\n        adv_step_size=adv_step_size,\n        adv_grad_norm=\'infinity\',\n        pgd_iterations=pgd_iterations,\n        pgd_epsilon=pgd_epsilon)\n    y_hat = np.dot(x0, w)\n    # The adversarial perturbation is constant across PGD iterations.\n    x_adv = x0 + pgd_epsilon * np.sign((y_hat - y0) * w.T)\n    y_hat_adv = np.dot(x_adv, w)\n    grad_w_labeled_loss = 2. * (y_hat - y0) * x0.T\n    grad_w_adv_loss = adv_multiplier * 2. * (y_hat_adv - y0) * x_adv.T\n    w_new = w - learning_rate * (grad_w_labeled_loss + grad_w_adv_loss)\n\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = model_fn(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(tf.keras.optimizers.SGD(learning_rate), loss=\'MSE\')\n    adv_model.fit(x=inputs, batch_size=1, steps_per_epoch=1)\n\n    self.assertAllClose(w_new, tf.keras.backend.get_value(model.weights[0]))\n\n  def test_evaluate_binary_classification_metrics(self):\n    # multi-label binary classification model\n    w = np.array([[4.0, 1.0, -5.0], [-3.0, 1.0, 2.0]])\n    x0 = np.array([[2.0, 3.0]])\n    y0 = np.array([[0.0, 1.0, 1.0]])\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = build_linear_keras_sequential_model(input_shape=(2,), weights=w)\n    model.add(tf.keras.layers.Lambda(tf.sigmoid))\n\n    adv_model = adversarial_regularization.AdversarialRegularization(model)\n    adv_model.compile(\n        optimizer=tf.keras.optimizers.SGD(0.1),\n        loss=\'squared_hinge\',\n        metrics=[\'accuracy\', \'ce\'])\n    metrics_values = adv_model.evaluate(inputs, steps=1)\n    results = dict(zip(adv_model.metrics_names, metrics_values))\n\n    y_hat = 1. / (1. + np.exp(-np.dot(x0, w)))  # [[0.26894, 0.99331, 0.01799]]\n    accuracy = np.mean(np.sign(y_hat - 0.5) == np.sign(y0 - 0.5))  # (1+1+0) / 3\n    cross_entropy = np.mean(y0 * -np.log(y_hat) + (1 - y0) * -np.log(1 - y_hat))\n\n    self.assertIn(\'binary_accuracy\', results)\n    self.assertIn(\'binary_crossentropy\', results)\n    self.assertAllClose(accuracy, results[\'binary_accuracy\'])\n    self.assertAllClose(cross_entropy, results[\'binary_crossentropy\'])\n\n  def test_evaluate_classification_metrics(self):\n    # multi-class logistic regression model\n    w = np.array([[4.0, 1.0, -5.0], [-3.0, 1.0, 2.0]])\n    x0 = np.array([[2.0, 3.0]])\n    y0 = np.array([[1]])\n    inputs = {\'feature\': tf.constant(x0), \'label\': tf.constant(y0)}\n    model = build_linear_keras_sequential_model(input_shape=(2,), weights=w)\n    model.add(tf.keras.layers.Softmax())\n\n    adv_model = adversarial_regularization.AdversarialRegularization(model)\n    adv_model.compile(\n        optimizer=tf.keras.optimizers.SGD(0.1),\n        loss=\'sparse_categorical_crossentropy\',\n        metrics=[\'accuracy\', \'ce\'])\n    metrics_values = adv_model.evaluate(inputs, steps=1)\n    results = dict(zip(adv_model.metrics_names, metrics_values))\n\n    logit = np.dot(x0, w)  # [[-1.,  5., -4.]]\n    accuracy = np.mean(np.argmax(logit, axis=-1) == y0)\n    cross_entropy = np.log(np.sum(np.exp(logit))) - np.reshape(logit[:, y0], ())\n\n    self.assertIn(\'sparse_categorical_accuracy\', results)\n    self.assertIn(\'sparse_categorical_crossentropy\', results)\n    self.assertAllClose(accuracy, results[\'sparse_categorical_accuracy\'])\n    self.assertAllClose(cross_entropy,\n                        results[\'sparse_categorical_crossentropy\'])\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'sequential_no_input_layer\',\n       build_linear_keras_sequential_model_no_input_layer),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_perturb_on_batch(self, model_fn):\n    w, x0, y0, lr, adv_config, _ = self._set_up_linear_regression()\n    inputs = {\'feature\': x0, \'label\': y0}\n    model = model_fn(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss=[\'MSE\'])\n    adv_inputs = adv_model.perturb_on_batch(inputs)\n\n    y_hat = np.dot(x0, w)\n    adv_step_size = adv_config.adv_neighbor_config.adv_step_size\n    x_adv = x0 + adv_step_size * np.sign((y_hat - y0) * w.T)\n    self.assertAllClose(x_adv, adv_inputs[\'feature\'])\n    self.assertAllClose(y0, adv_inputs[\'label\'])\n\n  def test_perturb_on_batch_custom_config(self):\n    w, x0, y0, lr, adv_config, _ = self._set_up_linear_regression()\n    inputs = {\'feature\': x0, \'label\': y0}\n    model = build_linear_keras_functional_model(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss=[\'MSE\'])\n\n    adv_step_size = 0.2  # A different value from config.adv_step_size\n    adv_inputs = adv_model.perturb_on_batch(inputs, adv_step_size=adv_step_size)\n\n    y_hat = np.dot(x0, w)\n    x_adv = x0 + adv_step_size * np.sign((y_hat - y0) * w.T)\n    self.assertAllClose(x_adv, adv_inputs[\'feature\'])\n    self.assertAllClose(y0, adv_inputs[\'label\'])\n\n  @parameterized.named_parameters([\n      (\'sequential\', build_linear_keras_sequential_model),\n      (\'sequential_no_input_layer\',\n       build_linear_keras_sequential_model_no_input_layer),\n      (\'functional\', build_linear_keras_functional_model),\n      (\'subclassed\', build_linear_keras_subclassed_model),\n  ])\n  def test_perturb_on_batch_pgd(self, model_fn):\n    w, x0, y0, lr, adv_config, _ = self._set_up_linear_regression()\n    pgd_epsilon = 4.5 * adv_config.adv_neighbor_config.adv_step_size\n    adv_config.adv_neighbor_config.pgd_iterations = 5\n    adv_config.adv_neighbor_config.pgd_epsilon = pgd_epsilon\n    inputs = {\'feature\': x0, \'label\': y0}\n    model = model_fn(input_shape=(2,), weights=w)\n    adv_model = adversarial_regularization.AdversarialRegularization(\n        model, label_keys=[\'label\'], adv_config=adv_config)\n    adv_model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss=[\'MSE\'])\n    adv_inputs = adv_model.perturb_on_batch(inputs)\n\n    y_hat = np.dot(x0, w)\n    x_adv = x0 + pgd_epsilon * np.sign((y_hat - y0) * w.T)\n    self.assertAllClose(x_adv, adv_inputs[\'feature\'])\n    self.assertAllClose(y0, adv_inputs[\'label\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/keras/graph_regularization.py,8,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Incorporates graph regularization into a Keras model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport neural_structured_learning.configs as nsl_configs\nfrom neural_structured_learning.keras import layers as nsl_layers\nimport tensorflow as tf\n\n\nclass GraphRegularization(tf.keras.Model):\n  """"""Class that wraps a given `Keras` model to include graph regularization.\n\n  Graph regularization is configured by an instance of\n  `nsl.configs.GraphRegConfig` and the resulting loss is added as a\n  regularization term to the model\'s training objective. The graph-regularized\n  model reuses the layers and variables from the base model. So, training this\n  model will also update the variables in the base model.\n\n  Note: This class expects input data to include neighor features corresponding\n  to the maximum number of neighbors used for graph regularization.\n\n  Example usage:\n\n  ```python\n  # Create a base model using the sequential, functional, or subclass API.\n  base_model = tf.keras.Sequential(...)\n\n  # Wrap the base model to include graph regularization using up to 1 neighbor\n  # per sample.\n  graph_config = nsl.configs.make_graph_reg_config(max_neighbors=1)\n  graph_model = nsl.keras.GraphRegularization(base_model, graph_config)\n\n  # Compile, train, and evaluate the graph-regularized model as usual.\n  graph_model.compile(\n      optimizer=\'adam\',\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n      metrics=[\'accuracy\'])\n  graph_model.fit(train_dataset, epochs=5)\n  graph_model.evaluate(test_dataset)\n  ```\n  """"""\n\n  def __init__(self, base_model, graph_reg_config=None):\n    """"""Class initializer.\n\n    Args:\n      base_model: Unregularized model to which the loss term resulting from\n        graph regularization will be added.\n      graph_reg_config: Instance of `nsl.configs.GraphRegConfig` that contains\n        configuration for graph regularization. Use\n        `nsl.configs.make_graph_reg_config` to construct one.\n    """"""\n\n    super(GraphRegularization, self).__init__(name=\'GraphRegularization\')\n    self.base_model = base_model\n    self.graph_reg_config = (\n        nsl_configs.GraphRegConfig()\n        if graph_reg_config is None else graph_reg_config)\n    self.nbr_features_layer = nsl_layers.NeighborFeatures(\n        self.graph_reg_config.neighbor_config)\n    self.regularizer = nsl_layers.PairwiseDistance(\n        self.graph_reg_config.distance_config, name=\'graph_loss\')\n\n  # This override is required in case \'self.base_model\' is a subclass model and\n  # has overridden the compile function.\n  def compile(self, *args, **kwargs):\n    super(GraphRegularization, self).compile(*args, **kwargs)\n    self.base_model.compile(*args, **kwargs)\n\n  compile.__doc__ = tf.keras.Model.compile.__doc__\n\n  # Override the evaluate and the predict methods so that we can use the base\n  # model for evaluation/prediction rather than the graph-regularized model.\n  # This is because once the graph-regularized Keras model is built, it expects\n  # neighbor features as input for all modes and not just for training.\n  def evaluate(self, *args, **kwargs):\n    return self.base_model.evaluate(*args, **kwargs)\n\n  evaluate.__doc__ = tf.keras.Model.evaluate.__doc__\n\n  def predict(self, *args, **kwargs):\n    return self.base_model.predict(*args, **kwargs)\n\n  predict.__doc__ = tf.keras.Model.predict.__doc__\n\n  def call(self, inputs, training=False, **kwargs):\n    """"""Incorporates graph regularization into the loss of `base_model`.\n\n    Graph regularization is done on the logits layer and only during training.\n\n    Args:\n      inputs: Dictionary containing sample features, neighbor features, and\n        neighbor weights in the same format as described in\n        `utils.unpack_neighbor_features`.\n      training: Boolean tensor that indicates if we are in training mode.\n      **kwargs: Additional keyword arguments to be passed to `self.base_model`.\n\n    Returns:\n      The output tensors for the wrapped graph-regularized model.\n    """"""\n    # Invoke the call() function of the neighbor features layer directly instead\n    # of invoking it as a callable to avoid Keras from wrapping placeholder\n    # tensors with the tf.identity() op.\n    sample_features, nbr_features, nbr_weights = self.nbr_features_layer.call(\n        inputs)\n    base_output = self.base_model(sample_features, training=training, **kwargs)\n\n    # For evaluation and prediction, we use the base model. So, this overridden\n    # call function will get invoked only for training.\n    has_nbr_inputs = nbr_weights is not None and nbr_features\n    if (has_nbr_inputs and self.graph_reg_config.multiplier > 0):\n      # Use logits for regularization.\n      sample_logits = base_output\n      nbr_logits = self.base_model(nbr_features, training=training, **kwargs)\n      graph_loss = self.regularizer(\n          sources=sample_logits, targets=nbr_logits, weights=nbr_weights)\n    else:\n      graph_loss = tf.constant(0, dtype=tf.float32)\n\n    # Note that add_metric() cannot be invoked in a control flow branch.\n    self.add_metric(graph_loss, name=\'graph_loss\', aggregation=\'mean\')\n    self.add_loss(self.graph_reg_config.multiplier * graph_loss)\n\n    return base_output\n'"
neural_structured_learning/keras/graph_regularization_test.py,41,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.keras.graph_regularization.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.keras import graph_regularization\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nFEATURE_NAME = \'x\'\nLABEL_NAME = \'y\'\nNBR_FEATURE_PREFIX = \'NL_nbr_\'\nNBR_WEIGHT_SUFFIX = \'_weight\'\nLEARNING_RATE = 0.01\n\n\ndef make_feature_spec(input_shape, max_neighbors):\n  """"""Returns a feature spec that can be used to parse tf.train.Examples.\n\n  Args:\n    input_shape: A list of integers representing the shape of the input feature\n      and corresponding neighbor features.\n    max_neighbors: The maximum neighbors per sample to be used for graph\n      regularization.\n  """"""\n  feature_spec = {\n      FEATURE_NAME:\n          tf.io.FixedLenFeature(input_shape, tf.float32),\n      LABEL_NAME:\n          tf.io.FixedLenFeature([1],\n                                tf.float32,\n                                default_value=tf.constant([0.0])),\n  }\n  for i in range(max_neighbors):\n    nbr_feature_key = \'{}{}_{}\'.format(NBR_FEATURE_PREFIX, i, FEATURE_NAME)\n    nbr_weight_key = \'{}{}{}\'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)\n    feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n        input_shape, tf.float32)\n    feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n        [1], tf.float32, default_value=tf.constant([0.0]))\n  return feature_spec\n\n\ndef build_linear_sequential_model(input_shape, weights, num_output=1):\n  model = tf.keras.Sequential()\n  model.add(\n      tf.keras.layers.InputLayer(input_shape=input_shape, name=FEATURE_NAME))\n  model.add(\n      tf.keras.layers.Dense(\n          num_output,\n          input_shape=input_shape,\n          use_bias=False,\n          name=\'dense\',\n          kernel_initializer=tf.keras.initializers.Constant(weights)))\n  return model\n\n\ndef build_linear_functional_model(input_shape, weights, num_output=1):\n  inputs = tf.keras.Input(shape=input_shape, name=FEATURE_NAME)\n  outputs = tf.keras.layers.Dense(\n      num_output,\n      use_bias=False,\n      kernel_initializer=tf.keras.initializers.Constant(weights))(\n          inputs)\n  return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\ndef build_linear_subclass_model(input_shape, weights, num_output=1):\n  del input_shape\n\n  class LinearModel(tf.keras.Model):\n\n    def __init__(self):\n      super(LinearModel, self).__init__()\n      self.dense = tf.keras.layers.Dense(\n          num_output,\n          use_bias=False,\n          name=\'dense\',\n          kernel_initializer=tf.keras.initializers.Constant(weights))\n\n    def call(self, inputs):\n      return self.dense(inputs[FEATURE_NAME])\n\n  return LinearModel()\n\n\ndef make_dataset(example_proto, input_shape, training, max_neighbors):\n  """"""Construct a tf.data.Dataset from the given Example.""""""\n\n  def make_parse_example_fn(feature_spec):\n\n    def parse_example(serialized_example_proto):\n      """"""Extracts relevant fields from the example_proto.""""""\n      feature_dict = tf.io.parse_single_example(serialized_example_proto,\n                                                feature_spec)\n      return feature_dict, feature_dict.pop(LABEL_NAME)\n\n    return parse_example\n\n  example = text_format.Parse(example_proto, tf.train.Example())\n  serialized_example = example.SerializeToString()\n  dataset = tf.data.Dataset.from_tensors(\n      tf.convert_to_tensor(serialized_example))\n  if training:\n    dataset = dataset.shuffle(10)\n  dataset = dataset.map(\n      make_parse_example_fn(make_feature_spec(input_shape, max_neighbors)))\n  dataset = dataset.batch(1)\n  return dataset\n\n\nclass GraphRegularizationTest(tf.test.TestCase, parameterized.TestCase):\n\n  def test_predict_regularized_model(self):\n    model = build_linear_functional_model(\n        input_shape=(2,), weights=np.array([1.0, -1.0]))\n    inputs = {FEATURE_NAME: tf.constant([[5.0, 3.0]])}\n\n    graph_reg_model = graph_regularization.GraphRegularization(model)\n    graph_reg_model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss=\'MSE\')\n\n    prediction = graph_reg_model.predict(x=inputs, steps=1, batch_size=1)\n\n    self.assertAllEqual([[1 * 5.0 + (-1.0) * 3.0]], prediction)\n\n  def test_predict_base_model(self):\n    model = build_linear_functional_model(\n        input_shape=(2,), weights=np.array([1.0, -1.0]))\n    inputs = {FEATURE_NAME: tf.constant([[5.0, 3.0]])}\n\n    graph_reg_model = graph_regularization.GraphRegularization(model)\n    graph_reg_model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss=\'MSE\')\n\n    prediction = model.predict(x=inputs, steps=1, batch_size=1)\n\n    self.assertAllEqual([[1 * 5.0 + (-1.0) * 3.0]], prediction)\n\n  def _train_and_check_params(self,\n                              example,\n                              model_fn,\n                              dense_layer_index,\n                              max_neighbors,\n                              weight,\n                              expected_grad_from_weight,\n                              distributed_strategy=None):\n    """"""Runs training for one step and verifies gradient-based updates.\n\n    This uses a linear regressor as the base model.\n\n    Args:\n      example: An instance of `tf.train.Example`.\n      model_fn: A function that builds a linear regression model.\n      dense_layer_index: The index of the dense layer in the linear regressor.\n      max_neighbors: The maximum number of neighbors for graph regularization.\n      weight: Initial value for the weights variable in the linear regressor.\n      expected_grad_from_weight: The expected gradient of the loss with\n        respect to the weights variable.\n      distributed_strategy: An instance of `tf.distribute.Strategy` specifying\n        the distributed strategy to use for training.\n    """"""\n\n    dataset = make_dataset(\n        example, input_shape=[2], training=True, max_neighbors=max_neighbors)\n\n    def _create_and_compile_graph_reg_model(model_fn, weight, max_neighbors):\n      """"""Creates and compiles a graph regularized model.\n\n      Args:\n        model_fn: A function that builds a linear regression model.\n        weight: Initial value for the weights variable in the linear regressor.\n        max_neighbors: The maximum number of neighbors for graph regularization.\n\n      Returns:\n        A pair containing the unregularized model and the graph regularized\n        model as `tf.keras.Model` instances.\n      """"""\n      model = model_fn((2,), weight)\n      graph_reg_config = configs.make_graph_reg_config(\n          max_neighbors=max_neighbors, multiplier=1)\n      graph_reg_model = graph_regularization.GraphRegularization(\n          model, graph_reg_config)\n      graph_reg_model.compile(\n          optimizer=tf.keras.optimizers.SGD(LEARNING_RATE), loss=\'MSE\')\n      return model, graph_reg_model\n\n    if distributed_strategy:\n      with distributed_strategy.scope():\n        model, graph_reg_model = _create_and_compile_graph_reg_model(\n            model_fn, weight, max_neighbors)\n    else:\n      model, graph_reg_model = _create_and_compile_graph_reg_model(\n          model_fn, weight, max_neighbors)\n\n    graph_reg_model.fit(x=dataset, epochs=1, steps_per_epoch=1)\n\n    # Compute the new weight value based on the gradient.\n    expected_weight = weight - LEARNING_RATE * (expected_grad_from_weight)\n\n    # Check that the weight parameter of the linear regressor has the correct\n    # value.\n    self.assertAllClose(expected_weight,\n                        model.layers[dense_layer_index].weights[0].value())\n\n  @parameterized.named_parameters([\n      (\'_sequential\', 0, build_linear_sequential_model),\n      (\'_functional\', 1, build_linear_functional_model),\n      (\'_subclass\', 0, build_linear_subclass_model),\n  ])\n  @test_util.run_in_graph_and_eager_modes\n  def test_graph_reg_model_one_neighbor_training(self, dense_layer_index,\n                                                 model_fn):\n    w = np.array([[4.0], [-3.0]])\n    x0 = np.array([[2.0, 3.0]])\n    x0_nbr0 = np.array([[2.5, 3.0]])\n    y0 = np.array([[0.0]])\n\n    example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 2.0, 3.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_x""\n                    value: { float_list { value: [ 2.5, 3.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""y""\n                    value: { float_list { value: 0.0 } }\n                  }\n                }\n              """"""\n\n    y_hat = np.dot(x0, w)  # -1.0\n    y_nbr = np.dot(x0_nbr0, w)  # 1.0\n\n    # The graph loss term is (y_hat - y_nbr)^2 since graph regularization is\n    # done on the final predictions.\n    grad_w = 2 * (y_hat - y0) * x0.T + 2 * (y_hat - y_nbr) * (\n        x0 - x0_nbr0).T  # [[-2.0], [-6.0]]\n\n    self._train_and_check_params(\n        example,\n        model_fn,\n        dense_layer_index,\n        max_neighbors=1,\n        weight=w,\n        expected_grad_from_weight=grad_w)\n\n  def _test_training_with_two_neighbors(self,\n                                        dense_layer_index,\n                                        model_fn,\n                                        distributed_strategy=None):\n    w = np.array([[4.0], [-3.0]])\n    x0 = np.array([[2.0, 3.0]])\n    x0_nbr0 = np.array([[2.5, 3.0]])\n    x0_nbr1 = np.array([[2.0, 2.0]])\n    y0 = np.array([[0.0]])\n\n    example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 2.0, 3.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_x""\n                    value: { float_list { value: [ 2.5, 3.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""NL_nbr_1_x""\n                    value: { float_list { value: [ 2.0, 2.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_1_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""y""\n                    value: { float_list { value: 0.0 } }\n                  }\n                }\n              """"""\n\n    y_hat = np.dot(x0, w)  # -1.0\n    y_nbr0 = np.dot(x0_nbr0, w)  # 1.0\n    y_nbr1 = np.dot(x0_nbr1, w)  # 2.0\n\n    # The distance metric for the graph loss is \'L2\'. So, the graph loss term is\n    # [(y_hat - y_nbr_0)^2 + (y_hat - y_nbr_1)^2] / 2\n    grad_w = 2 * (y_hat - y0) * x0.T + (y_hat - y_nbr0) * (x0 - x0_nbr0).T + (\n        y_hat - y_nbr1) * (x0 - x0_nbr1).T  # [[-3.0], [-9.0]]\n\n    self._train_and_check_params(\n        example,\n        model_fn,\n        dense_layer_index,\n        max_neighbors=2,\n        weight=w,\n        expected_grad_from_weight=grad_w,\n        distributed_strategy=distributed_strategy)\n\n  @parameterized.named_parameters([\n      (\'_sequential\', 0, build_linear_sequential_model),\n      (\'_functional\', 1, build_linear_functional_model),\n      (\'_subclass\', 0, build_linear_subclass_model),\n  ])\n  @test_util.run_in_graph_and_eager_modes\n  def test_graph_reg_model_two_neighbors_training(self, dense_layer_index,\n                                                  model_fn):\n    self._test_training_with_two_neighbors(dense_layer_index, model_fn)\n\n  @parameterized.named_parameters([\n      (\'_sequential\', 0, build_linear_sequential_model),\n      (\'_functional\', 1, build_linear_functional_model),\n      (\'_subclass\', 0, build_linear_subclass_model),\n  ])\n  @test_util.run_v2_only\n  def test_graph_reg_model_distributed_strategy(self, dense_layer_index,\n                                                model_fn):\n    self._test_training_with_two_neighbors(\n        dense_layer_index,\n        model_fn,\n        distributed_strategy=tf.distribute.MirroredStrategy())\n\n  def _train_and_check_eval_results(self,\n                                    train_example,\n                                    test_example,\n                                    model_fn,\n                                    max_neighbors,\n                                    weight,\n                                    distributed_strategy=None):\n    """"""Verifies eval results for the graph-regularized model.\n\n    This uses a linear regressor as the base model.\n\n    Args:\n      train_example: An instance of `tf.train.Example` used for training.\n      test_example: An instance of `tf.train.Example` used for evaluation.\n      model_fn: A function that builds a linear regression model.\n      max_neighbors: The maximum number of neighbors for graph regularization.\n      weight: Initial value for the weights variable in the linear regressor.\n      distributed_strategy: An instance of `tf.distribute.Strategy` specifying\n        the distributed strategy to use for training.\n    """"""\n\n    train_dataset = make_dataset(\n        train_example,\n        input_shape=[2],\n        training=True,\n        max_neighbors=max_neighbors)\n\n    test_dataset = make_dataset(\n        test_example, input_shape=[2], training=False, max_neighbors=0)\n\n    def _create_and_compile_graph_reg_model(model_fn, weight, max_neighbors):\n      """"""Creates and compiles a graph regularized model.\n\n      Args:\n        model_fn: A function that builds a linear regression model.\n        weight: Initial value for the weights variable in the linear regressor.\n        max_neighbors: The maximum number of neighbors for graph regularization.\n\n      Returns:\n        A pair containing the unregularized model and the graph regularized\n        model as `tf.keras.Model` instances.\n      """"""\n      model = model_fn((2,), weight)\n      graph_reg_config = configs.make_graph_reg_config(\n          max_neighbors=max_neighbors, multiplier=1)\n      graph_reg_model = graph_regularization.GraphRegularization(\n          model, graph_reg_config)\n      graph_reg_model.compile(\n          optimizer=tf.keras.optimizers.SGD(LEARNING_RATE),\n          loss=\'MSE\',\n          metrics=[\'accuracy\'])\n      return model, graph_reg_model\n\n    if distributed_strategy:\n      with distributed_strategy.scope():\n        model, graph_reg_model = _create_and_compile_graph_reg_model(\n            model_fn, weight, max_neighbors)\n    else:\n      model, graph_reg_model = _create_and_compile_graph_reg_model(\n          model_fn, weight, max_neighbors)\n\n    graph_reg_model.fit(x=train_dataset, epochs=1, steps_per_epoch=1)\n\n    # Evaluating the graph-regularized model should yield the same results\n    # as evaluating the base model as the former involves just using the\n    # base model for evaluation.\n    graph_reg_model_eval_results = dict(\n        zip(graph_reg_model.metrics_names,\n            graph_reg_model.evaluate(x=test_dataset)))\n    base_model_eval_results = dict(\n        zip(model.metrics_names, model.evaluate(x=test_dataset)))\n    self.assertAllClose(base_model_eval_results, graph_reg_model_eval_results)\n\n  @parameterized.named_parameters([\n      (\'_sequential\', build_linear_sequential_model),\n      (\'_functional\', build_linear_functional_model),\n      (\'_subclass\', build_linear_subclass_model),\n  ])\n  def test_graph_reg_model_evaluate(self, model_fn):\n    w = np.array([[4.0], [-3.0]])\n\n    train_example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 2.0, 3.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_x""\n                    value: { float_list { value: [ 2.5, 3.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_0_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""NL_nbr_1_x""\n                    value: { float_list { value: [ 2.0, 2.0 ] } }\n                  }\n                  feature {\n                    key: ""NL_nbr_1_weight""\n                    value: { float_list { value: 1.0 } }\n                  }\n                  feature {\n                    key: ""y""\n                    value: { float_list { value: 0.0 } }\n                  }\n                }\n              """"""\n\n    test_example = """"""\n                features {\n                  feature {\n                    key: ""x""\n                    value: { float_list { value: [ 4.0, 2.0 ] } }\n                  }\n                  feature {\n                    key: ""y""\n                    value: { float_list { value: 4.0 } }\n                  }\n                }\n              """"""\n    self._train_and_check_eval_results(\n        train_example,\n        test_example,\n        model_fn,\n        max_neighbors=2,\n        weight=w,\n        distributed_strategy=None)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/lib/__init__.py,0,"b'""""""Library APIs for Neural Structured Learning.""""""\n\nfrom neural_structured_learning.lib.abstract_gen_neighbor import GenNeighbor\nfrom neural_structured_learning.lib.adversarial_neighbor import gen_adv_neighbor\nfrom neural_structured_learning.lib.distances import jensen_shannon_divergence\nfrom neural_structured_learning.lib.distances import kl_divergence\nfrom neural_structured_learning.lib.distances import pairwise_distance_wrapper\nfrom neural_structured_learning.lib.regularizer import adv_regularizer\nfrom neural_structured_learning.lib.regularizer import virtual_adv_regularizer\nfrom neural_structured_learning.lib.utils import apply_feature_mask\nfrom neural_structured_learning.lib.utils import decay_over_time\nfrom neural_structured_learning.lib.utils import get_target_indices\nfrom neural_structured_learning.lib.utils import maximize_within_unit_norm\nfrom neural_structured_learning.lib.utils import normalize\nfrom neural_structured_learning.lib.utils import replicate_embeddings\nfrom neural_structured_learning.lib.utils import strip_neighbor_features\nfrom neural_structured_learning.lib.utils import unpack_neighbor_features\n\n__all__ = [\n    \'adv_regularizer\',\n    \'apply_feature_mask\',\n    \'decay_over_time\',\n    \'GenNeighbor\',\n    \'gen_adv_neighbor\',\n    \'get_target_indices\',\n    \'jensen_shannon_divergence\',\n    \'kl_divergence\',\n    \'maximize_within_unit_norm\',\n    \'normalize\',\n    \'pairwise_distance_wrapper\',\n    \'replicate_embeddings\',\n    \'strip_neighbor_features\',\n    \'unpack_neighbor_features\',\n    \'virtual_adv_regularizer\',\n]\n'"
neural_structured_learning/lib/abstract_gen_neighbor.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Abstract class for generating neighbors.\n\nThis abstract class will be inherited by classes with actual implementation for\ngenerating neigbors (e.g., adversarial neighbors or graph neighbors).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass GenNeighbor(object):\n  """"""Abstract class for generating neighbors.\n\n  This class is to be inherited by the class that actually implements the method\n  to generate neighbors.\n  """"""\n\n  def __init__(self):\n    raise NotImplementedError\n\n  def gen_neighbor(self):\n    raise NotImplementedError\n'"
neural_structured_learning/lib/adversarial_neighbor.py,17,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Generates adversarial neighbors.\n\nThis file provides the class(es) and the corresponding functional interface(s)\nfor generating adversarial neighbors.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom absl import logging\nfrom neural_structured_learning.lib import abstract_gen_neighbor as abs_gen\nfrom neural_structured_learning.lib import utils\nimport tensorflow as tf\n\n\ndef _apply_feature_constraints(feature, min_value, max_value):\n  """"""Constrains `feature` to be between `min_value` and `max_value`.""""""\n  if min_value is not None:\n    feature = tf.math.maximum(feature, min_value)\n  if max_value is not None:\n    feature = tf.math.minimum(feature, max_value)\n  return feature\n\n\nclass _GenAdvNeighbor(abs_gen.GenNeighbor):\n  """"""Class for generating adversarial neighbors based on gradient-based methods.\n\n  The core of this class implements the projected gradient descent (PGD)\n  operation:\n  ```\n  adv_neighbor = input_features\n  iterations = 10  # Number of iterations to run PGD.\n  for _ in range(iterations):\n    grad = gradient(adv_neighbor)\n    adv_neighbor = adv_neighbor + adv_step_size * grad\n    adv_neighbor = project(adv_neighbor)\n  ```\n  where `adv_step_size` is the step size (analogous to learning rate) for\n  searching/calculating adversarial neighbor, `gradient(x)` calculates the\n  gradient of the model at `x`, and `project(v)` projects the vector `v` onto\n  the epsilon ball.\n\n  Attributes:\n    labeled_loss: a scalar (`tf.float32`) tensor calculated from true labels (or\n      supervisions).\n    adv_config: a `nsl.configs.AdvNeighborConfig` object.\n    raise_invalid_gradient: (optional) a Boolean flag indicating whether to\n      raise an error when gradients cannot be computed on some input features.\n      There are three cases where gradients cannot be computed:  (1) The feature\n        is a `tf.SparseTensor`. (2) The feature is in a non-differentiable\n        `tf.DType`, like string or integer. (3) The feature is not involved in\n        loss computation.  If set to False, those input without gradient will be\n        ignored silently and not perturbed. (default=False)\n    pgd_model_fn: the model function. Takes in the input_features and produces a\n      prediction. This is required for PGD with more than one step.\n    pgd_loss_fn: the loss function. Calculates loss between prediction and\n      ground truth.\n  """"""\n\n  def __init__(self,\n               labeled_loss,\n               adv_config,\n               raise_invalid_gradient=False,\n               gradient_tape=None,\n               pgd_model_fn=None,\n               pgd_loss_fn=None):\n    self._labeled_loss = labeled_loss\n    self._adv_config = adv_config\n    self._raise_invalid_gradient = raise_invalid_gradient\n    self._gradient_tape = gradient_tape\n    self._pgd_model_fn = pgd_model_fn\n    self._pgd_loss_fn = pgd_loss_fn\n\n  def _compute_gradient(self, loss, dense_features, gradient_tape=None):\n    """"""Computes the gradient given a loss and dense features.""""""\n    feature_values = list(dense_features.values())\n    if gradient_tape is None:\n      grads = tf.gradients(loss, feature_values)\n    else:\n      grads = gradient_tape.gradient(loss, feature_values)\n\n    # The order of elements returned by .values() and .keys() are guaranteed\n    # corresponding to each other.\n    keyed_grads = dict(zip(dense_features.keys(), grads))\n\n    invalid_grads, valid_grads = self._split_dict(keyed_grads,\n                                                  lambda grad: grad is None)\n    # Two cases that grad can be invalid (None):\n    # (1) The feature is not differentiable, like strings or integers.\n    # (2) The feature is not involved in loss computation.\n    if invalid_grads:\n      if self._raise_invalid_gradient:\n        raise ValueError(\'Cannot perturb features \' + str(invalid_grads.keys()))\n      logging.log_first_n(logging.WARNING, \'Cannot perturb features %s\', 1,\n                          invalid_grads.keys())\n\n    # Guards against numerical errors. If the gradient is malformed (inf, -inf,\n    # or NaN) on a dimension, replace it with 0, which has the effect of not\n    # perturbing the original sample along that perticular dimension.\n    return tf.nest.map_structure(\n        lambda g: tf.where(tf.math.is_finite(g), g, tf.zeros_like(g)),\n        valid_grads)\n\n  # The _compose_as_dict and _decompose_as functions are similar to\n  # tf.nest.{flatten, pack_sequence_as} except that the composed representation\n  # is a dictionary of (name, value) pairs instead of a list of values. The\n  # names are needed for joining values from different inputs (e.g. input\n  # features and feature masks) with possibly missing values (e.g. no mask for\n  # some features).\n  def _compose_as_dict(self, inputs):\n    if isinstance(inputs, collections.Mapping):\n      return inputs\n    elif isinstance(inputs, (tuple, list)):\n      return dict(enumerate(inputs))  # index -> value\n    else:\n      return {\'\': inputs} if inputs is not None else {}\n\n  def _decompose_as(self, structure, values):\n    if isinstance(structure, collections.Mapping):\n      return values\n    elif isinstance(structure, (tuple, list)):\n      return [values[index] for index in range(len(structure))]\n    else:\n      return values[\'\'] if structure is not None else None\n\n  def _split_dict(self, dictionary, predicate_fn):\n    """"""Splits `dictionary` into 2 by bool(predicate_fn(key, value)).""""""\n    positives, negatives = {}, {}\n    for key, value in dictionary.items():\n      if predicate_fn(value):\n        positives[key] = value\n      else:\n        negatives[key] = value\n    return positives, negatives\n\n  def gen_neighbor(self, input_features, pgd_labels=None):\n    """"""Generates adversarial neighbors and the corresponding weights.\n\n    This function perturbs only *dense* tensors to generate adversarial\n    neighbors. No perturbation will be applied on sparse tensors  (e.g., string\n    or int). Therefore, in the generated adversarial neighbors, the values of\n    these sparse tensors will be kept the same as the input_features. In other\n    words, if input_features is a dictionary mapping feature names to tensors,\n    the dense features will be perturbed and the values of sparse features will\n    remain the same.\n\n    Arguments:\n      input_features: a dense (float32) tensor, a list of dense tensors, or a\n        dictionary of feature names and dense tensors. The shape of the\n        tensor(s) should be either:\n        (a) pointwise samples: [batch_size, feat_len], or\n        (b) sequence samples: [batch_size, seq_len, feat_len]\n      pgd_labels: the labels corresponding to each input. This should have shape\n        `[batch_size, 1]`. This is required for PGD-generated adversaries, and\n        unused otherwise.\n\n    Returns:\n      adv_neighbor: the perturbed example, with the same shape and structure as\n        input_features\n      adv_weight: a dense (float32) tensor with shape of [batch_size, 1],\n        representing the weight for each neighbor\n\n    Raises:\n      ValueError: if some of the `input_features` cannot be perturbed due to\n        (a) it is a `tf.SparseTensor`,\n        (b) it has a non-differentiable type like string or integer, or\n        (c) it is not involved in loss computation.\n        This error is suppressed if `raise_invalid_gradient` is set to False\n        (which is the default).\n    """"""\n    loss = self._labeled_loss\n    gradient_tape = self._gradient_tape\n\n    # Composes both features and feature_masks to dictionaries, so that the\n    # feature_masks can be looked up by key.\n    features = self._compose_as_dict(input_features)\n    dense_original_features, sparse_original_features = self._split_dict(\n        features, lambda feature: isinstance(feature, tf.Tensor))\n    feature_masks = self._compose_as_dict(self._adv_config.feature_mask)\n    feature_min = self._compose_as_dict(self._adv_config.clip_value_min)\n    feature_max = self._compose_as_dict(self._adv_config.clip_value_max)\n    if sparse_original_features:\n      sparse_keys = str(sparse_original_features.keys())\n      if self._raise_invalid_gradient:\n        raise ValueError(\'Cannot perturb non-Tensor input: \' + sparse_keys)\n      logging.log_first_n(logging.WARNING,\n                          \'Cannot perturb non-Tensor input: %s\', 1, sparse_keys)\n    dense_features = dense_original_features\n    for t in range(self._adv_config.pgd_iterations):\n      keyed_grads = self._compute_gradient(loss, dense_features, gradient_tape)\n      masked_grads = {\n          key: utils.apply_feature_mask(grad, feature_masks.get(key, None))\n          for key, grad in keyed_grads.items()\n      }\n\n      unit_perturbations = utils.maximize_within_unit_norm(\n          masked_grads, self._adv_config.adv_grad_norm)\n      perturbations = tf.nest.map_structure(\n          lambda t: t * self._adv_config.adv_step_size, unit_perturbations)\n      # Clip perturbations into epsilon ball here. Note that this ball is\n      # centered around the original input point.\n      diff = {}\n      bounded_diff = {}\n      for key, perturb in perturbations.items():\n        # Only include features for which perturbation occurred. There is\n        # nothing to project for features without perturbations.\n        diff[key] = dense_features[key] + perturb - dense_original_features[key]\n      if self._adv_config.pgd_epsilon is not None:\n        bounded_diff = utils.project_to_ball(diff, self._adv_config.pgd_epsilon,\n                                             self._adv_config.adv_grad_norm)\n      else:\n        bounded_diff = diff\n      # Backfill the rest of the dense features.\n      for key, feature in dense_features.items():\n        if key not in bounded_diff:\n          bounded_diff[key] = feature - dense_original_features[key]\n      adv_neighbor = dict(sparse_original_features)\n      for key, feature in dense_original_features.items():\n        adv_neighbor[key] = tf.stop_gradient(\n            _apply_feature_constraints(\n                feature +\n                bounded_diff[key] if key in perturbations else feature,\n                feature_min.get(key, None), feature_max.get(key, None)))\n\n      # Update for the next iteration.\n      if t < self._adv_config.pgd_iterations - 1:\n        inputs_t = self._decompose_as(input_features, adv_neighbor)\n        # Compute the new loss to calculate gradients with.\n        features = self._compose_as_dict(inputs_t)\n        dense_features, _ = self._split_dict(\n            features, lambda feature: isinstance(feature, tf.Tensor))\n        if gradient_tape is not None:\n          with gradient_tape:\n            # Gradient calculated against dense features only.\n            gradient_tape.watch(dense_features)\n            loss = self._pgd_loss_fn(pgd_labels, self._pgd_model_fn(inputs_t))\n        else:\n          loss = self._pgd_loss_fn(pgd_labels, self._pgd_model_fn(inputs_t))\n\n    # Converts the perturbed examples back to their original structure.\n    adv_neighbor = self._decompose_as(input_features, adv_neighbor)\n\n    batch_size = tf.shape(list(features.values())[0])[0]\n    adv_weight = tf.ones([batch_size, 1])\n\n    return adv_neighbor, adv_weight\n\n\ndef gen_adv_neighbor(input_features,\n                     labeled_loss,\n                     config,\n                     raise_invalid_gradient=False,\n                     gradient_tape=None,\n                     pgd_model_fn=None,\n                     pgd_loss_fn=None,\n                     pgd_labels=None):\n  """"""Generates adversarial neighbors for the given input and loss.\n\n  This function implements the following operation:\n  `adv_neighbor = input_features + adv_step_size * gradient`\n  where `adv_step_size` is the step size (analogous to learning rate) for\n  searching/calculating adversarial neighbor.\n\n  Arguments:\n    input_features: a dense (float32) tensor, a list of dense tensors, or a\n      dictionary of feature names and dense tensors. The shape of the tensor(s)\n      should be either:\n      (a) pointwise samples: `[batch_size, feat_len]`, or\n      (b) sequence samples: `[batch_size, seq_len, feat_len]`. Note that only\n        dense (`float`) tensors in `input_features` will be perturbed and all\n        other features (`int`, `string`, or `SparseTensor`) will be kept as-is\n        in the returning `adv_neighbor`.\n    labeled_loss: A scalar tensor of floating point type calculated from true\n      labels (or supervisions).\n    config: A `nsl.configs.AdvNeighborConfig` object containing the following\n      hyperparameters for generating adversarial samples.\n      - \'feature_mask\': mask (with 0-1 values) applied on the graident.\n      - \'adv_step_size\': step size to find the adversarial sample.\n      - \'adv_grad_norm\': type of tensor norm to normalize the gradient.\n    raise_invalid_gradient: (optional) A Boolean flag indicating whether to\n      raise an error when gradients cannot be computed on any input feature.\n      There are three cases where this error may happen: (1) The feature is a\n        `SparseTensor`. (2) The feature has a non-differentiable `dtype`, like\n        string or integer. (3) The feature is not involved in loss computation.\n        If set to `False` (default), those inputs without gradient will be\n        ignored silently and not perturbed.\n    gradient_tape: A `tf.GradientTape` object watching the calculation from\n      `input_features` to `labeled_loss`. Can be omitted if running in graph\n      mode.\n    pgd_model_fn: The model to generate adversaries for. Generates predictions\n      for a given set of inputs, in the shape of `input_features`.\n    pgd_loss_fn: The loss function. Takes samples of labels and a model\n      predictions.\n    pgd_labels: labels for the input features. This should have shape\n      `[batch_size, 1]`. Required to generate adversaries with PGD, unused\n      otherwise.\n\n  Returns:\n    adv_neighbor: The perturbed example, with the same shape and structure as\n      `input_features`.\n    adv_weight: A dense `Tensor` with shape of `[batch_size, 1]`,\n      representing the weight for each neighbor.\n\n  Raises:\n    ValueError: In case of `raise_invalid_gradient` is set and some of the input\n      features cannot be perturbed. See `raise_invalid_gradient` for situations\n      where this can happen.\n  """"""\n  adv_helper = _GenAdvNeighbor(\n      labeled_loss,\n      config,\n      raise_invalid_gradient,\n      gradient_tape,\n      pgd_model_fn=pgd_model_fn,\n      pgd_loss_fn=pgd_loss_fn)\n  return adv_helper.gen_neighbor(input_features, pgd_labels)\n'"
neural_structured_learning/lib/adversarial_neighbor_test.py,245,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.lib.gen_adv_neighbor.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.lib import adversarial_neighbor as adv_lib\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\ndef call_gen_adv_neighbor_with_gradient_tape(x, loss_fn, adv_config):\n  with tf.GradientTape() as tape:\n    tape.watch(x)\n    loss = loss_fn(x)\n  adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n      x, loss, adv_config, gradient_tape=tape)\n  return adv_neighbor\n\n\ndef call_gen_adv_neighbor_with_tf_function(x, loss_fn, adv_config):\n\n  @tf.function\n  def gen_adv_neighbor(x):\n    loss = loss_fn(x)\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(x, loss, adv_config)\n    return adv_neighbor\n\n  return gen_adv_neighbor(x)\n\n\ndef call_multi_iter_gen_adv_neighbor_with_gradient_tape(x, y, model_fn, loss_fn,\n                                                        adv_config):\n  with tf.GradientTape() as tape:\n    tape.watch(x)\n    loss = loss_fn(y, model_fn(x))\n  adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n      x,\n      loss,\n      adv_config,\n      gradient_tape=tape,\n      pgd_labels=y,\n      pgd_model_fn=model_fn,\n      pgd_loss_fn=loss_fn)\n  return adv_neighbor\n\n\ndef call_multi_iter_gen_adv_neighbor_with_tf_function(x, y, model_fn, loss_fn,\n                                                      adv_config):\n\n  @tf.function\n  def gen_adv_neighbor(x):\n    loss = loss_fn(y, model_fn(x))\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        pgd_labels=y,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn)\n    return adv_neighbor\n\n  return gen_adv_neighbor(x)\n\n\nclass GenAdvNeighborTest(tf.test.TestCase, parameterized.TestCase):\n\n  def test_gen_adv_neighbor_for_single_tensor_feature(self):\n    # Simple linear regression\n    x = tf.constant([[-1.0, 1.0]])\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0]])\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      y_hat = tf.matmul(x, w)\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=tf.constant(1.0), adv_step_size=0.1, adv_grad_norm=\'l2\')\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x, loss, adv_config, gradient_tape=tape)\n\n    # gradient = [[6, 8]], normalized gradient = [[0.6, 0.8]]\n    expected_neighbor = [[-1.0 + 0.1 * 0.6, 1.0 + 0.1 * 0.8]]\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n  def test_multi_iter_gen_adv_neighbor_for_single_tensor_feature(self):\n    # Simple linear regression.\n    x = tf.constant([[-1.0, 1.0]])\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0]])\n    loss_fn = tf.math.squared_difference\n    model_fn = lambda inp: tf.matmul(inp, w)\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      y_hat = model_fn(x)\n      loss = loss_fn(y, y_hat)\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=tf.constant(1.0),\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2)\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        gradient_tape=tape,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    # Take two steps in the gradient direction.\n    expected_neighbor = [[-1.0 + 0.1 * 0.6 * 2, 1.0 + 0.1 * 0.8 * 2]]\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n  def test_multi_iter_gen_adv_neighbor_proj_limits(self):\n    # Simple linear regression.\n    x = tf.constant([[-1.0, 1.0]])\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0]])\n    loss_fn = tf.math.squared_difference\n    model_fn = lambda input: tf.matmul(input, w)\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      y_hat = model_fn(x)\n      loss = loss_fn(y, y_hat)\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=tf.constant(1.0),\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2,\n        pgd_epsilon=0.15)\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        gradient_tape=tape,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    # Take two steps in the gradient direction. Project back onto epsilon ball\n    # after iteration 2.\n    expected_neighbor = [[-1.0 + 0.1 * 0.6 * 1.5, 1.0 + 0.1 * 0.8 * 1.5]]\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n  def test_gen_adv_neighbor_for_tensor_list(self):\n    x = [tf.constant([[-1.0]]), tf.constant([[1.0]])]\n    y = tf.constant([0.0])\n    w = [tf.constant([[3.0]]), tf.constant([[4.0]])]\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      y_hat = tf.matmul(x[0], w[0]) + tf.matmul(x[1], w[1])\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=tf.constant(1.0), adv_step_size=0.1, adv_grad_norm=\'l2\')\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x, loss, adv_config, gradient_tape=tape)\n\n    # gradient = [[6, 8]], normalized gradient = [[0.6, 0.8]]\n    expected_neighbor = [[[-1.0 + 0.1 * 0.6]], [[1.0 + 0.1 * 0.8]]]\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n  def test_multi_iter_gen_adv_neighbor_for_tensor_list(self):\n    x = [tf.constant([[-1.0]]), tf.constant([[1.0]])]\n    y = tf.constant([0.0])\n    w = [tf.constant([[3.0]]), tf.constant([[4.0]])]\n    loss_fn = tf.math.squared_difference\n    model_fn = (lambda inp: tf.matmul(inp[0], w[0]) + tf.matmul(inp[1], w[1]))\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      y_hat = tf.matmul(x[0], w[0]) + tf.matmul(x[1], w[1])\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=tf.constant(1.0),\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2)\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        gradient_tape=tape,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    # gradient = [[6, 8]], normalized gradient = [[0.6, 0.8]]\n    expected_neighbor = [[[-1.0 + 0.1 * 0.6 * 2]], [[1.0 + 0.1 * 0.8 * 2]]]\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n  def _test_gen_adv_neighbor_for_feature_columns_setup(self):\n    # For linear regression\n    x = {\n        \'fc1\': tf.constant([[-1.0]]),\n        \'fc2\': tf.constant([[1.0]]),\n    }\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0]])\n\n    # gradient = [[6, 8]], normalized gradient = [[0.6, 0.8]]\n    expected_neighbor_fc1 = [[-1.0 + 0.1 * 0.6]]\n    expected_neighbor_fc2 = [[1.0 + 0.1 * 0.8]]\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask={}, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    return x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_for_feature_columns(self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config = (\n        self._test_gen_adv_neighbor_for_feature_columns_setup())\n\n    # Simple linear regression\n    x_stacked = tf.concat([x[\'fc1\'], x[\'fc2\']], axis=1)\n    y_hat = tf.matmul(x_stacked, w)\n    loss = tf.math.squared_difference(y, y_hat)\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(x, loss, adv_config)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n\n  def _test_multi_iter_gen_adv_neighbor_for_feature_columns_setup(self):\n    # For linear regression\n    x = {\n        \'fc1\': tf.constant([[-1.0]]),\n        \'fc2\': tf.constant([[1.0]]),\n    }\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0]])\n\n    # gradient = [[6, 8]], normalized gradient = [[0.6, 0.8]]\n    # Two iterations of 0.6 * 0.1, clipped.\n    expected_neighbor_fc1 = [[-1.0 + 0.13 * 0.6]]\n    # Two iterations of 0.8 * 0.1, clipped.\n    expected_neighbor_fc2 = [[1.0 + 0.13 * 0.8]]\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask={},\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2,\n        pgd_epsilon=0.13)\n    return x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_multi_iter_gen_adv_neighbor_for_feature_columns(self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config = (\n        self._test_multi_iter_gen_adv_neighbor_for_feature_columns_setup())\n    x_stacked = tf.concat([x[\'fc1\'], x[\'fc2\']], axis=1)\n    model_fn = (\n        lambda inp: tf.matmul(tf.concat([inp[\'fc1\'], inp[\'fc2\']], axis=1), w))\n    loss_fn = tf.math.squared_difference\n    y_hat = tf.matmul(x_stacked, w)\n    loss = tf.math.squared_difference(y, y_hat)\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n\n  def test_gen_adv_neighbor_for_feature_columns_v2(self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config = (\n        self._test_gen_adv_neighbor_for_feature_columns_setup())\n\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      # Simple linear regression\n      x_stacked = tf.concat([x[\'fc1\'], x[\'fc2\']], axis=1)\n      y_hat = tf.matmul(x_stacked, w)\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x, loss, adv_config, gradient_tape=tape)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n\n  def test_multi_iter_gen_adv_neighbor_for_feature_columns_v2(self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config = (\n        self._test_multi_iter_gen_adv_neighbor_for_feature_columns_setup())\n    loss_fn = tf.math.squared_difference\n    model_fn = (\n        lambda inp: tf.matmul(tf.concat([inp[\'fc1\'], inp[\'fc2\']], axis=1), w))\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      # Simple linear regression\n      x_stacked = tf.concat([x[\'fc1\'], x[\'fc2\']], axis=1)\n      y_hat = tf.matmul(x_stacked, w)\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        gradient_tape=tape,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n\n  def _test_gen_adv_neighbor_for_feature_columns_with_unused_setup(self):\n    # For linear regression.\n    x = {\n        \'fc1\': tf.constant([[-1.0]]),\n        \'fc2\': tf.constant([[1.0]]),\n        \'fc_unused\': tf.constant([[0.0]]),\n    }\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0]])\n\n    # gradient = [[6, 8]], normalized gradient = [[0.6, 0.8]]\n    expected_neighbor_fc1 = [[-1.0 + 0.1 * 0.6]]\n    expected_neighbor_fc2 = [[1.0 + 0.1 * 0.8]]\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    return x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_for_feature_columns_with_unused(self):\n    # Simple linear regression\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config = (\n        self._test_gen_adv_neighbor_for_feature_columns_with_unused_setup())\n    x_stacked = tf.concat([x[\'fc1\'], x[\'fc2\']], axis=1)\n    y_hat = tf.matmul(x_stacked, w)\n    loss = tf.math.squared_difference(y, y_hat)\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(x, loss, adv_config)\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n    self.assertAllClose([[0.0]], actual_neighbor[\'fc_unused\'])\n\n  def test_gen_adv_neighbor_for_feature_columns_with_unused_v2(self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config = (\n        self._test_gen_adv_neighbor_for_feature_columns_with_unused_setup())\n\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      # Simple linear regression\n      x_stacked = tf.concat([x[\'fc1\'], x[\'fc2\']], axis=1)\n      y_hat = tf.matmul(x_stacked, w)\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x, loss, adv_config, gradient_tape=tape)\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n    self.assertAllClose([[0.0]], actual_neighbor[\'fc_unused\'])\n\n  def _test_multi_iter_gen_adv_neighbor_for_feature_columns_with_unused_setup(\n      self):\n    # For linear regression.\n    x = {\n        \'fc1\': tf.constant([[-1.0]]),\n        \'fc2\': tf.constant([[1.0]]),\n        \'fc_unused\': tf.constant([[0.0]]),\n    }\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0]])\n\n    # gradient = [[6, 8]], normalized gradient = [[0.6, 0.8]]\n    # Two iterations of 0.6 * 0.1, clipped.\n    expected_neighbor_fc1 = [[-1.0 + 0.13 * 0.6]]\n    # Two iterations of 0.8 * 0.1, clipped.\n    expected_neighbor_fc2 = [[1.0 + 0.13 * 0.8]]\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask={},\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2,\n        pgd_epsilon=0.13)\n    return x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_multi_iter_gen_adv_neighbor_for_feature_columns_with_unused(self):\n    # Simple linear regression\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config = (\n        self.\n        _test_multi_iter_gen_adv_neighbor_for_feature_columns_with_unused_setup(\n        ))\n    x_stacked = tf.concat([x[\'fc1\'], x[\'fc2\']], axis=1)\n    model_fn = (\n        lambda inp: tf.matmul(tf.concat([inp[\'fc1\'], inp[\'fc2\']], axis=1), w))\n    loss_fn = tf.math.squared_difference\n    y_hat = tf.matmul(x_stacked, w)\n    loss = tf.math.squared_difference(y, y_hat)\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n    self.assertAllClose([[0.0]], actual_neighbor[\'fc_unused\'])\n\n  def test_multi_iter_gen_adv_neighbor_for_feature_columns_with_unused_v2(self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2, adv_config = (\n        self.\n        _test_multi_iter_gen_adv_neighbor_for_feature_columns_with_unused_setup(\n        ))\n\n    loss_fn = tf.math.squared_difference\n    model_fn = (\n        lambda inp: tf.matmul(tf.concat([inp[\'fc1\'], inp[\'fc2\']], axis=1), w))\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      # Simple linear regression\n      x_stacked = tf.concat([x[\'fc1\'], x[\'fc2\']], axis=1)\n      y_hat = tf.matmul(x_stacked, w)\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        gradient_tape=tape,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n    self.assertAllClose([[0.0]], actual_neighbor[\'fc_unused\'])\n\n  def _test_gen_adv_neighbor_for_feature_columns_with_int_feature(self):\n    # For linear regression\n    x = {\n        \'fc1\': tf.constant([[-1.0]]),\n        \'fc2\': tf.constant([[1.0]]),\n        \'fc_int\': tf.constant([[2]], dtype=tf.dtypes.int32),\n    }\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0], [1.0]])\n    # gradient = [[18, 24]], normalized gradient = [[0.6, 0.8]]\n    expected_neighbor_fc1 = [[-1.0 + 0.1 * 0.6]]\n    expected_neighbor_fc2 = [[1.0 + 0.1 * 0.8]]\n    return x, y, w, expected_neighbor_fc1, expected_neighbor_fc2\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_for_feature_columns_with_int_feature(self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2 = (\n        self._test_gen_adv_neighbor_for_feature_columns_with_int_feature())\n\n    # Simple linear regression.\n    x_stacked = tf.concat(\n        [x[\'fc1\'], x[\'fc2\'],\n         tf.cast(x[\'fc_int\'], tf.dtypes.float32)], axis=1)\n    y_hat = tf.matmul(x_stacked, w)\n    loss = tf.math.squared_difference(y, y_hat)\n\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(x, loss, adv_config)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n    self.assertAllClose([[2]], actual_neighbor[\'fc_int\'])\n\n  def test_gen_adv_neighbor_for_feature_columns_with_int_feature_v2(self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2 = (\n        self._test_gen_adv_neighbor_for_feature_columns_with_int_feature())\n\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      # Simple linear regression.\n      x_stacked = tf.concat(\n          [x[\'fc1\'], x[\'fc2\'],\n           tf.cast(x[\'fc_int\'], tf.dtypes.float32)], axis=1)\n      y_hat = tf.matmul(x_stacked, w)\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x, loss, adv_config, gradient_tape=tape)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n    self.assertAllClose([[2]], actual_neighbor[\'fc_int\'])\n\n  def _test_multi_iter_gen_adv_neighbor_for_feature_columns_with_int_feature(\n      self):\n    # For linear regression\n    x = {\n        \'fc1\': tf.constant([[-1.0]]),\n        \'fc2\': tf.constant([[1.0]]),\n        \'fc_int\': tf.constant([[2]], dtype=tf.dtypes.int32),\n    }\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0], [1.0]])\n    # Two iterations of 0.6 * 0.1, clipped.\n    expected_neighbor_fc1 = [[-1.0 + 0.13 * 0.6]]\n    # Two iterations of 0.8 * 0.1, clipped.\n    expected_neighbor_fc2 = [[1.0 + 0.13 * 0.8]]\n    return x, y, w, expected_neighbor_fc1, expected_neighbor_fc2\n\n  @test_util.deprecated_graph_mode_only\n  def test_multi_iter_gen_adv_neighbor_for_feature_columns_with_int_feature(\n      self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2 = (\n        self\n        ._test_multi_iter_gen_adv_neighbor_for_feature_columns_with_int_feature(\n        ))\n\n    # Simple linear regression.\n    x_stacked = tf.concat(\n        [x[\'fc1\'], x[\'fc2\'],\n         tf.cast(x[\'fc_int\'], tf.dtypes.float32)], axis=1)\n    y_hat = tf.matmul(x_stacked, w)\n    loss = tf.math.squared_difference(y, y_hat)\n    loss_fn = tf.math.squared_difference\n\n    @tf.function\n    def model_fn(inp):\n      x_stacked = tf.concat(\n          [inp[\'fc1\'], inp[\'fc2\'],\n           tf.cast(inp[\'fc_int\'], tf.dtypes.float32)],\n          axis=1)\n      return tf.matmul(x_stacked, w)\n\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None,\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2,\n        pgd_epsilon=0.13)\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n    self.assertAllClose([[2]], actual_neighbor[\'fc_int\'])\n\n  def test_multi_iter_gen_adv_neighbor_for_feature_columns_with_int_feature_v2(\n      self):\n    x, y, w, expected_neighbor_fc1, expected_neighbor_fc2 = (\n        self\n        ._test_multi_iter_gen_adv_neighbor_for_feature_columns_with_int_feature(\n        ))\n    loss_fn = tf.math.squared_difference\n\n    @tf.function\n    def model_fn(inp):\n      x_stacked = tf.concat(\n          [inp[\'fc1\'], inp[\'fc2\'],\n           tf.cast(inp[\'fc_int\'], tf.dtypes.float32)],\n          axis=1)\n      return tf.matmul(x_stacked, w)\n\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      # Simple linear regression.\n      x_stacked = tf.concat(\n          [x[\'fc1\'], x[\'fc2\'],\n           tf.cast(x[\'fc_int\'], tf.dtypes.float32)], axis=1)\n      y_hat = tf.matmul(x_stacked, w)\n      loss = tf.math.squared_difference(y, y_hat)\n\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None,\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2,\n        pgd_epsilon=0.13)\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        gradient_tape=tape,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(expected_neighbor_fc1, actual_neighbor[\'fc1\'])\n    self.assertAllClose(expected_neighbor_fc2, actual_neighbor[\'fc2\'])\n    self.assertAllClose([[2]], actual_neighbor[\'fc_int\'])\n\n  def _test_gen_adv_neighbor_should_ignore_sparse_tensors_setup(self):\n    # sparse_feature represents [[1, 0, 2], [0, 3, 0]].\n    sparse_feature = tf.SparseTensor(\n        indices=[[0, 0], [0, 2], [1, 1]],\n        values=[1.0, 2.0, 3.0],\n        dense_shape=(2, 3))\n    x = {\'sparse\': sparse_feature, \'dense\': tf.constant([[-1.0], [1.0]])}\n    w_sparse = tf.constant([[5.0], [4.0], [3.0]])\n    w_dense = tf.constant([[6.0]])\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    return x, w_sparse, w_dense, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_should_ignore_sparse_tensors(self):\n    x, w_sparse, w_dense, adv_config = (\n        self._test_gen_adv_neighbor_should_ignore_sparse_tensors_setup())\n    prod_sparse = tf.sparse.reduce_sum(w_sparse * x[\'sparse\'])\n    prod_dense = tf.reduce_sum(input_tensor=w_dense * x[\'dense\'])\n    loss = prod_dense + prod_sparse\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(x, loss, adv_config)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # No perturbation on sparse features.\n    actual_sparse = actual_neighbor[\'sparse\']\n    self.assertAllEqual(x[\'sparse\'].dense_shape, actual_sparse.dense_shape)\n    self.assertAllEqual(x[\'sparse\'].indices, actual_sparse.indices)\n    self.assertAllClose(x[\'sparse\'].values, actual_sparse.values)\n    # gradient = w_dense\n    # perturbation = adv_step_size * sign(w_dense) = 0.1\n    self.assertAllClose([[-0.9], [1.1]], actual_neighbor[\'dense\'])\n\n  def test_gen_adv_neighbor_should_ignore_sparse_tensors_v2(self):\n    x, w_sparse, w_dense, adv_config = (\n        self._test_gen_adv_neighbor_should_ignore_sparse_tensors_setup())\n\n    with tf.GradientTape() as tape:\n      tape.watch([x[\'sparse\'].values, x[\'dense\']])\n      prod_sparse = tf.reduce_sum(\n          tf.sparse.sparse_dense_matmul(x[\'sparse\'], w_sparse))\n      prod_dense = tf.reduce_sum(input_tensor=w_dense * x[\'dense\'])\n      loss = prod_dense + prod_sparse\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x, loss, adv_config, gradient_tape=tape)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # No perturbation on sparse features.\n    actual_sparse = actual_neighbor[\'sparse\']\n    self.assertAllEqual(x[\'sparse\'].dense_shape, actual_sparse.dense_shape)\n    self.assertAllEqual(x[\'sparse\'].indices, actual_sparse.indices)\n    self.assertAllClose(x[\'sparse\'].values, actual_sparse.values)\n    # gradient = w_dense\n    # perturbation = adv_step_size * sign(w_dense) = 0.1\n    self.assertAllClose([[-0.9], [1.1]], actual_neighbor[\'dense\'])\n\n  def _test_multi_iter_gen_adv_neighbor_should_ignore_sparse_tensors_setup(\n      self):\n\n    @tf.function\n    def model_fn(inp):\n      prod_sparse = tf.reduce_sum(\n          tf.sparse.sparse_dense_matmul(inp[\'sparse\'], w_sparse))\n      prod_dense = tf.reduce_sum(input_tensor=w_dense * inp[\'dense\'])\n      return prod_sparse + prod_dense\n\n    @tf.function\n    def loss_fn(label, pred):\n      return tf.abs(label - pred)\n\n    # sparse_feature represents [[1, 0, 2], [0, 3, 0]].\n    sparse_feature = tf.SparseTensor(\n        indices=[[0, 0], [0, 2], [1, 1]],\n        values=[1.0, 2.0, 3.0],\n        dense_shape=(2, 3))\n    x = {\'sparse\': sparse_feature, \'dense\': tf.constant([[-1.0], [1.0]])}\n    w_sparse = tf.constant([[5.0], [4.0], [3.0]])\n    w_dense = tf.constant([[6.0]])\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None,\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2,\n        pgd_epsilon=0.13)\n    return x, w_sparse, w_dense, adv_config, model_fn, loss_fn\n\n  @test_util.deprecated_graph_mode_only\n  def test_multi_iter_gen_adv_neighbor_should_ignore_sparse_tensors(self):\n    x, w_sparse, w_dense, adv_config, model_fn, loss_fn = (\n        self\n        ._test_multi_iter_gen_adv_neighbor_should_ignore_sparse_tensors_setup())\n    y = tf.constant([0.0])\n\n    prod_sparse = tf.reduce_sum(\n        tf.sparse.sparse_dense_matmul(x[\'sparse\'], w_sparse))\n    prod_dense = tf.reduce_sum(input_tensor=w_dense * x[\'dense\'])\n    loss = tf.abs(y - (prod_dense + prod_sparse))\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        pgd_labels=y,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # No perturbation on sparse features.\n    actual_sparse = actual_neighbor[\'sparse\']\n    self.assertAllEqual(x[\'sparse\'].dense_shape, actual_sparse.dense_shape)\n    self.assertAllEqual(x[\'sparse\'].indices, actual_sparse.indices)\n    self.assertAllClose(x[\'sparse\'].values, actual_sparse.values)\n    # gradient = w_dense\n    # perturbation = adv_step_size * sign(w_dense) * 2 = 0.2, clipped to 0.13.\n    self.assertAllClose([[-0.87], [1.13]], actual_neighbor[\'dense\'])\n\n  def test_multi_iter_gen_adv_neighbor_should_ignore_sparse_tensors_v2(self):\n    x, w_sparse, w_dense, adv_config, model_fn, loss_fn = (\n        self\n        ._test_multi_iter_gen_adv_neighbor_should_ignore_sparse_tensors_setup())\n    y = tf.constant([0.0])\n\n    with tf.GradientTape() as tape:\n      tape.watch([x[\'sparse\'].values, x[\'dense\']])\n      prod_sparse = tf.reduce_sum(\n          tf.sparse.sparse_dense_matmul(x[\'sparse\'], w_sparse))\n      prod_dense = tf.reduce_sum(input_tensor=w_dense * x[\'dense\'])\n      loss = prod_dense + prod_sparse\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        pgd_labels=y,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        gradient_tape=tape)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # No perturbation on sparse features.\n    actual_sparse = actual_neighbor[\'sparse\']\n    self.assertAllEqual(x[\'sparse\'].dense_shape, actual_sparse.dense_shape)\n    self.assertAllEqual(x[\'sparse\'].indices, actual_sparse.indices)\n    self.assertAllClose(x[\'sparse\'].values, actual_sparse.values)\n    # gradient = w_dense\n    # perturbation = adv_step_size * sign(w_dense) * 2 = 0.2, clipped to 0.13.\n    self.assertAllClose([[-0.87], [1.13]], actual_neighbor[\'dense\'])\n\n  def _test_gen_adv_neighbor_to_raise_for_sparse_tensors_setup(self):\n    sparse_feature = tf.SparseTensor(\n        indices=[[0, 0], [0, 2], [1, 1]],\n        values=[1.0, 2.0, 3.0],\n        dense_shape=(2, 3))\n    x = {\'sparse\': sparse_feature, \'dense\': tf.constant([[-1.0], [1.0]])}\n    w_sparse = tf.constant([[5.0], [4.0], [3.0]])\n    w_dense = tf.constant([[6.0]])\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    return x, w_sparse, w_dense, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_to_raise_for_sparse_tensors(self):\n    x, w_sparse, w_dense, adv_config = (\n        self._test_gen_adv_neighbor_to_raise_for_sparse_tensors_setup())\n    prod_sparse = tf.sparse.reduce_sum(w_sparse * x[\'sparse\'])\n    prod_dense = tf.reduce_sum(input_tensor=w_dense * x[\'dense\'])\n    loss = prod_dense + prod_sparse\n\n    with self.assertRaisesRegex(ValueError, \'Cannot perturb.*sparse\'):\n      adv_lib.gen_adv_neighbor(x, loss, adv_config, raise_invalid_gradient=True)\n\n  def test_gen_adv_neighbor_to_raise_for_sparse_tensors_v2(self):\n    x, w_sparse, w_dense, adv_config = (\n        self._test_gen_adv_neighbor_to_raise_for_sparse_tensors_setup())\n\n    with tf.GradientTape() as tape:\n      tape.watch([x[\'sparse\'].values, x[\'dense\']])\n      prod_sparse = tf.reduce_sum(\n          tf.sparse.sparse_dense_matmul(x[\'sparse\'], w_sparse))\n      prod_dense = tf.reduce_sum(input_tensor=w_dense * x[\'dense\'])\n      loss = prod_dense + prod_sparse\n\n    with self.assertRaisesRegex(ValueError, \'Cannot perturb.*sparse\'):\n      adv_lib.gen_adv_neighbor(\n          x, loss, adv_config, raise_invalid_gradient=True, gradient_tape=tape)\n\n  def _test_gen_adv_neighbor_to_raise_for_nondifferentiable_input_setup(self):\n    x = {\n        \'float\': tf.constant([[-1.0]]),\n        \'int\': tf.constant([[2]], dtype=tf.dtypes.int32),\n    }\n    y = tf.constant([0.0])\n    w = tf.constant([[3.0], [4.0]])\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    return x, y, w, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_to_raise_for_nondifferentiable_input(self):\n    x, y, w, adv_config = (\n        self._test_gen_adv_neighbor_to_raise_for_nondifferentiable_input_setup(\n        ))\n    x_stacked = tf.concat(\n        [x[\'float\'], tf.cast(x[\'int\'], tf.dtypes.float32)], axis=1)\n    y_hat = tf.matmul(x_stacked, w)\n    loss = tf.math.squared_difference(y, y_hat)\n\n    with self.assertRaisesRegex(ValueError, \'Cannot perturb.*int\'):\n      adv_lib.gen_adv_neighbor(x, loss, adv_config, raise_invalid_gradient=True)\n\n  def test_gen_adv_neighbor_to_raise_for_nondifferentiable_input_v2(self):\n    x, y, w, adv_config = (\n        self._test_gen_adv_neighbor_to_raise_for_nondifferentiable_input_setup(\n        ))\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      x_stacked = tf.concat(\n          [x[\'float\'], tf.cast(x[\'int\'], tf.dtypes.float32)], axis=1)\n      y_hat = tf.matmul(x_stacked, w)\n      loss = tf.math.squared_difference(y, y_hat)\n\n    with self.assertRaisesRegex(ValueError, \'Cannot perturb.*int\'):\n      adv_lib.gen_adv_neighbor(\n          x, loss, adv_config, raise_invalid_gradient=True, gradient_tape=tape)\n\n  def _test_gen_adv_neighbor_to_raise_for_disconnected_input_setup(self):\n    x = {\n        \'f1\': tf.constant([[1.0]]),\n        \'f2\': tf.constant([[2.0]]),\n    }\n    w = tf.constant([3.0])\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    return x, w, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_to_raise_for_disconnected_input(self):\n    x, w, adv_config = (\n        self._test_gen_adv_neighbor_to_raise_for_disconnected_input_setup())\n    loss = tf.reduce_sum(input_tensor=w * x[\'f1\'])\n\n    with self.assertRaisesRegex(ValueError, \'Cannot perturb.*f2\'):\n      adv_lib.gen_adv_neighbor(x, loss, adv_config, raise_invalid_gradient=True)\n\n  def test_gen_adv_neighbor_to_raise_for_disconnected_input_v2(self):\n    x, w, adv_config = (\n        self._test_gen_adv_neighbor_to_raise_for_disconnected_input_setup())\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      loss = tf.reduce_sum(input_tensor=w * x[\'f1\'])\n\n    with self.assertRaisesRegex(ValueError, \'Cannot perturb.*f2\'):\n      adv_lib.gen_adv_neighbor(\n          x, loss, adv_config, raise_invalid_gradient=True, gradient_tape=tape)\n\n  def test_gen_adv_neighbor_not_knowing_input_in_advance(self):\n\n    @tf.function\n    def _gen_adv_neighbor(x):\n      w = tf.constant([[3.0], [4.0]])\n      loss = tf.matmul(x, w)\n      adv_config = configs.AdvNeighborConfig(\n          feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n      adv_neighbor, _ = adv_lib.gen_adv_neighbor(x, loss, adv_config)\n      return adv_neighbor\n\n    x = tf.constant([[1.0, 1.0], [0.0, 0.0], [-1.0, -1.0]])\n    actual_neighbor = self.evaluate(_gen_adv_neighbor(x))\n\n    # gradient = w\n    # perturbation = adv_step_size * normalize(gradient) = [0.06, 0.08]\n    expected_neighbor = [[1.06, 1.08], [0.06, 0.08], [-0.94, -0.92]]\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n  def test_multi_iter_gen_adv_neighbor_not_knowing_input_in_advance(self):\n\n    @tf.function\n    def _gen_adv_neighbor(x):\n      w = tf.constant([[3.0], [4.0]])\n      loss = tf.matmul(x, w)\n      y = tf.constant([0.0])\n      model_fn = lambda inp: tf.matmul(inp, w)\n\n      @tf.function\n      def loss_fn(_, pred):\n        return pred\n\n      adv_config = configs.AdvNeighborConfig(\n          feature_mask=None,\n          adv_step_size=0.1,\n          adv_grad_norm=\'l2\',\n          pgd_epsilon=0.15,\n          pgd_iterations=2)\n      adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n          x,\n          loss,\n          adv_config,\n          pgd_labels=y,\n          pgd_model_fn=model_fn,\n          pgd_loss_fn=loss_fn)\n      return adv_neighbor\n\n    x = tf.constant([[1.0, 1.0], [0.0, 0.0], [-1.0, -1.0]])\n    actual_neighbor = self.evaluate(_gen_adv_neighbor(x))\n\n    # gradient = w\n    # perturbation = 2 * adv_step_size * normalize(gradient) = [0.12, 0.16]\n    # Clipping perturbation to 0.15 epsilon gives [0.09, 0.12].\n    expected_neighbor = [[1.09, 1.12], [0.09, 0.12], [-0.91, -0.88]]\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n  def _test_gen_adv_neighbor_for_all_input_disconnected_setup(self):\n    x = tf.constant([[1.0]])\n    loss = tf.constant(1.0)\n    adv_config = configs.AdvNeighborConfig()\n    return x, loss, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_for_all_input_disconnected(self):\n    x, loss, adv_config = (\n        self._test_gen_adv_neighbor_for_all_input_disconnected_setup())\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(x, loss, adv_config)\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllEqual(x, actual_neighbor)\n\n  def test_gen_adv_neighbor_for_all_input_disconnected_v2(self):\n    x, loss, adv_config = (\n        self._test_gen_adv_neighbor_for_all_input_disconnected_setup())\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x, loss, adv_config, gradient_tape=tape)\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllEqual(x, actual_neighbor)\n\n  def _test_gen_adv_neighbor_for_features_with_different_shapes_setup(self):\n    w1 = tf.constant(1.0, shape=(2, 2, 3))\n    w2 = tf.constant(1.0, shape=(2, 2))\n    f1 = tf.constant([[[[0.0, 0.1, 0.2], [0.3, 0.4, 0.5]],\n                       [[0.6, 0.7, 0.8], [0.9, 1.0, 1.1]]],\n                      [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n                       [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]])\n    f2 = tf.constant([[[1.2, 1.3], [1.4, 1.5]], [[0.0, 0.0], [0.0, 0.0]]])\n    x = {\'f1\': f1, \'f2\': f2}\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None, adv_step_size=0.1, adv_grad_norm=\'l2\')\n    return x, w1, w2, adv_config\n\n  @test_util.deprecated_graph_mode_only\n  def test_gen_adv_neighbor_for_features_with_different_shapes(self):\n    x, w1, w2, adv_config = (\n        self._test_gen_adv_neighbor_for_features_with_different_shapes_setup())\n    loss = tf.reduce_sum(input_tensor=w1 *\n                         x[\'f1\']) + tf.reduce_sum(input_tensor=w2 * x[\'f2\'])\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(x, loss, adv_config)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # gradient = w\n    # perturbation = adv_step_size * normalize(gradient) = 0.025 in each dim\n    perturbation = 0.025\n    self.assertAllClose(x[\'f1\'] + perturbation, actual_neighbor[\'f1\'])\n    self.assertAllClose(x[\'f2\'] + perturbation, actual_neighbor[\'f2\'])\n\n  def test_gen_adv_neighbor_for_features_with_different_shapes_v2(self):\n    x, w1, w2, adv_config = (\n        self._test_gen_adv_neighbor_for_features_with_different_shapes_setup())\n    with tf.GradientTape() as tape:\n      tape.watch(list(x.values()))\n      loss = tf.reduce_sum(input_tensor=w1 *\n                           x[\'f1\']) + tf.reduce_sum(input_tensor=w2 * x[\'f2\'])\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x, loss, adv_config, gradient_tape=tape)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # gradient = w\n    # perturbation = adv_step_size * normalize(gradient) = 0.05 in each dim\n    perturbation = 0.025\n    self.assertAllClose(x[\'f1\'] + perturbation, actual_neighbor[\'f1\'])\n    self.assertAllClose(x[\'f2\'] + perturbation, actual_neighbor[\'f2\'])\n\n  def _test_multi_iter_gen_adv_neighbor_for_features_with_different_shapes_setup(\n      self):\n    w1 = tf.constant(1.0, shape=(2, 2, 3))\n    w2 = tf.constant(1.0, shape=(2, 2))\n    f1 = tf.constant([[[[0.0, 0.1, 0.2], [0.3, 0.4, 0.5]],\n                       [[0.6, 0.7, 0.8], [0.9, 1.0, 1.1]]],\n                      [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n                       [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]])\n    f2 = tf.constant([[[1.2, 1.3], [1.4, 1.5]], [[0.0, 0.0], [0.0, 0.0]]])\n    x = {\'f1\': f1, \'f2\': f2}\n\n    @tf.function\n    def model_fn(inp):\n      return tf.reduce_sum(input_tensor=w1 * inp[\'f1\']) + tf.reduce_sum(\n          input_tensor=w2 * inp[\'f2\'])\n\n    @tf.function\n    def loss_fn(_, pred):\n      return pred\n\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None,\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        pgd_iterations=2,\n        pgd_epsilon=0.15)\n    return x, w1, w2, adv_config, model_fn, loss_fn\n\n  @test_util.deprecated_graph_mode_only\n  def test_multi_iter_gen_adv_neighbor_for_features_with_different_shapes(self):\n    x, w1, w2, adv_config, model_fn, loss_fn = (\n        self.\n        _test_multi_iter_gen_adv_neighbor_for_features_with_different_shapes_setup(\n        ))\n    y = tf.constant([0.0])\n\n    loss = tf.reduce_sum(input_tensor=w1 *\n                         x[\'f1\']) + tf.reduce_sum(input_tensor=w2 * x[\'f2\'])\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # gradient = w\n    # perturbation = adv_step_size * normalize(gradient) = 0.025 in each dim\n    # Epsilon 0.15, projects back to roughly 0.04330\n    # f2 does not need to be clipped.\n    perturbation = 0.025 * 2\n    projected_perturb = perturbation * 0.15 / tf.sqrt(16 * tf.square(0.05))\n    self.assertAllClose(x[\'f1\'] + projected_perturb, actual_neighbor[\'f1\'])\n    self.assertAllClose(x[\'f2\'] + projected_perturb, actual_neighbor[\'f2\'])\n\n  def test_multi_iter_gen_adv_neighbor_for_features_with_different_shapes_v2(\n      self):\n    x, w1, w2, adv_config, model_fn, loss_fn = (\n        self.\n        _test_multi_iter_gen_adv_neighbor_for_features_with_different_shapes_setup(\n        ))\n    y = tf.constant([0.0])\n\n    with tf.GradientTape() as tape:\n      tape.watch(list(x.values()))\n      loss = tf.reduce_sum(input_tensor=w1 *\n                           x[\'f1\']) + tf.reduce_sum(input_tensor=w2 * x[\'f2\'])\n\n    adv_neighbor, _ = adv_lib.gen_adv_neighbor(\n        x,\n        loss,\n        adv_config,\n        gradient_tape=tape,\n        pgd_model_fn=model_fn,\n        pgd_loss_fn=loss_fn,\n        pgd_labels=y)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # gradient = w\n    # perturbation = adv_step_size * normalize(gradient) = 0.025 in each dim\n    # Epsilon 0.15, projects back to roughly 0.04330\n    # f2 does not need to be clipped.\n    perturbation = 0.025 * 2\n    projected_perturb = perturbation * 0.15 / tf.sqrt(16 * tf.square(0.05))\n    self.assertAllClose(x[\'f1\'] + projected_perturb, actual_neighbor[\'f1\'])\n    self.assertAllClose(x[\'f2\'] + projected_perturb, actual_neighbor[\'f2\'])\n\n  @parameterized.named_parameters([\n      (\'gradient_tape\', call_gen_adv_neighbor_with_gradient_tape),\n      (\'tf_function\', call_gen_adv_neighbor_with_tf_function),\n  ])\n  def test_gen_adv_neighbor_respects_feature_constraints(\n      self, gen_adv_neighbor_fn):\n    x = tf.constant([[0.0, 1.0]])\n    w = tf.constant([[-1.0, 1.0]])\n\n    loss_fn = lambda x: tf.linalg.matmul(x, w, transpose_b=True)\n    adv_config = configs.AdvNeighborConfig(\n        feature_mask=None,\n        adv_step_size=0.1,\n        adv_grad_norm=\'l2\',\n        clip_value_min=0.0,\n        clip_value_max=1.0)\n    adv_neighbor = gen_adv_neighbor_fn(x, loss_fn, adv_config)\n    actual_neighbor = self.evaluate(adv_neighbor)\n    self.assertAllClose(x, actual_neighbor)\n\n  @parameterized.named_parameters([\n      (\'gradient_tape\', call_gen_adv_neighbor_with_gradient_tape),\n      (\'tf_function\', call_gen_adv_neighbor_with_tf_function),\n  ])\n  def test_gen_adv_neighbor_respects_per_feature_constraints(\n      self, gen_adv_neighbor_fn):\n    w1 = tf.constant(1.0, shape=(2, 2, 3))\n    w2 = tf.constant(-1.0, shape=(2, 2))\n    f1 = np.array([[[[0.0, 0.1, 0.2], [0.3, 0.4, 0.5]],\n                    [[0.6, 0.7, 0.8], [0.9, 1.0, 1.1]]]],\n                  dtype=np.float32)\n    f2 = np.array([[[0.0, 0.33], [0.66, 1.0]]], dtype=np.float32)\n    x = {\'f1\': tf.constant(f1), \'f2\': tf.constant(f2)}\n\n    def loss_fn(x):\n      return tf.reduce_sum(w1 * x[\'f1\']) + tf.reduce_sum(w2 * x[\'f2\'])\n\n    adv_step_size = 0.5\n    adv_config = configs.AdvNeighborConfig(\n        adv_step_size=adv_step_size,\n        adv_grad_norm=\'infinity\',\n        clip_value_min={\'f2\': 0.0},\n        clip_value_max={\'f1\': 1.0})\n    adv_neighbor = gen_adv_neighbor_fn(x, loss_fn, adv_config)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # gradient = w, perturbation = adv_step_size * sign(w)\n    expected_neighbor = {\n        \'f1\': np.minimum(f1 + adv_step_size, 1.0),\n        \'f2\': np.maximum(f2 - adv_step_size, 0.0),\n    }\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n  @parameterized.named_parameters([\n      (\'gradient_tape\', call_multi_iter_gen_adv_neighbor_with_gradient_tape),\n      (\'tf_function\', call_multi_iter_gen_adv_neighbor_with_tf_function),\n  ])\n  def test_gen_adv_neighbor_multi_iter_respects_feature_constraints(\n      self, gen_adv_neighbor_fn):\n    w1 = tf.constant(1.0, shape=(2, 2, 3))\n    w2 = tf.constant(-1.0, shape=(2, 2))\n    f1 = np.array([[[[0.0, 0.1, 0.2], [0.3, 0.4, 0.5]],\n                    [[0.6, 0.7, 0.8], [0.9, 1.0, 1.1]]]],\n                  dtype=np.float32)\n    f2 = np.array([[[0.0, 0.33], [0.66, 1.0]]], dtype=np.float32)\n    x = {\'f1\': tf.constant(f1), \'f2\': tf.constant(f2)}\n    y = tf.constant([0.0])\n\n    def model_fn(x):\n      return tf.reduce_sum(w1 * x[\'f1\']) + tf.reduce_sum(w2 * x[\'f2\'])\n\n    def loss_fn(_, pred):\n      return pred\n\n    adv_step_size = 0.5\n    adv_config = configs.AdvNeighborConfig(\n        adv_step_size=adv_step_size,\n        adv_grad_norm=\'infinity\',\n        pgd_epsilon=0.4,\n        pgd_iterations=2,\n        clip_value_min={\'f2\': 0.0},\n        clip_value_max={\'f1\': 1.0})\n    adv_neighbor = gen_adv_neighbor_fn(x, y, model_fn, loss_fn, adv_config)\n    actual_neighbor = self.evaluate(adv_neighbor)\n\n    # gradient = w, perturbation = min(adv_step_size * sign(w), 0.4)\n    expected_neighbor = {\n        \'f1\': np.minimum(f1 + 0.4, 1.0),\n        \'f2\': np.maximum(f2 - 0.4, 0.0),\n    }\n    self.assertAllClose(expected_neighbor, actual_neighbor)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/lib/distances.py,40,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Distance functions used in Neural Structured Learning.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport neural_structured_learning.configs as configs\n\nimport tensorflow as tf\n\n\ndef _assert_multinomial_distribution(input_tensor, axis):\n  """"""Assert input has valid multinomial distribution along `axis`.""""""\n  sum_of_multinomial_distribution = tf.reduce_sum(\n      input_tensor=input_tensor, axis=axis)\n  return [\n      tf.debugging.assert_non_negative(input_tensor),\n      tf.debugging.assert_near(\n          sum_of_multinomial_distribution,\n          tf.constant(1.0),\n          message=\'x and/or y is not a proper probability distribution\'),\n  ]\n\n\ndef _assert_valid_axis(ndims, axis):\n  """"""Assert the condition `-ndims < axis <= ndims` if `axis` is not `None`.""""""\n  if axis and (axis < -ndims or axis >= ndims):\n    raise ValueError(\'axis = %d not in [%d, %d)\' % (axis, -ndims, ndims))\n\n\ndef _kl_divergence_fn(true_dist, predicted_dist):\n  epsilon = 1e-7  # A small increment to add to avoid taking a log of zero.\n  return true_dist * tf.math.log(true_dist + epsilon) - true_dist * tf.math.log(\n      predicted_dist + epsilon)\n\n\ndef kl_divergence(\n    labels,\n    predictions,\n    axis=None,\n    weights=1.0,\n    scope=None,\n    loss_collection=tf.compat.v1.GraphKeys.LOSSES,\n    reduction=tf.compat.v1.losses.Reduction.SUM_BY_NONZERO_WEIGHTS):\n  """"""Adds a KL-divergence to the training procedure.\n\n  For brevity, let `P = labels` and `Q = predictions`. The\n  Kullback-Leibler divergence `KL(P||Q)` is:\n\n  ```\n  KL(P||Q) = P * log(P) - P * log(Q)\n  ```\n\n  Note: the function assumes that `predictions` and `labels` are the values of\n  a multinomial distribution, i.e., each value is the probability of the\n  corresponding class.\n\n  For the usage of `weights` and `reduction`, please refer to `tf.losses`.\n\n  Args:\n    labels: `Tensor` of type `float32` or `float64`, with shape `[d1, ..., dN,\n      num_classes]`, represents the target distribution.\n    predictions: `Tensor` of the same type and shape as `labels`, represents\n      the predicted distribution.\n    axis: The dimension along which the KL divergence is computed. The values\n      of `labels` and `predictions` along `axis` should meet the requirements\n      of a multinomial distribution.\n    weights: (optional) `Tensor` whose rank is either 0, or the same as that of\n      `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n      be either `1`, or the same as the corresponding `losses` dimension).\n    scope: The scope for the operations performed in computing the loss.\n    loss_collection: Collection to which the loss will be added.\n    reduction: Type of reduction to apply to the loss.\n\n  Returns:\n    Weighted loss `float` `Tensor`. If `reduction` is `NONE`, this has the same\n    shape as `labels`, otherwise, it is a scalar.\n  Raises:\n    InvalidArgumentError: If `labels` or `predictions` don\'t meet the\n      requirements of a multinomial distribution.\n    ValueError: If `axis` is `None`, if the shape of `predictions` doesn\'t\n      match that of `labels`, or if the shape of `weights` is invalid.\n  """"""\n  with tf.compat.v1.name_scope(scope, \'kl_divergence\',\n                               (predictions, labels, weights)) as scope:\n    labels = tf.cast(labels, tf.dtypes.float32)\n    predictions = tf.cast(predictions, tf.dtypes.float32)\n    predictions.get_shape().assert_is_compatible_with(labels.get_shape())\n    if axis is None:\n      raise ValueError(\'You must specify ""axis"".\')\n    _assert_valid_axis(labels.get_shape().ndims, axis)\n    assert_list = _assert_multinomial_distribution(\n        labels, axis) + _assert_multinomial_distribution(predictions, axis)\n    with tf.control_dependencies(assert_list):\n      divergence_tensor = _kl_divergence_fn(labels, predictions)\n      divergence = tf.reduce_sum(\n          input_tensor=divergence_tensor, axis=(axis,), keepdims=True)\n      return tf.compat.v1.losses.compute_weighted_loss(\n          divergence, weights, scope, loss_collection, reduction=reduction)\n\n\ndef jensen_shannon_divergence(\n    labels,\n    predictions,\n    axis=None,\n    weights=1.0,\n    scope=None,\n    loss_collection=tf.compat.v1.GraphKeys.LOSSES,\n    reduction=tf.compat.v1.losses.Reduction.SUM_BY_NONZERO_WEIGHTS):\n  """"""Adds a Jensen-Shannon divergence to the training procedure.\n\n  For brevity, let `P = labels`, `Q = predictions`, `KL(P||Q)` be the\n  Kullback-Leibler divergence as defined in the description of the\n  `nsl.lib.kl_divergence` function."". The Jensen-Shannon divergence (JSD) is\n\n  ```\n  M = (P + Q) / 2\n  JSD(P||Q) = KL(P||M) / 2 + KL(Q||M) / 2\n  ```\n\n  This function assumes that `predictions` and `labels` are the values of a\n  multinomial distribution, i.e., each value is the probability of the\n  corresponding class.\n\n  For the usage of `weights` and `reduction`, please refer to `tf.losses`.\n\n  Args:\n    labels: `Tensor` of type `float32` or `float64`, with shape `[d1, ..., dN,\n      num_classes]`, represents the target distribution.\n    predictions: `Tensor` of the same type and shape as `labels`, represents\n      the predicted distribution.\n    axis: The dimension along which the Jensen-Shannon divergence is computed.\n      The values of `labels` and `predictions` along `axis` should meet the\n      requirements of a multinomial distribution.\n    weights: (optional) `Tensor` whose rank is either 0, or the same as that of\n      `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n      be either `1`, or the same as the corresponding `losses` dimension).\n    scope: The scope for the operations performed in computing the loss.\n    loss_collection: Collection to which the loss will be added.\n    reduction: Type of reduction to apply to the loss.\n\n  Returns:\n    Weighted loss `float` `Tensor`. If `reduction` is\n    `tf.compat.v1.losses.Reduction.MEAN`, this has the same shape as `labels`,\n    otherwise, it is a scalar.\n  Raises:\n    InvalidArgumentError: If `labels` or `predictions` don\'t meet the\n      requirements of a multinomial distribution.\n    ValueError: If `axis` is `None`, the shape of `predictions` doesn\'t match\n      that of `labels`, or if the shape of `weights` is invalid.\n  """"""\n  with tf.compat.v1.name_scope(scope, \'jensen_shannon_divergence\',\n                               (predictions, labels, weights)) as scope:\n    labels = tf.cast(labels, tf.dtypes.float32)\n    predictions = tf.cast(predictions, tf.dtypes.float32)\n    predictions.get_shape().assert_is_compatible_with(labels.get_shape())\n    if axis is None:\n      raise ValueError(\'You must specify ""axis"".\')\n    _assert_valid_axis(labels.get_shape().ndims, axis)\n    assert_list = _assert_multinomial_distribution(\n        labels, axis) + _assert_multinomial_distribution(predictions, axis)\n    with tf.control_dependencies(assert_list):\n      means = 0.5 * (labels + predictions)\n      divergence_tensor = 0.5 * _kl_divergence_fn(\n          labels, means) + 0.5 * _kl_divergence_fn(predictions, means)\n      divergence = tf.reduce_sum(\n          input_tensor=divergence_tensor, axis=(axis,), keepdims=True)\n      return tf.compat.v1.losses.compute_weighted_loss(\n          divergence, weights, scope, loss_collection, reduction=reduction)\n\n\ndef _apply_transform(batched_tensor, transform_type, axis=None):\n  """"""Applies the given transform function to `batched_tensor` along `axis`.""""""\n  if transform_type == configs.TransformType.SOFTMAX:\n    return tf.nn.softmax(batched_tensor, axis=axis)\n  else:\n    raise ValueError(\'Invalid TransformType %s.\' % transform_type)\n\n\ndef _select_distance_fn(key):\n  """"""Selects the distance function.""""""\n  if key == configs.DistanceType.L1:\n    return tf.compat.v1.losses.absolute_difference\n  elif key == configs.DistanceType.L2:\n    return tf.compat.v1.losses.mean_squared_error\n  elif key == configs.DistanceType.COSINE:\n    return tf.compat.v1.losses.cosine_distance\n  elif key == configs.DistanceType.JENSEN_SHANNON_DIVERGENCE:\n    return jensen_shannon_divergence\n  elif key == configs.DistanceType.KL_DIVERGENCE:\n    return kl_divergence\n  else:\n    raise ValueError(\'Invalid configs.DistanceType %s.\' % key)\n\n\ndef _is_axis_required_in_distance_fn(key):\n  return key in (configs.DistanceType.COSINE,\n                 configs.DistanceType.JENSEN_SHANNON_DIVERGENCE,\n                 configs.DistanceType.KL_DIVERGENCE)\n\n\ndef _is_reduced_by_average(key):\n  return key in (tf.compat.v1.losses.Reduction.MEAN,\n                 tf.compat.v1.losses.Reduction.SUM_BY_NONZERO_WEIGHTS,\n                 tf.compat.v1.losses.Reduction.SUM_OVER_BATCH_SIZE,\n                 tf.compat.v1.losses.Reduction.SUM_OVER_NONZERO_WEIGHTS)\n\n\ndef pairwise_distance_wrapper(sources,\n                              targets,\n                              weights=1.0,\n                              distance_config=None):\n  """"""A wrapper to compute the pairwise distance between `sources` and `targets`.\n\n  `distances = weights * distance_config.distance_type(sources, targets)`\n\n  This wrapper calculates the weighted distance between `(sources, targets)`\n  pairs, and provides an option to return the distance as the sum over the\n  difference along the given axis, when vector based distance is needed.\n\n  For the usage of `weights` and `reduction`, please refer to `tf.losses`. For\n  the usage of `sum_over_axis`, see the following examples:\n\n  Given target tensors with shape `[batch_size, features]`, the reduction set to\n  `tf.compat.v1.losses.Reduction.MEAN`, and `sum_over_axis` set to the last\n  dimension, the weighted average distance of sample pairs will be returned.\n  For example: With a distance_config(\'L2\', sum_over_axis=-1), the distance\n  between [[1, 1], [2, 2], [0, 2], [5, 5]] and [[1, 1], [0, 2], [4, 4], [1, 4]]\n  will be {(0+0) + (4+0) + (16+4) + (16+1)}/4 = 10.25\n\n  If `sum_over_axis` is `None`, the weighted average distance of feature pairs\n  (instead of sample pairs) will be returned. For example: With a\n  distance_config(\'L2\'), the distance between\n  [[1, 1], [2, 2], [0, 2], [5, 5]] and [[1, 1], [0, 2], [4, 4], [1, 4]] will be\n  {(0+0) + (4+0) + (16+4) + (16+1)}/8 = 5.125\n\n  If `transform_fn` is not `None`, the transform function is applied to both\n  `sources` and `targets` before computing the distance. For example:\n  `distance_config(\'KL_DIVERGENCE\', sum_over_axis=-1, transform_fn=\'SOFTMAX\')`\n  treats `sources` and `targets` as logits, and computes the KL-divergence\n  between the two probability distributions.\n\n  Args:\n    sources: `Tensor` of type `float32` or `float64`.\n    targets: `Tensor` of the same type and shape as `sources`.\n    weights: (optional) `Tensor` whose rank is either 0, or the same as that of\n      `targets`, and must be broadcastable to `targets` (i.e., all dimensions\n      must be either `1`, or the same as the corresponding distance dimension).\n    distance_config: An instance of `nsl.configs.DistanceConfig` that contains\n      the following configuration (or hyperparameters) for computing distances:\n      (a) `distance_type`: Type of distance function to apply.\n      (b) `reduction`: Type of distance reduction. See `tf.losses.Reduction`.\n      (c) `sum_over_axis`: (optional) The distance is the sum over the\n        difference along the specified axis. Note that if `sum_over_axis` is not\n        `None` and the rank of `weights` is non-zero, then the size of `weights`\n        along `sum_over_axis` must be 1.\n      (d) `transform_fn`: (optional) If set, both `sources` and `targets` will\n        be transformed before calculating the distance. If set to \'SOFTMAX\', it\n        will be performed on the axis specified by \'sum_over_axis\', or -1 if the\n        axis is not specified. If `None`, the default distance config will be\n        used.\n\n  Returns:\n    Weighted distance scalar `Tensor`. If `reduction` is\n      `tf.compat.v1.losses.Reduction.MEAN`, this has the same shape as\n      `targets`.\n  Raises:\n    ValueError: If the shape of targets doesn\'t match that of sources, or if the\n      shape of weights is invalid.\n    TypeError: If the distance function gets an unexpected keyword argument.\n  """"""\n  if distance_config is None:\n    distance_config = configs.DistanceConfig()  # Default configs.\n\n  tf.compat.v1.losses.Reduction.validate(distance_config.reduction)\n\n  if distance_config.transform_fn is not configs.TransformType.NONE:\n    sources = _apply_transform(sources, distance_config.transform_fn,\n                               distance_config.sum_over_axis)\n    targets = _apply_transform(targets, distance_config.transform_fn,\n                               distance_config.sum_over_axis)\n\n  sum_over_axis = distance_config.sum_over_axis\n  # Validates the `sum_over_axis`\n  _assert_valid_axis(sources.get_shape().ndims, sum_over_axis)\n  distance_fn = _select_distance_fn(distance_config.distance_type)\n  if distance_config.distance_type == configs.DistanceType.COSINE:\n    # Cosine distance function assumes input tensors have been unit-normalized\n    sources = tf.nn.l2_normalize(sources, axis=sum_over_axis)\n    targets = tf.nn.l2_normalize(targets, axis=sum_over_axis)\n  if _is_axis_required_in_distance_fn(distance_config.distance_type):\n    distances = distance_fn(\n        labels=sources,\n        predictions=targets,\n        weights=weights,\n        axis=sum_over_axis,\n        reduction=distance_config.reduction,\n        loss_collection=None)\n  else:\n    distances = distance_fn(\n        labels=sources,\n        predictions=targets,\n        weights=weights,\n        reduction=distance_config.reduction,\n        loss_collection=None)\n    if sum_over_axis is not None and _is_reduced_by_average(\n        distance_config.reduction):\n      # The distance is divided by the size of targets tensor, so we need to\n      # rescale the distance by multiplying the size of axis. Note, the distance\n      # function with `axis` as a required argument (e.g., consine distance)\n      # does not need to be rescaled.\n      weights = tf.convert_to_tensor(value=weights)\n      weights_shape = weights.get_shape().as_list()\n      if weights_shape and weights_shape[sum_over_axis] != 1:\n        raise ValueError(\'Shape of weights along the axis %d must be 1.\' %\n                         sum_over_axis)\n      distances *= sources.shape.dims[sum_over_axis].value\n  return distances\n'"
neural_structured_learning/lib/distances_test.py,47,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.lib.distances.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.lib import distances\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass DistancesTest(tf.test.TestCase):\n\n  def _kl_func(self, x, y):\n    eps = 1e-7\n    return x * np.log(x + eps) - x * np.log(y + eps)\n\n  def _jsd_func(self, x, y):\n    m = 0.5 * (x + y)\n    return 0.5 * self._kl_func(x, m) + 0.5 * self._kl_func(y, m)\n\n  def _softmax_func(self, x, axis=-1):\n    exp_x = np.exp(x)\n    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)\n    return exp_x / sum_exp_x\n\n  def testKLDivergence(self):\n    source_tensor = np.array([[1, 0, 0], [0.1, 0.2, 0.7]])\n    target_tensor = np.array([[1, 0, 0], [0.1, 0.9, 0]])\n\n    expected_tensor = np.sum(self._kl_func(source_tensor, target_tensor), -1)\n    expected_value = np.mean(expected_tensor)\n    distance_tensor = distances.kl_divergence(\n        source_tensor, target_tensor, axis=-1)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, expected_value)\n\n  def testJensenShannonDivergence(self):\n    source_tensor = np.array([[1, 0, 0], [0.1, 0.2, 0.7]])\n    target_tensor = np.array([[1, 0, 0], [0.1, 0.9, 0]])\n\n    expected_tensor = np.sum(self._jsd_func(source_tensor, target_tensor), -1)\n    expected_value = np.mean(expected_tensor)\n    distance_tensor = distances.jensen_shannon_divergence(\n        source_tensor, target_tensor, axis=-1)\n\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, expected_value)\n\n  def testInvalidMultinomialDistribution(self):\n    source_tensor = np.array([[1, 0, 0], [0.1, 0.2, 0.8]])\n    target_tensor = np.array([[1, 0, 0], [0.1, 0.9, 0]])\n    with self.cached_session():\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        distance_tensor = distances.kl_divergence(\n            source_tensor, target_tensor, axis=-1)\n        distance_tensor.eval()\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        distance_tensor = distances.jensen_shannon_divergence(\n            source_tensor, target_tensor, axis=-1)\n        distance_tensor.eval()\n\n  def testInvalidNegativeMultinomialDistribution(self):\n    source_tensor = np.array([[1, 0, 0], [-0.1, 0.3, 0.8]])\n    target_tensor = np.array([[1, 0, 0], [0.1, 0.9, 0]])\n    with self.cached_session():\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        distance_tensor = distances.kl_divergence(\n            source_tensor, target_tensor, axis=-1)\n        distance_tensor.eval()\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        distance_tensor = distances.jensen_shannon_divergence(\n            source_tensor, target_tensor, axis=-1)\n        distance_tensor.eval()\n\n  def testInvalidAxisForDivergence(self):\n    source_tensor = np.array([[1, 0, 0], [1, 0, 0]])\n    target_tensor = np.array([[1, 0, 0], [0, 1, 0]])\n    with self.cached_session():\n      with self.assertRaises(ValueError):\n        distance_tensor = distances.kl_divergence(\n            source_tensor, target_tensor, axis=2)\n        distance_tensor.eval()\n      with self.assertRaises(ValueError):\n        distance_tensor = distances.jensen_shannon_divergence(\n            source_tensor, target_tensor, axis=2)\n        distance_tensor.eval()\n\n  def testL1Distance(self):\n    source_tensor = tf.constant([[1, 1], [2, 2], [0, 2], [5, 5]],\n                                dtype=\'float32\')\n    target_tensor = tf.constant([[1, 1], [0, 2], [4, 4], [1, 4]],\n                                dtype=\'float32\')\n    distance_config = configs.DistanceConfig(\'l1\', sum_over_axis=-1)\n    distance_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, distance_config=distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, 3.25)\n\n  def testL2Distance(self):\n    source_tensor = tf.constant([[1, 1], [2, 2], [0, 2], [5, 5]],\n                                dtype=\'float32\')\n    target_tensor = tf.constant([[1, 1], [0, 2], [4, 4], [1, 4]],\n                                dtype=\'float32\')\n    distance_config = configs.DistanceConfig(\'l2\', sum_over_axis=-1)\n    distance_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, distance_config=distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, 10.25)\n\n  def testCosineDistance(self):\n    source_tensor = tf.constant([[1, 1], [1, 1], [3, 4], [-1, -1]],\n                                dtype=\'float32\')\n    target_tensor = tf.constant([[1, 1], [5, 5], [4, 3], [1, 1]],\n                                dtype=\'float32\')\n    distance_config = configs.DistanceConfig(\'cosine\', sum_over_axis=-1)\n    distance_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, distance_config=distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value,\n                          0.51)  # sum([0.0, 1.0, 0.04, 2.0]) / 4\n\n  def testJensenShannonDistance(self):\n    source_tensor = np.array([[1, 0, 0], [0.1, 0.2, 0.7]], dtype=\'float32\')\n    target_tensor = np.array([[1, 0, 0], [0.1, 0.9, 0]], dtype=\'float32\')\n    expected_tensor = np.sum(self._jsd_func(source_tensor, target_tensor), -1)\n    expected_value = np.mean(expected_tensor)\n    distance_config = configs.DistanceConfig(\n        \'jensen_shannon_divergence\', sum_over_axis=-1)\n    distance_tensor = distances.pairwise_distance_wrapper(\n        tf.constant(source_tensor),\n        tf.constant(target_tensor),\n        distance_config=distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, expected_value)\n\n  def testJensenShannonDistanceFromLogit(self):\n    source = np.array([[1, 2, 3], [1, -1, 2]], dtype=\'float32\')\n    target = np.array([[1, 2, 3], [1, 0, -1]], dtype=\'float32\')\n\n    expected_value = np.mean(\n        np.sum(\n            self._jsd_func(\n                self._softmax_func(source), self._softmax_func(target)), -1))\n\n    distance_config = configs.DistanceConfig(\n        \'jensen_shannon_divergence\', transform_fn=\'softmax\', sum_over_axis=-1)\n    distance_tensor = distances.pairwise_distance_wrapper(\n        tf.constant(source),\n        tf.constant(target),\n        distance_config=distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, expected_value)\n\n  def testKLDistance(self):\n    source_tensor = np.array([[1, 0, 0], [0.1, 0.2, 0.7]], dtype=\'float32\')\n    target_tensor = np.array([[1, 0, 0], [0.1, 0.9, 0]], dtype=\'float32\')\n\n    expected_value = np.mean(\n        np.sum(self._kl_func(source_tensor, target_tensor), -1))\n\n    distance_config = configs.DistanceConfig(\'kl_divergence\', sum_over_axis=-1)\n    distance_tensor = distances.pairwise_distance_wrapper(\n        tf.constant(source_tensor),\n        tf.constant(target_tensor),\n        distance_config=distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, expected_value)\n\n  def testKLDistanceFromLogit(self):\n    source = np.array([[1, 2, 3], [1, -1, 2]], dtype=\'float32\')\n    target = np.array([[1, 2, 3], [1, 0, -1]], dtype=\'float32\')\n\n    expected_value = np.mean(\n        np.sum(\n            self._kl_func(\n                self._softmax_func(source), self._softmax_func(target)), -1))\n\n    distance_config = configs.DistanceConfig(\n        \'kl_divergence\', transform_fn=\'softmax\', sum_over_axis=-1)\n    distance_tensor = distances.pairwise_distance_wrapper(\n        tf.constant(source),\n        tf.constant(target),\n        distance_config=distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, expected_value)\n\n  def testWeightedDistance(self):\n    source_tensor = tf.constant([[1, 1], [2, 2], [0, 2], [5, 5]],\n                                dtype=\'float32\')\n    target_tensor = tf.constant([[1, 1], [0, 2], [4, 4], [1, 4]],\n                                dtype=\'float32\')\n    weights = tf.constant([[1], [0], [0.5], [0.5]], dtype=\'float32\')\n\n    l1_distance_config = configs.DistanceConfig(\'l1\', sum_over_axis=-1)\n    l1_distance_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, weights, l1_distance_config)\n    l2_distance_config = configs.DistanceConfig(\'l2\', sum_over_axis=-1)\n    l2_distance_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, weights, l2_distance_config)\n    with self.cached_session() as sess:\n      l1_distance_value = sess.run(l1_distance_tensor)\n      self.assertAllClose(l1_distance_value, 5.5 / 3)\n      l2_distance_value = sess.run(l2_distance_tensor)\n      self.assertAllClose(l2_distance_value, 18.5 / 3)\n\n  def testDistanceReductionNone(self):\n    source_tensor = tf.constant([[1, 1], [2, 2], [0, 2], [5, 5]],\n                                dtype=\'float32\')\n    target_tensor = tf.constant([[1, 1], [0, 2], [4, 4], [1, 4]],\n                                dtype=\'float32\')\n    weights = tf.constant([[1], [0], [0.5], [0.5]], dtype=\'float32\')\n\n    distance_config = configs.DistanceConfig(\n        \'l1\', tf.compat.v1.losses.Reduction.NONE, sum_over_axis=-1)\n    distance_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, weights, distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value,\n                          [[0.0, 0.0], [0.0, 0.0], [2.0, 1.0], [2.0, 0.5]])\n\n  def testDistanceReductionSum(self):\n    source_tensor = tf.constant([[1, 1], [2, 2], [0, 2], [5, 5]],\n                                dtype=\'float32\')\n    target_tensor = tf.constant([[1, 1], [0, 2], [4, 4], [1, 4]],\n                                dtype=\'float32\')\n    weights = tf.constant([[1], [0], [0.5], [0.5]], dtype=\'float32\')\n\n    distance_sum_config = configs.DistanceConfig(\n        \'l1\', tf.compat.v1.losses.Reduction.SUM, sum_over_axis=-1)\n    distance_sum_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, weights, distance_sum_config)\n    with self.cached_session() as sess:\n      distance_sum_value = sess.run(distance_sum_tensor)\n      self.assertAllClose(distance_sum_value, 5.5)\n\n  def testDistanceReductionMean(self):\n    source_tensor = tf.constant([[1, 1], [2, 2], [0, 2], [5, 5]],\n                                dtype=\'float32\')\n    target_tensor = tf.constant([[1, 1], [0, 2], [4, 4], [1, 4]],\n                                dtype=\'float32\')\n    weights = tf.constant([[1], [0], [0.5], [0.5]], dtype=\'float32\')\n\n    distance_mean_config = configs.DistanceConfig(\n        \'l1\', tf.compat.v1.losses.Reduction.MEAN, sum_over_axis=-1)\n    distance_mean_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, weights, distance_mean_config)\n    with self.cached_session() as sess:\n      distance_mean_value = sess.run(distance_mean_tensor)\n      self.assertAllClose(distance_mean_value, 5.5 / 2.0)\n\n  def testDistanceWithoutSumOverAxis(self):\n    source_tensor = tf.constant([[1, 1], [2, 2], [0, 2], [5, 5]],\n                                dtype=\'float32\')\n    target_tensor = tf.constant([[1, 1], [0, 2], [4, 4], [1, 4]],\n                                dtype=\'float32\')\n    weights = tf.constant([[1], [0], [0.5], [0.5]], dtype=\'float32\')\n\n    distance_config = configs.DistanceConfig(\'l1\')\n    distance_tensor = distances.pairwise_distance_wrapper(\n        source_tensor, target_tensor, weights, distance_config)\n    with self.cached_session() as sess:\n      distance_value = sess.run(distance_tensor)\n      self.assertAllClose(distance_value, 5.5 / 6)\n\n  def testDistanceWithTransformButNoSumOverAxis(self):\n    source = np.array([[1, 1], [2, 2], [0, 2], [10, -10]], dtype=\'float32\')\n    target = np.array([[0, 0], [0, 2], [1, 3], [3, 3]], dtype=\'float32\')\n\n    distance_config = configs.DistanceConfig(\n        distance_type=\'l1\',\n        reduction=tf.compat.v1.losses.Reduction.NONE,\n        transform_fn=\'softmax\')\n    distance_tensor = distances.pairwise_distance_wrapper(\n        tf.constant(source),\n        tf.constant(target),\n        distance_config=distance_config)\n\n    expected_distance = np.abs(\n        self._softmax_func(source) - self._softmax_func(target))\n    with self.cached_session() as sess:\n      distance = sess.run(distance_tensor)\n      self.assertAllClose(distance, expected_distance)\n\n  def testDistanceInvalidWeightShapeAlongAxis(self):\n    source_tensor = tf.constant(1.0, dtype=\'float32\', shape=[4, 2])\n    target_tensor = tf.constant(1.0, dtype=\'float32\', shape=[4, 2])\n    weights = tf.constant(1.0, dtype=\'float32\', shape=[4, 2])\n\n    distance_config = configs.DistanceConfig(sum_over_axis=-1)\n    with self.assertRaises(ValueError):\n      distance_tensor = distances.pairwise_distance_wrapper(\n          source_tensor, target_tensor, weights, distance_config)\n      distance_tensor.eval()\n\n  def testDistanceInvalidAxis(self):\n    source_tensor = tf.constant(1.0, dtype=\'float32\', shape=[4, 2])\n    target_tensor = tf.constant(1.0, dtype=\'float32\', shape=[4, 2])\n    weights = tf.constant(1.0, dtype=\'float32\', shape=[4, 2])\n\n    distance_config = configs.DistanceConfig(sum_over_axis=2)\n    with self.assertRaises(ValueError):\n      distance_tensor = distances.pairwise_distance_wrapper(\n          source_tensor, target_tensor, weights, distance_config)\n      distance_tensor.eval()\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/lib/regularizer.py,6,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Provides functional interface(s) to generate regularizer(s).""""""\nfrom neural_structured_learning.lib import distances\nfrom neural_structured_learning.lib import utils\n\nimport tensorflow as tf\n\n\ndef adv_regularizer(adv_neighbors, target_scores, model_fn, loss_fn):\n  """"""Calculates adversarial loss from generated adversarial samples.\n\n  Args:\n    adv_neighbors: dense `float32` tensor, with two possible shapes: (a)\n      `[batch_size, feat_len]` for pointwise samples, or (b)\n      `[batch_size, seq_len, feat_len]` for sequence samples.\n    target_scores: target tensor used to compute loss.\n    model_fn: a method that has input tensor (same shape as `adv_neighbors`),\n      `is_train` and `reuse` as inputs, and returns predicted logits.\n    loss_fn: a loss function that has `target` and `prediction` as inputs, and\n      returns a `float32` scalar.\n\n  Returns:\n    adv_loss: a `float32` denoting the adversarial loss.\n  """"""\n  adv_predictions = model_fn(\n      adv_neighbors, is_train=tf.constant(False), reuse=True)\n  adv_loss = loss_fn(target_scores, adv_predictions)\n  tf.compat.v1.summary.scalar(\'adv_loss\', adv_loss)\n  return adv_loss\n\n\ndef _virtual_adv_regularizer(input_layer, embedding_fn, virtual_adv_config,\n                             embedding, seed_perturbation):\n  """"""Function to calculate virtual adversarial loss without randomness.""""""\n  neighbor_config = virtual_adv_config.adv_neighbor_config\n\n  def normalize_with_mask(perturbation):\n    perturbation = utils.apply_feature_mask(perturbation,\n                                            neighbor_config.feature_mask)\n    return utils.normalize(perturbation, neighbor_config.adv_grad_norm)\n\n  def loss_fn(embedding, perturbed_embedding):\n    return distances.pairwise_distance_wrapper(\n        sources=embedding,\n        targets=perturbed_embedding,\n        distance_config=virtual_adv_config.distance_config)\n\n  perturbation = normalize_with_mask(seed_perturbation)\n\n  # Uses the power iteration method and the finite difference method to\n  # approximate the direction which increases virtual adversarial loss the most.\n  for _ in range(virtual_adv_config.num_approx_steps):\n    with tf.GradientTape() as tape:\n      scaled_perturbation = virtual_adv_config.approx_difference * perturbation\n      tape.watch(scaled_perturbation)\n      virtual_adv_embedding = embedding_fn(input_layer + scaled_perturbation)\n      virtual_adv_loss = loss_fn(embedding, virtual_adv_embedding)\n    grad = tape.gradient(virtual_adv_loss, scaled_perturbation)\n    perturbation = tf.stop_gradient(normalize_with_mask(grad))\n\n  final_perturbation = neighbor_config.adv_step_size * perturbation\n  virtual_adv_embedding = embedding_fn(input_layer + final_perturbation)\n  # The gradient shouldn\'t be populated through the original embedding because\n  # our goal is to drag the embedding of the virtual adversarial example to be\n  # as close as that of the original example, but not the other way around.\n  original_embedding = tf.stop_gradient(embedding)\n  return loss_fn(original_embedding, virtual_adv_embedding)\n\n\ndef virtual_adv_regularizer(input_layer,\n                            embedding_fn,\n                            virtual_adv_config,\n                            embedding=None):\n  """"""Calculates virtual adversarial loss for the given input.\n\n  Virtual adversarial loss is defined as the distance between the embedding of\n  the input and that of a slightly perturbed input. Optimizing this loss helps\n  smooth models locally.\n\n  Reference paper: https://arxiv.org/pdf/1704.03976.pdf\n\n  Args:\n    input_layer: a dense tensor for input features whose first dimension is the\n      training batch size.\n    embedding_fn: a unary function that computes the embedding for the given\n      `input_layer` input.\n    virtual_adv_config: an `nsl.configs.VirtualAdvConfig` object that specifies\n      parameters for generating adversarial examples and computing the\n      adversarial loss.\n    embedding: (optional) a dense tensor representing the embedding of\n      `input_layer`. If not provided, it will be calculated as\n      `embedding_fn(input_layer)`.\n\n  Returns:\n    virtual_adv_loss: a `float32` denoting the virtural adversarial loss.\n  """"""\n\n  if embedding is None:\n    embedding = embedding_fn(input_layer)\n\n  seed_perturbation = tf.random.normal(tf.shape(input=input_layer))\n  return _virtual_adv_regularizer(input_layer, embedding_fn, virtual_adv_config,\n                                  embedding, seed_perturbation)\n'"
neural_structured_learning/lib/regularizer_test.py,12,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for neural_structured_learning.lib.regularizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.lib import regularizer\nimport numpy as np\nimport tensorflow as tf\n\n\ndef mock_model_fn(adv_input, is_train, reuse):\n  """"""Mock model_fn that returns input directly.""""""\n  del is_train, reuse\n  return adv_input\n\n\ndef mock_loss_fn(onehot_labels, logits):\n  """"""Mock loss_fn that returns MSE loss.""""""\n  return tf.keras.losses.mean_squared_error(onehot_labels, logits)\n\n\nclass RegularizerTest(tf.test.TestCase):\n  """"""Tests regularizer methods.""""""\n\n  def testAdvRegularizer(self):\n    """"""Tests adv_regularizer returns expected adv_loss.""""""\n    adv_neighbor = np.array([1., 1., 1., 0., 1.])\n    target = np.array([1., 1., 1., 1., 1.])\n\n    adv_loss = regularizer.adv_regularizer(\n        tf.constant(adv_neighbor), tf.constant(target), mock_model_fn,\n        mock_loss_fn)\n    actual_loss = self.evaluate(adv_loss)\n    self.assertNear(\n        actual_loss,\n        np.sum((adv_neighbor - target)**2.0) / len(target), 1e-5)\n\n  def testVirtualAdvRegularizer(self):\n    """"""Tests virtual_adv_regularizer returning expected loss.""""""\n    np_input = np.array([[1.0, -1.0]])\n    tf_input = tf.constant(np_input)\n    np_weights = np.array([[1.0, 5.0], [2.0, 2.0]])\n    tf_weights = tf.constant(np_weights)\n    # Linear transformation and L2 loss makes the Hessian matrix constant.\n    embedding_fn = lambda x: tf.matmul(x, tf_weights)\n    step_size = 0.1\n    vadv_config = configs.VirtualAdvConfig(\n        adv_neighbor_config=configs.AdvNeighborConfig(\n            feature_mask=None,\n            adv_step_size=step_size,\n            adv_grad_norm=configs.NormType.L2),\n        distance_config=configs.DistanceConfig(\n            distance_type=configs.DistanceType.L2, sum_over_axis=-1),\n        num_approx_steps=1,\n        approx_difference=1e-3)  # enlarged for numerical stability\n    np_seed = np.array([[0.6, 0.8]])\n    tf_seed = tf.constant(np_seed)\n    vadv_loss = regularizer._virtual_adv_regularizer(tf_input, embedding_fn,\n                                                     vadv_config,\n                                                     embedding_fn(tf_input),\n                                                     tf_seed)\n\n    actual_loss = self.evaluate(vadv_loss)\n\n    hessian = 2 * np.dot(np_weights, np_weights.T)\n    approx = np.matmul(np_seed, hessian)\n    approx *= step_size / np.linalg.norm(approx, axis=-1, keepdims=True)\n    expected_loss = np.linalg.norm(np.matmul(approx, np_weights))**2\n    self.assertNear(actual_loss, expected_loss, err=1e-5)\n\n  def testVirtualAdvRegularizerMultiStepApproximation(self):\n    """"""Tests virtual_adv_regularizer with multi-step approximation.""""""\n    np_input = np.array([[0.28, -0.96]])\n    tf_input = tf.constant(np_input)\n    embedding_fn = lambda x: x\n    vadv_config = configs.VirtualAdvConfig(\n        adv_neighbor_config=configs.AdvNeighborConfig(\n            feature_mask=None,\n            adv_step_size=1,\n            adv_grad_norm=configs.NormType.L2),\n        distance_config=configs.DistanceConfig(\n            distance_type=configs.DistanceType.COSINE, sum_over_axis=-1),\n        num_approx_steps=20,\n        approx_difference=1)\n    np_seed = np.array([[0.6, 0.8]])\n    tf_seed = tf.constant(np_seed)\n    vadv_loss = regularizer._virtual_adv_regularizer(tf_input, embedding_fn,\n                                                     vadv_config,\n                                                     embedding_fn(tf_input),\n                                                     tf_seed)\n\n    actual_loss = self.evaluate(vadv_loss)\n\n    x = np_input\n    hessian = np.dot(x, x.T) * np.identity(2) - np.dot(x.T, x)\n    hessian /= np.linalg.norm(x)**4\n    approx = np.matmul(np_seed, hessian)\n    approx /= np.linalg.norm(approx, axis=-1, keepdims=True)\n    expected_loss = np.matmul(np.matmul(approx, hessian), np.transpose(approx))\n    self.assertNear(actual_loss, expected_loss, err=1e-5)\n\n  def testVirtualAdvRegularizerRandomPerturbation(self):\n    """"""Tests virtual_adv_regularizer with num_approx_steps=0.""""""\n    input_layer = tf.constant([[1.0, -1.0]])\n    embedding_fn = lambda x: x\n    step_size = 0.1\n    vadv_config = configs.VirtualAdvConfig(\n        adv_neighbor_config=configs.AdvNeighborConfig(\n            feature_mask=None,\n            adv_step_size=step_size,\n            adv_grad_norm=configs.NormType.L2),\n        distance_config=configs.DistanceConfig(\n            distance_type=configs.DistanceType.L2, sum_over_axis=-1),\n        num_approx_steps=0)\n    vadv_loss = regularizer.virtual_adv_regularizer(input_layer, embedding_fn,\n                                                    vadv_config)\n    actual_loss = self.evaluate(vadv_loss)\n\n    # The identity embedding_fn makes the virtual adversarial loss immune to the\n    # direction of the perturbation, only the size matters.\n    expected_loss = step_size**2  # square loss\n    self.assertNear(actual_loss, expected_loss, err=1e-5)\n\n  def testVirtualAdvRegularizerDefaultConfig(self):\n    """"""Tests virtual_adv_regularizer with default config.""""""\n    input_layer = tf.constant([[1.0, -1.0]])\n    embedding_fn = lambda x: x\n    vadv_config = configs.VirtualAdvConfig()\n    vadv_loss = regularizer.virtual_adv_regularizer(input_layer, embedding_fn,\n                                                    vadv_config)\n    actual_loss = self.evaluate(vadv_loss)\n    self.assertAllGreaterEqual(actual_loss, 0.0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/lib/utils.py,86,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions for Neural Structured Learning.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport neural_structured_learning.configs as configs\nimport six\nimport tensorflow as tf\n\n\ndef normalize(tensor, norm_type, epsilon=1e-6):\n  """"""Normalizes the values in `tensor` with respect to a specified vector norm.\n\n  This op assumes that the first axis of `tensor` is the batch dimension, and\n  calculates the norm over all other axes. For example, if `tensor` is\n  `tf.constant(1.0, shape=[2, 3, 4])`, its L2 norm (calculated along all the\n  dimensions other than the first dimension) will be `[[sqrt(12)], [sqrt(12)]]`.\n  Hence, this tensor will be normalized by dividing by\n  `[[sqrt(12)], [sqrt(12)]]`.\n\n  Note that `tf.norm` is not used here since it only allows the norm to be\n  calculated over one axis, not multiple axes.\n\n  Args:\n    tensor: a tensor to be normalized. Can have any shape with the first axis\n      being the batch dimension that will not be normalized across.\n    norm_type: one of `nsl.configs.NormType`, the type of vector norm.\n    epsilon: a lower bound value for the norm to avoid division by 0.\n\n  Returns:\n    A normalized tensor with the same shape and type as `tensor`.\n  """"""\n  if isinstance(norm_type, str):  # Allows string to be converted into NormType.\n    norm_type = configs.NormType(norm_type)\n\n  target_axes = list(range(1, len(tensor.get_shape())))\n  if norm_type == configs.NormType.INFINITY:\n    norm = tf.reduce_max(\n        input_tensor=tf.abs(tensor), axis=target_axes, keepdims=True)\n    norm = tf.maximum(norm, epsilon)\n    normalized_tensor = tensor / norm\n  elif norm_type == configs.NormType.L1:\n    norm = tf.reduce_sum(\n        input_tensor=tf.abs(tensor), axis=target_axes, keepdims=True)\n    norm = tf.maximum(norm, epsilon)\n    normalized_tensor = tensor / norm\n  elif norm_type == configs.NormType.L2:\n    normalized_tensor = tf.nn.l2_normalize(\n        tensor, axis=target_axes, epsilon=epsilon**2)\n  else:\n    raise NotImplementedError(\'Unrecognized or unimplemented ""norm_type"": %s\' %\n                              norm_type)\n  return normalized_tensor\n\n\ndef _expand_to_rank(vector, rank):\n  """"""Expands a batched scalar to a tensor of certain rank.""""""\n  return tf.reshape(vector, shape=[-1] + [1] * (rank - 1))\n\n\ndef project_to_ball(t_dict, radius, norm_type, epsilon=1e-6):\n  """"""Projects a tensor to the epsilon ball in the given norm.\n\n  Only L-infinity and L2 norms are currently supported.\n\n  Args:\n    t_dict: A dictionary of tensors to project to the epsilon ball. The first\n      dimension of each tensor (the batch_size) must all be equal.\n    radius: the radius of the ball.\n    norm_type: One of `nsl.configs.NormType`. Currently L1 norm is not\n      supported.\n    epsilon: Used to avoid division by 0.\n\n  Returns:\n    A dictionary of tensors projected to the epsilon ball.\n  """"""\n  if norm_type not in {configs.NormType.INFINITY, configs.NormType.L2}:\n    raise NotImplementedError(\'Only L2 and L-infinity norms are implemented.\')\n  if norm_type == configs.NormType.INFINITY:\n    for key, tensor in t_dict.items():\n      t_dict[key] = tf.clip_by_value(\n          tensor, clip_value_min=-radius, clip_value_max=radius)\n    return t_dict\n  if norm_type == configs.NormType.L2:\n\n    def squared_global_norm(tensor):\n      """"""Calculate squared sum of elements for a tensor.""""""\n      target_axes = list(range(1, len(tensor.get_shape())))\n      return tf.reduce_sum(input_tensor=tf.square(tensor), axis=target_axes)\n\n    tensors = tf.nest.flatten(t_dict)\n    norms = tf.nest.map_structure(squared_global_norm, tensors)\n    global_norm = tf.sqrt(tf.maximum(tf.add_n(norms), epsilon**2))\n    scale = tf.where(global_norm <= radius,\n                     tf.ones_like(global_norm),\n                     radius / global_norm)\n\n    def clip_to_norm(tensor):\n      """"""For each sample, clip the tensor to the ball if necessary.""""""\n      shaped_scale = _expand_to_rank(scale, len(tensor.get_shape()))\n      return shaped_scale * tensor\n\n    return tf.nest.pack_sequence_as(\n        t_dict, tf.nest.map_structure(clip_to_norm, tensors))\n\n\ndef maximize_within_unit_norm(weights, norm_type, epsilon=1e-6):\n  """"""Solves the maximization problem weights^T*x with the constraint norm(x)=1.\n\n  This op solves a batch of maximization problems at one time. The first axis of\n  `weights` is assumed to be the batch dimension, and each ""row"" is treated as\n  an independent maximization problem.\n\n  This op is mainly used to generate adversarial examples (e.g., FGSM proposed\n  by Goodfellow et al.). Specifically, the `weights` are gradients, and `x` is\n  the adversarial perturbation. The desired perturbation is the one causing the\n  largest loss increase. In this op, the loss increase is approximated by the\n  dot product between the gradient and the perturbation, as in the first-order\n  Taylor approximation of the loss function.\n\n  Args:\n    weights: A `Tensor` or a collection of `Tensor` objects representing a batch\n      of weights to define the maximization objective. If this is a collection,\n      the first dimension of all `Tensor` objects should be the same (i.e. batch\n      size).\n    norm_type: One of `nsl.configs.NormType`, the type of the norm in the\n      constraint.\n    epsilon: A lower bound value for the norm to avoid division by 0.\n\n  Returns:\n    A `Tensor` or a collection of `Tensor` objects (with the same structure and\n    shape as `weights`) representing a batch of adversarial perturbations as the\n    solution to the maximization problems.\n  """"""\n  if isinstance(norm_type, six.string_types):\n    # Allows string to be converted into NormType.\n    norm_type = configs.NormType(norm_type)\n\n  if norm_type == configs.NormType.INFINITY:\n    return tf.nest.map_structure(tf.sign, weights)\n\n  tensors = tf.nest.flatten(weights)\n  tensor_ranks = [t.shape.rank for t in tensors]\n\n  if not tensors:  # `weights` is an empty collection.\n    return weights\n\n  def reduce_across_tensors(reduce_fn, input_tensors):\n    reduced_within_tensor = [\n        reduce_fn(t, axis=list(range(1, rank)))\n        for t, rank in zip(input_tensors, tensor_ranks)\n    ]\n    if len(input_tensors) == 1:\n      return reduced_within_tensor[0]\n    return reduce_fn(tf.stack(reduced_within_tensor, axis=-1), axis=-1)\n\n  if norm_type == configs.NormType.L2:\n    squared_norm = reduce_across_tensors(tf.reduce_sum,\n                                         [tf.square(t) for t in tensors])\n    inv_global_norm = tf.math.rsqrt(tf.maximum(squared_norm, epsilon**2))\n    normalized_tensors = [\n        tensor * _expand_to_rank(inv_global_norm, rank)\n        for tensor, rank in zip(tensors, tensor_ranks)\n    ]\n    return tf.nest.pack_sequence_as(weights, normalized_tensors)\n  elif norm_type == configs.NormType.L1:\n    # For L1 norm, the solution is to put 1 or -1 at a dimension with maximum\n    # absolute value, and 0 at others. In case of multiple dimensions having the\n    # same maximum absolute value, any distribution among them will do. Here we\n    # choose to distribute evenly among those dimensions for efficient\n    # implementation.\n    abs_tensors = [tf.abs(t) for t in tensors]\n    max_elem = reduce_across_tensors(tf.reduce_max, abs_tensors)\n    is_max_elem = [\n        tf.cast(tf.equal(t, _expand_to_rank(max_elem, rank)), t.dtype)\n        for t, rank in zip(abs_tensors, tensor_ranks)\n    ]\n    num_nonzero = reduce_across_tensors(tf.reduce_sum, is_max_elem)\n    denominator = tf.maximum(num_nonzero, epsilon)\n    mask = [\n        is_max * tf.sign(t) / _expand_to_rank(denominator, rank)\n        for t, rank, is_max in zip(tensors, tensor_ranks, is_max_elem)\n    ]\n    return tf.nest.pack_sequence_as(weights, mask)\n\n  raise NotImplementedError(\'Unrecognized or unimplemented ""norm_type"": %s\' %\n                            norm_type)\n\n\ndef get_target_indices(logits, labels, adv_target_config):\n  """"""Selects targeting classes for adversarial attack (classification only).\n\n  Args:\n    logits: tensor of shape `[batch_size, num_classes]` and dtype=`tf.float32`.\n    labels: `int` tensor with a shape of `[batch_size]` containing the ground\n      truth labels.\n    adv_target_config: instance of `nsl.configs.AdvTargetConfig` specifying the\n      adversarial target configuration.\n\n  Returns:\n    Tensor of shape `[batch_size]` and dtype=`tf.int32` of indices of targets.\n  """"""\n  num_classes = tf.shape(input=logits)[-1]\n  if adv_target_config.target_method == configs.AdvTargetType.SECOND:\n    _, top2_indices = tf.nn.top_k(logits, k=2)\n    indices = tf.reshape(top2_indices[:, 1], [-1])\n  elif adv_target_config.target_method == configs.AdvTargetType.LEAST:\n    indices = tf.argmin(input=logits, axis=-1, output_type=tf.dtypes.int32)\n  elif adv_target_config.target_method == configs.AdvTargetType.RANDOM:\n    batch_size = tf.shape(input=logits)[0]\n    indices = tf.random.uniform([batch_size],\n                                maxval=num_classes,\n                                dtype=tf.dtypes.int32,\n                                seed=adv_target_config.random_seed)\n  elif adv_target_config.target_method == configs.AdvTargetType.GROUND_TRUTH:\n    indices = labels\n  else:\n    raise NotImplementedError(\'Unrecognized or unimplemented ""target_method""\')\n  return indices\n\n\ndef _replicate_index(index_array, replicate_times):\n  """"""Replicates index in `index_array` by the values in `replicate_times`.""""""\n  batch_size = tf.shape(input=replicate_times)[0]\n  replicated_idx_array = tf.TensorArray(\n      dtype=tf.dtypes.int32, size=batch_size, infer_shape=False)\n  init_iter = tf.constant(0)\n\n  index_less_than_batch_size = lambda i, *unused_args: i < batch_size\n\n  def duplicate_index(i, outputs):\n    """"""Duplicates the current index by the value in the replicate_times.""""""\n    outputs = outputs.write(i, tf.tile([index_array[i]], [replicate_times[i]]))\n    return i + 1, outputs\n\n  # Replicate the indices by the number of times indicated in \'replicate_times\'.\n  # For example, given `index_array = [0, 1, 2]`, `replicate_times = [3, 0, 1]`,\n  # the `replicated_idx_array`  will be `[[0, 0, 0], [2]]`.\n  unused_iter, replicated_idx_array = tf.while_loop(\n      cond=index_less_than_batch_size,\n      body=duplicate_index,\n      loop_vars=[init_iter, replicated_idx_array])\n  # Concats \'replicated_idx_array\' as a single tensor, which can be  used for\n  # duplicating the input embeddings by \'embedding_lookup\'.\n  replicated_idx = tf.reshape(replicated_idx_array.concat(), shape=[-1])\n  return replicated_idx\n\n\ndef replicate_embeddings(embeddings, replicate_times):\n  """"""Replicates the given `embeddings` by `replicate_times`.\n\n  This function is useful when comparing the same instance with multiple other\n  instances. For example, given a seed and its neighbors, this function can be\n  used to replicate the embeddings of the seed by the number of its neighbors,\n  such that the distances between the seed and its neighbors can be computed\n  efficiently.\n\n  The `replicate_times` argument is either a scalar, or a 1-D tensor.\n  For example, if\n\n  ```\n  embeddings = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n  ```\n\n  then we would have the following results for different `replicate_times`\n  arguments:\n\n  ```\n  replicate_times = 2\n  result = [[0, 1, 2], [0, 1, 2], [3, 4, 5], [3, 4, 5], [6, 7, 8], [6, 7, 8]]\n  ```\n\n  and\n\n  ```\n  replicate_times = [3, 0, 1]\n  result = [[0, 1, 2], [0, 1, 2], [0, 1, 2], [6, 7, 8]]\n  ```\n\n  Args:\n    embeddings: A Tensor of shape `[batch_size, d1, ..., dN]`.\n    replicate_times: An integer scalar or an integer 1-D Tensor of shape `[batch\n      size]`. Each element indicates the number of times the corresponding row\n      in `embeddings` should be replicated.\n\n  Returns:\n    A Tensor of shape `[N, d1, ..., dN]`, where `N` is the sum of all elements\n      in `replicate_times`.\n\n  Raises:\n    InvalidArgumentError: If any value in `replicate_times` is negative.\n    TypeError: If `replicate_times` contains any value that cannot be cast to\n      the `int32` type.\n  """"""\n  with tf.control_dependencies(\n      [tf.debugging.assert_greater_equal(replicate_times, 0)]):\n    replicate_times = tf.cast(replicate_times, tf.dtypes.int32)\n    batch_size = tf.shape(input=embeddings)[0]\n    idx_array = tf.range(batch_size, dtype=\'int32\')\n    if replicate_times.get_shape().ndims == 0:\n      lookup_idx = tf.tile(tf.expand_dims(idx_array, -1), [1, replicate_times])\n      lookup_idx = tf.reshape(lookup_idx, [batch_size * replicate_times])\n    else:\n      lookup_idx = _replicate_index(idx_array, replicate_times)\n    output_embeddings = tf.gather(embeddings, lookup_idx)\n    return output_embeddings\n\n\ndef _select_decay_fn(key):\n  if key == configs.DecayType.EXPONENTIAL_DECAY:\n    return tf.compat.v1.train.exponential_decay\n  elif key == configs.DecayType.INVERSE_TIME_DECAY:\n    return tf.compat.v1.train.inverse_time_decay\n  elif key == configs.DecayType.NATURAL_EXP_DECAY:\n    return tf.compat.v1.train.natural_exp_decay\n  else:\n    raise ValueError(\'Invalid configs.DecayType %s.\' % key)\n\n\ndef decay_over_time(global_step, decay_config, init_value=1.0):\n  r""""""Returns a decayed value of `init_value` over time.\n\n  When training a model with a regularizer, the objective function can be\n  formulated as the following:\n  $$objective = \\lambda_1 * loss + \\lambda_2 * regularization$$\n\n  This function can be used for three cases:\n\n  1. Incrementally diminishing the importance of the loss term, by applying a\n     decay function to the $$\\lambda_1$$ over time. We\'ll denote this by writing\n     $$\\lambda_1$$ = decay_over_time(`init_value`).\n  2. Incrementally increasing the importance of the regularization term, by\n     setting $$\\lambda_2$$ = `init_value` - decay_over_time(`init_value`).\n  3. Combining the above two cases, namely, setting $$\\lambda_1$$ =\n     decay_over_time(`init_value`) and $$\\lambda_2$$ = `init_value` -\n     decay_over_time(`init_value`).\n\n  This function requires a `global_step` value to compute the decayed value.\n\n  Args:\n    global_step: A scalar `int32` or `int64` Tensor or a Python number. Must be\n      positive.\n    decay_config: A `nsl.configs.DecayConfig` for computing the decay value.\n    init_value: A scalar Tensor to set the initial value to be decayed.\n\n  Returns:\n    A scalar `float` Tensor.\n  """"""\n  decayed_value = tf.cast(init_value, tf.dtypes.float32)\n  decay_fn = _select_decay_fn(decay_config.decay_type)\n  decayed_value = decay_fn(\n      decayed_value,\n      global_step=global_step,\n      decay_steps=decay_config.decay_steps,\n      decay_rate=decay_config.decay_rate)\n  decayed_value = tf.maximum(decayed_value, decay_config.min_value)\n  return decayed_value\n\n\ndef apply_feature_mask(features, feature_mask=None):\n  """"""Applies a feature mask on `features` if the `feature_mask` is not `None`.\n\n  Args:\n    features: A dense tensor representing features.\n    feature_mask: A dense tensor with values in `[0, 1]` and a broadcastable\n      shape to `features`. If not set, or set to `None`, the `features` are\n      returned unchanged.\n\n  Returns:\n    A dense tensor having the same shape as `features`.\n  """"""\n  if feature_mask is None:\n    return features\n  # feature_mask values need to be in [0, 1].\n  with tf.control_dependencies([\n      tf.debugging.assert_greater_equal(feature_mask, 0.0),\n      tf.debugging.assert_less_equal(feature_mask, 1.0)\n  ]):\n    return features * tf.cast(feature_mask, features.dtype)\n\n\ndef _interleave_and_merge(tensors,\n                          pre_merge_dynamic_shape_tensor,\n                          keep_rank,\n                          is_sparse=False):\n  """"""Concatenates a list of tensors in an interleaved manner.\n\n  For example, suppose `pre_merge_dynamic_shape_tensor` is `[B, D_1, D_2, ...,\n  D_d]`, where `B` is the batch size. For sparse tensors (i.e., when `is_sparse`\n  is `True`), the interleaving is obtained by first expanding the dimension of\n  each tensor on axis 1 and then concatenating the tensors along axis 1. For\n  dense tensors (i.e., when `is_sparse` is `False`), the interleaving is\n  obtained by stacking tensors along axis 1. In both cases, the resulting shape\n  of the interleaved tensor will be `[B, N, D_1, D_2, ...D_d]`, where `N` is the\n  number of entries in `tensors`. If `keep_rank` is `True`, the original rank\n  and the original sizes of all dimensions except for the first dimension are\n  retained; the interleaved tensor is reshaped to `[(BxN), D_1, D_2, ...D_d]`.\n  If `keep_rank` is `False`, then the interleaved tensor is returned as is.\n\n  Args:\n    tensors: List of tensors with compatible shapes. Either all of them should\n      be dense or all of them should be sparse.\n    pre_merge_dynamic_shape_tensor: A 1-D tensor representing the dynamic shape\n      of each tensor in `tensors`.\n    keep_rank: Boolean indicating whether to retain the rank from the input or\n      to introduce a new dimension (axis 1).\n    is_sparse: (optional) Boolean indicating if entries in `tensors` are sparse\n      or not.\n\n  Returns:\n    An interleaved concatenation of `tensors`. If `keep_rank` is `True`, the\n    rank is the same compared to entries in `tensors`, but the size of its first\n    dimension is multiplied by a factor of the number of entries in `tensors`.\n    Otherwise, the result will have rank one more than the rank of `tensors`,\n    where the size of the new dimension (axis 1) is equal to the\n    number of entries in `tensors`.  Note that if `tensors` is empty, then a\n    value of `None` is returned.\n\n  Raises:\n    ValueError: If any entry in `tensors` has an incompatible shape.\n  """"""\n  if not tensors:\n    return None\n  # The first dimension in the resulting interleaved tensor will be inferred.\n  merged_shape = tf.concat(\n      [tf.constant([-1]), pre_merge_dynamic_shape_tensor[1:]], axis=0)\n\n  if is_sparse:\n    # This is the equivalent of tf.stack() for sparse tensors.\n    concatenated_tensors = tf.sparse.concat(\n        axis=1, sp_inputs=[tf.sparse.expand_dims(t, 1) for t in tensors])\n    return (tf.sparse.reshape(concatenated_tensors, shape=merged_shape)\n            if keep_rank else concatenated_tensors)\n  else:\n    stacked_tensors = tf.stack(tensors, axis=1)\n    return (tf.reshape(stacked_tensors, shape=merged_shape)\n            if keep_rank else stacked_tensors)\n\n\ndef unpack_neighbor_features(features, neighbor_config, keep_rank=True):\n  """"""Extracts sample features, neighbor features, and neighbor weights.\n\n  For example, suppose `features` contains a single sample feature named\n  \'F0\', the batch size is 2, and each sample has 3 neighbors. Then `features`\n  might look like the following:\n\n  ```\n  features = {\n      \'F0\': tf.constant(11.0, shape=[2, 4]),\n      \'NL_nbr_0_F0\': tf.constant(22.0, shape=[2, 4]),\n      \'NL_nbr_0_weight\': tf.constant(0.25, shape=[2, 1]),\n      \'NL_nbr_1_F0\': tf.constant(33.0, shape=[2, 4]),\n      \'NL_nbr_1_weight\': tf.constant(0.75, shape=[2, 1]),\n      \'NL_nbr_2_F0\': tf.constant(44.0, shape=[2, 4]),\n      \'NL_nbr_2_weight\': tf.constant(1.0, shape=[2, 1]),\n  },\n  ```\n\n  where `NL_nbr_<i>_F0` represents the corresponding neighbor features for the\n  sample feature \'F0\', and `NL_nbr_<i>_weight` represents its neighbor weights.\n  The specific values for each key (tensors) in this dictionary are for\n  illustrative purposes only. The first dimension of all tensors is the batch\n  size.\n\n  Example invocation:\n\n  ```\n  neighbor_config = nsl.configs.make_graph_reg_config(max_neighbors=3)\n  sample_features, nbr_features, nbr_weights = nsl.lib.unpack_neighbor_features(\n      features, neighbor_config)\n  ```\n\n  After performing these calls, we would have `sample_features` set to:\n\n  ```\n  { \'F0\': tf.constant(11.0, shape=[2, 4]) },\n  ```\n\n  `neighbor_features` set to:\n\n  ```\n  # The key in this dictionary will contain the original sample\'s feature name.\n  # The shape of the corresponding tensor will be 6x4, which is the result of\n  # doing an interleaved merge of three 2x4 tensors along axis 0.\n  {\n    \'F0\': tf.constant([[22, 22, 22, 22], [33, 33, 33, 33], [44, 44, 44, 44],\n                       [22, 22, 22, 22], [33, 33, 33, 33], [44, 44, 44, 44]]),\n  },\n  ```\n  and `neighbor_weights` set to:\n\n  ```\n  # The shape of this tensor is 6x1, which is the result of doing an\n  # interleaved merge of three 2x1 tensors along axis 0.\n  tf.constant([[0.25], [0.75], [1.0], [0.25], [0.75], [1.0]])\n  ```\n\n  Args:\n    features: Dictionary of tensors mapping feature names (sample features,\n      neighbor features, and neighbor weights) to tensors. For each sample\n      feature, all its corresponding neighbor features and neighbor weights must\n      be included. All tensors should have a rank that is at least 2, where the\n      first dimension is the batch size. The shape of every sample feature\n      tensor should be identical to each of its corresponding neighbor feature\n      tensors. The shape of each neighbor weight tensor is expected to be `[B,\n      1]`, where `B` is the batch size. Neighbor weight tensors cannot be sparse\n      tensors.\n    neighbor_config: An instance of `nsl.configs.GraphNeighborConfig`.\n    keep_rank: Boolean indicating whether to retain the rank from the input or\n      to introduce a new dimension for the neighborhood size (axis 1). Defaults\n      to `True`.\n\n  Returns:\n    sample_features: a dictionary mapping feature names to tensors. The shape\n      of these tensors remains unchanged from the input.\n    neighbor_features: a dictionary mapping feature names to tensors, where\n      these feature names are identical to the corresponding feature names in\n      `sample_features`. Further, for each feature in this dictionary, the\n      resulting tensor represents an interleaved concatenated version of all\n      corresponding neighbor feature tensors that exist. So, if the original\n      sample feature has a shape `[B, D_1, D_2, ...., D_d]`, then the shape of\n      the returned `neighbor_features` will be `[(BxN), D_1, D_2, ..., D_d]` if\n      `keep_rank` is `True`, and `[B, N, D_1, D_2, ..., D_d]` if `keep_rank` is\n      `False`. If `num_neighbors` is 0, then an empty dictionary is returned.\n    neighbor_weights: a tensor containing floating point weights. If `keep_rank`\n      is True, `neighbor_weights` will have shape `[(BxN), 1]`. Otherwise, it\n      will have shape `[B, N, 1]` This also represents an interleaved\n      concatenation of neighbor weight values across all neighbors. The rank of\n      this tensor remains unchanged. If `num_neighbors` is 0, then a value of\n      `None` is returned.\n\n  Raises:\n    KeyError: If the input does not contain all corresponding neighbor features\n      for every sample feature.\n    ValueError: If the tensors of samples and corresponding neighbors don\'t have\n      the same shape.\n  """"""\n\n  def check_shape_compatibility(tensors, expected_shape):\n    """"""Checks shape compatibility of the given tensors with `expected_shape`.\n\n    Args:\n      tensors: List of tensors whose static shapes will be checked for\n        compatibility with `expected_shape`.\n      expected_shape: Instance of `TensorShape` representing the expected static\n        shape of each tensor in `tensors`.\n    """"""\n    for tensor in tensors:\n      tensor.get_shape().assert_is_compatible_with(expected_shape)\n\n  # Iterate through the \'features\' dictionary to populate sample_features,\n  # neighbor_features, and neighbor_weights in one pass.\n  sample_features = dict()\n  neighbor_features = dict()\n  for feature_name, feature_value in features.items():\n    # Every value in \'features\' is expected to have rank > 1, i.e, \'features\'\n    # should have been batched to include the extra batch dimension.\n    feature_shape = feature_value.get_shape().with_rank_at_least(2)\n\n    if feature_name.startswith(neighbor_config.prefix):\n      continue\n\n    sample_features[feature_name] = feature_value\n\n    # If graph_reg_config.max_neighbors is 0, then neighbor_feature_list will\n    # be empty.\n    neighbor_feature_list = [\n        features[\'{}{}_{}\'.format(neighbor_config.prefix, i, feature_name)]\n        for i in range(neighbor_config.max_neighbors)\n    ]\n\n    # For a given sample feature, aggregate all of its corresponding neighbor\n    # features together. Achieve this by doing an interleaved merge of the\n    # neighbor feature tensors across all neighbors.\n\n    # Populate the \'neighbor_features\' dictionary only if there at least one\n    # neighbor feature.\n    if neighbor_feature_list:\n      check_shape_compatibility(neighbor_feature_list, feature_shape)\n      neighbor_features[feature_name] = _interleave_and_merge(\n          neighbor_feature_list,\n          tf.shape(input=feature_value),\n          keep_rank,\n          is_sparse=isinstance(feature_value, tf.sparse.SparseTensor))\n\n  # If num_neighbors is 0, then neighbor_weights_list will be empty and\n  # neighbor_weights will be \'None\'.\n  neighbor_weights_list = [\n      features[\'{}{}{}\'.format(neighbor_config.prefix, i,\n                               neighbor_config.weight_suffix)]\n      for i in range(neighbor_config.max_neighbors)\n  ]\n\n  # Neighbor weight tensors should have a shape of [B, 1].\n  check_shape_compatibility(neighbor_weights_list, [None, 1])\n  neighbor_weights = _interleave_and_merge(neighbor_weights_list, [-1, 1],\n                                           keep_rank)\n\n  return sample_features, neighbor_features, neighbor_weights\n\n\ndef strip_neighbor_features(features, neighbor_config):\n  """"""Strips graph neighbor features from a feature dictionary.\n\n  Args:\n    features: Dictionary of tensors mapping feature names to tensors. This\n      dictionary includes sample features but may or may not include\n      corresponding neighbor features for each sample feature.\n    neighbor_config: An instance of `nsl.configs.GraphNeighborConfig`.\n\n  Returns:\n    A dictionary mapping only sample feature names to tensors. Neighbor\n    features in the input are not included.\n  """"""\n\n  return {\n      key: value\n      for key, value in features.items()\n      if not key.startswith(neighbor_config.prefix)\n  }\n'"
neural_structured_learning/lib/utils_test.py,168,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.lib.utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl.testing import parameterized\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.lib import utils\nimport numpy as np\nimport tensorflow as tf\n\n\nclass UtilsTest(tf.test.TestCase, parameterized.TestCase):\n\n  def testNormalizeInf(self):\n    target_tensor = tf.constant([[1.0, 2.0, -4.0], [-1.0, 5.0, -3.0]])\n    normalized_tensor = self.evaluate(\n        utils.normalize(target_tensor, \'infinity\'))\n    expected_tensor = tf.constant([[0.25, 0.5, -1.0], [-0.2, 1.0, -0.6]])\n    self.assertAllEqual(normalized_tensor, expected_tensor)\n\n  def testProjectToBallL1(self):\n    target_tensor = tf.constant([[1.0, 2.0, -4.0]])\n    with self.assertRaises(NotImplementedError):\n      self.evaluate(\n          utils.project_to_ball(target_tensor, 0.2, configs.NormType.L1))\n\n  # The 3 test cases for this are as follows: (1) normalize both components. (2)\n  # Project the one sample that exceeds the radius back to the ball. (3)\n  # Both are within the ball, do nothing..\n  @parameterized.parameters((1, 1.0 / 3, 1.0 / 15), (4, 1.0, 4.0 / 15),\n                            (16, 1.0, 1.0))\n  def testProjectToBallL2(self, eps, first_factor, second_factor):\n    target_tensor_dict = {\n        \'f1\': tf.constant([[1.0, -2.0, 2.0], [2.0, 10.0, 11.0]])\n    }\n    projected_tensor_dict = self.evaluate(\n        utils.project_to_ball(target_tensor_dict, eps, configs.NormType.L2))\n    expected_tensor = target_tensor_dict[\'f1\'] * tf.constant([[first_factor],\n                                                              [second_factor]])\n    self.assertAllEqual(projected_tensor_dict[\'f1\'], expected_tensor)\n\n  # First test case, the radius is large enough that neither sample point is\n  # clipped. The second test case, the first sample point is clipped to radius\n  # 2. Since the second point has norm 1, it remains unchanged.\n  @parameterized.parameters((100.0, 1.0, 1.0),\n                            (2.0, 2.0 / np.sqrt(252.0 + 169.0), 1.0))\n  def testProjectToBallL2MultipleFeatures(self, radius, factor1, factor2):\n    # Sum of squares is 25 + 9 + 49 + 169 = 252 for element 1, and 1 for element\n    # 2.\n    f1 = tf.constant([[[[0.0, 3.0, -4.0], [1.0, 2.0, -2.0]],\n                       [[2.0, 3.0, 6.0], [3.0, 4.0, 12.0]]],\n                      [[[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n                       [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]])\n    # Sum of squares is 25 + 144 = 169 for element 1, 0 for element 2.\n    f2 = tf.constant([[[3.0, 4.0], [12.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]])\n    input_dict = {\'f1\': f1, \'f2\': f2}\n    projected_tensor_dict = self.evaluate(\n        utils.project_to_ball(input_dict, radius, configs.NormType.L2))\n    expected_f1_sample1 = f1[0] * factor1\n    expected_f1_sample2 = f1[1] * factor2\n    expected_f2_sample1 = f2[0] * factor1\n    expected_f2_sample2 = f2[1] * factor2\n\n    self.assertAllEqual(projected_tensor_dict[\'f1\'][0], expected_f1_sample1)\n    self.assertAllEqual(projected_tensor_dict[\'f1\'][1], expected_f1_sample2)\n    self.assertAllEqual(projected_tensor_dict[\'f2\'][0], expected_f2_sample1)\n    self.assertAllEqual(projected_tensor_dict[\'f2\'][1], expected_f2_sample2)\n\n  def testProjectToBallL2WithZero(self):\n    input_dict = {\'f1\': tf.constant(0.0, shape=[2, 3])}\n    projected_tensor_dict = self.evaluate(\n        utils.project_to_ball(input_dict, 0.5, configs.NormType.L2))\n    expected_tensor = tf.constant(0.0, shape=[2, 3])\n    self.assertAllEqual(projected_tensor_dict[\'f1\'], expected_tensor)\n\n  # The 3 test cases for this are as follows: (1) normalize both components. (2)\n  # Clip components that exceed the radius, but preserve others. (3) Both\n  # samples are within the ball, do nothing.\n  @parameterized.parameters(([1.0, 2.0, -4.0], 0.5, [0.5, 0.5, -0.5]),\n                            ([1.0, 2.0, -4.0], 1.5, [1.0, 1.5, -1.5]),\n                            ([1.0, 2.0, -4.0], 5.0, [1.0, 2.0, -4.0]))\n  def testProjectToBallLInf(self, input_tensor, eps, expected_tensor):\n    input_dict = {\'f1\': tf.constant(input_tensor)}\n    projected_tensor_dict = self.evaluate(\n        utils.project_to_ball(input_dict, eps, configs.NormType.INFINITY))\n    self.assertAllEqual(projected_tensor_dict[\'f1\'],\n                        tf.constant(expected_tensor))\n\n  def testProjectToBallLInfMultipleFeatures(self):\n    f1 = tf.constant([[1.0, 2.0, -4.0], [-1.0, 3.0, 5.0]])\n    f2 = tf.constant([[1.0, 6.0], [2.0, 4.0]])\n    input_dict = {\'f1\': f1, \'f2\': f2}\n    projected_tensor_dict = self.evaluate(\n        utils.project_to_ball(input_dict, 1.5, configs.NormType.INFINITY))\n    expected_f1 = tf.constant([[1.0, 1.5, -1.5], [-1.0, 1.5, 1.5]])\n    expected_f2 = tf.constant([[1.0, 1.5], [1.5, 1.5]])\n    self.assertAllEqual(projected_tensor_dict[\'f1\'], expected_f1)\n    self.assertAllEqual(projected_tensor_dict[\'f2\'], expected_f2)\n\n  def testNormalizeInfWithOnes(self):\n    target_tensor = tf.constant(1.0, shape=[2, 4])\n    normalized_tensor = self.evaluate(\n        utils.normalize(target_tensor, \'infinity\'))\n    expected_tensor = tf.constant(1.0, shape=[2, 4])\n    self.assertAllEqual(normalized_tensor, expected_tensor)\n\n  def testNormalizeInfWithZero(self):\n    tensor = tf.constant(0.0, shape=[2, 3])\n    normalized_tensor = self.evaluate(utils.normalize(tensor, \'infinity\'))\n    expected_tensor = tf.constant(0.0, shape=[2, 3])\n    self.assertAllEqual(normalized_tensor, expected_tensor)\n\n  def testNormalizeL1(self):\n    # target_tensor = [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]]\n    target_tensor = tf.constant(1.0, shape=[2, 4])\n    normalized_tensor = self.evaluate(utils.normalize(target_tensor, \'l1\'))\n    # L1 norm of target_tensor (other than batch/1st dim) is [4, 4]; therefore\n    # normalized_tensor = [[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]]\n    expected_tensor = tf.constant(0.25, shape=[2, 4])\n    self.assertAllEqual(normalized_tensor, expected_tensor)\n\n  def testNormalizeL1WithZero(self):\n    tensor = tf.constant(0.0, shape=[2, 3])\n    normalized_tensor = self.evaluate(utils.normalize(tensor, \'l1\'))\n    expected_tensor = tf.constant(0.0, shape=[2, 3])\n    self.assertAllEqual(normalized_tensor, expected_tensor)\n\n  def testNormalizeL2(self):\n    # target_tensor = [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]]\n    target_tensor = tf.constant(1.0, shape=[2, 4])\n    normalized_tensor = self.evaluate(utils.normalize(target_tensor, \'l2\'))\n    # L2 norm of target_tensor (other than batch/1st dim) is:\n    # [sqrt(1^2+1^2+1^2+1^2), sqrt(1^2+1^2+1^2+1^2)] = [2, 2], and therefore\n    # normalized_tensor = [[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]]\n    expected_tensor = tf.constant(0.5, shape=[2, 4])\n    self.assertAllEqual(normalized_tensor, expected_tensor)\n\n  def testMaximizeWithinUnitNormInf(self):\n    weights = tf.constant([[1.0, 2.0, -4.0], [-1.0, 5.0, -3.0]])\n    actual = self.evaluate(utils.maximize_within_unit_norm(weights, \'infinity\'))\n    expected = tf.constant([[1.0, 1.0, -1.0], [-1.0, 1.0, -1.0]])\n    self.assertAllEqual(actual, expected)\n\n  def testMaximizeWithinUnitNormL1(self):\n    weights = tf.constant([[3.0, -4.0, -5.0], [1.0, 1.0, 0.0]])\n    actual = self.evaluate(utils.maximize_within_unit_norm(weights, \'l1\'))\n    expected = tf.constant([[0.0, 0.0, -1.0], [0.5, 0.5, 0.0]])\n    self.assertAllEqual(actual, expected)\n\n  def testMaximizeWithinUnitNormL2(self):\n    weights = tf.constant([[3.0, -4.0], [-7.0, 24.0]])\n    actual = self.evaluate(utils.maximize_within_unit_norm(weights, \'l2\'))\n    # Weights are normalized by their L2 norm: [[5], [25]]\n    expected = tf.constant([[0.6, -0.8], [-0.28, 0.96]])\n    self.assertAllEqual(actual, expected)\n\n  def testMaximizeWithinUnitNormWithNestedStructure(self):\n    weights = {\'w\': tf.constant([[3., -4.], [-4., 4.]])}\n    actual = self.evaluate(utils.maximize_within_unit_norm(weights, \'l1\'))\n    expected = {\'w\': np.array([[0., -1.], [-0.5, 0.5]])}\n    self.assertAllClose(actual, expected)\n\n  def testMaximizeWithinUnitNormWithMultipleInputs(self):\n    weights = {\n        \'w1\': tf.constant([[1., 2.], [-4., 4.]]),\n        \'w2\': tf.constant([[-2.], [-7.]]),\n    }\n    actual = self.evaluate(utils.maximize_within_unit_norm(weights, \'l2\'))\n    expected = {\n        \'w1\': np.array([[1. / 3., 2. / 3.], [-4. / 9., 4. / 9.]]),\n        \'w2\': np.array([[-2. / 3.], [-7. / 9.]]),\n    }\n    self.assertAllClose(actual, expected)\n\n  @parameterized.parameters(\'l2\', \'l1\', \'infinity\')\n  def testMaximizeWithinUnitNormL2WithZeroInputShouldReturnZero(self, norm):\n    weights = tf.constant([[0.0, 0.0]])\n    actual = self.evaluate(utils.maximize_within_unit_norm(weights, norm))\n    self.assertAllEqual(actual, weights)\n\n  def testReplicateEmbeddingsWithConstant(self):\n    """"""Test the replicate_embeddings function with constant replicate_times.""""""\n    input_embeddings = tf.constant([\n        [[1., 2., 4.], [3., 5., 8.]],\n        [[2., 10., 3.], [1., 1., 1.]],\n        [[4., 8., 1.], [8., 4., 1.]],\n    ])\n    output_embeddings = self.evaluate(\n        utils.replicate_embeddings(input_embeddings, 2))\n    expected_embeddings = [\n        [[1., 2., 4.], [3., 5., 8.]],\n        [[1., 2., 4.], [3., 5., 8.]],\n        [[2., 10., 3.], [1., 1., 1.]],\n        [[2., 10., 3.], [1., 1., 1.]],\n        [[4., 8., 1.], [8., 4., 1.]],\n        [[4., 8., 1.], [8., 4., 1.]],\n    ]\n    self.assertAllEqual(expected_embeddings, output_embeddings)\n\n  def testReplicateEmbeddingsWithIndexArray(self):\n    """"""Test the replicate_embeddings function with 1-D replicate_times.""""""\n    input_embeddings = tf.constant([\n        [[1., 2., 4.], [3., 5., 8.]],\n        [[2., 10., 3.], [1., 1., 1.]],\n        [[4., 8., 1.], [8., 4., 1.]],\n    ])\n    replicate_times = tf.constant([2, 0, 1])\n    output_embeddings = self.evaluate(\n        utils.replicate_embeddings(input_embeddings, replicate_times))\n    expected_embeddings = [\n        [[1., 2., 4.], [3., 5., 8.]],\n        [[1., 2., 4.], [3., 5., 8.]],\n        [[4., 8., 1.], [8., 4., 1.]],\n    ]\n    self.assertAllEqual(expected_embeddings, output_embeddings)\n\n  def testReplicateEmbeddingsWithDynamicBatchSize(self):\n    """"""Test the replicate_embeddings function with a dynamic batch size.""""""\n    emb1 = [[1, 2, 3], [3, 2, 1]]\n    emb2 = [[4, 5, 6], [6, 5, 4]]\n    emb3 = [[7, 8, 9], [9, 8, 7]]\n    input_embeddings = np.array([emb1, emb2, emb3], dtype=np.float32)\n    replicate_times = np.array([2, 1, 2], dtype=np.int32)\n\n    @tf.function(\n        input_signature=(tf.TensorSpec(\n            (None, 2, 3), tf.float32), tf.TensorSpec((None,), tf.int32)))\n    def _replicate_with_dynamic_batch_size(embeddings, replicate_times):\n      return utils.replicate_embeddings(embeddings, replicate_times)\n\n    output_embeddings = self.evaluate(\n        _replicate_with_dynamic_batch_size(input_embeddings, replicate_times))\n    self.assertAllEqual(output_embeddings, [emb1, emb1, emb2, emb3, emb3])\n\n  def testInvalidRepeatTimes(self):\n    """"""Test the replicate_embeddings function with invalid repeat_times.""""""\n    input_embeddings = tf.constant([\n        [[1., 2., 4.], [3., 5., 8.]],\n        [[2., 10., 3.], [1., 1., 1.]],\n        [[4., 8., 1.], [8., 4., 1.]],\n    ])\n    replicate_times = tf.constant([-1, 0, 1])\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(\n          utils.replicate_embeddings(input_embeddings, replicate_times))\n\n\nclass GetTargetIndicesTest(tf.test.TestCase):\n\n  def testGetSecondIndices(self):\n    """"""Test get_target_indices function with AdvTargetType.SECOND.""""""\n    logits = tf.constant([[0.1, 0.2, 0.7], [0.3, 0.5, 0.2]], dtype=\'float32\')\n    labels = tf.constant([2, 1], dtype=\'int32\')\n    adv_target_config = configs.AdvTargetConfig(\n        target_method=configs.AdvTargetType.SECOND)\n    self.assertAllEqual(\n        tf.constant([1, 0], dtype=\'int32\'),\n        self.evaluate(\n            utils.get_target_indices(logits, labels, adv_target_config)))\n\n  def testGetLeastIndices(self):\n    """"""Test get_target_indices function with AdvTargetType.LEAST.""""""\n    logits = tf.constant([[0.1, 0.2, 0.7], [0.3, 0.5, 0.2]], dtype=\'float32\')\n    labels = tf.constant([2, 1], dtype=\'int32\')\n    adv_target_config = configs.AdvTargetConfig(\n        target_method=configs.AdvTargetType.LEAST)\n    self.assertAllEqual(\n        tf.constant([0, 2], dtype=\'int32\'),\n        self.evaluate(\n            utils.get_target_indices(logits, labels, adv_target_config)))\n\n  def testGetGroundTruthIndices(self):\n    """"""Test get_target_indices function with AdvTargetType.GROUND_TRUTH.""""""\n    logits = tf.constant([[0.1, 0.2, 0.7], [0.3, 0.5, 0.2]], dtype=\'float32\')\n    labels = tf.constant([2, 1], dtype=\'int32\')\n    adv_target_config = configs.AdvTargetConfig(\n        target_method=configs.AdvTargetType.GROUND_TRUTH)\n    self.assertAllEqual(\n        tf.constant([2, 1], dtype=\'int32\'),\n        self.evaluate(\n            utils.get_target_indices(logits, labels, adv_target_config)))\n\n  def testGetRandomIndices(self):\n    """"""Test get_target_indices function with AdvTargetType.RANDOM.""""""\n    logits = tf.constant([[0.1, 0.2, 0.7], [0.3, 0.5, 0.2]], dtype=\'float32\')\n    labels = tf.constant([2, 1], dtype=\'int32\')\n    adv_target_config = configs.AdvTargetConfig(\n        target_method=configs.AdvTargetType.RANDOM, random_seed=1)\n    self.assertAllEqual(\n        tf.constant([0, 2], dtype=\'int32\'),\n        self.evaluate(\n            utils.get_target_indices(logits, labels, adv_target_config)))\n\n\ndef decay_over_time_wrapper(config):\n\n  @tf.function\n  def decay_over_time(global_step, init_value=1.0):\n    return utils.decay_over_time(global_step, config, init_value)\n\n  return decay_over_time\n\n\nclass DecayOverTimeTest(tf.test.TestCase):\n\n  def testExponentialDecay(self):\n    """"""Test the decay_over_time function with exponential decay applied.""""""\n    init_value = 0.1\n    decay_step = 10\n    global_step = 5\n    decay_rate = 0.96\n    expected_value = init_value * decay_rate**(global_step / decay_step)\n    config = configs.DecayConfig(decay_step, decay_rate)\n    decayed_value = decay_over_time_wrapper(config)(global_step, init_value)\n    self.assertAllClose(decayed_value, expected_value, 1e-6)\n\n  def testBoundedDecay(self):\n    """"""Test the decay_over_time function with bounded decay value.""""""\n    init_value = 0.1\n    min_value = 0.99\n    decay_step = 10\n    global_step = 5\n    decay_rate = 0.96\n    bounded_config = configs.DecayConfig(decay_step, decay_rate, min_value)\n    bounded_value = decay_over_time_wrapper(bounded_config)(global_step,\n                                                            init_value)\n    self.assertAllClose(bounded_value, min_value, 1e-6)\n\n  def testInverseTimeDecay(self):\n    """"""Test the decay_over_time function with inverse time decay applied.""""""\n    init_value = 0.1\n    decay_step = 10\n    global_step = 5\n    decay_rate = 0.9\n    expected_value = init_value / (1 + decay_rate * global_step / decay_step)\n    config = configs.DecayConfig(\n        decay_step, decay_rate, decay_type=configs.DecayType.INVERSE_TIME_DECAY)\n    decayed_value = decay_over_time_wrapper(config)(global_step, init_value)\n    self.assertAllClose(decayed_value, expected_value, 1e-6)\n\n  def testNaturalExpDecay(self):\n    """"""Test the decay_over_time function with natural exp decay applied.""""""\n    init_value = 0.1\n    decay_step = 10\n    global_step = 5\n    decay_rate = 0.9\n    expected_value = init_value * math.exp(\n        -decay_rate * global_step / decay_step)\n    config = configs.DecayConfig(\n        decay_step, decay_rate, decay_type=configs.DecayType.NATURAL_EXP_DECAY)\n    decayed_value = decay_over_time_wrapper(config)(global_step, init_value)\n    self.assertAllClose(decayed_value, expected_value, 1e-6)\n\n  def testDefaultInitValueWithExponentialDecay(self):\n    """"""Test the decay_over_time function with default init value.""""""\n    decay_step = 10\n    global_step = 5\n    decay_rate = 0.96\n    expected_value = decay_rate**(global_step / decay_step)\n    config = configs.DecayConfig(decay_step, decay_rate)\n    decayed_value = decay_over_time_wrapper(config)(global_step)\n    self.assertAllClose(decayed_value, expected_value, 1e-6)\n\n  def testApplyFeatureMask(self):\n    """"""Test the apply_feature_mask function.""""""\n    features = [[1.0, 1.0], [2.0, 2.0]]\n    mask = [0.0, 1.0]\n    masked_features = utils.apply_feature_mask(\n        tf.constant(features), tf.constant(mask))\n    actual = self.evaluate(masked_features)\n    self.assertAllClose(actual, [[0.0, 1.0], [0.0, 2.0]], 1e-6)\n\n  def testApplyFeatureMaskWithNone(self):\n    """"""Test the apply_feature_mask function with \'None\' feature mask.""""""\n    features = [[1.0, 1.0], [2.0, 2.0]]\n    masked_features = utils.apply_feature_mask(tf.constant(features))\n    actual = self.evaluate(masked_features)\n    self.assertAllClose(actual, features, 1e-6)\n\n  def testApplyFeatureMaskWithInvalidMaskNegative(self):\n    """"""Test the apply_feature_mask function with mask value < 0.""""""\n    features = [[1.0, 1.0], [2.0, 2.0]]\n    mask = [-1.0, 1.0]\n    # In eager mode, the arguments are validated once `tf.debugging.assert_*` is\n    # called (in `utils.apply_feature_mask`). In graph mode, the call to\n    # `tf.debugging.assert_*` only creates an Op, and the actual validation\n    # happens when the graph is run. The behavior in graph mode may change in\n    # the future to validate statically known arguments (e.g. `tf.constant`) at\n    # Op-creation time. Enclosing both Op creation and evaluation is\n    # an `assertRaises` block handles all cases.\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      masked_features = utils.apply_feature_mask(\n          tf.constant(features), tf.constant(mask))\n      self.evaluate(masked_features)\n\n  def testApplyFeatureMaskWithInvalidMaskTooLarge(self):\n    """"""Test the apply_feature_mask function with mask value > 1.""""""\n    features = [[1.0, 1.0], [2.0, 2.0]]\n    mask = [1.0, 2.0]\n    # In eager mode, the arguments are validated once `tf.debugging.assert_*` is\n    # called (in `utils.apply_feature_mask`). In graph mode, the call to\n    # `tf.debugging.assert_*` only creates an Op, and the actual validation\n    # happens when the graph is run. The behavior in graph mode may change in\n    # the future to validate statically known arguments (e.g. `tf.constant`) at\n    # Op-creation time. Enclosing both Op creation and evaluation is\n    # an `assertRaises` block handles all cases.\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      masked_features = utils.apply_feature_mask(\n          tf.constant(features), tf.constant(mask))\n      self.evaluate(masked_features)\n\n\nclass UnpackNeighborFeaturesTest(tf.test.TestCase):\n  """"""Tests unpacking of sample feature, neighbor features, and neighbor weights.\n\n    This class currently expects a fixed number of neighbors per sample.\n  """"""\n\n  def testSampleFeatureOnlyExtractionWithNoNeighbors(self):\n    """"""Test sample feature extraction without neighbor features.""""""\n    # Simulate batch size of 1.\n    features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'F1\': tf.constant([[3.0, 4.0, 5.0]]),\n    }\n\n    expected_sample_features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'F1\': tf.constant([[3.0, 4.0, 5.0]]),\n    }\n\n    neighbor_config = configs.GraphNeighborConfig(max_neighbors=0)\n    sample_features, nbr_features, nbr_weights = utils.unpack_neighbor_features(\n        features, neighbor_config)\n    self.assertIsNone(nbr_weights)\n\n    sample_features, nbr_features = self.evaluate(\n        [sample_features, nbr_features])\n    self.assertAllEqual(sample_features[\'F0\'], expected_sample_features[\'F0\'])\n    self.assertAllEqual(sample_features[\'F1\'], expected_sample_features[\'F1\'])\n    self.assertEmpty(nbr_features)\n\n  def testSampleFeatureOnlyExtractionWithNeighbors(self):\n    """"""Test sample feature extraction with neighbor features.""""""\n    # Simulate batch size of 1.\n    features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'F1\': tf.constant([[3.0, 4.0, 5.0]]),\n        \'NL_nbr_0_F0\': tf.constant([[1.1, 2.1]]),\n        \'NL_nbr_0_F1\': tf.constant([[3.1, 4.1, 5.1]]),\n        \'NL_nbr_0_weight\': tf.constant([[0.25]]),\n        \'NL_nbr_1_F0\': tf.constant([[1.2, 2.2]]),\n        \'NL_nbr_1_F1\': tf.constant([[3.2, 4.2, 5.2]]),\n        \'NL_nbr_1_weight\': tf.constant([[0.75]]),\n    }\n\n    expected_sample_features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'F1\': tf.constant([[3.0, 4.0, 5.0]]),\n    }\n\n    neighbor_config = configs.GraphNeighborConfig(max_neighbors=0)\n    sample_features, nbr_features, nbr_weights = utils.unpack_neighbor_features(\n        features, neighbor_config)\n    self.assertIsNone(nbr_weights)\n\n    sample_features, nbr_features = self.evaluate(\n        [sample_features, nbr_features])\n    self.assertAllEqual(sample_features[\'F0\'], expected_sample_features[\'F0\'])\n    self.assertAllEqual(sample_features[\'F1\'], expected_sample_features[\'F1\'])\n    self.assertEmpty(nbr_features)\n\n  def testBatchedSampleAndNeighborFeatureExtraction(self):\n    """"""Test input contains two samples with one feature and three neighbors.""""""\n    # Simulate a batch size of 2.\n    features = {\n        \'F0\': tf.constant(11.0, shape=[2, 2]),\n        \'NL_nbr_0_F0\': tf.constant(22.0, shape=[2, 2]),\n        \'NL_nbr_0_weight\': tf.constant(0.25, shape=[2, 1]),\n        \'NL_nbr_1_F0\': tf.constant(33.0, shape=[2, 2]),\n        \'NL_nbr_1_weight\': tf.constant(0.75, shape=[2, 1]),\n        \'NL_nbr_2_F0\': tf.constant(44.0, shape=[2, 2]),\n        \'NL_nbr_2_weight\': tf.constant(1.0, shape=[2, 1]),\n    }\n\n    expected_sample_features = {\n        \'F0\': tf.constant(11.0, shape=[2, 2]),\n    }\n\n    # The key in this dictionary will contain the original sample\'s feature\n    # name. The shape of the corresponding tensor will be 6x2, which is the\n    # result of doing an interleaved merge of three 2x2 tensors along axis 0.\n    expected_neighbor_features = {\n        \'F0\':\n            tf.constant([[22.0, 22.0], [33.0, 33.0], [44.0, 44.0], [22.0, 22.0],\n                         [33.0, 33.0], [44.0, 44.0]]),\n    }\n    # The shape of this tensor is 6x1, which is the result of doing an\n    # interleaved merge of three 2x1 tensors along axis 0.\n    expected_neighbor_weights = tf.constant([[0.25], [0.75], [1.0], [0.25],\n                                             [0.75], [1.0]])\n\n    neighbor_config = configs.GraphNeighborConfig(max_neighbors=3)\n    sample_features, nbr_features, nbr_weights = self.evaluate(\n        utils.unpack_neighbor_features(features, neighbor_config))\n\n    self.assertAllEqual(sample_features[\'F0\'], expected_sample_features[\'F0\'])\n    self.assertAllEqual(nbr_features[\'F0\'], expected_neighbor_features[\'F0\'])\n    self.assertAllEqual(nbr_weights, expected_neighbor_weights)\n\n  def testExtraNeighborFeaturesIgnored(self):\n    """"""Test that extra neighbor features are ignored.""""""\n    # Simulate a batch size of 1 for simplicity.\n    features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'NL_nbr_0_F0\': tf.constant([[1.1, 2.1]]),\n        \'NL_nbr_0_weight\': tf.constant([[0.25]]),\n        \'NL_nbr_1_F0\': tf.constant([[1.2, 2.2]]),\n        \'NL_nbr_1_weight\': tf.constant([[0.75]]),\n    }\n\n    expected_sample_features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n    }\n\n    expected_neighbor_features = {\n        \'F0\': tf.constant([[1.1, 2.1]]),\n    }\n    expected_neighbor_weights = tf.constant([[0.25]])\n\n    neighbor_config = configs.GraphNeighborConfig(max_neighbors=1)\n    sample_features, nbr_features, nbr_weights = self.evaluate(\n        utils.unpack_neighbor_features(features, neighbor_config))\n\n    self.assertAllEqual(sample_features[\'F0\'], expected_sample_features[\'F0\'])\n    self.assertAllEqual(nbr_features[\'F0\'], expected_neighbor_features[\'F0\'])\n    self.assertAllEqual(nbr_weights, expected_neighbor_weights)\n\n  def testEmptyFeatures(self):\n    """"""Test unpack_neighbor_features with empty input.""""""\n    features = {}\n    neighbor_config = configs.GraphNeighborConfig(max_neighbors=0)\n    sample_features, nbr_features, nbr_weights = utils.unpack_neighbor_features(\n        features, neighbor_config)\n    self.assertIsNone(nbr_weights)\n\n    # We create a dummy tensor so that the computation graph is not empty.\n    dummy_tensor = tf.constant(1.0)\n    sample_features, nbr_features, dummy_tensor = self.evaluate(\n        [sample_features, nbr_features, dummy_tensor])\n    self.assertEmpty(sample_features)\n    self.assertEmpty(nbr_features)\n\n  def testInvalidRank(self):\n    """"""Input containing rank 1 tensors raises ValueError.""""""\n    # Simulate a batch size of 1 for simplicity.\n    features = {\n        \'F0\': tf.constant([1.0, 2.0]),\n        \'NL_nbr_0_F0\': tf.constant([1.1, 2.1]),\n        \'NL_nbr_0_weight\': tf.constant([0.25]),\n    }\n\n    with self.assertRaises(ValueError):\n      neighbor_config = configs.GraphNeighborConfig(max_neighbors=1)\n      utils.unpack_neighbor_features(features, neighbor_config)\n\n  def testInvalidNeighborWeightRank(self):\n    """"""Input containing a rank 3 neighbor weight tensor raises ValueError.""""""\n    features = {\n        \'F0\': tf.constant([1.0, 2.0]),\n        \'NL_nbr_0_F0\': tf.constant([1.1, 2.1]),\n        \'NL_nbr_0_weight\': tf.constant([[[0.25]]]),\n    }\n\n    with self.assertRaises(ValueError):\n      neighbor_config = configs.GraphNeighborConfig(max_neighbors=1)\n      utils.unpack_neighbor_features(features, neighbor_config)\n\n  def testMissingNeighborFeature(self):\n    """"""Missing neighbor feature raises KeyError.""""""\n    # Simulate a batch size of 1 for simplicity.\n    features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'NL_nbr_0_F0\': tf.constant([[1.1, 2.1]]),\n        \'NL_nbr_0_weight\': tf.constant([[0.25]]),\n        \'NL_nbr_1_weight\': tf.constant([[0.75]]),\n    }\n\n    with self.assertRaises(KeyError):\n      neighbor_config = configs.GraphNeighborConfig(max_neighbors=2)\n      utils.unpack_neighbor_features(features, neighbor_config)\n\n  def testMissingNeighborWeight(self):\n    """"""Missing neighbor weight raises KeyError.""""""\n    # Simulate a batch size of 1 for simplicity.\n    features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'NL_nbr_0_F0\': tf.constant([[1.1, 2.1]]),\n        \'NL_nbr_0_weight\': tf.constant([[0.25]]),\n        \'NL_nbr_1_F0\': tf.constant([[1.2, 2.2]]),\n    }\n\n    with self.assertRaises(KeyError):\n      neighbor_config = configs.GraphNeighborConfig(max_neighbors=2)\n      utils.unpack_neighbor_features(features, neighbor_config)\n\n  def testSampleAndNeighborFeatureShapeIncompatibility(self):\n    """"""Sample feature and neighbor feature have incompatible shapes.""""""\n    # Simulate a batch size of 1 for simplicity.\n    # The shape of the sample feature is 1x2 while the shape of the\n    # corresponding neighbor feature 1x3.\n    features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'NL_nbr_0_F0\': tf.constant([[1.1, 2.1, 3.1]]),\n        \'NL_nbr_0_weight\': tf.constant([[0.25]]),\n    }\n\n    with self.assertRaises(ValueError):\n      neighbor_config = configs.GraphNeighborConfig(max_neighbors=1)\n      utils.unpack_neighbor_features(features, neighbor_config)\n\n  def testNeighborFeatureShapeIncompatibility(self):\n    """"""One neighbor feature has an incompatible shape.""""""\n    # Simulate a batch size of 1 for simplicity.\n    # The shape of the sample feature and one neighbor feature is 1x2, while the\n    # shape of another neighbor feature 1x3.\n    features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'NL_nbr_0_F0\': tf.constant([[1.1, 2.1]]),\n        \'NL_nbr_0_weight\': tf.constant([[0.25]]),\n        \'NL_nbr_1_F0\': tf.constant([[1.2, 2.2, 3.2]]),\n        \'NL_nbr_1_weight\': tf.constant([[0.5]]),\n    }\n\n    with self.assertRaises(ValueError):\n      neighbor_config = configs.GraphNeighborConfig(max_neighbors=2)\n      utils.unpack_neighbor_features(features, neighbor_config)\n\n  def testNeighborWeightShapeIncompatibility(self):\n    """"""One neighbor weight has an incompatibile shape.""""""\n    # Simulate a batch size of 1 for simplicity.\n    # The shape of one neighbor weight is 1x2 instead of 1x1.\n    features = {\n        \'F0\': tf.constant([[1.0, 2.0]]),\n        \'NL_nbr_0_F0\': tf.constant([[1.1, 2.1]]),\n        \'NL_nbr_0_weight\': tf.constant([[0.25]]),\n        \'NL_nbr_1_F0\': tf.constant([[1.2, 2.2]]),\n        \'NL_nbr_1_weight\': tf.constant([[0.5, 0.75]]),\n    }\n\n    with self.assertRaises(ValueError):\n      neighbor_config = configs.GraphNeighborConfig(max_neighbors=2)\n      utils.unpack_neighbor_features(features, neighbor_config)\n\n  def testSparseFeature(self):\n    """"""Test the case when the sample has a sparse feature.""""""\n    # Simulate batch size of 2.\n    features = {\n        \'F0\':\n            tf.constant(11.0, shape=[2, 2]),\n        \'F1\':\n            tf.SparseTensor(\n                indices=[[0, 0], [0, 1]], values=[1.0, 2.0], dense_shape=[2,\n                                                                          4]),\n        \'NL_nbr_0_F0\':\n            tf.constant(22.0, shape=[2, 2]),\n        \'NL_nbr_0_F1\':\n            tf.SparseTensor(\n                indices=[[1, 0], [1, 1]], values=[3.0, 4.0], dense_shape=[2,\n                                                                          4]),\n        \'NL_nbr_0_weight\':\n            tf.constant(0.25, shape=[2, 1]),\n        \'NL_nbr_1_F0\':\n            tf.constant(33.0, shape=[2, 2]),\n        \'NL_nbr_1_F1\':\n            tf.SparseTensor(\n                indices=[[0, 2], [1, 3]], values=[5.0, 6.0], dense_shape=[2,\n                                                                          4]),\n        \'NL_nbr_1_weight\':\n            tf.constant(0.75, shape=[2, 1]),\n    }\n\n    expected_sample_features = {\n        \'F0\':\n            tf.constant(11.0, shape=[2, 2]),\n        \'F1\':\n            tf.SparseTensor(\n                indices=[[0, 0], [0, 1]], values=[1.0, 2.0], dense_shape=[2,\n                                                                          4]),\n    }\n\n    # The keys in this dictionary will contain the original sample\'s feature\n    # names.\n    expected_neighbor_features = {\n        # The shape of the corresponding tensor for \'F0\' will be 4x2, which is\n        # the result of doing an interleaved merge of two 2x2 tensors along\n        # axis 0.\n        \'F0\':\n            tf.constant([[22, 22], [33, 33], [22, 22], [33, 33]]),\n        # The shape of the corresponding tensor for \'F1\' will be 4x4, which is\n        # the result of doing an interleaved merge of two 2x4 tensors along\n        # axis 0.\n        \'F1\':\n            tf.SparseTensor(\n                indices=[[1, 2], [2, 0], [2, 1], [3, 3]],\n                values=[5.0, 3.0, 4.0, 6.0],\n                dense_shape=[4, 4]),\n    }\n    # The shape of this tensor is 4x1, which is the result of doing an\n    # interleaved merge of two 2x1 tensors along axis 0.\n    expected_neighbor_weights = tf.constant([[0.25], [0.75], [0.25], [0.75]])\n\n    neighbor_config = configs.GraphNeighborConfig(max_neighbors=2)\n    sample_features, nbr_features, nbr_weights = self.evaluate(\n        utils.unpack_neighbor_features(features, neighbor_config))\n\n    self.assertAllEqual(sample_features[\'F0\'], expected_sample_features[\'F0\'])\n    self.assertAllEqual(sample_features[\'F1\'].values,\n                        expected_sample_features[\'F1\'].values)\n    self.assertAllEqual(sample_features[\'F1\'].indices,\n                        expected_sample_features[\'F1\'].indices)\n    self.assertAllEqual(sample_features[\'F1\'].dense_shape,\n                        expected_sample_features[\'F1\'].dense_shape)\n    self.assertAllEqual(nbr_features[\'F0\'], expected_neighbor_features[\'F0\'])\n    self.assertAllEqual(nbr_features[\'F1\'].values,\n                        expected_neighbor_features[\'F1\'].values)\n    self.assertAllEqual(nbr_features[\'F1\'].indices,\n                        expected_neighbor_features[\'F1\'].indices)\n    self.assertAllEqual(nbr_features[\'F1\'].dense_shape,\n                        expected_neighbor_features[\'F1\'].dense_shape)\n    self.assertAllEqual(nbr_weights, expected_neighbor_weights)\n\n  def testDynamicBatchSizeAndFeatureShape(self):\n    """"""Test the case when the batch size and feature shape are both dynamic.""""""\n    # Use a dynamic batch size and a dynamic feature shape. The former\n    # corresponds to the first dimension of the tensors defined below, and the\n    # latter corresonponds to the second dimension of \'sample_features\' and\n    # \'neighbor_i_features\'.\n\n    feature_specs = {\n        \'F0\': tf.TensorSpec((None, None, 3), tf.float32),\n        \'NL_nbr_0_F0\': tf.TensorSpec((None, None, 3), tf.float32),\n        \'NL_nbr_0_weight\': tf.TensorSpec((None, 1), tf.float32),\n        \'NL_nbr_1_F0\': tf.TensorSpec((None, None, 3), tf.float32),\n        \'NL_nbr_1_weight\': tf.TensorSpec((None, 1), tf.float32)\n    }\n\n    # Specify a batch size of 3 and a pre-batching feature shape of 2x3 at run\n    # time.\n    sample1 = [[1, 2, 3], [3, 2, 1]]\n    sample2 = [[4, 5, 6], [6, 5, 4]]\n    sample3 = [[7, 8, 9], [9, 8, 7]]\n    sample_features = [sample1, sample2, sample3]  # 3x2x3\n\n    neighbor_0_features = [[[1, 3, 5], [5, 3, 1]], [[7, 9, 11], [11, 9, 7]],\n                           [[13, 15, 17], [17, 15, 13]]]  # 3x2x3\n    neighbor_0_weights = [[0.25], [0.5], [0.75]]  # 3x1\n\n    neighbor_1_features = [[[2, 4, 6], [6, 4, 2]], [[8, 10, 12], [12, 10, 8]],\n                           [[14, 16, 18], [18, 16, 14]]]  # 3x2x3\n    neighbor_1_weights = [[0.75], [0.5], [0.25]]  # 3x1\n\n    expected_sample_features = {\'F0\': sample_features}\n\n    features = {\n        \'F0\': sample_features,\n        \'NL_nbr_0_F0\': neighbor_0_features,\n        \'NL_nbr_0_weight\': neighbor_0_weights,\n        \'NL_nbr_1_F0\': neighbor_1_features,\n        \'NL_nbr_1_weight\': neighbor_1_weights\n    }\n\n    # The key in this dictionary will contain the original sample\'s feature\n    # name. The shape of the corresponding tensor will be 6x2x3, which is the\n    # result of doing an interleaved merge of 2 3x2x3 tensors along axis 0.\n    expected_neighbor_features = {\n        \'F0\': [[[1, 3, 5], [5, 3, 1]], [[2, 4, 6], [6, 4, 2]],\n               [[7, 9, 11], [11, 9, 7]], [[8, 10, 12], [12, 10, 8]],\n               [[13, 15, 17], [17, 15, 13]], [[14, 16, 18], [18, 16, 14]]],\n    }\n    # The shape of this tensor is 6x1, which is the result of doing an\n    # interleaved merge of two 3x1 tensors along axis 0.\n    expected_neighbor_weights = [[0.25], [0.75], [0.5], [0.5], [0.75], [0.25]]\n\n    neighbor_config = configs.GraphNeighborConfig(max_neighbors=2)\n\n    @tf.function(input_signature=[feature_specs])\n    def _unpack_neighbor_features(features):\n      return utils.unpack_neighbor_features(features, neighbor_config)\n\n    sample_feats, nbr_feats, nbr_weights = self.evaluate(\n        _unpack_neighbor_features(features))\n\n    self.assertAllEqual(sample_feats[\'F0\'], expected_sample_features[\'F0\'])\n    self.assertAllEqual(nbr_feats[\'F0\'], expected_neighbor_features[\'F0\'])\n    self.assertAllEqual(nbr_weights, expected_neighbor_weights)\n\n\nclass StripNeighborFeaturesTest(tf.test.TestCase):\n  """"""Tests removal of neighbor features from a feature dictionary.""""""\n\n  def testEmptyFeatures(self):\n    """"""Tests strip_neighbor_features with empty input.""""""\n    features = dict()\n    neighbor_config = configs.GraphNeighborConfig()\n    sample_features = utils.strip_neighbor_features(features, neighbor_config)\n\n    # We create a dummy tensor so that the computation graph is not empty.\n    dummy_tensor = tf.constant(1.0)\n    sample_features, dummy_tensor = self.evaluate(\n        [sample_features, dummy_tensor])\n    self.assertEmpty(sample_features)\n\n  def testNoNeighborFeatures(self):\n    """"""Tests strip_neighbor_features when there are no neighbor features.""""""\n    features = {\'F0\': tf.constant(11.0, shape=[2, 2])}\n    neighbor_config = configs.GraphNeighborConfig()\n    sample_features = utils.strip_neighbor_features(features, neighbor_config)\n\n    expected_sample_features = {\'F0\': tf.constant(11.0, shape=[2, 2])}\n\n    sample_features = self.evaluate(sample_features)\n\n    # Check that only the sample features are retained.\n    feature_keys = sorted(sample_features.keys())\n    self.assertListEqual(feature_keys, [\'F0\'])\n\n    # Check that the values of the sample feature remains unchanged.\n    self.assertAllEqual(sample_features[\'F0\'], expected_sample_features[\'F0\'])\n\n  def testBatchedFeatures(self):\n    """"""Tests strip_neighbor_features with batched input features.""""""\n    features = {\n        \'F0\':\n            tf.constant(11.0, shape=[2, 2]),\n        \'F1\':\n            tf.SparseTensor(\n                indices=[[0, 0], [0, 1]], values=[1.0, 2.0], dense_shape=[2,\n                                                                          4]),\n        \'NL_nbr_0_F0\':\n            tf.constant(22.0, shape=[2, 2]),\n        \'NL_nbr_0_F1\':\n            tf.SparseTensor(\n                indices=[[1, 0], [1, 1]], values=[3.0, 4.0], dense_shape=[2,\n                                                                          4]),\n        \'NL_nbr_0_weight\':\n            tf.constant(0.25, shape=[2, 1]),\n    }\n    neighbor_config = configs.GraphNeighborConfig()\n    sample_features = utils.strip_neighbor_features(features, neighbor_config)\n\n    expected_sample_features = {\n        \'F0\':\n            tf.constant(11.0, shape=[2, 2]),\n        \'F1\':\n            tf.SparseTensor(\n                indices=[[0, 0], [0, 1]], values=[1.0, 2.0], dense_shape=[2,\n                                                                          4]),\n    }\n\n    sample_features = self.evaluate(sample_features)\n\n    # Check that only the sample features are retained.\n    feature_keys = sorted(sample_features.keys())\n    self.assertListEqual(feature_keys, [\'F0\', \'F1\'])\n\n    # Check that the values of the sample features remain unchanged.\n    self.assertAllEqual(sample_features[\'F0\'], expected_sample_features[\'F0\'])\n    self.assertAllEqual(sample_features[\'F1\'].values,\n                        expected_sample_features[\'F1\'].values)\n    self.assertAllEqual(sample_features[\'F1\'].indices,\n                        expected_sample_features[\'F1\'].indices)\n    self.assertAllEqual(sample_features[\'F1\'].dense_shape,\n                        expected_sample_features[\'F1\'].dense_shape)\n\n  def testFeaturesWithDynamicBatchSizeAndFeatureShape(self):\n    """"""Tests the case when the batch size and feature shape are both dynamic.""""""\n    # Use a dynamic batch size and a dynamic feature shape. The former\n    # corresponds to the first dimension of the tensors defined below, and the\n    # latter corresonponds to the second dimension of \'sample_features\' and\n    # \'neighbor_i_features\'.\n\n    feature_specs = {\n        \'F0\': tf.TensorSpec((None, None, 3), tf.float32),\n        \'NL_nbr_0_F0\': tf.TensorSpec((None, None, 3), tf.float32),\n        \'NL_nbr_0_weight\': tf.TensorSpec((None, 1), tf.float32),\n    }\n\n    # Specify a batch size of 3 and a pre-batching feature shape of 2x3 at run\n    # time.\n    sample1 = [[1, 2, 3], [3, 2, 1]]\n    sample2 = [[4, 5, 6], [6, 5, 4]]\n    sample3 = [[7, 8, 9], [9, 8, 7]]\n    sample_features = [sample1, sample2, sample3]  # 3x2x3\n\n    neighbor_0_features = [[[1, 3, 5], [5, 3, 1]], [[7, 9, 11], [11, 9, 7]],\n                           [[13, 15, 17], [17, 15, 13]]]  # 3x2x3\n    neighbor_0_weights = [[0.25], [0.5], [0.75]]  # 3x1\n\n    expected_sample_features = {\'F0\': sample_features}\n\n    features = {\n        \'F0\': sample_features,\n        \'NL_nbr_0_F0\': neighbor_0_features,\n        \'NL_nbr_0_weight\': neighbor_0_weights,\n    }\n\n    neighbor_config = configs.GraphNeighborConfig()\n\n    @tf.function(input_signature=[feature_specs])\n    def _strip_neighbor_features(features):\n      return utils.strip_neighbor_features(features, neighbor_config)\n\n    sample_features = self.evaluate(_strip_neighbor_features(features))\n\n    # Check that only the sample features are retained.\n    feature_keys = sorted(sample_features.keys())\n    self.assertListEqual(feature_keys, [\'F0\'])\n\n    # Check that the value of the sample feature remains unchanged.\n    self.assertAllEqual(sample_features[\'F0\'], expected_sample_features[\'F0\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/tools/__init__.py,0,"b'""""""Tools and APIs for preparing data for Neural Structured Learning.\n\nIn addition to the functions exported here, two of the modules can be invoked\nfrom the command-line as follows:\n\n```sh\n$ python -m neural_structured_learning.tools.build_graph ...\n$ python -m neural_structured_learning.tools.pack_nbrs ...\n```\n\nFor details on the command-line usage for these programs, see the\n`nsl.tools.build_graph` and `nsl.tools.pack_nbrs` documentation.\n""""""\n\nfrom neural_structured_learning.tools.build_graph import build_graph\nfrom neural_structured_learning.tools.graph_utils import add_edge\nfrom neural_structured_learning.tools.graph_utils import add_undirected_edges\nfrom neural_structured_learning.tools.graph_utils import read_tsv_graph\nfrom neural_structured_learning.tools.graph_utils import write_tsv_graph\nfrom neural_structured_learning.tools.pack_nbrs import pack_nbrs\n'"
neural_structured_learning/tools/build_docs.py,0,"b'# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""Tool to generate api_docs for neural_structured_learning.\n\n# How to run\n\nInstall tensorflow_docs if needed:\n\n```\npip install git+https://github.com/tensorflow/docs\n```\n\nRun the docs generator:\n\n```shell\npython build_docs.py \\\n--output_dir=/tmp/neural_structured_learning_api\n```\n\nNote:\n  If duplicate or spurious docs are generated, consider\n  blacklisting them via the `private_map` argument below. Or\n  `api_generator.doc_controls`\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\n\nimport neural_structured_learning as nsl\n\nfrom tensorflow_docs.api_generator import doc_controls\nfrom tensorflow_docs.api_generator import generate_lib\nfrom tensorflow_docs.api_generator import public_api\n\nflags.DEFINE_string(""output_dir"", ""/tmp/neural_structured_learning_api"",\n                    ""Where to output the docs"")\nflags.DEFINE_string(\n    ""code_url_prefix"",\n    ""https://github.com/tensorflow/neural-structured-learning/blob/master/neural_structured_learning"",\n    ""The url prefix for links to code."")\n\nflags.DEFINE_bool(""search_hints"", True,\n                  ""Include metadata search hints in the generated files"")\n\nflags.DEFINE_string(""site_path"", ""neural_structured_learning/api_docs/python"",\n                    ""Path prefix in the _toc.yaml"")\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  do_not_generate_docs_for = []\n\n  for blocked_doc in do_not_generate_docs_for:\n    doc_controls.do_not_generate_docs(blocked_doc)\n\n  doc_generator = generate_lib.DocGenerator(\n      root_title=""Neural Structured Learning"",\n      py_modules=[(""nsl"", nsl)],\n      code_url_prefix=FLAGS.code_url_prefix,\n      search_hints=FLAGS.search_hints,\n      site_path=FLAGS.site_path,\n      # local_definitions_filter ensures that shared modules are only\n      # documented in the location that defines them, instead of every location\n      # that imports them.\n      callbacks=[public_api.local_definitions_filter])\n  doc_generator.build(output_dir=FLAGS.output_dir)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
neural_structured_learning/tools/build_graph.py,11,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Program & library to build a graph from dense features (embeddings).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\nimport six\nimport tensorflow as tf\n\n# Norm used if the computed norm of an embedding is less than this value.\n# This value is the same as the default for tf.math.l2_normalize.\n_MIN_NORM = np.float64(1e-6)\n\n\ndef _read_tfrecord_examples(filenames, id_feature_name, embedding_feature_name):\n  """"""Reads and returns the embeddings stored in the Examples in `filename`.\n\n  Args:\n    filenames: A list of names of TFRecord files containing `tf.train.Example`\n      objects.\n    id_feature_name: Name of the feature that identifies the Example\'s ID.\n    embedding_feature_name: Name of the feature that identifies the Example\'s\n        embedding.\n\n  Returns:\n    A dict mapping each instance ID to its L2-normalized embedding, represented\n    by a 1-D numpy.ndarray. The ID is expected to be contained in the singleton\n    bytes_list feature named by \'id_feature_name\', and the embedding is\n    expected to be contained in the float_list feature named by\n    \'embedding_feature_name\'.\n  """"""\n  def parse_tf_record_examples(filename):\n    """"""Generator that returns the tensorflow.Examples in `filename`.\n\n    Args:\n      filename: Name of the TFRecord file containing tensorflow.Examples.\n\n    Yields:\n      The tensorflow.Examples contained in the file.\n    """"""\n    for raw_record in tf.data.TFRecordDataset([filename]):\n      example = tf.train.Example()\n      example.ParseFromString(raw_record.numpy())\n      yield example\n\n  def l2_normalize(v):\n    """"""Returns the L2-norm of the vector `v`.\n\n    Args:\n      v: A 1-D vector (either a list or numpy array).\n\n    Returns:\n      The L2-normalized version of `v`. The result will have an L2-norm of 1.0.\n    """"""\n    l2_norm = np.linalg.norm(v)\n    return v / max(l2_norm, _MIN_NORM)\n\n  embeddings = {}\n  for filename in filenames:\n    start_time = time.time()\n    logging.info(\'Reading tf.train.Examples from TFRecord file: %s...\',\n                 filename)\n    for tf_example in parse_tf_record_examples(filename):\n      f_map = tf_example.features.feature\n      if id_feature_name not in f_map:\n        logging.error(\'No feature named ""%s"" found in input Example: %s\',\n                      id_feature_name, tf_example.ShortDebugString())\n        continue\n      ex_id = f_map[id_feature_name].bytes_list.value[0].decode(\'utf-8\')\n      if embedding_feature_name not in f_map:\n        logging.error(\'No feature named ""%s"" found in input with ID ""%s""\',\n                      embedding_feature_name, ex_id)\n        continue\n      embedding_list = f_map[embedding_feature_name].float_list.value\n      embeddings[ex_id] = l2_normalize(embedding_list)\n    logging.info(\'Done reading %d tf.train.Examples from: %s (%.2f seconds).\',\n                 len(embeddings), filename, (time.time() - start_time))\n  return embeddings\n\n\ndef _write_edges(embeddings, threshold, f):\n  """"""Writes relevant edges to `f` among pairs of the given `embeddings`.\n\n  This function considers all distinct pairs of nodes in `embeddings`,\n  computes the dot product between all such pairs, and writes any edge to `f`\n  for which the similarity is at least the given `threshold`.\n\n  Args:\n    embeddings: A `dict`: node_id -> embedding.\n    threshold: A `float` representing an inclusive lower-bound on the cosine\n        similarity for an edge to be added.\n    f: A file object to which all edges are written in TSV format. The caller is\n        responsible for opening and closing this file.\n\n  Returns:\n    The number of bi-direction edges written to the file.\n  """"""\n  start_time = time.time()\n  edge_cnt = 0\n  all_combos = itertools.combinations(six.iteritems(embeddings), 2)\n  for (i, emb_i), (j, emb_j) in all_combos:\n    weight = np.dot(emb_i, emb_j)\n    if weight >= threshold:\n      f.write(\'%s\\t%s\\t%f\\n\' % (i, j, weight))\n      f.write(\'%s\\t%s\\t%f\\n\' % (j, i, weight))\n      edge_cnt += 1\n      if (edge_cnt % 1000000) == 0:\n        logging.info(\'Wrote %d edges in %.2f seconds....\', edge_cnt,\n                     (time.time() - start_time))\n\n  return edge_cnt\n\n\ndef build_graph(embedding_files,\n                output_graph_path,\n                similarity_threshold=0.8,\n                id_feature_name=\'id\',\n                embedding_feature_name=\'embedding\'):\n  """"""Builds a graph based on dense embeddings and persists it in TSV format.\n\n  This function reads input instances from one or more TFRecord files, each\n  containing `tf.train.Example` protos. Each input example is expected to\n  contain at least the following 2 features:\n\n  *   `id`: A singleton `bytes_list` feature that identifies each example.\n  *   `embedding`: A `float_list` feature that contains the (dense) embedding of\n       each example.\n\n  `id` and `embedding` are not necessarily the literal feature names; if your\n  features have different names, you can specify them using the\n  `id_feature_name` and `embedding_feature_name` arguments, respectively.\n\n  This function then computes the cosine similarity between all pairs of input\n  examples based on their associated embeddings. An edge is written to the TSV\n  file named by `output_graph_path` for each pair whose similarity is at least\n  as large as `similarity_threshold`. Each output edge is represented by a TSV\n  line in the `output_graph_path` file with the following form:\n\n  ```\n  source_id<TAB>target_id<TAB>edge_weight\n  ```\n\n  All edges in the output will be symmetric (i.e., if edge `A--w-->B` exists in\n  the output, then so will edge `B--w-->A`).\n\n  Note that this function can also be invoked as a binary from a shell. Sample\n  usage:\n\n  `python -m neural_structured_learning.tools.build_graph` [*flags*]\n  *embedding_file.tfr... output_graph.tsv*\n\n  For details about this program\'s flags, run:\n\n  ```\n  python -m neural_structured_learning.tools.build_graph --help\n  ```\n\n  Args:\n    embedding_files: A list of names of TFRecord files containing\n      `tf.train.Example` objects, which in turn contain dense embeddings.\n    output_graph_path: Name of the file to which the output graph in TSV format\n      should be written.\n    similarity_threshold: Threshold used to determine which edges to retain in\n      the resulting graph.\n    id_feature_name: The name of the feature in the input `tf.train.Example`\n      objects representing the ID of examples.\n    embedding_feature_name: The name of the feature in the input\n      `tf.train.Example` objects representing the embedding of examples.\n  """"""\n  embeddings = _read_tfrecord_examples(embedding_files, id_feature_name,\n                                       embedding_feature_name)\n  start_time = time.time()\n  logging.info(\'Building graph and writing edges to TSV file: %s\',\n               output_graph_path)\n  with open(output_graph_path, \'w\') as f:\n    edge_cnt = _write_edges(embeddings, similarity_threshold, f)\n    logging.info(\n        \'Wrote graph containing %d bi-directional edges (%.2f seconds).\',\n        edge_cnt, (time.time() - start_time))\n\n\ndef _main(argv):\n  """"""Main function for invoking the `nsl.tools.build_graph` function.""""""\n  flag = flags.FLAGS\n  flag.showprefixforinfo = False\n  if len(argv) < 3:\n    raise app.UsageError(\n        \'Invalid number of arguments; expected 2 or more, got %d\' %\n        (len(argv) - 1))\n\n  build_graph(argv[1:-1], argv[-1], flag.similarity_threshold,\n              flag.id_feature_name, flag.embedding_feature_name)\n\n\nif __name__ == \'__main__\':\n  flags.DEFINE_string(\n      \'id_feature_name\', \'id\',\n      """"""Name of the singleton bytes_list feature in each input Example\n      whose value is the Example\'s ID."""""")\n  flags.DEFINE_string(\n      \'embedding_feature_name\', \'embedding\',\n      """"""Name of the float_list feature in each input Example\n      whose value is the Example\'s (dense) embedding."""""")\n  flags.DEFINE_float(\n      \'similarity_threshold\', 0.8,\n      """"""Lower bound on the cosine similarity required for an edge\n      to be created between two nodes."""""")\n\n  # Ensure TF 2.0 behavior even if TF 1.X is installed.\n  tf.compat.v1.enable_v2_behavior()\n  app.run(_main)\n'"
neural_structured_learning/tools/build_graph_test.py,3,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.tools.build_graph.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom neural_structured_learning.tools import build_graph as build_graph_lib\nfrom neural_structured_learning.tools import graph_utils\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\n\nclass BuildGraphTest(absltest.TestCase):\n\n  def _create_embedding_file(self):\n    return self.create_tempfile(\'embeddings.tfr\').full_path\n\n  def _create_graph_file(self):\n    return self.create_tempfile(\'graph.tsv\').full_path\n\n  def _write_embeddings(self, embedding_output_path):\n    example1 = """"""\n                features {\n                  feature {\n                    key: ""id""\n                    value: { bytes_list { value: [ ""A"" ] } }\n                  }\n                  feature {\n                    key: ""embedding""\n                    value: { float_list { value: [ 1, 1, 0 ] } }\n                  }\n                }\n              """"""\n    example2 = """"""\n                features {\n                  feature {\n                    key: ""id""\n                    value: { bytes_list { value: [ ""B"" ] } }\n                  }\n                  feature {\n                    key: ""embedding""\n                    value: { float_list { value: [ 1, 0, 1] } }\n                  }\n                }\n              """"""\n    example3 = """"""\n                features {\n                  feature {\n                    key: ""id""\n                    value: { bytes_list { value: [ ""C"" ] } }\n                  }\n                  feature {\n                    key: ""embedding""\n                    value: { float_list { value: [ 0, 1, 1] } }\n                  }\n                }\n              """"""\n    # The embedding vectors above are chosen so that the cosine of the angle\n    # between each pair of them is 0.5.\n    with tf.io.TFRecordWriter(embedding_output_path) as writer:\n      for example_str in [example1, example2, example3]:\n        example = text_format.Parse(example_str, tf.train.Example())\n        writer.write(example.SerializeToString())\n\n  def testGraphBuildingNoThresholding(self):\n    """"""All edges whose weight is greater than 0 are retained.""""""\n    embedding_path = self._create_embedding_file()\n    self._write_embeddings(embedding_path)\n    graph_path = self._create_graph_file()\n    build_graph_lib.build_graph([embedding_path],\n                                graph_path,\n                                similarity_threshold=0)\n    g_actual = graph_utils.read_tsv_graph(graph_path)\n    self.assertDictEqual(\n        g_actual, {\n            \'A\': {\n                \'B\': 0.5,\n                \'C\': 0.5\n            },\n            \'B\': {\n                \'A\': 0.5,\n                \'C\': 0.5\n            },\n            \'C\': {\n                \'A\': 0.5,\n                \'B\': 0.5\n            }\n        })\n\n  def testGraphBuildingWithThresholding(self):\n    """"""Edges below the similarity threshold are not part of the graph.""""""\n    embedding_path = self._create_embedding_file()\n    self._write_embeddings(embedding_path)\n    graph_path = self._create_graph_file()\n    build_graph_lib.build_graph([embedding_path],\n                                graph_path,\n                                similarity_threshold=0.51)\n    g_actual = graph_utils.read_tsv_graph(graph_path)\n    self.assertDictEqual(g_actual, {})\n\n\nif __name__ == \'__main__\':\n  # Ensure TF 2.0 behavior even if TF 1.X is installed.\n  tf.compat.v1.enable_v2_behavior()\n  absltest.main()\n'"
neural_structured_learning/tools/graph_utils.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions for manipulating (weighted) graphs.\n\nThe functions in this module assume that weighted graphs are represented by\nnested dictionaries, where the outer dictionary maps each edge source ID to\nan inner dictionary that maps each edge target ID to that edge\'s weight. So\nfor example, the graph containing the edges:\n\n```\nA -- 0.5 --> B\nA -- 0.9 --> C\nB -- 0.4 --> A\nB -- 1.0 --> C\nC -- 0.8 --> D\n```\n\nwould be represented by the dictionary:\n\n```\n{ ""A"": { ""B"": 0.5, ""C"": 0.9 },\n  ""B"": { ""A"": 0.4, ""C"": 1.0 },\n  ""C"": { ""D"": 0.8 }\n}\n```\n\nIn the documention, we say a graph is represented by a `dict`:\nsource_id -> (target_id -> weight).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nfrom absl import logging\nimport six\n\n\ndef add_edge(graph, edge):\n  """"""Adds an edge to a given graph.\n\n  If an edge between the two nodes already exists, the one with the largest\n  weight is retained.\n\n  Args:\n    graph: A `dict`: source_id -> (target_id -> weight) to be augmented.\n    edge: A `list` (or `tuple`) of the form `[source, target, weight]`, where\n      `source` and `target` are strings, and `weight` is a numeric value of\n      type `string` or `float`. The \'weight\' component is optional; if not\n      supplied, it defaults to 1.0.\n\n  Returns:\n    `None`. Instead, this function has a side-effect on the `graph` argument.\n  """"""\n  source = edge[0]\n  if source not in graph: graph[source] = {}\n  t_dict = graph[source]\n  target = edge[1]\n  weight = float(edge[2]) if len(edge) > 2 else 1.0\n  if target not in t_dict or weight > t_dict[target]:\n    t_dict[target] = weight\n\n\ndef add_undirected_edges(graph):\n  """"""Makes all edges of the given `graph` bi-directional.\n\n  Updates `graph` to include a reversed version of each of its edges. Multiple\n  edges between the same source and target node IDs are combined by picking the\n  edge with the largest weight.\n\n  Args:\n    graph: A `dict`: source -> (target -> weight) as returned by the\n      `read_tsv_graph` function.\n\n  Returns:\n    `None`. Instead, this function has a side-effect on the `graph` argument.\n  """"""\n  def all_graph_edges():\n    # Make a copy of all source IDs to avoid concurrent iteration failure.\n    sources = list(graph.keys())\n    for source in sources:\n      # Make a copy of source\'s out-edges to avoid concurrent iteration failure.\n      out_edges = dict(graph[source])\n      for target, weight in six.iteritems(out_edges):\n        yield (source, target, weight)\n\n  start_time = time.time()\n  logging.info(\'Making all edges bi-directional...\')\n  for s, t, w in all_graph_edges():\n    add_edge(graph, [t, s, w])\n  logging.info(\'Done (%.2f seconds). Total graph nodes: %d\',\n               (time.time() - start_time), len(graph))\n\n\ndef read_tsv_graph(filename):\n  r""""""Reads the file `filename` containing graph edges in TSV format.\n\n  Args:\n    filename: Name of a TSV file specifying the edges of a graph. Each line of\n      the input file should be the specification of a single graph edge in the\n      form `source\\<TAB\\>target[\\<TAB\\>weight]`. If supplied, `weight` should\n      be a floating point number; if not supplied, it defaults to 1.0. Multiple\n      edges between the same source and target node IDs are combined by picking\n      the edge with the largest weight.\n\n  Returns:\n    A graph represented as a `dict`: source -> (target -> weight).\n  """"""\n  start_time = time.time()\n  logging.info(\'Reading graph file: %s...\', filename)\n  graph = {}\n  edge_cnt = 0\n  with open(filename, \'rU\') as f:\n    for tsv_line in f:\n      edge = tsv_line.rstrip(\'\\n\').split(\'\\t\')\n      add_edge(graph, edge)\n      edge_cnt += 1\n  logging.info(\'Done reading %d edges from: %s (%.2f seconds).\', edge_cnt,\n               filename, (time.time() - start_time))\n  return graph\n\n\ndef write_tsv_graph(filename, graph):\n  """"""Writes the given `graph` to the file `filename` in TSV format.\n\n  Args:\n    filename: Name of the file to which TSV output is written. The TSV lines are\n      written in the same form as the input expected by `read_tsv_graph()`.\n    graph: A `dict` source_id -> (target_id -> weight) representing the graph.\n\n  Returns:\n    `None`. Instead, this has the side-effect or writing output to a file.\n  """"""\n  start_time = time.time()\n  logging.info(\'Writing graph to TSV file: %s\', filename)\n  with open(filename, \'w\') as f:\n    for s, t_dict in six.iteritems(graph):\n      for t, w in six.iteritems(t_dict):\n        f.write(\'%s\\t%s\\t%f\\n\' % (s, t, w))\n  logging.info(\'Done writing graph to TSV file: %s (%.2f seconds).\',\n               filename, (time.time() - start_time))\n'"
neural_structured_learning/tools/graph_utils_test.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.tools.graph_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nfrom absl.testing import absltest\nfrom neural_structured_learning.tools import graph_utils\n\nGRAPH = {\'A\': {\'B\': 0.5, \'C\': 0.9}, \'B\': {\'A\': 0.4, \'C\': 1.0}, \'D\': {\'A\': 0.75}}\n\n\nclass GraphUtilsTest(absltest.TestCase):\n\n  def testAddEdge(self):\n    graph = {}\n    graph_utils.add_edge(graph, [\'A\', \'B\', \'0.5\'])\n    graph_utils.add_edge(graph, [\'A\', \'C\', 0.7])  # Tests that the edge\n    graph_utils.add_edge(graph, [\'A\', \'C\', 0.9])  # ...with maximal weight\n    graph_utils.add_edge(graph, [\'A\', \'C\', 0.8])  # ...is used.\n    graph_utils.add_edge(graph, (\'B\', \'A\', \'0.4\'))\n    graph_utils.add_edge(graph, (\'B\', \'C\'))  # Tests default weight\n    graph_utils.add_edge(graph, (\'D\', \'A\', 0.75))\n    self.assertDictEqual(graph, GRAPH)\n\n  def testAddUndirectedEdges(self):\n    g_actual = copy.deepcopy(GRAPH)\n    graph_utils.add_undirected_edges(g_actual)\n    self.assertDictEqual(\n        g_actual, {\n            \'A\': {\n                \'B\': 0.5,\n                \'C\': 0.9,\n                \'D\': 0.75\n            },\n            \'B\': {\n                \'A\': 0.5,  # Note, changed from 0.4 to 0.5\n                \'C\': 1.0\n            },\n            \'C\': {         # Added\n                \'A\': 0.9,  # Added\n                \'B\': 1.0   # Added\n            },\n            \'D\': {\n                \'A\': 0.75\n            }\n        })\n\n  def testReadAndWriteTsvGraph(self):\n    path = self.create_tempfile(\'graph.tsv\').full_path\n    graph_utils.write_tsv_graph(path, GRAPH)\n    read_graph = graph_utils.read_tsv_graph(path)\n    self.assertDictEqual(read_graph, GRAPH)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
neural_structured_learning/tools/pack_nbrs.py,17,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Program & library to prepare input for graph-based NSL.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom neural_structured_learning.tools import graph_utils\nimport six\nimport tensorflow as tf\n\n\ndef _read_tfrecord_examples(filename, id_feature_name):\n  """"""Returns a dict containing the Examples read from a TFRecord file.\n\n  Args:\n    filename: Name of the TFRecord file to read. Each `tf.train.Example` in the\n      input is expected to have a feature named `id` that maps to a singleton\n      `bytes_list` value.\n    id_feature_name: Name of the singleton `bytes_list` feature in each input\n      `tf.train.Example` whose value is the Example\'s ID.\n\n  Returns:\n    A dictionary that maps the ID of each Example to that Example.\n  """"""\n  def parse_example(raw_record):\n    """"""Parses and returns a single record of a TFRecord file.""""""\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    return example\n\n  def get_id(tf_example):\n    """"""Returns the (singleton) value of the Example\'s `id_feature_name` feature.\n\n    Args:\n      tf_example: The `tensorflow.Example` from which to extract the ID feature.\n        This is expected to contain a singleton bytes_list value.\n\n    Returns: The ID feature value as a (decoded) string.\n    """"""\n    id_feature = tf_example.features.feature[id_feature_name].bytes_list\n    return id_feature.value[0].decode(\'utf-8\')\n\n  start_time = time.time()\n  logging.info(\'Reading tf.train.Examples from TFRecord file: %s...\', filename)\n  result = {}\n  for raw_record in tf.data.TFRecordDataset([filename]):\n    tf_example = parse_example(raw_record)\n    result[get_id(tf_example)] = tf_example\n  logging.info(\'Done reading %d tf.train.Examples from: %s (%.2f seconds).\',\n               len(result), filename, (time.time() - start_time))\n  return result\n\n\ndef _join_examples(seed_exs, nbr_exs, graph, max_nbrs):\n  r""""""Joins the `seeds` and `nbrs` Examples using the edges in `graph`.\n\n  This generator joins and augments each labeled Example in `seed_exs` with the\n  features of at most `max_nbrs` of the seed\'s neighbors according to the given\n  `graph`, and yields each merged result.\n\n  Args:\n    seed_exs: A `dict` mapping node IDs to labeled Examples.\n    nbr_exs: A `dict` mapping node IDs to unlabeled Examples.\n    graph: A `dict`: source -> (target, weight).\n    max_nbrs: The maximum number of neighbors to merge into each seed Example,\n      or `None` if the number of neighbors per node is unlimited.\n\n  Yields:\n    The result of merging each seed Example with the features of its neighbors,\n    as described by the module comment.\n  """"""\n  # A histogram of the out-degrees of all seed Examples. The keys of this dict\n  # range from 0 to \'max_nbrs\' (inclusive) if \'max_nbrs\' is finite.\n  out_degree_count = collections.Counter()\n\n  def has_ex(node_id):\n    """"""Returns true iff \'node_id\' is in the \'seed_exs\' or \'nbr_exs dict\'.""""""\n    result = (node_id in seed_exs) or (node_id in nbr_exs)\n    if not result:\n      logging.warning(\'No tf.train.Example found for edge target ID: ""%s""\',\n                      node_id)\n    return result\n\n  def lookup_ex(node_id):\n    """"""Returns the Example from `seed_exs` or `nbr_exs` with the given ID.""""""\n    return seed_exs[node_id] if node_id in seed_exs else nbr_exs[node_id]\n\n  def join_seed_to_nbrs(seed_id):\n    """"""Joins the seed with ID `seed_id` to its out-edge graph neighbors.\n\n    This also has the side-effect of maintaining the `out_degree_count`.\n\n    Args:\n      seed_id: The ID of the seed Example to start from.\n\n    Returns:\n      A list of (nbr_wt, nbr_id) pairs (in decreasing weight order) of the\n      seed Example\'s top `max_nbrs` neighbors. So the resulting list will have\n      size at most `max_nbrs`, but it may be less (or even empty if the seed\n      Example has no out-edges).\n    """"""\n    nbr_dict = graph[seed_id] if seed_id in graph else {}\n    nbr_wt_ex_list = [(nbr_wt, nbr_id)\n                      for (nbr_id, nbr_wt) in six.iteritems(nbr_dict)\n                      if has_ex(nbr_id)]\n    result = sorted(nbr_wt_ex_list, reverse=True)[:max_nbrs]\n    out_degree_count[len(result)] += 1\n    return result\n\n  def merge_examples(seed_ex, nbr_wt_ex_list):\n    """"""Merges neighbor Examples into the given seed Example `seed_ex`.\n\n    Args:\n      seed_ex: A labeled Example.\n      nbr_wt_ex_list: A list of (nbr_wt, nbr_id) pairs (in decreasing nbr_wt\n         order) representing the neighbors of \'seed_ex\'.\n\n    Returns:\n      The Example that results from merging the features of the neighbor\n      Examples (as well as creating a feature for each neighbor\'s edge weight)\n      into `seed_ex`. See the `join()` description above for how the neighbor\n      features are named in the result.\n    """"""\n    # Make a deep copy of the seed Example to augment.\n    merged_ex = tf.train.Example()\n    merged_ex.CopyFrom(seed_ex)\n\n    # Add a feature for the number of neighbors.\n    merged_ex.features.feature[\'NL_num_nbrs\'].int64_list.value.append(\n        len(nbr_wt_ex_list))\n\n    # Enumerate the neighbors, and merge in the features of each.\n    for index, (nbr_wt, nbr_id) in enumerate(nbr_wt_ex_list):\n      prefix = \'NL_nbr_{}_\'.format(index)\n      # Add the edge weight value as a new singleton float feature.\n      weight_feature = prefix + \'weight\'\n      merged_ex.features.feature[weight_feature].float_list.value.append(nbr_wt)\n      # Copy each of the neighbor Examples features, prefixed with \'prefix\'.\n      nbr_ex = lookup_ex(nbr_id)\n      for (feature_name, feature_val) in six.iteritems(nbr_ex.features.feature):\n        new_feature = merged_ex.features.feature[prefix + feature_name]\n        new_feature.CopyFrom(feature_val)\n    return merged_ex\n\n  start_time = time.time()\n  logging.info(\n      \'Joining seed and neighbor tf.train.Examples with graph edges...\')\n  for (seed_id, seed_ex) in six.iteritems(seed_exs):\n    yield merge_examples(seed_ex, join_seed_to_nbrs(seed_id))\n  logging.info(\n      \'Done creating and writing %d merged tf.train.Examples (%.2f seconds).\',\n      len(seed_exs), (time.time() - start_time))\n  logging.info(\'Out-degree histogram: %s\', sorted(out_degree_count.items()))\n\n\ndef pack_nbrs(labeled_examples_path,\n              unlabeled_examples_path,\n              graph_path,\n              output_training_data_path,\n              add_undirected_edges=False,\n              max_nbrs=None,\n              id_feature_name=\'id\'):\n  """"""Prepares input for graph-based Neural Structured Learning and persists it.\n\n  In particular, this function merges into each labeled training example the\n  features from its out-edge neighbor examples according to a supplied\n  similarity graph, and persists the resulting (augmented) training data.\n\n  Each `tf.train.Example` read from the files identified by\n  `labeled_examples_path` and `unlabeled_examples_path` is expected to have a\n  feature that contains its ID (represented as a singleton `bytes_list` value);\n  the name of this feature is specified by the value of `id_feature_name`.\n\n  Each edge in the graph specified by `graph_path` is identified by a source\n  instance ID, a target instance ID, and an optional edge weight. These edges\n  are specified by TSV lines of the following form:\n\n  ```\n  source_id<TAB>target_id[<TAB>edge_weight]\n  ```\n\n  If no `edge_weight` is specified, it defaults to 1.0. If the input graph is\n  not symmetric and if `add_undirected_edges` is `True`, then all edges will be\n  treated as bi-directional. To build a graph based on the similarity of\n  instances\' dense embeddings, see `nsl.tools.build_graph`.\n\n  This function merges into each labeled example the features of that example\'s\n  out-edge neighbors according to that instance\'s in-edges in the graph. If a\n  value is specified for `max_nbrs`, then at most that many neighbors\' features\n  are merged into each labeled instance (based on which neighbors have the\n  largest edge weights, with ties broken using instance IDs).\n\n  Here\'s how the merging process works. For each labeled example, the features\n  of its `i`\'th out-edge neighbor will be prefixed by `NL_nbr_<i>_`, with\n  indexes `i` in the half-open interval `[0, K)`, where K is the minimum of\n  `max_nbrs` and the number of the labeled example\'s out-edges in the graph. A\n  feature named `NL_nbr_<i>_weight` will also be merged into the labeled example\n  whose value will be the neighbor\'s corresponding edge weight. The top\n  neighbors to use in this process are selected by consulting the input graph\n  and selecting the labeled example\'s out-edge neighbors with the largest edge\n  weight; ties are broken by preferring neighbor IDs with larger lexicographic\n  order. Finally, a feature named `NL_num_nbrs` is set on the result (a\n  singleton `int64_list`) denoting the number of neighbors `K` merged into the\n  labeled example.\n\n  Finally, the merged examples are written to a TFRecord file named by\n  `output_training_data_path`.\n\n  Note that this function can also be invoked as a binary from a shell. Sample\n  usage:\n\n  `python -m neural_structured_learning.tools.pack_nbrs` [*flags*]\n  *labeled.tfr unlabeled.tfr graph.tsv output.tfr*\n\n  For details about this program\'s flags, run:\n\n  ```\n  python -m neural_structured_learning.tools.pack_nbrs --help\n  ```\n\n  Args:\n    labeled_examples_path: Names a TFRecord file containing labeled\n      `tf.train.Example` instances.\n    unlabeled_examples_path: Names a TFRecord file containing unlabeled\n      `tf.train.Example` instances. This can be an empty string if there are no\n      unlabeled examples.\n    graph_path: Names a TSV file that specifies a graph as a set of edges\n      representing similarity relationships.\n    output_training_data_path: Path to a file where the resulting augmented\n      training data in the form of `tf.train.Example` instances will be\n      persisted in the TFRecord format.\n    add_undirected_edges: `Boolean` indicating whether or not to treat adges as\n      bi-directional.\n    max_nbrs: The maximum number of neighbors to use to generate the augmented\n      training data for downstream training.\n    id_feature_name: The name of the feature in the input labeled and unlabeled\n      `tf.train.Example` objects representing the ID of examples.\n  """"""\n  start_time = time.time()\n\n  # Read seed and neighbor TFRecord input files.\n  seed_exs = _read_tfrecord_examples(labeled_examples_path, id_feature_name)\n  # Unlabeled neighbor input instances are optional. If not provided, all\n  # neighbors used will be labeled instances.\n  nbr_exs = _read_tfrecord_examples(\n      unlabeled_examples_path,\n      id_feature_name) if unlabeled_examples_path else {}\n\n  # Read the input graph in TSV format, and conditionally reverse all its edges.\n  graph = graph_utils.read_tsv_graph(graph_path)\n  if add_undirected_edges:\n    graph_utils.add_undirected_edges(graph)\n\n  # Join the edges with the seed and neighbor Examples, and write out the\n  # results to the output TFRecord file.\n  with tf.io.TFRecordWriter(output_training_data_path) as writer:\n    for merged_ex in _join_examples(seed_exs, nbr_exs, graph, max_nbrs):\n      writer.write(merged_ex.SerializeToString())\n  logging.info(\'Output written to TFRecord file: %s.\',\n               output_training_data_path)\n  logging.info(\'Total running time: %.2f minutes.\',\n               (time.time() - start_time) / 60.0)\n\n\ndef _main(argv):\n  """"""Main function for invoking the `nsl.tools.pack_nbrs` function.""""""\n  flag = flags.FLAGS\n  flag.showprefixforinfo = False\n  # Check that the correct number of arguments have been provided.\n  if len(argv) != 5:\n    raise app.UsageError(\'Invalid number of arguments; expected 4, got %d\' %\n                         (len(argv) - 1))\n\n  pack_nbrs(\n      labeled_examples_path=argv[1],\n      unlabeled_examples_path=argv[2],\n      graph_path=argv[3],\n      output_training_data_path=argv[4],\n      add_undirected_edges=flag.add_undirected_edges,\n      max_nbrs=flag.max_nbrs,\n      id_feature_name=flag.id_feature_name)\n\n\nif __name__ == \'__main__\':\n  flags.DEFINE_integer(\n      \'max_nbrs\', None,\n      \'The maximum number of neighbors to merge into each labeled Example.\')\n  flags.DEFINE_string(\n      \'id_feature_name\', \'id\',\n      """"""Name of the singleton bytes_list feature in each input Example\n      whose value is the Example\'s ID."""""")\n  flags.DEFINE_bool(\n      \'add_undirected_edges\', False,\n      """"""By default, the set of neighbors of a node S are\n      only those nodes T such that there is an edge S-->T in the input graph. If\n      this flag is True, all edges of the graph will be made symmetric before\n      determining each node\'s neighbors (and in the case where edges S-->T and\n      T-->S exist in the input graph with weights w1 and w2, respectively, the\n      weight of the symmetric edge will be max(w1, w2))."""""")\n\n  # Ensure TF 2.0 behavior even if TF 1.X is installed.\n  tf.compat.v1.enable_v2_behavior()\n  app.run(_main)\n'"
neural_structured_learning/tools/pack_nbrs_test.py,22,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.tools.pack_nbrs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom neural_structured_learning.tools import graph_utils\nfrom neural_structured_learning.tools import pack_nbrs as pack_nbrs_lib\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\n\ndef _read_tfrecord_examples(filename):\n  """"""Returns a dict with Examples read from a TFRecord file and keyed by ID.""""""\n  result = {}\n  for raw_record in tf.data.TFRecordDataset([filename]):\n    tf_example = tf.train.Example()\n    tf_example.ParseFromString(raw_record.numpy())\n    id_feature = tf_example.features.feature[\'id\'].bytes_list\n    result[id_feature.value[0].decode(\'utf-8\')] = tf_example\n  return result\n\n\ndef _example_a():\n  """"""Returns the features for node A as a `tf.train.Example` instance.""""""\n  example_str = """"""\n                  features {\n                    feature {\n                      key: ""id""\n                      value: { bytes_list { value: [ ""A"" ] } }\n                    }\n                    feature {\n                      key: ""some_int_feature""\n                      value: { int64_list { value: [ 1, 1 ] } }\n                    }\n                    feature {\n                      key: ""label""\n                      value: { int64_list { value: [ 0 ] } }\n                    }\n                  }\n                """"""\n  return text_format.Parse(example_str, tf.train.Example())\n\n\ndef _example_b():\n  """"""Returns the features for node B as a `tf.train.Example` instance.""""""\n  example_str = """"""\n                  features {\n                    feature {\n                      key: ""id""\n                      value: { bytes_list { value: [ ""B"" ] } }\n                    }\n                    feature {\n                      key: ""some_int_feature""\n                      value: { int64_list { value: [ 2, 2 ] } }\n                    }\n                    feature {\n                      key: ""label""\n                      value: { int64_list { value: [ 1 ] } }\n                    }\n                  }\n                """"""\n  return text_format.Parse(example_str, tf.train.Example())\n\n\ndef _example_c():\n  """"""Returns the features for node C as a `tf.train.Example` instance.""""""\n  example_str = """"""\n                  features {\n                    feature {\n                      key: ""id""\n                      value: { bytes_list { value: [ ""C"" ] } }\n                    }\n                    feature {\n                      key: ""some_int_feature""\n                      value: { int64_list { value: [ 3, 3 ] } }\n                    }\n                    feature {\n                      key: ""label""\n                      value: { int64_list { value: [ 2 ] } }\n                    }\n                  }\n                """"""\n  return text_format.Parse(example_str, tf.train.Example())\n\n\ndef _num_neighbors_example(num_neighbors):\n  """"""Returns a `tf.train.Example` with the \'NL_num_nbrs\' feature.""""""\n  example_str = """"""\n                  features {\n                    feature {\n                      key: ""NL_num_nbrs""\n                      value: { int64_list { value: [ %d ] } }\n                    }\n                  }\n                """""" % (\n                    num_neighbors)\n  return text_format.Parse(example_str, tf.train.Example())\n\n\ndef _node_as_neighbor(example, neighbor_id, edge_weight):\n  """"""Returns a `tf.train.Example` containing neighbor features.""""""\n  result = tf.train.Example()\n  nbr_prefix = \'NL_nbr_{}_\'.format(neighbor_id)\n\n  # Add the edge weight value as a new singleton float feature.\n  weight_feature = nbr_prefix + \'weight\'\n  result.features.feature[weight_feature].float_list.value.append(edge_weight)\n\n  # Copy each of the neighbor Example\'s features, prefixed with \'nbr_prefix\'.\n  for (feature_name, feature_val) in example.features.feature.items():\n    new_feature = result.features.feature[nbr_prefix + feature_name]\n    new_feature.CopyFrom(feature_val)\n  return result\n\n\ndef _write_examples(examples_file, examples):\n  """"""Writes the given `examples` to the TFRecord file named `examples_file`.""""""\n  with tf.io.TFRecordWriter(examples_file) as writer:\n    for example in examples:\n      writer.write(example.SerializeToString())\n\n\n# Note that this is an asymmetric/directed graph.\n_GRAPH = {\'A\': {\'B\': 0.5, \'C\': 0.9}, \'B\': {\'A\': 0.4, \'C\': 1.0}}\n\n\ndef _augmented_a_directed_one_nbr():\n  """"""Returns an augmented `tf.train.Example` instance for node A.""""""\n  augmented_a = _example_a()\n  augmented_a.MergeFrom(_node_as_neighbor(_example_c(), 0, 0.9))\n  augmented_a.MergeFrom(_num_neighbors_example(1))\n  return augmented_a\n\n\ndef _augmented_a_directed_two_nbrs():\n  """"""Returns an augmented `tf.train.Example` instance for node A.""""""\n  augmented_a = _example_a()\n  augmented_a.MergeFrom(_node_as_neighbor(_example_c(), 0, 0.9))\n  augmented_a.MergeFrom(_node_as_neighbor(_example_b(), 1, 0.5))\n  augmented_a.MergeFrom(_num_neighbors_example(2))\n  return augmented_a\n\n\ndef _augmented_a_undirected_one_nbr():\n  """"""Returns an augmented `tf.train.Example` instance for node A.""""""\n  return _augmented_a_directed_one_nbr()\n\n\ndef _augmented_a_undirected_two_nbrs():\n  """"""Returns an augmented `tf.train.Example` instance for node A.""""""\n  return _augmented_a_directed_two_nbrs()\n\n\ndef _augmented_c_directed():\n  """"""Returns an augmented `tf.train.Example` instance for node C.""""""\n  augmented_c = _example_c()\n  augmented_c.MergeFrom(_num_neighbors_example(0))\n  return augmented_c\n\n\ndef _augmented_c_undirected_one_nbr_b():\n  """"""Returns an augmented `tf.train.Example` instance for node C with nbr B.""""""\n  augmented_c = _example_c()\n  augmented_c.MergeFrom(_node_as_neighbor(_example_b(), 0, 1.0))\n  augmented_c.MergeFrom(_num_neighbors_example(1))\n  return augmented_c\n\n\ndef _augmented_c_undirected_one_nbr_a():\n  """"""Returns an augmented `tf.train.Example` instance for node C with nbr A.""""""\n  augmented_c = _example_c()\n  augmented_c.MergeFrom(_node_as_neighbor(_example_a(), 0, 0.9))\n  augmented_c.MergeFrom(_num_neighbors_example(1))\n  return augmented_c\n\n\ndef _augmented_c_undirected_two_nbrs():\n  """"""Returns an augmented `tf.train.Example` instance for node C.""""""\n  augmented_c = _example_c()\n  augmented_c.MergeFrom(_node_as_neighbor(_example_b(), 0, 1.0))\n  augmented_c.MergeFrom(_node_as_neighbor(_example_a(), 1, 0.9))\n  augmented_c.MergeFrom(_num_neighbors_example(2))\n  return augmented_c\n\n\nclass PackNbrsTest(absltest.TestCase):\n\n  def setUp(self):\n    super(PackNbrsTest, self).setUp()\n    # Write graph edges (as a TSV file).\n    self._graph_path = self._create_tmp_file(\'graph.tsv\')\n    graph_utils.write_tsv_graph(self._graph_path, _GRAPH)\n    # Write labeled training Examples.\n    self._training_examples_path = self._create_tmp_file(\'train_data.tfr\')\n    _write_examples(self._training_examples_path, [_example_a(), _example_c()])\n    # Write unlabeled neighbor Examples.\n    self._neighbor_examples_path = self._create_tmp_file(\'neighbor_data.tfr\')\n    _write_examples(self._neighbor_examples_path, [_example_b()])\n    # Create output file\n    self._output_nsl_training_data_path = self._create_tmp_file(\n        \'nsl_train_data.tfr\')\n\n  def _create_tmp_file(self, filename):\n    return self.create_tempfile(filename).full_path\n\n  def testDirectedGraphUnlimitedNbrsNoNeighborExamples(self):\n    """"""Tests pack_nbrs() with an empty second argument (neighbor examples).\n\n    In this case, the edge A-->B is dangling because there will be no Example\n    named ""B"" in the input.\n    """"""\n    pack_nbrs_lib.pack_nbrs(\n        self._training_examples_path,\n        \'\',\n        self._graph_path,\n        self._output_nsl_training_data_path,\n        add_undirected_edges=False)\n    expected_nsl_train_data = {\n        # Node A has only one neighbor, namely C.\n        \'A\': _augmented_a_directed_one_nbr(),\n        # C has no neighbors in the directed case.\n        \'C\': _augmented_c_directed()\n    }\n    actual_nsl_train_data = _read_tfrecord_examples(\n        self._output_nsl_training_data_path)\n    self.assertDictEqual(actual_nsl_train_data, expected_nsl_train_data)\n\n  def testUndirectedGraphUnlimitedNbrsNoNeighborExamples(self):\n    """"""Tests pack_nbrs() with an empty second argument (neighbor examples).\n\n    In this case, the edge A-->B is dangling because there will be no Example\n    named ""B"" in the input.\n    """"""\n    pack_nbrs_lib.pack_nbrs(\n        self._training_examples_path,\n        \'\',\n        self._graph_path,\n        self._output_nsl_training_data_path,\n        add_undirected_edges=True)\n    expected_nsl_train_data = {\n        # Node A has only one neighbor, namely C.\n        \'A\': _augmented_a_directed_one_nbr(),\n        # C\'s only neighbor in the undirected case is A.\n        \'C\': _augmented_c_undirected_one_nbr_a()\n    }\n    actual_nsl_train_data = _read_tfrecord_examples(\n        self._output_nsl_training_data_path)\n    self.assertDictEqual(actual_nsl_train_data, expected_nsl_train_data)\n\n  def testDirectedGraphUnlimitedNbrs(self):\n    pack_nbrs_lib.pack_nbrs(\n        self._training_examples_path,\n        self._neighbor_examples_path,\n        self._graph_path,\n        self._output_nsl_training_data_path,\n        add_undirected_edges=False)\n    expected_nsl_train_data = {\n        \'A\': _augmented_a_directed_two_nbrs(),\n        \'C\': _augmented_c_directed()\n    }\n    actual_nsl_train_data = _read_tfrecord_examples(\n        self._output_nsl_training_data_path)\n    self.assertDictEqual(actual_nsl_train_data, expected_nsl_train_data)\n\n  def testDirectedGraphLimitedNbrs(self):\n    pack_nbrs_lib.pack_nbrs(\n        self._training_examples_path,\n        self._neighbor_examples_path,\n        self._graph_path,\n        self._output_nsl_training_data_path,\n        add_undirected_edges=False,\n        max_nbrs=1)\n    expected_nsl_train_data = {\n        \'A\': _augmented_a_directed_one_nbr(),\n        \'C\': _augmented_c_directed()\n    }\n    actual_nsl_train_data = _read_tfrecord_examples(\n        self._output_nsl_training_data_path)\n    self.assertDictEqual(actual_nsl_train_data, expected_nsl_train_data)\n\n  def testUndirectedGraphUnlimitedNbrs(self):\n    pack_nbrs_lib.pack_nbrs(\n        self._training_examples_path,\n        self._neighbor_examples_path,\n        self._graph_path,\n        self._output_nsl_training_data_path,\n        add_undirected_edges=True)\n    expected_nsl_train_data = {\n        \'A\': _augmented_a_undirected_two_nbrs(),\n        \'C\': _augmented_c_undirected_two_nbrs()\n    }\n    actual_nsl_train_data = _read_tfrecord_examples(\n        self._output_nsl_training_data_path)\n    self.assertDictEqual(actual_nsl_train_data, expected_nsl_train_data)\n\n  def testUndirectedGraphLimitedNbrs(self):\n    pack_nbrs_lib.pack_nbrs(\n        self._training_examples_path,\n        self._neighbor_examples_path,\n        self._graph_path,\n        self._output_nsl_training_data_path,\n        add_undirected_edges=True,\n        max_nbrs=1)\n    expected_nsl_train_data = {\n        \'A\': _augmented_a_undirected_one_nbr(),\n        \'C\': _augmented_c_undirected_one_nbr_b()\n    }\n    actual_nsl_train_data = _read_tfrecord_examples(\n        self._output_nsl_training_data_path)\n    self.assertDictEqual(actual_nsl_train_data, expected_nsl_train_data)\n\n\nif __name__ == \'__main__\':\n  # Ensure TF 2.0 behavior even if TF 1.X is installed.\n  tf.compat.v1.enable_v2_behavior()\n  absltest.main()\n'"
research/a2n/__init__.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
research/a2n/dataset.py,36,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A class representing a dataset input pipeline and an iterator.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport clueweb_text_graph\nimport numpy as np\nimport tensorflow as tf\n\n\ndef get_graph_nbrhd(train_graph, ent, exclude_tuple):\n  """"""Helper to get neighbor entities excluding a particular tuple.""""""\n  es, er, et = exclude_tuple\n  neighborhood = [nbr for nbr in train_graph.kg_data[ent]\n                  if ent != es or nbr != et or\n                  # er not in train_graph.kg_data[ent][nbr]]\n                  (train_graph.kg_data[ent][nbr] - set([er]))]\n  if train_graph.add_reverse_graph:\n    rev_nighborhood = [nbr for nbr in train_graph.reverse_kg_data[ent]\n                       if ent != et or nbr != es or\n                       # er not in train_graph.reverse_kg_data[ent][nbr]]\n                       (train_graph.reverse_kg_data[ent][nbr] - set([er]))]\n    neighborhood += rev_nighborhood\n  neighborhood = np.array(list(set(neighborhood)), dtype=np.int)\n  return neighborhood\n\n\ndef get_graph_nbrhd_with_rels(train_graph, ent, exclude_tuple):\n  """"""Helper to get neighbor (rels, ents) excluding a particular tuple.""""""\n  es, er, et = exclude_tuple\n  neighborhood = [[r, nbr] for nbr in train_graph.kg_data[ent]\n                  for r in train_graph.kg_data[ent][nbr]\n                  # if r != er]\n                  if ent != es or nbr != et or r != er]\n  if not neighborhood:\n    neighborhood = [[]]\n  # if train_graph.add_reverse_graph:\n  #   rev_nighborhood = [nbr for nbr in train_graph.reverse_kg_data[ent]\n  #                      if ent != et or nbr != es or\n  #                      # er not in train_graph.reverse_kg_data[ent][nbr]]\n  #                      (train_graph.reverse_kg_data[ent][nbr] - set([er]))]\n  #   neighborhood += rev_nighborhood\n  neighborhood = np.array(neighborhood, dtype=np.int)\n  return neighborhood\n\n\ndef get_graph_nbrhd_text(train_graph, ent, max_text_len):\n  """"""Helper to get neighbor text relations.""""""\n  neighborhood = []\n  for nbr in train_graph.kg_text_data[ent]:\n    for text in train_graph.kg_text_data[ent][nbr]:\n      text_edge = [nbr] + text\n      text_edge = text_edge[:max_text_len+1]\n      len_to_pad = max_text_len + 1 - len(text_edge)\n      if len_to_pad:\n        text_edge += [train_graph.vocab[train_graph.mask_token]] * len_to_pad\n      neighborhood.append(text_edge)\n  if not neighborhood:\n    neighborhood = [[]]\n  # if train_graph.add_reverse_graph:\n  #   rev_nighborhood = [nbr for nbr in train_graph.reverse_kg_data[ent]\n  #                      if ent != et or nbr != es or\n  #                      # er not in train_graph.reverse_kg_data[ent][nbr]]\n  #                      (train_graph.reverse_kg_data[ent][nbr] - set([er]))]\n  #   neighborhood += rev_nighborhood\n  neighborhood = np.array(neighborhood, dtype=np.int)\n  return neighborhood\n\n\ndef get_graph_nbrhd_embd_text(train_graph, ent, max_text_nbrs):\n  """"""Helper to get neighbor text relations from embedded data.""""""\n  neighborhood = []\n  neighborhood_emb = []\n  for nbr in train_graph.kg_text_data[ent]:\n    for sid in train_graph.kg_text_data[ent][nbr]:\n      neighborhood.append(nbr)\n      eid = train_graph.emb_id_map[sid]\n      neighborhood_emb.append(train_graph.embeddings[eid])\n  if not neighborhood:\n    neighborhood = [[]]\n    neighborhood_emb = [np.zeros(train_graph.embeddings[0].size)]\n  neighborhood = np.array(neighborhood, dtype=np.int)\n  neighborhood_emb = np.array(neighborhood_emb, dtype=np.float32)\n  if neighborhood.shape[0] > max_text_nbrs:\n    ids = np.random.choice(np.range(neighborhood.shape[0]),\n                           size=max_text_nbrs, replace=False)\n    neighborhood = neighborhood[ids]\n    neighborhood_emb = neighborhood_emb[ids]\n  else:\n    neighborhood = sample_or_pad(neighborhood, max_text_nbrs,\n                                 pad_value=train_graph.ent_pad)\n    neighborhood_emb = sample_or_pad(neighborhood_emb, max_text_nbrs,\n                                     pad_value=0)\n\n  return neighborhood, neighborhood_emb\n\n# def get_graph_nbrhd_text_target(train_graph, ent, max_text_len):\n#   """"""Helper to get neighbor text relations.""""""\n#   neighborhood = []\n#   for nbr in train_graph.kg_text_data[ent]:\n#     for text in train_graph.kg_text_data[ent][nbr]:\n#       text_edge = [nbr] + text\n#       text_edge = text_edge[:max_text_len+1]\n#       len_to_pad = max_text_len + 1 - len(text_edge)\n#       if len_to_pad:\n#         text_edge += [train_graph.vocab[train_graph.mask_token]] * len_to_pad\n#       neighborhood.append(text_edge)\n#   if not neighborhood:\n#     neighborhood = [[]]\n#   # if train_graph.add_reverse_graph:\n#   #   rev_nighborhood = [nbr for nbr in train_graph.reverse_kg_data[ent]\n#   #                      if ent != et or nbr != es or\n#   #                      # er not in train_graph.reverse_kg_data[ent][nbr]]\n#   #                      (train_graph.reverse_kg_data[ent][nbr] - set([er]))]\n#   #   neighborhood += rev_nighborhood\n#   neighborhood = np.array(neighborhood, dtype=np.int)\n#   return neighborhood\n\n\ndef _proc_paths(paths, er=None, et=None, max_length=1, pad=(-1, -1)):\n  """"""Process path from string to list of ints, exculde query tuple and pad.""""""\n  assert len(pad) == 2\n  out = []\n  for path in paths:\n    p = map(int, path.strip().split("" ""))\n    p += list(pad)*(max_length - int(0.5*len(p)))\n    if er:\n      if p[0] == er and p[1] == et:\n        continue\n    out.append(p)\n  return out\n\n\ndef get_graph_nbrhd_paths(train_graph, ent, exclude_tuple):\n  """"""Helper to get neighbor (rels, ents) excluding a particular tuple.""""""\n  es, er, et = exclude_tuple\n  neighborhood = []\n  for i in range(train_graph.max_path_length):\n    if ent == es:\n      paths = _proc_paths(train_graph.paths[i][ent], er, et,\n                          train_graph.max_path_length,\n                          (train_graph.rel_pad, train_graph.ent_pad))\n    else:\n      paths = _proc_paths(train_graph.paths[i][ent],\n                          max_length=train_graph.max_path_length,\n                          pad=(train_graph.rel_pad, train_graph.ent_pad))\n    neighborhood += paths\n  if not neighborhood:\n    neighborhood = [[]]\n  neighborhood = np.array(neighborhood, dtype=np.int)\n  return neighborhood\n\n\ndef _sample_next_edges(edges, to_sample):\n  if len(edges) < to_sample:\n    return edges\n  sample_ids = np.random.choice(range(len(edges)), size=to_sample,\n                                replace=False)\n  return [edges[i] for i in sample_ids]\n\n\ndef get_graph_nbrhd_paths_randwalk(train_graph, ent, exclude_tuple,\n                                   max_length=1,\n                                   max_paths=200, terminate_prob=0.1,\n                                   pad=(-1, -1)):\n  """"""Helper to get paths through random walk excluding a particular tuple.""""""\n  _, er, et = exclude_tuple\n  nsample_per_step = int(max_paths ** (1.0 / train_graph.max_path_length))\n  neighborhood = []\n  # paths of length one\n  init_edges = list(train_graph.next_edges[ent] - set((er, et)))\n  current_paths = _sample_next_edges(init_edges, nsample_per_step)\n  current_paths = map(list, current_paths)\n  # import pdb; pdb.set_trace()\n  # outlog = """"\n  for _ in range(max_length-1):\n    next_paths = []\n    # outlog += ""current_paths: "" + str(current_paths) + ""\\n""\n    while len(current_paths) > 0:\n      path = current_paths.pop()\n      # outlog += ""path:"" + str(path) + ""\\n""\n      if np.random.random() <= terminate_prob:\n        # Terminate this path\n        neighborhood.append(path + list(pad)*(max_length - int(0.5*len(path))))\n        # outlog += ""adding to paths, ""\n        # outlog += ""nbd:"" + str(neighborhood) + ""\\n""\n      else:\n        # Expand the node\n        prev_ents = path[1::2]\n        last_ent = path[-1]\n        next_edges = _sample_next_edges(list(train_graph.next_edges[last_ent]),\n                                        nsample_per_step)\n        # outlog += ""next_edges:"" +  str(next_edges) + ""\\n""\n        for r, e2 in next_edges:\n          # outlog += ""\\t edge:"" + str(r) + str(e2) + ""\\n""\n          if e2 in prev_ents:\n            # outlog += ""skipped "" + str(e2) + ""\\n""\n            continue\n          next_paths.append(path + [r, e2])\n        # outlog += ""next_paths:"" +  str(next_paths) + ""\\n""\n    current_paths = next_paths\n  if current_paths:\n    for path in current_paths:\n      neighborhood.append(path + list(pad)*(max_length - int(0.5*len(path))))\n\n  # outlog += ""final: "" + str(neighborhood) + ""\\n""\n  # print(outlog)\n  if not neighborhood:\n    neighborhood = [[]]\n  # import pdb; pdb.set_trace()\n  neighborhood = np.array(neighborhood, dtype=np.int)\n  return neighborhood\n\n\ndef sample_or_pad(arr, max_size, pad_value=-1):\n  """"""Helper to pad arr along axis 0 to max_size or subsample to max_size.""""""\n  arr_shape = arr.shape\n  if arr.size == 0:\n    if isinstance(pad_value, list):\n      result = np.ones((max_size, len(pad_value)), dtype=arr.dtype) * pad_value\n    else:\n      result = np.ones((max_size,), dtype=arr.dtype) * pad_value\n  elif arr.shape[0] > max_size:\n    if arr.ndim == 1:\n      result = np.random.choice(arr, size=max_size, replace=False)\n    else:\n      idx = np.arange(arr.shape[0])\n      np.random.shuffle(idx)\n      result = arr[idx[:max_size], :]\n  else:\n    padding = np.ones((max_size-arr.shape[0],) + arr_shape[1:],\n                      dtype=arr.dtype)\n    if isinstance(pad_value, list):\n      for i in range(len(pad_value)):\n        padding[..., i] *= pad_value[i]\n    else:\n      padding *= pad_value\n    result = np.concatenate((arr, padding), axis=0)\n  # result = np.pad(arr,\n  #                 [[0, max_size-arr.shape[0]]] + ([[0, 0]] * (arr.ndim-1)),\n  #                 ""constant"", constant_values=pad_value)\n  return result\n\n\nclass Dataset(object):\n  """"""A class representing a training dataset for KB.\n    Dataset.dataset is a tf.data.Dataset object and Dataset.iterator is an\n    iterator over the dataset.\n    Dataset.input_tensors are the input tensors used in downstream model, it\n    can be returned by Dataset.get_input_tensors()\n    Dataset iteration parameters are:\n    batchsize: size of each mini-batch\n    num_epochs: number of epochs to iterate over the dataset\n\n    Each tuple is processed to include the neighborhoods of the entities using\n    the following parameters:\n    max_neighbors: maximum number of entities in the neighborhood, if None this\n    is specified from train_graph\n    max_negatives: maximum  number of negative entities samples for each example\n\n  """"""\n\n  def __init__(self, data_graph, train_graph=None, mode=""train"",\n               max_negatives=2, max_neighbors=None, num_epochs=20,\n               batchsize=64, model_type=""attention"",\n               max_text_len=None, max_text_neighbors=None, val_graph=None):\n    """"""Initialize the Dataset object.""""""\n    if not train_graph:\n      train_graph = data_graph\n    self.train_graph = train_graph\n    self.data_graph = data_graph\n    self.mode = mode\n    if mode != ""train"":\n      if max_negatives:\n        self.max_negatives = max_negatives\n      else:\n        self.max_negatives = train_graph.ent_vocab_size - 1\n    else:\n      if not max_negatives and mode == ""train"":\n        raise ValueError(""Must provide max_negatives value for training."")\n      self.max_negatives = max_negatives\n    if max_neighbors:\n      self.max_neighbors = max_neighbors\n    else:\n      self.max_neighbors = train_graph.max_neighbors\n    self.num_epochs = num_epochs\n    self.batchsize = batchsize\n    self.iterator = None\n    self.input_tensors = None\n    self.output_shapes = None\n    self.model_type = model_type\n    self.max_text_len = max_text_len\n    self.max_text_neighbors = max_text_neighbors\n    self.val_graph = val_graph\n\n  def _tuple_iterator(self):\n    """"""Iterate over training tuples.""""""\n    if self.mode == ""train"":\n      np.random.shuffle(self.data_graph.tuple_store)\n    for example in self.data_graph.tuple_store:\n      s, r, t = example\n      yield s, r, t, False\n      # if self.train_graph.add_inverse_edge:\n      #   inv_r = self.train_graph.get_inverse_relation_from_id(r)\n      #   yield t, inv_r, s, False\n      if self.model_type not in \\\n          [""source_rel_attention"", ""source_path_attention""]:\n        yield s, r, t, True\n\n  def featurize_each_example(self, example_tuple):\n    """"""Convert each example into padded arrays for input to model.""""""\n    s, r, t, reverse = example_tuple\n    if not reverse:\n      all_targets = self.train_graph.all_reachable_e2[(s, r)]\n      if self.mode != ""train"":\n        # add all correct candidate from val/test set\n        all_targets |= self.data_graph.all_reachable_e2[(s, r)]\n        if self.val_graph:\n          # if provided also remove val tuples for testing\n          all_targets |= self.val_graph.all_reachable_e2[(s, r)]\n    else:\n      all_targets = self.train_graph.all_reachable_e2_reverse[(t, r)]\n      if self.mode != ""train"":\n        # add all correct candidate from val/test set\n        all_targets |= self.data_graph.all_reachable_e2_reverse[(t, r)]\n        if self.val_graph:\n          # if provided also remove val tuples for testing\n          all_targets |= self.val_graph.all_reachable_e2[(s, r)]\n        # switch s and t\n        s, t = t, s\n    candidate_negatives = list(\n        self.train_graph.all_entities -\n        (all_targets | set([t]) | set([self.train_graph.ent_pad]))\n    )\n    # if len(candidate_negatives) > self.max_negatives:\n    #   negatives = np.random.choice(candidate_negatives,\n    #                                size=self.max_negatives,\n    #                                replace=False)\n    # else:\n    #   negatives = np.array(candidate_negatives)\n    negatives = sample_or_pad(\n        np.array(candidate_negatives, dtype=np.int), self.max_negatives,\n        pad_value=self.train_graph.ent_pad\n    )\n    # negatives is an array of shape (max_negatives)\n    # candidates will have shape (max_negatives + 1), i.e including the target\n    candidates = np.insert(negatives, 0, t, axis=0)\n\n    if self.model_type == ""source_rel_attention"":\n      nbrhd_fn = get_graph_nbrhd_with_rels\n      pad_value = [self.train_graph.rel_pad, self.train_graph.ent_pad]\n    elif self.model_type == ""source_path_attention"":\n      # nbrhd_fn = get_graph_nbrhd_paths\n      nbrhd_fn = lambda x, y, z: get_graph_nbrhd_paths_randwalk(\n          x, y, z, max_length=self.train_graph.max_path_length,\n          max_paths=self.max_neighbors, terminate_prob=0.1,\n          pad=(self.train_graph.rel_pad, self.train_graph.ent_pad)\n      )\n      pad_value = [self.train_graph.rel_pad, self.train_graph.ent_pad] * \\\n        self.train_graph.max_path_length\n    else:\n      nbrhd_fn = get_graph_nbrhd\n      pad_value = self.train_graph.ent_pad\n    if self.model_type == ""distmult"":\n      nbrs_s = np.array([], dtype=np.int)\n      nbrs_candidates = np.array([], dtype=np.int)\n    elif self.model_type in [""source_attention"", ""source_rel_attention"",\n                             ""source_path_attention""]:\n      nbrs_s = sample_or_pad(nbrhd_fn(self.train_graph, s, (s, r, t)),\n                             self.max_neighbors,\n                             pad_value=pad_value)\n      if isinstance(self.train_graph, clueweb_text_graph.CWTextGraph):\n        # this func does paddding in there\n        text_nbrs_s, text_nbrs_s_emb = get_graph_nbrhd_embd_text(\n            self.train_graph, s, self.max_text_neighbors)\n      elif self.max_text_len:\n        text_pad_value = [self.train_graph.ent_pad] + \\\n              [self.train_graph.vocab[self.train_graph.mask_token]] * \\\n              self.max_text_len\n        text_nbrs_s = sample_or_pad(\n            get_graph_nbrhd_text(self.train_graph, s, self.max_text_len),\n            self.max_text_neighbors, pad_value=text_pad_value\n        )\n      nbrs_candidates = np.array([], dtype=np.int)\n    else:\n      nbrs_s = sample_or_pad(nbrhd_fn(self.train_graph, s, (s, r, t)),\n                             self.max_neighbors,\n                             pad_value=pad_value)\n      nbrs_t = sample_or_pad(nbrhd_fn(self.train_graph, t, (s, r, t)),\n                             self.max_neighbors,\n                             pad_value=pad_value)\n      nbrs_negatives = np.array(\n          [sample_or_pad(nbrhd_fn(self.train_graph, cand, (s, r, t)),\n                         self.max_neighbors,\n                         pad_value=pad_value)\n           for cand in negatives]\n      )\n      # import pdb; pdb.set_trace()\n      nbrs_candidates = np.concatenate(\n          (np.expand_dims(nbrs_t, 0), nbrs_negatives), axis=0\n      )\n    if self.mode != ""train"":\n      labels = [t]\n    else:\n      labels = np.zeros(candidates.shape[0], dtype=np.int)\n      labels[0] = 1\n      idx = np.arange(candidates.shape[0])\n      np.random.shuffle(idx)\n      candidates = candidates[idx]\n      if self.model_type == ""attention"":\n        nbrs_candidates = nbrs_candidates[idx]\n      labels = labels[idx]\n    # import ipdb; ipdb.set_trace()\n    if isinstance(self.train_graph, clueweb_text_graph.CWTextGraph):\n      return s, nbrs_s, text_nbrs_s, r, candidates, nbrs_candidates, labels, \\\n             text_nbrs_s_emb\n    elif self.max_text_len:\n      return s, nbrs_s, text_nbrs_s, r, candidates, nbrs_candidates, labels\n    return s, nbrs_s, r, candidates, nbrs_candidates, labels\n\n  def create_dataset_iterator(self, num_parallel=64, prefetch=5,\n                              shuffle_buffer=-1):\n    """"""Create a tf.data.Dataset input pipeline and a dataset iterator.""""""\n    # dataset = tf.data.Dataset.from_generator(\n    #     self._tuple_iterator,\n    #     (tf.int64, tf.int64, tf.int64),\n    #     (tf.TensorShape([]), tf.TensorShape([]), tf.TensorShape([]))\n    #     # (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.int64),\n    #     # (tf.TensorShape([]), tf.TensorShape([self.max_neighbors]),\n    #     #  tf.TensorShape([1]), tf.TensorShape([self.max_negatives + 1])\n    #     #  tf.TensorShape([self.max_negatives + 1, self.max_neighbors]),\n    #     #  tf.TensorShape([self.max_negatives + 1]))\n    # )\n    # if device == ""worker"":\n    #   data_device = tf.device(""/job:worker"")\n    # else:\n    #   data_device = tf.device(""/cpu:0"")\n    # with data_device:\n    dataset = tf.data.Dataset.from_generator(\n        self._tuple_iterator, tf.int64, tf.TensorShape([4])\n    )\n    if self.mode == ""train"":\n      if shuffle_buffer == -1:\n        shuffle_buffer = self.train_graph.tuple_store.shape[0]\n      dataset = dataset.shuffle(shuffle_buffer)\n    # pylint: disable=g-long-lambda\n    output_dtypes = [tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.int64]\n    if isinstance(self.train_graph, clueweb_text_graph.CWTextGraph):\n      output_dtypes.append(tf.int64)\n      output_dtypes.append(tf.float32)\n    elif self.max_text_len:\n      output_dtypes.append(tf.int64)\n    dataset = dataset.map(lambda example_tuple: tf.py_func(\n        self.featurize_each_example, [example_tuple],\n        output_dtypes\n    ), num_parallel_calls=num_parallel)\n    dataset = dataset.repeat(self.num_epochs)\n    dataset = dataset.batch(self.batchsize)\n    dataset = dataset.prefetch(prefetch)\n    self.dataset = dataset\n    _ = self.get_output_shapes()\n    # if self.mode != ""train"":\n    #   self.iterator = dataset.make_initializable_iterator()\n    # else:\n    #   self.iterator = dataset.make_one_shot_iterator()\n\n  def get_output_shapes(self):\n    """"""Set shapes of tensors.""""""\n    if not self.output_shapes:\n      s_shape = tf.TensorShape([None])\n      r_shape = tf.TensorShape([None])\n      candidates_shape = tf.TensorShape([None, None])\n      if self.model_type == ""distmult"":\n        nbrs_s_shape = tf.TensorShape([None])\n        nbrs_candidates_shape = tf.TensorShape([None])\n      elif self.model_type == ""source_attention"":\n        nbrs_s_shape = tf.TensorShape([None, self.max_neighbors])\n        nbrs_candidates_shape = tf.TensorShape([None])\n      elif self.model_type == ""source_rel_attention"":\n        nbrs_s_shape = tf.TensorShape([None, self.max_neighbors, 2])\n        nbrs_candidates_shape = tf.TensorShape([None])\n      elif self.model_type == ""source_path_attention"":\n        nbrs_s_shape = tf.TensorShape(\n            [None, self.max_neighbors, 2*self.train_graph.max_path_length]\n        )\n        nbrs_candidates_shape = tf.TensorShape([None])\n      else:\n        nbrs_s_shape = tf.TensorShape([None, self.max_neighbors])\n        nbrs_candidates_shape = tf.TensorShape([None, None, self.max_neighbors])\n      labels_shape = tf.TensorShape([None, None])\n      if isinstance(self.train_graph, clueweb_text_graph.CWTextGraph):\n        text_nbrs_s_shape = tf.TensorShape([None, self.max_text_neighbors])\n        text_nbrs_s_emb_shape = tf.TensorShape([None, self.max_text_neighbors,\n                                                None])\n        self.output_shapes = (s_shape, nbrs_s_shape, text_nbrs_s_shape, r_shape,\n                              candidates_shape, nbrs_candidates_shape,\n                              labels_shape, text_nbrs_s_emb_shape)\n      elif self.max_text_len:\n        text_nbrs_s_shape = tf.TensorShape([None, self.max_text_neighbors,\n                                            self.max_text_len+1])\n        self.output_shapes = (s_shape, nbrs_s_shape, text_nbrs_s_shape, r_shape,\n                              candidates_shape, nbrs_candidates_shape,\n                              labels_shape)\n      else:\n        self.output_shapes = (s_shape, nbrs_s_shape, r_shape, candidates_shape,\n                              nbrs_candidates_shape, labels_shape)\n    return self.output_shapes\n\n  # def set_input_tensors_shape(self):\n  #   self.input_tensors[0].set_shape([None])\n  #   self.input_tensors[1].set_shape([None, self.max_neighbors])\n  #   self.input_tensors[2].set_shape([None])\n  #   # self.input_tensors[3].set_shape([None, self.max_negatives + 1])\n  #   # self.input_tensors[4].set_shape(\n  #   #     [None, self.max_negatives + 1, self.max_neighbors]\n  #   # )\n  #   # self.input_tensors[5].set_shape([None, self.max_negatives + 1])\n  #   self.input_tensors[3].set_shape([None, None])\n  #   self.input_tensors[4].set_shape(\n  #       [None, None, self.max_neighbors]\n  #   )\n  #   self.input_tensors[5].set_shape([None, None])\n\n  # def get_input_tensors(self):\n  #   if not self.iterator:\n  #     self.create_dataset_iterator()\n  #   if not self.input_tensors:\n  #     self.input_tensors = self.iterator.get_next()\n  #     self.set_input_tensors_shape()\n  #   return self.input_tensors\n\n\n\n'"
research/a2n/encoders.py,117,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Neural network encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport utils\n\n\nclass Encoder(object):\n  """"""Abstract class representing an encoder object.""""""\n\n  def __init__(self):\n    # Collection is used to store tensors useful for analyzing/debugging\n    self.collection = {}\n\n  def add_to_collection(self, name, tensor):\n    self.collection[name] = tensor\n\n  def get_from_collection(self, name):\n    if name not in self.collection:\n      return None\n    return self.collection[name]\n\n  def make_feed_dict(self):\n    raise NotImplementedError(""No make_feed_dict implementation"")\n\n\nclass EmbeddingLookup(Encoder):\n  """"""A simple embedding lookup encoder.""""""\n\n  def __init__(self, emb_dim, is_train, train_dropout=1.0,\n               input_dim=None, embeddings=None, scope=""embeddings"",\n               use_tanh=False, num_ps_tasks=None):\n    super(EmbeddingLookup, self).__init__()\n    self.emb_dim = emb_dim\n    self.is_train = is_train\n    self.dropout = train_dropout\n    self.use_tanh = use_tanh\n    with tf.variable_scope(scope):\n      if embeddings:\n        self.embeddings = embeddings\n      else:\n        partitioner = None\n        if num_ps_tasks:\n          partitioner = tf.min_max_variable_partitioner(\n              max_partitions=num_ps_tasks\n          )\n        self.embeddings = tf.get_variable(\n            ""embeddings"", shape=(input_dim, self.emb_dim),\n            initializer=tf.glorot_uniform_initializer(),\n            partitioner=partitioner\n        )\n    if not embeddings:\n      utils.add_variable_summaries(self.embeddings, scope)\n\n  def lookup(self, inputs):\n    """"""Lookup embeddings for inputs.""""""\n    embedding_layer = tf.nn.embedding_lookup(\n        self.embeddings, inputs\n    )\n    if self.use_tanh:\n      embedding_layer = tf.nn.tanh(embedding_layer)\n    output = tf.cond(\n        self.is_train,\n        lambda: tf.nn.dropout(embedding_layer, self.dropout, name=""dropout""),\n        lambda: embedding_layer\n    )\n    return output\n\n  def make_feed_dict(self):\n    return {}\n\n\nclass NbrAttentionEmbedding(Encoder):\n  """"""Compose embedding by attending to neighbors using bilinear dot product.""""""\n\n  def __init__(self, input_dim, is_train, train_dropout=1.0,\n               emb_dim=None, proj_w=None, scope=""attention""):\n    super(NbrAttentionEmbedding, self).__init__()\n    self.input_dim = input_dim\n    self.scope = scope\n    self.is_train = is_train\n    self.dropout = train_dropout\n    if emb_dim:\n      self.emb_dim = emb_dim\n    else:\n      # Keep embedding dimension same as input node embedding\n      self.emb_dim = self.input_dim\n    with tf.variable_scope(scope):\n      if proj_w:\n        self.proj_w = proj_w\n      else:\n        self.proj_w = tf.get_variable(\n            ""W_attention"", shape=(2 * self.input_dim, self.emb_dim),\n            initializer=tf.glorot_uniform_initializer()\n        )\n    if not proj_w:\n      utils.add_variable_summaries(self.proj_w, self.scope + ""/W_attention"")\n\n  def attend(self, node, neighbors, query, nbr_mask, name=""""):\n    """"""Bilinear attention with a diagonal matrix of query.""""""\n    node_query = tf.expand_dims(node * query, 1)\n    nbr_scores = tf.squeeze(tf.matmul(node_query, neighbors, transpose_b=True),\n                            axis=1)\n    # mask out non-existing neighbors by adding a large negative number\n    nbr_scores += (1 - nbr_mask) * (-1e7)\n    # attention_probs = tf.squeeze(tf.nn.softmax(nbr_scores, axis=-1), axis=-1)\n    attention_probs = tf.nn.softmax(nbr_scores, axis=-1)\n    self.add_to_collection(""attention_probs"", attention_probs)\n    # add summary to monitor attention weights\n    utils.add_histogram_summary(attention_probs,\n                                self.scope + ""/"" + name + ""/attention_probs"")\n    attention_emb = tf.reduce_sum(\n        tf.expand_dims(attention_probs, -1) * neighbors, 1\n    )\n    # Now concat attention_emb with node embedding and then project to emb_dim\n    concat_emb = tf.concat([node, attention_emb], -1)\n    output_emb = tf.matmul(concat_emb, self.proj_w)\n    output = tf.cond(\n        self.is_train,\n        lambda: tf.nn.dropout(output_emb, self.dropout, name=""dropout""),\n        lambda: output_emb\n    )\n    return output\n\n  def make_feed_dict(self):\n    return {}\n\n\nclass SigmoidNbrAttentionEmbedding(Encoder):\n  """"""Compose embedding by attending to neighbors using bilinear dot product.""""""\n\n  def __init__(self, input_dim, is_train, train_dropout=1.0,\n               emb_dim=None, proj_w=None, scope=""attention"", average=False):\n    super(SigmoidNbrAttentionEmbedding, self).__init__()\n    self.input_dim = input_dim\n    self.scope = scope\n    self.is_train = is_train\n    self.dropout = train_dropout\n    self.average = average\n    if emb_dim:\n      self.emb_dim = emb_dim\n    else:\n      # Keep embedding dimension same as input node embedding\n      self.emb_dim = self.input_dim\n    with tf.variable_scope(scope):\n      if proj_w:\n        self.proj_w = proj_w\n      else:\n        self.proj_w = tf.get_variable(\n            ""W_attention"", shape=(2 * self.input_dim, self.emb_dim),\n            initializer=tf.glorot_uniform_initializer()\n        )\n    if not proj_w:\n      utils.add_variable_summaries(self.proj_w, self.scope + ""/W_attention"")\n\n  def attend(self, node, neighbors, query, nbr_mask, name=""""):\n    """"""Bilinear attention with a diagonal matrix of query.""""""\n    node_query = tf.expand_dims(node * query, 1)\n    nbr_scores = tf.squeeze(tf.matmul(node_query, neighbors, transpose_b=True),\n                            axis=1)\n    # mask out non-existing neighbors by adding a large negative number\n    nbr_scores += (1 - nbr_mask) * (-1e7)\n    attention_probs = tf.nn.sigmoid(nbr_scores)\n    self.add_to_collection(""attention_probs"", attention_probs)\n    # add summary to monitor attention weights\n    utils.add_histogram_summary(attention_probs,\n                                self.scope + ""/"" + name + ""/attention_probs"")\n    attention_emb = tf.reduce_sum(\n        tf.expand_dims(attention_probs, -1) * neighbors, 1\n    )\n    if self.average:\n      weights_sum = tf.reduce_sum(attention_probs, axis=-1, keep_dims=True)\n      attention_emb /= tf.maximum(weights_sum, 1e-6)\n    else:\n      # apply tanh to normalize\n      attention_emb = tf.nn.tanh(attention_emb)\n    # Now concat attention_emb with node embedding and then project to emb_dim\n    concat_emb = tf.concat([node, attention_emb], -1)\n    output_emb = tf.matmul(concat_emb, self.proj_w)\n    output = tf.cond(\n        self.is_train,\n        lambda: tf.nn.dropout(output_emb, self.dropout, name=""dropout""),\n        lambda: output_emb\n    )\n    return output\n\n  def make_feed_dict(self):\n    return {}\n\n\nclass CosineNbrAttentionEmbedding(Encoder):\n  """"""Compose embedding by attending to neighbors using simple dot product.\n  Concatenates and projects (node, query_rel) to an embedding that is used to\n  attend to the neighbor embeddings.\n  """"""\n\n  def __init__(self, input_dim, is_train, train_dropout=1.0,\n               emb_dim=None, proj_e=None, proj_w=None, scope=""attention""):\n    super(CosineNbrAttentionEmbedding, self).__init__()\n    self.input_dim = input_dim\n    self.scope = scope\n    self.is_train = is_train\n    self.dropout = train_dropout\n    if emb_dim:\n      self.emb_dim = emb_dim\n    else:\n      # Keep embedding dimension same as input node embedding\n      self.emb_dim = self.input_dim\n    with tf.variable_scope(scope):\n      if proj_e:\n        self.proj_e = proj_e\n      else:\n        self.proj_e = tf.get_variable(\n            ""W_embed"", shape=(2 * self.input_dim, self.emb_dim),\n            initializer=tf.glorot_uniform_initializer()\n        )\n      if proj_w:\n        self.proj_w = proj_w\n      else:\n        self.proj_w = tf.get_variable(\n            ""W_attention"", shape=(2 * self.input_dim, self.emb_dim),\n            initializer=tf.glorot_uniform_initializer()\n        )\n    if not proj_w:\n      utils.add_variable_summaries(self.proj_w, self.scope + ""/W_attention"")\n\n  def attend(self, node, neighbors, query, nbr_mask, name=""""):\n    """"""Bilinear attention with a diagonal matrix of query.""""""\n    node_query = tf.concat([node, query], axis=-1)\n    node_emb = tf.matmul(node_query, self.proj_e)\n    node_emb = tf.expand_dims(node_emb, 1)\n    nbr_scores = tf.squeeze(\n        tf.matmul(node_emb, neighbors, transpose_b=True), axis=1\n    )\n    # mask out non-existing neighbors by adding a large negative number\n    nbr_scores += (1 - nbr_mask) * (-1e7)\n    attention_probs = tf.squeeze(tf.nn.softmax(nbr_scores, axis=-1))\n    self.add_to_collection(""attention_probs"", attention_probs)\n    # add summary to monitor attention weights\n    utils.add_histogram_summary(attention_probs,\n                                self.scope + ""/"" + name + ""/attention_probs"")\n    attention_emb = tf.reduce_sum(\n        tf.expand_dims(attention_probs, -1) * neighbors, 1\n    )\n    # Now concat attention_emb with node embedding and then project to emb_dim\n    concat_emb = tf.concat([node, attention_emb], -1)\n    output_emb = tf.matmul(concat_emb, self.proj_w)\n    output = tf.cond(\n        self.is_train,\n        lambda: tf.nn.dropout(output_emb, self.dropout, name=""dropout""),\n        lambda: output_emb\n    )\n\n    return output\n\n  def make_feed_dict(self):\n    return {}\n\n\nclass RelAttentionEmbedding(NbrAttentionEmbedding):\n  """"""Compose embedding by attending to neighboring relations.""""""\n\n  def get_attention_probs(self, query, neighbors, nbr_mask, name=""""):\n    """"""Get neighbor attention probabilities given query.""""""\n    query = tf.expand_dims(query, 1)\n    nbr_scores = tf.squeeze(tf.matmul(query, neighbors, transpose_b=True),\n                            axis=1)\n    # mask out non-existing neighbors by adding a large negative number\n    nbr_scores += (1 - nbr_mask) * (-1e7)\n    # attention_probs = tf.squeeze(tf.nn.softmax(nbr_scores, axis=-1), axis=-1)\n    attention_probs = tf.nn.softmax(nbr_scores, axis=-1)\n    self.add_to_collection(""attention_probs"", attention_probs)\n    # add summary to monitor attention weights\n    utils.add_histogram_summary(attention_probs,\n                                self.scope + ""/"" + name + ""/attention_probs"")\n    return attention_probs\n\n  def attend(self, node, neighbors, query, nbr_mask, name=""""):\n    """"""Bilinear attention with a diagonal matrix of query.""""""\n    nbrs_rels, nbrs_ents = neighbors\n    attention_probs = self.get_attention_probs(query, nbrs_rels, nbr_mask, name)\n    attention_emb = tf.reduce_sum(\n        tf.expand_dims(attention_probs, -1) * nbrs_ents, 1\n    )\n    # Now concat attention_emb with node embedding and then project to emb_dim\n    concat_emb = tf.concat([node, attention_emb], -1)\n    output_emb = tf.matmul(concat_emb, self.proj_w)\n    output = tf.cond(\n        self.is_train,\n        lambda: tf.nn.dropout(output_emb, self.dropout, name=""dropout""),\n        lambda: output_emb\n    )\n    return output\n\n  def make_feed_dict(self):\n    return {}\n\n\nclass EmbedAlternateSeq(Encoder):\n  """"""Given a sequence of even length, embed pairs of adjacent elements.\n     Input is (batchsize, max_seqlength)\n     seqlength should be even\n     Output is (batchsize, 0.5*max_seqlength, emb_dim)\n\n     This will embed all even and all odd elements of the sequence separately,\n     concatenate the even embedding with the odd embeddings and then project the\n     result to emb_dim.\n     This is useful to project a sequence of [(rel, ent), ...] into a sequence\n     of vectors for each (rel, ent) pair.\n  """"""\n\n  def __init__(self, emb_dim, is_train, train_dropout=1.0, input_dim_a=None,\n               input_dim_b=None, embeddings_a=None, embeddings_b=None,\n               scope=""embed_pairs""):\n    super(EmbedAlternateSeq, self).__init__()\n    self.emb_dim = emb_dim\n    self.is_train = is_train\n    self.dropout = train_dropout\n    with tf.variable_scope(scope):\n      if embeddings_a:\n        self.embeddings_a = embeddings_a\n      else:\n        self.embeddings_a = tf.get_variable(\n            ""embeddings_a"", shape=(input_dim_a, self.emb_dim),\n            initializer=tf.glorot_uniform_initializer()\n        )\n      if embeddings_b:\n        self.embeddings_b = embeddings_b\n      else:\n        self.embeddings_b = tf.get_variable(\n            ""embeddings_b"", shape=(input_dim_b, self.emb_dim),\n            initializer=tf.glorot_uniform_initializer()\n        )\n      self.proj_w = tf.get_variable(\n          ""W_embed_pair"", shape=(2 * self.emb_dim, self.emb_dim),\n          initializer=tf.glorot_uniform_initializer()\n      )\n    if not embeddings_a:\n      utils.add_variable_summaries(self.embeddings_a, scope)\n    if not embeddings_b:\n      utils.add_variable_summaries(self.embeddings_b, scope)\n\n  def _lookup(self, embeddings, inputs):\n    embedding_layer = tf.nn.embedding_lookup(\n        embeddings, inputs\n    )\n    output = tf.cond(\n        self.is_train,\n        lambda: tf.nn.dropout(embedding_layer, self.dropout, name=""dropout""),\n        lambda: embedding_layer\n    )\n    return output\n\n  def embed(self, inputs):\n    """"""Embed the input.""""""\n    seq_embed_a = self._lookup(self.embeddings_a, inputs[:, 0::2])\n    seq_embed_b = self._lookup(self.embeddings_b, inputs[:, 1::2])\n    seq_embeddings = tf.concat([seq_embed_a, seq_embed_b], axis=-1)\n    seq_embeddings_flat = tf.reshape(seq_embeddings, (-1, 2*self.emb_dim))\n    final_embeddings_flat = tf.matmul(seq_embeddings_flat, self.proj_w)\n    final_embeddings = tf.reshape(\n        final_embeddings_flat,\n        tf.concat([tf.shape(seq_embeddings)[:2], [self.emb_dim]], 0)\n    )\n    output = tf.cond(\n        self.is_train,\n        lambda: tf.nn.dropout(final_embeddings, self.dropout, name=""dropout""),\n        lambda: final_embeddings\n    )\n    return output\n\n\nclass AverageSeqEncoder(Encoder):\n  """"""Encode a sequence by averaging the input embeddings.""""""\n\n  def __init__(self, emb_dim, max_seq_len):\n    super(AverageSeqEncoder, self).__init__()\n    self.max_seq_len = max_seq_len\n    self.emb_dim = emb_dim\n\n  def embed(self, inputs, mask):\n    inputs = inputs * tf.expand_dims(mask, -1)\n    counts = tf.reduce_sum(mask, -1, keep_dims=True)\n    sum_inp = tf.reduce_sum(inputs, axis=1)\n    output = sum_inp / tf.maximum(counts, 1)\n    self.add_to_collection(""output"", output)\n    return output\n\n\nclass PositionSumSeqEncoder(Encoder):\n  """"""Encode a sequence by averaging the input embeddings.""""""\n\n  def __init__(self, emb_dim, max_seq_len, scope=""seqmodel""):\n    super(PositionSumSeqEncoder, self).__init__()\n    self.max_seq_len = max_seq_len\n    self.emb_dim = emb_dim\n    with tf.variable_scope(scope):\n      self.pos_w = tf.get_variable(\n          ""position_weights"", shape=(self.max_seq_len, self.emb_dim),\n          initializer=tf.glorot_uniform_initializer()\n      )\n\n  def embed(self, inputs, mask):\n    inputs = inputs * tf.expand_dims(mask, -1)\n    # counts = tf.reduce_sum(mask, -1, keep_dims=True)\n    weights = tf.expand_dims(self.pos_w, 0)\n    output = tf.reduce_sum(inputs * weights, axis=1)\n    # output = sum_inp / tf.maximum(counts, 1)\n    self.add_to_collection(""output"", output)\n    return output\n\n\nclass ConvSeqEncoder(Encoder):\n  """"""Encode a sequence using a Convolution Model.""""""\n\n  def __init__(\n      self, emb_dim, input_dim, max_seq_len, is_train,\n      train_dropout=1.0, filter_widths=(1, 2), num_filters=64, scope=""CNN"",\n      nonlinearity=""tanh""\n  ):\n    super(ConvSeqEncoder, self).__init__()\n    self.max_seq_len = max_seq_len\n    self.emb_dim = emb_dim\n    self.filter_widths = filter_widths\n    self.num_filters = num_filters\n    self.filters = {}\n    self.is_train = is_train\n    self.dropout = train_dropout\n    self.input_dim = input_dim\n    self.nonlinearity = nonlinearity\n    with tf.variable_scope(scope):\n      for filter_width in self.filter_widths:\n        filter_shape = [filter_width, self.input_dim, 1, self.num_filters]\n        w_filter = tf.get_variable(\n            ""W_filterwidth%d"" % filter_width, shape=filter_shape,\n            dtype=tf.float32, initializer=tf.truncated_normal_initializer()\n        )\n        b = tf.get_variable(\n            ""b_filterwidth%d"" % filter_width, shape=[self.num_filters],\n            dtype=tf.float32, initializer=tf.constant_initializer(0.1)\n        )\n        self.filters[filter_width] = (w_filter, b)\n      n_out = self.num_filters * len(self.filter_widths)\n      w_final = tf.get_variable(\n          ""W_affine"", shape=[n_out, self.emb_dim],\n          initializer=tf.glorot_uniform_initializer()\n      )\n      b_final = tf.get_variable(\n          ""b_afffine"", shape=[self.emb_dim],\n          initializer=tf.constant_initializer(0.01)\n      )\n      self.proj_params = (w_final, b_final)\n\n  def embed(self, inputs, mask):\n    """"""Embed sequence using a layer of 2d Convolution.""""""\n    # Create CNN\n    inp = inputs * tf.expand_dims(tf.cast(mask, tf.float32), -1)\n    inp = tf.expand_dims(inp, -1)\n    outputs = []\n    for filter_width in self.filter_widths:\n      w_filter, b = self.filters[filter_width]\n      conv = tf.nn.conv2d(inp, w_filter, strides=[1, 1, 1, 1],\n                          padding=""VALID"", name=""conv"")\n      conv_bias = tf.nn.bias_add(conv, b)\n      # conv_bias = tf.contrib.layers.batch_norm(conv_bias)\n      if self.nonlinearity == ""relu"":\n        conv_bias = tf.nn.relu(conv_bias, name=""relu"")\n      else:\n        conv_bias = tf.nn.tanh(conv_bias, name=""tanh"")\n      pooled = tf.nn.max_pool(\n          conv_bias, ksize=[1, self.max_seq_len - filter_width + 1, 1, 1],\n          strides=[1, 1, 1, 1], padding=""VALID"", name=""max_pool""\n      )\n\n      outputs.append(pooled)\n    n_out = self.num_filters * len(self.filter_widths)\n    h_out = tf.concat(outputs, 3)\n    conv_output = tf.reshape(h_out, [-1, n_out])\n\n    proj_w, proj_b = self.proj_params\n    conv_embedding = tf.nn.xw_plus_b(conv_output, proj_w, proj_b, name=""affine"")\n    out_mask = tf.greater(tf.reduce_sum(mask, -1, keep_dims=True), 0)\n    out_mask = tf.cast(out_mask, tf.float32)\n    conv_embedding = conv_embedding * out_mask\n    output = tf.cond(\n        self.is_train,\n        lambda: tf.nn.dropout(conv_embedding, self.dropout, name=""dropout""),\n        lambda: conv_embedding\n    )\n    return output\n\n\nclass ConvTextEncoder(Encoder):\n  """"""Encode a text sequence using a convolution model.""""""\n\n  def __init__(\n      self, vocab_size, word_emb_dim, output_emb_dim, max_seq_len, is_train,\n      train_dropout=1.0, filter_widths=(3, 5, 7), num_filters=64,\n      scope=""TextCNN"", num_ps_tasks=None, nonlinearity=""tanh""\n  ):\n    super(ConvTextEncoder, self).__init__()\n    self.vocab_size = vocab_size\n    self.word_emb_dim = word_emb_dim\n    self.output_emb_dim = output_emb_dim\n    self.is_train = is_train\n    self.dropout = train_dropout\n    self.max_seq_len = max_seq_len\n    with tf.variable_scope(scope):\n      self.word_embedding_encoder = EmbeddingLookup(\n          word_emb_dim, is_train, train_dropout=train_dropout,\n          input_dim=vocab_size, num_ps_tasks=num_ps_tasks\n      )\n      self.cnn_encoder = ConvSeqEncoder(\n          output_emb_dim, word_emb_dim, max_seq_len, is_train,\n          train_dropout=train_dropout, filter_widths=filter_widths,\n          num_filters=num_filters, nonlinearity=nonlinearity\n      )\n\n  def embed(self, inputs, mask):\n    word_embeddings = self.word_embedding_encoder.lookup(inputs)\n    output = self.cnn_encoder.embed(word_embeddings, mask)\n    return output\n\n'"
research/a2n/generate_random_graph.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Generate a random graph for testing.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nimport numpy as np\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""output"", None, ""Path to output file"")\nflags.DEFINE_integer(""num_entities"", 20, ""Number of entities in graph"")\nflags.DEFINE_integer(""num_relations"", 5, ""Number of relation types in graph"")\nflags.DEFINE_integer(""num_edges"", 20, ""Number of edges in graph"")\n\nflags.mark_flag_as_required(""output"")\n\n\ndef generate_graph():\n  with open(FLAGS.output, ""w+"") as f:\n    for _ in range(FLAGS.num_edges):\n      s = np.random.randint(FLAGS.num_entities)\n      r = np.random.randint(FLAGS.num_relations)\n      t = np.random.randint(FLAGS.num_entities)\n      while t != s:\n        t = np.random.randint(FLAGS.num_entities)\n      f.write(""%d\\t%d\\t%d\\n"" % (s, r, t))\n\n\ndef main(argv):\n  del argv\n  generate_graph()\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
research/a2n/graph.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A class to read and store a graph of tuples in memory.\n\nThe graph file is a tab separated file with each line containing a tuple.\nThe first element of the tuple is source entity, second elemnt is the relation\nand the third element is the target entity.\n\nGraph.kg_data stores the graph data in a hash table with source entity as key.\nIt has the following structure:\n{s: {t1: [r1, ...], ...}}\nSo, kg_data[e1][e2] is a list of all relations filling in (e1, ?, e2).\n\nIf Graph.add_reverse_graph is True then reverse tuples are stored in\nGraph.reverse_kg_data\n\nIf Graph.add_inverse_edge is True then for every relation r, the inverse of that\nrelation is also added to the graph, so for a (s, r, t) tuple in graph the\ntuple (t, inv_r, s) is also added.\nNote that only one of add_reverse_graph or add_inverse_edge should be true for\na particular graph.\n\nGraph.tuple_store contains the graph tuple stored as (num_tuples, 3) array.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom collections import defaultdict\n\nimport csv\nfrom absl import logging\nimport numpy as np\n\n\nclass Graph(object):\n  """"""\n    Read a knowledge graph to memory\n  """"""\n\n  def __init__(\n      self, kg_file, entity_vocab=False, relation_vocab=False,\n      add_reverse_graph=False, add_inverse_edge=False, mode=""train"",\n      max_path_length=None, **kwargs\n  ):\n    """"""Init Graph class.""""""\n    del kwargs\n    if add_reverse_graph and add_inverse_edge:\n      raise ValueError(\n          ""Only one of add_reverse_graph or add_inverse_edge should be used""\n      )\n    self._raw_kg_file = kg_file\n    self.add_reverse_graph = add_reverse_graph\n    self.add_inverse_edge = add_inverse_edge\n    if add_inverse_edge:\n      self.inverse_relation_prefix = ""INVERSE:""\n    # vocab maps from name to integer id\n    if entity_vocab:\n      self.entity_vocab = entity_vocab\n    else:\n      self.entity_vocab = {}\n    if relation_vocab:\n      self.relation_vocab = relation_vocab\n    else:\n      self.relation_vocab = {}\n    self.ent_vocab_size = len(self.entity_vocab)\n    self.rel_vocab_size = len(self.relation_vocab)\n    self._num_edges = 0\n\n    self.kg_data = defaultdict(dict)\n    self.next_edges = defaultdict(set)\n    if self.add_reverse_graph:\n      self.reverse_kg_data = defaultdict(dict)\n      self.reverse_next_edges = defaultdict(set)\n    self.entity_pad_token = ""ePAD""\n    self.relation_pad_token = ""rPAD""\n    # self.no_op_relation = ""NO_OP""\n    self.max_kg_relations = None\n    # self.max_ent_sampled = max_ent_sampled\n    self.mode = mode\n    self.read_graph(mode)\n    # inverse vocab maps from integer id to name\n    self.inverse_relation_vocab = {\n        v: k for k, v in self.relation_vocab.iteritems()\n    }\n    self.inverse_entity_vocab = {v: k for k, v in self.entity_vocab.iteritems()}\n    if mode == ""train"":\n      self.all_entities = set(self.entity_vocab.values())\n      self.max_neighbors = self._max_neighbors()\n    self.all_reachable_e2 = defaultdict(set)\n    self.all_reachable_e2_reverse = defaultdict(set)\n    self.tuple_store = []\n    self.create_tuple_store()\n    logging.debug(""Graph data read"")\n    logging.info(""Entity vocab: %d"", self.ent_vocab_size)\n    logging.info(""Relation vocab: %d"", self.rel_vocab_size)\n    logging.info(""Number of edges: %d"", self._num_edges)\n    self.max_path_length = max_path_length\n    # if mode == ""train"" and self.max_path_length:\n    #   self.store_paths()\n    # num_reachable_e2s = map(len, self.all_reachable_e2.values())\n    # logging.debug(""Mean target e2: %.2f +- %.2f and Max target e2: %d"" % (\n    #     np.mean(num_reachable_e2s), np.std(num_reachable_e2s),\n    #     max(num_reachable_e2s)\n    # ))\n\n  def get_inverse_relation_from_name(self, rname):\n    """"""Given a relation name, get the name of inverse relation.""""""\n    if rname.startswith(self.inverse_relation_prefix):\n      inv_rname = rname.strip(self.inverse_relation_prefix)\n    else:\n      inv_rname = self.inverse_relation_prefix + rname\n    return inv_rname\n\n  def get_inverse_relation_from_id(self, r):\n    """"""Given a relation id (from vocab), get the id of the inverse relation.""""""\n    rname = self.inverse_relation_vocab[r]\n    inv_rname = self.get_inverse_relation_from_name(rname)\n    inv_r = self.relation_vocab[inv_rname]\n    return inv_r\n\n  def _max_neighbors(self):\n    """"""Helper to find neighbors statistics.""""""\n    max_nbrs = 0\n    num_nbrs = []\n    max_ent = None\n    for e1 in self.kg_data:\n      nbrs = set(self.kg_data[e1].keys())\n      if self.add_reverse_graph:\n        nbrs |= set(self.reverse_kg_data[e1].keys())\n      if len(nbrs) > max_nbrs:\n        max_nbrs = len(nbrs)\n        max_ent = self.inverse_entity_vocab[e1]\n      num_nbrs.append(len(nbrs))\n    logging.info(""Average number of neighbors: %.2f +- %.2f"",\n                 np.mean(num_nbrs), np.std(num_nbrs))\n    logging.info(""Max neighbors %d of entity %s"", max_nbrs, max_ent)\n    return max_nbrs\n\n  def read_graph(self, mode=""train""):\n    """"""Read the knowledge graph.""""""\n    logging.debug(""Reading graph from %s"", self._raw_kg_file)\n    with open(self._raw_kg_file, ""r"") as f:\n      kg_file = csv.reader(f, delimiter=""\\t"")\n      skipped = 0\n      for line in kg_file:\n        e1 = line[0].strip()\n        if e1 not in self.entity_vocab:\n          if mode != ""train"":\n            skipped += 1\n            continue\n          self.entity_vocab[e1] = self.ent_vocab_size\n          self.ent_vocab_size += 1\n        e1 = self.entity_vocab[e1]\n\n        r = line[1].strip()\n        if r not in self.relation_vocab:\n          if mode != ""train"":\n            skipped += 1\n            continue\n          self.relation_vocab[r] = self.rel_vocab_size\n          self.rel_vocab_size += 1\n        if self.add_inverse_edge:\n          inv_r = self.inverse_relation_prefix + r\n          if inv_r not in self.relation_vocab:\n            self.relation_vocab[inv_r] = self.rel_vocab_size\n            self.rel_vocab_size += 1\n          inv_r = self.relation_vocab[inv_r]\n        r = self.relation_vocab[r]\n\n        e2 = line[2].strip()\n        if e2 not in self.entity_vocab:\n          if mode != ""train"":\n            skipped += 1\n            continue\n          self.entity_vocab[e2] = self.ent_vocab_size\n          self.ent_vocab_size += 1\n        e2 = self.entity_vocab[e2]\n\n        if e2 not in self.kg_data[e1]:\n          self.kg_data[e1][e2] = []\n        self.kg_data[e1][e2].append(r)\n        self.next_edges[e1].add((r, e2))\n        if self.add_inverse_edge:\n          if e1 not in self.kg_data[e2]:\n            self.kg_data[e2][e1] = []\n          self.kg_data[e2][e1].append(inv_r)\n          self.next_edges[e2].add((inv_r, e1))\n          self._num_edges += 1\n        if self.add_reverse_graph:\n          if e1 not in self.reverse_kg_data[e2]:\n            self.reverse_kg_data[e2][e1] = []\n          self.reverse_kg_data[e2][e1].append(r)\n          self.reverse_next_edges[e2].add((r, e1))\n        # if self.mode != \'train\':\n        #     self.tuple_store.append((e1, r, e2))\n\n        self._num_edges += 1\n\n    logging.info(""Skipped %d tuples in mode %s"", skipped, mode)\n\n    # if mode == ""train"" and self.no_op_relation not in self.relation_vocab:\n    #   self.relation_vocab[self.no_op_relation] = self.rV\n    #   self.rV += 1\n    if mode == ""train"" and self.entity_pad_token not in self.entity_vocab:\n      self.entity_vocab[self.entity_pad_token] = self.ent_vocab_size\n      self.ent_vocab_size += 1\n    if mode == ""train"" and self.relation_pad_token not in self.relation_vocab:\n      self.relation_vocab[self.relation_pad_token] = self.rel_vocab_size\n      self.rel_vocab_size += 1\n\n    self.ent_pad = self.entity_vocab[self.entity_pad_token]\n    self.rel_pad = self.relation_vocab[self.relation_pad_token]\n\n    # if self.mode != \'train\':\n    #     self.tuple_store = np.array(self.tuple_store)\n    # self.all_reachable_e2 = defaultdict(set)\n\n    if not self.max_kg_relations:\n      max_out = 0\n      for e1 in self.kg_data:\n        nout = 0\n        for e2 in self.kg_data[e1]:\n          nout += len(self.kg_data[e1][e2])\n        max_out = max(max_out, nout)\n      logging.info(""Max outgoing rels kg: %d"", max_out)\n      self.max_kg_relations = max_out\n\n  def create_tuple_store(self, train_graph=None, only_one_hop=False):\n    """"""Create a numpy store for training or validation tuples.""""""\n    self.tuple_store = []\n    skipped = 0\n    for e1 in self.kg_data:\n      for e2 in self.kg_data[e1]:\n        if only_one_hop and train_graph:\n          reachable = e1 in train_graph.kg_data and \\\n                      e2 in train_graph.kg_data[e1]\n          reachable = reachable or (\n              e1 in train_graph.kg_text_data and \\\n              e2 in train_graph.kg_text_data[e1]\n          )\n        else:\n          reachable = True\n        if reachable:\n          for r in self.kg_data[e1][e2]:\n            self.tuple_store.append((e1, r, e2))\n            # if self.mode == ""train"":\n            self.all_reachable_e2[(e1, r)].add(e2)\n            # if self.add_reverse_graph:\n            self.all_reachable_e2_reverse[(e2, r)].add(e1)\n        else:\n          skipped += len(self.kg_data[e1][e2])\n    self.tuple_store = np.array(self.tuple_store)\n    logging.info(""Unreachable %s tuples skipped: %d"", self.mode, skipped)\n    logging.info(""Remaining %s tuples: %d"", self.mode,\n                 self.tuple_store.shape[0])\n\n  def store_paths(self):\n    """"""Find and store all paths from all entities upto max_path_length.""""""\n    self.paths = [defaultdict(list) for _ in range(self.max_path_length)]\n    # Add all paths of length 1\n    for e in self.kg_data:\n      self.paths[0][e] = [""%d %d"" % (r, e2) for e2 in self.kg_data[e]\n                          for r in self.kg_data[e][e2]]\n    # Add all paths of length > 1\n    for i in range(1, self.max_path_length):\n      for e in self.kg_data:\n        for path in self.paths[i-1][e]:\n          all_prev_e = map(int, path.strip().split("" "")[1::2])\n          last_e = all_prev_e[-1]\n          # last_r = int(path.strip().split("" "")[-2])\n          if last_e not in self.kg_data:\n            continue\n          new_paths = [path + "" %d %d"" % (r, e2) for e2 in self.kg_data[last_e]\n                       for r in self.kg_data[last_e][e2]\n                       if e2 not in all_prev_e]\n          self.paths[i][e] += new_paths\n    # import pdb; pdb.set_trace()\n\n  def get_next_kg_actions(\n      self, current_ents, query_rels, max_kg_relations=None, mode=""train"",\n      all_answers=None\n  ):\n    """"""Get all next actions (edge, next_entity) from current nodes.""""""\n    if not max_kg_relations:\n      max_kg_relations = self.max_kg_relations\n    actions = np.ones((current_ents.shape[0], max_kg_relations, 2),\n                      dtype=np.int32)\n    actions[:, :, 0] *= self.entity_vocab[self.entity_pad_token]\n    actions[:, :, 1] *= self.relation_vocab[self.relation_pad_token]\n    for i in range(current_ents.shape[0]):\n      e1 = current_ents[i]\n      # actions[i, 0, 0] = e1\n      # actions[i, 0, 1] = self.relation_vocab[self.no_op_relation]\n      naction = 0\n      for e2 in self.kg_data[e1]:\n        if naction == max_kg_relations:\n          break\n        for r in self.kg_data[e1][e2]:\n          if naction == max_kg_relations:\n            break\n          # if r == query_rels[i] and e2 == answers[i]:\n          if mode == ""train"" and r == query_rels[i] and e2 in all_answers[i]:\n            actions[i, naction, 0] = self.ePAD\n            actions[i, naction, 1] = self.rPAD\n          else:\n            actions[i, naction, 0] = e2\n            actions[i, naction, 1] = r\n          naction += 1\n\n    return actions\n\n  def get_next_kg_actions_sampled(\n      self, current_ents, all_answers, query_rels, all_negatives,\n      max_kg_relations=None\n  ):\n    """"""Sample next actions for training.""""""\n    if not max_kg_relations:\n      max_kg_relations = self.max_kg_relations\n    actions = np.ones((current_ents.shape[0], max_kg_relations, 2),\n                      dtype=np.int32)\n    actions[:, :, 0] *= self.entity_vocab[self.entity_pad_token]\n    actions[:, :, 1] *= self.relation_vocab[self.relation_pad_token]\n    for i in range(current_ents.shape[0]):\n      e1 = current_ents[i]\n      actions[i, 0, 0] = e1\n      # actions[i, 0, 1] = self.relation_vocab[self.no_op_relation]\n      nactions = 0\n      ents = all_answers[i] + all_negatives[i]\n      np.random.shuffle(ents)\n      answers = set(all_answers[i])\n      # negatives = set(all_negatives[i])\n      nrels = max_kg_relations / len(ents)\n      for e2 in ents:\n        # if nactions == self.max_kg_relations:\n        #     break\n        if nactions >= max_kg_relations:\n          logging.info(""reached max kg relations"")\n          break\n        # if nactions >= 0.5 * self.max_kg_relations:\n        #     # import ipdb; ipdb.set_trace()\n        #     break\n        # import ipdb; ipdb.set_trace()\n        if e2 in self.kg_data[e1]:\n          rels = self.kg_data[e1][e2]\n        else:\n          rels = []\n        if len(rels) > nrels:\n          rels = np.random.choice(rels, size=nrels, replace=False)\n        # if e2 in answers:\n          # take all positive relations\n        for rel in rels:\n          # if nactions >= 0.5 * self.max_kg_relations:\n          #     break\n          if nactions >= max_kg_relations:\n            logging.info(""reached max kg relations"")\n            break\n          if rel == query_rels[i] and e2 in answers:\n            actions[i, nactions, 0] = self.ePAD\n            actions[i, nactions, 1] = self.rPAD\n          else:\n            actions[i, nactions, 0] = e2\n            actions[i, nactions, 1] = rel\n          nactions += 1\n    return actions\n'"
research/a2n/losses.py,2,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Implementation of various loss functions.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef softmax_crossentropy(logits, labels):\n  batch_loss = tf.nn.softmax_cross_entropy_with_logits(\n      logits=logits, labels=labels\n  )\n  return tf.reduce_mean(batch_loss)\n\n'"
research/a2n/metrics.py,27,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Evaluation Metrics.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef mrr(scores, candidates, labels):\n  """"""Compute Mean Reciprocal Rank of labels in scores.\n\n  Args:\n    scores (tf.Tensor): batchsize, max_candidates tensor of scores\n    candidates (tf.Tensor): batchsize, max_candidates tensor of candidate ids\n    labels (tf.Tensor): batchsize tensor of ground truth labels\n  Returns:\n    rr (tf.Tensor): batchsize tensor of Reciprocal Rank values\n  """"""\n  _, top_score_ids = tf.nn.top_k(scores, k=tf.shape(scores)[-1])\n  batch_indices = tf.cumsum(\n      tf.ones_like(candidates, dtype=tf.int32), axis=0, exclusive=True\n  )\n  indices = tf.concat([tf.expand_dims(batch_indices, axis=-1),\n                       tf.expand_dims(top_score_ids, -1)], -1)\n  sorted_candidates = tf.gather_nd(candidates, indices)\n  # label_ids = tf.expand_dims(tf.argmax(labels, axis=1), 1)\n  label_rank_indices = tf.where(\n      tf.equal(sorted_candidates, labels)\n  )\n  # +1 because top rank should be 1 not 0\n  ranks = label_rank_indices[:, 1] + 1\n  rr = 1.0 / tf.cast(ranks, tf.float32)\n  return rr  # , ranks, label_rank_indices, sorted_candidates, top_score_ids\n\n\ndef hits_at_k(scores, candidates, labels, k=10):\n  """"""Compute hits@k.\n\n  Args:\n    scores (tf.Tensor): batchsize, max_candidates tensor of scores\n    candidates (tf.Tensor): batchsize, max_candidates tensor of candidate ids\n    labels (tf.Tensor): batchsize tensor of ground truth labels\n    k: values of k to evaluate hits@k\n  Returns:\n    rr (tf.Tensor): batchsize tensor of Reciprocal Rank values\n  """"""\n  _, top_score_ids = tf.nn.top_k(scores, k=k)\n  batch_indices = tf.cumsum(\n      tf.ones_like(top_score_ids, dtype=tf.int32), axis=0, exclusive=True\n  )\n  indices = tf.concat([tf.expand_dims(batch_indices, axis=-1),\n                       tf.expand_dims(top_score_ids, -1)], -1)\n  sorted_candidates = tf.gather_nd(candidates, indices)\n  # label_ids = tf.expand_dims(tf.argmax(labels, axis=1), 1)\n  hits = tf.reduce_max(\n      tf.cast(tf.equal(sorted_candidates, labels), tf.float32), 1\n  )\n  return hits  # , sorted_candidates, top_score_ids\n'"
research/a2n/models.py,59,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Models for the knowledge graph.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport encoders\nimport tensorflow as tf\n\n\ndef attention_kbc_model(config, train_graph, is_train_ph, input_tensors):\n  """"""Use attention model to score candidates for kbc.""""""\n  # with tf.variable_scope(scope, reuse=reuse):\n    # if reuse:\n    #   tf_scope.reuse_variables()\n  s, nbrs_s, r, candidates, nbrs_candidates = input_tensors\n  model = {}\n  entity_encoder = encoders.EmbeddingLookup(\n      config.emb_dim, is_train_ph, train_dropout=config.entity_encoder_dropout,\n      input_dim=train_graph.ent_vocab_size,\n      scope=""entity_embeddings""\n  )\n  model[""entity_encoder""] = entity_encoder\n  if config.use_separate_attention_emb:\n    init_entity_encoder = encoders.EmbeddingLookup(\n        config.emb_dim, is_train_ph,\n        train_dropout=config.init_entity_encoder_dropout,\n        input_dim=train_graph.ent_vocab_size, scope=""init_entity_embeddings""\n    )\n  else:\n    init_entity_encoder = entity_encoder\n  model[""init_entity_encoder""] = init_entity_encoder\n  relation_encoder = encoders.EmbeddingLookup(\n      config.emb_dim, is_train_ph,\n      train_dropout=config.relation_encoder_dropout,\n      input_dim=train_graph.rel_vocab_size, scope=""relation_embeddings""\n  )\n  model[""relation_encoder""] = relation_encoder\n\n  attention_encoder = encoders.NbrAttentionEmbedding(\n      config.emb_dim, is_train_ph,\n      train_dropout=config.attention_encoder_dropout,\n      emb_dim=config.emb_dim, scope=""attention""\n  )\n  model[""attention_encoder""] = attention_encoder\n\n  source_emb = entity_encoder.lookup(s)\n  nbrs_source_emb = init_entity_encoder.lookup(nbrs_s)\n  relation_emb = relation_encoder.lookup(r)\n  candidates_emb = entity_encoder.lookup(candidates)\n  candidates_emb_flat = tf.reshape(\n      candidates_emb, (-1, entity_encoder.emb_dim)\n  )\n  nbrs_candidates_emb = init_entity_encoder.lookup(nbrs_candidates)\n  if config.max_neighbors:\n    max_neighbors = config.max_neighbors\n  else:\n    max_neighbors = train_graph.max_neighbors\n  nbrs_candidates_emb_flat = tf.reshape(\n      nbrs_candidates_emb, (-1, max_neighbors, init_entity_encoder.emb_dim)\n  )\n  relation_emb_expand = tf.expand_dims(relation_emb, 1)\n  relation_emb_tile = tf.tile(\n      relation_emb_expand,  # [1, config.max_negatives+1, 1]\n      tf.concat([[1], tf.shape(candidates)[-1:], [1]], 0)\n  )\n  relation_emb_tile_flat = tf.reshape(relation_emb_tile, [-1, config.emb_dim])\n\n  # Perform attention to construct feature vectors\n  mask_nbrs_s = tf.cast(tf.not_equal(nbrs_s, train_graph.ent_pad), tf.float32)\n  mask_nbrs_candidates = tf.cast(\n      tf.reshape(tf.not_equal(nbrs_candidates, train_graph.ent_pad),\n                 (-1, max_neighbors)),\n      tf.float32\n  )\n\n  source_vec = attention_encoder.attend(\n      source_emb, nbrs_source_emb, relation_emb, mask_nbrs_s, name=""source""\n  )\n  # source_vec_tile = tf.tile(\n  #     tf.expand_dims(source_vec, 1), [1, config.max_negatives + 1, 1]\n  # )\n  candidates_vec = attention_encoder.attend(\n      candidates_emb_flat, nbrs_candidates_emb_flat, relation_emb_tile_flat,\n      mask_nbrs_candidates, name=""candidates""\n  )\n  candidates_vec = tf.reshape(\n      candidates_vec,\n      tf.concat([tf.shape(candidates_emb)[:2], [config.emb_dim]], 0)\n  )\n\n  # Score candidates\n  source_dot_query = source_vec * relation_emb\n  scores = tf.squeeze(\n      tf.matmul(\n          tf.expand_dims(source_dot_query, 1), candidates_vec, transpose_b=True\n      ),\n      axis=1\n  )\n\n  # loss = losses.softmax_crossentropy(logits=candidates_scores, labels=labels)\n\n  return scores, model\n\n\ndef source_attention_kbc_model(\n    config, train_graph, is_train_ph, input_tensors,\n    model_type=""source_attention""\n):\n  """"""Use attention model to score candidates for kbc.""""""\n  # with tf.variable_scope(scope, reuse=reuse):\n    # if reuse:\n    #   tf_scope.reuse_variables()\n  if config.clueweb_data:\n    s, nbrs_s, text_nbrs_s, text_nbrs_s_emb, r, candidates = input_tensors\n  elif config.text_kg_file:\n    s, nbrs_s, text_nbrs_s, r, candidates = input_tensors\n  else:\n    s, nbrs_s, r, candidates = input_tensors\n  model = {}\n  entity_encoder = encoders.EmbeddingLookup(\n      config.emb_dim, is_train_ph, train_dropout=config.entity_encoder_dropout,\n      input_dim=train_graph.ent_vocab_size,\n      scope=""entity_embeddings"", num_ps_tasks=None\n  )\n  model[""entity_encoder""] = entity_encoder\n  relation_encoder = encoders.EmbeddingLookup(\n      config.emb_dim, is_train_ph,\n      train_dropout=config.relation_encoder_dropout,\n      input_dim=train_graph.rel_vocab_size, scope=""relation_embeddings""\n  )\n  model[""relation_encoder""] = relation_encoder\n\n  if config.attention_type == ""bilinear"":\n    attention_encoder = encoders.NbrAttentionEmbedding(\n        config.emb_dim, is_train_ph,\n        train_dropout=config.attention_encoder_dropout,\n        emb_dim=config.emb_dim, scope=""attention""\n    )\n  elif config.attention_type == ""sigmoid_bilinear"":\n    attention_encoder = encoders.SigmoidNbrAttentionEmbedding(\n        config.emb_dim, is_train_ph,\n        train_dropout=config.attention_encoder_dropout,\n        emb_dim=config.emb_dim, scope=""attention"",\n        average=False\n    )\n  elif config.attention_type == ""sigmoid_avg_bilinear"":\n    attention_encoder = encoders.SigmoidNbrAttentionEmbedding(\n        config.emb_dim, is_train_ph,\n        train_dropout=config.attention_encoder_dropout,\n        emb_dim=config.emb_dim, scope=""attention"",\n        average=True\n    )\n  elif config.attention_type == ""cosine"":\n    attention_encoder = encoders.CosineNbrAttentionEmbedding(\n        config.emb_dim, is_train_ph,\n        train_dropout=config.attention_encoder_dropout,\n        emb_dim=config.emb_dim, scope=""attention""\n    )\n  elif config.attention_type == ""relation"":\n    attention_relation_encoder = encoders.EmbeddingLookup(\n        config.emb_dim, is_train_ph,\n        train_dropout=config.relation_encoder_dropout,\n        input_dim=train_graph.rel_vocab_size,\n        scope=""attention_relation_embeddings""\n    )\n    model[""attention_relation_encoder""] = attention_relation_encoder\n    attention_encoder = encoders.RelAttentionEmbedding(\n        config.emb_dim, is_train_ph,\n        train_dropout=config.attention_encoder_dropout,\n        emb_dim=config.emb_dim, scope=""attention""\n    )\n\n  model[""attention_encoder""] = attention_encoder\n\n  source_emb = entity_encoder.lookup(s)\n  relation_emb = relation_encoder.lookup(r)\n  candidates_emb = entity_encoder.lookup(candidates)\n  if model_type == ""source_rel_attention"":\n    if config.attention_type == ""relation"":\n      # nbrs_rel_emb = relation_encoder.lookup(nbrs_s[:, :, 0])\n      nbrs_rel_emb = attention_relation_encoder.lookup(nbrs_s[:, :, 0])\n      nbrs_ent_emb = entity_encoder.lookup(nbrs_s[:, :, 1])\n      nbrs_source_emb = (nbrs_rel_emb, nbrs_ent_emb)\n    else:\n      if config.use_separate_attention_emb:\n        nbrs_encoder = encoders.EmbedAlternateSeq(\n            config.emb_dim, is_train_ph,\n            train_dropout=config.init_entity_encoder_dropout,\n            input_dim_a=train_graph.rel_vocab_size,\n            input_dim_b=train_graph.ent_vocab_size\n        )\n      else:\n        nbrs_encoder = encoders.EmbedAlternateSeq(\n            config.emb_dim, is_train_ph,\n            train_dropout=config.init_entity_encoder_dropout,\n            embeddings_a=relation_encoder.embeddings,\n            embeddings_b=entity_encoder.embeddings)\n      model[""nbrs_encoder""] = nbrs_encoder\n      nbrs_s_flat = tf.reshape(nbrs_s, (-1, 2))\n      nbrs_source_emb_flat = tf.squeeze(nbrs_encoder.embed(nbrs_s_flat), axis=1)\n      if config.max_neighbors:\n        max_neighbors = config.max_neighbors\n      else:\n        max_neighbors = train_graph.ent_vocab_size\n      nbrs_source_emb = tf.reshape(nbrs_source_emb_flat,\n                                   (-1, max_neighbors, config.emb_dim))\n    mask_nbrs_s = tf.cast(\n        tf.not_equal(nbrs_s[:, :, 1], train_graph.ent_pad), tf.float32\n    )\n  elif model_type == ""source_path_attention"":\n    if config.use_separate_attention_emb:\n      nbrs_encoder = encoders.EmbedAlternateSeq(\n          config.emb_dim, is_train_ph,\n          train_dropout=config.init_entity_encoder_dropout,\n          input_dim_a=train_graph.rel_vocab_size,\n          input_dim_b=train_graph.ent_vocab_size)\n    else:\n      nbrs_encoder = encoders.EmbedAlternateSeq(\n          config.emb_dim, is_train_ph,\n          train_dropout=config.init_entity_encoder_dropout,\n          embeddings_a=relation_encoder.embeddings,\n          embeddings_b=entity_encoder.embeddings)\n    model[""nbrs_encoder""] = nbrs_encoder\n    nbrs_s_flat = tf.reshape(nbrs_s, (-1, config.max_path_length * 2))\n    nbrs_source_emb_flat = nbrs_encoder.embed(nbrs_s_flat)\n    # path_encoder = encoders.AverageSeqEncoder(config.emb_dim,\n    #                                           config.max_path_length)\n    path_encoder = encoders.PositionSumSeqEncoder(config.emb_dim,\n                                                  config.max_path_length)\n    model[""path_encoder""] = path_encoder\n    path_mask = tf.cast(\n        tf.not_equal(\n            tf.reshape(nbrs_s[:, :, 1::2], (-1, config.max_path_length)),\n            train_graph.ent_pad\n        ), tf.float32\n    )\n    path_embeddings = path_encoder.embed(nbrs_source_emb_flat, path_mask)\n    if config.max_neighbors:\n      max_neighbors = config.max_neighbors\n    else:\n      max_neighbors = train_graph.ent_vocab_size\n    nbrs_source_emb = tf.reshape(path_embeddings,\n                                 (-1, max_neighbors, config.emb_dim))\n    mask_nbrs_s = tf.cast(\n        tf.not_equal(nbrs_s[:, :, 1], train_graph.ent_pad), tf.float32\n    )\n  else:\n    if config.use_separate_attention_emb:\n      init_entity_encoder = encoders.EmbeddingLookup(\n          config.emb_dim, is_train_ph,\n          train_dropout=config.init_entity_encoder_dropout,\n          input_dim=train_graph.ent_vocab_size, scope=""init_entity_embeddings""\n      )\n    else:\n      init_entity_encoder = entity_encoder\n    model[""init_entity_encoder""] = init_entity_encoder\n    nbrs_source_emb = init_entity_encoder.lookup(nbrs_s)\n    mask_nbrs_s = tf.cast(tf.not_equal(nbrs_s, train_graph.ent_pad), tf.float32)\n\n  if config.text_kg_file or config.clueweb_data:\n    if config.text_kg_file:\n      max_text_len = config.max_text_len or train_graph.max_text_len\n      text_encoder = encoders.ConvTextEncoder(\n          train_graph.word_vocab_size, config.emb_dim, config.emb_dim,\n          max_text_len, is_train_ph,\n          train_dropout=config.text_encoder_dropout,\n          filter_widths=map(int, config.text_encoder_filter_widths),\n          num_filters=config.text_encoder_num_filters,\n          nonlinearity=config.text_encoder_nonlinearity,\n          num_ps_tasks=None\n      )\n      model[""text_encoder""] = text_encoder\n      text_ents = text_nbrs_s[:, :, 0]\n      text_rels = text_nbrs_s[:, :, 1:]\n      text_rels_flat = tf.reshape(text_rels, (-1, max_text_len))\n      text_mask = tf.cast(\n          tf.not_equal(text_rels_flat,\n                       train_graph.vocab[train_graph.mask_token]),\n          tf.float32\n      )\n      text_emb_flat = text_encoder.embed(text_rels_flat, text_mask)\n      text_emb_dim = config.emb_dim\n    else:\n      text_ents = text_nbrs_s\n      text_emb_flat = tf.reshape(text_nbrs_s_emb, (-1, config.text_emb_dim))\n      text_emb_dim = config.text_emb_dim\n    text_rels_mask = tf.cast(\n        tf.not_equal(text_ents, train_graph.ent_pad), tf.float32\n    )\n    mask_nbrs_s = tf.concat([mask_nbrs_s, text_rels_mask], axis=1)\n    # text_ent_emb = entity_encoder.lookup(text_ents) * tf.expand_dims(\n    #     text_rels_mask, -1)\n    text_ent_emb = entity_encoder.lookup(text_ents)\n    if config.attention_type == ""relation"":\n      text_final_emb = tf.reshape(\n          text_emb_flat, (-1, config.max_text_neighbors, text_emb_dim)\n      )\n      nbrs_rel_emb, nbrs_ent_emb = nbrs_source_emb\n      all_nbrs_rel_emb = tf.concat([nbrs_rel_emb, text_final_emb], axis=1)\n      all_nbrs_ent_emb = tf.concat([nbrs_ent_emb, text_ent_emb], axis=1)\n      nbrs_source_emb = (all_nbrs_rel_emb, all_nbrs_ent_emb)\n    else:\n      text_ent_emb_flat = tf.reshape(text_ent_emb, (-1, config.emb_dim))\n      text_emb_concat = tf.concat([text_emb_flat, text_ent_emb_flat], axis=-1)\n      with tf.variable_scope(""text_rel_prject""):\n        w_project = tf.get_variable(\n            ""W_project"", shape=(text_emb_dim + config.emb_dim, config.emb_dim),\n            initializer=tf.glorot_uniform_initializer()\n        )\n      text_emb = tf.matmul(text_emb_concat, w_project)\n      text_final_emb = tf.reshape(\n          text_emb, (-1, config.max_text_neighbors, config.emb_dim)\n      )\n      nbrs_source_emb = tf.concat([nbrs_source_emb, text_final_emb], axis=1)\n    # import pdb; pdb.set_trace()\n\n  # Perform attention to construct source feature vectors\n  source_vec = attention_encoder.attend(\n      source_emb, nbrs_source_emb, relation_emb, mask_nbrs_s, name=""source""\n  )\n\n  # Score candidates\n  source_dot_query = source_vec * relation_emb\n  scores = tf.squeeze(\n      tf.matmul(\n          tf.expand_dims(source_dot_query, 1), candidates_emb, transpose_b=True\n      ),\n      axis=1\n  )\n\n  return scores, model\n\n\ndef distmult_kbc_model(config, train_graph, is_train_ph, input_tensors):\n  """"""Use DistMult model to score candidates for kbc.""""""\n  s, r, candidates = input_tensors\n  model = {}\n  entity_encoder = encoders.EmbeddingLookup(\n      config.emb_dim, is_train_ph, train_dropout=config.entity_encoder_dropout,\n      input_dim=train_graph.ent_vocab_size,\n      scope=""entity_embeddings"", use_tanh=config.use_tanh\n  )\n  model[""entity_encoder""] = entity_encoder\n\n  relation_encoder = encoders.EmbeddingLookup(\n      config.emb_dim, is_train_ph,\n      train_dropout=config.relation_encoder_dropout,\n      input_dim=train_graph.rel_vocab_size, scope=""relation_embeddings"",\n      use_tanh=config.use_tanh\n  )\n  model[""relation_encoder""] = relation_encoder\n\n  source_emb = entity_encoder.lookup(s)\n  relation_emb = relation_encoder.lookup(r)\n  candidates_emb = entity_encoder.lookup(candidates)\n\n  # Score candidates\n  source_dot_query = source_emb * relation_emb\n  scores = tf.squeeze(\n      tf.matmul(\n          tf.expand_dims(source_dot_query, 1), candidates_emb, transpose_b=True\n      ),\n      axis=1\n  )\n\n  return scores, model\n'"
research/a2n/train.py,72,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Main logic for training the A2N model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gc\nimport math\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport clueweb_text_graph\nimport dataset\nimport graph\nimport losses\nimport metrics\nimport models\nimport numpy as np\nimport slim\nfrom tensorboard.plugins import projector\nimport tensorflow as tf\nfrom tensorflow.python.training.summary_io import SummaryWriterCache\nimport text_graph\nimport utils\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""kg_file"", None, ""path to kg file"")\nflags.DEFINE_string(""output_dir"", None, ""output dir for summaries/logs"")\nflags.DEFINE_string(""dev_kg_file"", None, ""path to dev kg file"")\nflags.DEFINE_string(""test_kg_file"", None, ""path to test kg file"")\nflags.DEFINE_string(""model_path"", None, ""path to model if testing only"")\nflags.DEFINE_boolean(""evaluate"", False, ""run eval loop"")\nflags.DEFINE_boolean(""test_only"", False, ""if test only"")\nflags.DEFINE_integer(""global_step"", None,\n                     ""global_step to restore model for testing"")\nflags.DEFINE_integer(""num_epochs"", 5, ""number of train epochs"")\nflags.DEFINE_integer(""batchsize"", 64, ""batchsize for training"")\nflags.DEFINE_integer(""test_batchsize"", 10, ""batchsize for testing"")\nflags.DEFINE_integer(""max_neighbors"", None,\n                     ""maximum neighbors to use during training"")\nflags.DEFINE_integer(""max_negatives"", None,\n                     ""maximum number of negative entities to sample"")\nflags.DEFINE_integer(""emb_dim"", 100,\n                     ""dimension of entity and relation embeddings"")\nflags.DEFINE_float(""entity_encoder_dropout"", 1.0,\n                   ""dropout for entity embeddings"")\nflags.DEFINE_float(""relation_encoder_dropout"", 1.0,\n                   ""dropout for relation embeddings"")\nflags.DEFINE_float(""init_entity_encoder_dropout"", 1.0,\n                   ""dropout for init entity embeddings in attention"")\nflags.DEFINE_float(""attention_encoder_dropout"", 1.0,\n                   ""dropout for attention encoder"")\nflags.DEFINE_boolean(""use_separate_attention_emb"", False,\n                     ""use separate entity embeddings for computing attention"")\nflags.DEFINE_integer(""num_parallel_preprocess"", 64,\n                     ""number of processes to use in dataset preprocessing"")\nflags.DEFINE_integer(""prefetch_examples"", 10, ""number of examples to prefetch"")\nflags.DEFINE_integer(""shuffle_buffer"", 50000,\n                     ""buffer for shuffling training examples"")\nflags.DEFINE_float(""learning_rate"", 0.001, ""learning for optimizer"")\nflags.DEFINE_float(""grad_clip"", None, ""Clip gradient norm during training"")\nflags.DEFINE_integer(""save_every"", 100, ""save model every this many steps"")\nflags.DEFINE_string(""entity_names_file"", None,\n                    ""mapping of Freebase mid to names"")\nflags.DEFINE_enum(""model"", ""attention"",\n                  [""distmult"", ""attention"", ""source_attention"",\n                   ""source_rel_attention"", ""source_path_attention""],\n                  ""the model to use"")\nflags.DEFINE_bool(""use_tanh"", False, ""use tanh non-linearity on embeddings"")\nflags.DEFINE_enum(""attention_type"", ""bilinear"",\n                  [""bilinear"", ""cosine"", ""sigmoid_bilinear"",\n                   ""sigmoid_avg_bilinear"", ""relation""],\n                  ""type of attention to use for attention model"")\nflags.DEFINE_bool(""analyze"", False, ""analyze model"")\nflags.DEFINE_integer(""max_path_length"", None,\n                     ""maximum path length for path attention models"")\nflags.DEFINE_string(""text_kg_file"", None, ""path to text data"")\nflags.DEFINE_integer(""max_text_len"", None, ""max length of text"")\nflags.DEFINE_integer(""max_vocab_size"", None, ""max number of text words"")\nflags.DEFINE_integer(""min_word_freq"", None, ""min freq threshold for text words"")\nflags.DEFINE_integer(""max_text_neighbors"", None, ""max text neighbors"")\nflags.DEFINE_float(""text_encoder_dropout"", 1.0, ""dropout for text cnn"")\nflags.DEFINE_list(""text_encoder_filter_widths"", [""3"", ""5"", ""7""],\n                  ""filter widths for cnn"")\nflags.DEFINE_enum(""text_encoder_nonlinearity"", ""tanh"", [""relu"", ""tanh""],\n                  ""non-linearity to use for TextCNN"")\nflags.DEFINE_integer(""text_encoder_num_filters"", 64, ""num filters for cnn"")\n\nflags.DEFINE_string(""clueweb_sentences"", None,\n                    ""path to clueweb sentences (or data formatted like cw)"")\nflags.DEFINE_string(""clueweb_data"", None,\n                    ""path to clueweb data (or data formatted like cw)"")\nflags.DEFINE_string(""clueweb_embeddings"", None,\n                    ""path to clueweb embeddings (or data formatted like cw)"")\nflags.DEFINE_integer(""text_emb_dim"", None, ""embedding dim for clueweb text"")\nflags.DEFINE_integer(""subsample_text_rels"", None,\n                     ""subsample text to max this many per pair"")\n\nflags.DEFINE_string(""master"", ""local"",\n                    """"""BNS name of the TensorFlow master to use."""""")\nflags.DEFINE_integer(""task"", 0,\n                     """"""Task id of the replica running the training."""""")\nflags.DEFINE_integer(""ps_tasks"", 0, """"""Number of tasks in the ps job.\n                            If 0 no ps job is used."""""")\n\nflags.mark_flag_as_required(""kg_file"")\nflags.mark_flag_as_required(""output_dir"")\n\n\ndef add_embedding_to_projector(projector_config, emb_name, emb_metadata_path):\n  embedding_conf = projector_config.embeddings.add()\n  embedding_conf.tensor_name = emb_name\n  embedding_conf.metadata_path = emb_metadata_path\n\n\ndef get_train_op(loss, optimizer, grad_clip=None, global_step=None):\n  """"""Make a train_op apply gradients to loss using optimizer.\n\n  Args:\n   loss: the loss function to optimize\n   optimizer: the optimizer to compute and apply gradients\n   grad_clip: clip gradient norms by the value supplied (default dont clip)\n   global_step: tf.placeholder for global_step\n\n  Returns:\n   train_op: the training op to run\n   grads_and_vars: the gradients and variables for debugging\n   var_names: the variable names for debugging\n   capped_grads_and_vars: for debugging\n  """"""\n  variables = tf.trainable_variables()\n  grads_and_vars = optimizer.compute_gradients(loss, variables)\n  var_names = [v.name for v in variables]\n  logging.info(""Trainable variables:"")\n  for var in var_names:\n    logging.info(""\\t %s"", var)\n  logging.debug(grads_and_vars)\n  grad_var_norms = [(tf.global_norm([gv[1]]), tf.global_norm([gv[0]]))\n                    for gv in grads_and_vars]\n\n  if grad_clip:\n    capped_grads_and_vars = [(tf.clip_by_norm(gv[0], grad_clip), gv[1])\n                             for gv in grads_and_vars]\n  else:\n    capped_grads_and_vars = grads_and_vars\n  # norms of gradients for debugging\n  # grad_norms = [tf.sqrt(tf.reduce_sum(tf.square(grad)))\n  #               for grad, _ in grads_and_vars]\n  train_op = optimizer.apply_gradients(capped_grads_and_vars,\n                                       global_step=global_step)\n  return train_op, grad_var_norms, var_names, capped_grads_and_vars\n\n\ndef read_graph_data(\n    kg_file, add_reverse_graph, add_inverse_edge, mode,\n    num_epochs, batchsize, max_neighbors, max_negatives,\n    train_graph=None, text_kg_file=None, val_graph=None\n):\n  """"""Read graph, create dataset and build model.""""""\n  # Read graphs and create datasets\n  entity_vocab = relation_vocab = None\n  if train_graph:\n    entity_vocab = train_graph.entity_vocab\n    relation_vocab = train_graph.relation_vocab\n  if FLAGS.clueweb_data and mode == ""train"":\n    graph_type = clueweb_text_graph.CWTextGraph\n    text_kg_file = FLAGS.clueweb_data\n  elif text_kg_file and mode == ""train"":\n    graph_type = text_graph.TextGraph\n    text_kg_file = FLAGS.text_kg_file\n  else:\n    graph_type = graph.Graph\n    text_kg_file = None\n  k_graph = graph_type(\n      text_kg_file=text_kg_file,\n      skip_new=True,\n      max_text_len=FLAGS.max_text_len,\n      max_vocab_size=FLAGS.max_vocab_size,\n      min_word_freq=FLAGS.min_word_freq,\n      kg_file=kg_file,\n      add_reverse_graph=add_reverse_graph,\n      add_inverse_edge=add_inverse_edge, mode=mode,\n      entity_vocab=entity_vocab, relation_vocab=relation_vocab,\n      max_path_length=FLAGS.max_path_length if mode == ""train"" else None,\n      embeddings_file=FLAGS.clueweb_embeddings,\n      sentence_vocab_file=FLAGS.clueweb_sentences,\n      subsample=FLAGS.subsample_text_rels\n  )\n  if FLAGS.text_kg_file:\n    max_text_len = FLAGS.max_text_len\n    if mode == ""train"":\n      max_text_len = max_text_len or k_graph.max_text_len\n    elif train_graph:\n      max_text_len = max_text_len or train_graph.max_text_len\n  else:\n    max_text_len = None\n  k_data = dataset.Dataset(data_graph=k_graph, train_graph=train_graph,\n                           mode=mode, num_epochs=num_epochs,\n                           batchsize=batchsize,\n                           max_neighbors=max_neighbors,\n                           max_negatives=max_negatives,\n                           model_type=FLAGS.model,\n                           max_text_len=max_text_len,\n                           max_text_neighbors=FLAGS.max_text_neighbors,\n                           val_graph=val_graph)\n  # Create the training data iterator and return the input tensors\n  # with tf.device(""/job:worker""):\n  k_data.create_dataset_iterator(\n      num_parallel=FLAGS.num_parallel_preprocess,\n      prefetch=FLAGS.prefetch_examples,\n      shuffle_buffer=FLAGS.shuffle_buffer\n      # , device=""worker"" if FLAGS.master != ""local"" else ""cpu""\n  )\n\n  return k_graph, k_data\n\n\ndef create_model(train_graph, iterator):\n  """"""Create model and placeholders.""""""\n  if FLAGS.clueweb_data:\n    s, nbrs_s, text_nbrs_s, r, candidates, nbrs_candidates, labels, text_nbrs_s_emb = iterator.get_next()\n  elif FLAGS.text_kg_file:\n    s, nbrs_s, text_nbrs_s, r, candidates, nbrs_candidates, labels = \\\n      iterator.get_next()\n  else:\n    s, nbrs_s, r, candidates, nbrs_candidates, labels = iterator.get_next()\n\n  # Create the attention model, this returns candidates scores and the model\n  # encoders in a dict for creating feed_dict for all encoders\n  is_train_ph = tf.placeholder_with_default(True, shape=[],\n                                            name=""is_train_ph"")\n  if FLAGS.model == ""attention"":\n    with tf.variable_scope(""attention_model"", reuse=False):\n      candidate_scores, model = models.attention_kbc_model(\n          FLAGS, train_graph, is_train_ph,\n          (s, nbrs_s, r, candidates, nbrs_candidates)\n      )\n  elif FLAGS.model == ""source_attention"":\n    with tf.variable_scope(""s_attention_model"", reuse=False):\n      candidate_scores, model = models.source_attention_kbc_model(\n          FLAGS, train_graph, is_train_ph,\n          (s, nbrs_s, r, candidates)\n      )\n  elif FLAGS.model in [""source_rel_attention"", ""source_path_attention""]:\n    if FLAGS.clueweb_data:\n      input_tensors = (s, nbrs_s, text_nbrs_s, text_nbrs_s_emb, r, candidates)\n    elif FLAGS.text_kg_file:\n      input_tensors = (s, nbrs_s, text_nbrs_s, r, candidates)\n    else:\n      input_tensors = (s, nbrs_s, r, candidates)\n    with tf.variable_scope(""s_attention_model"", reuse=False):\n      candidate_scores, model = models.source_attention_kbc_model(\n          FLAGS, train_graph, is_train_ph,\n          input_tensors, model_type=FLAGS.model\n      )\n  elif FLAGS.model == ""distmult"":\n    with tf.variable_scope(""distmult_model"", reuse=False):\n      candidate_scores, model = models.distmult_kbc_model(\n          FLAGS, train_graph, is_train_ph,\n          (s, r, candidates)\n      )\n  if FLAGS.clueweb_data:\n    inputs = (s, nbrs_s, text_nbrs_s, text_nbrs_s_emb,\n              r, candidates, nbrs_candidates)\n  elif FLAGS.text_kg_file:\n    inputs = (s, nbrs_s, text_nbrs_s, r, candidates, nbrs_candidates)\n  else:\n    inputs = (s, nbrs_s, r, candidates, nbrs_candidates)\n\n  return candidate_scores, candidates, labels, model, is_train_ph, inputs\n\n\ndef evaluate():\n  """"""Run evaluation on dev or test data.""""""\n  add_inverse_edge = FLAGS.model in \\\n                     [""source_rel_attention"", ""source_path_attention""]\n  if FLAGS.clueweb_data:\n    train_graph = clueweb_text_graph.CWTextGraph(\n        text_kg_file=FLAGS.clueweb_data,\n        embeddings_file=FLAGS.clueweb_embeddings,\n        sentence_vocab_file=FLAGS.clueweb_sentences,\n        skip_new=True,\n        kg_file=FLAGS.kg_file,\n        add_reverse_graph=not add_inverse_edge,\n        add_inverse_edge=add_inverse_edge,\n        subsample=FLAGS.subsample_text_rels\n    )\n  elif FLAGS.text_kg_file:\n    train_graph = text_graph.TextGraph(\n        text_kg_file=FLAGS.text_kg_file,\n        skip_new=True,\n        max_text_len=FLAGS.max_text_len,\n        max_vocab_size=FLAGS.max_vocab_size,\n        min_word_freq=FLAGS.min_word_freq,\n        kg_file=FLAGS.kg_file,\n        add_reverse_graph=not add_inverse_edge,\n        add_inverse_edge=add_inverse_edge,\n        max_path_length=FLAGS.max_path_length\n    )\n  else:\n    train_graph = graph.Graph(\n        kg_file=FLAGS.kg_file,\n        add_reverse_graph=not add_inverse_edge,\n        add_inverse_edge=add_inverse_edge,\n        max_path_length=FLAGS.max_path_length\n    )\n  # train_graph, _ = read_graph_data(\n  #     kg_file=FLAGS.kg_file,\n  #     add_reverse_graph=(FLAGS.model != ""source_rel_attention""),\n  #     add_inverse_edge=(FLAGS.model == ""source_rel_attention""),\n  #     mode=""train"", num_epochs=FLAGS.num_epochs, batchsize=FLAGS.batchsize,\n  #     max_neighbors=FLAGS.max_neighbors,\n  #     max_negatives=FLAGS.max_negatives\n  # )\n  val_graph = None\n  if FLAGS.dev_kg_file:\n    val_graph, eval_data = read_graph_data(\n        kg_file=FLAGS.dev_kg_file,\n        add_reverse_graph=not add_inverse_edge,\n        add_inverse_edge=add_inverse_edge,\n        # add_reverse_graph=False,\n        # add_inverse_edge=False,\n        mode=""dev"", num_epochs=1, batchsize=FLAGS.test_batchsize,\n        max_neighbors=FLAGS.max_neighbors,\n        max_negatives=FLAGS.max_negatives, train_graph=train_graph,\n        text_kg_file=FLAGS.text_kg_file\n    )\n  if FLAGS.test_kg_file:\n    _, eval_data = read_graph_data(\n        kg_file=FLAGS.test_kg_file,\n        add_reverse_graph=not add_inverse_edge,\n        add_inverse_edge=add_inverse_edge,\n        # add_reverse_graph=False,\n        # add_inverse_edge=False,\n        mode=""test"", num_epochs=1, batchsize=FLAGS.test_batchsize,\n        max_neighbors=FLAGS.max_neighbors,\n        max_negatives=None, train_graph=train_graph,\n        text_kg_file=FLAGS.text_kg_file,\n        val_graph=val_graph\n    )\n  if not FLAGS.dev_kg_file and not FLAGS.test_kg_file:\n    raise ValueError(""Evalution without a dev or test file!"")\n\n  iterator = eval_data.dataset.make_initializable_iterator()\n  candidate_scores, candidates, labels, model, is_train_ph, inputs = \\\n    create_model(train_graph, iterator)\n\n  # Create eval metrics\n  # if FLAGS.dev_kg_file:\n  batch_rr = metrics.mrr(candidate_scores, candidates, labels)\n  mrr, mrr_update = tf.metrics.mean(batch_rr)\n  mrr_summary = tf.summary.scalar(""MRR"", mrr)\n\n  all_hits, all_hits_update, all_hits_summaries = [], [], []\n  for k in [1, 3, 10]:\n    batch_hits = metrics.hits_at_k(candidate_scores, candidates, labels, k=k)\n    hits, hits_update = tf.metrics.mean(batch_hits)\n    hits_summary = tf.summary.scalar(""Hits_at_%d"" % k, hits)\n    all_hits.append(hits)\n    all_hits_update.append(hits_update)\n    all_hits_summaries.append(hits_summary)\n  hits = tf.group(*all_hits)\n  hits_update = tf.group(*all_hits_update)\n\n  global_step = tf.Variable(0, name=""global_step"", trainable=False)\n  current_step = tf.Variable(0, name=""current_step"", trainable=False,\n                             collections=[tf.GraphKeys.LOCAL_VARIABLES])\n  incr_current_step = tf.assign_add(current_step, 1)\n  reset_current_step = tf.assign(current_step, 0)\n\n  slim.get_or_create_global_step(graph=tf.get_default_graph())\n\n  # best_hits = tf.Variable(0., trainable=False)\n  # best_step = tf.Variable(0, trainable=False)\n  # with tf.control_dependencies([hits]):\n  #   update_best_hits = tf.cond(tf.greater(hits, best_hits),\n  #                              lambda: tf.assign(best_hits, hits),\n  #                              lambda: 0.)\n  #   update_best_step = tf.cond(tf.greater(hits, best_hits),\n  #                              lambda: tf.assign(best_step, global_step),\n  #                              lambda: 0)\n  # best_hits_summary = tf.summary.scalar(""Best Hits@10"", best_hits)\n  # best_step_summary = tf.summary.scalar(""Best Step"", best_step)\n\n  nexamples = eval_data.data_graph.tuple_store.shape[0]\n  if eval_data.data_graph.add_reverse_graph:\n    nexamples *= 2\n  num_batches = math.ceil(nexamples / float(FLAGS.test_batchsize))\n  local_init_op = tf.local_variables_initializer()\n\n  if FLAGS.analyze:\n    entity_names = utils.read_entity_name_mapping(FLAGS.entity_names_file)\n    session = tf.Session()\n    # summary_writer = tf.summary.FileWriter(FLAGS.output_dir, session.graph)\n    init_op = tf.global_variables_initializer()\n    session.run(init_op)\n    session.run(local_init_op)\n    saver = tf.train.Saver(tf.trainable_variables())\n    ckpt_path = FLAGS.model_path + ""/model.ckpt-%d"" % FLAGS.global_step\n    attention_probs = model[""attention_encoder""].get_from_collection(\n        ""attention_probs""\n    )\n    if FLAGS.clueweb_data:\n      s, nbrs_s, text_nbrs_s, text_nbrs_s_emb, r, candidates, _ = inputs\n    elif FLAGS.text_kg_file:\n      s, nbrs_s, text_nbrs_s, r, candidates, _ = inputs\n    else:\n      s, nbrs_s, r, candidates, _ = inputs\n    saver.restore(session, ckpt_path)\n    session.run(iterator.initializer)\n    num_attention = 5\n    nsteps = 0\n    outf_correct = open(FLAGS.output_dir + ""/analyze_correct.txt"", ""w+"")\n    outf_incorrect = open(\n        FLAGS.output_dir + ""/analyze_incorrect.txt"", ""w+""\n    )\n    ncorrect = 0\n    analyze_outputs = [candidate_scores, s, nbrs_s, r, candidates, labels,\n                       attention_probs]\n    if FLAGS.text_kg_file:\n      analyze_outputs.append(text_nbrs_s)\n    while True:\n      try:\n        analyze_vals = session.run(analyze_outputs, {is_train_ph: False})\n        if FLAGS.text_kg_file:\n          cscores, se, nbrs, qr, cands, te, nbr_attention_probs, text_nbrs = \\\n            analyze_vals\n        else:\n          cscores, se, nbrs, qr, cands, te, nbr_attention_probs = analyze_vals\n        # import pdb; pdb.set_trace()\n        pred_ids = cscores.argmax(1)\n        for i in range(se.shape[0]):\n          sname = train_graph.inverse_entity_vocab[se[i]]\n          if sname in entity_names:\n            sname = entity_names[sname]\n          rname = train_graph.inverse_relation_vocab[qr[i]]\n          pred_target = cands[i, pred_ids[i]]\n          pred_name = train_graph.inverse_entity_vocab[pred_target]\n          if pred_name in entity_names:\n            pred_name = entity_names[pred_name]\n          tname = train_graph.inverse_entity_vocab[te[i][0]]\n          if tname in entity_names:\n            tname = entity_names[tname]\n          if te[i][0] == pred_target:\n            outf = outf_correct\n            ncorrect += 1\n          else:\n            outf = outf_incorrect\n          outf.write(""\\n(%d) %s, %s, ? \\t Pred: %s \\t Target: %s"" %\n                     (nsteps+i+1, sname, rname, pred_name, tname))\n          top_nbrs_index = np.argsort(nbr_attention_probs[i, :])[::-1]\n          outf.write(""\\nTop Nbrs:"")\n          for j in range(num_attention):\n            nbr_index = top_nbrs_index[j]\n            if nbr_index < FLAGS.max_neighbors:\n              nbr_id = nbrs[i, nbr_index, :]\n              nbr_name = """"\n              for k in range(0, nbrs.shape[-1], 2):\n                ent_name = train_graph.inverse_entity_vocab[nbr_id[k+1]]\n                if ent_name in entity_names:\n                  ent_name = entity_names[ent_name]\n                rel_name = train_graph.inverse_relation_vocab[nbr_id[k]]\n                nbr_name += ""(%s, %s)"" % (rel_name, ent_name)\n            else:\n              # Text Relation\n              text_nbr_ids = text_nbrs[i, nbr_index - FLAGS.max_neighbors, :]\n              text_nbr_ent = text_nbr_ids[0]\n              ent_name = train_graph.inverse_entity_vocab[text_nbr_ent]\n              if ent_name in entity_names:\n                ent_name = entity_names[ent_name]\n              rel_name = train_graph.get_relation_text(text_nbr_ids[1:])\n              nbr_name = ""(%s, %s)"" % (rel_name, ent_name)\n            outf.write(""\\n\\t\\t %s Prob: %.4f"" %\n                       (nbr_name, nbr_attention_probs[i, nbr_index]))\n        nsteps += se.shape[0]\n        tf.logging.info(""Current hits@1: %.3f"", ncorrect * 1.0 / (nsteps))\n\n      except tf.errors.OutOfRangeError:\n        break\n    outf_correct.close()\n    outf_incorrect.close()\n    return\n\n  class DataInitHook(tf.train.SessionRunHook):\n\n    def after_create_session(self, sess, coord):\n      sess.run(iterator.initializer)\n      sess.run(reset_current_step)\n\n  if FLAGS.test_only:\n    ckpt_path = FLAGS.model_path + ""/model.ckpt-%d"" % FLAGS.global_step\n    slim.evaluation.evaluate_once(\n        master=FLAGS.master,\n        checkpoint_path=ckpt_path,\n        logdir=FLAGS.output_dir,\n        variables_to_restore=tf.trainable_variables() + [global_step],\n        initial_op=tf.group(local_init_op, iterator.initializer),\n        # initial_op=iterator.initializer,\n        num_evals=num_batches,\n        eval_op=tf.group(mrr_update, hits_update, incr_current_step),\n        eval_op_feed_dict={is_train_ph: False},\n        final_op=tf.group(mrr, hits),\n        final_op_feed_dict={is_train_ph: False},\n        summary_op=tf.summary.merge([mrr_summary]+ all_hits_summaries),\n        hooks=[DataInitHook(),\n               tf.train.LoggingTensorHook(\n                   {""mrr"": mrr, ""hits"": hits, ""step"": current_step},\n                   every_n_iter=1\n               )]\n    )\n  else:\n    slim.evaluation.evaluation_loop(\n        master=FLAGS.master,\n        checkpoint_dir=FLAGS.model_path,\n        logdir=FLAGS.output_dir,\n        variables_to_restore=tf.trainable_variables() + [global_step],\n        initial_op=tf.group(local_init_op, iterator.initializer),\n        # initial_op=iterator.initializer,\n        num_evals=num_batches,\n        eval_op=tf.group(mrr_update, hits_update, incr_current_step),\n        eval_op_feed_dict={is_train_ph: False},\n        final_op=tf.group(mrr, hits),\n        final_op_feed_dict={is_train_ph: False},\n        summary_op=tf.summary.merge([mrr_summary] +  all_hits_summaries),\n        max_number_of_evaluations=None,\n        eval_interval_secs=60,\n        hooks=[DataInitHook(),\n               tf.train.LoggingTensorHook(\n                   {""mrr"": mrr, ""hits"": hits, ""step"": current_step},\n                   every_n_iter=1\n               )]\n    )\n\n\ndef train():\n  """"""Running the main training loop with given parameters.""""""\n  if FLAGS.task == 0 and not tf.gfile.Exists(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n\n  # Read train/dev/test graphs, create datasets and model\n  add_inverse_edge = FLAGS.model in \\\n                     [""source_rel_attention"", ""source_path_attention""]\n  train_graph, train_data = read_graph_data(\n      kg_file=FLAGS.kg_file,\n      add_reverse_graph=not add_inverse_edge,\n      add_inverse_edge=add_inverse_edge,\n      mode=""train"",\n      num_epochs=FLAGS.num_epochs, batchsize=FLAGS.batchsize,\n      max_neighbors=FLAGS.max_neighbors,\n      max_negatives=FLAGS.max_negatives,\n      text_kg_file=FLAGS.text_kg_file\n  )\n\n  worker_device = ""/job:{}"".format(FLAGS.brain_job_name)\n  with tf.device(\n      tf.train.replica_device_setter(\n          FLAGS.ps_tasks, worker_device=worker_device)):\n    iterator = train_data.dataset.make_one_shot_iterator()\n    candidate_scores, _, labels, model, is_train_ph, _ = create_model(\n        train_graph, iterator\n    )\n\n  # Create train loss and training op\n  loss = losses.softmax_crossentropy(logits=candidate_scores, labels=labels)\n  optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n  global_step = tf.Variable(0, name=""global_step"", trainable=False)\n  train_op = get_train_op(loss, optimizer, FLAGS.grad_clip,\n                          global_step=global_step)\n  tf.summary.scalar(""Loss"", loss)\n\n  run_options = tf.RunOptions(report_tensor_allocations_upon_oom=True)\n  session_config = tf.ConfigProto(log_device_placement=True)\n\n  # Create tf training session\n  scaffold = tf.train.Scaffold(saver=tf.train.Saver(max_to_keep=1000))\n  # ckpt_hook = tf.train.CheckpointSaverHook(\n  #     checkpoint_dir=FLAGS.output_dir, scaffold=scaffold,\n  #     save_steps=FLAGS.save_every\n  # )\n  # summary_hook = tf.train.SummarySaverHook(\n  #     save_secs=60, output_dir=FLAGS.output_dir,\n  #     summary_op=tf.summary.merge_all()\n  # )\n  session = tf.train.MonitoredTrainingSession(\n      master=FLAGS.master,\n      is_chief=(FLAGS.task == 0),\n      checkpoint_dir=FLAGS.output_dir,\n      save_checkpoint_steps=FLAGS.save_every,\n      scaffold=scaffold,\n      save_summaries_secs=60,\n      # hooks=[summary_hook],\n      # chief_only_hooks=[ckpt_hook],\n      config=session_config\n  )\n\n  # Create embeddings visualization\n  if FLAGS.task == 0:\n    utils.save_embedding_vocabs(FLAGS.output_dir, train_graph,\n                                FLAGS.entity_names_file)\n    pconfig = projector.ProjectorConfig()\n    add_embedding_to_projector(\n        pconfig, model[""entity_encoder""].embeddings.name.split("":"")[0],\n        os.path.join(FLAGS.output_dir, ""entity_vocab.tsv"")\n    )\n    add_embedding_to_projector(\n        pconfig, model[""relation_encoder""].embeddings.name.split("":"")[0],\n        os.path.join(FLAGS.output_dir, ""relation_vocab.tsv"")\n    )\n    if FLAGS.text_kg_file:\n      word_embeddings = model[""text_encoder""].word_embedding_encoder.embeddings\n      add_embedding_to_projector(\n          pconfig, word_embeddings.name.split("":"")[0],\n          os.path.join(FLAGS.output_dir, ""word_vocab.tsv"")\n      )\n    projector.visualize_embeddings(\n        SummaryWriterCache.get(FLAGS.output_dir), pconfig\n    )\n\n  # Main training loop\n  running_total_loss = 0.\n  nsteps = 0\n  gc.collect()\n  while True:\n    try:\n      current_loss, _, _ = session.run(\n          [loss, train_op, global_step],\n          # feed_dict={is_train_ph: True, handle: train_iterator_handle},\n          feed_dict={is_train_ph: True},\n          options=run_options\n      )\n      nsteps += 1\n      running_total_loss += current_loss\n      tf.logging.info(""Step %d, loss: %.3f, running avg loss: %.3f"",\n                      nsteps, current_loss, running_total_loss / nsteps)\n      if nsteps %2 == 0:\n        gc.collect()\n    except tf.errors.OutOfRangeError:\n      tf.logging.info(""End of Traning Epochs after %d steps"", nsteps)\n      break\n\n\ndef main(argv):\n  del argv\n  if FLAGS.test_only or FLAGS.evaluate or FLAGS.analyze:\n    evaluate()\n  else:\n    train()\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
research/a2n/utils.py,12,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions for project A2N.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport tensorflow as tf\n\n\ndef combine_dict(init_dict, add_dict):\n  """"""Add add_dict to init_dict and return init_dict.""""""\n  for k, v in add_dict.iteritems():\n    init_dict[k] = v\n  return init_dict\n\n\ndef add_variable_summaries(var, var_name_scope):\n  """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n  with tf.name_scope(\'summaries/\' + var_name_scope):\n    mean = tf.reduce_mean(var)\n    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n    var_max = tf.reduce_max(var)\n    var_min = tf.reduce_min(var)\n    tf.summary.scalar(\'mean\', mean)\n    tf.summary.scalar(\'stddev\', stddev)\n    tf.summary.scalar(\'max\', var_max)\n    tf.summary.scalar(\'min\', var_min)\n    tf.summary.histogram(\'histogram\', var)\n\n\ndef add_histogram_summary(var, var_name_scope):\n  """"""Just adds a histogram summary for the variable.""""""\n  with tf.name_scope(\'summaries/\' + var_name_scope):\n    tf.summary.histogram(\'histogram\', var)\n\n\ndef read_entity_name_mapping(entity_names_file):\n  """"""Read mapping from entity mid to names.""""""\n  entity_names = {}\n  with open(entity_names_file) as gf:\n    if entity_names_file.endswith(\'.gz\'):\n      f = gzip.GzipFile(fileobj=gf)\n    else:\n      f = gf\n    for line in f:\n      contents = line.strip().split(\'\\t\')\n      if len(contents) < 2:\n        continue\n      # mid, name = contents\n      mid = contents[0]\n      name = contents[1]\n      entity_names[\'/\' + mid] = name\n  return entity_names\n\n\ndef save_embedding_vocabs(output_dir, graph, entity_names_file=None):\n  """"""Save entity and relation vocabs to file.""""""\n  # Read entity names\n  entity_names = None\n  if entity_names_file:\n    entity_names = read_entity_name_mapping(entity_names_file)\n  # Save entity vocab\n  with open(output_dir + \'/entity_vocab.tsv\', \'w+\') as f:\n    for i in range(graph.ent_vocab_size):\n      name = graph.inverse_entity_vocab[i]\n      if entity_names and name in entity_names:\n        name += \'/\' + entity_names[name]\n      f.write(name + \'\\n\')\n  with open(output_dir + \'/relation_vocab.tsv\', \'w+\') as f:\n    for i in range(graph.rel_vocab_size):\n      f.write(graph.inverse_relation_vocab[i] + \'\\n\')\n  if hasattr(graph, \'vocab\'):\n    with open(output_dir + \'/word_vocab.tsv\', \'w+\') as f:\n      for i in range(graph.word_vocab_size):\n        f.write(graph.inverse_word_vocab[i] + \'\\n\')\n'"
research/kg_hyp_emb/__init__.py,0,b''
research/kg_hyp_emb/config.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Default configuration parameters.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nCONFIG = {\n    \'string\': {\n        \'dataset\': (\'Dataset\', \'WN18RR\'),\n        \'model\': (\'Model\', \'RotE\'),\n        \'data_dir\': (\'Path to data directory\', \'data/\'),\n        \'save_dir\': (\'Path to logs directory\', \'logs/\'),\n        \'loss_fn\': (\'Loss function to use\', \'SigmoidCrossEntropy\'),\n        \'initializer\': (\'Which initializer to use\', \'GlorotNormal\'),\n        \'regularizer\': (\'Regularizer\', \'N3\'),\n        \'optimizer\': (\'Optimizer\', \'Adam\'),\n        \'bias\': (\'Bias term\', \'learn\'),\n        \'dtype\': (\'Precision to use\', \'float32\'),\n    },\n    \'float\': {\n        \'lr\': (\'Learning rate\', 1e-3),\n        \'lr_decay\': (\'Learning rate decay\', 0.96),\n        \'min_lr\': (\'Minimum learning rate decay\', 1e-5),\n        \'gamma\': (\'Margin for distance-based losses\', 0),\n        \'entity_reg\': (\'Regularization weight for entity embeddings\', 0),\n        \'rel_reg\': (\'Regularization weight for relation embeddings\', 0),\n    },\n    \'integer\': {\n        \'patience\': (\'Number of validation steps before early stopping\', 20),\n        \'valid\': (\'Number of epochs before computing validation metrics\', 5),\n        \'checkpoint\': (\'Number of epochs before checkpointing the model\', 5),\n        \'max_epochs\': (\'Maximum number of epochs to train for\', 400),\n        \'rank\': (\'Embeddings dimension\', 500),\n        \'batch_size\': (\'Batch size\', 500),\n        \'neg_sample_size\':\n            (\'Negative sample size, -1 to use loss without negative sampling\',\n             50),\n    },\n    \'boolean\': {\n        \'train_c\': (\'Whether to train the hyperbolic curvature or not\', True),\n        \'debug\': (\'If debug is true, only use 1000 examples for\'\n                  \' debugging purposes\', False),\n        \'save_logs\':\n            (\'Whether to save the training logs or print to stdout\', True),\n        \'save_model\': (\'Whether to save the model weights\', False)\n    }\n}\n'"
research/kg_hyp_emb/train.py,4,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Train a Knowledge Graph completion model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport json\nimport logging as native_logging\nimport os\nimport sys\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom kg_hyp_emb.config import CONFIG\nfrom kg_hyp_emb.datasets.datasets import DatasetFn\nfrom kg_hyp_emb.learning.trainer import KGTrainer\nimport kg_hyp_emb.models as models\nimport kg_hyp_emb.utils.train as train_utils\nimport tensorflow as tf\n\nflag_fns = {\n    \'string\': flags.DEFINE_string,\n    \'integer\': flags.DEFINE_integer,\n    \'boolean\': flags.DEFINE_boolean,\n    \'float\': flags.DEFINE_float,\n}\nfor dtype, flag_fn in flag_fns.items():\n  for arg, (description, default) in CONFIG[dtype].items():\n    flag_fn(arg, default=default, help=description)\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  # get logger\n  if FLAGS.save_logs:\n    if not os.path.exists(os.path.join(FLAGS.save_dir, \'train.log\')):\n      os.makedirs(FLAGS.save_dir)\n      write_mode = \'w\'\n    else:\n      write_mode = \'a\'\n    stream = open(os.path.join(FLAGS.save_dir, \'train.log\'), write_mode)\n    log_handler = native_logging.StreamHandler(stream)\n    print(\'Saving logs in {}\'.format(FLAGS.save_dir))\n  else:\n    log_handler = native_logging.StreamHandler(sys.stdout)\n  formatter = native_logging.Formatter(\n      \'%(asctime)s %(levelname)-8s %(message)s\')\n  log_handler.setFormatter(formatter)\n  log_handler.setLevel(logging.INFO)\n  logger = logging.get_absl_logger()\n  logger.addHandler(log_handler)\n\n  # load data\n  dataset_path = os.path.join(FLAGS.data_dir, FLAGS.dataset)\n  dataset = DatasetFn(dataset_path, FLAGS.debug)\n  sizes = dataset.get_shape()\n  train_examples_reversed = dataset.get_examples(\'train\')\n  valid_examples = dataset.get_examples(\'valid\')\n  test_examples = dataset.get_examples(\'test\')\n  filters = dataset.get_filters()\n  logging.info(\'\\t Dataset shape: %s\', (str(sizes)))\n\n  # save config\n  config_path = os.path.join(FLAGS.save_dir, \'config.json\')\n  if FLAGS.save_logs:\n    with open(config_path, \'w\') as fjson:\n      json.dump(train_utils.get_config_dict(), fjson)\n\n  # create and build model\n  tf.keras.backend.set_floatx(FLAGS.dtype)\n  model = getattr(models, FLAGS.model)(sizes, FLAGS)\n  model.build(input_shape=(1, 3))\n  trainable_params = train_utils.count_params(model)\n  trainer = KGTrainer(sizes, FLAGS)\n  logging.info(\'\\t Total number of trainable parameters %s\', (trainable_params))\n\n  # restore or create checkpoint\n  if FLAGS.save_model:\n    ckpt = tf.train.Checkpoint(\n        step=tf.Variable(0), optimizer=trainer.optimizer, net=model)\n    manager = tf.train.CheckpointManager(ckpt, FLAGS.save_dir, max_to_keep=1)\n    if manager.latest_checkpoint:\n      ckpt.restore(manager.latest_checkpoint)\n      logging.info(\'\\t Restored from %s\', (manager.latest_checkpoint))\n    else:\n      logging.info(\'\\t Initializing from scratch.\')\n  else:\n    logging.info(\'\\t Initializing from scratch.\')\n\n  # train model\n  logging.info(\'\\t Start training\')\n  early_stopping_counter = 0\n  best_mrr = None\n  best_epoch = None\n  best_weights = None\n  if FLAGS.save_model:\n    epoch = ckpt.step\n  else:\n    epoch = 0\n\n  if int(epoch) < FLAGS.max_epochs:\n    while int(epoch) < FLAGS.max_epochs:\n      if FLAGS.save_model:\n        epoch.assign_add(1)\n      else:\n        epoch += 1\n\n      # Train step\n      start = time.perf_counter()\n      train_batch = train_examples_reversed.batch(FLAGS.batch_size)\n      train_loss = trainer.train_step(model, train_batch).numpy()\n      end = time.perf_counter()\n      execution_time = (end - start)\n      logging.info(\'\\t Epoch %i | train loss: %.4f | total time: %.4f\',\n                   int(epoch), train_loss, execution_time)\n\n      if FLAGS.save_model and int(epoch) % FLAGS.checkpoint == 0:\n        save_path = manager.save()\n        logging.info(\'\\t Saved checkpoint for epoch %i: %s\', int(epoch),\n                     save_path)\n\n      if int(epoch) % FLAGS.valid == 0:\n        # compute valid loss\n        valid_batch = valid_examples.batch(FLAGS.batch_size)\n        valid_loss = trainer.valid_step(model, valid_batch).numpy()\n        logging.info(\'\\t Epoch %i | average valid loss: %.4f\', int(epoch),\n                     valid_loss)\n\n        # compute validation metrics\n        valid = train_utils.avg_both(*model.eval(valid_examples, filters))\n        logging.info(train_utils.format_metrics(valid, split=\'valid\'))\n\n        # early stopping\n        valid_mrr = valid[\'MRR\']\n        if not best_mrr or valid_mrr > best_mrr:\n          best_mrr = valid_mrr\n          early_stopping_counter = 0\n          best_epoch = int(epoch)\n          best_weights = copy.copy(model.get_weights())\n        else:\n          early_stopping_counter += 1\n          if early_stopping_counter == FLAGS.patience:\n            logging.info(\'\\t Early stopping\')\n            break\n\n    logging.info(\'\\t Optimization finished\')\n    logging.info(\'\\t Evaluating best model from epoch %s\', best_epoch)\n    model.set_weights(best_weights)\n    if FLAGS.save_model:\n      model.save_weights(os.path.join(FLAGS.save_dir, \'best_model.ckpt\'))\n\n    # validation metrics\n    valid = train_utils.avg_both(*model.eval(valid_examples, filters))\n    logging.info(train_utils.format_metrics(valid, split=\'valid\'))\n\n    # test metrics\n    test = train_utils.avg_both(*model.eval(test_examples, filters))\n    logging.info(train_utils.format_metrics(test, split=\'test\'))\n  else:\n    logging.info(\'\\t Training completed\')\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
neural_structured_learning/keras/layers/__init__.py,0,"b'""""""Keras layers for Neural Structured Learning.""""""\n\nfrom neural_structured_learning.keras.layers.neighbor_features import make_missing_neighbor_inputs\nfrom neural_structured_learning.keras.layers.neighbor_features import NeighborFeatures\nfrom neural_structured_learning.keras.layers.pairwise_distance import PairwiseDistance\n\n__all__ = [\n    \'make_missing_neighbor_inputs\', \'NeighborFeatures\', \'PairwiseDistance\'\n]\n'"
neural_structured_learning/keras/layers/layers_test.py,17,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Integration tests for neural_structured_learning.keras.layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import parameterized\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.keras import layers\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n_ERR_TOL = 3e-5  # Tolerance when comparing floats.\n\n\n# TODO(ppham27): Update models to use NeighborFeatures\ndef _make_functional_regularized_model(distance_config):\n  """"""Makes a model with `PairwiseDistance` and the functional API.""""""\n\n  def _make_unregularized_model(inputs, num_classes):\n    """"""Makes standard 1 layer MLP with logistic regression.""""""\n    x = tf.keras.layers.Dense(16, activation=\'relu\')(inputs)\n    return tf.keras.Model(inputs, outputs=tf.keras.layers.Dense(num_classes)(x))\n\n  # Each example has 4 features and 2 neighbors, each with an edge weight.\n  inputs = (tf.keras.Input(shape=(4,), dtype=tf.float32, name=\'features\'),\n            tf.keras.Input(shape=(2, 4), dtype=tf.float32, name=\'neighbors\'),\n            tf.keras.Input(\n                shape=(2, 1), dtype=tf.float32, name=\'neighbor_weights\'))\n  features, neighbors, neighbor_weights = inputs\n  unregularized_model = _make_unregularized_model(features, 3)\n  logits = unregularized_model(features)\n  model = tf.keras.Model(inputs=inputs, outputs=logits)\n  # Add regularization.\n  regularizer = layers.PairwiseDistance(distance_config)\n  graph_loss = regularizer(\n      sources=logits,\n      targets=unregularized_model(neighbors),\n      weights=neighbor_weights)\n  model.add_loss(graph_loss)\n  model.add_metric(graph_loss, aggregation=\'mean\', name=\'graph_loss\')\n  return model\n\n\nclass _PairwiseRegularizedModel(tf.keras.Model):\n  """"""Example model for using `PairwiseDistance` by subclassing.""""""\n\n  def __init__(self, distance_config, **kwargs):\n    super(_PairwiseRegularizedModel, self).__init__(**kwargs)\n    self._regularizer = layers.PairwiseDistance(\n        distance_config, name=\'graph_loss\')\n    self._unregularized_model = tf.keras.Sequential([\n        tf.keras.layers.Dense(16, activation=\'relu\'),\n        tf.keras.layers.Dense(3),\n    ])\n\n  def call(self, inputs):\n    features = inputs[\'features\']\n    neighbors = inputs[\'neighbors\']\n    neighbor_weights = inputs[\'neighbor_weights\']\n    # Forward pass.\n    logits = self._unregularized_model(features)\n    # Add regularization.\n    graph_loss = self._regularizer(\n        sources=logits,\n        targets=self._unregularized_model(neighbors),\n        weights=neighbor_weights)\n    self.add_loss(graph_loss)\n    self.add_metric(graph_loss, aggregation=\'mean\', name=\'graph_loss\')\n    return logits\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass LayersTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for neural_structured_learning.keras.layers.""""""\n\n  @parameterized.parameters(\n      itertools.product(\n          (_make_functional_regularized_model, _PairwiseRegularizedModel),\n          configs.DistanceType.all()))\n  def testModelFitAndEvaluate(self, model_fn, distance_type):\n    """"""Fit and evaluate models with various distance configurations.""""""\n    # Set up graph-regularized model.\n    distance_config = configs.DistanceConfig(\n        distance_type=distance_type,\n        transform_fn=configs.TransformType.SOFTMAX,\n        sum_over_axis=-1)\n    model = model_fn(distance_config)\n    model.compile(\n        optimizer=tf.keras.optimizers.SGD(),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\n            tf.keras.metrics.SparseCategoricalAccuracy(),\n            tf.keras.metrics.SparseCategoricalCrossentropy(from_logits=True),\n        ])\n    # Fit and evaluate the model on dummy data that has 8 examples.\n    features = {\n        \'features\': np.random.normal(size=(8, 4)),\n        \'neighbors\': np.random.normal(size=(8, 2, 4)),\n        \'neighbor_weights\': np.random.uniform(size=(8, 2, 1)),\n    }\n    labels = np.random.randint(0, 3, size=8)\n    train_history = model.fit(features, labels, batch_size=2, epochs=16).history\n    evaluation_results = dict(\n        zip(model.metrics_names, model.evaluate(features, labels,\n                                                batch_size=4)))\n    # Assert that losses and metrics were evaluated.\n    self.assertAllGreater(train_history[\'graph_loss\'], 0.)\n    self.assertGreater(evaluation_results[\'graph_loss\'], 0.)\n    self.assertAllClose(\n        train_history[\'loss\'],\n        np.add(train_history[\'graph_loss\'],\n               train_history[\'sparse_categorical_crossentropy\']), _ERR_TOL)\n    self.assertNear(\n        evaluation_results[\'loss\'], evaluation_results[\'graph_loss\'] +\n        evaluation_results[\'sparse_categorical_crossentropy\'], _ERR_TOL)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/keras/layers/neighbor_features.py,11,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Input layer to unpack neighbor features for graph regularization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport attr\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.lib import utils\nimport tensorflow as tf\n\n\ndef make_missing_neighbor_inputs(neighbor_config,\n                                 inputs,\n                                 weight_dtype=tf.float32):\n  """"""Makes additional inputs for neighbor features if necessary.\n\n  Args:\n    neighbor_config: An instance of `configs.GraphNeighborConfig` specifying the\n      number of neighbors and how neighbor features should be named.\n    inputs: Dictionary of input tensors that may be missing neighbor features.\n      The keys are the features names. See `utils.unpack_neighbor_features` for\n      expected names of neighbor features and weights.\n    weight_dtype: `tf.Dtype` for neighbors weights. Defaults to `tf.float32`.\n\n  Returns:\n    A dictionary of neighbor feature and weight tensors that do not already\n    exist in `inputs`. The keys are specified according to `neighbor_config`.\n  """"""\n  existing_feature_names = set(inputs.keys())\n  neighbor_inputs = {}\n  for i in range(neighbor_config.max_neighbors):  # For each potential neighbor.\n    # Weight of the neighbor.\n    weight_name = \'{}{}{}\'.format(neighbor_config.prefix, i,\n                                  neighbor_config.weight_suffix)\n    if weight_name not in existing_feature_names:\n      neighbor_inputs[weight_name] = tf.keras.Input((1,),\n                                                    dtype=weight_dtype,\n                                                    name=weight_name)\n    # For inputs without existing neighbor features, replicate them.\n    for feature_name, tensor in inputs.items():\n      if feature_name.startswith(neighbor_config.prefix):\n        continue\n      neighbor_feature_name = \'{}{}_{}\'.format(neighbor_config.prefix, i,\n                                               feature_name)\n      if neighbor_feature_name not in existing_feature_names:\n        neighbor_inputs[neighbor_feature_name] = tf.keras.Input(\n            tensor.shape[1:],\n            batch_size=tensor.shape[0],\n            dtype=tensor.dtype,\n            name=neighbor_feature_name,\n            ragged=isinstance(tensor, tf.RaggedTensor),\n            sparse=isinstance(tensor, tf.sparse.SparseTensor))\n  return neighbor_inputs\n\n\nclass NeighborFeatures(tf.keras.layers.Layer):\n  """"""A layer to unpack a dictionary of sample features and neighbor features.\n\n  Missing neighbor inputs will also be created for the functional API.\n  """"""\n\n  def __init__(self,\n               neighbor_config=None,\n               feature_names=None,\n               weight_dtype=None,\n               **kwargs):\n    """"""Initializes an instance of `NeighborFeatures`.\n\n    Args:\n      neighbor_config: A `configs.GraphNeighborConfig` instance describing\n        neighbor attributes.\n      feature_names: Optional[List[Text]], names denoting the keys of features\n        for which to create neighbor inputs. If `None`, all features are assumed\n        to have corresponding neighbor features.\n      weight_dtype: `tf.DType` for `neighbor_weights`. Defaults to `tf.float32`.\n      **kwargs: Additional arguments to be passed `tf.keras.layers.Layer`.\n    """"""\n    super(NeighborFeatures, self).__init__(\n        autocast=False,\n        dtype=kwargs.pop(\'dtype\') if \'dtype\' in kwargs else weight_dtype,\n        **kwargs)\n    self._neighbor_config = (\n        configs.GraphNeighborConfig()\n        if neighbor_config is None else attr.evolve(neighbor_config))\n    self._feature_names = (\n        feature_names if feature_names is None else set(feature_names))\n\n  def call(self, inputs, keep_rank=True):\n    """"""Extracts neighbor features and weights from a dictionary of inputs.\n\n    This function is a wrapper around `utils.unpack_neighbor_features`. See\n    `utils.unpack_neighbor_features` for documentation on the expected input\n    format and return values.\n\n    Args:\n      inputs: Dictionary of `tf.Tensor` features with keys for neighbors and\n        weights described by `neighbor_config`.\n      keep_rank: Boolean indicating whether each value of `neighbor_features`\n        retains the rank from the corresponding value in `sample_features`\n        by merging the neighborhood size with the batch_size dimension, or\n        contains an extra neighborhood dimension at axis 1. Defaults to `True`.\n\n    Returns:\n      A tuple (sample_features, neighbor_features, neighbor_weights) of tensors.\n      See `utils.unpack_neighbor_features` for a detailed description.\n    """"""\n    return utils.unpack_neighbor_features(\n        inputs, self._neighbor_config, keep_rank=keep_rank)\n\n  def _include_feature(self, name):\n    """"""Decides if the feature specified by `name` should be a model input.""""""\n    return (self._feature_names is None or name in self._feature_names or\n            name.startswith(self._neighbor_config.prefix))\n\n  def __call__(self, inputs, *args, **kwargs):\n    """"""Calls the layer and updates `inputs` with new features if necessary.\n\n    Args:\n      inputs: A dictionary of tensors keyed by their feature names. See\n        `utils.unpack_neighbor_features` for expected names of neighbor feature\n        and weight tensors. If `inputs` is missing any neighbor feature and\n        weight tensors, the dictionary will be updated with additional inputs\n        corresponding to neighbor features and weights. These additional inputs\n        should be passed to `tf.keras.Model` when using the functional API.\n      *args: Positional arguments forwarded to `call` and `Layer.__call__`.\n      **kwargs: Keyword arguments forwarded to `call` and `Layer.__call__`.\n\n    Returns:\n      A tuple (sample_features, neighbor_features, neighbor_weights) of tensors.\n      See `utils.unpack_neighbor_features` for a detailed description.\n    """"""\n    filtered_inputs = {\n        feature_name: feature_tensor\n        for feature_name, feature_tensor in inputs.items()\n        if self._include_feature(feature_name)\n    }\n    missing_neighbor_inputs = make_missing_neighbor_inputs(\n        self._neighbor_config, filtered_inputs, weight_dtype=self.dtype)\n    # Mutate `inputs` for Functional API.\n    inputs.update(missing_neighbor_inputs)\n    filtered_inputs.update(missing_neighbor_inputs)\n    # Only unpack the relevant inputs.\n    return super(NeighborFeatures, self).__call__(filtered_inputs, *args,\n                                                  **kwargs)\n\n  def get_config(self):\n    config = super(NeighborFeatures, self).get_config()\n    config[\'neighbor_config\'] = attr.asdict(self._neighbor_config)\n    config[\'feature_names\'] = (\n        list(self._feature_names) if self._feature_names is not None else None)\n    return config\n\n  @classmethod\n  def from_config(cls, config):\n    return cls(\n        configs.GraphNeighborConfig(**config.pop(\'neighbor_config\')), **config)\n'"
neural_structured_learning/keras/layers/neighbor_features_test.py,16,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.keras.layers.neighbor_features.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.keras.layers import neighbor_features as neighbor_features_lib\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\ndef _make_model(neighbor_config, inputs, keep_rank, weight_dtype=None):\n  """"""Makes a model that exercises the `NeighborFeatures` layer.\n\n  The model takes a dictionary of sample features as input and unpacks the\n  dictionary into sample features, neighbor features, and neighbor weights.\n\n  Args:\n    neighbor_config: An instance of `configs.GraphNeighborConfig`.\n    inputs: A `tf.keras.Input` or a nested structure of `tf.keras.Input`s.\n    keep_rank: Whether to retain the rank of the original input tensors by\n      merging the neighborhood size with the batch_size dimension, or add an\n      extra neighborhood size dimension.\n    weight_dtype: Optional `tf.DType` for weights.\n\n  Returns:\n    An instance of `tf.keras.Model`.\n  """"""\n  # Create inputs for neighbor features and unpack.\n  neighbor_features_layer = neighbor_features_lib.NeighborFeatures(\n      neighbor_config, weight_dtype=weight_dtype)\n  sample_features, neighbor_features, neighbor_weights = (\n      neighbor_features_layer(inputs, keep_rank=keep_rank))\n  return tf.keras.Model(\n      inputs=inputs,\n      outputs=(sample_features, neighbor_features, neighbor_weights))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass NeighborFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for layers.NeighborFeatures.""""""\n\n  @parameterized.named_parameters([\n      (\'nokeep_rank\', False),\n      (\'keep_rank\', True),\n  ])\n  def testDense(self, keep_rank):\n    """"""Tests creating image neighbors.""""""\n    # Make fake 8x8 images.\n    batch_size = 4\n    image_height = 8\n    image_width = 8\n    features = {\n        \'image\':\n            np.random.randint(\n                0, 256, size=(batch_size, image_height, image_width,\n                              1)).astype(np.uint8),\n        \'NL_nbr_0_image\':\n            np.random.randint(\n                0, 256, size=(batch_size, image_height, image_width,\n                              1)).astype(np.uint8),\n        \'NL_nbr_1_image\':\n            np.random.randint(\n                0, 256, size=(batch_size, image_height, image_width,\n                              1)).astype(np.uint8),\n        \'NL_nbr_2_image\':\n            np.random.randint(\n                0, 256, size=(batch_size, image_height, image_width,\n                              1)).astype(np.uint8),\n        \'NL_nbr_0_weight\':\n            np.random.uniform(size=(batch_size, 1)).astype(np.float32),\n        \'NL_nbr_1_weight\':\n            np.random.uniform(size=(batch_size, 1)).astype(np.float32),\n        \'NL_nbr_2_weight\':\n            np.random.uniform(size=(batch_size, 1)).astype(np.float32),\n    }\n\n    num_neighbors = 3\n    model = _make_model(\n        configs.GraphNeighborConfig(max_neighbors=num_neighbors), {\n            \'image\':\n                tf.keras.Input((image_height, image_width, 1),\n                               dtype=tf.uint8,\n                               name=\'image\'),\n        }, keep_rank)\n    samples, neighbors, weights = self.evaluate(model(features))\n    samples, neighbors = (samples[\'image\'], neighbors[\'image\'])\n    # Check that samples are unchanged.\n    self.assertAllEqual(samples, features[\'image\'])\n    # Check that neighbors and weights are grouped together for each sample.\n    for i in range(batch_size):\n      self.assertAllEqual(\n          neighbors[(i * num_neighbors):((i + 1) * num_neighbors)]\n          if keep_rank else neighbors[i],\n          np.stack([\n              features[\'NL_nbr_0_image\'][i],\n              features[\'NL_nbr_1_image\'][i],\n              features[\'NL_nbr_2_image\'][i],\n          ]))\n      self.assertAllEqual(\n          np.split(weights, batch_size)[i] if keep_rank else weights[i],\n          np.stack([\n              features[\'NL_nbr_0_weight\'][i],\n              features[\'NL_nbr_1_weight\'][i],\n              features[\'NL_nbr_2_weight\'][i],\n          ]))\n\n  @parameterized.named_parameters([\n      (\'nokeep_rank\', False),\n      (\'keep_rank\', True),\n  ])\n  def testSparse(self, keep_rank):\n    """"""Tests the layer with a variable number of neighbors.""""""\n    batch_size = 4\n    input_size = 2\n    features = {\n        \'input\':\n            tf.sparse.from_dense(\n                np.random.normal(size=(batch_size, input_size))),\n        # Every sample but the last has 1 neighbor.\n        \'NL_nbr_0_input\':\n            tf.RaggedTensor.from_row_starts(\n                values=np.random.normal(size=(batch_size - 1) * input_size),\n                row_starts=[0, 2, 4, 6]).to_sparse(),\n        \'NL_nbr_0_weight\':\n            np.expand_dims(np.array([0.9, 0.3, 0.6, 0.]), -1),\n        # Only the 1st and 3rd sample have a second neighbor.\n        \'NL_nbr_1_input\':\n            tf.RaggedTensor.from_row_starts(\n                values=np.random.normal(size=(batch_size - 2) * input_size),\n                row_starts=[0, 2, 2, 4]).to_sparse(),\n        \'NL_nbr_1_weight\':\n            np.expand_dims(np.array([0.25, 0., 0.75, 0.]), -1),\n    }\n\n    model = _make_model(\n        configs.GraphNeighborConfig(max_neighbors=2),\n        {\'input\': tf.keras.Input(input_size, dtype=tf.float64)}, keep_rank,\n        tf.float64)\n    samples, neighbors, weights = self.evaluate(model(features))\n    # Check that samples are unchanged.\n    self.assertAllClose(samples[\'input\'].values,\n                        self.evaluate(features[\'input\'].values))\n    # Check that weights are grouped together and have the right shape.\n    self.assertAllClose(\n        weights,\n        np.array([0.9, 0.25, 0.3, 0., 0.6, 0.75, 0.,\n                  0.]).reshape((batch_size * 2,\n                                1) if keep_rank else (batch_size, 2, 1)))\n    # Check that neighbors are grouped together.\n    dense_neighbors = self.evaluate(tf.sparse.to_dense(neighbors[\'input\'], -1.))\n    neighbor0 = self.evaluate(\n        tf.sparse.to_dense(features[\'NL_nbr_0_input\'], -1))\n    neighbor1 = self.evaluate(\n        tf.sparse.to_dense(features[\'NL_nbr_1_input\'], -1))\n    for i in range(batch_size):\n      actual = (\n          np.split(dense_neighbors, batch_size)[i]\n          if keep_rank else dense_neighbors[i])\n      self.assertAllEqual(actual, np.stack([neighbor0[i], neighbor1[i]]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
neural_structured_learning/keras/layers/pairwise_distance.py,7,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A layer to compute pairwise distances in Neural Structured Learning.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport attr\nimport enum\n\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.lib import distances\nfrom neural_structured_learning.lib import utils\nimport tensorflow as tf\n\n\nclass PairwiseDistance(tf.keras.layers.Layer):\n  """"""A layer for computing a pairwise distance in Keras models.\n\n  With `Model.add_loss`, this layer can be used to build a Keras model with\n  graph regularization.\n\n  Example:\n\n  ```python\n  def regularize_model(unregularized_model, inputs):\n    features, neighbors, neighbor_weights = inputs\n    # Standard logistic regression.\n    logits = unregularized_model(features)\n    model = tf.keras.Model(inputs=inputs, outputs=logits)\n    # Add regularization.\n    regularizer = layers.PairwiseDistance(\n        configs.DistanceConfig(sum_over_axis=-1))\n    graph_loss = regularizer(\n        sources=logits,\n        targets=unregularized_model(neighbors),\n        weights=neighbor_weights)\n    model.add_loss(graph_loss)\n    model.add_metric(graph_loss, aggregation=""mean"", name=""graph_loss"")\n    return model\n  ```\n\n  This layer makes some assumptions about how the input is shaped. Either (a)\n  the first dimension of `sources` should divide the first dimension of\n  `targets`, with the rest of dimensions being equal or (b) `targets` should\n  have an additional neighborhood size dimension at axis -2, the last dimension\n  of `sources` and `targets` should match, and all other dimensions of `sources`\n  should also match with corresponding dimensions in `targets`. See\n  `_replicate_sources` for details.\n  """"""\n\n  def __init__(self, distance_config=None, **kwargs):\n    super(PairwiseDistance, self).__init__(**kwargs)\n    self._distance_config = (\n        configs.DistanceConfig()\n        if distance_config is None else attr.evolve(distance_config))\n\n  def _replicate_sources(self, sources, targets):\n    """"""Replicates `sources` to match the shape of `targets`.\n\n    `targets` should either have an additional neighborhood size dimension at\n    axis -2 or be of the same rank as `sources`. If `targets` has an additional\n    dimension and `sources` has rank k, the first k - 1 dimensions and last\n    dimension of `sources` and `targets` should match. If `sources` and\n    `targets` have the same rank, the last k - 1 dimensions should match and the\n    first dimension of `targets` should be a multiple of the first dimension of\n    `sources`. This multiple represents the fixed neighborhood size of each\n    sample.\n\n    Args:\n      sources: Tensor with shape [..., feature_size] from which distance will be\n        calculated.\n      targets: Either a tensor with shape [..., neighborhood_size, feature_size]\n        or [sources.shape[0] * neighborhood_size] + sources.shape[1:].\n\n    Returns:\n      `sources` replicated to be shape-compatible with `targets`.\n    """"""\n    # Depending on the rank of `sources` and `targets`, decide to broadcast\n    # first, or replicate directly.\n    if (sources.shape.ndims is not None and targets.shape.ndims is not None and\n        sources.shape.ndims + 1 == targets.shape.ndims):\n      return tf.broadcast_to(\n          tf.expand_dims(sources, axis=-2), tf.shape(targets))\n\n    return utils.replicate_embeddings(\n        sources,\n        tf.shape(targets)[0] // tf.shape(sources)[0])\n\n  def call(self, inputs, weights=None):\n    """"""Replicates sources and computes pairwise distance.\n\n    Args:\n      inputs: Symbolic inputs. Should be (sources, targets) if `weights` is\n        non-symbolic. Otherwise, should be (sources, targets, weights).\n      weights: If target weights are not symbolic, `weights` should be passed as\n        a separate argument. In this case, `inputs` should have length 2.\n\n    Returns:\n      Pairwise distance tensor.\n    """"""\n    if weights is None:\n      sources, targets, weights = inputs\n    else:\n      sources, targets = inputs\n\n    return distances.pairwise_distance_wrapper(\n        sources=self._replicate_sources(sources, targets),\n        targets=targets,\n        weights=weights,\n        distance_config=self._distance_config)\n\n  def __call__(self, sources, targets=None, weights=1., **kwargs):\n    # __call__ is overridden so when constructing the model the user can pass\n    # keyword arguments. Within the framework, Keras will always pass arguments\n    # in a list.\n    # If targets is None and len(sources) > 1, assume the function is being\n    # called in a cloned context with all symbolic inputs.\n    if targets is None and len(sources) == 3:\n      return super(PairwiseDistance, self).__call__(sources, **kwargs)\n\n    if targets is None and len(sources) == 2:\n      return super(PairwiseDistance, self).__call__(\n          sources, weights=weights, **kwargs)\n\n    # Otherwise assume that the user is calling the function.\n    if targets is None:\n      raise ValueError(""No targets provided."")\n\n    if tf.get_static_value(weights) is None:\n      return super(PairwiseDistance, self).__call__((sources, targets, weights),\n                                                    **kwargs)\n\n    return super(PairwiseDistance, self).__call__(\n        (sources, targets), weights=tf.get_static_value(weights), **kwargs)\n\n  def get_config(self):\n    distance_config = attr.asdict(self._distance_config)\n    distance_config.update({\n        k: v.value\n        for k, v in distance_config.items()\n        if isinstance(v, enum.Enum)\n    })\n    config = super(PairwiseDistance, self).get_config()\n    config[""distance_config""] = distance_config\n    return config\n\n  @classmethod\n  def from_config(cls, config):\n    return cls(\n        configs.DistanceConfig(**config[""distance_config""]),\n        name=config.get(""name""))\n'"
neural_structured_learning/keras/layers/pairwise_distance_test.py,14,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for neural_structured_learning.keras.layers.pairwise_distance.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport neural_structured_learning.configs as configs\nfrom neural_structured_learning.keras.layers import pairwise_distance as pairwise_distance_lib\nimport numpy as np\nfrom scipy import special\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n_ERR_TOL = 3e-5  # Tolerance when comparing floats.\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass PairwiseDistanceTest(tf.test.TestCase):\n  """"""Tests for `pairwise_distance_lib.PairwiseDistance`.""""""\n\n  def testName(self):\n    """"""Tests that the name is propagated to the base layer.""""""\n    regularizer = pairwise_distance_lib.PairwiseDistance()\n    self.assertEqual(regularizer.name, \'pairwise_distance\')\n\n    regularizer = pairwise_distance_lib.PairwiseDistance(name=\'regularizer\')\n    self.assertEqual(regularizer.name, \'regularizer\')\n\n  def testCall(self):\n    """"""Makes a function from config and runs it.""""""\n    regularizer = pairwise_distance_lib.PairwiseDistance(\n        configs.DistanceConfig(\n            distance_type=configs.DistanceType.KL_DIVERGENCE, sum_over_axis=-1),\n        name=\'kl_loss\')\n    # Run a computation.\n    example = np.array([0.3, 0.3, 0.4])\n    neighbors = np.array([[0.9, 0.05, 0.05]])\n    kl_loss = self.evaluate(regularizer(example, neighbors))\n    # Assert correctness of KL divergence calculation.\n    self.assertNear(kl_loss, np.sum(special.kl_div(example, neighbors)),\n                    _ERR_TOL)\n\n  def testWeights(self):\n    """"""Tests that weights are propagated to the distance function.""""""\n    regularizer = pairwise_distance_lib.PairwiseDistance(\n        configs.DistanceConfig(\n            distance_type=configs.DistanceType.KL_DIVERGENCE, sum_over_axis=-1),\n        name=\'weighted_kl_loss\')\n    example = np.array([0.1, 0.4, 0.5])\n    neighbors = np.array([[0.6, 0.2, 0.2], [0.9, 0.01, 0.09]])\n    neighbor_weight = 0.5\n    loss = self.evaluate(regularizer(example, neighbors, neighbor_weight))\n    self.assertAllClose(\n        loss,\n        neighbor_weight *\n        np.mean(np.sum(special.kl_div(example, neighbors), -1)), _ERR_TOL)\n\n  def testAssertions(self):\n    """"""Tests that assertions still work with Keras.""""""\n    distance_config = configs.DistanceConfig(\n        distance_type=configs.DistanceType.JENSEN_SHANNON_DIVERGENCE,\n        sum_over_axis=-1)\n    regularizer = pairwise_distance_lib.PairwiseDistance(distance_config)\n    # Try Jennsen-Shannon divergence on an improper probability distribution.\n    with self.assertRaisesRegex(\n        tf.errors.InvalidArgumentError,\n        \'x and/or y is not a proper probability distribution\'):\n      self.evaluate(regularizer(np.array([0.6, 0.5]), np.array([[0.25, 0.75]])))\n\n  def testCallOverride(self):\n    """"""Tests the overrides of Layer.__call__.""""""\n\n    # Default distance configuration is mean squared error.\n    def _distance_fn(x, y):\n      return np.mean(np.square(x - y))\n\n    # Common input.\n    sources = np.array([[1., 1., 1., 1.]])\n    targets = np.array([[[4., 3., 2., 1.]]])\n    unweighted_distance = _distance_fn(sources, targets)\n\n    def _make_symbolic_weights_model():\n      """"""Makes a model where the weights are provided as input.""""""\n      # Shape doesn\'t include batch dimension.\n      inputs = {\n          \'sources\': tf.keras.Input(4),\n          \'targets\': tf.keras.Input((1, 4)),\n          \'weights\': tf.keras.Input((1, 1)),\n      }\n      pairwise_distance_fn = pairwise_distance_lib.PairwiseDistance()\n      outputs = pairwise_distance_fn(**inputs)\n      return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    weights = np.array([[[2.]]])\n    expected_distance = unweighted_distance * weights\n    model = _make_symbolic_weights_model()\n    self.assertNear(\n        self.evaluate(\n            model({\n                \'sources\': sources,\n                \'targets\': targets,\n                \'weights\': weights,\n            })), expected_distance, _ERR_TOL)\n\n    def _make_fixed_weights_model(weights):\n      """"""Makes a model where the weights are a static constant.""""""\n      # Shape doesn\'t include batch dimension.\n      inputs = {\n          \'sources\': tf.keras.Input(4),\n          \'targets\': tf.keras.Input((1, 4)),\n      }\n      pairwise_distance_fn = pairwise_distance_lib.PairwiseDistance()\n      outputs = pairwise_distance_fn(weights=weights, **inputs)\n      return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model = _make_fixed_weights_model(0.25)\n    expected_distance = 0.25 * unweighted_distance\n    self.assertNear(\n        self.evaluate(model({\n            \'sources\': sources,\n            \'targets\': targets,\n        })), expected_distance, _ERR_TOL)\n    # Considers invalid input.\n    with self.assertRaisesRegex(ValueError, \'No targets provided\'):\n      pairwise_distance_lib.PairwiseDistance()(np.ones(5))\n\n  def testReplicateSources(self):\n    """"""Tests when sources and targets have the same rank.""""""\n\n    def _make_model(sources_shape, targets_shape):\n      """"""Makes a model where `sources` and `targets` have the same rank.""""""\n      sources = tf.keras.Input(sources_shape, name=\'sources\')\n      targets = tf.keras.Input(targets_shape, name=\'targets\')\n      outputs = pairwise_distance_lib.PairwiseDistance(\n          configs.DistanceConfig(\n              distance_type=configs.DistanceType.KL_DIVERGENCE,\n              reduction=tf.compat.v1.losses.Reduction.NONE,\n              sum_over_axis=-1))(sources, targets)\n      return tf.keras.Model(inputs=[sources, targets], outputs=outputs)\n\n    model = _make_model(sources_shape=(4,), targets_shape=(4,))\n    # Test when first dimension of targets is a multiple of the batch size.\n    sources = np.array([\n        [0.1, 0.2, 0.3, 0.4],\n        [0.25, 0.25, 0.25, 0.25],\n    ])\n    targets = np.array([\n        [0.6, 0.2, 0.1, 0.1],\n        [0.4, 0.5, 0.075, 0.025],\n        [0.001, 0.333, 0.333, 0.333],\n        [0.9, 0.05, 0.03, 0.02],\n    ])\n    kl_divergence = self.evaluate(model([sources, targets]))\n    expected = np.sum(\n        special.kl_div(np.repeat(sources, 2, axis=0), targets),\n        -1,\n        keepdims=True)\n    self.assertAllClose(kl_divergence, expected, _ERR_TOL)\n    # Test when that the shapes are not compatible.\n    with self.assertRaisesRegex(ValueError, \'Shapes\\\\s.+\\\\sare incompatible\'):\n      self.evaluate(model([sources, targets[1:]]))\n    # And also the case when targets has a neighborhood size dimension.\n    model = _make_model(sources_shape=(4,), targets_shape=(2, 4))\n    self.assertAllClose(\n        self.evaluate(model([sources, targets.reshape((-1, 2, 4))])),\n        expected.reshape((-1, 2, 1)), _ERR_TOL)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
research/gam/gam/__init__.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
research/kg_hyp_emb/datasets/__init__.py,0,b''
research/kg_hyp_emb/datasets/datasets.py,2,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Dataset class for loading and processing KG datasets.""""""\n\nimport os\nimport pickle as pkl\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass DatasetFn(object):\n  """"""Knowledge Graph dataset class.""""""\n\n  def __init__(self, data_path, debug):\n    """"""Creates KG dataset object for data loading.\n\n    Args:\n      data_path: Path to directory containing train/valid/test pickle files\n        produced by process.py.\n      debug: boolean indicating whether to use debug mode or not. If true, the\n        dataset will only contain 1000 examples for debugging.\n    """"""\n    self.data_path = data_path\n    self.debug = debug\n    self.data = {}\n    for split in [\'train\', \'test\', \'valid\']:\n      file_path = os.path.join(self.data_path, split + \'.pickle\')\n      with open(file_path, \'rb\') as in_file:\n        self.data[split] = pkl.load(in_file)\n    filters_file = open(os.path.join(self.data_path, \'to_skip.pickle\'), \'rb\')\n    self.to_skip = pkl.load(filters_file)\n    filters_file.close()\n    max_axis = np.max(self.data[\'train\'], axis=0)\n    self.n_entities = int(max(max_axis[0], max_axis[2]) + 1)\n    self.n_predicates = int(max_axis[1] + 1) * 2\n\n  def get_filters(self,):\n    """"""Return filter dict to compute ranking metrics in the filtered setting.""""""\n    return self.to_skip\n\n  def get_examples(self, split):\n    """"""Get examples in a split.\n\n    Args:\n      split: String indicating the split to use (train/valid/test).\n\n    Returns:\n      examples: tf.data.Dataset contatining KG triples in a split.\n    """"""\n    examples = self.data[split]\n    if split == \'train\':\n      copy = np.copy(examples)\n      tmp = np.copy(copy[:, 0])\n      copy[:, 0] = copy[:, 2]\n      copy[:, 2] = tmp\n      copy[:, 1] += self.n_predicates // 2\n      examples = np.vstack((examples, copy))\n    if self.debug:\n      examples = examples[:1000]\n      examples = examples.astype(np.int64)\n    tf_dataset = tf.data.Dataset.from_tensor_slices(examples)\n    if split == \'train\':\n      buffer_size = examples.shape[0]\n      tf_dataset.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\n    return tf_dataset\n\n  def get_shape(self):\n    """"""Returns KG dataset shape.""""""\n    return self.n_entities, self.n_predicates, self.n_entities\n'"
research/kg_hyp_emb/datasets/process.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""KG dataset pre-processing functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport pickle\n\nimport numpy as np\n\n\ndef get_idx(path):\n  """"""Maps entities and relations to unique ids.\n\n  Args:\n    path: path to directory with raw dataset files (tab-separated\n      tain/valid/test triples).\n\n  Returns:\n    ent2idx: Dictionary mapping raw entities to unique ids.\n    rel2idx: Dictionary mapping raw relations to unique ids.\n  """"""\n  entities, relations = set(), set()\n  for split in [\'train\', \'valid\', \'test\']:\n    with open(os.path.join(path, split), \'r\') as lines:\n      for line in lines:\n        lhs, rel, rhs = line.strip().split(\'\\t\')\n        entities.add(lhs)\n        entities.add(rhs)\n        relations.add(rel)\n  ent2idx = {x: i for (i, x) in enumerate(sorted(entities))}\n  rel2idx = {x: i for (i, x) in enumerate(sorted(relations))}\n  return ent2idx, rel2idx\n\n\ndef to_np_array(dataset_file, ent2idx, rel2idx):\n  """"""Map raw dataset file to numpy array with unique ids.\n\n  Args:\n    dataset_file: Path to file contatining raw triples in a split.\n    ent2idx: Dictionary mapping raw entities to unique ids.\n    rel2idx: Dictionary mapping raw relations to unique ids.\n\n  Returns:\n    Numpy array of size n_examples x 3 mapping the raw dataset file to ids.\n  """"""\n  examples = []\n  with open(dataset_file, \'r\') as lines:\n    for line in lines:\n      lhs, rel, rhs = line.strip().split(\'\\t\')\n      try:\n        examples.append([ent2idx[lhs], rel2idx[rel], ent2idx[rhs]])\n      except ValueError:\n        continue\n  return np.array(examples).astype(\'int64\')\n\n\ndef get_filters(examples, n_relations):\n  """"""Create filtering lists for evaluation.\n\n  Args:\n    examples: Numpy array of size n_examples x 3 contatining KG triples.\n    n_relations: Int indicating the total number of relations in the KG.\n\n  Returns:\n    lhs_final: Dictionary mapping queries (entity, relation) to filtered\n               entities for left-hand-side prediction.\n    rhs_final: Dictionary mapping queries (entity, relation) to filtered\n               entities for right-hand-side prediction.\n  """"""\n  lhs_filters = collections.defaultdict(set)\n  rhs_filters = collections.defaultdict(set)\n  for lhs, rel, rhs in examples:\n    rhs_filters[(lhs, rel)].add(rhs)\n    lhs_filters[(rhs, rel + n_relations)].add(lhs)\n  lhs_final = {}\n  rhs_final = {}\n  for k, v in lhs_filters.items():\n    lhs_final[k] = sorted(list(v))\n  for k, v in rhs_filters.items():\n    rhs_final[k] = sorted(list(v))\n  return lhs_final, rhs_final\n\n\ndef process_dataset(path):\n  """"""Maps entities and relations to ids and saves corresponding pickle arrays.\n\n  Args:\n    path: Path to dataset directory.\n\n  Returns:\n    examples: Dictionary mapping splits to with Numpy array contatining\n              corresponding KG triples.\n    filters: Dictionary containing filters for lhs and rhs predictions.\n  """"""\n  ent2idx, rel2idx = get_idx(dataset_path)\n  examples = {}\n  splits = [\'train\', \'valid\', \'test\']\n  for split in splits:\n    dataset_file = os.path.join(path, split)\n    examples[split] = to_np_array(dataset_file, ent2idx, rel2idx)\n  all_examples = np.concatenate([examples[split] for split in splits], axis=0)\n  lhs_skip, rhs_skip = get_filters(all_examples, len(rel2idx))\n  filters = {\'lhs\': lhs_skip, \'rhs\': rhs_skip}\n  return examples, filters\n\n\nif __name__ == \'__main__\':\n  for dataset_name in os.listdir(\'data/\'):\n    dataset_path = os.path.join(\'data/\', dataset_name)\n    dataset_examples, dataset_filters = process_dataset(dataset_path)\n    for dataset_split in [\'train\', \'valid\', \'test\']:\n      save_path = os.path.join(dataset_path, dataset_split + \'.pickle\')\n      with open(save_path, \'wb\') as save_file:\n        pickle.dump(dataset_examples[dataset_split], save_file)\n    with open(os.path.join(dataset_path, \'to_skip.pickle\'), \'wb\') as save_file:\n      pickle.dump(dataset_filters, save_file)\n'"
research/kg_hyp_emb/learning/__init__.py,0,b''
research/kg_hyp_emb/learning/losses.py,11,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Loss functions for KG with support for optional negative sampling.""""""\n\nimport abc\nimport tensorflow as tf\n\n\nclass LossFn(abc.ABC):\n  """"""Abstract loss function for KG embeddings.""""""\n\n  def __init__(self, sizes, neg_sample_size):\n    """"""Initialize KG loss function.\n\n    Args:\n      sizes: Tuple of size 3 containing (n_entities, n_rels, n_entities).\n      neg_sample_size: Integer indicating the number of negative samples to use.\n    """"""\n    self.n_entities = sizes[0]\n    self.n_predicates = sizes[1]\n    self.neg_sample_size = neg_sample_size\n    self.use_neg_sampling = neg_sample_size > 0\n    self.gamma = tf.Variable(\n        self.neg_sample_size * tf.keras.backend.ones(1) / self.n_entities,\n        trainable=False)\n\n  @abc.abstractmethod\n  def loss_from_logits(self, logits, full_labels, labels):\n    """"""Computes KG loss.\n\n    Args:\n      logits: Tensor of size batch_size x n_entities containing predictions.\n      full_labels: Tensor of size batch_size x n_entities containing one-hot\n        labels.\n      labels: Tensor of size batch_size x 1 containing sparse labels (index of\n        correct tail entity).\n\n    Returns:\n      Average loss within batch.\n    """"""\n    pass\n\n  def get_neg_sample_mask(self, logits, full_labels):\n    """"""Generates negative sampling mask.\n\n    Args:\n      logits: Tensor of size batch_size x n_entities containing predictions.\n      full_labels: Tensor of size batch_size x n_entities containing one-hot\n        labels.\n\n    Returns:\n      neg_sample_mask: Tensor of size batch_size x n_entities with ones and\n                       zeros (one indicates that the corresonding example\n                       is masked).\n    """"""\n    neg_sample_mask = tf.random.uniform(tf.shape(logits), dtype=logits.dtype)\n    neg_sample_mask = tf.cast(neg_sample_mask > self.gamma, logits.dtype)\n    neg_sample_mask = -1e6 * tf.maximum(neg_sample_mask - full_labels, 0)\n    return neg_sample_mask\n\n  def calculate_loss(self, model, input_batch):\n    """"""Computes loss with or without negative sampling.\n\n    Args:\n      model: tf.keras.Model KG embedding model.\n      input_batch: Tensor of size batch_size x 3 containing input triples.\n\n    Returns:\n      Average loss within the input_batch.\n    """"""\n    labels = input_batch[:, 2]\n    logits = model(input_batch, eval_mode=True)\n    full_labels = tf.one_hot(labels, depth=self.n_entities, dtype=logits.dtype)\n    if self.use_neg_sampling:\n      # mask some values for negative sampling\n      neg_sample_mask = self.get_neg_sample_mask(logits, full_labels)\n      # mask logits to only keep target and negative examples\' scores\n      logits = logits + neg_sample_mask\n    return self.loss_from_logits(logits, full_labels, labels)\n\n\nclass SigmoidCrossEntropy(LossFn):\n  """"""Sigmoid cross entropy loss.""""""\n\n  def loss_from_logits(self, logits, full_labels, labels):\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(full_labels, logits))\n\n\nclass SoftmaxCrossEntropy(LossFn):\n  """"""Softmax cross entropy loss.""""""\n\n  def loss_from_logits(self, logits, full_labels, labels):\n    return tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits))\n'"
research/kg_hyp_emb/learning/regularizers.py,8,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Regularizers for KG embeddings.""""""\n\nimport abc\nimport tensorflow as tf\n\n\nclass KGRegularizer(tf.keras.regularizers.Regularizer, abc.ABC):\n  """"""KG embedding regularizers.""""""\n\n  def __init__(self, reg_weight):\n    """"""Initializes KG embedding regularizer.\n\n    Args:\n      reg_weight: regularization weight\n    """"""\n    super(KGRegularizer, self).__init__()\n    self.reg_weight = tf.keras.backend.cast_to_floatx(reg_weight)\n\n  def __call__(self, x):\n    """"""Compute regularization for input embeddings.\n\n    Args:\n      x: Tensor of size batch_size x embedding_dimension to regularize.\n\n    Returns:\n      Regularization term.\n    """"""\n    if not self.reg_weight:\n      return tf.keras.backend.constant(0.)\n    else:\n      return self.reg_weight * self.compute_norm(x)\n\n  @abc.abstractmethod\n  def compute_norm(self, x):\n    """"""Computes embeddings\' norms for regularization.""""""\n    pass\n\n  def get_config(self):\n    return {\'reg_weight\': float(self.reg_weight)}\n\n\nclass NoReg(KGRegularizer):\n  """"""No regularization.""""""\n\n  def __call__(self, x):\n    return tf.keras.backend.constant(0.)\n\n  def get_config(self):\n    return {\'reg_weight\': 0}\n\n\nclass L1(KGRegularizer):\n  """"""L1 regularization.""""""\n\n  def compute_norm(self, x):\n    return tf.reduce_sum(tf.abs(x))\n\n\nclass L2(KGRegularizer):\n  """"""L2 regularization.""""""\n\n  def compute_norm(self, x):\n    return tf.reduce_sum(tf.square(x))\n\n\nclass L3(KGRegularizer):\n  """"""L3 regularization.""""""\n\n  def compute_norm(self, x):\n    return tf.reduce_sum(tf.abs(x)**3)\n\n\nclass N2(KGRegularizer):\n  """"""Nuclear 2-norm regularization.""""""\n\n  def compute_norm(self, x):\n    return tf.reduce_sum(tf.norm(x, ord=2, axis=1)**3)\n'"
research/kg_hyp_emb/learning/trainer.py,15,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Module to train knowledge graph embedding models.""""""\n\nfrom kg_hyp_emb.learning import losses\nimport tensorflow as tf\n\n\nclass KGTrainer(object):\n  """"""KG embedding trainer object.""""""\n\n  def __init__(self, sizes, args):\n    """"""Initialize KG trainer.\n\n    Args:\n      sizes: Tuple of size 3 containing (n_entities, n_rels, n_entities).\n      args: Namespace with config arguments (see config.py for detailed overview\n        of arguments supported).\n    """"""\n    if args.optimizer == \'Adagrad\':\n      self.optimizer = tf.keras.optimizers.Adagrad(\n          learning_rate=args.lr, initial_accumulator_value=0.0, epsilon=1e-10)\n    elif args.optimizer == \'Adam\':\n      self.optimizer = tf.keras.optimizers.Adam(\n          learning_rate=args.lr,\n          beta_1=0.9,\n          beta_2=0.999,\n          epsilon=1e-08,\n          amsgrad=False)\n    else:\n      self.optimizer = getattr(tf.keras.optimizers, args.optimizer)(\n          learning_rate=args.lr)\n    self.loss_fn = getattr(losses, args.loss_fn)(sizes, args.neg_sample_size)\n    self.lr_decay = args.lr_decay\n    self.min_lr = args.min_lr\n\n  def reduce_lr(self,):\n    """"""Reduces learning rate.""""""\n    old_lr = float(tf.keras.backend.get_value(self.optimizer.lr))\n    if old_lr > self.min_lr:\n      new_lr = old_lr * self.lr_decay\n      new_lr = max(new_lr, self.min_lr)\n      tf.keras.backend.set_value(self.optimizer.lr, new_lr)\n\n  def valid_step(self, model, examples):\n    """"""Computes validation loss.\n\n    Args:\n      model: tf.keras.Model KG embedding model.\n      examples: tf.data.Dataset containing KG validation triples.\n\n    Returns:\n      Average validation loss.\n    """"""\n    total_loss = tf.keras.backend.constant(0.0)\n    counter = tf.keras.backend.constant(0.0)\n    for input_batch in examples:\n      counter += 1.0\n      total_loss += self.loss_fn.calculate_loss(model, input_batch)\n    return total_loss / counter\n\n  @tf.function\n  def train_step(self, model, examples):\n    """"""Compute training loss and back-propagate gradients.\n\n    Args:\n      model: tf.keras.Model KG embedding model.\n      examples: tf.data.Dataset containing KG training triples.\n\n    Returns:\n      Average training loss.\n    """"""\n    total_loss = tf.keras.backend.constant(0.0)\n    counter = tf.keras.backend.constant(0.0)\n    for input_batch in examples:\n      counter += 1.0\n      with tf.GradientTape() as tape:\n        loss = self.loss_fn.calculate_loss(model, input_batch)\n      gradients = zip(\n          tape.gradient(loss, model.trainable_variables),\n          model.trainable_variables)\n      self.optimizer.apply_gradients(gradients)\n      total_loss += loss\n    return total_loss / counter\n'"
research/kg_hyp_emb/models/__init__.py,0,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Knowledge graph embedding models.""""""\n\nfrom kg_hyp_emb.models.complex import Complex\nfrom kg_hyp_emb.models.complex import RotatE\nfrom kg_hyp_emb.models.euclidean import AttE\nfrom kg_hyp_emb.models.euclidean import CTDecomp\nfrom kg_hyp_emb.models.euclidean import MurE\nfrom kg_hyp_emb.models.euclidean import RefE\nfrom kg_hyp_emb.models.euclidean import RotE\nfrom kg_hyp_emb.models.euclidean import TransE\nfrom kg_hyp_emb.models.hyperbolic import AttH\nfrom kg_hyp_emb.models.hyperbolic import RefH\nfrom kg_hyp_emb.models.hyperbolic import RotH\nfrom kg_hyp_emb.models.hyperbolic import TransH\n'"
research/kg_hyp_emb/models/base.py,10,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Abstract class for Knowledge Graph embedding models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nfrom kg_hyp_emb.learning import regularizers\nimport numpy as np\nimport tensorflow as tf\n\n\nclass KGModel(tf.keras.Model, abc.ABC):\n  """"""Abstract Knowledge Graph embedding model class.\n\n  Module to define basic operations in KG embedding models, including embedding\n  initialization, computing embeddings and triples\' scores.\n  """"""\n\n  def __init__(self, sizes, args):\n    """"""Initialize KG embedding model.\n\n    Args:\n      sizes: Tuple of size 3 containing (n_entities, n_rels, n_entities).\n      args: Namespace with config arguments (see config.py for detailed overview\n        of arguments supported).\n    """"""\n    super(KGModel, self).__init__()\n    self.sizes = sizes\n    self.rank = args.rank\n    self.bias = args.bias\n    self.initializer = getattr(tf.keras.initializers, args.initializer)\n    self.entity_regularizer = getattr(regularizers, args.regularizer)(\n        args.entity_reg)\n    self.rel_regularizer = getattr(regularizers, args.regularizer)(args.rel_reg)\n    self.entity = tf.keras.layers.Embedding(\n        input_dim=sizes[0],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.entity_regularizer,\n        name=\'entity_embeddings\')\n    self.rel = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'relation_embeddings\')\n    train_biases = self.bias == \'learn\'\n    self.bh = tf.keras.layers.Embedding(\n        input_dim=sizes[0],\n        output_dim=1,\n        embeddings_initializer=\'zeros\',\n        name=\'head_biases\',\n        trainable=train_biases)\n    self.bt = tf.keras.layers.Embedding(\n        input_dim=sizes[0],\n        output_dim=1,\n        embeddings_initializer=\'zeros\',\n        name=\'tail_biases\',\n        trainable=train_biases)\n    self.gamma = tf.Variable(\n        initial_value=args.gamma * tf.keras.backend.ones(1), trainable=False)\n\n  @abc.abstractmethod\n  def get_queries(self, input_tensor):\n    """"""Get query embeddings using head and relationship for an index tensor.\n\n    Args:\n      input_tensor: Tensor of size batch_size x 3 containing triples\' indices.\n\n    Returns:\n      Tensor of size batch_size x embedding_dimension representing queries\'\n      embeddings.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def get_rhs(self, input_tensor):\n    """"""Get right hand side (tail) embeddings for an index tensor.\n\n    Args:\n      input_tensor: Tensor of size batch_size x 3 containing triples\' indices.\n\n    Returns:\n      Tensor of size batch_size x embedding_dimension representing tail\n      entities\' embeddings.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def get_candidates(self):\n    """"""Get all candidate tail embeddings in a knowledge graph dataset.\n\n    Returns:\n      Tensor of size n_entities x embedding_dimension representing embeddings\n      for all enitities in the KG.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def similarity_score(self, lhs, rhs, eval_mode):\n    """"""Computes a similarity score between queries and tail embeddings.\n\n    Args:\n      lhs: Tensor of size B1 x embedding_dimension containing queries\'\n        embeddings.\n      rhs: Tensor of size B2 x embedding_dimension containing tail entities\'\n        embeddings.\n      eval_mode: boolean to indicate whether to compute all pairs of scores or\n        not. If False, B1 must be equal to B2.\n\n    Returns:\n      Tensor representing similarity scores. If eval_mode is False, this tensor\n      has size B1 x 1, otherwise it has size B1 x B2.\n    """"""\n    pass\n\n  def call(self, input_tensor, eval_mode=False):\n    """"""Forward pass of KG embedding models.\n\n    Args:\n      input_tensor: Tensor of size batch_size x 3 containing triples\' indices.\n      eval_mode: boolean to indicate whether to compute scores against all\n        possible tail entities in the KG, or only individual triples\' scores.\n\n    Returns:\n      Tensor containing triple scores. If eval_mode is False, this tensor\n      has size batch_size x 1, otherwise it has size batch_size x n_entities\n      where n_entities is the total number of entities in the KG.\n    """"""\n    lhs = self.get_queries(input_tensor)\n    lhs_biases = self.bh(input_tensor[:, 0])\n    if eval_mode:\n      rhs = self.get_candidates()\n      rhs_biases = self.bt.embeddings\n    else:\n      rhs = self.get_rhs(input_tensor)\n      rhs_biases = self.bt(input_tensor[:, 2])\n    predictions = self.score(lhs, lhs_biases, rhs, rhs_biases, eval_mode)\n    return predictions\n\n  def score(self, lhs, lhs_biases, rhs, rhs_biases, eval_mode):\n    """"""Compute triple scores using embeddings and biases.""""""\n    score = self.similarity_score(lhs, rhs, eval_mode)\n    if self.bias == \'constant\':\n      return score + self.gamma\n    elif self.bias == \'learn\':\n      if eval_mode:\n        return score + lhs_biases + tf.transpose(rhs_biases)\n      else:\n        return score + lhs_biases + rhs_biases\n    else:\n      return score\n\n  def get_scores_targets(self, input_tensor):\n    """"""Computes triples\' scores as well as scores againts all possible entities.\n\n    Args:\n      input_tensor: Tensor of size batch_size x 3 containing triples\' indices.\n\n    Returns:\n      scores: Numpy array of size batch_size x n_entities containing queries\'\n              scores against all possible entities in the KG.\n      targets: Numpy array of size batch_size x 1 containing triples\' scores.\n    """"""\n    cand = self.get_candidates()\n    cand_biases = self.bt.embeddings\n    lhs = self.get_queries(input_tensor)\n    lhs_biases = self.bh(input_tensor[:, 0])\n    rhs = self.get_rhs(input_tensor)\n    rhs_biases = self.bt(input_tensor[:, 2])\n    scores = self.score(lhs, lhs_biases, cand, cand_biases, eval_mode=True)\n    targets = self.score(lhs, lhs_biases, rhs, rhs_biases, eval_mode=False)\n    return scores.numpy(), targets.numpy()\n\n  def eval(self, examples, filters, batch_size=1000):\n    """"""Compute ranking-based evaluation metrics.\n\n    Args:\n      examples: Tensor of size n_examples x 3 containing triples\' indices.\n      filters: Dict representing entities to skip per query for evaluation in\n        the filtered setting.\n      batch_size: batch size to use to compute scores.\n\n    Returns:\n      Evaluation metrics (mean rank, mean reciprocical rank and hits).\n    """"""\n    mean_rank = {}\n    mean_reciprocal_rank = {}\n    hits_at = {}\n    total_examples = examples.cardinality().numpy()\n    batch_size = min(batch_size, total_examples)\n    for missing in [\'rhs\', \'lhs\']:\n      ranks = np.ones(total_examples)\n      for counter, input_tensor in enumerate(examples.batch(batch_size)):\n        if batch_size * counter >= total_examples:\n          break\n        # reverse triple for head prediction\n        if missing == \'lhs\':\n          input_tensor = tf.concat([\n              input_tensor[:, 2:], input_tensor[:, 1:2] + self.sizes[1] // 2,\n              input_tensor[:, 0:1]\n          ],\n                                   axis=1)\n        scores, targets = self.get_scores_targets(input_tensor)\n        for i, query in enumerate(input_tensor):\n          query = query.numpy()\n          filter_out = filters[missing][(query[0], query[1])]\n          filter_out += [query[2]]\n          scores[i, filter_out] = -1e6\n        ranks[counter * batch_size:(counter + 1) * batch_size] += np.sum(\n            (scores >= targets), axis=1)\n\n      # compute ranking metrics\n      mean_rank[missing] = np.mean(ranks)\n      mean_reciprocal_rank[missing] = np.mean(1. / ranks)\n      hits_at[missing] = {}\n      for k in (1, 3, 10):\n        hits_at[missing][k] = np.mean(ranks <= k)\n    return mean_rank, mean_reciprocal_rank, hits_at\n'"
research/kg_hyp_emb/models/complex.py,11,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Complex embedding models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom kg_hyp_emb.models.base import KGModel\nimport tensorflow as tf\n\n\nclass BaseC(KGModel):\n  """"""Base model class for complex embeddings.""""""\n\n  def __init__(self, sizes, args):\n    """"""Initialize complex KG embedding model.\n\n    Args:\n      sizes: Tuple of size 3 containing (n_entities, n_rels, n_entities).\n      args: Namespace with config arguments (see config.py for detailed overview\n        of arguments supported).\n    """"""\n    assert args.rank % 2 == 0, (""Complex models must have an even embedding ""\n                                ""dimension."")\n    super(BaseC, self).__init__(sizes, args)\n    self.half_rank = self.rank // 2\n\n  def get_rhs(self, input_tensor):\n    return self.entity(input_tensor[:, 2])\n\n  def get_candidates(self,):\n    return self.entity.embeddings\n\n  def similarity_score(self, lhs, rhs, eval_mode):\n    lhs = lhs[:, :self.half_rank], lhs[:, self.half_rank:]\n    rhs = rhs[:, :self.half_rank], rhs[:, self.half_rank:]\n    if eval_mode:\n      return tf.matmul(lhs[0], tf.transpose(rhs[0])) + tf.matmul(\n          lhs[1], tf.transpose(rhs[1]))\n    else:\n      return tf.reduce_sum(\n          lhs[0] * rhs[0] + lhs[1] * rhs[1], axis=-1, keepdims=True)\n\n  def get_factors(self, input_tensor):\n    lhs = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    rhs = self.entity(input_tensor[:, 2])\n\n    lhs = lhs[:, :self.half_rank], lhs[:, self.half_rank:]\n    rel = rel[:, :self.half_rank], rel[:, self.half_rank:]\n    rhs = rhs[:, :self.half_rank], rhs[:, self.half_rank:]\n\n    lhs = tf.sqrt(lhs[0]**2 + lhs[1]**2)\n    rel = tf.sqrt(rel[0]**2 + rel[1]**2)\n    rhs = tf.sqrt(rhs[0]**2 + rhs[1]**2)\n\n    return lhs, rel, rhs\n\n\nclass Complex(BaseC):\n  """"""Complex embeddings for simple link prediction.""""""\n\n  def get_queries(self, input_tensor):\n    lhs = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    lhs = lhs[:, :self.half_rank], lhs[:, self.half_rank:]\n    rel = rel[:, :self.half_rank], rel[:, self.half_rank:]\n\n    return tf.concat(\n        [lhs[0] * rel[0] - lhs[1] * rel[1], lhs[0] * rel[1] + lhs[1] * rel[0]],\n        axis=1)\n\n\nclass RotatE(BaseC):\n  """"""Complex embeddings with Euclidean rotations.""""""\n\n  def get_queries(self, input_tensor):\n    lhs = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    lhs = lhs[:, :self.half_rank], lhs[:, self.half_rank:]\n    rel = rel[:, :self.half_rank], rel[:, self.half_rank:]\n\n    rel_norm = tf.sqrt(rel[0]**2 + rel[1]**2)\n    cos = tf.math.divide_no_nan(rel[0], rel_norm)\n    sin = tf.math.divide_no_nan(rel[1], rel_norm)\n\n    return tf.concat([lhs[0] * cos - lhs[1] * sin, lhs[0] * sin + lhs[1] * cos],\n                     axis=1)\n'"
research/kg_hyp_emb/models/euclidean.py,17,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Euclidean embedding models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom kg_hyp_emb.models.base import KGModel\nfrom kg_hyp_emb.utils import euclidean as euc_utils\nimport numpy as np\nimport tensorflow as tf\n\n\nclass BaseE(KGModel):\n  """"""Base model class for Euclidean embeddings.""""""\n\n  def get_rhs(self, input_tensor):\n    rhs = self.entity(input_tensor[:, 2])\n    return rhs\n\n  def get_candidates(self,):\n    cands = self.entity.embeddings\n    return cands\n\n  def similarity_score(self, lhs, rhs, eval_mode):\n    if self.sim == \'dot\':\n      if eval_mode:\n        score = tf.matmul(lhs, tf.transpose(rhs))\n      else:\n        score = tf.reduce_sum(lhs * rhs, axis=-1, keepdims=True)\n    elif self.sim == \'dist\':\n      score = -euc_utils.euc_sq_distance(lhs, rhs, eval_mode)\n    else:\n      raise AttributeError(\'Similarity function {} not recognized\'.format(\n          self.sim))\n    return score\n\n\nclass CTDecomp(BaseE):\n  """"""Canonical tensor decomposition.""""""\n\n  def __init__(self, sizes, args):\n    super(CTDecomp, self).__init__(sizes, args)\n    self.sim = \'dot\'\n\n  def get_queries(self, input_tensor):\n    entity = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    return tf.multiply(entity, rel)\n\n\nclass TransE(BaseE):\n  """"""Euclidean translations.""""""\n\n  def __init__(self, sizes, args):\n    super(TransE, self).__init__(sizes, args)\n    self.sim = \'dist\'\n\n  def get_queries(self, input_tensor):\n    entity = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    return entity + rel\n\n\nclass RotE(BaseE):\n  """"""2x2 Givens rotations.""""""\n\n  def __init__(self, sizes, args):\n    super(RotE, self).__init__(sizes, args)\n    self.rel_diag = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'rotation_weights\')\n    self.sim = \'dist\'\n\n  def get_queries(self, input_tensor):\n    entity = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    rel_diag = self.rel_diag(input_tensor[:, 1])\n    return euc_utils.givens_rotations(rel_diag, entity) + rel\n\n\nclass RefE(BaseE):\n  """"""2x2 Givens reflections.""""""\n\n  def __init__(self, sizes, args):\n    super(RefE, self).__init__(sizes, args)\n    self.rel_diag = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'reflection_weights\')\n    self.sim = \'dist\'\n\n  def get_queries(self, input_tensor):\n    entity = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    rel_diag = self.rel_diag(input_tensor[:, 1])\n    return euc_utils.givens_reflection(rel_diag, entity) + rel\n\n\nclass MurE(BaseE):\n  """"""Diagonal scaling.""""""\n\n  def __init__(self, sizes, args):\n    super(MurE, self).__init__(sizes, args)\n    self.rel_diag = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'scaling_weights\')\n    self.sim = \'dist\'\n\n  def get_queries(self, input_tensor):\n    entity = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    rel_diag = self.rel_diag(input_tensor[:, 1])\n    return rel_diag * entity + rel\n\n\nclass AttE(BaseE):\n  """"""Euclidean attention model that combines reflections and rotations.""""""\n\n  def __init__(self, sizes, args):\n    super(AttE, self).__init__(sizes, args)\n    self.sim = \'dist\'\n\n    # reflection\n    self.ref = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'reflection_weights\')\n\n    # rotation\n    self.rot = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'rotation_weights\')\n\n    # attention\n    self.context_vec = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'context_embeddings\')\n    self.scale = tf.keras.backend.ones(1) / np.sqrt(self.rank)\n\n  def get_reflection_queries(self, entity, ref):\n    queries = euc_utils.givens_reflection(ref, entity)\n    return tf.reshape(queries, (-1, 1, self.rank))\n\n  def get_rotation_queries(self, entity, rot):\n    queries = euc_utils.givens_rotations(rot, entity)\n    return tf.reshape(queries, (-1, 1, self.rank))\n\n  def get_queries(self, input_tensor):\n    entity = self.entity(input_tensor[:, 0])\n    rel = self.rel(input_tensor[:, 1])\n    rot = self.rot(input_tensor[:, 1])\n    ref = self.ref(input_tensor[:, 1])\n    context_vec = self.context_vec(input_tensor[:, 1])\n    ref_q = self.get_reflection_queries(entity, ref)\n    rot_q = self.get_rotation_queries(entity, rot)\n\n    # self-attention mechanism\n    cands = tf.concat([ref_q, rot_q], axis=1)\n    context_vec = tf.reshape(context_vec, (-1, 1, self.rank))\n    att_weights = tf.reduce_sum(\n        context_vec * cands * self.scale, axis=-1, keepdims=True)\n    att_weights = tf.nn.softmax(att_weights, axis=-1)\n    res = tf.reduce_sum(att_weights * cands, axis=1) + rel\n    return res\n'"
research/kg_hyp_emb/models/hyperbolic.py,23,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Hyperbolic embedding models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom kg_hyp_emb.models.base import KGModel\nfrom kg_hyp_emb.utils import euclidean as euc_utils\nfrom kg_hyp_emb.utils import hyperbolic as hyp_utils\nimport numpy as np\nimport tensorflow as tf\n\n\nclass BaseH(KGModel):\n  """"""Base model class for hyperbolic embeddings.""""""\n\n  def __init__(self, sizes, args):\n    """"""Initialize Hyperbolic KG embedding model.\n\n    Args:\n      sizes: Tuple of size 3 containing (n_entities, n_rels, n_entities).\n      args: Namespace with config arguments (see config.py for detailed overview\n        of arguments supported).\n    """"""\n    super(BaseH, self).__init__(sizes, args)\n    self.c = tf.Variable(\n        initial_value=tf.keras.backend.ones(1), trainable=args.train_c)\n\n  def get_rhs(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    return hyp_utils.expmap0(self.entity(input_tensor[:, 2]), c)\n\n  def get_candidates(self,):\n    c = tf.math.softplus(self.c)\n    return hyp_utils.expmap0(self.entity.embeddings, c)\n\n  def similarity_score(self, lhs, rhs, eval_mode):\n    c = tf.math.softplus(self.c)\n    return -hyp_utils.hyp_distance(lhs, rhs, c, eval_mode)**2\n\n\nclass TransH(BaseH):\n  """"""Hyperbolic translation with parameters defined in tangent space.""""""\n\n  def get_queries(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    lhs = hyp_utils.expmap0(self.entity(input_tensor[:, 0]), c)\n    rel = hyp_utils.expmap0(self.rel(input_tensor[:, 1]), c)\n    res = hyp_utils.mobius_add(lhs, rel, c)\n    return res\n\n\nclass RotH(BaseH):\n  """"""Hyperbolic rotation model using 2 x 2 givens rotations.""""""\n\n  def __init__(self, sizes, args):\n    super(RotH, self).__init__(sizes, args)\n    self.rot = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'rotation_weights\')\n    self.rot_trans = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'translation_weights\')\n\n  def get_queries(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    head = hyp_utils.expmap0(self.entity(input_tensor[:, 0]), c)\n    trans = hyp_utils.expmap0(self.rot_trans(input_tensor[:, 1]), c)\n    lhs = hyp_utils.mobius_add(head, trans, c)\n    rot = euc_utils.givens_rotations(self.rot(input_tensor[:, 1]), lhs)\n    rot = hyp_utils.project(rot, c)\n    rel = hyp_utils.expmap0(self.rel(input_tensor[:, 1]), c)\n    return hyp_utils.mobius_add(rot, rel, c)\n\n\nclass RefH(BaseH):\n  """"""Hyperbolic reflection model with 2 x 2 reflections.""""""\n\n  def __init__(self, sizes, args):\n    super(RefH, self).__init__(sizes, args)\n    self.ref = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'reflection_weights\')\n\n  def get_queries(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    head = hyp_utils.expmap0(self.entity(input_tensor[:, 0]), c)\n    ref = euc_utils.givens_reflection(self.ref(input_tensor[:, 1]), head)\n    lhs = hyp_utils.project(ref, c)\n    rel = hyp_utils.expmap0(self.rel(input_tensor[:, 1]), c)\n    return hyp_utils.mobius_add(lhs, rel, c)\n\n\nclass AttH(BaseH):\n  """"""Hyperbolic attention model that combines reflections and rotations.""""""\n\n  def __init__(self, sizes, args):\n    super(AttH, self).__init__(sizes, args)\n    self.sim = \'dist\'\n\n    # reflection\n    self.ref = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'reflection_weights\')\n\n    # rotation\n    self.rot = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'rotation_weights\')\n\n    # attention\n    self.context_vec = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name=\'context_embeddings\')\n    self.scale = tf.keras.backend.ones(1) / np.sqrt(self.rank)\n\n  def get_reflection_queries(self, entity, ref):\n    queries = euc_utils.givens_reflection(ref, entity)\n    return tf.reshape(queries, (-1, 1, self.rank))\n\n  def get_rotation_queries(self, entity, rot):\n    queries = euc_utils.givens_rotations(rot, entity)\n    return tf.reshape(queries, (-1, 1, self.rank))\n\n  def get_queries(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    entity = self.entity(input_tensor[:, 0])\n\n    # candidates\n    rot = self.rot(input_tensor[:, 1])\n    ref = self.ref(input_tensor[:, 1])\n    ref_q = self.get_reflection_queries(entity, ref)\n    rot_q = self.get_rotation_queries(entity, rot)\n    cands = tf.concat([ref_q, rot_q], axis=1)\n\n    # self-attention mechanism\n    context_vec = self.context_vec(input_tensor[:, 1])\n    context_vec = tf.reshape(context_vec, (-1, 1, self.rank))\n    att_weights = tf.reduce_sum(\n        context_vec * cands * self.scale, axis=-1, keepdims=True)\n    att_weights = tf.nn.softmax(att_weights, axis=-1)\n    att_q = tf.reduce_sum(att_weights * cands, axis=1)\n    lhs = hyp_utils.expmap0(att_q, c)\n\n    # hyperbolic translation\n    rel = hyp_utils.expmap0(self.rel(input_tensor[:, 1]), c)\n    return hyp_utils.mobius_add(lhs, rel, c)\n'"
research/kg_hyp_emb/utils/__init__.py,0,b''
research/kg_hyp_emb/utils/euclidean.py,18,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Euclidean utility functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef euc_sq_distance(x, y, eval_mode=False):\n  """"""Computes Euclidean squared distance.\n\n  Args:\n    x: Tensor of size B1 x d\n    y: Tensor of size B2 x d\n    eval_mode: boolean indicating whether to compute all pairwise distances or\n      not. If eval_mode=False, must have B1=B2.\n\n  Returns:\n    Tensor of size B1 x B2 if eval_mode=True, otherwise Tensor of size B1 x 1.\n  """"""\n  x2 = tf.math.reduce_sum(x * x, axis=-1, keepdims=True)\n  y2 = tf.math.reduce_sum(y * y, axis=-1, keepdims=True)\n  if eval_mode:\n    y2 = tf.transpose(y2)\n    xy = tf.linalg.matmul(x, y, transpose_b=True)\n  else:\n    xy = tf.math.reduce_sum(x * y, axis=-1, keepdims=True)\n  return x2 + y2 - 2 * xy\n\n\ndef givens_reflection(r, x):\n  """"""Applies 2x2 reflections.\n\n  Args:\n    r: Tensor of size B x d representing relfection parameters per example.\n    x: Tensor of size B x d representing points to reflect.\n\n  Returns:\n    Tensor of size B x s representing reflection of x by r.\n  """"""\n  batch_size = tf.shape(r)[0]\n  givens = tf.reshape(r, (batch_size, -1, 2))\n  givens = givens / tf.norm(givens, ord=2, axis=-1, keepdims=True)\n  x = tf.reshape(x, (batch_size, -1, 2))\n  x_ref = givens[:, :, 0:1] * tf.concat(\n      (x[:, :, 0:1], -x[:, :, :1]), axis=-1) + givens[:, :, 1:] * tf.concat(\n          (x[:, :, 1:], x[:, :, 0:1]), axis=-1)\n  return tf.reshape(x_ref, (batch_size, -1))\n\n\ndef givens_rotations(r, x):\n  """"""Applies 2x2 rotations.\n\n  Args:\n    r: Tensor of size B x d representing rotation parameters per example.\n    x: Tensor of size B x d representing points to rotate.\n\n  Returns:\n    Tensor of size B x s representing rotation of x by r.\n  """"""\n  batch_size = tf.shape(r)[0]\n  givens = tf.reshape(r, (batch_size, -1, 2))\n  givens = givens / tf.norm(givens, ord=2, axis=-1, keepdims=True)\n  x = tf.reshape(x, (batch_size, -1, 2))\n  x_rot = givens[:, :, 0:1] * x + givens[:, :, 1:] * tf.concat(\n      (-x[:, :, 1:], x[:, :, 0:1]), axis=-1)\n  return tf.reshape(x_rot, (batch_size, -1))\n'"
research/kg_hyp_emb/utils/hyperbolic.py,21,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Hyperbolic utility functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nMIN_NORM = 1e-15\nMAX_TANH_ARG = 15.0\nBALL_EPS = {tf.float32: 4e-3, tf.float64: 1e-5}\n\n################## MATH FUNCTIONS #################\n\n\ndef artanh(x):\n  eps = BALL_EPS[x.dtype]\n  return tf.atanh(tf.minimum(tf.maximum(x, -1 + eps), 1 - eps))\n\n\ndef tanh(x):\n  return tf.tanh(tf.minimum(tf.maximum(x, -MAX_TANH_ARG), MAX_TANH_ARG))\n\n\n################## HYP OPS ########################\n\n\ndef expmap0(u, c):\n  """"""Hyperbolic exponential map at zero in the Poincare ball model.\n\n  Args:\n    u: Tensor of size B x dimension representing tangent vectors.\n    c: Tensor of size 1 representing the absolute hyperbolic curvature.\n\n  Returns:\n    Tensor of shape B x dimension.\n  """"""\n  sqrt_c = tf.sqrt(c)\n  u_norm = tf.maximum(tf.norm(u, axis=-1, keepdims=True), MIN_NORM)\n  gamma_1 = tanh(sqrt_c * u_norm) * u / (sqrt_c * u_norm)\n  return project(gamma_1, c)\n\n\ndef logmap0(y, c):\n  """"""Hyperbolic logarithmic map at zero in the Poincare ball model.\n\n  Args:\n    y: Tensor of size B x dimension representing hyperbolic points.\n    c: Tensor of size 1 representing the absolute hyperbolic curvature.\n\n  Returns:\n    Tensor of shape B x dimension.\n  """"""\n  sqrt_c = tf.sqrt(c)\n  y_norm = tf.maximum(tf.norm(y, axis=-1, keepdims=True), MIN_NORM)\n  return y / y_norm / sqrt_c * artanh(sqrt_c * y_norm)\n\n\ndef project(x, c):\n  """"""Projects points to the Poincare ball.\n\n  Args:\n    x: Tensor of size B x dimension.\n    c: Tensor of size 1 representing the absolute hyperbolic curvature.\n\n  Returns:\n    Tensor of shape B x dimension where each row is a point that lies within\n    the Poincare ball.\n  """"""\n  eps = BALL_EPS[x.dtype]\n  return tf.clip_by_norm(t=x, clip_norm=(1. - eps) / tf.sqrt(c), axes=[1])\n\n\ndef mobius_add(x, y, c):\n  """"""Element-wise Mobius addition.\n\n  Args:\n    x: Tensor of size B x dimension representing hyperbolic points.\n    y: Tensor of size B x dimension representing hyperbolic points.\n    c: Tensor of size 1 representing the absolute hyperbolic curvature.\n\n  Returns:\n    Tensor of shape B x dimension representing the element-wise Mobius addition\n    of x and y.\n  """"""\n  cx2 = c * tf.reduce_sum(x * x, axis=-1, keepdims=True)\n  cy2 = c * tf.reduce_sum(y * y, axis=-1, keepdims=True)\n  cxy = c * tf.reduce_sum(x * y, axis=-1, keepdims=True)\n  num = (1 + 2 * cxy + cy2) * x + (1 - cx2) * y\n  denom = 1 + 2 * cxy + cx2 * cy2\n  return project(num / tf.maximum(denom, MIN_NORM), c)\n\n\n################## HYP DISTANCE ###################\n\n\ndef hyp_distance(x, y, c, eval_mode=False):\n  """"""Hyperbolic distance on the Poincare ball.\n\n  Args:\n    x: Tensor of size B1 x d\n    y: Tensor of size B2 x d\n    c: Tensor of size 1 representing the absolute hyperbolic curvature.\n    eval_mode: boolean indicating whether to compute all pairwise distances or\n      not. If eval_mode=False, must have B1=B2.\n\n  Returns:\n    Tensor of size B1 x B2 if eval_mode=True, otherwise Tensor of size B1 x 1.\n  """"""\n  sqrt_c = tf.sqrt(c)\n  x2 = tf.reduce_sum(x * x, axis=-1, keepdims=True)\n  if eval_mode:\n    y2 = tf.transpose(tf.reduce_sum(y * y, axis=-1, keepdims=True))\n    xy = tf.matmul(x, y, transpose_b=True)\n  else:\n    y2 = tf.reduce_sum(y * y, axis=-1, keepdims=True)\n    xy = tf.reduce_sum(x * y, axis=-1, keepdims=True)\n  c1 = 1 - 2 * c * xy + c * y2\n  c2 = 1 - c * x2\n  num = tf.sqrt(tf.square(c1) * x2 + tf.square(c2) * y2 - (2 * c1 * c2) * xy)\n  denom = 1 - 2 * c * xy + tf.square(c) * x2 * y2\n  pairwise_norm = num / tf.maximum(denom, MIN_NORM)\n  dist = artanh(sqrt_c * pairwise_norm)\n  return 2 * dist / sqrt_c\n'"
research/kg_hyp_emb/utils/train.py,1,"b'# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Training utility functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom kg_hyp_emb.config import CONFIG\nimport numpy as np\n\nFLAGS = flags.FLAGS\n\n\ndef get_config_dict():\n  """"""Maps FLAGS to dictionnary in order to save it in json format.""""""\n  config = {}\n  for _, arg_dict in CONFIG.items():\n    for arg, _ in arg_dict.items():\n      config[arg] = getattr(FLAGS, arg)\n  return config\n\n\ndef count_params(model):\n  """"""Counts the total number of trainable parameters in a KG embedding model.\n\n  Args:\n    model: A tf.keras.Model KG embedding model.\n\n  Returns:\n    Integer representing the number of trainable variables.\n  """"""\n  total = 0\n  for x in model.trainable_variables:\n    total += np.prod(x.shape)\n  return total\n\n\ndef avg_both(mrs, mrrs, hits):\n  """"""Aggregate metrics for left- and right-hand-side predictions.\n\n  Args:\n    mrs: Dictionary with mean ranks for lhs and rhs.\n    mrrs: Dictionary with mean reciprocical ranks for lhs and rhs.\n    hits: Dictionary with hits at 1, 3, 10 for lhs and rhs.\n\n  Returns:\n    Dictionary with averaged metrics.\n  """"""\n  mr = (mrs[\'lhs\'] + mrs[\'rhs\']) / 2.\n  mrr = (mrrs[\'lhs\'] + mrrs[\'rhs\']) / 2.\n  h = []\n  for k in [1, 3, 10]:\n    h += [(hits[\'lhs\'][k] + hits[\'rhs\'][k]) / 2.]\n  return {\'MR\': mr, \'MRR\': mrr, \'hits@[1,3,10]\': h}\n\n\ndef format_metrics(metrics, split):\n  """"""Formats metrics for logging.\n\n  Args:\n    metrics: Dictionary with metrics.\n    split: String indicating the KG dataset split.\n\n  Returns:\n    String with formatted metrics.\n  """"""\n  result = \'\\t {} MR: {:.2f} | \'.format(split, metrics[\'MR\'])\n  result += \'MRR: {:.3f} | \'.format(metrics[\'MRR\'])\n  result += \'H@1: {:.3f} | \'.format(metrics[\'hits@[1,3,10]\'][0])\n  result += \'H@3: {:.3f} | \'.format(metrics[\'hits@[1,3,10]\'][1])\n  result += \'H@10: {:.3f}\'.format(metrics[\'hits@[1,3,10]\'][2])\n  return result\n'"
neural_structured_learning/examples/preprocess/cora/preprocess_cora_dataset.py,17,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""Tool that preprocesses Cora data for Graph Keras trainers.\n\nThe Cora dataset can be downloaded from:\nhttps://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n\nIn particular, this tool does the following:\n(a) Converts Cora data (cora.content) into TF Examples,\n(b) Parses the Cora citation graph (cora.cites),\n(c) Merges/combines the TF Examples and the graph, and\n(d) Writes the training and test data in TF Record format.\n\nThe \'cora.content\' has the following TSV format:\n\n  publication_id<TAB>word_1<TAB>word_2<TAB>...<TAB>publication_label\n\nEach line of cora.content is a publication that:\n- Has an integer \'publication_id\'\n- Described by a 0/1-valued word vector indicating the absence/presence of the\n  corresponding word from the dictionary. In other words, each \'word_k\' is\n  either 0 or 1.\n- Has a string \'publication_label\' representing the publication category.\n\nThe \'cora.cites\' is a TSV file that specifies a graph as a set of edges\nrepresenting citation relationships among publications. \'cora.cites\' has the\nfollowing TSV format:\n\n  source_publication_id<TAB>target_publication_id\n\nEach line of cora.cites represents an edge that \'source_publication_id\' cites\n\'target_publication_id\'.\n\nThis tool first converts all the \'cora.content\' into TF Examples. Then for\ntraining data, this tool merges into each labeled Example the features of that\nExample\'s neighbors according to that instance\'s edges in the graph. Finally,\nthe merged training examples are written to a TF Record file. The test data\nwill be written to a TF Record file w/o joining with the neighbors.\n\nSample usage:\n\n$ python preprocess_cora_dataset.py --max_nbrs=5\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport random\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom neural_structured_learning.tools import graph_utils\nimport six\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\nFLAGS.showprefixforinfo = False\n\nflags.DEFINE_string(\n    \'input_cora_content\', \'/tmp/cora/cora.content\',\n    """"""Input file for Cora content that contains ID, words and labels."""""")\nflags.DEFINE_string(\'input_cora_graph\', \'/tmp/cora/cora.cites\',\n                    """"""Input file for Cora citation graph in TSV format."""""")\nflags.DEFINE_integer(\n    \'max_nbrs\', None,\n    \'The maximum number of neighbors to merge into each labeled Example.\')\nflags.DEFINE_float(\n    \'train_percentage\', 0.8,\n    """"""The percentage of examples to be created as training data. The rest\n    are created as test data."""""")\nflags.DEFINE_string(\n    \'output_train_data\', \'/tmp/cora/train_merged_examples.tfr\',\n    """"""Output file for training data merged with graph in TF Record format."""""")\nflags.DEFINE_string(\'output_test_data\', \'/tmp/cora/test_examples.tfr\',\n                    """"""Output file for test data in TF Record format."""""")\n\n\ndef _int64_feature(*value):\n  """"""Returns int64 tf.train.Feature from a bool / enum / int / uint.""""""\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=list(value)))\n\n\ndef _bytes_feature(value):\n  """"""Returns bytes tf.train.Feature from a string.""""""\n  return tf.train.Feature(\n      bytes_list=tf.train.BytesList(value=[value.encode(\'utf-8\')]))\n\n\ndef parse_cora_content(in_file, train_percentage):\n  """"""Converts the Cora content (in TSV) to `tf.train.Example` instances.\n\n  This function parses Cora content (in TSV), converts string labels to integer\n  label IDs, randomly splits the data into training and test sets, and returns\n  the training and test sets as outputs.\n\n  Args:\n    in_file: A string indicating the input file path.\n    train_percentage: A float indicating the percentage of training examples\n      over the dataset.\n\n  Returns:\n    train_examples: A dict with keys being example IDs (string) and values being\n    `tf.train.Example` instances.\n    test_examples: A dict with keys being example IDs (string) and values being\n    `tf.train.Example` instances.\n  """"""\n  # Provides a mapping from string labels to integer indices.\n  label_index = {\n      \'Case_Based\': 0,\n      \'Genetic_Algorithms\': 1,\n      \'Neural_Networks\': 2,\n      \'Probabilistic_Methods\': 3,\n      \'Reinforcement_Learning\': 4,\n      \'Rule_Learning\': 5,\n      \'Theory\': 6,\n  }\n  # Fixes the random seed so the train/test split can be reproduced.\n  random.seed(1)\n  train_examples = {}\n  test_examples = {}\n  with open(in_file, \'rU\') as cora_content:\n    for line in cora_content:\n      entries = line.rstrip(\'\\n\').split(\'\\t\')\n      # entries contains [ID, Word1, Word2, ..., Label]; \'Words\' are 0/1 values.\n      words = map(int, entries[1:-1])\n      example_id = entries[0]\n      features = {\n          \'id\': _bytes_feature(example_id),\n          \'words\': _int64_feature(*words),\n          \'label\': _int64_feature(label_index[entries[-1]]),\n      }\n      example_features = tf.train.Example(\n          features=tf.train.Features(feature=features))\n      if random.uniform(0, 1) <= train_percentage:  # for train/test split.\n        train_examples[example_id] = example_features\n      else:\n        test_examples[example_id] = example_features\n\n  return train_examples, test_examples\n\n\ndef _join_examples(seed_exs, nbr_exs, graph, max_nbrs):\n  r""""""Joins the `seeds` and `nbrs` Examples using the edges in `graph`.\n\n  This generator joins and augments each labeled Example in `seed_exs` with the\n  features of at most `max_nbrs` of the seed\'s neighbors according to the given\n  `graph`, and yields each merged result.\n\n  Args:\n    seed_exs: A `dict` mapping node IDs to labeled Examples.\n    nbr_exs: A `dict` mapping node IDs to unlabeled Examples.\n    graph: A `dict`: source -> (target, weight).\n    max_nbrs: The maximum number of neighbors to merge into each seed Example,\n      or `None` if the number of neighbors per node is unlimited.\n\n  Yields:\n    The result of merging each seed Example with the features of its neighbors,\n    as described by the module comment.\n  """"""\n  # A histogram of the out-degrees of all seed Examples. The keys of this dict\n  # range from 0 to \'max_nbrs\' (inclusive) if \'max_nbrs\' is finite.\n  out_degree_count = collections.Counter()\n\n  def has_ex(node_id):\n    """"""Returns true iff \'node_id\' is in the \'seed_exs\' or \'nbr_exs dict\'.""""""\n    result = (node_id in seed_exs) or (node_id in nbr_exs)\n    if not result:\n      logging.warning(\'No tf.train.Example found for edge target ID: ""%s""\',\n                      node_id)\n    return result\n\n  def lookup_ex(node_id):\n    """"""Returns the Example from `seed_exs` or `nbr_exs` with the given ID.""""""\n    return seed_exs[node_id] if node_id in seed_exs else nbr_exs[node_id]\n\n  def join_seed_to_nbrs(seed_id):\n    """"""Joins the seed with ID `seed_id` to its out-edge graph neighbors.\n\n    This also has the side-effect of maintaining the `out_degree_count`.\n\n    Args:\n      seed_id: The ID of the seed Example to start from.\n\n    Returns:\n      A list of (nbr_wt, nbr_id) pairs (in decreasing weight order) of the\n      seed Example\'s top `max_nbrs` neighbors. So the resulting list will have\n      size at most `max_nbrs`, but it may be less (or even empty if the seed\n      Example has no out-edges).\n    """"""\n    nbr_dict = graph[seed_id] if seed_id in graph else {}\n    nbr_wt_ex_list = [(nbr_wt, nbr_id)\n                      for (nbr_id, nbr_wt) in six.iteritems(nbr_dict)\n                      if has_ex(nbr_id)]\n    result = sorted(nbr_wt_ex_list, reverse=True)[:max_nbrs]\n    out_degree_count[len(result)] += 1\n    return result\n\n  def merge_examples(seed_ex, nbr_wt_ex_list):\n    """"""Merges neighbor Examples into the given seed Example `seed_ex`.\n\n    Args:\n      seed_ex: A labeled Example.\n      nbr_wt_ex_list: A list of (nbr_wt, nbr_id) pairs (in decreasing nbr_wt\n        order) representing the neighbors of \'seed_ex\'.\n\n    Returns:\n      The Example that results from merging the features of the neighbor\n      Examples (as well as creating a feature for each neighbor\'s edge weight)\n      into `seed_ex`. See the `join()` description above for how the neighbor\n      features are named in the result.\n    """"""\n    # Make a deep copy of the seed Example to augment.\n    merged_ex = tf.train.Example()\n    merged_ex.CopyFrom(seed_ex)\n\n    # Add a feature for the number of neighbors.\n    merged_ex.features.feature[\'NL_num_nbrs\'].int64_list.value.append(\n        len(nbr_wt_ex_list))\n\n    # Enumerate the neighbors, and merge in the features of each.\n    for index, (nbr_wt, nbr_id) in enumerate(nbr_wt_ex_list):\n      prefix = \'NL_nbr_{}_\'.format(index)\n      # Add the edge weight value as a new singleton float feature.\n      weight_feature = prefix + \'weight\'\n      merged_ex.features.feature[weight_feature].float_list.value.append(nbr_wt)\n      # Copy each of the neighbor Examples features, prefixed with \'prefix\'.\n      nbr_ex = lookup_ex(nbr_id)\n      for (feature_name, feature_val) in six.iteritems(nbr_ex.features.feature):\n        new_feature = merged_ex.features.feature[prefix + feature_name]\n        new_feature.CopyFrom(feature_val)\n    return merged_ex\n\n  start_time = time.time()\n  logging.info(\n      \'Joining seed and neighbor tf.train.Examples with graph edges...\')\n  for (seed_id, seed_ex) in six.iteritems(seed_exs):\n    yield merge_examples(seed_ex, join_seed_to_nbrs(seed_id))\n  logging.info(\n      \'Done creating and writing %d merged tf.train.Examples (%.2f seconds).\',\n      len(seed_exs), (time.time() - start_time))\n  logging.info(\'Out-degree histogram: %s\', sorted(out_degree_count.items()))\n\n\ndef main(unused_argv):\n  start_time = time.time()\n\n  # Parses Cora content into TF Examples.\n  train_examples, test_examples = parse_cora_content(FLAGS.input_cora_content,\n                                                     FLAGS.train_percentage)\n\n  graph = graph_utils.read_tsv_graph(FLAGS.input_cora_graph)\n  graph_utils.add_undirected_edges(graph)\n\n  # Joins \'train_examples\' with \'graph\'. \'test_examples\' are used as *unlabeled*\n  # neighbors for transductive learning purpose. In other words, the labels of\n  # test_examples are not used.\n  with tf.io.TFRecordWriter(FLAGS.output_train_data) as writer:\n    for merged_example in _join_examples(train_examples, test_examples, graph,\n                                         FLAGS.max_nbrs):\n      writer.write(merged_example.SerializeToString())\n\n  logging.info(\'Output training data written to TFRecord file: %s.\',\n               FLAGS.output_train_data)\n\n  # Writes \'test_examples\' out w/o joining with the graph since graph\n  # regularization is used only during training, not testing/serving.\n  with tf.io.TFRecordWriter(FLAGS.output_test_data) as writer:\n    for example in six.itervalues(test_examples):\n      writer.write(example.SerializeToString())\n\n  logging.info(\'Output test data written to TFRecord file: %s.\',\n               FLAGS.output_test_data)\n  logging.info(\'Total running time: %.2f minutes.\',\n               (time.time() - start_time) / 60.0)\n\n\nif __name__ == \'__main__\':\n  # Ensures TF 2.0 behavior even if TF 1.X is installed.\n  tf.compat.v1.enable_v2_behavior()\n  app.run(main)\n'"
research/gam/gam/data/__init__.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
research/gam/gam/data/dataset.py,1,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Data containers for Graph Agreement Models.""""""\nimport collections\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nfrom .preprocessing import split_train_val\nimport scipy\nimport tensorflow as tf\n\n\nclass Dataset(object):\n  """"""A container for datasets.\n\n  In this dataset, each sample has the same number of features.\n  This class manages different splits of the data for train, validation, test\n  and unlabeled. These sets of samples are disjoint.\n  """"""\n\n  def __init__(self,\n               name,\n               features,\n               labels,\n               indices_train,\n               indices_test,\n               indices_val,\n               indices_unlabeled,\n               num_classes=None,\n               feature_preproc_fn=lambda x: x):\n    self.name = name\n    self.features = features\n    self.labels = labels\n\n    self.indices_train = indices_train\n    self.indices_val = indices_val\n    self.indices_test = indices_test\n    self.indices_unlabeled = indices_unlabeled\n    self.feature_preproc_fn = feature_preproc_fn\n\n    self.num_val = self.indices_val.shape[0]\n    self.num_test = self.indices_test.shape[0]\n\n    self.num_samples = labels.shape[0]\n    self.features_shape = features.shape[1:]\n    self.num_features = np.prod(features.shape[1:])\n    self.num_classes = 1 + max(labels) if num_classes is None else num_classes\n\n  @staticmethod\n  def build_from_splits(name,\n                        inputs_train,\n                        labels_train,\n                        inputs_val,\n                        labels_val,\n                        inputs_test,\n                        labels_test,\n                        inputs_unlabeled,\n                        labels_unlabeled=None,\n                        num_classes=None,\n                        feature_preproc_fn=lambda x: x):\n    """"""Build Dataset from splits.""""""\n    num_train = inputs_train.shape[0]\n    num_val = inputs_val.shape[0]\n    num_unlabeled = inputs_unlabeled.shape[0]\n    num_test = inputs_test.shape[0]\n\n    if labels_unlabeled is None:\n      labels_unlabeled = np.zeros(\n          shape=(num_unlabeled,), dtype=labels_train[0].dtype)\n    features = np.concatenate(\n        (inputs_train, inputs_val, inputs_unlabeled, inputs_test))\n    labels = np.concatenate(\n        (labels_train, labels_val, labels_unlabeled, labels_test))\n\n    indices_train = np.arange(num_train)\n    indices_val = np.arange(num_train, num_train + num_val)\n    indices_unlabeled = np.arange(num_train + num_val,\n                                  num_train + num_val + num_unlabeled)\n    indices_test = np.arange(num_train + num_val + num_unlabeled,\n                             num_train + num_val + num_unlabeled + num_test)\n\n    return Dataset(\n        name=name,\n        features=features,\n        labels=labels,\n        indices_train=indices_train,\n        indices_test=indices_test,\n        indices_val=indices_val,\n        indices_unlabeled=indices_unlabeled,\n        num_classes=num_classes,\n        feature_preproc_fn=feature_preproc_fn)\n\n  @staticmethod\n  def build_from_features(name,\n                          features,\n                          labels,\n                          indices_train,\n                          indices_test,\n                          indices_val=None,\n                          indices_unlabeled=None,\n                          percent_val=0.2,\n                          seed=None,\n                          num_classes=None,\n                          feature_preproc_fn=lambda x: x):\n    """"""Build Dataset from features.""""""\n    if indices_val is None:\n      rng = np.random.RandomState(seed=seed)\n      indices_train, indices_val = split_train_val(\n          np.arange(indices_train.shape[0]), percent_val, rng)\n\n    return Dataset(\n        name=name,\n        features=features,\n        labels=labels,\n        indices_train=indices_train,\n        indices_test=indices_test,\n        indices_val=indices_val,\n        indices_unlabeled=indices_unlabeled,\n        num_classes=num_classes,\n        feature_preproc_fn=feature_preproc_fn)\n\n  def copy(self,\n           name=None,\n           features=None,\n           labels=None,\n           indices_train=None,\n           indices_test=None,\n           indices_val=None,\n           indices_unlabeled=None,\n           num_classes=None,\n           feature_preproc_fn=None):\n    """"""Copies Dataset.""""""\n    name = name if name is not None else self.name\n    features = features if features is not None else self.features\n    labels = labels if labels is not None else self.labels\n    indices_train = (\n        indices_train if indices_train is not None else self.indices_train)\n    indices_test = (\n        indices_test if indices_test is not None else self.indices_test)\n    indices_val = indices_val if indices_val is not None else self.indices_val\n    indices_unlabeled = (\n        indices_unlabeled\n        if indices_unlabeled is not None else self.indices_unlabeled)\n    num_classes = num_classes if num_classes is not None else self.num_classes\n    feature_preproc_fn = (\n        feature_preproc_fn\n        if feature_preproc_fn is not None else self.feature_preproc_fn)\n    return Dataset(\n        name=name,\n        features=features,\n        labels=labels,\n        indices_train=indices_train,\n        indices_test=indices_test,\n        indices_val=indices_val,\n        indices_unlabeled=indices_unlabeled,\n        num_classes=num_classes,\n        feature_preproc_fn=feature_preproc_fn)\n\n  def copy_labels(self):\n    return np.copy(self.labels)\n\n  def get_features(self, indices):\n    """"""Returns the features of the samples with the provided indices.""""""\n    f = self.features[indices]\n    f = self.feature_preproc_fn(f)\n    return f\n\n  def get_labels(self, indices):\n    return self.labels[indices]\n\n  def num_train(self):\n    return self.indices_train.shape[0]\n\n  def num_val(self):\n    return self.indices_val.shape[0]\n\n  def num_test(self):\n    return self.indices_test.shape[0]\n\n  def num_unlabeled(self):\n    return self.indices_unlabeled.shape[0]\n\n  def get_indices_train(self):\n    return self.indices_train\n\n  def get_indices_unlabeled(self):\n    return self.indices_unlabeled\n\n  def get_indices_val(self):\n    return self.indices_val\n\n  def get_indices_test(self):\n    return self.indices_test\n\n  def label_samples(self, indices_samples, new_labels):\n    """"""Updates the labels of the samples with the provided indices.\n\n    Arguments:\n      indices_samples: Array of integers representing the sample indices to\n        update. These must samples that are not already considered training\n        samples, otherwise they will be duplicated.\n      new_labels: Array of integers containing the new labels for each of the\n        samples in indices_samples.\n    """"""\n    # Update the labels.\n    self.update_labels(indices_samples, new_labels)\n\n    # Mark the newly labeled nodes as training samples. Note that for\n    # efficiency we simply concatenate the new samples to the existing training\n    # indices, without checking if they already exist.\n    self.indices_train = np.concatenate((self.indices_train, indices_samples),\n                                        axis=0)\n    # Remove the recently labeled samples from the unlabeled set.\n    indices_samples = set(indices_samples)\n    self.indices_unlabeled = np.asarray(\n        [u for u in self.indices_unlabeled if u not in indices_samples])\n\n  def update_labels(self, indices, new_labels):\n    """"""Updates the labels of the samples with the provided indices.\n\n    Arguments:\n      indices: A list of integers representing sample indices.\n      new_labels: A list of integers representing the new labels of th samples\n        in indices_samples.\n    """"""\n    indices = np.asarray(indices)\n    new_labels = np.asarray(new_labels)\n    self.labels[indices] = new_labels\n\n  def save_to_pickle(self, file_path):\n    pickle.dump(self, open(file_path, \'w\'))\n\n  @staticmethod\n  def load_from_pickle(file_path):\n    dataset = pickle.load(open(file_path, \'r\'))\n    return dataset\n\n\nclass GraphDataset(Dataset):\n  """"""Data container for SSL datasets.""""""\n\n  class Edge(object):\n\n    def __init__(self, src, tgt, weight=None):\n      self.src = src\n      self.tgt = tgt\n      self.weight = weight\n\n  def __init__(self,\n               name,\n               features,\n               labels,\n               edges,\n               indices_train,\n               indices_test,\n               indices_val=None,\n               indices_unlabeled=None,\n               percent_val=0.2,\n               seed=None,\n               num_classes=None,\n               feature_preproc_fn=lambda x: x):\n    self.edges = edges\n\n    if indices_val is None:\n      rng = np.random.RandomState(seed=seed)\n      indices_train, indices_val = split_train_val(\n          np.arange(indices_train.shape[0]), percent_val, rng)\n\n    super(GraphDataset, self).__init__(\n        name=name,\n        features=features,\n        labels=labels,\n        indices_train=indices_train,\n        indices_test=indices_test,\n        indices_val=indices_val,\n        indices_unlabeled=indices_unlabeled,\n        num_classes=num_classes,\n        feature_preproc_fn=feature_preproc_fn)\n\n  def copy(self,\n           name=None,\n           features=None,\n           labels=None,\n           edges=None,\n           indices_train=None,\n           indices_test=None,\n           indices_val=None,\n           indices_unlabeled=None,\n           num_classes=None,\n           feature_preproc_fn=None):\n    name = name if name is not None else self.name\n    features = features if features is not None else self.features\n    labels = labels if labels is not None else self.labels\n    indices_train = (\n        indices_train if indices_train is not None else self.indices_train)\n    indices_test = (\n        indices_test if indices_test is not None else self.indices_test)\n    indices_val = indices_val if indices_val is not None else self.indices_val\n    indices_unlabeled = (\n        indices_unlabeled\n        if indices_unlabeled is not None else self.indices_unlabeled)\n    num_classes = num_classes if num_classes is not None else self.num_classes\n    feature_preproc_fn = (\n        feature_preproc_fn\n        if feature_preproc_fn is not None else self.feature_preproc_fn)\n    edges = edges if edges is not None else self.edges\n    return GraphDataset(\n        name=name,\n        features=features,\n        labels=labels,\n        edges=edges,\n        indices_train=indices_train,\n        indices_test=indices_test,\n        indices_val=indices_val,\n        indices_unlabeled=indices_unlabeled,\n        num_classes=num_classes,\n        feature_preproc_fn=feature_preproc_fn)\n\n  def get_edges(self,\n                src_labeled=None,\n                tgt_labeled=None,\n                label_must_match=False):\n    """"""Returns the graph edges satisfying the requested labeling.\n\n    Args:\n        src_labeled: Boolean specifying whether the source of the edge should be\n          labeled (if True), or unlabeled (if False). If None, then the source\n          can be either labeled or unlabeled.\n        tgt_labeled: Boolean specifying whether the target of the edge should be\n          labeled (if True), or unlabeled (if False). If None, then the target\n          can be either labeled or unlabeled.\n        label_must_match: Boolean specifying whether the source and the target\n          nodes of the returned edges are required to have the same label.\n\n    Returns:\n        A list of edges.\n    """"""\n    labeled_mask = np.full((self.num_samples,), False)\n    labeled_mask[self.get_indices_train()] = True\n\n    def _labeled_cond(idx, is_labeled):\n      return (is_labeled is None) or (is_labeled == labeled_mask[idx])\n\n    def _agreement_cond(edge):\n      return self.get_labels(edge.src) == self.get_labels(edge.tgt)\n\n    agreement_cond = _agreement_cond if label_must_match else lambda e: True\n    return [\n        e for e in self.edges if _labeled_cond(e.src, src_labeled) and\n        _labeled_cond(e.tgt, tgt_labeled) and agreement_cond(e)\n    ]\n\n\nclass PlanetoidDataset(GraphDataset):\n  """"""Data container for Planetoid datasets.""""""\n\n  @staticmethod\n  def build_from_adjacency_matrix(name,\n                                  adj,\n                                  features,\n                                  train_mask,\n                                  val_mask,\n                                  test_mask,\n                                  labels,\n                                  row_normalize=False):\n    """"""Build from adjacency matrix.""""""\n    # Extract train, val, test, unlabeled indices.\n    train_indices = np.where(train_mask)[0]\n    test_indices = np.where(test_mask)[0]\n    val_indices = np.where(val_mask)[0]\n    unlabeled_mask = np.logical_not(train_mask | test_mask | val_mask)\n    unlabeled_indices = np.where(unlabeled_mask)[0]\n\n    # Extract node features.\n    if row_normalize:\n      features = PlanetoidDataset.preprocess_features(features)\n\n    features = np.float32(features.todense())\n\n    # Extract labels.\n    labels = np.argmax(labels, axis=-1)\n    num_classes = max(labels) + 1\n\n    # Extract edges.\n    adj = scipy.sparse.coo_matrix(adj)\n    edges = [\n        PlanetoidDataset.Edge(src, tgt, val)\n        for src, tgt, val in zip(adj.row, adj.col, adj.data)\n    ]\n\n    return PlanetoidDataset(\n        name=name,\n        features=features,\n        labels=labels,\n        edges=edges,\n        indices_train=train_indices,\n        indices_test=test_indices,\n        indices_val=val_indices,\n        indices_unlabeled=unlabeled_indices,\n        num_classes=num_classes)\n\n  @staticmethod\n  def preprocess_features(features):\n    """"""Row-normalize feature matrix.""""""\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = scipy.sparse.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    return features\n\n\nclass GCNDataset(GraphDataset):\n  """"""Data container for Planetoid datasets.""""""\n\n  def __init__(self,\n               name,\n               features,\n               support,\n               num_features_nonzero,\n               features_sparse,\n               labels,\n               edges,\n               indices_train,\n               indices_test,\n               indices_val,\n               indices_unlabeled,\n               num_classes,\n               feature_preproc_fn=lambda x: x):\n    # Convert to Dataset format.\n    super(GCNDataset, self).__init__(\n        name=name,\n        features=features,\n        labels=labels,\n        edges=edges,\n        indices_train=indices_train,\n        indices_test=indices_test,\n        indices_val=indices_val,\n        indices_unlabeled=indices_unlabeled,\n        num_classes=num_classes,\n        feature_preproc_fn=feature_preproc_fn)\n\n    # Save the GCN specific information.\n    self.support = support\n    self.num_features_nonzero = num_features_nonzero\n    self.features_sparse = features_sparse\n\n  @staticmethod\n  def build_from_adjacency_matrix(name,\n                                  adj,\n                                  features,\n                                  train_mask,\n                                  val_mask,\n                                  test_mask,\n                                  labels,\n                                  row_normalize=False):\n    """"""Build from adjacency matrix.""""""\n    # Extract train, val, test, unlabeled indices.\n    train_indices = np.where(train_mask)[0]\n    test_indices = np.where(test_mask)[0]\n    val_indices = np.where(val_mask)[0]\n    unlabeled_mask = np.logical_not(train_mask | test_mask | val_mask)\n    unlabeled_indices = np.where(unlabeled_mask)[0]\n\n    # Extract node features.\n    if row_normalize:\n      features = GCNDataset.preprocess_features(features)\n\n    features = np.float32(features.todense())\n\n    # Extract labels.\n    labels = np.argmax(labels, axis=-1)\n    num_classes = max(labels) + 1\n\n    # Extract edges.\n    adj = scipy.sparse.coo_matrix(adj)\n    edges = [\n        GCNDataset.Edge(src, tgt, val)\n        for src, tgt, val in zip(adj.row, adj.col, adj.data)\n    ]\n\n    # Preprocessing of adjacency matrix for simple GCN model and conversion to\n    # tuple representation.\n    adj_normalized = GCNDataset.normalize_adj(adj +\n                                              scipy.eye(adj.shape[0])).astype(\n                                                  np.float32)\n    support = GCNDataset.sparse_to_tuple(adj_normalized)\n\n    features_matrix = (\n        GCNDataset.row_normalize(features).astype(np.float32)\n        if row_normalize else features.astype(np.float32))\n    features_sparse = GCNDataset.sparse_to_tuple(features_matrix)\n    num_features_nonzero = features_sparse[1].shape\n    return GCNDataset(\n        name=name,\n        features=features_matrix,\n        support=support,\n        num_features_nonzero=num_features_nonzero,\n        features_sparse=features_sparse,\n        labels=labels,\n        edges=edges,\n        indices_train=train_indices,\n        indices_test=test_indices,\n        indices_val=val_indices,\n        indices_unlabeled=unlabeled_indices,\n        num_classes=num_classes,\n        feature_preproc_fn=lambda x: x)\n\n  @staticmethod\n  def preprocess_features(features):\n    """"""Row-normalize feature matrix.""""""\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = scipy.sparse.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    return features\n\n  @staticmethod\n  def normalize_adj(adj):\n    """"""Symmetrically normalize adjacency matrix.""""""\n    adj = scipy.sparse.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = scipy.sparse.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n\n  @staticmethod\n  def row_normalize(features):\n    """"""Row-normalize feature matrix.""""""\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = scipy.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    return features\n\n  @staticmethod\n  def sparse_to_tuple(sparse_mx):\n    """"""Convert sparse matrix to tuple representation.""""""\n\n    def to_tuple(mx):\n      if not scipy.sparse.isspmatrix_coo(mx):\n        mx = scipy.sparse.coo_matrix(mx)\n      coords = np.vstack((mx.row, mx.col)).transpose()\n      values = mx.data\n      shape = mx.shape\n      return coords, values, shape\n\n    if isinstance(sparse_mx, list):\n      for i in range(len(sparse_mx)):\n        sparse_mx[i] = to_tuple(sparse_mx[i])\n    else:\n      sparse_mx = to_tuple(sparse_mx)\n\n    return sparse_mx\n\n  def copy(self,\n           name=None,\n           features=None,\n           labels=None,\n           edges=None,\n           indices_train=None,\n           indices_test=None,\n           indices_val=None,\n           indices_unlabeled=None,\n           num_classes=None,\n           feature_preproc_fn=lambda x: x,\n           support=None,\n           num_features_nonzero=None,\n           features_sparse=None):\n    name = name if name is not None else self.name\n    features = features if features is not None else self.features\n    labels = labels if labels is not None else self.labels\n    edges = edges if edges is not None else self.edges\n    indices_train = (\n        indices_train if indices_train is not None else self.indices_train)\n    indices_val = (indices_val if indices_val is not None else self.indices_val)\n    indices_test = (\n        indices_test if indices_test is not None else self.indices_test)\n    indices_unlabeled = (\n        indices_unlabeled\n        if indices_unlabeled is not None else self.indices_unlabeled)\n    num_classes = num_classes if num_classes is not None else self.num_classes\n    feature_preproc_fn = (\n        feature_preproc_fn\n        if feature_preproc_fn is not None else self.feature_preproc_fn)\n    support = support if support is not None else self.support\n    num_features_nonzero = (\n        num_features_nonzero\n        if num_features_nonzero is not None else self.num_features_nonzero)\n    features_sparse = (\n        features_sparse\n        if features_sparse is not None else self.features_sparse)\n    return GCNDataset(\n        name=name,\n        features=features,\n        labels=labels,\n        edges=edges,\n        indices_train=indices_train,\n        indices_test=indices_test,\n        indices_val=indices_val,\n        indices_unlabeled=indices_unlabeled,\n        num_classes=num_classes,\n        feature_preproc_fn=feature_preproc_fn,\n        support=support,\n        num_features_nonzero=num_features_nonzero,\n        features_sparse=features_sparse)\n\n\nclass CotrainDataset(object):\n  """"""A wrapper around a Dataset object, adding co-training functionality.\n\n  Attributes:\n    dataset: A Dataset object.\n    keep_label_proportions: A boolean specifying whether every self-labeling\n      step will keep the label proportions from the original training data. If\n      so, this class keeps statistics of the label distribution.\n    inductive: A boolean specifying if the dataset is inductive. If True, then\n      the unlabeled samples include the validation and test samples.\n    labels_original: Stores the original labels of all samples in `dataset`. We\n      do this because via the self-labeling step, the method `label_samples` can\n      rewrite the original (correct) labels, which are needed for evaluation.\n    label_prop: Stores the ratio of samples for each label in the original\n      training set.\n  """"""\n\n  def __init__(self, dataset, keep_label_proportions=False, inductive=False):\n    self.dataset = dataset\n    self.keep_label_proportions = keep_label_proportions\n    self.inductive = inductive\n\n    # Make a copy of the labels because we will modify them through\n    # self-labeling. We need the originals to monitor the accuracy.\n    self.labels_original = dataset.copy_labels()\n\n    # If we are in an inductive setting, the validation and test samples are\n    # not seen at training time, so they cannot be labeled via self-labeling.\n    # Otherwise, they are treated as any unlabeled samples.\n    if not self.inductive:\n      self.dataset.indices_unlabeled = np.concatenate(\n          (dataset.indices_unlabeled, dataset.indices_val,\n           dataset.indices_test))\n\n    # If when labeling new data we want to keep the proportions of the labels\n    # in the original labeled data, then save these proportions.\n    if keep_label_proportions:\n      labeled_indices = list(self.get_indices_train())\n      labeled_indices_labels = dataset.get_labels(labeled_indices)\n      label_counts = collections.Counter(labeled_indices_labels)\n      num_labels = np.float(len(labeled_indices))\n      assert num_labels > 0, \'There are no labeled samples in the dataset.\'\n      self.label_prop = {\n          label: count / num_labels for label, count in label_counts.items()\n      }\n    else:\n      self.label_prop = None\n\n  def label_samples(self, indices_samples, new_labels):\n    """"""Updates the labels of the samples with the provided indices.\n\n    Arguments:\n      indices_samples: Array of integers representing the sample indices to\n        update. These must samples that are not already considered training\n        samples, otherwise they will be duplicated.\n      new_labels: Array of integers containing the new labels for each of the\n        samples in indices_samples.\n    """"""\n    self.dataset.label_samples(indices_samples, new_labels)\n\n  def _compute_label_correctness(self, indices):\n    if indices.shape[0] == 0:\n      return 0.0\n    labels = self.get_labels(indices)\n    labels_orig = self.get_original_labels(indices)\n    correct_labels = np.sum(labels == labels_orig)\n    ratio_correct = float(correct_labels) / indices.shape[0]\n    return ratio_correct\n\n  def compute_dataset_statistics(self, selected_samples, summary_writer, step):\n    """"""Computes statistics about the correctness of the labels of a Dataset.""""""\n    # Compute how many new nodes are labeled correctly.\n    ratio_correct_new_labels = self._compute_label_correctness(selected_samples)\n\n    # Compute how many nodes are labeled correctly in total.\n    ratio_correct_total = self._compute_label_correctness(\n        self.get_indices_train())\n\n    num_new_labels = len(selected_samples)\n    logging.info(\'Number of newly labeled nodes: %d\', num_new_labels)\n    logging.info(\n        \'Ratio correct new labels: %f. \'\n        \'Ratio correct labels total: %f.\', ratio_correct_new_labels,\n        ratio_correct_total)\n\n    # Compute how many test nodes have been labeled, and their correctness.\n    # Note: This is an expensive step, and should be skipped for large datasets.\n    if not self.inductive:\n      indices_train = set(self.get_indices_train())\n      indices_test = set(self.get_indices_test())\n      labeled_test_indices = indices_train.intersection(indices_test)\n      labeled_test_indices = np.asarray(list(labeled_test_indices))\n      ratio_correct_labeled_test = self._compute_label_correctness(\n          labeled_test_indices)\n      logging.info(\'Number of labeled test nodes: %d\',\n                   labeled_test_indices.shape[0])\n      logging.info(\'Ratio correct labels for labeled test nodes: %f.\',\n                   ratio_correct_labeled_test)\n\n    # Compute the distribution of labels for all labeled nodes.\n    labels_new_labeled_nodes = self.get_labels(selected_samples)\n    labels_counts = collections.Counter(labels_new_labeled_nodes)\n    logging.info(\'--------- Label distribution for NEW nodes -----------------\')\n    logging.info(labels_counts)\n\n    labels_all_labeled_nodes = self.get_labels(self.get_indices_train())\n    labels_counts = collections.Counter(labels_all_labeled_nodes)\n    logging.info(\'-------- Label distribution for ALL LABELED nodes ----------\')\n    logging.info(labels_counts)\n\n    # Show Tensorboard summaries.\n    if summary_writer is not None:\n      summary = tf.Summary()\n      summary.value.add(\n          tag=\'data/ratio_correct_labels_new\',\n          simple_value=ratio_correct_new_labels)\n      summary.value.add(\n          tag=\'data/ratio_correct_labels_total\',\n          simple_value=ratio_correct_total)\n      summary.value.add(tag=\'data/num_new_nodes\', simple_value=num_new_labels)\n      if not self.inductive:\n        summary.value.add(\n            tag=\'data/num_labeled_test\',\n            simple_value=labeled_test_indices.shape[0])\n        if labeled_test_indices.shape[0] > 0:\n          summary.value.add(\n              tag=\'data/ratio_correct_labeled_test\',\n              simple_value=ratio_correct_labeled_test)\n      summary_writer.add_summary(summary, step)\n      summary_writer.flush()\n\n    return ratio_correct_new_labels, ratio_correct_total\n\n  def get_indices_train(self):\n    """"""Returns the indices of the labeled samples that can be used to train.""""""\n    return self.dataset.get_indices_train()\n\n  def get_indices_val(self):\n    """"""Returns the indices of the samples that can be used to validate.""""""\n    return self.dataset.get_indices_val()\n\n  def get_indices_test(self):\n    """"""Returns the indices of the samples that can be used to test.""""""\n    return self.dataset.get_indices_test()\n\n  def get_indices_unlabeled(self):\n    """"""Returns the indices of the samples that can be self-labeled.""""""\n    return self.dataset.get_indices_unlabeled()\n\n  def get_features(self, indices):\n    """"""Returns the features of the requested indices.""""""\n    return self.dataset.get_features(indices)\n\n  def get_labels(self, indices):\n    """"""Returns the labels of the requested indices.""""""\n    return self.dataset.get_labels(indices)\n\n  def get_original_labels(self, indices):\n    """"""Returns the labels of the requested indices in the original dataset.\n\n    This is different than `get_labels` because the current labels of a sample\n    may have been changed via self-labeling.\n\n    Args:\n      indices: A list or array of sample indices.\n\n    Returns:\n      An array of labels.\n    """"""\n    return self.labels_original[indices]\n\n  @property\n  def num_samples(self):\n    """"""Returns the total number of samples in the dataset.""""""\n    return self.dataset.num_samples\n\n  @property\n  def num_features(self):\n    """"""Returns the number of features of the samples in the dataset.""""""\n    return self.dataset.num_features\n\n  @property\n  def features(self):\n    """"""Returns the entire features matrix.\n\n    This includes all of train, validation, test and unlabeled samples.\n    """"""\n    return self.dataset.features\n\n  @property\n  def features_shape(self):\n    """"""Returns the shape of the input features, not including batch size.""""""\n    return self.dataset.features_shape\n\n  @property\n  def num_classes(self):\n    """"""Returns the number of classes of the samples in the dataset.""""""\n    return self.dataset.num_classes\n\n  @property\n  def support(self):\n    """"""Returns the number of support of the features matrix.\n\n    This is only supported if the dataset is a GCNDataset.\n    """"""\n    assert isinstance(\n        self.dataset,\n        GCNDataset), \'The property `support` is only supported by GCNDataset.\'\n    return self.dataset.support\n\n  @property\n  def num_features_nonzero(self):\n    """"""Returns the number of features that are non-zero.\n\n    This is only supported if the dataset is a GCNDataset.\n    """"""\n    assert isinstance(\n        self.dataset, GCNDataset\n    ), \'The property `num_features_nonzero` is only supported by GCNDataset.\'\n    return self.dataset.num_features_nonzero\n\n  def num_train(self):\n    """"""Returns the number of training samples in the dataset.""""""\n    return self.dataset.num_train()\n\n  def num_val(self):\n    """"""Returns the number of validation samples in the dataset.""""""\n    return self.dataset.num_val()\n\n  def num_test(self):\n    """"""Returns the number of test samples in the dataset.""""""\n    return self.dataset.num_test()\n\n  def num_unlabeled(self):\n    """"""Returns the number of unlabeled samples in the dataset.""""""\n    return self.dataset.num_unlabeled()\n\n  def save_state_to_file(self, output_path):\n    """"""Saves the class attributes that are being updated during cotraining.""""""\n    indices_unlabeled = self.get_indices_unlabeled()\n    indices_train = self.get_indices_train()\n    labels_train = self.get_labels(indices_train)\n    with open(os.path.join(output_path, \'indices_train.txt\'), \'w\') as f:\n      np.savetxt(f, indices_train[None], delimiter=\',\')\n    with open(os.path.join(output_path, \'indices_unlabeled.txt\'), \'w\') as f:\n      np.savetxt(f, indices_unlabeled[None], delimiter=\',\')\n    with open(os.path.join(output_path, \'labels_train.txt\'), \'w\') as f:\n      np.savetxt(f, labels_train[None], delimiter=\',\')\n\n  def restore_state_from_file(self, path):\n    """"""Restore the class attributes that are being updated during cotraining.""""""\n    file_indices_train = os.path.join(path, \'indices_train.txt\')\n    file_indices_unlabeled = os.path.join(path, \'indices_unlabeled.txt\')\n    file_labels_train = os.path.join(path, \'labels_train.txt\')\n    indices_type = (\n        np.uint32\n        if self.dataset.num_samples < np.iinfo(np.uint32).max else np.uint64)\n    if os.path.exists(file_indices_train) \\\n        and os.path.exists(file_indices_unlabeled) \\\n        and os.path.exists(file_labels_train):\n      with open(file_indices_train, \'r\') as f:\n        indices_train = np.genfromtxt(f, delimiter=\',\', dtype=indices_type)\n      with open(file_indices_unlabeled, \'r\') as f:\n        indices_unlabeled = np.genfromtxt(f, delimiter=\',\', dtype=indices_type)\n      with open(file_labels_train, \'r\') as f:\n        labels_train = np.genfromtxt(f, delimiter=\',\', dtype=np.uint32)\n      self.dataset.indices_train = indices_train\n      self.dataset.indices_unlabeled = indices_unlabeled\n      self.dataset.update_labels(indices_train, labels_train)\n    else:\n      logging.info(\'No data checkpoints found.\')\n\n  def copy_labels(self):\n    return self.dataset.copy_labels()\n\n  def get_edges(self,\n                src_labeled=None,\n                tgt_labeled=None,\n                label_must_match=False):\n    return self.dataset.get_edges(\n        src_labeled=src_labeled,\n        tgt_labeled=tgt_labeled,\n        label_must_match=label_must_match)\n'"
research/gam/gam/data/loaders.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Data loaders for Graph Agreement Models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nimport os\nimport pickle\nimport sys\n\nfrom .dataset import Dataset\nfrom .dataset import PlanetoidDataset\nimport networkx as nx\nimport numpy as np\nfrom .preprocessing import convert_image\nfrom .preprocessing import split_train_val_unlabeled\nfrom scipy import sparse as sp\nimport tensorflow_datasets as tfds\n\n\ndef load_data_tf_datasets(dataset_name, target_num_train_per_class,\n                          target_num_val, seed):\n  """"""Load and preprocess data from tensorflow_datasets.""""""\n  logging.info(\'Loading and preprocessing data from tensorflow datasets...\')\n  # Load train data.\n  ds = tfds.load(dataset_name, split=tfds.Split.TRAIN, batch_size=-1)\n  ds = tfds.as_numpy(ds)\n  train_inputs, train_labels = ds[\'image\'], ds[\'label\']\n  # Load test data.\n  ds = tfds.load(dataset_name, split=tfds.Split.TEST, batch_size=-1)\n  ds = tfds.as_numpy(ds)\n  test_inputs, test_labels = ds[\'image\'], ds[\'label\']\n\n  # Remove extra dimensions of size 1.\n  train_labels = np.squeeze(train_labels)\n  test_labels = np.squeeze(test_labels)\n\n  logging.info(\'Splitting data...\')\n  data = split_train_val_unlabeled(train_inputs, train_labels,\n                                   target_num_train_per_class, target_num_val,\n                                   seed)\n  train_inputs = data[0]\n  train_labels = data[1]\n  val_inputs = data[2]\n  val_labels = data[3]\n  unlabeled_inputs = data[4]\n  unlabeled_labels = data[5]\n\n  logging.info(\'Converting data to Dataset format...\')\n  data = Dataset.build_from_splits(\n      name=dataset_name,\n      inputs_train=train_inputs,\n      labels_train=train_labels,\n      inputs_val=val_inputs,\n      labels_val=val_labels,\n      inputs_test=test_inputs,\n      labels_test=test_labels,\n      inputs_unlabeled=unlabeled_inputs,\n      labels_unlabeled=unlabeled_labels,\n      feature_preproc_fn=convert_image)\n  return data\n\n\ndef load_data_realistic_ssl(dataset_name, data_path, label_map_path):\n  """"""Loads data from the `ealistic Evaluation of Deep SSL Algorithms`.""""""\n  logging.info(\'Loading data from pickle at %s.\', data_path)\n  train_set, validation_set, test_set = pickle.load(open(data_path, \'rb\'))\n  train_inputs = train_set[\'images\']\n  train_labels = train_set[\'labels\']\n  val_inputs = validation_set[\'images\']\n  val_labels = validation_set[\'labels\']\n  test_inputs = test_set[\'images\']\n  test_labels = test_set[\'labels\']\n  # Load label map that specifies which trainining labeles are available.\n  train_indices = json.load(open(label_map_path, \'r\'))\n  train_indices = [\n      int(key.encode(\'ascii\', \'ignore\')) for key in train_indices[\'values\']\n  ]\n  train_indices = np.asarray(train_indices)\n\n  # Select the loaded train indices, and make the rest unlabeled.\n  unlabeled_mask = np.ones((train_inputs.shape[0],), dtype=np.bool)\n  unlabeled_mask[train_indices] = False\n  unlabeled_inputs = train_inputs[unlabeled_mask]\n  unlabeled_labels = train_labels[unlabeled_mask]\n  train_inputs = train_inputs[train_indices]\n  train_labels = train_labels[train_indices]\n\n  # Select a feature preprocessing function, depending on the dataset.\n  feature_preproc_fn = ((lambda image: image)\n                        if dataset_name == \'cifar10\' else convert_image)\n\n  data = Dataset.build_from_splits(\n      name=dataset_name,\n      inputs_train=train_inputs,\n      labels_train=train_labels,\n      inputs_val=val_inputs,\n      labels_val=val_labels,\n      inputs_test=test_inputs,\n      labels_test=test_labels,\n      inputs_unlabeled=unlabeled_inputs,\n      labels_unlabeled=unlabeled_labels,\n      feature_preproc_fn=feature_preproc_fn)\n  return data\n\n\ndef load_from_planetoid_files(dataset_name, path):\n  """"""Loads Planetoid data in GCN format, as released with the GCN code.\n\n  This function is adapted from https://github.com/tkipf/gcn.\n\n  This function assumes that the following files can be found at the location\n  specified by `path`:\n\n  ind.dataset_str.x          => the feature vectors of the training instances\n                                as scipy.sparse.csr.csr_matrix object.\n  ind.dataset_str.tx         => the feature vectors of the test instances as\n                                scipy.sparse.csr.csr_matrix object.\n  ind.dataset_str.allx       => the feature vectors of both labeled and\n                                unlabeled training instances (a superset of\n                                ind.dataset_str.x) as\n                                scipy.sparse.csr.csr_matrix object.\n  ind.dataset_str.y          => the one-hot labels of the labeled training\n                                instances as numpy.ndarray object.\n  ind.dataset_str.ty         => the one-hot labels of the test instances as\n                                numpy.ndarray object.\n  ind.dataset_str.ally       => the labels for instances in\n                                ind.dataset_str.allx as numpy.ndarray object.\n  ind.dataset_str.graph      => a dict in the format\n                                {index: [index_of_neighbor_nodes]} as\n                                collections.defaultdict object.\n  ind.dataset_str.test.index => the indices of test instances in graph, for\n                                the inductive setting as list object.\n\n  Args:\n    dataset_name: A string representing the dataset name (e.g., `cora`).\n    path: Path to the directory containing the files.\n\n  Returns:\n    All data input files loaded (as well the training/test data).\n  """"""\n\n  def _sample_mask(idx, l):\n    """"""Create mask.""""""\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\n\n  def _parse_index_file(filename):\n    """"""Parse index file.""""""\n    index = []\n    for line in open(filename):\n      index.append(int(line.strip()))\n    return index\n\n  def _load_file(name):\n    """"""Load from data file.""""""\n    filename = \'ind.{}.{}\'.format(dataset_name, name)\n    filename = os.path.join(path, filename)\n    with open(filename, \'rb\') as f:\n      if sys.version_info > (3, 0):\n        return pickle.load(f, encoding=\'latin1\')  # pylint: disable=unexpected-keyword-arg\n      else:\n        return pickle.load(f)\n\n  x = _load_file(\'x\')\n  y = _load_file(\'y\')\n  tx = _load_file(\'tx\')\n  ty = _load_file(\'ty\')\n  allx = _load_file(\'allx\')\n  ally = _load_file(\'ally\')\n  graph = _load_file(\'graph\')\n\n  filename = \'ind.{}.test.index\'.format(dataset_name)\n  filename = os.path.join(path, filename)\n  test_idx_reorder = _parse_index_file(filename)\n  test_idx_range = np.sort(test_idx_reorder)\n\n  if dataset_name == \'citeseer\':\n    # Fix citeseer dataset (there are some isolated nodes in the graph).\n    # Find isolated nodes, add them as zero-vecs into the right position.\n    test_idx_range_full = range(\n        min(test_idx_reorder),\n        max(test_idx_reorder) + 1)\n    tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n    tx_extended[test_idx_range - min(test_idx_range), :] = tx\n    tx = tx_extended\n    ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n    ty_extended[test_idx_range - min(test_idx_range), :] = ty\n    ty = ty_extended\n\n  features = sp.vstack((allx, tx)).tolil()\n  features[test_idx_reorder, :] = features[test_idx_range, :]\n  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n\n  labels = np.vstack((ally, ty))\n  labels[test_idx_reorder, :] = labels[test_idx_range, :]\n\n  idx_test = test_idx_range.tolist()\n  idx_train = range(len(y))\n  idx_val = range(len(y), len(y) + 500)\n\n  train_mask = _sample_mask(idx_train, labels.shape[0])\n  val_mask = _sample_mask(idx_val, labels.shape[0])\n  test_mask = _sample_mask(idx_test, labels.shape[0])\n\n  y_train = np.zeros(labels.shape)\n  y_val = np.zeros(labels.shape)\n  y_test = np.zeros(labels.shape)\n  y_train[train_mask, :] = labels[train_mask, :]\n  y_val[val_mask, :] = labels[val_mask, :]\n  y_test[test_mask, :] = labels[test_mask, :]\n\n  return (adj, features, y_train, y_val, y_test, train_mask, val_mask,\n          test_mask, labels)\n\n\ndef load_data_planetoid(name, path, splits_path=None, row_normalize=False,\n                        data_container_class=PlanetoidDataset):\n  """"""Load Planetoid data.""""""\n  if splits_path is None:\n    # Load from file in Planetoid format.\n    (adj, features, _, _, _, train_mask, val_mask, test_mask,\n     labels) = load_from_planetoid_files(name, path)\n  else:\n    # Otherwise load from a path where we saved a pickle with random splits.\n    logging.info(\'Loading from splits path: %s\', splits_path)\n    (adj, features, _, _, _, train_mask, val_mask, test_mask,\n     labels) = pickle.load(open(splits_path, \'rb\'))\n\n  return data_container_class.build_from_adjacency_matrix(\n      name,\n      adj,\n      features,\n      train_mask,\n      val_mask,\n      test_mask,\n      labels,\n      row_normalize=row_normalize)\n'"
research/gam/gam/data/preprocessing.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions for data preprocessing.\n\nThis file contains functions for preprocessing data, such as splitting into\ntrain, validation and unlabeled samples.\n""""""\n\nfrom __future__ import absolute_import\n\nimport numpy as np\n\n\ndef convert_image(image):\n  """"""Convert an image from pixels in {0, ..., 255} to floats in [-1, 1].""""""\n  image = image * (1. / 255) - 0.5\n  image *= 2.\n  return image\n\n\ndef split_train_val(indices, ratio_val, rng, max_num_val=None):\n  """"""Split the train sample indices into train and validation.\n\n  Args:\n    indices: A numpy array containing the indices of the training samples.\n    ratio_val: A float number between (0, 1) representing the ratio of samples\n      to use for validation.\n    rng: A random number generator.\n    max_num_val: An integer representing the maximum number of samples to\n      include in the validation set.\n\n  Returns:\n    Two numpy arrays containing the subset of indices used for training, and\n    validation, respectively.\n  """"""\n  num_samples = indices.shape[0]\n  num_val = int(ratio_val * num_samples)\n  if max_num_val and num_val > max_num_val:\n    num_val = max_num_val\n  ind = np.arange(0, num_samples)\n  rng.shuffle(ind)\n  ind_val = ind[:num_val]\n  ind_train = ind[num_val:]\n  return ind_train, ind_val\n\n\ndef split_train_val_unlabeled(train_inputs,\n                              train_labels,\n                              target_num_train_per_class,\n                              target_num_val,\n                              seed=None):\n  """"""Splits the training data into train, validation and unlabeled samples.\n\n  Arguments:\n      train_inputs: A numpy array containing the training inputs, where the\n        first dimension represents the samples.\n      train_labels: A numpy array containing the training labels, where the\n        first dimension represents the samples.\n      target_num_train_per_class: An integer representing the number of samples\n        from each class to keep for training.\n      target_num_val: An integer representing the number of samples to keep for\n        validation (in total). We do not make the selection per class.\n      seed: Integer representing the seed for the random number generator that\n        splits the data.\n\n  Returns:\n    train_inputs: A numpy array in the same format as `train_inputs` containing\n      the new train inputs.\n    train_labels: A numpy array in the same format as `train_labels` containing\n      the new train labels.\n    val_inputs: A numpy array in the same format as `train_inputs` containing\n      the validation inputs.\n    val_labels: A numpy array in the same format as `train_labels` containing\n      the validation labels.\n    unlabeled_inputs: A numpy array in the same format as `train_inputs`\n      containing the inputs for the unlabeled nodes.\n    unlabeled_labels: A numpy array in the same format as `train_labels`\n      containing the labels of the samples that will be considered unlabeled.\n  """"""\n  num_train = train_inputs.shape[0]\n  num_val = target_num_val\n  num_classes = max(train_labels) + 1\n\n  assert target_num_val < num_train, \'Too many validation samples required.\'\n\n  # Split the train samples into train and validation.\n  ind = np.arange(0, num_train)\n  rng = np.random.RandomState(seed)\n  rng.shuffle(ind)\n  ind_val = ind[:num_val]\n  ind_train = ind[num_val:]\n  val_inputs = train_inputs[ind_val]\n  val_labels = train_labels[ind_val]\n  train_inputs = train_inputs[ind_train]\n  train_labels = train_labels[ind_train]\n\n  # Out of the remaining training samples, select a fixed number of samples\n  # from each class.\n  ind_train = []\n  for i in range(num_classes):\n    ind_class = np.where(train_labels == i)[0]\n    assert len(ind_class) >= target_num_train_per_class, \\\n           (\'Not enough labels for class %d to select %d labels per class. \'\n            \'Please select a target_num_train_per_class flag lower than %d.\' %\n            (i, target_num_train_per_class, len(ind_class)))\n    rng.shuffle(ind_class)\n    selected_ind = ind_class[:target_num_train_per_class]\n    ind_train.extend(selected_ind)\n\n  # Having selected the train indices, save the remaining unlabeled nodes in a\n  # different group.\n  set_ind_train = set(ind_train)\n  num_train = num_train - num_val\n  ind_unlabeled = [i for i in range(num_train) if i not in set_ind_train]\n  ind_train = np.asarray(ind_train)\n  ind_unlabeled = np.asarray(ind_unlabeled)\n  unlabeled_inputs = train_inputs[ind_unlabeled]\n  unlabeled_labels = train_labels[ind_unlabeled]\n  train_inputs = train_inputs[ind_train]\n  train_labels = train_labels[ind_train]\n\n  return (train_inputs, train_labels, val_inputs, val_labels, unlabeled_inputs,\n          unlabeled_labels)\n'"
research/gam/gam/data/robustness.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions for evaluating robustness.""""""\nimport logging\n\nfrom .dataset import GCNDataset\nfrom .dataset import GraphDataset\nimport numpy as np\nimport scipy\nfrom scipy.sparse import coo_matrix\n\n\ndef compute_percent_correct(data):\n  """"""Compute the ratio of edges connecting two nodes with the same label.\n\n  Args:\n    data: A GraphDataset object.\n\n  Returns:\n    A float representing the ratio of edges among all graph edges that are\n    connecting nodes with similar labels.\n  """"""\n  correct_edges = 0\n  for edge in data.edges:\n    if data.get_labels(edge.src) == data.get_labels(edge.tgt):\n      correct_edges = correct_edges + 1\n  total_edges = len(data.edges)\n  return float(correct_edges) / total_edges, correct_edges, total_edges\n\n\ndef add_noisy_edges(data, target_ratio_correct, symmetrical=True):\n  """"""Add noisy graph edges between nodes with different labels.\n\n  Args:\n    data: A GraphDataset object.\n    target_ratio_correct: A float representing the target ratio of correct edges\n      (i.e. edges connecting nodes with the same label) we want the returned\n      graph to have.\n    symmetrical: A boolean specifying whether the wrong edges are added in both\n      directions (from a source node to a target node, and in reverse as well).\n\n  Returns:\n    A GraphDataset object matching the content of `data` in all but the graph\n    edges. This new graph will have more more edges, with a total ratio of\n    `target_ratio_correct` edges connecting two nodes with similar label.\n  """"""\n  # Compute the original number of correct edges.\n  ratio_correct, correct_edges, total_edges = compute_percent_correct(data)\n  logging.info(\n      \'Ratio correct edges in the initial dataset: %.2f count_correct = %d  \'\n      \'count all = %d\', ratio_correct, correct_edges, total_edges)\n  assert target_ratio_correct < ratio_correct, (\n      \'Cannot achieve requested ratio of %f correct edges by adding more wrong \'\n      \'edges, since the original graph already has %f correct edges.\') % (\n          target_ratio_correct, ratio_correct)\n\n  # Convert edges to sparse matrix, for faster generation of wrong edges.\n  num_edges = len(data.edges)\n  num_nodes = data.num_samples\n  rows = np.zeros((num_edges,), dtype=data.edges[0].src.dtype)\n  cols = np.zeros((num_edges,), dtype=data.edges[0].tgt.dtype)\n  vals = np.zeros((num_edges,), dtype=data.edges[0].weight.dtype)\n  for i, edge in enumerate(data.edges):\n    rows[i] = edge.src\n    cols[i] = edge.tgt\n    vals[i] = edge.weight\n  adj = coo_matrix((vals, (rows, cols)), shape=(num_nodes, num_nodes))\n  adj = adj.tocsr()\n\n  # Add wrong edges.\n  num_edges_to_add = int(correct_edges / target_ratio_correct - total_edges)\n  logging.info(\'Adding %d wrong edges...\', num_edges_to_add)\n  if symmetrical:\n    # We add half as many edges, because each edge is now added in both\n    # directions.\n    num_edges_to_add = num_edges_to_add // 2\n  for _ in range(num_edges_to_add):\n    added = False\n    while not added:\n      i = np.random.choice(data.num_samples)\n      j = np.random.choice(data.num_samples)\n      if (i != j and data.get_labels(i) != data.get_labels(j) and\n          not adj[i, j]):\n        adj[i, j] = 1\n        if symmetrical:\n          adj[j, i] = 1\n        added = True\n\n  # Convert adj back to edges.\n  adj = adj.tocoo()\n  edges = [\n      GraphDataset.Edge(src, tgt, val)\n      for src, tgt, val in zip(adj.row, adj.col, adj.data)]\n\n  if isinstance(data, GCNDataset):\n    # Preprocessing of adjacency matrix for simple GCN model and conversion to\n    # tuple representation.\n    adj_normalized = GCNDataset.normalize_adj(\n        adj + scipy.eye(adj.shape[0])).astype(np.float32)\n    support = GCNDataset.sparse_to_tuple(adj_normalized)\n\n    data_noisy = data.copy(\n        edges=edges,\n        support=support)\n  else:\n    data_noisy = data.copy(edges=edges)\n\n  # Compute the final number of correct edges, to verify it matches the target.\n  ratio_correct, _, _ = compute_percent_correct(data_noisy)\n  logging.info(\'Ratio correct edges in the noisy dataset: %.2f\', ratio_correct)\n\n  return data_noisy\n'"
research/gam/gam/experiments/__init__.py,0,b''
research/gam/gam/experiments/helper.py,6,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper functions for GAMs.""""""\n\nfrom ..models.cnn import ImageCNNAgreement\nfrom ..models.gcn import GCN\nfrom ..models.mlp import MLP\nfrom ..models.wide_resnet import WideResnet\n\nimport tensorflow as tf\n\n\ndef parse_layers_string(layers_string):\n  """"""Convert a layer size string (e.g., `128_64_32`) to a list of integers.""""""\n  if not layers_string:\n    return ()\n  num_hidden = layers_string.split(\'_\')\n  num_hidden = [int(num) for num in num_hidden]\n  return num_hidden\n\n\ndef get_model_cls(model_name,\n                  data,\n                  dataset_name,\n                  hidden=None,\n                  dropout=None,\n                  **unused_kwargs):\n  """"""Picks the models depending on the provided configuration flags.""""""\n  # Create model classification.\n  if model_name == \'mlp\':\n    hidden = parse_layers_string(hidden) if hidden is not None else ()\n    return MLP(\n        output_dim=data.num_classes,\n        hidden_sizes=hidden,\n        activation=tf.nn.leaky_relu,\n        name=\'mlp_cls\')\n  elif model_name == \'cnn\':\n    if dataset_name in (\'mnist\', \'fashion_mnist\'):\n      channels = 1\n    elif dataset_name in (\'cifar10\', \'cifar100\', \'svhn_cropped\', \'svhn\'):\n      channels = 3\n    else:\n      raise ValueError(\'Dataset name `%s` unsupported.\' % dataset_name)\n    return ImageCNNAgreement(\n        output_dim=data.num_classes,\n        channels=channels,\n        activation=tf.nn.leaky_relu,\n        name=\'cnn_cls\')\n  elif model_name == \'wide_resnet\':\n    return WideResnet(\n        num_classes=data.num_classes,\n        lrelu_leakiness=0.1,\n        horizontal_flip=dataset_name in (\'cifar10\',),\n        random_translation=False,\n        gaussian_noise=dataset_name not in (\'svhn\', \'svhn_cropped\'),\n        width=2,\n        num_residual_units=4,\n        name=\'wide_resnet_cls\')\n  elif model_name == \'gcn\':\n    hidden = parse_layers_string(hidden) if hidden is not None else ()\n    assert len(\n        hidden\n    ) == 1, \'GCN implementation currently supports only one hidden layer.\'\n    return GCN(\n        input_dim=data.num_features,\n        output_dim=data.num_classes,\n        hidden=hidden[0],\n        dropout=dropout,\n        aggregation=None,\n        hidden_aggregation=(),\n        activation=tf.nn.leaky_relu,\n        is_binary_classification=False,\n        name=\'gcn_cls\')\n  else:\n    raise NotImplementedError()\n\n\ndef get_model_agr(model_name,\n                  dataset_name,\n                  hidden_aggreg=None,\n                  aggregation_agr_inputs=\'dist\',\n                  hidden=None,\n                  **unused_kwargs):\n  """"""Create agreement model.""""""\n  hidden = parse_layers_string(hidden) if hidden is not None else ()\n  hidden_aggreg = (\n      parse_layers_string(hidden_aggreg) if hidden_aggreg is not None else ())\n  if model_name == \'mlp\':\n    return MLP(\n        output_dim=1,\n        hidden_sizes=hidden,\n        activation=tf.nn.leaky_relu,\n        aggregation=aggregation_agr_inputs,\n        hidden_aggregation=hidden_aggreg,\n        is_binary_classification=True,\n        name=\'mlp_agr\')\n  elif model_name == \'cnn\':\n    if dataset_name in (\'mnist\', \'fashion_mnist\'):\n      channels = 1\n    elif dataset_name in (\'cifar10\', \'cifar100\', \'svhn_cropped\', \'svhn\'):\n      channels = 3\n    else:\n      raise ValueError(\'Dataset name `%s` unsupported.\' % dataset_name)\n    return ImageCNNAgreement(\n        output_dim=1,\n        channels=channels,\n        activation=tf.nn.leaky_relu,\n        aggregation=aggregation_agr_inputs,\n        hidden_aggregation=hidden_aggreg,\n        is_binary_classification=True,\n        name=\'cnn_agr\')\n  elif model_name == \'wide_resnet\':\n    return WideResnet(\n        num_classes=1,\n        lrelu_leakiness=0.1,\n        horizontal_flip=dataset_name in (\'cifar10\',),\n        random_translation=False,\n        gaussian_noise=dataset_name not in (\'svhn\', \'svhn_cropped\'),\n        width=2,\n        num_residual_units=4,\n        name=\'wide_resnet_cls\',\n        is_binary_classification=True,\n        aggregation=aggregation_agr_inputs,\n        activation=tf.nn.leaky_relu,\n        hidden_aggregation=hidden_aggreg)\n  else:\n    raise NotImplementedError()\n'"
research/gam/gam/experiments/run_train_gam.py,2,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Run script for training Graph Agreement Models on MNIST and other datasets.\n\nThroughout this file, the suffix ""_cls"" refers to the classification model, and\n""_agr"" to the agreement model.\n\nThe supported datasets are the following tensorflow_datasets:\nmnist, cifar10, cifar100, svhn_cropped, fashion_mnist.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nfrom logging import config\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom ..data.dataset import Dataset\nfrom ..data.loaders import load_data_planetoid\nfrom ..data.loaders import load_data_realistic_ssl\nfrom ..data.loaders import load_data_tf_datasets\nfrom .helper import get_model_agr\nfrom .helper import get_model_cls\nimport numpy as np\nimport tensorflow as tf\nfrom ..trainer.trainer_cotrain import TrainerCotraining\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\n    \'dataset_name\', \'cifar10\',\n    \'Dataset name. Supported options are: mnist, cifar10, cifar100, \'\n    \'svhn_cropped, fashion_mnist.\')\nflags.DEFINE_string(\n    \'data_source\', \'tensorflow_datasets\', \'Data source. Valid options are: \'\n    \'`tensorflow_datasets`, `realistic_ssl`, `planetoid`.\')\nflags.DEFINE_integer(\n    \'target_num_train_per_class\', 400,\n    \'Number of samples per class to use for training.\')\nflags.DEFINE_integer(\n    \'target_num_val\', 1000,\n    \'Number of samples to be used for validation.\')\nflags.DEFINE_integer(\n    \'seed\', 123,\n    \'Seed used by the random number generators.\')\nflags.DEFINE_bool(\n    \'load_preprocessed\', False,\n    \'Specifies whether to load data already preprocessed. If False, it reads\'\n    \'the original data and splits it.\')\nflags.DEFINE_bool(\n    \'save_preprocessed\', False,\n    \'Specifies whether the preprocessed should be saved to pickle.\')\nflags.DEFINE_string(\n    \'filename_preprocessed_data\', \'preprocessed_data.pickle\',\n    \'Name of the pickle file where the preprocessed data will be loaded from \'\n    \'or stored.\')\nflags.DEFINE_string(\n    \'label_map_path\', \'\',\n    \'Path to the json files containing the label sample indices for \'\n    \'Realistic SSL.\')\nflags.DEFINE_string(\'data_output_dir\', \'./outputs\',\n                    \'Path to a folder where to save the preprocessed dataset.\')\nflags.DEFINE_string(\n    \'output_dir\', \'./outputs\',\n    \'Path to a folder where checkpoints, summaries and other outputs are \'\n    \'stored.\')\nflags.DEFINE_string(\'logging_config\', \'\', \'Path to logging configuration file.\')\nflags.DEFINE_string(\n    \'model_cls\', \'mlp\', \'Model type for the classification model. \'\n    \'Options are: `mlp`, `cnn`, `wide_resnet`\')\nflags.DEFINE_string(\n    \'model_agr\', \'mlp\',\n    \'Model type for the agreement model. Options are: `mlp`, `cnn`, \'\n    \'`wide_resnet`.\')\nflags.DEFINE_float(\'learning_rate_cls\', 0.001,\n                   \'Initial learning rate of the classification model.\')\nflags.DEFINE_float(\'learning_rate_agr\', 0.001,\n                   \'Initial learning rate of the agreement model.\')\nflags.DEFINE_float(\'learning_rate_decay_cls\', None,\n                   \'Learning rate decay factor for the classification model.\')\nflags.DEFINE_float(\'learning_rate_decay_agr\', None,\n                   \'Learning rate decay factor for the agreement model.\')\nflags.DEFINE_float(\'lr_decay_rate_cls\', None,\n                   \'Learning rate decay rate for the classification model.\')\nflags.DEFINE_integer(\'lr_decay_steps_cls\', None,\n                     \'Learning rate decay steps for the classification model.\')\nflags.DEFINE_float(\'lr_decay_rate_agr\', None,\n                   \'Learning rate decay rate for the agreement model.\')\nflags.DEFINE_integer(\'lr_decay_steps_agr\', None,\n                     \'Learning rate decay steps for the agreement model.\')\nflags.DEFINE_integer(\n    \'num_epochs_per_decay_cls\', 350,\n    \'Number of epochs after which the learning rate decays for the \'\n    \'classification model.\')\nflags.DEFINE_integer(\n    \'num_epochs_per_decay_agr\', 350,\n    \'Number of epochs after which the learning rate decays for the \'\n    \'agreement model.\')\nflags.DEFINE_integer(\'max_num_iter_cotrain\', 100, \'Number of epochs to train.\')\nflags.DEFINE_integer(\'min_num_iter_cls\', 200,\n                     \'Minimum number of epochs to train for.\')\nflags.DEFINE_integer(\'max_num_iter_cls\', 100000,\n                     \'Maximum number of epochs to train for.\')\nflags.DEFINE_integer(\n    \'num_iter_after_best_val_cls\', 2000,\n    \'Minimum number of iterations to train the classification model for after \'\n    \'the best validation accuracy is improved.\')\nflags.DEFINE_integer(\n    \'min_num_iter_agr\', 200,\n    \'Minimum number of iterations to train the agreement model for.\')\nflags.DEFINE_integer(\n    \'max_num_iter_agr\', 100000,\n    \'Maximum number of iterations to train the agreement model for.\')\nflags.DEFINE_integer(\n    \'num_iter_after_best_val_agr\', 2000,\n    \'Minimum number of iterations to train the agreement model for after \'\n    \'the best validation accuracy is improved.\')\nflags.DEFINE_integer(\n    \'num_samples_to_label\', 500,\n    \'Number of samples to label after each co-train iteration.\')\nflags.DEFINE_float(\n    \'min_confidence_new_label\', 0.4,\n    \'Minimum confidence required for a sample to be added to the labeled set.\')\nflags.DEFINE_bool(\n    \'keep_label_proportions\', True,\n    \'Whether the newly labeled nodes should have the same label proportions\'\n    \'as the original labeled data.\')\nflags.DEFINE_integer(\n    \'num_warm_up_iter_agr\', 1,\n    \'Minimum number of co-train iterations the agreement must be trained \'\n    \'before it is used in the classifier.\')\nflags.DEFINE_float(\'ratio_valid_agr\', 0.1,\n                   \'Ratio of edges used for validating the agreement model.\')\nflags.DEFINE_integer(\n    \'max_samples_valid_agr\', 10000,\n    \'Max number of samples to set aside for validating the agreement model.\')\nflags.DEFINE_string(\n    \'hidden_cls\', \'128_64_32\',\n    \'String representing the number of units of the hidden layers of the \'\n    \'classification model. This is encoded as a sequence of numbers separated \'\n    \'by underscores (e.g., `128_64_32`), where each number is the number of \'\n    \'units in a layer, counting from the inputs towards outputs\')\nflags.DEFINE_string(\n    \'hidden_agr\', \'128_64_32\',\n    \'String representing the number of units of the hidden layers of the \'\n    \'agreement model.\')\nflags.DEFINE_string(\n    \'hidden_aggreg\', \'\',\n    \'String representing the number of units of the hidden layers of the \'\n    \'aggregation network of the agreement model.\')\nflags.DEFINE_float(\n    \'weight_decay_cls\', None,\n    \'Weight of the L2 penalty on the classification model weights.\')\nflags.DEFINE_string(\n    \'weight_decay_schedule_cls\', None,\n    \'Schedule for decaying the weight decay in the classification model. \'\n    \'Choose bewteen None or linear.\')\nflags.DEFINE_float(\'weight_decay_agr\', None,\n                   \'Weight of the L2 penalty on the agreement model weights.\')\nflags.DEFINE_string(\n    \'weight_decay_schedule_agr\', None,\n    \'Schedule for decaying the weight decay in the agreement model. Choose \'\n    \'between None or linear.\')\nflags.DEFINE_integer(\'batch_size_agr\', 512, \'Batch size for agreement model.\')\nflags.DEFINE_integer(\'batch_size_cls\', 512,\n                     \'Batch size for classification model.\')\nflags.DEFINE_float(\n    \'gradient_clip\', None,\n    \'The gradient clipping global norm value. If None, no clipping is done.\')\nflags.DEFINE_integer(\n    \'logging_step_cls\', 200,\n    \'Print summary of the classification model training every this number of \'\n    \'iterations.\')\nflags.DEFINE_integer(\n    \'logging_step_agr\', 200,\n    \'Print summary of the agreement model training every this number of \'\n    \'iterations.\')\nflags.DEFINE_integer(\n    \'summary_step_cls\', 100,\n    \'Print summary of classification model training every this number of \'\n    \'iterations.\')\nflags.DEFINE_integer(\n    \'summary_step_agr\', 100,\n    \'Print summary of the agreement model training every this number of \'\n    \'iterations.\')\nflags.DEFINE_integer(\n    \'eval_step_cls\', 100,\n    \'Evaluate classification model every this number of iterations.\')\nflags.DEFINE_integer(\n    \'eval_step_agr\', 100,\n    \'Evaluate the agreement model every this number of iterations.\')\nflags.DEFINE_bool(\n    \'warm_start_cls\', False,\n    \'Whether to reinitialize the parameters of the classification model before \'\n    \'retraining (if False), or use the ones from the previous cotrain\'\n    \' iteration.\')\nflags.DEFINE_bool(\n    \'warm_start_agr\', False,\n    \'Whether to reinitialize the parameters of the agreement model before \'\n    \'retraining (if False), or use the ones from the previous cotrain \'\n    \'iteration.\')\nflags.DEFINE_bool(\'use_perfect_agreement\', False,\n                  \'Whether to use perfect agreement.\')\nflags.DEFINE_bool(\'use_perfect_classifier\', False,\n                  \'Whether to use perfect classifier.\')\nflags.DEFINE_float(\'reg_weight_ll\', 0.00,\n                   \'Regularization weight for labeled-labeled edges.\')\nflags.DEFINE_float(\'reg_weight_lu\', 0.1,\n                   \'Regularization weight for labeled-unlabeled edges.\')\nflags.DEFINE_float(\'reg_weight_uu\', 0.05,\n                   \'Regularization weight for unlabeled-unlabeled edges.\')\nflags.DEFINE_integer(\n    \'num_pairs_reg\', 128,\n    \'Number of pairs of nodes to use in the agreement loss term of the \'\n    \'classification model.\')\nflags.DEFINE_float(\n    \'reg_weight_vat\', 0.0,\n    \'Regularization weight for the virtual adversarial training (VAT) loss.\')\nflags.DEFINE_bool(\n    \'use_ent_min\', False,\n    \'A boolean specifying whether to add entropy minimization to VAT.\')\nflags.DEFINE_string(\n    \'aggregation_agr_inputs\', \'dist\',\n    \'Operation to apply on the pair of nodes in the agreement model. \'\n    \'Available options are `add`, `dist`, `concat`, `project_add`,\'\n    \'`project_dist`, `project_concat` and None.\')\nflags.DEFINE_bool(\n    \'penalize_neg_agr\', True,\n    \'Whether to encourage differences when agreement is negative.\')\nflags.DEFINE_bool(\n    \'use_l2_cls\', False,\n    \'Whether to use L2 loss for the classifier, not cross entropy.\')\nflags.DEFINE_bool(\n    \'first_iter_original\', True,\n    \'Whether to use the original model in the first iteration, without self \'\n    \'labeling or agreement loss.\')\nflags.DEFINE_bool(\'inductive\', True,\n                  \'Whether to use an inductive or transductive SSL setting.\')\nflags.DEFINE_string(\n    \'experiment_suffix\', \'\',\n    \'A suffix you might want to add at the end of the experiment name to\'\n    \'identify it.\')\nflags.DEFINE_bool(\n    \'eval_acc_pred_by_agr\', False,\n    \'Whether to compute the accuracy of a classification model that makes \'\n    \'label predictions using the agreement model`s predictions. This is done\'\n    \'by computing the majority vote of the labeled samples, weighted by the \'\n    \' agreement model. This is for monitoring the progress only.\')\nflags.DEFINE_integer(\n    \'num_neighbors_pred_by_agr\', 50,\n    \'Number of labeled samples to use when predicting by agreement.\')\nflags.DEFINE_string(\n    \'optimizer\', \'adam\',\n    \'Which optimizer to use. Valid options are `adam`, `amsgrad`.\')\nflags.DEFINE_bool(\n    \'load_from_checkpoint\', False,\n    \'Whether to load the trained model and the data that has been self-labeled \'\n    \'from a previous run, if available. This is useful if a process can get \'\n    \'preempted or interrupted.\')\n\n\ndef load_data():\n  """"""Loads data.""""""\n  if FLAGS.data_source == \'tensorflow_datasets\':\n    return load_data_tf_datasets(FLAGS.dataset_name,\n                                 FLAGS.target_num_train_per_class,\n                                 FLAGS.target_num_val, FLAGS.seed)\n  elif FLAGS.data_source == \'realistic_ssl\':\n    return load_data_realistic_ssl(FLAGS.dataset_name,\n                                   FLAGS.filename_preprocessed_data,\n                                   FLAGS.label_map_path)\n  elif FLAGS.data_source == \'planetoid\':\n    return load_data_planetoid(\n        FLAGS.dataset_name, FLAGS.preprocessed_data_dir, row_normalize=False)\n  raise ValueError(\'Unsupported dataset source name: %s\' % FLAGS.data_source)\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError(\'Too many command-line arguments.\')\n\n  if FLAGS.logging_config:\n    print(\'Setting logging configuration: \', FLAGS.logging_config)\n    config.fileConfig(FLAGS.logging_config)\n\n  # Set random seed.\n  np.random.seed(FLAGS.seed)\n  tf.set_random_seed(FLAGS.seed)\n\n  ############################################################################\n  #                               DATA                                       #\n  ############################################################################\n  # Potentially create a folder where to save the preprocessed data.\n  if not os.path.exists(FLAGS.data_output_dir):\n    os.makedirs(FLAGS.data_output_dir)\n\n  # Load and potentially preprocess data.\n  if FLAGS.load_preprocessed:\n    logging.info(\'Loading preprocessed data...\')\n    path = os.path.join(FLAGS.data_output_dir, FLAGS.filename_preprocessed_data)\n    data = Dataset.load_from_pickle(path)\n  else:\n    data = load_data()\n    if FLAGS.save_preprocessed:\n      assert FLAGS.output_dir\n      path = os.path.join(FLAGS.data_output_dir,\n                          FLAGS.filename_preprocessed_data)\n      data.save_to_pickle(path)\n      logging.info(\'Preprocessed data saved to %s.\', path)\n\n  ############################################################################\n  #                            PREPARE OUTPUTS                               #\n  ############################################################################\n  # Put together parameters to create a model name.\n  model_name = FLAGS.model_cls\n  model_name += (\'_\' + FLAGS.hidden_cls) if FLAGS.model_cls == \'mlp\' else \'\'\n  model_name += \'-\' + FLAGS.model_agr\n  model_name += (\'_\' + FLAGS.hidden_agr) if FLAGS.model_agr == \'mlp\' else \'\'\n  model_name += \'-aggr_\' + FLAGS.aggregation_agr_inputs\n  model_name += (\'_\' + FLAGS.hidden_aggreg) if FLAGS.hidden_aggreg else \'\'\n  model_name += (\n      \'-add_%d-conf_%.2f-iterCls_%d-iterAgr_%d-batchCls_%d\' %\n      (FLAGS.num_samples_to_label, FLAGS.min_confidence_new_label,\n       FLAGS.max_num_iter_cls, FLAGS.max_num_iter_agr, FLAGS.batch_size_cls))\n  model_name += ((\'-wdecayCls_%.4f\' %\n                  FLAGS.weight_decay_cls) if FLAGS.weight_decay_cls else \'\')\n  model_name += ((\'-wdecayAgr_%.4f\' %\n                  FLAGS.weight_decay_agr) if FLAGS.weight_decay_agr else \'\')\n  model_name += \'-LL_%s_LU_%s_UU_%s\' % (str(\n      FLAGS.reg_weight_ll), str(FLAGS.reg_weight_lu), str(FLAGS.reg_weight_uu))\n  model_name += \'-perfAgr\' if FLAGS.use_perfect_agreement else \'\'\n  model_name += \'-perfCls\' if FLAGS.use_perfect_classifier else \'\'\n  model_name += \'-keepProp\' if FLAGS.keep_label_proportions else \'\'\n  model_name += \'-PenNegAgr\' if FLAGS.penalize_neg_agr else \'\'\n  model_name += \'-transd\' if not FLAGS.inductive else \'\'\n  model_name += \'-L2\' if FLAGS.use_l2_cls else \'-CE\'\n  model_name += \'-seed_\' + str(FLAGS.seed)\n  model_name += FLAGS.experiment_suffix\n  logging.info(\'Model name: %s\', model_name)\n\n  # Create directories for model checkpoints, summaries, and\n  # self-labeled data backup.\n  summary_dir = os.path.join(FLAGS.output_dir, \'summaries\', FLAGS.dataset_name,\n                             model_name)\n  checkpoints_dir = os.path.join(FLAGS.output_dir, \'checkpoints\',\n                                 FLAGS.dataset_name, model_name)\n  data_dir = os.path.join(FLAGS.data_output_dir, \'data_checkpoints\',\n                          FLAGS.dataset_name, model_name)\n  if not os.path.exists(checkpoints_dir):\n    os.makedirs(checkpoints_dir)\n  if not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n  ############################################################################\n  #                            MODEL SETUP                                   #\n  ############################################################################\n  # Select the model based on the provided FLAGS.\n  model_cls = get_model_cls(\n      model_name=FLAGS.model_cls,\n      data=data,\n      dataset_name=FLAGS.dataset_name,\n      hidden=FLAGS.hidden_cls)\n\n  # Create agreement model.\n  model_agr = get_model_agr(\n      model_name=FLAGS.model_agr,\n      dataset_name=FLAGS.dataset_name,\n      hidden_aggreg=FLAGS.hidden_aggreg,\n      aggregation_agr_inputs=FLAGS.aggregation_agr_inputs,\n      hidden=FLAGS.hidden_agr)\n\n  # Train.\n  trainer = TrainerCotraining(\n      model_cls=model_cls,\n      model_agr=model_agr,\n      max_num_iter_cotrain=FLAGS.max_num_iter_cotrain,\n      min_num_iter_cls=FLAGS.min_num_iter_cls,\n      max_num_iter_cls=FLAGS.max_num_iter_cls,\n      num_iter_after_best_val_cls=FLAGS.num_iter_after_best_val_cls,\n      min_num_iter_agr=FLAGS.min_num_iter_agr,\n      max_num_iter_agr=FLAGS.max_num_iter_agr,\n      num_iter_after_best_val_agr=FLAGS.num_iter_after_best_val_agr,\n      num_samples_to_label=FLAGS.num_samples_to_label,\n      min_confidence_new_label=FLAGS.min_confidence_new_label,\n      keep_label_proportions=FLAGS.keep_label_proportions,\n      num_warm_up_iter_agr=FLAGS.num_warm_up_iter_agr,\n      optimizer=tf.train.AdamOptimizer,\n      gradient_clip=FLAGS.gradient_clip,\n      batch_size_agr=FLAGS.batch_size_agr,\n      batch_size_cls=FLAGS.batch_size_cls,\n      learning_rate_cls=FLAGS.learning_rate_cls,\n      learning_rate_agr=FLAGS.learning_rate_agr,\n      enable_summaries=True,\n      enable_summaries_per_model=True,\n      summary_dir=summary_dir,\n      summary_step_cls=FLAGS.summary_step_cls,\n      summary_step_agr=FLAGS.summary_step_agr,\n      logging_step_cls=FLAGS.logging_step_cls,\n      logging_step_agr=FLAGS.logging_step_agr,\n      eval_step_cls=FLAGS.eval_step_cls,\n      eval_step_agr=FLAGS.eval_step_agr,\n      checkpoints_dir=checkpoints_dir,\n      checkpoints_step=1,\n      data_dir=data_dir,\n      abs_loss_chg_tol=1e-10,\n      rel_loss_chg_tol=1e-7,\n      loss_chg_iter_below_tol=30,\n      use_perfect_agr=FLAGS.use_perfect_agreement,\n      use_perfect_cls=FLAGS.use_perfect_classifier,\n      warm_start_cls=FLAGS.warm_start_cls,\n      warm_start_agr=FLAGS.warm_start_agr,\n      ratio_valid_agr=FLAGS.ratio_valid_agr,\n      max_samples_valid_agr=FLAGS.max_samples_valid_agr,\n      weight_decay_cls=FLAGS.weight_decay_cls,\n      weight_decay_schedule_cls=FLAGS.weight_decay_schedule_cls,\n      weight_decay_schedule_agr=FLAGS.weight_decay_schedule_agr,\n      weight_decay_agr=FLAGS.weight_decay_agr,\n      reg_weight_ll=FLAGS.reg_weight_ll,\n      reg_weight_lu=FLAGS.reg_weight_lu,\n      reg_weight_uu=FLAGS.reg_weight_uu,\n      reg_weight_vat=FLAGS.reg_weight_vat,\n      use_ent_min=FLAGS.use_ent_min,\n      num_pairs_reg=FLAGS.num_pairs_reg,\n      penalize_neg_agr=FLAGS.penalize_neg_agr,\n      use_l2_cls=FLAGS.use_l2_cls,\n      first_iter_original=FLAGS.first_iter_original,\n      inductive=FLAGS.inductive,\n      seed=FLAGS.seed,\n      eval_acc_pred_by_agr=FLAGS.eval_acc_pred_by_agr,\n      num_neighbors_pred_by_agr=FLAGS.num_neighbors_pred_by_agr,\n      lr_decay_rate_cls=FLAGS.lr_decay_rate_cls,\n      lr_decay_steps_cls=FLAGS.lr_decay_steps_cls,\n      lr_decay_rate_agr=FLAGS.lr_decay_rate_agr,\n      lr_decay_steps_agr=FLAGS.lr_decay_steps_agr,\n      load_from_checkpoint=FLAGS.load_from_checkpoint)\n\n  ############################################################################\n  #                            TRAIN                                         #\n  ############################################################################\n  trainer.train(data)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
research/gam/gam/experiments/run_train_gam_graph.py,2,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Run script for training Graph Agreement Models on datasets with a graph.\n\nThroughout this file, the suffix ""_cls"" refers to the classification model, and\n""_agr"" to the agreement model.\n\nThe supported datasets are the following datasets: cora, citeseer, pubmed.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nfrom logging import config\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom ..data.dataset import GCNDataset\nfrom ..data.dataset import PlanetoidDataset\nfrom ..data.loaders import load_data_planetoid\nfrom ..data.robustness import add_noisy_edges\nfrom .helper import get_model_agr\nfrom .helper import get_model_cls\nimport numpy as np\nimport tensorflow as tf\nfrom ..trainer.trainer_cotrain import TrainerCotraining\n\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\n    \'dataset_name\', \'cora\',\n    \'Dataset name. Supported options are: cora, citeseer, pubmed.\')\nflags.DEFINE_string(\'data_path\', \'\', \'Path to data.\')\nflags.DEFINE_bool(\'row_normalize\', False,\n                  \'Whether to row normalize the data. Important for GCN.\')\nflags.DEFINE_string(\'data_output_dir\', \'./outputs\',\n                    \'Path to a folder where to save the preprocessed dataset.\')\nflags.DEFINE_integer(\'target_num_train_per_class\', 20,\n                     \'Number of samples per class to use for training.\')\nflags.DEFINE_integer(\'target_num_val\', 1000,\n                     \'Number of samples to be used for validation.\')\nflags.DEFINE_float(\n    \'target_ratio_correct\', None,\n    \'Ratio of correct edges that we want the graph to have, after adding\'\n    \'`wrong` edges between nodes with different labels. This is parameters is\'\n    \'introduced to test the robustness of GAM to noisy edges.\')\nflags.DEFINE_integer(\'seed\', 123, \'Seed used by the random number generators.\')\nflags.DEFINE_string(\n    \'output_dir\', \'./outputs\',\n    \'Path to a folder where checkpoints, summaries and other outputs are \'\n    \'stored.\')\nflags.DEFINE_string(\'logging_config\', \'\', \'Path to logging configuration file.\')\nflags.DEFINE_string(\n    \'model_cls\', \'mlp\', \'Model type for the classification model. \'\n    \'Options are: `mlp`, `cnn`, `wide_resnet`, `gcn`.\')\nflags.DEFINE_string(\n    \'model_agr\', \'mlp\',\n    \'Model type for the agreement model. Options are: `mlp`, `cnn`, \'\n    \'`wide_resnet`.\')\nflags.DEFINE_float(\'learning_rate_cls\', 0.001,\n                   \'Initial learning rate of the classification model.\')\nflags.DEFINE_float(\'learning_rate_agr\', 0.001,\n                   \'Initial learning rate of the agreement model.\')\nflags.DEFINE_float(\'learning_rate_decay_cls\', None,\n                   \'Learning rate decay factor for the classification model.\')\nflags.DEFINE_float(\'learning_rate_decay_agr\', None,\n                   \'Learning rate decay factor for the agreement model.\')\nflags.DEFINE_float(\'lr_decay_rate_cls\', None,\n                   \'Learning rate decay rate for the classification model.\')\nflags.DEFINE_integer(\'lr_decay_steps_cls\', None,\n                     \'Learning rate decay steps for the classification model.\')\nflags.DEFINE_float(\'lr_decay_rate_agr\', None,\n                   \'Learning rate decay rate for the agreement model.\')\nflags.DEFINE_integer(\'lr_decay_steps_agr\', None,\n                     \'Learning rate decay steps for the agreement model.\')\nflags.DEFINE_integer(\'max_num_iter_cotrain\', 200, \'Number of epochs to train.\')\nflags.DEFINE_integer(\'min_num_iter_cls\', 200,\n                     \'Minimum number of epochs to train for.\')\nflags.DEFINE_integer(\'max_num_iter_cls\', 100000,\n                     \'Maximum number of epochs to train for.\')\nflags.DEFINE_integer(\n    \'num_iter_after_best_val_cls\', 2000,\n    \'Minimum number of iterations to train the classification model for after \'\n    \'the best validation accuracy is improved.\')\nflags.DEFINE_integer(\n    \'min_num_iter_agr\', 200,\n    \'Minimum number of iterations to train the agreement model for.\')\nflags.DEFINE_integer(\n    \'max_num_iter_agr\', 100000,\n    \'Maximum number of iterations to train the agreement model for.\')\nflags.DEFINE_integer(\n    \'num_iter_after_best_val_agr\', 2000,\n    \'Minimum number of iterations to train the agreement model for after \'\n    \'the best validation accuracy is improved.\')\nflags.DEFINE_integer(\n    \'num_samples_to_label\', 200,\n    \'Number of samples to label after each co-train iteration.\')\nflags.DEFINE_float(\n    \'min_confidence_new_label\', 0.4,\n    \'Minimum confidence required for a sample to be added to the labeled set.\')\nflags.DEFINE_bool(\n    \'keep_label_proportions\', True,\n    \'Whether the newly labeled nodes should have the same label proportions\'\n    \'as the original labeled data.\')\nflags.DEFINE_integer(\n    \'num_warm_up_iter_agr\', 1,\n    \'Minimum number of co-train iterations the agreement must be trained \'\n    \'before it is used in the classifier.\')\nflags.DEFINE_float(\'ratio_valid_agr\', 0.1,\n                   \'Ratio of edges used for validating the agreement model.\')\nflags.DEFINE_integer(\n    \'max_samples_valid_agr\', 10000,\n    \'Max number of samples to set aside for validating the agreement model.\')\nflags.DEFINE_string(\n    \'hidden_cls\', \'128\',\n    \'String representing the number of units of the hidden layers of the \'\n    \'classification model. This is encoded as a sequence of numbers separated \'\n    \'by underscores (e.g., `128_64_32`), where each number is the number of \'\n    \'units in a layer, counting from the inputs towards outputs\')\nflags.DEFINE_string(\n    \'hidden_agr\', \'128\',\n    \'String representing the number of units of the hidden layers of the \'\n    \'agreement model.\')\nflags.DEFINE_string(\n    \'hidden_aggreg\', \'\',\n    \'String representing the number of units of the hidden layers of the \'\n    \'aggregation network of the agreement model.\')\nflags.DEFINE_float(\n    \'weight_decay_cls\', None,\n    \'Weight of the L2 penalty on the classification model weights.\')\nflags.DEFINE_string(\n    \'weight_decay_schedule_cls\', None,\n    \'Schedule for decaying the weight decay in the classification model. \'\n    \'Choose bewteen None or linear.\')\nflags.DEFINE_float(\'weight_decay_agr\', None,\n                   \'Weight of the L2 penalty on the agreement model weights.\')\nflags.DEFINE_string(\n    \'weight_decay_schedule_agr\', None,\n    \'Schedule for decaying the weight decay in the agreement model. Choose \'\n    \'between None or linear.\')\nflags.DEFINE_float(\'dropout\', None, \'Dropout rate (1 - keep probability).\')\nflags.DEFINE_integer(\'batch_size_agr\', 128, \'Batch size for agreement model.\')\nflags.DEFINE_integer(\'batch_size_cls\', 128,\n                     \'Batch size for classification model.\')\nflags.DEFINE_float(\n    \'gradient_clip\', None,\n    \'The gradient clipping global norm value. If None, no clipping is done.\')\nflags.DEFINE_integer(\n    \'logging_step_cls\', 200,\n    \'Print summary of the classification model training every this number of \'\n    \'iterations.\')\nflags.DEFINE_integer(\n    \'logging_step_agr\', 200,\n    \'Print summary of the agreement model training every this number of \'\n    \'iterations.\')\nflags.DEFINE_integer(\n    \'summary_step_cls\', 100,\n    \'Print summary of classification model training every this number of \'\n    \'iterations.\')\nflags.DEFINE_integer(\n    \'summary_step_agr\', 100,\n    \'Print summary of the agreement model training every this number of \'\n    \'iterations.\')\nflags.DEFINE_integer(\n    \'eval_step_cls\', 5,\n    \'Evaluate classification model every this number of iterations.\')\nflags.DEFINE_integer(\n    \'eval_step_agr\', 10,\n    \'Evaluate the agreement model every this number of iterations.\')\nflags.DEFINE_bool(\n    \'warm_start_cls\', False,\n    \'Whether to reinitialize the parameters of the classification model before \'\n    \'retraining (if False), or use the ones from the previous cotrain\'\n    \' iteration.\')\nflags.DEFINE_bool(\n    \'warm_start_agr\', False,\n    \'Whether to reinitialize the parameters of the agreement model before \'\n    \'retraining (if False), or use the ones from the previous cotrain \'\n    \'iteration.\')\nflags.DEFINE_bool(\'use_perfect_agreement\', False,\n                  \'Whether to use perfect agreement.\')\nflags.DEFINE_bool(\'use_perfect_classifier\', False,\n                  \'Whether to use perfect classifier.\')\nflags.DEFINE_float(\'reg_weight_ll\', 0.00,\n                   \'Regularization weight for labeled-labeled edges.\')\nflags.DEFINE_float(\'reg_weight_lu\', 0.1,\n                   \'Regularization weight for labeled-unlabeled edges.\')\nflags.DEFINE_float(\'reg_weight_uu\', 0.05,\n                   \'Regularization weight for unlabeled-unlabeled edges.\')\nflags.DEFINE_integer(\n    \'num_pairs_reg\', 128,\n    \'Number of pairs of nodes to use in the agreement loss term of the \'\n    \'classification model.\')\nflags.DEFINE_float(\n    \'reg_weight_vat\', 0.0,\n    \'Regularization weight for the virtual adversarial training (VAT) loss.\')\nflags.DEFINE_bool(\n    \'use_ent_min\', False,\n    \'A boolean specifying whether to add entropy minimization to VAT.\')\nflags.DEFINE_string(\n    \'aggregation_agr_inputs\', \'dist\',\n    \'Operation to apply on the pair of nodes in the agreement model. \'\n    \'Available options are `add`, `dist`, `concat`, `project_add`,\'\n    \'`project_dist`, `project_concat` and None.\')\nflags.DEFINE_bool(\n    \'penalize_neg_agr\', False,\n    \'Whether to encourage differences when agreement is negative.\')\nflags.DEFINE_bool(\n    \'use_l2_cls\', False,\n    \'Whether to use L2 loss for the classifier, not cross entropy.\')\nflags.DEFINE_bool(\n    \'first_iter_original\', True,\n    \'Whether to use the original model in the first iteration, without self \'\n    \'labeling or agreement loss.\')\nflags.DEFINE_bool(\'inductive\', False,\n                  \'Whether to use an inductive or transductive SSL setting.\')\nflags.DEFINE_string(\n    \'experiment_suffix\', \'\',\n    \'A suffix you might want to add at the end of the experiment name to\'\n    \'identify it.\')\nflags.DEFINE_bool(\n    \'eval_acc_pred_by_agr\', False,\n    \'Whether to compute the accuracy of a classification model that makes \'\n    \'label predictions using the agreement model`s predictions. This is done\'\n    \'by computing the majority vote of the labeled samples, weighted by the \'\n    \' agreement model. This is for monitoring the progress only.\')\nflags.DEFINE_integer(\n    \'num_neighbors_pred_by_agr\', 50,\n    \'Number of labeled samples to use when predicting by agreement.\')\nflags.DEFINE_string(\n    \'optimizer\', \'adam\',\n    \'Which optimizer to use. Valid options are `adam`, `amsgrad`.\')\nflags.DEFINE_bool(\n    \'load_from_checkpoint\', False,\n    \'Whether to load the trained model and the data that has been self-labeled \'\n    \'from a previous run, if available. This is useful if a process can get \'\n    \'preempted or interrupted.\')\nflags.DEFINE_bool(\n    \'always_agree\', False,\n    \'Whether the agreement model should always return agreement. \'\n    \'This is equivalent to Neural Graph Machines.\')\nflags.DEFINE_bool(\n    \'add_negative_edges_agr\', True,\n    \'Whether to add fake negative edges when training the agreement model, in\'\n    \'order to keep the classes balanced.\')\nflags.DEFINE_bool(\'use_graph\', True,\n                  \'Whether to use the graph edges, or any pair of samples.\')\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError(\'Too many command-line arguments.\')\n\n  if FLAGS.logging_config:\n    print(\'Setting logging configuration: \', FLAGS.logging_config)\n    config.fileConfig(FLAGS.logging_config)\n\n  # Set random seed.\n  np.random.seed(FLAGS.seed)\n  tf.set_random_seed(FLAGS.seed)\n\n  ############################################################################\n  #                               DATA                                       #\n  ############################################################################\n  # Load data.\n  data_class = GCNDataset if FLAGS.model_cls == \'gcn\' else PlanetoidDataset\n  data = load_data_planetoid(\n      name=FLAGS.dataset_name,\n      path=FLAGS.data_path,\n      row_normalize=FLAGS.row_normalize,\n      data_container_class=data_class)\n\n  # Potentially add noisy edges. This can be used to asses the robustness of\n  # GAM to noisy edges. See `Robustness` section of our paper.\n  if FLAGS.target_ratio_correct:\n    data = add_noisy_edges(data, FLAGS.target_ratio_correct)\n\n  ############################################################################\n  #                            PREPARE OUTPUTS                               #\n  ############################################################################\n  # Put together parameters to create a model name.\n  model_name = FLAGS.model_cls\n  model_name += (\'_\' + FLAGS.hidden_cls) if FLAGS.model_cls == \'mlp\' else \'\'\n  model_name += \'-\' + FLAGS.model_agr\n  model_name += (\'_\' + FLAGS.hidden_agr) if FLAGS.model_agr == \'mlp\' else \'\'\n  model_name += \'-aggr_\' + FLAGS.aggregation_agr_inputs\n  model_name += (\'_\' + FLAGS.hidden_aggreg) if FLAGS.hidden_aggreg else \'\'\n  model_name += (\n      \'-add_%d-conf_%.2f-iterCls_%d-iterAgr_%d-batchCls_%d\' %\n      (FLAGS.num_samples_to_label, FLAGS.min_confidence_new_label,\n       FLAGS.max_num_iter_cls, FLAGS.max_num_iter_agr, FLAGS.batch_size_cls))\n  model_name += ((\'-wdecayCls_%.4f\' %\n                  FLAGS.weight_decay_cls) if FLAGS.weight_decay_cls else \'\')\n  model_name += ((\'-wdecayAgr_%.4f\' %\n                  FLAGS.weight_decay_agr) if FLAGS.weight_decay_agr else \'\')\n  model_name += \'-LL_%s_LU_%s_UU_%s\' % (str(\n      FLAGS.reg_weight_ll), str(FLAGS.reg_weight_lu), str(FLAGS.reg_weight_uu))\n  model_name += \'-perfAgr\' if FLAGS.use_perfect_agreement else \'\'\n  model_name += \'-perfCls\' if FLAGS.use_perfect_classifier else \'\'\n  model_name += \'-keepProp\' if FLAGS.keep_label_proportions else \'\'\n  model_name += \'-PenNegAgr\' if FLAGS.penalize_neg_agr else \'\'\n  model_name += \'-VAT\' if FLAGS.reg_weight_vat > 0 else \'\'\n  model_name += \'ENT\' if FLAGS.reg_weight_vat > 0 and FLAGS.use_ent_min else \'\'\n  model_name += \'-transd\' if not FLAGS.inductive else \'\'\n  model_name += \'-L2\' if FLAGS.use_l2_cls else \'-CE\'\n  model_name += \'-graph\' if FLAGS.use_graph else \'-noGraph\'\n  model_name += \'-rowNorm\' if FLAGS.row_normalize else \'\'\n  model_name += \'-seed_\' + str(FLAGS.seed)\n  model_name += FLAGS.experiment_suffix\n  logging.info(\'Model name: %s\', model_name)\n\n  # Create directories for model checkpoints, summaries, and\n  # self-labeled data backup.\n  summary_dir = os.path.join(FLAGS.output_dir, \'summaries\', FLAGS.dataset_name,\n                             model_name)\n  checkpoints_dir = os.path.join(FLAGS.output_dir, \'checkpoints\',\n                                 FLAGS.dataset_name, model_name)\n  data_dir = os.path.join(FLAGS.data_output_dir, \'data_checkpoints\',\n                          FLAGS.dataset_name, model_name)\n  if not os.path.exists(checkpoints_dir):\n    os.makedirs(checkpoints_dir)\n  if not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n  ############################################################################\n  #                            MODEL SETUP                                   #\n  ############################################################################\n  # Create classification model.\n  model_cls = get_model_cls(\n      model_name=FLAGS.model_cls,\n      data=data,\n      dataset_name=FLAGS.dataset_name,\n      hidden=FLAGS.hidden_cls)\n\n  # Create agreement model.\n  model_agr = get_model_agr(\n      model_name=FLAGS.model_agr,\n      dataset_name=FLAGS.dataset_name,\n      hidden_aggreg=FLAGS.hidden_aggreg,\n      aggregation_agr_inputs=FLAGS.aggregation_agr_inputs,\n      hidden=FLAGS.hidden_agr)\n\n  # Train.\n  trainer = TrainerCotraining(\n      model_cls=model_cls,\n      model_agr=model_agr,\n      max_num_iter_cotrain=FLAGS.max_num_iter_cotrain,\n      min_num_iter_cls=FLAGS.min_num_iter_cls,\n      max_num_iter_cls=FLAGS.max_num_iter_cls,\n      num_iter_after_best_val_cls=FLAGS.num_iter_after_best_val_cls,\n      min_num_iter_agr=FLAGS.min_num_iter_agr,\n      max_num_iter_agr=FLAGS.max_num_iter_agr,\n      num_iter_after_best_val_agr=FLAGS.num_iter_after_best_val_agr,\n      num_samples_to_label=FLAGS.num_samples_to_label,\n      min_confidence_new_label=FLAGS.min_confidence_new_label,\n      keep_label_proportions=FLAGS.keep_label_proportions,\n      num_warm_up_iter_agr=FLAGS.num_warm_up_iter_agr,\n      optimizer=tf.train.AdamOptimizer,\n      gradient_clip=FLAGS.gradient_clip,\n      batch_size_agr=FLAGS.batch_size_agr,\n      batch_size_cls=FLAGS.batch_size_cls,\n      learning_rate_cls=FLAGS.learning_rate_cls,\n      learning_rate_agr=FLAGS.learning_rate_agr,\n      enable_summaries=True,\n      enable_summaries_per_model=True,\n      summary_dir=summary_dir,\n      summary_step_cls=FLAGS.summary_step_cls,\n      summary_step_agr=FLAGS.summary_step_agr,\n      logging_step_cls=FLAGS.logging_step_cls,\n      logging_step_agr=FLAGS.logging_step_agr,\n      eval_step_cls=FLAGS.eval_step_cls,\n      eval_step_agr=FLAGS.eval_step_agr,\n      checkpoints_dir=checkpoints_dir,\n      checkpoints_step=1,\n      data_dir=data_dir,\n      abs_loss_chg_tol=1e-10,\n      rel_loss_chg_tol=1e-7,\n      loss_chg_iter_below_tol=30,\n      use_perfect_agr=FLAGS.use_perfect_agreement,\n      use_perfect_cls=FLAGS.use_perfect_classifier,\n      warm_start_cls=FLAGS.warm_start_cls,\n      warm_start_agr=FLAGS.warm_start_agr,\n      ratio_valid_agr=FLAGS.ratio_valid_agr,\n      max_samples_valid_agr=FLAGS.max_samples_valid_agr,\n      weight_decay_cls=FLAGS.weight_decay_cls,\n      weight_decay_schedule_cls=FLAGS.weight_decay_schedule_cls,\n      weight_decay_schedule_agr=FLAGS.weight_decay_schedule_agr,\n      weight_decay_agr=FLAGS.weight_decay_agr,\n      reg_weight_ll=FLAGS.reg_weight_ll,\n      reg_weight_lu=FLAGS.reg_weight_lu,\n      reg_weight_uu=FLAGS.reg_weight_uu,\n      num_pairs_reg=FLAGS.num_pairs_reg,\n      reg_weight_vat=FLAGS.reg_weight_vat,\n      use_ent_min=FLAGS.use_ent_min,\n      penalize_neg_agr=FLAGS.penalize_neg_agr,\n      use_l2_cls=FLAGS.use_l2_cls,\n      first_iter_original=FLAGS.first_iter_original,\n      inductive=FLAGS.inductive,\n      seed=FLAGS.seed,\n      eval_acc_pred_by_agr=FLAGS.eval_acc_pred_by_agr,\n      num_neighbors_pred_by_agr=FLAGS.num_neighbors_pred_by_agr,\n      lr_decay_rate_cls=FLAGS.lr_decay_rate_cls,\n      lr_decay_steps_cls=FLAGS.lr_decay_steps_cls,\n      lr_decay_rate_agr=FLAGS.lr_decay_rate_agr,\n      lr_decay_steps_agr=FLAGS.lr_decay_steps_agr,\n      load_from_checkpoint=FLAGS.load_from_checkpoint,\n      use_graph=FLAGS.use_graph,\n      always_agree=FLAGS.always_agree,\n      add_negative_edges_agr=FLAGS.add_negative_edges_agr)\n\n  ############################################################################\n  #                            TRAIN                                         #\n  ############################################################################\n  trainer.train(data)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
research/gam/gam/models/__init__.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
research/gam/gam/models/cnn.py,71,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A convolutional neural network architecture for image classification.\n\nThis architecture is used in the TensorFlow tutorial for CIFAR10:\nhttps://www.tensorflow.org/tutorials/images/deep_cnn\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .models_base import Model\n\nimport tensorflow as tf\n\n\nclass ImageCNNAgreement(Model):\n  """"""Convolutional Neural Network for image classification.\n\n  It assumes the inputs are images of shape width x height x channels.\n  The precise architecture follows the Tensorflow CNN\n  tutorial at https://www.tensorflow.org/tutorials/images/deep_cnn.\n  Note that this CNN works both for the agreement and classification models.\n  In the agreement case, the provided inputs will be a tuple\n  (inputs_src, inputs_target), which are aggregated into one input after the\n  convolution layers, right before the fully connected network that makes the\n  final prediction.\n\n  Attributes:\n    output_dim: Integer representing the number of classes.\n    channels: Integer representing the number of channels in the input images\n      (e.g., 1 for black and white, 3 for RGB).\n    aggregation: String representing an aggregation operation, that is applied\n      on the two inputs of the agreement model, after they are encoded through\n      the convolution layers. See superclass attributes for details.\n    activation: An activation function to be applied to the outputs of each\n      fully connected layer of the aggregation network.\n    is_binary_classification: Boolean specifying if this is model for\n      binary classification. If so, it uses a different loss function and\n      returns predictions with a single dimension, batch size.\n    name: String representing the model name.\n  """"""\n\n  def __init__(self,\n               output_dim,\n               channels,\n               aggregation=None,\n               hidden_aggregation=(),\n               activation=tf.nn.leaky_relu,\n               is_binary_classification=False,\n               name=\'cnn_agr\'):\n    super(ImageCNNAgreement, self).__init__(\n        aggregation=aggregation,\n        hidden_aggregation=hidden_aggregation,\n        activation=activation)\n    self.output_dim = output_dim\n    self.channels = channels\n    self.is_binary_classification = is_binary_classification\n    self.name = name\n\n  def _construct_layers(self, inputs):\n    """"""Creates all hidden layers of the model, before the prediction layer.\n\n    Args:\n      inputs: A tensor containing the model inputs. The first dimension is the\n        batch size.\n\n    Returns:\n      A tuple containing the encoded representation of the inputs and a\n      dictionary of regularization parameters.\n    """"""\n\n    # A dictionary of parameters on top of which we add weight decay.\n    reg_params = {}\n\n    # Convolution 1.\n    with tf.variable_scope(\'conv1\') as scope:\n      kernel = tf.get_variable(\n          \'kernel\',\n          shape=[5, 5, self.channels, 64],\n          initializer=tf.truncated_normal_initializer(stddev=5e-2,\n                                                      dtype=tf.float32),\n          dtype=tf.float32)\n      conv = tf.nn.conv2d(inputs, kernel, [1, 1, 1, 1], padding=\'SAME\')\n      biases = tf.get_variable(\n          \'biases\',\n          [64],\n          initializer=tf.constant_initializer(0.0),\n          dtype=tf.float32)\n      pre_activation = tf.nn.bias_add(conv, biases)\n      conv1 = tf.nn.relu(pre_activation, name=scope.name)\n\n    # Max pooling 1.\n    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                           padding=\'SAME\', name=\'pool1\')\n    # Local Response Normalization 1.\n    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                      name=\'norm1\')\n    # Convolution 2.\n    with tf.variable_scope(\'conv2\') as scope:\n      kernel = tf.get_variable(\n          \'kernel\',\n          shape=[5, 5, 64, 64],\n          initializer=tf.truncated_normal_initializer(stddev=5e-2,\n                                                      dtype=tf.float32),\n          dtype=tf.float32)\n      conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n      biases = tf.get_variable(\n          \'biases\',\n          [64],\n          initializer=tf.constant_initializer(0.1),\n          dtype=tf.float32)\n      pre_activation = tf.nn.bias_add(conv, biases)\n      conv2 = tf.nn.relu(pre_activation, name=scope.name)\n\n    # Local Response Normalization 2.\n    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                      name=\'norm2\')\n    # Max pooling 2.\n    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                           strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool2\')\n\n    # Layer 3.\n    with tf.variable_scope(\'local3\') as scope:\n      # Move everything into depth so we can perform a single matrix multiply.\n      reshape = tf.keras.layers.Flatten()(pool2)\n      dim = reshape.get_shape()[1].value\n      weights = tf.get_variable(\n          \'weights\',\n          shape=[dim, 384],\n          initializer=tf.truncated_normal_initializer(\n              stddev=0.04, dtype=tf.float32),\n          dtype=tf.float32)\n      reg_params[weights.name] = weights\n      biases = tf.get_variable(\n          \'biases\', [384],\n          initializer=tf.constant_initializer(0.1),\n          dtype=tf.float32)\n      local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n\n    # Layer 4.\n    with tf.variable_scope(\'local4\') as scope:\n      weights = tf.get_variable(\n          \'weights\',\n          shape=[384, 192],\n          initializer=tf.truncated_normal_initializer(\n              stddev=0.04, dtype=tf.float32),\n          dtype=tf.float32)\n      reg_params[weights.name] = weights\n      biases = tf.get_variable(\n          \'biases\', [192],\n          initializer=tf.constant_initializer(0.1),\n          dtype=tf.float32)\n      local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n\n    return local4, reg_params\n\n  def get_encoding_and_params(self, inputs, **unused_kwargs):\n    """"""Creates the model hidden representations and prediction ops.\n\n    For this model, the hidden representation is the last layer\n    before the logit computation. The predictions are unnormalized logits.\n\n    Args:\n      inputs: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      **unused_kwargs: Other unused keyword arguments.\n\n    Returns:\n      encoding: A tensor containing an encoded batch of samples. The first\n        dimension corresponds to the batch size.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    # Build layers.\n    with tf.variable_scope(self.name):\n      if isinstance(inputs, (list, tuple)):\n        # If we have multiple inputs (e.g., in the case of the agreement model),\n        # split into left and right inputs, compute the hidden representation of\n        # each branch, then aggregate.\n        left = inputs[0]\n        right = inputs[1]\n        with tf.variable_scope(\'encoding\'):\n          hidden1, reg_params = self._construct_layers(left)\n        with tf.variable_scope(\'encoding\', reuse=True):\n          hidden2, _ = self._construct_layers(right)\n        hidden = self._aggregate((hidden1, hidden2))\n      else:\n        with tf.variable_scope(\'encoding\'):\n          hidden, reg_params = self._construct_layers(inputs)\n\n      # Store model variables for easy access.\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES,\n          scope=tf.get_default_graph().get_name_scope())\n      all_vars = {var.name: var for var in variables}\n\n    return hidden, all_vars, reg_params\n\n  def get_predictions_and_params(self, encoding, is_train, **kwargs):\n    """"""Creates the model prediction op.\n\n    For this model, the hidden representation is the last layer\n    before the logit computation. The predictions are unnormalized logits.\n\n    Args:\n      encoding: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      is_train: A placeholder representing a boolean value that specifies if\n        this model will be used for training or for test.\n      **kwargs: Other keyword arguments.\n\n    Returns:\n      predictions: A tensor of logits. For multiclass classification its\n        shape is (num_samples, num_classes), where the second dimension contains\n        a logit per class. For binary classification, its shape is\n        (num_samples,), where each element is the probability of class 1 for\n        that sample.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    # Build layers.\n    with tf.variable_scope(self.name + \'/prediction\'):\n      # We store all variables on which we apply weight decay in a dictionary.\n      reg_params = {}\n\n      # Create the output layer of the predictions.\n      input_size = encoding.get_shape().dims[-1].value\n      weights = tf.get_variable(\n          \'W_outputs\',\n          shape=(input_size, self.output_dim),\n          initializer=tf.truncated_normal_initializer(\n              stddev=1.0 / float(input_size), dtype=tf.float32))\n      biases = tf.get_variable(\n          \'b_outputs\',\n          initializer=tf.zeros([self.output_dim], dtype=tf.float32))\n      predictions = tf.add(\n          tf.matmul(encoding, weights), biases, name=\'predictions\')\n\n      if self.is_binary_classification:\n        predictions = predictions[:, 0]\n\n      # Store model variables for easy access.\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES,\n          scope=tf.get_default_graph().get_name_scope())\n      all_vars = {var.name: var for var in variables}\n\n    return predictions, all_vars, reg_params\n\n  def get_loss(self,\n               predictions,\n               targets,\n               name_scope=\'loss\',\n               reg_params=None,\n               **kwargs):\n    """"""Returns a loss between the provided targets and predictions.\n\n    For binary classification, this loss is sigmoid cross entropy. For\n    multi-class classification, it is softmax cross entropy.\n    A weight decay loss is also added to the parameters passed in reg_params.\n\n    Arguments:\n      predictions: A tensor of predictions. For multiclass classification its\n        shape is (num_samples, num_classes), where the second dimension contains\n        a logit per class. For binary classification, its shape is\n        (num_samples,), where each element is the probability of class 1 for\n        that sample.\n      targets: A tensor of targets of shape (num_samples,), where each row\n        contains the label index of the corresponding sample.\n      name_scope: A string containing the name scope used in TensorFlow.\n      reg_params: A dictonary of parameters, mapping from name to parameter, for\n        the variables to be included in the weight decay loss. If None, no\n        weight decay is applied.\n      **kwargs: Keyword arguments, potentially containing the weight of the\n        regularization term, passed under the name `weight_decay`. If this is\n        not provided, it defaults to 0.004.\n\n    Returns:\n      loss: The cummulated loss value.\n    """"""\n    reg_params = reg_params if reg_params is not None else {}\n    weight_decay = kwargs[\'weight_decay\'] if \'weight_decay\' in kwargs else 0.004\n\n    with tf.name_scope(name_scope):\n      # Cross entropy error.\n      if self.is_binary_classification:\n        loss = tf.reduce_sum(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                labels=targets, logits=predictions))\n      else:\n        loss = tf.losses.softmax_cross_entropy(targets, predictions)\n\n      # Weight decay loss.\n      if weight_decay is not None:\n        for var in reg_params.values():\n          loss += weight_decay * tf.nn.l2_loss(var)\n    return loss\n\n  def normalize_predictions(self, predictions):\n    """"""Converts predictions to probabilities.\n\n    Arguments:\n      predictions: A tensor of logits. For multiclass classification its shape\n        is (num_samples, num_classes), where the second dimension contains a\n        logit per class. For binary classification, its shape is (num_samples,),\n        where each element is the probability of class 1 for that sample.\n\n    Returns:\n      A tensor of the same shape as predictions, with values between [0, 1]\n    representing probabilities.\n    """"""\n    if self.is_binary_classification:\n      return tf.nn.sigmoid(predictions)\n    return tf.nn.softmax(predictions, axis=-1)\n'"
research/gam/gam/models/gcn.py,33,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Graph Convolution Networks implementation adapted from https://github.com/tkipf/gcn.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .models_base import glorot\nfrom .models_base import Model\nimport tensorflow as tf\n\n# Global unique layer ID dictionary for layer name assignment.\n_LAYER_UIDS = {}\n\n\ndef get_layer_uid(layer_name=\'\'):\n  """"""Helper function, assigns unique layer IDs.""""""\n  if layer_name not in _LAYER_UIDS:\n    _LAYER_UIDS[layer_name] = 1\n    return 1\n  else:\n    _LAYER_UIDS[layer_name] += 1\n    return _LAYER_UIDS[layer_name]\n\n\ndef sparse_dropout(x, keep_prob, noise_shape):\n  """"""Dropout for sparse tensors.""""""\n  random_tensor = keep_prob\n  random_tensor += tf.random_uniform(noise_shape)\n  dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n  pre_out = tf.sparse_retain(x, dropout_mask)\n  return tf.SparseTensor(\n      indices=pre_out.indices,\n      values=pre_out.values / keep_prob,\n      dense_shape=pre_out.dense_shape)\n\n\ndef dot(x, y, sparse=False):\n  """"""Wrapper for tf.matmul (sparse vs dense).""""""\n  if sparse:\n    res = tf.sparse_tensor_dense_matmul(x, y)\n  else:\n    res = tf.matmul(x, y)\n  return res\n\n\nclass GCN(Model):\n  """"""Graph Convolution Networks.\n\n  Attributes:\n    input_dim: Integer representing the number of input features.\n    output_dim: Integer representing the number of classes.\n    hidden: Integer representing the number of hidden units in the first layer\n      of the network.\n    dropout: Float representing the dropout probability during training.\n    aggregation: String representing an aggregation operation, that is applied\n      on the two inputs of the agreement model, after they are encoded through\n      the convolution layers. See superclass attributes for details.\n    activation: An activation function to be applied to the outputs of each\n      fully connected layer of the aggregation network.\n    is_binary_classification: Boolean specifying if this is model for binary\n      classification. If so, it uses a different loss function and returns\n      predictions with a single dimension, batch size.\n    name: String representing the model name.\n  """"""\n\n  def __init__(self,\n               input_dim,\n               output_dim,\n               hidden,\n               dropout=0.5,\n               aggregation=None,\n               hidden_aggregation=(),\n               activation=tf.nn.leaky_relu,\n               is_binary_classification=False,\n               name=\'GCN\'):\n    super(GCN, self).__init__(\n        aggregation=aggregation,\n        hidden_aggregation=hidden_aggregation,\n        activation=activation)\n\n    dropout = 0.5 if dropout is None else dropout\n\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.num_supports = 1\n    self.hidden = hidden\n    self.dropout = dropout\n    self.name = name\n    self.is_binary_classification = is_binary_classification\n\n  def get_encoding_and_params(self, inputs, is_train, support,\n                              num_features_nonzero, **unused_kwargs):\n    """"""Creates the model hidden representations and prediction ops.\n\n    For this model, the hidden representation is the last layer of the MLP,\n    before the logit computation. The predictions are unnormalized logits.\n\n    Args:\n      inputs: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      is_train: A boolean placeholder specifying if this is a training or\n        testing setting.\n      support: TODO(dattias, kvis-google): add.\n      num_features_nonzero: Number of non-zero features.\n      **unused_kwargs: Other unused keyword arguments.\n\n    Returns:\n      encoding: A tensor containing an encoded batch of samples. The first\n        dimension corresponds to the batch size.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    # Build layers.\n    with tf.variable_scope(self.name + \'/encoding\'):\n      hidden, reg_params = self._construct_encoding(inputs, is_train, support,\n                                                    num_features_nonzero)\n\n      # Store model variables for easy access.\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES,\n          scope=tf.get_default_graph().get_name_scope())\n      all_vars = {var.name: var for var in variables}\n\n    return hidden, all_vars, reg_params\n\n  def _construct_encoding(self, inputs, is_train, support,\n                          num_features_nonzero):\n    """"""Create weight variables.""""""\n    dropout = (\n        tf.constant(self.dropout, tf.float32) * tf.cast(is_train, tf.float32))\n\n    layer_1 = GraphConvolution(\n        input_dim=self.input_dim,\n        output_dim=self.hidden,\n        activation=tf.nn.relu,\n        dropout=dropout,\n        sparse_inputs=True,\n        num_features_nonzero=num_features_nonzero,\n        support=support,\n        name=\'GraphConvolution1\')\n    encoding = layer_1(inputs)\n    reg_params = layer_1.vars\n\n    return encoding, reg_params\n\n  def get_predictions_and_params(self, encoding, is_train, **kwargs):\n    """"""Creates the model prediction op.\n\n    For this model, the hidden representation is the last layer of the MLP,\n    before the logit computation. The predictions are unnormalized logits.\n\n    Args:\n      encoding: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      is_train: A placeholder representing a boolean value that specifies if\n        this model will be used for training or for test.\n      **kwargs: Other keyword arguments.\n\n    Returns:\n      predictions: A tensor of logits. For multiclass classification its\n        shape is (num_samples, num_classes), where the second dimension contains\n        a logit per class. For binary classification, its shape is\n        (num_samples,), where each element is the probability of class 1 for\n        that sample.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    reg_params = {}\n    support = kwargs[\'support\']\n    num_features_nonzero = kwargs[\'num_features_nonzero\']\n\n    # Build layers.\n    with tf.variable_scope(self.name + \'/prediction\'):\n      dropout = (\n          tf.constant(self.dropout, tf.float32) * tf.cast(is_train, tf.float32))\n\n      layer_2 = GraphConvolution(\n          input_dim=self.hidden,\n          output_dim=self.output_dim,\n          activation=lambda x: x,\n          dropout=dropout,\n          num_features_nonzero=num_features_nonzero,\n          support=support,\n          name=\'GraphConvolution2\')\n      predictions = layer_2(encoding)\n\n      if self.is_binary_classification:\n        predictions = predictions[:, 0]\n\n      # Store model variables for easy access.\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES,\n          scope=tf.get_default_graph().get_name_scope())\n      all_vars = {var.name: var for var in variables}\n\n    return predictions, all_vars, reg_params\n\n  def get_loss(self,\n               predictions,\n               targets,\n               name_scope=\'loss\',\n               reg_params=None,\n               **kwargs):\n    """"""Returns a loss between the provided targets and predictions.\n\n    For binary classification, this loss is sigmoid cross entropy. For\n    multi-class classification, it is softmax cross entropy.\n    A weight decay loss is also added to the parameters passed in reg_params.\n\n    Arguments:\n      predictions: A tensor of predictions. For multiclass classification its\n        shape is (num_samples, num_classes), where the second dimension contains\n        a logit per class. For binary classification, its shape is\n        (num_samples,), where each element is the probability of class 1 for\n        that sample.\n      targets: A tensor of targets of shape (num_samples,), where each row\n        contains the label index of the corresponding sample.\n      name_scope: A string containing the name scope used in TensorFlow.\n      reg_params: A dictonary of parameters, mapping from name to parameter, for\n        the variables to be included in the weight decay loss. If None, no\n        weight decay is applied.\n      **kwargs: Keyword arguments, potentially containing the weight of the\n        regularization term, passed under the name `weight_decay`. If this is\n        not provided, it defaults to 0.0.\n\n    Returns:\n      loss: The cummulated loss value.\n    """"""\n    reg_params = reg_params if reg_params is not None else {}\n    weight_decay = kwargs[\'weight_decay\'] if \'weight_decay\' in kwargs else None\n\n    with tf.name_scope(name_scope):\n      # Cross entropy error.\n      if self.is_binary_classification:\n        loss = tf.reduce_sum(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                labels=targets, logits=predictions))\n      else:\n        loss = tf.losses.softmax_cross_entropy(targets, predictions)\n      # Weight decay loss.\n      if weight_decay is not None:\n        for var in reg_params.values():\n          loss = loss + weight_decay * tf.nn.l2_loss(var)\n    return loss\n\n  def normalize_predictions(self, predictions):\n    """"""Converts predictions to probabilities.\n\n    Arguments:\n      predictions: A tensor of logits. For multiclass classification its shape\n        is (num_samples, num_classes), where the second dimension contains a\n        logit per class. For binary classification, its shape is (num_samples,),\n        where each element is the probability of class 1 for that sample.\n\n    Returns:\n      A tensor of the same shape as predictions, with values between [0, 1]\n    representing probabilities.\n    """"""\n    if self.is_binary_classification:\n      return tf.nn.sigmoid(predictions)\n    return tf.nn.softmax(predictions, axis=-1)\n\n\nclass GraphConvolution(object):\n  """"""Graph convolution layer.""""""\n\n  def __init__(self,\n               input_dim,\n               output_dim,\n               support,\n               num_features_nonzero,\n               dropout=0.,\n               sparse_inputs=False,\n               activation=tf.nn.relu,\n               bias=False,\n               featureless=False,\n               name=None):\n    if not name:\n      layer = self.__class__.__name__.lower()\n      name = layer + \'_\' + str(get_layer_uid(layer))\n\n    self.name = name\n    self.vars = {}\n    self.dropout = dropout\n    self.act = activation\n    self.support = support\n    self.sparse_inputs = sparse_inputs\n    self.featureless = featureless\n    self.bias = bias\n\n    # Helper variable for sparse dropout.\n    self.num_features_nonzero = num_features_nonzero\n\n    with tf.variable_scope(self.name + \'_vars\'):\n      self.vars[\'weights\'] = tf.get_variable(\n          name=\'weights\', initializer=glorot([input_dim, output_dim]))\n      if self.bias:\n        self.vars[\'bias\'] = tf.get_variable(\n            name=\'bias\', initializer=tf.zeros(shape=[output_dim]))\n\n  def __call__(self, inputs):\n    with tf.name_scope(self.name):\n      outputs = self._call(inputs)\n      return outputs\n\n  def _call(self, inputs):\n    """"""Run over inputs.""""""\n    x = inputs\n\n    # Dropout.\n    if self.sparse_inputs:\n      x = sparse_dropout(x, 1 - self.dropout, self.num_features_nonzero)\n    else:\n      x = tf.nn.dropout(x, 1 - self.dropout)\n\n    # Convolve.\n    if not self.featureless:\n      pre_sup = dot(x, self.vars[\'weights\'], sparse=self.sparse_inputs)\n    else:\n      pre_sup = self.vars[\'weights\']\n    support = dot(self.support, pre_sup, sparse=True)\n    output = support\n\n    # Bias.\n    if self.bias:\n      output += self.vars[\'bias\']\n\n    return self.act(output)\n'"
research/gam/gam/models/mlp.py,28,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Implementation of a Multilayer Perceptron for classification.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .models_base import glorot\nfrom .models_base import Model\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass MLP(Model):\n  """"""Multilayer Perceptron for binary and multi-class classification.\n\n  Attributes:\n    output_dim: Integer representing the number of classes.\n    hidden_sizes: List containing the sizes of the hidden layers.\n    activation: An activation function to apply to the output of each hidden\n      layer.\n    aggregation: String representing an aggregation operation that could be\n      applied to the inputs. Valid options: None, `add`. If None, then no\n      aggregation is performed. If `add`, the first half of the features\n      dimension is added to the second half (see the `_aggregate` function\n      for details).\n    hidden_aggregation: A tuple or list of integers representing the number of\n      hidden units in each layer of the projection network described above.\n    is_binary_classification: Boolean specifying if this is model for\n      binary classification. If so, it uses a different loss function and\n      returns predictions with a single dimension, batch size.\n    name: String representing the model name.\n  """"""\n\n  def __init__(self,\n               output_dim,\n               hidden_sizes,\n               activation=tf.nn.leaky_relu,\n               aggregation=None,\n               hidden_aggregation=(),\n               is_binary_classification=False,\n               name=\'MLP\'):\n    super(MLP, self).__init__(\n        aggregation=aggregation,\n        hidden_aggregation=hidden_aggregation,\n        activation=activation)\n    self.output_dim = output_dim\n    self.hidden_sizes = hidden_sizes\n    self.is_binary_classification = is_binary_classification\n    self.name = name\n\n  def _construct_layers(self, inputs):\n    """"""Creates all hidden layers of the model, before the prediction layer.\n\n    Args:\n      inputs: A tensor containing the model inputs. The first dimension is the\n        batch size.\n\n    Returns:\n      A tuple containing the encoded representation of the inputs and a\n      dictionary of regularization parameters.\n    """"""\n    reg_params = {}\n    # Reshape inputs in case they are not of shape (batch_size, features).\n    num_features = np.prod(inputs.shape[1:])\n    inputs = tf.reshape(inputs, [-1, num_features])\n    hidden = inputs\n    for layer_index, output_size in enumerate(self.hidden_sizes):\n      input_size = hidden.get_shape().dims[-1].value\n      weights_name = \'W_\' + str(layer_index)\n      weights = tf.get_variable(\n          name=weights_name,\n          initializer=glorot((input_size, output_size)),\n          use_resource=True)\n      reg_params[weights_name] = weights\n      biases = tf.get_variable(\n          \'b_\' + str(layer_index),\n          initializer=tf.zeros([output_size], dtype=tf.float32),\n          use_resource=True)\n      hidden = self.activation(tf.nn.xw_plus_b(hidden, weights, biases))\n    return hidden, reg_params\n\n  def get_encoding_and_params(self, inputs, **unused_kwargs):\n    """"""Creates the model hidden representations and prediction ops.\n\n    For this model, the hidden representation is the last layer of the MLP,\n    before the logit computation. The predictions are unnormalized logits.\n\n    Args:\n      inputs: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      **unused_kwargs: Other unused keyword arguments.\n\n    Returns:\n      encoding: A tensor containing an encoded batch of samples. The first\n        dimension corresponds to the batch size.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    # Build layers.\n    with tf.variable_scope(self.name):\n      if isinstance(inputs, (tuple, list)):\n        with tf.variable_scope(\'encoding\'):\n          hidden1, reg_params = self._construct_layers(inputs[0])\n        with tf.variable_scope(\'encoding\', reuse=True):\n          hidden2, _ = self._construct_layers(inputs[1])\n        hidden = self._aggregate((hidden1, hidden2))\n      else:\n        with tf.variable_scope(\'encoding\'):\n          hidden, reg_params = self._construct_layers(inputs)\n\n      # Store model variables for easy access.\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES,\n          scope=tf.get_default_graph().get_name_scope())\n      all_vars = {var.name: var for var in variables}\n\n    return hidden, all_vars, reg_params\n\n  def get_predictions_and_params(self, encoding, is_train, **kwargs):\n    """"""Creates the model prediction op.\n\n    For this model, the hidden representation is the last layer of the MLP,\n    before the logit computation. The predictions are unnormalized logits.\n\n    Args:\n      encoding: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      is_train: A placeholder representing a boolean value that specifies if\n        this model will be used for training or for test.\n      **kwargs: Other keyword arguments.\n\n    Returns:\n      predictions: A tensor of logits. For multiclass classification its\n        shape is (num_samples, num_classes), where the second dimension contains\n        a logit per class. For binary classification, its shape is\n        (num_samples,), where each element is the probability of class 1 for\n        that sample.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    reg_params = {}\n\n    # Build layers.\n    with tf.variable_scope(self.name + \'/prediction\'):\n      input_size = encoding.get_shape().dims[-1].value\n      weights = tf.get_variable(\n          \'W_outputs\',\n          initializer=glorot((input_size, self.output_dim)),\n          use_resource=True)\n      reg_params[\'W_outputs\'] = weights\n      biases = tf.get_variable(\n          \'b_outputs\',\n          initializer=tf.zeros([self.output_dim], dtype=tf.float32),\n          use_resource=True)\n      predictions = tf.nn.xw_plus_b(encoding, weights, biases,\n                                    name=\'predictions\')\n      if self.is_binary_classification:\n        predictions = predictions[:, 0]\n\n      # Store model variables for easy access.\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES,\n          scope=tf.get_default_graph().get_name_scope())\n      all_vars = {var.name: var for var in variables}\n\n    return predictions, all_vars, reg_params\n\n  def get_loss(self,\n               predictions,\n               targets,\n               name_scope=\'loss\',\n               reg_params=None,\n               **kwargs):\n    """"""Returns a loss between the provided targets and predictions.\n\n    For binary classification, this loss is sigmoid cross entropy. For\n    multi-class classification, it is softmax cross entropy.\n    A weight decay loss is also added to the parameters passed in reg_params.\n\n    Arguments:\n      predictions: A tensor of predictions. For multiclass classification its\n        shape is (num_samples, num_classes), where the second dimension contains\n        a logit per class. For binary classification, its shape is\n        (num_samples,), where each element is the probability of class 1 for\n        that sample.\n      targets: A tensor of targets of shape (num_samples,), where each row\n        contains the label index of the corresponding sample.\n      name_scope: A string containing the name scope used in TensorFlow.\n      reg_params: A dictonary of parameters, mapping from name to parameter, for\n        the variables to be included in the weight decay loss. If None, no\n        weight decay is applied.\n      **kwargs: Keyword arguments, potentially containing the weight of the\n        regularization term, passed under the name `weight_decay`. If this is\n        not provided, it defaults to 0.0.\n\n    Returns:\n      loss: The cummulated loss value.\n    """"""\n    reg_params = reg_params if reg_params is not None else {}\n    weight_decay = kwargs[\'weight_decay\'] if \'weight_decay\' in kwargs else None\n\n    with tf.name_scope(name_scope):\n      # Cross entropy error.\n      if self.is_binary_classification:\n        loss = tf.reduce_sum(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                labels=targets, logits=predictions))\n      else:\n        loss = tf.losses.softmax_cross_entropy(targets, predictions)\n      # Weight decay loss.\n      if weight_decay is not None:\n        for var in reg_params.values():\n          loss = loss + weight_decay * tf.nn.l2_loss(var)\n    return loss\n\n  def normalize_predictions(self, predictions):\n    """"""Converts predictions to probabilities.\n\n    Arguments:\n      predictions: A tensor of logits. For multiclass classification its shape\n        is (num_samples, num_classes), where the second dimension contains a\n        logit per class. For binary classification, its shape is (num_samples,),\n        where each element is the probability of class 1 for that sample.\n\n    Returns:\n      A tensor of the same shape as predictions, with values between [0, 1]\n    representing probabilities.\n    """"""\n    if self.is_binary_classification:\n      return tf.nn.sigmoid(predictions)\n    return tf.nn.softmax(predictions, axis=-1)\n'"
research/gam/gam/models/models_base.py,13,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Code used by all models use by Graph Agreement Models.""""""\nimport abc\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef glorot(shape, name=None):\n  """"""Glorot & Bengio (AISTATS 2010) initialization.""""""\n  init_range = np.sqrt(6.0 / (shape[0] + shape[1]))\n  initial = tf.random_uniform(\n      shape, minval=-init_range, maxval=init_range, dtype=tf.float32, name=name)\n  return initial\n\n\nclass Model(object):\n  """"""Superclass for models used for the classification or agreement model.\n\n  Attributes:\n    aggregation: A string representing the way to aggregate two inputs provided\n      in the function `_aggregate`. This is designed for the agreement model,\n      where the source and target inputs need to be integrated. The allowed\n      options are the following:\n        None: no aggregation is performed.\n        - `add`: the two inputs are added\n        - `dist`: squared distance between the two inputs, elementwise.\n        - `concat`: the two inputs are concatenated along axis=1.\n        - `project_add`: the two inputs are first projected to a different space\n          using a fully connected network with the hidden units provided in\n          hidden_aggregation, and followed by the activation function in\n          activation. Then the projected inputs are added, like in `add`.\n        - `project_dist`: The two inputs are projected as in `project_add`,\n          followed by a distance calculation like in `dist`.\n        - `project_concat`: The two inputs are projected as in `project_add`,\n          followed by a concatenation like in `concat`.\n    hidden_aggregation: A tuple or list of integers representing the number of\n      hidden units in each layer of the projection network described above.\n    activation: An activation function to be applied to the outputs of each\n      fully connected layer of the aggregation multilayer perceptron.\n  """"""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, aggregation=None, hidden_aggregation=(),\n               activation=lambda x: x):\n    self.aggregation = aggregation\n    self.hidden_aggregation = hidden_aggregation\n    self.activation = activation\n\n    assert aggregation in (None, \'add\', \'dist\', \'concat\', \'project_add\',\n                           \'project_dist\', \'project_concat\')\n\n  @abc.abstractmethod\n  def get_predictions_and_params(self, inputs, is_train, **kwargs):\n    """"""Creates model variables and returns the predictions and parameters.""""""\n    pass\n\n  @abc.abstractmethod\n  def get_loss(self, predictions, targets, reg_params, **kwargs):\n    """"""Returns the loss op to be optimized in order to train this model.""""""\n    pass\n\n  @abc.abstractmethod\n  def normalize_predictions(self, predictions):\n    """"""Convert predictions to probabilities.""""""\n    pass\n\n  def __call__(self, inputs, **kwargs):\n    return self.get_predictions_and_params(inputs, **kwargs)\n\n  def save(self, variables, path, session):\n    """"""Saves a model using a Tensorflow Saver.""""""\n    saver = tf.train.Saver(variables)\n    save_path = saver.save(session, path)\n    logging.info(\'Model saved in file: %s\', save_path)\n\n  def load(self, variables, path, session):\n    """"""Loads a model using a Tensorflow Saver.""""""\n    saver = tf.train.Saver(variables)\n    saver.restore(session, path)\n    logging.info(\'Model restored from file: %s\', path)\n\n  def _aggregate(self, inputs):\n    """"""Aggregates the input features.\n\n    Because this MLP can be used both by the classification and agreement\n    models, the provided inputs could be a single batch of features for the\n    classification model, or a tuple of two batches (src_features, tgt_features)\n    for the agreement model. In the latter case, the two inputs need to be\n    aggregated before they are passed through the MLP. Here we provide several\n    options for how this aggregation can be done, specified by the `aggregate`\n    class attribute. The valid options are:\n    - None: no aggregation is performed (this is used by the classification\n            model).\n    - add: The two inputs are added: src_features + tgt_features.\n    - dist: The two inputs are aggregated into their squared difference.\n    - concat: The two inputs are concatenated along the features dimension.\n    - project_add: The two inputs are projected to another space, and then\n                   added.\n    - project_dist: The two inputs are projected to another space, and then\n                    we compute squared element-wise distance.\n    - project_concat: The two inputs are projected to another space, and then\n                      concatenated.\n\n    Arguments:\n      inputs: A batch of features of shape (batch_size, num_features) or\n        a tuple of two such batches of features.\n    Returns:\n      A batch of aggregated features of shape (batch_size, new_num_features),\n      where the feature dimension size may have changed.\n    """"""\n    if self.aggregation is None:\n      return inputs\n    # If it requires aggregation, we assume the inputs are passes as a tuple.\n    left = inputs[0]\n    right = inputs[1]\n\n    # In case the aggregation option requires projection, we first do this.\n    if self.aggregation in (\'project_add\', \'project_dist\', \'project_concat\'):\n      left = self._project(left)\n      right = self._project(right, reuse=True)\n\n    if self.aggregation.endswith(\'add\'):\n      return left + right\n    if self.aggregation.endswith(\'dist\'):\n      return tf.square(left - right)\n    elif self.aggregation.endswith(\'concat\'):\n      return tf.concat((left, right), axis=1)\n    else:\n      raise NotImplementedError()\n\n  def _project(self, inputs, reuse=tf.compat.v1.AUTO_REUSE):\n    """"""Projects the provided inputs using a multilayer perceptron.\n\n    Arguments:\n      inputs: A batch of features whose first dimension is the batch size.\n      reuse: A boolean specifying whether to reuse the same projection weights\n        or create new ones.\n\n    Returns:\n      Projected inputs, having the same batch size as the first dimension.\n    """"""\n    with tf.variable_scope(\'aggregation\', reuse=reuse):\n      # Reshape inputs in case they have more than 2 dimensions.\n      num_features = inputs.get_shape().dims[1:]\n      num_features = np.prod([f.value for f in num_features])\n      if len(inputs.shape) > 2:\n        inputs = tf.reshape(inputs, [-1, num_features])\n      # Create the fully connected layers.\n      hidden = inputs\n      for layer_index, num_units in enumerate(self.hidden_aggregation):\n        input_size = hidden.get_shape().dims[-1].value\n        weights = tf.get_variable(\n            \'W_\' + str(layer_index),\n            initializer=glorot((input_size, num_units)),\n            use_resource=True)\n        bias = tf.get_variable(\n            \'b_\' + str(layer_index),\n            shape=(num_units,),\n            initializer=tf.zeros_initializer(),\n            use_resource=True)\n        hidden = self.activation(tf.nn.xw_plus_b(hidden, weights, bias))\n      return hidden\n'"
research/gam/gam/models/wide_resnet.py,53,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A Resnet implementation from `Realistic Evaluation of Deep SSL Algorithms`.\n\nFollowing this paper from Google Brain:\nhttps://papers.nips.cc/paper/7585-realistic-evaluation-of-deep-semi-supervised-learning-algorithms\nand this Github repository:\nhttps://github.com/brain-research/realistic-ssl-evaluation\n""""""\nfrom .models_base import Model\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef fast_flip(images, is_training):\n  """"""Flips the input images when training.""""""\n\n  def func(inp):\n    batch_size = tf.shape(inp)[0]\n    flips = tf.to_float(\n        tf.random_uniform([batch_size, 1, 1, 1], 0, 2, tf.int32))\n    flipped_inp = tf.reverse(inp, [2])\n    return flips * flipped_inp + (1 - flips) * images\n\n  return tf.cond(is_training, lambda: func(images), lambda: images)\n\n\nclass WideResnet(Model):\n  """"""Resnet implementation from `Realistic Evaluation of Deep SSL Algorithms`.\n\n  Attributes:\n    num_classes: Integer representing the number of classes.\n    lrelu_leakiness: A float representing the weight of the Leaky Relu\n      parameter.\n    horizontal_flip: A boolean specifying whether we do random horizontal flips\n      of the training data, for data augmentation.\n    random_translation: A boolean specifying whether we do random translations\n      of the training data, for data augmentation.\n    gaussian_noise: Boolean specifying whether to add Gaussian noise to the\n      inputs.\n    width: Integer representing the size of the convolutional filter.\n    num_residual_units: Integer representing the number of residual units.\n    name: String representing the model name.\n    aggregation: String representing an aggregation operation that could be\n      applied to the inputs. See superclass attributes for details.\n    hidden_aggregation: A tuple or list of integers representing the number of\n      units in each layer of aggregation multilayer percepron. After the inputs\n      are passed through the encoding layers, before aggregation they are passed\n      through a fully connected network with these numbers of hidden units in\n      each layer.\n    activation: An activation function to be applied to the outputs of each\n      fully connected layer in the aggregation network.\n    is_binary_classification: Boolean specifying if this is model for binary\n      classification. If so, it uses a different loss function and returns\n      predictions with a single dimension, batch size.\n  """"""\n\n  def __init__(self,\n               num_classes,\n               lrelu_leakiness,\n               horizontal_flip,\n               random_translation,\n               gaussian_noise,\n               width,\n               num_residual_units,\n               name=""wide_resnet"",\n               ema_factor=None,\n               is_binary_classification=False,\n               aggregation=None,\n               activation=tf.nn.leaky_relu,\n               hidden_aggregation=()):\n    super(WideResnet, self).__init__(\n        aggregation=aggregation,\n        hidden_aggregation=hidden_aggregation,\n        activation=activation)\n    self.name = name\n    self.num_classes = num_classes\n    self.is_binary_classification = is_binary_classification\n    self.lrelu_leakiness = lrelu_leakiness\n    self.horizontal_flip = horizontal_flip\n    self.random_translation = random_translation\n    self.gaussian_noise = gaussian_noise\n    self.width = width\n    self.num_residual_units = num_residual_units\n    self.ema_factor = ema_factor\n\n  def get_encoding_and_params(self,\n                              inputs,\n                              is_train,\n                              update_batch_stats=True,\n                              **unused_kwargs):\n    """"""Creates the model hidden representations and prediction ops.\n\n    For this model, the hidden representation is the last layer\n    before the logit computation. The predictions are unnormalized logits.\n\n    Args:\n      inputs: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      is_train: A placeholder representing a boolean value that specifies if\n        this model will be used for training or for test.\n      update_batch_stats: Boolean specifying whether to update the batch norm\n        statistics.\n      **unused_kwargs: Other unused keyword arguments.\n\n    Returns:\n      encoding: A tensor containing an encoded batch of samples. The first\n        dimension corresponds to the batch size.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    # Build layers.\n    with tf.variable_scope(self.name):\n      if isinstance(inputs, (list, tuple)):\n        with tf.variable_scope(""encoding""):\n          left = self._get_encoding(inputs[0], is_train, update_batch_stats)\n        with tf.variable_scope(""encoding"", reuse=True):\n          right = self._get_encoding(inputs[1], is_train, update_batch_stats)\n        encoding = self._aggregate((left, right))\n      else:\n        with tf.variable_scope(""encoding""):\n          encoding = self._get_encoding(inputs, is_train, update_batch_stats)\n\n      # Store model variables for easy access.\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES,\n          scope=tf.get_default_graph().get_name_scope())\n      all_vars = {var.name: var for var in variables}\n\n      reg_params = {}\n\n    return encoding, all_vars, reg_params\n\n  def get_predictions_and_params(self, encoding, is_train, **kwargs):\n    """"""Creates the model prediction op.\n\n    For this model, the hidden representation is the last layer of the MLP,\n    before the logit computation. The predictions are unnormalized logits.\n\n    Args:\n      encoding: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      is_train: A placeholder representing a boolean value that specifies if\n        this model will be used for training or for test.\n      **kwargs: Other keyword arguments.\n\n    Returns:\n      logits: A tensor of logits. For multiclass classification its\n        shape is (num_samples, num_classes), where the second dimension contains\n        a logit per class. For binary classification, its shape is\n        (num_samples,), where each element is the probability of class 1 for\n        that sample.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    # Logits layer.\n    with tf.variable_scope(self.name + ""/prediction""):\n      w_init = tf.glorot_normal_initializer()\n      logits = tf.layers.dense(\n          encoding, self.num_classes, kernel_initializer=w_init)\n      if self.is_binary_classification:\n        logits = logits[:, 0]\n\n      # Store model variables for easy access.\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES,\n          scope=tf.get_default_graph().get_name_scope())\n      all_vars = {var.name: var for var in variables}\n\n    # No regularization parameters.\n    reg_params = {}\n\n    return logits, all_vars, reg_params\n\n  def _get_encoding(self, inputs, is_train, update_batch_stats, **kwargs):\n    """"""Creates the model hidden representations and prediction ops.\n\n    For this model, the hidden representation is the last layer before the logit\n    computation. The predictions are unnormalized logits.\n\n    Args:\n      inputs: A tensor containing the model inputs. The first dimension is the\n        batch size.\n      is_train: A placeholder representing a boolean value that specifies if\n        this model will be used for training or for test.\n      update_batch_stats: Boolean specifying whether to update the batch norm\n        statistics.\n      **kwargs: Other keyword arguments.\n\n    Returns:\n      encoding: A tensor containing an encoded batch of samples. The first\n        dimension corresponds to the batch size.\n      all_vars: A dictionary mapping from variable name to TensorFlow op\n        containing all variables used in this model.\n      reg_params: A dictionary mapping from a variable name to a Tensor of\n        parameters which will be used for regularization.\n    """"""\n    # Helper functions\n    def _conv(name, x, filter_size, in_filters, out_filters, strides):\n      """"""Convolution.""""""\n      with tf.variable_scope(name):\n        n = filter_size * filter_size * out_filters\n        kernel = tf.get_variable(\n            ""DW"",\n            [filter_size, filter_size, in_filters, out_filters],\n            tf.float32,\n            initializer=tf.random_normal_initializer(stddev=np.sqrt(2.0 / n)),\n        )\n        return tf.nn.conv2d(x, kernel, strides, padding=""SAME"")\n\n    def _relu(x, leakiness=0.0):\n      """"""Relu, with optional leaky support.""""""\n      return tf.where(tf.less(x, 0.0), leakiness * x, x, name=""leaky_relu"")\n\n    def _residual(x,\n                  in_filter,\n                  out_filter,\n                  stride,\n                  activate_before_residual=False):\n      """"""Residual unit with 2 sub layers.""""""\n      if activate_before_residual:\n        with tf.variable_scope(""shared_activation""):\n          x = tf.layers.batch_normalization(\n              x, axis=1, scale=True, training=is_train)\n          x = _relu(x, self.lrelu_leakiness)\n          orig_x = x\n      else:\n        with tf.variable_scope(""residual_only_activation""):\n          orig_x = x\n          x = tf.layers.batch_normalization(\n              x, axis=1, scale=True, training=is_train)\n          x = _relu(x, self.lrelu_leakiness)\n\n      with tf.variable_scope(""sub1""):\n        x = _conv(""conv1"", x, 3, in_filter, out_filter, stride)\n\n      with tf.variable_scope(""sub2""):\n        x = tf.layers.batch_normalization(\n            x, axis=1, scale=True, training=is_train)\n        x = _relu(x, self.lrelu_leakiness)\n        x = _conv(""conv2"", x, 3, out_filter, out_filter, [1, 1, 1, 1])\n\n      with tf.variable_scope(""sub_add""):\n        if in_filter != out_filter:\n          orig_x = _conv(""conv1x1"", orig_x, 1, in_filter, out_filter, stride)\n        x += orig_x\n      return x\n\n    x = inputs\n    tf.summary.image(""images_in_net"", x)\n    if self.horizontal_flip:\n      x = fast_flip(x, is_training=is_train)\n    if self.random_translation:\n      raise NotImplementedError(""Random translations are not implemented yet."")\n    if self.gaussian_noise:\n      x = tf.cond(is_train, lambda: x + tf.random_normal(tf.shape(x)) * 0.15,\n                  lambda: x)\n    x = _conv(""init_conv"", x, 3, 3, 16, [1, 1, 1, 1])\n\n    activate_before_residual = [True, False, False]\n    res_func = _residual\n    filters = [16, 16 * self.width, 32 * self.width, 64 * self.width]\n\n    with tf.variable_scope(""unit_1_0""):\n      x = res_func(x, filters[0], filters[1], [1, 1, 1, 1],\n                   activate_before_residual[0])\n    for i in range(1, self.num_residual_units):\n      with tf.variable_scope(""unit_1_%d"" % i):\n        x = res_func(x, filters[1], filters[1], [1, 1, 1, 1], False)\n\n    with tf.variable_scope(""unit_2_0""):\n      x = res_func(x, filters[1], filters[2], [1, 2, 2, 1],\n                   activate_before_residual[1])\n    for i in range(1, self.num_residual_units):\n      with tf.variable_scope(""unit_2_%d"" % i):\n        x = res_func(x, filters[2], filters[2], [1, 1, 1, 1], False)\n\n    with tf.variable_scope(""unit_3_0""):\n      x = res_func(x, filters[2], filters[3], [1, 2, 2, 1],\n                   activate_before_residual[2])\n    for i in range(1, self.num_residual_units):\n      with tf.variable_scope(""unit_3_%d"" % i):\n        x = res_func(x, filters[3], filters[3], [1, 1, 1, 1], False)\n\n    with tf.variable_scope(""unit_last""):\n      x = tf.layers.batch_normalization(\n          x, axis=1, scale=True, training=is_train)\n      x = _relu(x, self.lrelu_leakiness)\n      # Global average pooling.\n      x = tf.reduce_mean(x, [1, 2])\n\n    return x\n\n  def get_loss(self,\n               predictions,\n               targets,\n               name_scope=""loss"",\n               reg_params=None,\n               **kwargs):\n    weight_decay = kwargs[""weight_decay""] if ""weight_decay"" in kwargs else 0.0\n\n    with tf.name_scope(name_scope):\n      if self.is_binary_classification:\n        loss = tf.reduce_sum(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                labels=targets, logits=predictions))\n      else:\n        # Cross entropy error\n        loss = tf.reduce_mean(\n            tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(\n                    labels=targets, logits=predictions),\n                axis=-1))\n      # Weight decay loss\n      if reg_params:\n        for var in reg_params.values():\n          loss += weight_decay * tf.nn.l2_loss(var)\n    return loss\n\n  def normalize_predictions(self, predictions):\n    if self.is_binary_classification:\n      return tf.nn.sigmoid(predictions)\n    return tf.nn.softmax(predictions, axis=-1)\n'"
research/gam/gam/trainer/__init__.py,0,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
research/gam/gam/trainer/adversarial_dense.py,18,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions used for Virtual Adversarial Training on dense feature matrices.""""""\nimport tensorflow as tf\n\nepsilon = 5\nnum_power_iterations = 1\nxi = 1e-6\nscale_r = False\n\n\ndef kl_divergence_with_logit(q_logit, p_logit):\n  """"""Computes KL-divergence between to sets of logits.""""""\n  q = tf.nn.softmax(q_logit)\n  qlogq = -tf.nn.softmax_cross_entropy_with_logits_v2(labels=q, logits=q_logit)\n  qlogp = -tf.nn.softmax_cross_entropy_with_logits_v2(labels=q, logits=p_logit)\n  return qlogq - qlogp\n\n\ndef get_normalized_vector(d):\n  """"""Normalizes the provided input vector.""""""\n  d /= (1e-12 + tf.reduce_max(tf.abs(d), keep_dims=True))\n  d /= tf.sqrt(1e-6 + tf.reduce_sum(tf.pow(d, 2.0), keep_dims=True))\n  return d\n\n\ndef get_normalizing_constant(d):\n  """"""Returns the normalizing constant to scale the VAT perturbation vector.""""""\n  c = 1e-12 + tf.reduce_max(tf.abs(d), keep_dims=True)\n  c *= tf.sqrt(1e-6 + tf.reduce_sum(tf.pow(d, 2.0), keep_dims=True))\n  return c\n\n\ndef get_loss_vat(inputs, predictions, is_train, model, predictions_var_scope):\n  """"""Computes the virtual adversarial loss for the provided inputs.\n\n  Args:\n    inputs: A batch of input features, where the batch is the first dimension.\n    predictions: The logits predicted by a model on the provided inputs.\n    is_train: A boolean placeholder specifying if this is a training or testing\n      setting.\n    model: The model that generated the logits.\n    predictions_var_scope: Variable scope for obtaining the predictions.\n\n  Returns:\n    A float value representing the virtual adversarial loss.\n  """"""\n  r_vadv = generate_virtual_adversarial_perturbation(\n      inputs, predictions, model, predictions_var_scope, is_train=is_train)\n  predictions = tf.stop_gradient(predictions)\n  logit_p = predictions\n  new_inputs = tf.add(inputs, r_vadv)\n  with tf.variable_scope(\n      predictions_var_scope, auxiliary_name_scope=False, reuse=True):\n    encoding_m, _, _ = model.get_encoding_and_params(\n        inputs=new_inputs, is_train=is_train, update_batch_stats=False)\n    logit_m, _, _ = model.get_predictions_and_params(\n        encoding=encoding_m, is_train=is_train)\n  loss = kl_divergence_with_logit(logit_p, logit_m)\n  return tf.reduce_mean(loss)\n\n\ndef generate_virtual_adversarial_perturbation(inputs,\n                                              logits,\n                                              model,\n                                              predictions_var_scope,\n                                              is_train=True):\n  """"""Generates an adversarial perturbation for virtual adversarial training.\n\n  Args:\n    inputs: A batch of input features, where the batch is the first dimension.\n    logits: The logits predicted by a model on the provided inputs.\n    model: The model that generated the logits.\n    predictions_var_scope: Variable scope for obtaining the predictions.\n    is_train: A boolean placeholder specifying if this is a training or testing\n      setting.\n\n  Returns:\n    A Tensor of the same shape as the inputs containing the adversarial\n    perturbation for these inputs.\n  """"""\n  d = tf.random_normal(shape=tf.shape(inputs))\n\n  for _ in range(num_power_iterations):\n    d = xi * get_normalized_vector(d)\n    logit_p = logits\n    with tf.variable_scope(\n        predictions_var_scope, auxiliary_name_scope=False, reuse=True):\n      encoding_m, _, _ = model.get_encoding_and_params(\n          inputs=d + inputs, is_train=is_train, update_batch_stats=False)\n      logit_m, _, _ = model.get_predictions_and_params(\n          encoding=encoding_m, is_train=is_train)\n    dist = kl_divergence_with_logit(logit_p, logit_m)\n    grad = tf.gradients(dist, [d], aggregation_method=2)[0]\n    d = tf.stop_gradient(grad)\n\n  r_vadv = get_normalized_vector(d)\n  if scale_r:\n    r_vadv *= get_normalizing_constant(inputs)\n  r_vadv *= epsilon\n  return r_vadv\n\n\ndef entropy_y_x(logits):\n  """"""Entropy term to add to VAT with entropy minimization.\n\n  Args:\n    logits: A Tensor containing the predicted logits for a batch of samples.\n\n  Returns:\n    The entropy minimization loss.\n  """"""\n  p = tf.nn.softmax(logits)\n  return tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits_v2(labels=p, logits=logits))\n'"
research/gam/gam/trainer/adversarial_sparse.py,24,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions used for Virtual Adversarial Training on sparse feature matrices.""""""\n\nfrom .adversarial_dense import get_normalized_vector\nfrom .adversarial_dense import get_normalizing_constant\nimport tensorflow as tf\n\nepsilon = 5\nnum_power_iterations = 1\nxi = 1e-6\nscale_r = False\n\n\ndef kl_divergence_with_logit(q_logit, p_logit, mask):\n  """"""Computes KL-divergence between to sets of logits for the masked samples.""""""\n  q = tf.nn.softmax(q_logit)\n  num_non_zero = tf.reduce_sum(mask)\n  qlogq = -tf.nn.softmax_cross_entropy_with_logits_v2(labels=q, logits=q_logit)\n  qlogq = qlogq * mask / num_non_zero\n  qlogp = -tf.nn.softmax_cross_entropy_with_logits_v2(labels=q, logits=p_logit)\n  qlogp = qlogp * mask / num_non_zero\n  return qlogq - qlogp\n\n\ndef get_loss_vat(inputs, predictions, mask, is_train, model, placeholders,\n                 predictions_var_scope):\n  """"""Computes the virtual adversarial loss for the provided inputs.\n\n  Args:\n    inputs: A batch of input features, where the batch is the first dimension.\n    predictions: The logits predicted by a model on the provided inputs.\n    mask: A tensor of booleans specifying which samples to apply the virtual\n      adversarial loss to.\n    is_train: A boolean placeholder specifying if this is a training or testing\n      setting.\n    model: The model that generated the logits.\n    placeholders: Placeholders for model encodings.\n    predictions_var_scope: Variable scope for obtaining the predictions.\n\n  Returns:\n    A float value representing the virtual adversarial loss.\n  """"""\n  mask = tf.cast(mask, dtype=tf.float32)\n  r_vadv = generate_virtual_adversarial_perturbation(\n      inputs,\n      predictions,\n      model,\n      placeholders,\n      mask,\n      predictions_var_scope,\n      is_train=is_train)\n  predictions = tf.stop_gradient(predictions)\n  logit_p = predictions\n  new_inputs = tf.sparse_add(inputs, r_vadv)\n  with tf.variable_scope(\n      predictions_var_scope, auxiliary_name_scope=False, reuse=True):\n    encoding_m, _, _ = model.get_encoding_and_params(\n        inputs=new_inputs,\n        is_train=is_train,\n        update_batch_stats=False,\n        **placeholders)\n    logit_m, _, _ = model.get_predictions_and_params(\n        encoding=encoding_m, is_train=is_train, **placeholders)\n  num_non_zero = tf.reduce_sum(mask)\n  loss = kl_divergence_with_logit(logit_p, logit_m, mask)\n  return tf.reduce_sum(loss) / num_non_zero\n\n\ndef generate_virtual_adversarial_perturbation(inputs,\n                                              logits,\n                                              model,\n                                              placeholders,\n                                              mask,\n                                              predictions_var_scope,\n                                              is_train=True):\n  """"""Generates an adversarial perturbation for virtual adversarial training.\n\n  Args:\n    inputs: A batch of input features, where the batch is the first dimension.\n    logits: The logits predicted by a model on the provided inputs.\n    model: The model that generated the logits.\n    placeholders: A dictionary mapping string names to Tensorflow placeholders\n      that are passed to the models when generating the predictions.\n    mask: A tensor of booleans specifying which samples to apply the virtual\n      adversarial loss to.\n    predictions_var_scope: Variable scope for obtaining the predictions.\n    is_train: A boolean placeholder specifying if this is a training or testing\n      setting.\n\n  Returns:\n    A Tensor of the same shape as the inputs containing the adversarial\n    perturbation for these inputs.\n  """"""\n  # Generate random perturbations.\n  d = tf.random_normal(shape=tf.shape(inputs))\n  # Only apply perturbations on the masked samples.\n  d = tf.multiply(d, mask[:, None])\n\n  for _ in range(num_power_iterations):\n    d = xi * get_normalized_vector(d)\n    logit_p = logits\n    new_inputs = tf.add(tf.sparse_tensor_to_dense(inputs), d)\n    new_inputs = tf.sparse.from_dense(new_inputs)\n    with tf.variable_scope(\n        predictions_var_scope, auxiliary_name_scope=False, reuse=True):\n      encoding_m, _, _ = model.get_encoding_and_params(\n          inputs=new_inputs,\n          is_train=is_train,\n          update_batch_stats=False,\n          **placeholders)\n      logit_m, _, _ = model.get_predictions_and_params(\n          encoding=encoding_m, is_train=is_train, **placeholders)\n    dist = kl_divergence_with_logit(logit_p, logit_m, mask)\n    grad = tf.gradients(dist, [d], aggregation_method=2)[0]\n    d = tf.stop_gradient(grad)\n\n  r_vadv = get_normalized_vector(d)\n  if scale_r:\n    r_vadv *= get_normalizing_constant(inputs.values)\n  r_vadv *= epsilon\n\n  return tf.sparse.from_dense(r_vadv)\n\n\ndef logsoftmax(x):\n  """"""Softmax where the inputs are logits and the outputs remain logits.""""""\n  xdev = x - tf.reduce_max(x, 1, keep_dims=True)\n  lsm = xdev - tf.log(tf.reduce_sum(tf.exp(xdev), 1, keep_dims=True))\n  return lsm\n\n\ndef entropy_y_x(logits, mask):\n  """"""Entropy term to add to VAT with entropy minimization.\n\n  Args:\n    logits: A Tensor containing the predicted logits for a batch of samples.\n    mask: A boolean Tensor specifying which samples to use in the calculation of\n      the entropy.\n\n  Returns:\n    The entropy minimization loss.\n  """"""\n  mask = tf.cast(mask, dtype=tf.float32)\n  p = tf.nn.softmax(logits)\n  ent = tf.reduce_sum(p * logsoftmax(logits), 1)\n  ent = tf.reduce_sum(tf.multiply(ent, mask)) / tf.reduce_sum(mask)\n  return -ent\n'"
research/gam/gam/trainer/trainer_agreement.py,44,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Trainer for the agreement model in Graph Agreement Models without a graph.\n\nThis class contains functionality that allows for training an agreement model\nto be used as part of Graph Agreement Models.\nThis implementation does not use a provided graph, but samples random pairs\nof samples.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport logging\nimport os\n\nfrom ..data.preprocessing import split_train_val\nimport numpy as np\nimport tensorflow as tf\nfrom .trainer_base import batch_iterator\nfrom .trainer_base import Trainer\n\n\ndef accuracy_binary(normalized_preds, labels, threshold=0.5):\n  """"""Accuracy with probabilities for binary classification.""""""\n  predictions = tf.greater_equal(normalized_preds, threshold)\n  labels = tf.cast(labels, tf.bool)\n  accuracy = tf.cast(tf.equal(predictions, labels), tf.float32)\n  return tf.reduce_mean(accuracy)\n\n\nclass TrainerAgreement(Trainer):\n  """"""Trainer for the agreement model.\n\n  Attributes:\n    model: A Model used to decide if two samples should have the same label.\n    is_train: A placeholder for a boolean value specyfing if the model is used\n      for train or evaluation.\n    data: A CotrainDataset object.\n    optimizer: Optimizer used for training the agreement model.\n    min_num_iter: An integer representing the minimum number of iterations to\n      train the agreement model.\n    max_num_iter: An integer representing the maximum number of iterations to\n      train the agreement model.\n    num_iter_after_best_val: An integer representing the number of extra\n      iterations to perform after improving the validation set accuracy.\n    max_num_iter_cotrain: An integer representing the maximum number of cotrain\n      iterations to train for.\n    num_warm_up_iter: The agreement model will return 0 for the first\n      `num_warm_up_iter` co-training iterations.\n    batch_size: Batch size for used when training and evaluating the agreement\n      model.\n    gradient_clip: A float number representing the maximum gradient norm allowed\n      if we do gradient clipping. If None, no gradient clipping is performed.\n    enable_summaries: Boolean specifying whether to enable variable summaries.\n    summary_step: Integer representing the summary step size.\n    summary_dir: String representing the path to a directory where to save the\n      variable summaries.\n    logging_step: Integer representing the number of iterations after which we\n      log the loss of the model.\n    eval_step: Integer representing the number of iterations after which we\n      evaluate the model.\n    abs_loss_chg_tol: A float representing the absolute tolerance for checking\n      if the training loss has converged. If the difference between the current\n      loss and previous loss is less than `abs_loss_chg_tol`, we count this\n      iteration towards convergence (see `loss_chg_iter_below_tol`).\n    rel_loss_chg_tol: A float representing the relative tolerance for checking\n      if the training loss has converged. If the ratio between the current loss\n      and previous loss is less than `rel_loss_chg_tol`, we count this iteration\n      towards convergence (see `loss_chg_iter_below_tol`).\n    loss_chg_iter_below_tol: An integer representing the number of consecutive\n      iterations that pass the convergence criteria before stopping training.\n    warm_start: Whether the agreement model parameters are initialized at their\n      best value in the previous cotrain iteration. If False, they are\n      reinitialized.\n    checkpoints_dir: Path to the folder where to store TensorFlow model\n      checkpoints.\n    weight_decay: Weight decay value.\n    weight_decay_schedule: Schedule for the weight decay variable.\n    num_pairs_eval_random: Integer representing the number of pairs to use for\n      evaluation. These pairs are randomly drawn from all datasets, including\n      validation and test. This is only used for monitoring the performance, but\n      is not involved in training the agreement model.\n    agree_by_default: Boolean specifying whether to return agreement by default\n      or disagreement by default when the agreement model is not warmed up.\n    percent_val: Ratio of samples to use for validation.\n    max_num_samples_val: Maximum number of samples to include in the validation\n      set.\n    seed: Integer representing the seed for the random number generator.\n    use_graph: Boolean specifying whether to use the graph edges, or any pair of\n      samples.\n    add_negative_edges: Boolean specifying whether to add fake negative edges\n      when training the agreement model, in order to keep the classes balanced.\n      Only applies when `use_graph` is True.\n  """"""\n\n  def __init__(self,\n               model,\n               data,\n               optimizer,\n               lr_initial,\n               min_num_iter,\n               max_num_iter,\n               num_iter_after_best_val,\n               max_num_iter_cotrain,\n               num_warm_up_iter,\n               batch_size,\n               gradient_clip=None,\n               enable_summaries=False,\n               summary_step=1,\n               summary_dir=None,\n               logging_step=1,\n               eval_step=1,\n               abs_loss_chg_tol=1e-10,\n               rel_loss_chg_tol=1e-7,\n               loss_chg_iter_below_tol=20,\n               warm_start=False,\n               checkpoints_dir=None,\n               weight_decay=None,\n               weight_decay_schedule=None,\n               num_pairs_eval_random=1000,\n               agree_by_default=False,\n               percent_val=0.1,\n               max_num_samples_val=10000,\n               seed=None,\n               lr_decay_steps=None,\n               lr_decay_rate=None,\n               use_graph=False,\n               add_negative_edges=False):\n    super(TrainerAgreement, self).__init__(\n        model=model,\n        abs_loss_chg_tol=abs_loss_chg_tol,\n        rel_loss_chg_tol=rel_loss_chg_tol,\n        loss_chg_iter_below_tol=loss_chg_iter_below_tol)\n    self.data = data\n    self.optimizer = optimizer\n    self.min_num_iter = min_num_iter\n    self.max_num_iter = max_num_iter\n    self.num_iter_after_best_val = num_iter_after_best_val\n    self.max_num_iter_cotrain = max_num_iter_cotrain\n    self.num_warm_up_iter = num_warm_up_iter\n    self.batch_size = batch_size\n    self.gradient_clip = gradient_clip\n    self.enable_summaries = enable_summaries\n    self.summary_step = summary_step\n    self.summary_dir = summary_dir\n    self.checkpoints_dir = checkpoints_dir\n    self.logging_step = logging_step\n    self.eval_step = eval_step\n    self.num_iter_trained = 0\n    self.warm_start = warm_start\n    self.checkpoint_path = (\n        os.path.join(checkpoints_dir, \'agree_best.ckpt\')\n        if checkpoints_dir is not None else None)\n    self.weight_decay = weight_decay\n    self.weight_decay_schedule = weight_decay_schedule\n    self.num_pairs_eval_random = num_pairs_eval_random\n    self.agree_by_default = agree_by_default\n    self.ratio_val = percent_val\n    self.max_num_samples_val = max_num_samples_val\n    self.original_var_scope = None\n    self.lr_initial = lr_initial\n    self.lr_decay_steps = lr_decay_steps\n    self.lr_decay_rate = lr_decay_rate\n    self.use_graph = use_graph\n    self.add_negative_edges = add_negative_edges\n\n    # Build TensorFlow graph.\n    logging.info(\'Building TensorFlow agreement graph...\')\n    # The agreement model computes the label agreement between two samples.\n    # We will refer to these samples as the src and tgt sample, using\n    # graph terminology.\n\n    # Create placeholders, and assign to these variables by default.\n    features_shape = [None] + list(data.features_shape)\n    src_features = tf.placeholder(\n        tf.float32, shape=features_shape, name=\'src_features\')\n    tgt_features = tf.placeholder(\n        tf.float32, shape=features_shape, name=\'tgt_features\')\n    # Create a placeholder for the agreement labels.\n    labels = tf.placeholder(tf.float32, shape=(None,), name=\'labels\')\n    # Create a placeholder specifying if this is train time.\n    is_train = tf.placeholder_with_default(False, shape=[], name=\'is_train\')\n\n    # Create variables and predictions.\n    predictions, normalized_predictions, variables, reg_params = (\n        self.create_agreement_prediction(src_features, tgt_features, is_train))\n\n    # Create a variable for weight decay that may be updated later.\n    weight_decay_var, weight_decay_update = self._create_weight_decay_var(\n        weight_decay, weight_decay_schedule)\n\n    # Create counter for the total number of agreement train iterations.\n    iter_agr_total, iter_agr_total_update = self._create_counter()\n\n    # Create loss.\n    loss_op = self.model.get_loss(\n        predictions=predictions,\n        targets=labels,\n        reg_params=reg_params,\n        weight_decay=weight_decay_var)\n\n    # Create accuracy.\n    accuracy = accuracy_binary(normalized_predictions, labels)\n\n    # Create optimizer.\n    self.global_step = tf.train.get_or_create_global_step()\n    if self.lr_decay_steps is not None and self.lr_decay_rate is not None:\n      self.lr = tf.train.exponential_decay(\n          self.lr_initial,\n          self.global_step,\n          self.lr_decay_steps,\n          self.lr_decay_rate,\n          staircase=True)\n      self.optimizer = optimizer(self.lr)\n    else:\n      self.optimizer = optimizer(lr_initial)\n\n    # Create train op.\n    grads_and_vars = self.optimizer.compute_gradients(\n        loss_op,\n        tf.trainable_variables(scope=tf.get_default_graph().get_name_scope()))\n    # Clip gradients.\n    if self.gradient_clip:\n      variab = [elem[1] for elem in grads_and_vars]\n      gradients = [elem[0] for elem in grads_and_vars]\n      gradients, _ = tf.clip_by_global_norm(gradients, self.gradient_clip)\n      grads_and_vars = tuple(zip(gradients, variab))\n    with tf.control_dependencies(\n        tf.get_collection(\n            tf.GraphKeys.UPDATE_OPS,\n            scope=tf.get_default_graph().get_name_scope())):\n      train_op = self.optimizer.apply_gradients(\n          grads_and_vars, global_step=self.global_step)\n\n    # Create Tensorboard summaries.\n    if self.enable_summaries:\n      summaries = [tf.summary.scalar(\'loss_agreement_inner\', loss_op)]\n      self.summary_op = tf.summary.merge(summaries)\n\n    # Create a saver for the model trainable variables.\n    self.trainable_vars = [v for _, v in grads_and_vars]\n\n    # Put together the subset of variables to save and restore from the best\n    # validation accuracy as we train the agreement model in one cotrain round.\n    vars_to_save = self.trainable_vars + []\n    if isinstance(weight_decay_var, tf.Variable):\n      vars_to_save.append(weight_decay_var)\n    self.saver = tf.train.Saver(vars_to_save)\n\n    # Put together all variables that need to be saved in case the process is\n    # interrupted and needs to be restarted.\n    self.vars_to_save = [iter_agr_total]\n    if isinstance(weight_decay_var, tf.Variable):\n      self.vars_to_save.append(weight_decay_var)\n    if self.warm_start:\n      self.vars_to_save += self.trainable_vars\n\n    # More variables to be initialized after the session is created.\n    self.is_initialized = False\n\n    self.rng = np.random.RandomState(seed)\n    self.src_features = src_features\n    self.tgt_features = tgt_features\n    self.labels = labels\n    self.predictions = predictions\n    self.normalized_predictions = normalized_predictions\n    self.variables = variables\n    self.reg_params = reg_params\n    self.weight_decay_var = weight_decay_var\n    self.weight_decay_update = weight_decay_update\n    self.iter_agr_total = iter_agr_total\n    self.iter_agr_total_update = iter_agr_total_update\n    self.accuracy = accuracy\n    self.train_op = train_op\n    self.loss_op = loss_op\n    self.batch_size_actual = tf.shape(self.predictions)[0]\n    self.reset_optimizer = tf.variables_initializer(self.optimizer.variables())\n    self.is_train = is_train\n\n  def create_agreement_prediction(self, src_features, tgt_features, is_train,\n                                  **unused_kwargs):\n    """"""Creates the agreement prediction TensorFlow subgraph.\n\n    This function is intended to be used both from inside TrainerAgreement for\n    training an agreement model, but also from the TrainerClassification class\n    when creating the agreement loss term.\n\n    Arguments:\n      src_features: A Tensor or Placeholder of shape (batch_size, num_features)\n        containing the features of the source sample of an edge.\n      tgt_features: A Tensor or Placeholder of shape (batch_size, num_features)\n        containing the features of the target sample of an edge.\n      is_train: A boolean Placeholder specifying if this a train or test regime.\n      unused_kwargs: Other unused keyword arguments, which we allow in order to\n        create a common interface with TrainerPerfectAgreement.\n\n    Returns:\n      predictions: A Tensor of shape (batch_size,) containing the agreement\n        prediction logits.\n      normalized_predictions: A Tensor of shape (batch_size,) with values in\n        [0, 1], containing the agreement prediction probabilities.\n      variables: A dictionary of trainable variables mapping from a string name\'\n        to a TensorFlow variable.\n      reg_params: A dictionary of variables that are used in the regularization\n        weight decay term. It maps from a string name to a TensorFlow variable.\n    """"""\n    # The following lines are a trick that allows to reuse the same agreement\n    # computation TensorFlow graph both from TrainerAgreement and from outside\n    # it (e.g. from TrainerClassification). In order to reuse the graph, we\n    # need to force the same variable scope, no matter where this function is\n    # called from, and to enable variable reuse after the first time it is\n    # called.\n    reuse = True\n    if self.original_var_scope is None:\n      self.original_var_scope = tf.get_variable_scope()\n      reuse = tf.AUTO_REUSE\n    with tf.variable_scope(\n        self.original_var_scope, auxiliary_name_scope=False, reuse=reuse):\n      # Create variables and predictions.\n      # Can replace the encoding step once there is a shared\n      # encoding between the classification and agreement model.\n      encoding, variables, reg_params = self.model.get_encoding_and_params(\n          inputs=(src_features, tgt_features), is_train=is_train)\n      predictions, variables_pred, reg_params_pred = (\n          self.model.get_predictions_and_params(\n              encoding=encoding, is_train=is_train))\n      variables.update(variables_pred)\n      reg_params.update(reg_params_pred)\n      normalized_predictions = self.model.normalize_predictions(predictions)\n      return predictions, normalized_predictions, variables, reg_params\n\n  def _create_weight_decay_var(self, weight_decay_initial,\n                               weight_decay_schedule):\n    """"""Creates a weight decay variable that can be updated using a schedule.""""""\n    weight_decay_var = None\n    weight_decay_update = None\n    if weight_decay_schedule is None:\n      if weight_decay_initial is None:\n        weight_decay_var = None\n      else:\n        weight_decay_var = tf.constant(\n            weight_decay_initial, dtype=tf.float32, name=\'weight_decay\')\n    elif weight_decay_schedule == \'linear\':\n      weight_decay_var = tf.get_variable(\n          name=\'weight_decay\',\n          initializer=tf.constant(\n              weight_decay_initial, name=\'weight_decay_initial\'),\n          use_resource=True,\n          trainable=False)\n      update_rate = weight_decay_initial / float(self.max_num_iter_cotrain)\n      weight_decay_update = weight_decay_var.assign(weight_decay_var -\n                                                    update_rate)\n    else:\n      return NotImplementedError(\n          \'Schedule %s is not implemented for the weight decay variable.\' %\n          str(weight_decay_schedule))\n    return weight_decay_var, weight_decay_update\n\n  def _create_counter(self):\n    """"""Creates a cummulative iteration counter for all agreement steps.""""""\n    iter_agr_total = tf.get_variable(\n        name=\'iter_agr_total\',\n        initializer=tf.constant(0, name=\'iter_agr_total\'),\n        use_resource=True,\n        trainable=False)\n    iter_agr_total_update = iter_agr_total.assign_add(1)\n    return iter_agr_total, iter_agr_total_update\n\n  def _construct_feed_dict(self, data_iterator, is_train):\n    """"""Construct feed dictionary containing features and labels.""""""\n    try:\n      neighbors, agreement_labels = next(data_iterator)\n      src_features = self.data.get_features(neighbors[:, 0])\n      tgt_features = self.data.get_features(neighbors[:, 1])\n      feed_dict = {\n          self.src_features: src_features,\n          self.tgt_features: tgt_features,\n          self.labels: agreement_labels,\n          self.is_train: is_train\n      }\n      return feed_dict\n    except StopIteration:\n      # If the iterator has finished, return None.\n      return None\n\n  def _eval_random_pairs(self, data, session):\n    """"""Evaluate on random pairs of nodes, and estimate accuracy.\n\n    We do this to get an estimate of how well the agreement model is doing\n    with respect to the true labels. This is for monitoring only, and is not\n    used when training.\n\n    Arguments:\n      data: A CotrainDataset object.\n      session: A TensorFlow session.\n\n    Returns:\n      acc: Total accuracy on random pairs of samples.\n    """"""\n    # Select at random num_pairs_eval_random pairs of nodes.\n    src_indices = self.rng.random_integers(0, data.num_samples - 1,\n                                           (self.num_pairs_eval_random,))\n    tgt_indices = self.rng.random_integers(0, data.num_samples - 1,\n                                           (self.num_pairs_eval_random,))\n    src_features = data.get_features(src_indices)\n    tgt_features = data.get_features(tgt_indices)\n    src_labels = data.get_original_labels(src_indices)\n    tgt_labels = data.get_original_labels(tgt_indices)\n    agreement_labels = src_labels == tgt_labels\n    feed_dict = {\n        self.src_features: src_features,\n        self.tgt_features: tgt_features,\n        self.labels: agreement_labels.astype(np.float32)\n    }\n    # Evaluate agreement.\n    acc = session.run(self.accuracy, feed_dict=feed_dict)\n    return acc\n\n  def _eval_train(self, session, feed_dict):\n    """"""Computes the accuracy of the predictions for the provided batch.\n\n    This calculates the accuracy for both class 1 (agreement) and class 0\n    (disagreement).\n\n    Arguments:\n      session: A TensorFlow session.\n      feed_dict: A train feed dictionary.\n\n    Returns:\n      train_acc: The computed train accuracy.\n      acc_0: Accuracy for class 0.\n      acc_1: Accuracy for class 1.\n    """"""\n    train_acc, pred, targ = session.run(\n        (self.accuracy, self.normalized_predictions, self.labels),\n        feed_dict=feed_dict)\n\n    # Assume the threshold is at 0.5, and binarize the predictions.\n    binary_pred = pred > 0.5\n    targ = targ.astype(np.int32)\n    acc_per_sample = binary_pred == targ\n    acc_1 = acc_per_sample[targ == 1]\n    if acc_1.shape[0] > 0:\n      acc_1 = sum(acc_1) / np.float32(len(acc_1))\n    else:\n      acc_1 = -1\n    acc_0 = acc_per_sample[targ == 0]\n    if acc_0.shape[0] > 0:\n      acc_0 = sum(acc_0) / np.float32(len(acc_0))\n    else:\n      acc_0 = -1\n    return train_acc, acc_0, acc_1\n\n  def _eval_validation(self, data_iterator_val, num_samples_val, session):\n    """"""Evaluate the current model on validation data.\n\n    Args:\n      data_iterator_val: An iterator that generates batches of edges and\n        agreement labels.\n      num_samples_val: Number of sample pairs to use for validation. Since the\n        number of combinations of samples in `labeled_nodes_val` can be very\n        high, for validation we use only `num_samples_val` pairs.\n      session: A TensorFlow session.\n\n    Returns:\n      Total accuracy on random pairs of samples.\n    """"""\n    feed_dict_val = self._construct_feed_dict(data_iterator_val, is_train=False)\n    cummulative_val_acc = 0.0\n    samples_seen = 0\n    while feed_dict_val is not None and samples_seen < num_samples_val:\n      val_acc, batch_size_actual = session.run(\n          (self.accuracy, self.batch_size_actual), feed_dict=feed_dict_val)\n      cummulative_val_acc += val_acc * batch_size_actual\n      samples_seen += batch_size_actual\n      feed_dict_val = self._construct_feed_dict(\n          data_iterator_val, is_train=False)\n    cummulative_val_acc /= samples_seen\n    return cummulative_val_acc\n\n  def _select_val_set(self,\n                      labeled_samples,\n                      num_samples,\n                      data,\n                      ratio_pos_to_neg=None):\n    """"""Select a validation set for the agreement model.\n\n    This is chosen by randomly selecting num_samples pairs of labeled nodes.\n    For nodes, the agreement labels are 1.0 if the two nodes in a pair have the\n    same label, or 0.0 otherwise.\n\n    Arguments:\n      labeled_samples: An array of integers representing the indices of the\n        labeled nodes.\n      num_samples: An integer representing the desired number of validation\n        samples.\n      data: A dataset object used to provided the labels of the labeled samples.\n      ratio_pos_to_neg: A float repesenting the ratio of positive to negative\n        samples.\n\n    Returns:\n      neighbors: An array of shape (num_samples, 2), where each row represents\n        a pair of indices chosed from labeled_samples.\n      agreement: An array of floats whose elements are either 1.0 or 0.0,\n        representing the agreement value, as explained above.\n    """"""\n    neighbors = np.empty(shape=(num_samples, 2), dtype=np.int32)\n    agreement = np.empty(shape=(num_samples,), dtype=np.float32)\n    num_added = 0\n    while num_added < num_samples:\n      pair = self.rng.choice(labeled_samples, 2)\n      pair_agrees = data.get_labels(pair[0]) == data.get_labels(pair[1])\n      if ratio_pos_to_neg:\n        # Keep positives and negatives balanced by rejection sampling.\n        if ratio_pos_to_neg < 1 and not pair_agrees:\n          random_number = self.rng.rand(1)[0]\n          if random_number > ratio_pos_to_neg:\n            continue\n        elif ratio_pos_to_neg > 1 and pair_agrees:\n          random_number = self.rng.rand(1)[0]\n          if random_number > 1.0 / ratio_pos_to_neg:\n            continue\n      neighbors[num_added][0] = pair[0]\n      neighbors[num_added][1] = pair[1]\n      agreement[num_added] = pair_agrees\n      num_added += 1\n    return neighbors, agreement\n\n  def _compute_ratio_pos_neg(self, labels):\n    """"""Compute the agreement positive to negative sample ratio.\n\n    Arguments:\n      labels: An array containing labels for the labeled samples. Note that\n        these are the labels for the classification task, not for the agreement\n        prediction task, so they are in range [0, num_classes - 1].\n\n    Returns:\n      A float representing the ratio of positive / negative agreement labels.\n    """"""\n    # Compute how many of each label we have.\n    label_counts = collections.Counter(labels)\n    label_counts = np.asarray([count for count in label_counts.values()])\n    # Convert the counts to ratios.\n    label_counts = label_counts / np.sum(label_counts).astype(np.float32)\n    # Use the ratios to compute the probability that a randomly sampled pair\n    # of samples will have the same label.\n    ratio = np.sum([r * r for r in label_counts])\n    return ratio\n\n  def train(self, data, session=None, **kwargs):\n    """"""Train an agreement model.""""""\n\n    summary_writer = kwargs[\'summary_writer\']\n    logging.info(\'Training agreement model...\')\n\n    if not self.is_initialized:\n      self.is_initialized = True\n    else:\n      if self.weight_decay_update is not None:\n        session.run(self.weight_decay_update)\n        logging.info(\'New weight decay value:  %f\',\n                     session.run(self.weight_decay_var))\n\n    # Construct data iterator.\n    if self.use_graph:\n      edges_train, agreement_train, edges_val, agreement_val = \\\n        self._get_neighbors(data)\n      num_samples_train = agreement_train.shape[0]\n      num_samples_val = agreement_val.shape[0]\n    else:\n      labeled_samples = data.get_indices_train()\n      num_labeled_samples = len(labeled_samples)\n      num_samples_train = num_labeled_samples * num_labeled_samples\n      num_samples_val = min(\n          int(num_samples_train * self.ratio_val), self.max_num_samples_val)\n\n    if num_samples_train == 0:\n      logging.info(\'No samples to train agreement. Skipping...\')\n      return None\n\n    if not self.warm_start:\n      # Re-initialize variables.\n      initializers = [v.initializer for v in self.trainable_vars]\n      initializers.append(self.global_step.initializer)\n      session.run(initializers)\n      # Reset the optimizer state (e.g., momentum).\n      session.run(self.reset_optimizer)\n\n    logging.info(\n        \'Training agreement with %d samples and validation on %d samples.\',\n        num_samples_train, num_samples_val)\n\n    # Create an iterator over training data pairs.\n    if self.use_graph:\n      # If we use the graph, then the training data consists of graph edges\n      # and the agreement (1.0 or 0.0) between them.\n      data_iterator_train = self._get_train_edge_iterator(\n          edges_train,\n          agreement_train,\n          self.batch_size,\n          data,\n          add_negatives=self.add_negative_edges)\n    else:\n      # If we don\'t use the graph, then the training data consists of pairs of\n      # labeled sampels, and the agreement (1.0 or 0.0) between them.\n\n      # Compute ratio of positives to negative samples.\n      labeled_samples_labels = data.get_labels(labeled_samples)\n      ratio_pos_to_neg = self._compute_ratio_pos_neg(labeled_samples_labels)\n\n      # Split data into train and validation.\n      labeled_samples_train, labeled_nodes_val = self._select_val_samples(\n          labeled_samples, self.ratio_val)\n\n      # Create an iterator over training data pairs.\n      data_iterator_train = self._pair_iterator(\n          labeled_samples_train, data, ratio_pos_neg=ratio_pos_to_neg)\n\n    # Start training.\n    best_val_acc = -1\n    checkpoint_saved = False\n    step = 0\n    iter_below_tol = 0\n    min_num_iter = self.min_num_iter\n    has_converged = step >= self.max_num_iter\n    if not has_converged:\n      self.num_iter_trained += 1\n    prev_loss_val = np.inf\n    while not has_converged:\n      feed_dict = self._construct_feed_dict(data_iterator_train, is_train=True)\n\n      if self.enable_summaries and step % self.summary_step == 0:\n        loss_val, summary, iter_total, _ = session.run(\n            [self.loss_op, self.summary_op, self.iter_agr_total, self.train_op],\n            feed_dict=feed_dict)\n        summary_writer.add_summary(summary, iter_total)\n        summary_writer.flush()\n      else:\n        loss_val, _ = session.run((self.loss_op, self.train_op),\n                                  feed_dict=feed_dict)\n\n      # Log the loss, if necessary.\n      if step % self.logging_step == 0:\n        logging.info(\'Agreement step %6d | Loss: %10.4f\', step, loss_val)\n\n      # Run validation, if necessary.\n      if step % self.eval_step == 0:\n        if num_samples_val == 0:\n          logging.info(\'Skipping validation. No validation samples available.\')\n          break\n\n        # Evaluate on the selected validation data.\n        if self.use_graph:\n          data_iterator_val = batch_iterator(\n              edges_val,\n              agreement_val,\n              batch_size=self.batch_size,\n              shuffle=False,\n              allow_smaller_batch=True,\n              repeat=False)\n        else:\n          data_iterator_val = self._pair_iterator(\n              labeled_nodes_val, data, ratio_pos_neg=ratio_pos_to_neg)\n        val_acc = self._eval_validation(data_iterator_val, num_samples_val,\n                                        session)\n\n        # Evaluate over a random choice of sample pairs, either labeled or not.\n        acc_random = self._eval_random_pairs(data, session)\n\n        # Evaluate the accuracy on the latest train batch. We track this to make\n        # sure the agreement model is able to fit the training data, but can be\n        # eliminated if efficiency is an issue.\n        acc_train, acc_0_train, acc_1_train = self._eval_train(\n            session, feed_dict)\n\n        if self.enable_summaries:\n          summary = tf.Summary()\n          summary.value.add(\n              tag=\'AgreementModel/train_acc\', simple_value=acc_train)\n          summary.value.add(tag=\'AgreementModel/val_acc\', simple_value=val_acc)\n          if acc_random is not None:\n            summary.value.add(\n                tag=\'AgreementModel/random_acc\', simple_value=acc_random)\n          iter_total = session.run(self.iter_agr_total)\n          summary_writer.add_summary(summary, iter_total)\n          summary_writer.flush()\n        if step % self.logging_step == 0 or val_acc > best_val_acc:\n          logging.info(\n              \'Agreement step %6d | Loss: %10.4f | val_acc: %.4f |\'\n              \'random_acc: %.4f | acc_train: %.4f | acc_train_cls_0: %.4f | \'\n              \'acc_train_cls_1: %.4f\', step, loss_val, val_acc, acc_random,\n              acc_train, acc_0_train, acc_1_train)\n        if val_acc > best_val_acc:\n          best_val_acc = val_acc\n          if self.checkpoint_path:\n            self.saver.save(\n                session, self.checkpoint_path, write_meta_graph=False)\n            checkpoint_saved = True\n          # If we reached 100% accuracy, stop.\n          if best_val_acc >= 1.00:\n            logging.info(\'Reached 100% accuracy. Stopping...\')\n            break\n          # Go for at least num_iter_after_best_val more iterations.\n          min_num_iter = max(self.min_num_iter,\n                             step + self.num_iter_after_best_val)\n          logging.info(\n              \'Achieved best validation. \'\n              \'Extending to at least %d iterations...\', min_num_iter)\n\n      step += 1\n      has_converged, iter_below_tol = self.check_convergence(\n          prev_loss_val,\n          loss_val,\n          step,\n          self.max_num_iter,\n          iter_below_tol,\n          min_num_iter=min_num_iter)\n      session.run(self.iter_agr_total_update)\n      prev_loss_val = loss_val\n\n    # Return to the best model.\n    if checkpoint_saved:\n      logging.info(\'Restoring best model...\')\n      self.saver.restore(session, self.checkpoint_path)\n\n    return best_val_acc\n\n  def predict(self, session, src_features, tgt_features, **unused_kwargs):\n    """"""Predict agreement for the provided pairs of samples.\n\n    Note that here we don\'t need to use the src_indices and tgt_indices, but\n    we keep them as inputs to this function because we want to have a common\n    interface with the TrainerPerfectAgreement class.\n\n    Arguments:\n      session: A TensorFlow session where to run the model.\n      src_features: An array of shape (num_samples, num_features) containing the\n        features of the first element of the pair.\n      tgt_features: An array of shape (num_samples, num_features) containing the\n        features of the second element of the pair.\n\n    Returns:\n      An array containing the predicted agreement value for each pair of\n      provided samples.\n    """"""\n    if self.num_iter_trained >= self.num_warm_up_iter:\n      feed_dict = {\n          self.src_features: src_features,\n          self.tgt_features: tgt_features,\n      }\n      predictions = session.run(\n          self.normalized_predictions, feed_dict=feed_dict)\n      return predictions\n    if self.agree_by_default:\n      # Predict always agreement.\n      return np.ones(shape=(len(src_features),), dtype=np.float32)\n    # Predict always disagreement.\n    return np.zeros(shape=(len(src_features),), dtype=np.float32)\n\n  def predict_label_by_agreement(self, session, indices, num_neighbors=100):\n    """"""Predict class labels using agreement with other labeled samples.\n\n    Uses the agreement model to compute the agreement of a test sample with a\n    subset of the labeled samples. Then it calculates the label distribution\n    as a weighted average of the labeled samples, using the predicted agreement\n    scores as weights.\n\n    Arguments:\n      session: A TensorFlow seession.\n      indices: A list of integers representing the indices of the test samples\n        to label.\n      num_neighbors: An integer representing the number of labeled samples to\n        compare each test sample with. The higher this number, the more accurate\n        the predictions, but also the more expensive.\n\n    Returns:\n      acc: The accuracy of this agreement based classifier on the provided\n        sample indices.\n    """"""\n    # Limit the number of labeled samples to compare with, for efficiency\n    # reasons. At the moment we pick a random subset of labeled samples, but\n    # perhaps there better ways (e.g. the closest samples in embedding space).\n    train_indices = self.data.get_indices_train()\n    num_train = train_indices.shape[0]\n    if num_train > num_neighbors:\n      selected = self.rng.choice(num_train, num_neighbors, replace=False)\n      train_indices = train_indices[selected]\n    num_labeled = train_indices.shape[0]\n    train_labels = self.data.get_labels(train_indices)\n    train_labels_1hot = np.zeros((num_labeled, self.data.num_classes))\n    train_labels_1hot[np.arange(num_labeled), train_labels] = 1\n    # For each sample for which we want to make predictions, we compute the\n    # agreement with all selected labeled samples.\n    agreement = np.zeros((num_labeled, 1))\n    acc = 0.0\n    for index_u in indices:\n      # Pair the unlabeled sample with multiple labeled samples, and predict the\n      # agreement in batches.\n      features_u = self.data.get_features(index_u)\n      features_u_batch = features_u[None].repeat(self.batch_size, axis=0)\n      index_u_batch = np.repeat(index_u, self.batch_size)\n      idx_start = 0\n      while idx_start < num_labeled:\n        # Select a batch of labeled samples.\n        idx_end = idx_start + self.batch_size\n        if idx_end > num_labeled:\n          idx_end = num_labeled\n          features_u_repeated = features_u[None].repeat(\n              idx_end - idx_start, axis=0)\n          index_u_repeated = np.repeat(index_u, idx_end - idx_start)\n        else:\n          features_u_repeated = features_u_batch\n          index_u_repeated = index_u_batch\n        batch_indices_l = train_indices[idx_start:idx_end]\n        features_l = self.data.get_features(batch_indices_l)\n        batch_agreement = self.predict(\n            session=session,\n            src_features=features_l,\n            tgt_features=features_u_repeated,\n            src_indices=batch_indices_l,\n            tgt_indices=index_u_repeated)\n        agreement[idx_start:idx_end, 0] = batch_agreement\n        idx_start = idx_end\n      # Cummulate the agreement weights per label.\n      vote_per_label = np.sum(train_labels_1hot * agreement, axis=0)\n      is_correct = (\n          np.argmax(vote_per_label) == self.data.get_original_labels(index_u))\n      acc += is_correct\n    if indices:\n      acc /= len(indices)\n    logging.info(\'Majority vote accuracy: %.2f.\', acc)\n    return acc\n\n  def _pair_iterator(self, labeled_nodes, data, ratio_pos_neg=None):\n    """"""An iterator over pairs of samples for training the agreement model.\n\n    Provides batches of node pairs, including their features and the agreement\n    label (i.e. whether their labels agree).\n\n    Arguments:\n      labeled_nodes:  An array of integers representing the indices of the\n        labeled samples.\n      data: A Dataset object used to provided the labels of the labeled samples.\n      ratio_pos_neg: A float representing the ratio of positive to negative\n        samples in the training set. If this is provided, the train iterator\n        will do rejection sampling based on this ratio to keep the training data\n        balanced. If None, we sample uniformly.\n\n    Yields:\n      neighbors_batch: An array of shape (batch_size, 2), where each row\n        represents a pair of sample indices used for training. It will not\n        include pairs of samples that are in the provided neighbors_val.\n      agreement_batch: An array of shape (batch_size,) with binary values,\n        where each row represents whether the labels of the corresponding\n        neighbor pair agree (1.0) or not (0.0).\n    """"""\n    neighbors_batch = np.empty(shape=(self.batch_size, 2), dtype=np.int32)\n    agreement_batch = np.empty(shape=(self.batch_size,), dtype=np.float32)\n    while True:\n      num_added = 0\n      while num_added < self.batch_size:\n        pair = self.rng.choice(labeled_nodes, 2)\n        agreement = data.get_labels(pair[0]) == data.get_labels(pair[1])\n        if ratio_pos_neg is not None:\n          # Keep positives and negatives balanced.\n          if ratio_pos_neg < 1 and not agreement:\n            random_number = self.rng.rand(1)[0]\n            if random_number > ratio_pos_neg:\n              continue\n          elif ratio_pos_neg > 1 and agreement:\n            random_number = self.rng.rand(1)[0]\n            if random_number > 1.0 / ratio_pos_neg:\n              continue\n        neighbors_batch[num_added][0] = pair[0]\n        neighbors_batch[num_added][1] = pair[1]\n        agreement_batch[num_added] = agreement\n        num_added += 1\n      yield neighbors_batch, agreement_batch\n\n  def _select_val_samples(self, labeled_samples, ratio_val):\n    """"""Split the labeled samples into a train and a validation set.\n\n    The agreement model is trained using pairs of labeled samples from the train\n    set, and is evaluated on pairs of labeled samples from the validation set.\n\n    Arguments:\n      labeled_samples:\n      ratio_val: A number between (0, 1) representing the ratio of all labeled\n        samples to be set aside for validation.\n\n    Returns:\n      labeled_samples_train: An array containig a subset of the provided\n        labeled_samples which will be used for training.\n      labeled_samples_val: An array containig a subset of the provided\n        labeled_samples which will be used for validation. The train and\n        validation indices are non-overlapping.\n    """"""\n    num_labeled_samples = labeled_samples.shape[0]\n    num_labeled_samples_val = int(num_labeled_samples * ratio_val)\n    self.rng.shuffle(labeled_samples)\n    labeled_samples_val = labeled_samples[:num_labeled_samples_val]\n    labeled_samples_train = labeled_samples[num_labeled_samples_val:]\n    return labeled_samples_train, labeled_samples_val\n\n  def _get_neighbors(self, data):\n    """"""Collects edges between labeled nodes, used to train the agreement model.\n\n    Args:\n      data: A SSLDataset object.\n\n    Returns:\n      A tuple containing (edges_train, agreement_train, edges_val,\n      agreement_val) containing the edges used for training, the agreement\n      between the train labels, the edges used for validation, and the\n      agreement between the validation labels.\n    """"""\n    edges = data.get_edges(src_labeled=True, tgt_labeled=True)\n\n    if not edges:\n      empty_edges = np.zeros(shape=(0, 2), dtype=np.int32)\n      empty_agreement = np.zeros(shape=(0,), dtype=np.float32)\n      return empty_edges, empty_agreement, empty_edges, empty_agreement\n\n    edges = np.stack([(e.src, e.tgt) for e in edges])\n    agreement = np.equal(\n        data.get_labels(edges[:, 0]), data.get_labels(edges[:, 1]))\n\n    # Select validation set for agreement.\n    train_ind, val_ind = split_train_val(\n        np.arange(agreement.shape[0]),\n        self.ratio_val,\n        self.rng,\n        max_num_val=self.max_num_samples_val)\n\n    return (edges[train_ind], agreement[train_ind], edges[val_ind],\n            agreement[val_ind])\n\n  def _get_train_edge_iterator(self,\n                               edges,\n                               agreement,\n                               batch_size,\n                               data,\n                               add_negatives=False):\n    if add_negatives:\n      # Separate the positive from the negative edges.\n      edges_pos = edges[agreement]\n      edges_neg = edges[np.logical_not(agreement)]\n\n      num_pos = edges_pos.shape[0]\n      num_neg = edges_neg.shape[0]\n      num_neg_needed = max(num_pos - num_neg, 0)\n\n      # A batch will have an equal number of positive and negative edges.\n      half_batch = min(batch_size // 2, num_pos)\n\n      # Add extra negative edges to match the number of positive.\n      # For now fill in with zeros.\n      if num_neg_needed > 0:\n        edges_neg_with_extras = np.zeros_like(edges_pos)\n        edges_neg_with_extras[:num_neg] = edges_neg\n      else:\n        edges_neg_with_extras = edges_neg\n\n      batch_agreement = np.zeros((2 * half_batch,))\n      batch_agreement[:half_batch] = 1.0\n\n      labeled_nodes_indices = data.get_indices_train()\n\n      keep_going = num_pos > 0\n      while keep_going:\n        if num_neg_needed > 0:\n          # Select some random negative edges to fill in the remaining\n          # num_neg_needed.\n          for i in range(num_neg_needed):\n            while True:\n              pair = np.random.choice(labeled_nodes_indices, size=2)\n              if data.get_labels(pair[0]) != data.get_labels(pair[1]):\n                break\n            edges_neg_with_extras[num_neg + i] = pair\n\n        # Create batches with half_batch positives and half_batch negatives,\n        np.random.shuffle(edges_pos)\n        np.random.shuffle(edges_neg_with_extras)\n        for start_index in range(0, num_pos, half_batch):\n          end_index = start_index + half_batch\n          if end_index > num_pos:\n            break\n          batch_edges = np.concatenate(\n              (edges_pos[start_index:end_index],\n               edges_neg_with_extras[start_index:end_index]))\n          yield batch_edges, batch_agreement\n    else:\n      iterator = batch_iterator(\n          edges,\n          targets=agreement.astype(float),\n          batch_size=batch_size,\n          shuffle=True,\n          allow_smaller_batch=False,\n          repeat=True)\n      for data in iterator:\n        yield data\n\n\nclass TrainerPerfectAgreement(object):\n  """"""Trainer for an agreement model that always predicts the correct value.""""""\n\n  def __init__(self, data):\n    self.data = data\n    self.model = None\n    self.vars_to_save = []\n\n    # Save the true labels in a TensorFlow variable, which is used in the\n    # create_agreement_prediction function.\n    with tf.variable_scope(\'perfect_agreement\'):\n      indices = np.arange(data.num_samples)\n      self.labels = tf.get_variable(\n          \'labels_original\', initializer=self.data.get_original_labels(indices))\n      self.original_var_scope = tf.get_variable_scope()\n\n  def train(self, unused_data, unused_session=None, **unused_kwargs):\n    logging.info(\'Perfect agreement, no need to train...\')\n\n  def predict(self, unused_session, unused_src_features, unused_tgt_features,\n              src_indices, tgt_indices):\n    """"""Predict agreement for the provided pairs of samples.\n\n    The predictions are perfect according to the original dataset ground truth\n    labels.\n    The function contains many unused arguments, in order to conform with the\n    interface of the TrainerAgreement class.\n\n    Arguments:\n      unused_session: A TensorFlow session where to run the model.\n      unused_src_features: An array of shape (num_samples, num_features)\n        containing the features of the first element of the pair.\n      unused_tgt_features: An array of shape (num_samples, num_features)\n        containing the features of the second element of the pair.\n      src_indices: An array of integers containing the index of each sample in\n        self.data of the samples in src_features.\n      tgt_indices: An array of integers containing the index of each sample in\n        self.data of the samples in tgt_features.\n\n    Returns:\n      An array containing the predicted agreement value for each pair of\n      provided samples.\n    """"""\n    agreement = [\n        self.data.get_original_labels(s) == self.data.get_original_labels(t)\n        for s, t in zip(src_indices, tgt_indices)\n    ]\n    return np.asarray(agreement, dtype=np.float32)\n\n  def create_agreement_prediction(self, src_indices, tgt_indices,\n                                  **unused_kwargs):\n    """"""Creates the agreement prediction TensorFlow subgraph.\n\n    This function is the equivalent of `create_agreement_prediction` in\n    TrainerAgreement, but here we use the oracle labels to make the agreement\n    prediction.\n\n    Arguments:\n      src_indices: A Tensor or Placeholder of shape (batch_size,) containing the\n        indices of the samples that are the sources of the edges.\n      tgt_indices: A Tensor or Placeholder of shape (batch_size,) containing the\n        indices of the samples that are the targets of the edges.\n      unused_kwargs: Other unused keyword arguments, which we allow in order to\n        create a common interface with TrainerAgreement.\n\n    Returns:\n      predictions: None, because this model doesn\'t do logits computations, but\n        we still return something in order to keep the same function outputs as\n        TrainerAgreement.\n      normalized_predictions: A Tensor of shape (batch_size,) with values in\n        {0, 1}, containing the agreement prediction probabilities.\n      variables: An empty dictionary of trainable variables, because this model\n        does not have any trainable variables.\n      reg_params: An empty dictionary of variables that are used in the\n        regularization weight decay term, because this model doesn\'t have\n        regularization variables.\n    """"""\n    with tf.variable_scope(\n        self.original_var_scope, auxiliary_name_scope=False, reuse=True):\n      src_labels = tf.gather(self.labels, src_indices)\n      tgt_labels = tf.gather(self.labels, tgt_indices)\n      agreement = tf.equal(src_labels, tgt_labels)\n    return None, tf.cast(agreement, tf.float32), {}, {}\n\n  def predict_label_by_agreement(self,\n                                 indices,\n                                 num_neighbors=100,\n                                 **unused_kwargs):\n    """"""Predict class labels using agreement with other labeled samples.\n\n    Uses the agreement model to compute the agreement of a test sample with a\n    subset of the labeled samples. Then it calculates the label distribution\n    as a weighted average of the labeled samples, using the predicted agreement\n    scores as weights.\n\n    Arguments:\n      indices: A list of integers representing the indices of the test samples\n        to label.\n      num_neighbors: An integer representing the number of labeled samples to\n        compare each test sample with. The higher this number, the more accurate\n        the predictions, but also the more expensive.\n      **unused_kwargs: Other keyword arguments that may be provided just to have\n        a simiar interface as TrainerAgreement, when calling\n        predict_label_by_agreement from the classification model.\n\n    Returns:\n      acc: The accuracy of this agreement based classifier on the provided\n        sample indices.\n    """"""\n    # Limit the number of labeled samples to compare with, for efficiency\n    # reasons. At the moment we pick a random subset of labeled samples, but\n    # perhaps there better ways (e.g. the closest samples in embedding space).\n    train_indices = np.asarray(list(self.data.get_indices_train()))\n    if len(train_indices) > num_neighbors:\n      np.random.shuffle(train_indices)\n      train_indices = train_indices[:num_neighbors]\n    num_labeled = train_indices.shape[0]\n    train_labels = self.data.get_labels(train_indices)\n    train_labels_1hot = np.zeros((num_labeled, self.data.num_classes))\n    train_labels_1hot[np.arange(num_labeled), train_labels] = 1\n    train_labels_original = self.data.get_original_labels(train_indices)\n    # For each sample for which we want to make predictions, we compute the\n    # agreement with all selected labeled samples.\n    acc = 0.0\n    for index_u in indices:\n      label_u = self.data.get_original_labels(index_u)\n      agreement = train_labels_original == label_u\n      # Cummulate the agreement weights per label.\n      vote_per_label = np.sum(train_labels_1hot[agreement], axis=0)\n      is_correct = (\n          np.argmax(vote_per_label) == self.data.get_original_labels(index_u))\n      acc += is_correct\n    if indices:\n      acc /= len(indices)\n    logging.info(\'Majority vote accuracy: %.2f.\', acc)\n    return acc\n\n\nclass TrainerAgreementAlwaysAgree(object):\n  """"""Trainer for an agreement model that always predicts that samples agree.\n\n  The goal of this class is to simulate the behavior of the Neural Graph\n  Machines model, which assumes that two nodes connected by a graph\n  always have the same label.\n  """"""\n\n  def __init__(self, data, **unused_kwargs):\n    self.data = data\n    self.vars_to_save = []\n\n  def train(self, *unused_args, **unused_kwargs):\n    logging.info(\'Using NGM, agreement always returns 1. no need to train...\')\n\n  def predict(self, unused_session, unused_src_features, unused_tgt_features,\n              src_indices, unused_tgt_indices):\n    """"""Predict agreement for the provided pairs of samples.\n\n    The function contains many unused arguments, in order to conform with the\n    interface of the TrainerAgreement class.\n\n    Arguments:\n      unused_session: A TensorFlow session where to run the model.\n      unused_src_features: An array of shape (num_samples, num_features)\n        containing the features of the first element of the pair.\n      unused_tgt_features: An array of shape (num_samples, num_features)\n        containing the features of the second element of the pair.\n      src_indices: An array of integers containing the index of each sample in\n        self.data of the samples in src_features.\n      unused_tgt_indices: An array of integers containing the index of each\n        sample in self.data of the samples in tgt_features.\n\n    Returns:\n      An array containing the predicted agreement value for each pair of\n      provided samples.\n    """"""\n    num_samples = src_indices.shape[0]\n    return np.ones((num_samples,), dtype=np.float32)\n\n  def create_agreement_prediction(self, src_indices, *unused_args,\n                                  **unused_kwargs):\n    """"""Creates the agreement prediction TensorFlow subgraph.\n\n    This function is the equivalent of `create_agreement_prediction` in\n    TrainerAgreement, but here we always predict 1.0.\n\n    Arguments:\n      src_indices: A Tensor or Placeholder of shape (batch_size,) containing the\n        indices of the samples that are the sources of the edges.\n      unused_args: Other unused arguments, which we allow in order to create a\n        common interface with TrainerAgreement.\n      unused_kwargs: Other unused keyword arguments, which we allow in order to\n        create a common interface with TrainerAgreement.\n\n    Returns:\n      predictions: None, because this model doesn\'t do logits computations, but\n        we still return something in order to keep the same function outputs as\n        TrainerAgreement.\n      normalized_predictions: A Tensor of shape (batch_size,) with values in\n        {0, 1}, containing the agreement prediction probabilities.\n      variables: An empty dictionary of trainable variables, because this model\n        does not have any trainable variables.\n      reg_params: An empty dictionary of variables that are used in the\n        regularization weight decay term, because this model doesn\'t have\n        regularization variables.\n    """"""\n    return None, tf.ones((tf.shape(src_indices)[0],), tf.float32), {}, {}\n'"
research/gam/gam/trainer/trainer_base.py,9,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functionality common in all Graph Agreement Models trainers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport numpy as np\nimport tensorflow as tf\n\n\ndef batch_iterator(inputs,\n                   targets=None,\n                   batch_size=None,\n                   shuffle=False,\n                   allow_smaller_batch=False,\n                   repeat=True):\n  """"""A generator that provides batches of samples from the provided inputs.""""""\n  if isinstance(inputs, set):\n    inputs = np.asarray(list(inputs))\n  if not isinstance(inputs, (np.ndarray, list)):\n    raise TypeError(\'Unsupported data type %s encountered.\' % type(inputs))\n  if targets is not None and not isinstance(targets, (np.ndarray, list)):\n    raise TypeError(\'Unsupported data type %s encountered.\' % type(targets))\n  num_samples = len(inputs)\n  if batch_size is None:\n    batch_size = num_samples\n  if batch_size > num_samples:\n    allow_smaller_batch = True\n  keep_going = True\n  while keep_going:\n    indexes = np.arange(0, num_samples)\n    if shuffle:\n      np.random.shuffle(indexes)\n    shuffled_inputs = inputs[indexes]\n    if targets is not None:\n      shuffled_targets = targets[indexes]\n    for start_index in range(0, num_samples, batch_size):\n      if allow_smaller_batch:\n        end_index = min(start_index + batch_size, num_samples)\n      else:\n        end_index = start_index + batch_size\n        if end_index > num_samples:\n          break\n      batch_inputs = shuffled_inputs[start_index:end_index]\n      if targets is None:\n        yield batch_inputs\n      else:\n        batch_targets = shuffled_targets[start_index:end_index]\n        yield batch_inputs, batch_targets\n    if not repeat:\n      keep_going = False\n\n\ndef variable_summaries(var):\n  """"""Attach summaries to a tensor (for TensorBoard visualizations).""""""\n  name = var.name[var.name.rfind(\'/\') + 1:var.name.rfind(\':\')]\n  with tf.name_scope(name):\n    mean = tf.reduce_mean(var)\n    tf.summary.scalar(\'mean\', mean)\n    with tf.name_scope(\'stddev\'):\n      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n    tf.summary.scalar(\'stddev\', stddev)\n    tf.summary.scalar(\'max\', tf.reduce_max(var))\n    tf.summary.scalar(\'min\', tf.reduce_min(var))\n    tf.summary.histogram(\'histogram\', var)\n\n\nclass Trainer(object):\n  """"""Abstract class for model trainers.""""""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self,\n               model,\n               abs_loss_chg_tol=1e-10,\n               rel_loss_chg_tol=1e-5,\n               loss_chg_iter_below_tol=10):\n    self.model = model\n    self.abs_loss_chg_tol = abs_loss_chg_tol\n    self.rel_loss_chg_tol = rel_loss_chg_tol\n    self.loss_chg_iter_below_tol = loss_chg_iter_below_tol\n\n  @abc.abstractmethod\n  def train(self, data, **kwargs):\n    pass\n\n  def check_convergence(self,\n                        prev_loss,\n                        loss,\n                        step,\n                        max_iter,\n                        iter_below_tol,\n                        min_num_iter=0):\n    """"""Checks if training for a model has converged.""""""\n    has_converged = False\n\n    # Check if we have reached the desired loss tolerance.\n    loss_diff = abs(prev_loss - loss)\n    if loss_diff < self.abs_loss_chg_tol or abs(\n        loss_diff / prev_loss) < self.rel_loss_chg_tol:\n      iter_below_tol += 1\n    else:\n      iter_below_tol = 0\n    if iter_below_tol >= self.loss_chg_iter_below_tol:\n      # print(\'Loss value converged.\')\n      has_converged = True\n\n    # Make sure that irrespective of the stop criteria, the minimum required\n    # number of iterations is achieved.\n    if step < min_num_iter:\n      has_converged = False\n    else:\n      has_converged = True\n\n    # Make sure we don\'t exceed the max allowed number of iterations.\n    if max_iter is not None and step >= max_iter:\n      print(\'Maximum number of iterations reached.\')\n      has_converged = True\n    return has_converged, iter_below_tol\n'"
research/gam/gam/trainer/trainer_classification.py,79,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Trainer for classification models for Graph Agreement Models without a graph.\n\nThis class contains functionality that allows for training a classification\nmodel to be used as part of Graph Agreement Models.\nThis implementation does not use a provided graph, but samples random pairs\nof samples.\n""""""\nimport logging\nimport os\n\nfrom .adversarial_dense import entropy_y_x\nfrom .adversarial_dense import get_loss_vat\nimport numpy as np\nimport tensorflow as tf\nfrom .trainer_base import batch_iterator\nfrom .trainer_base import Trainer\n\n\nclass TrainerClassification(Trainer):\n  """"""Trainer for the classifier component of a Graph Agreement Model.\n\n  Attributes:\n    model: A Model object that is used to provide the architecture of the\n      classification model.\n    is_train: A placeholder for a boolean value specyfing if the model is used\n      for train or evaluation.\n    data: A CotrainDataset object.\n    trainer_agr: A TrainerAgreement or TrainerPerfectAgreement object.\n    optimizer: Optimizer used for training the classification model.\n    batch_size: Batch size for used when training and evaluating the\n      classification model.\n    gradient_clip: A float number representing the maximum gradient norm allowed\n      if we do gradient clipping. If None, no gradient clipping is performed.\n    min_num_iter: An integer representing the minimum number of iterations to\n      train the classification model.\n    max_num_iter: An integer representing the maximum number of iterations to\n      train the classification model.\n    num_iter_after_best_val: An integer representing the number of extra\n      iterations to perform after improving the validation set accuracy.\n    max_num_iter_cotrain: An integer representing the maximum number of cotrain\n      iterations to train for.\n    reg_weight_ll: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      labeled-labeled pairs of samples.\n    reg_weight_lu: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      labeled-unlabeled pairs of samples.\n    reg_weight_uu: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      unlabeled-unlabeled pairs of samples.\n    num_pairs_reg: An integer representing the number of sample pairs of each\n      type (LL, LU, UU) to include in each computation of the classification\n      model loss.\n    iter_cotrain: A Tensorflow variable containing the current cotrain\n      iteration.\n    reg_weight_vat: A float representing the weight of the virtual adversarial\n      training (VAT) regularization loss in the classification model loss\n      function.\n    use_ent_min: A boolean specifying whether to use entropy regularization with\n      VAT.\n    enable_summaries: Boolean specifying whether to enable variable summaries.\n    summary_step: Integer representing the summary step size.\n    summary_dir: String representing the path to a directory where to save the\n      variable summaries.\n    logging_step: Integer representing the number of iterations after which we\n      log the loss of the model.\n    eval_step: Integer representing the number of iterations after which we\n      evaluate the model.\n    warm_start: Whether the model parameters are initialized at their best value\n      in the previous cotrain iteration. If False, they are reinitialized.\n      gradient_clip=None,\n    abs_loss_chg_tol: A float representing the absolute tolerance for checking\n      if the training loss has converged. If the difference between the current\n      loss and previous loss is less than `abs_loss_chg_tol`, we count this\n      iteration towards convergence (see `loss_chg_iter_below_tol`).\n    rel_loss_chg_tol: A float representing the relative tolerance for checking\n      if the training loss has converged. If the ratio between the current loss\n      and previous loss is less than `rel_loss_chg_tol`, we count this iteration\n      towards convergence (see `loss_chg_iter_below_tol`).\n    loss_chg_iter_below_tol: An integer representing the number of consecutive\n      iterations that pass the convergence criteria before stopping training.\n    checkpoints_dir: Path to the folder where to store TensorFlow model\n      checkpoints.\n    weight_decay: Weight for the weight decay term in the classification model\n      loss.\n    weight_decay_schedule: Schedule how to adjust the classification weight\n      decay weight after every cotrain iteration.\n    penalize_neg_agr: Whether to not only encourage agreement between samples\n      that the agreement model believes should have the same label, but also\n      penalize agreement when two samples agree when the agreement model\n      predicts they should disagree.\n    first_iter_original:  A boolean specifying whether the first cotrain\n      iteration trains the original classification model (with no agreement\n      term).\n    use_l2_classif: Whether to use L2 loss for classification, as opposed to the\n      whichever loss is specified in the provided model_cls.\n    seed: Seed used by all the random number generators in this class.\n    use_graph: Boolean specifying whether the agreement loss is applied to graph\n      edges, as opposed to random pairs of samples.\n  """"""\n\n  def __init__(self,\n               model,\n               data,\n               trainer_agr,\n               optimizer,\n               lr_initial,\n               batch_size,\n               min_num_iter,\n               max_num_iter,\n               num_iter_after_best_val,\n               max_num_iter_cotrain,\n               reg_weight_ll,\n               reg_weight_lu,\n               reg_weight_uu,\n               num_pairs_reg,\n               iter_cotrain,\n               reg_weight_vat=0.0,\n               use_ent_min=False,\n               enable_summaries=False,\n               summary_step=1,\n               summary_dir=None,\n               warm_start=False,\n               gradient_clip=None,\n               logging_step=1,\n               eval_step=1,\n               abs_loss_chg_tol=1e-10,\n               rel_loss_chg_tol=1e-7,\n               loss_chg_iter_below_tol=30,\n               checkpoints_dir=None,\n               weight_decay=None,\n               weight_decay_schedule=None,\n               penalize_neg_agr=False,\n               first_iter_original=True,\n               use_l2_classif=True,\n               seed=None,\n               lr_decay_steps=None,\n               lr_decay_rate=None,\n               use_graph=False):\n    super(TrainerClassification, self).__init__(\n        model=model,\n        abs_loss_chg_tol=abs_loss_chg_tol,\n        rel_loss_chg_tol=rel_loss_chg_tol,\n        loss_chg_iter_below_tol=loss_chg_iter_below_tol)\n    self.data = data\n    self.trainer_agr = trainer_agr\n    self.batch_size = batch_size\n    self.min_num_iter = min_num_iter\n    self.max_num_iter = max_num_iter\n    self.num_iter_after_best_val = num_iter_after_best_val\n    self.max_num_iter_cotrain = max_num_iter_cotrain\n    self.enable_summaries = enable_summaries\n    self.summary_step = summary_step\n    self.summary_dir = summary_dir\n    self.warm_start = warm_start\n    self.gradient_clip = gradient_clip\n    self.logging_step = logging_step\n    self.eval_step = eval_step\n    self.checkpoint_path = (\n        os.path.join(checkpoints_dir, \'classif_best.ckpt\')\n        if checkpoints_dir is not None else None)\n    self.weight_decay_initial = weight_decay\n    self.weight_decay_schedule = weight_decay_schedule\n    self.num_pairs_reg = num_pairs_reg\n    self.reg_weight_ll = reg_weight_ll\n    self.reg_weight_lu = reg_weight_lu\n    self.reg_weight_uu = reg_weight_uu\n    self.reg_weight_vat = reg_weight_vat\n    self.use_ent_min = use_ent_min\n    self.penalize_neg_agr = penalize_neg_agr\n    self.use_l2_classif = use_l2_classif\n    self.first_iter_original = first_iter_original\n    self.iter_cotrain = iter_cotrain\n    self.lr_initial = lr_initial\n    self.lr_decay_steps = lr_decay_steps\n    self.lr_decay_rate = lr_decay_rate\n    self.use_graph = use_graph\n\n    # Build TensorFlow graph.\n    logging.info(\'Building classification TensorFlow graph...\')\n\n    # Create placeholders.\n    # First obtain the features shape from the dataset, and append a batch_size\n    # dimension to it (i.e., `None` to allow for variable batch size).\n    features_shape = [None] + list(data.features_shape)\n    input_features = tf.placeholder(\n        tf.float32, shape=features_shape, name=\'input_features\')\n    input_features_unlabeled = tf.placeholder(\n        tf.float32, shape=features_shape, name=\'input_features_unlabeled\')\n    input_labels = tf.placeholder(tf.int64, shape=(None,), name=\'input_labels\')\n    one_hot_labels = tf.one_hot(\n        input_labels, data.num_classes, name=\'input_labels_one_hot\')\n    # Create a placeholder specifying if this is train time.\n    is_train = tf.placeholder_with_default(False, shape=[], name=\'is_train\')\n\n    # Create variables and predictions.\n    with tf.variable_scope(\'predictions\'):\n      encoding, variables_enc, reg_params_enc = (\n          self.model.get_encoding_and_params(\n              inputs=input_features, is_train=is_train))\n      self.variables = variables_enc\n      self.reg_params = reg_params_enc\n      predictions, variables_pred, reg_params_pred = (\n          self.model.get_predictions_and_params(\n              encoding=encoding, is_train=is_train))\n      self.variables.update(variables_pred)\n      self.reg_params.update(reg_params_pred)\n      normalized_predictions = self.model.normalize_predictions(predictions)\n      predictions_var_scope = tf.get_variable_scope()\n\n    # Create predictions on unlabeled data, which is only used for VAT loss.\n    with tf.variable_scope(\'predictions\', reuse=True):\n      encoding_unlabeled, _, _ = self.model.get_encoding_and_params(\n          inputs=input_features_unlabeled,\n          is_train=is_train,\n          update_batch_stats=False)\n      predictions_unlabeled, _, _ = (\n          self.model.get_predictions_and_params(\n              encoding=encoding_unlabeled, is_train=is_train))\n\n    # Create a variable for weight decay that may be updated.\n    weight_decay_var, weight_decay_update = self._create_weight_decay_var(\n        weight_decay, weight_decay_schedule)\n\n    # Create counter for classification iterations.\n    iter_cls_total, iter_cls_total_update = self._create_counter()\n\n    # Create loss.\n    with tf.name_scope(\'loss\'):\n      if self.use_l2_classif:\n        loss_supervised = tf.square(one_hot_labels - normalized_predictions)\n        loss_supervised = tf.reduce_sum(loss_supervised, axis=-1)\n        loss_supervised = tf.reduce_mean(loss_supervised)\n      else:\n        loss_supervised = self.model.get_loss(\n            predictions=predictions, targets=one_hot_labels, weight_decay=None)\n\n      # Agreement regularization loss.\n      loss_agr = self._get_agreement_reg_loss(data, is_train, features_shape)\n      # If the first co-train iteration trains the original model (for\n      # comparison purposes), then we do not add an agreement loss.\n      if self.first_iter_original:\n        loss_agr_weight = tf.cast(tf.greater(iter_cotrain, 0), tf.float32)\n        loss_agr = loss_agr * loss_agr_weight\n\n      # Weight decay loss.\n      loss_reg = 0.0\n      if weight_decay_var is not None:\n        for var in self.reg_params.values():\n          loss_reg += weight_decay_var * tf.nn.l2_loss(var)\n\n      # Adversarial loss, in case we want to add VAT on top of GAM.\n      loss_vat = get_loss_vat(input_features_unlabeled, predictions_unlabeled,\n                              is_train, model, predictions_var_scope)\n      num_unlabeled = tf.shape(input_features_unlabeled)[0]\n      loss_vat = tf.cond(\n          tf.greater(num_unlabeled, 0), lambda: loss_vat, lambda: 0.0)\n      if self.use_ent_min:\n        # Use entropy minimization with VAT (i.e. VATENT).\n        loss_ent = entropy_y_x(predictions_unlabeled)\n        loss_vat = loss_vat + tf.cond(\n            tf.greater(num_unlabeled, 0), lambda: loss_ent, lambda: 0.0)\n      loss_vat = loss_vat * self.reg_weight_vat\n      if self.first_iter_original:\n        # Do not add the adversarial loss in the first iteration if\n        # the first iteration trains the plain baseline model.\n        weight_loss_vat = tf.cond(\n            tf.greater(iter_cotrain, 0), lambda: 1.0, lambda: 0.0)\n        loss_vat = loss_vat * weight_loss_vat\n\n      # Total loss.\n      loss_op = loss_supervised + loss_agr + loss_reg + loss_vat\n\n    # Create accuracy.\n    accuracy = tf.equal(tf.argmax(normalized_predictions, 1), input_labels)\n    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n\n    # Create Tensorboard summaries.\n    if self.enable_summaries:\n      summaries = [\n          tf.summary.scalar(\'loss_supervised\', loss_supervised),\n          tf.summary.scalar(\'loss_agr\', loss_agr),\n          tf.summary.scalar(\'loss_reg\', loss_reg),\n          tf.summary.scalar(\'loss_total\', loss_op)\n      ]\n      self.summary_op = tf.summary.merge(summaries)\n\n    # Create learning rate schedule and optimizer.\n    self.global_step = tf.train.get_or_create_global_step()\n    if self.lr_decay_steps is not None and self.lr_decay_rate is not None:\n      self.lr = tf.train.exponential_decay(\n          self.lr_initial,\n          self.global_step,\n          self.lr_decay_steps,\n          self.lr_decay_rate,\n          staircase=True)\n      self.optimizer = optimizer(self.lr)\n    else:\n      self.optimizer = optimizer(lr_initial)\n\n    # Get trainable variables and compute gradients.\n    grads_and_vars = self.optimizer.compute_gradients(\n        loss_op,\n        tf.trainable_variables(scope=tf.get_default_graph().get_name_scope()))\n    # Clip gradients.\n    if self.gradient_clip:\n      variab = [elem[1] for elem in grads_and_vars]\n      gradients = [elem[0] for elem in grads_and_vars]\n      gradients, _ = tf.clip_by_global_norm(gradients, self.gradient_clip)\n      grads_and_vars = tuple(zip(gradients, variab))\n    with tf.control_dependencies(\n        tf.get_collection(\n            tf.GraphKeys.UPDATE_OPS,\n            scope=tf.get_default_graph().get_name_scope())):\n      train_op = self.optimizer.apply_gradients(\n          grads_and_vars, global_step=self.global_step)\n\n    # Create a saver for model variables.\n    trainable_vars = [v for _, v in grads_and_vars]\n\n    # Put together the subset of variables to save and restore from the best\n    # validation accuracy as we train the agreement model in one cotrain round.\n    vars_to_save = trainable_vars + []\n    if isinstance(weight_decay_var, tf.Variable):\n      vars_to_save.append(weight_decay_var)\n    saver = tf.train.Saver(vars_to_save)\n\n    # Put together all variables that need to be saved in case the process is\n    # interrupted and needs to be restarted.\n    self.vars_to_save = [iter_cls_total, self.global_step]\n    if isinstance(weight_decay_var, tf.Variable):\n      self.vars_to_save.append(weight_decay_var)\n    if self.warm_start:\n      self.vars_to_save.extend([v for v in self.variables])\n\n    # More variables to be initialized after the session is created.\n    self.is_initialized = False\n\n    self.rng = np.random.RandomState(seed)\n    self.input_features = input_features\n    self.input_features_unlabeled = input_features_unlabeled\n    self.input_labels = input_labels\n    self.predictions = predictions\n    self.normalized_predictions = normalized_predictions\n    self.weight_decay_var = weight_decay_var\n    self.weight_decay_update = weight_decay_update\n    self.iter_cls_total = iter_cls_total\n    self.iter_cls_total_update = iter_cls_total_update\n    self.accuracy = accuracy\n    self.train_op = train_op\n    self.loss_op = loss_op\n    self.saver = saver\n    self.batch_size_actual = tf.shape(self.predictions)[0]\n    self.reset_optimizer = tf.variables_initializer(self.optimizer.variables())\n    self.is_train = is_train\n\n  def _create_weight_decay_var(self, weight_decay_initial,\n                               weight_decay_schedule):\n    """"""Creates a weight decay variable that can be updated using a schedule.""""""\n    weight_decay_var = None\n    weight_decay_update = None\n    if weight_decay_schedule is None:\n      if weight_decay_initial is not None:\n        weight_decay_var = tf.constant(\n            weight_decay_initial, dtype=tf.float32, name=\'weight_decay\')\n      else:\n        weight_decay_var = None\n    elif weight_decay_schedule == \'linear\':\n      weight_decay_var = tf.get_variable(\n          name=\'weight_decay\',\n          initializer=tf.constant(\n              weight_decay_initial, name=\'weight_decay_initial\'),\n          use_resource=True,\n          trainable=False)\n      update_rate = weight_decay_initial / float(self.max_num_iter_cotrain)\n      weight_decay_update = weight_decay_var.assign_sub(update_rate)\n    return weight_decay_var, weight_decay_update\n\n  def _create_counter(self):\n    """"""Creates a cummulative iteration counter for all classification steps.""""""\n    iter_cls_total = tf.get_variable(\n        name=\'iter_cls_total\',\n        initializer=tf.constant(0, name=\'iter_cls_total\'),\n        use_resource=True,\n        trainable=False)\n    iter_cls_total_update = iter_cls_total.assign_add(1)\n    return iter_cls_total, iter_cls_total_update\n\n  def _get_agreement_reg_loss(self, data, is_train, features_shape):\n    """"""Computes the regularization loss coming from the agreement term.\n\n    This is calculated using the following idea: we incur a loss for pairs of\n    samples that should have the same label, but for which the predictions of\n    the classification model are not equal. The loss incured by each pair is\n    proportionate to the distance between the two predictions, as well as the\n    confidence we have that they should agree.\n\n    In the case of pairs where both samples are labeled (LL), the agreement\n    confidence is 1.0. When at least one sample is unlabeled (LU, UU), then we\n    use the agreement model to estimate this confidence.\n\n    Note that for the pairs where a label is available, we can compute this loss\n    wrt. the actual label, instead of the classifier predictions. However, when\n    both samples are labeled (LL), for one of them we use the prediction and for\n    the other the true label -- otherwise there are no gradients to proagate.\n\n    Arguments:\n      data: A CotrainDataset object.\n      is_train: A placeholder for a boolean that specifies if this is function\n        is called as part of model training or inference.\n      features_shape: A tuple of integers containing the number of features in\n        each dimension of the inputs, not including batch size.\n\n    Returns:\n      The computed agreement loss op.\n    """"""\n    # Select num_pairs_reg pairs of samples from each category LL, LU, UU.\n    # for which to do the regularization.\n    indices_lu_left = tf.placeholder(dtype=tf.int64, shape=(None,))\n    indices_lu_right = tf.placeholder(dtype=tf.int64, shape=(None,))\n    indices_uu_left = tf.placeholder(dtype=tf.int64, shape=(None,))\n    indices_uu_right = tf.placeholder(dtype=tf.int64, shape=(None,))\n\n    features_ll_right = tf.placeholder(dtype=tf.float32, shape=features_shape)\n    features_lu_left = tf.placeholder(dtype=tf.float32, shape=features_shape)\n    features_lu_right = tf.placeholder(dtype=tf.float32, shape=features_shape)\n    features_uu_left = tf.placeholder(dtype=tf.float32, shape=features_shape)\n    features_uu_right = tf.placeholder(dtype=tf.float32, shape=features_shape)\n\n    labels_ll_left_idx = tf.placeholder(dtype=tf.int64, shape=(None,))\n    labels_ll_right_idx = tf.placeholder(dtype=tf.int64, shape=(None,))\n    labels_lu_left_idx = tf.placeholder(dtype=tf.int64, shape=(None,))\n\n    labels_ll_left = tf.one_hot(labels_ll_left_idx, data.num_classes)\n    labels_lu_left = tf.one_hot(labels_lu_left_idx, data.num_classes)\n\n    with tf.variable_scope(\'predictions\', reuse=True):\n      encoding, _, _ = self.model.get_encoding_and_params(\n          inputs=features_ll_right, is_train=is_train, update_batch_stats=False)\n      predictions_ll_right, _, _ = self.model.get_predictions_and_params(\n          encoding=encoding, is_train=is_train)\n      predictions_ll_right = self.model.normalize_predictions(\n          predictions_ll_right)\n\n      encoding, _, _ = self.model.get_encoding_and_params(\n          inputs=features_lu_right, is_train=is_train, update_batch_stats=False)\n      predictions_lu_right, _, _ = self.model.get_predictions_and_params(\n          encoding=encoding, is_train=is_train)\n      predictions_lu_right = self.model.normalize_predictions(\n          predictions_lu_right)\n\n      encoding, _, _ = self.model.get_encoding_and_params(\n          inputs=features_uu_left, is_train=is_train, update_batch_stats=False)\n      predictions_uu_left, _, _ = self.model.get_predictions_and_params(\n          encoding=encoding, is_train=is_train)\n      predictions_uu_left = self.model.normalize_predictions(\n          predictions_uu_left)\n\n      encoding, _, _ = self.model.get_encoding_and_params(\n          inputs=features_uu_right, is_train=is_train, update_batch_stats=False)\n      predictions_uu_right, _, _ = self.model.get_predictions_and_params(\n          encoding=encoding, is_train=is_train)\n      predictions_uu_right = self.model.normalize_predictions(\n          predictions_uu_right)\n\n    # Compute Euclidean distance between the label distributions that the\n    # classification model predicts for the src and tgt of each pair.\n    # Stop gradients need to be added\n    # The case where there are no more uu or lu\n    # edges at the end of training, so the shapes don\'t match needs fixing.\n    left = tf.concat((labels_ll_left, labels_lu_left, predictions_uu_left),\n                     axis=0)\n    right = tf.concat(\n        (predictions_ll_right, predictions_lu_right, predictions_uu_right),\n        axis=0)\n    dists = tf.reduce_sum(tf.square(left - right), axis=-1)\n\n    # Estimate a weight for each distance, depending on the predictions\n    # of the agreement model. For the labeled samples, we can use the actual\n    # agreement between the labels, no need to estimate.\n    agreement_ll = tf.cast(\n        tf.equal(labels_ll_left_idx, labels_ll_right_idx), dtype=tf.float32)\n    _, agreement_lu, _, _ = self.trainer_agr.create_agreement_prediction(\n        src_features=features_lu_left,\n        tgt_features=features_lu_right,\n        is_train=is_train,\n        src_indices=indices_lu_left,\n        tgt_indices=indices_lu_right)\n    _, agreement_uu, _, _ = self.trainer_agr.create_agreement_prediction(\n        src_features=features_uu_left,\n        tgt_features=features_uu_right,\n        is_train=is_train,\n        src_indices=indices_uu_left,\n        tgt_indices=indices_uu_right)\n    agreement = tf.concat((agreement_ll, agreement_lu, agreement_uu), axis=0)\n    if self.penalize_neg_agr:\n      # Since the agreement is predicting scores between [0, 1], anything\n      # under 0.5 should represent disagreement. Therefore, we want to encourage\n      # agreement whenever the score is > 0.5, otherwise don\'t incur any loss.\n      agreement = tf.nn.relu(agreement - 0.5)\n\n    # Create a Tensor containing the weights assigned to each pair in the\n    # agreement regularization loss, depending on how many samples in the pair\n    # were labeled. This weight can be either reg_weight_ll, reg_weight_lu,\n    # or reg_weight_uu.\n    num_ll = tf.shape(predictions_ll_right)[0]\n    num_lu = tf.shape(predictions_lu_right)[0]\n    num_uu = tf.shape(predictions_uu_left)[0]\n    weights = tf.concat(\n        (self.reg_weight_ll * tf.ones(num_ll,), self.reg_weight_lu *\n         tf.ones(num_lu,), self.reg_weight_uu * tf.ones(num_uu,)),\n        axis=0)\n\n    # Scale each distance by its agreement weight and regularzation weight.\n    loss = tf.reduce_mean(dists * weights * agreement)\n\n    self.indices_lu_left = indices_lu_left\n    self.indices_lu_right = indices_lu_right\n    self.indices_uu_left = indices_uu_left\n    self.indices_uu_right = indices_uu_right\n    self.features_ll_right = features_ll_right\n    self.features_lu_left = features_lu_left\n    self.features_lu_right = features_lu_right\n    self.features_uu_left = features_uu_left\n    self.features_uu_right = features_uu_right\n    self.labels_ll_left = labels_ll_left_idx\n    self.labels_ll_right = labels_ll_right_idx\n    self.labels_lu_left = labels_lu_left_idx\n    self.agreement_lu = agreement_lu\n\n    return loss\n\n  def _construct_feed_dict(self,\n                           data_iterator,\n                           split,\n                           pair_ll_iterator=None,\n                           pair_lu_iterator=None,\n                           pair_uu_iterator=None,\n                           data_iterator_unlabeled=None):\n    """"""Construct feed dictionary.""""""\n    try:\n      input_indices = next(data_iterator)\n      # Select the labels. Use the true, correct labels, at test time, and the\n      # self-labeled ones at train time.\n      labels = (\n          self.data.get_original_labels(input_indices)\n          if split == \'test\' else self.data.get_labels(input_indices))\n      feed_dict = {\n          self.input_features: self.data.get_features(input_indices),\n          self.input_labels: labels,\n          self.is_train: split == \'train\'\n      }\n      if data_iterator_unlabeled is not None:\n        # This is not None only when using VAT regularization.\n        try:\n          input_indices = next(data_iterator_unlabeled)\n          input_features = self.data.get_features(input_indices)\n        except StopIteration:\n          input_features = np.zeros([0] + list(self.data.features_shape))\n        feed_dict.update({self.input_features_unlabeled: input_features})\n      if pair_ll_iterator is not None:\n        _, _, _, features_tgt, labels_src, labels_tgt = next(pair_ll_iterator)\n        feed_dict.update({\n            self.features_ll_right: features_tgt,\n            self.labels_ll_left: labels_src,\n            self.labels_ll_right: labels_tgt\n        })\n      if pair_lu_iterator is not None:\n        indices_src, indices_tgt, features_src, features_tgt, labels_src, _ = (\n            next(pair_lu_iterator))\n        feed_dict.update({\n            self.indices_lu_left: indices_src,\n            self.indices_lu_right: indices_tgt,\n            self.features_lu_left: features_src,\n            self.features_lu_right: features_tgt,\n            self.labels_lu_left: labels_src\n        })\n      if pair_uu_iterator is not None:\n        indices_src, indices_tgt, features_src, features_tgt, _, _ = next(\n            pair_uu_iterator)\n        feed_dict.update({\n            self.indices_uu_left: indices_src,\n            self.indices_uu_right: indices_tgt,\n            self.features_uu_left: features_src,\n            self.features_uu_right: features_tgt\n        })\n      return feed_dict\n    except StopIteration:\n      # If the iterator has finished, return None.\n      return None\n\n  def pair_iterator(self, src_indices, tgt_indices, batch_size, data):\n    """"""Iterator over pairs of samples.\n\n    The first element of the pair is selected from the src_indices, and the\n    second element is selected from tgt_indices.\n\n    Arguments:\n      src_indices: Numpy array containing the indices available for the source\n        node.\n      tgt_indices: Numpy array containing the indices available for the tgt\n        node.\n      batch_size: An integer representing the desired batch size.\n      data: A CotrainDataset object used to extract the features and labels.\n\n    Yields:\n      indices_src, indices_tgt, features_src, features_tgt, labels_src,\n      labels_tgt\n    """"""\n\n    def _select_from_pool(indices):\n      """"""Selects batch_size indices from the provided list.""""""\n      num_indices = len(indices)\n      if num_indices > 0:\n        idxs = self.rng.randint(0, high=num_indices, size=(batch_size,))\n        indices_batch = indices[idxs]\n        features_batch = data.get_features(indices_batch)\n        labels_batch = data.get_labels(indices_batch)\n      else:\n        features_shape = [0] + list(data.features_shape)\n        indices_batch = np.zeros(shape=(0,), dtype=np.int64)\n        features_batch = np.zeros(shape=features_shape, dtype=np.float32)\n        labels_batch = np.zeros(shape=(0,), dtype=np.int64)\n      return indices_batch, features_batch, labels_batch\n\n    while True:\n      indices_src, features_src, labels_src = _select_from_pool(src_indices)\n      indices_tgt, features_tgt, labels_tgt = _select_from_pool(tgt_indices)\n      yield (indices_src, indices_tgt, features_src, features_tgt, labels_src,\n             labels_tgt)\n\n  def edge_iterator(self, data, batch_size, labeling):\n    """"""An iterator over graph edges.\n\n    Args:\n      data: A CotrainDataset object used to extract the features and labels.\n      batch_size:  An integer representing the desired batch size.\n      labeling: A string which can be `ll`, `lu` or `uu`, that is used to\n        represent the type of edges to return, where `ll` refers to\n        labeled-labeled, `lu` refers to labeled-unlabeled, and `uu` refers to\n        unlabeled-unlabeled.\n\n    Yields:\n      indices_src, indices_tgt, features_src, features_tgt, labels_src,\n      labels_tgt\n    """"""\n    if labeling == \'ll\':\n      edges = data.get_edges(\n          src_labeled=True, tgt_labeled=True, label_must_match=True)\n    elif labeling == \'lu\':\n      edges = (\n          data.get_edges(src_labeled=True, tgt_labeled=False) +\n          data.get_edges(src_labeled=False, tgt_labeled=True))\n    elif labeling == \'uu\':\n      edges = data.get_edges(src_labeled=False, tgt_labeled=False)\n    else:\n      raise ValueError(\'Unsupported value for parameter `labeling`.\')\n\n    if not edges:\n      indices = np.zeros(shape=(0,), dtype=np.int32)\n      features = np.zeros(\n          shape=[\n              0,\n          ] + list(data.features_shape), dtype=np.float32)\n      labels = np.zeros(shape=(0,), dtype=np.int64)\n      while True:\n        yield (indices, indices, features, features, labels, labels)\n\n    edges = np.stack([(e.src, e.tgt) for e in edges])\n    iterator = batch_iterator(\n        inputs=edges,\n        batch_size=batch_size,\n        shuffle=True,\n        allow_smaller_batch=False,\n        repeat=True)\n\n    for edge in iterator:\n      indices_src = edge[:, 0]\n      indices_tgt = edge[:, 1]\n      features_src = data.get_features(indices_src)\n      features_tgt = data.get_features(indices_tgt)\n      labels_src = data.get_labels(indices_src)\n      labels_tgt = data.get_labels(indices_tgt)\n      yield (indices_src, indices_tgt, features_src, features_tgt, labels_src,\n             labels_tgt)\n\n  def _evaluate(self, indices, split, session, summary_writer):\n    """"""Evaluates the samples with the provided indices.""""""\n    data_iterator_val = batch_iterator(\n        indices,\n        batch_size=self.batch_size,\n        shuffle=False,\n        allow_smaller_batch=True,\n        repeat=False)\n    feed_dict_val = self._construct_feed_dict(data_iterator_val, split)\n    cummulative_acc = 0.0\n    num_samples = 0\n    while feed_dict_val is not None:\n      val_acc, batch_size_actual = session.run(\n          (self.accuracy, self.batch_size_actual), feed_dict=feed_dict_val)\n      cummulative_acc += val_acc * batch_size_actual\n      num_samples += batch_size_actual\n      feed_dict_val = self._construct_feed_dict(data_iterator_val, split)\n    if num_samples > 0:\n      cummulative_acc /= num_samples\n\n    if self.enable_summaries:\n      summary = tf.Summary()\n      summary.value.add(\n          tag=\'ClassificationModel/\' + split + \'_acc\',\n          simple_value=cummulative_acc)\n      iter_cls_total = session.run(self.iter_cls_total)\n      summary_writer.add_summary(summary, iter_cls_total)\n      summary_writer.flush()\n\n    return cummulative_acc\n\n  def train(self, data, session=None, **kwargs):\n    """"""Train the classification model on the provided dataset.\n\n    Arguments:\n      data: A CotrainDataset object.\n      session: A TensorFlow session or None.\n      **kwargs: Other keyword arguments.\n\n    Returns:\n      best_test_acc: A float representing the test accuracy at the iteration\n        where the validation accuracy is maximum.\n      best_val_acc: A float representing the best validation accuracy.\n    """"""\n    summary_writer = kwargs[\'summary_writer\']\n    logging.info(\'Training classifier...\')\n\n    if not self.is_initialized:\n      self.is_initialized = True\n    else:\n      if self.weight_decay_update is not None:\n        session.run(self.weight_decay_update)\n        logging.info(\'New weight decay value:  %f\',\n                     session.run(self.weight_decay_var))\n      # Reset the optimizer state (e.g., momentum).\n      session.run(self.reset_optimizer)\n\n    if not self.warm_start:\n      # Re-initialize variables.\n      initializers = [v.initializer for v in self.variables.values()]\n      initializers.append(self.global_step.initializer)\n      session.run(initializers)\n\n    # Construct data iterator.\n    logging.info(\'Training classifier with %d samples...\', data.num_train())\n    train_indices = data.get_indices_train()\n    unlabeled_indices = data.get_indices_unlabeled()\n    val_indices = data.get_indices_val()\n    test_indices = data.get_indices_test()\n    # Create an iterator for labeled samples for the supervised term.\n    data_iterator_train = batch_iterator(\n        train_indices,\n        batch_size=self.batch_size,\n        shuffle=True,\n        allow_smaller_batch=False,\n        repeat=True)\n    # Create an iterator for unlabeled samples for the VAT loss term.\n    data_iterator_unlabeled = batch_iterator(\n        unlabeled_indices,\n        batch_size=self.batch_size,\n        shuffle=True,\n        allow_smaller_batch=False,\n        repeat=True)\n    # Create iterators for ll, lu, uu pairs of samples for the agreement term.\n    if self.use_graph:\n      pair_ll_iterator = self.edge_iterator(\n          data, batch_size=self.num_pairs_reg, labeling=\'ll\')\n      pair_lu_iterator = self.edge_iterator(\n          data, batch_size=self.num_pairs_reg, labeling=\'lu\')\n      pair_uu_iterator = self.edge_iterator(\n          data, batch_size=self.num_pairs_reg, labeling=\'uu\')\n    else:\n      pair_ll_iterator = self.pair_iterator(train_indices, train_indices,\n                                            self.num_pairs_reg, data)\n      pair_lu_iterator = self.pair_iterator(train_indices, unlabeled_indices,\n                                            self.num_pairs_reg, data)\n      pair_uu_iterator = self.pair_iterator(unlabeled_indices,\n                                            unlabeled_indices,\n                                            self.num_pairs_reg, data)\n\n    step = 0\n    iter_below_tol = 0\n    min_num_iter = self.min_num_iter\n    has_converged = step >= self.max_num_iter\n    prev_loss_val = np.inf\n    best_test_acc = -1\n    best_val_acc = -1\n    checkpoint_saved = False\n    while not has_converged:\n      feed_dict = self._construct_feed_dict(\n          data_iterator=data_iterator_train,\n          split=\'train\',\n          pair_ll_iterator=pair_ll_iterator,\n          pair_lu_iterator=pair_lu_iterator,\n          pair_uu_iterator=pair_uu_iterator,\n          data_iterator_unlabeled=data_iterator_unlabeled)\n      if self.enable_summaries and step % self.summary_step == 0:\n        loss_val, summary, iter_cls_total, _ = session.run(\n            [self.loss_op, self.summary_op, self.iter_cls_total, self.train_op],\n            feed_dict=feed_dict)\n        summary_writer.add_summary(summary, iter_cls_total)\n        summary_writer.flush()\n      else:\n        loss_val, _ = session.run((self.loss_op, self.train_op),\n                                  feed_dict=feed_dict)\n\n      # Log the loss, if necessary.\n      if step % self.logging_step == 0:\n        logging.info(\'Classification step %6d | Loss: %10.4f\', step, loss_val)\n\n      # Evaluate, if necessary.\n      if step % self.eval_step == 0:\n        val_acc = self._evaluate(val_indices, \'val\', session, summary_writer)\n        test_acc = self._evaluate(test_indices, \'test\', session, summary_writer)\n\n        if step % self.logging_step == 0 or val_acc > best_val_acc:\n          logging.info(\n              \'Classification step %6d | Loss: %10.4f | val_acc: %10.4f | \'\n              \'test_acc: %10.4f\', step, loss_val, val_acc, test_acc)\n        if val_acc > best_val_acc:\n          best_val_acc = val_acc\n          best_test_acc = test_acc\n          if self.checkpoint_path:\n            self.saver.save(\n                session, self.checkpoint_path, write_meta_graph=False)\n            checkpoint_saved = True\n          # Go for at least num_iter_after_best_val more iterations.\n          min_num_iter = max(self.min_num_iter,\n                             step + self.num_iter_after_best_val)\n          logging.info(\n              \'Achieved best validation. \'\n              \'Extending to at least %d iterations...\', min_num_iter)\n\n      step += 1\n      has_converged, iter_below_tol = self.check_convergence(\n          prev_loss_val,\n          loss_val,\n          step,\n          self.max_num_iter,\n          iter_below_tol,\n          min_num_iter=min_num_iter)\n      session.run(self.iter_cls_total_update)\n      prev_loss_val = loss_val\n\n    # Return to the best model.\n    if checkpoint_saved:\n      logging.info(\'Restoring best model...\')\n      self.saver.restore(session, self.checkpoint_path)\n\n    return best_test_acc, best_val_acc\n\n  def predict(self, session, indices, is_train):\n    """"""Make predictions for the provided sample indices.""""""\n    num_inputs = len(indices)\n    idx_start = 0\n    predictions = []\n    while idx_start < num_inputs:\n      idx_end = min(idx_start + self.batch_size, num_inputs)\n      batch_indices = indices[idx_start:idx_end]\n      input_features = self.data.get_features(batch_indices)\n      batch_predictions = session.run(\n          self.normalized_predictions,\n          feed_dict={\n              self.input_features: input_features,\n              self.is_train: is_train\n          })\n      predictions.append(batch_predictions)\n      idx_start = idx_end\n    if not predictions:\n      return np.zeros((0, self.data.num_classes), dtype=np.float32)\n    return np.concatenate(predictions, axis=0)\n\n\nclass TrainerPerfectClassification(Trainer):\n  """"""Trainer for a classifier that always predicts the correct value.""""""\n\n  def __init__(self, data):\n    self.data = data\n    self.vars_to_save = []\n\n  def train(self, unused_data, unused_session=None, **unused_kwargs):\n    logging.info(\'Perfect classifier, no need to train...\')\n    return 1.0, 1.0\n\n  def predict(self, unused_session, indices_unlabeled, **unused_kwargs):\n    labels = self.data.get_original_labels(indices_unlabeled)\n    num_samples = len(indices_unlabeled)\n    predictions = np.zeros((num_samples, self.data.num_classes))\n    predictions[np.arange(num_samples), labels] = 1.0\n    return predictions\n'"
research/gam/gam/trainer/trainer_classification_gcn.py,92,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Trainer for classification models for Graph Agreement Models without a graph.\n\nThis class contains functionality that allows for training a classification\nmodel to be used as part of Graph Agreement Models.\nThis implementation does not use a provided graph, but samples random pairs\nof samples.\n""""""\nimport logging\nimport os\n\nfrom .adversarial_sparse import entropy_y_x\nfrom .adversarial_sparse import get_loss_vat\nimport numpy as np\nimport tensorflow as tf\nfrom .trainer_base import batch_iterator\nfrom .trainer_base import Trainer\n\n\nclass TrainerClassificationGCN(Trainer):\n  """"""Trainer for the classifier component of a Graph Agreement Model.\n\n  Attributes:\n    model: A Model object that is used to provide the architecture of the\n      classification model.\n    is_train: A placeholder for a boolean value specyfing if the model is used\n      for train or evaluation.\n    data: A CotrainDataset object.\n    trainer_agr: A TrainerAgreement or TrainerPerfectAgreement object.\n    optimizer: Optimizer used for training the classification model.\n    batch_size: Batch size for used when training and evaluating the\n      classification model.\n    gradient_clip: A float number representing the maximum gradient norm allowed\n      if we do gradient clipping. If None, no gradient clipping is performed.\n    min_num_iter: An integer representing the minimum number of iterations to\n      train the classification model.\n    max_num_iter: An integer representing the maximum number of iterations to\n      train the classification model.\n    num_iter_after_best_val: An integer representing the number of extra\n      iterations to perform after improving the validation set accuracy.\n    max_num_iter_cotrain: An integer representing the maximum number of cotrain\n      iterations to train for.\n    reg_weight_ll: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      labeled-labeled pairs of samples.\n    reg_weight_lu: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      labeled-unlabeled pairs of samples.\n    reg_weight_uu: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      unlabeled-unlabeled pairs of samples.\n    num_pairs_reg: An integer representing the number of sample pairs of each\n      type (LL, LU, UU) to include in each computation of the classification\n      model loss.\n    iter_cotrain: A Tensorflow variable containing the current cotrain\n      iteration.\n    reg_weight_vat: A float representing the weight of the virtual adversarial\n      training (VAT) regularization loss in the classification model loss\n      function.\n    use_ent_min: A boolean specifying whether to use entropy regularization with\n      VAT.\n    enable_summaries: Boolean specifying whether to enable variable summaries.\n    summary_step: Integer representing the summary step size.\n    summary_dir: String representing the path to a directory where to save the\n      variable summaries.\n    logging_step: Integer representing the number of iterations after which we\n      log the loss of the model.\n    eval_step: Integer representing the number of iterations after which we\n      evaluate the model.\n    warm_start: Whether the model parameters are initialized at their best value\n      in the previous cotrain iteration. If False, they are reinitialized.\n      gradient_clip=None,\n    abs_loss_chg_tol: A float representing the absolute tolerance for checking\n      if the training loss has converged. If the difference between the current\n      loss and previous loss is less than `abs_loss_chg_tol`, we count this\n      iteration towards convergence (see `loss_chg_iter_below_tol`).\n    rel_loss_chg_tol: A float representing the relative tolerance for checking\n      if the training loss has converged. If the ratio between the current loss\n      and previous loss is less than `rel_loss_chg_tol`, we count this iteration\n      towards convergence (see `loss_chg_iter_below_tol`).\n    loss_chg_iter_below_tol: An integer representing the number of consecutive\n      iterations that pass the convergence criteria before stopping training.\n    checkpoints_dir: Path to the folder where to store TensorFlow model\n      checkpoints.\n    weight_decay: Weight for the weight decay term in the classification model\n      loss.\n    weight_decay_schedule: Schedule how to adjust the classification weight\n      decay weight after every cotrain iteration.\n    penalize_neg_agr: Whether to not only encourage agreement between samples\n      that the agreement model believes should have the same label, but also\n      penalize agreement when two samples agree when the agreement model\n      predicts they should disagree.\n    first_iter_original:  A boolean specifying whether the first cotrain\n      iteration trains the original classification model (with no agreement\n      term).\n    use_l2_classif: Whether to use L2 loss for classification, as opposed to the\n      whichever loss is specified in the provided model_cls.\n    seed: Seed used by all the random number generators in this class.\n    use_graph: Boolean specifying whether the agreement loss is applied to graph\n      edges, as opposed to random pairs of samples.\n  """"""\n\n  def __init__(self,\n               model,\n               data,\n               trainer_agr,\n               optimizer,\n               lr_initial,\n               batch_size,\n               min_num_iter,\n               max_num_iter,\n               num_iter_after_best_val,\n               max_num_iter_cotrain,\n               reg_weight_ll,\n               reg_weight_lu,\n               reg_weight_uu,\n               num_pairs_reg,\n               iter_cotrain,\n               reg_weight_vat=0.0,\n               use_ent_min=False,\n               enable_summaries=False,\n               summary_step=1,\n               summary_dir=None,\n               warm_start=False,\n               gradient_clip=None,\n               logging_step=1,\n               eval_step=1,\n               abs_loss_chg_tol=1e-10,\n               rel_loss_chg_tol=1e-7,\n               loss_chg_iter_below_tol=30,\n               checkpoints_dir=None,\n               weight_decay=None,\n               weight_decay_schedule=None,\n               penalize_neg_agr=False,\n               first_iter_original=True,\n               use_l2_classif=True,\n               seed=None,\n               lr_decay_steps=None,\n               lr_decay_rate=None,\n               use_graph=False):\n    super(TrainerClassificationGCN, self).__init__(\n        model=model,\n        abs_loss_chg_tol=abs_loss_chg_tol,\n        rel_loss_chg_tol=rel_loss_chg_tol,\n        loss_chg_iter_below_tol=loss_chg_iter_below_tol)\n    self.data = data\n    self.trainer_agr = trainer_agr\n    self.batch_size = batch_size\n    self.min_num_iter = min_num_iter\n    self.max_num_iter = max_num_iter\n    self.num_iter_after_best_val = num_iter_after_best_val\n    self.max_num_iter_cotrain = max_num_iter_cotrain\n    self.enable_summaries = enable_summaries\n    self.summary_step = summary_step\n    self.summary_dir = summary_dir\n    self.warm_start = warm_start\n    self.gradient_clip = gradient_clip\n    self.logging_step = logging_step\n    self.eval_step = eval_step\n    self.checkpoint_path = (\n        os.path.join(checkpoints_dir, \'classif_best.ckpt\')\n        if checkpoints_dir is not None else None)\n    self.weight_decay_initial = weight_decay\n    self.weight_decay_schedule = weight_decay_schedule\n    self.num_pairs_reg = num_pairs_reg\n    self.reg_weight_ll = reg_weight_ll\n    self.reg_weight_lu = reg_weight_lu\n    self.reg_weight_uu = reg_weight_uu\n    self.reg_weight_vat = reg_weight_vat\n    self.use_ent_min = use_ent_min\n    self.penalize_neg_agr = penalize_neg_agr\n    self.use_l2_classif = use_l2_classif\n    self.first_iter_original = first_iter_original\n    self.iter_cotrain = iter_cotrain\n    self.lr_initial = lr_initial\n    self.lr_decay_steps = lr_decay_steps\n    self.lr_decay_rate = lr_decay_rate\n    self.use_graph = use_graph\n\n    # Build TensorFlow graph.\n    logging.info(\'Building classification TensorFlow graph...\')\n\n    # Create placeholders.\n    input_indices = tf.placeholder(\n        tf.int64, shape=(None,), name=\'input_indices\')\n    input_indices_unlabeled = tf.placeholder(\n        tf.int32, shape=(None,), name=\'input_indices_unlabeled\')\n    input_labels = tf.placeholder(tf.int64, shape=(None,), name=\'input_labels\')\n\n    # Create a placeholder specifying if this is train time.\n    is_train = tf.placeholder_with_default(False, shape=[], name=\'is_train\')\n\n    # Create some placeholders specific to GCN.\n    self.support_op = tf.sparse_placeholder(tf.float32, name=\'support\')\n    self.features_op = tf.sparse_placeholder(tf.float32, name=\'features\')\n    self.num_features_nonzero_op = tf.placeholder(\n        tf.int32, name=\'num_features_nonzero\')\n\n    # Save the data required to fill in these placeholders. We don\'t add them\n    # directly in the graph as constants in order to avoid saving large\n    # checkpoints.\n    self.support = data.support\n    self.features = data.dataset.features_sparse\n    self.num_features_nonzero = data.num_features_nonzero\n\n    # Create variables and predictions.\n    with tf.variable_scope(\'predictions\'):\n      encoding, variables_enc, reg_params_enc = (\n          self.model.get_encoding_and_params(\n              inputs=self.features_op,\n              is_train=is_train,\n              support=self.support_op,\n              num_features_nonzero=self.num_features_nonzero_op))\n      self.variables = variables_enc\n      self.reg_params = reg_params_enc\n      predictions, variables_pred, reg_params_pred = (\n          self.model.get_predictions_and_params(\n              encoding=encoding,\n              is_train=is_train,\n              support=self.support_op,\n              num_features_nonzero=self.num_features_nonzero_op))\n      self.variables.update(variables_pred)\n      self.reg_params.update(reg_params_pred)\n      normalized_predictions = self.model.normalize_predictions(predictions)\n      predictions_var_scope = tf.get_variable_scope()\n\n      predictions_batch = tf.gather(predictions, input_indices, axis=0)\n      normalized_predictions_batch = tf.gather(\n          normalized_predictions, input_indices, axis=0)\n      one_hot_labels = tf.one_hot(\n          input_labels, data.num_classes, name=\'targets_one_hot\')\n\n    # Create a variable for weight decay that may be updated.\n    weight_decay_var, weight_decay_update = self._create_weight_decay_var(\n        weight_decay, weight_decay_schedule)\n\n    # Create counter for classification iterations.\n    iter_cls_total, iter_cls_total_update = self._create_counter()\n\n    # Create loss.\n    with tf.name_scope(\'loss\'):\n      if self.use_l2_classif:\n        loss_supervised = tf.square(one_hot_labels -\n                                    normalized_predictions_batch)\n        loss_supervised = tf.reduce_sum(loss_supervised, axis=-1)\n        loss_supervised = tf.reduce_mean(loss_supervised)\n      else:\n        loss_supervised = self.model.get_loss(\n            predictions=predictions_batch,\n            targets=one_hot_labels,\n            weight_decay=None)\n\n      # Agreement regularization loss.\n      loss_agr = self._get_agreement_reg_loss(data, is_train)\n      # If the first co-train iteration trains the original model (for\n      # comparison purposes), then we do not add an agreement loss.\n      if self.first_iter_original:\n        loss_agr_weight = tf.cast(tf.greater(iter_cotrain, 0), tf.float32)\n        loss_agr = loss_agr * loss_agr_weight\n\n      # Weight decay loss.\n      loss_reg = 0.0\n      if weight_decay_var is not None:\n        for var in self.reg_params.values():\n          loss_reg += weight_decay_var * tf.nn.l2_loss(var)\n\n      # Adversarial loss, in case we want to add VAT on top of GAM.\n      ones = tf.fill(tf.shape(input_indices_unlabeled), 1.0)\n      unlabeled_mask = tf.scatter_nd(\n          input_indices_unlabeled[:, None],\n          updates=ones,\n          shape=[\n              data.num_samples,\n          ],\n          name=\'unlabeled_mask\')\n      placeholders = {\n          \'support\': self.support_op,\n          \'num_features_nonzero\': self.num_features_nonzero_op\n      }\n      loss_vat = get_loss_vat(\n          inputs=self.features_op,\n          predictions=predictions,\n          mask=unlabeled_mask,\n          is_train=is_train,\n          model=model,\n          placeholders=placeholders,\n          predictions_var_scope=predictions_var_scope)\n      num_unlabeled = tf.shape(input_indices_unlabeled)[0]\n      loss_vat = tf.cond(\n          tf.greater(num_unlabeled, 0), lambda: loss_vat, lambda: 0.0)\n      if self.use_ent_min:\n        # Use entropy minimization with VAT (i.e. VATENT).\n        loss_ent = entropy_y_x(predictions, unlabeled_mask)\n        loss_vat = loss_vat + tf.cond(\n            tf.greater(num_unlabeled, 0), lambda: loss_ent, lambda: 0.0)\n      loss_vat = loss_vat * self.reg_weight_vat\n      if self.first_iter_original:\n        # Do not add the adversarial loss in the first iteration if\n        # the first iteration trains the plain baseline model.\n        weight_loss_vat = tf.cond(\n            tf.greater(iter_cotrain, 0), lambda: 1.0, lambda: 0.0)\n        loss_vat = loss_vat * weight_loss_vat\n\n      # Total loss.\n      loss_op = loss_supervised + loss_agr + loss_reg + loss_vat\n\n    # Create accuracy.\n    accuracy = tf.equal(\n        tf.argmax(normalized_predictions_batch, 1), input_labels)\n    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n\n    # Create Tensorboard summaries.\n    if self.enable_summaries:\n      summaries = [\n          tf.summary.scalar(\'loss_supervised\', loss_supervised),\n          tf.summary.scalar(\'loss_agr\', loss_agr),\n          tf.summary.scalar(\'loss_reg\', loss_reg),\n          tf.summary.scalar(\'loss_total\', loss_op)\n      ]\n      self.summary_op = tf.summary.merge(summaries)\n\n    # Create learning rate schedule and optimizer.\n    self.global_step = tf.train.get_or_create_global_step()\n    if self.lr_decay_steps is not None and self.lr_decay_rate is not None:\n      self.lr = tf.train.exponential_decay(\n          self.lr_initial,\n          self.global_step,\n          self.lr_decay_steps,\n          self.lr_decay_rate,\n          staircase=True)\n      self.optimizer = optimizer(self.lr)\n    else:\n      self.optimizer = optimizer(lr_initial)\n\n    # Get trainable variables and compute gradients.\n    grads_and_vars = self.optimizer.compute_gradients(\n        loss_op,\n        tf.trainable_variables(scope=tf.get_default_graph().get_name_scope()))\n    # Clip gradients.\n    if self.gradient_clip:\n      variab = [elem[1] for elem in grads_and_vars]\n      gradients = [elem[0] for elem in grads_and_vars]\n      gradients, _ = tf.clip_by_global_norm(gradients, self.gradient_clip)\n      grads_and_vars = tuple(zip(gradients, variab))\n    with tf.control_dependencies(\n        tf.get_collection(\n            tf.GraphKeys.UPDATE_OPS,\n            scope=tf.get_default_graph().get_name_scope())):\n      train_op = self.optimizer.apply_gradients(\n          grads_and_vars, global_step=self.global_step)\n\n    # Create a saver for model variables.\n    trainable_vars = [v for _, v in grads_and_vars]\n\n    # Put together the subset of variables to save and restore from the best\n    # validation accuracy as we train the agreement model in one cotrain round.\n    vars_to_save = trainable_vars + []\n    if isinstance(weight_decay_var, tf.Variable):\n      vars_to_save.append(weight_decay_var)\n    saver = tf.train.Saver(vars_to_save)\n\n    # Put together all variables that need to be saved in case the process is\n    # interrupted and needs to be restarted.\n    self.vars_to_save = [iter_cls_total, self.global_step]\n    if isinstance(weight_decay_var, tf.Variable):\n      self.vars_to_save.append(weight_decay_var)\n    if self.warm_start:\n      self.vars_to_save.extend([v for v in self.variables])\n\n    # More variables to be initialized after the session is created.\n    self.is_initialized = False\n\n    self.rng = np.random.RandomState(seed)\n    self.input_indices = input_indices\n    self.input_indices_unlabeled = input_indices_unlabeled\n    self.input_labels = input_labels\n    self.predictions = predictions\n    self.normalized_predictions = normalized_predictions\n    self.normalized_predictions_batch = normalized_predictions_batch\n    self.weight_decay_var = weight_decay_var\n    self.weight_decay_update = weight_decay_update\n    self.iter_cls_total = iter_cls_total\n    self.iter_cls_total_update = iter_cls_total_update\n    self.accuracy = accuracy\n    self.train_op = train_op\n    self.loss_op = loss_op\n    self.saver = saver\n    self.batch_size_actual = tf.shape(self.predictions)[0]\n    self.reset_optimizer = tf.variables_initializer(self.optimizer.variables())\n    self.is_train = is_train\n\n  def _create_weight_decay_var(self, weight_decay_initial,\n                               weight_decay_schedule):\n    """"""Creates a weight decay variable that can be updated using a schedule.""""""\n    weight_decay_var = None\n    weight_decay_update = None\n    if weight_decay_schedule is None:\n      if weight_decay_initial is not None:\n        weight_decay_var = tf.constant(\n            weight_decay_initial, dtype=tf.float32, name=\'weight_decay\')\n      else:\n        weight_decay_var = None\n    elif weight_decay_schedule == \'linear\':\n      weight_decay_var = tf.get_variable(\n          name=\'weight_decay\',\n          initializer=tf.constant(\n              weight_decay_initial, name=\'weight_decay_initial\'),\n          use_resource=True,\n          trainable=False)\n      update_rate = weight_decay_initial / float(self.max_num_iter_cotrain)\n      weight_decay_update = weight_decay_var.assign_sub(update_rate)\n    return weight_decay_var, weight_decay_update\n\n  def _create_counter(self):\n    """"""Creates a cummulative iteration counter for all classification steps.""""""\n    iter_cls_total = tf.get_variable(\n        name=\'iter_cls_total\',\n        initializer=tf.constant(0, name=\'iter_cls_total\'),\n        use_resource=True,\n        trainable=False)\n    iter_cls_total_update = iter_cls_total.assign_add(1)\n    return iter_cls_total, iter_cls_total_update\n\n  def _get_agreement_reg_loss(self, data, is_train):\n    """"""Computes the regularization loss coming from the agreement term.\n\n    This is calculated using the following idea: we incur a loss for pairs of\n    samples that should have the same label, but for which the predictions of\n    the classification model are not equal. The loss incured by each pair is\n    proportionate to the distance between the two predictions, as well as the\n    confidence we have that they should agree.\n\n    In the case of pairs where both samples are labeled (LL), the agreement\n    confidence is 1.0. When at least one sample is unlabeled (LU, UU), then we\n    use the agreement model to estimate this confidence.\n\n    Note that for the pairs where a label is available, we can compute this loss\n    wrt. the actual label, instead of the classifier predictions. However, when\n    both samples are labeled (LL), for one of them we use the prediction and for\n    the other the true label -- otherwise there are no gradients to proagate.\n\n    Arguments:\n      data: A CotrainDataset object.\n      is_train: A placeholder for a boolean that specifies if this is function\n        is called as part of model training or inference.\n\n    Returns:\n      The computed agreement loss op.\n    """"""\n    # Select num_pairs_reg pairs of samples from each category LL, LU, UU.\n    # for which to do the regularization.\n    indices_ll_right = tf.placeholder(dtype=tf.int64, shape=(None,))\n    indices_lu_left = tf.placeholder(dtype=tf.int64, shape=(None,))\n    indices_lu_right = tf.placeholder(dtype=tf.int64, shape=(None,))\n    indices_uu_left = tf.placeholder(dtype=tf.int64, shape=(None,))\n    indices_uu_right = tf.placeholder(dtype=tf.int64, shape=(None,))\n\n    # First obtain the features shape from the dataset, and append a batch_size\n    # dimension to it (i.e., `None` to allow for variable batch size).\n    features_shape = [None] + list(data.features_shape)\n    features_ll_right = tf.placeholder(dtype=tf.float32, shape=features_shape)\n    features_lu_left = tf.placeholder(dtype=tf.float32, shape=features_shape)\n    features_lu_right = tf.placeholder(dtype=tf.float32, shape=features_shape)\n    features_uu_left = tf.placeholder(dtype=tf.float32, shape=features_shape)\n    features_uu_right = tf.placeholder(dtype=tf.float32, shape=features_shape)\n\n    labels_ll_left_idx = tf.placeholder(dtype=tf.int64, shape=(None,))\n    labels_ll_right_idx = tf.placeholder(dtype=tf.int64, shape=(None,))\n    labels_lu_left_idx = tf.placeholder(dtype=tf.int64, shape=(None,))\n\n    labels_ll_left = tf.one_hot(labels_ll_left_idx, data.num_classes)\n    labels_lu_left = tf.one_hot(labels_lu_left_idx, data.num_classes)\n\n    with tf.variable_scope(\'predictions\', reuse=True):\n      # Obtain predictions for all nodes in the graph.\n      encoding_all, _, _ = self.model.get_encoding_and_params(\n          inputs=self.features_op,\n          is_train=is_train,\n          support=self.support_op,\n          num_features_nonzero=self.num_features_nonzero_op,\n          update_batch_stats=False)\n      predictions_all, _, _ = self.model.get_predictions_and_params(\n          encoding=encoding_all,\n          is_train=is_train,\n          support=self.support_op,\n          num_features_nonzero=self.num_features_nonzero_op)\n      predictions_all = self.model.normalize_predictions(predictions_all)\n\n      # Select the nodes of interest.\n      predictions_ll_right = tf.gather(predictions_all, indices_ll_right)\n      predictions_lu_right = tf.gather(predictions_all, indices_lu_right)\n      predictions_uu_left = tf.gather(predictions_all, indices_uu_left)\n      predictions_uu_right = tf.gather(predictions_all, indices_uu_right)\n\n    # Compute Euclidean distance between the label distributions that the\n    # classification model predicts for the src and tgt of each pair.\n    # Stop gradients need to be added\n    # The case where there are no more uu or lu\n    # edges at the end of training, so the shapes don\'t match needs fixing.\n    left = tf.concat((labels_ll_left, labels_lu_left, predictions_uu_left),\n                     axis=0)\n    right = tf.concat(\n        (predictions_ll_right, predictions_lu_right, predictions_uu_right),\n        axis=0)\n    dists = tf.reduce_sum(tf.square(left - right), axis=-1)\n\n    # Estimate a weight for each distance, depending on the predictions\n    # of the agreement model. For the labeled samples, we can use the actual\n    # agreement between the labels, no need to estimate.\n    agreement_ll = tf.cast(\n        tf.equal(labels_ll_left_idx, labels_ll_right_idx), dtype=tf.float32)\n    _, agreement_lu, _, _ = self.trainer_agr.create_agreement_prediction(\n        src_features=features_lu_left,\n        tgt_features=features_lu_right,\n        is_train=is_train,\n        src_indices=indices_lu_left,\n        tgt_indices=indices_lu_right)\n    _, agreement_uu, _, _ = self.trainer_agr.create_agreement_prediction(\n        src_features=features_uu_left,\n        tgt_features=features_uu_right,\n        is_train=is_train,\n        src_indices=indices_uu_left,\n        tgt_indices=indices_uu_right)\n    agreement = tf.concat((agreement_ll, agreement_lu, agreement_uu), axis=0)\n    if self.penalize_neg_agr:\n      # Since the agreement is predicting scores between [0, 1], anything\n      # under 0.5 should represent disagreement. Therefore, we want to encourage\n      # agreement whenever the score is > 0.5, otherwise don\'t incur any loss.\n      agreement = tf.nn.relu(agreement - 0.5)\n\n    # Create a Tensor containing the weights assigned to each pair in the\n    # agreement regularization loss, depending on how many samples in the pair\n    # were labeled. This weight can be either reg_weight_ll, reg_weight_lu,\n    # or reg_weight_uu.\n    num_ll = tf.shape(predictions_ll_right)[0]\n    num_lu = tf.shape(predictions_lu_right)[0]\n    num_uu = tf.shape(predictions_uu_left)[0]\n    weights = tf.concat(\n        (self.reg_weight_ll * tf.ones(num_ll,), self.reg_weight_lu *\n         tf.ones(num_lu,), self.reg_weight_uu * tf.ones(num_uu,)),\n        axis=0)\n\n    # Scale each distance by its agreement weight and regularzation weight.\n    loss = tf.reduce_mean(dists * weights * agreement)\n\n    self.indices_ll_right = indices_ll_right\n    self.indices_lu_left = indices_lu_left\n    self.indices_lu_right = indices_lu_right\n    self.indices_uu_left = indices_uu_left\n    self.indices_uu_right = indices_uu_right\n    self.features_ll_right = features_ll_right\n    self.features_lu_left = features_lu_left\n    self.features_lu_right = features_lu_right\n    self.features_uu_left = features_uu_left\n    self.features_uu_right = features_uu_right\n    self.labels_ll_left = labels_ll_left_idx\n    self.labels_ll_right = labels_ll_right_idx\n    self.labels_lu_left = labels_lu_left_idx\n    self.agreement_lu = agreement_lu\n\n    return loss\n\n  def _construct_feed_dict(self,\n                           input_indices,\n                           split,\n                           pair_ll_iterator=None,\n                           pair_lu_iterator=None,\n                           pair_uu_iterator=None,\n                           unlabeled_indices=None):\n    """"""Construct feed dictionary.""""""\n    try:\n      # Select the labels. Use the true, correct labels, at test time, and the\n      # self-labeled ones at train time.\n      labels = (\n          self.data.get_original_labels(input_indices)\n          if split == \'test\' else self.data.get_labels(input_indices))\n      feed_dict = {\n          self.input_indices: input_indices,\n          self.input_labels: labels,\n          self.is_train: split == \'train\',\n          self.features_op: self.features,\n          self.support_op: self.support,\n          self.num_features_nonzero_op: self.num_features_nonzero,\n      }\n      if unlabeled_indices is not None:\n        # This is not None only when using VAT regularization.\n        feed_dict.update({self.input_indices_unlabeled: unlabeled_indices})\n      if pair_ll_iterator is not None:\n        _, indices_tgt, _, features_tgt, labels_src, labels_tgt = next(\n            pair_ll_iterator)\n        feed_dict.update({\n            self.features_ll_right: features_tgt,\n            self.indices_ll_right: indices_tgt,\n            self.labels_ll_left: labels_src,\n            self.labels_ll_right: labels_tgt\n        })\n      if pair_lu_iterator is not None:\n        indices_src, indices_tgt, features_src, features_tgt, labels_src, _ = (\n            next(pair_lu_iterator))\n        feed_dict.update({\n            self.indices_lu_left: indices_src,\n            self.indices_lu_right: indices_tgt,\n            self.features_lu_left: features_src,\n            self.features_lu_right: features_tgt,\n            self.labels_lu_left: labels_src\n        })\n      if pair_uu_iterator is not None:\n        indices_src, indices_tgt, features_src, features_tgt, _, _ = next(\n            pair_uu_iterator)\n        feed_dict.update({\n            self.indices_uu_left: indices_src,\n            self.indices_uu_right: indices_tgt,\n            self.features_uu_left: features_src,\n            self.features_uu_right: features_tgt\n        })\n      return feed_dict\n    except StopIteration:\n      # If the iterator has finished, return None.\n      return None\n\n  def pair_iterator(self, src_indices, tgt_indices, batch_size, data):\n    """"""Iterator over pairs of samples.\n\n    The first element of the pair is selected from the src_indices, and the\n    second element is selected from tgt_indices.\n\n    Arguments:\n      src_indices: Numpy array containing the indices available for the source\n        node.\n      tgt_indices: Numpy array containing the indices available for the tgt\n        node.\n      batch_size: An integer representing the desired batch size.\n      data: A CotrainDataset object used to extract the features and labels.\n\n    Yields:\n      indices_src, indices_tgt, features_src, features_tgt, labels_src,\n      labels_tgt\n    """"""\n\n    def _select_from_pool(indices):\n      """"""Selects batch_size indices from the provided list.""""""\n      num_indices = len(indices)\n      if num_indices > 0:\n        idxs = self.rng.randint(0, high=num_indices, size=(batch_size,))\n        indices_batch = indices[idxs]\n        features_batch = data.get_features(indices_batch)\n        labels_batch = data.get_labels(indices_batch)\n      else:\n        features_shape = [0] + list(data.features_shape)\n        indices_batch = np.zeros(shape=(0,), dtype=np.int64)\n        features_batch = np.zeros(shape=features_shape, dtype=np.float32)\n        labels_batch = np.zeros(shape=(0,), dtype=np.int64)\n      return indices_batch, features_batch, labels_batch\n\n    while True:\n      indices_src, features_src, labels_src = _select_from_pool(src_indices)\n      indices_tgt, features_tgt, labels_tgt = _select_from_pool(tgt_indices)\n      yield (indices_src, indices_tgt, features_src, features_tgt, labels_src,\n             labels_tgt)\n\n  def edge_iterator(self, data, batch_size, labeling):\n    """"""An iterator over graph edges.\n\n    Args:\n      data: A CotrainDataset object used to extract the features and labels.\n      batch_size:  An integer representing the desired batch size.\n      labeling: A string which can be `ll`, `lu` or `uu`, that is used to\n        represent the type of edges to return, where `ll` refers to\n        labeled-labeled, `lu` refers to labeled-unlabeled, and `uu` refers to\n        unlabeled-unlabeled.\n\n    Yields:\n      indices_src, indices_tgt, features_src, features_tgt, labels_src,\n      labels_tgt\n    """"""\n    if labeling == \'ll\':\n      edges = data.get_edges(\n          src_labeled=True, tgt_labeled=True, label_must_match=True)\n    elif labeling == \'lu\':\n      edges = (\n          data.get_edges(src_labeled=True, tgt_labeled=False) +\n          data.get_edges(src_labeled=False, tgt_labeled=True))\n    elif labeling == \'uu\':\n      edges = data.get_edges(src_labeled=False, tgt_labeled=False)\n    else:\n      raise ValueError(\'Unsupported value for parameter `labeling`.\')\n\n    if not edges:\n      indices = np.zeros(shape=(0,), dtype=np.int32)\n      features = np.zeros(\n          shape=[\n              0,\n          ] + list(data.features_shape), dtype=np.float32)\n      labels = np.zeros(shape=(0,), dtype=np.int64)\n      while True:\n        yield (indices, indices, features, features, labels, labels)\n\n    edges = np.stack([(e.src, e.tgt) for e in edges])\n    iterator = batch_iterator(\n        inputs=edges,\n        batch_size=batch_size,\n        shuffle=True,\n        allow_smaller_batch=False,\n        repeat=True)\n\n    for edge in iterator:\n      indices_src = edge[:, 0]\n      indices_tgt = edge[:, 1]\n      features_src = data.get_features(indices_src)\n      features_tgt = data.get_features(indices_tgt)\n      labels_src = data.get_labels(indices_src)\n      labels_tgt = data.get_labels(indices_tgt)\n      yield (indices_src, indices_tgt, features_src, features_tgt, labels_src,\n             labels_tgt)\n\n  def _evaluate(self, indices, split, session, summary_writer):\n    """"""Evaluates the samples with the provided indices.""""""\n    feed_dict_val = self._construct_feed_dict(indices, split)\n    val_acc = session.run(self.accuracy, feed_dict=feed_dict_val)\n\n    if self.enable_summaries:\n      summary = tf.Summary()\n      summary.value.add(\n          tag=\'ClassificationModel/\' + split + \'_acc\', simple_value=val_acc)\n      iter_cls_total = session.run(self.iter_cls_total)\n      summary_writer.add_summary(summary, iter_cls_total)\n      summary_writer.flush()\n\n    return val_acc\n\n  def train(self, data, session=None, **kwargs):\n    """"""Train the classification model on the provided dataset.\n\n    Arguments:\n      data: A CotrainDataset object.\n      session: A TensorFlow session or None.\n      **kwargs: Other keyword arguments.\n\n    Returns:\n      best_test_acc: A float representing the test accuracy at the iteration\n        where the validation accuracy is maximum.\n      best_val_acc: A float representing the best validation accuracy.\n    """"""\n    summary_writer = kwargs[\'summary_writer\']\n    logging.info(\'Training classifier...\')\n\n    if not self.is_initialized:\n      self.is_initialized = True\n    else:\n      if self.weight_decay_update is not None:\n        session.run(self.weight_decay_update)\n        logging.info(\'New weight decay value:  %f\',\n                     session.run(self.weight_decay_var))\n      # Reset the optimizer state (e.g., momentum).\n      session.run(self.reset_optimizer)\n\n    if not self.warm_start:\n      # Re-initialize variables.\n      initializers = [v.initializer for v in self.variables.values()]\n      initializers.append(self.global_step.initializer)\n      session.run(initializers)\n\n    # Construct data iterator.\n    logging.info(\'Training classifier with %d samples...\', data.num_train())\n    train_indices = data.get_indices_train()\n    unlabeled_indices = data.get_indices_unlabeled()\n    val_indices = data.get_indices_val()\n    test_indices = data.get_indices_test()\n\n    # Create iterators for ll, lu, uu pairs of samples for the agreement term.\n    if self.use_graph:\n      pair_ll_iterator = self.edge_iterator(\n          data, batch_size=self.num_pairs_reg, labeling=\'ll\')\n      pair_lu_iterator = self.edge_iterator(\n          data, batch_size=self.num_pairs_reg, labeling=\'lu\')\n      pair_uu_iterator = self.edge_iterator(\n          data, batch_size=self.num_pairs_reg, labeling=\'uu\')\n    else:\n      pair_ll_iterator = self.pair_iterator(train_indices, train_indices,\n                                            self.num_pairs_reg, data)\n      pair_lu_iterator = self.pair_iterator(train_indices, unlabeled_indices,\n                                            self.num_pairs_reg, data)\n      pair_uu_iterator = self.pair_iterator(unlabeled_indices,\n                                            unlabeled_indices,\n                                            self.num_pairs_reg, data)\n\n    step = 0\n    iter_below_tol = 0\n    min_num_iter = self.min_num_iter\n    has_converged = step >= self.max_num_iter\n    prev_loss_val = np.inf\n    best_test_acc = -1\n    best_val_acc = -1\n    checkpoint_saved = False\n    while not has_converged:\n      feed_dict = self._construct_feed_dict(\n          input_indices=train_indices,\n          unlabeled_indices=unlabeled_indices,\n          split=\'train\',\n          pair_ll_iterator=pair_ll_iterator,\n          pair_lu_iterator=pair_lu_iterator,\n          pair_uu_iterator=pair_uu_iterator)\n      if self.enable_summaries and step % self.summary_step == 0:\n        loss_val, summary, iter_cls_total, _ = session.run(\n            [self.loss_op, self.summary_op, self.iter_cls_total, self.train_op],\n            feed_dict=feed_dict)\n        summary_writer.add_summary(summary, iter_cls_total)\n        summary_writer.flush()\n      else:\n        loss_val, _ = session.run((self.loss_op, self.train_op),\n                                  feed_dict=feed_dict)\n\n      # Log the loss, if necessary.\n      if step % self.logging_step == 0:\n        logging.info(\'Classification step %6d | Loss: %10.4f\', step, loss_val)\n\n      # Evaluate, if necessary.\n      if step % self.eval_step == 0:\n        val_acc = self._evaluate(val_indices, \'val\', session, summary_writer)\n        test_acc = self._evaluate(test_indices, \'test\', session, summary_writer)\n\n        if step % self.logging_step == 0 or val_acc > best_val_acc:\n          logging.info(\n              \'Classification step %6d | Loss: %10.4f | val_acc: %10.4f | \'\n              \'test_acc: %10.4f\', step, loss_val, val_acc, test_acc)\n        if val_acc > best_val_acc:\n          best_val_acc = val_acc\n          best_test_acc = test_acc\n          if self.checkpoint_path:\n            self.saver.save(\n                session, self.checkpoint_path, write_meta_graph=False)\n            checkpoint_saved = True\n          # Go for at least num_iter_after_best_val more iterations.\n          min_num_iter = max(self.min_num_iter,\n                             step + self.num_iter_after_best_val)\n          logging.info(\n              \'Achieved best validation. \'\n              \'Extending to at least %d iterations...\', min_num_iter)\n\n      step += 1\n      has_converged, iter_below_tol = self.check_convergence(\n          prev_loss_val,\n          loss_val,\n          step,\n          self.max_num_iter,\n          iter_below_tol,\n          min_num_iter=min_num_iter)\n      session.run(self.iter_cls_total_update)\n      prev_loss_val = loss_val\n\n    # Return to the best model.\n    if checkpoint_saved:\n      logging.info(\'Restoring best model...\')\n      self.saver.restore(session, self.checkpoint_path)\n\n    return best_test_acc, best_val_acc\n\n  def predict(self, session, indices, is_train):\n    """"""Make predictions for the provided sample indices.""""""\n    if not indices:\n      return np.zeros((0, self.data.num_classes), dtype=np.float32)\n    feed_dict = {\n        self.input_indices: indices,\n        self.is_train: is_train,\n        self.num_features_nonzero_op: self.num_features_nonzero,\n        self.features_op: self.features,\n        self.support_op: self.support\n    }\n    predictions = session.run(\n        self.normalized_predictions_batch, feed_dict=feed_dict)\n    return predictions\n'"
research/gam/gam/trainer/trainer_cotrain.py,11,"b'# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Trains a classification model and a label agreement model using co-training.\n\nThis class makes use of a Trainer for the classification model and a trainer\nfor the agreement model, and alternatively trains each of them. After each\niteration some unlabeled samples are labeled using the classification model,\nsuch that in the next iteration both models are re-trained using more labeled\ndata.\n\nThroughout this file, the suffix ""_cls"" refers to the classification model, and\n""_agr"" to the agreement model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\n\nfrom ..data.dataset import CotrainDataset\nfrom ..models.gcn import GCN\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom .trainer_agreement import TrainerAgreement\nfrom .trainer_agreement import TrainerAgreementAlwaysAgree\nfrom .trainer_agreement import TrainerPerfectAgreement\nfrom .trainer_base import Trainer\nfrom .trainer_classification import TrainerClassification\nfrom .trainer_classification import TrainerPerfectClassification\nfrom .trainer_classification_gcn import TrainerClassificationGCN\n\n\nclass TrainerCotraining(Trainer):\n  """"""Trainer for a co-training model with agreement.\n\n  Attributes:\n    model_cls: An object whose type is a subclass of Model, representing the\n      model for the sample classifier.\n    model_agr: An object whose type is a subclass of Model, representing the\n      model for the agreement model.\n    max_num_iter_cotrain: An integer representing the maximum number of cotrain\n      iterations to perform.\n    min_num_iter_cls: An integer representing the minimum number of iterations\n      to train the classification model for.\n    max_num_iter_cls: An integer representing the maximum number of iterations\n      to train the classification model for.\n    num_iter_after_best_val_cls: An integer representing the number of extra\n      iterations to perform after improving the validation accuracy of the\n      classification model.\n    min_num_iter_agr: An integer representing the minimum number of iterations\n      to train the agreement model for.\n    max_num_iter_agr: An integer representing the maximum number of iterations\n      to train the agreement model.\n    num_iter_after_best_val_agr: An integer representing the number of extra\n      iterations to perform after improving the agreement validation accuracy.\n    num_samples_to_label: Maximum number of samples to self-label after each\n      cotrain iteration, provided that they have confidence higher than the\n      min_confidence_new_label threshold.\n    min_confidence_new_label: A float number between [0, 1] representing the\n      minimum confidence the prediction for an unlabeled sample needs to have in\n      order to allow it to be self-labeled. The confidence is the maximum\n      probability the classification model assigns to any of the classes.\n    keep_label_proportions: A boolean specifying whether to choose samples for\n      self-labeling such that we maintain the original label proportions.\n    num_warm_up_iter_agr: An integer representing the number of times we need to\n      train the agreement model (i.e. number of cotrain iterations that train\n      the agreement) before we start using it in the classification model\'s\n      loss. While the agreement is not warmed up, the agreement model will\n      always predict either disagreement, or agreement, by default, depending on\n      the argument `agree_by_default`.\n    optimizer: An optimizer.\n    gradient_clip: A float number representing the maximum gradient norm allowed\n      if we do gradient clipping. If None, no gradient clipping is performed.\n    batch_size_agr: An integer representing the batch size of the agreement\n      model.\n    batch_size_cls: An integer representing the batch size of the classification\n      model. This is used for the supervised component of the loss and for\n      evaluation.\n    learning_rate_cls: A float representing the learning rate used when training\n      the classification model.\n    learning_rate_agr: A float representing the learning rate used when training\n      the agreement model.\n    warm_start_cls: Boolean specifying if the classification model is trained\n      from scratch in every cotrain itertion (if False), or if it continues from\n      the parameter values in the previous cotrain iteration (if True).\n    warm_start_agr: Boolean specifying if the agreement model is trained from\n      scratch in every cotrain itertion (if False), or if it continues from the\n      parameter values in the previous cotrain iteration (if True).\n    enable_summaries: Boolean specifying whether to write TensorBoard summaries\n      for the cotrain progress.\n    enable_summaries_per_model: Boolean specifying whether to write TensorBoard\n      summaries for the classification and agreement model progress.\n    summary_dir: Directory path where to save the Tensorflow summaries.\n    summary_step_cls: Integer representing the number of iterations after which\n      to write TensorFlow summaries for the classification model.\n    summary_step_agr: Integer representing the number of iterations after which\n      to write TensorFlow summaries for the agreement model.\n    logging_step_cls: Integer representing the number of iterations after which\n      to log the loss and other training metrics for the classification model.\n    logging_step_agr: Integer representing the number of iterations after which\n      to log the loss and other training metrics for the agreement model.\n    eval_step_cls: Integer representing the number of iterations after which to\n      evaluate the classification model.\n    eval_step_agr: Integer representing the number of iterations after which to\n      evaluate the agreement model.\n    checkpoints_step: Integer representing the number of iterations after which\n      to save checkpoints.\n    checkpoints_dir: Directory where to save checkpoints.\n    data_dir: Directory where to write some files that contain self-labeled data\n      backup.\n    abs_loss_chg_tol: A float representing the absolute tolerance for checking\n      if the training loss has converged. If the difference between the current\n      loss and previous loss is less than `abs_loss_chg_tol`, we count this\n      iteration towards convergence (see `loss_chg_iter_below_tol`).\n    rel_loss_chg_tol: A float representing the relative tolerance for checking\n      if the training loss has converged. If the ratio between the current loss\n      and previous loss is less than `rel_loss_chg_tol`, we count this iteration\n      towards convergence (see `loss_chg_iter_below_tol`).\n    loss_chg_iter_below_tol: An integer representing the number of consecutive\n      iterations that pass the convergence criteria before stopping training.\n    use_perfect_agr: Boolean specifying whether to use a perfect agreement model\n      that peeks at the correct test labels (for debugging only).\n    use_perfect_cls: Boolean specifying whether to use a perfect classification\n      model that peeks at the correct test labels (for debugging only).\n    ratio_valid_agr: Ratio of the labeled sample pairs to use for validation\n      whent training the agreement model.\n    max_samples_valid_agr: Maximum number of sample pairs to use for validation\n      whent training the agreement model.\n    weight_decay_cls: Weight for the weight decay term in the classification\n      model loss.\n    weight_decay_schedule_cls: Schedule how to adjust the classification weight\n      decay weight after every cotrain iteration.\n    weight_decay_agr: Weight for the weight decay term in the agreement model\n      loss.\n    weight_decay_schedule_agr: Schedule how to adjust the agreement weight decay\n      weight after every cotrain iteration.\n    reg_weight_ll: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      labeled-labeled pairs of samples.\n    reg_weight_lu: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      labeled-unlabeled pairs of samples.\n    reg_weight_uu: A float representing the weight of the agreement loss term\n      component of the classification model loss function, between\n      unlabeled-unlabeled pairs of samples.\n    num_pairs_reg: An integer representing the number of sample pairs of each\n      type (LL, LU, UU) to include in each computation of the classification\n      model loss.\n    reg_weight_vat: A float representing the weight of the virtual adversarial\n      training (VAT) regularization loss in the classification model loss\n      function.\n    use_ent_min: A boolean specifying whether to use entropy regularization with\n      VAT.\n    penalize_neg_agr: Whether to not only encourage agreement between samples\n      that the agreement model believes should have the same label, but also\n      penalize agreement when two samples agree when the agreement model\n      predicts they should disagree.\n    use_l2_cls: Whether to use L2 loss for classification, as opposed to the\n      whichever loss is specified in the provided model_cls.\n    first_iter_original: A boolean specifying whether the first cotrain\n      iteration trains the original classification model (with no agreement\n      term). We do this to evaluate how well a baseline model would do without\n      the agreement. If true, there is no self-labeling after the first\n      iteration, which trains original model. Self-labeling will be used only in\n      the iterations that do include the agreement term.\n    inductive: Boolean specifying whether this is an inductive or transductive\n      setting. If inductive, then the validation and test labels are never seen\n      when training the classification model. If transductive, the inputs of the\n      test and validation samples are available at training time and can be used\n      in the agreement loss term of the classification model as unsupervised\n      regularization, and can also be labeled via self-labeling.\n    seed: An integer representing the seed for the random number generator used\n      when selecting batches of samples.\n    eval_acc_pred_by_agr: Boolean specifying whether to evaluate the accuracy of\n      a model that uses our trained agreement model to make predictions for the\n      test samples, in a way similar to k-nearest neighbors, where the distance\n      is given by the agreement model predictions.\n    num_neighbors_pred_by_agr: An integer representing the number of neighbors\n      to use when predicting by agreement. Note that this needs to be at least\n      as much as the number of classes.\n    load_from_checkpoint: A boolean specifying whethe the trained models are\n      loaded from checkpoint, if one is available. If False, the models are\n      always trained from scratch.\n    use_graph: Boolean specifying whether to use to apply the agreement model on\n      the graph edges, or otherwise use random pairs of samples.\n    always_agree: Whether the agreement model should return 1.0 always (i.e. the\n      samples always agree), to simulate the Neural Graph Machines model.\n    add_negative_edges_agr:\n  """"""\n\n  def __init__(self,\n               model_cls,\n               model_agr,\n               max_num_iter_cotrain,\n               min_num_iter_cls,\n               max_num_iter_cls,\n               num_iter_after_best_val_cls,\n               min_num_iter_agr,\n               max_num_iter_agr,\n               num_iter_after_best_val_agr,\n               num_samples_to_label,\n               min_confidence_new_label=0.0,\n               keep_label_proportions=False,\n               num_warm_up_iter_agr=1,\n               optimizer=tf.train.AdamOptimizer,\n               gradient_clip=None,\n               batch_size_agr=128,\n               batch_size_cls=128,\n               learning_rate_cls=1e-3,\n               learning_rate_agr=1e-3,\n               warm_start_cls=False,\n               warm_start_agr=False,\n               enable_summaries=True,\n               enable_summaries_per_model=False,\n               summary_dir=None,\n               summary_step_cls=1000,\n               summary_step_agr=1000,\n               logging_step_cls=1,\n               logging_step_agr=1,\n               eval_step_cls=1,\n               eval_step_agr=1,\n               checkpoints_step=None,\n               checkpoints_dir=None,\n               data_dir=None,\n               abs_loss_chg_tol=1e-10,\n               rel_loss_chg_tol=1e-7,\n               loss_chg_iter_below_tol=30,\n               use_perfect_agr=False,\n               use_perfect_cls=False,\n               ratio_valid_agr=0,\n               max_samples_valid_agr=None,\n               weight_decay_cls=None,\n               weight_decay_schedule_cls=None,\n               weight_decay_agr=None,\n               weight_decay_schedule_agr=None,\n               reg_weight_ll=0,\n               reg_weight_lu=0,\n               reg_weight_uu=0,\n               num_pairs_reg=100,\n               reg_weight_vat=0,\n               use_ent_min=False,\n               penalize_neg_agr=False,\n               use_l2_cls=True,\n               first_iter_original=True,\n               inductive=False,\n               seed=None,\n               eval_acc_pred_by_agr=False,\n               num_neighbors_pred_by_agr=20,\n               lr_decay_rate_cls=None,\n               lr_decay_steps_cls=None,\n               lr_decay_rate_agr=None,\n               lr_decay_steps_agr=None,\n               load_from_checkpoint=False,\n               use_graph=False,\n               always_agree=False,\n               add_negative_edges_agr=False):\n    assert not enable_summaries or (enable_summaries and\n                                    summary_dir is not None)\n    assert checkpoints_step is None or (checkpoints_step is not None and\n                                        checkpoints_dir is not None)\n    super(TrainerCotraining, self).__init__(\n        model=None,\n        abs_loss_chg_tol=abs_loss_chg_tol,\n        rel_loss_chg_tol=rel_loss_chg_tol,\n        loss_chg_iter_below_tol=loss_chg_iter_below_tol)\n    self.model_cls = model_cls\n    self.model_agr = model_agr\n    self.max_num_iter_cotrain = max_num_iter_cotrain\n    self.min_num_iter_cls = min_num_iter_cls\n    self.max_num_iter_cls = max_num_iter_cls\n    self.num_iter_after_best_val_cls = num_iter_after_best_val_cls\n    self.min_num_iter_agr = min_num_iter_agr\n    self.max_num_iter_agr = max_num_iter_agr\n    self.num_iter_after_best_val_agr = num_iter_after_best_val_agr\n    self.num_samples_to_label = num_samples_to_label\n    self.min_confidence_new_label = min_confidence_new_label\n    self.keep_label_proportions = keep_label_proportions\n    self.num_warm_up_iter_agr = num_warm_up_iter_agr\n    self.optimizer = optimizer\n    self.gradient_clip = gradient_clip\n    self.batch_size_agr = batch_size_agr\n    self.batch_size_cls = batch_size_cls\n    self.learning_rate_cls = learning_rate_cls\n    self.learning_rate_agr = learning_rate_agr\n    self.warm_start_cls = warm_start_cls\n    self.warm_start_agr = warm_start_agr\n    self.enable_summaries = enable_summaries\n    self.enable_summaries_per_model = enable_summaries_per_model\n    self.summary_step_cls = summary_step_cls\n    self.summary_step_agr = summary_step_agr\n    self.summary_dir = summary_dir\n    self.logging_step_cls = logging_step_cls\n    self.logging_step_agr = logging_step_agr\n    self.eval_step_cls = eval_step_cls\n    self.eval_step_agr = eval_step_agr\n    self.checkpoints_step = checkpoints_step\n    self.checkpoints_dir = checkpoints_dir\n    self.data_dir = data_dir\n    self.use_perfect_agr = use_perfect_agr\n    self.use_perfect_cls = use_perfect_cls\n    self.ratio_valid_agr = ratio_valid_agr\n    self.max_samples_valid_agr = max_samples_valid_agr\n    self.weight_decay_cls = weight_decay_cls\n    self.weight_decay_schedule_cls = weight_decay_schedule_cls\n    self.weight_decay_agr = weight_decay_agr\n    self.weight_decay_schedule_agr = weight_decay_schedule_agr\n    self.reg_weight_ll = reg_weight_ll\n    self.reg_weight_lu = reg_weight_lu\n    self.reg_weight_uu = reg_weight_uu\n    self.num_pairs_reg = num_pairs_reg\n    self.reg_weight_vat = reg_weight_vat\n    self.use_ent_min = use_ent_min\n    self.penalize_neg_agr = penalize_neg_agr\n    self.use_l2_classif = use_l2_cls\n    self.first_iter_original = first_iter_original\n    self.inductive = inductive\n    self.seed = seed\n    self.eval_acc_pred_by_agr = eval_acc_pred_by_agr\n    self.num_neighbors_pred_by_agr = num_neighbors_pred_by_agr\n    self.lr_decay_rate_cls = lr_decay_rate_cls\n    self.lr_decay_steps_cls = lr_decay_steps_cls\n    self.lr_decay_rate_agr = lr_decay_rate_agr\n    self.lr_decay_steps_agr = lr_decay_steps_agr\n    self.load_from_checkpoint = load_from_checkpoint\n    self.use_graph = use_graph\n    self.always_agree = always_agree\n    self.add_negative_edges_agr = add_negative_edges_agr\n\n  def _select_samples_to_label(self, data, trainer_cls, session):\n    """"""Selects which samples to label next.\n\n    Arguments:\n      data: A CotrainData object.\n      trainer_cls: A TrainerClassification object.\n      session: A TensorFlow Session.\n\n    Returns:\n      selected_samples: numpy array containing the indices of the samples to be\n        labeled.\n      selected_labels: numpy array containing the indices of the labels to\n        assign to each of the selected nodes.\n    """"""\n    # Select the candidate samples for self-labeling, and make predictions.\n    # Remove the validation samples from the unlabeled data, if there, to avoid\n    # self-labeling them.\n    indices_unlabeled = data.get_indices_unlabeled()\n    val_ind = set(data.get_indices_val())\n    indices_unlabeled = np.asarray(\n        [ind for ind in indices_unlabeled if ind not in val_ind])\n    predictions = trainer_cls.predict(\n        session, indices_unlabeled, is_train=False)\n\n    # Select most confident nodes. Compute confidence and most confident label,\n    # which will be used as the new label.\n    predicted_label = np.argmax(predictions, axis=-1)\n    confidence = predictions[np.arange(predicted_label.shape[0]),\n                             predicted_label]\n    # Sort from most confident to least confident.\n    indices_sorted = np.argsort(confidence)[::-1]\n    indices_unlabeled = indices_unlabeled[indices_sorted]\n    confidence = confidence[indices_sorted]\n    predicted_label = predicted_label[indices_sorted]\n\n    # Keep only samples that have at least min_confidence_new_label confidence.\n    confident_indices = np.argwhere(\n        confidence > self.min_confidence_new_label)[:, 0]\n    if confident_indices.shape[0] == 0:\n      logging.info(\n          \'No unlabeled nodes with confidence > %.2f. \'\n          \'Skipping self-labeling...\', self.min_confidence_new_label)\n      selected_samples = np.zeros((0,), dtype=np.int64)\n      selected_labels = np.zeros((0,), dtype=np.int64)\n      return selected_samples, selected_labels\n\n    if data.keep_label_proportions:\n      # Pick the top num_samples_to_label most confident nodes, while making\n      # sure the ratio of the labels are kept.\n      # First keep only nodes which achieve the min required confidence.\n      num_confident = len(confident_indices)\n      nodes_with_min_conf = indices_unlabeled[:num_confident]\n      labels_with_min_conf = predicted_label[:num_confident]\n      # Out of these, select the desired number of samples per class,\n      # according to class proportions.\n      selected_samples = []\n      selected_labels = []\n      for label, prop in data.label_prop.items():\n        num_samples_to_select = int(prop * self.num_samples_to_label)\n        label_idxs = np.where(labels_with_min_conf == label)[0]\n        if len(label_idxs) <= num_samples_to_select:\n          # Select all available samples labeled with this label.\n          selected_samples.append(nodes_with_min_conf[label_idxs])\n          selected_labels.append(labels_with_min_conf[label_idxs])\n        elif num_samples_to_select > 0:\n          # Select the first ones, since they are sorted by confidence.\n          selected_samples.append(\n              nodes_with_min_conf[label_idxs][:num_samples_to_select])\n          selected_labels.append(\n              labels_with_min_conf[label_idxs][:num_samples_to_select])\n      selected_samples = np.concatenate(selected_samples)\n      selected_labels = np.concatenate(selected_labels)\n    else:\n      # Pick the top num_samples_to_label most confident nodes,\n      # irrespective of their labels.\n      idx = np.amax(confident_indices)\n      max_idx = min(self.num_samples_to_label - 1, idx)\n      selected_samples = indices_unlabeled[:max_idx + 1]\n      selected_labels = predicted_label[:max_idx + 1]\n\n    return selected_samples, selected_labels\n\n  def _extend_label_set(self, data, trainer_cls, session):\n    """"""Extend labeled set by self-labeling with most confident predictions.""""""\n    # Select which nodes to label next, and predict their labels.\n    selected_samples, selected_labels = self._select_samples_to_label(\n        data, trainer_cls, session)\n    # Replace the labels of the new nodes with the predicted labels.\n    if selected_samples.shape[0] > 0:\n      data.label_samples(selected_samples, selected_labels)\n    return selected_samples\n\n  def train(self, data, **kwargs):\n    # Create a wrapper around the dataset, that also accounts for some\n    # cotrain specific attributes and functions.\n    data = CotrainDataset(\n        data,\n        keep_label_proportions=self.keep_label_proportions,\n        inductive=self.inductive)\n\n    if os.path.exists(self.data_dir) and self.load_from_checkpoint:\n      # If this session is restored from a previous run, then we load the\n      # self-labeled data from the last checkpoint.\n      logging.info(\'Number of labeled samples before restoring: %d\',\n                   data.num_train())\n      logging.info(\'Restoring self-labeled data from %s...\', self.data_dir)\n      data.restore_state_from_file(self.data_dir)\n      logging.info(\'Number of labeled samples after restoring: %d\',\n                   data.num_train())\n\n    # Build graph.\n    logging.info(\'Building graph...\')\n\n    # Create a iteration counter.\n    iter_cotrain, iter_cotrain_update = self._create_counter()\n\n    if self.use_perfect_agr:\n      # A perfect agreement model used for model.\n      trainer_agr = TrainerPerfectAgreement(data=data)\n    else:\n      with tf.variable_scope(\'AgreementModel\'):\n        if self.always_agree:\n          trainer_agr = TrainerAgreementAlwaysAgree(data=data)\n        else:\n          trainer_agr = TrainerAgreement(\n              model=self.model_agr,\n              data=data,\n              optimizer=self.optimizer,\n              gradient_clip=self.gradient_clip,\n              min_num_iter=self.min_num_iter_agr,\n              max_num_iter=self.max_num_iter_agr,\n              num_iter_after_best_val=self.num_iter_after_best_val_agr,\n              max_num_iter_cotrain=self.max_num_iter_cotrain,\n              num_warm_up_iter=self.num_warm_up_iter_agr,\n              warm_start=self.warm_start_agr,\n              batch_size=self.batch_size_agr,\n              enable_summaries=self.enable_summaries_per_model,\n              summary_step=self.summary_step_agr,\n              summary_dir=self.summary_dir,\n              logging_step=self.logging_step_agr,\n              eval_step=self.eval_step_agr,\n              abs_loss_chg_tol=self.abs_loss_chg_tol,\n              rel_loss_chg_tol=self.rel_loss_chg_tol,\n              loss_chg_iter_below_tol=self.loss_chg_iter_below_tol,\n              checkpoints_dir=self.checkpoints_dir,\n              weight_decay=self.weight_decay_agr,\n              weight_decay_schedule=self.weight_decay_schedule_agr,\n              agree_by_default=False,\n              percent_val=self.ratio_valid_agr,\n              max_num_samples_val=self.max_samples_valid_agr,\n              seed=self.seed,\n              lr_decay_rate=self.lr_decay_rate_agr,\n              lr_decay_steps=self.lr_decay_steps_agr,\n              lr_initial=self.learning_rate_agr,\n              use_graph=self.use_graph,\n              add_negative_edges=self.add_negative_edges_agr)\n\n    if self.use_perfect_cls:\n      # A perfect classification model used for debugging purposes.\n      trainer_cls = TrainerPerfectClassification(data=data)\n    else:\n      with tf.variable_scope(\'ClassificationModel\'):\n        trainer_cls_class = (\n            TrainerClassificationGCN\n            if isinstance(self.model_cls, GCN) else TrainerClassification)\n        trainer_cls = trainer_cls_class(\n            model=self.model_cls,\n            data=data,\n            trainer_agr=trainer_agr,\n            optimizer=self.optimizer,\n            gradient_clip=self.gradient_clip,\n            batch_size=self.batch_size_cls,\n            min_num_iter=self.min_num_iter_cls,\n            max_num_iter=self.max_num_iter_cls,\n            num_iter_after_best_val=self.num_iter_after_best_val_cls,\n            max_num_iter_cotrain=self.max_num_iter_cotrain,\n            reg_weight_ll=self.reg_weight_ll,\n            reg_weight_lu=self.reg_weight_lu,\n            reg_weight_uu=self.reg_weight_uu,\n            num_pairs_reg=self.num_pairs_reg,\n            reg_weight_vat=self.reg_weight_vat,\n            use_ent_min=self.use_ent_min,\n            enable_summaries=self.enable_summaries_per_model,\n            summary_step=self.summary_step_cls,\n            summary_dir=self.summary_dir,\n            logging_step=self.logging_step_cls,\n            eval_step=self.eval_step_cls,\n            abs_loss_chg_tol=self.abs_loss_chg_tol,\n            rel_loss_chg_tol=self.rel_loss_chg_tol,\n            loss_chg_iter_below_tol=self.loss_chg_iter_below_tol,\n            warm_start=self.warm_start_cls,\n            checkpoints_dir=self.checkpoints_dir,\n            weight_decay=self.weight_decay_cls,\n            weight_decay_schedule=self.weight_decay_schedule_cls,\n            penalize_neg_agr=self.penalize_neg_agr,\n            use_l2_classif=self.use_l2_classif,\n            first_iter_original=self.first_iter_original,\n            seed=self.seed,\n            iter_cotrain=iter_cotrain,\n            lr_decay_rate=self.lr_decay_rate_cls,\n            lr_decay_steps=self.lr_decay_steps_cls,\n            lr_initial=self.learning_rate_cls,\n            use_graph=self.use_graph)\n\n    # Create a saver which saves only the variables that we would need to\n    # restore in case the training process is restarted.\n    vars_to_save = [iter_cotrain] + trainer_agr.vars_to_save + \\\n                   trainer_cls.vars_to_save\n    saver = tf.train.Saver(vars_to_save)\n\n    # Create a TensorFlow session. We allow soft placement in order to place\n    # any supported ops on GPU. The allow_growth option lets our process\n    # progressively use more gpu memory, per need basis, as opposed to\n    # allocating it all from the beginning.\n    config = tf.ConfigProto(allow_soft_placement=True)\n    config.gpu_options.allow_growth = True\n    session = tf.Session(config=config)\n\n    # Create a Tensorflow summary writer, shared by all models.\n    summary_writer = tf.summary.FileWriter(self.summary_dir, session.graph)\n\n    # Initialize the values of all variables and the train dataset iterator.\n    session.run(tf.global_variables_initializer())\n\n    # If a checkpoint with the variables already exists, we restore them.\n    if self.checkpoints_dir:\n      checkpts_path_cotrain = os.path.join(self.checkpoints_dir, \'cotrain.ckpt\')\n      if os.path.exists(checkpts_path_cotrain):\n        if self.load_from_checkpoint:\n          saver.restore(session, checkpts_path_cotrain)\n      else:\n        os.makedirs(checkpts_path_cotrain)\n    else:\n      checkpts_path_cotrain = None\n\n    # Create a progress bar showing how many samples are labeled.\n    pbar = tqdm(\n        total=data.num_samples - data.num_train(), desc=\'self-labeled nodes\')\n\n    logging.info(\'Starting co-training...\')\n    step = session.run(iter_cotrain)\n    stop = step >= self.max_num_iter_cotrain\n    best_val_acc = -1\n    test_acc_at_best = -1\n    iter_at_best = -1\n    while not stop:\n      logging.info(\'----------------- Cotrain step %6d -----------------\', step)\n      # Train the agreement model.\n      if self.first_iter_original and step == 0:\n        logging.info(\'First iteration trains the original classifier.\'\n                     \'No need to train the agreement model.\')\n        val_acc_agree = None\n        acc_pred_by_agr = None\n      else:\n        val_acc_agree = trainer_agr.train(\n            data, session=session, summary_writer=summary_writer)\n\n        if self.eval_acc_pred_by_agr:\n          # Evaluate the prediction accuracy by a majority vote model using the\n          # agreement model.\n          logging.info(\'Computing agreement majority vote predictions on \'\n                       \'test data...\')\n          acc_pred_by_agr = trainer_agr.predict_label_by_agreement(\n              session, data.get_indices_test(), self.num_neighbors_pred_by_agr)\n        else:\n          acc_pred_by_agr = None\n\n      # Train classification model.\n      test_acc, val_acc = trainer_cls.train(\n          data, session=session, summary_writer=summary_writer)\n\n      if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        test_acc_at_best = test_acc\n        iter_at_best = step\n\n      if self.enable_summaries:\n        summary = tf.Summary()\n        summary.value.add(tag=\'cotrain/test_acc\', simple_value=test_acc)\n        summary.value.add(tag=\'cotrain/val_acc\', simple_value=val_acc)\n        if val_acc_agree is not None:\n          summary.value.add(\n              tag=\'cotrain/val_acc_agree\', simple_value=val_acc_agree)\n        if acc_pred_by_agr is not None:\n          summary.value.add(\n              tag=\'cotrain/acc_predict_by_agreement\',\n              simple_value=acc_pred_by_agr)\n        summary_writer.add_summary(summary, step)\n        summary_writer.flush()\n\n      logging.info(\n          \'--------- Cotrain step %6d | Accuracy val: %10.4f | \'\n          \'Accuracy test: %10.4f ---------\', step, val_acc, test_acc)\n\n      if self.first_iter_original and step == 0:\n        logging.info(\'No self-labeling because the first iteration trains the \'\n                     \'original classifier for evaluation purposes.\')\n        step += 1\n      else:\n        # Extend labeled set by self-labeling.\n        logging.info(\'Self-labeling...\')\n        selected_samples = self._extend_label_set(data, trainer_cls, session)\n\n        # If no new data points are added to the training set, stop.\n        num_new_labels = len(selected_samples)\n        pbar.update(num_new_labels)\n        if num_new_labels > 0:\n          data.compute_dataset_statistics(selected_samples, summary_writer,\n                                          step)\n        else:\n          logging.info(\'No new samples labeled. Stopping...\')\n          stop = True\n\n        step += 1\n        stop |= step >= self.max_num_iter_cotrain\n\n        # Save model and dataset state in case of process preemption.\n        if self.checkpoints_step and step % self.checkpoints_step == 0:\n          self._save_state(saver, session, data, checkpts_path_cotrain)\n\n      session.run(iter_cotrain_update)\n      logging.info(\'________________________________________________________\')\n\n    logging.info(\n        \'Best validation acc: %.4f, corresponding test acc: %.4f at \'\n        \'iteration %d\', best_val_acc, test_acc_at_best, iter_at_best)\n    pbar.close()\n\n  def _create_counter(self):\n    """"""Creates a cotrain iteration counter.""""""\n    iter_cotrain = tf.get_variable(\n        name=\'iter_cotrain\',\n        initializer=tf.constant(0, name=\'iter_cotrain\'),\n        use_resource=True,\n        trainable=False)\n    iter_cotrain_update = iter_cotrain.assign_add(1)\n    return iter_cotrain, iter_cotrain_update\n\n  def _save_state(self, saver, session, data, checkpts_path):\n    """"""Saves the model and dataset state to files.""""""\n    # Save variable state\n    if checkpts_path:\n      logging.info(\'Saving cotrain checkpoint at %s.\', checkpts_path)\n      saver.save(session, checkpts_path, write_meta_graph=False)\n\n    # Save dataset state.\n    if self.data_dir:\n      logging.info(\'Saving self-labeled dataset backup.\')\n      data.save_state_to_file(self.data_dir)\n'"
