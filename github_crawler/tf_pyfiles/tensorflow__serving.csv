file_path,api_count,code
tensorflow_serving/apis/model_service_pb2.py,0,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/model_service.proto\n# To regenerate run\n# python -m grpc.tools.protoc --python_out=. --grpc_python_out=. -I. tensorflow_serving/apis/model_service.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow_serving.apis import get_model_status_pb2 as tensorflow__serving_dot_apis_dot_get__model__status__pb2\nfrom tensorflow_serving.apis import model_management_pb2 as tensorflow__serving_dot_apis_dot_model__management__pb2\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n    name=\'tensorflow_serving/apis/model_service.proto\',\n    package=\'tensorflow.serving\',\n    syntax=\'proto3\',\n    serialized_pb=_b(\n        \'\\n+tensorflow_serving/apis/model_service.proto\\x12\\x12tensorflow.serving\\x1a.tensorflow_serving/apis/get_model_status.proto\\x1a.tensorflow_serving/apis/model_management.proto2\\xe7\\x01\\n\\x0cModelService\\x12g\\n\\x0eGetModelStatus\\x12).tensorflow.serving.GetModelStatusRequest\\x1a*.tensorflow.serving.GetModelStatusResponse\\x12n\\n\\x19HandleReloadConfigRequest\\x12\\\'.tensorflow.serving.ReloadConfigRequest\\x1a(.tensorflow.serving.ReloadConfigResponseB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\'\n    ),\n    dependencies=[\n        tensorflow__serving_dot_apis_dot_get__model__status__pb2.DESCRIPTOR,\n        tensorflow__serving_dot_apis_dot_model__management__pb2.DESCRIPTOR,\n    ])\n\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n\n_MODELSERVICE = _descriptor.ServiceDescriptor(\n    name=\'ModelService\',\n    full_name=\'tensorflow.serving.ModelService\',\n    file=DESCRIPTOR,\n    index=0,\n    options=None,\n    serialized_start=164,\n    serialized_end=395,\n    methods=[\n        _descriptor.MethodDescriptor(\n            name=\'GetModelStatus\',\n            full_name=\'tensorflow.serving.ModelService.GetModelStatus\',\n            index=0,\n            containing_service=None,\n            input_type=tensorflow__serving_dot_apis_dot_get__model__status__pb2.\n            _GETMODELSTATUSREQUEST,\n            output_type=tensorflow__serving_dot_apis_dot_get__model__status__pb2.\n            _GETMODELSTATUSRESPONSE,\n            options=None,\n        ),\n        _descriptor.MethodDescriptor(\n            name=\'HandleReloadConfigRequest\',\n            full_name=\n            \'tensorflow.serving.ModelService.HandleReloadConfigRequest\',\n            index=1,\n            containing_service=None,\n            input_type=tensorflow__serving_dot_apis_dot_model__management__pb2.\n            _RELOADCONFIGREQUEST,\n            output_type=tensorflow__serving_dot_apis_dot_model__management__pb2.\n            _RELOADCONFIGRESPONSE,\n            options=None,\n        ),\n    ])\n_sym_db.RegisterServiceDescriptor(_MODELSERVICE)\n\nDESCRIPTOR.services_by_name[\'ModelService\'] = _MODELSERVICE\n\n# @@protoc_insertion_point(module_scope)\n'"
tensorflow_serving/apis/model_service_pb2_grpc.py,0,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/model_service.proto\n# To regenerate run\n# python -m grpc.tools.protoc --python_out=. --grpc_python_out=. -I. tensorflow_serving/apis/model_service.proto\n\nimport grpc\n\nfrom tensorflow_serving.apis import get_model_status_pb2 as tensorflow__serving_dot_apis_dot_get__model__status__pb2\nfrom tensorflow_serving.apis import model_management_pb2 as tensorflow__serving_dot_apis_dot_model__management__pb2\n\n\nclass ModelServiceStub(object):\n  """"""ModelService provides methods to query and update the state of the server,\n  e.g. which models/versions are being served.\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.GetModelStatus = channel.unary_unary(\n        \'/tensorflow.serving.ModelService/GetModelStatus\',\n        request_serializer=tensorflow__serving_dot_apis_dot_get__model__status__pb2.GetModelStatusRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_get__model__status__pb2.GetModelStatusResponse.FromString,\n        )\n    self.HandleReloadConfigRequest = channel.unary_unary(\n        \'/tensorflow.serving.ModelService/HandleReloadConfigRequest\',\n        request_serializer=\n        tensorflow__serving_dot_apis_dot_model__management__pb2.\n        ReloadConfigRequest.SerializeToString,\n        response_deserializer=\n        tensorflow__serving_dot_apis_dot_model__management__pb2.\n        ReloadConfigResponse.FromString,\n    )\n\n\nclass ModelServiceServicer(object):\n  """"""ModelService provides methods to query and update the state of the server,\n  e.g. which models/versions are being served.\n  """"""\n\n  def GetModelStatus(self, request, context):\n    """"""Gets status of model. If the ModelSpec in the request does not specify\n    version, information about all versions of the model will be returned. If\n    the ModelSpec in the request does specify a version, the status of only\n    that version will be returned.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def HandleReloadConfigRequest(self, request, context):\n    """"""Reloads the set of served models. The new config supersedes the old one,\n    so if a model is omitted from the new config it will be unloaded and no\n    longer served.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_ModelServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'GetModelStatus\':\n          grpc.unary_unary_rpc_method_handler(\n              servicer.GetModelStatus,\n              request_deserializer=\n              tensorflow__serving_dot_apis_dot_get__model__status__pb2.\n              GetModelStatusRequest.FromString,\n              response_serializer=\n              tensorflow__serving_dot_apis_dot_get__model__status__pb2.\n              GetModelStatusResponse.SerializeToString,\n          ),\n      \'HandleReloadConfigRequest\':\n          grpc.unary_unary_rpc_method_handler(\n              servicer.HandleReloadConfigRequest,\n              request_deserializer=\n              tensorflow__serving_dot_apis_dot_model__management__pb2.\n              ReloadConfigRequest.FromString,\n              response_serializer=\n              tensorflow__serving_dot_apis_dot_model__management__pb2.\n              ReloadConfigResponse.SerializeToString,\n          ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.serving.ModelService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
tensorflow_serving/apis/prediction_service_pb2.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n#\n# To regenerate run\n# python -m grpc.tools.protoc --python_out=. --grpc_python_out=. -I. tensorflow_serving/apis/prediction_service.proto\n\n### @@AUTOGENERATED SECTION STARTS HERE@@\n\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/prediction_service.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2\nfrom tensorflow_serving.apis import get_model_metadata_pb2 as tensorflow__serving_dot_apis_dot_get__model__metadata__pb2\nfrom tensorflow_serving.apis import inference_pb2 as tensorflow__serving_dot_apis_dot_inference__pb2\nfrom tensorflow_serving.apis import predict_pb2 as tensorflow__serving_dot_apis_dot_predict__pb2\nfrom tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/prediction_service.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n0tensorflow_serving/apis/prediction_service.proto\\x12\\x12tensorflow.serving\\x1a,tensorflow_serving/apis/classification.proto\\x1a\\x30tensorflow_serving/apis/get_model_metadata.proto\\x1a\\\'tensorflow_serving/apis/inference.proto\\x1a%tensorflow_serving/apis/predict.proto\\x1a(tensorflow_serving/apis/regression.proto2\\xfc\\x03\\n\\x11PredictionService\\x12\\x61\\n\\x08\\x43lassify\\x12).tensorflow.serving.ClassificationRequest\\x1a*.tensorflow.serving.ClassificationResponse\\x12X\\n\\x07Regress\\x12%.tensorflow.serving.RegressionRequest\\x1a&.tensorflow.serving.RegressionResponse\\x12R\\n\\x07Predict\\x12\\"".tensorflow.serving.PredictRequest\\x1a#.tensorflow.serving.PredictResponse\\x12g\\n\\x0eMultiInference\\x12).tensorflow.serving.MultiInferenceRequest\\x1a*.tensorflow.serving.MultiInferenceResponse\\x12m\\n\\x10GetModelMetadata\\x12+.tensorflow.serving.GetModelMetadataRequest\\x1a,.tensorflow.serving.GetModelMetadataResponseB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_classification__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_inference__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_predict__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_regression__pb2.DESCRIPTOR,])\n\n\n\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n\n_PREDICTIONSERVICE = _descriptor.ServiceDescriptor(\n  name=\'PredictionService\',\n  full_name=\'tensorflow.serving.PredictionService\',\n  file=DESCRIPTOR,\n  index=0,\n  options=None,\n  serialized_start=291,\n  serialized_end=799,\n  methods=[\n  _descriptor.MethodDescriptor(\n    name=\'Classify\',\n    full_name=\'tensorflow.serving.PredictionService.Classify\',\n    index=0,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_classification__pb2._CLASSIFICATIONREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_classification__pb2._CLASSIFICATIONRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Regress\',\n    full_name=\'tensorflow.serving.PredictionService.Regress\',\n    index=1,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_regression__pb2._REGRESSIONREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_regression__pb2._REGRESSIONRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Predict\',\n    full_name=\'tensorflow.serving.PredictionService.Predict\',\n    index=2,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_predict__pb2._PREDICTREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_predict__pb2._PREDICTRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'MultiInference\',\n    full_name=\'tensorflow.serving.PredictionService.MultiInference\',\n    index=3,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_inference__pb2._MULTIINFERENCEREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_inference__pb2._MULTIINFERENCERESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'GetModelMetadata\',\n    full_name=\'tensorflow.serving.PredictionService.GetModelMetadata\',\n    index=4,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2._GETMODELMETADATAREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2._GETMODELMETADATARESPONSE,\n    options=None,\n  ),\n])\n_sym_db.RegisterServiceDescriptor(_PREDICTIONSERVICE)\n\nDESCRIPTOR.services_by_name[\'PredictionService\'] = _PREDICTIONSERVICE\n\n# @@protoc_insertion_point(module_scope)\n\n### @@AUTOGENERATED SECTION ENDS HERE@@\n'"
tensorflow_serving/apis/prediction_service_pb2_grpc.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n# source: tensorflow_serving/apis/prediction_service.proto\n# To regenerate run\n# python -m grpc.tools.protoc --python_out=. --grpc_python_out=. -I. tensorflow_serving/apis/prediction_service.proto\nimport grpc\n\nfrom tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2\nfrom tensorflow_serving.apis import get_model_metadata_pb2 as tensorflow__serving_dot_apis_dot_get__model__metadata__pb2\nfrom tensorflow_serving.apis import inference_pb2 as tensorflow__serving_dot_apis_dot_inference__pb2\nfrom tensorflow_serving.apis import predict_pb2 as tensorflow__serving_dot_apis_dot_predict__pb2\nfrom tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2\n\n\nclass PredictionServiceStub(object):\n  """"""open source marker; do not remove\n  PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Classify = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Classify\',\n        request_serializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.FromString,\n        )\n    self.Regress = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Regress\',\n        request_serializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.FromString,\n        )\n    self.Predict = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Predict\',\n        request_serializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.FromString,\n        )\n    self.MultiInference = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/MultiInference\',\n        request_serializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.FromString,\n        )\n    self.GetModelMetadata = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/GetModelMetadata\',\n        request_serializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.FromString,\n        )\n\n\nclass PredictionServiceServicer(object):\n  """"""open source marker; do not remove\n  PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n\n  def Classify(self, request, context):\n    """"""Classify.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Regress(self, request, context):\n    """"""Regress.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Predict(self, request, context):\n    """"""Predict -- provides access to loaded TensorFlow model.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def MultiInference(self, request, context):\n    """"""MultiInference API for multi-headed models.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def GetModelMetadata(self, request, context):\n    """"""GetModelMetadata - provides access to metadata for loaded models.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_PredictionServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Classify\': grpc.unary_unary_rpc_method_handler(\n          servicer.Classify,\n          request_deserializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.SerializeToString,\n      ),\n      \'Regress\': grpc.unary_unary_rpc_method_handler(\n          servicer.Regress,\n          request_deserializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.SerializeToString,\n      ),\n      \'Predict\': grpc.unary_unary_rpc_method_handler(\n          servicer.Predict,\n          request_deserializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.SerializeToString,\n      ),\n      \'MultiInference\': grpc.unary_unary_rpc_method_handler(\n          servicer.MultiInference,\n          request_deserializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.SerializeToString,\n      ),\n      \'GetModelMetadata\': grpc.unary_unary_rpc_method_handler(\n          servicer.GetModelMetadata,\n          request_deserializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.serving.PredictionService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
tensorflow_serving/example/mnist_client.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#!/usr/bin/env python2.7\n\n""""""A client that talks to tensorflow_model_server loaded with mnist model.\n\nThe client downloads test images of mnist data set, queries the service with\nsuch test images to get predictions, and calculates the inference error rate.\n\nTypical usage example:\n\n    mnist_client.py --num_tests=100 --server=localhost:9000\n""""""\n\nfrom __future__ import print_function\n\nimport sys\nimport threading\n\n# This is a placeholder for a Google-internal import.\n\nimport grpc\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\nimport mnist_input_data\n\n\ntf.compat.v1.app.flags.DEFINE_integer(\n    \'concurrency\', 1, \'maximum number of concurrent inference requests\')\ntf.compat.v1.app.flags.DEFINE_integer(\'num_tests\', 100, \'Number of test images\')\ntf.compat.v1.app.flags.DEFINE_string(\'server\', \'\',\n                                     \'PredictionService host:port\')\ntf.compat.v1.app.flags.DEFINE_string(\'work_dir\', \'/tmp\', \'Working directory. \')\nFLAGS = tf.app.flags.FLAGS\n\n\nclass _ResultCounter(object):\n  """"""Counter for the prediction results.""""""\n\n  def __init__(self, num_tests, concurrency):\n    self._num_tests = num_tests\n    self._concurrency = concurrency\n    self._error = 0\n    self._done = 0\n    self._active = 0\n    self._condition = threading.Condition()\n\n  def inc_error(self):\n    with self._condition:\n      self._error += 1\n\n  def inc_done(self):\n    with self._condition:\n      self._done += 1\n      self._condition.notify()\n\n  def dec_active(self):\n    with self._condition:\n      self._active -= 1\n      self._condition.notify()\n\n  def get_error_rate(self):\n    with self._condition:\n      while self._done != self._num_tests:\n        self._condition.wait()\n      return self._error / float(self._num_tests)\n\n  def throttle(self):\n    with self._condition:\n      while self._active == self._concurrency:\n        self._condition.wait()\n      self._active += 1\n\n\ndef _create_rpc_callback(label, result_counter):\n  """"""Creates RPC callback function.\n\n  Args:\n    label: The correct label for the predicted example.\n    result_counter: Counter for the prediction result.\n  Returns:\n    The callback function.\n  """"""\n  def _callback(result_future):\n    """"""Callback function.\n\n    Calculates the statistics for the prediction result.\n\n    Args:\n      result_future: Result future of the RPC.\n    """"""\n    exception = result_future.exception()\n    if exception:\n      result_counter.inc_error()\n      print(exception)\n    else:\n      sys.stdout.write(\'.\')\n      sys.stdout.flush()\n      response = numpy.array(\n          result_future.result().outputs[\'scores\'].float_val)\n      prediction = numpy.argmax(response)\n      if label != prediction:\n        result_counter.inc_error()\n    result_counter.inc_done()\n    result_counter.dec_active()\n  return _callback\n\n\ndef do_inference(hostport, work_dir, concurrency, num_tests):\n  """"""Tests PredictionService with concurrent requests.\n\n  Args:\n    hostport: Host:port address of the PredictionService.\n    work_dir: The full path of working directory for test data set.\n    concurrency: Maximum number of concurrent requests.\n    num_tests: Number of test images to use.\n\n  Returns:\n    The classification error rate.\n\n  Raises:\n    IOError: An error occurred processing test data set.\n  """"""\n  test_data_set = mnist_input_data.read_data_sets(work_dir).test\n  channel = grpc.insecure_channel(hostport)\n  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n  result_counter = _ResultCounter(num_tests, concurrency)\n  for _ in range(num_tests):\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = \'mnist\'\n    request.model_spec.signature_name = \'predict_images\'\n    image, label = test_data_set.next_batch(1)\n    request.inputs[\'images\'].CopyFrom(\n        tf.make_tensor_proto(image[0], shape=[1, image[0].size]))\n    result_counter.throttle()\n    result_future = stub.Predict.future(request, 5.0)  # 5 seconds\n    result_future.add_done_callback(\n        _create_rpc_callback(label[0], result_counter))\n  return result_counter.get_error_rate()\n\n\ndef main(_):\n  if FLAGS.num_tests > 10000:\n    print(\'num_tests should not be greater than 10k\')\n    return\n  if not FLAGS.server:\n    print(\'please specify server host:port\')\n    return\n  error_rate = do_inference(FLAGS.server, FLAGS.work_dir,\n                            FLAGS.concurrency, FLAGS.num_tests)\n  print(\'\\nInference error rate: %s%%\' % (error_rate * 100))\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.app.run()\n'"
tensorflow_serving/example/mnist_input_data.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#!/usr/bin/env python2.7\n\n""""""Functions for downloading and reading MNIST data.""""""\n\nfrom __future__ import print_function\n\nimport gzip\nimport os\n\nimport numpy\nfrom six.moves import urllib\n\n# CVDF mirror of http://yann.lecun.com/exdb/mnist/\nSOURCE_URL = \'https://storage.googleapis.com/cvdf-datasets/mnist/\'\nTRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\nTRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\nTEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\nTEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\nVALIDATION_SIZE = 5000\n\n\ndef maybe_download(filename, work_directory):\n  """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n  if not os.path.exists(work_directory):\n    os.mkdir(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not os.path.exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded %s %d bytes.\' % (filename, statinfo.st_size))\n  return filepath\n\n\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n\n\ndef extract_images(filename):\n  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n  print(\'Extracting %s\' % filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST image file: %s\' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\n\n\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\n\n\ndef extract_labels(filename, one_hot=False):\n  """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n  print(\'Extracting %s\' % filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST label file: %s\' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\n\n\nclass DataSet(object):\n  """"""Class encompassing test, validation and training MNIST data set.""""""\n\n  def __init__(self, images, labels, fake_data=False, one_hot=False):\n    """"""Construct a DataSet. one_hot arg is used only if fake_data is true.""""""\n\n    if fake_data:\n      self._num_examples = 10000\n      self.one_hot = one_hot\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          \'images.shape: %s labels.shape: %s\' % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      # Convert from [0, 255] -> [0.0, 1.0].\n      images = images.astype(numpy.float32)\n      images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n\n  @property\n  def images(self):\n    return self._images\n\n  @property\n  def labels(self):\n    return self._labels\n\n  @property\n  def num_examples(self):\n    return self._num_examples\n\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n\n  def next_batch(self, batch_size, fake_data=False):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if fake_data:\n      fake_image = [1] * 784\n      if self.one_hot:\n        fake_label = [1] + [0] * 9\n      else:\n        fake_label = 0\n      return [fake_image for _ in range(batch_size)], [\n          fake_label for _ in range(batch_size)\n      ]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\n\n\ndef read_data_sets(train_dir, fake_data=False, one_hot=False):\n  """"""Return training, validation and testing data sets.""""""\n\n  class DataSets(object):\n    pass\n\n  data_sets = DataSets()\n\n  if fake_data:\n    data_sets.train = DataSet([], [], fake_data=True, one_hot=one_hot)\n    data_sets.validation = DataSet([], [], fake_data=True, one_hot=one_hot)\n    data_sets.test = DataSet([], [], fake_data=True, one_hot=one_hot)\n    return data_sets\n\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n\n  data_sets.train = DataSet(train_images, train_labels)\n  data_sets.validation = DataSet(validation_images, validation_labels)\n  data_sets.test = DataSet(test_images, test_labels)\n\n  return data_sets\n'"
tensorflow_serving/example/mnist_saved_model.py,41,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#! /usr/bin/env python\nr""""""Train and export a simple Softmax Regression TensorFlow model.\n\nThe model is from the TensorFlow ""MNIST For ML Beginner"" tutorial. This program\nsimply follows all its training instructions, and uses TensorFlow SavedModel to\nexport the trained model with proper signatures that can be loaded by standard\ntensorflow_model_server.\n\nUsage: mnist_saved_model.py [--training_iteration=x] [--model_version=y] \\\n    export_dir\n""""""\n\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n# This is a placeholder for a Google-internal import.\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import lookup_ops\n\nimport mnist_input_data\n\ntf.compat.v1.app.flags.DEFINE_integer(\'training_iteration\', 1000,\n                                      \'number of training iterations.\')\ntf.compat.v1.app.flags.DEFINE_integer(\'model_version\', 1,\n                                      \'version number of the model.\')\ntf.compat.v1.app.flags.DEFINE_string(\'work_dir\', \'/tmp\', \'Working directory.\')\nFLAGS = tf.compat.v1.app.flags.FLAGS\n\ntf.compat.v1.disable_eager_execution()\n\n\ndef main(_):\n  if len(sys.argv) < 2 or sys.argv[-1].startswith(\'-\'):\n    print(\'Usage: mnist_saved_model.py [--training_iteration=x] \'\n          \'[--model_version=y] export_dir\')\n    sys.exit(-1)\n  if FLAGS.training_iteration <= 0:\n    print(\'Please specify a positive value for training iteration.\')\n    sys.exit(-1)\n  if FLAGS.model_version <= 0:\n    print(\'Please specify a positive value for version number.\')\n    sys.exit(-1)\n\n  # Train model\n  print(\'Training model...\')\n  mnist = mnist_input_data.read_data_sets(FLAGS.work_dir, one_hot=True)\n  sess = tf.compat.v1.InteractiveSession()\n  serialized_tf_example = tf.compat.v1.placeholder(tf.string, name=\'tf_example\')\n  feature_configs = {\n      \'x\': tf.io.FixedLenFeature(shape=[784], dtype=tf.float32),\n  }\n  tf_example = tf.io.parse_example(serialized_tf_example, feature_configs)\n  x = tf.identity(tf_example[\'x\'], name=\'x\')  # use tf.identity() to assign name\n  y_ = tf.compat.v1.placeholder(\'float\', shape=[None, 10])\n  w = tf.Variable(tf.zeros([784, 10]))\n  b = tf.Variable(tf.zeros([10]))\n  sess.run(tf.compat.v1.global_variables_initializer())\n  y = tf.nn.softmax(tf.matmul(x, w) + b, name=\'y\')\n  cross_entropy = -tf.math.reduce_sum(y_ * tf.math.log(y))\n  train_step = tf.compat.v1.train.GradientDescentOptimizer(0.01).minimize(\n      cross_entropy)\n  values, indices = tf.nn.top_k(y, 10)\n  table = lookup_ops.index_to_string_table_from_tensor(\n      tf.constant([str(i) for i in range(10)]))\n  prediction_classes = table.lookup(tf.dtypes.cast(indices, tf.int64))\n  for _ in range(FLAGS.training_iteration):\n    batch = mnist.train.next_batch(50)\n    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  accuracy = tf.math.reduce_mean(tf.cast(correct_prediction, \'float\'))\n  print(\'training accuracy %g\' % sess.run(\n      accuracy, feed_dict={\n          x: mnist.test.images,\n          y_: mnist.test.labels\n      }))\n  print(\'Done training!\')\n\n  # Export model\n  # WARNING(break-tutorial-inline-code): The following code snippet is\n  # in-lined in tutorials, please update tutorial documents accordingly\n  # whenever code changes.\n  export_path_base = sys.argv[-1]\n  export_path = os.path.join(\n      tf.compat.as_bytes(export_path_base),\n      tf.compat.as_bytes(str(FLAGS.model_version)))\n  print(\'Exporting trained model to\', export_path)\n  builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_path)\n\n  # Build the signature_def_map.\n  classification_inputs = tf.compat.v1.saved_model.utils.build_tensor_info(\n      serialized_tf_example)\n  classification_outputs_classes = tf.compat.v1.saved_model.utils.build_tensor_info(\n      prediction_classes)\n  classification_outputs_scores = tf.compat.v1.saved_model.utils.build_tensor_info(\n      values)\n\n  classification_signature = (\n      tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n          inputs={\n              tf.compat.v1.saved_model.signature_constants.CLASSIFY_INPUTS:\n                  classification_inputs\n          },\n          outputs={\n              tf.compat.v1.saved_model.signature_constants\n              .CLASSIFY_OUTPUT_CLASSES:\n                  classification_outputs_classes,\n              tf.compat.v1.saved_model.signature_constants\n              .CLASSIFY_OUTPUT_SCORES:\n                  classification_outputs_scores\n          },\n          method_name=tf.compat.v1.saved_model.signature_constants\n          .CLASSIFY_METHOD_NAME))\n\n  tensor_info_x = tf.compat.v1.saved_model.utils.build_tensor_info(x)\n  tensor_info_y = tf.compat.v1.saved_model.utils.build_tensor_info(y)\n\n  prediction_signature = (\n      tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n          inputs={\'images\': tensor_info_x},\n          outputs={\'scores\': tensor_info_y},\n          method_name=tf.compat.v1.saved_model.signature_constants\n          .PREDICT_METHOD_NAME))\n\n  builder.add_meta_graph_and_variables(\n      sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n      signature_def_map={\n          \'predict_images\':\n              prediction_signature,\n          tf.compat.v1.saved_model.signature_constants\n          .DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n              classification_signature,\n      },\n      main_op=tf.compat.v1.tables_initializer(),\n      strip_default_attrs=True)\n\n  builder.save()\n\n  print(\'Done exporting!\')\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.app.run()\n'"
tensorflow_serving/example/resnet_client.py,0,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A client that performs inferences on a ResNet model using the REST API.\n\nThe client downloads a test image of a cat, queries the server over the REST API\nwith the test image repeatedly and measures how long it takes to respond.\n\nThe client expects a TensorFlow Serving ModelServer running a ResNet SavedModel\nfrom:\n\nhttps://github.com/tensorflow/models/tree/master/official/resnet#pre-trained-model\n\nThe SavedModel must be one that can take JPEG images as inputs.\n\nTypical usage example:\n\n    resnet_client.py\n""""""\n\nfrom __future__ import print_function\n\nimport base64\nimport requests\n\n# The server URL specifies the endpoint of your server running the ResNet\n# model with the name ""resnet"" and using the predict interface.\nSERVER_URL = \'http://localhost:8501/v1/models/resnet:predict\'\n\n# The image URL is the location of the image we should send to the server\nIMAGE_URL = \'https://tensorflow.org/images/blogs/serving/cat.jpg\'\n\n\ndef main():\n  # Download the image\n  dl_request = requests.get(IMAGE_URL, stream=True)\n  dl_request.raise_for_status()\n\n  # Compose a JSON Predict request (send JPEG image in base64).\n  jpeg_bytes = base64.b64encode(dl_request.content).decode(\'utf-8\')\n  predict_request = \'{""instances"" : [{""b64"": ""%s""}]}\' % jpeg_bytes\n\n  # Send few requests to warm-up the model.\n  for _ in range(3):\n    response = requests.post(SERVER_URL, data=predict_request)\n    response.raise_for_status()\n\n  # Send few actual requests and report average latency.\n  total_time = 0\n  num_requests = 10\n  for _ in range(num_requests):\n    response = requests.post(SERVER_URL, data=predict_request)\n    response.raise_for_status()\n    total_time += response.elapsed.total_seconds()\n    prediction = response.json()[\'predictions\'][0]\n\n  print(\'Prediction class: {}, avg latency: {} ms\'.format(\n      prediction[\'classes\'], (total_time*1000)/num_requests))\n\n\nif __name__ == \'__main__\':\n  main()\n'"
tensorflow_serving/example/resnet_client_grpc.py,5,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Send JPEG image to tensorflow_model_server loaded with ResNet model.\n\n""""""\n\nfrom __future__ import print_function\n\n# This is a placeholder for a Google-internal import.\n\nimport grpc\nimport requests\nimport tensorflow as tf\n\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\n# The image URL is the location of the image we should send to the server\nIMAGE_URL = \'https://tensorflow.org/images/blogs/serving/cat.jpg\'\n\ntf.app.flags.DEFINE_string(\'server\', \'localhost:8500\',\n                           \'PredictionService host:port\')\ntf.app.flags.DEFINE_string(\'image\', \'\', \'path to image in JPEG format\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  if FLAGS.image:\n    with open(FLAGS.image, \'rb\') as f:\n      data = f.read()\n  else:\n    # Download the image since we weren\'t given one\n    dl_request = requests.get(IMAGE_URL, stream=True)\n    dl_request.raise_for_status()\n    data = dl_request.content\n\n  channel = grpc.insecure_channel(FLAGS.server)\n  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n  # Send request\n  # See prediction_service.proto for gRPC request/response details.\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = \'resnet\'\n  request.model_spec.signature_name = \'serving_default\'\n  request.inputs[\'image_bytes\'].CopyFrom(\n      tf.make_tensor_proto(data, shape=[1]))\n  result = stub.Predict(request, 10.0)  # 10 secs timeout\n  print(result)\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.app.run()\n'"
tensorflow_serving/example/resnet_warmup.py,2,"b'# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Creates the tf_serving_warmup_requests file to warm up a ResNet SavedModel.\n\n   1. Invoke this script passing in the saved_model directory (including version\n        folder, the folder containing saved_model.pb) as an argument.\n   2. Restart tensorflow_model_server.\n\n   If unsure of the model directory, look for the output:\n   \'No warmup data file found at\' in the tensorflow_model_server\n   startup log\n\n   After the script is run, and tensorflow_model_server is restarted, to verify\n   it is working look for the output:\n   \'Starting to read warmup data for model at\' in the tensorflow_model_server\n   startup log\n\n   Usage example:\n     python resnet_warmup.py saved_model_dir\n""""""\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport requests\nimport tensorflow as tf\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_log_pb2\n\n\n# IMAGE_URLS are the locations of the images we use to warmup the model\nIMAGE_URLS = [\'https://tensorflow.org/images/blogs/serving/cat.jpg\',\n              # pylint: disable=g-line-too-long\n              \'https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg\',\n              # pylint: enable=g-line-too-long\n             ]\n\n\ndef main():\n  if len(sys.argv) != 2 or sys.argv[-1].startswith(\'-\'):\n    print(\'Usage: resnet_warmup.py saved_model_dir\')\n    sys.exit(-1)\n\n  model_dir = sys.argv[-1]\n  if not os.path.isdir(model_dir):\n    print(\'The saved model directory: %s does not exist. \'\n          \'Specify the path of an existing model.\' % model_dir)\n    sys.exit(-1)\n\n  # Create the assets.extra directory\n  assets_dir = os.path.join(model_dir, \'assets.extra\')\n  if not os.path.exists(assets_dir):\n    os.mkdir(assets_dir)\n\n  warmup_file = os.path.join(assets_dir, \'tf_serving_warmup_requests\')\n  with tf.io.TFRecordWriter(warmup_file) as writer:\n    for image in IMAGE_URLS:\n      # Download the image\n      dl_request = requests.get(image, stream=True)\n      dl_request.raise_for_status()\n      data = dl_request.content\n\n      # Create the inference request\n      request = predict_pb2.PredictRequest()\n      request.model_spec.name = \'resnet\'\n      request.model_spec.signature_name = \'serving_default\'\n      request.inputs[\'image_bytes\'].CopyFrom(\n          tf.make_tensor_proto(data, shape=[1]))\n\n      log = prediction_log_pb2.PredictionLog(\n          predict_log=prediction_log_pb2.PredictLog(request=request))\n      writer.write(log.SerializeToString())\n\n  print(\'Created the file \\\'%s\\\', restart tensorflow_model_server to warmup \'\n        \'the ResNet SavedModel.\' % warmup_file)\n\nif __name__ == \'__main__\':\n  main()\n'"
tensorflow_serving/model_servers/tensorflow_model_server_test.py,26,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for tensorflow_model_server.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport subprocess\nimport time\n\n# This is a placeholder for a Google-internal import.\n\nimport grpc\nfrom six.moves import range\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.profiler import profiler_client\nfrom tensorflow.python.saved_model import signature_constants\nfrom tensorflow_serving.apis import classification_pb2\nfrom tensorflow_serving.apis import get_model_metadata_pb2\nfrom tensorflow_serving.apis import get_model_status_pb2\nfrom tensorflow_serving.apis import inference_pb2\nfrom tensorflow_serving.apis import model_service_pb2_grpc\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\nfrom tensorflow_serving.apis import regression_pb2\nfrom tensorflow_serving.model_servers.test_util import tensorflow_model_server_test_base\n\nFLAGS = flags.FLAGS\n\nRPC_TIMEOUT = 5.0\nGRPC_SOCKET_PATH = \'/tmp/tf-serving.sock\'\n\n\nclass TensorflowModelServerTest(\n    tensorflow_model_server_test_base.TensorflowModelServerTestBase):\n  """"""This class defines integration test cases for tensorflow_model_server.""""""\n\n  @staticmethod\n  def __TestSrcDirPath(relative_path=\'\'):\n    return os.path.join(os.environ[\'TEST_SRCDIR\'],\n                        \'tf_serving/tensorflow_serving\', relative_path)\n\n  def __BuildModelConfigFile(self):\n    """"""Write a config file to disk for use in tests.\n\n    Substitutes placeholder for test directory with test directory path\n    in the configuration template file and writes it out to another file\n    used by the test.\n    """"""\n    with open(self._GetGoodModelConfigTemplate(), \'r\') as template_file:\n      config = template_file.read().replace(\'${TEST_HALF_PLUS_TWO_DIR}\',\n                                            self._GetSavedModelBundlePath())\n      config = config.replace(\'${TEST_HALF_PLUS_THREE_DIR}\',\n                              self._GetSavedModelHalfPlusThreePath())\n    with open(self._GetGoodModelConfigFile(), \'w\') as config_file:\n      config_file.write(config)\n\n  def setUp(self):\n    """"""Sets up integration test parameters.""""""\n    self.testdata_dir = TensorflowModelServerTest.__TestSrcDirPath(\n        \'servables/tensorflow/testdata\')\n    self.temp_dir = tf.test.get_temp_dir()\n    self.server_proc = None\n    self.__BuildModelConfigFile()\n\n  def tearDown(self):\n    """"""Deletes created configuration file.""""""\n    os.remove(self._GetGoodModelConfigFile())\n\n  def testGetModelStatus(self):\n    """"""Test ModelService.GetModelStatus implementation.""""""\n    model_path = self._GetSavedModelBundlePath()\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\', model_path)[1]\n\n    print(\'Sending GetModelStatus request...\')\n    # Send request\n    request = get_model_status_pb2.GetModelStatusRequest()\n    request.model_spec.name = \'default\'\n    channel = grpc.insecure_channel(model_server_address)\n    stub = model_service_pb2_grpc.ModelServiceStub(channel)\n    result = stub.GetModelStatus(request, RPC_TIMEOUT)  # 5 secs timeout\n    # Verify response\n    self.assertEqual(1, len(result.model_version_status))\n    self.assertEqual(123, result.model_version_status[0].version)\n    # OK error code (0) indicates no error occurred\n    self.assertEqual(0, result.model_version_status[0].status.error_code)\n\n  def testGetModelMetadata(self):\n    """"""Test PredictionService.GetModelMetadata implementation.""""""\n    model_path = self._GetSavedModelBundlePath()\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\', model_path)[1]\n\n    print(\'Sending GetModelMetadata request...\')\n    # Send request\n    request = get_model_metadata_pb2.GetModelMetadataRequest()\n    request.model_spec.name = \'default\'\n    request.metadata_field.append(\'signature_def\')\n    channel = grpc.insecure_channel(model_server_address)\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n    result = stub.GetModelMetadata(request, RPC_TIMEOUT)  # 5 secs timeout\n    # Verify response\n    self.assertEqual(\'default\', result.model_spec.name)\n    self.assertEqual(\n        self._GetModelVersion(model_path), result.model_spec.version.value)\n    self.assertEqual(1, len(result.metadata))\n    self.assertIn(\'signature_def\', result.metadata)\n\n  def testClassify(self):\n    """"""Test PredictionService.Classify implementation.""""""\n    model_path = self._GetSavedModelBundlePath()\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\', model_path)[1]\n\n    print(\'Sending Classify request...\')\n    # Prepare request\n    request = classification_pb2.ClassificationRequest()\n    request.model_spec.name = \'default\'\n    request.model_spec.signature_name = \'classify_x_to_y\'\n\n    example = request.input.example_list.examples.add()\n    example.features.feature[\'x\'].float_list.value.extend([2.0])\n\n    # Send request\n    channel = grpc.insecure_channel(model_server_address)\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n    result = stub.Classify(request, RPC_TIMEOUT)  # 5 secs timeout\n    # Verify response\n    self.assertEqual(1, len(result.result.classifications))\n    self.assertEqual(1, len(result.result.classifications[0].classes))\n    expected_output = 3.0\n    self.assertEqual(expected_output,\n                     result.result.classifications[0].classes[0].score)\n    self._VerifyModelSpec(result.model_spec, request.model_spec.name,\n                          request.model_spec.signature_name,\n                          self._GetModelVersion(model_path))\n\n  def testRegress(self):\n    """"""Test PredictionService.Regress implementation.""""""\n    model_path = self._GetSavedModelBundlePath()\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\', model_path)[1]\n\n    print(\'Sending Regress request...\')\n    # Prepare request\n    request = regression_pb2.RegressionRequest()\n    request.model_spec.name = \'default\'\n    request.model_spec.signature_name = \'regress_x_to_y\'\n\n    example = request.input.example_list.examples.add()\n    example.features.feature[\'x\'].float_list.value.extend([2.0])\n\n    # Send request\n    channel = grpc.insecure_channel(model_server_address)\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n    result = stub.Regress(request, RPC_TIMEOUT)  # 5 secs timeout\n    # Verify response\n    self.assertEqual(1, len(result.result.regressions))\n    expected_output = 3.0\n    self.assertEqual(expected_output, result.result.regressions[0].value)\n    self._VerifyModelSpec(result.model_spec, request.model_spec.name,\n                          request.model_spec.signature_name,\n                          self._GetModelVersion(model_path))\n\n  def testMultiInference(self):\n    """"""Test PredictionService.MultiInference implementation.""""""\n    model_path = self._GetSavedModelBundlePath()\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\', model_path)[1]\n\n    print(\'Sending MultiInference request...\')\n    # Prepare request\n    request = inference_pb2.MultiInferenceRequest()\n    request.tasks.add().model_spec.name = \'default\'\n    request.tasks[0].model_spec.signature_name = \'regress_x_to_y\'\n    request.tasks[0].method_name = \'tensorflow/serving/regress\'\n    request.tasks.add().model_spec.name = \'default\'\n    request.tasks[1].model_spec.signature_name = \'classify_x_to_y\'\n    request.tasks[1].method_name = \'tensorflow/serving/classify\'\n\n    example = request.input.example_list.examples.add()\n    example.features.feature[\'x\'].float_list.value.extend([2.0])\n\n    # Send request\n    channel = grpc.insecure_channel(model_server_address)\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n    result = stub.MultiInference(request, RPC_TIMEOUT)  # 5 secs timeout\n\n    # Verify response\n    self.assertEqual(2, len(result.results))\n    expected_output = 3.0\n    self.assertEqual(expected_output,\n                     result.results[0].regression_result.regressions[0].value)\n    self.assertEqual(\n        expected_output, result.results[1].classification_result\n        .classifications[0].classes[0].score)\n    for i in range(2):\n      self._VerifyModelSpec(result.results[i].model_spec,\n                            request.tasks[i].model_spec.name,\n                            request.tasks[i].model_spec.signature_name,\n                            self._GetModelVersion(model_path))\n\n  def testPredictSavedModel(self):\n    """"""Test PredictionService.Predict implementation with SavedModel.""""""\n    self._TestPredict(self._GetSavedModelBundlePath())\n\n  def _TestBadModel(self):\n    """"""Helper method to test against a bad model export.""""""\n    # Both SessionBundle and SavedModel use the same bad model path, but in the\n    # case of SavedModel, the export will get up-converted to a SavedModel.\n    # As the bad model will prevent the server from becoming ready, we set the\n    # wait_for_server_ready param to False to avoid blocking/timing out.\n    model_path = os.path.join(self.testdata_dir, \'bad_half_plus_two\'),\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\', model_path, wait_for_server_ready=False)[1]\n    with self.assertRaises(grpc.RpcError) as ectxt:\n      self.VerifyPredictRequest(\n          model_server_address, expected_output=3.0,\n          expected_version=self._GetModelVersion(model_path),\n          signature_name=\'\')\n    self.assertIs(grpc.StatusCode.FAILED_PRECONDITION,\n                  ectxt.exception.code())\n\n  def _TestBadModelUpconvertedSavedModel(self):\n    """"""Test Predict against a bad upconverted SavedModel model export.""""""\n    self._TestBadModel()\n\n  def testGoodModelConfig(self):\n    """"""Test server configuration from file works with valid configuration.""""""\n    model_server_address = TensorflowModelServerTest.RunServer(\n        None, None, model_config_file=self._GetGoodModelConfigFile())[1]\n\n    self.VerifyPredictRequest(\n        model_server_address, model_name=\'half_plus_two\', expected_output=3.0,\n        expected_version=self._GetModelVersion(self._GetSavedModelBundlePath()))\n    self.VerifyPredictRequest(\n        model_server_address, model_name=\'half_plus_two\',\n        expected_output=3.0, specify_output=False,\n        expected_version=self._GetModelVersion(self._GetSavedModelBundlePath()))\n\n    self.VerifyPredictRequest(\n        model_server_address, model_name=\'half_plus_three\', expected_output=4.0,\n        expected_version=self._GetModelVersion(\n            self._GetSavedModelHalfPlusThreePath()))\n    self.VerifyPredictRequest(\n        model_server_address, model_name=\'half_plus_three\', expected_output=4.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(\n            self._GetSavedModelHalfPlusThreePath()))\n\n  def testBadModelConfig(self):\n    """"""Test server model configuration from file fails for invalid file.""""""\n    proc = TensorflowModelServerTest.RunServer(\n        None,\n        None,\n        model_config_file=self._GetBadModelConfigFile(),\n        pipe=subprocess.PIPE,\n        wait_for_server_ready=False)[0]\n\n    error_message = (\'Error parsing text-format \'\n                     \'tensorflow.serving.ModelServerConfig\')\n    error_message = error_message.encode(\'utf-8\')\n    self.assertNotEqual(proc.stderr, None)\n    self.assertGreater(proc.stderr.read().find(error_message), -1)\n\n  def testModelConfigReload(self):\n    """"""Test model server polls filesystem for model configuration.""""""\n\n    base_config_proto = """"""\n    model_config_list: {{\n      config: {{\n        name: ""{name}"",\n        base_path: ""{model_path}"",\n        model_platform: ""tensorflow""\n      }}\n    }}\n    """"""\n\n    config_path = os.path.join(FLAGS.test_tmpdir, \'model_config.txt\')\n\n    # Write a config file serving half_plus_two model\n    with open(config_path, \'w\') as f:\n      f.write(\n          base_config_proto.format(\n              name=\'half_plus_two\', model_path=self._GetSavedModelBundlePath()))\n\n    poll_period = 1\n    model_server_address = TensorflowModelServerTest.RunServer(\n        None,\n        None,\n        model_config_file=config_path,\n        model_config_file_poll_period=poll_period)[1]\n\n    self.VerifyPredictRequest(\n        model_server_address,\n        model_name=\'half_plus_two\',\n        expected_output=3.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(self._GetSavedModelBundlePath()))\n\n    # Rewrite the config file with half_plus_three model\n    with open(config_path, \'w\') as f:\n      f.write(\n          base_config_proto.format(\n              name=\'half_plus_three\',\n              model_path=self._GetSavedModelHalfPlusThreePath()))\n\n    # Give modelserver time to poll and load the new config\n    time.sleep(poll_period + 1)\n\n    # Verify new model config was realized in model server\n    self.VerifyPredictRequest(\n        model_server_address,\n        model_name=\'half_plus_three\',\n        expected_output=4.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(\n            self._GetSavedModelHalfPlusThreePath()))\n\n  def testModelConfigReloadWithZeroPollPeriod(self):\n    """"""Test model server does not poll filesystem for model config.""""""\n\n    base_config_proto = """"""\n    model_config_list: {{\n      config: {{\n        name: ""{name}"",\n        base_path: ""{model_path}"",\n        model_platform: ""tensorflow""\n      }}\n    }}\n    """"""\n\n    config_path = os.path.join(FLAGS.test_tmpdir, \'model_config.txt\')\n\n    # Write a config file serving half_plus_two model\n    with open(config_path, \'w\') as f:\n      f.write(\n          base_config_proto.format(\n              name=\'half_plus_two\', model_path=self._GetSavedModelBundlePath()))\n\n    poll_period = 0\n    model_server_address = TensorflowModelServerTest.RunServer(\n        None,\n        None,\n        model_config_file=config_path,\n        model_config_file_poll_period=poll_period)[1]\n\n    self.VerifyPredictRequest(\n        model_server_address,\n        model_name=\'half_plus_two\',\n        expected_output=3.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(self._GetSavedModelBundlePath()))\n\n    # Rewrite the config file with half_plus_three model\n    with open(config_path, \'w\') as f:\n      f.write(\n          base_config_proto.format(\n              name=\'half_plus_three\',\n              model_path=self._GetSavedModelHalfPlusThreePath()))\n\n    # Give modelserver enough time to poll and load the new config should it\n    # have such a desire\n    time.sleep(poll_period + 1)\n\n    # Verify model server is still serving the old model config\n    self.VerifyPredictRequest(\n        model_server_address,\n        model_name=\'half_plus_two\',\n        expected_output=3.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(\n            self._GetSavedModelHalfPlusThreePath()))\n\n  def testGoodGrpcChannelArgs(self):\n    """"""Test server starts with grpc_channel_arguments specified.""""""\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\',\n        self._GetSavedModelBundlePath(),\n        grpc_channel_arguments=\n        \'grpc.max_connection_age_ms=2000,grpc.lb_policy_name=grpclb\')[1]\n    self.VerifyPredictRequest(\n        model_server_address,\n        expected_output=3.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(\n            self._GetSavedModelHalfPlusThreePath()))\n\n  def testClassifyREST(self):\n    """"""Test Classify implementation over REST API.""""""\n    model_path = self._GetSavedModelBundlePath()\n    host, port = TensorflowModelServerTest.RunServer(\'default\',\n                                                     model_path)[2].split(\':\')\n\n    # Prepare request\n    url = \'http://{}:{}/v1/models/default:classify\'.format(host, port)\n    json_req = {\'signature_name\': \'classify_x_to_y\', \'examples\': [{\'x\': 2.0}]}\n\n    # Send request\n    resp_data = None\n    try:\n      resp_data = tensorflow_model_server_test_base.CallREST(url, json_req)\n    except Exception as e:  # pylint: disable=broad-except\n      self.fail(\'Request failed with error: {}\'.format(e))\n\n    # Verify response\n    self.assertEqual(json.loads(resp_data.decode()), {\'results\': [[[\'\', 3.0]]]})\n\n  def testRegressREST(self):\n    """"""Test Regress implementation over REST API.""""""\n    model_path = self._GetSavedModelBundlePath()\n    host, port = TensorflowModelServerTest.RunServer(\'default\',\n                                                     model_path)[2].split(\':\')\n\n    # Prepare request\n    url = \'http://{}:{}/v1/models/default:regress\'.format(host, port)\n    json_req = {\'signature_name\': \'regress_x_to_y\', \'examples\': [{\'x\': 2.0}]}\n\n    # Send request\n    resp_data = None\n    try:\n      resp_data = tensorflow_model_server_test_base.CallREST(url, json_req)\n    except Exception as e:  # pylint: disable=broad-except\n      self.fail(\'Request failed with error: {}\'.format(e))\n\n    # Verify response\n    self.assertEqual(json.loads(resp_data.decode()), {\'results\': [3.0]})\n\n  def testPredictREST(self):\n    """"""Test Predict implementation over REST API.""""""\n    model_path = self._GetSavedModelBundlePath()\n    host, port = TensorflowModelServerTest.RunServer(\'default\',\n                                                     model_path)[2].split(\':\')\n\n    # Prepare request\n    url = \'http://{}:{}/v1/models/default:predict\'.format(host, port)\n    json_req = {\'instances\': [2.0, 3.0, 4.0]}\n\n    # Send request\n    resp_data = None\n    try:\n      resp_data = tensorflow_model_server_test_base.CallREST(url, json_req)\n    except Exception as e:  # pylint: disable=broad-except\n      self.fail(\'Request failed with error: {}\'.format(e))\n\n    # Verify response\n    self.assertEqual(\n        json.loads(resp_data.decode()), {\'predictions\': [3.0, 3.5, 4.0]})\n\n  def testPredictColumnarREST(self):\n    """"""Test Predict implementation over REST API with columnar inputs.""""""\n    model_path = self._GetSavedModelBundlePath()\n    host, port = TensorflowModelServerTest.RunServer(\'default\',\n                                                     model_path)[2].split(\':\')\n\n    # Prepare request\n    url = \'http://{}:{}/v1/models/default:predict\'.format(host, port)\n    json_req = {\'inputs\': [2.0, 3.0, 4.0]}\n\n    # Send request\n    resp_data = None\n    try:\n      resp_data = tensorflow_model_server_test_base.CallREST(url, json_req)\n    except Exception as e:  # pylint: disable=broad-except\n      self.fail(\'Request failed with error: {}\'.format(e))\n\n    # Verify response\n    self.assertEqual(\n        json.loads(resp_data.decode()), {\'outputs\': [3.0, 3.5, 4.0]})\n\n  def testGetStatusREST(self):\n    """"""Test ModelStatus implementation over REST API with columnar inputs.""""""\n    model_path = self._GetSavedModelBundlePath()\n    host, port = TensorflowModelServerTest.RunServer(\'default\',\n                                                     model_path)[2].split(\':\')\n\n    # Prepare request\n    url = \'http://{}:{}/v1/models/default\'.format(host, port)\n\n    # Send request\n    resp_data = None\n    try:\n      resp_data = tensorflow_model_server_test_base.CallREST(url, None)\n    except Exception as e:  # pylint: disable=broad-except\n      self.fail(\'Request failed with error: {}\'.format(e))\n\n    # Verify response\n    self.assertEqual(\n        json.loads(resp_data.decode()), {\n            \'model_version_status\': [{\n                \'version\': \'123\',\n                \'state\': \'AVAILABLE\',\n                \'status\': {\n                    \'error_code\': \'OK\',\n                    \'error_message\': \'\'\n                }\n            }]\n        })\n\n  def testGetModelMetadataREST(self):\n    """"""Test ModelStatus implementation over REST API with columnar inputs.""""""\n    model_path = self._GetSavedModelBundlePath()\n    host, port = TensorflowModelServerTest.RunServer(\'default\',\n                                                     model_path)[2].split(\':\')\n\n    # Prepare request\n    url = \'http://{}:{}/v1/models/default/metadata\'.format(host, port)\n\n    # Send request\n    resp_data = None\n    try:\n      resp_data = tensorflow_model_server_test_base.CallREST(url, None)\n    except Exception as e:  # pylint: disable=broad-except\n      self.fail(\'Request failed with error: {}\'.format(e))\n\n    try:\n      model_metadata_file = self._GetModelMetadataFile()\n      with open(model_metadata_file) as f:\n        expected_metadata = json.load(f)\n        # Verify response\n        # Note, we sort the JSON object before comparing. Formally JSON lists\n        # (aka arrays) are considered ordered and general comparison should NOT\n        # sort. In this case, the ""metadata"" does not have any ordering making\n        # the sort OK (and the test robust).\n        self.assertEqual(\n            tensorflow_model_server_test_base.SortedObject(\n                json.loads(resp_data.decode())),\n            tensorflow_model_server_test_base.SortedObject(expected_metadata))\n    except Exception as e:  # pylint: disable=broad-except\n      self.fail(\'Request failed with error: {}\'.format(e))\n\n  def testPrometheusEndpoint(self):\n    """"""Test ModelStatus implementation over REST API with columnar inputs.""""""\n    model_path = self._GetSavedModelBundlePath()\n    host, port = TensorflowModelServerTest.RunServer(\n        \'default\',\n        model_path,\n        monitoring_config_file=self._GetMonitoringConfigFile())[2].split(\':\')\n\n    # Prepare request\n    url = \'http://{}:{}/monitoring/prometheus/metrics\'.format(host, port)\n\n    # Send request\n    resp_data = None\n    try:\n      resp_data = tensorflow_model_server_test_base.CallREST(url, None)\n    except Exception as e:  # pylint: disable=broad-except\n      self.fail(\'Request failed with error: {}\'.format(e))\n\n    # Verify that there should be some metric type information.\n    self.assertIn(\'# TYPE\',\n                  resp_data.decode(\'utf-8\') if resp_data is not None else None)\n\n  def testPredictUDS(self):\n    """"""Test saved model prediction over a Unix domain socket.""""""\n    _ = TensorflowModelServerTest.RunServer(\'default\',\n                                            self._GetSavedModelBundlePath())\n    model_server_address = \'unix:%s\' % GRPC_SOCKET_PATH\n    self.VerifyPredictRequest(\n        model_server_address,\n        expected_output=3.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(\n            self._GetSavedModelHalfPlusThreePath()))\n\n  def testPredictOnTfLite(self):\n    """"""Test saved model prediction on a TF Lite mode.""""""\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\', self._GetTfLiteModelPath(), model_type=\'tflite\')[1]\n    self.VerifyPredictRequest(\n        model_server_address,\n        expected_output=3.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(self._GetTfLiteModelPath()))\n\n  def testPredictWithSignatureDefOnTfLite(self):\n    """"""Test saved model prediction on a TF Lite mode.""""""\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\', self._GetTfLiteModelWithSigDefPath(), model_type=\'tflite\')[1]\n    self.VerifyPredictRequest(\n        model_server_address,\n        expected_output=3.0,\n        specify_output=False,\n        expected_version=self._GetModelVersion(self._GetTfLiteModelPath()))\n\n  def test_tf_saved_model_save(self):\n    base_path = os.path.join(self.get_temp_dir(), \'tf_saved_model_save\')\n    export_path = os.path.join(base_path, \'00000123\')\n    root = tf.train.Checkpoint()\n    root.v1 = tf.Variable(3.)\n    root.v2 = tf.Variable(2.)\n    root.f = tf.function(\n        lambda x: {\'y\': root.v1 * root.v2 * x})\n    to_save = root.f.get_concrete_function(tf.TensorSpec(None, tf.float32))\n    tf.saved_model.experimental.save(root, export_path, to_save)\n    _, model_server_address, _ = TensorflowModelServerTest.RunServer(\n        \'default\', base_path)\n    expected_version = self._GetModelVersion(base_path)\n    self.VerifyPredictRequest(\n        model_server_address,\n        expected_output=12.0,\n        specify_output=False,\n        expected_version=expected_version)\n\n  def test_tf_saved_model_save_multiple_signatures(self):\n    base_path = os.path.join(self.get_temp_dir(), \'tf_saved_model_save\')\n    export_path = os.path.join(base_path, \'00000123\')\n    root = tf.train.Checkpoint()\n    root.f = tf.function(lambda x: {\'y\': 1.},\n                         input_signature=[tf.TensorSpec(None, tf.float32)])\n    root.g = tf.function(lambda x: {\'y\': 2.},\n                         input_signature=[tf.TensorSpec(None, tf.float32)])\n    tf.saved_model.experimental.save(\n        root, export_path,\n        signatures={\n            signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: root.f,\n            \'custom_signature_key\': root.g})\n    _, model_server_address, _ = TensorflowModelServerTest.RunServer(\n        \'default\', base_path)\n    expected_version = self._GetModelVersion(base_path)\n    self.VerifyPredictRequest(\n        model_server_address,\n        expected_output=2.0,\n        expected_version=expected_version,\n        signature_name=\'custom_signature_key\')\n    self.VerifyPredictRequest(\n        model_server_address,\n        expected_output=1.0,\n        expected_version=expected_version)\n\n  def test_sequential_keras_saved_model_save(self):\n    """"""Test loading a simple SavedModel created with Keras Sequential API.""""""\n    model = tf.keras.models.Sequential()\n\n    model.add(tf.keras.layers.Input(dtype=\'float32\', shape=(1,), name=\'x\'))\n    model.add(tf.keras.layers.Lambda(lambda x: x, name=\'y\'))\n\n    base_path = os.path.join(self.get_temp_dir(),\n                             \'keras_sequential_saved_model_save\')\n    export_path = os.path.join(base_path, \'00000123\')\n\n    tf.saved_model.save(model, export_path)\n\n    _, model_server_address, _ = TensorflowModelServerTest.RunServer(\n        \'default\', base_path)\n\n    expected_version = self._GetModelVersion(base_path)\n    self.VerifyPredictRequest(\n        model_server_address,\n        batch_input=True,\n        specify_output=False,\n        expected_output=2.0,\n        expected_version=expected_version)\n\n  def test_distrat_sequential_keras_saved_model_save(self):\n    """"""Test loading a Keras SavedModel with tf.distribute.""""""\n    # You need to call SetVirtualCpus in test setUp with the maximum value\n    # needed in any test if you use this in multiple tests. For now this is the\n    # only test using this functionality.\n    tensorflow_model_server_test_base.SetVirtualCpus(2)\n    strategy = tf.distribute.MirroredStrategy(devices=(\'/cpu:0\', \'/cpu:1\'))\n    with strategy.scope():\n      model = tf.keras.models.Sequential()\n\n      model.add(tf.keras.layers.Input(dtype=\'float32\', shape=(1,), name=\'x\'))\n      model.add(tf.keras.layers.Dense(1, kernel_initializer=\'ones\',\n                                      bias_initializer=\'zeros\'))\n      model.add(tf.keras.layers.Lambda(lambda x: x, name=\'y\'))\n\n    base_path = os.path.join(self.get_temp_dir(),\n                             \'keras_sequential_saved_model_save\')\n    export_path = os.path.join(base_path, \'00000123\')\n\n    tf.saved_model.save(model, export_path)\n\n    _, model_server_address, _ = TensorflowModelServerTest.RunServer(\n        \'default\', base_path)\n\n    expected_version = self._GetModelVersion(base_path)\n    self.VerifyPredictRequest(\n        model_server_address,\n        batch_input=True,\n        specify_output=False,\n        expected_output=2.0,\n        expected_version=expected_version)\n\n  def test_profiler_service_with_valid_trace_request(self):\n    """"""Test integration with profiler service by sending tracing requests.""""""\n\n    # Start model server\n    model_path = self._GetSavedModelBundlePath()\n    _, grpc_addr, rest_addr = TensorflowModelServerTest.RunServer(\n        \'default\', model_path)\n\n    # Prepare predict request\n    url = \'http://{}/v1/models/default:predict\'.format(rest_addr)\n    json_req = \'{""instances"": [2.0, 3.0, 4.0]}\'\n\n    # In a subprocess, send a REST predict request every second for 3 seconds\n    exec_command = (""wget {} --content-on-error=on -O- --post-data  \'{}\' ""\n                    ""--header=\'Content-Type:application/json\'"").format(\n                        url, json_req)\n    repeat_command = \'for n in {{1..3}}; do {} & sleep 1; done;\'.format(\n        exec_command)\n    proc = subprocess.Popen(\n        repeat_command,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE)\n\n    # Prepare args to ProfilerClient\n    logdir = os.path.join(self.temp_dir, \'logs\')\n    worker_list = \'\'\n    duration_ms = 1000\n    num_tracing_attempts = 3\n    os.makedirs(logdir)\n\n    # Send a tracing request\n    profiler_client.trace(grpc_addr, logdir, duration_ms, worker_list,\n                          num_tracing_attempts)\n\n    #  Log stdout & stderr of subprocess issuing predict requests for debugging\n    out, err = proc.communicate()\n    print(""stdout: \'{}\' | stderr: \'{}\'"".format(out, err))\n\n  def test_tf_text(self):\n    """"""Test TF Text.""""""\n    model_path = os.path.join(flags.os.environ[\'TEST_SRCDIR\'],\n                              \'tf_serving/tensorflow_serving\',\n                              \'servables/tensorflow/testdata\',\n                              \'tf_text_regression\')\n    model_server_address = TensorflowModelServerTest.RunServer(\n        \'default\',\n        model_path)[1]\n    self.VerifyPredictRequest(\n        model_server_address,\n        expected_output=3.0,\n        expected_version=self._GetModelVersion(model_path),\n        rpc_timeout=600)\n\n\nif __name__ == \'__main__\':\n  tf.enable_eager_execution()\n  tf.test.main()\n'"
tensorflow_serving/model_servers/tensorflow_model_server_test_client.py,3,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Manual test client for tensorflow_model_server.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# This is a placeholder for a Google-internal import.\n\nimport grpc\nimport tensorflow as tf\n\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.python.platform import flags\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\n\ntf.app.flags.DEFINE_string(\'server\', \'localhost:8500\',\n                           \'inception_inference service host:port\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  # Prepare request\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = \'default\'\n  request.inputs[\'x\'].dtype = types_pb2.DT_FLOAT\n  request.inputs[\'x\'].float_val.append(2.0)\n  request.output_filter.append(\'y\')\n  # Send request\n  channel = grpc.insecure_channel(FLAGS.server)\n  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n  print(stub.Predict(request, 5.0))  # 5 secs timeout\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
tensorflow_serving/batching/test_util/matrix_half_plus_two_saved_model.py,10,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport tensorflow as tf\n\n\ndef _generate_saved_model_for_matrix_half_plus_two(export_dir):\n  """"""Creates SavedModel for half plus two model that accepts batches of\n       3*3 matrices.\n       The model divides all elements in each matrix by 2 and adds 2 to them.\n       So, for one input matrix [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n       the result will be [[2.5, 3, 3.5], [4, 4.5, 5], [5.5, 6, 6.5]].\n    Args:\n      export_dir: The directory where to write SavedModel files.\n    """"""\n  builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n  with tf.Session() as session:\n    x = tf.placeholder(tf.float32, shape=[None, 3, 3], name=""x"")\n    a = tf.constant(0.5)\n    b = tf.constant(2.0)\n    y = tf.add(tf.multiply(a, x), b, name=""y"")\n    predict_signature_def = (\n        tf.saved_model.signature_def_utils.predict_signature_def({\n            ""x"": x\n        }, {""y"": y}))\n    signature_def_map = {\n        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n            predict_signature_def\n    }\n    session.run(tf.global_variables_initializer())\n    builder.add_meta_graph_and_variables(\n        session, [tf.saved_model.tag_constants.SERVING],\n        signature_def_map=signature_def_map)\n    builder.save()\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--output_dir"",\n      type=str,\n      default=""/tmp/matrix_half_plus_two/1"",\n      help=""The directory where to write SavedModel files."")\n  args = parser.parse_args()\n  _generate_saved_model_for_matrix_half_plus_two(args.output_dir)\n'"
tensorflow_serving/model_servers/test_util/tensorflow_model_server_test_base.py,5,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for tensorflow_model_server.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport atexit\nimport json\nimport os\nimport shlex\nimport socket\nimport subprocess\nimport time\n\n# This is a placeholder for a Google-internal import.\n\nimport grpc\nfrom six.moves import range\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.saved_model import signature_constants\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\nFLAGS = flags.FLAGS\n\nRPC_TIMEOUT = 5.0\nHTTP_REST_TIMEOUT_MS = 5000\nCHANNEL_WAIT_TIMEOUT = 5.0\nWAIT_FOR_SERVER_READY_INT_SECS = 60\nGRPC_SOCKET_PATH = \'/tmp/tf-serving.sock\'\n\n\ndef SetVirtualCpus(num_virtual_cpus):\n  """"""Create virtual CPU devices if they haven\'t yet been created.""""""\n  if num_virtual_cpus < 1:\n    raise ValueError(\'`num_virtual_cpus` must be at least 1 not %r\' %\n                     (num_virtual_cpus,))\n  physical_devices = tf.config.experimental.list_physical_devices(\'CPU\')\n  if not physical_devices:\n    raise RuntimeError(\'No CPUs found\')\n  configs = tf.config.experimental.get_virtual_device_configuration(\n      physical_devices[0])\n  if configs is None:\n    virtual_devices = [tf.config.experimental.VirtualDeviceConfiguration()\n                       for _ in range(num_virtual_cpus)]\n    tf.config.experimental.set_virtual_device_configuration(\n        physical_devices[0], virtual_devices)\n  else:\n    if len(configs) < num_virtual_cpus:\n      raise RuntimeError(\'Already configured with %d < %d virtual CPUs\' %\n                         (len(configs), num_virtual_cpus))\n\n\ndef PickUnusedPort():\n  s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n  s.bind((\'\', 0))\n  port = s.getsockname()[1]\n  s.close()\n  return port\n\n\ndef WaitForServerReady(port):\n  """"""Waits for a server on the localhost to become ready.""""""\n  for _ in range(0, WAIT_FOR_SERVER_READY_INT_SECS):\n    time.sleep(1)\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = \'intentionally_missing_model\'\n\n    try:\n      # Send empty request to missing model\n      channel = grpc.insecure_channel(\'localhost:{}\'.format(port))\n      stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n      stub.Predict(request, RPC_TIMEOUT)\n    except grpc.RpcError as error:\n      # Missing model error will have details containing \'Servable\'\n      if \'Servable\' in error.details():\n        print(\'Server is ready\')\n        break\n\n\ndef CallREST(url, req, max_attempts=60):\n  """"""Returns HTTP response body from a REST API call.""""""\n  for attempt in range(max_attempts):\n    try:\n      print(\'Attempt {}: Sending request to {} with data:\\n{}\'.format(\n          attempt, url, req))\n      json_data = json.dumps(req).encode(\'utf-8\') if req is not None else None\n      resp = urllib.request.urlopen(urllib.request.Request(url, data=json_data))\n      resp_data = resp.read()\n      print(\'Received response:\\n{}\'.format(resp_data))\n      resp.close()\n      return resp_data\n    except Exception as e:  # pylint: disable=broad-except\n      print(\'Failed attempt {}. Error: {}\'.format(attempt, e))\n      if attempt == max_attempts - 1:\n        raise\n      print(\'Retrying...\')\n      time.sleep(1)\n\n\ndef SortedObject(obj):\n  """"""Returns sorted object (with nested list/dictionaries).""""""\n  if isinstance(obj, dict):\n    return sorted((k, SortedObject(v)) for k, v in obj.items())\n  if isinstance(obj, list):\n    return sorted(SortedObject(x) for x in obj)\n  if isinstance(obj, tuple):\n    return list(sorted(SortedObject(x) for x in obj))\n  else:\n    return obj\n\n\nclass TensorflowModelServerTestBase(tf.test.TestCase):\n  """"""This class defines integration test cases for tensorflow_model_server.""""""\n\n  @staticmethod\n  def __TestSrcDirPath(relative_path=\'\'):\n    return os.path.join(os.environ[\'TEST_SRCDIR\'],\n                        \'tf_serving/tensorflow_serving\', relative_path)\n\n  @staticmethod\n  def GetArgsKey(*args, **kwargs):\n    return args + tuple(sorted(kwargs.items()))\n\n  # Maps string key -> 2-tuple of \'host:port\' string.\n  model_servers_dict = {}\n\n  @staticmethod\n  def RunServer(model_name,\n                model_path,\n                model_type=\'tf\',\n                model_config_file=None,\n                monitoring_config_file=None,\n                batching_parameters_file=None,\n                grpc_channel_arguments=\'\',\n                wait_for_server_ready=True,\n                pipe=None,\n                model_config_file_poll_period=None):\n    """"""Run tensorflow_model_server using test config.\n\n    A unique instance of server is started for each set of arguments.\n    If called with same arguments, handle to an existing server is\n    returned.\n\n    Args:\n      model_name: Name of model.\n      model_path: Path to model.\n      model_type: Type of model TensorFlow (\'tf\') or TF Lite (\'tflite\').\n      model_config_file: Path to model config file.\n      monitoring_config_file: Path to the monitoring config file.\n      batching_parameters_file: Path to batching parameters.\n      grpc_channel_arguments: Custom gRPC args for server.\n      wait_for_server_ready: Wait for gRPC port to be ready.\n      pipe: subpipe.PIPE object to read stderr from server.\n      model_config_file_poll_period: Period for polling the\n      filesystem to discover new model configs.\n\n    Returns:\n      3-tuple (<Popen object>, <grpc host:port>, <rest host:port>).\n\n    Raises:\n      ValueError: when both model_path and config_file is empty.\n    """"""\n    args_key = TensorflowModelServerTestBase.GetArgsKey(**locals())\n    if args_key in TensorflowModelServerTestBase.model_servers_dict:\n      return TensorflowModelServerTestBase.model_servers_dict[args_key]\n    port = PickUnusedPort()\n    rest_api_port = PickUnusedPort()\n    print((\'Starting test server on port: {} for model_name: \'\n           \'{}/model_config_file: {}\'.format(port, model_name,\n                                             model_config_file)))\n    command = os.path.join(\n        TensorflowModelServerTestBase.__TestSrcDirPath(\'model_servers\'),\n        \'tensorflow_model_server\')\n    command += \' --port=\' + str(port)\n    command += \' --rest_api_port=\' + str(rest_api_port)\n    command += \' --rest_api_timeout_in_ms=\' + str(HTTP_REST_TIMEOUT_MS)\n    command += \' --grpc_socket_path=\' + GRPC_SOCKET_PATH\n\n    if model_config_file:\n      command += \' --model_config_file=\' + model_config_file\n    elif model_path:\n      command += \' --model_name=\' + model_name\n      command += \' --model_base_path=\' + model_path\n    else:\n      raise ValueError(\'Both model_config_file and model_path cannot be empty!\')\n\n    if model_type == \'tflite\':\n      command += \' --prefer_tflite_model=true\'\n\n    if monitoring_config_file:\n      command += \' --monitoring_config_file=\' + monitoring_config_file\n\n    if model_config_file_poll_period is not None:\n      command += \' --model_config_file_poll_wait_seconds=\' + str(\n          model_config_file_poll_period)\n\n    if batching_parameters_file:\n      command += \' --enable_batching\'\n      command += \' --batching_parameters_file=\' + batching_parameters_file\n    if grpc_channel_arguments:\n      command += \' --grpc_channel_arguments=\' + grpc_channel_arguments\n    print(command)\n    proc = subprocess.Popen(shlex.split(command), stderr=pipe)\n    atexit.register(proc.kill)\n    print(\'Server started\')\n    if wait_for_server_ready:\n      WaitForServerReady(port)\n    hostports = (\n        proc,\n        \'localhost:\' + str(port),\n        \'localhost:\' + str(rest_api_port),\n    )\n    TensorflowModelServerTestBase.model_servers_dict[args_key] = hostports\n    return hostports\n\n  def VerifyPredictRequest(\n      self,\n      model_server_address,\n      expected_output,\n      expected_version,\n      model_name=\'default\',\n      specify_output=True,\n      batch_input=False,\n      signature_name=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,\n      rpc_timeout=RPC_TIMEOUT):\n    """"""Send PredictionService.Predict request and verify output.""""""\n    print(\'Sending Predict request...\')\n    # Prepare request\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = model_name\n    request.model_spec.signature_name = signature_name\n    request.inputs[\'x\'].dtype = types_pb2.DT_FLOAT\n    request.inputs[\'x\'].float_val.append(2.0)\n    dim = request.inputs[\'x\'].tensor_shape.dim.add()\n    dim.size = 1\n    if batch_input:\n      request.inputs[\'x\'].tensor_shape.dim.add().size = 1\n\n    if specify_output:\n      request.output_filter.append(\'y\')\n    # Send request\n    channel = grpc.insecure_channel(model_server_address)\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n    result = stub.Predict(request, rpc_timeout)  # 5 secs timeout\n    # Verify response\n    self.assertTrue(\'y\' in result.outputs)\n    self.assertEqual(types_pb2.DT_FLOAT, result.outputs[\'y\'].dtype)\n    self.assertEqual(1, len(result.outputs[\'y\'].float_val))\n    self.assertEqual(expected_output, result.outputs[\'y\'].float_val[0])\n    self._VerifyModelSpec(result.model_spec, request.model_spec.name,\n                          signature_name, expected_version)\n\n  def _GetSavedModelBundlePath(self):\n    """"""Returns a path to a model in SavedModel format.""""""\n    return os.path.join(self.testdata_dir, \'saved_model_half_plus_two_cpu\')\n\n  def _GetModelVersion(self, model_path):\n    """"""Returns version of SavedModel/SessionBundle in given path.\n\n    This method assumes there is exactly one directory with an \'int\' valued\n    directory name under `model_path`.\n\n    Args:\n      model_path: A string representing path to the SavedModel/SessionBundle.\n\n    Returns:\n      version of SavedModel/SessionBundle in given path.\n    """"""\n    return int(os.listdir(model_path)[0])\n\n  def _GetSavedModelHalfPlusThreePath(self):\n    """"""Returns a path to a half_plus_three model in SavedModel format.""""""\n    return os.path.join(self.testdata_dir, \'saved_model_half_plus_three\')\n\n  def _GetTfLiteModelPath(self):\n    """"""Returns a path to a model in TF Lite format.""""""\n    return os.path.join(self.testdata_dir, \'saved_model_half_plus_two_tflite\')\n\n  def _GetTfLiteModelWithSigDefPath(self):\n    """"""Returns a path to a model in TF Lite format.""""""\n    return os.path.join(self.testdata_dir,\n                        \'saved_model_half_plus_two_tflite_with_sigdef\')\n\n  def _GetSessionBundlePath(self):\n    """"""Returns a path to a model in SessionBundle format.""""""\n    return os.path.join(self.session_bundle_testdata_dir, \'half_plus_two\')\n\n  def _GetGoodModelConfigTemplate(self):\n    """"""Returns a path to a working configuration file template.""""""\n    return os.path.join(self.testdata_dir, \'good_model_config.txt\')\n\n  def _GetGoodModelConfigFile(self):\n    """"""Returns a path to a working configuration file.""""""\n    return os.path.join(self.temp_dir, \'good_model_config.conf\')\n\n  def _GetBadModelConfigFile(self):\n    """"""Returns a path to a improperly formatted configuration file.""""""\n    return os.path.join(self.testdata_dir, \'bad_model_config.txt\')\n\n  def _GetBatchingParametersFile(self):\n    """"""Returns a path to a batching configuration file.""""""\n    return os.path.join(self.testdata_dir, \'batching_config.txt\')\n\n  def _GetModelMetadataFile(self):\n    """"""Returns a path to a sample model metadata file.""""""\n    return os.path.join(self.testdata_dir, \'half_plus_two_model_metadata.json\')\n\n  def _GetMonitoringConfigFile(self):\n    """"""Returns a path to a monitoring configuration file.""""""\n    return os.path.join(self.testdata_dir, \'monitoring_config.txt\')\n\n  def _VerifyModelSpec(self,\n                       actual_model_spec,\n                       exp_model_name,\n                       exp_signature_name,\n                       exp_version):\n    """"""Verifies model_spec matches expected model name, signature, version.\n\n    Args:\n      actual_model_spec: An instance of ModelSpec proto.\n      exp_model_name: A string that represents expected model name.\n      exp_signature_name: A string that represents expected signature.\n      exp_version: An integer that represents expected version.\n\n    Returns:\n      None.\n    """"""\n    self.assertEqual(actual_model_spec.name, exp_model_name)\n    self.assertEqual(actual_model_spec.signature_name, exp_signature_name)\n    self.assertEqual(actual_model_spec.version.value, exp_version)\n\n  def _TestPredict(\n      self,\n      model_path,\n      batching_parameters_file=None,\n      signature_name=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY):\n    """"""Helper method to test prediction.\n\n    Args:\n      model_path:      Path to the model on disk.\n      batching_parameters_file: Batching parameters file to use (if None\n                                batching is not enabled).\n      signature_name: Signature name to expect in the PredictResponse.\n    """"""\n    model_server_address = TensorflowModelServerTestBase.RunServer(\n        \'default\',\n        model_path,\n        batching_parameters_file=batching_parameters_file)[1]\n    expected_version = self._GetModelVersion(model_path)\n    self.VerifyPredictRequest(model_server_address, expected_output=3.0,\n                              expected_version=expected_version,\n                              signature_name=signature_name)\n    self.VerifyPredictRequest(\n        model_server_address, expected_output=3.0, specify_output=False,\n        expected_version=expected_version, signature_name=signature_name)\n'"
tensorflow_serving/tools/pip_package/setup.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TensorFlow Serving Python API.\n\nTensorFlow Serving is a flexible, high-performance serving system for machine\nlearning models, designed for production environments.TensorFlow Serving makes\nit easy to deploy new algorithms and experiments, while keeping the same server\narchitecture and APIs. TensorFlow Serving provides out-of-the-box integration\nwith TensorFlow models, but can be easily extended to serve other types of\nmodels and data.\n\nThis package contains the TensorFlow Serving Python APIs.\n""""""\n\nimport sys\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nDOCLINES = __doc__.split(\'\\n\')\n\n# Set when releasing a new version of TensorFlow Serving (e.g. 1.0.0).\n_VERSION = \'0.0.0\'\n# Have this by default be open; releasing a new version will lock to TF version\n_TF_VERSION = \'>=1.2.0,<3\'\n_TF_VERSION_SANITIZED = _TF_VERSION.replace(\'-\', \'\')\n\nproject_name = \'tensorflow-serving-api\'\n# Set when building the pip package\nif \'--project_name\' in sys.argv:\n  project_name_idx = sys.argv.index(\'--project_name\')\n  project_name = sys.argv[project_name_idx + 1]\n  sys.argv.remove(\'--project_name\')\n  sys.argv.pop(project_name_idx)\n\n_TF_REQ = [\'tensorflow\'+_TF_VERSION_SANITIZED]\n\n# GPU build (note: the only difference is we depend on tensorflow-gpu so\n# pip doesn\'t overwrite it with the CPU build)\nif \'tensorflow-serving-api-gpu\' in project_name:\n  _TF_REQ = [\'tensorflow-gpu\'+_TF_VERSION_SANITIZED]\n\n\nREQUIRED_PACKAGES = [\n    \'grpcio>=1.0<2\',\n    \'protobuf>=3.6.0\',\n] + _TF_REQ\n\nsetup(\n    name=project_name,\n    version=_VERSION.replace(\'-\', \'\'),\n    author=\'Google Inc.\',\n    author_email=\'tensorflow-serving-dev@googlegroups.com\',\n    packages=find_packages(),\n    description=DOCLINES[0],\n    long_description=\'\\n\'.join(DOCLINES[2:]),\n    license=\'Apache 2.0\',\n    url=\'http://tensorflow.org/serving\',\n    keywords=\'tensorflow serving machine learning api libraries\',\n    install_requires=REQUIRED_PACKAGES,\n    classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n)\n'"
tensorflow_serving/servables/tensorflow/testdata/export_bad_half_plus_two.py,8,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Exports a toy TensorFlow model without signatures.\n\nExports half_plus_two TensorFlow model to /tmp/bad_half_plus_two/ without\nsignatures. This is used to test the fault-tolerance of tensorflow_model_server.\n""""""\n\nimport os\n\n# This is a placeholder for a Google-internal import.\n\nimport tensorflow as tf\n\n\ndef Export():\n  export_path = ""/tmp/bad_half_plus_two/00000123""\n  with tf.Session() as sess:\n    # Make model parameters a&b variables instead of constants to\n    # exercise the variable reloading mechanisms.\n    a = tf.Variable(0.5)\n    b = tf.Variable(2.0)\n\n    # Calculate, y = a*x + b\n    # here we use a placeholder \'x\' which is fed at inference time.\n    x = tf.placeholder(tf.float32)\n    y = tf.add(tf.multiply(a, x), b)\n\n    # Export the model without signatures.\n    # Note that the model is intentionally exported without using exporter,\n    # but using the same format. This is to avoid exporter creating default\n    # empty signatures upon export.\n    tf.global_variables_initializer().run()\n    saver = tf.train.Saver()\n    saver.export_meta_graph(\n        filename=os.path.join(export_path, ""export.meta""))\n    saver.save(sess,\n               os.path.join(export_path, ""export""),\n               write_meta_graph=False)\n\n\ndef main(_):\n  Export()\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
tensorflow_serving/servables/tensorflow/testdata/export_counter.py,16,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Exports a counter model.\n\nIt contains 4 signatures: get_counter incr_counter, incr_counter_by, and\nreset_counter, to test Predict service.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# This is a placeholder for a Google-internal import.\nimport tensorflow as tf\n\n\ndef save_model(sess, signature_def_map, output_dir):\n  """"""Saves the model with given signature def map.""""""\n  builder = tf.saved_model.builder.SavedModelBuilder(output_dir)\n  builder.add_meta_graph_and_variables(\n      sess, [tf.saved_model.tag_constants.SERVING],\n      signature_def_map=signature_def_map)\n  builder.save()\n\n\ndef build_signature_def_from_tensors(inputs, outputs, method_name):\n  """"""Builds signature def with inputs, outputs, and method_name.""""""\n  return tf.saved_model.signature_def_utils.build_signature_def(\n      inputs={\n          key: tf.saved_model.utils.build_tensor_info(tensor)\n          for key, tensor in inputs.items()\n      },\n      outputs={\n          key: tf.saved_model.utils.build_tensor_info(tensor)\n          for key, tensor in outputs.items()\n      },\n      method_name=method_name)\n\n\ndef export_model(output_dir):\n  """"""Exports the counter model.\n\n  Create three signatures: incr_counter, incr_counter_by, reset_counter.\n\n  *Notes*: These signatures are stateful and over-simplied only to demonstrate\n  Predict calls with only inputs or outputs. State is not supported in\n  TensorFlow Serving on most scalable or production hosting environments.\n\n  Args:\n    output_dir: string, output directory for the model.\n  """"""\n  tf.logging.info(""Exporting the counter model to %s."", output_dir)\n  method_name = tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n\n  graph = tf.Graph()\n  with graph.as_default(), tf.Session() as sess:\n    counter = tf.Variable(0.0, dtype=tf.float32, name=""counter"")\n\n    with tf.name_scope(""incr_counter_op"", values=[counter]):\n      incr_counter = counter.assign_add(1.0)\n\n    delta = tf.placeholder(dtype=tf.float32, name=""delta"")\n    with tf.name_scope(""incr_counter_by_op"", values=[counter, delta]):\n      incr_counter_by = counter.assign_add(delta)\n\n    with tf.name_scope(""reset_counter_op"", values=[counter]):\n      reset_counter = counter.assign(0.0)\n\n    sess.run(tf.global_variables_initializer())\n\n    signature_def_map = {\n        ""get_counter"":\n            build_signature_def_from_tensors({}, {""output"": counter},\n                                             method_name),\n        ""incr_counter"":\n            build_signature_def_from_tensors({}, {""output"": incr_counter},\n                                             method_name),\n        ""incr_counter_by"":\n            build_signature_def_from_tensors({\n                ""delta"": delta\n            }, {""output"": incr_counter_by}, method_name),\n        ""reset_counter"":\n            build_signature_def_from_tensors({}, {""output"": reset_counter},\n                                             method_name)\n    }\n    save_model(sess, signature_def_map, output_dir)\n\n\ndef main(unused_argv):\n  export_model(""/tmp/saved_model_counter/00000123"")\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two.py,64,"b'## Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Exports an example linear regression inference graph.\n\nExports a TensorFlow graph to `/tmp/saved_model/half_plus_two/` based on the\n`SavedModel` format.\n\nThis graph calculates,\n\n\\\\(\n  y = a*x + b\n\\\\)\n\nand/or, independently,\n\n\\\\(\n  y2 = a*x2 + c\n\\\\)\n\nwhere `a`, `b` and `c` are variables with `a=0.5` and `b=2` and `c=3`.\n\nOutput from this program is typically used to exercise SavedModel load and\nexecution code.\n\nTo create a CPU model:\n  bazel run -c opt saved_model_half_plus_two -- --device=cpu\n\nTo create a CPU model with Intel MKL-DNN optimizations:\n  bazel run -c opt saved_model_half_plus_two -- --device=mkl\n\nTo create GPU model:\n  bazel run --config=cuda -c opt saved_model_half_plus_two -- \\\n  --device=gpu\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\n# This is a placeholder for a Google-internal import.\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.lite.tools.signature import signature_def_utils\nfrom tensorflow.python.lib.io import file_io\n\nFLAGS = None\n\n\ndef _write_assets(assets_directory, assets_filename):\n  """"""Writes asset files to be used with SavedModel for half plus two.\n\n  Args:\n    assets_directory: The directory to which the assets should be written.\n    assets_filename: Name of the file to which the asset contents should be\n        written.\n\n  Returns:\n    The path to which the assets file was written.\n  """"""\n  if not file_io.file_exists(assets_directory):\n    file_io.recursive_create_dir(assets_directory)\n\n  path = os.path.join(\n      tf.compat.as_bytes(assets_directory), tf.compat.as_bytes(assets_filename))\n  file_io.write_string_to_file(path, ""asset-file-contents"")\n  return path\n\n\ndef _build_regression_signature(input_tensor, output_tensor):\n  """"""Helper function for building a regression SignatureDef.""""""\n  input_tensor_info = tf.saved_model.utils.build_tensor_info(input_tensor)\n  signature_inputs = {\n      tf.saved_model.signature_constants.REGRESS_INPUTS: input_tensor_info\n  }\n  output_tensor_info = tf.saved_model.utils.build_tensor_info(output_tensor)\n  signature_outputs = {\n      tf.saved_model.signature_constants.REGRESS_OUTPUTS: output_tensor_info\n  }\n  return tf.saved_model.signature_def_utils.build_signature_def(\n      signature_inputs, signature_outputs,\n      tf.saved_model.signature_constants.REGRESS_METHOD_NAME)\n\n\n# Possibly extend this to allow passing in \'classes\', but for now this is\n# sufficient for testing purposes.\ndef _build_classification_signature(input_tensor, scores_tensor):\n  """"""Helper function for building a classification SignatureDef.""""""\n  input_tensor_info = tf.saved_model.utils.build_tensor_info(input_tensor)\n  signature_inputs = {\n      tf.saved_model.signature_constants.CLASSIFY_INPUTS: input_tensor_info\n  }\n  output_tensor_info = tf.saved_model.utils.build_tensor_info(scores_tensor)\n  signature_outputs = {\n      tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:\n          output_tensor_info\n  }\n  return tf.saved_model.signature_def_utils.build_signature_def(\n      signature_inputs, signature_outputs,\n      tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME)\n\n\ndef _create_asset_file():\n  """"""Helper to create assets file. Returns a tensor for the filename.""""""\n  # Create an assets file that can be saved and restored as part of the\n  # SavedModel.\n  original_assets_directory = ""/tmp/original/export/assets""\n  original_assets_filename = ""foo.txt""\n  original_assets_filepath = _write_assets(original_assets_directory,\n                                           original_assets_filename)\n\n  # Set up the assets collection.\n  assets_filepath = tf.constant(original_assets_filepath)\n  tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, assets_filepath)\n  filename_tensor = tf.Variable(\n      original_assets_filename,\n      name=""filename_tensor"",\n      trainable=False,\n      collections=[])\n  return filename_tensor.assign(original_assets_filename)\n\n\ndef _write_mlmd(export_dir, mlmd_uuid):\n  """"""Writes an ML Metadata UUID into the assets.extra directory.\n\n  Args:\n    export_dir: The export directory for the SavedModel.\n    mlmd_uuid: The string to write as the ML Metadata UUID.\n\n  Returns:\n    The path to which the MLMD UUID was written.\n  """"""\n  assets_extra_directory = os.path.join(export_dir, ""assets.extra"")\n  if not file_io.file_exists(assets_extra_directory):\n    file_io.recursive_create_dir(assets_extra_directory)\n  path = os.path.join(assets_extra_directory, ""mlmd_uuid"")\n  file_io.write_string_to_file(path, mlmd_uuid)\n  return path\n\n\ndef _generate_saved_model_for_half_plus_two(export_dir,\n                                            as_text=False,\n                                            as_tflite=False,\n                                            as_tflite_with_sigdef=False,\n                                            use_main_op=False,\n                                            include_mlmd=False,\n                                            device_type=""cpu""):\n  """"""Generates SavedModel for half plus two.\n\n  Args:\n    export_dir: The directory to which the SavedModel should be written.\n    as_text: Writes the SavedModel protocol buffer in text format to disk.\n    as_tflite: Writes the Model in Tensorflow Lite format to disk.\n    as_tflite_with_sigdef: Writes the Model with SignatureDefs in Tensorflow\n      Lite format to disk.\n    use_main_op: Whether to supply a main op during SavedModel build time.\n    include_mlmd: Whether to include an MLMD key in the SavedModel.\n    device_type: Device to force ops to run on.\n  """"""\n  builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n\n  device_name = ""/cpu:0""\n  if device_type == ""gpu"":\n    device_name = ""/gpu:0""\n\n  with tf.Session(\n      graph=tf.Graph(),\n      config=tf.ConfigProto(log_device_placement=True)) as sess:\n    with tf.device(device_name):\n      # Set up the model parameters as variables to exercise variable loading\n      # functionality upon restore.\n      a = tf.Variable(0.5, name=""a"")\n      b = tf.Variable(2.0, name=""b"")\n      c = tf.Variable(3.0, name=""c"")\n\n      # Create a placeholder for serialized tensorflow.Example messages to be\n      # fed.\n      serialized_tf_example = tf.placeholder(\n          tf.string, name=""tf_example"", shape=[None])\n\n      # Parse the tensorflow.Example looking for a feature named ""x"" with a\n      # single floating point value.\n      feature_configs = {\n          ""x"": tf.FixedLenFeature([1], dtype=tf.float32),\n          ""x2"": tf.FixedLenFeature([1], dtype=tf.float32, default_value=[0.0])\n      }\n      # parse_example only works on CPU\n      with tf.device(""/cpu:0""):\n        tf_example = tf.parse_example(serialized_tf_example, feature_configs)\n\n      if as_tflite:\n        # TFLite v1 converter does not support unknown shape.\n        x = tf.ensure_shape(tf_example[""x""], (1, 1), name=""x"")\n      else:\n        # Use tf.identity() to assign name\n        x = tf.identity(tf_example[""x""], name=""x"")\n\n      if as_tflite_with_sigdef:\n        # Resulting TFLite model will have input named ""tflite_input"".\n        x = tf.ensure_shape(tf_example[""x""], (1, 1), name=""tflite_input"")\n\n      if device_type == ""mkl"":\n        # Create a small convolution op to trigger MKL\n        # The op will return 0s so this won\'t affect the\n        # resulting calculation.\n        o1 = tf.keras.layers.Conv2D(1, [1, 1])(tf.zeros((1, 16, 16, 1)))\n        y = o1[0, 0, 0, 0] + tf.add(tf.multiply(a, x), b)\n      else:\n        y = tf.add(tf.multiply(a, x), b)\n\n      y = tf.identity(y, name=""y"")\n\n      if device_type == ""mkl"":\n        # Create a small convolution op to trigger MKL\n        # The op will return 0s so this won\'t affect the\n        # resulting calculation.\n        o2 = tf.keras.layers.Conv2D(1, [1, 1])(tf.zeros((1, 16, 16, 1)))\n        y2 = o2[0, 0, 0, 0] + tf.add(tf.multiply(a, x), c)\n      else:\n        y2 = tf.add(tf.multiply(a, x), c)\n\n      y2 = tf.identity(y2, name=""y2"")\n\n      x2 = tf.identity(tf_example[""x2""], name=""x2"")\n\n      if device_type == ""mkl"":\n        # Create a small convolution op to trigger MKL\n        # The op will return 0s so this won\'t affect the\n        # resulting calculation.\n        o3 = tf.keras.layers.Conv2D(1, [1, 1])(tf.zeros((1, 16, 16, 1)))\n        y3 = o3[0, 0, 0, 0] + tf.add(tf.multiply(a, x2), c)\n      else:\n        # Add separate constants for x2, to prevent optimizers like TF-TRT from\n        # fusing the paths to compute y/y2 and y3 together.\n        a2 = tf.Variable(0.5, name=""a2"")\n        c2 = tf.Variable(3.0, name=""c2"")\n        y3 = tf.add(tf.multiply(a2, x2), c2)\n\n      y3 = tf.identity(y3, name=""y3"")\n\n    assign_filename_op = _create_asset_file()\n\n    # Set up the signature for Predict with input and output tensor\n    # specification.\n    predict_input_tensor = tf.saved_model.utils.build_tensor_info(x)\n    predict_signature_inputs = {""x"": predict_input_tensor}\n\n    predict_output_tensor = tf.saved_model.utils.build_tensor_info(y)\n    predict_signature_outputs = {""y"": predict_output_tensor}\n    predict_signature_def = (\n        tf.saved_model.signature_def_utils.build_signature_def(\n            predict_signature_inputs, predict_signature_outputs,\n            tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n\n    signature_def_map = {\n        ""regress_x_to_y"":\n            _build_regression_signature(serialized_tf_example, y),\n        ""regress_x_to_y2"":\n            _build_regression_signature(serialized_tf_example, y2),\n        ""regress_x2_to_y3"":\n            _build_regression_signature(x2, y3),\n        ""classify_x_to_y"":\n            _build_classification_signature(serialized_tf_example, y),\n        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n            predict_signature_def\n    }\n    # Initialize all variables and then save the SavedModel.\n    sess.run(tf.global_variables_initializer())\n\n    if as_tflite or as_tflite_with_sigdef:\n      converter = tf.lite.TFLiteConverter.from_session(sess, [x], [y])\n      tflite_model = converter.convert()\n      if as_tflite_with_sigdef:\n        k = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n        tflite_model = signature_def_utils.set_signature_defs(\n            tflite_model, {k: predict_signature_def})\n      open(export_dir + ""/model.tflite"", ""wb"").write(tflite_model)\n    else:\n      if use_main_op:\n        builder.add_meta_graph_and_variables(\n            sess, [tf.saved_model.tag_constants.SERVING],\n            signature_def_map=signature_def_map,\n            assets_collection=tf.get_collection(tf.GraphKeys.ASSET_FILEPATHS),\n            main_op=tf.group(tf.saved_model.main_op.main_op(),\n                             assign_filename_op))\n      else:\n        builder.add_meta_graph_and_variables(\n            sess, [tf.saved_model.tag_constants.SERVING],\n            signature_def_map=signature_def_map,\n            assets_collection=tf.get_collection(tf.GraphKeys.ASSET_FILEPATHS),\n            main_op=tf.group(assign_filename_op))\n\n  if not as_tflite:\n    builder.save(as_text)\n\n  if include_mlmd:\n    _write_mlmd(export_dir, ""test_mlmd_uuid"")\n\n\ndef main(_):\n  _generate_saved_model_for_half_plus_two(\n      FLAGS.output_dir, device_type=FLAGS.device)\n  print(""SavedModel generated for %(device)s at: %(dir)s"" % {\n      ""device"": FLAGS.device,\n      ""dir"": FLAGS.output_dir\n  })\n\n  _generate_saved_model_for_half_plus_two(\n      FLAGS.output_dir_pbtxt, as_text=True, device_type=FLAGS.device)\n  print(""SavedModel generated for %(device)s at: %(dir)s"" % {\n      ""device"": FLAGS.device,\n      ""dir"": FLAGS.output_dir_pbtxt\n  })\n\n  _generate_saved_model_for_half_plus_two(\n      FLAGS.output_dir_main_op, use_main_op=True, device_type=FLAGS.device)\n  print(""SavedModel generated for %(device)s at: %(dir)s "" % {\n      ""device"": FLAGS.device,\n      ""dir"": FLAGS.output_dir_main_op\n  })\n\n  _generate_saved_model_for_half_plus_two(\n      FLAGS.output_dir_tflite, as_tflite=True, device_type=FLAGS.device)\n  print(""SavedModel in TFLite format generated for %(device)s at: %(dir)s "" % {\n      ""device"": FLAGS.device,\n      ""dir"": FLAGS.output_dir_tflite,\n  })\n\n  _generate_saved_model_for_half_plus_two(\n      FLAGS.output_dir_mlmd, include_mlmd=True, device_type=FLAGS.device)\n  print(""SavedModel with MLMD generated for %(device)s at: %(dir)s "" % {\n      ""device"": FLAGS.device,\n      ""dir"": FLAGS.output_dir_mlmd,\n  })\n\n  _generate_saved_model_for_half_plus_two(\n      FLAGS.output_dir_tflite_with_sigdef, device_type=FLAGS.device,\n      as_tflite_with_sigdef=True)\n  print(""SavedModel in TFLite format with SignatureDef generated for ""\n        ""%(device)s at: %(dir)s "" % {\n            ""device"": FLAGS.device,\n            ""dir"": FLAGS.output_dir_tflite_with_sigdef,\n        })\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--output_dir"",\n      type=str,\n      default=""/tmp/saved_model_half_plus_two"",\n      help=""Directory where to output SavedModel."")\n  parser.add_argument(\n      ""--output_dir_pbtxt"",\n      type=str,\n      default=""/tmp/saved_model_half_plus_two_pbtxt"",\n      help=""Directory where to output the text format of SavedModel."")\n  parser.add_argument(\n      ""--output_dir_main_op"",\n      type=str,\n      default=""/tmp/saved_model_half_plus_two_main_op"",\n      help=""Directory where to output the SavedModel with a main op."")\n  parser.add_argument(\n      ""--output_dir_tflite"",\n      type=str,\n      default=""/tmp/saved_model_half_plus_two_tflite"",\n      help=""Directory where to output model in TensorFlow Lite format."")\n  parser.add_argument(\n      ""--output_dir_mlmd"",\n      type=str,\n      default=""/tmp/saved_model_half_plus_two_mlmd"",\n      help=""Directory where to output the SavedModel with ML Metadata."")\n  parser.add_argument(\n      ""--output_dir_tflite_with_sigdef"",\n      type=str,\n      default=""/tmp/saved_model_half_plus_two_tflite_with_sigdef"",\n      help=(""Directory where to output model with signature def in ""\n            ""TensorFlow Lite format.""))\n  parser.add_argument(\n      ""--device"",\n      type=str,\n      default=""cpu"",\n      help=""Force model to run on \'cpu\', \'mkl\', or \'gpu\'"")\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
