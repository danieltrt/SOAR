file_path,api_count,code
Utilities/benchmark_compile.py,0,"b'""""""\nRuns ""swift build"" in a variety of configurations, and outputs timing\ninformation to an xUnit file.\n""""""\n\n\nimport argparse\nimport psutil\nimport subprocess\nimport tempfile\nimport time\n\n\nfrom junit_xml import TestCase, TestSuite\n\n\ndef kill(pid):\n  proc = psutil.Process(pid)\n  for child in proc.children(recursive=True):\n    child.kill()\n  proc.kill()\n\n\ndef execute_benchmark(test_case, cmd, timeout):\n  start = time.time()\n  proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  try:\n    stdout, stderr = proc.communicate(timeout=timeout)\n    code = proc.returncode\n  except subprocess.TimeoutExpired as e:\n    kill(proc.pid)\n    stdout, stderr = proc.communicate()\n    code = 0\n    test_case.add_failure_info(str(e))\n\n  end = time.time()\n  test_case.stdout = stdout\n  test_case.stderr = stderr\n  test_case.elapsed_sec = end - start\n\n  if code != 0:\n    test_case.add_failure_info(\'Nonzero exit code: %d\' % code)\n    return test_case\n\n  return test_case\n\n\ndef benchmark(test_case, cmd, timeout=600):\n  with tempfile.TemporaryDirectory() as build_path:\n    cmd += [\'--build-path\', build_path]\n    return execute_benchmark(test_case, cmd, timeout)\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'swift\', help=\'path to swift executable\')\n  parser.add_argument(\'output\', help=\'where to write xUnit output\')\n  args = parser.parse_args()\n\n  test_cases = [\n      benchmark(\n          TestCase(\'debug build\'),\n          [args.swift, \'build\', \'--product\', \'TensorFlow\']\n      ),\n      benchmark(\n          TestCase(\'release build\'),\n          [args.swift, \'build\', \'-c\', \'release\', \'--product\', \'TensorFlow\']\n      ),\n\n      # The point of ""release build -Onone"" is to compile TensorFlow in\n      # ""-whole-module-optimization"" mode without ""-O"".\n      benchmark(\n          TestCase(\'release build -Onone\'),\n          [args.swift, \'build\', \'-c\', \'release\', \'--product\', \'TensorFlow\',\n           \'-Xswiftc\', \'-Onone\']\n      ),\n  ]\n\n  test_suite = TestSuite(\'swift-apis compile time\', test_cases)\n\n  with open(args.output, \'w\') as f:\n    TestSuite.to_file(f, [test_suite])\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Utilities/ReferenceImplementations/gru.py,9,"b'# Computes expected results for `testGRU()` in `Tests/TensorFlowTests/LayerTests.swift`.\n# Requires \'tensorflow>=2.0.0a0\' (e.g. ""pip install tensorflow==2.2.0"").\n\nimport sys\nimport numpy\nimport tensorflow as tf\n\n# Set random seed for repetable results\ntf.random.set_seed(0)\n\ndef indented(s):\n    return \'\\n\'.join([\'    \' + l for l in s.split(\'\\n\')])\n\ndef swift_tensor(name, tensor):\n    if hasattr(tensor, \'numpy\'):\n        tensor = tensor.numpy()\n    def format_float(x):\n        formatted = numpy.format_float_positional(x, unique=True)\n        if formatted[-1] == \'.\':\n            return formatted + \'0\'\n        return formatted\n    formatter = {\n        \'float_kind\': format_float\n    }\n    return \'let {} = Tensor<Float>(\\n{}\\n)\'.format(\n        name,\n        indented(numpy.array2string(tensor, separator=\',\', formatter=formatter)))\n\nunits = 4\ninput_dim = 3\ninput_length = 4\ngo_backwards = ""go_backwards"" in sys.argv\n\n# Initialize the keras model with the GRU.\ngru = tf.keras.layers.GRU(\n    input_dim=input_dim,\n    units=units, \n    activation=""tanh"", recurrent_activation=""sigmoid"",\n    return_sequences=True, return_state=True,\n    go_backwards=go_backwards)\n\nx_input = tf.keras.Input(shape=[input_length, input_dim])\n\ninitial_state = tf.keras.Input(shape=[units])\ninitial_state_input = [initial_state]\n\noutput = gru(x_input, initial_state=initial_state_input)\nmodel = tf.keras.Model(inputs=[x_input, initial_state_input], outputs=[output])\n\n[kernel, recurrent_kernel, bias] = gru.get_weights()\n\nupdate_kernel = kernel[:, :units]\nupdate_recurrent_kernel = recurrent_kernel[:, :units]\nreset_kernel = kernel[:, units: units * 2]\nreset_recurrent_kernel = recurrent_kernel[:, units: units * 2]\nnew_kernel = kernel[:, units * 2:]\nnew_recurrent_kernel = recurrent_kernel[:, units * 2:]\nupdate_bias = bias[0][:units]\nupdate_recurrent_bias = bias[1][:units]\nreset_bias = bias[0][units: units * 2]\nreset_recurrent_bias = bias[1][units: units * 2]\nnew_bias = bias[0][units * 2:]\nnew_recurrent_bias = bias[1][units * 2:]\n\n# Print the GRU weights.\nprint(swift_tensor(\'updateKernel\', update_kernel))\nprint(swift_tensor(\'resetKernel\', reset_kernel))\nprint(swift_tensor(\'outputKernel\', new_kernel))\nprint(swift_tensor(\'updateRecurrentKernel\', update_recurrent_kernel))\nprint(swift_tensor(\'resetRecurrentKernel\', reset_recurrent_kernel))\nprint(swift_tensor(\'outputRecurrentKernel\', new_recurrent_kernel))\nprint(swift_tensor(\'updateBias\', update_bias))\nprint(swift_tensor(\'resetBias\', reset_bias))\nprint(swift_tensor(\'outputBias\', new_bias))\nprint(swift_tensor(\'updateRecurrentBias\', update_recurrent_bias))\nprint(swift_tensor(\'resetRecurrentBias\', reset_recurrent_bias))\nprint(swift_tensor(\'outputRecurrentBias\', new_recurrent_bias))\n\n# Initialize input data and print it.\nx = tf.keras.initializers.GlorotUniform()(shape=[1, input_length, input_dim])\ninitial_state = [\n    tf.keras.initializers.GlorotUniform()(shape=[1, units]),\n]\nprint(swift_tensor(\'x\', x))\nprint(swift_tensor(\'initialState\', initial_state[0]))\n\n# Run forwards and backwards pass and print the results.\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    tape.watch(initial_state)\n    [[states, final_state]] = model([x, initial_state])\n    sum_output = tf.reduce_sum(states[0][-1])\n\n[grad_model, grad_x, grad_initial_state] = tape.gradient(sum_output, [model.variables, x, initial_state])\n[grad_kernel, grad_recurrent_kernel, grad_bias] = grad_model\n[grad_initial_state] = grad_initial_state\n\ngrad_update_kernel = grad_kernel[:, :units]\ngrad_update_recurrent_kernel = grad_recurrent_kernel[:, :units]\ngrad_reset_kernel = grad_kernel[:, units: units * 2]\ngrad_reset_recurrent_kernel = grad_recurrent_kernel[:, units: units * 2]\ngrad_new_kernel = grad_kernel[:, units * 2:]\ngrad_new_recurrent_kernel = grad_recurrent_kernel[:, units * 2:]\ngrad_update_bias = grad_bias[0][:units]\ngrad_update_recurrent_bias = grad_bias[1][:units]\ngrad_reset_bias = grad_bias[0][units: units * 2]\ngrad_reset_recurrent_bias = grad_bias[1][units: units * 2]\ngrad_new_bias = grad_bias[0][units * 2:]\ngrad_new_recurrent_bias = grad_bias[1][units * 2:]\n\nprint(swift_tensor(\'expectedSum\', sum_output))\nprint(swift_tensor(\'expectedStates\', states))\nprint(swift_tensor(\'expectedFinalState\', final_state))\nprint(swift_tensor(\'expectedGradX\', grad_x))\nprint(swift_tensor(\'expectedGradInitialState\', grad_initial_state))\nprint(swift_tensor(\'expectedGradUpdateKernel\', grad_update_kernel))\nprint(swift_tensor(\'expectedGradResetKernel\', grad_reset_kernel))\nprint(swift_tensor(\'expectedGradOutputKernel\', grad_new_kernel))\nprint(swift_tensor(\'expectedGradUpdateRecurrentKernel\', grad_update_recurrent_kernel))\nprint(swift_tensor(\'expectedGradResetRecurrentKernel\', grad_reset_recurrent_kernel))\nprint(swift_tensor(\'expectedGradOutputRecurrentKernel\', grad_new_recurrent_kernel))\nprint(swift_tensor(\'expectedGradUpdateBias\', grad_update_bias))\nprint(swift_tensor(\'expectedGradResetBias\', grad_reset_bias))\nprint(swift_tensor(\'expectedGradOutputBias\', grad_new_bias))\nprint(swift_tensor(\'expectedGradUpdateRecurrentBias\', grad_update_recurrent_bias))\nprint(swift_tensor(\'expectedGradResetRecurrentBias\', grad_reset_recurrent_bias))\nprint(swift_tensor(\'expectedGradOutputRecurrentBias\', grad_new_recurrent_bias))\n'"
Utilities/ReferenceImplementations/lstm.py,10,"b'# Computes expected results for `testLSTM()` in `Tests/TensorFlowTests/LayerTests.swift`.\n# Requires \'tensorflow>=2.0.0a0\' (e.g. ""pip install tensorflow==2.0.0b1"").\n\nimport numpy\nimport tensorflow as tf\n\ndef indented(s):\n    return \'\\n\'.join([\'    \' + l for l in s.split(\'\\n\')])\n\ndef swift_tensor(name, tensor):\n    if hasattr(tensor, \'numpy\'):\n        tensor = tensor.numpy()\n    def format_float(x):\n        formatted = numpy.format_float_positional(x, unique=True)\n        if formatted[-1] == \'.\':\n            return formatted + \'0\'\n        return formatted\n    formatter = {\n        \'float_kind\': format_float\n    }\n    return \'let {} = Tensor<Float>(\\n{}\\n)\'.format(\n        name,\n        indented(numpy.array2string(tensor, separator=\',\', formatter=formatter)))\n\n# Initialize the keras model with the LSTM.\nlstm = tf.keras.layers.LSTM(units=4, return_sequences=True, return_state=True)\nx_input = tf.keras.Input(shape=[4, 4])\ninitial_state_hidden_input = tf.keras.Input(shape=[4])\ninitial_state_cell_input = tf.keras.Input(shape=[4])\ninitial_state_input = [initial_state_hidden_input, initial_state_cell_input]\noutput = lstm(x_input, initial_state=initial_state_input)\nmodel = tf.keras.Model(inputs=[x_input, initial_state_input], outputs=[output])\n\n# Print the LSTM weights.\n[kernel, recurrent_kernel, bias] = lstm.get_weights()\nprint(swift_tensor(\'kernel\', kernel))\nprint(swift_tensor(\'recurrentKernel\', recurrent_kernel))\nprint(swift_tensor(\'bias\', bias))\n\n# Initialize input data and print it.\nx = tf.keras.initializers.GlorotUniform()(shape=[1, 4, 4])\ninitial_state = [\n    tf.keras.initializers.GlorotUniform()(shape=[1, 4]),\n    tf.keras.initializers.GlorotUniform()(shape=[1, 4])\n]\nprint(swift_tensor(\'x\', x))\nprint(swift_tensor(\'initialStateHidden\', initial_state[0]))\nprint(swift_tensor(\'initialStateCell\', initial_state[1]))\n\n# Run forwards and backwards pass and print the results.\nwith tf.GradientTape() as tape:\n  tape.watch(x)\n  tape.watch(initial_state)\n  [[states, _, output]] = model([x, initial_state])\n  sum_output = tf.reduce_sum(output)\n[grad_model, grad_x, grad_initial_state] = tape.gradient(sum_output, [model.variables, x, initial_state])\n[grad_kernel, grad_recurrent_kernel, grad_bias] = grad_model\n[grad_initial_state_hidden, grad_initial_state_cell] = grad_initial_state\nprint(swift_tensor(\'expectedStates\', states))\nprint(swift_tensor(\'expectedOutput\', output))\nprint(swift_tensor(\'expectedGradKernel\', grad_kernel))\nprint(swift_tensor(\'expectedGradRecurrentKernel\', grad_recurrent_kernel))\nprint(swift_tensor(\'expectedGradBias\', grad_bias))\nprint(swift_tensor(\'expectedGradX\', grad_x))\nprint(swift_tensor(\'expectedGradInitialStateHidden\', grad_initial_state_hidden))\nprint(swift_tensor(\'expectedGradInitialStateCell\', grad_initial_state_cell))\n'"
Utilities/ReferenceImplementations/optimizers.py,2,"b'# Optimizer correctness reference implementations for\n# Tests/TensorFlowTests/OptimizerTests.swift.\n\n# Tested with:\n# - Python 3.7.6\n# - tensorflow==2.2.0rc0\n# - tensorflow-addons==0.8.3\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, RMSprop, SGD\nfrom tensorflow_addons.optimizers import RectifiedAdam\n\nnp.set_printoptions(precision=None, floatmode=""unique"")\n\ndef test_optimizer(optimizer, step_count=1000):\n    var = tf.Variable([0, 0, 0], dtype=tf.float32)\n    grad = tf.Variable([-5, 0.1, 0.2], dtype=tf.dtypes.float32)\n    grads_and_vars = list(zip([grad], [var]))\n    for i in range(step_count):\n        optimizer.apply_gradients(grads_and_vars)\n\n    print(optimizer._name)\n    print(\n        ""- After {} steps:"".format(step_count),\n        np.array2string(var.read_value().numpy(), separator="", ""),\n    )\n\n\ntest_optimizer(Adam(lr=1e-3, epsilon=1e-7))\ntest_optimizer(Adam(lr=1e-3, epsilon=1e-7, amsgrad=True, name=""Adam (amsgrad)""))\ntest_optimizer(Adadelta(lr=1e-3, epsilon=1e-7))\ntest_optimizer(Adagrad(lr=1e-3, epsilon=1e-7))\ntest_optimizer(Adamax(lr=1e-3, epsilon=1e-7))\ntest_optimizer(RectifiedAdam(lr=1e-3, epsilon=1e-7))\ntest_optimizer(RMSprop(lr=1e-3, epsilon=1e-7))\ntest_optimizer(SGD(lr=1e-3))\n'"
Utilities/ReferenceImplementations/rnn.py,9,"b'# Computes expected results for `testRNN()` in `Tests/TensorFlowTests/LayerTests.swift`.\n# Requires \'tensorflow>=2.0.0a0\' (e.g. ""pip install tensorflow==2.2.0"").\n\nimport numpy\nimport tensorflow as tf\n\n# Set random seed for repetable results\ntf.random.set_seed(0)\n\ndef indented(s):\n    return \'\\n\'.join([\'    \' + l for l in s.split(\'\\n\')])\n\ndef swift_tensor(name, tensor):\n    if hasattr(tensor, \'numpy\'):\n        tensor = tensor.numpy()\n    def format_float(x):\n        formatted = numpy.format_float_positional(x, unique=True)\n        if formatted[-1] == \'.\':\n            return formatted + \'0\'\n        return formatted\n    formatter = {\n        \'float_kind\': format_float\n    }\n    return \'let {} = Tensor<Float>(\\n{}\\n)\'.format(\n        name,\n        indented(numpy.array2string(tensor, separator=\',\', formatter=formatter)))\n\n# Initialize the keras model with the SimpleRNN.\nrnn = tf.keras.layers.SimpleRNN(\n    units=4, activation=""tanh"", \n    return_sequences=True, return_state=True)\n\nx_input = tf.keras.Input(shape=[4, 4])\n\ninitial_state = tf.keras.Input(shape=[4])\ninitial_state_input = [initial_state]\n\noutput = rnn(x_input, initial_state=initial_state_input)\nmodel = tf.keras.Model(inputs=[x_input, initial_state_input], outputs=[output])\n\n# Print the SimpleRNN weights.\n[kernel, recurrent_kernel, bias] = rnn.get_weights()\nprint(swift_tensor(\'kernel\', kernel))\nprint(swift_tensor(\'recurrentKernel\', recurrent_kernel))\nprint(swift_tensor(\'bias\', bias))\n\n# Initialize input data and print it.\nx = tf.keras.initializers.GlorotUniform()(shape=[1, 4, 4])\ninitial_state = [\n    tf.keras.initializers.GlorotUniform()(shape=[1, 4]),\n]\nprint(swift_tensor(\'x\', x))\nprint(swift_tensor(\'initialState\', initial_state[0]))\n\n# Run forwards and backwards pass and print the results.\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    tape.watch(initial_state)\n    [[states, final_state]] = model([x, initial_state])\n    sum_output = tf.reduce_sum(states[0][-1])\n\n[grad_model, grad_x, grad_initial_state] = tape.gradient(sum_output, [model.variables, x, initial_state])\n[grad_kernel, grad_recurrent_kernel, grad_bias] = grad_model\n[grad_initial_state] = grad_initial_state\nprint(swift_tensor(\'expectedSum\', sum_output))\nprint(swift_tensor(\'expectedStates\', states))\nprint(swift_tensor(\'expectedFinalState\', final_state))\nprint(swift_tensor(\'expectedGradKernel\', grad_kernel))\nprint(swift_tensor(\'expectedGradRecurrentKernel\', grad_recurrent_kernel))\nprint(swift_tensor(\'expectedGradBias\', grad_bias))\nprint(swift_tensor(\'expectedGradX\', grad_x))\nprint(swift_tensor(\'expectedGradInitialState\', grad_initial_state))\n'"
Sources/TensorFlow/Bindings/generate_wrappers.py,8,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Generates some swift wrapper from some ops description protobuf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom builtins import bytes\n\nimport json\nimport os\nimport six\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.python.framework import c_api_util\n\nflags = tf.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n  \'api_def_path\',\n  None,\n  \'path to the api_def directory, e.g. tensorflow/core/api_def/base_api\')\n\nflags.DEFINE_string(\n  \'output_path\',\n  None,\n  \'path for the generated swift file\')\n\nflags.DEFINE_string(\n  \'dispatching_output_path\',\n  None,\n  \'path for the generated swift file\')\n\n_WARNING = """"""// !!! THIS CODE IS AUTOMATICALLY GENERATED, DO NOT EDIT BY HAND !!!\n//\n""""""\n\n_HEADER = """"""// Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the ""License"");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an ""AS IS"" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n""""""\n\n_DISPATCHER_TEMPLATE = \'\'\'@available(\n  *, deprecated, renamed: ""_Raw"",\n  message:\n    """"""\n  \'Raw\' has been renamed to \'_Raw\' to indicate that it is not a guaranteed/stable API.\n  """"""\n)\npublic typealias Raw = _Raw\n\n#if USING_X10_BACKEND\n{raw_dispatching_enum}\n#else\npublic typealias _Raw = _RawTFEager\n#endif\n\'\'\'\n\n_OUTPUT_FILE = \'RawOpsGenerated.swift\'\n_RENAMED_KEYWORDS = {\n  \'\': \'empty\',\n  \'in\': \'in_\',\n  \'var\': \'var_\',\n  \'where\': \'where_\',\n  \'if\': \'if_\',\n  \'for\': \'for_\',\n  \'Int\': \'Intt\',\n  \'while\': \'while_\',\n  \'switch\': \'switch_\',\n  \'protocol\': \'protocol_\',\n  \'init\': \'init_\'}\n\n_TYPE_PROTOCOLS = [\n  (set(), \'TensorFlowScalar\'),\n  ({types_pb2.DT_UINT8,\n    types_pb2.DT_UINT16,\n    types_pb2.DT_UINT32,\n    types_pb2.DT_UINT64}, \'UnsignedInteger & TensorFlowScalar\'),\n  ({types_pb2.DT_INT32,\n    types_pb2.DT_INT64}, \'TensorFlowIndex\'),\n  ({types_pb2.DT_UINT8,\n    types_pb2.DT_UINT16,\n    types_pb2.DT_UINT32,\n    types_pb2.DT_UINT64,\n    types_pb2.DT_INT8,\n    types_pb2.DT_INT16,\n    types_pb2.DT_INT32,\n    types_pb2.DT_INT64}, \'TensorFlowInteger\'),\n  ({types_pb2.DT_FLOAT,\n    types_pb2.DT_DOUBLE,\n    types_pb2.DT_HALF,\n    types_pb2.DT_BFLOAT16}, \'FloatingPoint & TensorFlowScalar\'),\n  ({types_pb2.DT_UINT8,\n    types_pb2.DT_UINT16,\n    types_pb2.DT_UINT32,\n    types_pb2.DT_UINT64,\n    types_pb2.DT_INT8,\n    types_pb2.DT_INT16,\n    types_pb2.DT_INT32,\n    types_pb2.DT_INT64,\n    types_pb2.DT_FLOAT,\n    types_pb2.DT_DOUBLE,\n    types_pb2.DT_HALF,\n    types_pb2.DT_BFLOAT16}, \'TensorFlowNumeric\')]\n\n_SWIFTIFIED_TYPES = {\n  types_pb2.DT_FLOAT: \'Float\',\n  types_pb2.DT_DOUBLE: \'Double\',\n  types_pb2.DT_INT32: \'Int32\',\n  types_pb2.DT_UINT8: \'UInt8\',\n  types_pb2.DT_INT16: \'Int16\',\n  types_pb2.DT_INT8: \'Int8\',\n  types_pb2.DT_INT64: \'Int64\',\n  types_pb2.DT_BOOL: \'Bool\',\n  types_pb2.DT_UINT16: \'UInt16\',\n  types_pb2.DT_UINT32: \'UInt32\',\n  types_pb2.DT_UINT64: \'UInt64\'}\n\n_SWIFTIFIED_ATTR_TYPES = {\n  \'int\': \'Int64\',\n  \'float\': \'Double\',\n  \'bool\': \'Bool\',\n  \'string\': \'String\',\n  \'type\': \'TensorDataType\',\n  \'shape\': \'TensorShape?\',\n  \'list(int)\': \'[Int32]\',\n  \'list(float)\': \'[Double]\',\n  \'list(bool)\': \'[Bool]\',\n  \'list(string)\': \'[String]\',\n  \'list(type)\': \'[TensorDataType]\',\n  \'list(shape)\': \'[TensorShape?]\'}\n\n_OMITTED_PARAMETER_NAMES = {\n  \'x\', \'y\', \'a\', \'b\', \'input\', \'tensor\', \'values\'}\n\n_START_COMMENT = \'///\'\n\nX10_OPS = {\n  ""CanonicalDims"", ""CheckSameDevice"", ""CheckSameDevice"", ""CheckSameDevice"",\n  ""CheckSameDevice"", ""CheckSamePrecision"", ""CheckSamePrecision"",\n  ""CheckSamePrecision"", ""CheckSamePrecision"",\n  ""Abs"", ""Acos"", ""Acosh"", ""AddV2"", ""All"", ""Any"", ""ApproximateEqual"", ""ArgMax"",\n  ""ArgMax"", ""ArgMin"", ""Asin"", ""Asinh"", ""Atan"", ""Atanh"", ""ConvertPadding"",\n  ""ConvertPadding2"", ""ConvertDataFormat"", ""ConvertDataFormat1"",\n  ""ConvertDataFormat4"", ""ConvertMirrorPadMode"", ""ReversedPaddings"",\n  ""AvgPool"", ""AvgPool3D"", ""AvgPool3DGrad"", ""AvgPoolGrad"", ""BatchMatMulV2"",\n  ""BroadcastGradientArgs"", ""BroadcastTo"", ""BroadcastTo"", ""Cast"", ""Ceil"",\n  ""ClipByValue"", ""ConcatV2"", ""Conv2D"", ""Conv2DBackpropFilter"",\n  ""Conv2DBackpropFilter"", ""Conv2DBackpropInput"", ""Conv2DBackpropInput"",\n  ""Conv3D"", ""Conv3DBackpropFilterV2"", ""Conv3DBackpropInputV2"", ""Cos"", ""Cosh"",\n  ""Cumprod"", ""Cumsum"", ""DepthwiseConv2dNative"",\n  ""DepthwiseConv2dNativeBackpropFilter"", ""DepthwiseConv2dNativeBackpropInput"",\n  ""DiagPart"", ""Div"", ""Elu"", ""EluGrad"", ""Equal"", ""Exp"", ""ExpandDims"", ""Expm1"",\n  ""Fill"", ""Fill"", ""Floor"", ""Gather"", ""GatherV2"", ""Greater"", ""GreaterEqual"",\n  ""InvertPermutation"", ""IsFinite"", ""IsInf"", ""IsNan"", ""LeakyRelu"",\n  ""LeakyReluGrad"", ""Less"", ""LessEqual"", ""LinSpace"", ""LinSpace"", ""Log"", ""Log1p"",\n   ""LogSoftmax"", ""LogicalAnd"", ""LogicalNot"", ""LogicalOr"",\n  ""MatMul"", ""Max"", ""MaxPool3D"", ""MaxPool3DGrad"", ""MaxPoolGradV2"",\n  ""MaxPoolGradV2"", ""MaxPoolV2"", ""MaxPoolV2"", ""Maximum"", ""Mean"", ""Mean"", ""Min"",\n  ""Minimum"", ""MirrorPad"", ""MirrorPadGrad"", ""Mod"", ""Mul"", ""Neg"", ""NotEqual"",\n  ""OneHot"", ""OneHot"", ""OnesLike"", ""Pack"", ""Pad"", ""PadV2"", ""PhysicalCast"", ""Pow"",\n  ""Prod"", ""Qr"", ""Range"", ""Rank"", ""Relu"", ""Relu6"", ""Relu6Grad"", ""ReluGrad"",\n  ""Reshape"", ""Reshape"", ""ReverseV2"", ""Round"", ""Rsqrt"", ""RsqrtGrad"", ""Select"",\n  ""Selu"", ""SeluGrad"", ""Shape"", ""Sigmoid"", ""SigmoidGrad"", ""Sign"", ""Sin"", ""Sinh"",\n  ""Size"", ""Slice"", ""Softmax"", ""SoftmaxCrossEntropyWithLogits"", ""Softplus"",\n  ""SoftplusGrad"", ""Softsign"", ""SoftsignGrad"",\n  ""SparseSoftmaxCrossEntropyWithLogits"", ""Split"", ""SplitV"", ""Sqrt"", ""Square"",\n  ""SquaredDifference"", ""Squeeze"", ""StatelessMultinomial"",\n  ""StatelessRandomNormal"", ""StatelessRandomNormal"", ""StatelessRandomUniform"",\n  ""StatelessRandomUniform"", ""StatelessRandomUniformInt"",\n  ""StatelessRandomUniformInt"", ""StatelessTruncatedNormal"",\n  ""StatelessTruncatedNormal"", ""StridedSlice"", ""StridedSliceGrad"", ""Sub"", ""Sum"",\n  ""Sum"", ""Tan"", ""Tanh"", ""TensorStridedSliceUpdate"", ""Tile"", ""ToDevice"",\n  ""Transpose"", ""Unpack"", ""UnsortedSegmentSum"", ""Xdivy"", ""ZerosLike"",\n  ""Rand"",\n}\n\nclass UnableToGenerateCodeError(Exception):\n  def __init__(self, details):\n    self.details = details\n    super(UnableToGenerateCodeError, self).__init__()\n\n  def __str__(self):\n    return self.details\n\n\nclass Op(object):\n  def __init__(self, op_def, api_def, enum_store, string_valued=False):\n    self.op_def = op_def\n    self.api_def = api_def\n    self.enum_store = enum_store\n    self.string_valued = string_valued\n    self.inferred_counts = dict()\n\n    # Collect all the input and output arguments.\n    self.input_args = [\n      Argument(arg_def, op=self)\n      for arg_def in self.op_def.input_arg]\n    self.output_args = [\n      Argument(arg_def, op=self)\n      for arg_def in self.op_def.output_arg]\n\n    # Collect all attributes.\n    self.attrs = [\n      Attribute(attr, op=self)\n      for attr in op_def.attr]\n    self.type_attrs = [\n      attr for attr in self.attrs\n      if attr.is_type_attr]\n\n  def swift_function(self):\n    return \'\'\'\n{documentation}@inlinable @inline(__always)\npublic static func {name}{generics}({input_args}\n){return_type} {{\n  {body}\n}}\'\'\'.format(\n      documentation=self._swift_documentation(),\n      name=self._swift_name(),\n      generics=self._swift_generics(),\n      input_args=self._swift_input_args(),\n      return_type=self._swift_return_type(),\n      body=self._swift_body())\n\n  def swift_dispatch_function(self, x10_supported=False):\n    return \'\'\'\n{documentation}@inlinable @inline(__always)\npublic static func {name}{generics}({input_args}\n){return_type} {{\n  {body}\n}}\'\'\'.format(\n      documentation=self._swift_documentation(),\n      name=self._swift_name(),\n      generics=self._swift_generics(),\n      input_args=self._swift_input_args(),\n      return_type=self._swift_return_type(),\n      body=self._swift_dispatch_body(x10_supported=x10_supported))\n\n  def _swift_documentation(self):\n    def comment_block(text, indent_level):\n      """"""Returns a commented block of text with some specified indentation.""""""\n      def indent(line_index):\n        if indent_level == 0:\n          return \'\'\n        if line_index:\n          return \'    \' * indent_level\n        return \'    \' * (indent_level - 1) + \'- \'\n\n      return \'\'.join([\n        (_START_COMMENT + \' \' + indent(line_index) + line + \'\\n\'\n         if line else _START_COMMENT + \'\\n\')\n        for line_index, line in enumerate(text.split(\'\\n\'))\n      ])\n\n    def append_list(doc, args, arg_type):\n      """"""Returns the documentation for lists of inputs/outputs/attributes.""""""\n      args = [arg for arg in args if arg.description]\n      if len(args) == 1:\n        block = \'%s %s: %s\' % (arg_type, args[0].name, args[0].description)\n        doc += _START_COMMENT + \'\\n\'\n        doc += comment_block(block, indent_level=1)\n      elif len(args) > 1:\n        doc += \'%s\\n%s - %ss:\\n\' % (_START_COMMENT, _START_COMMENT, arg_type)\n        for arg in args:\n          block = \'%s: %s\' % (arg.name, arg.description)\n          doc += comment_block(block, indent_level=2)\n      return doc\n\n    doc = \'\'\n    if self.api_def.summary:\n      doc = comment_block(self.api_def.summary, indent_level=0)\n    if self.api_def.description:\n      doc += _START_COMMENT + \'\\n\'\n      doc += comment_block(self.api_def.description, indent_level=0)\n    doc = append_list(doc, self.api_def.in_arg, \'Parameter\')\n    doc = append_list(doc, self.api_def.attr, \'Attr\')\n    doc = append_list(doc, self.api_def.out_arg, \'Output\')\n    if doc and not doc.endswith(\'\\n\'):\n      doc = doc + \'\\n\'\n    return doc\n\n  def _swift_name(self):\n    return swift_compatible_identifier(\n      self.op_def.name[0].lower() + self.op_def.name[1:])\n\n  def _swift_generics(self):\n    constraints = [\n      attr.generic_constraints(self.string_valued)\n      for attr in self.attrs]\n    constraints = [c for c in constraints if c is not None]\n    if len(constraints) == 1:\n      return \'<\' + \', \'.join(constraints) + \'>\'\n    if len(constraints) > 1:\n      return \'<\\n    \' + \',\\n    \'.join(constraints) + \'\\n>\'\n    return \'\'\n\n  def _swift_input_args(self):\n    args = \'\'\n    for arg in self.input_args:\n      args += \'\\n    %s: %s,\' % (arg.swift_arg_name, str(arg.swift_type(self.string_valued)))\n    for attr in self.attrs:\n      if not attr.is_inferred_type_attr and not attr.is_inferred_number_attr:\n        args += \'\\n    %s: %s%s,\' % (attr.swift_arg_name, attr.swift_type, attr.swift_default)\n    if args != \'\':\n      args = args[:-1]\n    return args\n\n  def _swift_return_type(self):\n    return_type = \'\'\n    if len(self.output_args) == 1:\n      return_type = \' -> \' + str(self.output_args[0].swift_type(self.string_valued))\n    elif len(self.output_args) > 1:\n      named_types = [\n        arg.swift_name + \': \' + str(arg.swift_type(self.string_valued))\n        for arg in self.output_args]\n      return_type = \' -> (\' + \', \'.join(named_types) + \')\'\n    return return_type\n\n  def _swift_body(self):\n    setters = []\n    for attr in self.attrs:\n      setters.append(attr.swift_setter(self.string_valued))\n    for arg in self.input_args:\n      setters.append(arg.swift_setter())\n    counts = [\'Int({})\'.format(arg.swift_count) for arg in self.output_args]\n    if len(self.output_args) == 0:\n      body = \'let nOutputs = 0\'\n    else:\n      body = \'let nOutputs = {}\'.format(\' + \'.join(counts))\n    body += \'\\n    let op = makeOp(""{}"", nOutputs)\\n    \'.format(self.op_def.name)\n    body += \'\\n    \'.join(setters)\n    if len(self.output_args) == 0:\n      return body + \'\\n    op.execute()\'\n    body += \'\\n    return op.execute({})\'.format(\', \'.join(counts))\n    return body\n\n  def _swift_dispatch_body(self, x10_supported=False):\n    names = []\n    tensors = []\n    backends = []\n    device_source = None\n    for arg in self.input_args:\n      names.append(arg.swift_name)\n      if arg.is_tensor_type(self.string_valued):\n        tensors.append(arg)\n        if arg.is_list:\n          backends.append(""commonBackend("" + arg.swift_name + "")"")\n        else:\n          device_source = arg\n          backends.append(arg.swift_name + "".handle.backend"")\n    for attr in self.attrs:\n      if not attr.is_inferred_type_attr and not attr.is_inferred_number_attr:\n        names.append(attr.swift_name)\n    names_filtered = []\n    for name in names:\n      if name in _OMITTED_PARAMETER_NAMES:\n        names_filtered.append(name)\n      else:\n        names_filtered.append(name + "": "" + name)\n    dispatch = self._swift_name() + ""("" + ("", "".join(names_filtered)) + "")""\n    if len(backends) == 0 and x10_supported:\n      print(""x10 unsupported: "" + str(self.swift_name()))\n    def do_conversion(arg):\n      return (""\\n      let {name} = {typename}(copying: {name}, to: .defaultTFEager)""\n              .format(name=arg.swift_name,\n                      typename=str(arg.swift_type(self.string_valued))))\n    def get_common_backend(x, y):\n      return ""commonBackend({}, {})"".format(x, y)\n    if len(backends) == 0 or (not x10_supported and (len(self.output_args) != 1\n      or not self.output_args[0].is_tensor_type(self.string_valued) or not device_source)):\n      return ""_RawTFEager."" + dispatch\n    if not x10_supported:\n      return """"""switch {backends} {{\n    case .XLA:\n      let output_device = {convert_device}.device{convert_tensors}\n      return {convert_type}(copying: _RawTFEager.{dispatch}, to: output_device)\n    case .TF_EAGER:\n      return _RawTFEager.{dispatch}\n  }}\n"""""".format(dispatch=dispatch,\n           convert_type=str(self.output_args[0].swift_type(self.string_valued)),\n           convert_device=str(device_source.swift_name),\n           convert_tensors = """".join(map(do_conversion, tensors)),\n           backends=reduce(get_common_backend, backends))\n    return """"""switch {backends} {{\n    case .XLA:\n      return _RawXLA.{dispatch}\n    case .TF_EAGER:\n      return _RawTFEager.{dispatch}\n  }}\n"""""".format(dispatch=dispatch,\n           backends=reduce(get_common_backend, backends))\n\n\nclass Argument(object):\n  def __init__(self, arg_def, op):\n    self.arg_def = arg_def\n    self.op = op\n    self.is_list = arg_def.number_attr is not u\'\' \\\n                   or arg_def.type_list_attr is not u\'\'\n\n  @property\n  def name(self):\n    return self.arg_def.name\n\n  @property\n  def swift_name(self):\n    return swift_compatible_identifier(\n      self.name[0].lower() + self.name[1:])\n\n  @property\n  def swift_arg_name(self):\n    name = self.swift_name\n    if name in _OMITTED_PARAMETER_NAMES:\n      name = \'_ \' + name\n    return name\n\n  def swift_type(self, string_valued=False):\n    return self.type.swift_type(\n      string_valued=self.allows_string and string_valued)\n\n  def swift_setter(self):\n    if self.is_list:\n      return \'op.addInputList({})\'.format(self.swift_name)\n    else:\n      return \'op.addInput({})\'.format(self.swift_name)\n\n  @property\n  def swift_count(self):\n    number_attr = self.arg_def.number_attr\n    if number_attr and number_attr in self.op.inferred_counts:\n      return self.op.inferred_counts[number_attr]\n    if self.arg_def.type_list_attr:\n      return self.op.inferred_counts[self.arg_def.type_list_attr]\n    return \'1\'\n\n  @property\n  def type(self):\n    number = self.arg_def.number_attr\n    if self.arg_def.type_attr:\n      type_attr = next(\n        attr for attr in self.op.type_attrs\n        if attr.name == self.arg_def.type_attr)\n      return Type(\'Tensor\', base_type=type_attr.swift_name, number=number)\n    if self.arg_def.type_list_attr:\n      type_attr = next(\n        attr for attr in self.op.type_attrs\n        if attr.name == self.arg_def.type_list_attr)\n      # There are never any numbered type lists.\n      return Type(type_attr.swift_name)\n    if self.arg_def.type in _SWIFTIFIED_TYPES:\n      base_type = _SWIFTIFIED_TYPES[self.arg_def.type]\n      return Type(\'Tensor\', base_type=base_type, number=number)\n    if self.arg_def.type == types_pb2.DT_STRING:\n      return Type(\'Tensor\', base_type=\'String\', number=number)\n    if self.arg_def.type == types_pb2.DT_RESOURCE:\n      return Type(\'ResourceHandle\', number=number)\n    if self.arg_def.type == types_pb2.DT_VARIANT:\n      return Type(\'VariantHandle\', number=number)\n    raise UnableToGenerateCodeError(\n      \'Unsupported type for argument ""%s"".\' % self.name)\n\n  def is_tensor_type(self, string_valued=False):\n    return self.type.kind == \'Tensor\' and not (\n        (self.allows_string and string_valued) or self.type.base_type == \'String\')\n\n  @property\n  def allows_string(self):\n    if self.arg_def.type_attr:\n      type_attr = next(\n        attr for attr in self.op.type_attrs\n        if attr.name == self.arg_def.type_attr)\n      return types_pb2.DT_STRING in type_attr.attr_def.allowed_values.list.type\n    return False\n\n\nclass Type(object):\n  def __init__(self, kind, base_type=None, number=None):\n    self.kind = kind\n    self.base_type = base_type\n    self.number = number\n\n  @property\n  def count(self):\n    return self.number if self.number else 1\n\n  def swift_type(self, string_valued=False):\n    if self.kind == \'Tensor\':\n      if self.base_type == \'String\' or string_valued:\n        name = \'StringTensor\'\n      else:\n        name = \'Tensor<\' + self.base_type + \'>\'\n    elif self.kind == \'TensorHandle\':\n      name = \'TensorHandle<\' + self.base_type + \'>\'\n    elif self.kind == \'ResourceHandle\':\n      name = \'ResourceHandle\'\n    elif self.kind == \'VariantHandle\':\n      name = \'VariantHandle\'\n    else:\n      name = self.kind\n    return (\'[%s]\' % name) if self.number else name\n\n\nclass Attribute(object):\n  """"""Represents information extracted from op `type` and `list(type)` attributes.""""""\n\n  def __init__(self, attr_def, op):\n    self.attr_def = attr_def\n    self.op = op\n    self.is_type_attr = attr_def.type in [\'type\', \'list(type)\']\n\n    # Check whether the value of this attribute can be\n    # inferred automatically (this only applies to\n    # type-valued attributes).\n    input_args = list(op.op_def.input_arg)\n    output_args = list(op.op_def.output_arg)\n    input_arg_type_attrs = set(\n      [arg.type_attr for arg in input_args] +\n      [arg.type_list_attr for arg in input_args])\n    output_arg_type_attrs = set(\n      [arg.type_attr for arg in output_args] +\n      [arg.type_list_attr for arg in output_args])\n    arg_type_attrs = input_arg_type_attrs.union(output_arg_type_attrs)\n    self.is_inferred_type_attr = attr_def.name in arg_type_attrs\n    self.is_output_type_attr = attr_def.name in output_arg_type_attrs\n    self.is_func_attr = self.attr_def.type == \'func\'\n\n    # We use this for obtaining the `_typeList` property.\n    self.input_arg = None\n    self.is_inferred_number_attr = False\n    for arg in self.op.input_args:\n      if self.attr_def.name in [arg.arg_def.type_attr,\n                                arg.arg_def.type_list_attr] or \\\n         self.attr_def.name == arg.arg_def.number_attr:\n        self.input_arg = arg\n        self.is_inferred_number_attr = True\n        break\n\n    # The following properties are only relevant for\n    # non-inferred-type-valued attributes.\n    self._swift_type = \'\'\n    self._use_enum = False\n    if not self.is_inferred_type_attr and not self.is_func_attr:\n      if self.attr_def.type not in _SWIFTIFIED_ATTR_TYPES:\n        raise UnableToGenerateCodeError(\n          \'Unsupported type for attribute ""%s"".\'\n          % self.attr_def.name)\n\n      # Get the arg type.\n      self._swift_type = _SWIFTIFIED_ATTR_TYPES[self.attr_def.type]\n\n      # Check if the arg is an enum type.\n      self._use_enum = False\n      if self.attr_def.type == \'string\':\n        allowed_values = tuple(sorted(self.attr_def.allowed_values.list.s))\n        if allowed_values:\n          self._swift_type = self.op.enum_store.maybe_add(\n            allowed_values, self.attr_def.name)\n          self._use_enum = True\n    if self.is_func_attr:\n      input_type = self.swift_name.capitalize() + \'In\'\n      output_type = self.swift_name.capitalize() + \'Out\'\n      self._swift_type = \'({}) -> {}\'.format(input_type, output_type)\n\n  @property\n  def name(self):\n    return self.attr_def.name\n\n  @property\n  def swift_name(self):\n    if self.is_inferred_type_attr:\n      return swift_compatible_identifier(\n        self.name, capitalize=True)\n    return swift_compatible_identifier(\n      self.name[0].lower() + self.name[1:])\n\n  @property\n  def swift_arg_name(self):\n    name = self.swift_name\n    if name in _OMITTED_PARAMETER_NAMES:\n      name = \'_ \' + name\n    return name\n\n  @property\n  def swift_type(self):\n    return self._swift_type\n\n  @property\n  def swift_default(self):\n    def swift_float(f):\n      if f == float(\'inf\'): return \'Double.infinity\'\n      if f == float(\'-inf\'): return \'-Double.infinity\'\n      return \'%g\' % f\n\n    if not self.is_inferred_type_attr and self.attr_def.default_value:\n      default_value = self.attr_def.default_value\n      if default_value.HasField(\'b\'):\n        default_value = str(default_value.b).lower()\n      elif default_value.HasField(\'i\'):\n        default_value = str(default_value.i)\n      elif default_value.HasField(\'f\'):\n        default_value = swift_float(default_value.f)\n      elif default_value.HasField(\'s\') and default_value.s:\n        s = str(default_value.s)\n        default_value = \'.\' + swift_compatible_identifier(s.lower()) \\\n            if self._use_enum else json.dumps(s) # \'""\' + s + \'""\'\n      elif default_value.HasField(\'list\'):\n        if default_value.list.i:\n          default_values = [str(s) for s in default_value.list.i]\n          default_value = \'[\' + \', \'.join(default_values) + \']\'\n        elif default_value.list.f:\n          default_values = [swift_float(s) for s in default_value.list.f]\n          default_value = \'[\' + \', \'.join(default_values) + \']\'\n        else:\n          default_value = None\n      else:\n        default_value = None\n      if default_value is not None:\n        default_value = default_value.replace(""\\t"", ""\\\\t"")\n        return \' = \' + default_value\n    return \'\'\n\n  def swift_setter(self, string_valued=False):\n    # Inferred-type-valued attributes.\n    if self.is_inferred_type_attr:\n      name = self.swift_name\n      if self.input_arg is not None:\n        name = self.input_arg.swift_name\n      if self.attr_def.type == \'list(type)\' or self.is_inferred_number_attr:\n        self.op.inferred_counts[self.name] = name + \'._typeList.count\'\n      if self.attr_def.type == \'list(type)\':\n        return \'op.updateAttribute(""{}"", {}._typeList)\'.format(self.name, name)\n      if string_valued and self.allows_string:\n        return \'op.updateAttribute(""{}"", TensorDataType(TF_STRING))\'.format(self.name)\n      return \'op.updateAttribute(""{}"", {}.tensorFlowDataType)\'.format(self.name, self.swift_name)\n\n    if self.is_inferred_number_attr:\n      # The following is used for inferring the lengths of output lists.\n      self.op.inferred_counts[self.name] = self.input_arg.swift_name + \'.count\'\n      return \'op.updateAttribute(""{}"", {}.count)\'.format(self.name, self.input_arg.swift_name)\n\n    if self.attr_def.type == \'int\':\n      # The following is used for inferring the lengths of output lists.\n      self.op.inferred_counts[self.name] = self.swift_name\n\n    # Remaining attributes.\n    value = self.swift_name + \'.cName\' if self._use_enum else self.swift_name\n    return \'op.updateAttribute(""{}"", {})\'.format(self.name, value)\n\n  def generic_constraints(self, string_valued):\n    # We use this for obtaining the `_typeList` property.\n    input_arg = None\n    if self.attr_def.type == \'list(type)\':\n      for arg in self.op.input_args:\n        if self.attr_def.name in [arg.arg_def.type_attr,\n                                  arg.arg_def.type_list_attr]:\n          input_arg = arg\n          break\n    if self.is_func_attr:\n      input_type = self.swift_name.capitalize() + \'In\'\n      output_type = self.swift_name.capitalize() + \'Out\'\n      return \'{}: TensorGroup,\\n    {}: TensorGroup\'.format(\n        input_type, output_type)\n    if not self.is_inferred_type_attr:\n      return None\n    protocol = None\n    if self.attr_def.type == \'list(type)\' and input_arg is None:\n      protocol = \'TensorGroup\'\n    elif self.attr_def.type == \'list(type)\':\n      protocol = \'TensorArrayProtocol\'\n    elif self.attr_def.type == \'type\':\n      if string_valued and self.allows_string:\n        return None\n      protocol = \'TensorFlowScalar\'\n      allowed_types = set(self.attr_def.allowed_values.list.type)\n      allowed_types &= set(_SWIFTIFIED_TYPES.keys())\n      for types, protocol_name in _TYPE_PROTOCOLS:\n        if allowed_types.issubset(types):\n          protocol = protocol_name\n          break\n    if protocol is not None:\n      return self.swift_name + \': \' + protocol\n    return None\n\n  @property\n  def allows_string(self):\n    return types_pb2.DT_STRING in self.attr_def.allowed_values.list.type\n\n\ndef swift_compatible_identifier(s, capitalize=False):\n  """"""Transforms an identifier to be more swift idiomatic.""""""\n  if capitalize:\n    s = s.capitalize()\n  without_underscores = []\n  capitalize_next_char = False\n  for c in s:\n    if c == \'-\' or c == \'_\' or c == \'(\' or c == \')\' or c == \'<\' or c == \'>\':\n      capitalize_next_char = True\n    elif capitalize_next_char:\n      capitalize_next_char = False\n      without_underscores.append(c.upper())\n    else:\n      without_underscores.append(c)\n  s = \'\'.join(without_underscores)\n  if s in _RENAMED_KEYWORDS:\n    return _RENAMED_KEYWORDS[s]\n  return s\n\n\nclass EnumStore(object):\n  """"""Stores details on string attributes represented as swift enums.""""""\n\n  def __init__(self):\n    self._entries = {}\n    self._type_names = set()\n\n  def enum_codes(self):\n    """"""Generates the swift code for enums.""""""\n    codes = []\n    entries = list(six.iteritems(self._entries))\n    for allowed_values, type_name in sorted(entries, key=lambda x: x[1]):\n      allowed_values = [str(a) for a in allowed_values]\n      codes.append(\n          # FIXME: Re-add `@_frozen` after SR-9739 is resolved.\n          # https://bugs.swift.org/browse/SR-9739\n          # \'@_frozen\\n\' +\n          \'  // @_frozen // SR-9739\\n\' +\n          \'  public enum {} {{\\n\'.format(type_name) +\n          \'\\n\'.join([\'    case {}\'.format(\n            swift_compatible_identifier(a.lower()))\n            for a in allowed_values]) +\n          \'\\n\\n\' +\n          \'    @inlinable\\n\' +\n          \'    var cName: String {\\n\' +\n          \'      @inline(__always)\\n\' +\n          \'      get {\\n\' +\n          \'        switch self {\\n\' +\n          \'\\n\'.join([\'        case .{}: return ""{}""\'.format(\n            swift_compatible_identifier(a.lower()), a)\n            for a in allowed_values]) +\n          \'\\n\' +\n          \'        }\\n\' +\n          \'      }\\n\' +\n          \'    }\\n\' +\n          \'  }\')\n    return codes\n\n  def enum_codes_forwarding(self):\n    codes = []\n    entries = list(six.iteritems(self._entries))\n    for allowed_values, type_name in sorted(entries, key=lambda x: x[1]):\n      codes.append(\'  public typealias {} = _RawTFEager.{}\'.format(type_name, type_name))\n    return codes\n\n  def maybe_add(self, allowed_values, attr_def_name):\n    if allowed_values in self._entries:\n      return self._entries[allowed_values]\n    type_name = swift_compatible_identifier(attr_def_name, capitalize=True)\n    base_typename = type_name\n    counter = 1\n    while type_name in self._type_names:\n      type_name = base_typename + str(counter)\n      counter += 1\n    self._type_names.add(type_name)\n    self._entries[allowed_values] = type_name\n    return type_name\n\n\ndef main(argv):\n  del argv  # Unused.\n  if FLAGS.output_path is None:\n    raise ValueError(\'No output_path has been set\')\n\n  api_def_map = c_api_util.ApiDefMap()\n\n  op_codes = []\n  op_codes_forwarding = []\n  enum_store = EnumStore()\n  op_names = api_def_map.op_names()\n  if FLAGS.api_def_path is not None:\n    for op_name in op_names:\n      path = os.path.join(FLAGS.api_def_path, \'api_def_%s.pbtxt\' % op_name)\n      if not tf.gfile.Exists(path):\n        continue\n      with tf.gfile.Open(path, \'r\') as fobj:\n        data = fobj.read()\n      try:\n        api_def_map.put_api_def(data)\n      except Exception as e:\n        print(\'Cannot load api def for %s: %s\' % (op_name, str(e)))\n\n  num_generated = 0\n  for op_name in sorted(op_names):\n    try:\n      if op_name[0] == \'_\': continue\n      op_def = api_def_map.get_op_def(op_name)\n      if any(a.is_ref for a in op_def.input_arg):\n        raise UnableToGenerateCodeError(\'has ref-valued input\')\n      if any(a.is_ref for a in op_def.output_arg):\n        raise UnableToGenerateCodeError(\'has ref-valued output\')\n      api_def = api_def_map.get_api_def(bytes(op_name, \'utf8\'))\n\n      # It would be nicer to handle `StringTensor` in a more\n      # general way by having `String` conform to `TensorFlowScalar`.\n      default_op = Op(op_def, api_def, enum_store, string_valued=False)\n      string_valued_op = Op(op_def, api_def, enum_store, string_valued=True)\n      default_code = default_op.swift_function()\n      string_valued_code = string_valued_op.swift_function()\n      op_codes.append(default_code)\n      string_valued_op_different = False\n      if string_valued_code != default_code:\n        string_valued_op_different = True\n        op_codes.append(string_valued_code)\n\n      default_code = default_op.swift_dispatch_function(x10_supported=op_name in X10_OPS)\n      string_valued_code = string_valued_op.swift_dispatch_function()\n      op_codes_forwarding.append(default_code)\n      if string_valued_op_different:\n        op_codes_forwarding.append(string_valued_code)\n\n      num_generated += 1\n    except UnableToGenerateCodeError as e:\n      print(\'Cannot generate code for %s: %s\' % (op_name, e.details))\n  print(\'Generated code for %d/%d ops.\' % (num_generated, len(op_names)))\n\n  version_codes = [\n      \'  static let generatedTensorFlowVersion = ""%s""\' % tf.__version__,\n      \'  static let generatedTensorFlowGitVersion = ""%s""\' % tf.__git_version__]\n\n  swift_code = (\n      _WARNING +\n      _HEADER +\n      \'import CTensorFlow\\n\\n\' +\n      \'@inlinable @inline(__always)\\n\' +\n      \'func makeOp(_ name: String, _ nOutputs: Int) -> TFTensorOperation {\\n\' +\n      \'  _ExecutionContext.makeOp(name, nOutputs)\\n\' +\n      \'}\\n\'+\n      \'\\n\\npublic enum _RawTFEager {\\n\\n\' +\n      \'\\n\'.join(version_codes) +\n      \'\\n\\n\' +\n      \'\\n\\n\'.join(enum_store.enum_codes()) +\n      \'\\n\\n\' +\n      \'\\n\'.join(op_codes) +\n      \'\\n\\n}\\n\')\n  with tf.gfile.Open(FLAGS.output_path, \'w\') as f:\n    f.write(swift_code)\n\n  swift_code = (\n      _WARNING +\n      _HEADER +\n      _DISPATCHER_TEMPLATE.format(raw_dispatching_enum=\n      \'public enum _Raw {\\n\\n\' +\n      \'\\n\'.join(version_codes) +\n      \'\\n\\n\' +\n      \'\\n\\n\'.join(enum_store.enum_codes_forwarding()) +\n      \'\\n\\n\' +\n      \'\\n\'.join(op_codes_forwarding) + \'\\n\\n}\'))\n  if FLAGS.dispatching_output_path:\n    with tf.gfile.Open(FLAGS.dispatching_output_path, \'w\') as f:\n      f.write(swift_code)\n\n\nif __name__ == \'__main__\':\n  tf.app.run(main)\n'"
