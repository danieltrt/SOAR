file_path,api_count,code
run.py,0,"b""# Copyright 2020 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport importlib\nimport json\nimport os\n\nimport matplotlib\nimport numpy as np\n\nfrom tensorforce import Agent, Environment, Runner\n\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Tensorforce runner')\n    # Agent arguments\n    parser.add_argument(\n        '-a', '--agent', type=str, default=None,\n        help='Agent (name, configuration JSON file, or library module)'\n    )\n    parser.add_argument(\n        '-n', '--network', type=str, default=None,\n        help='Network (name, configuration JSON file, or library module)'\n    )\n    # Environment arguments\n    parser.add_argument(\n        '-e', '--environment', type=str, default=None,\n        help='Environment (name, configuration JSON file, or library module)'\n    )\n    parser.add_argument(\n        '-l', '--level', type=str, default=None,\n        help='Level or game id, like `CartPole-v1`, if supported'\n    )\n    parser.add_argument(\n        '-m', '--max-episode-timesteps', type=int, default=None,\n        help='Maximum number of timesteps per episode'\n    )\n    parser.add_argument(\n        '--visualize', action='store_true',\n        help='Visualize agent--environment interaction, if supported'\n    )\n    parser.add_argument(\n        '--visualize-directory', type=str, default=None,\n        help='Directory to store videos of agent--environment interaction, if supported'\n    )\n    parser.add_argument(\n        '--import-modules', type=str, default=None,\n        help='Import comma-separated modules required for environment'\n    )\n    # Parallel execution arguments\n    parser.add_argument(\n        '--num-parallel', type=int, default=None,\n        help='Number of environment instances to execute in parallel'\n    )\n    parser.add_argument(\n        '--batch-agent-calls', action='store_true',\n        help='Batch agent calls for parallel environment execution'\n    )\n    parser.add_argument(\n        '--sync-timesteps', action='store_true',\n        help='Synchronize parallel environment execution on timestep-level'\n    )\n    parser.add_argument(\n        '--sync-episodes', action='store_true',\n        help='Synchronize parallel environment execution on episode-level'\n    )\n    parser.add_argument(\n        '--remote', type=str, choices=('multiprocessing', 'socket-client', 'socket-server'),\n        default=None, help='Communication mode for remote environment execution of parallelized'\n                           'environment execution'\n    )\n    parser.add_argument(\n        '--blocking', action='store_true', help='Remote environments should be blocking'\n    )\n    parser.add_argument(\n        '--host', type=str, default=None,\n        help='Socket server hostname(s) or IP address(es), single value or comma-separated list'\n    )\n    parser.add_argument(\n        '--port', type=str, default=None,\n        help='Socket server port(s), single value or comma-separated list, increasing sequence if'\n             'single host and port given'\n    )\n    # Runner arguments\n    parser.add_argument(\n        '-v', '--evaluation', action='store_true',\n        help='Run environment (last if multiple) in evaluation mode'\n    )\n    parser.add_argument('-p', '--episodes', type=int, default=None, help='Number of episodes')\n    parser.add_argument('-t', '--timesteps', type=int, default=None, help='Number of timesteps')\n    parser.add_argument('-u', '--updates', type=int, default=None, help='Number of agent updates')\n    parser.add_argument(\n        '--mean-horizon', type=int, default=1,\n        help='Number of episodes progress bar values and evaluation score are averaged over'\n    )\n    parser.add_argument(\n        '--save-best-agent', type=str, default=None,\n        help='Directory to save the best version of the agent according to the evaluation score'\n    )\n    # Logging arguments\n    parser.add_argument('-r', '--repeat', type=int, default=1, help='Number of repetitions')\n    parser.add_argument(\n        '--path', type=str, default=None,\n        help='Logging path, directory plus filename without extension'\n    )\n    parser.add_argument('--seaborn', action='store_true', help='Use seaborn')\n    args = parser.parse_args()\n\n    if args.import_modules is not None:\n        for module in args.import_modules.split(','):\n            importlib.import_module(name=module)\n\n    if args.path is None:\n        callback = None\n\n    else:\n        assert os.path.splitext(args.path)[1] == ''\n        assert args.episodes is not None and args.visualize is not None\n        rewards = [list() for _ in range(args.episodes)]\n        timesteps = [list() for _ in range(args.episodes)]\n        seconds = [list() for _ in range(args.episodes)]\n        agent_seconds = [list() for _ in range(args.episodes)]\n\n        def callback(r, p):\n            rewards[r.episodes - 1].append(float(r.episode_rewards[-1]))\n            timesteps[r.episodes - 1].append(int(r.episode_timesteps[-1]))\n            seconds[r.episodes - 1].append(float(r.episode_seconds[-1]))\n            agent_seconds[r.episodes - 1].append(float(r.episode_agent_seconds[-1]))\n            return True\n\n    if args.environment is None:\n        environment = None\n    else:\n        environment = dict(environment=args.environment)\n    if args.level is not None:\n        environment['level'] = args.level\n    if args.visualize:\n        environment['visualize'] = True\n    if args.visualize_directory is not None:\n        environment['visualize_directory'] = args.visualize_directory\n\n    if args.host is not None and ',' in args.host:\n        args.host = args.host.split(',')\n    if args.port is not None and ',' in args.port:\n        args.port = [int(x) for x in args.port.split(',')]\n    elif args.port is not None:\n        args.port = int(args.port)\n\n    if args.remote == 'socket-server':\n        Environment.create(\n            environment=environment, max_episode_timesteps=args.max_episode_timesteps,\n            remote=args.remote, port=args.port\n        )\n        return\n\n    if args.agent is None:\n        agent = None\n    else:\n        agent = dict(agent=args.agent)\n    if args.network is not None:\n        agent['network'] = args.network\n\n    for _ in range(args.repeat):\n        runner = Runner(\n            agent=agent, environment=environment, max_episode_timesteps=args.max_episode_timesteps,\n            evaluation=args.evaluation, num_parallel=args.num_parallel, remote=args.remote,\n            blocking=args.blocking, host=args.host, port=args.port\n        )\n        runner.run(\n            num_episodes=args.episodes, num_timesteps=args.timesteps, num_updates=args.updates,\n            batch_agent_calls=args.batch_agent_calls, sync_timesteps=args.sync_timesteps,\n            sync_episodes=args.sync_episodes, callback=callback, mean_horizon=args.mean_horizon,\n            save_best_agent=args.save_best_agent\n        )\n        runner.close()\n\n    if args.path is not None:\n        directory = os.path.split(args.path)[0]\n        if directory != '' and not os.path.isdir(directory):\n            os.makedirs(directory, exist_ok=True)\n\n        with open(args.path + '.json', 'w') as filehandle:\n            filehandle.write(\n                json.dumps(dict(\n                    rewards=rewards, timesteps=timesteps, seconds=seconds,\n                    agent_seconds=agent_seconds\n                ))\n            )\n\n        if args.seaborn:\n            import seaborn as sns\n            sns.set()\n\n        xs = np.arange(len(rewards))\n        min_rewards = np.amin(rewards, axis=1)\n        max_rewards = np.amax(rewards, axis=1)\n        median_rewards = np.median(rewards, axis=1)\n        plt.plot(xs, median_rewards, color='green', linewidth=2.0)\n        plt.fill_between(xs, min_rewards, max_rewards, color='green', alpha=0.4)\n        plt.xlabel('episodes')\n        plt.ylabel('reward')\n        plt.savefig(fname=(args.path + '.png'))\n\n\nif __name__ == '__main__':\n    main()\n"""
setup.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nfrom setuptools import find_packages, setup\nimport sys\n\n\n""""""\ntest: cd docs; make html; cd ..;\npip install --upgrade pip setuptools wheel twine\npython setup.py sdist bdist_wheel\ntwine upload --repository-url https://test.pypi.org/legacy/ dist/Tensorforce-0.5.4*\ntest: pip install --upgrade --index-url https://test.pypi.org/simple/ tensorforce\ntest: python; import tensorforce;\ntest: python tensorforce/examples/quickstart.py\ntwine upload dist/Tensorforce-0.5.4*\n""""""\n\n\nif sys.version_info.major != 3:\n    raise NotImplementedError(""Tensorforce is only compatible with Python 3."")\n\n\ntensorforce_directory = os.path.abspath(os.path.dirname(__file__))\n\n# Extract version from tensorforce/__init__.py\nwith open(os.path.join(tensorforce_directory, \'tensorforce\', \'__init__.py\'), \'r\') as filehandle:\n    for line in filehandle:\n        if line.startswith(\'__version__\'):\n            version = line[15:-2]\n\n# Extract long_description from README.md introduction\nlong_description = list()\nwith open(os.path.join(tensorforce_directory, \'README.md\'), \'r\') as filehandle:\n    lines = iter(filehandle)\n    line = next(lines)\n    if not line.startswith(\'# Tensorforce:\'):\n        raise NotImplementedError\n    long_description.append(line)\n    for line in lines:\n        if line == \'#### Introduction\\n\':\n            break\n    if next(lines) != \'\\n\':\n        raise NotImplementedError\n    while True:\n        line = next(lines)\n        if line == \'\\n\':\n            line = next(lines)\n            if line == \'\\n\':\n                break\n            else:\n                long_description.append(\'\\n\')\n                long_description.append(line)\n        else:\n            long_description.append(line)\n    while line == \'\\n\':\n        line = next(lines)\n    if not line.startswith(\'#### \'):\n        raise NotImplementedError\nlong_description = \'\'.join(long_description)\n\n\n# Extract install_requires from requirements.txt\ninstall_requires = list()\nwith open(os.path.join(tensorforce_directory, \'requirements.txt\'), \'r\') as filehandle:\n    for line in filehandle:\n        line = line.strip()\n        if line:\n            install_requires.append(line)\n\n# Readthedocs requires Sphinx extensions to be specified as part of install_requires.\nif os.environ.get(\'READTHEDOCS\', None) == \'True\':\n    install_requires.append(\'recommonmark\')\n\nsetup(\n    name=\'Tensorforce\',\n    version=version,\n    description=\'Tensorforce: a TensorFlow library for applied reinforcement learning\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    author=\'Alexander Kuhnle\',\n    author_email=\'tensorforce.team@gmail.com\',\n    url=\'http://github.com/tensorforce/tensorforce\',\n    packages=[\n        package for package in find_packages(exclude=(\'test\',))\n        if package.startswith(\'tensorforce\')\n    ],\n    download_url=\'https://github.com/tensorforce/tensorforce/archive/{}.tar.gz\'.format(version),\n    license=\'Apache 2.0\',\n    python_requires=\'>=3.5\',\n    install_requires=install_requires,\n    extras_require=dict(\n        tf=[\'tensorflow==2.0.1\'],\n        tf_gpu=[\'tensorflow-gpu==2.0.1\'],\n        tfa=[\'tensorflow-addons==0.6.0\'],\n        docs=[\'m2r\', \'recommonmark\', \'sphinx\', \'sphinx-rtd-theme\'],\n        tune=[\'hpbandster\'],\n        envs=[\'gym[all]\', \'gym-retro\', \'mazeexp\', \'vizdoom\'],\n        mazeexp=[\'mazeexp\'],\n        gym=[\'gym[all]\'],\n        retro=[\'gym-retro\'],\n        vizdoom=[\'vizdoom\'],\n        carla=[\'pygame\', \'opencv-python\']\n    ),\n    zip_safe=False\n)\n'"
tune.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport os\nimport pickle\n\nimport ConfigSpace as cs\nfrom hpbandster.core.nameserver import NameServer, nic_name_to_host\nfrom hpbandster.core.result import json_result_logger, logged_results_to_HBS_result\nfrom hpbandster.core.worker import Worker\nfrom hpbandster.optimizers import BOHB\nimport numpy as np\n\nfrom tensorforce.environments import Environment\nfrom tensorforce.execution import Runner\n\n\nclass TensorforceWorker(Worker):\n\n    def __init__(self, *args, environment=None, **kwargs):\n        # def __init__(self, run_id, nameserver=None, nameserver_port=None, logger=None, host=None, id=None, timeout=None):\n        super().__init__(*args, **kwargs)\n        assert environment is not None\n        self.environment = environment\n\n    def compute(self, config_id, config, budget, working_directory):\n        if self.environment.max_episode_timesteps() is None:\n            min_capacity = 1000 + config[\'batch_size\']\n        else:\n            min_capacity = self.environment.max_episode_timesteps() + config[\'batch_size\']\n        max_capacity = 100000\n        capacity = min(max_capacity, max(min_capacity, config[\'memory\'] * config[\'batch_size\']))\n        frequency = max(4, int(config[\'frequency\'] * config[\'batch_size\']))\n\n        if config[\'ratio_based\'] == \'yes\':\n            ratio_based = True\n            clipping_value = config[\'clipping_value\']\n        else:\n            ratio_based = False\n            clipping_value = 0.0\n\n        if config[\'baseline\'] == \'no\':\n            baseline_policy = None\n            baseline_objective = None\n            baseline_optimizer = None\n            estimate_horizon = False\n            estimate_terminal = False\n            estimate_advantage = False\n        else:\n            estimate_horizon = \'late\'\n            estimate_advantage = (config[\'estimate_advantage\'] == \'yes\')\n            if config[\'baseline\'] == \'same-policy\':\n                baseline_policy = None\n                baseline_objective = None\n                baseline_optimizer = None\n            elif config[\'baseline\'] == \'auto\':\n                # other modes, shared network/policy etc !!!\n                baseline_policy = dict(network=dict(type=\'auto\', internal_rnn=False))\n                baseline_objective = dict(\n                    type=\'value\', value=\'state\', huber_loss=0.0, early_reduce=False\n                )\n                baseline_optimizer = dict(\n                    type=\'adam\', learning_rate=config[\'baseline_learning_rate\']\n                )\n            else:\n                assert False\n\n        if config[\'l2_regularization\'] < 3e-5:  # yes/no better\n            l2_regularization = 0.0\n        else:\n            l2_regularization = config[\'l2_regularization\']\n\n        if config[\'entropy_regularization\'] < 3e-5:  # yes/no better\n            entropy_regularization = 0.0\n        else:\n            entropy_regularization = config[\'entropy_regularization\']\n\n        agent = dict(\n            agent=\'tensorforce\',\n            policy=dict(network=dict(type=\'auto\', internal_rnn=False)),\n            memory=dict(type=\'replay\', capacity=capacity),\n            update=dict(unit=\'timesteps\', batch_size=config[\'batch_size\'], frequency=frequency),\n            optimizer=dict(type=\'adam\', learning_rate=config[\'learning_rate\']),\n            objective=dict(\n                type=\'policy_gradient\', ratio_based=ratio_based, clipping_value=clipping_value,\n                early_reduce=False\n            ),\n            reward_estimation=dict(\n                horizon=config[\'horizon\'], discount=config[\'discount\'],\n                estimate_horizon=estimate_horizon, estimate_actions=False,\n                estimate_terminal=False, estimate_advantage=estimate_advantage\n            ),\n            baseline_policy=baseline_policy, baseline_objective=baseline_objective,\n            baseline_optimizer=baseline_optimizer,\n            preprocessing=None,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization\n        )\n\n        # num_episodes = list()\n        final_reward = list()\n        max_reward = list()\n        rewards = list()\n\n        for n in range(round(budget)):\n            runner = Runner(agent=agent, environment=self.environment)\n\n            # performance_threshold = runner.environment.max_episode_timesteps() - agent[\'reward_estimation\'][\'horizon\']\n\n            # def callback(r, p):\n            #     return True\n\n            runner.run(num_episodes=500, use_tqdm=False)\n            runner.close()\n\n            # num_episodes.append(len(runner.episode_rewards))\n            final_reward.append(float(np.mean(runner.episode_rewards[-20:], axis=0)))\n            average_rewards = [\n                float(np.mean(runner.episode_rewards[n: n + 20], axis=0))\n                for n in range(len(runner.episode_rewards) - 20)\n            ]\n            max_reward.append(float(np.amax(average_rewards, axis=0)))\n            rewards.append(list(runner.episode_rewards))\n\n        # mean_num_episodes = float(np.mean(num_episodes, axis=0))\n        mean_final_reward = float(np.mean(final_reward, axis=0))\n        mean_max_reward = float(np.mean(max_reward, axis=0))\n        # loss = mean_num_episodes - mean_final_reward - mean_max_reward\n        loss = -mean_final_reward - mean_max_reward\n\n        return dict(loss=loss, info=dict(rewards=rewards))\n\n    @staticmethod\n    def get_configspace():\n        """"""\n        It builds the configuration space with the needed hyperparameters.\n        It is easily possible to implement different types of hyperparameters.\n        Beside float-hyperparameters on a log scale, it is also able to handle categorical input parameter.\n        :return: ConfigurationsSpace-Object\n        """"""\n        configspace = cs.ConfigurationSpace()\n\n        memory = cs.hyperparameters.UniformIntegerHyperparameter(name=\'memory\', lower=2, upper=50)\n        configspace.add_hyperparameter(hyperparameter=memory)\n\n        batch_size = cs.hyperparameters.UniformIntegerHyperparameter(\n            name=\'batch_size\', lower=32, upper=8192, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=batch_size)\n\n        frequency = cs.hyperparameters.UniformFloatHyperparameter(\n            name=\'frequency\', lower=3e-2, upper=1.0, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=frequency)\n\n        learning_rate = cs.hyperparameters.UniformFloatHyperparameter(\n            name=\'learning_rate\', lower=1e-5, upper=3e-2, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=learning_rate)\n\n        horizon = cs.hyperparameters.UniformIntegerHyperparameter(\n            name=\'horizon\', lower=1, upper=50\n        )\n        configspace.add_hyperparameter(hyperparameter=horizon)\n\n        discount = cs.hyperparameters.UniformFloatHyperparameter(\n            name=\'discount\', lower=0.8, upper=1.0, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=discount)\n\n        ratio_based = cs.hyperparameters.CategoricalHyperparameter(\n            name=\'ratio_based\', choices=(\'no\', \'yes\')\n        )\n        configspace.add_hyperparameter(hyperparameter=ratio_based)\n\n        clipping_value = cs.hyperparameters.UniformFloatHyperparameter(\n            name=\'clipping_value\', lower=0.05, upper=0.5\n        )\n        configspace.add_hyperparameter(hyperparameter=clipping_value)\n\n        baseline = cs.hyperparameters.CategoricalHyperparameter(\n            name=\'baseline\', choices=(\'no\', \'auto\', \'same-policy\')\n        )\n        configspace.add_hyperparameter(hyperparameter=baseline)\n\n        baseline_learning_rate = cs.hyperparameters.UniformFloatHyperparameter(\n            name=\'baseline_learning_rate\', lower=1e-5, upper=3e-2, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=baseline_learning_rate)\n\n        estimate_advantage = cs.hyperparameters.CategoricalHyperparameter(\n            name=\'estimate_advantage\', choices=(\'no\', \'yes\')\n        )\n        configspace.add_hyperparameter(hyperparameter=estimate_advantage)\n\n        l2_regularization = cs.hyperparameters.UniformFloatHyperparameter(\n            name=\'l2_regularization\', lower=1e-5, upper=1.0, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=l2_regularization)\n\n        entropy_regularization = cs.hyperparameters.UniformFloatHyperparameter(\n            name=\'entropy_regularization\', lower=1e-5, upper=1.0, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=entropy_regularization)\n\n        configspace.add_condition(\n            condition=cs.EqualsCondition(child=clipping_value, parent=ratio_based, value=\'yes\')\n        )\n\n        configspace.add_condition(\n            condition=cs.NotEqualsCondition(\n                child=baseline_learning_rate, parent=baseline, value=\'no\'\n            )\n        )\n\n        configspace.add_condition(\n            condition=cs.NotEqualsCondition(\n                child=estimate_advantage, parent=baseline, value=\'no\'\n            )\n        )\n\n        return configspace\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Tensorforce hyperparameter tuner\')\n    parser.add_argument(\n        \'environment\', help=\'Environment (name, configuration JSON file, or library module)\'\n    )\n    parser.add_argument(\n        \'-l\', \'--level\', type=str, default=None,\n        help=\'Level or game id, like `CartPole-v1`, if supported\'\n    )\n    parser.add_argument(\n        \'-m\', \'--max-repeats\', type=int, default=1, help=\'Maximum number of repetitions\'\n    )\n    parser.add_argument(\n        \'-n\', \'--num-iterations\', type=int, default=1, help=\'Number of BOHB iterations\'\n    )\n    parser.add_argument(\n        \'-d\', \'--directory\', type=str, default=\'tuner\', help=\'Output directory\'\n    )\n    parser.add_argument(\n        \'-r\', \'--restore\', type=str, default=None, help=\'Restore from given directory\'\n    )\n    parser.add_argument(\'--id\', type=str, default=\'worker\', help=\'Unique worker id\')\n    args = parser.parse_args()\n\n    if args.level is None:\n        environment = Environment.create(environment=args.environment)\n    else:\n        environment = Environment.create(environment=args.environment, level=args.level)\n\n    if False:\n        host = nic_name_to_host(nic_name=None)\n        port = 123\n    else:\n        host = \'localhost\'\n        port = None\n\n    server = NameServer(run_id=args.id, working_directory=args.directory, host=host, port=port)\n    nameserver, nameserver_port = server.start()\n\n    worker = TensorforceWorker(\n        environment=environment, run_id=args.id, nameserver=nameserver,\n        nameserver_port=nameserver_port, host=host\n    )\n    # TensorforceWorker(run_id, nameserver=None, nameserver_port=None, logger=None, host=None, id=None, timeout=None)\n    # logger: logging.logger instance, logger used for debugging output\n    # id: anything with a __str__method, if multiple workers are started in the same process, you MUST provide a unique id for each one of them using the `id` argument.\n    # timeout: int or float, specifies the timeout a worker will wait for a new after finishing a computation before shutting down. Towards the end of a long run with multiple workers, this helps to shutdown idling workers. We recommend a timeout that is roughly half the time it would take for the second largest budget to finish. The default (None) means that the worker will wait indefinitely and never shutdown on its own.\n\n    worker.run(background=True)\n\n    # config = cs.sample_configuration().get_dictionary()\n    # print(config)\n    # res = worker.compute(config=config, budget=1, working_directory=\'.\')\n    # print(res)\n\n    if args.restore is None:\n        previous_result = None\n    else:\n        previous_result = logged_results_to_HBS_result(directory=args.restore)\n\n    result_logger = json_result_logger(directory=args.directory, overwrite=True)  # ???\n\n    optimizer = BOHB(\n        configspace=worker.get_configspace(), min_budget=0.5, max_budget=float(args.max_repeats),\n        run_id=args.id, working_directory=args.directory,\n        nameserver=nameserver, nameserver_port=nameserver_port, host=host,\n        result_logger=result_logger, previous_result=previous_result\n    )\n    # BOHB(configspace=None, eta=3, min_budget=0.01, max_budget=1, min_points_in_model=None, top_n_percent=15, num_samples=64, random_fraction=1 / 3, bandwidth_factor=3, min_bandwidth=1e-3, **kwargs)\n    # Master(run_id, config_generator, working_directory=\'.\', ping_interval=60, nameserver=\'127.0.0.1\', nameserver_port=None, host=None, shutdown_workers=True, job_queue_sizes=(-1,0), dynamic_queue_size=True, logger=None, result_logger=None, previous_result = None)\n    # logger: logging.logger like object, the logger to output some (more or less meaningful) information\n\n    results = optimizer.run(n_iterations=args.num_iterations)\n    # optimizer.run(n_iterations=1, min_n_workers=1, iteration_kwargs={})\n    # min_n_workers: int, minimum number of workers before starting the run\n\n    optimizer.shutdown(shutdown_workers=True)\n    server.shutdown()\n    environment.close()\n\n    with open(os.path.join(args.directory, \'results.pkl\'), \'wb\') as filehandle:\n        pickle.dump(results, filehandle)\n\n    print(\'Best found configuration:\', results.get_id2config_mapping()[results.get_incumbent_id()][\'config\'])\n    print(\'Runs:\', results.get_runs_by_id(config_id=results.get_incumbent_id()))\n    print(\'A total of {} unique configurations where sampled.\'.format(len(results.get_id2config_mapping())))\n    print(\'A total of {} runs where executed.\'.format(len(results.get_all_runs())))\n    print(\'Total budget corresponds to {:.1f} full function evaluations.\'.format(\n        sum([r.budget for r in results.get_all_runs()]) / args.max_repeats)\n    )\n\n\nif __name__ == \'__main__\':\n    main()\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Tensorforce documentation build configuration file, created by\n# sphinx-quickstart.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n\n# SPHINX_APIDOC_OPTIONS=members,undoc-members,inherited-members,show-inheritance sphinx-apidoc /data/coding/tensorforce/tensorforce -o tensorforce\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n# import CommonMark\nfrom m2r import M2R\nfrom recommonmark.transform import AutoStructify\n\nimport tensorforce\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'recommonmark\',\n    \'sphinx.ext.autodoc\', \'sphinx.ext.autosummary\', \'sphinx.ext.doctest\', \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\', \'sphinx.ext.viewcode\', \'sphinx.ext.githubpages\', \'sphinx.ext.napoleon\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\nsource_suffix = [\'.rst\', \'.md\']\n\n# # Source parsers.\n# source_parsers = {\'.md\': \'recommonmark.parser.CommonMarkParser\'}\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'Tensorforce\'\ncopyright = \'2018, Tensorforce Team\'\nauthor = \'Tensorforce Team\'\n\ngithub_doc_root = \'https://github.com/tensorforce/tensorforce/tree/master/docs/\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n\n# The short X.Y version.\nversion = tensorforce.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = tensorforce.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# Napoleon settings\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = False\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n\n# TODO !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n# autodoc_mock_imports = [\'go_vncdriver\', \'tensorflow\', \'deepmind_lab\', \'universe.spaces\', \'gym.spaces.discrete\', \'gym.wrappers\',\n#     \'mazeexp\', \'ale_python_interface\', \'msgpack\', \'msgpack_numpy\', \'cached_property\',\n#     \'tensorflow.python.training.adadelta\', \'tensorflow.python.training.adagrad\', \'tensorflow.python.training.adam\',\n#     \'tensorflow.python.training.gradient_descent\', \'tensorflow.python.training.momentum\', \'tensorflow.python.training.rmsprop\',\n#     \'tensorflow.core.util.event_pb2\']\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Tensorforcedoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'Tensorforce.tex\', \'Tensorforce Documentation\', \'Tensorforce Team\', \'manual\')\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \'tensorforce\', \'Tensorforce Documentation\', [author], 1)]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [(\n    master_doc, \'Tensorforce\', \'Tensorforce Documentation\', author, \'Tensorforce\',\n    \'One line description of project.\', \'Miscellaneous\'\n)]\n\n\n# -- Extension configuration -------------------------------------------------\n\nm2r = M2R()\n\n\ndef process_docstring(app, what, name, obj, options, lines):\n    """"""Enable markdown syntax in docstrings""""""\n\n    markdown = ""\\n"".join(lines)\n\n    # ast = cm_parser.parse(markdown)\n    # html = cm_renderer.render(ast)\n    rest = m2r(markdown)\n\n    rest.replace(""\\r\\n"", ""\\n"")\n    del lines[:]\n    lines.extend(rest.split(""\\n""))\n\n\nrecommonmark_config = dict(\n    url_resolver=(lambda url: github_doc_root + url), auto_toc_tree_section=\'Contents\',\n    enable_eval_rst=True\n)\n\n\ndef setup(app):\n    app.add_config_value(\'recommonmark_config\', recommonmark_config, True)\n    app.add_transform(AutoStructify)\n    app.connect(\'autodoc-process-docstring\', process_docstring)\n'"
examples/carla_examples.py,0,"b'""""""A collection of examples for CARLAEnvironment""""""\n\nimport pygame\n\nfrom tensorforce import Agent\nfrom tensorforce.environments import CARLAEnvironment\n\n\ndef training_example(num_episodes: int, max_episode_timesteps: int):\n    # Instantiate the environment (run the CARLA simulator before doing this!)\n    env = CARLAEnvironment(debug=True)\n\n    # Create your own agent (here is just an example)\n    agent = Agent.create(agent=\'ppo\',\n                         environment=env,\n                         max_episode_timesteps=max_episode_timesteps,\n                         batch_size=1)\n\n    # Training loop (you couldn\'t use a Runner instead)\n    # `weights_dir` and `record_dir` are `None` to prevent saving and recording\n    env.train(agent=agent, \n              num_episodes=num_episodes, max_episode_timesteps=max_episode_timesteps, \n              weights_dir=None, record_dir=None)  \n\n    pygame.quit()\n\n\ndef custom_env_example(num_episodes: int, max_episode_timesteps: int):\n    # import some libs\n    import carla\n    import numpy as np\n\n    from tensorforce.environments.carla_environment import CARLAEnvironment, SensorSpecs, env_utils\n\n    # Subclass `CARLAEnvironment` to customize it:\n    class MyCARLAEnvironment(CARLAEnvironment):\n        # Change actions space: (throttle, steer, brake, reverse)\n        ACTIONS_SPEC = dict(type=\'float\', shape=(4,), min_value=-1.0, max_value=1.0)\n        DEFAULT_ACTIONS = np.array([0.0, 0.0, 0.0, 0.0])\n\n        # Define your own mapping: actions -> carla.VehicleControl\n        def actions_to_control(self, actions):\n            self.control.throttle = float((actions[0] + 1) / 2.0)\n            self.control.steer = float(actions[1])\n            self.control.brake = float((actions[2] + 1) / 2.0)\n            self.control.reverse = bool(actions[3] > 0)\n            self.control.hand_brake = False\n\n        # Define which sensors to use:\n        def default_sensors(self) -> dict:\n            sensors = super().default_sensors()\n\n            # Substitute the default rgb camera with a semantic segmentation camera\n            sensors[\'camera\'] = SensorSpecs.segmentation_camera(position=\'front\', attachment_type=\'Rigid\',\n                                                                image_size_x=self.window_size[0],\n                                                                image_size_y=self.window_size[1],\n                                                                sensor_tick=self.tick_time)\n            # Add a radar sensor\n            sensors[\'radar\'] = SensorSpecs.radar(position=\'radar\', sensor_tick=self.tick_time)\n            return sensors\n\n        # Define a default agent (only used if env.train(agent=None, ...))\n        def default_agent(self, **kwargs) -> Agent:\n            return Agent.create(agent=\'ppo\',\n                                environment=self,\n                                max_episode_timesteps=kwargs.get(\'max_episode_timesteps\'),\n                                batch_size=1)\n\n        # Define your own reward function:\n        def reward(self, actions, time_cost=-2.0):\n            speed = env_utils.speed(self.vehicle)\n            speed_limit = self.vehicle.get_speed_limit()\n\n            if speed <= speed_limit:\n                speed_penalty = -1.0 if speed < speed_limit / 2 else 0.0\n            else:\n                speed_penalty = speed_limit - speed\n\n            return time_cost - self.collision_penalty * 2.0 + speed_penalty\n\n        def render(self, sensors_data: dict):\n            super().render(sensors_data)\n            env_utils.draw_radar_measurement(debug_helper=self.world.debug, data=sensors_data[\'radar\'])\n\n    # Training:\n    env = MyCARLAEnvironment(debug=True)\n\n    env.train(agent=None,  # pass None to use the default_agent\n              num_episodes=num_episodes, max_episode_timesteps=max_episode_timesteps,\n              weights_dir=None, record_dir=None)\n\n    pygame.quit()\n'"
examples/quickstart.py,1,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport logging\n\nimport tensorflow as tf\n\nfrom tensorforce.agents import Agent\nfrom tensorforce.environments import Environment\nfrom tensorforce.execution import Runner\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)\n\n\ndef main():\n    # Create an OpenAI-Gym environment\n    environment = Environment.create(environment=\'gym\', level=\'CartPole-v1\')\n\n    # Create a PPO agent\n    agent = Agent.create(\n        agent=\'ppo\', environment=environment,\n        # Automatically configured network\n        network=\'auto\',\n        # Optimization\n        batch_size=10, update_frequency=2, learning_rate=1e-3, subsampling_fraction=0.2,\n        optimization_steps=5,\n        # Reward estimation\n        likelihood_ratio_clipping=0.2, discount=0.99, estimate_terminal=False,\n        # Critic\n        critic_network=\'auto\',\n        critic_optimizer=dict(optimizer=\'adam\', multi_step=10, learning_rate=1e-3),\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None\n    )\n\n    # Initialize the runner\n    runner = Runner(agent=agent, environment=environment)\n\n    # Start the runner\n    runner.run(num_episodes=200)\n    runner.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorforce/__init__.py,3,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport logging\nimport os\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimport tensorflow as tf\n\n\ntf.compat.v1.disable_eager_execution()\n\n\nfrom tensorforce.exception import TensorforceError\nfrom tensorforce import util\nfrom tensorforce.environments import Environment\nfrom tensorforce.agents import Agent\nfrom tensorforce.execution import Runner\n\n\n# tf.get_logger().setLevel(\'WARNING\')\n# tf.autograph.set_verbosity(3)\n\n__all__ = [\'Agent\', \'Environment\', \'Runner\', \'TensorforceError\']\n\n__version__ = \'0.5.5\'\n\nlogging.getLogger(__name__).addHandler(logging.NullHandler())\n'"
tensorforce/exception.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\ndef is_iterable(x):\n    if isinstance(x, str):\n        return False\n    try:\n        iter(x)\n        return True\n    except TypeError:\n        return False\n\n\nclass TensorforceError(Exception):\n    """"""\n    Tensorforce error\n    """"""\n\n    def __init__(self, message):\n        if message[0].islower():\n            message = message[0].upper() + message[1:]\n        if message[-1] not in \'.!?\':\n            message = message + \'.\'\n        super().__init__(message)\n\n    @staticmethod\n    def unexpected():\n        return TensorforceError(message=""Unexpected error!"")\n\n    @staticmethod\n    def collision(name, value, group1, group2):\n        return TensorforceError(\n            message=""{name} collision between {group1} and {group2}: {value}."".format(\n                name=name, group1=group1, group2=group2, value=value\n            )\n        )\n\n    @staticmethod\n    def mismatch(name, value1, value2, argument=None):\n        if argument is None:\n            return TensorforceError(\n                message=""{name} mismatch: {value1} <-> {value2}."".format(\n                    name=name, value1=value1, value2=value2\n                )\n            )\n        else:\n            return TensorforceError(\n                message=""{name} mismatch for argument {argument}: {value1} <-> {value2}."".format(\n                    name=name, argument=argument, value1=value1, value2=value2\n                )\n            )\n\n    @staticmethod\n    def exists(name, value):\n        return TensorforceError(\n            message=""{name} already exists: {value}."".format(name=name, value=value)\n        )\n\n    @staticmethod\n    def required(name, argument, condition=None):\n        if condition is None:\n            return TensorforceError(\n                message=""Required {name} argument {argument}."".format(name=name, argument=argument)\n            )\n        else:\n            return TensorforceError(\n                message=""Required {name} argument {argument} given {condition}."".format(\n                    name=name, condition=condition, argument=argument\n                )\n            )\n\n    @staticmethod\n    def invalid(name, argument, condition=None):\n        if condition is None:\n            return TensorforceError(\n                message=""Invalid {name} argument {argument}."".format(name=name, argument=argument)\n            )\n        else:\n            return TensorforceError(\n                message=""Invalid {name} argument {argument} given {condition}."".format(\n                    name=name, condition=condition, argument=argument\n                )\n            )\n\n    @staticmethod\n    def type(name, argument, dtype, condition=None, hint=None):\n        if hint is None:\n            if condition is None:\n                return TensorforceError(\n                    message=""Invalid type for {name} argument {argument}: {type}."".format(\n                        name=name, argument=argument, type=dtype\n                    )\n                )\n            else:\n                return TensorforceError(\n                    message=""Invalid type for {name} argument {argument} given {condition}: {type}."".format(\n                        name=name, argument=argument, condition=condition, type=dtype\n                    )\n                )\n        else:\n            if condition is None:\n                return TensorforceError(\n                    message=""Invalid type for {name} argument {argument}: {type} {hint}."".format(\n                        name=name, argument=argument, type=dtype, hint=hint\n                    )\n                )\n            else:\n                return TensorforceError(\n                    message=""Invalid type for {name} argument {argument} given {condition}: {type} {hint}."".format(\n                        name=name, argument=argument, condition=condition, type=dtype, hint=hint\n                    )\n                )\n\n    @staticmethod\n    def value(name, argument, value, condition=None, hint=None):\n        if isinstance(value, dict):\n            value = str(value)\n        elif is_iterable(x=value):\n            value = \',\'.join(str(x) for x in value)\n        if hint is None:\n            if condition is None:\n                return TensorforceError(\n                    message=""Invalid value for {name} argument {argument}: {value}."".format(\n                        name=name, argument=argument, value=value\n                    )\n                )\n            else:\n                return TensorforceError(\n                    message=""Invalid value for {name} argument {argument} given {condition}: {value}."".format(\n                        name=name, argument=argument, condition=condition, value=value\n                    )\n                )\n        else:\n            if condition is None:\n                return TensorforceError(\n                    message=""Invalid value for {name} argument {argument}: {value} {hint}."".format(\n                        name=name, argument=argument, value=value, hint=hint\n                    )\n                )\n            else:\n                return TensorforceError(\n                    message=""Invalid value for {name} argument {argument} given {condition}: {value} {hint}."".format(\n                        name=name, argument=argument, condition=condition, value=value, hint=hint\n                    )\n                )\n'"
tensorforce/util.py,19,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError\n\n\nepsilon = 1e-6\n\n\nlog_levels = dict(\n    info=logging.INFO,\n    debug=logging.DEBUG,\n    critical=logging.CRITICAL,\n    warning=logging.WARNING,\n    fatal=logging.FATAL\n)\n\n\ndef debug(message):\n    logging.warning(\'{}: {}\'.format(datetime.now().strftime(\'%H:%M:%S-%f\')[:-3], message))\n\n\ndef is_iterable(x):\n    if isinstance(x, (str, dict, tf.Tensor)):\n        return False\n    try:\n        iter(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef product(xs, empty=1):\n    """"""Computes the product along the elements in an iterable.\n\n    Args:\n        xs: Iterable containing numbers.\n        empty: ??\n\n    Returns: Product along iterable.\n\n    """"""\n    result = None\n    for x in xs:\n        if result is None:\n            result = x\n        else:\n            result *= x\n\n    if result is None:\n        result = empty\n\n    return result\n\n\ndef compose(function1, function2):\n    def composed(*args, **kwargs):\n        return function1(function2(*args, **kwargs))\n    return composed\n\n\ndef tf_always_true(*args, **kwargs):\n    return tf.constant(value=True, dtype=tf_dtype(dtype=\'bool\'))\n\n\ndef fmap(function, xs, depth=-1, map_keys=False):\n    if xs is None:\n        assert depth <= 0\n        return None\n    elif isinstance(xs, tuple) and depth != 0:\n        return tuple(fmap(function=function, xs=x, depth=(depth - 1), map_keys=map_keys) for x in xs)\n    elif isinstance(xs, list) and depth != 0:\n        return [fmap(function=function, xs=x, depth=(depth - 1), map_keys=map_keys) for x in xs]\n    elif isinstance(xs, set) and depth != 0:\n        return {fmap(function=function, xs=x, depth=(depth - 1), map_keys=map_keys) for x in xs}\n    elif isinstance(xs, OrderedDict) and depth != 0:\n        if map_keys:\n            return OrderedDict((\n                (function(key), fmap(function=function, xs=x, depth=(depth - 1), map_keys=map_keys))\n                for key, x in xs.items()\n            ))\n        else:\n            return OrderedDict((\n                (key, fmap(function=function, xs=x, depth=(depth - 1), map_keys=map_keys))\n                for key, x in xs.items()\n            ))\n    elif isinstance(xs, dict) and depth != 0:\n        if map_keys:\n            return {\n                function(key): fmap(function=function, xs=x, depth=(depth - 1), map_keys=map_keys)\n                for key, x in xs.items()\n            }\n        else:\n            return {\n                key: fmap(function=function, xs=x, depth=(depth - 1), map_keys=map_keys)\n                for key, x in xs.items()\n            }\n    else:\n        assert depth <= 0\n        return function(xs)\n\n\ndef not_nan_inf(x):\n    return not np.isnan(x).any() and not np.isinf(x).any()\n\n\ndef reduce_all(predicate, xs):\n    if xs is None:\n        return False\n    elif isinstance(xs, tuple):\n        return all(reduce_all(predicate=predicate, xs=x) for x in xs)\n    elif isinstance(xs, list):\n        return all(reduce_all(predicate=predicate, xs=x) for x in xs)\n    elif isinstance(xs, set):\n        return all(reduce_all(predicate=predicate, xs=x) for x in xs)\n    elif isinstance(xs, dict):\n        return all(reduce_all(predicate=predicate, xs=x) for x in xs.values())\n    else:\n        return predicate(xs)\n\n\ndef flatten(xs):\n    if xs is None:\n        return None\n    elif isinstance(xs, (tuple, list, set)):\n        return [x for ys in xs for x in flatten(xs=ys)]\n    elif isinstance(xs, dict):\n        return [x for ys in xs.values() for x in flatten(xs=ys)]\n    else:\n        return [xs]\n\n\ndef zip_items(*args):\n    # assert len(args) > 0 and all(arg is None or isinstance(arg, dict) for arg in args)\n    # assert args[0] is not None\n    # for key in args[0]:\n    #     key_values = (key,) + tuple(None if arg is None else arg[key] for arg in args)\n    #     yield key_values\n    assert len(args) > 0\n    assert all(isinstance(arg, dict) and len(arg) == len(args[0]) for arg in args)\n    for key in args[0]:\n        key_values = (key,) + tuple(arg[key] for arg in args)\n        yield key_values\n\n\ndef deep_equal(xs, ys):\n    if isinstance(xs, dict):\n        if not isinstance(ys, dict):\n            return False\n        for _, x, y in zip_items(xs, ys):\n            if not deep_equal(xs=x, ys=y):\n                return False\n        return True\n    elif is_iterable(x=xs):\n        if not is_iterable(x=ys):\n            return False\n        for x, y in zip(xs, ys):\n            if not deep_equal(xs=x, ys=y):\n                return False\n        return True\n    else:\n        return xs == ys\n\n\ndef deep_disjoint_update(target, source):  # , ignore=()\n    for key, value in source.items():\n        if key not in target:\n            target[key] = value\n        # elif key in ignore:\n        #     continue\n        elif isinstance(target[key], dict):\n            if not isinstance(value, dict):\n                raise TensorforceError.mismatch(\n                    name=\'spec\', argument=key, value1=target[key], value2=value\n                )\n            deep_disjoint_update(target=target[key], source=value)\n        elif is_iterable(x=target[key]):\n            if not is_iterable(x=value) or len(target[key]) != len(value):\n                raise TensorforceError.mismatch(\n                    name=\'spec\', argument=key, value1=target[key], value2=value\n                )\n            for x, y in zip(target[key], value):\n                if x != y:\n                    raise TensorforceError.mismatch(\n                        name=\'spec\', argument=key, value1=target[key], value2=value\n                    )\n        elif target[key] != value:\n            raise TensorforceError.mismatch(\n                name=\'spec\', argument=key, value1=target[key], value2=value\n            )\n\n\ndef dtype(x):\n    for dtype, tf_dtype in tf_dtype_mapping.items():\n        if x.dtype == tf_dtype:\n            return dtype\n    else:\n        if x.dtype == tf.float32:\n            return \'float\'\n        else:\n            raise TensorforceError.value(name=\'util.dtype\', argument=\'x\', value=x.dtype)\n\n\ndef is_dtype(x, dtype):\n    for str_dtype, tf_dtype in tf_dtype_mapping.items():\n        if x.dtype == tf_dtype and dtype == str_dtype:\n            return True\n    else:\n        return False\n        # if x.dtype == tf.float32:\n        #     return \'float\'\n        # else:\n        #     raise TensorforceError.value(name=\'util.dtype\', argument=\'x\', value=x.dtype)\n\n\ndef rank(x):\n    return x.get_shape().ndims\n\n\ndef shape(x, unknown=-1):\n    return tuple(unknown if dims is None else dims for dims in x.get_shape().as_list())\n\n\ndef no_operation():\n    # Operation required, constant not enough.\n    # Returns false\n    return identity_operation(x=tf.constant(value=False, dtype=tf_dtype(dtype=\'bool\')))\n\n\ndef identity_operation(x, operation_name=None):\n    zero = tf.zeros_like(input=x)\n    if is_dtype(x=zero, dtype=\'bool\'):\n        x = tf.math.logical_or(x=x, y=zero, name=operation_name)\n    elif dtype(x=zero) in (\'int\', \'long\', \'float\'):\n        x = tf.math.add(x=x, y=zero, name=operation_name)\n    else:\n        raise TensorforceError.value(name=\'util.identity_operation\', argument=\'x\', value=x)\n    return x\n\n\ndef py_dtype(dtype):\n    if dtype == \'float\':  # or dtype == float or dtype == np.float32 or dtype == tf.float32:\n        return float\n    elif dtype == \'int\' or dtype == \'long\':\n    # dtype == int or dtype == np.int32 or dtype == tf.int32 or\n    # or dtype == np.int64 or dtype == tf.int64\n        return int\n    elif dtype == \'bool\':  # or dtype == bool or dtype == np.bool_ or dtype == tf.bool:\n        return bool\n    else:\n        raise TensorforceError.value(name=\'util.py_dtype\', argument=\'dtype\', value=dtype)\n\n\nnp_dtype_mapping = dict(bool=np.bool_, int=np.int32, long=np.int64, float=np.float32)\n\n\ndef np_dtype(dtype):\n    """"""Translates dtype specifications in configurations to numpy data types.\n    Args:\n        dtype: String describing a numerical type (e.g. \'float\') or numerical type primitive.\n\n    Returns: Numpy data type\n\n    """"""\n    if dtype in np_dtype_mapping:\n        return np_dtype_mapping[dtype]\n    else:\n        raise TensorforceError.value(name=\'util.np_dtype\', argument=\'dtype\', value=dtype)\n\n\ntf_dtype_mapping = dict(bool=tf.bool, int=tf.int32, long=tf.int64, float=tf.float32)\n\n\nreverse_dtype_mapping = {\n    bool: \'bool\', np.bool_: \'bool\', tf.bool: \'bool\',\n    int: \'int\', np.int32: \'int\', tf.int32: \'int\',\n    np.int64: \'long\', tf.int64: \'long\',\n    float: \'float\', np.float32: \'float\', tf.float32: \'float\'\n}\n\n\ndef tf_dtype(dtype):\n    """"""Translates dtype specifications in configurations to tensorflow data types.\n\n       Args:\n           dtype: String describing a numerical type (e.g. \'float\'), numpy data type,\n               or numerical type primitive.\n\n       Returns: TensorFlow data type\n\n    """"""\n    if dtype in tf_dtype_mapping:\n        return tf_dtype_mapping[dtype]\n    else:\n        raise TensorforceError.value(name=\'util.tf_dtype\', argument=\'dtype\', value=dtype)\n\n\ndef get_tensor_dependencies(tensor):\n    """"""\n    Utility method to get all dependencies (including placeholders) of a tensor (backwards through the graph).\n\n    Args:\n        tensor (tf.Tensor): The input tensor.\n\n    Returns: Set of all dependencies (including needed placeholders) for the input tensor.\n    """"""\n    dependencies = set()\n    dependencies.update(tensor.op.inputs)\n    for sub_op in tensor.op.inputs:\n        dependencies.update(get_tensor_dependencies(sub_op))\n    return dependencies\n\n\nreserved_names = {\n    \'states\', \'actions\', \'state\', \'action\', \'terminal\', \'reward\', \'deterministic\', \'optimization\',\n    # Types\n    \'bool\', \'int\', \'long\', \'float\',\n    # Value specification attributes\n    \'shape\', \'type\', \'num_values\', \'min_value\', \'max_value\'\n    # Special values?\n    \'equal\', \'loss\', \'same\', \'x\', \'*\'\n}\n\n\ndef join_scopes(*args):\n    return \'/\'.join(args)\n\n\ndef is_valid_name(name):\n    if not isinstance(name, str):\n        return False\n    if name == \'\':\n        return False\n    if \'/\' in name:\n        return False\n    if \'.\' in name:\n        return False\n    if name in reserved_names:\n        return False\n    return True\n\n\ndef is_nested(name):\n    return name in (\'states\', \'internals\', \'auxiliaries\', \'actions\')\n\n\ndef is_valid_type(dtype):\n    return dtype in (\'bool\', \'int\', \'long\', \'float\') or dtype in reverse_dtype_mapping\n\n\ndef is_valid_value_type(value_type):\n    return value_type in (\'state\', \'action\', \'tensor\')\n\n\ndef is_atomic_values_spec(values_spec):\n    return \'type\' in values_spec or \'shape\' in values_spec\n\n\ndef valid_values_spec(values_spec, value_type=\'tensor\', return_normalized=False):\n    if not is_valid_value_type(value_type=value_type):\n        raise TensorforceError.value(\n            name=\'util.valid_values_spec\', argument=\'value_type\', value=value_type\n        )\n\n    if is_atomic_values_spec(values_spec=values_spec):\n        value_spec = valid_value_spec(\n            value_spec=values_spec, value_type=value_type, return_normalized=return_normalized\n        )\n        return OrderedDict([(value_type, value_spec)])\n\n    if return_normalized:\n        normalized_spec = OrderedDict()\n\n    for name in sorted(values_spec):\n        if not is_valid_name(name=name):\n            raise TensorforceError.value(name=value_type, argument=\'name\', value=name)\n\n        result = valid_values_spec(\n            values_spec=values_spec[name], value_type=value_type,\n            return_normalized=return_normalized\n        )\n        if return_normalized:\n            if len(result) == 1 and next(iter(result)) == value_type:\n                normalized_spec[name] = result[value_type]\n            else:\n                for suffix, spec in result.items():\n                    if suffix == value_type:\n                        normalized_spec[name] = spec\n                    else:\n                        normalized_spec[join_scopes(name, suffix)] = spec\n\n    if return_normalized:\n        return normalized_spec\n    else:\n        return True\n\n\ndef valid_value_spec(\n    value_spec, value_type=\'tensor\', accept_underspecified=False, return_normalized=False\n):\n    if not is_valid_value_type(value_type=value_type):\n        raise TensorforceError.value(\n            name=\'util.valid_value_spec\', argument=\'value_type\', value=value_type\n        )\n\n    value_spec = dict(value_spec)\n\n    if return_normalized:\n        normalized_spec = dict()\n\n    if value_type == \'state\' and return_normalized:\n        dtype = value_spec.pop(\'type\', \'float\')\n    else:\n        dtype = value_spec.pop(\'type\')\n    if accept_underspecified and dtype is None:\n        if return_normalized:\n            normalized_spec[\'type\'] = None\n    elif accept_underspecified and is_iterable(x=dtype):\n        if not all(is_valid_type(dtype=x) for x in dtype):\n            raise TensorforceError.value(name=value_type, argument=\'type\', value=dtype)\n        if return_normalized:\n            normalized_spec[\'type\'] = tuple(reverse_dtype_mapping.get(x, x) for x in dtype)\n    else:\n        if not is_valid_type(dtype=dtype):\n            raise TensorforceError.value(name=value_type, argument=\'type\', value=dtype)\n        if return_normalized:\n            normalized_spec[\'type\'] = reverse_dtype_mapping.get(dtype, dtype)\n\n    if value_type == \'action\' and return_normalized:\n        shape = value_spec.pop(\'shape\', ())\n    else:\n        shape = value_spec.pop(\'shape\')\n    if accept_underspecified and shape is None:\n        if return_normalized:\n            normalized_spec[\'shape\'] = None\n    elif is_iterable(x=shape):\n        start = int(accept_underspecified and len(shape) > 0 and shape[0] is None)\n        if not all(isinstance(dims, int) for dims in shape[start:]):\n            raise TensorforceError.value(name=value_type, argument=\'shape\', value=shape)\n        if accept_underspecified:\n            if not all(dims >= -1 for dims in shape[start:]):\n                raise TensorforceError.value(name=value_type, argument=\'shape\', value=shape)\n        else:\n            if not all(dims > 0 or dims == -1 for dims in shape):\n                raise TensorforceError.value(name=value_type, argument=\'shape\', value=shape)\n        if return_normalized:\n            normalized_spec[\'shape\'] = tuple(shape)\n    elif return_normalized:\n        if not isinstance(shape, int):\n            raise TensorforceError.type(name=value_type, argument=\'shape\', value=shape)\n        if accept_underspecified:\n            if shape < -1:\n                raise TensorforceError.value(name=value_type, argument=\'shape\', value=shape)\n        else:\n            if not (shape > 0 or shape == -1):\n                raise TensorforceError.value(name=value_type, argument=\'shape\', value=shape)\n        if return_normalized:\n            normalized_spec[\'shape\'] = (shape,)\n\n    if value_type == \'tensor\':\n        if \'batched\' in value_spec:\n            batched = value_spec.pop(\'batched\')\n            if not isinstance(batched, bool):\n                raise TensorforceError.type(name=value_type, argument=\'batched\', value=batched)\n            if return_normalized:\n                normalized_spec[\'batched\'] = batched\n\n    if dtype == \'bool\' or (accept_underspecified and dtype is not None and \'bool\' in dtype):\n        pass\n\n    if dtype == \'int\' or (accept_underspecified and dtype is not None and \'int\' in dtype):\n        if \'num_values\' in value_spec or value_type in (\'state\', \'action\'):\n            if \'num_values\' not in value_spec:\n                raise TensorforceError.required(name=value_type, argument=\'num_values\')\n            num_values = value_spec.pop(\'num_values\')\n            if isinstance(num_values, (np.int32, np.int64)):\n                num_values = num_values.item()\n            if not isinstance(num_values, int):\n                raise TensorforceError.type(\n                    name=value_type, argument=\'num_values\', dtype=type(num_values)\n                )\n            if accept_underspecified:\n                if num_values < 0:\n                    raise TensorforceError.value(\n                        name=value_type, argument=\'num_values\', value=num_values\n                    )\n            else:\n                if num_values < 1:\n                    raise TensorforceError.value(\n                        name=value_type, argument=\'num_values\', value=num_values\n                    )\n            if return_normalized:\n                normalized_spec[\'num_values\'] = num_values\n\n    if dtype == \'long\' or (accept_underspecified and dtype is not None and \'long\' in dtype):\n        pass\n\n    if dtype == \'float\' or (accept_underspecified and dtype is not None and \'float\' in dtype):\n        if \'min_value\' in value_spec:\n            min_value = value_spec.pop(\'min_value\')\n            max_value = value_spec.pop(\'max_value\')\n            if isinstance(min_value, np_dtype(dtype=\'float\')):\n                min_value = min_value.item()\n            if isinstance(max_value, np_dtype(dtype=\'float\')):\n                max_value = max_value.item()\n            if not isinstance(min_value, float):\n                raise TensorforceError.type(name=value_type, argument=\'min_value\', value=min_value)\n            if not isinstance(max_value, float):\n                raise TensorforceError.type(name=value_type, argument=\'max_value\', value=max_value)\n            if min_value >= max_value:\n                raise TensorforceError.value(\n                    name=value_type, argument=\'min/max_value\', value=(min_value, max_value)\n                )\n            if return_normalized:\n                normalized_spec[\'min_value\'] = min_value\n                normalized_spec[\'max_value\'] = max_value\n\n    if len(value_spec) > 0:\n        raise TensorforceError.value(\n            name=\'util.valid_value_spec\', argument=\'value_spec\', value=list(value_spec)\n        )\n\n    if return_normalized:\n        return normalized_spec\n    else:\n        return True\n\n\ndef is_value_spec_more_specific(specific_value_spec, value_spec):\n    # Check type consistency\n    specific_dtype = specific_value_spec[\'type\']\n    dtype = value_spec[\'type\']\n    if dtype is None:\n        pass\n    elif is_iterable(x=dtype):\n        if is_iterable(x=specific_dtype):\n            if not all(x in dtype for x in specific_dtype):\n                return False\n        elif specific_dtype not in dtype:\n            return False\n    elif specific_dtype != dtype:\n        return False\n\n    # Check shape consistency\n    specific_shape = specific_value_spec[\'shape\']\n    shape = value_spec[\'shape\']\n    if shape is None:\n        pass\n    elif len(shape) > 0 and shape[0] is None:\n        if len(specific_shape) < len(shape) - 1:\n            return False\n        elif not all(\n            a == b or b == 0 for a, b in zip(specific_shape[-len(shape) + 1:], shape[1:])\n        ):\n            return False\n    elif len(specific_shape) > 0 and specific_shape[0] is None:\n        if len(specific_shape) - 1 > len(shape):\n            return False\n        elif not all(\n            a == b or b == 0 for a, b in zip(specific_shape[1:], shape[-len(specific_shape) + 1:])\n        ):\n            return False\n    else:\n        if len(specific_shape) != len(shape):\n            return False\n        elif not all(a == b or b == 0 for a, b in zip(specific_shape, shape)):\n            return False\n\n    # Check batched consistency\n\n    # Check num_values consistency\n    specific_num_values = specific_value_spec.get(\'num_values\', 0)\n    num_values = value_spec.get(\'num_values\', 0)\n    if not (specific_num_values == num_values or num_values == 0):\n        return False\n\n    # Check min/max_value consistency\n    if \'min_value\' not in specific_value_spec and \'min_value\' in value_spec:\n        return False\n\n    return True\n\n\ndef unify_value_specs(value_spec1, value_spec2):\n    if not valid_value_spec(value_spec=value_spec1, accept_underspecified=True):\n        raise TensorforceError.value(\n            name=\'util.unify_value_specs\', argument=\'value_spec1\', value=value_spec1\n        )\n    elif not valid_value_spec(value_spec=value_spec2, accept_underspecified=True):\n        raise TensorforceError.value(\n            name=\'util.unify_value_specs\', argument=\'value_spec2\', value=value_spec2\n        )\n\n    unified_value_spec = dict()\n\n    # Unify type\n    dtype1 = value_spec1[\'type\']\n    dtype2 = value_spec2[\'type\']\n    if dtype1 is None:\n        dtype = dtype2\n    elif dtype2 is None:\n        dtype = dtype1\n    elif is_iterable(x=dtype1):\n        if is_iterable(x=dtype2):\n            if all(x in dtype1 for x in dtype2):\n                dtype = dtype2\n            elif all(x in dtype2 for x in dtype1):\n                dtype = dtype1\n            else:\n                raise TensorforceError.mismatch(\n                    name=\'value-spec\', argument=\'type\', value1=dtype1, value2=dtype2\n                )\n        elif dtype2 in dtype1:\n            dtype = dtype2\n        else:\n            raise TensorforceError.mismatch(\n                name=\'value-spec\', argument=\'type\', value1=dtype1, value2=dtype2\n            )\n    elif is_iterable(x=dtype2):\n        if dtype1 in dtype2:\n            dtype = dtype1\n        else:\n            raise TensorforceError.mismatch(\n                name=\'value-spec\', argument=\'type\', value1=dtype1, value2=dtype2\n            )\n    elif dtype1 == dtype2:\n        dtype = dtype1\n    else:\n        raise TensorforceError.mismatch(\n            name=\'value-spec\', argument=\'type\', value1=dtype1, value2=dtype2\n        )\n    unified_value_spec[\'type\'] = dtype\n\n    # Unify shape\n    shape1 = value_spec1[\'shape\']\n    shape2 = value_spec2[\'shape\']\n    if shape1 is None:\n        shape = shape2\n    elif shape2 is None:\n        shape = shape1\n    else:\n        reverse_shape = list()\n        for n in range(max(len(shape1), len(shape2))):\n            if len(shape1) <= n:\n                if shape2[-n - 1] is not None:\n                    reverse_shape.append(shape2[-n - 1])\n            elif len(shape2) <= n:\n                if shape1[-n - 1] is not None:\n                    reverse_shape.append(shape1[-n - 1])\n                reverse_shape.append(shape1[-n - 1])\n            elif shape1[-n - 1] is None:\n                reverse_shape.append(shape2[-n - 1])\n            elif shape2[-n - 1] is None:\n                reverse_shape.append(shape1[-n - 1])\n            elif shape1[-n - 1] == 0:\n                reverse_shape.append(shape2[-n - 1])\n            elif shape2[-n - 1] == 0:\n                reverse_shape.append(shape1[-n - 1])\n            elif shape1[-n - 1] == -1:\n                reverse_shape.append(shape2[-n - 1])\n            elif shape2[-n - 1] == -1:\n                reverse_shape.append(shape1[-n - 1])\n            elif shape1[-n - 1] == shape2[-n - 1]:\n                reverse_shape.append(shape1[-n - 1])\n            else:\n                raise TensorforceError.mismatch(\n                    name=\'value-spec\', argument=\'shape\', value1=shape1, value2=shape2\n                )\n        shape = tuple(reversed(reverse_shape))\n    unified_value_spec[\'shape\'] = shape\n\n    # # Unify batched\n    # if \'batched\' in value_spec1 or \'batched\' in value_spec2:\n    #     batched1 = value_spec1.get(\'batched\', False)\n    #     batched2 = value_spec2.get(\'batched\', False)\n    #     if batched1 is batched2:\n    #         batched = batched1\n    #     else:\n    #         raise TensorforceError.mismatch(\n    #             name=\'value-spec\', argument=\'batched\', value1=batched1, value2=batched2\n    #         )\n    #     unified_value_spec[\'batched\'] = batched\n\n    # Unify num_values\n    if \'num_values\' in value_spec1 and \'num_values\' in value_spec2:\n        num_values1 = value_spec1[\'num_values\']\n        num_values2 = value_spec2[\'num_values\']\n        if num_values1 == 0:\n            num_values = num_values2\n        elif num_values2 == 0:\n            num_values = num_values1\n        elif num_values1 == num_values2:\n            # num_values = max(num_values1, num_values2)\n            num_values = num_values1\n        else:\n            raise TensorforceError.mismatch(\n                name=\'value-spec\', argument=\'num_values\', value1=num_values1, value2=num_values2\n            )\n        unified_value_spec[\'num_values\'] = num_values\n\n    # Unify min/max_value\n    if \'min_value\' in value_spec1 and \'min_value\' in value_spec2:\n        min_value = min(value_spec1[\'min_value\'], value_spec2[\'min_value\'])\n        max_value = max(value_spec1[\'max_value\'], value_spec2[\'max_value\'])\n        unified_value_spec[\'min_value\'] = min_value\n        unified_value_spec[\'max_value\'] = max_value\n\n    return unified_value_spec\n\n\ndef is_consistent_with_value_spec(value_spec, x):\n    if value_spec[\'type\'] is None:\n        pass\n    elif is_iterable(x=value_spec[\'type\']) and \\\n            any(is_dtype(x=x, dtype=dtype) for dtype in value_spec[\'type\']):\n        pass\n    elif is_dtype(x=x, dtype=value_spec[\'type\']):\n        pass\n    else:\n        return False\n    if value_spec[\'shape\'] is None:\n        pass\n    elif len(shape(x=x)) != len(value_spec[\'shape\']) + int(value_spec.get(\'batched\', True)):\n        return False\n    elif value_spec.get(\'batched\', True):\n        if not all(\n            a == b or b == 0 or b == -1 for a, b in zip(shape(x=x), (-1,) + value_spec[\'shape\'])\n        ):\n            return False\n    elif not all(a == b or b == 0 or b == -1 for a, b in zip(shape(x=x), value_spec[\'shape\'])):\n        return False\n    # num_values\n    # min/max_value\n    return True\n\n\ndef normalize_values(value_type, values, values_spec):\n    if not is_valid_value_type(value_type=value_type):\n        raise TensorforceError.value(\n            name=\'util.normalize_values\', argument=\'value_type\', value=value_type\n        )\n\n    if len(values_spec) == 1 and next(iter(values_spec)) == value_type:\n        # Spec defines only a single value\n        if isinstance(values, dict):\n            if len(values) != 1 or value_type not in values:\n                raise TensorforceError.value(\n                    name=\'util.normalize_values\', values=\'values\', value=values\n                )\n            return values\n\n        else:\n            return OrderedDict([(value_type, values)])\n\n    normalized_values = OrderedDict()\n    for normalized_name in values_spec:\n        value = values\n        for name in normalized_name.split(\'/\'):\n            value = value[name]\n        normalized_values[normalized_name] = value\n\n        # Check whether only expected values present!\n\n    return normalized_values\n\n\ndef unpack_values(value_type, values, values_spec):\n    if not is_valid_value_type(value_type=value_type):\n        raise TensorforceError.value(\n            name=\'util.unpack_values\', argument=\'value_type\', value=value_type\n        )\n\n    if len(values_spec) == 1 and next(iter(values_spec)) == value_type:\n        # Spec defines only a single value\n        return values[value_type]\n\n    unpacked_values = dict()\n    for normalized_name in values_spec:\n        unpacked_value = unpacked_values\n        names = normalized_name.split(\'/\')\n        for name in names[:-1]:\n            if name not in unpacked_value:\n                unpacked_value[name] = dict()\n            unpacked_value = unpacked_value[name]\n        unpacked_value[names[-1]] = values.pop(normalized_name)\n\n    if len(values) > 0:\n        raise TensorforceError.value(\n            name=\'util.unpack_values\', argument=\'values\', value=values\n        )\n\n    return unpacked_values\n\n\n# def get_object(obj, predefined_objects=None, default_object=None, kwargs=None):\n#     """"""\n#     Utility method to map some kind of object specification to its content,\n#     e.g. optimizer or baseline specifications to the respective classes.\n\n#     Args:\n#         obj: A specification dict (value for key \'type\' optionally specifies\n#                 the object, options as follows), a module path (e.g.,\n#                 my_module.MyClass), a key in predefined_objects, or a callable\n#                 (e.g., the class type object).\n#         predefined_objects: Dict containing predefined set of objects,\n#                 accessible via their key\n#         default_object: Default object is no other is specified\n#         kwargs: Arguments for object creation\n\n#     Returns: The retrieved object\n\n#     """"""\n#     args = ()\n#     kwargs = dict() if kwargs is None else kwargs\n\n#     if isinstance(obj, str) and os.path.isfile(obj):\n#         with open(obj, \'r\') as fp:\n#             obj = json.load(fp=fp)\n#     if isinstance(obj, dict):\n#         kwargs.update(obj)\n#         obj = kwargs.pop(\'type\', None)\n\n#     if predefined_objects is not None and obj in predefined_objects:\n#         obj = predefined_objects[obj]\n#     elif isinstance(obj, str):\n#         if obj.find(\'.\') != -1:\n#             module_name, function_name = obj.rsplit(\'.\', 1)\n#             module = importlib.import_module(module_name)\n#             obj = getattr(module, function_name)\n#         else:\n#             raise TensorforceError(""Error: object {} not found in predefined objects: {}"".format(\n#                 obj,\n#                 list(predefined_objects or ())\n#             ))\n#     elif callable(obj):\n#         pass\n#     elif default_object is not None:\n#         args = (obj,)\n#         obj = default_object\n#     else:\n#         # assumes the object is already instantiated\n#         return obj\n\n#     return obj(*args, **kwargs)\n\n\n# def prepare_kwargs(raw, string_parameter=\'name\'):\n#     """"""\n#     Utility method to convert raw string/diction input into a dictionary to pass\n#     into a function.  Always returns a dictionary.\n\n#     Args:\n#         raw: string or dictionary, string is assumed to be the name of the activation\n#                 activation function.  Dictionary will be passed through unchanged.\n\n#     Returns: kwargs dictionary for **kwargs\n\n#     """"""\n#     kwargs = dict()\n\n#     if isinstance(raw, dict):\n#         kwargs.update(raw)\n#     elif isinstance(raw, str):\n#         kwargs[string_parameter] = raw\n\n#     return kwargs\n\n\ndef strip_name_scope(name, base_scope):\n    if name.startswith(base_scope):\n        return name[len(base_scope):]\n    else:\n        return name\n\n\nclass SavableComponent(object):\n    """"""\n    Component that can save and restore its own state.\n    """"""\n\n    def register_saver_ops(self):\n        """"""\n        Registers the saver operations to the graph in context.\n        """"""\n\n        variables = self.get_savable_variables()\n        if variables is None or len(variables) == 0:\n            self._saver = None\n            return\n\n        base_scope = self._get_base_variable_scope()\n        variables_map = {strip_name_scope(v.name, base_scope): v for v in variables}\n\n        self._saver = tf.train.Saver(\n            var_list=variables_map,\n            reshape=False,\n            sharded=False,\n            max_to_keep=5,\n            keep_checkpoint_every_n_hours=10000.0,\n            name=None,\n            restore_sequentially=False,\n            saver_def=None,\n            builder=None,\n            defer_build=False,\n            allow_empty=True,\n            pad_step_number=False,\n            save_relative_paths=True\n        )\n\n    def get_savable_variables(self):\n        """"""\n        Returns the list of all the variables this component is responsible to save and restore.\n\n        Returns:\n            The list of variables that will be saved or restored.\n        """"""\n\n        raise NotImplementedError()\n\n    def save(self, sess, save_path, timestep=None):\n        """"""\n        Saves this component\'s managed variables.\n\n        Args:\n            sess: The session for which to save the managed variables.\n            save_path: The path to save data to.\n            timestep: Optional, the timestep to append to the file name.\n\n        Returns:\n            Checkpoint path where the model was saved.\n        """"""\n\n        if self._saver is None:\n            raise TensorforceError(""register_saver_ops should be called before save"")\n        return self._saver.save(\n            sess=sess,\n            save_path=save_path,\n            global_step=timestep,\n            write_meta_graph=False,\n            write_state=True,  # Do we need this?\n        )\n\n    def restore(self, sess, save_path):\n        """"""\n        Restores the values of the managed variables from disk location.\n\n        Args:\n            sess: The session for which to save the managed variables.\n            save_path: The path used to save the data to.\n        """"""\n\n        if self._saver is None:\n            raise TensorforceError(""register_saver_ops should be called before restore"")\n        self._saver.restore(sess=sess, save_path=save_path)\n\n    def _get_base_variable_scope(self):\n        """"""\n        Returns the portion of the variable scope that is considered a base for this component. The variables will be\n        saved with names relative to that scope.\n\n        Returns:\n            The name of the base variable scope, should always end with ""/"".\n        """"""\n\n        raise NotImplementedError()\n'"
test/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
test/test_agents.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom tensorforce import Environment\nfrom test.unittest_base import UnittestBase\n\n\nclass TestAgents(UnittestBase, unittest.TestCase):\n\n    agent = dict()\n    require_observe = True\n\n    def test_ac(self):\n        self.start_tests(name=\'AC\')\n        self.unittest(\n            agent=\'ac\', batch_size=4, network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2),\n            critic_network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)\n        )\n\n    def test_a2c(self):\n        self.start_tests(name=\'A2C\')\n        self.unittest(\n            agent=\'a2c\', batch_size=4, network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2),\n            critic_network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)\n        )\n\n    def test_dpg(self):\n        self.start_tests(name=\'DPG\')\n        self.unittest(\n            actions=dict(type=\'float\', shape=(), min_value=-1.0, max_value=1.0),\n            agent=\'dpg\', memory=100, batch_size=4,\n            network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2),\n            critic_network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)\n        )\n\n    def test_dqn(self):\n        self.start_tests(name=\'DQN\')\n        self.unittest(\n            actions=dict(type=\'int\', shape=(2,), num_values=4),\n            agent=\'dqn\', memory=100, batch_size=4,\n            network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)\n        )\n\n    def test_dueling_dqn(self):\n        self.start_tests(name=\'DuelingDQN\')\n        self.unittest(\n            actions=dict(type=\'int\', shape=(2,), num_values=4),\n            agent=\'dueling_dqn\', memory=100, batch_size=4,\n            network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)\n        )\n\n    def test_ppo(self):\n        self.start_tests(name=\'PPO\')\n        self.unittest(\n            agent=\'ppo\', batch_size=2, network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)\n        )\n\n    def test_trpo(self):\n        self.start_tests(name=\'TRPO\')\n        self.unittest(\n            agent=\'trpo\', batch_size=2, network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)\n        )\n\n    def test_vpg(self):\n        self.start_tests(name=\'VPG\')\n        self.unittest(\n            agent=\'vpg\', batch_size=2, network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)\n        )\n'"
test/test_constant_agent.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom test.unittest_agent import UnittestAgent\n\n\nclass TestConstantAgent(UnittestAgent, unittest.TestCase):\n\n    num_episodes = 2\n    config = dict(type=\'constant\')\n    has_experience = False\n    has_update = False\n'"
test/test_documentation.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom tensorforce import Agent, Environment, Runner\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestDocumentation(UnittestBase, unittest.TestCase):\n\n    def test_environment(self):\n        self.start_tests(name=\'getting-started-environment\')\n\n        environment = Environment.create(\n            environment=\'gym\', level=\'CartPole\', max_episode_timesteps=500\n        )\n        self.finished_test()\n\n        environment = Environment.create(environment=\'gym\', level=\'CartPole-v1\')\n        self.finished_test()\n\n        environment = Environment.create(\n            environment=\'test/data/environment.json\', max_episode_timesteps=500\n        )\n        self.finished_test()\n\n        environment = Environment.create(\n            environment=\'test.data.custom_env.CustomEnvironment\', max_episode_timesteps=10\n        )\n        self.finished_test()\n\n    def test_agent(self):\n        self.start_tests(name=\'getting-started-agent\')\n\n        environment = Environment.create(\n            environment=\'gym\', level=\'CartPole\', max_episode_timesteps=50\n        )\n        self.finished_test()\n\n        agent = Agent.create(\n            agent=\'tensorforce\', environment=environment, update=64,\n            objective=\'policy_gradient\', reward_estimation=dict(horizon=20)\n        )\n        self.finished_test()\n\n        agent = Agent.create(\n            agent=\'ppo\', environment=environment, batch_size=10, learning_rate=1e-3\n        )\n        self.finished_test()\n\n        agent = Agent.create(agent=\'test/data/agent.json\', environment=environment)\n        self.finished_test()\n\n    def test_execution(self):\n        self.start_tests(name=\'getting-started-execution\')\n\n        runner = Runner(\n            agent=\'test/data/agent.json\', environment=dict(environment=\'gym\', level=\'CartPole\'),\n            max_episode_timesteps=10\n        )\n        runner.run(num_episodes=10)\n        runner.run(num_episodes=5, evaluation=True)\n        runner.close()\n        self.finished_test()\n\n        # Create agent and environment\n        environment = Environment.create(\n            environment=\'test/data/environment.json\', max_episode_timesteps=10\n        )\n        agent = Agent.create(agent=\'test/data/agent.json\', environment=environment)\n\n        # Train for 200 episodes\n        for _ in range(10):\n            states = environment.reset()\n            terminal = False\n            while not terminal:\n                actions = agent.act(states=states)\n                states, terminal, reward = environment.execute(actions=actions)\n                agent.observe(terminal=terminal, reward=reward)\n\n        # Evaluate for 100 episodes\n        sum_rewards = 0.0\n        for _ in range(5):\n            states = environment.reset()\n            internals = agent.initial_internals()\n            terminal = False\n            while not terminal:\n                actions, internals = agent.act(states=states, internals=internals, evaluation=True)\n                states, terminal, reward = environment.execute(actions=actions)\n                sum_rewards += reward\n\n        sum_rewards / 100\n\n        # Close agent and environment\n        agent.close()\n        environment.close()\n\n        self.finished_test()\n\n    def test_readme(self):\n        self.start_tests(name=\'readme\')\n\n        # ====================\n\n        from tensorforce import Agent, Environment\n\n        # Pre-defined or custom environment\n        environment = Environment.create(\n            environment=\'gym\', level=\'CartPole\', max_episode_timesteps=500\n        )\n\n        # Instantiate a Tensorforce agent\n        agent = Agent.create(\n            agent=\'tensorforce\',\n            environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n            memory=1000,\n            update=dict(unit=\'timesteps\', batch_size=64),\n            optimizer=dict(type=\'adam\', learning_rate=3e-4),\n            policy=dict(network=\'auto\'),\n            objective=\'policy_gradient\',\n            reward_estimation=dict(horizon=20)\n        )\n\n        # Train for 300 episodes\n        for _ in range(1):\n\n            # Initialize episode\n            states = environment.reset()\n            terminal = False\n\n            while not terminal:\n                # Episode timestep\n                actions = agent.act(states=states)\n                states, terminal, reward = environment.execute(actions=actions)\n                agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n        environment.close()\n\n        # ====================\n\n        self.finished_test()\n\n    def test_states_actions_multi_input(self):\n        self.start_tests(name=\'states-actions-multi-input\')\n\n        agent, environment = self.prepare(\n            states=dict(\n                observation=dict(type=\'float\', shape=(16, 16, 3)),\n                attributes=dict(type=\'int\', shape=(4, 2), num_values=5)\n            ),\n            actions=dict(type=\'float\', shape=10),\n            policy=[\n                [\n                    dict(type=\'retrieve\', tensors=\'observation\'),\n                    dict(type=\'conv2d\', size=16),\n                    dict(type=\'flatten\'),\n                    dict(type=\'register\', tensor=\'obs-embedding\')\n                ],\n                [\n                    dict(type=\'retrieve\', tensors=\'attributes\'),\n                    dict(type=\'embedding\', size=16),\n                    dict(type=\'flatten\'),\n                    dict(type=\'register\', tensor=\'attr-embedding\')\n                ],\n                [\n                    dict(\n                        type=\'retrieve\', tensors=[\'obs-embedding\', \'attr-embedding\'],\n                        aggregation=\'concat\'\n                    ),\n                    dict(type=\'dense\', size=32)\n                ]\n            ]\n        )\n\n        agent.close()\n        environment.close()\n        self.finished_test()\n\n    def test_masking(self):\n        self.start_tests(name=\'masking\')\n\n        agent, environment = self.prepare(\n            states=dict(type=\'float\', shape=(10,)),\n            actions=dict(type=\'int\', shape=(), num_values=3)\n        )\n        states = environment.reset()\n        assert \'state\' in states and \'action_mask\' in states\n        states[\'action_mask\'] = [True, False, True]\n\n        action = agent.act(states=states)\n        assert action != 1\n\n        agent.close()\n        environment.close()\n        self.finished_test()\n'"
test/test_environments.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\nfrom threading import Thread\nimport unittest\n\nfrom tensorforce import Environment, Runner\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestEnvironments(UnittestBase, unittest.TestCase):\n\n    num_episodes = 2\n\n    @pytest.mark.skip(reason=\'problems with processes/sockets in travis\')\n    def test_remote_environments(self):\n        self.start_tests(name=\'remote-environments\')\n\n        agent = self.agent_spec(\n            require_observe=True, update=dict(unit=\'episodes\', batch_size=1),\n            parallel_interactions=2\n        )\n        environment = self.environment_spec()\n\n        runner = Runner(\n            agent=agent, environment=environment, num_parallel=2, remote=\'multiprocessing\'\n        )\n        runner.run(num_episodes=self.__class__.num_episodes, use_tqdm=False)\n        runner.close()\n        self.finished_test()\n\n        def server(port):\n            Environment.create(environment=environment, remote=\'socket-server\', port=port)\n\n        server1 = Thread(target=server, kwargs=dict(port=65432))\n        server2 = Thread(target=server, kwargs=dict(port=65433))\n        server1.start()\n        server2.start()\n        runner = Runner(\n            agent=agent, num_parallel=2, remote=\'socket-client\', host=\'127.0.0.1\', port=65432\n        )\n        runner.run(num_episodes=self.__class__.num_episodes, use_tqdm=False)\n        runner.close()\n        server1.join()\n        server2.join()\n\n        self.finished_test()\n\n    # @pytest.mark.skip(reason=\'not installed as part of travis\')\n    # def test_ale(self):\n    #     self.start_tests(name=\'ale\')\n    #     self.unittest(\n    #         environment=dict(environment=\'ale\', level=\'test/data/Breakout.bin\'), num_episodes=2\n    #     )\n\n    # @pytest.mark.skip(reason=\'not installed as part of travis\')\n    # def test_maze_explorer(self):\n    #     self.start_tests(name=\'maze-explorer\')\n    #     self.unittest(environment=dict(environment=\'mazeexp\', level=0))\n\n    # @pytest.mark.skip(reason=\'not installed as part of travis\')\n    # def test_open_sim(self):\n    #     self.start_tests(name=\'open-sim\')\n    #     self.unittest(environment=dict(environment=\'osim\', level=\'Arm2D\'))\n\n    def test_openai_gym(self):\n        self.start_tests(name=\'openai-gym\')\n\n        # state: box, action: discrete\n        self.unittest(environment=dict(environment=\'gym\', level=\'CartPole-v0\'))\n\n        # state: discrete, action: box\n        self.unittest(\n            environment=dict(environment=\'gym\', level=\'GuessingGame\', max_episode_steps=False)\n        )\n\n        # state: discrete, action: tuple(discrete)\n        from gym.envs.algorithmic import ReverseEnv\n        self.unittest(environment=ReverseEnv)\n\n        # state: tuple, action: discrete\n        from gym.envs.toy_text import BlackjackEnv\n        self.unittest(environment=BlackjackEnv())\n\n    @pytest.mark.skip(reason=\'breaks / takes too long\')\n    def test_openai_gym2(self):\n        # state: box, action: box with non-uniform bounds\n        # xvfb-run -s ""-screen 0 1400x900x24"" python -m unittest ...\n        self.unittest(environment=\'CarRacing-v0\')\n\n        # Classic control\n        self.unittest(environment=\'CartPole-v1\')\n        self.unittest(environment=\'MountainCar-v0\')\n        self.unittest(environment=\'MountainCarContinuous-v0\')\n        self.unittest(environment=\'Pendulum-v0\')\n        self.unittest(environment=\'Acrobot-v1\')\n\n        # Box2d\n        self.unittest(environment=\'LunarLander-v2\')\n        self.unittest(environment=\'LunarLanderContinuous-v2\')\n        self.unittest(environment=\'BipedalWalker-v3\')\n        self.unittest(environment=\'BipedalWalkerHardcore-v3\')\n        # above: self.unittest(environment=\'CarRacing-v0\')\n\n        # Toy text\n        # above: self.unittest(environment=\'Blackjack-v0\')\n        self.unittest(environment=\'KellyCoinflip-v0\')\n        self.unittest(environment=\'KellyCoinflipGeneralized-v0\')\n        self.unittest(environment=\'FrozenLake-v0\')\n        self.unittest(environment=\'FrozenLake8x8-v0\')\n        self.unittest(environment=\'CliffWalking-v0\')\n        self.unittest(environment=\'NChain-v0\')\n        self.unittest(environment=\'Roulette-v0\')\n        self.unittest(environment=\'Taxi-v3\')\n        self.unittest(environment=\'GuessingGame-v0\')\n        self.unittest(environment=\'HotterColder-v0\')\n\n        # Algorithmic\n        self.unittest(environment=\'Copy-v0\')\n        self.unittest(environment=\'RepeatCopy-v0\')\n        self.unittest(environment=\'ReversedAddition-v0\')\n        self.unittest(environment=\'ReversedAddition3-v0\')\n        self.unittest(environment=\'DuplicatedInput-v0\')\n        # above: self.unittest(environment=\'Reverse-v0\')\n\n        # Unit test\n        self.unittest(environment=\'CubeCrash-v0\')\n        self.unittest(environment=\'CubeCrashSparse-v0\')\n        self.unittest(environment=\'CubeCrashScreenBecomesBlack-v0\')\n        self.unittest(environment=\'MemorizeDigits-v0\')\n\n    def test_openai_retro(self):\n        self.start_tests(name=\'openai-retro\')\n        self.unittest(environment=dict(environment=\'retro\', level=\'Airstriker-Genesis\'))\n\n    @pytest.mark.skip(reason=\'not installed as part of travis\')\n    def test_ple(self):\n        self.start_tests(name=\'pygame-learning-environment\')\n        self.unittest(environment=dict(environment=\'ple\', level=\'Pong\'))\n\n    @pytest.mark.skip(reason=\'not installed as part of travis\')\n    def test_vizdoom(self):\n        self.start_tests(name=\'vizdoom\')\n        self.unittest(\n            environment=dict(environment=\'vizdoom\', level=\'test/data/basic.cfg\'), memory=1000\n        )\n'"
test/test_examples.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom tensorforce import Agent, Environment, Runner\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestExamples(UnittestBase, unittest.TestCase):\n\n    def test_quickstart(self):\n        self.start_tests(name=\'quickstart\')\n\n        # ====================\n\n        # Create an OpenAI-Gym environment\n        environment = Environment.create(environment=\'gym\', level=\'CartPole-v1\')\n\n        # Create a PPO agent\n        agent = Agent.create(\n            agent=\'ppo\', environment=environment,\n            # Automatically configured network\n            network=\'auto\',\n            # Optimization\n            batch_size=10, update_frequency=2, learning_rate=1e-3, subsampling_fraction=0.2,\n            optimization_steps=5,\n            # Reward estimation\n            likelihood_ratio_clipping=0.2, discount=0.99, estimate_terminal=False,\n            # Critic\n            critic_network=\'auto\',\n            critic_optimizer=dict(optimizer=\'adam\', multi_step=10, learning_rate=1e-3),\n            # Preprocessing\n            preprocessing=None,\n            # Exploration\n            exploration=0.0, variable_noise=0.0,\n            # Regularization\n            l2_regularization=0.0, entropy_regularization=0.0,\n            # TensorFlow etc\n            name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n            summarizer=None, recorder=None\n        )\n\n        # Initialize the runner\n        runner = Runner(agent=agent, environment=environment)\n\n        # Start the runner\n        runner.run(num_episodes=50)\n        runner.close()\n\n        # ====================\n\n        self.finished_test()\n\n    def test_temperature_controller(self):\n        self.start_tests(name=\'temperature-controller\')\n\n        # ====================\n\n        import pandas as pd\n        import matplotlib.pyplot as plt\n        import numpy as np\n        import math\n\n        ## Compute the response for a given action and current temperature\n        def respond(action, current_temp, tau):\n            return action + (current_temp - action) * math.exp(-1.0/tau)\n\n        ## Actions of a series of on, then off\n        sAction = pd.Series(np.array([1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0]))\n        sResponse = np.zeros(sAction.size)\n\n        ## Update the response with the response to the action\n        for i in range(sAction.size):\n            ## Get last response\n            if i == 0:\n                last_response = 0\n            else:\n                last_response = sResponse[i - 1]\n            sResponse[i] = respond(sAction[i], last_response, 3.0)\n\n        ## Assemble and plot\n        df = pd.DataFrame(list(zip(sAction, sResponse)), columns=[\'action\', \'response\'])\n        df.plot()\n\n        # ====================\n\n        def reward(temp):\n                delta = abs(temp - 0.5)\n                if delta < 0.1:\n                    return 0.0\n                else:\n                    return -delta + 0.1\n\n        temps = [x * 0.01 for x in range(100)]\n        rewards = [reward(x) for x in temps]\n\n        fig=plt.figure(figsize=(12, 4))\n\n        plt.scatter(temps, rewards)\n        plt.xlabel(\'Temperature\')\n        plt.ylabel(\'Reward\')\n        plt.title(\'Reward vs. Temperature\')\n\n        # ====================\n\n        ###-----------------------------------------------------------------------------\n        ## Imports\n        from tensorforce.environments import Environment\n        from tensorforce.agents import Agent\n\n        ###-----------------------------------------------------------------------------\n        ### Environment definition\n        class ThermostatEnvironment(Environment):\n            """"""This class defines a simple thermostat environment.  It is a room with\n            a heater, and when the heater is on, the room temperature will approach\n            the max heater temperature (usually 1.0), and when off, the room will\n            decay to a temperature of 0.0.  The exponential constant that determines\n            how fast it approaches these temperatures over timesteps is tau.\n            """"""\n            def __init__(self):\n                ## Some initializations.  Will eventually parameterize this in the constructor.\n                self.tau = 3.0\n                self.current_temp = np.random.random(size=(1,))\n\n                super().__init__()\n\n            def states(self):\n                return dict(type=\'float\', shape=(1,))\n\n            def actions(self):\n                """"""Action 0 means no heater, temperature approaches 0.0.  Action 1 means\n                the heater is on and the room temperature approaches 1.0.\n                """"""\n                return dict(type=\'int\', num_values=2)\n\n            # Optional, should only be defined if environment has a natural maximum\n            # episode length\n            def max_episode_timesteps(self):\n                return super().max_episode_timesteps()\n\n            # Optional\n            def close(self):\n                super().close()\n\n            def reset(self):\n                """"""Reset state.\n                """"""\n                # state = np.random.random(size=(1,))\n                self.timestep = 0\n                self.current_temp = np.random.random(size=(1,))\n                return self.current_temp\n\n            def response(self, action):\n                """"""Respond to an action.  When the action is 1, the temperature\n                exponentially decays approaches 1.0.  When the action is 0,\n                the current temperature decays towards 0.0.\n                """"""\n                return action + (self.current_temp - action) * math.exp(-1.0 / self.tau)\n\n            def reward_compute(self):\n                """""" The reward here is 0 if the current temp is between 0.4 and 0.6,\n                else it is distance the temp is away from the 0.4 or 0.6 boundary.\n                \n                Return the value within the numpy array, not the numpy array.\n                """"""\n                delta = abs(self.current_temp - 0.5)\n                if delta < 0.1:\n                    return 0.0\n                else:\n                    return -delta[0] + 0.1\n\n            def execute(self, actions):\n                ## Check the action is either 0 or 1 -- heater on or off.\n                assert actions == 0 or actions == 1\n\n                ## Increment timestamp\n                self.timestep += 1\n                \n                ## Update the current_temp\n                self.current_temp = self.response(actions)\n                \n                ## Compute the reward\n                reward = self.reward_compute()\n\n                ## The only way to go terminal is to exceed max_episode_timestamp.\n                ## terminal == False means episode is not done\n                ## terminal == True means it is done.\n                terminal = False\n                if self.timestep > self.max_episode_timesteps():\n                    terminal = True\n                \n                return self.current_temp, terminal, reward\n\n        ###-----------------------------------------------------------------------------\n        ### Create the environment\n        ###   - Tell it the environment class\n        ###   - Set the max timestamps that can happen per episode\n        environment = environment = Environment.create(\n            environment=ThermostatEnvironment,\n            max_episode_timesteps=100)\n\n        # ====================\n\n        agent = Agent.create(\n            agent=\'tensorforce\', environment=environment, update=64,\n            objective=\'policy_gradient\', reward_estimation=dict(horizon=1)\n        )\n\n        # ====================\n\n        ### Initialize\n        environment.reset()\n\n        ## Creation of the environment via Environment.create() creates\n        ## a wrapper class around the original Environment defined here.\n        ## That wrapper mainly keeps track of the number of timesteps.\n        ## In order to alter the attributes of your instance of the original\n        ## class, like to set the initial temp to a custom value, like here,\n        ## you need to access the `environment` member of this wrapped class.\n        ## That is why you see the way to set the current_temp like below.\n        environment.environment.current_temp = np.array([0.5])\n        states = environment.environment.current_temp\n\n        internals = agent.initial_internals()\n        terminal = False\n\n        ### Run an episode\n        temp = [environment.environment.current_temp[0]]\n        while not terminal:\n            actions, internals = agent.act(states=states, internals=internals, evaluation=True)\n            states, terminal, reward = environment.execute(actions=actions)\n            temp += [states[0]]\n\n        ### Plot the run\n        plt.figure(figsize=(12, 4))\n        ax=plt.subplot()\n        ax.set_ylim([0.0, 1.0])\n        plt.plot(range(len(temp)), temp)\n        plt.hlines(y=0.4, xmin=0, xmax=99, color=\'r\')\n        plt.hlines(y=0.6, xmin=0, xmax=99, color=\'r\')\n        plt.xlabel(\'Timestep\')\n        plt.ylabel(\'Temperature\')\n        plt.title(\'Temperature vs. Timestep\')\n        plt.show()\n\n        # Train for 200 episodes\n        for _ in range(50):\n            states = environment.reset()\n            terminal = False\n            while not terminal:\n                actions = agent.act(states=states)\n                states, terminal, reward = environment.execute(actions=actions)\n                agent.observe(terminal=terminal, reward=reward)\n\n        # ====================\n\n        ### Initialize\n        environment.reset()\n\n        ## Creation of the environment via Environment.create() creates\n        ## a wrapper class around the original Environment defined here.\n        ## That wrapper mainly keeps track of the number of timesteps.\n        ## In order to alter the attributes of your instance of the original\n        ## class, like to set the initial temp to a custom value, like here,\n        ## you need to access the `environment` member of this wrapped class.\n        ## That is why you see the way to set the current_temp like below.\n        environment.environment.current_temp = np.array([1.0])\n        states = environment.environment.current_temp\n\n        internals = agent.initial_internals()\n        terminal = False\n\n        ### Run an episode\n        temp = [environment.environment.current_temp[0]]\n        while not terminal:\n            actions, internals = agent.act(states=states, internals=internals, evaluation=True)\n            states, terminal, reward = environment.execute(actions=actions)\n            temp += [states[0]]\n\n        ### Plot the run\n        plt.figure(figsize=(12, 4))\n        ax=plt.subplot()\n        ax.set_ylim([0.0, 1.0])\n        plt.plot(range(len(temp)), temp)\n        plt.hlines(y=0.4, xmin=0, xmax=99, color=\'r\')\n        plt.hlines(y=0.6, xmin=0, xmax=99, color=\'r\')\n        plt.xlabel(\'Timestep\')\n        plt.ylabel(\'Temperature\')\n        plt.title(\'Temperature vs. Timestep\')\n        plt.show()\n\n        # ====================\n\n        self.finished_test()\n'"
test/test_layers.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestLayers(UnittestBase, unittest.TestCase):\n\n    num_timesteps = 2\n\n    def test_convolution(self):\n        self.start_tests(name=\'convolution\')\n\n        states = dict(type=\'float\', shape=(2, 3))\n        actions = dict(\n            bool_action=dict(type=\'bool\', shape=(2,)),\n            int_action=dict(type=\'int\', shape=(2, 2), num_values=4),\n            float_action=dict(type=\'float\', shape=(2,)),\n            bounded_action=dict(type=\'float\', shape=(2, 2), min_value=-0.5, max_value=0.5)\n        )\n        network = [\n            dict(type=\'conv1d\', size=8),\n            # dict(type=\'conv1d_transpose\', size=8),\n            dict(type=\'linear\', size=8)\n        ]\n        self.unittest(states=states, actions=actions, policy=dict(network=network))\n\n        states = dict(type=\'float\', shape=(2, 2, 3))\n        actions = dict(\n            bool_action=dict(type=\'bool\', shape=(2, 2,)),\n            int_action=dict(type=\'int\', shape=(2, 2, 2), num_values=4),\n            float_action=dict(type=\'float\', shape=(2, 2,)),\n            bounded_action=dict(type=\'float\', shape=(2, 2, 2), min_value=-0.5, max_value=0.5)\n        )\n        network = [\n            dict(type=\'conv2d\', size=8),\n            # dict(type=\'conv2d_transpose\', size=8),\n            dict(type=\'linear\', size=8)\n        ]\n        self.unittest(states=states, actions=actions, policy=dict(network=network))\n\n    def test_dense(self):\n        self.start_tests(name=\'dense\')\n\n        states = dict(type=\'float\', shape=(3,))\n        network = [\n            dict(type=\'dense\', size=8),\n            dict(type=\'linear\', size=8)\n        ]\n        self.unittest(states=states, policy=dict(network=network))\n\n    def test_embedding(self):\n        self.start_tests(name=\'embedding\')\n\n        states = dict(type=\'int\', shape=(), num_values=5)\n        network = [dict(type=\'embedding\', size=8)]\n        self.unittest(states=states, policy=dict(network=network))\n\n    def test_internal_rnn(self):\n        self.start_tests(name=\'internal-rnn\')\n\n        states = dict(type=\'float\', shape=(3,))\n        network = [\n            dict(type=\'internal_rnn\', cell=\'gru\', size=8, length=2),\n            dict(type=\'internal_lstm\', size=8, length=2)\n        ]\n        self.unittest(states=states, policy=dict(network=network))\n\n    def test_keras(self):\n        self.start_tests(name=\'keras\')\n\n        states = dict(type=\'float\', shape=(3,))\n        network = [dict(type=\'keras\', layer=\'Dense\', units=8)]\n        self.unittest(states=states, policy=dict(network=network))\n\n    def test_misc(self):\n        self.start_tests(name=\'misc\')\n\n        states = dict(type=\'float\', shape=(3, 2))\n        network = [\n            dict(type=\'activation\', nonlinearity=\'tanh\'),\n            dict(type=\'dropout\', rate=0.5),\n            dict(type=\'function\', function=(lambda x: x + 1.0)),\n            dict(type=\'reshape\', shape=6),\n            dict(function=(lambda x: x[:, :2]), output_spec=dict(shape=(2,)))\n        ]\n        self.unittest(states=states, policy=dict(network=network))\n\n        states = dict(type=\'float\', shape=(3,))\n        network = [\n            dict(type=\'block\', name=\'test\', layers=[\n                dict(type=\'dense\', size=3), dict(type=\'dense\', size=3)\n            ]),\n            dict(type=\'reuse\', layer=\'test\')\n        ]\n        self.unittest(states=states, policy=dict(network=network))\n\n        states = dict(type=\'float\', shape=(3,))\n        network = [\n            dict(type=\'register\', tensor=\'test\'),\n            dict(type=\'retrieve\', tensors=\'test\'),\n            dict(type=\'retrieve\', tensors=(\'*\', \'test\'), aggregation=\'product\')\n        ]\n        self.unittest(states=states, policy=dict(network=network))\n\n    def test_normalization(self):\n        self.start_tests(name=\'normalization\')\n\n        states = dict(type=\'float\', shape=(3,))\n        network = [\n            dict(type=\'exponential_normalization\'),\n            dict(type=\'instance_normalization\')\n        ]\n        self.unittest(states=states, require_observe=True, policy=dict(network=network))\n\n    def test_pooling(self):\n        self.start_tests(name=\'pooling\')\n\n        states = dict(type=\'float\', shape=(2, 3))\n        network = [\n            dict(type=\'pool1d\', reduction=\'average\'),\n            dict(type=\'flatten\')\n        ]\n        self.unittest(states=states, policy=dict(network=network))\n\n        states = dict(type=\'float\', shape=(2, 2, 3))\n        network = [\n            dict(type=\'pool2d\', reduction=\'max\'),\n            dict(type=\'pooling\', reduction=\'max\')\n        ]\n        self.unittest(states=states, policy=dict(network=network))\n\n    def test_preprocessing(self):\n        self.start_tests(name=\'preprocessing\')\n\n        states = dict(type=\'float\', shape=())\n        preprocessing = dict(\n            state=[\n                dict(type=\'sequence\', length=3, concatenate=False),\n                dict(type=\'clipping\', lower=-1.0, upper=1.0)\n            ], reward=[dict(type=\'clipping\', lower=-1.0, upper=1.0)]\n        )\n        network = [dict(type=\'dense\', name=\'test\', size=8)]\n        self.unittest(states=states, preprocessing=preprocessing, policy=dict(network=network))\n\n        states = dict(type=\'float\', shape=(4, 4, 3))\n        preprocessing = dict(\n            state=[\n                dict(type=\'image\', height=2, width=2, grayscale=True),\n                dict(type=\'deltafier\', concatenate=0),\n                dict(type=\'sequence\', length=4)\n            ], reward=dict(type=\'deltafier\')\n        )\n        network = [dict(type=\'reshape\', shape=32)]\n        agent, environment = self.prepare(\n            min_timesteps=4, states=states, require_all=True, policy=dict(network=network),\n            preprocessing=preprocessing\n        )\n\n        states = environment.reset()\n        terminal = False\n        while not terminal:\n            actions = agent.act(states=states)\n            states, terminal, reward = environment.execute(actions=actions)\n            agent.observe(terminal=terminal, reward=reward)\n\n        states = environment.reset()\n        internals = agent.initial_internals()\n        terminal = False\n        while not terminal:\n            actions, next_internals = agent.act(\n                states=states, internals=internals, independent=True\n            )\n            next_states, terminal, reward = environment.execute(actions=actions)\n            agent.experience(\n                states=states, internals=internals, actions=actions, terminal=terminal,\n                reward=reward\n            )\n            states = next_states\n            internals = next_internals\n        agent.update()\n\n    def test_rnn(self):\n        self.start_tests(name=\'rnn\')\n\n        states = dict(type=\'float\', shape=(2, 3))\n        network = [\n            dict(type=\'rnn\', cell=\'gru\', size=8, return_final_state=False),\n            dict(type=\'lstm\', size=8)\n        ]\n        self.unittest(states=states, policy=dict(network=network))\n'"
test/test_memories.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestMemories(UnittestBase, unittest.TestCase):\n\n    require_observe = True\n\n    def test_recent(self):\n        self.start_tests(name=\'recent\')\n\n        memory = dict(type=\'recent\')\n        update = dict(unit=\'timesteps\', batch_size=4)\n        self.unittest(update=update, memory=memory)\n\n        memory = dict(type=\'recent\')\n        update = dict(unit=\'episodes\', batch_size=1)\n        self.unittest(update=update, memory=memory)\n\n    def test_replay(self):\n        self.start_tests(name=\'replay\')\n\n        memory = dict(type=\'replay\')\n        update = dict(unit=\'timesteps\', batch_size=4)\n        self.unittest(update=update, memory=memory)\n\n        memory = dict(type=\'replay\')\n        update = dict(unit=\'episodes\', batch_size=1)\n        self.unittest(update=update, memory=memory)\n\n        memory = 100\n        update = 4\n        self.unittest(update=update, memory=memory)\n'"
test/test_objectives.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestObjectives(UnittestBase, unittest.TestCase):\n\n    require_observe = True\n\n    def test_deterministic_policy_gradient(self):\n        self.start_tests(name=\'deterministic-policy-gradient\')\n\n        objective = dict(type=\'deterministic_policy_gradient\')\n        self.unittest(actions=dict(type=\'float\', shape=()), objective=objective)\n\n    def test_plus(self):\n        self.start_tests(name=\'plus\')\n\n        objective = dict(type=\'plus\', objective1=\'value\', objective2=\'policy_gradient\')\n        self.unittest(objective=objective)\n\n    def test_policy_gradient(self):\n        self.start_tests(name=\'policy-gradient\')\n\n        objective = dict(type=\'policy_gradient\')\n        self.unittest(objective=objective)\n\n        objective = dict(type=\'policy_gradient\', ratio_based=True)\n        self.unittest(objective=objective)\n\n        objective = dict(type=\'policy_gradient\', clipping_value=1.0)\n        self.unittest(objective=objective)\n\n        objective = dict(type=\'policy_gradient\', ratio_based=True, clipping_value=0.2)\n        self.unittest(objective=objective)\n\n        objective = dict(type=\'policy_gradient\', early_reduce=True)\n        self.unittest(objective=objective)\n\n    def test_value(self):\n        self.start_tests(name=\'value\')\n\n        objective = dict(type=\'value\', value=\'state\')\n        self.unittest(objective=objective)\n\n        objective = dict(type=\'value\', value=\'action\')\n        self.unittest(exclude_bounded_action=True, objective=objective)\n\n        objective = dict(type=\'value\', huber_loss=1.0)\n        self.unittest(objective=objective)\n\n        objective = dict(type=\'value\', early_reduce=True)\n        self.unittest(objective=objective)\n'"
test/test_optimizers.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestOptimizers(UnittestBase, unittest.TestCase):\n\n    require_observe = True\n\n    def test_evolutionary(self):\n        self.start_tests(name=\'evolutionary\')\n\n        optimizer = dict(type=\'evolutionary\', learning_rate=1e-3)\n        self.unittest(optimizer=optimizer)\n\n    def test_meta_optimizer_wrapper(self):\n        self.start_tests(name=\'meta-optimizer-wrapper\')\n\n        optimizer = dict(\n            optimizer=\'adam\', learning_rate=1e-3, multi_step=5, subsampling_fraction=0.5,\n            clipping_threshold=1e-2, optimizing_iterations=3\n        )\n        self.unittest(optimizer=optimizer)\n        # agent.close()\n        # environment.close()\n\n    def test_natural_gradient(self):\n        self.start_tests(name=\'natural-gradient\')\n\n        optimizer = dict(type=\'natural_gradient\', learning_rate=1e-3)\n        self.unittest(optimizer=optimizer)\n\n    def test_plus(self):\n        self.start_tests(name=\'plus\')\n\n        optimizer = dict(\n            type=\'plus\', optimizer1=dict(type=\'adam\', learning_rate=1e-3),\n            optimizer2=dict(type=\'adagrad\', learning_rate=1e-3)\n        )\n        self.unittest(optimizer=optimizer)\n\n    def test_synchronization(self):\n        self.start_tests(name=\'synchronization\')\n\n        optimizer = dict(type=\'synchronization\')\n        self.unittest(\n            baseline_policy=dict(network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)),\n            baseline_optimizer=optimizer\n        )\n\n    def test_tf_optimizer(self):\n        self.start_tests(name=\'tf-optimizer\')\n\n        optimizer = dict(type=\'adam\', learning_rate=1e-3)\n        self.unittest(optimizer=optimizer)\n\n        try:\n            import tensorflow_addons as tfa\n\n            optimizer = dict(\n                type=\'radam\', learning_rate=1e-3, decoupled_weight_decay=0.01, lookahead=True,\n                moving_average=True\n            )\n            self.unittest(optimizer=optimizer)\n\n        except ModuleNotFoundError:\n            pass\n'"
test/test_parameters.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nimport numpy as np\n\nfrom tensorforce import util\nfrom test.unittest_base import UnittestBase\n\n\nclass TestParameters(UnittestBase, unittest.TestCase):\n\n    require_observe = True\n\n    def float_unittest(self, exploration):\n        agent, environment = self.prepare(min_timesteps=3, exploration=exploration)\n\n        states = environment.reset()\n        actions, exploration_output1 = agent.act(states=states, query=\'exploration\')\n        self.assertIsInstance(exploration_output1, util.np_dtype(dtype=\'float\'))\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        if not isinstance(exploration, dict) or exploration[\'type\'] == \'constant\':\n            actions, exploration_output2 = agent.act(states=states, query=\'exploration\')\n            self.assertEqual(exploration_output2, exploration_output1)\n            states, terminal, reward = environment.execute(actions=actions)\n            agent.observe(terminal=terminal, reward=reward)\n\n        else:\n            actions, exploration_output2 = agent.act(states=states, query=\'exploration\')\n            self.assertNotEqual(exploration_output2, exploration_output1)\n            states, terminal, reward = environment.execute(actions=actions)\n            agent.observe(terminal=terminal, reward=reward)\n\n        exploration_input = 0.5\n        actions, exploration_output = agent.act(\n            states=states, query=\'exploration\', exploration=exploration_input\n        )\n        self.assertEqual(exploration_output, exploration_input)\n\n        agent.close()\n        environment.close()\n\n        self.finished_test()\n\n    def long_unittest(self, horizon):\n        agent, environment = self.prepare(min_timesteps=3, reward_estimation=dict(horizon=horizon))\n\n        states = environment.reset()\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        _, horizon_output1 = agent.observe(terminal=terminal, reward=reward, query=\'horizon\')\n        self.assertIsInstance(horizon_output1, util.np_dtype(dtype=\'long\'))\n\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        _, horizon_output2 = agent.observe(terminal=terminal, reward=reward, query=\'horizon\')\n        if not isinstance(horizon, dict) or horizon[\'type\'] == \'constant\':\n            self.assertEqual(horizon_output2, horizon_output1)\n        else:\n            self.assertNotEqual(horizon_output2, horizon_output1)\n\n        actions = agent.act(states=states)\n        _, terminal, reward = environment.execute(actions=actions)\n        horizon_input = 3\n        _, horizon_output = agent.observe(\n            terminal=terminal, reward=reward, query=\'horizon\',\n            **{\'estimator/horizon\': horizon_input}\n        )\n        self.assertEqual(\n            horizon_output, np.asarray(horizon_input, dtype=util.np_dtype(dtype=\'long\'))\n        )\n\n        agent.close()\n        environment.close()\n\n        self.finished_test()\n\n    def test_constant(self):\n        self.start_tests(name=\'constant\')\n\n        exploration = 0.1\n        self.float_unittest(exploration=exploration)\n\n        horizon = 4\n        self.long_unittest(horizon=horizon)\n\n    def test_decaying(self):\n        # SPECIFICATION.MD\n        self.start_tests(name=\'decaying\')\n\n        exploration = dict(\n            type=\'decaying\', unit=\'timesteps\', decay=\'exponential\', initial_value=0.1,\n            decay_steps=1, decay_rate=0.5\n        )\n        self.float_unittest(exploration=exploration)\n\n        horizon = dict(\n            type=\'decaying\', dtype=\'long\', unit=\'timesteps\', decay=\'polynomial\',\n            initial_value=2.0, decay_steps=2, final_value=4.0, power=1.0\n        )\n        self.long_unittest(horizon=horizon)\n\n    def test_ornstein_uhlenbeck(self):\n        self.start_tests(name=\'ornstein-uhlenbeck\')\n\n        exploration = dict(type=\'ornstein_uhlenbeck\', absolute=True)\n        self.float_unittest(exploration=exploration)\n\n    def test_piecewise_constant(self):\n        self.start_tests(name=\'piecewise-constant\')\n\n        # first act at timestep 0\n        exploration = dict(\n            type=\'piecewise_constant\', unit=\'timesteps\', boundaries=[0], values=[0.1, 0.0]\n        )\n        self.float_unittest(exploration=exploration)\n\n        # first observe at timestep 1\n        horizon = dict(\n            type=\'piecewise_constant\', dtype=\'long\', unit=\'timesteps\', boundaries=[1],\n            values=[1, 2]\n        )\n        self.long_unittest(horizon=horizon)\n\n    def test_random(self):\n        self.start_tests(name=\'random\')\n\n        exploration = dict(type=\'random\', distribution=\'uniform\')\n        self.float_unittest(exploration=exploration)\n'"
test/test_precision.py,3,"b""# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom test.unittest_base import UnittestBase\n\n\nclass TestPrecision(UnittestBase, unittest.TestCase):\n\n    require_observe = True\n\n    def test_precision(self):\n        self.start_tests()\n\n        try:\n            # TODO: long=int32 since some operations like tf.math.maximum expect >= int32\n            util.np_dtype_mapping = dict(\n                bool=np.bool_, int=np.int16, long=np.int32, float=np.float16\n            )\n            util.tf_dtype_mapping = dict(\n                bool=tf.bool, int=tf.int16, long=tf.int32, float=tf.float16\n            )\n\n            # TODO: Keras RNNs use float32 which causes mismatch during optimization\n            self.unittest(\n                policy=dict(network=dict(type='auto', size=8, depth=1, internal_rnn=False))\n            )\n\n        except BaseException as exc:\n            raise exc\n            self.assertTrue(expr=False)\n\n        finally:\n            util.np_dtype_mapping = dict(\n                bool=np.bool_, int=np.int32, long=np.int64, float=np.float32\n            )\n            util.tf_dtype_mapping = dict(\n                bool=tf.bool, int=tf.int32, long=tf.int64, float=tf.float32\n            )\n"""
test/test_random_agent.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom test.unittest_agent import UnittestAgent\n\n\nclass TestRandomAgent(UnittestAgent, unittest.TestCase):\n\n    num_episodes = 2\n    config = dict(type=\'random\')\n    has_experience = False\n    has_update = False\n'"
test/test_reward_estimation.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestRewardEstimation(UnittestBase, unittest.TestCase):\n\n    agent = dict(\n        policy=dict(network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)),\n        update=dict(unit=\'episodes\', batch_size=1),\n        objective=\'policy_gradient\'\n    )\n    require_observe = True\n\n    def test_no_horizon_estimate(self):\n        self.start_tests(name=\'no horizon estimate\')\n\n        # zero horizon\n        reward_estimation = dict(horizon=0, discount=0.99, estimate_horizon=False)\n        self.unittest(reward_estimation=reward_estimation)\n\n        # horizon longer than episode\n        reward_estimation = dict(horizon=10, discount=0.99, estimate_horizon=False)\n        self.unittest(reward_estimation=reward_estimation)\n\n    def test_early_horizon_estimate(self):\n        self.start_tests(name=\'early horizon estimate\')\n\n        reward_estimation = dict(horizon=2, estimate_horizon=\'early\')\n        self.unittest(reward_estimation=reward_estimation)\n\n        reward_estimation = dict(horizon=2, estimate_horizon=\'early\', estimate_actions=True)\n        baseline_objective = \'policy_gradient\'\n        baseline_optimizer = \'adam\'\n        # TODO: action value doesn\'t exist for Beta\n        self.unittest(\n            exclude_bounded_action=True, reward_estimation=reward_estimation,\n            baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer\n        )\n\n        reward_estimation = dict(horizon=2, estimate_horizon=\'early\', estimate_terminal=True)\n        # TODO: currently requires same internal RNN horizon\n        baseline_policy = dict(network=dict(type=\'auto\', size=7, depth=1, internal_rnn=2))\n        baseline_objective = \'policy_gradient\'\n        self.unittest(\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_objective=baseline_objective\n        )\n\n        reward_estimation = dict(\n            horizon=2, estimate_horizon=\'early\', estimate_actions=True, estimate_terminal=True\n        )\n        # TODO: action value doesn\'t exist for Beta\n        baseline_policy = dict(\n            network=dict(type=\'auto\', size=7, depth=1, internal_rnn=1), use_beta_distribution=False\n        )\n        baseline_objective = \'policy_gradient\'\n        baseline_optimizer = \'adam\'\n        self.unittest(\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer\n        )\n\n    def test_late_horizon_estimate(self):\n        self.start_tests(name=\'late horizon estimate\')\n\n        reward_estimation = dict(horizon=2, estimate_horizon=\'late\')\n        baseline_objective = \'policy_gradient\'\n        self.unittest(reward_estimation=reward_estimation, baseline_objective=baseline_objective)\n\n        reward_estimation = dict(horizon=2, estimate_horizon=\'late\', estimate_actions=True)\n        baseline_objective = \'policy_gradient\'\n        baseline_optimizer = \'adam\'\n        # TODO: action value doesn\'t exist for Beta\n        self.unittest(\n            exclude_bounded_action=True, reward_estimation=reward_estimation,\n            baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer\n        )\n\n        reward_estimation = dict(horizon=2, estimate_horizon=\'late\', estimate_terminal=True)\n        baseline_policy = dict(network=dict(type=\'auto\', size=7, depth=1, internal_rnn=1))\n        baseline_optimizer = \'adam\'\n        self.unittest(\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=baseline_optimizer\n        )\n\n        reward_estimation = dict(\n            horizon=2, estimate_horizon=\'late\', estimate_actions=True, estimate_terminal=True\n        )\n        # TODO: action value doesn\'t exist for Beta\n        baseline_policy = dict(\n            network=dict(type=\'auto\', size=7, depth=1, internal_rnn=1), use_beta_distribution=False\n        )\n        baseline_objective = \'policy_gradient\'\n        baseline_optimizer = \'adam\'\n        self.unittest(\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer\n        )\n\n    def test_advantage_estimate(self):\n        self.start_tests(name=\'advantage estimate\')\n\n        reward_estimation = dict(horizon=2, estimate_horizon=False, estimate_advantage=True)\n        self.unittest(reward_estimation=reward_estimation)\n\n        reward_estimation = dict(\n            horizon=2, estimate_horizon=\'early\', estimate_actions=True, estimate_advantage=True\n        )\n        # TODO: currently incompatible with internal RNN\n        # TODO: action value doesn\'t exist for Beta\n        baseline_policy = dict(\n            network=dict(type=\'auto\', size=7, depth=1, internal_rnn=False),\n            use_beta_distribution=False\n        )\n        baseline_optimizer = \'adam\'\n        self.unittest(\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=baseline_optimizer\n        )\n\n        reward_estimation = dict(\n            horizon=2, estimate_horizon=\'late\', estimate_terminal=True, estimate_advantage=True\n        )\n        baseline_policy = dict(network=dict(type=\'auto\', size=7, depth=1, internal_rnn=1))\n        baseline_objective = \'policy_gradient\'\n        baseline_optimizer = \'adam\'\n        self.unittest(\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer\n        )\n'"
test/test_runner.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport copy\nimport time\nimport unittest\n\nfrom tensorforce import Runner\nfrom test.unittest_base import UnittestBase\n\n\nclass TestRunner(UnittestBase, unittest.TestCase):\n\n    min_timesteps = 6\n    agent = dict(\n        policy=dict(network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)),\n        update=dict(unit=\'episodes\', batch_size=12),\n        objective=\'policy_gradient\', reward_estimation=dict(horizon=3)\n    )\n    require_observe = True\n\n    def test_single(self):\n        self.start_tests(name=\'single\')\n\n        agent = self.agent_spec()\n        environment = self.environment_spec()\n        runner = Runner(agent=agent, environment=environment)\n\n        # default\n        runner.run(num_episodes=3, use_tqdm=False)\n        self.finished_test()\n\n        # evaluation\n        runner.run(num_episodes=1, use_tqdm=False, evaluation=False)\n        self.finished_test()\n\n        # episode callback\n        callback_episode_frequency = 2\n        self.num_callbacks = 0\n\n        def callback(r, p):\n            self.num_callbacks += 1\n            self.assertEqual(r.episodes, self.num_callbacks * callback_episode_frequency)\n\n        runner.run(\n            num_episodes=5, callback=callback,\n            callback_episode_frequency=callback_episode_frequency, use_tqdm=False\n        )\n        self.finished_test()\n\n        # timestep callback\n        callback_timestep_frequency = 3\n        self.num_callbacks = 0\n\n        def callback(r, p):\n            self.num_callbacks += 1\n            self.assertEqual(\n                r.episode_timestep[p], self.num_callbacks * callback_timestep_frequency\n            )\n\n        runner.run(\n            num_episodes=1, callback=callback,\n            callback_timestep_frequency=callback_timestep_frequency, use_tqdm=False\n        )\n        self.finished_test()\n\n        # multiple callbacks\n        self.is_callback1 = False\n        self.is_callback2 = False\n\n        def callback1(r, p):\n            self.is_callback1 = True\n\n        def callback2(r, p):\n            self.is_callback2 = True\n\n        runner.run(\n            num_episodes=1, callback=[callback1, callback2],\n            callback_timestep_frequency=callback_timestep_frequency, use_tqdm=False\n        )\n        runner.close()\n        self.finished_test(assertion=(self.is_callback1 and self.is_callback2))\n\n    def test_unbatched(self):\n        self.start_tests(name=\'unbatched\')\n\n        agent = self.agent_spec()\n        environment = self.environment_spec()\n\n        # default\n        runner = Runner(agent=agent, environment=environment, num_parallel=2)\n        runner.run(num_episodes=3, use_tqdm=False)\n        runner.close()\n        self.finished_test()\n\n        # episode callback\n        runner = Runner(agent=agent, environments=[environment, environment])\n        callback_episode_frequency = 2\n        self.num_callbacks = 0\n\n        def callback(r, p):\n            self.num_callbacks += 1\n            if self.num_callbacks % 2 == 0:\n                self.assertEqual(min(r.episode_timestep), 0)\n            self.assertEqual(r.episodes, self.num_callbacks * callback_episode_frequency)\n\n        runner.run(\n            num_episodes=5, callback=callback,\n            callback_episode_frequency=callback_episode_frequency, use_tqdm=False,\n            sync_episodes=True\n        )\n        self.finished_test()\n\n        # timestep callback\n        callback_timestep_frequency = 3\n\n        def callback(r, p):\n            self.assertEqual(r.episode_timestep[p] % callback_timestep_frequency, 0)\n\n        runner.run(\n            num_episodes=2, callback=callback,\n            callback_timestep_frequency=callback_timestep_frequency, use_tqdm=False\n        )\n        runner.close()\n        self.finished_test()\n\n        # evaluation synced\n        runner = Runner(agent=agent, environment=environment, num_parallel=2, evaluation=True)\n        self.num_evaluations = 0\n\n        def evaluation_callback(r):\n            self.num_evaluations += 1\n\n        runner.run(\n            num_episodes=1, use_tqdm=False, evaluation_callback=evaluation_callback,\n            sync_episodes=True\n        )\n        self.finished_test(assertion=(self.num_evaluations == 1))\n\n        # evaluation non-synced\n        runner.run(num_episodes=1, use_tqdm=False, evaluation_callback=evaluation_callback)\n        runner.close()\n        self.finished_test(assertion=(self.num_evaluations >= 2))\n\n    def test_batched(self):\n        self.start_tests(name=\'batched\')\n\n        agent = self.agent_spec()\n        environment = self.environment_spec()\n\n        # default\n        runner = Runner(agent=agent, environment=environment, num_parallel=2)\n        runner.run(num_episodes=3, use_tqdm=False, batch_agent_calls=True)\n        runner.close()\n        self.finished_test()\n\n        # episode callback\n        runner = Runner(agent=agent, environments=[environment, environment])\n        callback_episode_frequency = 2\n        self.num_callbacks = 0\n\n        def callback(r, p):\n            self.num_callbacks += 1\n            if self.num_callbacks % 2 == 0:\n                self.assertEqual(min(r.episode_timestep), 0)\n            self.assertEqual(r.episodes, self.num_callbacks * callback_episode_frequency)\n\n        runner.run(\n            num_episodes=5, callback=callback,\n            callback_episode_frequency=callback_episode_frequency, use_tqdm=False,\n            batch_agent_calls=True, sync_episodes=True\n        )\n        self.finished_test()\n\n        # timestep callback\n        callback_timestep_frequency = 3\n\n        def callback(r, p):\n            self.assertEqual(r.episode_timestep[p] % callback_timestep_frequency, 0)\n\n        runner.run(\n            num_episodes=2, callback=callback,\n            callback_timestep_frequency=callback_timestep_frequency, use_tqdm=False,\n            batch_agent_calls=True\n        )\n        runner.close()\n        self.finished_test()\n\n        # evaluation synced\n        runner = Runner(agent=agent, environment=environment, num_parallel=2, evaluation=True)\n        self.num_evaluations = 0\n\n        def evaluation_callback(r):\n            self.num_evaluations += 1\n\n        runner.run(\n            num_episodes=1, use_tqdm=False, evaluation_callback=evaluation_callback,\n            batch_agent_calls=True, sync_episodes=True\n        )\n        self.finished_test(assertion=(self.num_evaluations == 1))\n\n        # evaluation non-synced\n        runner.run(\n            num_episodes=1, use_tqdm=False, evaluation_callback=evaluation_callback,\n            batch_agent_calls=True\n        )\n        runner.close()\n        self.finished_test(assertion=(self.num_evaluations >= 2))\n'"
test/test_saving.py,0,"b""# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport time\nimport unittest\n\nfrom tensorforce import Agent, Environment\nfrom test.unittest_base import UnittestBase\n\n\nclass TestSaving(UnittestBase, unittest.TestCase):\n\n    min_timesteps = 3\n    require_observe = True\n\n    directory = 'test/test-saving'\n\n    def test_config(self):\n        # FEATURES.MD\n        self.start_tests(name='config')\n\n        # Remove directory if exists\n        if os.path.exists(path=self.__class__.directory):\n            for filename in os.listdir(path=self.__class__.directory):\n                os.remove(path=os.path.join(self.__class__.directory, filename))\n            os.rmdir(path=self.__class__.directory)\n\n        # default\n        saver = dict(directory=self.__class__.directory)\n        agent, environment = self.prepare(saver=saver)\n\n        states = environment.reset()\n        agent.close()\n\n        agent = Agent.load(directory=self.__class__.directory, environment=environment)\n\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n        environment.close()\n\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.json'))\n        os.remove(path=os.path.join(self.__class__.directory, 'checkpoint'))\n        os.remove(path=os.path.join(self.__class__.directory, 'graph.pbtxt'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.meta'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.meta'))\n        for filename in os.listdir(path=self.__class__.directory):\n            os.remove(path=os.path.join(self.__class__.directory, filename))\n            assert filename.startswith('events.out.tfevents.')\n            break\n        os.rmdir(path=self.__class__.directory)\n\n        self.finished_test()\n\n        # no load\n        saver = dict(directory=self.__class__.directory)\n        agent, environment = self.prepare(saver=saver)\n\n        states = environment.reset()\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n        environment.close()\n\n        saver = dict(directory=self.__class__.directory, load=False)\n        agent, environment = self.prepare(saver=saver)\n\n        states = environment.reset()\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n        environment.close()\n\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.json'))\n        os.remove(path=os.path.join(self.__class__.directory, 'checkpoint'))\n        os.remove(path=os.path.join(self.__class__.directory, 'graph.pbtxt'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.meta'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.meta'))\n        for filename in os.listdir(path=self.__class__.directory):\n            os.remove(path=os.path.join(self.__class__.directory, filename))\n            assert filename.startswith('events.out.tfevents.')\n            break\n        os.rmdir(path=self.__class__.directory)\n\n        self.finished_test()\n\n    # @pytest.mark.skip(reason='currently takes too long')\n    def test_config_extended(self):\n        self.start_tests(name='config extended')\n\n        # Remove directory if exists\n        if os.path.exists(path=self.__class__.directory):\n            for filename in os.listdir(path=self.__class__.directory):\n                os.remove(path=os.path.join(self.__class__.directory, filename))\n            os.rmdir(path=self.__class__.directory)\n\n        # filename\n        saver = dict(directory=self.__class__.directory, filename='test')\n        agent, environment = self.prepare(saver=saver)\n\n        states = environment.reset()\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n\n        agent = Agent.load(\n            directory=self.__class__.directory, filename='test', environment=environment\n        )\n\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n        environment.close()\n\n        os.remove(path=os.path.join(self.__class__.directory, 'test.json'))\n        os.remove(path=os.path.join(self.__class__.directory, 'checkpoint'))\n        os.remove(path=os.path.join(self.__class__.directory, 'graph.pbtxt'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-0.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-0.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-0.meta'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-1.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-1.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-1.meta'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-2.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-2.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'test-2.meta'))\n        for filename in os.listdir(path=self.__class__.directory):\n            os.remove(path=os.path.join(self.__class__.directory, filename))\n            assert filename.startswith('events.out.tfevents.')\n            break\n        os.rmdir(path=self.__class__.directory)\n\n        self.finished_test()\n\n        # frequency\n        saver = dict(directory=self.__class__.directory, frequency=1)\n        agent, environment = self.prepare(saver=saver)\n\n        states = environment.reset()\n        time.sleep(1)\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        time.sleep(1)\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n        environment.close()\n\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.json'))\n        os.remove(path=os.path.join(self.__class__.directory, 'checkpoint'))\n        os.remove(path=os.path.join(self.__class__.directory, 'graph.pbtxt'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.meta'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.meta'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-2.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-2.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-2.meta'))\n        for filename in os.listdir(path=self.__class__.directory):\n            os.remove(path=os.path.join(self.__class__.directory, filename))\n            assert filename.startswith('events.out.tfevents.'), filename\n            break\n        os.rmdir(path=self.__class__.directory)\n\n        self.finished_test()\n\n        # load filename\n        saver = dict(directory=self.__class__.directory)\n        agent, environment = self.prepare(saver=saver)\n\n        states = environment.reset()\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n        environment.close()\n\n        saver = dict(directory=self.__class__.directory, load='agent-0')\n        agent, environment = self.prepare(saver=saver)\n\n        states = environment.reset()\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n        environment.close()\n\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.json'))\n        os.remove(path=os.path.join(self.__class__.directory, 'checkpoint'))\n        os.remove(path=os.path.join(self.__class__.directory, 'graph.pbtxt'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-0.meta'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.meta'))\n        for filename in os.listdir(path=self.__class__.directory):\n            os.remove(path=os.path.join(self.__class__.directory, filename))\n            assert filename.startswith('events.out.tfevents.')\n            break\n        os.rmdir(path=self.__class__.directory)\n\n        self.finished_test()\n\n    def test_explicit(self):\n        # FEATURES.MD\n        self.start_tests(name='explicit')\n\n        # Remove directory if exists\n        if os.path.exists(path=self.__class__.directory):\n            for filename in os.listdir(path=self.__class__.directory):\n                os.remove(path=os.path.join(self.__class__.directory, filename))\n            os.rmdir(path=self.__class__.directory)\n\n        # TODO: currently Protobuf saving is not compatible with internal state RNNs\n        # episodes update to guarantee inequality between weights2 and weights3\n        agent, environment = self.prepare(\n            policy=dict(network=dict(type='auto', size=8, depth=1, internal_rnn=False)), memory=50,\n            update=dict(unit='episodes', batch_size=1)\n        )\n        states = environment.reset()\n\n        # save: default tensorflow format\n        weights0 = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        agent.save(directory=self.__class__.directory)\n        agent.close()\n        self.finished_test()\n\n        # load: only directory\n        agent = Agent.load(directory=self.__class__.directory, environment=environment)\n        x = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        self.assertTrue((x == weights0).all())\n        self.assertEqual(agent.timesteps, 0)\n        self.finished_test()\n\n        # one timestep\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        # save: numpy format, append timesteps\n        weights1 = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        agent.save(directory=self.__class__.directory, format='numpy', append='timesteps')\n        agent.close()\n        self.finished_test()\n\n        # load: numpy format and directory\n        agent = Agent.load(\n            directory=self.__class__.directory, format='numpy', environment=environment\n        )\n        x = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        self.assertTrue((x == weights1).all())\n        self.assertEqual(agent.timesteps, 1)\n        self.finished_test()\n\n        # one timestep\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\n        # save: numpy format, append timesteps\n        weights2 = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        agent.save(directory=self.__class__.directory, format='numpy', append='timesteps')\n        agent.close()\n        self.finished_test()\n\n        # load: numpy format and directory\n        agent = Agent.load(\n            directory=self.__class__.directory, format='numpy', environment=environment\n        )\n        x = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        self.assertTrue((x == weights2).all())\n        self.assertEqual(agent.timesteps, 2)\n        self.finished_test()\n\n        # one episode\n        while not terminal:\n            actions = agent.act(states=states)\n            states, terminal, reward = environment.execute(actions=actions)\n            agent.observe(terminal=terminal, reward=reward)\n\n        # save: hdf5 format, filename, append episodes\n        weights3 = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        self.assertFalse((weights3 == weights2).all())\n        self.assertEqual(agent.episodes, 1)\n        agent.save(\n            directory=self.__class__.directory, filename='agent2', format='hdf5', append='episodes'\n        )\n        agent.close()\n        self.finished_test()\n\n        # env close\n        environment.close()\n\n        # differing agent config: episode length, update, parallel_interactions\n        environment = Environment.create(environment=self.environment_spec(max_episode_timesteps=7))\n\n        # load: filename (hdf5 format implicit)\n        agent = Agent.load(\n            directory=self.__class__.directory, filename='agent2', environment=environment,\n            update=dict(unit='episodes', batch_size=2), parallel_interactions=2\n        )\n        x = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        self.assertTrue((x == weights3).all())\n        self.assertEqual(agent.episodes, 1)\n        agent.close()\n        self.finished_test()\n\n        # load: tensorflow format (filename explicit)\n        agent = Agent.load(\n            directory=self.__class__.directory, format='tensorflow', environment=environment,\n            update=dict(unit='episodes', batch_size=2), parallel_interactions=2\n        )\n        x = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        self.assertTrue((x == weights0).all())\n        self.assertEqual(agent.timesteps, 0)\n        self.assertEqual(agent.episodes, 0)\n        agent.close()\n        self.finished_test()\n\n        # load: numpy format, full filename including timesteps suffix\n        agent = Agent.load(\n            directory=self.__class__.directory, filename='agent-1', format='numpy',\n            environment=environment, update=dict(unit='episodes', batch_size=2),\n            parallel_interactions=2\n        )\n        x = agent.get_variable(variable='policy/policy-network/dense0/weights')\n        self.assertTrue((x == weights1).all())\n        self.assertEqual(agent.timesteps, 1)\n        self.assertEqual(agent.episodes, 0)\n        agent.close()\n        self.finished_test()\n\n        # load: pb-actonly format\n        agent = Agent.load(directory=self.__class__.directory, format='pb-actonly')\n        x = agent.session.run(fetches='agent/policy/policy-network/dense0/weights:0')\n        self.assertTrue((x == weights0).all())\n\n        # one episode\n        states = environment.reset()\n        internals = agent.initial_internals()\n        terminal = False\n        while not terminal:\n            actions, internals = agent.act(states=states, internals=internals)\n            states, terminal, _ = environment.execute(actions=actions)\n\n        agent.close()\n        environment.close()\n\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.json'))\n        os.remove(path=os.path.join(self.__class__.directory, 'checkpoint'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.data-00000-of-00001'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.index'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.meta'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent.pb'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-1.npz'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent-2.npz'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent2.json'))\n        os.remove(path=os.path.join(self.__class__.directory, 'agent2-1.hdf5'))\n        os.rmdir(path=self.__class__.directory)\n\n        self.finished_test()\n"""
test/test_seed.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nimport numpy as np\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestSeed(UnittestBase, unittest.TestCase):\n\n    require_observe = True\n\n    def test_seed(self):\n        self.start_tests()\n\n        states = dict(\n            int_state=dict(type=\'int\', shape=(2,), num_values=4),\n            float_state=dict(type=\'float\', shape=(2,)),\n        )\n        actions = dict(\n            int_action=dict(type=\'int\', shape=(2,), num_values=4),\n            float_action=dict(type=\'float\', shape=(2,)),\n        )\n\n        agent, environment = self.prepare(states=states, actions=actions, exploration=0.5, seed=0)\n\n        print_environment = False\n        print_agent = False\n\n        states = environment.reset()\n        if print_environment:\n            print(states[\'int_state\'])\n            print(states[\'float_state\'])\n        else:\n            self.assertTrue(expr=np.allclose(a=states[\'int_state\'], b=np.asarray([2, 3])))\n            self.assertTrue(expr=np.allclose(\n                a=states[\'float_state\'], b=np.asarray([-0.11054066, 1.02017271])\n            ))\n\n        actions = agent.act(states=states)\n        if print_agent:\n            print(actions[\'int_action\'])\n            print(actions[\'float_action\'])\n        else:\n            self.assertTrue(expr=np.allclose(a=actions[\'int_action\'], b=np.asarray([0, 0])))\n            self.assertTrue(expr=np.allclose(\n                a=actions[\'float_action\'], b=np.asarray([0.79587996, -0.7411721])\n            ))\n\n        states, terminal, reward = environment.execute(actions=actions)\n        updated = agent.observe(terminal=terminal, reward=reward)\n        if print_environment:\n            print(states[\'int_state\'])\n            print(states[\'float_state\'])\n            print(terminal, reward, updated)\n        else:\n            self.assertTrue(expr=np.allclose(a=states[\'int_state\'], b=np.asarray([2, 2])))\n            self.assertTrue(expr=np.allclose(\n                a=states[\'float_state\'], b=np.asarray([1.2565714, 0.2967472])\n            ))\n            self.assertFalse(expr=terminal)\n            self.assertEqual(first=reward, second=0.6888437030500962)\n            self.assertFalse(expr=updated)\n\n        actions = agent.act(states=states)\n        if print_agent:\n            print(actions[\'int_action\'])\n            print(actions[\'float_action\'])\n        else:\n            self.assertTrue(expr=np.allclose(a=actions[\'int_action\'], b=np.asarray([1, 0])))\n            self.assertTrue(expr=np.allclose(\n                a=actions[\'float_action\'],b=np.asarray([-0.58322495, -0.08754656])\n            ))\n\n        states, terminal, reward = environment.execute(actions=actions)\n        updated = agent.observe(terminal=terminal, reward=reward)\n        if print_environment:\n            print(states[\'int_state\'])\n            print(states[\'float_state\'])\n            print(terminal, reward, updated)\n        else:\n            self.assertTrue(expr=np.allclose(a=states[\'int_state\'], b=np.asarray([0, 2])))\n            self.assertTrue(expr=np.allclose(\n                a=states[\'float_state\'], b=np.asarray([-0.13370156, 1.07774381])\n            ))\n            self.assertFalse(expr=terminal)\n            self.assertEqual(first=reward, second=-0.15885683833831)\n            self.assertFalse(expr=updated)\n\n        actions = agent.act(states=states)\n        if print_agent:\n            print(actions[\'int_action\'])\n            print(actions[\'float_action\'])\n        else:\n            self.assertTrue(expr=np.allclose(a=actions[\'int_action\'], b=np.asarray([3, 1])))\n            self.assertTrue(expr=np.allclose(\n                a=actions[\'float_action\'], b=np.asarray([0.33305427, -0.21438375])\n            ))\n\n        states, terminal, reward = environment.execute(actions=actions)\n        updated = agent.observe(terminal=terminal, reward=reward)\n        if print_environment:\n            print(states[\'int_state\'])\n            print(states[\'float_state\'])\n            print(terminal, reward, updated)\n        else:\n            self.assertTrue(expr=np.allclose(a=states[\'int_state\'], b=np.asarray([1, 3])))\n            self.assertTrue(expr=np.allclose(\n                a=states[\'float_state\'], b=np.asarray([0.42808095, -1.03978785])\n            ))\n            self.assertFalse(expr=terminal)\n            self.assertEqual(first=reward, second=0.02254944273721704)\n            self.assertFalse(expr=updated)\n'"
test/test_specifications.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom tensorforce.core.memories import Replay\nfrom tensorforce.core.networks import LayerbasedNetwork\nfrom test.unittest_base import UnittestBase\n\n\nclass TestNetwork(LayerbasedNetwork):\n\n    def __init__(self, name, inputs_spec):\n        super().__init__(name=name, inputs_spec=inputs_spec)\n\n        self.layer1 = self.add_module(name=\'dense0\', module=dict(type=\'dense\', size=8))\n        self.layer2 = self.add_module(name=\'dense1\', module=dict(type=\'dense\', size=8))\n\n    def tf_apply(self, x, internals, return_internals=False):\n        x = self.layer2.apply(x=self.layer1.apply(x=next(iter(x.values()))))\n        if return_internals:\n            return x, dict()\n        else:\n            return x\n\n\nclass TestSpecifications(UnittestBase, unittest.TestCase):\n\n    def specification_unittest(self, network, memory):\n        states = dict(type=\'float\', shape=(3,))\n\n        agent, environment = self.prepare(\n            states=states, policy=dict(network=network), memory=memory\n        )\n\n        states = environment.reset()\n        actions = agent.act(states=states, independent=True)\n        states, terminal, reward = environment.execute(actions=actions)\n\n        agent.close()\n        environment.close()\n\n        self.finished_test()\n\n    def test_specifications(self):\n        # SPECIFICATION.MD\n        self.start_tests()\n\n        # default\n        self.specification_unittest(\n            network=dict(type=\'layered\', layers=[dict(type=\'dense\', size=8)]),\n            memory=dict(type=\'replay\', capacity=100)\n        )\n\n        # json\n        self.specification_unittest(\n            network=\'test/data/network.json\',\n            memory=dict(type=\'test/data/memory.json\', capacity=100)\n        )\n\n        # module\n        self.specification_unittest(\n            network=\'test.test_specifications.TestNetwork\',\n            memory=dict(type=\'tensorforce.core.memories.Replay\', capacity=100)\n        )\n\n        # callable\n        self.specification_unittest(\n            network=TestNetwork, memory=dict(type=Replay, capacity=100)\n        )\n\n        # default (+firstarg)\n        self.specification_unittest(\n            network=[dict(type=\'dense\', size=8)], memory=dict(capacity=100)\n        )\n'"
test/test_summaries.py,0,"b""# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport unittest\n\nimport numpy as np\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestSummaries(UnittestBase, unittest.TestCase):\n\n    require_observe = True\n\n    directory = 'test/test-summaries'\n\n    def test_summaries(self):\n        # FEATURES.MD\n        self.start_tests()\n\n        # Remove directory if exists\n        if os.path.exists(path=self.__class__.directory):\n            for directory in os.listdir(path=self.__class__.directory):\n                directory = os.path.join(self.__class__.directory, directory)\n                for filename in os.listdir(path=directory):\n                    os.remove(path=os.path.join(directory, filename))\n                os.rmdir(path=directory)\n            os.rmdir(path=self.__class__.directory)\n\n        # Remove directory if exists\n        if os.path.exists(path=self.__class__.directory):\n            for filename in os.listdir(path=self.__class__.directory):\n                os.remove(path=os.path.join(self.__class__.directory, filename))\n            os.rmdir(path=self.__class__.directory)\n\n        # TODO: 'dropout'\n        reward_estimation = dict(horizon=2, estimate_horizon='late')\n        baseline_policy = dict(network=dict(type='auto', size=8, depth=1, internal_rnn=1))\n        baseline_objective = 'policy_gradient'\n        baseline_optimizer = 'adam'\n\n        agent, environment = self.prepare(\n            summarizer=dict(\n                directory=self.__class__.directory, labels='all', frequency=2, custom=dict(\n                    audio=dict(type='audio', sample_rate=44100, max_outputs=1),\n                    histogram=dict(type='histogram'),\n                    image=dict(type='image', max_outputs=1),\n                    scalar=dict(type='scalar')\n                )\n            ), reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer\n        )\n\n        updated = False\n        while not updated:\n            states = environment.reset()\n            terminal = False\n            while not terminal:\n                actions = agent.act(states=states)\n                states, terminal, reward = environment.execute(actions=actions)\n                updated = agent.observe(terminal=terminal, reward=reward) or updated\n\n        agent.summarize(summary='image', value=np.zeros(shape=(2, 4, 2, 3)))\n        agent.summarize(summary='scalar', value=1.0, step=0)\n        agent.summarize(summary='scalar', value=2.0, step=1)\n        agent.close()\n        environment.close()\n        self.finished_test()\n\n        for directory in os.listdir(path=self.__class__.directory):\n            directory = os.path.join(self.__class__.directory, directory)\n            for filename in os.listdir(path=directory):\n                os.remove(path=os.path.join(directory, filename))\n                assert filename.startswith('events.out.tfevents.')\n                break\n            os.rmdir(path=directory)\n        os.rmdir(path=self.__class__.directory)\n\n        self.finished_test()\n"""
test/test_tensorforce_agent.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport unittest\n\nfrom tensorforce import Agent\nfrom test.unittest_agent import UnittestAgent\n\n\nclass TestTensorforceAgent(UnittestAgent, unittest.TestCase):\n\n    directory = \'test-recording\'\n\n    def test_act_experience_update(self):\n        self.start_tests(name=\'act-experience-update\')\n\n        agent, environment = self.prepare(\n            require_all=True, update=dict(unit=\'episodes\', batch_size=1)\n        )\n\n        for n in range(2):\n            states = environment.reset()\n            internals = agent.initial_internals()\n            terminal = False\n            while not terminal:\n                actions, internals = agent.act(states=states, internals=internals, independent=True)\n                next_states, terminal, reward = environment.execute(actions=actions)\n                agent.experience(\n                    states=states, internals=internals, actions=actions, terminal=terminal,\n                    reward=reward\n                )\n                states = next_states\n            agent.update()\n\n        self.finished_test()\n\n    def test_pretrain(self):\n        # FEATURES.MD\n        self.start_tests(name=\'pretrain\')\n\n        agent, environment = self.prepare(\n            require_all=True, recorder=dict(directory=self.__class__.directory)\n        )\n\n        for _ in range(3):\n            states = environment.reset()\n            terminal = False\n            while not terminal:\n                actions = agent.act(states=states)\n                states, terminal, reward = environment.execute(actions=actions)\n                agent.observe(terminal=terminal, reward=reward)\n\n        agent.close()\n\n        # recorder currently does not include internal states\n        agent = Agent.create(agent=self.agent_spec(\n            require_all=True,\n            policy=dict(network=dict(type=\'auto\', size=8, depth=1, internal_rnn=False))\n        ), environment=environment)\n\n        agent.pretrain(\n            directory=self.__class__.directory, num_iterations=2, num_traces=2, num_updates=3\n        )\n\n        agent.close()\n        environment.close()\n\n        for filename in os.listdir(path=self.__class__.directory):\n            os.remove(path=os.path.join(self.__class__.directory, filename))\n            assert filename.startswith(\'trace-\')\n        os.rmdir(path=self.__class__.directory)\n\n        self.finished_test()\n'"
test/unittest_agent.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom test.unittest_base import UnittestBase\n\n\nclass UnittestAgent(UnittestBase):\n    """"""\n    Collection of unit-tests for agent functionality.\n    """"""\n\n    require_observe = True\n\n    replacement_action = \'bool\'\n    has_experience = True\n    has_update = True\n\n    def test_single_state_action(self):\n        self.start_tests(name=\'single-state-action\')\n\n        states = dict(type=\'float\', shape=(1,))\n        if self.__class__.exclude_float_action:\n            actions = dict(type=self.__class__.replacement_action, shape=())\n        else:\n            actions = dict(type=\'float\', shape=())\n        self.unittest(states=states, actions=actions)\n\n    def test_full(self):\n        self.start_tests(name=\'full\')\n        self.unittest()\n\n    def test_query(self):\n        self.start_tests(name=\'query\')\n\n        states = dict(type=\'float\', shape=(1,))\n        actions = dict(type=self.__class__.replacement_action, shape=())\n\n        if self.__class__.has_update:\n            agent, environment = self.prepare(\n                # min_timesteps=2,  # too few steps for update otherwise\n                # states=states, actions=actions,\n                require_all=True,\n                # policy=dict(network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)),\n                # update=1\n                # TODO: shouldn\'t be necessary!\n            )\n\n        else:\n            agent, environment = self.prepare(states=states, actions=actions)\n\n        states = environment.reset()\n        query = agent.get_query_tensors(function=\'act\')\n        actions, queried = agent.act(states=states, query=query)\n        self.assertEqual(first=len(queried), second=len(query))\n\n        states, terminal, reward = environment.execute(actions=actions)\n\n        query = agent.get_query_tensors(function=\'observe\')\n        _, queried = agent.observe(terminal=terminal, reward=reward, query=query)\n        self.assertEqual(first=len(queried), second=len(query))\n\n        while not terminal:\n            actions = agent.act(states=states)\n            states, terminal, reward = environment.execute(actions=actions)\n            agent.observe(terminal=terminal, reward=reward, query=query)\n\n        states_batch = list()\n        internals_batch = list()\n        actions_batch = list()\n        terminal_batch = list()\n        reward_batch = list()\n\n        for _ in range(2):\n            states = environment.reset()\n            internals = agent.initial_internals()\n            terminal = False\n            while not terminal:\n                states_batch.append(states)\n                internals_batch.append(internals)\n                actions, internals = agent.act(states=states, internals=internals, independent=True)\n                actions_batch.append(actions)\n                states, terminal, reward = environment.execute(actions=actions)\n                terminal_batch.append(terminal)\n                reward_batch.append(reward)\n        if isinstance(states_batch[0], dict):\n            states_batch = OrderedDict(\n                (name, [states[name] for states in states_batch]) for name in states_batch[0]\n            )\n        if isinstance(internals_batch[0], dict):\n            internals_batch = OrderedDict(\n                (name, [internals[name] for internals in internals_batch])\n                for name in internals_batch[0]\n            )\n        if isinstance(actions_batch[0], dict):\n            actions_batch = OrderedDict(\n                (name, [actions[name] for actions in actions_batch]) for name in actions_batch[0]\n            )\n\n        if self.__class__.has_experience:\n            query = agent.get_query_tensors(function=\'experience\')\n            queried = agent.experience(\n                states=states_batch, internals=internals_batch, actions=actions_batch,\n                terminal=terminal_batch, reward=reward_batch, query=query\n            )\n            self.assertEqual(first=len(queried), second=len(query))\n\n        if self.__class__.has_update:\n            query = agent.get_query_tensors(function=\'update\')\n            queried = agent.update(query=query)\n            self.assertEqual(first=len(queried), second=len(query))\n\n        agent.close()\n        environment.close()\n\n        self.finished_test()\n'"
test/unittest_base.py,1,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom copy import deepcopy\nfrom datetime import datetime\nimport os\nimport sys\nimport warnings\n\nimport tensorflow as tf\n\nfrom tensorforce import Agent, Environment, Runner, TensorforceError\nfrom tensorforce.core.layers import Layer\nfrom test.unittest_environment import UnittestEnvironment\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass UnittestBase(object):\n    """"""\n    Unit-test base class.\n    """"""\n\n    # Unittest\n    num_updates = None\n    num_episodes = None\n    num_timesteps = None\n\n    # Environment\n    min_timesteps = 2\n    states = dict(\n        bool_state=dict(type=\'bool\', shape=(1,)),\n        int_state=dict(type=\'int\', shape=(2,), num_values=4),\n        float_state=dict(type=\'float\', shape=(1, 1, 2)),\n        bounded_state=dict(type=\'float\', shape=(), min_value=-0.5, max_value=0.5)\n    )\n    actions = dict(\n        bool_action=dict(type=\'bool\', shape=(1,)),\n        int_action=dict(type=\'int\', shape=(2,), num_values=4),\n        float_action=dict(type=\'float\', shape=(1, 1)),\n        bounded_action=dict(type=\'float\', shape=(2,), min_value=-0.5, max_value=0.5)\n    )\n    max_episode_timesteps = 5\n\n    # Exclude action types\n    exclude_bool_action = False\n    exclude_int_action = False\n    exclude_float_action = False\n    exclude_bounded_action = False\n\n    # Agent\n    agent = dict(\n        policy=dict(network=dict(type=\'auto\', size=8, depth=1, internal_rnn=2)), update=4,\n        objective=\'policy_gradient\', reward_estimation=dict(horizon=3)\n    )\n\n    # Tensorforce config\n    require_observe = False\n    require_all = False\n\n    def setUp(self):\n        warnings.filterwarnings(\n            action=\'ignore\',\n            message=\'Converting sparse IndexedSlices to a dense Tensor of unknown shape\'\n        )\n\n    def start_tests(self, name=None):\n        """"""\n        Start unit-test method.\n        """"""\n        if name is None:\n            sys.stdout.write(\'\\n{} {}: \'.format(\n                datetime.now().strftime(\'%H:%M:%S\'), self.__class__.__name__[4:]\n            ))\n        else:\n            sys.stdout.write(\'\\n{} {} ({}): \'.format(\n                datetime.now().strftime(\'%H:%M:%S\'), self.__class__.__name__[4:], name\n            ))\n        sys.stdout.flush()\n        tf.compat.v1.reset_default_graph()\n\n    def finished_test(self, assertion=None):\n        """"""\n        Finished unit-test.\n        """"""\n        if assertion is None:\n            assertion = True\n        else:\n            self.assertTrue(expr=assertion)\n        if assertion:\n            sys.stdout.write(\'.\')\n            sys.stdout.flush()\n\n    def environment_spec(\n        self, max_episode_timesteps=None, min_timesteps=None, states=None, actions=None,\n        exclude_bool_action=False, exclude_int_action=False, exclude_float_action=False,\n        exclude_bounded_action=False\n    ):\n        if states is None:\n            states = deepcopy(self.__class__.states)\n\n        if actions is None:\n            actions = deepcopy(self.__class__.actions)\n            if exclude_bool_action or self.__class__.exclude_bool_action:\n                actions.pop(\'bool_action\')\n            if exclude_int_action or self.__class__.exclude_int_action:\n                actions.pop(\'int_action\')\n            if exclude_float_action or self.__class__.exclude_float_action:\n                actions.pop(\'float_action\')\n            if exclude_bounded_action or self.__class__.exclude_bounded_action:\n                actions.pop(\'bounded_action\')\n\n        if min_timesteps is None:\n            min_timesteps = self.__class__.min_timesteps\n\n        if max_episode_timesteps is None:\n            max_episode_timesteps = min_timesteps + self.__class__.max_episode_timesteps\n\n        return dict(\n            environment=UnittestEnvironment, max_episode_timesteps=max_episode_timesteps,\n            states=states, actions=actions, min_timesteps=min_timesteps\n        )\n\n    def agent_spec(self, require_observe=False, require_all=False, **agent):\n        for key, value in self.__class__.agent.items():\n            if key not in agent:\n                agent[key] = value\n\n        if self.__class__.require_all or require_all:\n            config = None\n        elif self.__class__.require_observe or require_observe:\n            config = dict(api_functions=[\'reset\', \'act\', \'independent_act\', \'observe\'])\n        else:\n            config = dict(api_functions=[\'reset\', \'act\', \'independent_act\'])\n\n        return dict(agent=agent, config=config)\n\n    def prepare(\n        self,\n        # general environment\n        environment=None, max_episode_timesteps=None,\n        # unit-test environment\n        min_timesteps=None, states=None, actions=None,\n        # exclude action types\n        exclude_bool_action=False, exclude_int_action=False, exclude_float_action=False,\n        exclude_bounded_action=False,\n        # agent\n        require_observe=False, require_all=False, **agent\n    ):\n        """"""\n        Generic unit-test preparation.\n        """"""\n        Layer.layers = None\n\n        if environment is None:\n            environment = self.environment_spec(\n                max_episode_timesteps=max_episode_timesteps, min_timesteps=min_timesteps,\n                states=states, actions=actions, exclude_bool_action=exclude_bool_action,\n                exclude_int_action=exclude_int_action, exclude_float_action=exclude_float_action,\n                exclude_bounded_action=exclude_bounded_action\n            )\n            environment = Environment.create(environment=environment)\n\n        elif min_timesteps is None:\n            if max_episode_timesteps is None:\n                max_episode_timesteps = self.__class__.max_episode_timesteps\n\n            environment = Environment.create(\n                environment=environment, max_episode_timesteps=max_episode_timesteps\n            )\n\n        else:\n            raise TensorforceError.unexpected()\n\n        agent = self.agent_spec(require_observe=require_observe, require_all=require_all, **agent)\n\n        agent = Agent.create(agent=agent, environment=environment)\n\n        return agent, environment\n\n    def unittest(\n        self,\n        # runner\n        num_updates=None, num_episodes=None, num_timesteps=None,\n        # general environment\n        environment=None, max_episode_timesteps=None,\n        # unit-test environment\n        min_timesteps=None, states=None, actions=None,\n        # exclude action types\n        exclude_bool_action=False, exclude_int_action=False, exclude_float_action=False,\n        exclude_bounded_action=False,\n        # agent\n        require_observe=False, require_all=False, **agent\n    ):\n        """"""\n        Generic unit-test.\n        """"""\n        if environment is None:\n            environment = self.environment_spec(\n                max_episode_timesteps=max_episode_timesteps, min_timesteps=min_timesteps,\n                states=states, actions=actions, exclude_bool_action=exclude_bool_action,\n                exclude_int_action=exclude_int_action, exclude_float_action=exclude_float_action,\n                exclude_bounded_action=exclude_bounded_action\n            )\n            max_episode_timesteps = environment.pop(\'max_episode_timesteps\')  # runner argument\n\n        elif min_timesteps is not None:\n            raise TensorforceError.unexpected()\n\n        elif max_episode_timesteps is None:\n            max_episode_timesteps = self.__class__.max_episode_timesteps\n\n        agent = self.agent_spec(require_observe=require_observe, require_all=require_all, **agent)\n\n        assert (num_updates is not None) + (num_episodes is not None) + \\\n            (num_timesteps is not None) <= 1\n        if num_updates is None and num_episodes is None and num_timesteps is None:\n            num_updates = self.__class__.num_updates\n            num_episodes = self.__class__.num_episodes\n            num_timesteps = self.__class__.num_timesteps\n        if num_updates is None and num_episodes is None and num_timesteps is None:\n            num_updates = 2\n        assert (num_updates is not None) + (num_episodes is not None) + \\\n            (num_timesteps is not None) == 1\n\n        evaluation = not any([\n            require_all, require_observe, self.__class__.require_all,\n            self.__class__.require_observe\n        ])\n\n        runner = Runner(\n            agent=agent, environment=environment, max_episode_timesteps=max_episode_timesteps,\n            evaluation=evaluation\n        )\n        runner.run(\n            num_episodes=num_episodes, num_timesteps=num_timesteps, num_updates=num_updates,\n            use_tqdm=False\n        )\n        runner.close()\n\n        self.finished_test()\n'"
test/unittest_environment.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nfrom random import random\n\nimport numpy as np\n\nfrom tensorforce import Environment, TensorforceError, util\n\n\nclass UnittestEnvironment(Environment):\n    """"""\n    Unit-test mock environment.\n\n    Args:\n        states: States specification.\n        actions: Actions specification.\n        min_timesteps: Minimum number of timesteps.\n    """"""\n\n    def __init__(self, states, actions, min_timesteps):\n        super().__init__()\n\n        self.states_spec = OrderedDict((name, states[name]) for name in sorted(states))\n        self.actions_spec = OrderedDict((name, actions[name]) for name in sorted(actions))\n        self.min_timesteps = min_timesteps\n\n        self.random_states = self.__class__.random_states_function(\n            states_spec=self.states_spec, actions_spec=self.actions_spec\n        )\n        self.is_valid_actions = self.__class__.is_valid_actions_function(\n            actions_spec=self.actions_spec\n        )\n\n    def states(self):\n        return self.states_spec\n\n    def actions(self):\n        return self.actions_spec\n\n    @classmethod\n    def random_states_function(cls, states_spec, actions_spec=None):\n        if actions_spec is None:\n            if util.is_atomic_values_spec(values_spec=states_spec):\n                return (lambda: cls.random_state_function(state_spec=states_spec)())\n            else:\n                return (lambda: {\n                    name: cls.random_state_function(state_spec=state_spec)()\n                    for name, state_spec in states_spec.items()\n                })\n\n        elif util.is_atomic_values_spec(values_spec=states_spec):\n            if util.is_atomic_values_spec(values_spec=actions_spec):\n\n                def fn():\n                    random_states = cls.random_state_function(state_spec=states_spec)()\n                    if actions_spec[\'type\'] == \'int\':\n                        if not isinstance(random_states, dict):\n                            random_states = dict(state=random_states)\n                        mask = cls.random_mask(action_spec=actions_spec)\n                        random_states[\'action_mask\'] = mask\n                    return random_states\n\n            else:\n\n                def fn():\n                    random_states = cls.random_state_function(state_spec=states_spec)()\n                    for name, action_spec in actions_spec.items():\n                        if action_spec[\'type\'] == \'int\':\n                            if not isinstance(random_states, dict):\n                                random_states = dict(state=random_states)\n                            mask = cls.random_mask(action_spec=action_spec)\n                            random_states[name + \'_mask\'] = mask\n                    return random_states\n\n        else:\n            if util.is_atomic_values_spec(values_spec=actions_spec):\n\n                def fn():\n                    random_states = {\n                        name: cls.random_state_function(state_spec=state_spec)()\n                        for name, state_spec in states_spec.items()\n                    }\n                    if actions_spec[\'type\'] == \'int\':\n                        mask = cls.random_mask(action_spec=actions_spec)\n                        random_states[\'action_mask\'] = mask\n                    return random_states\n\n            else:\n\n                def fn():\n                    random_states = {\n                        name: cls.random_state_function(state_spec=state_spec)()\n                        for name, state_spec in states_spec.items()\n                    }\n                    for name, action_spec in actions_spec.items():\n                        if action_spec[\'type\'] == \'int\':\n                            mask = cls.random_mask(action_spec=action_spec)\n                            random_states[name + \'_mask\'] = mask\n                    return random_states\n\n        return fn\n\n    @classmethod\n    def random_state_function(cls, state_spec):\n        shape = state_spec[\'shape\']\n        dtype = state_spec.get(\'type\', \'float\')\n\n        if dtype == \'bool\':\n            return (lambda: np.random.random_sample(size=shape) >= 0.5)\n\n        elif dtype == \'int\':\n            num_values = state_spec[\'num_values\']\n            return (lambda: np.random.randint(low=0, high=num_values, size=shape))\n\n        elif dtype == \'float\':\n            if \'min_value\' in state_spec:\n                min_value = state_spec[\'min_value\']\n                max_value = state_spec[\'max_value\']\n                return (lambda: (\n                    min_value + (max_value - min_value) * np.random.random_sample(size=shape)\n                ))\n\n            else:\n                return (lambda: np.random.standard_normal(size=shape))\n\n    @classmethod\n    def random_mask(cls, action_spec):\n        shape = action_spec[\'shape\'] + (action_spec[\'num_values\'],)\n        mask = np.random.random_sample(size=shape)\n        min_mask = np.amin(mask, -1, keepdims=True)\n        max_mask = np.amax(mask, -1, keepdims=True)\n        threshold = np.random.random_sample(size=shape)\n        mask = mask < min_mask + threshold * (max_mask - min_mask)\n        assert mask.any(-1).all() and not mask.all(-1).any()\n        return mask\n\n    @classmethod\n    def is_valid_actions_function(cls, actions_spec):\n        if util.is_atomic_values_spec(values_spec=actions_spec):\n            return (lambda actions, states:\n                cls.is_valid_action_function(action_spec=actions_spec)(actions, \'action\', states)\n            )\n\n        else:\n            return (lambda actions, states: all(\n                cls.is_valid_action_function(action_spec=action_spec)(\n                    action=actions[name], name=name, states=states\n                ) for name, action_spec in actions_spec.items()\n            ))\n\n    @classmethod\n    def is_valid_action_function(cls, action_spec):\n        dtype = action_spec[\'type\']\n        shape = action_spec.get(\'shape\', ())\n\n        if dtype == \'bool\':\n            return (lambda action, name, states: (\n                (isinstance(action, util.np_dtype(\'bool\')) and shape == ()) or\n                (\n                    isinstance(action, np.ndarray) and\n                    action.dtype == util.np_dtype(\'bool\') and action.shape == shape\n                )\n            ))\n\n        elif dtype == \'int\':\n            num_values = action_spec[\'num_values\']\n            return (lambda action, name, states: (\n                (\n                    (isinstance(action, util.np_dtype(\'int\')) and shape == ()) or\n                    (\n                        isinstance(action, np.ndarray) and\n                        action.dtype == util.np_dtype(\'int\') and action.shape == shape\n                    )\n                ) and (0 <= action).all() and (action < num_values).all() and\n                np.take_along_axis(\n                    states[name + \'_mask\'], indices=np.expand_dims(action, axis=-1), axis=-1\n                ).all()\n            ))\n\n        elif dtype == \'float\':\n            if \'min_value\' in action_spec:\n                min_value = action_spec[\'min_value\']\n                max_value = action_spec[\'max_value\']\n                return (lambda action, name, states: (\n                    (\n                        (isinstance(action, util.np_dtype(\'float\')) and shape == ()) or\n                        (\n                            isinstance(action, np.ndarray) and\n                            action.dtype == util.np_dtype(\'float\') and action.shape == shape\n                        )\n                    ) and (min_value <= action).all() and (action <= max_value).all()\n                ))\n\n            else:\n                return (lambda action, name, states: (\n                    (isinstance(action, util.np_dtype(\'float\')) and shape == ()) or\n                    (\n                        isinstance(action, np.ndarray) and\n                        action.dtype == util.np_dtype(\'float\') and action.shape == shape\n                    )\n                ))\n\n    def reset(self):\n        self.timestep = 0\n        self._states = self.random_states()\n        return self._states\n\n    def execute(self, actions):\n        if not self.is_valid_actions(actions, self._states):\n            raise TensorforceError.value(name=\'execute\', argument=\'actions\', value=actions)\n\n        self.timestep += 1\n        self._states = self.random_states()\n        terminal = (self.timestep >= self.min_timesteps and random() < 0.25)\n        reward = -1.0 + 2.0 * random()\n\n        return self._states, terminal, reward\n'"
tensorforce/agents/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.agents.agent import Agent\n\nfrom tensorforce.agents.constant import ConstantAgent\nfrom tensorforce.agents.random import RandomAgent\nfrom tensorforce.agents.tensorforce import TensorforceAgent\n\nfrom tensorforce.agents.a2c import AdvantageActorCritic\nfrom tensorforce.agents.ac import ActorCritic\nfrom tensorforce.agents.dpg import DeterministicPolicyGradient\nfrom tensorforce.agents.dqn import DeepQNetwork\nfrom tensorforce.agents.dueling_dqn import DuelingDQN\nfrom tensorforce.agents.ppo import ProximalPolicyOptimization\nfrom tensorforce.agents.trpo import TrustRegionPolicyOptimization\nfrom tensorforce.agents.vpg import VanillaPolicyGradient\n\n\nA2C = A2CAgent = AdvantageActorCritic\nAC = ACAgent = ActorCritic\nDPG = DPGAgent = DeterministicPolicyGradient\nDQN = DQNAgent = DeepQNetwork\nDuelingDQNAgent = DuelingDQN\nPPO = PPOAgent = ProximalPolicyOptimization\nTRPO = TRPOAgent = TrustRegionPolicyOptimization\nVPG = VPGAgent = REINFORCE = VanillaPolicyGradient\n\n\nagents = dict(\n    a2c=AdvantageActorCritic, ac=ActorCritic, constant=ConstantAgent, default=TensorforceAgent,\n    dpg=DeterministicPolicyGradient, dqn=DeepQNetwork, dueling_dqn=DuelingDQN,\n    tensorforce=TensorforceAgent, ppo=ProximalPolicyOptimization, random=RandomAgent,\n    reinforce=VanillaPolicyGradient, trpo=TrustRegionPolicyOptimization, vpg=VanillaPolicyGradient\n)\n\n\n__all__ = [\n    \'A2C\', \'A2CAgent\', \'AC\', \'ACAgent\', \'ActorCritic\', \'AdvantageActorCritic\', \'Agent\', \'agents\',\n    \'ConstantAgent\', \'DeepQNetwork\', \'DeterministicPolicyGradient\', \'DPG\', \'DPGAgent\', \'DQN\',\n    \'DQNAgent\', \'DuelingDQN\', \'DuelingDQNAgent\', \'PPO\', \'PPOAgent\', \'ProximalPolicyOptimization\',\n    \'RandomAgent\', \'REINFORCE\', \'TensorforceAgent\', \'TRPO\', \'TRPOAgent\',\n    \'TrustRegionPolicyOptimization\', \'VanillaPolicyGradient\', \'VPG\', \'VPGAgent\'\n]\n'"
tensorforce/agents/a2c.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.agents import TensorforceAgent\n\n\nclass AdvantageActorCritic(TensorforceAgent):\n    """"""\n    [Advantage Actor-Critic](https://arxiv.org/abs/1602.01783) agent\n    (specification key: `a2c`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        batch_size (parameter, long > 0): Number of timesteps per update batch\n            (<span style=""color:#C00000""><b>required</b></span>).\n\n        network (""auto"" | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"", automatically configured\n            network).\n        use_beta_distribution (bool): Whether to use the Beta distribution for bounded continuous\n            actions by default.\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n\n        memory (int > 0): Batch memory capacity, has to fit at least maximum batch_size + maximum\n            network/estimator horizon + 1 timesteps\n            (<span style=""color:#00C000""><b>default</b></span>: minimum capacity, usually does not\n            need to be changed).\n        update_frequency (""never"" | parameter, long > 0): Frequency of updates\n            (<span style=""color:#00C000""><b>default</b></span>: batch_size).\n        learning_rate (parameter, float > 0.0): Optimizer learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 3e-4).\n\n        horizon (""episode"" | parameter, long >= 0): Horizon of discounted-sum reward estimation\n            before critic estimate\n            (<span style=""color:#00C000""><b>default</b></span>: 0).\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 0.99).\n        state_action_value (bool): Whether to estimate state-action values instead of state values\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        estimate_terminal (bool): Whether to estimate the value of (real) terminal states\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n\n        critic_network (specification): Critic network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"").\n        critic_optimizer (float > 0.0 | specification): Critic optimizer configuration, see\n            [optimizers](../modules/optimizers.html), a float instead specifies a custom weight for\n            the critic loss\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Required\n        self, states, actions, batch_size,\n        # Environment\n        max_episode_timesteps=None,\n        # Network\n        network=\'auto\', use_beta_distribution=True,\n        # Memory\n        memory=None,\n        # Optimization\n        update_frequency=None, learning_rate=3e-4,\n        # Reward estimation\n        horizon=0, discount=0.99, state_action_value=False, estimate_terminal=False,\n        # Critic\n        critic_network=\'auto\', critic_optimizer=1.0,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'a2c\',\n            states=states, actions=actions, batch_size=batch_size,\n            max_episode_timesteps=max_episode_timesteps,\n            network=network, use_beta_distribution=use_beta_distribution,\n            memory=memory,\n            update_frequency=update_frequency, learning_rate=learning_rate,\n            horizon=horizon, discount=discount, state_action_value=state_action_value,\n                estimate_terminal=estimate_terminal,\n            critic_network=critic_network, critic_optimizer=critic_optimizer,\n            preprocessing=preprocessing,\n            exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n            name=name, device=device, parallel_interactions=parallel_interactions, seed=seed,\n                execution=execution, saver=saver, summarizer=summarizer, recorder=recorder,\n                config=config\n        )\n\n        policy = dict(network=network, temperature=1.0, use_beta_distribution=use_beta_distribution)\n        if memory is None:\n            memory = dict(type=\'recent\')\n        else:\n            memory = dict(type=\'recent\', capacity=memory)\n        update = dict(unit=\'timesteps\', batch_size=batch_size)\n        if update_frequency is not None:\n            update[\'frequency\'] = update_frequency\n        optimizer = dict(type=\'adam\', learning_rate=learning_rate)\n        objective = \'policy_gradient\'\n        reward_estimation = dict(\n            horizon=horizon, discount=discount, estimate_horizon=\'early\',\n            estimate_actions=state_action_value, estimate_terminal=estimate_terminal,\n            estimate_advantage=True\n        )\n        baseline_policy = dict(network=critic_network)\n        if state_action_value:\n            baseline_objective = dict(type=\'value\', value=\'action\')\n        else:\n            baseline_objective = dict(type=\'value\', value=\'state\')\n\n        super().__init__(\n            # Agent\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=True, seed=seed,\n            recorder=recorder, config=config,\n            # Model\n            name=name, device=device, execution=execution, saver=saver, summarizer=summarizer,\n            preprocessing=preprocessing, exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=critic_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n'"
tensorforce/agents/ac.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.agents import TensorforceAgent\n\n\nclass ActorCritic(TensorforceAgent):\n    """"""\n    [Actor-Critic](???) agent\n    (specification key: `ac`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        batch_size (parameter, long > 0): Number of timesteps per update batch\n            (<span style=""color:#C00000""><b>required</b></span>).\n\n        network (""auto"" | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"", automatically configured\n            network).\n        use_beta_distribution (bool): Whether to use the Beta distribution for bounded continuous\n            actions by default.\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n\n        memory (int > 0): Batch memory capacity, has to fit at least maximum batch_size + maximum\n            network/estimator horizon + 1 timesteps\n            (<span style=""color:#00C000""><b>default</b></span>: minimum capacity, usually does not\n            need to be changed).\n        update_frequency (""never"" | parameter, long > 0): Frequency of updates\n            (<span style=""color:#00C000""><b>default</b></span>: batch_size).\n        learning_rate (parameter, float > 0.0): Optimizer learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 3e-4).\n\n        horizon (""episode"" | parameter, long >= 0): Horizon of discounted-sum reward estimation\n            before critic estimate\n            (<span style=""color:#00C000""><b>default</b></span>: 0).\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 0.99).\n        state_action_value (bool): Whether to estimate state-action values instead of state values\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        estimate_terminal (bool): Whether to estimate the value of (real) terminal states\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n\n        critic_network (specification): Critic network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"").\n        critic_optimizer (float > 0.0 | specification): Critic optimizer configuration, see\n            [optimizers](../modules/optimizers.html), a float instead specifies a custom weight for\n            the critic loss\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Required\n        self, states, actions, batch_size,\n        # Environment\n        max_episode_timesteps=None,\n        # Network\n        network=\'auto\', use_beta_distribution=True,\n        # Memory\n        memory=None,\n        # Optimization\n        update_frequency=None, learning_rate=3e-4,\n        # Reward estimation\n        horizon=0, discount=0.99, state_action_value=False, estimate_terminal=False,\n        # Critic\n        critic_network=\'auto\', critic_optimizer=1.0,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'ac\',\n            states=states, actions=actions, batch_size=batch_size,\n            max_episode_timesteps=max_episode_timesteps,\n            network=network, use_beta_distribution=use_beta_distribution,\n            memory=memory,\n            update_frequency=update_frequency, learning_rate=learning_rate,\n            horizon=horizon, discount=discount, state_action_value=state_action_value,\n                estimate_terminal=estimate_terminal,\n            critic_network=critic_network, critic_optimizer=critic_optimizer,\n            preprocessing=preprocessing,\n            exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n            name=name, device=device, parallel_interactions=parallel_interactions, seed=seed,\n                execution=execution, saver=saver, summarizer=summarizer, recorder=recorder,\n                config=config\n        )\n\n        policy = dict(network=network, temperature=1.0, use_beta_distribution=use_beta_distribution)\n        if memory is None:\n            memory = dict(type=\'recent\')\n        else:\n            memory = dict(type=\'recent\', capacity=memory)\n        if update_frequency is None:\n            update = dict(unit=\'timesteps\', batch_size=batch_size)\n        else:\n            update = dict(unit=\'timesteps\', batch_size=batch_size, frequency=update_frequency)\n        optimizer = dict(type=\'adam\', learning_rate=learning_rate)\n        objective = \'policy_gradient\'\n        reward_estimation = dict(\n            horizon=horizon, discount=discount, estimate_horizon=\'early\',\n            estimate_actions=state_action_value, estimate_terminal=estimate_terminal\n        )\n        baseline_policy = dict(network=critic_network)\n        if state_action_value:\n            baseline_objective = dict(type=\'value\', value=\'action\')\n        else:\n            baseline_objective = dict(type=\'value\', value=\'state\')\n\n        super().__init__(\n            # Agent\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=True, seed=seed,\n            recorder=recorder, config=config,\n            # Model\n            name=name, device=device, execution=execution, saver=saver, summarizer=summarizer,\n            preprocessing=preprocessing, exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=critic_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n'"
tensorforce/agents/agent.py,7,"b'# Copyright 2020 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport importlib\nimport json\nimport os\nimport random\nimport time\nfrom collections import OrderedDict\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorforce.agents\nfrom tensorforce import util, TensorforceError\n\n\nclass Agent(object):\n    """"""\n    Tensorforce agent interface.\n    """"""\n\n    @staticmethod\n    def create(agent=\'tensorforce\', environment=None, **kwargs):\n        """"""\n        Creates an agent from a specification.\n\n        Args:\n            agent (specification | Agent class/object): JSON file, specification key, configuration\n                dictionary, library module, or `Agent` class/object\n                (<span style=""color:#00C000""><b>default</b></span>: Policy agent).\n            environment (Environment object): Environment which the agent is supposed to be trained\n                on, environment-related arguments like state/action space specifications and\n                maximum episode length will be extract if given\n                (<span style=""color:#00C000""><b>recommended</b></span>).\n            kwargs: Additional arguments.\n        """"""\n        if isinstance(agent, Agent):\n            if environment is not None:\n                assert util.deep_equal(xs=agent.spec[\'states\'], ys=environment.states())\n                assert util.deep_equal(xs=agent.spec[\'actions\'], ys=environment.actions())\n                assert environment.max_episode_timesteps() is None or \\\n                    agent.spec[\'max_episode_timesteps\'] >= environment.max_episode_timesteps()\n\n            for key, value in kwargs.items():\n                if key == \'parallel_interactions\':\n                    assert agent.spec[key] >= value\n                else:\n                    assert agent.spec[key] == value\n\n            if agent.is_initialized:\n                agent.reset()\n            else:\n                agent.initialize()\n\n            return agent\n\n        elif isinstance(agent, type) and issubclass(agent, Agent):\n            if environment is not None:\n                if \'states\' in kwargs:\n                    assert util.deep_equal(xs=kwargs[\'states\'], ys=environment.states())\n                else:\n                    kwargs[\'states\'] = environment.states()\n                if \'actions\' in kwargs:\n                    assert util.deep_equal(xs=kwargs[\'actions\'], ys=environment.actions())\n                else:\n                    kwargs[\'actions\'] = environment.actions()\n                if environment.max_episode_timesteps() is None:\n                    pass\n                elif \'max_episode_timesteps\' in kwargs:\n                    assert kwargs[\'max_episode_timesteps\'] >= environment.max_episode_timesteps()\n                else:\n                    kwargs[\'max_episode_timesteps\'] = environment.max_episode_timesteps()\n\n            agent = agent(**kwargs)\n            assert isinstance(agent, Agent)\n            return Agent.create(agent=agent, environment=environment)\n\n        elif isinstance(agent, dict):\n            # Dictionary specification\n            agent.update(kwargs)\n            kwargs = dict(agent)\n            agent = kwargs.pop(\'agent\', kwargs.pop(\'type\', \'default\'))\n\n            return Agent.create(agent=agent, environment=environment, **kwargs)\n\n        elif isinstance(agent, str):\n            if os.path.isfile(agent):\n                # JSON file specification\n                with open(agent, \'r\') as fp:\n                    agent = json.load(fp=fp)\n                return Agent.create(agent=agent, environment=environment, **kwargs)\n\n            elif \'.\' in agent:\n                # Library specification\n                library_name, module_name = agent.rsplit(\'.\', 1)\n                library = importlib.import_module(name=library_name)\n                agent = getattr(library, module_name)\n                return Agent.create(agent=agent, environment=environment, **kwargs)\n\n            elif agent in tensorforce.agents.agents:\n                # Keyword specification\n                agent = tensorforce.agents.agents[agent]\n                return Agent.create(agent=agent, environment=environment, **kwargs)\n\n            else:\n                raise TensorforceError.value(name=\'Agent.create\', argument=\'agent\', dtype=agent)\n\n        else:\n            raise TensorforceError.type(name=\'Agent.create\', argument=\'agent\', dtype=type(agent))\n\n    @staticmethod\n    def load(directory=None, filename=None, format=None, environment=None, **kwargs):\n        """"""\n        Restores an agent from a specification directory/file.\n\n        Args:\n            directory (str): Checkpoint directory\n                (<span style=""color:#00C000""><b>default</b></span>: current directory ""."").\n            filename (str): Checkpoint filename, with or without append and extension\n                (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n            format (""tensorflow"" | ""numpy"" | ""hdf5"" | ""pb-actonly""): File format, ""pb-actonly"" loads\n                an act-only agent based on a Protobuf model\n                (<span style=""color:#00C000""><b>default</b></span>: format matching directory and\n                filename, required to be unambiguous).\n            environment (Environment object): Environment which the agent is supposed to be trained\n                on, environment-related arguments like state/action space specifications and\n                maximum episode length will be extract if given\n                (<span style=""color:#00C000""><b>recommended</b></span> unless ""pb-actonly"" format).\n            kwargs: Additional arguments, invalid for ""pb-actonly"" format.\n        """"""\n        if directory is None:\n            # default directory: current directory "".""\n            directory = \'.\'\n\n        if filename is None:\n            # default filename: ""agent""\n            filename = \'agent\'\n\n        agent = os.path.join(directory, os.path.splitext(filename)[0] + \'.json\')\n        if not os.path.isfile(agent) and agent[agent.rfind(\'-\') + 1: -5].isdigit():\n            agent = agent[:agent.rindex(\'-\')] + \'.json\'\n        if os.path.isfile(agent):\n            with open(agent, \'r\') as fp:\n                agent = json.load(fp=fp)\n            if \'agent\' in kwargs:\n                if \'agent\' in agent and agent[\'agent\'] != kwargs[\'agent\']:\n                    raise TensorforceError.value(\n                        name=\'Agent.load\', argument=\'agent\', value=kwargs[\'agent\']\n                    )\n                agent[\'agent\'] = kwargs.pop(\'agent\')\n        else:\n            agent = kwargs\n            kwargs = dict()\n\n        # Overwrite values\n        if environment is not None and environment.max_episode_timesteps() is not None:\n            if \'max_episode_timesteps\' in kwargs:\n                assert kwargs[\'max_episode_timesteps\'] >= environment.max_episode_timesteps()\n                agent[\'max_episode_timesteps\'] = kwargs[\'max_episode_timesteps\']\n            else:\n                agent[\'max_episode_timesteps\'] = environment.max_episode_timesteps()\n        if \'parallel_interactions\' in kwargs and kwargs[\'parallel_interactions\'] > 1:\n            agent[\'parallel_interactions\'] = kwargs[\'parallel_interactions\']\n\n        if format == \'pb-actonly\':\n            assert environment is None\n            assert len(kwargs) == 0\n            agent = ActonlyAgent(\n                path=os.path.join(directory, os.path.splitext(filename)[0] + \'.pb\'),\n                states=agent[\'states\'], actions=agent[\'actions\'], internals=agent.get(\'internals\'),\n                initial_internals=agent.get(\'initial_internals\')\n            )\n\n        else:\n            agent.pop(\'internals\', None)\n            agent.pop(\'initial_internals\', None)\n            agent = Agent.create(agent=agent, environment=environment, **kwargs)\n            agent.restore(directory=directory, filename=filename, format=format)\n\n        return agent\n\n    def __init__(\n        # Environment\n        self, states, actions, max_episode_timesteps=None,\n        # TensorFlow etc\n        parallel_interactions=1, buffer_observe=True, seed=None, recorder=None\n    ):\n        assert hasattr(self, \'spec\')\n\n        if seed is not None:\n            assert isinstance(seed, int)\n            random.seed(a=seed)\n            np.random.seed(seed=seed)\n\n        # States/actions specification\n        self.states_spec = util.valid_values_spec(\n            values_spec=states, value_type=\'state\', return_normalized=True\n        )\n        self.actions_spec = util.valid_values_spec(\n            values_spec=actions, value_type=\'action\', return_normalized=True\n        )\n        self.max_episode_timesteps = max_episode_timesteps\n\n        # Check for name overlap\n        for name in self.states_spec:\n            if name in self.actions_spec:\n                raise TensorforceError.collision(\n                    name=\'name\', value=name, group1=\'states\', group2=\'actions\'\n                )\n\n        # Parallel episodes\n        if isinstance(parallel_interactions, int):\n            if parallel_interactions <= 0:\n                raise TensorforceError.value(\n                    name=\'agent\', argument=\'parallel_interactions\', value=parallel_interactions,\n                    hint=\'<= 0\'\n                )\n            self.parallel_interactions = parallel_interactions\n        else:\n            raise TensorforceError.type(\n                name=\'agent\', argument=\'parallel_interactions\', dtype=type(parallel_interactions)\n            )\n\n        # Buffer observe\n        if isinstance(buffer_observe, bool):\n            if self.parallel_interactions > 1:\n                if not buffer_observe:\n                    raise TensorforceError.required(\n                        name=\'agent\', argument=\'buffer_observe\',\n                        condition=\'parallel_interactions > 1\'\n                    )\n                elif self.max_episode_timesteps is None:\n                    raise TensorforceError.required(\n                        name=\'agent\', argument=\'max_episode_timesteps\',\n                        condition=\'parallel_interactions > 1\'\n                    )\n            if not buffer_observe:\n                self.buffer_observe = 1\n            elif self.max_episode_timesteps is None:\n                self.buffer_observe = 100\n            else:\n                self.buffer_observe = self.max_episode_timesteps\n        elif isinstance(buffer_observe, int):\n            if buffer_observe <= 0:\n                raise TensorforceError.value(\n                    name=\'agent\', argument=\'buffer_observe\', value=buffer_observe, hint=\'<= 0\'\n                )\n            if self.parallel_interactions > 1:\n                raise TensorforceError.value(\n                    name=\'agent\', argument=\'buffer_observe\', value=buffer_observe,\n                    condition=\'parallel_interactions > 1\'\n                )\n            if self.max_episode_timesteps is None:\n                self.buffer_observe = buffer_observe\n            else:\n                self.buffer_observe = min(buffer_observe, self.max_episode_timesteps)\n        else:\n            raise TensorforceError.type(\n                name=\'agent\', argument=\'buffer_observe\', dtype=type(buffer_observe)\n            )\n\n        # Recorder\n        if recorder is None:\n            pass\n        elif not all(key in (\'directory\', \'frequency\', \'max-traces\', \'start\') for key in recorder):\n            raise TensorforceError.value(\n                name=\'agent\', argument=\'recorder\', value=list(recorder),\n                hint=\'not from {directory,frequency,max-traces,start}\'\n            )\n        self.recorder_spec = recorder if recorder is None else dict(recorder)\n\n        self.is_initialized = False\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def initialize(self):\n        """"""\n        Initializes the agent, usually done as part of Agent.create/load.\n        """"""\n        if self.is_initialized:\n            raise TensorforceError(\n                message=""Agent is already initialized, possibly as part of Agent.create().""\n            )\n\n        self.is_initialized = True\n\n        # Parallel terminal/reward buffers\n        self.terminal_buffers = np.ndarray(\n            shape=(self.parallel_interactions, self.buffer_observe),\n            dtype=util.np_dtype(dtype=\'long\')\n        )\n        self.reward_buffers = np.ndarray(\n            shape=(self.parallel_interactions, self.buffer_observe),\n            dtype=util.np_dtype(dtype=\'float\')\n        )\n\n        # Recorder buffers if required\n        if self.recorder_spec is not None:\n            self.states_buffers = OrderedDict()\n            self.actions_buffers = OrderedDict()\n            for name, spec in self.states_spec.items():\n                shape = (self.parallel_interactions, self.buffer_observe) + spec[\'shape\']\n                self.states_buffers[name] = np.ndarray(\n                    shape=shape, dtype=util.np_dtype(dtype=spec[\'type\'])\n                )\n            for name, spec in self.actions_spec.items():\n                shape = (self.parallel_interactions, self.buffer_observe) + spec[\'shape\']\n                self.actions_buffers[name] = np.ndarray(\n                    shape=shape, dtype=util.np_dtype(dtype=spec[\'type\'])\n                )\n                if spec[\'type\'] == \'int\':\n                    shape = (self.parallel_interactions, self.buffer_observe) + spec[\'shape\'] + \\\n                        (spec[\'num_values\'],)\n                    self.states_buffers[name + \'_mask\'] = np.ndarray(\n                        shape=shape, dtype=util.np_dtype(dtype=\'bool\')\n                    )\n\n            self.num_episodes = 0\n            self.record_states = OrderedDict(((name, list()) for name in self.states_spec))\n            self.record_actions = OrderedDict(((name, list()) for name in self.actions_spec))\n            for name, spec in self.actions_spec.items():\n                if spec[\'type\'] == \'int\':\n                    self.record_states[name + \'_mask\'] = list()\n            self.record_terminal = list()\n            self.record_reward = list()\n\n        # Parallel buffer indices\n        self.buffer_indices = np.zeros(\n            shape=(self.parallel_interactions,), dtype=util.np_dtype(dtype=\'int\')\n        )\n        self.timestep_completed = np.ndarray(\n            shape=(self.parallel_interactions,), dtype=util.np_dtype(dtype=\'bool\')\n        )\n\n        self.timesteps = 0\n        self.episodes = 0\n        self.updates = 0\n\n        # Setup Model\n        if not hasattr(self, \'model\'):\n            raise TensorforceError(message=""Missing agent attribute model."")\n\n        self.model.initialize()\n\n        self.internals_spec = self.model.internals_spec\n        self.auxiliaries_spec = self.model.auxiliaries_spec\n\n        if self.model.saver_directory is not None:\n            path = os.path.join(self.model.saver_directory, self.model.saver_filename + \'.json\')\n            try:\n                with open(path, \'w\') as fp:\n                    spec = OrderedDict(self.spec)\n                    spec[\'internals\'] = self.internals_spec\n                    spec[\'initial_internals\'] = self.initial_internals()\n                    json.dump(obj=spec, fp=fp, cls=TensorforceJSONEncoder)\n            except BaseException:\n                try:\n                    with open(path, \'w\') as fp:\n                        spec = OrderedDict()\n                        spec[\'states\'] = self.spec[\'states\']\n                        spec[\'actions\'] = self.spec[\'actions\']\n                        spec[\'internals\'] = self.internals_spec\n                        spec[\'initial_internals\'] = self.initial_internals()\n                        json.dump(obj=spec, fp=fp, cls=TensorforceJSONEncoder)\n                except BaseException:\n                    os.remove(path)\n                    raise\n\n        self.reset()\n\n    def close(self):\n        """"""\n        Closes the agent.\n        """"""\n        self.model.close()\n        self.model = None\n\n    def reset(self):\n        """"""\n        Resets all agent buffers and discards unfinished episodes.\n        """"""\n        self.buffer_indices = np.zeros(\n            shape=(self.parallel_interactions,), dtype=util.np_dtype(dtype=\'int\')\n        )\n        self.timestep_completed = np.ones(\n            shape=(self.parallel_interactions,), dtype=util.np_dtype(dtype=\'bool\')\n        )\n\n        self.timesteps, self.episodes, self.updates = self.model.reset()\n\n    def initial_internals(self):\n        """"""\n        Returns the initial internal agent state(s), to be used at the beginning of an episode as\n        `internals` argument for `act(...)` in independent mode\n\n        Returns:\n            dict[internal]: Dictionary containing initial internal agent state(s).\n        """"""\n        return OrderedDict(**self.model.internals_init)\n\n    def act(\n        self, states, internals=None, parallel=0, independent=False, deterministic=False,\n        evaluation=False, query=None, **kwargs\n    ):\n        """"""\n        Returns action(s) for the given state(s), needs to be followed by `observe(...)` unless\n        independent mode set via `independent`/`evaluation`.\n\n        Args:\n            states (dict[state] | iter[dict[state]]): Dictionary containing state(s) to be acted on\n                (<span style=""color:#C00000""><b>required</b></span>).\n            internals (dict[internal] | iter[dict[internal]]): Dictionary containing current\n                internal agent state(s), either given by `initial_internals()` at the beginning of\n                an episode or as return value of the preceding `act(...)` call\n                (<span style=""color:#C00000""><b>required</b></span> if independent mode and agent\n                has internal states).\n            parallel (int | iter[int]): Parallel execution index\n                (<span style=""color:#00C000""><b>default</b></span>: 0).\n            independent (bool): Whether act is not part of the main agent-environment interaction,\n                and this call is thus not followed by observe\n                (<span style=""color:#00C000""><b>default</b></span>: false).\n            deterministic (bool): Ff independent mode, whether to act deterministically, so no\n                exploration and sampling\n                (<span style=""color:#00C000""><b>default</b></span>: false).\n            evaluation (bool): Whether the agent is currently evaluated, implies independent and\n                deterministic\n                (<span style=""color:#00C000""><b>default</b></span>: false).\n            query (list[str]): Names of tensors to retrieve\n                (<span style=""color:#00C000""><b>default</b></span>: none).\n            kwargs: Additional input values, for instance, for dynamic hyperparameters.\n\n        Returns:\n            dict[action] | iter[dict[action]], dict[internal] | iter[dict[internal]] if `internals`\n            argument given, plus optional list[str]: Dictionary containing action(s), dictionary\n            containing next internal agent state(s) if independent mode, plus queried tensor values\n            if requested.\n        """"""\n        assert util.reduce_all(predicate=util.not_nan_inf, xs=states)\n\n        if evaluation:\n            if deterministic:\n                raise TensorforceError.invalid(\n                    name=\'agent.act\', argument=\'deterministic\', condition=\'evaluation = true\'\n                )\n            if independent:\n                raise TensorforceError.invalid(\n                    name=\'agent.act\', argument=\'independent\', condition=\'evaluation = true\'\n                )\n            deterministic = independent = True\n\n        if not independent:\n            if internals is not None:\n                raise TensorforceError.invalid(\n                    name=\'agent.act\', argument=\'internals\', condition=\'independent = false\'\n                )\n            if deterministic:\n                raise TensorforceError.invalid(\n                    name=\'agent.act\', argument=\'deterministic\', condition=\'independent = false\'\n                )\n\n        if independent:\n            internals_is_none = (internals is None)\n            if internals_is_none:\n                if len(self.model.internals_spec) > 0:\n                    raise TensorforceError.required(\n                        name=\'agent.act\', argument=\'internals\', condition=\'independent = true\'\n                    )\n                internals = OrderedDict()\n\n        # Batch states\n        batched = (not isinstance(parallel, int))\n        if batched:\n            if len(parallel) == 0:\n                raise TensorforceError.value(\n                    name=\'agent.act\', argument=\'parallel\', value=parallel, hint=\'zero-length\'\n                )\n            parallel = np.asarray(list(parallel))\n            if isinstance(states[0], dict):\n                states = OrderedDict((\n                    (name, np.asarray([states[n][name] for n in range(len(parallel))]))\n                    for name in states[0]\n                ))\n            else:\n                states = np.asarray(states)\n            if independent:\n                internals = OrderedDict((\n                    (name, np.asarray([internals[n][name] for n in range(len(parallel))]))\n                    for name in internals[0]\n                ))\n        else:\n            parallel = np.asarray([parallel])\n            states = util.fmap(\n                function=(lambda x: np.asarray([x])), xs=states,\n                depth=int(isinstance(states, dict))\n            )\n            if independent:\n                internals = util.fmap(function=(lambda x: np.asarray([x])), xs=internals, depth=1)\n\n        if not independent and not all(self.timestep_completed[n] for n in parallel):\n            raise TensorforceError(message=""Calling agent.act must be preceded by agent.observe."")\n\n        # Auxiliaries\n        auxiliaries = OrderedDict()\n        if isinstance(states, dict):\n            states = dict(states)\n            for name, spec in self.actions_spec.items():\n                if spec[\'type\'] == \'int\' and name + \'_mask\' in states:\n                    auxiliaries[name + \'_mask\'] = states.pop(name + \'_mask\')\n\n        # Normalize states dictionary\n        states = util.normalize_values(\n            value_type=\'state\', values=states, values_spec=self.states_spec\n        )\n\n        # Model.act()\n        if independent:\n            if query is None:\n                actions, internals = self.model.independent_act(\n                    states=states, internals=internals, auxiliaries=auxiliaries, parallel=parallel,\n                    deterministic=deterministic, **kwargs\n                )\n\n            else:\n                actions, internals, queried = self.model.independent_act(\n                    states=states, internals=internals, auxiliaries=auxiliaries, parallel=parallel,\n                    deterministic=deterministic, query=query, **kwargs\n                )\n\n        else:\n            if query is None:\n                actions, self.timesteps = self.model.act(\n                    states=states, auxiliaries=auxiliaries, parallel=parallel, **kwargs\n                )\n\n            else:\n                actions, self.timesteps, queried = self.model.act(\n                    states=states, auxiliaries=auxiliaries, parallel=parallel, query=query, **kwargs\n                )\n\n        if not independent:\n            for n in parallel:\n                self.timestep_completed[n] = False\n\n        if self.recorder_spec is not None and not independent and \\\n                self.episodes >= self.recorder_spec.get(\'start\', 0):\n            for n in range(len(parallel)):\n                index = self.buffer_indices[parallel[n]]\n                for name in self.states_spec:\n                    self.states_buffers[name][parallel[n], index] = states[name][n]\n                for name, spec in self.actions_spec.items():\n                    self.actions_buffers[name][parallel[n], index] = actions[name][n]\n                    if spec[\'type\'] == \'int\':\n                        name = name + \'_mask\'\n                        if name in auxiliaries:\n                            self.states_buffers[name][parallel[n], index] = auxiliaries[name][n]\n                        else:\n                            shape = (1,) + spec[\'shape\'] + (spec[\'num_values\'],)\n                            self.states_buffers[name][parallel[n], index] = np.full(\n                                shape=shape, fill_value=True, dtype=util.np_dtype(dtype=\'bool\')\n                            )\n\n        # Reverse normalized actions dictionary\n        actions = util.unpack_values(\n            value_type=\'action\', values=actions, values_spec=self.actions_spec\n        )\n\n        # Unbatch actions\n        if batched:\n            if isinstance(actions, dict):\n                actions = [\n                    OrderedDict(((name, actions[name][n]) for name in actions))\n                    for n in range(len(parallel))\n                ]\n        else:\n            actions = util.fmap(\n                function=(lambda x: x[0]), xs=actions, depth=int(isinstance(actions, dict))\n            )\n            if independent:\n                internals = util.fmap(function=(lambda x: x[0]), xs=internals, depth=1)\n\n        if independent and not internals_is_none:\n            if query is None:\n                return actions, internals\n            else:\n                return actions, internals, queried\n\n        else:\n            if independent and len(internals) > 0:\n                raise TensorforceError.unexpected()\n            if query is None:\n                return actions\n            else:\n                return actions, queried\n\n    def observe(self, reward, terminal=False, parallel=0, query=None, **kwargs):\n        """"""\n        Observes reward and whether a terminal state is reached, needs to be preceded by\n        `act(...)`.\n\n        Args:\n            reward (float | iter[float]): Reward\n                (<span style=""color:#C00000""><b>required</b></span>).\n            terminal (bool | 0 | 1 | 2 | iter[...]): Whether a terminal state is reached or 2 if\n                the episode was aborted (<span style=""color:#00C000""><b>default</b></span>: false).\n            parallel (int, iter[int]): Parallel execution index\n                (<span style=""color:#00C000""><b>default</b></span>: 0).\n            query (list[str]): Names of tensors to retrieve\n                (<span style=""color:#00C000""><b>default</b></span>: none).\n            kwargs: Additional input values, for instance, for dynamic hyperparameters.\n\n        Returns:\n            (bool | int, optional list[str]): Whether an update was performed, plus queried tensor\n            values if requested.\n        """"""\n        assert util.reduce_all(predicate=util.not_nan_inf, xs=reward)\n\n        if query is not None and self.parallel_interactions > 1:\n            raise TensorforceError.invalid(\n                name=\'agent.observe\', argument=\'query\', condition=\'parallel_interactions > 1\'\n            )\n\n        batched = (not isinstance(parallel, int))\n        if batched:\n            if len(parallel) == 0:\n                raise TensorforceError.value(\n                    name=\'agent.observe\', argument=\'parallel\', value=parallel, hint=\'zero-length\'\n                )\n            if query is not None:\n                raise TensorforceError.invalid(\n                    name=\'agent.observe\', argument=\'query\', condition=\'len(parallel) > 1\'\n                )\n        else:\n            terminal = [terminal]\n            reward = [reward]\n            parallel = [parallel]\n\n        if any(self.timestep_completed[n] for n in parallel):\n            raise TensorforceError(message=""Calling agent.observe must be preceded by agent.act."")\n\n        num_updates = 0\n        # TODO: Differently if not buffer_observe\n        for terminal, reward, parallel in zip(terminal, reward, parallel):\n            # Update terminal/reward buffer\n            if isinstance(terminal, bool):\n                terminal = int(terminal)\n            index = self.buffer_indices[parallel]\n            self.terminal_buffers[parallel, index] = terminal\n            self.reward_buffers[parallel, index] = reward\n            index += 1\n            self.buffer_indices[parallel] = index\n\n            if self.max_episode_timesteps is not None and index > self.max_episode_timesteps:\n                raise TensorforceError.value(\n                    name=\'agent.observe\', argument=\'index\', value=index,\n                    condition=\'> max_episode_timesteps\'\n                )\n\n            if terminal > 0 or index == self.buffer_observe or query is not None:\n                self.timestep_completed[parallel] = True\n                if query is None:\n                    updated = self.model_observe(parallel=parallel, **kwargs)\n                else:\n                    updated, queried = self.model_observe(parallel=parallel, query=query, **kwargs)\n\n            else:\n                # Increment buffer index\n                self.timestep_completed[parallel] = True\n                updated = False\n\n            num_updates += int(updated)\n\n        if batched:\n            updated = num_updates\n        else:\n            assert num_updates <= 1\n            updated = (num_updates == 1)\n\n        if query is None:\n            return updated\n        else:\n            return updated, queried\n\n    def model_observe(self, parallel, query=None, **kwargs):\n        assert self.timestep_completed[parallel]\n        index = self.buffer_indices[parallel]\n        terminal = self.terminal_buffers[parallel, :index]\n        reward = self.reward_buffers[parallel, :index]\n\n        if self.recorder_spec is not None and \\\n                self.episodes >= self.recorder_spec.get(\'start\', 0):\n            for name in self.states_spec:\n                self.record_states[name].append(\n                    np.array(self.states_buffers[name][parallel, :index])\n                )\n            for name, spec in self.actions_spec.items():\n                self.record_actions[name].append(\n                    np.array(self.actions_buffers[name][parallel, :index])\n                )\n                if spec[\'type\'] == \'int\':\n                    self.record_states[name + \'_mask\'].append(\n                        np.array(self.states_buffers[name + \'_mask\'][parallel, :index])\n                    )\n            self.record_terminal.append(np.array(terminal))\n            self.record_reward.append(np.array(reward))\n\n            if terminal[-1] > 0:\n                self.num_episodes += 1\n\n                if self.num_episodes == self.recorder_spec.get(\'frequency\', 1):\n                    directory = self.recorder_spec[\'directory\']\n                    if os.path.isdir(directory):\n                        files = sorted(\n                            f for f in os.listdir(directory)\n                            if os.path.isfile(os.path.join(directory, f))\n                            and f.startswith(\'trace-\')\n                        )\n                    else:\n                        os.makedirs(directory)\n                        files = list()\n                    max_traces = self.recorder_spec.get(\'max-traces\')\n                    if max_traces is not None and len(files) > max_traces - 1:\n                        for filename in files[:-max_traces + 1]:\n                            filename = os.path.join(directory, filename)\n                            os.remove(filename)\n\n                    filename = \'trace-{}-{}.npz\'.format(\n                        self.episodes, time.strftime(\'%Y%m%d-%H%M%S\')\n                    )\n                    filename = os.path.join(directory, filename)\n                    self.record_states = util.fmap(\n                        function=np.concatenate, xs=self.record_states, depth=1\n                    )\n                    self.record_actions = util.fmap(\n                        function=np.concatenate, xs=self.record_actions, depth=1\n                    )\n                    self.record_terminal = np.concatenate(self.record_terminal)\n                    self.record_reward = np.concatenate(self.record_reward)\n                    np.savez_compressed(\n                        filename, **self.record_states, **self.record_actions,\n                        terminal=self.record_terminal, reward=self.record_reward\n                    )\n                    self.record_states = util.fmap(\n                        function=(lambda x: list()), xs=self.record_states, depth=1\n                    )\n                    self.record_actions = util.fmap(\n                        function=(lambda x: list()), xs=self.record_actions, depth=1\n                    )\n                    self.record_terminal = list()\n                    self.record_reward = list()\n                    self.num_episodes = 0\n\n        # Reset buffer index\n        self.buffer_indices[parallel] = 0\n\n        # Model.observe()\n        if query is None:\n            updated, self.episodes, self.updates = self.model.observe(\n                terminal=terminal, reward=reward, parallel=[parallel], **kwargs\n            )\n            return updated\n\n        else:\n            updated, self.episodes, self.updates, queried = self.model.observe(\n                terminal=terminal, reward=reward, parallel=[parallel], query=query,\n                **kwargs\n            )\n            return updated, queried\n\n    def save(self, directory=None, filename=None, format=\'tensorflow\', append=None):\n        """"""\n        Saves the agent to a checkpoint.\n\n        Args:\n            directory (str): Checkpoint directory\n                (<span style=""color:#00C000""><b>default</b></span>: directory specified for\n                TensorFlow saver, otherwise current directory).\n            filename (str): Checkpoint filename, without extension\n                (<span style=""color:#00C000""><b>default</b></span>: filename specified for\n                TensorFlow saver, otherwise name of agent).\n            format (""tensorflow"" | ""numpy"" | ""hdf5""): File format, ""tensorflow"" uses TensorFlow\n                saver to store variables, graph meta information and an optimized Protobuf model\n                with an act-only graph, whereas the others only store variables as NumPy/HDF5 file\n                (<span style=""color:#00C000""><b>default</b></span>: TensorFlow format).\n            append (""timesteps"" | ""episodes"" | ""updates""): Append current timestep/episode/update to\n                checkpoint filename\n                (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        Returns:\n            str: Checkpoint path.\n        """"""\n        # TODO: Messes with required parallel disentangling, better to remove unfinished episodes\n        # from memory, but currently entire episode buffered anyway...\n        # Empty buffers before saving\n        for parallel in range(self.parallel_interactions):\n            if self.buffer_indices[parallel] > 0:\n                self.model_observe(parallel=parallel)\n\n        if directory is None:\n            # default directory: saver if given, otherwise current directory "".""\n            if self.model.saver_directory is None:\n                directory = \'.\'\n            else:\n                directory = self.model.saver_directory\n\n        if filename is None:\n            # default filename: saver which defaults to agent name\n            filename = self.model.saver_filename\n\n        path = self.model.save(directory=directory, filename=filename, format=format, append=append)\n\n        spec_path = os.path.join(directory, filename + \'.json\')\n        try:\n            with open(spec_path, \'w\') as fp:\n                spec = OrderedDict(self.spec)\n                spec[\'internals\'] = self.internals_spec\n                spec[\'initial_internals\'] = self.initial_internals()\n                json.dump(obj=spec, fp=fp, cls=TensorforceJSONEncoder)\n        except BaseException:\n            try:\n                with open(spec_path, \'w\') as fp:\n                    spec = OrderedDict()\n                    spec[\'states\'] = self.spec[\'states\']\n                    spec[\'actions\'] = self.spec[\'actions\']\n                    spec[\'internals\'] = self.internals_spec\n                    spec[\'initial_internals\'] = self.initial_internals()\n                    json.dump(obj=spec, fp=fp, cls=TensorforceJSONEncoder)\n            except BaseException:\n                os.remove(spec_path)\n\n        return path\n\n    def restore(self, directory=None, filename=None, format=None):\n        """"""\n        Restores the agent from a checkpoint.\n\n        Args:\n            directory (str): Checkpoint directory\n                (<span style=""color:#00C000""><b>default</b></span>: directory specified for\n                TensorFlow saver, otherwise current directory).\n            filename (str): Checkpoint filename, with or without append and extension\n                (<span style=""color:#00C000""><b>default</b></span>: filename specified for\n                TensorFlow saver, otherwise name of agent or latest checkpoint in directory).\n            format (""tensorflow"" | ""numpy"" | ""hdf5""): File format\n                (<span style=""color:#00C000""><b>default</b></span>: format matching directory and\n                filename, required to be unambiguous).\n        """"""\n        if not hasattr(self, \'model\'):\n            raise TensorforceError(message=""Missing agent attribute model."")\n\n        if not self.is_initialized:\n            self.initialize()\n\n        if directory is None:\n            # default directory: saver if given, otherwise current directory "".""\n            if self.model.saver_directory is None:\n                directory = \'.\'\n            else:\n                directory = self.model.saver_directory\n\n        if filename is None:\n            # default filename: saver which defaults to agent name\n            filename = self.model.saver_filename\n\n        # format implicitly given if file exists\n        if format is None and os.path.isfile(os.path.join(directory, filename)):\n            if \'.data-\' in filename:\n                filename = filename[:filename.index(\'.data-\')]\n                format = \'tensorflow\'\n            elif filename.endswith(\'.npz\'):\n                filename = filename[:-4]\n                format = \'numpy\'\n            elif filename.endswith(\'.hdf5\'):\n                filename = filename[:-5]\n                format = \'hdf5\'\n            elif filename.endswith(\'.h5\'):\n                filename = filename[:-3]\n                format = \'hdf5\'\n            else:\n                assert False\n        elif format is None and os.path.isfile(os.path.join(directory, filename + \'.meta\')):\n            format = \'tensorflow\'\n        elif format is None and os.path.isfile(os.path.join(directory, filename + \'.npz\')):\n            format = \'numpy\'\n        elif format is None and (\n            os.path.isfile(os.path.join(directory, filename + \'.hdf5\')) or\n            os.path.isfile(os.path.join(directory, filename + \'.h5\'))\n        ):\n            format = \'hdf5\'\n\n        else:\n            # infer format from directory\n            found = None\n            latest = -1\n            for name in os.listdir(directory):\n                if format in (None, \'numpy\') and name == filename + \'.npz\':\n                    assert found is None\n                    found = \'numpy\'\n                    latest = None\n                elif format in (None, \'numpy\') and name.startswith(filename) and \\\n                        name.endswith(\'.npz\'):\n                    assert found is None or found == \'numpy\'\n                    found = \'numpy\'\n                    n = int(name[len(filename) + 1: -4])\n                    if n > latest:\n                        latest = n\n                elif format in (None, \'hdf5\') and \\\n                        (name == filename + \'.hdf5\' or  name == filename + \'.h5\'):\n                    assert found is None\n                    found = \'hdf5\'\n                    latest = None\n                elif format in (None, \'hdf5\') and name.startswith(filename) and \\\n                        (name.endswith(\'.hdf5\') or name.endswith(\'.h5\')):\n                    assert found is None or found == \'hdf5\'\n                    found = \'hdf5\'\n                    n = int(name[len(filename) + 1: -5])\n                    if n > latest:\n                        latest = n\n\n            if latest == -1:\n                if format is None:\n                    format = \'tensorflow\'\n                else:\n                    assert format == \'tensorflow\'\n                if filename is None or not os.path.isfile(os.path.join(directory, filename + \'.meta\')):\n                    path = tf.compat.v1.train.latest_checkpoint(checkpoint_dir=directory, latest_filename=None)\n                    if \'/\' in path:\n                        filename = path[path.rindex(\'/\') + 1:]\n                    else:\n                        filename = path\n\n            else:\n                if format is None:\n                    format = found\n                else:\n                    assert format == found\n                if latest is not None:\n                    filename = filename + \'-\' + str(latest)\n\n        self.timesteps, self.episodes, self.updates = self.model.restore(\n            directory=directory, filename=filename, format=format\n        )\n\n    def get_variables(self):\n        """"""\n        Returns the names of all agent variables.\n\n        Returns:\n            list[str]: Names of variables.\n        """"""\n        return [\n            variable.name[len(self.model.name) + 1: -2] for variable in self.model.get_variables()\n        ]\n\n    def get_variable(self, variable):\n        """"""\n        Returns the value of the variable with the given name.\n\n        Args:\n            variable (string): Variable name\n                (<span style=""color:#C00000""><b>required</b></span>).\n\n        Returns:\n            numpy-array: Variable value.\n        """"""\n        return self.model.get_variable(variable=variable)\n\n    def assign_variable(self, variable, value):\n        """"""\n        Assigns the given value to the variable with the given name.\n\n        Args:\n            variable (string): Variable name\n                (<span style=""color:#C00000""><b>required</b></span>).\n            value (variable-compatible value): Value to assign to variable\n                (<span style=""color:#C00000""><b>required</b></span>).\n        """"""\n        self.model.assign_variable(variable=variable, value=value)\n\n    def summarize(self, summary, value, step=None):\n        """"""\n        Records a value for the given custom summary label (as specified via summarizer[custom]).\n\n        Args:\n            variable (string): Custom summary label\n                (<span style=""color:#C00000""><b>required</b></span>).\n            value (summary-compatible value): Summary value to record\n                (<span style=""color:#C00000""><b>required</b></span>).\n            step (int): Summary recording step\n                (<span style=""color:#00C000""><b>default</b></span>: current timestep).\n        """"""\n        self.model.summarize(summary=summary, value=value, step=step)\n\n    def get_output_tensors(self, function):\n        """"""\n        Returns the names of output tensors for the given function.\n\n        Args:\n            function (str): Function name\n                (<span style=""color:#C00000""><b>required</b></span>).\n\n        Returns:\n            list[str]: Names of output tensors.\n        """"""\n        if function in self.model.output_tensors:\n            return self.model.output_tensors[function]\n        else:\n            raise TensorforceError.value(\n                name=\'agent.get_output_tensors\', argument=\'function\', value=function\n            )\n\n    def get_available_summaries(self):\n        """"""\n        Returns the summary labels provided by the agent.\n\n        Returns:\n            list[str]: Available summary labels.\n        """"""\n        return self.model.get_available_summaries()\n\n    def get_query_tensors(self, function):\n        """"""\n        Returns the names of queryable tensors for the given function.\n\n        Args:\n            function (str): Function name\n                (<span style=""color:#C00000""><b>required</b></span>).\n\n        Returns:\n            list[str]: Names of queryable tensors.\n        """"""\n        if function in self.model.query_tensors:\n            return self.model.query_tensors[function]\n        else:\n            raise TensorforceError.value(\n                name=\'agent.get_query_tensors\', argument=\'function\', value=function\n            )\n\n\nclass ActonlyAgent(object):\n\n    def __init__(self, path, states, actions, internals=None, initial_internals=None):\n        self.states_spec = states\n        self.actions_spec = actions\n        if internals is None:\n            assert initial_internals is None\n            self.internals_spec = OrderedDict()\n            self._initial_internals = OrderedDict()\n        else:\n            assert list(internals) == list(initial_internals)\n            self.internals_spec = internals\n            self._initial_internals = initial_internals\n\n        with tf.io.gfile.GFile(name=path, mode=\'rb\') as filehandle:\n            graph_def = tf.compat.v1.GraphDef()\n            graph_def.ParseFromString(filehandle.read())\n        graph = tf.Graph()\n        with graph.as_default():\n            tf.graph_util.import_graph_def(graph_def=graph_def, name=\'\')\n        graph.finalize()\n        self.session = tf.compat.v1.Session(graph=graph)\n        self.session.__enter__()\n\n    def close(self):\n        self.session.__exit__(None, None, None)\n        tf.compat.v1.reset_default_graph()\n\n    def initial_internals(self):\n        return OrderedDict(**self._initial_internals)\n\n    def act(\n        self, states, internals=None, parallel=0, independent=True, deterministic=True,\n        evaluation=True, query=None, **kwargs\n    ):\n        # Invalid arguments\n        assert parallel == 0 and independent and deterministic and evaluation and \\\n            query is None and len(kwargs) == 0\n\n        assert util.reduce_all(predicate=util.not_nan_inf, xs=states)\n        internals_is_none = (internals is None)\n        if internals_is_none:\n            if len(self.internals_spec) > 0:\n                raise TensorforceError.required(name=\'agent.act\', argument=\'internals\')\n            internals = OrderedDict()\n\n        # Batch states\n        name = next(iter(self.states_spec))\n        batched = (np.asarray(states[name]).ndim > len(self.states_spec[name][\'shape\']))\n        if batched:\n            if isinstance(states[0], dict):\n                states = OrderedDict((\n                    (name, np.asarray([states[n][name] for n in range(len(parallel))]))\n                    for name in states[0]\n                ))\n            else:\n                states = np.asarray(states)\n            internals = OrderedDict((\n                (name, np.asarray([internals[n][name] for n in range(len(parallel))]))\n                for name in internals[0]\n            ))\n        else:\n            states = util.fmap(\n                function=(lambda x: np.asarray([x])), xs=states,\n                depth=int(isinstance(states, dict))\n            )\n            internals = util.fmap(function=(lambda x: np.asarray([x])), xs=internals, depth=1)\n\n        # Auxiliaries\n        auxiliaries = OrderedDict()\n        if isinstance(states, dict):\n            states = dict(states)\n            for name, spec in self.actions_spec.items():\n                if spec[\'type\'] == \'int\' and name + \'_mask\' in states:\n                    auxiliaries[name + \'_mask\'] = states.pop(name + \'_mask\')\n\n        # Normalize states dictionary\n        states = util.normalize_values(\n            value_type=\'state\', values=states, values_spec=self.states_spec\n        )\n\n        # Model.act()\n        fetches = (\n            {\n                name: util.join_scopes(\'agent.independent_act\', name + \'-output:0\')\n                for name in self.actions_spec\n            }, {\n                name: util.join_scopes(\'agent.independent_act\', name + \'-output:0\')\n                for name in self.internals_spec\n            }\n        )\n\n        feed_dict = dict()\n        for name, state in states.items():\n            feed_dict[util.join_scopes(\'agent\', name + \'-input:0\')] = state\n        for name, auxiliary in auxiliaries.items():\n            feed_dict[util.join_scopes(\'agent\', name + \'-input:0\')] = auxiliary\n        for name, internal in internals.items():\n            feed_dict[util.join_scopes(\'agent\', name + \'-input:0\')] = internal\n\n        actions, internals = self.session.run(fetches=fetches, feed_dict=feed_dict)\n\n        # Reverse normalized actions dictionary\n        actions = util.unpack_values(\n            value_type=\'action\', values=actions, values_spec=self.actions_spec\n        )\n\n        # Unbatch actions\n        if batched:\n            if isinstance(actions, dict):\n                actions = [\n                    OrderedDict(((name, actions[name][n]) for name in actions))\n                    for n in range(len(parallel))\n                ]\n        else:\n            actions = util.fmap(\n                function=(lambda x: x[0]), xs=actions, depth=int(isinstance(actions, dict))\n            )\n            internals = util.fmap(function=(lambda x: x[0]), xs=internals, depth=1)\n\n        if internals_is_none:\n            if len(internals) > 0:\n                raise TensorforceError.unexpected()\n            return actions\n        else:\n            return actions, internals\n\n\nclass TensorforceJSONEncoder(json.JSONEncoder):\n    """"""\n    Custom JSON encoder which is NumPy-compatible.\n    """"""\n\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super().default(obj)\n'"
tensorforce/agents/constant.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.agents import Agent\nfrom tensorforce.core.models import ConstantModel\n\n\nclass ConstantAgent(Agent):\n    """"""\n    Agent returning constant action values (specification key: `constant`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        action_values (dict[value]): Constant value per action\n            (<span style=""color:#00C000""><b>default</b></span>: false for binary boolean actions,\n            0 for discrete integer actions, 0.0 for continuous actions).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        name (string): Agent name, used e.g. for TensorFlow scopes\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0) &ndash; how frequently in timesteps to record\n            summaries (<span style=""color:#00C000""><b>default</b></span>: always).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all or list of summaries to\n            record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""graph"": graph summary</li>\n            <li>""parameters"": parameter scalars</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Environment\n        self, states, actions, max_episode_timesteps=None,\n        # Agent\n        action_values=None,\n        # TensorFlow etc\n        name=\'agent\', device=None, seed=None, summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'constant\',\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            action_values=action_values,\n            name=name, device=device, seed=seed, summarizer=summarizer, recorder=recorder,\n            config=config\n        )\n\n        super().__init__(\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=1, buffer_observe=True, seed=seed, recorder=recorder\n        )\n\n        self.model = ConstantModel(\n            # Model\n            name=name, device=device, parallel_interactions=self.parallel_interactions,\n            buffer_observe=self.buffer_observe, seed=seed, summarizer=summarizer, config=config,\n            states=self.states_spec, actions=self.actions_spec,\n            # ConstantModel\n            action_values=action_values\n        )\n'"
tensorforce/agents/dpg.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce import TensorforceError\nfrom tensorforce.agents import TensorforceAgent\n\n\nclass DeterministicPolicyGradient(TensorforceAgent):\n    """"""\n    [Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971) agent (specification key:\n    `dpg`). Action space is required to consist of only a single float action.\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        memory (int > 0): Replay memory capacity, has to fit at least maximum batch_size + maximum\n            network/estimator horizon + 1 timesteps\n            (<span style=""color:#C00000""><b>required</b></span>).\n        batch_size (parameter, long > 0): Number of timesteps per update batch\n            (<span style=""color:#C00000""><b>required</b></span>).\n\n        network (""auto"" | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"", automatically configured\n            network).\n        use_beta_distribution (bool): Whether to use the Beta distribution for bounded continuous\n            actions by default.\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n\n        update_frequency (""never"" | parameter, long > 0): Frequency of updates\n            (<span style=""color:#00C000""><b>default</b></span>: batch_size).\n        start_updating (parameter, long >= batch_size): Number of timesteps before first update\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        learning_rate (parameter, float > 0.0): Optimizer learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 3e-4).\n\n        horizon (""episode"" | parameter, long >= 0): Horizon of discounted-sum reward estimation\n            before critic estimate\n            (<span style=""color:#00C000""><b>default</b></span>: 0).\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 0.99).\n        estimate_terminal (bool): Whether to estimate the value of (real) terminal states\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n\n        critic_network (specification): Critic network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        critic_optimizer (float > 0.0 | specification): Critic optimizer configuration, see\n            [optimizers](../modules/optimizers.html), a float instead specifies a custom weight for\n            the critic loss\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Required\n        self, states, actions, memory, batch_size,\n        # Environment\n        max_episode_timesteps=None,\n        # Network\n        network=\'auto\', use_beta_distribution=True,\n        # Optimization\n        update_frequency=None, start_updating=None, learning_rate=3e-4,\n        # Reward estimation\n        horizon=0, discount=0.99, estimate_terminal=False,\n        # Critic\n        critic_network=\'auto\', critic_optimizer=1.0,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'dpg\',\n            states=states, actions=actions, memory=memory, batch_size=batch_size,\n            max_episode_timesteps=max_episode_timesteps,\n            network=network, use_beta_distribution=use_beta_distribution,\n            update_frequency=update_frequency, start_updating=start_updating,\n                learning_rate=learning_rate,\n            horizon=horizon, discount=discount, estimate_terminal=estimate_terminal,\n            critic_network=critic_network, critic_optimizer=critic_optimizer,\n            preprocessing=preprocessing,\n            exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n            name=name, device=device, parallel_interactions=parallel_interactions, seed=seed,\n                execution=execution, saver=saver, summarizer=summarizer, recorder=recorder,\n                config=config\n        )\n\n        policy = dict(network=network, temperature=0.0, use_beta_distribution=use_beta_distribution)\n        memory = dict(type=\'replay\', capacity=memory)\n        update = dict(unit=\'timesteps\', batch_size=batch_size)\n        if update_frequency is not None:\n            update[\'frequency\'] = update_frequency\n        if start_updating is not None:\n            update[\'start\'] = start_updating\n        optimizer = dict(type=\'adam\', learning_rate=learning_rate)\n        objective = \'deterministic_policy_gradient\'\n        reward_estimation = dict(\n            horizon=horizon, discount=discount, estimate_horizon=\'late\',\n            estimate_terminal=estimate_terminal, estimate_actions=True\n        )\n        baseline_policy = dict(network=critic_network, distributions=dict(float=\'gaussian\'))\n        baseline_objective = dict(type=\'value\', value=\'action\')\n\n        super().__init__(\n            # Agent\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=True, seed=seed,\n            recorder=recorder, config=config,\n            # Model\n            name=name, device=device, execution=execution, saver=saver, summarizer=summarizer,\n            preprocessing=preprocessing, exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=critic_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n\n        action_spec = next(iter(self.actions_spec.values()))\n        if len(self.actions_spec) > 1 or action_spec[\'type\'] != \'float\' or \\\n                (action_spec[\'shape\'] != () and action_spec[\'shape\'] != (1,)):\n            raise TensorforceError.value(\n                name=\'DeterministicPolicyGradient\', argument=\'actions\', value=actions,\n                hint=\'contains more than a single float action\'\n            )\n'"
tensorforce/agents/dqn.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.agents import TensorforceAgent\n\n\nclass DeepQNetwork(TensorforceAgent):\n    """"""\n    [Deep Q-Network](https://www.nature.com/articles/nature14236) agent (specification key: `dqn`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        memory (int > 0): Replay memory capacity, has to fit at least maximum batch_size + maximum\n            network/estimator horizon + 1 timesteps\n            (<span style=""color:#C00000""><b>required</b></span>).\n        batch_size (parameter, long > 0): Number of timesteps per update batch\n            (<span style=""color:#C00000""><b>required</b></span>).\n\n        network (""auto"" | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"", automatically configured\n            network).\n\n        update_frequency (""never"" | parameter, long > 0): Frequency of updates\n            (<span style=""color:#00C000""><b>default</b></span>: batch_size).\n        start_updating (parameter, long >= batch_size): Number of timesteps before first update\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        learning_rate (parameter, float > 0.0): Optimizer learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 3e-4).\n        huber_loss (parameter, float > 0.0): Huber loss threshold\n            (<span style=""color:#00C000""><b>default</b></span>: no huber loss).\n\n        horizon (""episode"" | parameter, long >= 0): Horizon of discounted-sum reward estimation\n            before critic estimate\n            (<span style=""color:#00C000""><b>default</b></span>: 0).\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 0.99).\n        estimate_terminal (bool): Whether to estimate the value of (real) terminal states\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n\n        target_sync_frequency (parameter, int > 0): Interval between target network updates\n            (<span style=""color:#00C000""><b>default</b></span>: every update).\n        target_update_weight (parameter, 0.0 < float <= 1.0): Target network update weight\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Required\n        self, states, actions, memory, batch_size,\n        # Environment\n        max_episode_timesteps=None,\n        # Network\n        network=\'auto\',\n        # Optimization\n        update_frequency=None, start_updating=None, learning_rate=3e-4, huber_loss=0.0,\n        # Reward estimation\n        horizon=0, discount=0.99, estimate_terminal=False,  # double_q_model=False !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n        # Target network\n        target_sync_frequency=1, target_update_weight=1.0,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'dqn\',\n            states=states, actions=actions, memory=memory, batch_size=batch_size,\n            max_episode_timesteps=max_episode_timesteps,\n            network=network,\n            update_frequency=update_frequency, start_updating=start_updating,\n                learning_rate=learning_rate, huber_loss=huber_loss,\n            horizon=horizon, discount=discount, estimate_terminal=estimate_terminal,\n            target_sync_frequency=target_sync_frequency, target_update_weight=target_update_weight,\n            preprocessing=preprocessing,\n            exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n            name=name, device=device, parallel_interactions=parallel_interactions, seed=seed,\n                execution=execution, saver=saver, summarizer=summarizer, recorder=recorder,\n                config=config\n        )\n\n        # Action value doesn\'t exist for Beta\n        policy = dict(\n            network=network, distributions=dict(float=\'gaussian\'), temperature=0.0,\n            use_beta_distribution=False, infer_state_value=\'action-values\'\n        )\n        memory = dict(type=\'replay\', capacity=memory)\n        update = dict(unit=\'timesteps\', batch_size=batch_size)\n        if update_frequency is not None:\n            update[\'frequency\'] = update_frequency\n        if start_updating is not None:\n            update[\'start\'] = start_updating\n        optimizer = dict(type=\'adam\', learning_rate=learning_rate)\n        objective = dict(type=\'value\', value=\'action\', huber_loss=huber_loss)\n        reward_estimation = dict(\n            horizon=horizon, discount=discount, estimate_horizon=\'late\',\n            estimate_terminal=estimate_terminal\n        )\n        baseline_policy = policy\n        baseline_optimizer = dict(\n            type=\'synchronization\', sync_frequency=target_sync_frequency,\n            update_weight=target_update_weight\n        )\n        baseline_objective = None\n\n        super().__init__(\n            # Agent\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=True, seed=seed,\n            recorder=recorder, config=config,\n            # Model\n            name=name, device=device, execution=execution, saver=saver, summarizer=summarizer,\n            preprocessing=preprocessing, exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=baseline_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n'"
tensorforce/agents/dueling_dqn.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce import TensorforceError\nfrom tensorforce.agents import TensorforceAgent\n\n\nclass DuelingDQN(TensorforceAgent):\n    """"""\n    [Dueling DQN](https://arxiv.org/abs/1511.06581) agent (specification key: `dueling_dqn`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        memory (int > 0): Replay memory capacity, has to fit at least maximum batch_size + maximum\n            network/estimator horizon + 1 timesteps\n            (<span style=""color:#C00000""><b>required</b></span>).\n        batch_size (parameter, long > 0): Number of timesteps per update batch\n            (<span style=""color:#C00000""><b>required</b></span>).\n\n        network (""auto"" | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"", automatically configured\n            network).\n        update_frequency (""never"" | parameter, long > 0): Frequency of updates\n            (<span style=""color:#00C000""><b>default</b></span>: batch_size).\n        start_updating (parameter, long >= batch_size): Number of timesteps before first update\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        learning_rate (parameter, float > 0.0): Optimizer learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 3e-4).\n        huber_loss (parameter, float > 0.0): Huber loss threshold\n            (<span style=""color:#00C000""><b>default</b></span>: no huber loss).\n\n        horizon (""episode"" | parameter, long >= 0): Horizon of discounted-sum reward estimation\n            before critic estimate\n            (<span style=""color:#00C000""><b>default</b></span>: 0).\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 0.99).\n        estimate_terminal (bool): Whether to estimate the value of (real) terminal states\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n\n        target_sync_frequency (parameter, int > 0): Interval between target network updates\n            (<span style=""color:#00C000""><b>default</b></span>: every update).\n        target_update_weight (parameter, 0.0 < float <= 1.0): Target network update weight\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Required\n        self, states, actions, memory, batch_size,\n        # Environment\n        max_episode_timesteps=None,\n        # Network\n        network=\'auto\',\n        # Optimization\n        update_frequency=None, start_updating=None, learning_rate=3e-4, huber_loss=0.0,\n        # Reward estimation\n        horizon=0, discount=0.99, estimate_terminal=False,  # double_q_model=False !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n        # Target network\n        target_sync_frequency=1, target_update_weight=1.0,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'dueling_dqn\',\n            states=states, actions=actions, memory=memory, batch_size=batch_size,\n            max_episode_timesteps=max_episode_timesteps,\n            network=network,\n            update_frequency=update_frequency, start_updating=start_updating,\n                learning_rate=learning_rate, huber_loss=huber_loss,\n            horizon=horizon, discount=discount, estimate_terminal=estimate_terminal,\n            target_sync_frequency=target_sync_frequency, target_update_weight=target_update_weight,\n            preprocessing=preprocessing,\n            exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n            name=name, device=device, parallel_interactions=parallel_interactions, seed=seed,\n                execution=execution, saver=saver, summarizer=summarizer, recorder=recorder,\n                config=config\n        )\n\n        # Action value doesn\'t exist for Beta\n        distributions = dict(int=dict(type=\'categorical\', advantage_based=True))\n        policy = dict(\n            network=network, distributions=distributions, temperature=0.0,\n            infer_state_value=\'action-values\'\n        )\n        memory = dict(type=\'replay\', capacity=memory)\n        update = dict(unit=\'timesteps\', batch_size=batch_size)\n        if update_frequency is not None:\n            update[\'frequency\'] = update_frequency\n        if start_updating is not None:\n            update[\'start\'] = start_updating\n        optimizer = dict(type=\'adam\', learning_rate=learning_rate)\n        objective = dict(type=\'value\', value=\'action\', huber_loss=huber_loss)\n        reward_estimation = dict(\n            horizon=horizon, discount=discount, estimate_horizon=\'late\',\n            estimate_terminal=estimate_terminal\n        )\n        baseline_policy = policy\n        baseline_optimizer = dict(\n            type=\'synchronization\', sync_frequency=target_sync_frequency,\n            update_weight=target_update_weight\n        )\n        baseline_objective = None\n\n        super().__init__(\n            # Agent\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=True, seed=seed,\n            recorder=recorder, config=config,\n            # Model\n            name=name, device=device, execution=execution, saver=saver, summarizer=summarizer,\n            preprocessing=preprocessing, exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=baseline_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n\n        if any(spec[\'type\'] != \'int\' for spec in self.actions_spec.values()):\n            raise TensorforceError.value(\n                name=\'DuelingDQN\', argument=\'actions\', value=actions, hint=\'contains non-int action\'\n            )\n'"
tensorforce/agents/ppo.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.agents import TensorforceAgent\n\n\nclass ProximalPolicyOptimization(TensorforceAgent):\n    """"""\n    [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) agent (specification key:\n    `ppo`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        batch_size (parameter, long > 0): Number of episodes per update batch\n            (<span style=""color:#C00000""><b>required</b></span>).\n\n        network (""auto"" | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"", automatically configured\n            network).\n        use_beta_distribution (bool): Whether to use the Beta distribution for bounded continuous\n            actions by default.\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n\n        memory (int > 0): Batch memory capacity, has to fit at least maximum batch_size + 1 episodes\n            (<span style=""color:#00C000""><b>default</b></span>: minimum capacity, usually does not\n            need to be changed).\n\n        update_frequency (""never"" | parameter, long > 0): Frequency of updates\n            (<span style=""color:#00C000""><b>default</b></span>: batch_size).\n        learning_rate (parameter, float > 0.0): Optimizer learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 3e-4).\n        subsampling_fraction (parameter, 0.0 < float <= 1.0): Fraction of batch timesteps to\n            subsample (<span style=""color:#00C000""><b>default</b></span>: 0.33).\n        optimization_steps (parameter, int > 0): Number of optimization steps\n            (<span style=""color:#00C000""><b>default</b></span>: 10).\n\n        likelihood_ratio_clipping (parameter, float > 0.0): Likelihood-ratio clipping threshold\n            (<span style=""color:#00C000""><b>default</b></span>: 0.2).\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 0.99).\n        estimate_terminal (bool): Whether to estimate the value of (real) terminal states\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n\n        critic_network (specification): Critic network configuration, see\n            [networks](../modules/networks.html), main policy will be used as critic if none\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        critic_optimizer (float > 0.0 | specification): Critic optimizer configuration, see\n            [optimizers](../modules/optimizers.html), main optimizer will be used for critic if\n            none, a float implies none and specifies a custom weight for the critic loss\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to fit at leastset separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Required\n        self, states, actions, max_episode_timesteps, batch_size,\n        # Network\n        network=\'auto\', use_beta_distribution=True,\n        # Memory\n        memory=None,\n        # Optimization\n        update_frequency=None, learning_rate=3e-4, subsampling_fraction=0.33, optimization_steps=10,\n        # Reward estimation\n        likelihood_ratio_clipping=0.2, discount=0.99, estimate_terminal=False,\n        # Critic\n        critic_network=None, critic_optimizer=None,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'ppo\',\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n                batch_size=batch_size,\n            network=network, use_beta_distribution=use_beta_distribution,\n            memory=memory,\n            update_frequency=update_frequency, learning_rate=learning_rate,\n                subsampling_fraction=subsampling_fraction, optimization_steps=optimization_steps,\n            likelihood_ratio_clipping=likelihood_ratio_clipping, discount=discount,\n                estimate_terminal=estimate_terminal,\n            critic_network=critic_network, critic_optimizer=critic_optimizer,\n            preprocessing=preprocessing,\n            exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n            name=name, device=device, parallel_interactions=parallel_interactions, seed=seed,\n                execution=execution, saver=saver, summarizer=summarizer, recorder=recorder,\n                config=config\n        )\n\n        policy = dict(network=network, temperature=1.0, use_beta_distribution=use_beta_distribution)\n        if memory is None:\n            memory = dict(type=\'recent\')\n        else:\n            memory = dict(type=\'recent\', capacity=memory)\n        if update_frequency is None:\n            update = dict(unit=\'episodes\', batch_size=batch_size)\n        else:\n            update = dict(unit=\'episodes\', batch_size=batch_size, frequency=update_frequency)\n        optimizer = dict(type=\'adam\', learning_rate=learning_rate)\n        optimizer = dict(\n            type=\'subsampling_step\', optimizer=optimizer, fraction=subsampling_fraction\n        )\n        optimizer = dict(type=\'multi_step\', optimizer=optimizer, num_steps=optimization_steps)\n        objective = dict(\n            type=\'policy_gradient\', ratio_based=True, clipping_value=likelihood_ratio_clipping\n        )\n        if critic_network is None:\n            reward_estimation = dict(horizon=\'episode\', discount=discount)\n            baseline_policy = None\n            assert critic_optimizer is None\n            baseline_objective = None\n        else:\n            reward_estimation = dict(\n                horizon=\'episode\', discount=discount,\n                estimate_horizon=(False if critic_network is None else \'early\'),\n                estimate_terminal=estimate_terminal, estimate_advantage=True\n            )\n            baseline_policy = dict(network=critic_network)\n            assert critic_optimizer is not None\n            baseline_objective = dict(type=\'value\', value=\'state\')\n\n        super().__init__(\n            # Agent\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=True, seed=seed,\n            recorder=recorder, config=config,\n            # Model\n            name=name, device=device, execution=execution, saver=saver, summarizer=summarizer,\n            preprocessing=preprocessing, exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=critic_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n'"
tensorforce/agents/random.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.agents import Agent\nfrom tensorforce.core.models import RandomModel\n\n\nclass RandomAgent(Agent):\n    """"""\n    Agent returning random action values (specification key: `random`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        name (string): Agent name, used e.g. for TensorFlow scopes\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0) &ndash; how frequently in timesteps to record\n            summaries (<span style=""color:#00C000""><b>default</b></span>: always).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all or list of summaries to\n            record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""graph"": graph summary</li>\n            <li>""parameters"": parameter scalars</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Environment\n        self, states, actions, max_episode_timesteps=None,\n        # TensorFlow etc\n        name=\'agent\', device=None, seed=None, summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'random\',\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            name=name, device=device, seed=seed, summarizer=summarizer, recorder=recorder,\n            config=config\n        )\n\n        super().__init__(\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=1, buffer_observe=True, seed=seed, recorder=recorder\n        )\n\n        self.model = RandomModel(\n            # Model\n            name=name, device=device, parallel_interactions=self.parallel_interactions,\n            buffer_observe=self.buffer_observe, seed=seed, summarizer=summarizer, config=config,\n            states=self.states_spec, actions=self.actions_spec,\n        )\n'"
tensorforce/agents/tensorforce.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nimport os\nfrom random import shuffle\n\nimport numpy as np\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.agents import Agent\nfrom tensorforce.core.models import TensorforceModel\n\n\nclass TensorforceAgent(Agent):\n    """"""\n    Tensorforce agent (specification key: `tensorforce`).\n\n    Highly configurable agent and basis for a broad class of deep reinforcement learning agents,\n    which act according to a policy parametrized by a neural network, leverage a memory module for\n    periodic updates based on batches of experience, and optionally employ a baseline/critic/target\n    policy for improved reward estimation.\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        policy (specification): Policy configuration, see [policies](../modules/policies.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""default"", action distributions\n            parametrized by an automatically configured network).\n        memory (int | specification): Memory configuration, see\n            [memories](../modules/memories.html)\n            (<span style=""color:#00C000""><b>default</b></span>: replay memory with either given or\n            minimum capacity).\n        update (int | specification): Model update configuration with the following attributes\n            (<span style=""color:#C00000""><b>required</b>,\n            <span style=""color:#00C000""><b>default</b></span>: timesteps batch size</span>):\n            <ul>\n            <li><b>unit</b> (<i>""timesteps"" | ""episodes""</i>) &ndash; unit for update attributes\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>batch_size</b> (<i>parameter, long > 0</i>) &ndash; size of update batch in\n            number of units (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>""never"" | parameter, long > 0</i>) &ndash; frequency of\n            updates (<span style=""color:#00C000""><b>default</b></span>: batch_size).</li>\n            <li><b>start</b> (<i>parameter, long >= batch_size</i>) &ndash; number of units\n            before first update (<span style=""color:#00C000""><b>default</b></span>: none).</li>\n            </ul>\n        optimizer (specification): Optimizer configuration, see\n            [optimizers](../modules/optimizers.html)\n            (<span style=""color:#00C000""><b>default</b></span>: Adam optimizer).\n        objective (specification): Optimization objective configuration, see\n            [objectives](../modules/objectives.html)\n            (<span style=""color:#C00000""><b>required</b></span>).\n        reward_estimation (specification): Reward estimation configuration with the following\n            attributes (<span style=""color:#C00000""><b>required</b></span>):\n            <ul>\n            <li><b>horizon</b> (<i>""episode"" | parameter, long >= 0</i>) &ndash; Horizon of\n            discounted-sum reward estimation\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>discount</b> (<i>parameter, 0.0 <= float <= 1.0</i>) &ndash; Discount factor for\n            future rewards of discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).</li>\n            <li><b>estimate_horizon</b> (<i>false | ""early"" | ""late""</i>) &ndash; Whether to\n            estimate the value of horizon states, and if so, whether to estimate early when\n            experience is stored, or late when it is retrieved\n            (<span style=""color:#00C000""><b>default</b></span>: ""late"" if any of the baseline_*\n            arguments is specified, else false).</li>\n            <li><b>estimate_actions</b> (<i>bool</i>) &ndash; Whether to estimate state-action\n            values instead of state values\n            (<span style=""color:#00C000""><b>default</b></span>: false).</li>\n            <li><b>estimate_terminal</b> (<i>bool</i>) &ndash; Whether to estimate the value of\n            (real) terminal states (<span style=""color:#00C000""><b>default</b></span>: false).</li>\n            <li><b>estimate_advantage</b> (<i>bool</i>) &ndash; Whether to estimate the advantage\n            by subtracting the current estimate\n            (<span style=""color:#00C000""><b>default</b></span>: false).</li>\n            </ul>\n\n        baseline_policy (specification): Baseline policy configuration, main policy will be used as\n            baseline if none\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        baseline_optimizer (float > 0.0 | specification): Baseline optimizer configuration, see\n            [optimizers](../modules/optimizers.html), main optimizer will be used for baseline if\n            none, a float implies none and specifies a custom weight for the baseline loss\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        baseline_objective (specification): Baseline optimization objective configuration, see\n            [objectives](../modules/objectives.html), main objective will be used for baseline if\n            none (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        buffer_observe (bool | int > 0): Maximum number of timesteps within an episode to buffer\n            before executing internal observe operations, to reduce calls to TensorFlow for\n            improved performance\n            (<span style=""color:#00C000""><b>default</b></span>: max_episode_timesteps or 1000,\n            unless summarizer specified).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Required\n        self, states, actions, update, objective, reward_estimation,\n        # Environment\n        max_episode_timesteps=None,\n        # Agent\n        policy=\'default\', memory=None, optimizer=\'adam\',\n        # Baseline\n        baseline_policy=None, baseline_optimizer=None, baseline_objective=None,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, buffer_observe=True, seed=None,\n        execution=None, saver=None, summarizer=None, recorder=None, config=None\n    ):\n        if not hasattr(self, \'spec\'):\n            self.spec = OrderedDict(\n                agent=\'tensorforce\',\n                states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n                policy=policy, memory=memory, update=update, optimizer=optimizer,\n                objective=objective, reward_estimation=reward_estimation,\n                baseline_policy=baseline_policy, baseline_optimizer=baseline_optimizer,\n                baseline_objective=baseline_objective,\n                preprocessing=preprocessing,\n                exploration=exploration, variable_noise=variable_noise,\n                l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n                name=name, device=device, parallel_interactions=parallel_interactions,\n                buffer_observe=buffer_observe, seed=seed, execution=execution, saver=saver,\n                summarizer=summarizer, recorder=recorder, config=config\n            )\n\n        if isinstance(update, int) or update[\'unit\'] == \'timesteps\':\n            if parallel_interactions > 1:\n                raise TensorforceError.value(\n                    name=\'agent\', argument=\'update\', value=update,\n                    condition=\'parallel_interactions > 1\'\n                )\n            if buffer_observe is not True:\n                raise TensorforceError.invalid(\n                    name=\'agent\', argument=\'buffer_observe\', condition=\'update[unit] = timesteps\'\n                )\n            buffer_observe = False\n\n        if buffer_observe is True and parallel_interactions == 1 and summarizer is not None:\n            buffer_observe = False\n\n        super().__init__(\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=buffer_observe, seed=seed,\n            recorder=recorder\n        )\n\n        if isinstance(update, int):\n            update = dict(unit=\'timesteps\', batch_size=update)\n\n        reward_estimation = dict(reward_estimation)\n        if reward_estimation[\'horizon\'] == \'episode\':\n            if max_episode_timesteps is None:\n                raise TensorforceError.value(\n                    name=\'agent\', argument=\'reward_estimation[horizon]\', value=\'episode\',\n                    condition=\'max_episode_timesteps is None\'\n                )\n            reward_estimation[\'horizon\'] = max_episode_timesteps\n\n        self.model = TensorforceModel(\n            # Model\n            name=name, device=device, parallel_interactions=self.parallel_interactions,\n            buffer_observe=self.buffer_observe, seed=seed, execution=execution, saver=saver,\n            summarizer=summarizer, config=config, states=self.states_spec,\n            actions=self.actions_spec, preprocessing=preprocessing, exploration=exploration,\n            variable_noise=variable_noise, l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=baseline_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization,\n            max_episode_timesteps=max_episode_timesteps\n        )\n\n        self.experience_size = self.model.estimator.capacity\n\n    def experience(\n        self, states, actions, terminal, reward, internals=None, query=None, **kwargs\n    ):\n        """"""\n        Feed experience traces.\n\n        Args:\n            states (dict[array[state]]): Dictionary containing arrays of states\n                (<span style=""color:#C00000""><b>required</b></span>).\n            actions (dict[array[action]]): Dictionary containing arrays of actions\n                (<span style=""color:#C00000""><b>required</b></span>).\n            terminal (array[bool]): Array of terminals\n                (<span style=""color:#C00000""><b>required</b></span>).\n            reward (array[float]): Array of rewards\n                (<span style=""color:#C00000""><b>required</b></span>).\n            internals (dict[state]): Dictionary containing arrays of internal agent states\n                (<span style=""color:#00C000""><b>default</b></span>: no internal states).\n            query (list[str]): Names of tensors to retrieve\n                (<span style=""color:#00C000""><b>default</b></span>: none).\n            kwargs: Additional input values, for instance, for dynamic hyperparameters.\n        """"""\n        assert (self.buffer_indices == 0).all()\n        assert util.reduce_all(predicate=util.not_nan_inf, xs=states)\n        assert internals is None or util.reduce_all(predicate=util.not_nan_inf, xs=internals)\n        assert util.reduce_all(predicate=util.not_nan_inf, xs=actions)\n        assert util.reduce_all(predicate=util.not_nan_inf, xs=reward)\n\n        # Auxiliaries\n        auxiliaries = OrderedDict()\n        if isinstance(states, dict):\n            states = OrderedDict(states)\n            for name, spec in self.actions_spec.items():\n                if spec[\'type\'] == \'int\' and name + \'_mask\' in states:\n                    auxiliaries[name + \'_mask\'] = np.asarray(states.pop(name + \'_mask\'))\n        auxiliaries = util.fmap(function=np.asarray, xs=auxiliaries, depth=1)\n\n        # Normalize states/actions dictionaries\n        states = util.normalize_values(\n            value_type=\'state\', values=states, values_spec=self.states_spec\n        )\n        actions = util.normalize_values(\n            value_type=\'action\', values=actions, values_spec=self.actions_spec\n        )\n        if internals is None:\n            internals = OrderedDict()\n\n        if isinstance(terminal, (bool, int)):\n            states = util.fmap(function=(lambda x: [x]), xs=states, depth=1)\n            internals = util.fmap(function=(lambda x: [x]), xs=internals, depth=1)\n            auxiliaries = util.fmap(function=(lambda x: [x]), xs=auxiliaries, depth=1)\n            actions = util.fmap(function=(lambda x: [x]), xs=actions, depth=1)\n            terminal = [terminal]\n            reward = [reward]\n\n        states = util.fmap(function=np.asarray, xs=states, depth=1)\n        internals = util.fmap(function=np.asarray, xs=internals, depth=1)\n        auxiliaries = util.fmap(function=np.asarray, xs=auxiliaries, depth=1)\n        actions = util.fmap(function=np.asarray, xs=actions, depth=1)\n\n        if isinstance(terminal, np.ndarray):\n            if terminal.dtype is util.np_dtype(dtype=\'bool\'):\n                zeros = np.zeros_like(terminal, dtype=util.np_dtype(dtype=\'long\'))\n                ones = np.ones_like(terminal, dtype=util.np_dtype(dtype=\'long\'))\n                terminal = np.where(terminal, ones, zeros)\n        else:\n            terminal = np.asarray([int(x) if isinstance(x, bool) else x for x in terminal])\n        reward = np.asarray(reward)\n\n        # Batch experiences split into episodes and at most size buffer_observe\n        last = 0\n        for index in range(1, len(terminal) + 1):\n            if terminal[index - 1] == 0 and index - last < self.experience_size:\n                continue\n\n            # Include terminal in batch if possible\n            if index < len(terminal) and terminal[index - 1] == 0 and terminal[index] > 0 and \\\n                    index - last < self.experience_size:\n                index += 1\n\n            function = (lambda x: x[last: index])\n            states_batch = util.fmap(function=function, xs=states, depth=1)\n            internals_batch = util.fmap(function=function, xs=internals, depth=1)\n            auxiliaries_batch = util.fmap(function=function, xs=auxiliaries, depth=1)\n            actions_batch = util.fmap(function=function, xs=actions, depth=1)\n            terminal_batch = terminal[last: index]\n            reward_batch = reward[last: index]\n            last = index\n\n            # Model.experience()\n            if query is None:\n                self.timesteps, self.episodes, self.updates = self.model.experience(\n                    states=states_batch, internals=internals_batch,\n                    auxiliaries=auxiliaries_batch, actions=actions_batch, terminal=terminal_batch,\n                    reward=reward_batch, **kwargs\n                )\n\n            else:\n                self.timesteps, self.episodes, self.updates, queried = self.model.experience(\n                    states=states_batch, internals=internals_batch,\n                    auxiliaries=auxiliaries_batch, actions=actions_batch, terminal=terminal_batch,\n                    reward=reward_batch, query=query, **kwargs\n                )\n\n        if query is not None:\n            return queried\n\n    def update(self, query=None, **kwargs):\n        """"""\n        Perform an update.\n\n        Args:\n            query (list[str]): Names of tensors to retrieve\n                (<span style=""color:#00C000""><b>default</b></span>: none).\n            kwargs: Additional input values, for instance, for dynamic hyperparameters.\n        """"""\n        # Model.update()\n        if query is None:\n            self.timesteps, self.episodes, self.updates = self.model.update(**kwargs)\n\n        else:\n            self.timesteps, self.episodes, self.updates, queried = self.model.update(\n                query=query, **kwargs\n            )\n            return queried\n\n    def pretrain(self, directory, num_iterations, num_traces=1, num_updates=1):\n        """"""\n        Pretrain from experience traces.\n\n        Args:\n            directory (path): Directory with experience traces, e.g. obtained via recorder; episode\n                length has to be consistent with agent configuration\n                (<span style=""color:#C00000""><b>required</b></span>).\n            num_iterations (int > 0): Number of iterations consisting of loading new traces and\n                performing multiple updates\n                (<span style=""color:#C00000""><b>required</b></span>).\n            num_traces (int > 0): Number of traces to load per iteration; has to at least satisfy\n                the update batch size\n                (<span style=""color:#00C000""><b>default</b></span>: 1).\n            num_updates (int > 0): Number of updates per iteration\n                (<span style=""color:#00C000""><b>default</b></span>: 1).\n        """"""\n        if not os.path.isdir(directory):\n            raise TensorforceError.value(\n                name=\'agent.pretrain\', argument=\'directory\', value=directory\n            )\n        files = sorted(\n            os.path.join(directory, f) for f in os.listdir(directory)\n            if os.path.isfile(os.path.join(directory, f)) and f.startswith(\'trace-\')\n        )\n        indices = list(range(len(files)))\n\n        for _ in range(num_iterations):\n            shuffle(indices)\n            if num_traces is None:\n                selection = indices\n            else:\n                selection = indices[:num_traces]\n\n            states = OrderedDict(((name, list()) for name in self.states_spec))\n            for name, spec in self.actions_spec.items():\n                if spec[\'type\'] == \'int\':\n                    states[name + \'_mask\'] = list()\n            actions = OrderedDict(((name, list()) for name in self.actions_spec))\n            terminal = list()\n            reward = list()\n            for index in selection:\n                trace = np.load(files[index])\n                for name in states:\n                    states[name].append(trace[name])\n                for name in actions:\n                    actions[name].append(trace[name])\n                terminal.append(trace[\'terminal\'])\n                reward.append(trace[\'reward\'])\n\n            states = util.fmap(function=np.concatenate, xs=states, depth=1)\n            actions = util.fmap(function=np.concatenate, xs=actions, depth=1)\n            terminal = np.concatenate(terminal)\n            reward = np.concatenate(reward)\n\n            self.experience(states=states, actions=actions, terminal=terminal, reward=reward)\n            for _ in range(num_updates):\n                self.update()\n            # TODO: self.obliviate()\n'"
tensorforce/agents/trpo.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.agents import TensorforceAgent\n\n\nclass TrustRegionPolicyOptimization(TensorforceAgent):\n    """"""\n    [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) agent (specification key:\n    `trpo`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        batch_size (parameter, long > 0): Number of episodes per update batch\n            (<span style=""color:#C00000""><b>required</b></span>).\n\n        network (""auto"" | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"", automatically configured\n            network).\n        use_beta_distribution (bool): Whether to use the Beta distribution for bounded continuous\n            actions by default.\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n\n        memory (int > 0): Batch memory capacity, has to fit at least maximum batch_size + 1 episodes\n            (<span style=""color:#00C000""><b>default</b></span>: minimum capacity, usually does not\n            need to be changed).\n\n        update_frequency (""never"" | parameter, long > 0): Frequency of updates\n            (<span style=""color:#00C000""><b>default</b></span>: batch_size).\n        learning_rate (parameter, float > 0.0): Optimizer learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 1e-3).\n\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 0.99).\n        estimate_terminal (bool): Whether to estimate the value of (real) terminal states\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n\n        critic_network (specification): Critic network configuration, see\n            [networks](../modules/networks.html), main policy will be used as critic if none\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        critic_optimizer (float > 0.0 | specification): Critic optimizer configuration, see\n            [optimizers](../modules/optimizers.html), main optimizer will be used for critic if\n            none, a float implies none and specifies a custom weight for the critic loss\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Required\n        self, states, actions, max_episode_timesteps, batch_size,\n        # Network\n        network=\'auto\', use_beta_distribution=True,\n        # Memory\n        memory=None,\n        # Optimization\n        update_frequency=None, learning_rate=1e-3,\n        # Reward estimation\n        discount=0.99, estimate_terminal=False,\n        # Critic\n        critic_network=None, critic_optimizer=None,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'trpo\',\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n                batch_size=batch_size,\n            network=network, use_beta_distribution=use_beta_distribution,\n            memory=memory,\n            update_frequency=update_frequency, learning_rate=learning_rate,\n            discount=discount, estimate_terminal=estimate_terminal,\n            critic_network=critic_network, critic_optimizer=critic_optimizer,\n            preprocessing=preprocessing,\n            exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n            name=name, device=device, parallel_interactions=parallel_interactions, seed=seed,\n                execution=execution, saver=saver, summarizer=summarizer, recorder=recorder,\n                config=config\n        )\n\n        policy = dict(network=network, temperature=1.0, use_beta_distribution=use_beta_distribution)\n        if memory is None:\n            memory = dict(type=\'recent\')\n        else:\n            memory = dict(type=\'recent\', capacity=memory)\n        if update_frequency is None:\n            update = dict(unit=\'episodes\', batch_size=batch_size)\n        else:\n            update = dict(unit=\'episodes\', batch_size=batch_size, frequency=update_frequency)\n        optimizer = dict(\n            type=\'natural_gradient\', learning_rate=learning_rate, cg_max_iterations=10,\n            cg_damping=1e-3\n        )\n        optimizer = dict(\n            type=\'optimizing_step\', optimizer=optimizer, ls_max_iterations=10, ls_accept_ratio=0.9,\n            ls_mode=\'exponential\', ls_parameter=0.5  # !!!!!!!!!!!!!\n        )\n        objective = dict(type=\'policy_gradient\', ratio_based=True)\n        if critic_network is None:\n            reward_estimation = dict(horizon=\'episode\', discount=discount)\n            baseline_policy = None\n            assert critic_optimizer is None\n            baseline_objective = None\n        else:\n            reward_estimation = dict(\n                horizon=\'episode\', discount=discount,\n                estimate_horizon=(False if critic_network is None else \'early\'),\n                estimate_terminal=estimate_terminal, estimate_advantage=True\n            )\n            baseline_policy = dict(network=critic_network)\n            assert critic_optimizer is not None\n            baseline_objective = dict(type=\'value\', value=\'state\')\n\n        super().__init__(\n            # Agent\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=True, seed=seed,\n            recorder=recorder, config=config,\n            # Model\n            name=name, device=device, execution=execution, saver=saver, summarizer=summarizer,\n            preprocessing=preprocessing, exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=critic_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n'"
tensorforce/agents/vpg.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.agents import TensorforceAgent\n\n\nclass VanillaPolicyGradient(TensorforceAgent):\n    """"""\n    [Vanilla Policy Gradient](https://link.springer.com/article/10.1007/BF00992696) aka REINFORCE\n    agent (specification key: `vpg`).\n\n    Args:\n        states (specification): States specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=""color:#C00000""><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create(...)`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create(...)`).\n\n        batch_size (parameter, long > 0): Number of episodes per update batch\n            (<span style=""color:#C00000""><b>required</b></span>).\n\n        network (""auto"" | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: ""auto"", automatically configured\n            network).\n        use_beta_distribution (bool): Whether to use the Beta distribution for bounded continuous\n            actions by default.\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n\n        memory (int > 0): Batch memory capacity, has to fit at least maximum batch_size + 1 episodes\n            (<span style=""color:#00C000""><b>default</b></span>: minimum capacity, usually does not\n            need to be changed).\n\n        update_frequency (""never"" | parameter, long > 0): Frequency of updates\n            (<span style=""color:#00C000""><b>default</b></span>: batch_size).\n        learning_rate (parameter, float > 0.0): Optimizer learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 3e-4).\n\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#00C000""><b>default</b></span>: 0.99).\n        estimate_terminal (bool): Whether to estimate the value of (real) terminal states\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n\n        baseline_network (specification): Baseline network configuration, see\n            [networks](../modules/networks.html), main policy will be used as baseline if none\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        baseline_optimizer (float > 0.0 | specification): Baseline optimizer configuration, see\n            [optimizers](../modules/optimizers.html), main optimizer will be used for baseline if\n            none, a float implies none and specifies a custom weight for the baseline loss\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        preprocessing (dict[specification]): Preprocessing as layer or list of layers, see\n            [preprocessing](../modules/preprocessing.html), specified per state-type or -name and\n            for reward\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n\n        exploration (parameter | dict[parameter], float >= 0.0): Exploration, global or per action,\n            defined as the probability for uniformly random output in case of `bool` and `int`\n            actions, and the standard deviation of Gaussian noise added to every output in case of\n            `float` actions (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        variable_noise (parameter, float >= 0.0): Standard deviation of Gaussian noise added to all\n            trainable float variables (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        l2_regularization (parameter, float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>:\n            0.0).\n        entropy_regularization (parameter, float >= 0.0): Scalar controlling entropy\n            regularization, to discourage the policy distribution being too ""certain"" / spiked\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n\n        name (string): Agent name, used e.g. for TensorFlow scopes and saver default filename\n            (<span style=""color:#00C000""><b>default</b></span>: ""agent"").\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: TensorFlow default).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or (centrally\n            controlled) agents within an environment\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed to set for Python, NumPy (both set globally!) and TensorFlow,\n            environment seed has to be set separately for a fully deterministic execution\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        execution (specification): TensorFlow execution configuration with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: standard): ...\n        saver (specification): TensorFlow saver configuration for periodic implicit saving, as\n            alternative to explicit saving via agent.save(...), with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; saver directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; model filename\n            (<span style=""color:#00C000""><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in seconds to save the\n            model (<span style=""color:#00C000""><b>default</b></span>: 600 seconds).</li>\n            <li><b>load</b> (<i>bool | str</i>) &ndash; whether to load the existing model, or\n            which model filename to load\n            (<span style=""color:#00C000""><b>default</b></span>: true).</li>\n            </ul>\n            <li><b>max-checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n        summarizer (specification): TensorBoard summarizer configuration with the following\n            attributes (<span style=""color:#00C000""><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0, dict[int > 0]</i>) &ndash; how frequently in\n            timesteps to record summaries for act-summaries if specified globally\n            (<span style=""color:#00C000""><b>default</b></span>: always),\n            otherwise specified for act-summaries via ""act"" in timesteps, for\n            observe/experience-summaries via ""observe""/""experience"" in episodes, and for\n            update/variables-summaries via ""update""/""variables"" in updates\n            (<span style=""color:#00C000""><b>default</b></span>: never).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=""color:#00C000""><b>default</b></span>: 10).</li>\n            <li><b>max-summaries</b> (<i>int > 0</i>) &ndash; maximum number of summaries to keep\n            (<span style=""color:#00C000""><b>default</b></span>: 5).</li>\n            <li><b>custom</b> (<i>dict[spec]</i>) &ndash; custom summaries which are recorded via\n            `agent.summarize(...)`, specification with either type ""scalar"", type ""histogram"" with\n            optional ""buckets"", type ""image"" with optional ""max_outputs""\n            (<span style=""color:#00C000""><b>default</b></span>: 3), or type ""audio""\n            (<span style=""color:#00C000""><b>default</b></span>: no custom summaries).</li>\n            <li><b>labels</b> (<i>""all"" | iter[string]</i>) &ndash; all excluding ""\\*-histogram""\n            labels, or list of summaries to record, from the following labels\n            (<span style=""color:#00C000""><b>default</b></span>: only ""graph""):</li>\n            <li>""distributions"" or ""bernoulli"", ""categorical"", ""gaussian"", ""beta"":\n            distribution-specific parameters</li>\n            <li>""dropout"": dropout zero fraction</li>\n            <li>""entropies"" or ""entropy"", ""action-entropies"": entropy of policy\n            distribution(s)</li>\n            <li>""graph"": graph summary</li>\n            <li>""kl-divergences"" or ""kl-divergence"", ""action-kl-divergences"": KL-divergence of\n            previous and updated polidcy distribution(s)</li>\n            <li>""losses"" or ""loss"", ""objective-loss"", ""regularization-loss"", ""baseline-loss"",\n            ""baseline-objective-loss"", ""baseline-regularization-loss"": loss scalars</li>\n            <li>""parameters"": parameter scalars</li>\n            <li>""relu"": ReLU activation zero fraction</li>\n            <li>""rewards"" or ""timestep-reward"", ""episode-reward"", ""raw-reward"", ""empirical-reward"",\n            ""estimated-reward"": reward scalar\n            </li>\n            <li>""update-norm"": update norm</li>\n            <li>""updates"": update mean and variance scalars</li>\n            <li>""updates-histogram"": update histograms</li>\n            <li>""variables"": variable mean and variance scalars</li>\n            <li>""variables-histogram"": variable histograms</li>\n            </ul>\n        recorder (specification): Experience traces recorder configuration, currently not including\n            internal states, with the following attributes\n            (<span style=""color:#00C000""><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=""color:#00C000""><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=""color:#00C000""><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=""color:#00C000""><b>default</b></span>: all).</li>\n    """"""\n\n    def __init__(\n        # Environment\n        self, states, actions, max_episode_timesteps, batch_size,\n        # Network\n        network=\'auto\', use_beta_distribution=True,\n        # Memory\n        memory=None,\n        # Optimization\n        update_frequency=None, learning_rate=3e-4,\n        # Reward estimation\n        discount=0.99, estimate_terminal=False,\n        # Baseline\n        baseline_network=None, baseline_optimizer=None,\n        # Preprocessing\n        preprocessing=None,\n        # Exploration\n        exploration=0.0, variable_noise=0.0,\n        # Regularization\n        l2_regularization=0.0, entropy_regularization=0.0,\n        # TensorFlow etc\n        name=\'agent\', device=None, parallel_interactions=1, seed=None, execution=None, saver=None,\n        summarizer=None, recorder=None, config=None\n    ):\n        self.spec = OrderedDict(\n            agent=\'vpg\',\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n                batch_size=batch_size,\n            network=network, use_beta_distribution=use_beta_distribution,\n            memory=memory,\n            update_frequency=update_frequency, learning_rate=learning_rate,\n            discount=discount, estimate_terminal=estimate_terminal,\n            baseline_network=baseline_network, baseline_optimizer=baseline_optimizer,\n            preprocessing=preprocessing,\n            exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization, entropy_regularization=entropy_regularization,\n            name=name, device=device, parallel_interactions=parallel_interactions, seed=seed,\n                execution=execution, saver=saver, summarizer=summarizer, recorder=recorder,\n                config=config\n        )\n\n        policy = dict(network=network, temperature=1.0, use_beta_distribution=use_beta_distribution)\n        if memory is None:\n            memory = dict(type=\'recent\')\n        else:\n            memory = dict(type=\'recent\', capacity=memory)\n        if update_frequency is None:\n            update = dict(unit=\'episodes\', batch_size=batch_size)\n        else:\n            update = dict(unit=\'episodes\', batch_size=batch_size, frequency=update_frequency)\n        optimizer = dict(type=\'adam\', learning_rate=learning_rate)\n        objective = \'policy_gradient\'\n        if baseline_network is None:\n            reward_estimation = dict(horizon=\'episode\', discount=discount)\n            baseline_policy = None\n            assert baseline_optimizer is None\n            baseline_objective = None\n        else:\n            reward_estimation = dict(\n                horizon=\'episode\', discount=discount,\n                estimate_horizon=(False if baseline_network is None else \'early\'),\n                estimate_terminal=estimate_terminal, estimate_advantage=True\n            )\n            baseline_policy = dict(network=baseline_network)\n            assert baseline_optimizer is not None\n            baseline_objective = dict(type=\'value\', value=\'state\')\n\n        super().__init__(\n            # Agent\n            states=states, actions=actions, max_episode_timesteps=max_episode_timesteps,\n            parallel_interactions=parallel_interactions, buffer_observe=True, seed=seed,\n            recorder=recorder, config=config,\n            # Model\n            name=name, device=device, execution=execution, saver=saver, summarizer=summarizer,\n            preprocessing=preprocessing, exploration=exploration, variable_noise=variable_noise,\n            l2_regularization=l2_regularization,\n            # TensorforceModel\n            policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline_policy=baseline_policy,\n            baseline_optimizer=baseline_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n'"
tensorforce/core/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.module import Module\nfrom tensorforce.core.parameters import parameter_modules\n\n# Require parameter_modules\nfrom tensorforce.core.layers import layer_modules\nfrom tensorforce.core.memories import memory_modules\nfrom tensorforce.core.objectives import objective_modules\nfrom tensorforce.core.optimizers import optimizer_modules\n\n# Require layer_modules\nfrom tensorforce.core.distributions import distribution_modules\nfrom tensorforce.core.networks import network_modules\n\n# Require network_modules\n\n\n__all__ = [\n    \'distribution_modules\', \'layer_modules\', \'memory_modules\', \'Module\', \'network_modules\',\n    \'optimizer_modules\', \'parameter_modules\'\n]\n'"
tensorforce/core/module.py,67,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nimport importlib\nimport json\nfrom math import sqrt\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\n\n\nclass Module(object):\n    """"""\n    Base class for modules.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    global_scope = None\n    scope_stack = None\n    while_counter = None\n    cond_counter = None\n\n    global_tensors_spec = None\n    global_tensors = None  # per agent, main module, or so\n    queryable_tensors = None\n\n    global_summary_step = None\n\n    @staticmethod\n    def register_tensor(name, spec, batched):\n        if \'/\' in name:\n            raise TensorforceError.value(\n                name=\'Module.register_tensor\', argument=\'name\', value=name, hint=\'contains /\'\n            )\n\n        if Module.global_scope is None:  # ???\n            raise TensorforceError.unexpected()\n\n        scoped_name = name\n\n        # if scoped_name in Module.global_tensors_spec:\n        #     raise TensorforceError(""Global tensor already exists: {}."".format(scoped_name))\n\n        # optional? better to put in spec?\n        spec = dict(spec)\n        spec[\'batched\'] = batched\n\n        if scoped_name in Module.global_tensors_spec and \\\n                spec != Module.global_tensors_spec[scoped_name]:\n            raise TensorforceError.mismatch(\n                name=\'Module.register_tensor\', argument=\'spec\', value1=spec, value2=Module.global_tensors_spec[scoped_name]\n            )\n\n        if not util.valid_value_spec(value_spec=spec):\n            raise TensorforceError.unexpected()\n\n        if \'batched\' in spec and spec[\'batched\'] != batched:\n            raise TensorforceError.unexpected()\n\n        Module.global_tensors_spec[scoped_name] = spec\n\n    @staticmethod\n    def get_tensor_spec(name):\n        if name not in Module.global_tensors_spec:\n            raise TensorforceError.value(\n                name=\'Module.get_tensor_spec\', argument=\'name\', value=name\n            )\n\n        spec = dict(Module.global_tensors_spec[name])\n        spec.pop(\'batched\')\n\n        return spec\n\n    @staticmethod\n    def update_tensor(name, tensor):\n        if name not in Module.global_tensors_spec:\n            raise TensorforceError.value(\n                name=\'Module.update_tensor\', argument=\'name\', value=name\n            )\n\n        scoped_name = name\n        spec = Module.global_tensors_spec[scoped_name]\n\n        if not util.is_consistent_with_value_spec(value_spec=spec, x=tensor):\n            raise TensorforceError.value(\n                name=\'Module.update_tensor\', argument=\'tensor\', value=tensor\n            )\n\n        scoped_name = util.join_scopes(*Module.global_scope, name)\n\n        previous = Module.global_tensors.get(scoped_name)\n        Module.global_tensors[scoped_name] = tensor\n        if Module.cond_counter == 0 and Module.while_counter == 0:\n            Module.queryable_tensors[scoped_name] = tensor\n\n        return previous\n\n    @staticmethod\n    def update_tensors(**kwargs):\n        for name, tensor in kwargs.items():\n            Module.update_tensor(name=name, tensor=tensor)\n\n    @staticmethod\n    def retrieve_tensor(name):\n        if name not in Module.global_tensors_spec:\n            raise TensorforceError.value(\n                name=\'Module.retrieve_tensor\', argument=\'name\', value=name\n            )\n\n        for n in range(len(Module.global_scope) + 1):\n            partial_scope = Module.global_scope[:len(Module.global_scope) - n]\n            scoped_name = util.join_scopes(*partial_scope, name)\n            if scoped_name in Module.global_tensors:\n                break\n        else:\n            raise TensorforceError.value(\n                name=\'Module.retrieve_tensor\', argument=\'name\', value=name\n            )\n\n        return Module.global_tensors[scoped_name]\n\n    is_add_module = False\n\n    # Set internal attributes\n    set_parent = None\n\n    # Inherit arguments\n    inherit_l2_regularization = None\n    inherit_summary_labels = None\n\n    def __init__(self, name, device=None, summary_labels=None, l2_regularization=None):\n        # Internal attributes\n        self.parent = Module.set_parent\n        # self.scope = None\n        self.is_subscope = None\n        self.modules = OrderedDict()\n        self.trainable_modules = OrderedDict()\n        self.saved_modules = OrderedDict()\n        self.is_initialized = False\n        self.variables = None\n        self.trainable_variables = None\n        self.saved_variables = None\n        self.output_tensors = None\n        self.query_tensors = None\n        self.available_summaries = None\n\n        # name\n        if not util.is_valid_name(name=name):\n            raise TensorforceError.value(name=\'module\', argument=\'name\', value=name)\n        # summary_labels\n        if summary_labels is not None and \\\n                not all(isinstance(label, str) for label in summary_labels):\n            raise TensorforceError.value(\n                name=\'module\', argument=\'summary_labels\', value=summary_labels\n            )\n        # device\n        # ???\n\n        self.name = name\n        self.device = device\n        if summary_labels is None:\n            # Otherwise inherit arguments\n            self.summary_labels = Module.inherit_summary_labels\n        elif summary_labels == \'all\':\n            self.summary_labels = summary_labels\n        else:\n            self.summary_labels = set(summary_labels)\n\n        if not Module.is_add_module:\n            Module.global_scope = list()\n            Module.global_tensors_spec = OrderedDict()\n\n        if Module.inherit_l2_regularization is None and l2_regularization is None:\n            self.l2_regularization = None  # otherwise infinite recursion\n        elif l2_regularization is not None:\n            from tensorforce.core import parameter_modules\n            self.l2_regularization = None  # for first module\n            self.l2_regularization = self.add_module(\n                name=\'l2-regularization\', module=l2_regularization, modules=parameter_modules,\n                is_trainable=False, dtype=\'float\', min_value=0.0\n            )\n        else:\n            # Otherwise inherit arguments\n            self.l2_regularization = Module.inherit_l2_regularization\n\n    def tf_initialize(self):\n        pass\n\n    def tf_regularize(self):\n        zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n\n        if len(self.trainable_variables) == 0:\n            regularization_loss = zero\n\n        else:\n            l2_regularization = self.l2_regularization.value()\n\n            def no_l2_regularization():\n                return zero\n\n            def apply_l2_regularization():\n                l2_variables = list()\n                for variable in self.trainable_variables.values():\n                    if variable.dtype != util.tf_dtype(dtype=\'float\'):\n                        variable = tf.dtypes.cast(x=variable, dtype=util.tf_dtype(dtype=\'float\'))\n                    l2_variables.append(tf.reduce_sum(input_tensor=tf.square(x=variable)))\n                return l2_regularization * tf.math.add_n(inputs=l2_variables)\n\n            skip_l2_regularization = tf.math.equal(x=l2_regularization, y=zero)\n            regularization_loss = self.cond(\n                pred=skip_l2_regularization, true_fn=no_l2_regularization,\n                false_fn=apply_l2_regularization\n            )\n\n        for module in self.trainable_modules.values():\n            regularization_loss += module.regularize()\n\n        return regularization_loss\n\n    def initialize(self):\n        # Check whether module is already initialized\n        if self.is_initialized:\n            raise TensorforceError(message=""Module is already initialized."")\n\n        # Set internal attributes\n        self.is_initialized = True\n        self.variables = OrderedDict()\n        self.trainable_variables = OrderedDict()\n        self.saved_variables = OrderedDict()\n        self.output_tensors = dict()\n        self.query_tensors = dict()\n        self.available_summaries = set()\n\n        if self.parent is None:\n            Module.global_scope = list()\n            Module.while_counter = 0\n            Module.cond_counter = 0\n\n        # TensorFlow device and scope\n        Module.global_scope.append(self.name)\n        if self.device is not None:\n            self.device = tf.device(device_name_or_function=self.device)\n            self.device.__enter__()\n        self.scope = tf.name_scope(name=self.name)\n\n        with self.scope:\n            if self.parent is None:\n\n                # Global timestep\n                self.global_timestep = self.add_variable(\n                    name=\'global-timestep\', dtype=\'long\', shape=(), is_trainable=False,\n                    initializer=\'zeros\', shared=\'global-timestep\', \n                )\n                collection = self.graph.get_collection(name=\'global_step\')\n                if len(collection) == 0:\n                    self.graph.add_to_collection(name=\'global_step\', value=self.global_timestep)\n\n                if self.summarizer_spec is not None:\n                    with tf.name_scope(name=\'summarizer\'):\n\n                        directory = self.summarizer_spec[\'directory\']\n                        if os.path.isdir(directory):\n                            directories = sorted(\n                                d for d in os.listdir(directory)\n                                if os.path.isdir(os.path.join(directory, d))\n                                and d.startswith(\'summary-\')\n                            )\n                        else:\n                            os.makedirs(directory)\n                            directories = list()\n                        max_summaries = self.summarizer_spec.get(\'max-summaries\', 5)\n                        if len(directories) > max_summaries - 1:\n                            for subdir in directories[:len(directories) - max_summaries + 1]:\n                                subdir = os.path.join(directory, subdir)\n                                os.remove(os.path.join(subdir, os.listdir(subdir)[0]))\n                                os.rmdir(subdir)\n\n                        logdir = os.path.join(directory, time.strftime(\'summary-%Y%m%d-%H%M%S\'))\n                        flush_millis = (self.summarizer_spec.get(\'flush\', 10) * 1000)\n                        self.summarizer = tf.summary.create_file_writer(\n                            logdir=logdir, max_queue=None, flush_millis=flush_millis,\n                            filename_suffix=None\n                        )\n                        self.summarizer_init = self.summarizer.init()\n                        self.summarizer_flush = self.summarizer.flush()\n                        self.summarizer_close = self.summarizer.close()\n\n                        default_summarizer = self.summarizer.as_default()\n                        default_summarizer.__enter__()\n\n                        if self.summary_labels == \'all\' or \'graph\' in self.summary_labels:\n                            pass\n\n                    Module.global_summary_step = \'timestep\'\n                    condition = tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n                    record_summaries = tf.summary.record_if(condition=condition)\n                    record_summaries.__enter__()\n\n                # Assignment values\n                self.assignment_input = dict(\n                    bool=self.add_placeholder(\n                        name=\'assignment-bool\', dtype=\'bool\', shape=None, batched=False\n                    ), int=self.add_placeholder(\n                        name=\'assignment-int\', dtype=\'int\', shape=None, batched=False\n                    ), long=self.add_placeholder(\n                        name=\'assignment-long\', dtype=\'long\', shape=None, batched=False\n                    ), float=self.add_placeholder(\n                        name=\'assignment-float\', dtype=\'float\', shape=None, batched=False\n                    )\n                )\n\n                # delayed global-timestep assign operation\n                self.global_timestep.assign(\n                    value=self.assignment_input[\'long\'], name=\'global-timestep-assign\'\n                )\n\n                # Global episode\n                self.global_episode = self.add_variable(\n                    name=\'global-episode\', dtype=\'long\', shape=(), is_trainable=False,\n                    initializer=\'zeros\', shared=\'global-episode\'\n                )\n\n                # Global update\n                self.global_update = self.add_variable(\n                    name=\'global-update\', dtype=\'long\', shape=(), is_trainable=False,\n                    initializer=\'zeros\', shared=\'global-update\'\n                )\n\n                Module.global_tensors = OrderedDict(\n                    timestep=self.global_timestep, episode=self.global_episode,\n                    update=self.global_update\n                )\n\n                if self.summarizer_spec is not None:\n                    if len(self.summarizer_spec.get(\'custom\', ())) > 0:\n                        self.summarize_input = self.add_placeholder(\n                            name=\'summarize\', dtype=\'float\', shape=None, batched=False\n                        )\n                        self.summarize_step_input = self.add_placeholder(\n                            name=\'summarize-step\', dtype=\'long\', shape=(), batched=False,\n                            default=tf.identity(input=self.global_timestep)\n                        )\n                        self.custom_summaries = OrderedDict()\n                        for name, summary in self.summarizer_spec[\'custom\'].items():\n                            if summary[\'type\'] == \'audio\':\n                                self.custom_summaries[name] = tf.summary.audio(\n                                    name=name, data=self.summarize_input,\n                                    sample_rate=summary[\'sample_rate\'],\n                                    step=self.summarize_step_input,\n                                    max_outputs=summary.get(\'max_outputs\', 3),\n                                    encoding=summary.get(\'encoding\')\n                                )\n                            elif summary[\'type\'] == \'histogram\':\n                                self.custom_summaries[name] = tf.summary.histogram(\n                                    name=name, data=self.summarize_input,\n                                    step=self.summarize_step_input,\n                                    buckets=summary.get(\'buckets\')\n                                )\n                            elif summary[\'type\'] == \'image\':\n                                self.custom_summaries[name] = tf.summary.image(\n                                    name=name, data=self.summarize_input,\n                                    step=self.summarize_step_input,\n                                    max_outputs=summary.get(\'max_outputs\', 3)\n                                )\n                            elif summary[\'type\'] == \'scalar\':\n                                self.custom_summaries[name] = tf.summary.scalar(\n                                    name=name,\n                                    data=tf.reshape(tensor=self.summarize_input, shape=()),\n                                    step=self.summarize_step_input\n                                )\n                            else:\n                                raise TensorforceError.value(\n                                    name=\'custom summary\', argument=\'type\', value=summary[\'type\'],\n                                    hint=\'not in {audio,histogram,image,scalar}\'\n                                )\n\n                    record_summaries.__exit__(None, None, None)\n\n                    Module.global_summary_step = \'update\'\n                    if \'frequency\' not in self.summarizer_spec or \\\n                            isinstance(self.summarizer_spec[\'frequency\'], int):\n                        condition = tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n\n                    elif \'variables\' in self.summarizer_spec[\'frequency\']:\n                        step = Module.retrieve_tensor(name=Module.global_summary_step)\n                        frequency = tf.constant(\n                            value=self.summarizer_spec[\'frequency\'][\'variables\'],\n                            dtype=util.tf_dtype(dtype=\'long\')\n                        )\n                        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n                        condition = (\n                            lambda: tf.math.equal(x=tf.math.mod(x=step, y=frequency), y=zero)\n                        )\n\n                    else:\n                        condition = tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\'))\n\n                    record_summaries = tf.summary.record_if(condition=condition)\n                    record_summaries.__enter__()\n\n            for module in self.modules.values():\n                module.initialize()\n            self.tf_initialize()\n\n            if self.parent is None and self.summarizer_spec is not None:\n                record_summaries.__exit__(None, None, None)\n                Module.global_summary_step = None\n\n        self.scope = None\n        if self.device is not None:\n            self.device.__exit__(None, None, None)\n        Module.global_scope.pop()\n\n        if self.parent is None:\n            assert len(Module.global_scope) == 0\n            Module.global_scope = None\n            Module.while_counter = None\n            Module.cond_counter = None\n            Module.global_tensors = None\n\n        # Internal TensorFlow functions, prefixed by \'tf_\'\n        for attribute in sorted(dir(self)):\n            if attribute.startswith(\'tf_\') and attribute != \'tf_initialize\':\n                function_name = attribute[3:]\n\n                if not util.is_valid_name(name=function_name):\n                    raise TensorforceError.unexpected()\n                if hasattr(self, function_name):\n                    raise TensorforceError.unexpected()\n\n                tf_function = getattr(self, attribute)\n                if not callable(tf_function):\n                    raise TensorforceError.unexpected()\n\n                function = self.create_tf_function(\n                    name=\'{}.{}\'.format(self.name, function_name), tf_function=tf_function\n                )\n\n                setattr(self, function_name, function)\n\n        #  API TensorFlow functions, prefixed by \'api_\'\n        for attribute in sorted(dir(self)):\n            if attribute.startswith(\'api_\'):\n                function_name = attribute[4:]\n                assert hasattr(self, \'config\')\n                if self.config is not None and \'api_functions\' in self.config and \\\n                        function_name not in self.config[\'api_functions\']:\n                    continue\n\n                if function_name in (\'act\', \'independent_act\'):\n                    Module.global_summary_step = \'timestep\'\n                elif function_name in (\'observe\', \'experience\'):\n                    Module.global_summary_step = \'episode\'\n                elif function_name == \'update\':\n                    Module.global_summary_step = \'update\'\n\n                if self.summarizer_spec is not None:\n                    if \'frequency\' not in self.summarizer_spec:\n                        condition = tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n\n                    elif isinstance(self.summarizer_spec[\'frequency\'], int):\n                        if function_name in (\'act\', \'independent_act\'):\n                            step = self.global_timestep\n                            frequency = tf.constant(\n                                value=self.summarizer_spec[\'frequency\'],\n                                dtype=util.tf_dtype(dtype=\'long\')\n                            )\n                            zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n                            condition = (\n                                lambda: tf.math.equal(x=tf.math.mod(x=step, y=frequency), y=zero)\n                            )\n                        elif function_name in (\'reset\', \'independent_act\'):\n                            condition = tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\'))\n                        else:\n                            condition = tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n\n                    elif function_name in self.summarizer_spec[\'frequency\']:\n                        if function_name in (\'act\', \'independent_act\'):\n                            step = self.global_timestep\n                        elif function_name in (\'observe\', \'experience\'):\n                            step = self.global_episode\n                        elif function_name == \'update\':\n                            step = self.global_update\n                        elif function_name == \'reset\':\n                            raise TensorforceError.value(\n                                name=\'module\', argument=\'summarizer[frequency]\',\n                                value=function_name,\n                                hint=\'not in {act,experience,observe,update}\'\n                            )\n                        else:\n                            raise TensorforceError.value(\n                                name=\'module\', argument=\'summarizer[frequency]\',\n                                value=function_name,\n                                hint=\'not in {act,experience,observe,update}\'\n                            )\n                        frequency = tf.constant(\n                            value=self.summarizer_spec[\'frequency\'][function_name],\n                            dtype=util.tf_dtype(dtype=\'long\')\n                        )\n                        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n                        condition = (\n                            lambda: tf.math.equal(x=tf.math.mod(x=step, y=frequency), y=zero)\n                        )\n\n                    else:\n                        condition = tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\'))\n\n                    record_summaries = tf.summary.record_if(condition=condition)\n                    record_summaries.__enter__()\n\n                if not util.is_valid_name(name=function_name):\n                    raise TensorforceError.unexpected()\n                if hasattr(self, function_name):\n                    raise TensorforceError.unexpected()\n\n                api_function = getattr(self, attribute)\n                if not callable(api_function):\n                    raise TensorforceError.unexpected()\n\n                function = self.create_api_function(\n                    name=\'{}.{}\'.format(self.name, function_name), api_function=api_function\n                )\n\n                setattr(self, function_name, function)\n\n                if self.summarizer_spec is not None:\n                    record_summaries.__exit__(None, None, None)\n                    Module.global_summary_step = None\n\n        if self.parent is None:\n            self.graph_summary = None  # TODO!\n            if self.summarizer_spec is not None:\n                default_summarizer.__exit__(None, None, None)\n\n    def create_tf_function(self, name, tf_function):\n        # Call internal TensorFlow function\n        def fn(*args, **kwargs):\n            if self.is_subscope:\n                Module.global_scope.append(self.name)\n            if self.device is not None:\n                self.device.__enter__()\n            scope = tf.name_scope(name=name)\n            Module.scope_stack.append(scope)\n            scope.__enter__()\n            results = tf_function(*args, **kwargs)\n            scope.__exit__(None, None, None)\n            Module.scope_stack.pop()\n            if self.device is not None:\n                self.device.__exit__(None, None, None)\n            if self.is_subscope:\n                Module.global_scope.pop()\n            return results\n\n        return fn\n\n    def create_api_function(self, name, api_function):\n        # Call API TensorFlow function\n        Module.global_scope = list()\n        Module.scope_stack = list()\n        Module.while_counter = 0\n        Module.cond_counter = 0\n        Module.global_tensors = OrderedDict()\n        Module.queryable_tensors = OrderedDict()\n\n        if self.device is not None:\n            self.device.__enter__()\n        scope = tf.name_scope(name=name)\n        Module.scope_stack.append(scope)\n        scope.__enter__()\n\n        results = api_function()\n        assert all(x.name.endswith(\'-output:0\') for x in util.flatten(xs=results))\n        self.output_tensors[name[name.index(\'.\') + 1:]] = [\n            x.name[len(name) + 1: -9] for x in util.flatten(xs=results)\n        ]\n\n        # Function-level identity operation for retrieval\n        query_tensors = set()\n        for scoped_name, tensor in Module.queryable_tensors.items():\n            tensor = util.identity_operation(x=tensor, operation_name=(scoped_name + \'-query\'))\n            assert tensor.name.endswith(\'-query:0\')\n            assert scoped_name not in query_tensors\n            query_tensors.add(scoped_name)\n        self.query_tensors[name[name.index(\'.\') + 1:]] = sorted(query_tensors)\n\n        scope.__exit__(None, None, None)\n        Module.scope_stack.pop()\n        if self.device is not None:\n            self.device.__exit__(None, None, None)\n\n        assert len(Module.global_scope) == 0\n        Module.global_scope = None\n        assert len(Module.scope_stack) == 0\n        Module.scope_stack = None\n        Module.while_counter = None\n        Module.cond_counter = None\n        Module.global_tensors = None\n        Module.queryable_tensors = None\n\n        def fn(query=None, **kwargs):\n            # Feed_dict dictionary\n            feed_dict = dict()\n            for key, arg in kwargs.items():\n                if arg is None:\n                    continue\n                elif isinstance(arg, dict):\n                    # Support single nesting (for states, internals, actions)\n                    for key, arg in arg.items():\n                        feed_dict[util.join_scopes(self.name, key) + \'-input:0\'] = arg\n                else:\n                    feed_dict[util.join_scopes(self.name, key) + \'-input:0\'] = arg\n            if not all(isinstance(x, str) and x.endswith(\'-input:0\') for x in feed_dict):\n                raise TensorforceError.value(\n                    name=api_function, argument=\'inputs\', value=list(feed_dict)\n                )\n\n            # Fetches value/tuple\n            fetches = util.fmap(function=(lambda x: x.name), xs=results)\n            if query is not None:\n                # If additional tensors are to be fetched\n                query = util.fmap(\n                    function=(lambda x: util.join_scopes(name, x) + \'-query:0\'), xs=query\n                )\n                if util.is_iterable(x=fetches):\n                    fetches = tuple(fetches) + (query,)\n                else:\n                    fetches = (fetches, query)\n            if not util.reduce_all(\n                predicate=(lambda x: x.endswith(\'-output:0\') or x.endswith(\'-query:0\')), xs=fetches\n            ):\n                raise TensorforceError.value(\n                    name=api_function, argument=\'outputs\', value=list(fetches)\n                )\n\n            # TensorFlow session call\n            fetched = self.monitored_session.run(fetches=fetches, feed_dict=feed_dict)\n\n            return fetched\n\n        return fn\n\n    def cond(self, pred, true_fn, false_fn):\n\n        def true_fn_wrapper():\n            for scope in Module.scope_stack:\n                scope.__enter__()\n            result = true_fn()\n            for scope in reversed(Module.scope_stack):\n                scope.__exit__(None, None, None)\n            return result\n\n        def false_fn_wrapper():\n            for scope in Module.scope_stack:\n                scope.__enter__()\n            result = false_fn()\n            for scope in reversed(Module.scope_stack):\n                scope.__exit__(None, None, None)\n            return result\n\n        Module.cond_counter += 1\n        x = tf.cond(pred=pred, true_fn=true_fn_wrapper, false_fn=false_fn_wrapper)\n        Module.cond_counter -= 1\n        return x\n\n    def while_loop(\n        self, cond, body, loop_vars, shape_invariants=None, parallel_iterations=10,\n        back_prop=False, swap_memory=False, maximum_iterations=None\n    ):\n        Module.while_counter += 1\n        if maximum_iterations is not None and maximum_iterations.dtype is not tf.int32:\n            maximum_iterations = tf.dtypes.cast(x=maximum_iterations, dtype=tf.int32)\n        x = tf.while_loop(\n            cond=cond, body=body, loop_vars=loop_vars, shape_invariants=shape_invariants,\n            parallel_iterations=parallel_iterations, back_prop=back_prop,\n            swap_memory=swap_memory, maximum_iterations=maximum_iterations\n        )\n        Module.while_counter -= 1\n        return x\n\n    def add_variable(\n        self, name, dtype, shape, is_trainable, initializer=\'zeros\', is_saved=True, summarize=None,\n        shared=None\n    ):\n        # name\n        if not util.is_valid_name(name=name):\n            raise TensorforceError.value(name=\'Module.add_variable\', argument=\'name\', value=name)\n        elif name in self.variables:\n            raise TensorforceError.exists(name=\'variable\', value=name)\n        # dtype\n        if not util.is_valid_type(dtype=dtype):\n            raise TensorforceError.value(name=\'Module.add_variable\', argument=\'dtype\', value=dtype)\n        # shape\n        if not util.is_iterable(x=shape) or not all(isinstance(dims, int) for dims in shape):\n            raise TensorforceError.value(name=\'Module.add_variable\', argument=\'shape\', value=shape)\n        elif not all(dims > 0 for dims in shape):\n            raise TensorforceError.value(name=\'Module.add_variable\', argument=\'shape\', value=shape)\n        # is_trainable\n        if not isinstance(is_trainable, bool):\n            raise TensorforceError.type(\n                name=\'Module.add_variable\', argument=\'is_trainable\', dtype=type(is_trainable)\n            )\n        elif is_trainable and dtype != \'float\':\n            raise TensorforceError.value(\n                name=\'Module.add_variable\', argument=\'is_trainable\', value=is_trainable,\n                condition=\'dtype != float\'\n            )\n        # initializer\n        initializer_names = (\n            \'normal\', \'normal-relu\', \'orthogonal\', \'orthogonal-relu\', \'zeros\', \'ones\'\n        )\n        if not isinstance(initializer, (util.py_dtype(dtype=dtype), np.ndarray, tf.Tensor)) and \\\n                initializer not in initializer_names:\n            raise TensorforceError.value(\n                name=\'Module.add_variable\', argument=\'initializer\', value=initializer\n            )\n        elif isinstance(initializer, np.ndarray) and \\\n                initializer.dtype != util.np_dtype(dtype=dtype):\n            raise TensorforceError.type(\n                name=\'Module.add_variable\', argument=\'initializer\', dtype=type(initializer)\n            )\n        elif isinstance(initializer, tf.Tensor) and util.dtype(x=initializer) != dtype:\n            raise TensorforceError.type(\n                name=\'Module.add_variable\', argument=\'initializer\', dtype=type(initializer)\n            )\n        # is_saved\n        if not isinstance(is_saved, bool):\n            raise TensorforceError.type(\n                name=\'Module.add_variable\', argument=\'is_saved\', dtype=type(is_saved)\n            )\n        # summarize\n        if summarize is not None and not isinstance(summarize, bool):\n            raise TensorforceError.type(\n                name=\'Module.add_variable\', argument=\'summarize\', dtype=type(summarize)\n            )\n        # shared\n        if shared is not None and not isinstance(shared, str):\n            raise TensorforceError.type(\n                name=\'Module.add_variable\', argument=\'shared\',dtype=type(shared)\n            )\n\n        variable = None\n\n        if shared is not None and len(self.graph.get_collection(name=shared)) > 0:\n            # Retrieve shared variable from TensorFlow\n            collection = self.graph.get_collection(name=shared)\n            if len(collection) > 1:\n                raise TensorforceError.unexpected()\n            variable = collection[0]\n\n        else:\n            tf_dtype = util.tf_dtype(dtype=dtype)\n\n            # Variable initializer\n            if isinstance(initializer, util.py_dtype(dtype=dtype)):\n                initializer = tf.constant(value=initializer, dtype=tf_dtype, shape=shape)\n            elif isinstance(initializer, np.ndarray):\n                if initializer.shape != shape:\n                    raise TensorforceError.mismatch(\n                        name=\'Module.add_variable\', value1=\'shape\', value2=\'initializer\'\n                    )\n                initializer = tf.constant(value=initializer, dtype=tf_dtype)\n            elif isinstance(initializer, tf.Tensor):\n                if util.shape(x=initializer) != shape:\n                    raise TensorforceError.mismatch(\n                        name=\'Module.add_variable\', value1=\'shape\', value2=\'initializer\'\n                    )\n                initializer = initializer\n            elif not isinstance(initializer, str):\n                raise TensorforceError(""Invalid variable initializer: {}"".format(initializer))\n            elif initializer[:6] == \'normal\':\n                if dtype != \'float\':\n                    raise TensorforceError(\n                        message=""Invalid variable initializer value for non-float variable: {}."".format(\n                            initializer\n                        )\n                    )\n                if initializer[6:] == \'-relu\':\n                    stddev = min(0.1, sqrt(2.0 / util.product(xs=shape[:-1])))\n                else:\n                    stddev = min(0.1, sqrt(2.0 / (util.product(xs=shape[:-1]) + shape[-1])))\n                initializer = tf.random.normal(shape=shape, stddev=stddev, dtype=tf_dtype)\n            elif initializer[:10] == \'orthogonal\':\n                if dtype != \'float\':\n                    raise TensorforceError(\n                        message=""Invalid variable initializer value for non-float variable: {}."".format(\n                            initializer\n                        )\n                    )\n                if len(shape) < 2:\n                    raise TensorforceError(\n                        message=""Invalid variable initializer value for 0/1-rank variable: {}."".format(\n                            initializer\n                        )\n                    )\n                normal = np.random.normal(size=(util.product(xs=shape[:-1]), shape[-1]))\n                u, _, v = np.linalg.svd(a=normal, full_matrices=False)\n                orthogonal = u if u.shape[1] == shape[-1] else v\n                if initializer[10:] == \'-relu\':\n                    orthogonal = orthogonal * sqrt(2.0)\n                initializer = tf.constant(value=orthogonal.reshape(shape), dtype=tf_dtype)\n            elif initializer == \'zeros\':\n                initializer = tf.zeros(shape=shape, dtype=tf_dtype)\n            elif initializer == \'ones\':\n                initializer = tf.ones(shape=shape, dtype=tf_dtype)\n\n            # Variable\n            variable = tf.Variable(\n                initial_value=initializer, trainable=is_trainable, validate_shape=True, name=name,\n                dtype=tf_dtype, shape=shape\n            )\n\n            # Register shared variable with TensorFlow\n            if shared is not None:\n                self.graph.add_to_collection(name=shared, value=variable)\n\n        # Register variable\n        self.variables[name] = variable\n        if is_trainable:\n            self.trainable_variables[name] = variable\n        if is_saved:\n            self.saved_variables[name] = variable\n\n        # Add summary\n        if (summarize is None and is_trainable) or summarize:\n            variable = self.add_summary(\n                label=\'variables\', name=name, tensor=variable, mean_variance=True\n            )\n            variable = self.add_summary(label=\'variables-histogram\', name=name, tensor=variable)\n\n        # get/assign operation (delayed for global-timestep)\n        util.identity_operation(x=variable, operation_name=(name + \'-output\'))\n        if name != \'global-timestep\':\n            parent = self\n            while parent.parent is not None:\n                parent = parent.parent\n            variable.assign(value=parent.assignment_input[dtype], name=(name + \'-assign\'))\n\n        return variable\n\n    def add_placeholder(self, name, dtype, shape, batched, default=None):\n        # name\n        name = name + \'-input\'\n        if not util.is_valid_name(name=name):\n            raise TensorforceError.value(name=\'placeholder\', argument=\'name\', value=name)\n        # dtype\n        if not util.is_valid_type(dtype=dtype):\n            raise TensorforceError.value(name=\'placeholder\', argument=\'dtype\', value=dtype)\n        # shape\n        if shape is not None and (\n            not util.is_iterable(x=shape) or not all(isinstance(dims, int) for dims in shape)\n        ):\n            raise TensorforceError.type(name=\'placeholder\', argument=\'shape\', value=shape)\n        elif shape is not None and not all(dims > 0 for dims in shape):\n            raise TensorforceError.value(name=\'placeholder\', argument=\'shape\', value=shape)\n        # batched\n        if not isinstance(batched, bool):\n            raise TensorforceError.type(name=\'placeholder\', argument=\'batched\', value=batched)\n        # default\n        if default is not None:\n            # if batched:\n            #     raise TensorforceError.unexpected()\n            if not isinstance(default, tf.Tensor):\n                raise TensorforceError.unexpected()\n            elif not util.is_dtype(x=default, dtype=dtype):\n                raise TensorforceError.unexpected()\n\n        # Placeholder\n        if shape is None:\n            assert not batched\n        elif batched:\n            shape = (None,) + shape\n        if default is None:\n            dtype = util.tf_dtype(dtype=dtype)\n            placeholder = tf.compat.v1.placeholder(dtype=dtype, shape=shape, name=name)\n        else:\n            # check dtype and shape !!!\n            assert shape is not None\n            placeholder = tf.compat.v1.placeholder_with_default(\n                input=default, shape=shape, name=name\n            )\n\n        return placeholder\n\n    def is_summary_logged(self, label):\n        # Check whether any summaries are logged\n        if self.summary_labels is None:\n            return False\n\n        # Check whether not in while loop\n        if Module.while_counter > 0:\n            return False\n        # Check whether not in nested condition\n        if Module.cond_counter > 1:\n            return False\n\n        # Temporary\n        if label == \'variables\' or label == \'variables-histogram\':\n            return False\n\n        # Check whether given label is logged\n        if util.is_iterable(x=label):\n            assert all(not x.endswith(\'-histogram\') for x in label)\n            if self.summary_labels != \'all\' and all(x not in self.summary_labels for x in label):\n                return False\n        else:\n            if (self.summary_labels != \'all\' or label.endswith(\'-histogram\')) and \\\n                    label not in self.summary_labels:\n                return False\n\n        return True\n\n    def add_summary(\n        self, label, name, tensor, pass_tensors=None, return_summaries=False, mean_variance=False,\n        enumerate_last_rank=False\n    ):\n        # should be ""labels"" !!!\n        # label\n        if util.is_iterable(x=label):\n            if not all(isinstance(x, str) for x in label):\n                raise TensorforceError.value(\n                    name=\'Module.add_summary\', argument=\'label\', value=label\n                )\n        else:\n            if not isinstance(label, str):\n                raise TensorforceError.type(\n                    name=\'Module.add_summary\', argument=\'label\', dtype=type(label)\n                )\n        # name\n        if not isinstance(name, str):\n            raise TensorforceError.type(\n                name=\'Module.add_summary\', argument=\'name\', dtype=type(name)\n            )\n        # tensor\n        if not isinstance(tensor, (tf.Tensor, tf.Variable)):\n            raise TensorforceError.type(\n                name=\'Module.add_summary\', argument=\'tensor\', dtype=type(tensor)\n            )\n        # pass_tensors\n        if util.is_iterable(x=pass_tensors):\n            if not all(isinstance(x, (tf.Tensor, tf.IndexedSlices)) for x in pass_tensors):\n                raise TensorforceError.value(\n                    name=\'Module.add_summary\', argument=\'pass_tensors\', value=pass_tensors\n                )\n        elif pass_tensors is not None:\n            if not isinstance(pass_tensors, tf.Tensor):\n                raise TensorforceError.type(\n                    name=\'Module.add_summary\', argument=\'pass_tensors\', dtype=type(pass_tensors)\n                )\n        # enumerate_last_rank\n        if not isinstance(enumerate_last_rank, bool):\n            raise TensorforceError.type(\n                name=\'Module.add_summary\', argument=\'enumerate_last_rank\', dtype=type(tensor)\n            )\n\n        if pass_tensors is None:\n            pass_tensors = tensor\n\n        # Check whether summary is logged\n        if not self.is_summary_logged(label=label):\n            return pass_tensors\n\n        # Add to available summaries\n        if util.is_iterable(x=label):\n            self.available_summaries.update(label)\n        else:\n            self.available_summaries.add(label)\n\n        # Handle enumerate_last_rank\n        if enumerate_last_rank:\n            dims = util.shape(x=tensor)[-1]\n            tensors = OrderedDict([(name + str(n), tensor[..., n]) for n in range(dims)])\n        else:\n            tensors = OrderedDict([(name, tensor)])\n\n        if mean_variance:\n            for name in list(tensors):\n                tensor = tensors.pop(name)\n                mean, variance = tf.nn.moments(x=tensor, axes=tuple(range(util.rank(x=tensor))))\n                tensors[name + \'-mean\'] = mean\n                tensors[name + \'-variance\'] = variance\n\n        # Scope handling\n        if Module.scope_stack is not None:\n            for scope in reversed(Module.scope_stack[1:]):\n                scope.__exit__(None, None, None)\n            if len(Module.global_scope) > 0:\n                temp_scope = tf.name_scope(name=\'/\'.join(Module.global_scope))\n                temp_scope.__enter__()\n            tensors = util.fmap(function=util.identity_operation, xs=tensors)\n\n        # TensorFlow summaries\n        assert Module.global_summary_step is not None\n        step = Module.retrieve_tensor(name=Module.global_summary_step)\n        summaries = list()\n        for name, tensor in tensors.items():\n            shape = util.shape(x=tensor)\n            if shape == ():\n                summaries.append(tf.summary.scalar(name=name, data=tensor, step=step))\n            elif shape == (-1,):\n                tensor = tf.math.reduce_sum(input_tensor=tensor, axis=0)\n                summaries.append(tf.summary.scalar(name=name, data=tensor, step=step))\n            elif shape == (1,):\n                tensor = tf.squeeze(input=tensor, axis=-1)\n                summaries.append(tf.summary.scalar(name=name, data=tensor, step=step))\n            elif shape == (-1, 1):\n                tensor = tf.math.reduce_sum(input_tensor=tf.squeeze(input=tensor, axis=-1), axis=0)\n                summaries.append(tf.summary.scalar(name=name, data=tensor, step=step))\n            else:\n                # General tensor as histogram\n                assert not util.is_iterable(x=label) and label.endswith(\'-histogram\')\n                summaries.append(tf.summary.histogram(name=name, data=tensor, step=step))\n\n        # Scope handling\n        if Module.scope_stack is not None:\n            if len(Module.global_scope) > 0:\n                temp_scope.__exit__(None, None, None)\n            for scope in Module.scope_stack[1:]:\n                scope.__enter__()\n\n        with tf.control_dependencies(control_inputs=summaries):\n            return util.fmap(function=util.identity_operation, xs=pass_tensors)\n\n    @staticmethod\n    def get_module_class_and_kwargs(\n        name, module=None, modules=None, default_module=None, disable_first_arg=False, **kwargs\n    ):\n        # name\n        if not util.is_valid_name(name=name):\n            raise TensorforceError.value(name=\'Module.add_module\', argument=\'name\', value=name)\n        # module\n        # ???\n        # modules\n        if modules is not None and not isinstance(modules, dict):\n            raise TensorforceError.type(\n                name=\'Module.add_module\', argument=\'modules\', dtype=type(modules)\n            )\n        # default_module\n        # ???\n        if isinstance(module, dict):\n            # Dictionary module specification (type either given via \'type\' or \'default_module\')\n            util.deep_disjoint_update(target=kwargs, source=module)\n            module = kwargs.pop(\'type\', default_module)\n            return Module.get_module_class_and_kwargs(\n                name=name, module=module, modules=modules, default_module=default_module,\n                disable_first_arg=True, **kwargs\n            )\n\n        elif isinstance(module, str):\n            if os.path.isfile(module):\n                # JSON file module specification\n                with open(module, \'r\') as fp:\n                    module = json.load(fp=fp)\n                return Module.get_module_class_and_kwargs(\n                    name=name, module=module, modules=modules, default_module=default_module,\n                    disable_first_arg=True, **kwargs\n                )\n\n            elif \'.\' in module:\n                # Library module specification\n                library_name, module_name = module.rsplit(\'.\', 1)\n                library = importlib.import_module(name=library_name)\n                module = getattr(library, module_name)\n                return Module.get_module_class_and_kwargs(\n                    name=name, module=module, modules=modules, default_module=default_module,\n                    disable_first_arg=True, **kwargs\n                )\n\n            elif modules is not None and module in modules:\n                # Keyword module specification\n                return Module.get_module_class_and_kwargs(\n                    name=name, module=modules[module], default_module=default_module,\n                    disable_first_arg=True, **kwargs\n                )\n\n            elif \'default\' in modules or default_module is not None:\n                # Default module specification\n                if \'_first_arg\' in kwargs:\n                    raise TensorforceError.invalid(name=\'Module.add_module\', argument=\'_first_arg\')\n                if module is not None:\n                    if disable_first_arg:\n                        raise TensorforceError.value(\n                            name=\'Module.add_module\', argument=\'module\', value=module\n                        )\n                    kwargs[\'_first_arg\'] = module\n                if default_module is None:\n                    default_module = modules[\'default\']\n                return Module.get_module_class_and_kwargs(\n                    name=name, module=default_module, modules=modules, **kwargs\n                )\n\n            else:\n                raise TensorforceError.value(\n                    name=\'Module.add_module\', argument=\'module\', value=module\n                )\n\n        elif not callable(module) and (\'default\' in modules or default_module is not None):\n            # Default module specification\n            if \'_first_arg\' in kwargs:\n                raise TensorforceError.invalid(name=\'Module.add_module\', argument=\'_first_arg\')\n            if module is not None:\n                kwargs[\'_first_arg\'] = module\n            if default_module is None:\n                default_module = modules[\'default\']\n            return Module.get_module_class_and_kwargs(\n                name=name, module=default_module, modules=modules, **kwargs\n            )\n\n        elif callable(module):\n            # for key, arg in kwargs.items():\n            #     assert arg is not None, (key, arg)\n            #     if arg is None:\n            #         assert False\n            #         kwargs.pop(key)\n            first_arg = kwargs.pop(\'_first_arg\', None)\n            return module, first_arg, kwargs\n\n        else:\n            raise TensorforceError.value(name=\'Module.add_module\', argument=\'module\', value=module)\n\n    def add_module(\n        self, name, module=None, modules=None, default_module=None, is_trainable=True,\n        is_saved=True, is_subscope=False, **kwargs\n    ):\n        # name\n        if name in self.modules:\n            raise TensorforceError.exists(name=\'module\', value=name)\n        # is_trainable\n        if not isinstance(is_trainable, bool):\n            raise TensorforceError.exists(\n                name=\'Module.add_module\', argument=\'is_trainable\', dtype=type(name)\n            )\n        # is_saved\n        if not isinstance(is_saved, bool):\n            raise TensorforceError.type(\n                name=\'Module.add_module\', argument=\'is_saved\', dtype=type(is_saved)\n            )\n\n        module_cls, first_arg, kwargs = Module.get_module_class_and_kwargs(\n            name=name, module=module, modules=modules, default_module=default_module, **kwargs\n        )\n\n        # Final callable module specification\n        if Module.global_scope is None:\n            raise TensorforceError.unexpected()\n\n        # Global scope handling\n        Module.is_add_module = True\n        if is_subscope:\n            Module.global_scope.append(name)\n\n        # Set internal attributes\n        Module.set_parent = self\n\n        # Inherit arguments\n        Module.inherit_l2_regularization = self.l2_regularization\n        Module.inherit_summary_labels = self.summary_labels\n\n        # Module constructor\n        if first_arg is None:\n            module = module_cls(name, **kwargs)\n        else:\n            module = module_cls(name, first_arg, **kwargs)\n\n        # Reset\n        Module.set_parent = None\n        Module.inherit_l2_regularization = None\n        Module.inherit_summary_labels = None\n\n        # Global scope handling\n        if is_subscope:\n            Module.global_scope.pop()\n        Module.is_add_module = False\n\n        # Internal attributes\n        module.is_subscope = is_subscope\n\n        # Register module\n        self.modules[name] = module\n        if is_trainable:\n            self.trainable_modules[name] = module\n        if is_saved:\n            self.saved_modules[name] = module\n\n        return module\n\n    def get_variables(self, only_trainable=False, only_saved=False):\n        # only_trainable\n        if not isinstance(only_trainable, bool):\n            raise TensorforceError.type(\n                name=\'Module.get_variables\', argument=\'only_trainable\', dtype=type(only_trainable)\n            )\n        # only_saved\n        if not isinstance(only_saved, bool):\n            raise TensorforceError.type(\n                name=\'Module.get_variables\', argument=\'only_saved\', dtype=type(only_saved)\n            )\n        # not both\n        if only_trainable and only_saved:\n            raise TensorforceError(\n                message=""Module.get_variables argument only_trainable should not be used together ""\n                        ""with argument only_saved.""\n            )\n\n        if only_trainable:\n            # Only trainable variables\n            variables = list(self.trainable_variables.values())\n            for module in self.trainable_modules.values():\n                variables.extend(module.get_variables(only_trainable=only_trainable))\n\n        elif only_saved:\n            # Only saved variables\n            variables = list(self.saved_variables.values())\n            for module in self.saved_modules.values():\n                variables.extend(module.get_variables(only_saved=only_saved))\n\n        else:\n            # All variables\n            variables = list(self.variables.values())\n            for module in self.modules.values():\n                variables.extend(module.get_variables(only_trainable=only_trainable))\n\n        return variables\n\n    def get_available_summaries(self):\n        summaries = set(self.available_summaries)\n        for module in self.modules.values():\n            summaries.update(module.get_available_summaries())\n        return sorted(summaries)\n'"
tensorforce/environments/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.environments.environment import Environment, RemoteEnvironment\n\nfrom tensorforce.environments.multiplayer_environment import MultiplayerEnvironment\nfrom tensorforce.environments.multiprocessing_environment import MultiprocessingEnvironment\nfrom tensorforce.environments.socket_environment import SocketEnvironment\n\nfrom tensorforce.environments.arcade_learning_environment import ArcadeLearningEnvironment\nfrom tensorforce.environments.maze_explorer import MazeExplorer\nfrom tensorforce.environments.openai_gym import OpenAIGym\nfrom tensorforce.environments.openai_retro import OpenAIRetro\nfrom tensorforce.environments.open_sim import OpenSim\nfrom tensorforce.environments.pygame_learning_environment import PyGameLearningEnvironment\nfrom tensorforce.environments.vizdoom import ViZDoom\nfrom tensorforce.environments.carla_environment import CARLAEnvironment\n\n\nenvironments = dict(\n    default=OpenAIGym,\n    ale=ArcadeLearningEnvironment, arcade_learning_environment=ArcadeLearningEnvironment,\n    mazeexp=MazeExplorer, maze_explorer=MazeExplorer,\n    gym=OpenAIGym, openai_gym=OpenAIGym,\n    retro=OpenAIRetro, openai_retro=OpenAIRetro,\n    osim=OpenSim, open_sim=OpenSim,\n    ple=PyGameLearningEnvironment, pygame_learning_environment=PyGameLearningEnvironment,\n    vizdoom=ViZDoom,\n    carla=CARLAEnvironment, carla_environment=CARLAEnvironment\n)\n\n\n__all__ = [\n    \'ArcadeLearningEnvironment\', \'Environment\', \'MazeExplorer\', \'MultiplayerEnvironment\',\n    \'MultiprocessingEnvironment\', \'OpenAIGym\', \'OpenAIRetro\', \'OpenSim\',\n    \'PyGameLearningEnvironment\', \'RemoteEnvironment\', \'SocketEnvironment\', \'ViZDoom\',\n    \'CARLAEnvironment\'\n]\n'"
tensorforce/environments/arcade_learning_environment.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport numpy as np\n\nfrom tensorforce.environments import Environment\n\n\nclass ArcadeLearningEnvironment(Environment):\n    """"""\n    [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)\n    adapter (specification key: `ale`, `arcade_learning_environment`).\n\n    May require:\n    ```bash\n    sudo apt-get install libsdl1.2-dev libsdl-gfx1.2-dev libsdl-image1.2-dev cmake\n\n    git clone https://github.com/mgbellemare/Arcade-Learning-Environment.git\n    cd Arcade-Learning-Environment\n\n    mkdir build && cd build\n    cmake -DUSE_SDL=ON -DUSE_RLGLUE=OFF -DBUILD_EXAMPLES=ON ..\n    make -j 4\n    cd ..\n\n    pip3 install .\n    ```\n\n    Args:\n        level (string): ALE rom file\n            (<span style=""color:#C00000""><b>required</b></span>).\n        loss_of_life_termination: Signals a terminal state on loss of life\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        loss_of_life_reward (float): Reward/Penalty on loss of life (negative values are a penalty)\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        repeat_action_probability (float): Repeats last action with given probability\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        visualize (bool): Whether to visualize interaction\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        frame_skip (int > 0): Number of times to repeat an action without observing\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        seed (int): Random seed\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n    """"""\n\n    def __init__(\n        self, level, life_loss_terminal=False, life_loss_punishment=0.0,\n        repeat_action_probability=0.0, visualize=False, frame_skip=1, seed=None\n    ):\n        super().__init__()\n\n        from ale_python_interface import ALEInterface\n\n        self.environment = ALEInterface()\n        self.rom_file = level\n\n        self.life_loss_terminal = life_loss_terminal\n        self.life_loss_punishment = life_loss_punishment\n\n        self.environment.setFloat(b\'repeat_action_probability\', repeat_action_probability)\n        self.environment.setBool(b\'display_screen\', visualize)\n        self.environment.setInt(b\'frame_skip\', frame_skip)\n        if seed is not None:\n            self.environment.setInt(b\'random_seed\', seed)\n\n        # All set commands must be done before loading the ROM.\n        self.environment.loadROM(rom_file=self.rom_file.encode())\n        self.available_actions = tuple(self.environment.getLegalActionSet())\n\n        # Full list of actions:\n        # No-Op, Fire, Up, Right, Left, Down, Up Right, Up Left, Down Right, Down Left, Up Fire,\n        # Right Fire, Left Fire, Down Fire, Up Right Fire, Up Left Fire, Down Right Fire, Down Left\n        # Fire\n\n    def __str__(self):\n        return super().__str__() + \'({})\'.format(self.rom_file)\n\n    def states(self):\n        width, height = self.environment.getScreenDims()\n        return dict(type=\'float\', shape=(height, width, 3))\n\n    def actions(self):\n        return dict(type=\'int\', num_values=len(self.available_actions))\n\n    def close(self):\n        self.environment.__del__()\n        self.environment = None\n\n    def get_states(self):\n        screen = np.copy(self.environment.getScreenRGB(screen_data=self.screen))\n        screen = screen.astype(dtype=np.float32) / 255.0\n        return screen\n\n    def reset(self):\n        self.environment.reset_game()\n        width, height = self.environment.getScreenDims()\n        self.screen = np.empty((height, width, 3), dtype=np.uint8)\n        self.lives = self.environment.lives()\n        return self.get_states()\n\n    def execute(self, actions):\n        reward = self.environment.act(action=self.available_actions[actions])\n        terminal = self.environment.game_over()\n        states = self.get_states()\n\n        next_lives = self.environment.lives()\n        if next_lives < self.lives:\n            if self.life_loss_terminal:\n                terminal = True\n            elif self.life_loss_punishment > 0.0:\n                reward -= self.life_loss_punishment\n            self.lives = next_lives\n\n        return states, terminal, reward\n'"
tensorforce/environments/carla_environment.py,0,"b'import math\nimport numpy as np\n\nfrom typing import Optional\nfrom datetime import datetime\n\nfrom tensorforce.agents import Agent\nfrom tensorforce.environments import Environment\n\ntry:\n    import carla\n    import pygame\n\n    from tensorforce.environments.carla import env_utils\n    from tensorforce.environments.carla.env_utils import WAYPOINT_DICT\n    from tensorforce.environments.carla.sensors import Sensor, SensorSpecs\n    from tensorforce.environments.carla.synchronous_mode import CARLASyncContext\nexcept ImportError:\n    pass\n\n\nclass CARLAEnvironment(Environment):\n    """"""A TensorForce Environment for the [CARLA driving simulator](https://github.com/carla-simulator/carla).\n        - This environment is ""synchronized"" with the server, meaning that the server waits for a client tick. For a\n          detailed explanation of this, please refer to https://carla.readthedocs.io/en/latest/adv_synchrony_timestep/.\n        - Subclass to customize the behaviour of states, actions, sensors, reward function, agent, training loop, etc.\n\n       Requires, you to:\n        - Install `pygame`, `opencv`\n        - Install the CARLA simulator (version >= 0.9.8): https://carla.readthedocs.io/en/latest/start_quickstart\n        - Install CARLA\'s Python bindings:\n        --> Follow this [guide](https://carla.readthedocs.io/en/latest/build_system/#pythonapi), if you have trouble\n            with that then follow the above steps.\n        --> `cd your-path-to/CARLA_0.9.x/PythonAPI/carla/dist/`\n        --> Extract `carla-0.9.x-py3.5-YOUR_OS-x86_64.egg` where `YOUR_OS` depends on your OS, i.e. `linux` or `windows`\n        --> Create a `setup.py` file within the extracted folder and write the following:\n          ```python\n          from distutils.core import setup\n\n          setup(name=\'carla\',\n                version=\'0.9.x\',\n                py_modules=[\'carla\'])\n          ```\n        --> Install via pip: `pip install -e ~/CARLA_0.9.x/PythonAPI/carla/dist/carla-0.9.x-py3.5-YOUR_OS-x86_64`\n        - Run the CARLA simulator from command line: `your-path-to/CARLA_0.9.x/./CarlaUE4.sh` or (CarlaUE4.exe)\n        --> To use less resources add these flags: `-windowed -ResX=8 -ResY=8 --quality-level=Low`\n\n        Hardware requirements (recommended):\n        - GPU: dedicated, with at least 2/4 GB.\n        - RAM: 16 GB suggested.\n        - CPU: multicore, at least 4.\n        - Note: on my hardware (i7 4700HQ 4C/8T, GT 750M 4GB, 16GB RAM) I achieve about 20 FPS in low quality mode.\n\n        Example usage:\n        - See [tensorforce/examples](https://github.com/tensorforce/tensorforce/tree/master/examples)\n    \n        Known Issues:\n        - TensorForce\'s Runner is currently not compatible with this environment!\n\n        Author:\n        - Luca Anzalone (@luca96)\n    """"""\n    # States and actions specifications:\n    # Actions: throttle or brake, steer, reverse (bool)\n    ACTIONS_SPEC = dict(type=\'float\', shape=(3,), min_value=-1.0, max_value=1.0)\n    DEFAULT_ACTIONS = np.array([0.0, 0.0, 0.0])\n\n    # Vehicle: speed, control (4), accelerometer (x, y, z), gyroscope (x, y, z), position (x, y), compass\n    VEHICLE_FEATURES_SPEC = dict(type=\'float\', shape=(14,))\n\n    # Road: intersection (bool), junction (bool), speed_limit, lane_width, lane_change, left_lane, right_lane\n    ROAD_FEATURES_SPEC = dict(type=\'float\', shape=(8,))\n\n    # TODO: add a loading map functionality (specified or at random) - load_map\n    def __init__(self, address=\'localhost\', port=2000, timeout=2.0, image_shape=(150, 200, 3), window_size=(800, 600),\n                 vehicle_filter=\'vehicle.*\', sensors: dict = None, route_resolution=2.0, fps=30.0, render=True,\n                 debug=False):\n        """"""\n        :param address: CARLA simulator\'s id address. Required only if the simulator runs on a different machine.\n        :param port: CARLA simulator\'s port.\n        :param timeout: connection timeout.\n        :param image_shape: shape of the images observations.\n        :param window_size: pygame\'s window size. Meaningful only if `visualize=True`.\n        :param vehicle_filter: use to spawn a particular vehicle (e.g. \'vehicle.tesla.model3\') or class of vehicles\n            (e.g. \'vehicle.audi.*\')\n        :param sensors: specifies which sensors should be equipped to the vehicle, better specified by subclassing\n            `default_sensors()`.\n        :param route_resolution: route planner resolution grain.\n        :param fps: maximum framerate, it depends on your compiting power.\n        :param render: if True a pygame window is shown.\n        :param debug: enable to display some useful information about the vehicle.\n        """"""\n        super().__init__()\n        env_utils.init_pygame()\n\n        self.timeout = timeout\n        self.client = env_utils.get_client(address, port, self.timeout)\n        self.world = self.client.get_world()  # type: carla.World\n        self.map = self.world.get_map()  # type: carla.Map\n        self.synchronous_context = None\n\n        # set fix fps:\n        self.world.apply_settings(carla.WorldSettings(\n            no_rendering_mode=False,\n            synchronous_mode=False,\n            fixed_delta_seconds=1.0 / fps))\n\n        # vehicle\n        self.vehicle_filter = vehicle_filter\n        self.vehicle = None  # type: carla.Vehicle\n\n        # actions\n        self.control = None  # type: carla.VehicleControl\n        self.prev_actions = None\n\n        # weather\n        # TODO: add weather support\n\n        # visualization and debugging stuff\n        self.image_shape = image_shape\n        self.image_size = (image_shape[1], image_shape[0])\n        self.DEFAULT_IMAGE = np.zeros(shape=self.image_shape, dtype=np.float32)\n        self.fps = fps\n        self.tick_time = 1.0 / self.fps\n        self.should_render = render\n        self.should_debug = debug\n        self.clock = pygame.time.Clock()\n\n        if self.should_render:\n            self.window_size = window_size\n            self.font = env_utils.get_font(size=13)\n            self.display = env_utils.get_display(window_size)\n\n        # variables for reward computation\n        self.collision_penalty = 0.0\n\n        # vehicle sensors suite\n        self.sensors_spec = sensors if isinstance(sensors, dict) else self.default_sensors()\n        self.sensors = dict()\n\n    def states(self):\n        return dict(image=dict(shape=self.image_shape),\n                    vehicle_features=self.VEHICLE_FEATURES_SPEC,\n                    road_features=self.ROAD_FEATURES_SPEC,\n                    previous_actions=self.ACTIONS_SPEC)\n\n    def actions(self):\n        return self.ACTIONS_SPEC\n\n    def reset(self, soft=False):\n        self._reset_world(soft=soft)\n\n        # reset actions\n        self.control = carla.VehicleControl()\n        self.prev_actions = self.DEFAULT_ACTIONS\n\n        observation = env_utils.replace_nans(self._get_observation(sensors_data={}))\n        return observation\n\n    def reward(self, actions, time_cost=-1.0, a=2.0):\n        """"""An example reward function. Subclass to define your own.""""""\n        speed = env_utils.speed(self.vehicle)\n        speed_limit = self.vehicle.get_speed_limit()\n\n        if speed <= speed_limit:\n            speed_penalty = 0.0\n        else:\n            speed_penalty = a * (speed_limit - speed)\n\n        return time_cost - self.collision_penalty + speed_penalty\n\n    def execute(self, actions, record_path: str = None):\n        self.prev_actions = actions\n\n        pygame.event.get()\n        self.clock.tick()\n\n        sensors_data = self.world_step(actions, record_path=record_path)\n\n        reward = self.reward(actions)\n        terminal = self.terminal_condition()\n        next_state = env_utils.replace_nans(self._get_observation(sensors_data))\n\n        self.collision_penalty = 0.0\n\n        return next_state, terminal, reward\n\n    def terminal_condition(self):\n        """"""Tells whether the episode is terminated or not. Override with your own termination condition.""""""\n        return False\n\n    def close(self):\n        super().close()\n\n        if self.vehicle:\n            self.vehicle.destroy()\n\n        for sensor in self.sensors.values():\n            sensor.destroy()\n\n    def train(self, agent: Optional[Agent], num_episodes: int, max_episode_timesteps: int, weights_dir=\'weights/agents\',\n              agent_name=\'carla-agent\', load_agent=False, record_dir=\'data/recordings\', skip_frames=25):\n        record_path = None\n        should_record = isinstance(record_dir, str)\n        should_save = isinstance(weights_dir, str)\n\n        if agent is None:\n            print(f\'Using default agent...\')\n            agent = self.default_agent(max_episode_timesteps=max_episode_timesteps)\n\n        try:\n            if load_agent:\n                agent.load(directory=os.path.join(weights_dir, agent_name), filename=agent_name, environment=self,\n                           format=\'tensorflow\')\n                print(\'Agent loaded.\')\n\n            for episode in range(num_episodes):\n                states = self.reset()\n                total_reward = 0.0\n\n                if should_record:\n                    record_path = env_utils.get_record_path(base_dir=record_dir)\n                    print(f\'Recording in {record_path}.\')\n\n                with self.synchronous_context:\n                    self.skip(num_frames=skip_frames)\n                    t0 = datetime.now()\n\n                    for i in range(max_episode_timesteps):\n                        actions = agent.act(states)\n                        states, terminal, reward = self.execute(actions, record_path=record_path)\n\n                        total_reward += reward\n                        terminal = terminal or (i == max_episode_timesteps - 1)\n\n                        if agent.observe(reward, terminal):\n                            print(f\'{i + 1}/{max_episode_timesteps} -> update performed.\')\n\n                        if terminal:\n                            elapsed = str(datetime.now() - t0).split(\'.\')[0]\n                            print(f\'Episode-{episode} completed in {elapsed}, total_reward: {round(total_reward, 2)}\\n\')\n                            break\n\n                if should_save:\n                    env_utils.save_agent(agent, agent_name, directory=weights_dir)\n                    print(\'Agent saved.\')\n        finally:\n            self.close()\n\n    def default_sensors(self) -> dict:\n        """"""Returns a predefined dict of sensors specifications""""""\n        return dict(imu=SensorSpecs.imu(),\n                    collision=SensorSpecs.collision_detector(callback=self.on_collision),\n                    camera=SensorSpecs.rgb_camera(position=\'top\',\n                                                  image_size_x=self.window_size[0], image_size_y=self.window_size[1],\n                                                  sensor_tick=self.tick_time))\n\n    def default_agent(self, **kwargs) -> Agent:\n        """"""Returns a predefined agent for this environment""""""\n        raise NotImplementedError(\'Implement this to define your own default agent!\')\n\n    def on_collision(self, event, penalty=1000.0):\n        impulse = math.sqrt(utils.vector_norm(event.normal_impulse))\n        actor_type = event.other_actor.type_id\n\n        if \'pedestrian\' in actor_type:\n            self.collision_penalty += penalty * impulse\n\n        elif \'vehicle\' in actor_type:\n            self.collision_penalty += penalty / 2.0 * impulse\n        else:\n            self.collision_penalty += penalty * impulse\n\n    def render(self, sensors_data: dict):\n        """"""Renders sensors\' output""""""\n        image = sensors_data[\'camera\']\n        env_utils.display_image(self.display, image, window_size=self.window_size)\n\n    def debug(self, actions):\n        env_utils.display_text(self.display, self.font, text=self.debug_text(actions), origin=(16, 12),\n                               offset=(0, 16))\n\n    def debug_text(self, actions):\n        return [\'%d FPS\' % self.clock.get_fps(),\n                \'\',\n                \'Throttle: %.2f\' % self.control.throttle,\n                \'Steer: %.2f\' % self.control.steer,\n                \'Brake: %.2f\' % self.control.brake,\n                \'Reverse: %s\' % (\'T\' if self.control.reverse else \'F\'),\n                \'Hand brake: %s\' % (\'T\' if self.control.hand_brake else \'F\'),\n                \'Gear: %s\' % {-1: \'R\', 0: \'N\'}.get(self.control.gear),\n                \'\',\n                \'Speed %.1f km/h\' % env_utils.speed(self.vehicle),\n                \'Speed limit %.1f km/h\' % self.vehicle.get_speed_limit(),\n                \'\',\n                \'Reward: %.2f\' % self.reward(actions),\n                \'Collision penalty: %.2f\' % self.collision_penalty]\n\n    def skip(self, num_frames=10):\n        """"""Skips the given amount of frames""""""\n        for _ in range(num_frames):\n            self.synchronous_context.tick(timeout=self.timeout)\n\n        if num_frames > 0:\n            print(f\'Skipped {num_frames} frames.\')\n\n    def before_world_step(self):\n        """"""Callback: called before world.tick()""""""\n        pass\n\n    def after_world_step(self, sensors_data: dict):\n        """"""Callback: called after world.tick().""""""\n        pass\n\n    def on_sensors_data(self, data: dict) -> dict:\n        """"""Callback. Triggers when a world\'s \'tick\' occurs, meaning that data from sensors are been collected because a\n        simulation step of the CARLA\'s world has been completed.\n            - Use this method to preprocess sensors\' output data for: rendering, observation, ...\n        """"""\n        data[\'camera\'] = self.sensors[\'camera\'].convert_image(data[\'camera\'])\n        return data\n\n    def world_step(self, actions, record_path: str = None):\n        """"""Applies the actions to the vehicle, and updates the CARLA\'s world""""""\n        # [pre-tick updates] Apply control to update the vehicle\n        self.actions_to_control(actions)\n        self.vehicle.apply_control(self.control)\n\n        self.before_world_step()\n\n        # Advance the simulation and wait for sensors\' data.\n        data = self.synchronous_context.tick(timeout=self.timeout)\n        data = self.on_sensors_data(data)\n\n        # [post-tick updates] Update world-related stuff\n        self.after_world_step(data)\n\n        # Draw and debug:\n        if self.should_render:\n            self.render(sensors_data=data)\n\n            if self.should_debug:\n                self.debug(actions)\n\n            pygame.display.flip()\n\n            if isinstance(record_path, str):\n                env_utils.pygame_save(self.display, record_path)\n\n        return data\n\n    def _reset_world(self, soft=False):\n        # init actor\n        if not soft:\n            spawn_point = env_utils.random_spawn_point(self.map)\n        else:\n            spawn_point = self.spawn_point\n\n        if self.vehicle is None:\n            blueprint = env_utils.random_blueprint(self.world, actor_filter=self.vehicle_filter)\n            self.vehicle = env_utils.spawn_actor(self.world, blueprint, spawn_point)  # type: carla.Vehicle\n\n            self._create_sensors()\n            self.synchronous_context = CARLASyncContext(self.world, self.sensors, fps=self.fps)\n        else:\n            self.vehicle.apply_control(carla.VehicleControl())\n            self.vehicle.set_velocity(carla.Vector3D(x=0.0, y=0.0, z=0.0))\n            self.vehicle.set_transform(spawn_point)\n\n        # reset reward variables\n        self.collision_penalty = 0.0\n\n    def actions_to_control(self, actions):\n        """"""Specifies the mapping between an actions vector and the vehicle\'s control.""""""\n        # throttle and brake are mutual exclusive:\n        self.control.throttle = float(actions[0]) if actions[0] > 0 else 0.0\n        self.control.brake = float(-actions[0]) if actions[0] < 0 else 0.0\n\n        # steering\n        self.control.steer = float(actions[1])\n\n        # reverse motion:\n        self.control.reverse = bool(actions[2] > 0)\n\n    def _get_observation(self, sensors_data: dict):\n        image = sensors_data.get(\'camera\', self.DEFAULT_IMAGE)\n\n        if image.shape != self.image_shape:\n            image = env_utils.resize(image, size=self.image_size)\n\n        # Normalize image\'s pixels value to -1, +1\n        observation = dict(image=(2 * image - 255.0) / 255.0,\n                           vehicle_features=self._get_vehicle_features(),\n                           road_features=self._get_road_features(),\n                           previous_actions=self.prev_actions)\n        return observation\n\n    def _get_vehicle_features(self):\n        t = self.vehicle.get_transform()\n        control = self.vehicle.get_control()\n\n        imu_sensor = self.sensors[\'imu\']\n        gyroscope = imu_sensor.gyroscope\n        accelerometer = imu_sensor.accelerometer\n\n        return [\n            env_utils.speed(self.vehicle),\n            control.gear,\n            control.steer,\n            control.throttle,\n            control.brake,\n            # Accelerometer:\n            accelerometer[0],\n            accelerometer[1],\n            accelerometer[2],\n            # Gyroscope:\n            gyroscope[0],\n            gyroscope[1],\n            gyroscope[2],\n            # Location\n            t.location.x,\n            t.location.y,\n            # Compass:\n            math.radians(imu_sensor.compass)]\n\n    def _get_road_features(self):\n        waypoint = self.map.get_waypoint(self.vehicle.get_location())\n        speed_limit = self.vehicle.get_speed_limit()\n\n        return [float(waypoint.is_intersection),\n                float(waypoint.is_junction),\n                waypoint.lane_width,\n                math.log2(speed_limit),\n                # Lane:\n                WAYPOINT_DICT[\'lane_type\'][waypoint.lane_type],\n                WAYPOINT_DICT[\'lane_change\'][waypoint.lane_change],\n                WAYPOINT_DICT[\'lane_marking_type\'][waypoint.left_lane_marking.type],\n                WAYPOINT_DICT[\'lane_marking_type\'][waypoint.right_lane_marking.type]]\n\n    def _create_sensors(self):\n        for name, args in self.sensors_spec.items():\n            kwargs = args.copy()\n            sensor = Sensor.create(sensor_type=kwargs.pop(\'type\'), parent_actor=self.vehicle, **kwargs)\n\n            if name == \'world\':\n                raise ValueError(f\'Cannot name a sensor `world` because is reserved.\')\n\n            self.sensors[name] = sensor\n'"
tensorforce/environments/environment.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom datetime import datetime\nimport importlib\nimport json\nimport os\nimport sys\nfrom threading import Thread\nimport time\nfrom traceback import format_tb\n\nfrom tensorforce import TensorforceError, util\nimport tensorforce.environments\n\n\nclass Environment(object):\n    """"""\n    Tensorforce environment interface.\n    """"""\n\n    @staticmethod\n    def create(\n        environment=None, max_episode_timesteps=None, remote=None, blocking=False, host=None,\n        port=None, **kwargs\n    ):\n        """"""\n        Creates an environment from a specification. In case of ""socket-server"" remote mode, runs\n        environment in server communication loop until closed.\n\n        Args:\n            environment (specification | Environment class/object): JSON file, specification key,\n                configuration dictionary, library module, `Environment` class/object, or gym.Env\n                (<span style=""color:#C00000""><b>required</b>, invalid for ""socket-client"" remote\n                mode</span>).\n            max_episode_timesteps (int > 0): Maximum number of timesteps per episode, overwrites\n                the environment default if defined\n                (<span style=""color:#00C000""><b>default</b></span>: environment default, invalid\n                for ""socket-client"" remote mode).\n            remote (""multiprocessing"" | ""socket-client"" | ""socket-server""): Communication mode for\n                remote environment execution of parallelized environment execution, ""socket-client""\n                mode requires a corresponding ""socket-server"" running, and ""socket-server"" mode\n                runs environment in server communication loop until closed\n                (<span style=""color:#00C000""><b>default</b></span>: local execution).\n            blocking (bool): Whether remote environment calls should be blocking\n                (<span style=""color:#00C000""><b>default</b></span>: not blocking, invalid unless\n                ""multiprocessing"" or ""socket-client"" remote mode).\n            host (str): Socket server hostname or IP address\n                (<span style=""color:#C00000""><b>required</b></span> only for ""socket-client"" remote\n                mode).\n            port (int): Socket server port\n                (<span style=""color:#C00000""><b>required</b></span> only for ""socket-client/server""\n                remote mode).\n            kwargs: Additional arguments.\n        """"""\n        if remote not in (\'multiprocessing\', \'socket-client\'):\n            if blocking:\n                raise TensorforceError.invalid(\n                    name=\'Environment.create\', argument=\'blocking\',\n                    condition=\'no multiprocessing/socket-client instance\'\n                )\n        if remote not in (\'socket-client\', \'socket-server\'):\n            if host is not None:\n                raise TensorforceError.invalid(\n                    name=\'Environment.create\', argument=\'host\', condition=\'no socket instance\'\n                )\n            elif port is not None:\n                raise TensorforceError.invalid(\n                    name=\'Environment.create\', argument=\'port\', condition=\'no socket instance\'\n                )\n\n        if remote == \'multiprocessing\':\n            from tensorforce.environments import MultiprocessingEnvironment\n            environment = MultiprocessingEnvironment(\n                blocking=blocking, environment=environment,\n                max_episode_timesteps=max_episode_timesteps, **kwargs\n            )\n            return environment\n\n        elif remote == \'socket-client\':\n            if environment is not None:\n                raise TensorforceError.invalid(\n                    name=\'Environment.create\', argument=\'environment\',\n                    condition=\'socket-client instance\'\n                )\n            elif max_episode_timesteps is not None:\n                raise TensorforceError.invalid(\n                    name=\'Environment.create\', argument=\'max_episode_timesteps\',\n                    condition=\'socket-client instance\'\n                )\n            elif len(kwargs) > 0:\n                raise TensorforceError.invalid(\n                    name=\'Environment.create\', argument=\'kwargs\',\n                    condition=\'socket-client instance\'\n                )\n            from tensorforce.environments import SocketEnvironment\n            environment = SocketEnvironment(host=host, port=port, blocking=blocking)\n            return environment\n\n        elif remote == \'socket-server\':\n            from tensorforce.environments import SocketEnvironment\n            SocketEnvironment.remote(\n                port=port, environment=environment, max_episode_timesteps=max_episode_timesteps,\n                **kwargs\n            )\n\n        elif isinstance(environment, (EnvironmentWrapper, RemoteEnvironment)):\n            if max_episode_timesteps is not None:\n                raise TensorforceError.invalid(\n                    name=\'Environment.create\', argument=\'max_episode_timesteps\',\n                    condition=\'EnvironmentWrapper instance\'\n                )\n            if len(kwargs) > 0:\n                raise TensorforceError.invalid(\n                    name=\'Environment.create\', argument=\'kwargs\',\n                    condition=\'EnvironmentWrapper instance\'\n                )\n            return environment\n\n        elif isinstance(environment, type) and \\\n                issubclass(environment, (EnvironmentWrapper, RemoteEnvironment)):\n            raise TensorforceError.type(\n                name=\'Environment.create\', argument=\'environment\', dtype=type(environment)\n            )\n\n        elif isinstance(environment, Environment):\n            if max_episode_timesteps is not None:\n                environment = EnvironmentWrapper(\n                    environment=environment, max_episode_timesteps=max_episode_timesteps\n                )\n            return environment\n\n        elif isinstance(environment, type) and issubclass(environment, Environment):\n            environment = environment(**kwargs)\n            assert isinstance(environment, Environment)\n            return Environment.create(\n                environment=environment, max_episode_timesteps=max_episode_timesteps\n            )\n\n        elif isinstance(environment, dict):\n            # Dictionary specification\n            util.deep_disjoint_update(target=kwargs, source=environment)\n            environment = kwargs.pop(\'environment\', kwargs.pop(\'type\', \'default\'))\n            assert environment is not None\n            if max_episode_timesteps is None:\n                max_episode_timesteps = kwargs.pop(\'max_episode_timesteps\', None)\n\n            return Environment.create(\n                environment=environment, max_episode_timesteps=max_episode_timesteps, **kwargs\n            )\n\n        elif isinstance(environment, str):\n            if os.path.isfile(environment):\n                # JSON file specification\n                with open(environment, \'r\') as fp:\n                    environment = json.load(fp=fp)\n\n                util.deep_disjoint_update(target=kwargs, source=environment)\n                environment = kwargs.pop(\'environment\', kwargs.pop(\'type\', \'default\'))\n                assert environment is not None\n                if max_episode_timesteps is None:\n                    max_episode_timesteps = kwargs.pop(\'max_episode_timesteps\', None)\n\n                return Environment.create(\n                    environment=environment, max_episode_timesteps=max_episode_timesteps, **kwargs\n                )\n\n            elif \'.\' in environment:\n                # Library specification\n                library_name, module_name = environment.rsplit(\'.\', 1)\n                library = importlib.import_module(name=library_name)\n                environment = getattr(library, module_name)\n                return Environment.create(\n                    environment=environment, max_episode_timesteps=max_episode_timesteps, **kwargs\n                )\n\n            elif environment in tensorforce.environments.environments:\n                # Keyword specification\n                environment = tensorforce.environments.environments[environment]\n                return Environment.create(\n                    environment=environment, max_episode_timesteps=max_episode_timesteps, **kwargs\n                )\n\n            else:\n                # Default: OpenAI Gym\n                try:\n                    return Environment.create(\n                        environment=\'gym\', level=environment,\n                        max_episode_timesteps=max_episode_timesteps, **kwargs\n                    )\n                except TensorforceError:\n                    raise TensorforceError.value(\n                        name=\'Environment.create\', argument=\'environment\', value=environment\n                    )\n\n        else:\n            # Default: OpenAI Gym\n            from gym import Env\n            if isinstance(environment, Env) or \\\n                    (isinstance(environment, type) and issubclass(environment, Env)):\n                return Environment.create(\n                    environment=\'gym\', level=environment,\n                    max_episode_timesteps=max_episode_timesteps, **kwargs\n                )\n\n            else:\n                raise TensorforceError.type(\n                    name=\'Environment.create\', argument=\'environment\', dtype=type(environment)\n                )\n\n    def __init__(self):\n        # first two arguments, if applicable: level, visualize=False\n        self._max_episode_timesteps = None\n        self._expect_receive = None\n        self._actions = None\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def states(self):\n        """"""\n        Returns the state space specification.\n\n        Returns:\n            specification: Arbitrarily nested dictionary of state descriptions with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; state data type\n            (<span style=""color:#00C000""><b>default</b></span>: ""float"").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>num_states</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        """"""\n        raise NotImplementedError\n\n    def actions(self):\n        """"""\n        Returns the action space specification.\n\n        Returns:\n            specification: Arbitrarily nested dictionary of action descriptions with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>""bool"" | ""int"" | ""float""</i>) &ndash; action data type\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=""color:#00C000""><b>default</b></span>: scalar).</li>\n            <li><b>num_actions</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=""color:#C00000""><b>required</b></span> for type ""int"").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=""color:#00C000""><b>optional</b></span> for type ""float"").</li>\n            </ul>\n        """"""\n        raise NotImplementedError\n\n    def max_episode_timesteps(self):\n        """"""\n        Returns the maximum number of timesteps per episode.\n\n        Returns:\n            int: Maximum number of timesteps per episode.\n        """"""\n        return self._max_episode_timesteps\n\n    def close(self):\n        """"""\n        Closes the environment.\n        """"""\n        pass\n\n    def reset(self):\n        """"""\n        Resets the environment to start a new episode.\n\n        Returns:\n            dict[state]: Dictionary containing initial state(s) and auxiliary information.\n        """"""\n        raise NotImplementedError\n\n    def execute(self, actions):\n        """"""\n        Executes the given action(s) and advances the environment by one step.\n\n        Args:\n            actions (dict[action]): Dictionary containing action(s) to be executed\n                (<span style=""color:#C00000""><b>required</b></span>).\n\n        Returns:\n            dict[state], bool | 0 | 1 | 2, float: Dictionary containing next state(s), whether\n            a terminal state is reached or 2 if the episode was aborted, and observed reward.\n        """"""\n        raise NotImplementedError\n\n    def start_reset(self):\n        if self._expect_receive is not None:\n            raise TensorforceError.unexpected()\n        self._expect_receive = \'reset\'\n\n    def start_execute(self, actions):\n        if self._expect_receive is not None:\n            raise TensorforceError.unexpected()\n        self._expect_receive = \'execute\'\n        assert self._actions is None\n        self._actions = actions\n\n    def receive_execute(self):\n        if self._expect_receive == \'reset\':\n            self._expect_receive = None\n            return self.reset(), -1, None\n\n        elif self._expect_receive == \'execute\':\n            self._expect_receive = None\n            assert self._actions is not None\n            states, terminal, reward = self.execute(actions=self._actions)\n            self._actions = None\n            return states, int(terminal), reward\n\n        else:\n            raise TensorforceError.unexpected()\n\n\nclass EnvironmentWrapper(Environment):\n\n    def __init__(self, environment, max_episode_timesteps):\n        super().__init__()\n\n        if isinstance(environment, EnvironmentWrapper):\n            raise TensorforceError.unexpected()\n        if environment.max_episode_timesteps() is not None and \\\n                environment.max_episode_timesteps() < max_episode_timesteps:\n            raise TensorforceError.unexpected()\n\n        self.environment = environment\n        self.environment._max_episode_timesteps = max_episode_timesteps\n        self._max_episode_timesteps = max_episode_timesteps\n        self._timestep = None\n\n\n    def __str__(self):\n        return str(self.environment)\n\n    def states(self):\n        return self.environment.states()\n\n    def actions(self):\n        return self.environment.actions()\n\n    def close(self):\n        return self.environment.close()\n\n    def reset(self):\n        self._timestep = 0\n        return self.environment.reset()\n\n    def execute(self, actions):\n        if self._timestep is None:\n            raise TensorforceError(\n                message=""An environment episode has to be initialized by calling reset() first.""\n            )\n        assert self._timestep < self._max_episode_timesteps\n        states, terminal, reward = self.environment.execute(actions=actions)\n        terminal = int(terminal)\n        self._timestep += 1\n        if terminal == 0 and self._timestep >= self._max_episode_timesteps:\n            terminal = 2\n        if terminal > 0:\n            self._timestep = None\n        return states, terminal, reward\n\n    def __getattr__(self, name):\n        try:\n            return super().__getattr__(name)\n        except BaseException:\n            return getattr(self.environment, name)\n\n    # def __delattr__(self, name):\n    # def __setattr__(self, name, value):\n\n\nclass RemoteEnvironment(Environment):\n\n    @classmethod\n    def proxy_send(cls, connection, function, **kwargs):\n        raise NotImplementedError\n\n    @classmethod\n    def proxy_receive(cls, connection):\n        raise NotImplementedError\n\n    @classmethod\n    def proxy_close(cls, connection):\n        raise NotImplementedError\n\n    @classmethod\n    def remote_send(cls, connection, success, result):\n        raise NotImplementedError\n\n    @classmethod\n    def remote_receive(cls, connection):\n        raise NotImplementedError\n\n    @classmethod\n    def remote_close(cls, connection):\n        raise NotImplementedError\n\n    @classmethod\n    def remote(cls, connection, environment, max_episode_timesteps=None, **kwargs):\n        try:\n            environment = Environment.create(\n                environment=environment, max_episode_timesteps=max_episode_timesteps, **kwargs\n            )\n\n            while True:\n                function, kwargs = cls.remote_receive(connection=connection)\n\n                if function in (\'reset\', \'execute\'):\n                    environment_start = time.time()\n                result = getattr(environment, function)(**kwargs)\n                if function in (\'reset\', \'execute\'):\n                    seconds = time.time() - environment_start\n                    if function == \'reset\':\n                        result = (result, seconds)\n                    else:\n                        result += (seconds,)\n\n                cls.remote_send(connection=connection, success=True, result=result)\n\n                if function == \'close\':\n                    break\n\n        except BaseException:\n            try:\n                environment.close()\n            except BaseException:\n                pass\n            finally:\n                etype, value, traceback = sys.exc_info()\n                cls.remote_send(\n                    connection=connection, success=False,\n                    result=(str(etype), str(value), format_tb(traceback))\n                )\n\n        finally:\n            cls.remote_close(connection=connection)\n\n    def __init__(self, connection, blocking=False):\n        super().__init__()\n        self.connection = connection\n        self.blocking = blocking\n        self.observation = None\n        self.thread = None\n\n    def send(self, function, **kwargs):\n        if self._expect_receive is not None:\n            assert function != \'close\'\n            self.close()\n            raise TensorforceError.unexpected()\n        self._expect_receive = function\n\n        try:\n            self.__class__.proxy_send(connection=self.connection, function=function, **kwargs)\n        except BaseException:\n            self.__class__.proxy_close(connection=self.connection)\n            raise\n\n    def receive(self, function):\n        if self._expect_receive != function:\n            assert function != \'close\'\n            self.close()\n            raise TensorforceError.unexpected()\n        self._expect_receive = None\n\n        try:\n            success, result = self.__class__.proxy_receive(connection=self.connection)\n        except BaseException:\n            self.__class__.proxy_close(connection=self.connection)\n            raise\n\n        if success:\n            return result\n        else:\n            self.__class__.proxy_close(connection=self.connection)\n            etype, value, traceback = result\n            raise TensorforceError(message=\'{}: {}\'.format(etype, value)).with_traceback(traceback)\n\n    def __str__(self):\n        self.send(function=\'__str__\')\n        return self.receive(function=\'__str__\')\n\n    def states(self):\n        self.send(function=\'states\')\n        return self.receive(function=\'states\')\n\n    def actions(self):\n        self.send(function=\'actions\')\n        return self.receive(function=\'actions\')\n\n    def max_episode_timesteps(self):\n        self.send(function=\'max_episode_timesteps\')\n        return self.receive(function=\'max_episode_timesteps\')\n\n    def close(self):\n        if self.thread is not None:\n            self.thread.join()\n        if self._expect_receive is not None:\n            self.receive(function=self._expect_receive)\n        self.send(function=\'close\')\n        self.receive(function=\'close\')\n        self.__class__.proxy_close(connection=self.connection)\n        self.connection = None\n        self.observation = None\n        self.thread = None\n\n    def reset(self):\n        self.episode_seconds = 0.0\n        self.send(function=\'reset\')\n        states, seconds = self.receive(function=\'reset\')\n        self.episode_seconds += seconds\n        return states\n\n    def execute(self, actions):\n        self.send(function=\'execute\', actions=actions)\n        states, terminal, reward, seconds = self.receive(function=\'execute\')\n        self.episode_seconds += seconds\n        return states, int(terminal), reward\n\n    def start_reset(self):\n        self.episode_seconds = 0.0\n        if self.blocking:\n            self.send(function=\'reset\')\n        else:\n            if self.thread is not None:  # TODO: not expected\n                self.thread.join()\n            self.observation = None\n            self.thread = Thread(target=self.finish_reset)\n            self.thread.start()\n\n    def finish_reset(self):\n        assert self.thread is not None and self.observation is None\n        self.observation = (self.reset(), -1, None)\n        self.thread = None\n\n    def start_execute(self, actions):\n        if self.blocking:\n            self.send(function=\'execute\', actions=actions)\n        else:\n            assert self.thread is None and self.observation is None\n            self.thread = Thread(target=self.finish_execute, kwargs=dict(actions=actions))\n            self.thread.start()\n\n    def finish_execute(self, actions):\n        assert self.thread is not None and self.observation is None\n        self.observation = self.execute(actions=actions)\n        self.thread = None\n\n    def receive_execute(self):\n        if self.blocking:\n            if self._expect_receive == \'reset\':\n                return self.receive(function=\'reset\'), -1, None\n            else:\n                states, terminal, reward = self.receive(function=\'execute\')\n                return states, int(terminal), reward\n        else:\n            if self.thread is not None:\n                # assert self.observation is None\n                return None\n            else:\n                assert self.observation is not None\n                observation = self.observation\n                self.observation = None\n                return observation\n'"
tensorforce/environments/maze_explorer.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.environments import Environment\n\n\nclass MazeExplorer(Environment):\n    """"""\n    [MazeExplorer](https://github.com/mryellow/maze_explorer) environment adapter (specification\n    key: `mazeexp`, `maze_explorer`).\n\n    May require:\n    ```bash\n    sudo apt-get install freeglut3-dev\n\n    pip3 install mazeexp\n    ```\n\n    Args:\n        level (int): Game mode, see [GitHub](https://github.com/mryellow/maze_explorer)\n            (<span style=""color:#C00000""><b>required</b></span>).\n        visualize (bool): Whether to visualize interaction\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n    """"""\n\n    @classmethod\n    def levels(cls):\n        import mazeexp\n\n        return list(range(len(mazeexp.engine.config.modes)))\n\n    def __init__(self, level, visualize=False):\n        super().__init__()\n\n        import mazeexp\n\n        assert level in MazeExplorer.levels()\n\n        self.environment = mazeexp.MazeExplorer(mode_id=level, visible=visualize)\n\n    def __str__(self):\n        return super().__str__() + \'({})\'.format(self.environment.mode_id)\n\n    def states(self):\n        if self.environment.observation_chans > 1:\n            shape = (self.environment.observation_num, self.environment.observation_chans)\n        else:\n            shape = (self.environment.observation_num,)\n        return dict(type=\'float\', shape=shape)\n\n    def actions(self):\n        return dict(type=\'int\', num_actions=self.environment.actions_num)\n\n    def close(self):\n        self.environment.reset()\n        self.environment = None\n\n    def reset(self):\n        return self.environment.reset()\n\n    def execute(self, actions):\n        state, reward, terminal, _ = self.environment.act(action=actions)\n        return state, terminal, reward\n'"
tensorforce/environments/multiplayer_environment.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce import TensorforceError\n\n\nclass MultiplayerEnvironment(object):\n    """"""\n    Multi-player environment base class.\n    """"""\n\n    def __init__(self, num_players):\n        self.num_players = num_players\n\n    def states(self):\n        """"""\n        Return the state space. Might include subdicts if multiple states are \n        available simultaneously.\n\n        Returns:\n            States specification, with the following attributes\n                (required):\n                - type: one of \'bool\', \'int\', \'float\' (default: \'float\').\n                - shape: integer, or list/tuple of integers (required).\n        """"""\n        raise NotImplementedError\n\n    def actions(self):\n        """"""\n        Return the action space. Might include subdicts if multiple actions are \n        available simultaneously.\n\n        Returns:\n            actions (spec, or dict of specs): Actions specification, with the following attributes\n                (required):\n                - type: one of \'bool\', \'int\', \'float\' (required).\n                - shape: integer, or list/tuple of integers (default: []).\n                - num_actions: integer (required if type == \'int\').\n                - min_value and max_value: float (optional if type == \'float\', default: none).\n        """"""\n        raise NotImplementedError\n\n    def close(self):\n        pass\n\n    def reset(self):\n        # no return\n        raise NotImplementedError\n\n    def get_state(self, player):\n        # return state for player\n        raise NotImplementedError\n\n    def execute(self, actions, player):\n        # return terminal, reward\n        raise NotImplementedError\n'"
tensorforce/environments/multiprocessing_environment.py,0,"b'# Copyright 2020 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom multiprocessing import Pipe, Process\n\nfrom tensorforce.environments import RemoteEnvironment\n\n\nclass MultiprocessingEnvironment(RemoteEnvironment):\n    """"""\n    An earlier version of this code (#634) was originally developed by Vincent Belus (@vbelus).\n    """"""\n\n    @classmethod\n    def proxy_send(cls, connection, function, **kwargs):\n        connection[0].send(obj=(function, kwargs))\n\n    @classmethod\n    def proxy_receive(cls, connection):\n        return connection[0].recv()\n\n    @classmethod\n    def proxy_close(cls, connection):\n        connection[0].close()\n        connection[1].join()\n\n    @classmethod\n    def remote_send(cls, connection, success, result):\n        connection.send(obj=(success, result))\n\n    @classmethod\n    def remote_receive(cls, connection):\n        return connection.recv()\n\n    @classmethod\n    def remote_close(cls, connection):\n        connection.close()\n\n    def __init__(self, environment, blocking=False, max_episode_timesteps=None, **kwargs):\n        proxy_connection, remote_connection = Pipe(duplex=True)\n        process = Process(\n            target=self.__class__.remote, kwargs=dict(\n                connection=remote_connection, environment=environment,\n                max_episode_timesteps=max_episode_timesteps, **kwargs\n            )\n        )\n        process.start()\n        super().__init__(connection=(proxy_connection, process), blocking=blocking)\n'"
tensorforce/environments/open_sim.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.environments import Environment\n\n\nclass OpenSim(Environment):\n    """"""\n    [OpenSim](http://osim-rl.stanford.edu/) environment adapter (specification key: `osim`,\n    `open_sim`).\n\n    Args:\n        level (\'Arm2D\' | \'L2Run\' | \'Prosthetics\'): Environment id\n            (<span style=""color:#C00000""><b>required</b></span>).\n        visualize (bool): Whether to visualize interaction\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        integrator_accuracy (float): Integrator accuracy\n            (<span style=""color:#00C000""><b>default</b></span>: 5e-5).\n    """"""\n\n    @classmethod\n    def levels(cls):\n        return [\'Arm2D\', \'L2Run\', \'Prosthetics\']\n\n    def __init__(self, level, visualize=False, integrator_accuracy=5e-5):\n        super().__init__()\n\n        from osim.env import L2RunEnv, Arm2DEnv, ProstheticsEnv\n\n        environments = dict(Arm2D=Arm2DEnv, L2Run=L2RunEnv, Prosthetics=ProstheticsEnv)\n\n        self.environment = environments[level](\n            visualize=visualize, integrator_accuracy=integrator_accuracy\n        )\n\n    def __str__(self):\n        return super().__str__() + \'({})\'.format(self.environment)\n\n    def states(self):\n        return dict(type=\'float\', shape=self.environment.get_observation_space_size())\n\n    def actions(self):\n        return dict(type=\'float\', shape=self.environment.get_action_space_size())\n\n    def close(self):\n        self.environment.close()\n        self.environment = None\n\n    def reset(self):\n        return self.environment.reset()\n\n    def execute(self, actions):\n        states, reward, terminal, _ = self.env.step(action=actions)\n        return states, terminal, reward\n'"
tensorforce/environments/openai_gym.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport numpy as np\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.environments import Environment\n\n\nclass OpenAIGym(Environment):\n    """"""\n    [OpenAI Gym](https://gym.openai.com/) environment adapter (specification key: `gym`,\n    `openai_gym`).\n\n    May require:\n    ```bash\n    pip3 install gym\n    pip3 install gym[all]\n    ```\n\n    Args:\n        level (string | gym.Env): Gym id or instance\n            (<span style=""color:#C00000""><b>required</b></span>).\n        visualize (bool): Whether to visualize interaction\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        max_episode_steps (false | int > 0): Whether to terminate an episode after a while,\n            and if so, maximum number of timesteps per episode\n            (<span style=""color:#00C000""><b>default</b></span>: Gym default).\n        terminal_reward (float): Additional reward for early termination, if otherwise\n            indistinguishable from termination due to maximum number of timesteps\n            (<span style=""color:#00C000""><b>default</b></span>: Gym default).\n        reward_threshold (float): Gym environment argument, the reward threshold before the task is\n            considered solved\n            (<span style=""color:#00C000""><b>default</b></span>: Gym default).\n        drop_states_indices (list[int]): Drop states indices\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        visualize_directory (string): Visualization output directory\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        kwargs: Additional Gym environment arguments.\n    """"""\n\n    @classmethod\n    def levels(cls):\n        import gym\n\n        return list(gym.envs.registry.env_specs)\n\n    @classmethod\n    def create_level(cls, level, max_episode_steps, reward_threshold, **kwargs):\n        import gym\n\n        requires_register = False\n\n        # Find level\n        if level not in gym.envs.registry.env_specs:\n            if max_episode_steps is None:  # interpret as false if level does not exist\n                max_episode_steps = False\n            env_specs = list(gym.envs.registry.env_specs)\n            if level + \'-v0\' in gym.envs.registry.env_specs:\n                env_specs.insert(0, level + \'-v0\')\n            search = level\n            level = None\n            for name in env_specs:\n                if search == name[:name.rindex(\'-v\')]:\n                    if level is None:\n                        level = name\n                    if max_episode_steps is False and \\\n                            gym.envs.registry.env_specs[name].max_episode_steps is not None:\n                        continue\n                    elif max_episode_steps != gym.envs.registry.env_specs[name].max_episode_steps:\n                        continue\n                    level = name\n                    break\n            else:\n                if level is None:\n                    raise TensorforceError.value(name=\'OpenAIGym\', argument=\'level\', value=level)\n        assert level in cls.levels()\n\n        # Check/update attributes\n        if max_episode_steps is None:\n            max_episode_steps = gym.envs.registry.env_specs[level].max_episode_steps\n            if max_episode_steps is None:\n                max_episode_steps = False\n        elif max_episode_steps != gym.envs.registry.env_specs[level].max_episode_steps:\n            if not (\n                (max_episode_steps is False) and\n                (gym.envs.registry.env_specs[level].max_episode_steps is None)\n            ):\n                requires_register = True\n        if reward_threshold is None:\n            reward_threshold = gym.envs.registry.env_specs[level].reward_threshold\n        elif reward_threshold != gym.envs.registry.env_specs[level].reward_threshold:\n            requires_register = True\n        # if tags is None:\n        #     tags = dict(gym.envs.registry.env_specs[level].tags)\n        #     if \'wrapper_config.TimeLimit.max_episode_steps\' in tags and \\\n        #             max_episode_steps is not None:\n        #         tags.pop(\'wrapper_config.TimeLimit.max_episode_steps\')\n        elif tags != gym.envs.registry.env_specs[level].tags:\n            requires_register = True\n\n        # Modified specification\n        if requires_register:\n            entry_point = gym.envs.registry.env_specs[level].entry_point\n            _kwargs = dict(gym.envs.registry.env_specs[level]._kwargs)\n            nondeterministic = gym.envs.registry.env_specs[level].nondeterministic\n\n            if \'-v\' in level and level[level.rindex(\'-v\') + 2:].isdigit():\n                version = int(level[level.rindex(\'-v\') + 2:])\n                level = level[:level.rindex(\'-v\') + 2]\n            else:\n                version = -1\n            while True:\n                version += 1\n                if level + str(version) not in gym.envs.registry.env_specs:\n                    level = level + str(version)\n                    break\n\n            gym.register(\n                id=level, entry_point=entry_point, reward_threshold=reward_threshold,\n                nondeterministic=nondeterministic,\n                max_episode_steps=(None if max_episode_steps is False else max_episode_steps),\n                kwargs=_kwargs\n            )\n            assert level in cls.levels()\n\n        return gym.make(id=level, **kwargs), max_episode_steps\n\n    def __init__(\n        self, level, visualize=False, max_episode_steps=None, terminal_reward=0.0,\n        reward_threshold=None, drop_states_indices=None, visualize_directory=None, **kwargs\n    ):\n        super().__init__()\n\n        import gym\n        import gym.wrappers\n\n        self.level = level\n        self.visualize = visualize\n        self.terminal_reward = terminal_reward\n\n        if isinstance(level, gym.Env):\n            self.environment = self.level\n            self.level = self.level.__class__.__name__\n            self.max_episode_steps = max_episode_steps\n        elif isinstance(level, type) and issubclass(level, gym.Env):\n            self.environment = self.level(**kwargs)\n            self.level = self.level.__class__.__name__\n            self.max_episode_steps = max_episode_steps\n        else:\n            self.environment, self.max_episode_steps = self.__class__.create_level(\n                level=self.level, max_episode_steps=max_episode_steps,\n                reward_threshold=reward_threshold, **kwargs\n            )\n\n        if visualize_directory is not None:\n            self.environment = gym.wrappers.Monitor(\n                env=self.environment, directory=visualize_directory\n            )\n\n        self.states_spec = OpenAIGym.specs_from_gym_space(\n            space=self.environment.observation_space, ignore_value_bounds=True  # TODO: not ignore?\n        )\n        if drop_states_indices is None:\n            self.drop_states_indices = None\n        else:\n            assert util.is_atomic_values_spec(values_spec=self.states_spec)\n            self.drop_states_indices = sorted(drop_states_indices)\n            assert len(self.states_spec[\'shape\']) == 1\n            num_dropped = len(self.drop_states_indices)\n            self.states_spec[\'shape\'] = (self.states_spec[\'shape\'][0] - num_dropped,)\n\n        self.actions_spec = OpenAIGym.specs_from_gym_space(\n            space=self.environment.action_space, ignore_value_bounds=False\n        )\n\n    def __str__(self):\n        return super().__str__() + \'({})\'.format(self.level)\n\n    def states(self):\n        return self.states_spec\n\n    def actions(self):\n        return self.actions_spec\n\n    def max_episode_timesteps(self):\n        if self.max_episode_steps is False:\n            return super().max_episode_timesteps()\n        else:\n            return self.max_episode_steps\n\n    def close(self):\n        self.environment.close()\n        self.environment = None\n\n    def reset(self):\n        import gym.wrappers\n\n        if isinstance(self.environment, gym.wrappers.Monitor):\n            self.environment.stats_recorder.done = True\n        states = self.environment.reset()\n        self.timestep = 0\n        states = OpenAIGym.flatten_state(state=states, states_spec=self.states_spec)\n        if self.drop_states_indices is not None:\n            for index in reversed(self.drop_states_indices):\n                states = np.concatenate([states[:index], states[index + 1:]])\n        return states\n\n    def execute(self, actions):\n        if self.visualize:\n            self.environment.render()\n        actions = OpenAIGym.unflatten_action(action=actions)\n        states, reward, terminal, _ = self.environment.step(actions)\n        self.timestep += 1\n        if self.timestep == self.max_episode_steps:\n            assert terminal\n            terminal = 2\n        elif terminal:\n            assert self.max_episode_steps is None or self.max_episode_steps is False or \\\n                self.timestep < self.max_episode_steps\n            reward += self.terminal_reward\n            terminal = 1\n        else:\n            terminal = 0\n        states = OpenAIGym.flatten_state(state=states, states_spec=self.states_spec)\n        if self.drop_states_indices is not None:\n            for index in reversed(self.drop_states_indices):\n                states = np.concatenate([states[:index], states[index + 1:]])\n        return states, terminal, reward\n\n    @staticmethod\n    def specs_from_gym_space(space, ignore_value_bounds):\n        import gym\n\n        if isinstance(space, gym.spaces.Discrete):\n            return dict(type=\'int\', shape=(), num_values=space.n)\n\n        elif isinstance(space, gym.spaces.MultiBinary):\n            return dict(type=\'bool\', shape=space.n)\n\n        elif isinstance(space, gym.spaces.MultiDiscrete):\n            if (space.nvec == space.nvec.item(0)).all():\n                return dict(type=\'int\', shape=space.nvec.shape, num_values=space.nvec.item(0))\n            else:\n                specs = dict()\n                nvec = space.nvec.flatten()\n                shape = \'-\'.join(str(x) for x in space.nvec.shape)\n                for n in range(nvec.shape[0]):\n                    specs[\'gymmdc{}-{}\'.format(n, shape)] = dict(\n                        type=\'int\', shape=(), num_values=nvec[n]\n                    )\n                return specs\n\n        elif isinstance(space, gym.spaces.Box):\n            if ignore_value_bounds:\n                return dict(type=\'float\', shape=space.shape)\n            elif (space.low == space.low.item(0)).all() and (space.high == space.high.item(0)).all():\n                return dict(\n                    type=\'float\', shape=space.shape, min_value=space.low.item(0),\n                    max_value=space.high.item(0)\n                )\n            else:\n                specs = dict()\n                low = space.low.flatten()\n                high = space.high.flatten()\n                shape = \'-\'.join(str(x) for x in space.low.shape)\n                for n in range(low.shape[0]):\n                    specs[\'gymbox{}-{}\'.format(n, shape)] = dict(\n                        type=\'float\', shape=(), min_value=low[n], max_value=high[n]\n                    )\n                return specs\n\n        elif isinstance(space, gym.spaces.Tuple):\n            specs = dict()\n            n = 0\n            for n, space in enumerate(space.spaces):\n                spec = OpenAIGym.specs_from_gym_space(\n                    space=space, ignore_value_bounds=ignore_value_bounds\n                )\n                if \'type\' in spec:\n                    specs[\'gymtpl{}\'.format(n)] = spec\n                else:\n                    for name, spec in spec.items():\n                        specs[\'gymtpl{}-{}\'.format(n, name)] = spec\n            return specs\n\n        elif isinstance(space, gym.spaces.Dict):\n            specs = dict()\n            for space_name, space in space.spaces.items():\n                spec = OpenAIGym.specs_from_gym_space(\n                    space=space, ignore_value_bounds=ignore_value_bounds\n                )\n                if \'type\' in spec:\n                    specs[space_name] = spec\n                else:\n                    for name, spec in spec.items():\n                        specs[\'{}-{}\'.format(space_name, name)] = spec\n            return specs\n\n        else:\n            raise TensorforceError(\'Unknown Gym space.\')\n\n    @staticmethod\n    def flatten_state(state, states_spec):\n        if isinstance(state, tuple):\n            states = dict()\n            for n, state in enumerate(state):\n                if \'gymtpl{}\'.format(n) in states_spec:\n                    spec = states_spec[\'gymtpl{}\'.format(n)]\n                else:\n                    spec = None\n                    for name in states_spec:\n                        if name.startswith(\'gymtpl{}-\'.format(n)):\n                            assert spec is None\n                            spec = states_spec[name]\n                    assert spec is not None\n                state = OpenAIGym.flatten_state(state=state, states_spec=spec)\n                if isinstance(state, dict):\n                    for name, state in state.items():\n                        states[\'gymtpl{}-{}\'.format(n, name)] = state\n                else:\n                    states[\'gymtpl{}\'.format(n)] = state\n            return states\n\n        elif isinstance(state, dict):\n            states = dict()\n            for state_name, state in state.items():\n                if state_name in states_spec:\n                    spec = states_spec[state_name]\n                else:\n                    spec = None\n                    for name in states_spec:\n                        if name.startswith(\'{}-\'.format(state_name)):\n                            assert spec is None\n                            spec = states_spec[name]\n                    assert spec is not None\n                state = OpenAIGym.flatten_state(state=state, states_spec=spec)\n                if isinstance(state, dict):\n                    for name, state in state.items():\n                        states[\'{}-{}\'.format(state_name, name)] = state\n                else:\n                    states[state_name] = state\n            return states\n\n        elif np.isinf(state).any() or np.isnan(state).any():\n            raise TensorforceError(""State contains inf or nan."")\n\n        elif \'gymbox0\' in states_spec:\n            states = dict()\n            state = state.flatten()\n            shape = \'-\'.join(str(x) for x in state.shape)\n            for n in range(state.shape[0]):\n                states[\'gymbox{}-{}\'.format(n, shape)] = state[n]\n            return states\n\n        elif \'gymmdc0\' in states_spec:\n            states = dict()\n            state = state.flatten()\n            shape = \'-\'.join(str(x) for x in state.shape)\n            for n in range(state.shape[0]):\n                states[\'gymmdc{}-{}\'.format(n, shape)] = state[n]\n            return states\n\n        else:\n            return state\n\n    @staticmethod\n    def unflatten_action(action):\n        if not isinstance(action, dict):\n            if np.isinf(action).any() or np.isnan(action).any():\n                raise TensorforceError(""Action contains inf or nan."")\n\n            return action\n\n        elif all(name.startswith(\'gymmdc\') for name in action) or \\\n                all(name.startswith(\'gymbox\') for name in action) or \\\n                all(name.startswith(\'gymtpl\') for name in action):\n            space_type = next(iter(action))[:6]\n            actions = list()\n            n = 0\n            while True:\n                if any(name.startswith(space_type + str(n) + \'-\') for name in action):\n                    inner_action = [\n                        value for name, value in action.items()\n                        if name.startswith(space_type + str(n))\n                    ]\n                    assert len(inner_action) == 1\n                    actions.append(OpenAIGym.unflatten_action(action=inner_action[0]))\n                elif any(name == space_type + str(n) for name in action):\n                    actions.append(OpenAIGym.unflatten_action(action=action[space_type + str(n)]))\n                else:\n                    break\n                n += 1\n            if all(name.startswith(\'gymmdc\') for name in action) or \\\n                    all(name.startswith(\'gymbox\') for name in action):\n                name = next(iter(action))\n                shape = tuple(int(x) for x in name[name.index(\'-\') + 1:].split(\'-\'))\n                return np.array(actions).reshape(shape)\n            else:\n                return tuple(actions)\n\n        else:\n            actions = dict()\n            for name, action in action.items():\n                if \'-\' in name:\n                    name, inner_name = name.split(\'-\', 1)\n                    if name not in actions:\n                        actions[name] = dict()\n                    actions[name][inner_name] = action\n                else:\n                    actions[name] = action\n            for name, action in actions.items():\n                if isinstance(action, dict):\n                    actions[name] = OpenAIGym.unflatten_action(action=action)\n            return actions\n'"
tensorforce/environments/openai_retro.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.environments import OpenAIGym\n\n\nclass OpenAIRetro(OpenAIGym):\n    """"""\n    [OpenAI Retro](https://github.com/openai/retro) environment adapter (specification key:\n    `retro`, `openai_retro`).\n\n    May require:\n    ```bash\n    pip3 install gym-retro\n    ```\n\n    Args:\n        level (string): Game id\n            (<span style=""color:#C00000""><b>required</b></span>).\n        visualize (bool): Whether to visualize interaction\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        monitor_directory (string): Monitor output directory\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        kwargs: Additional Retro environment arguments.\n    """"""\n\n    @classmethod\n    def levels(cls):\n        import retro\n\n        return list(retro.data.list_games())\n\n    @classmethod\n    def create_level(cls, level, max_episode_steps, reward_threshold, tags, **kwargs):\n        import retro\n\n        assert max_episode_steps is False and reward_threshold is None and tags is None\n\n        return retro.make(game=level, **kwargs), max_episode_steps\n\n    def __init__(self, level, visualize=False, visualize_directory=None, **kwargs):\n        import retro\n\n        super().__init__(\n            level=level, visualize=visualize, max_episode_steps=False, terminal_reward=0.0,\n            reward_threshold=None, tags=None, visualize_directory=visualize_directory, **kwargs\n        )\n'"
tensorforce/environments/pygame_learning_environment.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nimport os\n\nimport numpy as np\n\nfrom tensorforce import TensorforceError\nfrom tensorforce.environments import Environment\n\n\nclass PyGameLearningEnvironment(Environment):\n    """"""\n    [PyGame Learning Environment](https://github.com/ntasfi/PyGame-Learning-Environment/)\n    environment adapter (specification key: `ple`, `pygame_learning_environment`).\n\n    May require:\n    ```bash\n    sudo apt-get install git python3-dev python3-setuptools python3-numpy python3-opengl \\\n    libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libsdl1.2-dev \\\n    libportmidi-dev libswscale-dev libavformat-dev libavcodec-dev libtiff5-dev libx11-6 \\\n    libx11-dev fluid-soundfont-gm timgm6mb-soundfont xfonts-base xfonts-100dpi xfonts-75dpi \\\n    xfonts-cyrillic fontconfig fonts-freefont-ttf libfreetype6-dev\n\n    pip3 install git+https://github.com/pygame/pygame.git\n\n    pip3 install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n    ```\n\n    Args:\n        level (string | subclass of `ple.games.base`): Game instance or name of class in\n            `ple.games`, like ""Catcher"", ""Doom"", ""FlappyBird"", ""MonsterKong"", ""Pixelcopter"", \n            ""Pong"", ""PuckWorld"", ""RaycastMaze"", ""Snake"", ""WaterWorld""\n            (<span style=""color:#C00000""><b>required</b></span>).\n        visualize (bool): Whether to visualize interaction\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        frame_skip (int > 0): Number of times to repeat an action without observing\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        fps (int > 0): The desired frames per second we want to run our game at\n            (<span style=""color:#00C000""><b>default</b></span>: 30).\n    """"""\n\n    @classmethod\n    def levels(cls):\n        import ple\n\n        levels = list()\n        for level in dir(ple.games):\n            level_cls = getattr(ple.games, level)\n            if isinstance(level_cls, type) and issubclass(level_cls, ple.games.base.PyGameWrapper):\n                levels.append(level)\n        return levels\n\n    def __init__(self, level, visualize=False, frame_skip=1, fps=30):\n        super().__init__()\n\n        import ple\n\n        if isinstance(level, str):\n            assert level in PyGameLearningEnvironment.levels()\n            level = getattr(ple.games, level)()\n\n        if not visualize:\n            os.putenv(\'SDL_VIDEODRIVER\', \'fbcon\')\n            os.environ[\'SDL_VIDEODRIVER\'] = \'dummy\'\n\n        self.environment = ple.PLE(\n            game=level, fps=fps, frame_skip=frame_skip, display_screen=visualize\n            # num_steps=1, reward_values={}, force_fps=True, add_noop_action=True, NOOP=K_F15,\n            # state_preprocessor=None, rng=24\n        )\n        self.environment.init()\n\n        self.has_game_state = self.environment.getGameStateDims() is not None\n        self.available_actions = tuple(self.environment.getActionSet())\n\n    def __str__(self):\n        return super().__str__() + \'({})\'.format(self.environment.__class__.__name__)\n\n    def states(self):\n        if self.has_game_state:\n            return OrderedDict(\n                screen=dict(type=\'float\', shape=(tuple(self.environment.getScreenDims()) + (3,))),\n                state=dict(type=\'float\', shape=(tuple(self.environment.getGameStateDims) + (3,)))\n            )\n        else:\n            return dict(type=\'float\', shape=(tuple(self.environment.getScreenDims()) + (3,)))\n\n    def actions(self):\n        return dict(type=\'int\', shape=(), num_values=len(self.available_actions))\n\n    def close(self):\n        self.environment = None\n\n    def get_states(self):\n        screen = self.environment.getScreenRGB().astype(dtype=np.float32) / 255.0\n        if self.has_game_state:\n            return OrderedDict(screen=screen, state=self.environment.getGameState())\n        else:\n            return screen\n\n    def reset(self):\n        self.environment.reset_game()\n        return self.get_states()\n\n    def execute(self, actions):\n        if self.environment.game_over():\n            raise TensorforceError.unexpected()\n        reward = self.environment.act(action=self.available_actions[actions])\n        terminal = self.environment.game_over()\n        states = self.get_states()\n        return states, terminal, reward\n'"
tensorforce/environments/socket_environment.py,0,"b'# Copyright 2020 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom socket import SHUT_RDWR, socket as Socket\n\nimport msgpack\nimport msgpack_numpy\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.environments import RemoteEnvironment\n\n\nmsgpack_numpy.patch()\n\n\nclass SocketEnvironment(RemoteEnvironment):\n    """"""\n    An earlier version of this code (#626) was originally developed as part of the following work:\n\n    Rabault, J., Kuhnle, A (2019). Accelerating Deep Reinforcement Leaning strategies of Flow\n    Control through a multi-environment approach. Physics of Fluids.\n    """"""\n\n    MAX_BYTES = 4096\n\n    @classmethod\n    def remote(cls, port, environment, max_episode_timesteps=None, **kwargs):\n        socket = Socket()\n        socket.bind((\'\', port))\n        socket.listen(1)\n        connection, address = socket.accept()\n        socket.close()\n        super().remote(\n            connection=connection, environment=environment,\n            max_episode_timesteps=max_episode_timesteps, **kwargs\n        )\n\n    @classmethod\n    def proxy_send(cls, connection, function, **kwargs):\n        str_function = function.encode()\n        num_bytes = len(str_function)\n        str_num_bytes = \'{:08d}\'.format(num_bytes).encode()\n        bytes_sent = connection.send(str_num_bytes + str_function)\n        if bytes_sent != num_bytes + 8:\n            raise TensorforceError.unexpected()\n\n        str_kwargs = msgpack.packb(o=kwargs)\n        num_bytes = len(str_kwargs)\n        str_num_bytes = \'{:08d}\'.format(num_bytes).encode()\n        bytes_sent = connection.send(str_num_bytes + str_kwargs)\n        if bytes_sent != num_bytes + 8:\n            raise TensorforceError.unexpected()\n\n    @classmethod\n    def proxy_receive(cls, connection):\n        str_success = connection.recv(1)\n        if len(str_success) != 1:\n            raise TensorforceError.unexpected()\n        success = bool(str_success)\n\n        str_num_bytes = connection.recv(8)\n        if len(str_num_bytes) != 8:\n            raise TensorforceError.unexpected()\n        num_bytes = int(str_num_bytes.decode())\n        str_result = b\'\'\n        for n in range(num_bytes // cls.MAX_BYTES):\n            str_result += connection.recv(cls.MAX_BYTES)\n            if len(str_result) != n * cls.MAX_BYTES:\n                raise TensorforceError.unexpected()\n        str_result += connection.recv(num_bytes % cls.MAX_BYTES)\n        if len(str_result) != num_bytes:\n            raise TensorforceError.unexpected()\n        result = msgpack.unpackb(packed=str_result)\n        decode = (lambda x: x.decode() if isinstance(x, bytes) else x)\n        result = util.fmap(function=decode, xs=result, map_keys=True)\n\n        return success, result\n\n    @classmethod\n    def proxy_close(cls, connection):\n        connection.shutdown(SHUT_RDWR)\n        connection.close()\n\n    @classmethod\n    def remote_send(cls, connection, success, result):\n        str_success = str(int(success)).encode()\n        bytes_sent = connection.send(str_success)\n        if bytes_sent != 1:\n            raise TensorforceError.unexpected()\n\n        str_result = msgpack.packb(o=result)\n        num_bytes = len(str_result)\n        str_num_bytes = \'{:08d}\'.format(num_bytes).encode()\n        bytes_sent = connection.send(str_num_bytes + str_result)\n        assert bytes_sent == num_bytes + 8\n        if bytes_sent != num_bytes + 8:\n            raise TensorforceError.unexpected()\n\n    @classmethod\n    def remote_receive(cls, connection):\n        str_num_bytes = connection.recv(8)\n        if len(str_num_bytes) != 8:\n            raise TensorforceError.unexpected()\n        num_bytes = int(str_num_bytes.decode())\n        str_function = b\'\'\n        for n in range(num_bytes // cls.MAX_BYTES):\n            str_function += connection.recv(cls.MAX_BYTES)\n            if len(str_function) != n * cls.MAX_BYTES:\n                raise TensorforceError.unexpected()\n        str_function += connection.recv(num_bytes % cls.MAX_BYTES)\n        if len(str_function) != num_bytes:\n            raise TensorforceError.unexpected()\n        function = str_function.decode()\n\n        str_num_bytes = connection.recv(8)\n        if len(str_num_bytes) != 8:\n            raise TensorforceError.unexpected()\n        num_bytes = int(str_num_bytes.decode())\n        str_kwargs = b\'\'\n        for n in range(num_bytes // cls.MAX_BYTES):\n            str_kwargs += connection.recv(cls.MAX_BYTES)\n            if len(str_kwargs) != n * cls.MAX_BYTES:\n                raise TensorforceError.unexpected()\n        str_kwargs += connection.recv(num_bytes % cls.MAX_BYTES)\n        if len(str_kwargs) != num_bytes:\n            raise TensorforceError.unexpected()\n        kwargs = msgpack.unpackb(packed=str_kwargs)\n        decode = (lambda x: x.decode() if isinstance(x, bytes) else x)\n        kwargs = util.fmap(function=decode, xs=kwargs, map_keys=True)\n\n        return function, kwargs\n\n    @classmethod\n    def remote_close(cls, connection):\n        connection.shutdown(SHUT_RDWR)\n        connection.close()\n\n    def __init__(self, host, port, blocking=False):\n        socket = Socket()\n        socket.connect((host, port))\n        super().__init__(connection=socket, blocking=blocking)\n'"
tensorforce/environments/vizdoom.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nimport itertools\n\nimport numpy as np\n\nfrom tensorforce.environments import Environment\n\n\nclass ViZDoom(Environment):\n    """"""\n    [ViZDoom](https://github.com/mwydmuch/ViZDoom) environment adapter (specification key:\n    `vizdoom`).\n\n    May require:\n    ```bash\n    sudo apt-get install g++ build-essential libsdl2-dev zlib1g-dev libmpg123-dev libjpeg-dev \\\n    libsndfile1-dev nasm tar libbz2-dev libgtk2.0-dev make cmake git chrpath timidity \\\n    libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip libboost-all-dev \\\n    liblua5.1-dev\n\n    pip3 install vizdoom\n    ```\n\n    Args:\n        level (string): ViZDoom configuration file\n            (<span style=""color:#C00000""><b>required</b></span>).\n        include_variables (bool): Whether to include game variables to state\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        factored_action (bool): Whether to use factored action representation\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        visualize (bool): Whether to visualize interaction\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        frame_skip (int > 0): Number of times to repeat an action without observing\n            (<span style=""color:#00C000""><b>default</b></span>: 12).\n        seed (int): Random seed\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n    """"""\n\n    def __init__(\n        self, level, visualize=False, include_variables=False, factored_action=False,\n        frame_skip=12, seed=None\n    ):\n        super().__init__()\n\n        from vizdoom import DoomGame, Mode, ScreenFormat, ScreenResolution\n\n        self.config_file = level\n        self.include_variables = include_variables\n        self.factored_action = factored_action\n        self.visualize = visualize\n        self.frame_skip = frame_skip\n\n        self.environment = DoomGame()\n        self.environment.load_config(self.config_file)\n        if self.visualize:\n            self.environment.set_window_visible(True)\n            self.environment.set_mode(Mode.ASYNC_PLAYER)\n        else:\n            self.environment.set_window_visible(False)\n            self.environment.set_mode(Mode.PLAYER)\n        # e.g. CRCGCB, RGB24, GRAY8\n        self.environment.set_screen_format(ScreenFormat.RGB24)\n        # e.g. RES_320X240, RES_640X480, RES_1920X1080\n        self.environment.set_screen_resolution(ScreenResolution.RES_640X480)\n        self.environment.set_depth_buffer_enabled(False)\n        self.environment.set_labels_buffer_enabled(False)\n        self.environment.set_automap_buffer_enabled(False)\n        if seed is not None:\n            self.environment.setSeed(seed)\n        self.environment.init()\n\n        self.state_shape = (480, 640, 3)\n        self.num_variables = self.environment.get_available_game_variables_size()\n        self.num_buttons = self.environment.get_available_buttons_size()\n        self.available_actions = [\n            tuple(a) for a in itertools.product([0, 1], repeat=self.num_buttons)\n        ]\n\n    def __str__(self):\n        return super().__str__() + \'({})\'.format(self.config_file)\n\n    def states(self):\n        if self.include_variables:\n            return OrderedDict(\n                screen=dict(type=\'float\', shape=self.state_shape),\n                variables=dict(type=\'float\', shape=self.num_variables)\n            )\n        else:\n            return dict(type=\'float\', shape=self.state_shape)\n\n    def actions(self):\n        if self.factored_action:\n            return dict(type=\'bool\', shape=self.num_buttons)\n        else:\n            return dict(type=\'int\', shape=(), num_values=len(self.available_actions))\n\n    def close(self):\n        self.environment.close()\n        self.environment = None\n\n    def get_states(self):\n        state = self.environment.get_state()\n        screen = state.screen_buffer.astype(dtype=np.float32) / 255.0\n        if self.include_variables:\n            return OrderedDict(screen=screen, variables=state.game_variables)\n        else:\n            return screen\n\n    def reset(self):\n        self.environment.new_episode()\n        self.current_states = self.get_states()\n        return self.current_states\n\n    def execute(self, actions):\n        if self.factored_action:\n            action = np.where(actions, 1.0, 0.0)\n        else:\n            action = self.available_actions[actions]\n        if self.visualize:\n            self.environment.set_action(action)\n            reward = 0.0\n            for _ in range(self.frame_skip):\n                self.environment.advance_action()\n                reward += self.environment.get_last_reward()\n        else:\n            reward = self.environment.make_action(list(action), self.frame_skip)\n        terminal = self.environment.is_episode_finished()\n        if not terminal:\n            self.current_states = self.get_states()\n        return self.current_states, terminal, reward\n'"
tensorforce/execution/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.execution.runner import Runner\n\n\n__all__ = [\'Runner\']\n'"
tensorforce/execution/runner.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nimport time\nfrom tqdm import tqdm\n\nimport numpy as np\n\nfrom tensorforce import Agent, Environment, TensorforceError, util\nfrom tensorforce.environments import RemoteEnvironment\n\n\nclass Runner(object):\n    """"""\n    Tensorforce runner utility.\n\n    Args:\n        agent (specification | Agent object): Agent specification or object, the latter is not\n            closed automatically as part of `runner.close()`, parallel_interactions is implicitly\n            specified as / expected to be at least num_parallel, -1 if evaluation\n            (<span style=""color:#C00000""><b>required</b></span>).\n        environment (specification | Environment object): Environment specification or object, the\n            latter is not closed automatically as part of `runner.close()`\n            (<span style=""color:#C00000""><b>required</b></span>, or alternatively `environments`,\n            invalid for ""socket-client"" remote mode).\n        max_episode_timesteps (int > 0): Maximum number of timesteps per episode, overwrites the\n            environment default if defined\n            (<span style=""color:#00C000""><b>default</b></span>: environment default, invalid for\n            ""socket-client"" remote mode).\n        evaluation (bool): Whether to run the (last if multiple) environment in evaluation mode\n            (<span style=""color:#00C000""><b>default</b></span>: no evaluation).\n        num_parallel (int > 0): Number of environment instances to execute in parallel\n            (<span style=""color:#00C000""><b>default</b></span>: no parallel execution, implicitly\n            specified by `environments`).\n        environments (list[specification | Environment object]): Environment specifications or\n            objects to execute in parallel, the latter are not closed automatically as part of\n            `runner.close()`\n            (<span style=""color:#00C000""><b>default</b></span>: no parallel execution,\n            alternatively specified via `environment` and `num_parallel`, invalid for\n            ""socket-client"" remote mode).\n        remote (""multiprocessing"" | ""socket-client""): Communication mode for remote environment\n            execution of parallelized environment execution, not compatible with environment(s)\n            given as Environment objects, ""socket-client"" mode requires a corresponding\n            ""socket-server"" running\n            (<span style=""color:#00C000""><b>default</b></span>: local execution).\n        blocking (bool): Whether remote environment calls should be blocking, only valid if remote\n            mode given\n            (<span style=""color:#00C000""><b>default</b></span>: not blocking, invalid unless\n            ""multiprocessing"" or ""socket-client"" remote mode).\n        host (str, iter[str]): Socket server hostname(s) or IP address(es)\n            (<span style=""color:#C00000""><b>required</b></span> only for ""socket-client"" remote\n            mode).\n        port (int, iter[int]): Socket server port(s), increasing sequence if single host and port\n            given\n            (<span style=""color:#C00000""><b>required</b></span> only for ""socket-client"" remote\n            mode).\n    """"""\n\n    def __init__(\n        self, agent, environment=None, max_episode_timesteps=None, evaluation=False,\n        num_parallel=None, environments=None, remote=None, blocking=False, host=None, port=None\n    ):\n        if environment is None and environments is None:\n            if remote != \'socket-client\':\n                raise TensorforceError.required(\n                    name=\'Runner\', argument=\'environment or environments\'\n                )\n            if num_parallel is None:\n                raise TensorforceError.required(\n                    name=\'Runner\', argument=\'num_parallel\', condition=\'socket-client remote mode\'\n                )\n            environments = [None for _ in range(num_parallel)]\n\n        elif environment is None:\n            if environments is None:\n                raise TensorforceError.required(\n                    name=\'Runner\', argument=\'environment or environments\'\n                )\n            if not util.is_iterable(x=environments):\n                raise TensorforceError.type(\n                    name=\'Runner\', argument=\'environments\', value=environments\n                )\n            if len(environments) == 0:\n                raise TensorforceError.value(\n                    name=\'Runner\', argument=\'environments\', value=environments\n                )\n            if num_parallel is not None and num_parallel != len(environments):\n                raise TensorforceError.value(\n                    name=\'Runner\', argument=\'num_parallel\', value=num_parallel,\n                    hint=\'!= len(environments)\'\n                )\n            num_parallel = len(environments)\n            environments = list(environments)\n\n        elif num_parallel is None:\n            if environments is not None:\n                raise TensorforceError.invalid(\n                    name=\'Runner\', argument=\'environments\', condition=\'environment is specified\'\n                )\n            num_parallel = 1\n            environments = [environment]\n\n        else:\n            if environments is not None:\n                raise TensorforceError.invalid(\n                    name=\'Runner\', argument=\'environments\', condition=\'environment is specified\'\n                )\n            if isinstance(environment, Environment):\n                raise TensorforceError.type(\n                    name=\'Runner\', argument=\'environment\', dtype=type(environment),\n                    condition=\'num_parallel\', hint=\'is not specification\'\n                )\n            environments = [environment for _ in range(num_parallel)]\n\n        if port is None or isinstance(port, int):\n            if isinstance(host, str):\n                port = [port + n for n in range(num_parallel)]\n            else:\n                port = [port for _ in range(num_parallel)]\n        else:\n            if len(port) != num_parallel:\n                raise TensorforceError.value(\n                    name=\'Runner\', argument=\'len(port)\', value=len(port), hint=\'!= num_parallel\'\n                )\n        if host is None or isinstance(host, str):\n            host = [host for _ in range(num_parallel)]\n        else:\n            if len(host) != num_parallel:\n                raise TensorforceError.value(\n                    name=\'Runner\', argument=\'len(host)\', value=len(host), hint=\'!= num_parallel\'\n                )\n\n        self.environments = list()\n        self.is_environment_external = isinstance(environments[0], Environment)\n        environment = Environment.create(\n            environment=environments[0], max_episode_timesteps=max_episode_timesteps,\n            remote=remote, blocking=blocking, host=host[0], port=port[0]\n        )\n        self.is_environment_remote = isinstance(environment, RemoteEnvironment)\n        states = environment.states()\n        actions = environment.actions()\n        self.environments.append(environment)\n\n        for n, environment in enumerate(environments[1:], start=1):\n            assert isinstance(environment, Environment) == self.is_environment_external\n            environment = Environment.create(\n                environment=environment, max_episode_timesteps=max_episode_timesteps,\n                remote=remote, blocking=blocking, host=host[n], port=port[n]\n            )\n            assert isinstance(environment, RemoteEnvironment) == self.is_environment_remote\n            assert environment.states() == states\n            assert environment.actions() == actions\n            self.environments.append(environment)\n\n        self.evaluation = evaluation\n\n        self.is_agent_external = isinstance(agent, Agent)\n        if num_parallel - int(self.evaluation) > 1:\n            self.agent = Agent.create(\n                agent=agent, environment=environment,\n                parallel_interactions=(num_parallel - int(self.evaluation))\n            )\n        else:\n            self.agent = Agent.create(agent=agent, environment=environment)\n\n    def close(self):\n        if hasattr(self, \'tqdm\'):\n            self.tqdm.close()\n        if not self.is_agent_external:\n            self.agent.close()\n        if not self.is_environment_external:\n            for environment in self.environments:\n                environment.close()\n\n    # TODO: make average reward another possible criteria for runner-termination\n    def run(\n        self,\n        # General\n        num_episodes=None, num_timesteps=None, num_updates=None,\n        # Parallel\n        batch_agent_calls=False, sync_timesteps=False, sync_episodes=False, num_sleep_secs=0.001,\n        # Callback\n        callback=None, callback_episode_frequency=None, callback_timestep_frequency=None,\n        # Tqdm\n        use_tqdm=True, mean_horizon=1,\n        # Evaluation\n        evaluation=False, save_best_agent=None, evaluation_callback=None\n    ):\n        """"""\n        Run experiment.\n\n        Args:\n            num_episodes (int > 0): Number of episodes to run experiment\n                (<span style=""color:#00C000""><b>default</b></span>: no episode limit).\n            num_timesteps (int > 0): Number of timesteps to run experiment\n                (<span style=""color:#00C000""><b>default</b></span>: no timestep limit).\n            num_updates (int > 0): Number of agent updates to run experiment\n                (<span style=""color:#00C000""><b>default</b></span>: no update limit).\n            batch_agent_calls (bool): Whether to batch agent calls for parallel environment\n                execution\n                (<span style=""color:#00C000""><b>default</b></span>: separate call per environment).\n            sync_timesteps (bool): Whether to synchronize parallel environment execution on\n                timestep-level, implied by batch_agent_calls\n                (<span style=""color:#00C000""><b>default</b></span>: not synchronized unless\n                batch_agent_calls).\n            sync_episodes (bool): Whether to synchronize parallel environment execution on\n                episode-level\n                (<span style=""color:#00C000""><b>default</b></span>: not synchronized).\n            num_sleep_secs (float): Sleep duration if no environment is ready\n                (<span style=""color:#00C000""><b>default</b></span>: one milliseconds).\n            callback ((Runner, parallel) -> bool): Callback function taking the runner instance\n                plus parallel index and returning a boolean value indicating whether execution\n                should continue \n                (<span style=""color:#00C000""><b>default</b></span>: callback always true).\n            callback_episode_frequency (int): Episode interval between callbacks\n                (<span style=""color:#00C000""><b>default</b></span>: every episode).\n            callback_timestep_frequency (int): Timestep interval between callbacks\n                (<span style=""color:#00C000""><b>default</b></span>: not specified).\n            use_tqdm (bool): Whether to display a tqdm progress bar for the experiment run\n                (<span style=""color:#00C000""><b>default</b></span>: true), with the following\n                additional information (averaged over number of episodes given via mean_horizon):\n                <ul>\n                <li>reward &ndash; cumulative episode reward</li>\n                <li>ts/ep &ndash; timesteps per episode</li>\n                <li>sec/ep &ndash; seconds per episode</li>\n                <li>ms/ts &ndash; milliseconds per timestep</li>\n                <li>agent &ndash; percentage of time spent on agent computation</li>\n                <li>comm &ndash; if remote environment execution, percentage of time spent on\n                communication</li>\n                </ul>\n            mean_horizon (int): Number of episodes progress bar values and evaluation score are\n                averaged over (<span style=""color:#00C000""><b>default</b></span>: not averaged).\n            evaluation (bool): Whether to run in evaluation mode, only valid if a single\n                environment (<span style=""color:#00C000""><b>default</b></span>: no evaluation).\n            save_best_agent (string): Directory to save the best version of the agent according to\n                the evaluation score\n                (<span style=""color:#00C000""><b>default</b></span>: best agent is not saved).\n            evaluation_callback (int | Runner -> float): Callback function taking the runner\n                instance and returning an evaluation score\n                (<span style=""color:#00C000""><b>default</b></span>: cumulative evaluation reward\n                averaged over mean_horizon episodes).\n        """"""\n        # General\n        if num_episodes is None:\n            self.num_episodes = float(\'inf\')\n        else:\n            self.num_episodes = num_episodes\n        if num_timesteps is None:\n            self.num_timesteps = float(\'inf\')\n        else:\n            self.num_timesteps = num_timesteps\n        if num_updates is None:\n            self.num_updates = float(\'inf\')\n        else:\n            self.num_updates = num_updates\n\n        # Parallel\n        self.batch_agent_calls = batch_agent_calls\n        self.sync_timesteps = sync_timesteps or self.batch_agent_calls\n        self.sync_episodes = sync_episodes\n        self.num_sleep_secs = num_sleep_secs\n\n        # Callback\n        assert callback_episode_frequency is None or callback_timestep_frequency is None\n        if callback_episode_frequency is None and callback_timestep_frequency is None:\n            callback_episode_frequency = 1\n        if callback_episode_frequency is None:\n            self.callback_episode_frequency = float(\'inf\')\n        else:\n            self.callback_episode_frequency = callback_episode_frequency\n        if callback_timestep_frequency is None:\n            self.callback_timestep_frequency = float(\'inf\')\n        else:\n            self.callback_timestep_frequency = callback_timestep_frequency\n        if callback is None:\n            self.callback = (lambda r, p: True)\n        elif util.is_iterable(x=callback):\n            def sequential_callback(runner, parallel):\n                result = True\n                for fn in callback:\n                    x = fn(runner, parallel)\n                    if isinstance(result, bool):\n                        result = result and x\n                return result\n            self.callback = sequential_callback\n        else:\n            def boolean_callback(runner, parallel):\n                result = callback(runner, parallel)\n                if isinstance(result, bool):\n                    return result\n                else:\n                    return True\n            self.callback = boolean_callback\n\n        # Experiment statistics\n        self.episode_rewards = list()\n        self.episode_timesteps = list()\n        self.episode_seconds = list()\n        self.episode_agent_seconds = list()\n        if self.is_environment_remote:\n            self.episode_env_seconds = list()\n        if self.evaluation or evaluation:\n            self.evaluation_rewards = list()\n            self.evaluation_timesteps = list()\n            self.evaluation_seconds = list()\n            self.evaluation_agent_seconds = list()\n            if self.is_environment_remote:\n                self.evaluation_env_seconds = list()\n            if len(self.environments) == 1:\n                # for tqdm\n                self.episode_rewards = self.evaluation_rewards\n                self.episode_timesteps = self.evaluation_timesteps\n                self.episode_seconds = self.evaluation_seconds\n                self.episode_agent_seconds = self.evaluation_agent_seconds\n                if self.is_environment_remote:\n                    self.episode_env_seconds = self.evaluation_env_seconds\n        else:\n            # for tqdm\n            self.evaluation_rewards = self.episode_rewards\n            self.evaluation_timesteps = self.episode_timesteps\n            self.evaluation_seconds = self.episode_seconds\n            self.evaluation_agent_seconds = self.episode_agent_seconds\n            if self.is_environment_remote:\n                self.evaluation_env_seconds = self.episode_env_seconds\n\n        # Timestep/episode/update counter\n        self.timesteps = 0\n        self.episodes = 0\n        self.updates = 0\n\n        # Tqdm\n        if use_tqdm:\n            if hasattr(self, \'tqdm\'):\n                self.tqdm.close()\n\n            assert self.num_episodes != float(\'inf\') or self.num_timesteps != float(\'inf\')\n            inner_callback = self.callback\n\n            if self.num_episodes != float(\'inf\'):\n                # Episode-based tqdm (default option if both num_episodes and num_timesteps set)\n                assert self.num_episodes != float(\'inf\')\n                bar_format = (\n                    \'{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}, reward={postfix[0]:.2f}, ts/ep=\'\n                    \'{postfix[1]}, sec/ep={postfix[2]:.2f}, ms/ts={postfix[3]:.1f}, agent=\'\n                    \'{postfix[4]:.1f}%]\'\n                )\n                postfix = [0.0, 0, 0.0, 0.0, 0.0]\n                if self.is_environment_remote:\n                    bar_format = bar_format[:-1] + \', comm={postfix[5]:.1f}%]\'\n                    postfix.append(0.0)\n\n                self.tqdm = tqdm(\n                    desc=\'Episodes\', total=self.num_episodes, bar_format=bar_format,\n                    initial=self.episodes, postfix=postfix\n                )\n                self.tqdm_last_update = self.episodes\n\n                def tqdm_callback(runner, parallel):\n                    if len(runner.evaluation_rewards) > 0:\n                        mean_reward = float(np.mean(runner.evaluation_rewards[-mean_horizon:]))\n                        runner.tqdm.postfix[0] = mean_reward\n                    if len(runner.episode_timesteps) > 0:\n                        mean_ts_per_ep = int(np.mean(runner.episode_timesteps[-mean_horizon:]))\n                        mean_sec_per_ep = float(np.mean(runner.episode_seconds[-mean_horizon:]))\n                        mean_agent_sec = float(\n                            np.mean(runner.episode_agent_seconds[-mean_horizon:])\n                        )\n                        mean_ms_per_ts = mean_sec_per_ep * 1000.0 / mean_ts_per_ep\n                        mean_rel_agent = mean_agent_sec * 100.0 / mean_sec_per_ep\n                        runner.tqdm.postfix[1] = mean_ts_per_ep\n                        runner.tqdm.postfix[2] = mean_sec_per_ep\n                        runner.tqdm.postfix[3] = mean_ms_per_ts\n                        runner.tqdm.postfix[4] = mean_rel_agent\n                    if runner.is_environment_remote and len(runner.episode_env_seconds) > 0:\n                        mean_env_sec = float(np.mean(runner.episode_env_seconds[-mean_horizon:]))\n                        mean_rel_comm = (mean_agent_sec + mean_env_sec) * 100.0 / mean_sec_per_ep\n                        mean_rel_comm = 100.0 - mean_rel_comm\n                        runner.tqdm.postfix[5] = mean_rel_comm\n                    runner.tqdm.update(n=(runner.episodes - runner.tqdm_last_update))\n                    runner.tqdm_last_update = runner.episodes\n                    return inner_callback(runner, parallel)\n\n            else:\n                # Timestep-based tqdm\n                self.tqdm = tqdm(\n                    desc=\'Timesteps\', total=self.num_timesteps, initial=self.timesteps,\n                    postfix=dict(mean_reward=\'n/a\')\n                )\n                self.tqdm_last_update = self.timesteps\n\n                def tqdm_callback(runner, parallel):\n                    # sum_timesteps_reward = sum(runner.timestep_rewards[num_mean_reward:])\n                    # num_timesteps = min(num_mean_reward, runner.evaluation_timestep)\n                    # mean_reward = sum_timesteps_reward / num_episodes\n                    runner.tqdm.set_postfix(mean_reward=\'n/a\')\n                    runner.tqdm.update(n=(runner.timesteps - runner.tqdm_last_update))\n                    runner.tqdm_last_update = runner.timesteps\n                    return inner_callback(runner, parallel)\n\n            self.callback = tqdm_callback\n\n        # Evaluation\n        if evaluation and (self.evaluation or len(self.environments) > 1):\n            raise TensorforceError.unexpected()\n        self.evaluation_run = self.evaluation or evaluation\n        self.save_best_agent = save_best_agent\n        if evaluation_callback is None:\n            self.evaluation_callback = (lambda r: None)\n        else:\n            self.evaluation_callback = evaluation_callback\n        if self.save_best_agent is not None:\n            inner_evaluation_callback = self.evaluation_callback\n\n            def mean_reward_callback(runner):\n                result = inner_evaluation_callback(runner)\n                if result is None:\n                    return float(np.mean(runner.evaluation_rewards[-mean_horizon:]))\n                else:\n                    return result\n\n            self.evaluation_callback = mean_reward_callback\n            self.best_evaluation_score = None\n\n        # Episode statistics\n        self.episode_reward = [0.0 for _ in self.environments]\n        self.episode_timestep = [0 for _ in self.environments]\n        # if self.batch_agent_calls:\n        #     self.episode_agent_second = 0.0\n        #     self.episode_start = time.time()\n        if self.evaluation_run:\n            self.episode_agent_second = [0.0 for _ in self.environments[:-1]]\n            self.episode_start = [time.time() for _ in self.environments[:-1]]\n        else:\n            self.episode_agent_second = [0.0 for _ in self.environments]\n            self.episode_start = [time.time() for _ in self.environments]\n        self.evaluation_agent_second = 0.0\n        self.evaluation_start = time.time()\n\n        # Values\n        self.terminate = 0\n        self.prev_terminals = [-1 for _ in self.environments]\n        self.states = [None for _ in self.environments]\n        self.terminals = [None for _ in self.environments]\n        self.rewards = [None for _ in self.environments]\n        if self.evaluation_run:\n            self.evaluation_internals = self.agent.initial_internals()\n\n        # Required if agent was previously stopped mid-episode\n        self.agent.reset()\n\n        # Reset environments\n        for environment in self.environments:\n            environment.start_reset()\n\n        # Runner loop\n        while any(terminal <= 0 for terminal in self.prev_terminals):\n            self.terminals = [None for _ in self.terminals]\n\n            if self.batch_agent_calls:\n                # Retrieve observations (only if not already terminated)\n                while any(terminal is None for terminal in self.terminals):\n                    for n in range(len(self.environments)):\n                        if self.terminals[n] is not None:\n                            # Already received\n                            continue\n                        elif self.prev_terminals[n] <= 0:\n                            # Receive if not terminal\n                            observation = self.environments[n].receive_execute()\n                            if observation is None:\n                                continue\n                            self.states[n], self.terminals[n], self.rewards[n] = observation\n                        else:\n                            # Terminal\n                            self.states[n] = None\n                            self.terminals[n] = self.prev_terminals[n]\n                            self.rewards[n] = None\n\n                self.handle_observe_joint()\n                self.handle_act_joint()\n\n            # Parallel environments loop\n            no_environment_ready = True\n            for n in range(len(self.environments)):\n\n                if self.prev_terminals[n] > 0:\n                    # Continue if episode terminated (either sync_episodes or finished)\n                    self.terminals[n] = self.prev_terminals[n]\n                    continue\n\n                elif self.batch_agent_calls:\n                    # Handled before parallel environments loop\n                    pass\n\n                elif self.sync_timesteps:\n                    # Wait until environment is ready\n                    while True:\n                        observation = self.environments[n].receive_execute()\n                        if observation is not None:\n                            break\n\n                else:\n                    # Check whether environment is ready, otherwise continue\n                    observation = self.environments[n].receive_execute()\n                    if observation is None:\n                        self.terminals[n] = self.prev_terminals[n]\n                        continue\n\n                no_environment_ready = False\n                if not self.batch_agent_calls:\n                    self.states[n], self.terminals[n], self.rewards[n] = observation\n\n                # Check whether evaluation environment\n                if self.evaluation_run and n == (len(self.environments) - 1):\n                    if self.terminals[n] == -1:\n                        # Initial act\n                        self.handle_act_evaluation()\n                    else:\n                        # Observe\n                        self.handle_observe_evaluation()\n                        if self.terminals[n] == 0:\n                            # Act\n                            self.handle_act_evaluation()\n                        else:\n                            # Terminal\n                            self.handle_terminal_evaluation()\n\n                else:\n                    if self.terminals[n] == -1:\n                        # Initial act\n                        self.handle_act(parallel=n)\n                    else:\n                        # Observe\n                        self.handle_observe(parallel=n)\n                        if self.terminals[n] == 0:\n                            # Act\n                            self.handle_act(parallel=n)\n                        else:\n                            # Terminal\n                            self.handle_terminal(parallel=n)\n\n            self.prev_terminals = list(self.terminals)\n\n            # Sync_episodes: Reset if all episodes terminated\n            if self.sync_episodes and all(terminal > 0 for terminal in self.terminals):\n                num_episodes_left = self.num_episodes - self.episodes\n                num_noneval_environments = len(self.environments) - int(self.evaluation_run)\n                for n in range(min(num_noneval_environments, num_episodes_left)):\n                    self.prev_terminals[n] = -1\n                    self.environments[n].start_reset()\n                if self.evaluation_run and num_episodes_left > 0:\n                    self.prev_terminals[-1] = -1\n                    self.environments[-1].start_reset()\n\n            # Sleep if no environment was ready\n            if no_environment_ready:\n                time.sleep(self.num_sleep_secs)\n\n    def handle_act(self, parallel):\n        if self.batch_agent_calls:\n            self.environments[parallel].start_execute(actions=self.actions[parallel])\n\n        else:\n            agent_start = time.time()\n            actions = self.agent.act(states=self.states[parallel], parallel=parallel)\n            self.episode_agent_second[parallel] += time.time() - agent_start\n\n            self.environments[parallel].start_execute(actions=actions)\n\n        # Update episode statistics\n        self.episode_timestep[parallel] += 1\n\n        # Maximum number of timesteps or timestep callback (after counter increment!)\n        self.timesteps += 1\n        if ((\n            self.episode_timestep[parallel] % self.callback_timestep_frequency == 0 and\n            not self.callback(self, parallel)\n        ) or self.timesteps >= self.num_timesteps):\n            self.terminate = 2\n\n    def handle_act_joint(self):\n        parallel = [\n            n for n in range(len(self.environments) - int(self.evaluation_run))\n            if self.terminals[n] <= 0\n        ]\n        if len(parallel) > 0:\n            agent_start = time.time()\n            self.actions = self.agent.act(\n                states=[self.states[p] for p in parallel], parallel=parallel\n            )\n            agent_second = (time.time() - agent_start) / len(parallel)\n            for p in parallel:\n                self.episode_agent_second[p] += agent_second\n            self.actions = [\n                self.actions[parallel.index(n)] if n in parallel else None\n                for n in range(len(self.environments))\n            ]\n\n        if self.evaluation_run and self.terminals[-1] <= 0:\n            agent_start = time.time()\n            self.actions[-1], self.evaluation_internals = self.agent.act(\n                states=self.states[-1], internals=self.evaluation_internals, evaluation=True\n            )\n            self.episode_agent_second[-1] += time.time() - agent_start\n\n    def handle_act_evaluation(self):\n        if self.batch_agent_calls:\n            actions = self.actions[-1]\n\n        else:\n            agent_start = time.time()\n            actions, self.evaluation_internals = self.agent.act(\n                states=self.states[-1], internals=self.evaluation_internals, evaluation=True\n            )\n            self.evaluation_agent_second += time.time() - agent_start\n\n        self.environments[-1].start_execute(actions=actions)\n\n        # Update episode statistics\n        self.episode_timestep[-1] += 1\n\n        # Maximum number of timesteps or timestep callback (after counter increment!)\n        if self.evaluation_run and len(self.environments) == 1:\n            self.timesteps += 1\n            if ((\n                self.episode_timestep[-1] % self.callback_timestep_frequency == 0 and\n                not self.callback(self, parallel)\n            ) or self.timesteps >= self.num_timesteps):\n                self.terminate = 2\n\n    def handle_observe(self, parallel):\n        # Update episode statistics\n        self.episode_reward[parallel] += self.rewards[parallel]\n\n        # Not terminal but finished\n        if self.terminals[parallel] == 0 and self.terminate == 2:\n            self.terminals[parallel] = 2\n\n        # Observe unless batch_agent_calls\n        if not self.batch_agent_calls:\n            agent_start = time.time()\n            updated = self.agent.observe(\n                terminal=self.terminals[parallel], reward=self.rewards[parallel], parallel=parallel\n            )\n            self.episode_agent_second[parallel] += time.time() - agent_start\n            self.updates += int(updated)\n\n        # Maximum number of updates (after counter increment!)\n        if self.updates >= self.num_updates:\n            self.terminate = 2\n\n    def handle_observe_joint(self):\n        parallel = [\n            n for n in range(len(self.environments) - int(self.evaluation_run))\n            if self.prev_terminals[n] <= 0 and self.terminals[n] >= 0\n        ]\n        if len(parallel) > 0:\n            agent_start = time.time()\n            updated = self.agent.observe(\n                terminal=[self.terminals[p] for p in parallel],\n                reward=[self.rewards[p] for p in parallel], parallel=parallel\n            )\n            agent_second = (time.time() - agent_start) / len(parallel)\n            for p in parallel:\n                self.episode_agent_second[p] += agent_second\n            self.updates += updated\n\n    def handle_observe_evaluation(self):\n        # Update episode statistics\n        self.episode_reward[-1] += self.rewards[-1]\n\n        # Reset agent if terminal\n        if self.terminals[-1] > 0 or self.terminate == 2:\n            agent_start = time.time()\n            self.evaluation_agent_second += time.time() - agent_start\n\n    def handle_terminal(self, parallel):\n        # Update experiment statistics\n        self.episode_rewards.append(self.episode_reward[parallel])\n        self.episode_timesteps.append(self.episode_timestep[parallel])\n        self.episode_seconds.append(time.time() - self.episode_start[parallel])\n        self.episode_agent_seconds.append(self.episode_agent_second[parallel])\n        if self.is_environment_remote:\n            self.episode_env_seconds.append(self.environments[parallel].episode_seconds)\n\n        # Maximum number of episodes or episode callback (after counter increment!)\n        self.episodes += 1\n        if self.terminate == 0 and ((\n            self.episodes % self.callback_episode_frequency == 0 and\n            not self.callback(self, parallel)\n        ) or self.episodes >= self.num_episodes):\n            self.terminate = 1\n\n        # Reset episode statistics\n        self.episode_reward[parallel] = 0.0\n        self.episode_timestep[parallel] = 0\n        self.episode_agent_second[parallel] = 0.0\n        self.episode_start[parallel] = time.time()\n\n        # Reset environment\n        if self.terminate == 0 and not self.sync_episodes:\n            self.terminals[parallel] = -1\n            self.environments[parallel].start_reset()\n\n    def handle_terminal_evaluation(self):\n        # Update experiment statistics\n        self.evaluation_rewards.append(self.episode_reward[-1])\n        self.evaluation_timesteps.append(self.episode_timestep[-1])\n        self.evaluation_seconds.append(time.time() - self.evaluation_start)\n        self.evaluation_agent_seconds.append(self.evaluation_agent_second)\n        if self.is_environment_remote:\n            self.evaluation_env_seconds.append(self.environments[-1].episode_seconds)\n\n        # Evaluation callback\n        if self.save_best_agent is not None:\n            evaluation_score = self.evaluation_callback(self)\n            assert isinstance(evaluation_score, float)\n            if self.best_evaluation_score is None:\n                self.best_evaluation_score = evaluation_score\n            elif evaluation_score > self.best_evaluation_score:\n                self.best_evaluation_score = evaluation_score\n                self.agent.save(\n                    directory=self.save_best_agent, filename=\'best-model\', append=None\n                )\n        else:\n            self.evaluation_callback(self)\n\n        # Maximum number of episodes or episode callback (after counter increment!)\n        if self.evaluation_run and len(self.environments) == 1:\n            self.episodes += 1\n            if self.terminate == 0 and ((\n                self.episodes % self.callback_episode_frequency == 0 and\n                not self.callback(self, 0)\n            ) or self.episodes >= self.num_episodes):\n                self.terminate = 1\n\n        # Reset episode statistics\n        self.episode_reward[-1] = 0.0\n        self.episode_timestep[-1] = 0\n        self.evaluation_agent_second = 0.0\n        self.evaluation_start = time.time()\n\n        # Reset environment\n        if self.terminate == 0 and not self.sync_episodes:\n            self.terminals[-1] = 0\n            self.environments[-1].start_reset()\n            self.evaluation_internals = self.agent.initial_internals()\n'"
test/data/custom_env.py,0,"b""import numpy as np\n\nfrom tensorforce import Environment\n\n\nclass CustomEnvironment(Environment):\n\n    def __init__(self):\n        super().__init__()\n\n    def states(self):\n        return dict(type='float', shape=(8,))\n\n    def actions(self):\n        return dict(type='int', num_values=4)\n\n    # Optional, should only be defined if environment has a natural maximum duration\n    def max_episode_timesteps(self):\n        return super().max_episode_timesteps()\n\n    # Optional\n    def close(self):\n        super().close()\n\n    def reset(self):\n        state = np.random.random(size=(8,))\n        return state\n\n    def execute(self, actions):\n        next_state = np.random.random(size=(8,))\n        terminal = np.random.random() < 0.5\n        reward = np.random.random()\n        return next_state, terminal, reward\n"""
tensorforce/core/distributions/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.distributions.distribution import Distribution\n\nfrom tensorforce.core.distributions.bernoulli import Bernoulli\nfrom tensorforce.core.distributions.beta import Beta\nfrom tensorforce.core.distributions.categorical import Categorical\nfrom tensorforce.core.distributions.gaussian import Gaussian\n\n\ndistribution_modules = dict(\n    bernoulli=Bernoulli, beta=Beta, categorical=Categorical, gaussian=Gaussian\n)\n\n\n__all__ = [\'Bernoulli\', \'Beta\', \'Categorical\', \'Distribution\', \'distribution_modules\', \'Gaussian\']\n'"
tensorforce/core/distributions/bernoulli.py,23,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import layer_modules, Module\nfrom tensorforce.core.distributions import Distribution\n\n\nclass Bernoulli(Distribution):\n    """"""\n    Bernoulli distribution, for binary boolean actions (specification key: `bernoulli`).\n\n    Args:\n        name (string): Distribution name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        action_spec (specification): Action specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        embedding_shape (iter[int > 0]): Embedding shape\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, action_spec, embedding_shape, summary_labels=None):\n        super().__init__(\n            name=name, action_spec=action_spec, embedding_shape=embedding_shape,\n            summary_labels=summary_labels\n        )\n\n        input_spec = dict(type=\'float\', shape=self.embedding_shape)\n\n        if len(self.embedding_shape) == 1:\n            action_size = util.product(xs=self.action_spec[\'shape\'], empty=0)\n            self.logit = self.add_module(\n                name=\'logit\', module=\'linear\', modules=layer_modules, size=action_size,\n                input_spec=input_spec\n            )\n\n        else:\n            if len(self.embedding_shape) < 1 or len(self.embedding_shape) > 3:\n                raise TensorforceError.value(\n                    name=name, argument=\'embedding_shape\', value=self.embedding_shape,\n                    hint=\'invalid rank\'\n                )\n            if self.embedding_shape[:-1] == self.action_spec[\'shape\'][:-1]:\n                size = self.action_spec[\'shape\'][-1]\n            elif self.embedding_shape[:-1] == self.action_spec[\'shape\']:\n                size = 0\n            else:\n                raise TensorforceError.value(\n                    name=name, argument=\'embedding_shape\', value=self.embedding_shape,\n                    hint=\'not flattened and incompatible with action shape\'\n                )\n            self.logit = self.add_module(\n                name=\'logit\', module=\'linear\', modules=layer_modules, size=size,\n                input_spec=input_spec\n            )\n\n        Module.register_tensor(\n            name=(self.name + \'-probability\'),\n            spec=dict(type=\'float\', shape=self.action_spec[\'shape\']), batched=True\n        )\n\n    def tf_parametrize(self, x):\n        one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        shape = (-1,) + self.action_spec[\'shape\']\n\n        # Logit\n        logit = self.logit.apply(x=x)\n        if len(self.embedding_shape) == 1:\n            logit = tf.reshape(tensor=logit, shape=shape)\n\n        # States value\n        states_value = logit\n\n        # Sigmoid for corresponding probability\n        probability = tf.sigmoid(x=logit)\n\n        # Clip probability for numerical stability\n        probability = tf.clip_by_value(\n            t=probability, clip_value_min=epsilon, clip_value_max=(one - epsilon)\n        )\n\n        # ""Normalized"" logits\n        true_logit = tf.math.log(x=probability)\n        false_logit = tf.math.log(x=(one - probability))\n\n        Module.update_tensor(name=(self.name + \'-probability\'), tensor=probability)\n\n        return true_logit, false_logit, probability, states_value\n\n    def tf_sample(self, parameters, temperature):\n        true_logit, false_logit, probability, _ = parameters\n\n        summary_probability = probability\n        for _ in range(len(self.action_spec[\'shape\'])):\n            summary_probability = tf.math.reduce_mean(input_tensor=summary_probability, axis=1)\n\n        true_logit, false_logit, probability = self.add_summary(\n            label=(\'distributions\', \'bernoulli\'), name=\'probability\', tensor=summary_probability,\n            pass_tensors=(true_logit, false_logit, probability)\n        )\n\n        half = tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n\n        # Deterministic: true if >= 0.5\n        definite = tf.greater_equal(x=probability, y=half)\n\n        # Non-deterministic: sample true if >= uniform distribution\n        e_true_logit = tf.math.exp(x=(true_logit / temperature))\n        e_false_logit = tf.math.exp(x=(false_logit / temperature))\n        probability = e_true_logit / (e_true_logit + e_false_logit)\n        uniform = tf.random.uniform(\n            shape=tf.shape(input=probability), dtype=util.tf_dtype(dtype=\'float\')\n        )\n        sampled = tf.greater_equal(x=probability, y=uniform)\n\n        return tf.where(condition=(temperature < epsilon), x=definite, y=sampled)\n\n    def tf_log_probability(self, parameters, action):\n        true_logit, false_logit, _, _ = parameters\n\n        return tf.where(condition=action, x=true_logit, y=false_logit)\n\n    def tf_entropy(self, parameters):\n        true_logit, false_logit, probability, _ = parameters\n\n        one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n\n        return -probability * true_logit - (one - probability) * false_logit\n\n    def tf_kl_divergence(self, parameters1, parameters2):\n        true_logit1, false_logit1, probability1, _ = parameters1\n        true_logit2, false_logit2, _, _ = parameters2\n\n        true_log_prob_ratio = true_logit1 - true_logit2\n        false_log_prob_ratio = false_logit1 - false_logit2\n\n        one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n\n        return probability1 * true_log_prob_ratio + (one - probability1) * false_log_prob_ratio\n\n    def tf_action_value(self, parameters, action):\n        true_logit, false_logit, _, states_value = parameters\n\n        # if action is None:\n        # states_value = tf.expand_dims(input=states_value, axis=-1)\n        # logits = tf.stack(values=(false_logit, true_logit), axis=-1)\n\n        logits = tf.where(condition=action, x=true_logit, y=false_logit)\n\n        return states_value + logits\n\n    def tf_states_value(self, parameters):\n        _, _, _, states_value = parameters\n\n        return states_value\n'"
tensorforce/core/distributions/beta.py,48,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom math import log\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import layer_modules, Module\nfrom tensorforce.core.distributions import Distribution\n\n\nclass Beta(Distribution):\n    """"""\n    Beta distribution, for bounded continuous actions (specification key: `beta`).\n\n    Args:\n        name (string): Distribution name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        action_spec (specification): Action specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        embedding_shape (iter[int > 0]): Embedding shape\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, action_spec, embedding_shape, summary_labels=None):\n        super().__init__(\n            name=name, action_spec=action_spec, embedding_shape=embedding_shape,\n            summary_labels=summary_labels\n        )\n\n        input_spec = dict(type=\'float\', shape=self.embedding_shape)\n\n        if len(self.embedding_shape) == 1:\n            action_size = util.product(xs=self.action_spec[\'shape\'], empty=0)\n            self.alpha = self.add_module(\n                name=\'alpha\', module=\'linear\', modules=layer_modules, size=action_size,\n                input_spec=input_spec\n            )\n            self.beta = self.add_module(\n                name=\'beta\', module=\'linear\', modules=layer_modules, size=action_size,\n                input_spec=input_spec\n            )\n\n        else:\n            if len(self.embedding_shape) < 1 or len(self.embedding_shape) > 3:\n                raise TensorforceError.value(\n                    name=name, argument=\'embedding_shape\', value=self.embedding_shape,\n                    hint=\'invalid rank\'\n                )\n            if self.embedding_shape[:-1] == self.action_spec[\'shape\'][:-1]:\n                size = self.action_spec[\'shape\'][-1]\n            elif self.embedding_shape[:-1] == self.action_spec[\'shape\']:\n                size = 0\n            else:\n                raise TensorforceError.value(\n                    name=name, argument=\'embedding_shape\', value=self.embedding_shape,\n                    hint=\'not flattened and incompatible with action shape\'\n                )\n            self.alpha = self.add_module(\n                name=\'alpha\', module=\'linear\', modules=layer_modules, size=size,\n                input_spec=input_spec\n            )\n            self.beta = self.add_module(\n                name=\'beta\', module=\'linear\', modules=layer_modules, size=size,\n                input_spec=input_spec\n            )\n\n        Module.register_tensor(\n            name=(self.name + \'-alpha\'), spec=dict(type=\'float\', shape=self.action_spec[\'shape\']),\n            batched=True\n        )\n        Module.register_tensor(\n            name=(self.name + \'-beta\'), spec=dict(type=\'float\', shape=self.action_spec[\'shape\']),\n            batched=True\n        )\n\n    def tf_parametrize(self, x):\n        # Softplus to ensure alpha and beta >= 1\n        one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        log_epsilon = tf.constant(value=log(util.epsilon), dtype=util.tf_dtype(dtype=\'float\'))\n        shape = (-1,) + self.action_spec[\'shape\']\n\n        # Alpha\n        alpha = self.alpha.apply(x=x)\n        # epsilon < 1.0, hence negative\n        alpha = tf.clip_by_value(t=alpha, clip_value_min=log_epsilon, clip_value_max=-log_epsilon)\n        alpha = tf.math.softplus(features=alpha) + one\n        if len(self.embedding_shape) == 1:\n            alpha = tf.reshape(tensor=alpha, shape=shape)\n\n        # Beta\n        beta = self.beta.apply(x=x)\n        # epsilon < 1.0, hence negative\n        beta = tf.clip_by_value(t=beta, clip_value_min=log_epsilon, clip_value_max=-log_epsilon)\n        beta = tf.math.softplus(features=beta) + one\n        if len(self.embedding_shape) == 1:\n            beta = tf.reshape(tensor=beta, shape=shape)\n\n        # Alpha + Beta\n        alpha_beta = tf.maximum(x=(alpha + beta), y=epsilon)\n\n        # Log norm\n        log_norm = tf.math.lgamma(x=alpha) + tf.math.lgamma(x=beta) - tf.math.lgamma(x=alpha_beta)\n\n        Module.update_tensor(name=(self.name + \'-alpha\'), tensor=alpha)\n        Module.update_tensor(name=(self.name + \'-beta\'), tensor=beta)\n\n        return alpha, beta, alpha_beta, log_norm\n\n    def tf_sample(self, parameters, temperature):\n        alpha, beta, alpha_beta, _ = parameters\n\n        summary_alpha = alpha\n        summary_beta = beta\n        for _ in range(len(self.action_spec[\'shape\'])):\n            summary_alpha = tf.math.reduce_mean(input_tensor=summary_alpha, axis=1)\n            summary_beta = tf.math.reduce_mean(input_tensor=summary_beta, axis=1)\n\n        alpha, beta, alpha_beta = self.add_summary(\n            label=(\'distributions\', \'beta\'), name=\'alpha\', tensor=summary_alpha,\n            pass_tensors=(alpha, beta, alpha_beta)\n        )\n        alpha, beta, alpha_beta = self.add_summary(\n            label=(\'distributions\', \'beta\'), name=\'beta\', tensor=summary_beta,\n            pass_tensors=(alpha, beta, alpha_beta)\n        )\n\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n\n        # Deterministic: mean as action\n        definite = beta / alpha_beta\n\n        # Non-deterministic: sample action using gamma distribution\n        alpha_sample = tf.random.gamma(\n            shape=(), alpha=alpha, dtype=util.tf_dtype(dtype=\'float\')\n        )\n        beta_sample = tf.random.gamma(\n            shape=(), alpha=beta, dtype=util.tf_dtype(dtype=\'float\')\n        )\n\n        sampled = beta_sample / tf.maximum(x=(alpha_sample + beta_sample), y=epsilon)\n\n        sampled = tf.where(condition=(temperature < epsilon), x=definite, y=sampled)\n\n        min_value = tf.constant(\n            value=self.action_spec[\'min_value\'], dtype=util.tf_dtype(dtype=\'float\')\n        )\n        max_value = tf.constant(\n            value=self.action_spec[\'max_value\'], dtype=util.tf_dtype(dtype=\'float\')\n        )\n\n        return min_value + (max_value - min_value) * sampled\n\n    def tf_log_probability(self, parameters, action):\n        alpha, beta, _, log_norm = parameters\n\n        min_value = tf.constant(\n            value=self.action_spec[\'min_value\'], dtype=util.tf_dtype(dtype=\'float\')\n        )\n        max_value = tf.constant(\n            value=self.action_spec[\'max_value\'], dtype=util.tf_dtype(dtype=\'float\')\n        )\n\n        action = (action - min_value) / (max_value - min_value)\n\n        one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n\n        action = tf.minimum(x=action, y=(one - epsilon))\n\n        return tf.math.xlogy(x=(beta - one), y=tf.maximum(x=action, y=epsilon)) + \\\n            (alpha - one) * tf.math.log1p(x=(-action)) - log_norm\n\n    def tf_entropy(self, parameters):\n        alpha, beta, alpha_beta, log_norm = parameters\n\n        one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n\n        if util.tf_dtype(dtype=\'float\') in (tf.float32, tf.float64):\n            digamma_alpha = tf.math.digamma(x=alpha)\n            digamma_beta = tf.math.digamma(x=beta)\n            digamma_alpha_beta = tf.math.digamma(x=alpha_beta)\n        else:\n            digamma_alpha = tf.dtypes.cast(\n                x=tf.math.digamma(x=tf.dtypes.cast(x=alpha, dtype=tf.float32)),\n                dtype=util.tf_dtype(dtype=\'float\')\n            )\n            digamma_beta = tf.dtypes.cast(\n                x=tf.math.digamma(x=tf.dtypes.cast(x=beta, dtype=tf.float32)),\n                dtype=util.tf_dtype(dtype=\'float\')\n            )\n            digamma_alpha_beta = tf.dtypes.cast(\n                x=tf.math.digamma(x=tf.dtypes.cast(x=alpha_beta, dtype=tf.float32)),\n                dtype=util.tf_dtype(dtype=\'float\')\n            )\n\n        return log_norm - (beta - one) * digamma_beta - (alpha - one) * digamma_alpha + \\\n            (alpha_beta - one - one) * digamma_alpha_beta\n\n    def tf_kl_divergence(self, parameters1, parameters2):\n        alpha1, beta1, alpha_beta1, log_norm1 = parameters1\n        alpha2, beta2, alpha_beta2, log_norm2 = parameters2\n\n        if util.tf_dtype(dtype=\'float\') in (tf.float32, tf.float64):\n            digamma_alpha1 = tf.math.digamma(x=alpha1)\n            digamma_beta1 = tf.math.digamma(x=beta1)\n            digamma_alpha_beta1 = tf.math.digamma(x=alpha_beta1)\n        else:\n            digamma_alpha1 = tf.dtypes.cast(\n                x=tf.math.digamma(x=tf.dtypes.cast(x=alpha1, dtype=tf.float32)),\n                dtype=util.tf_dtype(dtype=\'float\')\n            )\n            digamma_beta1 = tf.dtypes.cast(\n                x=tf.math.digamma(x=tf.dtypes.cast(x=beta1, dtype=tf.float32)),\n                dtype=util.tf_dtype(dtype=\'float\')\n            )\n            digamma_alpha_beta1 = tf.dtypes.cast(\n                x=tf.math.digamma(x=tf.dtypes.cast(x=alpha_beta1, dtype=tf.float32)),\n                dtype=util.tf_dtype(dtype=\'float\')\n            )\n\n        return log_norm2 - log_norm1 - digamma_beta1 * (beta2 - beta1) - \\\n            digamma_alpha1 * (alpha2 - alpha1) + digamma_alpha_beta1 * \\\n            (alpha_beta2 - alpha_beta1)\n'"
tensorforce/core/distributions/categorical.py,37,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import layer_modules, Module\nfrom tensorforce.core.distributions import Distribution\n\n\nclass Categorical(Distribution):\n    """"""\n    Categorical distribution, for discrete integer actions (specification key: `categorical`).\n\n    Args:\n        name (string): Distribution name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        action_spec (specification): Action specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        embedding_shape (iter[int > 0]): Embedding shape\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        advantage_based (bool): Whether to compute action values as state value plus advantage\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, action_spec, embedding_shape, advantage_based=False, summary_labels=None\n    ):\n        super().__init__(\n            name=name, action_spec=action_spec, embedding_shape=embedding_shape,\n            summary_labels=summary_labels\n        )\n\n        input_spec = dict(type=\'float\', shape=self.embedding_shape)\n        num_values = self.action_spec[\'num_values\']\n\n        self.state_value = None\n        if len(self.embedding_shape) == 1:\n            action_size = util.product(xs=self.action_spec[\'shape\'])\n            self.action_values = self.add_module(\n                name=\'action_values\', module=\'linear\', modules=layer_modules,\n                size=(action_size * num_values), input_spec=input_spec\n            )\n            if advantage_based:\n                self.state_value = self.add_module(\n                    name=\'states_value\', module=\'linear\', modules=layer_modules, size=action_size,\n                    input_spec=input_spec\n                )\n\n        else:\n            if advantage_based:\n                raise TensorforceError.invalid(\n                    name=name, argument=\'advantage_based\', condition=\'embedding shape\'\n                )\n            if len(self.embedding_shape) < 1 or len(self.embedding_shape) > 3:\n                raise TensorforceError.value(\n                    name=name, argument=\'embedding_shape\', value=self.embedding_shape,\n                    hint=\'invalid rank\'\n                )\n            if self.embedding_shape[:-1] == self.action_spec[\'shape\'][:-1]:\n                size = self.action_spec[\'shape\'][-1]\n            elif self.embedding_shape[:-1] == self.action_spec[\'shape\']:\n                size = 1\n            else:\n                raise TensorforceError.value(\n                    name=name, argument=\'embedding_shape\', value=self.embedding_shape,\n                    hint=\'not flattened and incompatible with action shape\'\n                )\n            self.action_values = self.add_module(\n                name=\'action_values\', module=\'linear\', modules=layer_modules,\n                size=(size * num_values), input_spec=input_spec\n            )\n\n        Module.register_tensor(\n            name=(self.name + \'-probabilities\'),\n            spec=dict(type=\'float\', shape=(self.action_spec[\'shape\'] + (num_values,))),\n            batched=True\n        )\n        Module.register_tensor(\n            name=(self.name + \'-values\'),\n            spec=dict(type=\'float\', shape=(self.action_spec[\'shape\'] + (num_values,))),\n            batched=True\n        )\n\n    def tf_parametrize(self, x, mask):\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        shape = (-1,) + self.action_spec[\'shape\'] + (self.action_spec[\'num_values\'],)\n\n        # Action values\n        action_values = self.action_values.apply(x=x)\n        action_values = tf.reshape(tensor=action_values, shape=shape)\n\n        if self.state_value is None:\n            # Implicit states value (TODO: experimental)\n            states_value = tf.reduce_logsumexp(input_tensor=action_values, axis=-1)\n\n        else:\n            # Explicit states value and advantage-based action values\n            states_value = self.state_value.apply(x=x)\n            states_value = tf.reshape(tensor=states_value, shape=shape[:-1])\n            action_values = tf.expand_dims(input=states_value, axis=-1) + action_values\n            action_values -= tf.math.reduce_mean(input_tensor=action_values, axis=-1, keepdims=True)\n\n        # TODO: before or after states_value?\n        min_float = tf.fill(\n            dims=tf.shape(input=action_values), value=util.tf_dtype(dtype=\'float\').min\n        )\n        action_values = tf.where(condition=mask, x=action_values, y=min_float)\n\n        # Softmax for corresponding probabilities\n        probabilities = tf.nn.softmax(logits=action_values, axis=-1)\n\n        # ""Normalized"" logits\n        logits = tf.math.log(x=tf.maximum(x=probabilities, y=epsilon))\n\n        Module.update_tensor(name=(self.name + \'-probabilities\'), tensor=probabilities)\n        Module.update_tensor(name=(self.name + \'-values\'), tensor=action_values)\n\n        return logits, probabilities, states_value, action_values\n\n    def tf_sample(self, parameters, temperature):\n        logits, probabilities, _, action_values = parameters\n\n        summary_probs = probabilities\n        for _ in range(len(self.action_spec[\'shape\'])):\n            summary_probs = tf.math.reduce_mean(input_tensor=summary_probs, axis=1)\n\n        logits, probabilities = self.add_summary(\n            label=(\'distributions\', \'categorical\'), name=\'probabilities\', tensor=summary_probs,\n            pass_tensors=(logits, probabilities), enumerate_last_rank=True\n        )\n\n        one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n\n        # Deterministic: maximum likelihood action\n        definite = tf.argmax(input=action_values, axis=-1)\n        definite = tf.dtypes.cast(x=definite, dtype=util.tf_dtype(\'int\'))\n\n        # Set logits to minimal value\n        min_float = tf.fill(dims=tf.shape(input=logits), value=util.tf_dtype(dtype=\'float\').min)\n        logits = logits / temperature\n        logits = tf.where(condition=(probabilities < epsilon), x=min_float, y=logits)\n\n        # Non-deterministic: sample action using Gumbel distribution\n        uniform_distribution = tf.random.uniform(\n            shape=tf.shape(input=logits), minval=epsilon, maxval=(one - epsilon),\n            dtype=util.tf_dtype(dtype=\'float\')\n        )\n        gumbel_distribution = -tf.math.log(x=-tf.math.log(x=uniform_distribution))\n        sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1)\n        sampled = tf.dtypes.cast(x=sampled, dtype=util.tf_dtype(\'int\'))\n\n        return tf.where(condition=(temperature < epsilon), x=definite, y=sampled)\n\n    def tf_log_probability(self, parameters, action):\n        logits, _, _, _ = parameters\n\n        if util.tf_dtype(dtype=\'int\') not in (tf.int32, tf.int64):\n            action = tf.dtypes.cast(x=action, dtype=tf.int32)\n\n        logits = tf.gather(\n            params=logits, indices=tf.expand_dims(input=action, axis=-1), batch_dims=-1\n        )\n\n        return tf.squeeze(input=logits, axis=-1)\n\n    def tf_entropy(self, parameters):\n        logits, probabilities, _, _ = parameters\n\n        return -tf.reduce_sum(input_tensor=(probabilities * logits), axis=-1)\n\n    def tf_kl_divergence(self, parameters1, parameters2):\n        logits1, probabilities1, _, _ = parameters1\n        logits2, _, _, _ = parameters2\n\n        log_prob_ratio = logits1 - logits2\n\n        return tf.reduce_sum(input_tensor=(probabilities1 * log_prob_ratio), axis=-1)\n\n    def tf_action_value(self, parameters, action=None):\n        _, _, _, action_values = parameters\n\n        if action is not None:\n            if util.tf_dtype(dtype=\'int\') not in (tf.int32, tf.int64):\n                action = tf.dtypes.cast(x=action, dtype=tf.int32)\n\n            action = tf.expand_dims(input=action, axis=-1)\n            action_values = tf.gather(params=action_values, indices=action, batch_dims=-1)\n            action_values = tf.squeeze(input=action_values, axis=-1)\n\n        return action_values  # states_value + tf.squeeze(input=logits, axis=-1)\n\n    def tf_states_value(self, parameters):\n        _, _, states_value, _ = parameters\n\n        return states_value\n'"
tensorforce/core/distributions/distribution.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core import Module\n\n\nclass Distribution(Module):\n    """"""\n    Base class for policy distributions.\n\n    Args:\n        name (string): Distribution name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        action_spec (specification): Action specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        embedding_shape (iter[int > 0]): Embedding shape\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, action_spec, embedding_shape, summary_labels=None):\n        super().__init__(name=name, summary_labels=summary_labels, l2_regularization=0.0)\n\n        self.action_spec = action_spec\n        self.embedding_shape = tuple(embedding_shape)\n\n    def tf_parametrize(self, x):\n        raise NotImplementedError\n\n    def tf_sample(self, parameters, temperature):\n        raise NotImplementedError\n\n    def tf_log_probability(self, parameters, action):\n        raise NotImplementedError\n\n    def tf_entropy(self, parameters):\n        raise NotImplementedError\n\n    def tf_kl_divergence(self, parameters1, parameters2):\n        raise NotImplementedError\n\n    def tf_action_value(self, parameters, action=None):\n        raise NotImplementedError\n\n    def tf_states_value(self, parameters):\n        raise NotImplementedError\n'"
tensorforce/core/distributions/gaussian.py,40,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom math import e, log, pi\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import layer_modules, Module\nfrom tensorforce.core.distributions import Distribution\n\n\nclass Gaussian(Distribution):\n    """"""\n    Gaussian distribution, for unbounded continuous actions (specification key: `gaussian`).\n\n    Args:\n        name (string): Distribution name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        action_spec (specification): Action specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        embedding_shape (iter[int > 0]): Embedding shape\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, action_spec, embedding_shape, summary_labels=None):\n        super().__init__(\n            name=name, action_spec=action_spec, embedding_shape=embedding_shape,\n            summary_labels=summary_labels\n        )\n\n        input_spec = dict(type=\'float\', shape=self.embedding_shape)\n\n        if len(self.embedding_shape) == 1:\n            action_size = util.product(xs=self.action_spec[\'shape\'], empty=0)\n            self.mean = self.add_module(\n                name=\'mean\', module=\'linear\', modules=layer_modules, size=action_size,\n                input_spec=input_spec\n            )\n            self.log_stddev = self.add_module(\n                name=\'log-stddev\', module=\'linear\', modules=layer_modules, size=action_size,\n                input_spec=input_spec\n            )\n\n        else:\n            if len(self.embedding_shape) < 1 or len(self.embedding_shape) > 3:\n                raise TensorforceError.value(\n                    name=name, argument=\'embedding_shape\', value=self.embedding_shape,\n                    hint=\'invalid rank\'\n                )\n            if self.embedding_shape[:-1] == self.action_spec[\'shape\'][:-1]:\n                size = self.action_spec[\'shape\'][-1]\n            elif self.embedding_shape[:-1] == self.action_spec[\'shape\']:\n                size = 0\n            else:\n                raise TensorforceError.value(\n                    name=name, argument=\'embedding_shape\', value=self.embedding_shape,\n                    hint=\'not flattened and incompatible with action shape\'\n                )\n            self.mean = self.add_module(\n                name=\'mean\', module=\'linear\', modules=layer_modules, size=size,\n                input_spec=input_spec\n            )\n            self.log_stddev = self.add_module(\n                name=\'log-stddev\', module=\'linear\', modules=layer_modules, size=size,\n                input_spec=input_spec\n            )\n\n        Module.register_tensor(\n            name=(self.name + \'-mean\'), spec=dict(type=\'float\', shape=self.action_spec[\'shape\']),\n            batched=True\n        )\n        Module.register_tensor(\n            name=(self.name + \'-stddev\'), spec=dict(type=\'float\', shape=self.action_spec[\'shape\']),\n            batched=True\n        )\n\n    def tf_parametrize(self, x):\n        log_epsilon = tf.constant(value=log(util.epsilon), dtype=util.tf_dtype(dtype=\'float\'))\n        shape = (-1,) + self.action_spec[\'shape\']\n\n        # Mean\n        mean = self.mean.apply(x=x)\n        if len(self.embedding_shape) == 1:\n            mean = tf.reshape(tensor=mean, shape=shape)\n\n        # Log standard deviation\n        log_stddev = self.log_stddev.apply(x=x)\n        if len(self.embedding_shape) == 1:\n            log_stddev = tf.reshape(tensor=log_stddev, shape=shape)\n\n        # Clip log_stddev for numerical stability\n        # epsilon < 1.0, hence negative\n        log_stddev = tf.clip_by_value(\n            t=log_stddev, clip_value_min=log_epsilon, clip_value_max=-log_epsilon\n        )\n\n        # Standard deviation\n        stddev = tf.exp(x=log_stddev)\n\n        Module.update_tensor(name=(self.name + \'-mean\'), tensor=mean)\n        Module.update_tensor(name=(self.name + \'-stddev\'), tensor=stddev)\n\n        return mean, stddev, log_stddev\n\n    def tf_sample(self, parameters, temperature):\n        mean, stddev, _ = parameters\n\n        summary_mean = mean\n        summary_stddev = stddev\n        for _ in range(len(self.action_spec[\'shape\'])):\n            summary_mean = tf.math.reduce_mean(input_tensor=summary_mean, axis=1)\n            summary_stddev = tf.math.reduce_mean(input_tensor=summary_stddev, axis=1)\n\n        mean, stddev = self.add_summary(\n            label=(\'distributions\', \'gaussian\'), name=\'mean\', tensor=summary_mean,\n            pass_tensors=(mean, stddev)\n        )\n        mean, stddev = self.add_summary(\n            label=(\'distributions\', \'gaussian\'), name=\'stddev\', tensor=summary_stddev,\n            pass_tensors=(mean, stddev)\n        )\n\n        normal_distribution = tf.random.normal(\n            shape=tf.shape(input=mean), dtype=util.tf_dtype(dtype=\'float\')\n        )\n        action = mean + stddev * temperature * normal_distribution\n\n        # Clip if bounded action\n        if \'min_value\' in self.action_spec:\n            min_value = tf.constant(\n                value=self.action_spec[\'min_value\'], dtype=util.tf_dtype(dtype=\'float\')\n            )\n            max_value = tf.constant(\n                value=self.action_spec[\'max_value\'], dtype=util.tf_dtype(dtype=\'float\')\n            )\n            action = tf.clip_by_value(t=action, clip_value_min=min_value, clip_value_max=max_value)\n\n        return action\n\n    def tf_log_probability(self, parameters, action):\n        mean, stddev, log_stddev = parameters\n\n        half = tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n        two = tf.constant(value=2.0, dtype=util.tf_dtype(dtype=\'float\'))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        pi_const = tf.constant(value=pi, dtype=util.tf_dtype(dtype=\'float\'))\n\n        sq_mean_distance = tf.square(x=(action - mean))\n        sq_stddev = tf.maximum(x=tf.square(x=stddev), y=epsilon)\n\n        return -half * sq_mean_distance / sq_stddev - log_stddev - \\\n            half * tf.math.log(x=(two * pi_const))\n\n    def tf_entropy(self, parameters):\n        _, _, log_stddev = parameters\n\n        half = tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n        two = tf.constant(value=2.0, dtype=util.tf_dtype(dtype=\'float\'))\n        e_const = tf.constant(value=e, dtype=util.tf_dtype(dtype=\'float\'))\n        pi_const = tf.constant(value=pi, dtype=util.tf_dtype(dtype=\'float\'))\n\n        return log_stddev + half * tf.math.log(x=(two * pi_const * e_const))\n\n    def tf_kl_divergence(self, parameters1, parameters2):\n        mean1, stddev1, log_stddev1 = parameters1\n        mean2, stddev2, log_stddev2 = parameters2\n\n        half = tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n\n        log_stddev_ratio = log_stddev2 - log_stddev1\n        sq_mean_distance = tf.square(x=(mean1 - mean2))\n        sq_stddev1 = tf.square(x=stddev1)\n        sq_stddev2 = tf.maximum(x=tf.square(x=stddev2), y=epsilon)\n\n        return log_stddev_ratio + half * (sq_stddev1 + sq_mean_distance) / sq_stddev2 - half\n\n    def tf_action_value(self, parameters, action):\n        mean, stddev, log_stddev = parameters\n\n        half = tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n        two = tf.constant(value=2.0, dtype=util.tf_dtype(dtype=\'float\'))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        pi_const = tf.constant(value=pi, dtype=util.tf_dtype(dtype=\'float\'))\n\n        sq_mean_distance = tf.square(x=(action - mean))\n        sq_stddev = tf.maximum(x=tf.square(x=stddev), y=epsilon)\n\n        return -half * sq_mean_distance / sq_stddev - two * log_stddev - \\\n            tf.math.log(x=(two * pi_const))\n\n    def tf_states_value(self, parameters):\n        _, _, log_stddev = parameters\n\n        half = tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n        two = tf.constant(value=2.0, dtype=util.tf_dtype(dtype=\'float\'))\n        pi_const = tf.constant(value=pi, dtype=util.tf_dtype(dtype=\'float\'))\n\n        return -log_stddev - half * tf.math.log(x=(two * pi_const))\n'"
tensorforce/core/estimators/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.estimators.estimator import Estimator\n\n\n__all__ = [\'Estimator\']\n'"
tensorforce/core/estimators/estimator.py,137,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import Module, parameter_modules\n\n\nclass Estimator(Module):\n    """"""\n    Value estimator.\n\n    Args:\n        name (string): Estimator name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        values_spec (specification): Values specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        horizon (parameter, long >= 0): Horizon of discounted-sum reward estimation\n            (<span style=""color:#C00000""><b>required</b></span>).\n        discount (parameter, 0.0 <= float <= 1.0): Discount factor for future rewards of\n            discounted-sum reward estimation\n            (<span style=""color:#C00000""><b>required</b></span>).\n        estimate_horizon (false | ""early"" | ""late""): Whether to estimate the value of horizon\n            states, and if so, whether to estimate early when experience is stored, or late when it\n            is retrieved\n            (<span style=""color:#C00000""><b>required</b></span>).\n        estimate_actions (bool): Whether to estimate state-action values instead of state values\n            (<span style=""color:#C00000""><b>required</b></span>).\n        estimate_terminal (bool): Whether to estimate the value of terminal states\n            (<span style=""color:#C00000""><b>required</b></span>).\n        estimate_advantage (bool): Whether to estimate the advantage by subtracting the current\n            estimate (<span style=""color:#C00000""><b>required</b></span>).\n        min_capacity (int > 0): Minimum buffer capacity\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        max_past_horizon (int >= 0): Maximum past horizon\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, values_spec, horizon, discount, estimate_horizon, estimate_actions,\n        estimate_terminal, estimate_advantage, min_capacity, max_past_horizon, device=None,\n        summary_labels=None\n    ):\n        super().__init__(name=name, device=device, summary_labels=summary_labels)\n\n        self.values_spec = values_spec\n\n        # Horizon\n        self.horizon = self.add_module(\n            name=\'horizon\', module=horizon, modules=parameter_modules, dtype=\'long\', min_value=0\n        )\n\n        # Discount\n        self.discount = self.add_module(\n            name=\'discount\', module=discount, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0, max_value=1.0\n        )\n\n        # Baseline settings\n        assert estimate_horizon in (False, \'early\', \'late\')\n        self.estimate_horizon = estimate_horizon\n        self.estimate_actions = estimate_actions\n        self.estimate_terminal = estimate_terminal\n        self.estimate_advantage = estimate_advantage\n\n        # Capacity\n        if self.estimate_horizon == \'early\':\n            self.capacity = max(self.horizon.max_value() + 1, min_capacity, max_past_horizon)\n        else:\n            self.capacity = max(self.horizon.max_value() + 1, min_capacity)\n\n    def min_future_horizon(self):\n        if self.estimate_horizon == \'late\':\n            return self.horizon.min_value() + 1\n        else:\n            return self.horizon.min_value()\n\n    def max_future_horizon(self):\n        if self.estimate_horizon == \'late\':\n            return self.horizon.max_value() + 1\n        else:\n            return self.horizon.max_value()\n\n    def tf_future_horizon(self):\n        if self.estimate_horizon == \'late\':\n            return self.horizon.value() + tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        else:\n            return self.horizon.value()\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        # Value buffers\n        self.buffers = OrderedDict()\n        for name, spec in self.values_spec.items():\n            if util.is_nested(name=name):\n                self.buffers[name] = OrderedDict()\n                for inner_name, spec in spec.items():\n                    shape = (self.capacity,) + spec[\'shape\']\n                    self.buffers[name][inner_name] = self.add_variable(\n                        name=(inner_name + \'-buffer\'), dtype=spec[\'type\'], shape=shape,\n                        is_trainable=False\n                    )\n            else:\n                shape = (self.capacity,) + spec[\'shape\']\n                self.buffers[name] = self.add_variable(\n                    name=(name + \'-buffer\'), dtype=spec[\'type\'], shape=shape, is_trainable=False\n                )\n\n        # Buffer index (modulo capacity, next index to write to)\n        self.buffer_index = self.add_variable(\n            name=\'buffer-index\', dtype=\'long\', shape=(), is_trainable=False, initializer=\'zeros\'\n        )\n\n    def tf_reset(self, baseline=None):\n        # Constants and parameters\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n        horizon = self.horizon.value()\n        discount = self.discount.value()\n\n        # Overwritten buffer indices\n        num_overwritten = tf.minimum(x=self.buffer_index, y=capacity)\n        indices = tf.range(start=(self.buffer_index - num_overwritten), limit=self.buffer_index)\n        indices = tf.math.mod(x=indices, y=capacity)\n\n        # Get overwritten values\n        values = OrderedDict()\n        for name, buffer in self.buffers.items():\n            if util.is_nested(name=name):\n                values[name] = OrderedDict()\n                for inner_name, buffer in buffer.items():\n                    values[name][inner_name] = tf.gather(params=buffer, indices=indices)\n            else:\n                values[name] = tf.gather(params=buffer, indices=indices)\n\n        states = values[\'states\']\n        internals = values[\'internals\']\n        auxiliaries = values[\'auxiliaries\']\n        actions = values[\'actions\']\n        terminal = values[\'terminal\']\n        reward = values[\'reward\']\n        terminal = values[\'terminal\']\n\n        # Reset buffer index\n        with tf.control_dependencies(control_inputs=util.flatten(xs=values)):\n            assignment = self.buffer_index.assign(value=zero, read_value=False)\n\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            assertions = list()\n            # Check whether exactly one terminal (, unless empty?)\n            assertions.append(\n                tf.debugging.assert_equal(\n                    x=tf.math.count_nonzero(input=terminal, dtype=util.tf_dtype(dtype=\'long\')),\n                    y=one, message=""Timesteps do not contain exactly one terminal.""\n                )\n            )\n            # Check whether last value is terminal\n            assertions.append(\n                tf.debugging.assert_equal(\n                    x=tf.math.greater(x=terminal[-1], y=zero),\n                    y=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')),\n                    message=""Terminal is not the last timestep.""\n                )\n            )\n\n        # Get number of values\n        with tf.control_dependencies(control_inputs=assertions):\n            if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                num_values = tf.shape(input=terminal, out_type=util.tf_dtype(dtype=\'long\'))[0]\n            else:\n                num_values = tf.dtypes.cast(\n                    x=tf.shape(input=terminal)[0], dtype=util.tf_dtype(dtype=\'long\')\n                )\n\n        # Horizon baseline value\n        if self.estimate_horizon == \'early\' and baseline is not None:\n            # Dependency horizon\n            # TODO: handle arbitrary non-optimization horizons!\n            past_horizon = baseline.past_horizon(is_optimization=False)\n            assertion = tf.debugging.assert_equal(\n                x=past_horizon, y=zero,\n                message=""Temporary: baseline cannot depend on previous states.""\n            )\n\n            # Baseline estimate\n            horizon_start = num_values - tf.maximum(x=(num_values - horizon), y=one)\n            _states = OrderedDict()\n            for name, state in states.items():\n                _states[name] = state[horizon_start:]\n            _internals = OrderedDict()\n            for name, internal in internals.items():\n                _internals[name] = internal[horizon_start:]\n            _auxiliaries = OrderedDict()\n            for name, auxiliary in auxiliaries.items():\n                _auxiliaries[name] = auxiliary[horizon_start:]\n\n            with tf.control_dependencies(control_inputs=(assertion,)):\n                # some_state = next(iter(states.values()))\n                # if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                #     batch_size = tf.shape(input=some_state, out_type=util.tf_dtype(dtype=\'long\'))[0]\n                # else:\n                #     batch_size = tf.dtypes.cast(\n                #         x=tf.shape(input=some_state)[0], dtype=util.tf_dtype(dtype=\'long\')\n                #     )\n                batch_size = num_values - horizon_start\n                starts = tf.range(start=batch_size, dtype=util.tf_dtype(dtype=\'long\'))\n                lengths = tf.ones(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n                Module.update_tensors(dependency_starts=starts, dependency_lengths=lengths)\n\n            if self.estimate_actions:\n                _actions = OrderedDict()\n                for name, action in actions.items():\n                    _actions[name] = action[horizon_start:]\n                horizon_estimate = baseline.actions_value(\n                    states=_states, internals=_internals, auxiliaries=_auxiliaries, actions=_actions\n                )\n            else:\n                horizon_estimate = baseline.states_value(\n                    states=_states, internals=_internals, auxiliaries=_auxiliaries\n                )\n\n            # Expand rewards beyond terminal\n            terminal_zeros = tf.zeros(shape=(horizon,), dtype=util.tf_dtype(dtype=\'float\'))\n            if self.estimate_terminal:\n                rewards = tf.concat(\n                    values=(reward[:-1], horizon_estimate[-1:], terminal_zeros), axis=0\n                )\n\n            else:\n                with tf.control_dependencies(control_inputs=(assertion,)):\n                    last_reward = tf.where(\n                        condition=tf.math.greater(x=terminal[-1], y=one),\n                        x=horizon_estimate[-1], y=reward[-1]\n                    )\n                    rewards = tf.concat(\n                        values=(reward[:-1], (last_reward,), terminal_zeros), axis=0\n                    )\n\n            # Remove last if necessary\n            horizon_end = tf.where(\n                condition=tf.math.less_equal(x=num_values, y=horizon), x=zero,\n                y=(num_values - horizon)\n            )\n            horizon_estimate = horizon_estimate[:horizon_end]\n\n            # Expand missing estimates with zeros\n            terminal_size = tf.minimum(x=horizon, y=num_values)\n            terminal_estimate = tf.zeros(\n                shape=(terminal_size,), dtype=util.tf_dtype(dtype=\'float\')\n            )\n            horizon_estimate = tf.concat(values=(horizon_estimate, terminal_estimate), axis=0)\n\n        else:\n            # Expand rewards beyond terminal\n            terminal_zeros = tf.zeros(shape=(horizon,), dtype=util.tf_dtype(dtype=\'float\'))\n            rewards = tf.concat(values=(reward, terminal_zeros), axis=0)\n\n            # Zero estimate\n            horizon_estimate = tf.zeros(shape=(num_values,), dtype=util.tf_dtype(dtype=\'float\'))\n\n        # Calculate discounted sum\n        def cond(discounted_sum, horizon):\n            return tf.math.greater_equal(x=horizon, y=zero)\n\n        def body(discounted_sum, horizon):\n            # discounted_sum = tf.compat.v1.Print(\n            #     discounted_sum, (horizon, discounted_sum, rewards[horizon:]), summarize=10\n            # )\n            discounted_sum = discount * discounted_sum\n            discounted_sum = discounted_sum + rewards[horizon: horizon + num_values]\n            return discounted_sum, horizon - one\n\n        values[\'reward\'], _ = self.while_loop(\n            cond=cond, body=body, loop_vars=(horizon_estimate, horizon), back_prop=False\n        )\n\n        return values\n\n    def tf_enqueue(self, states, internals, auxiliaries, actions, terminal, reward, baseline=None):\n        # Constants and parameters\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n        horizon = self.horizon.value()\n        discount = self.discount.value()\n\n        assertions = list()\n        # Check whether horizon at most capacity\n        assertions.append(tf.debugging.assert_less_equal(\n            x=horizon, y=capacity,\n            message=""Estimator capacity has to be at least the same as the estimation horizon.""\n        ))\n        # Check whether at most one terminal\n        assertions.append(\n            tf.debugging.assert_less_equal(\n                x=tf.math.count_nonzero(\n                    input=terminal, dtype=util.tf_dtype(dtype=\'long\')\n                ), y=one, message=""Timesteps contain more than one terminal.""\n            )\n        )\n        # Check whether, if any, last value is terminal\n        assertions.append(\n            tf.debugging.assert_equal(\n                x=tf.reduce_any(input_tensor=tf.math.greater(x=terminal, y=zero)),\n                y=tf.math.greater(x=terminal[-1], y=zero),\n                message=""Terminal is not the last timestep.""\n            )\n        )\n\n        # Get number of overwritten values\n        with tf.control_dependencies(control_inputs=assertions):\n            if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                num_values = tf.shape(input=terminal, out_type=util.tf_dtype(dtype=\'long\'))[0]\n            else:\n                num_values = tf.dtypes.cast(\n                    x=tf.shape(input=terminal)[0], dtype=util.tf_dtype(dtype=\'long\')\n                )\n            overwritten_start = tf.maximum(x=self.buffer_index, y=capacity)\n            overwritten_limit = tf.maximum(x=(self.buffer_index + num_values), y=capacity)\n            num_overwritten = overwritten_limit - overwritten_start\n\n        def update_overwritten_rewards():\n            # Get relevant buffer rewards\n            buffer_limit = self.buffer_index + tf.minimum(\n                x=(num_overwritten + horizon), y=capacity\n            )\n            buffer_indices = tf.range(start=self.buffer_index, limit=buffer_limit)\n            buffer_indices = tf.math.mod(x=buffer_indices, y=capacity)\n            rewards = tf.gather(params=self.buffers[\'reward\'], indices=buffer_indices)\n\n            # Get relevant values rewards\n            values_limit = tf.maximum(x=(num_overwritten + horizon - capacity), y=zero)\n            rewards = tf.concat(values=(rewards, reward[:values_limit]), axis=0)\n\n            # Horizon baseline value\n            if self.estimate_horizon == \'early\':\n                assert baseline is not None\n                # Baseline estimate\n                buffer_indices = buffer_indices[horizon + one:]\n                _states = OrderedDict()\n                for name, buffer in self.buffers[\'states\'].items():\n                    state = tf.gather(params=buffer, indices=buffer_indices)\n                    _states[name] = tf.concat(\n                        values=(state, states[name][:values_limit + one]), axis=0\n                    )\n                _internals = OrderedDict()\n                for name, buffer in self.buffers[\'internals\'].items():\n                    internal = tf.gather(params=buffer, indices=buffer_indices)\n                    _internals[name] = tf.concat(\n                        values=(internal, internals[name][:values_limit + one]), axis=0\n                    )\n                _auxiliaries = OrderedDict()\n                for name, buffer in self.buffers[\'auxiliaries\'].items():\n                    auxiliary = tf.gather(params=buffer, indices=buffer_indices)\n                    _auxiliaries[name] = tf.concat(\n                        values=(auxiliary, auxiliaries[name][:values_limit + one]), axis=0\n                    )\n\n                # Dependency horizon\n                # TODO: handle arbitrary non-optimization horizons!\n                past_horizon = baseline.past_horizon(is_optimization=False)\n                assertion = tf.debugging.assert_equal(\n                    x=past_horizon, y=zero,\n                    message=""Temporary: baseline cannot depend on previous states.""\n                )\n                with tf.control_dependencies(control_inputs=(assertion,)):\n                    some_state = next(iter(_states.values()))\n                    if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                        batch_size = tf.shape(input=some_state, out_type=util.tf_dtype(dtype=\'long\'))[0]\n                    else:\n                        batch_size = tf.dtypes.cast(\n                            x=tf.shape(input=some_state)[0], dtype=util.tf_dtype(dtype=\'long\')\n                        )\n                    starts = tf.range(start=batch_size, dtype=util.tf_dtype(dtype=\'long\'))\n                    lengths = tf.ones(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n                    Module.update_tensors(dependency_starts=starts, dependency_lengths=lengths)\n\n                if self.estimate_actions:\n                    _actions = OrderedDict()\n                    for name, buffer in self.buffers[\'actions\'].items():\n                        action = tf.gather(params=buffer, indices=buffer_indices)\n                        _actions[name] = tf.concat(\n                            values=(action, actions[name][:values_limit]), axis=0\n                        )\n                    horizon_estimate = baseline.actions_value(\n                        states=_states, internals=_internals, auxiliaries=_auxiliaries,\n                        actions=_actions\n                    )\n                else:\n                    horizon_estimate = baseline.states_value(\n                        states=_states, internals=_internals, auxiliaries=_auxiliaries\n                    )\n\n            else:\n                # Zero estimate\n                horizon_estimate = tf.zeros(\n                    shape=(num_overwritten,), dtype=util.tf_dtype(dtype=\'float\')\n                )\n\n            # Calculate discounted sum\n            def cond(discounted_sum, horizon):\n                return tf.math.greater_equal(x=horizon, y=zero)\n\n            def body(discounted_sum, horizon):\n                # discounted_sum = tf.compat.v1.Print(\n                #     discounted_sum, (horizon, discounted_sum, rewards[horizon:]), summarize=10\n                # )\n                discounted_sum = discount * discounted_sum\n                discounted_sum = discounted_sum + rewards[horizon: horizon + num_overwritten]\n                return discounted_sum, horizon - one\n\n            discounted_sum, _ = self.while_loop(\n                cond=cond, body=body, loop_vars=(horizon_estimate, horizon), back_prop=False\n            )\n\n            assertions = [\n                tf.debugging.assert_equal(\n                    x=tf.shape(input=horizon_estimate), y=tf.shape(input=discounted_sum),\n                    message=""Estimation check.""\n                ),\n                tf.debugging.assert_equal(\n                    x=tf.shape(input=rewards, out_type=util.tf_dtype(dtype=\'long\'))[0],\n                    y=(horizon + num_overwritten), message=""Estimation check.""\n                )\n            ]\n\n            # Overwrite buffer rewards\n            with tf.control_dependencies(control_inputs=assertions):\n                indices = tf.range(\n                    start=self.buffer_index, limit=(self.buffer_index + num_overwritten)\n                )\n                indices = tf.math.mod(x=indices, y=capacity)\n                indices = tf.expand_dims(input=indices, axis=1)\n\n            assignment = self.buffers[\'reward\'].scatter_nd_update(\n                indices=indices, updates=discounted_sum\n            )\n\n            with tf.control_dependencies(control_inputs=(assignment,)):\n                return util.no_operation()\n\n        any_overwritten = tf.math.greater(x=num_overwritten, y=zero)\n        updated_rewards = self.cond(\n            pred=any_overwritten, true_fn=update_overwritten_rewards, false_fn=util.no_operation\n        )\n\n        # Overwritten buffer indices\n        with tf.control_dependencies(control_inputs=(updated_rewards,)):\n            indices = tf.range(start=overwritten_start, limit=overwritten_limit)\n            indices = tf.math.mod(x=indices, y=capacity)\n\n        # Get overwritten values\n        with tf.control_dependencies(control_inputs=(indices,)):\n            overwritten_values = OrderedDict()\n            for name, buffer in self.buffers.items():\n                if util.is_nested(name=name):\n                    overwritten_values[name] = OrderedDict()\n                    for inner_name, buffer in buffer.items():\n                        overwritten_values[name][inner_name] = tf.gather(\n                            params=buffer, indices=indices\n                        )\n                else:\n                    overwritten_values[name] = tf.gather(params=buffer, indices=indices)\n\n        # Buffer indices to (over)write\n        with tf.control_dependencies(control_inputs=util.flatten(xs=overwritten_values)):\n            indices = tf.range(start=self.buffer_index, limit=(self.buffer_index + num_values))\n            indices = tf.math.mod(x=indices, y=capacity)\n            indices = tf.expand_dims(input=indices, axis=1)\n\n        # Write new values\n        with tf.control_dependencies(control_inputs=(indices,)):\n            values = dict(\n                states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n                terminal=terminal, reward=reward\n            )\n            assignments = list()\n            for name, buffer in self.buffers.items():\n                if util.is_nested(name=name):\n                    for inner_name, buffer in buffer.items():\n                        assignment = buffer.scatter_nd_update(\n                            indices=indices, updates=values[name][inner_name]\n                        )\n                        assignments.append(assignment)\n                else:\n                    assignment = buffer.scatter_nd_update(indices=indices, updates=values[name])\n                    assignments.append(assignment)\n\n        # Increment buffer index\n        with tf.control_dependencies(control_inputs=assignments):\n            assignment = self.buffer_index.assign_add(delta=num_values, read_value=False)\n\n        # Return overwritten values or no-op\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            any_overwritten = tf.math.greater(x=num_overwritten, y=zero)\n            overwritten_values = util.fmap(\n                function=util.identity_operation, xs=overwritten_values\n            )\n            return any_overwritten, overwritten_values\n\n    def tf_complete(self, baseline, memory, indices, reward):\n        if self.estimate_horizon == \'late\':\n            assert baseline is not None\n            zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n            one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n\n            # Baseline dependencies\n            past_horizon = baseline.past_horizon(is_optimization=False)\n            assertion = tf.debugging.assert_equal(\n                x=past_horizon, y=zero,\n                message=""Temporary: baseline cannot depend on previous states.""\n            )\n            with tf.control_dependencies(control_inputs=(assertion,)):\n                if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                    batch_size = tf.shape(input=reward, out_type=util.tf_dtype(dtype=\'long\'))[0]\n                else:\n                    batch_size = tf.dtypes.cast(\n                        x=tf.shape(input=reward)[0], dtype=util.tf_dtype(dtype=\'long\')\n                    )\n                starts = tf.range(start=batch_size, dtype=util.tf_dtype(dtype=\'long\'))\n                lengths = tf.ones(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n                Module.update_tensors(dependency_starts=starts, dependency_lengths=lengths)\n\n            horizon = self.horizon.value()\n            discount = self.discount.value()\n\n            if self.estimate_actions:\n                # horizon change: see timestep-based batch sampling\n                horizons, (states, internals, auxiliaries, terminal) = memory.successors(\n                    indices=indices, horizon=(horizon + one),\n                    final_values=(\'states\', \'internals\', \'auxiliaries\', \'terminal\')\n                )\n                # TODO: Double DQN would require main policy here\n                actions = baseline.act(\n                    states=states, internals=internals, auxiliaries=auxiliaries,\n                    return_internals=False\n                )\n                horizon_estimate = baseline.actions_value(\n                    states=states, internals=internals, auxiliaries=auxiliaries, actions=actions\n                )\n            else:\n                # horizon change: see timestep-based batch sampling\n                horizons, (states, internals, auxiliaries, terminal) = memory.successors(\n                    indices=indices, horizon=(horizon + one),\n                    final_values=(\'states\', \'internals\', \'auxiliaries\', \'terminal\')\n                )\n                horizon_estimate = baseline.states_value(\n                    states=states, internals=internals, auxiliaries=auxiliaries\n                )\n\n            exponent = tf.dtypes.cast(x=horizons, dtype=util.tf_dtype(dtype=\'float\'))\n            discounts = tf.math.pow(x=discount, y=exponent)\n            if not self.estimate_terminal:\n                with tf.control_dependencies(control_inputs=(assertion,)):\n                    discounts = tf.where(\n                        condition=tf.math.equal(x=terminal, y=one),\n                        x=tf.zeros_like(input=discounts, dtype=util.tf_dtype(dtype=\'float\')),\n                        y=discounts\n                    )\n            reward = reward + discounts * tf.stop_gradient(input=horizon_estimate)\n            # TODO: stop gradients?\n\n        return reward\n\n    def tf_estimate(self, baseline, memory, indices, reward, is_baseline_optimized):\n        if self.estimate_advantage:\n            assert baseline is not None\n            zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n\n            # Baseline dependencies\n            past_horizon = baseline.past_horizon(is_optimization=is_baseline_optimized)\n            assertion = tf.debugging.assert_equal(\n                x=past_horizon, y=zero,\n                message=""Temporary: baseline cannot depend on previous states.""\n            )\n            with tf.control_dependencies(control_inputs=(assertion,)):\n                if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                    batch_size = tf.shape(input=reward, out_type=util.tf_dtype(dtype=\'long\'))[0]\n                else:\n                    batch_size = tf.dtypes.cast(\n                        x=tf.shape(input=reward)[0], dtype=util.tf_dtype(dtype=\'long\')\n                    )\n                starts = tf.range(start=batch_size, dtype=util.tf_dtype(dtype=\'long\'))\n                lengths = tf.ones(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n                Module.update_tensors(dependency_starts=starts, dependency_lengths=lengths)\n\n            if self.estimate_actions:\n                states, internals, auxiliaries, actions = memory.retrieve(\n                    indices=indices, values=(\'states\', \'internals\', \'auxiliaries\', \'actions\')\n                )\n                critic_estimate = baseline.actions_value(\n                    states=states, internals=internals, auxiliaries=auxiliaries, actions=actions\n                )\n            else:\n                states, internals, auxiliaries = memory.retrieve(\n                    indices=indices, values=(\'states\', \'internals\', \'auxiliaries\')\n                )\n                critic_estimate = baseline.states_value(\n                    states=states, internals=internals, auxiliaries=auxiliaries\n                )\n\n            reward = reward - critic_estimate\n\n        return reward\n'"
tensorforce/core/layers/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.layers.layer import Layer, StatefulLayer, TemporalLayer, TransformationBase\n\nfrom tensorforce.core.layers.convolution import Conv1d, Conv2d  # Conv1dTranspose, Conv2dTranspose\nfrom tensorforce.core.layers.dense import Dense\nfrom tensorforce.core.layers.embedding import Embedding\nfrom tensorforce.core.layers.internal_rnn import InternalGru, InternalLstm, InternalRnn\nfrom tensorforce.core.layers.keras import Keras\nfrom tensorforce.core.layers.linear import Linear\nfrom tensorforce.core.layers.misc import Activation, Block, Dropout, Function, Register, Reshape, \\\n    Retrieve, Reuse\nfrom tensorforce.core.layers.normalization import ExponentialNormalization, InstanceNormalization\nfrom tensorforce.core.layers.pooling import Flatten, Pooling, Pool1d, Pool2d\nfrom tensorforce.core.layers.preprocessing import Clipping, Deltafier, Image, PreprocessingLayer, \\\n    Sequence\nfrom tensorforce.core.layers.rnn import Gru, Lstm, Rnn\n\n\nlayer_modules = dict(\n    activation=Activation, block=Block, clipping=Clipping, conv1d=Conv1d, conv2d=Conv2d,\n    # conv1d_transpose=Conv1dTranspose, conv2d_transpose=Conv2dTranspose,\n    default=Function, deltafier=Deltafier, dense=Dense, dropout=Dropout, embedding=Embedding,\n    exponential_normalization=ExponentialNormalization, flatten=Flatten, function=Function,\n    gru=Gru, image=Image, instance_normalization=InstanceNormalization, internal_gru=InternalGru,\n    internal_lstm=InternalLstm, internal_rnn=InternalRnn, keras=Keras, linear=Linear, lstm=Lstm,\n    pooling=Pooling, pool1d=Pool1d, pool2d=Pool2d, register=Register, reshape=Reshape,\n    retrieve=Retrieve, reuse=Reuse, rnn=Rnn, sequence=Sequence\n)\n\n\n__all__ = [\n    \'Activation\', \'Block\', \'Clipping\', \'Conv1d\', \'Conv2d\',  # \'Conv1dTranspose\', \'Conv2dTranspose\',\n    \'Deltafier\', \'Dense\', \'Dropout\', \'Embedding\', \'ExponentialNormalization\', \'Flatten\',\n    \'Function\', \'GRU\', \'Image\', \'InstanceNormalization\', \'InternalGru\', \'InternalLayer\',\n    \'InternalLstm\', \'InternalRnn\', \'Keras\', \'Layer\', \'layer_modules\', \'Linear\', \'Lstm\',\n    \'Nonlinearity\', \'Pooling\', \'Pool1d\', \'Pool2d\', \'PreprocessingLayer\', \'Register\', \'Reshape\',\n    \'Retrieve\', \'Rnn\', \'Sequence\', \'StatefulLayer\', \'TemporalLayer\', \'TransformationBase\'\n]\n'"
tensorforce/core/layers/convolution.py,4,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom math import ceil\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.utils.conv_utils import conv_output_length, deconv_output_length\n\nfrom tensorforce import TensorforceError\nfrom tensorforce.core.layers import TransformationBase\n\n\nclass Conv1d(TransformationBase):\n    """"""\n    1-dimensional convolutional layer (specification key: `conv1d`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        window (int > 0): Window size\n            (<span style=""color:#00C000""><b>default</b></span>: 3).\n        stride (int > 0): Stride size\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        padding (\'same\' | \'valid\'): Padding type, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/nn/convolution>`__\n            (<span style=""color:#00C000""><b>default</b></span>: \'same\').\n        dilation (int > 0 | (int > 0, int > 0)): Dilation value\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: relu).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, size, window=3, stride=1, padding=\'same\', dilation=1, bias=True,\n        activation=\'relu\', dropout=0.0, is_trainable=True, input_spec=None, summary_labels=None,\n        l2_regularization=None\n    ):\n        self.window = window\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        super().__init__(\n            name=name, size=size, bias=bias, activation=activation, dropout=dropout,\n            is_trainable=is_trainable, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=(0, 0))\n\n    def get_output_spec(self, input_spec):\n        length = conv_output_length(\n            input_length=input_spec[\'shape\'][0], filter_size=self.window, padding=self.padding,\n            stride=self.stride, dilation=self.dilation\n        )\n\n        shape = (length,)\n\n        if self.squeeze:\n            input_spec[\'shape\'] = shape\n        else:\n            input_spec[\'shape\'] = shape + (self.size,)\n\n        input_spec.pop(\'min_value\', None)\n        input_spec.pop(\'max_value\', None)\n\n        return input_spec\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        in_size = self.input_spec[\'shape\'][1]\n\n        initializer = \'orthogonal\'\n        if self.activation is not None and self.activation.nonlinearity == \'relu\':\n            initializer += \'-relu\'\n\n        self.weights = self.add_variable(\n            name=\'weights\', dtype=\'float\', shape=(self.window, in_size, self.size),\n            is_trainable=self.is_trainable, initializer=initializer\n        )\n\n    def tf_apply(self, x):\n        x = tf.nn.conv1d(\n            input=x, filters=self.weights, stride=self.stride, padding=self.padding.upper(),\n            dilations=self.dilation\n        )\n\n        return super().tf_apply(x=x)\n\n\nclass Conv2d(TransformationBase):\n    """"""\n    2-dimensional convolutional layer (specification key: `conv2d`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        window (int > 0 | (int > 0, int > 0)): Window size\n            (<span style=""color:#00C000""><b>default</b></span>: 3).\n        stride (int > 0 | (int > 0, int > 0)): Stride size\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        padding (\'same\' | \'valid\'): Padding type, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/nn/convolution>`__\n            (<span style=""color:#00C000""><b>default</b></span>: \'same\').\n        dilation (int > 0 | (int > 0, int > 0)): Dilation value\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: ""relu"").\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, size, window=3, stride=1, padding=\'same\', dilation=1, bias=True,\n        activation=\'relu\', dropout=0.0, is_trainable=True, input_spec=None, summary_labels=None,\n        l2_regularization=None\n    ):\n        if isinstance(window, int):\n            self.window = (window, window)\n        elif len(window) == 2:\n            self.window = tuple(window)\n        else:\n            raise TensorforceError(""Invalid window argument for conv2d layer: {}."".format(window))\n        if isinstance(stride, int):\n            self.stride = (1, stride, stride, 1)\n        elif len(stride) == 2:\n            self.stride = (1, stride[0], stride[1], 1)\n        else:\n            raise TensorforceError(""Invalid stride argument for conv2d layer: {}."".format(stride))\n        self.padding = padding\n        if isinstance(dilation, int):\n            self.dilation = (1, dilation, dilation, 1)\n        elif len(dilation) == 2:\n            self.dilation = (1, dilation[0], dilation[1], 1)\n        else:\n            raise TensorforceError(\n                ""Invalid dilation argument for conv2d layer: {}."".format(dilation)\n            )\n\n        super().__init__(\n            name=name, size=size, bias=bias, activation=activation, dropout=dropout,\n            is_trainable=is_trainable, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=(0, 0, 0))\n\n    def get_output_spec(self, input_spec):\n        height = conv_output_length(\n            input_length=input_spec[\'shape\'][0], filter_size=self.window[0], padding=self.padding,\n            stride=self.stride[1], dilation=self.dilation[1]\n        )\n        width = conv_output_length(\n            input_length=input_spec[\'shape\'][1], filter_size=self.window[1], padding=self.padding,\n            stride=self.stride[2], dilation=self.dilation[2]\n        )\n        shape = (height, width)\n\n        if self.squeeze:\n            input_spec[\'shape\'] = shape\n        else:\n            input_spec[\'shape\'] = shape + (self.size,)\n\n        input_spec.pop(\'min_value\', None)\n        input_spec.pop(\'max_value\', None)\n\n        return input_spec\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        in_size = self.input_spec[\'shape\'][2]\n\n        initializer = \'orthogonal\'\n        if self.activation is not None and self.activation.nonlinearity == \'relu\':\n            initializer += \'-relu\'\n\n        self.weights = self.add_variable(\n            name=\'weights\', dtype=\'float\', shape=(self.window + (in_size, self.size)),\n            is_trainable=self.is_trainable, initializer=initializer\n        )\n\n    def tf_apply(self, x):\n        x = tf.nn.conv2d(\n            input=x, filters=self.weights, strides=self.stride, padding=self.padding.upper(),\n            dilations=self.dilation\n        )\n\n        return super().tf_apply(x=x)\n\n\n# class Conv1dTranspose(TransformationBase):\n#     """"""\n#     1-dimensional transposed convolutional layer, also known as deconvolution layer\n#     (specification key: `deconv1d`).\n\n#     Args:\n#         name (string): Layer name\n#             (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n#         size (int >= 0): Layer output size, 0 implies additionally removing the axis\n#             (<span style=""color:#C00000""><b>required</b></span>).\n#         window (int > 0): Window size\n#             (<span style=""color:#00C000""><b>default</b></span>: 3).\n#         output_width (int > 0): Output width\n#             (<span style=""color:#00C000""><b>default</b></span>: same as input).\n#         stride (int > 0): Stride size\n#             (<span style=""color:#00C000""><b>default</b></span>: 1).\n#         padding (\'same\' | \'valid\'): Padding type, see\n#             `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/nn/convolution>`__\n#             (<span style=""color:#00C000""><b>default</b></span>: \'same\').\n#         dilation (int > 0 | (int > 0, int > 0)): Dilation value\n#             (<span style=""color:#00C000""><b>default</b></span>: 1).\n#         bias (bool): Whether to add a trainable bias variable\n#             (<span style=""color:#00C000""><b>default</b></span>: true).\n#         activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n#             \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n#             (<span style=""color:#00C000""><b>default</b></span>: ""relu"").\n#         dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n#             (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n#         is_trainable (bool): Whether layer variables are trainable\n#             (<span style=""color:#00C000""><b>default</b></span>: true).\n#         input_spec (specification): Input tensor specification\n#             (<span style=""color:#00C000""><b>internal use</b></span>).\n#         summary_labels (\'all\' | iter[string]): Labels of summaries to record\n#             (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n#         l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n#             (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n#     """"""\n\n#     def __init__(\n#         self, name, size, window=3, output_width=None, stride=1, padding=\'same\', dilation=1,\n#         bias=True, activation=\'relu\', dropout=0.0, is_trainable=True, input_spec=None,\n#         summary_labels=None, l2_regularization=None\n#     ):\n#         self.window = window\n#         self.output_width = output_width\n#         self.stride = stride\n#         self.padding = padding\n#         self.dilation = dilation\n\n#         super().__init__(\n#             name=name, size=size, bias=bias, activation=activation, dropout=dropout,\n#             is_trainable=is_trainable, input_spec=input_spec, summary_labels=summary_labels,\n#             l2_regularization=l2_regularization\n#         )\n\n#     def default_input_spec(self):\n#         return dict(type=\'float\', shape=(0, 0))\n\n#     def get_output_spec(self, input_spec):\n#         length = deconv_output_length(\n#             input_length=input_spec[\'shape\'][0], filter_size=self.window, padding=self.padding,\n#             stride=self.stride, dilation=self.dilation\n#         )\n\n#         if self.output_width is None:\n#             self.output_width = length\n\n#         shape = (length,)\n\n#         if self.squeeze:\n#             input_spec[\'shape\'] = shape\n#         else:\n#             input_spec[\'shape\'] = shape + (self.size,)\n\n#         input_spec.pop(\'min_value\', None)\n#         input_spec.pop(\'max_value\', None)\n\n#         return input_spec\n\n#     def tf_initialize(self):\n#         super().tf_initialize()\n\n#         in_size = self.input_spec[\'shape\'][1]\n\n#         initializer = \'orthogonal\'\n#         if self.activation is not None and self.activation.nonlinearity == \'relu\':\n#             initializer += \'-relu\'\n\n#         self.weights = self.add_variable(\n#             name=\'weights\', dtype=\'float\', shape=(self.window, in_size, self.size),\n#             is_trainable=self.is_trainable, initializer=initializer\n#         )\n\n#     def tf_apply(self, x):\n#         x = tf.nn.conv1d_transpose(\n#             input=x, filters=self.weights, output_shape=(1, self.output_width, self.size),\n#             strides=self.stride, padding=self.padding.upper(), dilations=self.dilation\n#         )\n\n#         return super().tf_apply(x=x)\n\n\n# class Conv2dTranspose(TransformationBase):\n#     """"""\n#     2-dimensional transposed convolutional layer, also known as deconvolution layer\n#     (specification key: `deconv2d`).\n\n#     Args:\n#         name (string): Layer name\n#             (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n#         size (int >= 0): Layer output size, 0 implies additionally removing the axis\n#             (<span style=""color:#C00000""><b>required</b></span>).\n#         window (int > 0 | (int > 0, int > 0)): Window size\n#             (<span style=""color:#00C000""><b>default</b></span>: 3).\n#         output_shape (int > 0 | (int > 0, int > 0)): Output shape\n#             (<span style=""color:#00C000""><b>default</b></span>: same as input).\n#         stride (int > 0 | (int > 0, int > 0)): Stride size\n#             (<span style=""color:#00C000""><b>default</b></span>: 1).\n#         padding (\'same\' | \'valid\'): Padding type, see\n#             `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/nn/convolution>`__\n#             (<span style=""color:#00C000""><b>default</b></span>: \'same\').\n#         dilation (int > 0 | (int > 0, int > 0)): Dilation value\n#             (<span style=""color:#00C000""><b>default</b></span>: 1).\n#         bias (bool): Whether to add a trainable bias variable\n#             (<span style=""color:#00C000""><b>default</b></span>: true).\n#         activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n#             \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n#             (<span style=""color:#00C000""><b>default</b></span>: ""relu"").\n#         dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n#             (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n#         is_trainable (bool): Whether layer variables are trainable\n#             (<span style=""color:#00C000""><b>default</b></span>: true).\n#         input_spec (specification): Input tensor specification\n#             (<span style=""color:#00C000""><b>internal use</b></span>).\n#         summary_labels (\'all\' | iter[string]): Labels of summaries to record\n#             (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n#         l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n#             (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n#     """"""\n\n#     def __init__(\n#         self, name, size, window=3, output_shape=None, stride=1, padding=\'same\', dilation=1,\n#         bias=True, activation=\'relu\', dropout=0.0, is_trainable=True, input_spec=None,\n#         summary_labels=None, l2_regularization=None\n#     ):\n#         if isinstance(window, int):\n#             self.window = (window, window)\n#         elif len(window) == 2:\n#             self.window = tuple(window)\n#         else:\n#             raise TensorforceError(\n#                 ""Invalid window argument for conv2d_transpose layer: {}."".format(window)\n#             )\n#         if output_shape is None:\n#             self.output_shape = None\n#         elif isinstance(output_shape, int):\n#             self.output_shape = (1, output_shape, output_shape, size)\n#         elif len(output_shape) == 2:\n#             self.output_shape = (1, output_shape[0], output_shape[1], size)\n#         else:\n#             raise TensorforceError(\n#                 ""Invalid output_shape argument for conv2d_transpose layer: {}."".format(output_shape)\n#             )\n#         if isinstance(stride, int):\n#             self.stride = (1, stride, stride, 1)\n#         elif len(stride) == 2:\n#             self.stride = (1, stride[0], stride[1], 1)\n#         else:\n#             raise TensorforceError(\n#                 ""Invalid stride argument for conv2d_transpose layer: {}."".format(stride)\n#             )\n#         if isinstance(dilation, int):\n#             self.dilation = (1, dilation, dilation, 1)\n#         elif len(dilation) == 2:\n#             self.dilation = (1, dilation[0], dilation[1], 1)\n#         else:\n#             raise TensorforceError(\n#                 ""Invalid dilation argument for conv2d_transpose layer: {}."".format(dilation)\n#             )\n#         self.padding = padding\n\n#         super().__init__(\n#             name=name, size=size, bias=bias, activation=activation, dropout=dropout,\n#             is_trainable=is_trainable, input_spec=input_spec, summary_labels=summary_labels,\n#             l2_regularization=l2_regularization\n#         )\n\n#     def default_input_spec(self):\n#         return dict(type=\'float\', shape=(0, 0, 0))\n\n#     def get_output_spec(self, input_spec):\n#         height = deconv_output_length(\n#             input_length=input_spec[\'shape\'][0], filter_size=self.window[0], padding=self.padding,\n#             stride=self.stride[1], dilation=self.dilation[1]\n#         )\n#         width = deconv_output_length(\n#             input_length=input_spec[\'shape\'][1], filter_size=self.window[1], padding=self.padding,\n#             stride=self.stride[2], dilation=self.dilation[2]\n#         )\n\n#         if self.output_shape is None:\n#             self.output_shape = (1, height, width, self.size)\n\n#         shape = (height, width)\n\n#         if self.squeeze:\n#             input_spec[\'shape\'] = shape\n#         else:\n#             input_spec[\'shape\'] = shape + (self.size,)\n\n#         input_spec.pop(\'min_value\', None)\n#         input_spec.pop(\'max_value\', None)\n\n#         return input_spec\n\n#     def tf_initialize(self):\n#         super().tf_initialize()\n\n#         in_size = self.input_spec[\'shape\'][2]\n\n#         initializer = \'orthogonal\'\n#         if self.activation is not None and self.activation.nonlinearity == \'relu\':\n#             initializer += \'-relu\'\n\n#         self.weights = self.add_variable(\n#             name=\'weights\', dtype=\'float\', shape=(self.window + (in_size, self.size)),\n#             is_trainable=self.is_trainable, initializer=initializer\n#         )\n\n#     def tf_apply(self, x):\n#         x = tf.nn.conv2d_transpose(\n#             input=x, filters=self.weights, output_shape=self.output_shape, strides=self.stride,\n#             padding=self.padding.upper(), dilations=self.dilation\n#         )\n\n#         return super().tf_apply(x=x)\n'"
tensorforce/core/layers/dense.py,2,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce.core.layers import TransformationBase\n\n\nclass Dense(TransformationBase):\n    """"""\n    Dense fully-connected layer (specification key: `dense`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: tanh).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, size, bias=True, activation=\'tanh\', dropout=0.0, is_trainable=True,\n        input_spec=None, summary_labels=None, l2_regularization=None\n    ):\n        super().__init__(\n            name=name, size=size, bias=bias, activation=activation, dropout=dropout,\n            is_trainable=is_trainable, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=(0,))\n\n    def get_output_spec(self, input_spec):\n        if self.squeeze:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:-1]\n        else:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:-1] + (self.size,)\n        input_spec.pop(\'min_value\', None)\n        input_spec.pop(\'max_value\', None)\n\n        return input_spec\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        initializer = \'orthogonal\'\n        if self.activation is not None and self.activation.nonlinearity == \'relu\':\n            initializer += \'-relu\'\n\n        in_size = self.input_spec[\'shape\'][0]\n        self.weights = self.add_variable(\n            name=\'weights\', dtype=\'float\', shape=(in_size, self.size),\n            is_trainable=self.is_trainable, initializer=initializer\n        )\n\n    def tf_apply(self, x):\n        # tf.assert_rank_in(x=x, ranks=(2, 3, 4))\n        x = tf.matmul(a=x, b=self.weights)\n\n        return super().tf_apply(x=x)\n'"
tensorforce/core/layers/embedding.py,4,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core.layers import TransformationBase\n\n\nclass Embedding(TransformationBase):\n    """"""\n    Embedding layer (specification key: `embedding`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        num_embeddings (int > 0): If set, specifies the number of embeddings\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        max_norm (float): If set, embeddings are clipped if their L2-norm is larger\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: tanh).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments for potential parent class.\n    """"""\n\n    def __init__(\n        self, name, size, num_embeddings=None, max_norm=None, bias=True, activation=\'tanh\',\n        dropout=0.0, is_trainable=True, input_spec=None, summary_labels=None,\n        l2_regularization=None\n    ):\n        """"""\n        Embedding constructor.\n\n        Args:\n            size (int >= 0): Layer output size, 0 implies additionally removing the axis\n                (**required**).\n            bias (bool): Whether to add a trainable bias variable (default: false).\n            activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n                \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n                (default: \'tanh\').\n        """"""\n        super().__init__(\n            name=name, size=size, bias=bias, activation=activation, dropout=dropout,\n            is_trainable=is_trainable, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        self.num_embeddings = num_embeddings\n        self.max_norm = max_norm\n\n    def default_input_spec(self):\n        return dict(type=(\'int\', \'bool\'), shape=None, num_values=0)\n\n    def get_output_spec(self, input_spec):\n        input_spec[\'type\'] = \'float\'\n        if not self.squeeze:\n            if input_spec[\'shape\'] is None:\n                input_spec[\'shape\'] = (None, self.size)\n            else:\n                input_spec[\'shape\'] = input_spec[\'shape\'] + (self.size,)\n        input_spec.pop(\'num_values\', None)\n\n        return input_spec\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        if self.num_embeddings is None:\n            if self.input_spec[\'type\'] == \'bool\':\n                self.num_embeddings = 2\n            elif self.input_spec[\'type\'] == \'int\':\n                self.num_embeddings = self.input_spec[\'num_values\']\n                if self.num_embeddings == 0:\n                    raise TensorforceError.value(\n                        name=\'input_spec\', argument=\'num_values\', value=self.num_embeddings\n                    )\n\n        initializer = \'normal\'\n        if self.activation is not None and self.activation.nonlinearity == \'relu\':\n            initializer += \'-relu\'\n\n        self.weights = self.add_variable(\n            name=\'embeddings\', dtype=\'float\', shape=(self.num_embeddings, self.size),\n            is_trainable=self.is_trainable, initializer=initializer\n        )\n\n    def tf_apply(self, x):\n        if util.tf_dtype(\'int\') not in (tf.int32, tf.int64):\n            x = tf.dtypes.cast(x=x, dtype=tf.int32)\n        elif util.is_dtype(x=x, dtype=\'bool\'):\n            x = tf.dtypes.cast(x=x, dtype=util.tf_dtype(\'int\'))\n\n        x = tf.nn.embedding_lookup(params=self.weights, ids=x, max_norm=self.max_norm)\n\n        return super().tf_apply(x=x)\n'"
tensorforce/core/layers/internal_rnn.py,10,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nfrom math import sqrt\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core.layers import StatefulLayer, TransformationBase\n\n\nclass InternalRnn(StatefulLayer, TransformationBase):\n    """"""\n    Internal state RNN cell layer (specification key: `internal_rnn`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        cell (\'gru\' | \'lstm\'): The recurrent cell type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        length (parameter, long > 0): For truncated backpropagation through time\n            (<span style=""color:#C00000""><b>required</b></span>).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: tanh).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments for Keras RNN cell layer, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/layers>`__.\n    """"""\n\n    def __init__(\n        self, name, cell, size, length, bias=True, activation=\'tanh\', dropout=0.0,\n        is_trainable=True, input_spec=None, summary_labels=None, l2_regularization=None, **kwargs\n    ):\n        self.cell_type = cell\n\n        super().__init__(\n            name=name, size=size, bias=bias, activation=activation, dropout=dropout,\n            is_trainable=is_trainable, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization, optimization_horizon=length\n        )\n\n        if self.cell_type == \'gru\':\n            self.cell = tf.keras.layers.GRUCell(\n                units=self.size, name=\'cell\', **kwargs  # , dtype=util.tf_dtype(dtype=\'float\')\n            )\n        elif self.cell_type == \'lstm\':\n            self.cell = tf.keras.layers.LSTMCell(\n                units=self.size, name=\'cell\', **kwargs  # , dtype=util.tf_dtype(dtype=\'float\')\n            )\n        else:\n            raise TensorforceError.unexpected()\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=(0,))\n\n    def get_output_spec(self, input_spec):\n        if self.squeeze:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:-1]\n        else:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:-1] + (self.size,)\n        input_spec.pop(\'min_value\', None)\n        input_spec.pop(\'max_value\', None)\n\n        return input_spec\n\n    @classmethod\n    def internals_spec(cls, layer=None, cell=None, size=None, **kwargs):\n        internals_spec = OrderedDict()\n\n        if \'state\' in internals_spec:\n            raise TensorforceError.unexpected()\n\n        if layer is None:\n            assert cell is not None and size is not None\n        else:\n            assert cell is None and size is None\n            cell = layer.cell_type\n            size = layer.size\n\n        if cell == \'gru\':\n            shape = (size,)\n        elif cell == \'lstm\':\n            shape = (2, size)\n        internals_spec[\'state\'] = dict(type=\'float\', shape=shape)\n\n        return internals_spec\n\n    def internals_init(self):\n        internals_init = OrderedDict()\n\n        if self.cell_type == \'gru\':\n            shape = (self.size,)\n        elif self.cell_type == \'lstm\':\n            shape = (2, self.size)\n\n        stddev = min(0.1, sqrt(2.0 / self.size))\n        internals_init[\'state\'] = np.random.normal(scale=stddev, size=shape)\n\n        return internals_init\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        self.cell.build(input_shape=self.input_spec[\'shape\'][0])\n\n        for variable in self.cell.trainable_weights:\n            name = variable.name[variable.name.rindex(self.name + \'/\') + len(self.name) + 1: -2]\n            self.variables[name] = variable\n            if self.is_trainable:\n                self.trainable_variables[name] = variable\n        for variable in self.cell.non_trainable_weights:\n            name = variable.name[variable.name.rindex(self.name + \'/\') + len(self.name) + 1: -2]\n            self.variables[name] = variable\n\n    def tf_iterative_step(self, x, previous):\n        state = previous[\'state\']\n\n        if self.cell_type == \'gru\':\n            state = (state,)\n        elif self.cell_type == \'lstm\':\n            state = (state[:, 0, :], state[:, 1, :])\n\n        if util.tf_dtype(dtype=\'float\') not in (tf.float32, tf.float64):\n            x = tf.dtypes.cast(x=x, dtype=tf.float32)\n            state = util.fmap(function=(lambda x: tf.dtypes.cast(x=x, dtype=tf.float32)), xs=state)\n            state = tf.dtypes.cast(x=state, dtype=tf.float32)\n\n        x, state = self.cell(inputs=x, states=state)\n\n        if util.tf_dtype(dtype=\'float\') not in (tf.float32, tf.float64):\n            x = tf.dtypes.cast(x=x, dtype=util.tf_dtype(dtype=\'float\'))\n            state = util.fmap(\n                function=(lambda x: tf.dtypes.cast(x=x, dtype=util.tf_dtype(dtype=\'float\'))),\n                xs=state\n            )\n\n        if self.cell_type == \'gru\':\n            state = state[0]\n        elif self.cell_type == \'lstm\':\n            state = tf.stack(values=state, axis=1)\n\n        return x, OrderedDict(state=state)\n\n\nclass InternalGru(InternalRnn):\n    """"""\n    Internal state GRU cell layer (specification key: `internal_gru`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        cell (\'gru\' | \'lstm\'): The recurrent cell type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        length (parameter, long > 0): For truncated backpropagation through time\n            (<span style=""color:#C00000""><b>required</b></span>).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments for Keras GRU layer, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell>`__.\n    """"""\n\n    def __init__(\n        self, name, size, length, bias=False, activation=None, dropout=0.0, is_trainable=True,\n        input_spec=None, summary_labels=None, l2_regularization=None, **kwargs\n    ):\n        super().__init__(\n            name=name, cell=\'gru\', size=size, length=length, bias=bias, activation=activation,\n            dropout=dropout, is_trainable=is_trainable, input_spec=input_spec,\n            summary_labels=summary_labels, l2_regularization=l2_regularization, **kwargs\n        )\n\n    @classmethod\n    def internals_spec(cls, layer=None, **kwargs):\n        if layer is None:\n            return super().internals_spec(cell=\'gru\', **kwargs)\n        else:\n            return super().internals_spec(layer=layer)\n\n\nclass InternalLstm(InternalRnn):\n    """"""\n    Internal state LSTM cell layer (specification key: `internal_lstm`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        cell (\'gru\' | \'lstm\'): The recurrent cell type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        length (parameter, long > 0): For truncated backpropagation through time\n            (<span style=""color:#C00000""><b>required</b></span>).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments for Keras LSTM layer, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell>`__.\n    """"""\n\n    def __init__(\n        self, name, size, length, bias=False, activation=None, dropout=0.0, is_trainable=True,\n        input_spec=None, summary_labels=None, l2_regularization=None, **kwargs\n    ):\n        super().__init__(\n            name=name, cell=\'lstm\', size=size, length=length, bias=bias, activation=activation,\n            dropout=dropout, is_trainable=is_trainable, input_spec=input_spec,\n            summary_labels=summary_labels, l2_regularization=l2_regularization, **kwargs\n        )\n\n    @classmethod\n    def internals_spec(cls, layer=None, **kwargs):\n        if layer is None:\n            return super().internals_spec(cell=\'lstm\', **kwargs)\n        else:\n            return super().internals_spec(layer=layer)\n'"
tensorforce/core/layers/keras.py,2,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.layers import Layer\n\n\nclass Keras(Layer):\n    """"""\n    Keras layer (specification key: `keras`).\n\n    Args:\n        layer (string): Keras layer class name, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/layers>`__\n            (<span style=""color:#C00000""><b>required</b></span>).\n        kwargs: Arguments for the Keras layer, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/layers>`__.\n    """"""\n\n    def __init__(\n        self, name, layer, input_spec=None, summary_labels=None, l2_regularization=None, **kwargs\n    ):\n        self.keras_layer = getattr(tf.keras.layers, layer)(\n            name=name, dtype=util.tf_dtype(dtype=\'float\'), input_shape=input_spec[\'shape\'],\n            **kwargs\n        )\n\n        super().__init__(\n            name=name, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n    def default_input_spec(self):\n        return dict(type=None, shape=None)\n\n    def get_output_spec(self, input_spec):\n        shape = self.keras_layer.compute_output_shape(input_shape=((None,) + input_spec[\'shape\']))\n\n        return dict(type=\'float\', shape=shape[1:])\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        self.keras_layer.build(input_shape=((None,) + self.input_spec[\'shape\']))\n\n        for variable in self.keras_layer.trainable_weights:\n            name = variable.name[variable.name.rindex(self.name + \'/\') + len(self.name) + 1: -2]\n            self.variables[name] = variable\n            self.trainable_variables[name] = variable\n        for variable in self.keras_layer.non_trainable_weights:\n            name = variable.name[variable.name.rindex(self.name + \'/\') + len(self.name) + 1: -2]\n            self.variables[name] = variable\n\n    def tf_regularize(self):\n        regularization_loss = super().tf_regularize()\n\n        if len(self.keras_layer.losses) > 0:\n            regularization_loss += tf.math.add_n(inputs=self.keras_layer.losses)\n\n        return regularization_loss\n\n    def tf_apply(self, x, **kwargs):\n        return self.keras_layer.call(inputs=x)\n'"
tensorforce/core/layers/layer.py,57,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nimport tensorforce.core\nfrom tensorforce.core import Module, parameter_modules\nfrom tensorforce.core.parameters import Parameter\n\n\nclass Layer(Module):\n    """"""\n    Base class for neural network layers.\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    layers = None\n\n    def __init__(self, name, input_spec=None, summary_labels=None, l2_regularization=None):\n        super().__init__(\n            name=name, summary_labels=summary_labels, l2_regularization=l2_regularization\n        )\n\n        self.input_spec = self.default_input_spec()\n        self.input_spec = util.valid_value_spec(\n            value_spec=self.input_spec, accept_underspecified=True, return_normalized=True\n        )\n\n        if input_spec is not None:\n            input_spec = util.valid_value_spec(\n                value_spec=input_spec, accept_underspecified=True, return_normalized=True\n            )\n\n            self.input_spec = util.unify_value_specs(\n                value_spec1=self.input_spec, value_spec2=input_spec\n            )\n\n        # Copy so that spec can be modified\n        self.output_spec = self.get_output_spec(input_spec=dict(self.input_spec))\n        self.output_spec = util.valid_value_spec(\n            value_spec=self.output_spec, accept_underspecified=True, return_normalized=True\n        )\n\n        # Register layer globally\n        if Layer.layers is None:\n            Layer.layers = OrderedDict()\n        # if self.name in Layer.layers:\n        #     raise TensorforceError.unexpected()\n        Layer.layers[self.name] = self\n\n    @classmethod\n    def output_spec(cls, input_spec, **kwargs):\n        return dict(input_spec)\n\n    def default_input_spec(self):\n        """"""\n        Returns the general, context-independent input tensor specification of this layer.\n\n        Returns:\n            General input tensor specification.\n        """"""\n        raise NotImplementedError\n\n    def get_output_spec(self, input_spec):\n        """"""\n        Returns the output tensor specification for a given input tensor specification.\n\n        Args:\n            input_spec (specification): Input tensor specification.\n\n        Returns:\n            Output tensor specification.\n        """"""\n        return input_spec\n\n    def add_module(self, *args, **kwargs):\n        layer = super().add_module(*args, **kwargs)\n\n        if not isinstance(layer, (Layer, Parameter)):\n            raise TensorforceError.type(name=\'layer\', argument=\'sub-module\', dtype=type(layer))\n\n        return layer\n\n    def tf_apply(self, x):\n        return x\n\n    def create_tf_function(self, name, tf_function):\n        if tf_function.__name__ == \'tf_apply\':\n\n            def validated_tf_function(x):\n                if self.input_spec is not None and \\\n                        not util.is_consistent_with_value_spec(value_spec=self.input_spec, x=x):\n                    raise TensorforceError.value(name=\'layer.apply\', argument=\'input\', value=x)\n\n                x = tf_function(x=x)\n\n                if self.output_spec is not None and \\\n                        not util.is_consistent_with_value_spec(value_spec=self.output_spec, x=x):\n                    raise TensorforceError.value(name=\'layer.apply\', argument=\'output\', value=x)\n\n                return x\n\n            return super().create_tf_function(name=name, tf_function=validated_tf_function)\n\n        else:\n            return super().create_tf_function(name=name, tf_function=tf_function)\n\n\nclass TransformationBase(Layer):\n    """"""\n    Base class for transformation layers.\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments for potential parent class.\n    """"""\n\n    def __init__(\n        self, name, size, bias=False, activation=None, dropout=0.0, is_trainable=True,\n        input_spec=None, summary_labels=None, l2_regularization=None, **kwargs\n    ):\n        self.squeeze = (size == 0)\n        self.size = max(size, 1)\n        self.activation = None\n        self.dropout = None\n\n        super().__init__(\n            name=name, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization, **kwargs\n        )\n\n        self.bias = bias\n        if activation is None:\n            self.activation = None\n        else:\n            self.activation = self.add_module(\n                name=(self.name + \'-activation\'), module=\'activation\',\n                modules=tensorforce.core.layer_modules, nonlinearity=activation,\n                input_spec=self.output_spec\n            )\n        if dropout is None or dropout == 0.0:\n            self.dropout = None\n        else:\n            self.dropout = self.add_module(\n                name=(self.name + \'-dropout\'), module=\'dropout\',\n                modules=tensorforce.core.layer_modules, rate=dropout, input_spec=self.output_spec\n            )\n        self.is_trainable = is_trainable\n\n    def specify_input_output_spec(self, input_spec):\n        super().specify_input_output_spec(input_spec=input_spec)\n\n        if self.activation is not None:\n            self.activation.specify_input_output_spec(input_spec=self.output_spec)\n            assert self.activation.output_spec == self.output_spec\n        if self.dropout is not None:\n            self.dropout.specify_input_output_spec(input_spec=self.output_spec)\n            assert self.dropout.output_spec == self.output_spec\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        if self.bias:\n            self.bias = self.add_variable(\n                name=\'bias\', dtype=\'float\', shape=(self.size,), is_trainable=self.is_trainable,\n                initializer=(\'zeros\' if self.is_trainable else \'normal\')\n            )\n\n        else:\n            self.bias = None\n\n    def tf_apply(self, x):\n        # shape = self.get_output_spec()[\'shape\']\n        # if self.squeeze:\n        #     shape = shape + (1,)\n        # if not util.is_dtype(x=x, dtype=\'float\') or util.shape(x=x)[1:] != shape:\n        #     raise TensorforceError(""Invalid input tensor for generic layer: {}."".format(x))\n\n        if self.bias is not None:\n            x = tf.nn.bias_add(value=x, bias=self.bias)\n\n        if self.squeeze:\n            x = tf.squeeze(input=x, axis=-1)\n\n        if self.activation is not None:\n            x = self.activation.apply(x=x)\n\n        if self.dropout is not None:\n            x = self.dropout.apply(x=x)\n\n        return x\n\n\nclass TemporalLayer(Layer):\n    """"""\n    Base class for temporal layers, i.e. layers with a temporal dependency on previous states.\n    """"""\n\n    def __init__(\n        self, name, processing, dependency_horizon, input_spec=None, summary_labels=None,\n        l2_regularization=None, **kwargs\n    ):\n        """"""\n        Temporal layer constructor.\n\n        Args:\n            processing (\'cumulative\' | \'iterative\'): Temporal processing type (**required**).\n            dependency_horizon (parameter, long >= 0): (**required**).\n            kwargs: Additional arguments for potential parent class.\n        """"""\n        super().__init__(\n            name=name, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization, **kwargs\n        )\n\n        if processing not in (\'cumulative\', \'iterative\'):\n            raise TensorforceError.value(\n                name=\'temporal-layer\', argument=\'processing\', value=processing,\n                hint=\'not in {cumulative,iterative}\'\n            )\n\n        self.processing = processing\n\n        self.dependency_horizon = self.add_module(\n            name=\'dependency-horizon\', module=dependency_horizon, modules=parameter_modules,\n            is_trainable=False, dtype=\'long\', min_value=0\n        )\n\n    def tf_apply(self, x, initial=None):\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        dependency_starts = Module.retrieve_tensor(name=\'dependency_starts\')\n        dependency_lengths = Module.retrieve_tensor(name=\'dependency_lengths\')\n        if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n            batch_size = tf.shape(input=dependency_starts, out_type=util.tf_dtype(dtype=\'long\'))[0]\n        else:\n            batch_size = tf.dtypes.cast(\n                x=tf.shape(input=dependency_starts)[0], dtype=util.tf_dtype(dtype=\'long\')\n            )\n        zeros = tf.zeros(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n        ones = tf.ones(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n        # maximum_iterations = tf.math.reduce_max(input_tensor=lengths, axis=0)\n        horizon = self.dependency_horizon.value() + one  # including 0th step\n        starts = dependency_starts + tf.maximum(x=(dependency_lengths - horizon), y=zeros)\n        lengths = dependency_lengths - tf.maximum(x=(dependency_lengths - horizon), y=zeros)\n        horizon = tf.minimum(x=horizon, y=tf.math.reduce_max(input_tensor=lengths, axis=0))\n\n        if self.processing == \'cumulative\':\n\n            def body(indices, remaining, xs):\n                current_x = tf.gather(params=x, indices=indices)\n                current_x = tf.expand_dims(input=current_x, axis=1)\n                xs = tf.concat(values=(xs, current_x), axis=1)\n                remaining -= tf.where(condition=tf.math.equal(x=remaining, y=zeros), x=zeros, y=ones)\n                indices += tf.where(condition=tf.math.equal(x=remaining, y=zeros), x=zeros, y=ones)\n                return indices, remaining, xs\n\n            initial_xs = tf.zeros(\n                shape=((batch_size, 0) + self.output_spec[\'shape\']),\n                dtype=util.tf_dtype(dtype=self.output_spec[\'type\'])\n            )\n\n            final_indices, final_remaining, final_xs = self.while_loop(\n                cond=util.tf_always_true, body=body, loop_vars=(starts, lengths, initial_xs),\n                back_prop=True, maximum_iterations=horizon\n            )\n\n            # initial_xs = tf.gather(params=x, indices=starts)\n            # initial_xs = tf.expand_dims(input=initial_xs, axis=1)\n            # missing = tf.expand_dims(input=horizon, axis=0) - lengths\n            # missing -= tf.where(condition=tf.math.equal(x=missing, y=zeros), x=zeros, y=ones)\n            # starts += tf.where(condition=tf.math.equal(x=missing, y=zeros), x=ones, y=zeros)\n\n            # final_indices, final_counter, final_xs = self.while_loop(\n            #     cond=util.tf_always_true, body=body, loop_vars=(starts, missing, initial_xs),\n            #     back_prop=True, maximum_iterations=(horizon - one)\n            # )\n\n        elif self.processing == \'iterative\':\n\n            def body(indices, remaining, current_x, current_aggregates):\n                current_x = tf.gather(params=x, indices=indices)\n                next_x, next_aggregates = self.iterative_step(\n                    x=current_x, previous=current_aggregates\n                )\n                with tf.control_dependencies(control_inputs=(current_x, next_x)):\n                    is_finished = tf.math.equal(x=remaining, y=zeros)\n                    if isinstance(next_aggregates, dict):\n                        for name, current_aggregate, next_aggregate in util.zip_items(\n                            current_aggregates, next_aggregates\n                        ):\n                            condition = is_finished\n                            for _ in range(util.rank(x=current_aggregate) - 1):\n                                condition = tf.expand_dims(input=condition, axis=1)\n                            next_aggregates[name] = tf.where(\n                                condition=condition, x=current_aggregate, y=next_aggregate\n                            )\n                    else:\n                        condition = is_finished\n                        for _ in range(util.rank(x=current_aggregates) - 1):\n                            condition = tf.expand_dims(input=condition, axis=1)\n                        next_aggregates = tf.where(\n                            condition=condition, x=current_aggregates, y=next_aggregates\n                        )\n                    remaining -= tf.where(condition=is_finished, x=zeros, y=ones)\n                    indices += tf.where(\n                        condition=tf.math.equal(x=remaining, y=zeros), x=zeros, y=ones\n                    )\n                return indices, remaining, next_x, next_aggregates\n\n            initial_x = tf.zeros(\n                shape=((batch_size,) + self.output_spec[\'shape\']),\n                dtype=util.tf_dtype(dtype=self.output_spec[\'type\'])\n            )\n\n            if initial is None:\n                initial_aggregates = self.initial_values()\n            else:\n                initial_aggregates = initial\n\n            final_indices, final_remaining, final_x, final_aggregates = self.while_loop(\n                cond=util.tf_always_true, body=body,\n                loop_vars=(starts, lengths, initial_x, initial_aggregates), back_prop=True,\n                maximum_iterations=horizon\n            )\n\n        # assertions = [\n        #     tf.debugging.assert_equal(\n        #         x=final_indices, y=(tf.math.cumsum(x=dependency_lengths) - ones)\n        #     ),\n        #     tf.debugging.assert_equal(\n        #         x=tf.math.reduce_sum(input_tensor=final_remaining, axis=0), y=zero\n        #     )\n        # ]\n\n        # with tf.control_dependencies(control_inputs=assertions):\n        if self.processing == \'cumulative\':\n            return super().tf_apply(x=self.cumulative_apply(xs=final_xs))\n        elif self.processing == \'iterative\':\n            if initial is None:\n                return util.identity_operation(x=super().tf_apply(x=final_x))\n            else:\n                return util.identity_operation(x=super().tf_apply(x=final_x)), final_aggregates\n\n    def tf_cumulative_apply(self, xs):\n        raise NotImplementedError\n\n    def tf_initial_values(self):\n        raise NotImplementedError\n\n    def tf_iterative_step(self, x, previous):\n        raise NotImplementedError\n\n    def create_tf_function(self, name, tf_function):\n        if tf_function.__name__ == \'tf_apply\':\n\n            def validated_tf_function(x, initial=None):\n                if not util.is_consistent_with_value_spec(value_spec=self.input_spec, x=x):\n                    raise TensorforceError.value(name=\'layer.apply\', argument=\'input\', value=x)\n\n                # initial spec!\n\n                if initial is None:\n                    x = tf_function(x=x)\n                else:\n                    x, final = tf_function(x=x, initial=initial)\n\n                if not util.is_consistent_with_value_spec(value_spec=self.output_spec, x=x):\n                    raise TensorforceError.value(name=\'layer.apply\', argument=\'output\', value=x)\n\n                if initial is None:\n                    return x\n                else:\n                    return x, final\n\n            return super().create_tf_function(name=name, tf_function=validated_tf_function)\n\n        elif tf_function.__name__ == \'tf_cumulative_apply\':\n\n            def validated_tf_function(xs):\n                x = xs[:, 0, :]\n                if not util.is_consistent_with_value_spec(value_spec=self.input_spec, x=x):\n                    raise TensorforceError.value(name=\'layer.apply\', argument=\'input\', value=x)\n\n                x = tf_function(xs=xs)\n\n                if not util.is_consistent_with_value_spec(value_spec=self.output_spec, x=x):\n                    raise TensorforceError.value(name=\'layer.apply\', argument=\'output\', value=x)\n\n                return x\n\n            return super().create_tf_function(name=name, tf_function=validated_tf_function)\n\n        elif tf_function.__name__ == \'tf_iterative_step\':\n\n            def validated_tf_function(x, previous):\n                if not util.is_consistent_with_value_spec(value_spec=self.input_spec, x=x):\n                    raise TensorforceError.value(name=\'layer.apply\', argument=\'input\', value=x)\n\n                # previous spec!\n\n                x, previous = tf_function(x=x, previous=previous)\n\n                if not util.is_consistent_with_value_spec(value_spec=self.output_spec, x=x):\n                    raise TensorforceError.value(name=\'layer.apply\', argument=\'output\', value=x)\n\n                return x, previous\n\n            return super().create_tf_function(name=name, tf_function=validated_tf_function)\n\n        else:\n            return super().create_tf_function(name=name, tf_function=tf_function)\n\n\nclass StatefulLayer(TemporalLayer):\n    """"""\n    Base class for stateful layers, i.e. layers with a temporally evolving internal state.\n    """"""\n\n    def __init__(\n        self, name, optimization_horizon, input_spec=None, summary_labels=None,\n        l2_regularization=None, **kwargs\n    ):\n        """"""\n        Stateful layer constructor.\n\n        Args:\n            optimization_horizon (parameter, long > 0): (**required**).\n            kwargs: Additional arguments for potential parent class.\n        """"""\n        super().__init__(\n            name=name, processing=\'iterative\', dependency_horizon=optimization_horizon,\n            input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization, **kwargs\n        )\n\n    @classmethod\n    def internals_spec(cls, layer=None, **kwargs):\n        raise NotImplementedError\n\n    def internals_init(self):\n        raise NotImplementedError\n\n    # internals spec below!\n\n    # def create_tf_function(self, name, tf_function):\n    #     # if name[-6:] != \'.apply\':\n    #     if tf_function.__name__ == \'tf_apply\' or tf_function.__name__ == \'tf_apply_step\':\n\n    #         def validated_tf_function(x, previous):\n    #             if not util.is_consistent_with_value_spec(value_spec=self.input_spec, x=x):\n    #                 raise TensorforceError(""Invalid input arguments for tf_apply."")\n    #             if not all(\n    #                 util.is_consistent_with_value_spec(value_spec=spec, x=previous[name])\n    #                 for name, spec in self.__class__.internals_spec(layer=self).items()\n    #             ):\n    #                 raise TensorforceError(""Invalid input arguments for tf_apply."")\n\n    #             x, previous = tf_function(x=x, previous=previous)\n\n    #             if not util.is_consistent_with_value_spec(value_spec=self.output_spec, x=x):\n    #                 raise TensorforceError(""Invalid output arguments for tf_apply."")\n    #             if not all(\n    #                 util.is_consistent_with_value_spec(value_spec=spec, x=previous[name])\n    #                 for name, spec in self.__class__.internals_spec(layer=self).items()\n    #             ):\n    #                 raise TensorforceError(""Invalid input arguments for tf_apply."")\n\n    #             return x, previous\n\n    #         return super().create_tf_function(name=name, tf_function=validated_tf_function)\n\n    #     else:\n    #         return super().create_tf_function(name=name, tf_function=tf_function)\n\n\n    # def tf_apply(self, x, **internals):\n\n    #     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name=\'???\'))\n\n    #     # def true_fn():\n    #     batch_size = tf.shape(\n    #         input=next(iter(internals.values())), out_type=util.tf_dtype(dtype=\'long\')\n    #     )[0]\n    #     zeros = tf.zeros(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n    #     ones = tf.ones(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n\n    #     def body(indices, remaining, current_x, current_internals):\n    #         current_x = tf.gather(params=x, indices=indices)\n    #         next_x, next_internals = self.apply_step(x=current_x, **current_internals)\n    #         is_finished = tf.math.equal(x=remaining, y=zeros)\n    #         for name, internal, next_internal in util.zip_items(current_internals, next_internals):\n    #             next_internals[name] = tf.where(\n    #                 condition=is_finished, x=internal, y=next_internal\n    #             )\n    #         remaining -= tf.where(condition=is_finished, x=zeros, y=ones)\n    #         indices += tf.where(condition=tf.math.equal(x=remaining, y=zeros), x=zeros, y=ones)\n    #         return indices, remaining, next_x, next_internals\n\n    #     starts = Module.retrieve_tensor(name=\'sequence_starts\')\n    #     lengths = Module.retrieve_tensor(name=\'sequence_lengths\')\n    #     initial_x = tf.gather(params=x, indices=starts)  # could be constant zeros!\n    #     maximum_iterations = tf.math.reduce_max(input_tensor=lengths, axis=0)\n    #     final_indices, final_remaining, final_x, final_internals = self.while_loop(\n    #         cond=util.tf_always_true, body=body, loop_vars=(starts, lengths, initial_x, internals),\n    #         back_prop=True, maximum_iterations=maximum_iterations\n    #     )\n\n    #     assertions = [\n    #         tf.debugging.assert_equal(x=final_indices, y=(tf.math.cumsum(x=lengths) - ones)),\n    #         tf.debugging.assert_equal(\n    #             x=tf.math.reduce_sum(input_tensor=final_remaining, axis=0),\n    #             y=tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n    #         )\n    #     ]\n\n    #     with tf.control_dependencies(control_inputs=assertions):\n    #         return super().tf_apply(x=final_x), final_internals\n\n    #     # return final_x, final_internals\n\n    #     # def false_fn():\n    #     #     return self.apply_step(x=x, **internals)\n\n    #     # x, internals = self.cond(pred=optimization, true_fn=true_fn, false_fn=false_fn)\n\n    #     # return super().tf_apply(x=x), internals\n'"
tensorforce/core/layers/linear.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce import TensorforceError\nimport tensorforce.core\nfrom tensorforce.core.layers import Layer\n\n\nclass Linear(Layer):\n    """"""\n    Linear layer (specification key: `linear`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, size, bias=True, is_trainable=True, input_spec=None, summary_labels=None,\n        l2_regularization=None\n    ):\n        self.size = size\n        self.bias = bias\n        self.is_trainable = is_trainable\n\n        super().__init__(\n            name=name, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=None)\n\n    def get_output_spec(self, input_spec):\n        if len(input_spec[\'shape\']) == 1:\n            self.linear = self.add_module(\n                name=(self.name + \'-linear\'), module=\'dense\',\n                modules=tensorforce.core.layer_modules, size=self.size, bias=self.bias,\n                activation=None, dropout=0.0, is_trainable=self.is_trainable, input_spec=input_spec\n            )\n\n        elif len(input_spec[\'shape\']) == 2:\n            self.linear = self.add_module(\n                name=(self.name + \'-linear\'), module=\'conv1d\',\n                modules=tensorforce.core.layer_modules, size=self.size, window=1, bias=self.bias,\n                activation=None, dropout=0.0, is_trainable=self.is_trainable, input_spec=input_spec\n            )\n\n        elif len(input_spec[\'shape\']) == 3:\n            self.linear = self.add_module(\n                name=(self.name + \'-linear\'), module=\'conv2d\',\n                modules=tensorforce.core.layer_modules, size=self.size, window=1, bias=self.bias,\n                activation=None, dropout=0.0, is_trainable=self.is_trainable, input_spec=input_spec\n            )\n\n        else:\n            raise TensorforceError.unexpected()\n\n        return self.linear.get_output_spec(input_spec=input_spec)\n\n    def tf_apply(self, x):\n        return self.linear.apply(x=x)\n'"
tensorforce/core/layers/misc.py,25,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import Counter\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nimport tensorforce.core\nfrom tensorforce.core import Module, parameter_modules\nfrom tensorforce.core.layers import Layer\n\n\nclass Activation(Layer):\n    """"""\n    Activation layer (specification key: `activation`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        nonlinearity (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Nonlinearity\n            (<span style=""color:#C00000""><b>required</b></span>).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, nonlinearity, input_spec=None, summary_labels=None\n    ):\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n        # Nonlinearity\n        if nonlinearity not in (\n            \'crelu\', \'elu\', \'leaky-relu\', \'none\', \'relu\', \'selu\', \'sigmoid\', \'softmax\', \'softplus\',\n            \'softsign\', \'swish\', \'tanh\'\n        ):\n            raise TensorforceError.value(\n                name=\'activation\', argument=\'nonlinearity\', value=nonlinearity\n            )\n        self.nonlinearity = nonlinearity\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=None)\n\n    def tf_apply(self, x):\n        if self.nonlinearity == \'crelu\':\n            x = tf.nn.crelu(features=x)\n\n        elif self.nonlinearity == \'elu\':\n            x = tf.nn.elu(features=x)\n\n        elif self.nonlinearity == \'leaky-relu\':\n            x = tf.nn.leaky_relu(features=x, alpha=0.2)  # alpha argument???\n\n        elif self.nonlinearity == \'none\':\n            pass\n\n        elif self.nonlinearity == \'relu\':\n            x = tf.nn.relu(features=x)\n            x = self.add_summary(\n                label=\'relu\', name=\'relu\', tensor=tf.math.zero_fraction(value=x), pass_tensors=x\n            )\n\n        elif self.nonlinearity == \'selu\':\n            x = tf.nn.selu(features=x)\n\n        elif self.nonlinearity == \'sigmoid\':\n            x = tf.sigmoid(x=x)\n\n        elif self.nonlinearity == \'softmax\':\n            x = tf.nn.softmax(logits=x)\n\n        elif self.nonlinearity == \'softplus\':\n            x = tf.nn.softplus(features=x)\n\n        elif self.nonlinearity == \'softsign\':\n            x = tf.nn.softsign(features=x)\n\n        elif self.nonlinearity == \'swish\':\n            # https://arxiv.org/abs/1710.05941\n            x = tf.sigmoid(x=x) * x\n\n        elif self.nonlinearity == \'tanh\':\n            x = tf.nn.tanh(x=x)\n\n        return x\n\n\nclass Block(Layer):\n    """"""\n    Block of layers (specification key: `block`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        layers (iter[specification]): Layers configuration, see [layers](../modules/layers.html)\n            (<span style=""color:#C00000""><b>required</b></span>).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n    """"""\n\n    def __init__(self, name, layers, input_spec=None):\n        # TODO: handle internal states and combine with layered network\n        if len(layers) == 0:\n            raise TensorforceError.value(\n                name=\'block\', argument=\'layers\', value=layers, hint=\'zero length\'\n            )\n\n        self._input_spec = input_spec\n        self.layers = layers\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=None)\n\n    def default_input_spec(self):\n        layer_counter = Counter()\n        for n, layer_spec in enumerate(self.layers):\n            if \'name\' in layer_spec:\n                layer_spec = dict(layer_spec)\n                layer_name = layer_spec.pop(\'name\')\n            else:\n                if isinstance(layer_spec.get(\'type\'), str):\n                    layer_type = layer_spec[\'type\']\n                else:\n                    layer_type = \'layer\'\n                layer_name = layer_type + str(layer_counter[layer_type])\n                layer_counter[layer_type] += 1\n\n            # layer_name = self.name + \'-\' + layer_name\n            self.layers[n] = self.add_module(\n                name=layer_name, module=layer_spec, modules=tensorforce.core.layer_modules,\n                input_spec=self._input_spec\n            )\n            self._input_spec = self.layers[n].output_spec\n\n        return self.layers[0].default_input_spec()\n\n    def get_output_spec(self, input_spec):\n        for layer in self.layers:\n            input_spec = layer.get_output_spec(input_spec=input_spec)\n        return input_spec\n\n    def tf_apply(self, x):\n        for layer in self.layers:\n            x = layer.apply(x=x)\n        return x\n\n\nclass Dropout(Layer):\n    """"""\n    Dropout layer (specification key: `dropout`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        rate (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#C00000""><b>required</b></span>).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, rate, input_spec=None, summary_labels=None):\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n        # Rate\n        self.rate = self.add_module(\n            name=\'rate\', module=rate, modules=parameter_modules, dtype=\'float\', min_value=0.0,\n            max_value=1.0\n        )\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=None)\n\n    def set_input_spec(self, spec):\n        super().set_input_spec(spec=spec)\n\n        if spec[\'type\'] != \'float\':\n            raise TensorforceError.value(\n                name=\'dropout\', argument=\'input_spec\', value=spec[\'type\'], hint=\'!= float\'\n            )\n\n    def tf_apply(self, x):\n        rate = self.rate.value()\n\n        def no_dropout():\n            return x\n\n        def apply_dropout():\n            dropout = tf.nn.dropout(x=x, rate=rate)\n            return self.add_summary(\n                label=\'dropout\', name=\'dropout\', tensor=tf.math.zero_fraction(value=dropout),\n                pass_tensors=dropout\n            )\n\n        skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name=\'deterministic\'))\n        zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n        skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))\n        return self.cond(pred=skip_dropout, true_fn=no_dropout, false_fn=apply_dropout)\n\n\nclass Function(Layer):\n    """"""\n    Custom TensorFlow function layer (specification key: `function`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        function (lambda[x -> x]): TensorFlow function\n            (<span style=""color:#C00000""><b>required</b></span>).\n        output_spec (specification): Output tensor specification containing type and/or shape\n            information (<span style=""color:#00C000""><b>default</b></span>: same as input).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    # (requires function as first argument)\n    def __init__(\n        self, name, function, output_spec=None, input_spec=None, summary_labels=None,\n        l2_regularization=None\n    ):\n        self.output_spec = output_spec\n\n        super().__init__(\n            name=name, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        self.function = function\n\n    def default_input_spec(self):\n        return dict(type=None, shape=None)\n\n    def get_output_spec(self, input_spec):\n        if self.output_spec is not None:\n            input_spec.update(self.output_spec)\n\n        return input_spec\n\n    def tf_apply(self, x):\n        return self.function(x)\n\n\nclass Register(Layer):\n    """"""\n    Tensor retrieval layer, which is useful when defining more complex network architectures which\n    do not follow the sequential layer-stack pattern, for instance, when handling multiple inputs\n    (specification key: `register`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        tensor (string): Name under which tensor will be registered\n            (<span style=""color:#C00000""><b>required</b></span>).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, tensor, input_spec=None, summary_labels=None):\n        """"""\n        Register layer constructor.\n\n        Args:\n        """"""\n        if not isinstance(tensor, str):\n            raise TensorforceError.type(name=\'register\', argument=\'tensor\', dtype=type(tensor))\n\n        self.tensor = tensor\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n        Module.register_tensor(name=self.tensor, spec=self.input_spec, batched=True)\n\n        self.output_spec = None\n\n    def default_input_spec(self):\n        return dict(type=None, shape=None)\n\n    def tf_apply(self, x):\n        last_scope = Module.global_scope.pop()\n        Module.update_tensor(name=self.tensor, tensor=x)\n        Module.global_scope.append(last_scope)\n\n        return x\n\n\nclass Reshape(Layer):\n    """"""\n    Reshape layer (specification key: `reshape`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        shape (<i>int | iter[int]</i>): New shape\n            (<span style=""color:#C00000""><b>required</b></span>).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, shape, input_spec=None, summary_labels=None):\n        if isinstance(shape, int):\n            self.shape = (shape,)\n        else:\n            self.shape = tuple(shape)\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n    def default_input_spec(self):\n        return dict(type=None, shape=None)\n\n    def get_output_spec(self, input_spec):\n        if util.product(xs=input_spec[\'shape\']) != util.product(xs=self.shape):\n            raise TensorforceError.value(name=\'Reshape\', argument=\'shape\', value=self.shape)\n\n        input_spec[\'shape\'] = self.shape\n\n        return input_spec\n\n    def tf_apply(self, x):\n        x = tf.reshape(tensor=x, shape=((-1,) + self.shape))\n\n        return x\n\n\nclass Retrieve(Layer):\n    """"""\n    Tensor retrieval layer, which is useful when defining more complex network architectures which\n    do not follow the sequential layer-stack pattern, for instance, when handling multiple inputs\n    (specification key: `retrieve`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        tensors (iter[string]): Names of global tensors to retrieve, for instance, state names or\n            previously registered global tensor names\n            (<span style=""color:#C00000""><b>required</b></span>).\n        aggregation (\'concat\' | \'product\' | \'stack\' | \'sum\'): Aggregation type in case of multiple\n            tensors\n            (<span style=""color:#00C000""><b>default</b></span>: \'concat\').\n        axis (int >= 0): Aggregation axis, excluding batch axis\n            (<span style=""color:#00C000""><b>default</b></span>: 0).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, tensors, aggregation=\'concat\', axis=0, input_spec=None, summary_labels=None\n    ):\n        if not isinstance(tensors, str) and not util.is_iterable(x=tensors):\n            raise TensorforceError.type(name=\'retrieve\', argument=\'tensors\', dtype=type(tensors))\n        elif util.is_iterable(x=tensors) and len(tensors) == 0:\n            raise TensorforceError.value(\n                name=\'retrieve\', argument=\'tensors\', value=tensors, hint=\'zero length\'\n            )\n        if aggregation not in (\'concat\', \'product\', \'stack\', \'sum\'):\n            raise TensorforceError.value(\n                name=\'retrieve\', argument=\'aggregation\', value=aggregation,\n                hint=\'not in {concat,product,stack,sum}\'\n            )\n\n        self.tensors = (tensors,) if isinstance(tensors, str) else tuple(tensors)\n        self.aggregation = aggregation\n        self.axis = axis\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n        self.input_spec = None\n\n    def default_input_spec(self):\n        return dict(type=None, shape=None)\n\n    def get_output_spec(self, input_spec):\n        if len(self.tensors) == 1:\n            return Module.get_tensor_spec(name=self.tensors[0])\n\n        # Get tensor types and shapes\n        dtypes = list()\n        shapes = list()\n        for tensor in self.tensors:\n            # Tensor specification\n            if tensor == \'*\':\n                spec = input_spec\n            else:\n                spec = Module.get_tensor_spec(name=tensor)\n            dtypes.append(spec[\'type\'])\n            shapes.append(spec[\'shape\'])\n\n        # Check tensor types\n        if all(dtype == dtypes[0] for dtype in dtypes):\n            dtype = dtypes[0]\n        else:\n            raise TensorforceError.value(name=\'retrieve\', argument=\'tensor types\', value=dtypes)\n\n        if self.aggregation == \'concat\':\n            if any(len(shape) != len(shapes[0]) for shape in shapes):\n                raise TensorforceError.value(\n                    name=\'retrieve\', argument=\'tensor shapes\', value=shapes\n                )\n            elif any(\n                shape[n] != shapes[0][n] for shape in shapes for n in range(len(shape))\n                if n != self.axis\n            ):\n                raise TensorforceError.value(\n                    name=\'retrieve\', argument=\'tensor shapes\', value=shapes\n                )\n            shape = tuple(\n                sum(shape[n] for shape in shapes) if n == self.axis else shapes[0][n]\n                for n in range(len(shapes[0]))\n            )\n\n        elif self.aggregation == \'stack\':\n            if any(len(shape) != len(shapes[0]) for shape in shapes):\n                raise TensorforceError.value(\n                    name=\'retrieve\', argument=\'tensor shapes\', value=shapes\n                )\n            elif any(shape[n] != shapes[0][n] for shape in shapes for n in range(len(shape))):\n                raise TensorforceError.value(\n                    name=\'retrieve\', argument=\'tensor shapes\', value=shapes\n                )\n            shape = tuple(\n                len(shapes) if n == self.axis else shapes[0][n - int(n > self.axis)]\n                for n in range(len(shapes[0]) + 1)\n            )\n\n        else:\n            # Check and unify tensor shapes\n            for shape in shapes:\n                if len(shape) != len(shapes[0]):\n                    raise TensorforceError.value(\n                        name=\'retrieve\', argument=\'tensor shapes\', value=shapes\n                    )\n                if any(x != y and x != 1 and y != 1 for x, y in zip(shape, shapes[0])):\n                    raise TensorforceError.value(\n                        name=\'retrieve\', argument=\'tensor shapes\', value=shapes\n                    )\n            shape = tuple(max(shape[n] for shape in shapes) for n in range(len(shapes[0])))\n\n        # Missing num_values, min/max_value!!!\n        return dict(type=dtype, shape=shape)\n\n    def tf_apply(self, x):\n        if len(self.tensors) == 1:\n            if self.tensors == \'*\':\n                return x\n            else:\n                last_scope = Module.global_scope.pop()\n                x = Module.retrieve_tensor(name=self.tensors[0])\n                Module.global_scope.append(last_scope)\n                return x\n\n        tensors = list()\n        for tensor in self.tensors:\n            if tensor == \'*\':\n                tensors.append(x)\n            else:\n                last_scope = Module.global_scope.pop()\n                tensors.append(Module.retrieve_tensor(name=tensor))\n                Module.global_scope.append(last_scope)\n\n        shape = self.output_spec[\'shape\']\n        for n, tensor in enumerate(tensors):\n            for axis in range(util.rank(x=tensor), len(shape)):\n                tensor = tf.expand_dims(input=tensor, axis=axis)\n            tensors[n] = tensor\n\n        if self.aggregation == \'concat\':\n            x = tf.concat(values=tensors, axis=(self.axis + 1))\n\n        elif self.aggregation == \'product\':\n            x = tf.stack(values=tensors, axis=(self.axis + 1))\n            x = tf.reduce_prod(input_tensor=x, axis=(self.axis + 1))\n\n        elif self.aggregation == \'stack\':\n            x = tf.stack(values=tensors, axis=(self.axis + 1))\n\n        elif self.aggregation == \'sum\':\n            x = tf.stack(values=tensors, axis=(self.axis + 1))\n            x = tf.reduce_sum(input_tensor=x, axis=(self.axis + 1))\n\n        return x\n\n\nclass Reuse(Layer):\n    """"""\n    Reuse layer (specification key: `reuse`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        layer (string): Name of a previously defined layer\n            (<span style=""color:#C00000""><b>required</b></span>).\n        is_trainable (bool): Whether reused layer variables are kept trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n    """"""\n\n    def __init__(self, name, layer, is_trainable=True, input_spec=None):\n        self.layer = layer\n        self.is_trainable = is_trainable\n\n        super().__init__(\n            name=name, input_spec=input_spec, summary_labels=None, l2_regularization=0.0\n        )\n\n    def default_input_spec(self):\n        # from tensorforce.core.networks import Network\n\n        # if not isinstance(self.parent, Network):\n        #     raise TensorforceError.unexpected()\n\n        # if self.layer not in self.parent.modules:\n        #     raise TensorforceError.unexpected()\n\n        # self.layer = self.parent.modules[self.layer]\n\n        if self.layer not in Layer.layers:\n            raise TensorforceError.value(name=\'reuse\', argument=\'layer\', value=self.layer)\n\n        self.layer = Layer.layers[self.layer]\n\n        return dict(self.layer.input_spec)\n\n    def get_output_spec(self, input_spec):\n        return self.layer.get_output_spec(input_spec=input_spec)\n\n    def tf_apply(self, x):\n        return self.layer.apply(x=x)\n\n    def get_variables(self, only_trainable=False, only_saved=False):\n        variables = super().get_variables(only_trainable=only_trainable, only_saved=only_saved)\n\n        if only_trainable and self.is_trainable:\n            variables.extend(self.layer.get_variables(\n                only_trainable=only_trainable, only_saved=only_saved\n            ))\n        elif only_saved:\n            pass\n        else:\n            variables.extend(self.layer.get_variables(\n                only_trainable=only_trainable, only_saved=only_saved\n            ))\n\n        return variables\n\n    def get_available_summaries(self):\n        summaries = super().get_available_summaries()\n        summaries.update(self.layer.get_available_summaries())\n        return sorted(summaries)\n'"
tensorforce/core/layers/normalization.py,22,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import Module, parameter_modules\nfrom tensorforce.core.layers import Layer\n\n\nclass ExponentialNormalization(Layer):\n    """"""\n    Normalization layer based on the exponential moving average (specification key:\n    `exponential_normalization`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        decay (parameter, 0.0 <= float <= 1.0): Decay rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.999).\n        axes (iter[int >= 0]): Normalization axes, excluding batch axis\n            (<span style=""color:#00C000""><b>default</b></span>: all but last axis).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, decay=0.999, axes=None, input_spec=None, summary_labels=None):\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n        self.decay = self.add_module(\n            name=\'decay\', module=decay, modules=parameter_modules, dtype=\'float\', min_value=0.0,\n            max_value=1.0\n        )\n\n        self.axes = axes if axes is None else tuple(axes)\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=None)\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        shape = self.input_spec[\'shape\']\n        if self.axes is None:\n            if len(shape) > 0:\n                self.axes = tuple(range(len(shape) - 1))\n                shape = tuple(1 for _ in shape[:-1]) + (shape[-1],)\n            else:\n                self.axes = ()\n        else:\n            shape = tuple(1 if axis in self.axes else dims for axis, dims in enumerate(shape))\n        shape = (1,) + shape\n\n        self.moving_mean = self.add_variable(\n            name=\'mean\', dtype=\'float\', shape=shape, is_trainable=False, initializer=\'zeros\'\n        )\n\n        self.moving_variance = self.add_variable(\n            name=\'variance\', dtype=\'float\', shape=shape, is_trainable=False, initializer=\'zeros\'\n        )\n\n        self.after_first_call = self.add_variable(\n            name=\'after-first-call\', dtype=\'bool\', shape=(), is_trainable=False,\n            initializer=\'zeros\'\n        )\n\n        # self.update_on_optimization = self.add_variable(\n        #     name=\'update-on-optimization\', dtype=\'bool\', shape=(), is_trainable=False,\n        #     initializer=\'zeros\'\n        # )\n\n    def tf_apply(self, x):\n\n        def no_update():\n            return self.moving_mean, self.moving_variance\n\n        def apply_update():\n            one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n            axes = tuple(1 + axis for axis in self.axes)\n\n            decay = self.decay.value()\n            batch_size = tf.dtypes.cast(x=tf.shape(input=x)[0], dtype=util.tf_dtype(dtype=\'float\'))\n            decay = tf.math.pow(x=decay, y=batch_size)\n\n            mean = tf.math.reduce_mean(input_tensor=x, axis=axes, keepdims=True)\n            mean = tf.where(\n                condition=self.after_first_call,\n                x=(decay * self.moving_mean + (one - decay) * mean), y=mean\n            )\n\n            variance = tf.reduce_mean(\n                input_tensor=tf.math.squared_difference(x=x, y=mean), axis=axes, keepdims=True\n            )\n            variance = tf.where(\n                condition=self.after_first_call,\n                x=(decay * self.moving_variance + (one - decay) * variance), y=variance\n            )\n\n            with tf.control_dependencies(control_inputs=(mean, variance)):\n                assignment = self.after_first_call.assign(\n                    value=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')),\n                    read_value=False\n                )\n\n            with tf.control_dependencies(control_inputs=(assignment,)):\n                variance = self.moving_variance.assign(value=variance)\n                mean = self.moving_mean.assign(value=mean)\n\n            return mean, variance\n\n        # optimization = Module.retrieve_tensor(name=\'optimization\')\n        # update_on_optimization = tf.where(\n        #     condition=self.after_first_call, x=self.update_on_optimization, y=optimization\n        # )\n        # update_on_optimization = self.update_on_optimization.assign(value=update_on_optimization)\n        # skip_update = tf.math.logical_or(\n        #     x=Module.retrieve_tensor(name=\'independent\'),\n        #     y=tf.math.not_equal(x=update_on_optimization, y=optimization)\n        # )\n        skip_update = Module.retrieve_tensor(name=\'independent\')\n\n        mean, variance = self.cond(pred=skip_update, true_fn=no_update, false_fn=apply_update)\n\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        reciprocal_stddev = tf.math.rsqrt(x=tf.maximum(x=variance, y=epsilon))\n\n        x = (x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)\n\n        return x\n\n\nclass InstanceNormalization(Layer):\n    """"""\n    Instance normalization layer (specification key: `instance_normalization`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        axes (iter[int >= 0]): Normalization axes, excluding batch axis\n            (<span style=""color:#00C000""><b>default</b></span>: all).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, axes=None, input_spec=None, summary_labels=None):\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n        self.axes = axes if axes is None else tuple(axes)\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=None)\n\n    def tf_apply(self, x):\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n\n        if self.axes is None:\n            mean, variance = tf.nn.moments(\n                x=x, axes=tuple(range(1, len(self.input_spec[\'shape\']))), keepdims=True\n            )\n        else:\n            mean, variance = tf.nn.moments(\n                x=x, axes=tuple(1 + axis for axis in self.axes), keepdims=True\n            )\n\n        reciprocal_stddev = tf.math.rsqrt(x=tf.maximum(x=variance, y=epsilon))\n\n        x = (x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)\n\n        return x\n'"
tensorforce/core/layers/pooling.py,12,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom math import ceil\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core.layers import Layer\n\n\nclass Pooling(Layer):\n    """"""\n    Pooling layer (global pooling) (specification key: `pooling`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        reduction (\'concat\' | \'max\' | \'mean\' | \'product\' | \'sum\'): Pooling type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, reduction, input_spec=None, summary_labels=None):\n        # Reduction\n        if reduction not in (\'concat\', \'max\', \'mean\', \'product\', \'sum\'):\n            raise TensorforceError.value(name=\'pooling\', argument=\'reduction\', value=reduction)\n        self.reduction = reduction\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=None)\n\n    def get_output_spec(self, input_spec):\n        if self.reduction == \'concat\':\n            input_spec[\'shape\'] = (util.product(xs=input_spec[\'shape\']),)\n        elif self.reduction in (\'max\', \'mean\', \'product\', \'sum\'):\n            input_spec[\'shape\'] = (input_spec[\'shape\'][-1],)\n        input_spec.pop(\'min_value\', None)\n        input_spec.pop(\'max_value\', None)\n\n        return input_spec\n\n    def tf_apply(self, x):\n        if self.reduction == \'concat\':\n            return tf.reshape(tensor=x, shape=(-1, util.product(xs=util.shape(x)[1:])))\n\n        elif self.reduction == \'max\':\n            for _ in range(util.rank(x=x) - 2):\n                x = tf.reduce_max(input_tensor=x, axis=1)\n            return x\n\n        elif self.reduction == \'mean\':\n            for _ in range(util.rank(x=x) - 2):\n                x = tf.reduce_mean(input_tensor=x, axis=1)\n            return x\n\n        elif self.reduction == \'product\':\n            for _ in range(util.rank(x=x) - 2):\n                x = tf.reduce_prod(input_tensor=x, axis=1)\n            return x\n\n        elif self.reduction == \'sum\':\n            for _ in range(util.rank(x=x) - 2):\n                x = tf.reduce_sum(input_tensor=x, axis=1)\n            return x\n\n\nclass Flatten(Pooling):\n    """"""\n    Flatten layer (specification key: `flatten`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, input_spec=None, summary_labels=None):\n        super().__init__(\n            name=name, reduction=\'concat\', input_spec=input_spec, summary_labels=summary_labels\n        )\n\n    def tf_apply(self, x):\n        if self.input_spec[\'shape\'] == ():\n            return tf.expand_dims(input=x, axis=1)\n\n        else:\n            return super().tf_apply(x=x)\n\n\nclass Pool1d(Layer):\n    """"""\n    1-dimensional pooling layer (local pooling) (specification key: `pool1d`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        reduction (\'average\' | \'max\'): Pooling type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        window (int > 0): Window size\n            (<span style=""color:#00C000""><b>default</b></span>: 2).\n        stride (int > 0): Stride size\n            (<span style=""color:#00C000""><b>default</b></span>: 2).\n        padding (\'same\' | \'valid\'): Padding type, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/nn/convolution>`__\n            (<span style=""color:#00C000""><b>default</b></span>: \'same\').\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, reduction, window=2, stride=2, padding=\'same\', input_spec=None,\n        summary_labels=None\n    ):\n        self.reduction = reduction\n        if isinstance(window, int):\n            self.window = (1, 1, window, 1)\n        else:\n            raise TensorforceError(""Invalid window argument for pool1d layer: {}."".format(window))\n        if isinstance(stride, int):\n            self.stride = (1, 1, stride, 1)\n        else:\n            raise TensorforceError(""Invalid stride argument for pool1d layer: {}."".format(stride))\n        self.padding = padding\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=(0, 0))\n\n    def get_output_spec(self, input_spec):\n        if self.padding == \'same\':\n            input_spec[\'shape\'] = (\n                ceil(input_spec[\'shape\'][0] / self.stride[2]),\n                input_spec[\'shape\'][1]\n            )\n        elif self.padding == \'valid\':\n            input_spec[\'shape\'] = (\n                ceil((input_spec[\'shape\'][0] - (self.window[2] - 1)) / self.stride[2]),\n                input_spec[\'shape\'][1]\n            )\n\n        return input_spec\n\n    def tf_apply(self, x):\n        x = tf.expand_dims(input=x, axis=1)\n\n        if self.reduction == \'average\':\n            x = tf.nn.avg_pool(\n                input=x, ksize=self.window, strides=self.stride, padding=self.padding.upper()\n            )\n\n        elif self.reduction == \'max\':\n            x = tf.nn.max_pool(\n                input=x, ksize=self.window, strides=self.stride, padding=self.padding.upper()\n            )\n\n        x = tf.squeeze(input=x, axis=1)\n\n        return x\n\n\nclass Pool2d(Layer):\n    """"""\n    2-dimensional pooling layer (local pooling) (specification key: `pool2d`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        reduction (\'average\' | \'max\'): Pooling type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        window (int > 0 | (int > 0, int > 0)): Window size\n            (<span style=""color:#00C000""><b>default</b></span>: 2).\n        stride (int > 0 | (int > 0, int > 0)): Stride size\n            (<span style=""color:#00C000""><b>default</b></span>: 2).\n        padding (\'same\' | \'valid\'): Padding type, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/nn/convolution>`__\n            (<span style=""color:#00C000""><b>default</b></span>: \'same\').\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, reduction, window=2, stride=2, padding=\'same\', input_spec=None,\n        summary_labels=None\n    ):\n        self.reduction = reduction\n        if isinstance(window, int):\n            self.window = (1, window, window, 1)\n        elif len(window) == 2:\n            self.window = (1, window[0], window[1], 1)\n        else:\n            raise TensorforceError(""Invalid window argument for pool2d layer: {}."".format(window))\n        if isinstance(stride, int):\n            self.stride = (1, stride, stride, 1)\n        elif len(window) == 2:\n            self.stride = (1, stride[0], stride[1], 1)\n        else:\n            raise TensorforceError(""Invalid stride argument for pool2d layer: {}."".format(stride))\n        self.padding = padding\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=(0, 0, 0))\n\n    def get_output_spec(self, input_spec):\n        if self.padding == \'same\':\n            input_spec[\'shape\'] = (\n                ceil(input_spec[\'shape\'][0] / self.stride[1]),\n                ceil(input_spec[\'shape\'][1] / self.stride[2]),\n                input_spec[\'shape\'][2]\n            )\n        elif self.padding == \'valid\':\n            input_spec[\'shape\'] = (\n                ceil((input_spec[\'shape\'][0] - (self.window[1] - 1)) / self.stride[1]),\n                ceil((input_spec[\'shape\'][1] - (self.window[2] - 1)) / self.stride[2]),\n                input_spec[\'shape\'][2]\n            )\n\n        return input_spec\n\n    def tf_apply(self, x):\n        if self.reduction == \'average\':\n            x = tf.nn.avg_pool(\n                input=x, ksize=self.window, strides=self.stride, padding=self.padding.upper()\n            )\n\n        elif self.reduction == \'max\':\n            x = tf.nn.max_pool(\n                input=x, ksize=self.window, strides=self.stride, padding=self.padding.upper()\n            )\n\n        return x\n'"
tensorforce/core/layers/preprocessing.py,29,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.layers import Layer\n\n\nclass PreprocessingLayer(Layer):\n\n    def tf_reset(self):\n        raise NotImplementedError\n\n\nclass Clipping(Layer):\n    """"""\n    Clipping layer (specification key: `clipping`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        upper (parameter, float): Upper clipping value\n            (<span style=""color:#C00000""><b>required</b></span>).\n        lower (parameter, float): Lower clipping value\n            (<span style=""color:#00C000""><b>default</b></span>: negative upper value).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, upper, lower=None, input_spec=None, summary_labels=None):\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n        if lower is None:\n            self.lower = None\n            self.upper = self.add_module(\n                name=\'upper\', module=lower, modules=parameter_modules, dtype=\'float\', min_value=0.0\n            )\n        else:\n            self.lower = self.add_module(\n                name=\'lower\', module=lower, modules=parameter_modules, dtype=\'float\'\n            )\n            self.upper = self.add_module(\n                name=\'upper\', module=upper, modules=parameter_modules, dtype=\'float\'\n            )\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=None)\n\n    def tf_apply(self, x):\n        upper = self.upper.value()\n        if self.lower is None:\n            lower = -upper\n        else:\n            lower = self.lower.value()\n\n        assertion = tf.debugging.assert_greater_equal(\n            x=upper, y=lower, message=""Incompatible lower and upper clipping bound.""\n        )\n\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            return tf.clip_by_value(t=x, clip_value_min=lower, clip_value_max=upper)\n\n\nclass Deltafier(PreprocessingLayer):\n    """"""\n    Deltafier layer computing the difference between the current and the previous input; can only\n    be used as preprocessing layer (specification key: `deltafier`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        concatenate (False | int >= 0): Whether to concatenate instead of replace deltas with\n            input, and if so, concatenation axis\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, concatenate=False, input_spec=None, summary_labels=None):\n        self.concatenate = concatenate\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n    @classmethod\n    def output_spec(cls, concatenate=False, input_spec=None, **kwargs):\n        input_spec = super().output_spec(input_spec=input_spec)\n\n        if concatenate is not False:\n            input_spec[\'shape\'] = tuple(\n                2 * dims if axis == concatenate else dims\n                for axis, dims in enumerate(input_spec[\'shape\'])\n            )\n\n        return input_spec\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=None)\n\n    def get_output_spec(self, input_spec):\n        if self.concatenate is not False:\n            input_spec[\'shape\'] = tuple(\n                2 * dims if axis == self.concatenate else dims\n                for axis, dims in enumerate(input_spec[\'shape\'])\n            )\n\n        return input_spec\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        self.has_previous = self.add_variable(\n            name=\'has-previous\', dtype=\'bool\', shape=(), is_trainable=False, initializer=\'zeros\'\n        )\n\n        self.previous = self.add_variable(\n            name=\'previous\', dtype=\'float\', shape=((1,) + self.input_spec[\'shape\']),\n            is_trainable=False, initializer=\'zeros\'\n        )\n\n    def tf_reset(self):\n        assignment = self.has_previous.assign(\n            value=tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\')), read_value=False\n        )\n        return assignment\n\n    def tf_apply(self, x):\n        assertion = tf.debugging.assert_equal(\n            x=tf.shape(input=x)[0], y=1,\n            message=""Deltafier preprocessor currently not compatible with batched Agent.act.""\n        )\n\n        def first_delta():\n            assignment = self.has_previous.assign(\n                value=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')), read_value=False\n            )\n            with tf.control_dependencies(control_inputs=(assignment,)):\n                return tf.concat(values=(tf.zeros_like(input=x[:1]), x[1:] - x[:-1]), axis=0)\n\n        def later_delta():\n            return x - tf.concat(values=(self.previous, x[:-1]), axis=0)\n\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            delta = self.cond(pred=self.has_previous, true_fn=later_delta, false_fn=first_delta)\n\n            assignment = self.previous.assign(value=x[-1:], read_value=False)\n\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            if self.concatenate is False:\n                return util.identity_operation(x=delta)\n            else:\n                return tf.concat(values=(x, delta), axis=(self.concatenate + 1))\n\n\nclass Image(Layer):\n    """"""\n    Image preprocessing layer (specification key: `image`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        height (int): Height of resized image\n            (<span style=""color:#00C000""><b>default</b></span>: no resizing or relative to width).\n        width (int): Width of resized image\n            (<span style=""color:#00C000""><b>default</b></span>: no resizing or relative to height).\n        grayscale (bool | iter[float]): Turn into grayscale image, optionally using given weights\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, height=None, width=None, grayscale=False, input_spec=None, summary_labels=None\n    ):\n        self.height = height\n        self.width = width\n        self.grayscale = grayscale\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n    @classmethod\n    def output_spec(cls, height=None, width=None, grayscale=False, input_spec=None, **kwargs):\n        input_spec = super().output_spec(input_spec=input_spec)\n\n        if height is not None:\n            if width is None:\n                width = round(height * input_spec[\'shape\'][1] / input_spec[\'shape\'][0])\n            input_spec[\'shape\'] = (height, width, input_spec[\'shape\'][2])\n        elif width is not None:\n            height = round(width * input_spec[\'shape\'][0] / input_spec[\'shape\'][1])\n            input_spec[\'shape\'] = (height, width, input_spec[\'shape\'][2])\n        if not isinstance(grayscale, bool) or grayscale:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:2] + (1,)\n\n        return input_spec\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=(0, 0, 0))\n\n    def get_output_spec(self, input_spec):\n        if self.height is not None:\n            if self.width is None:\n                self.width = round(self.height * input_spec[\'shape\'][1] / input_spec[\'shape\'][0])\n            input_spec[\'shape\'] = (self.height, self.width, input_spec[\'shape\'][2])\n        elif self.width is not None:\n            self.height = round(self.width * input_spec[\'shape\'][0] / input_spec[\'shape\'][1])\n            input_spec[\'shape\'] = (self.height, self.width, input_spec[\'shape\'][2])\n\n        if not isinstance(self.grayscale, bool) or self.grayscale:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:2] + (1,)\n\n        return input_spec\n\n    def tf_apply(self, x):\n        if self.height is not None:\n            x = tf.image.resize(images=x, size=(self.height, self.width))\n\n        if not isinstance(self.grayscale, bool):\n            weights = tf.constant(\n                value=self.grayscale, dtype=util.tf_dtype(dtype=\'float\'),\n                shape=(1, 1, 1, len(self.grayscale))\n            )\n            x = tf.reduce_sum(input_tensor=(x * weights), axis=3, keepdims=True)\n        elif self.grayscale:\n            x = tf.image.rgb_to_grayscale(images=x)\n\n        return x\n\n\nclass Sequence(PreprocessingLayer):\n    """"""\n    Sequence layer stacking the current and previous inputs; can only be used as preprocessing\n    layer (specification key: `sequence`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        length (int > 0): Number of inputs to concatenate\n            (<span style=""color:#C00000""><b>required</b></span>).\n        axis (int >= 0): Concatenation axis, excluding batch axis\n            (<span style=""color:#00C000""><b>default</b></span>: last axis).\n        concatenate (bool): Whether to concatenate inputs at given axis, otherwise introduce new\n            sequence axis\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, length, axis=-1, concatenate=True, input_spec=None, summary_labels=None\n    ):\n        assert length > 1\n        self.length = length\n        self.axis = axis\n        self.concatenate = concatenate\n\n        super().__init__(name=name, input_spec=input_spec, summary_labels=summary_labels)\n\n    @classmethod\n    def output_spec(cls, length, axis=-1, concatenate=True, input_spec=None, **kwargs):\n        input_spec = super().output_spec(input_spec=input_spec)\n\n        if concatenate:\n            if axis == -1:\n                axis = len(input_spec[\'shape\']) - 1\n            input_spec[\'shape\'] = tuple(\n                length * dims if axis == axis else dims\n                for axis, dims in enumerate(input_spec[\'shape\'])\n            )\n\n        else:\n            if axis == -1:\n                axis = len(input_spec[\'shape\'])\n            shape = input_spec[\'shape\']\n            input_spec[\'shape\'] = shape[:axis] + (length,) + shape[axis:]\n\n        return input_spec\n\n    def default_input_spec(self):\n        return dict(type=None, shape=None)\n\n    def get_output_spec(self, input_spec):\n        if self.concatenate:\n            if self.axis == -1:\n                self.axis = len(input_spec[\'shape\']) - 1\n            input_spec[\'shape\'] = tuple(\n                self.length * dims if axis == self.axis else dims\n                for axis, dims in enumerate(input_spec[\'shape\'])\n            )\n\n        else:\n            if self.axis == -1:\n                self.axis = len(input_spec[\'shape\'])\n            shape = input_spec[\'shape\']\n            input_spec[\'shape\'] = shape[:self.axis] + (self.length,) + shape[self.axis:]\n\n        return input_spec\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        self.has_previous = self.add_variable(\n            name=\'has-previous\', dtype=\'bool\', shape=(), is_trainable=False, initializer=\'zeros\'\n        )\n\n        shape = self.input_spec[\'shape\']\n        if self.concatenate:\n            shape = (1,) + shape[:self.axis] + (shape[self.axis] * (self.length - 1),) + \\\n                shape[self.axis + 1:]\n        else:\n            shape = (1,) + shape[:self.axis] + (self.length - 1,) + shape[self.axis:]\n        self.previous = self.add_variable(\n            name=\'previous\', dtype=\'float\', shape=shape, is_trainable=False, initializer=\'zeros\'\n        )\n\n    def tf_reset(self):\n        assignment = self.has_previous.assign(\n            value=tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\')), read_value=False\n        )\n        return assignment\n\n    def tf_apply(self, x):\n        assertion = tf.debugging.assert_equal(\n            x=tf.shape(input=x)[0], y=1,\n            message=""Sequence preprocessor currently not compatible with batched Agent.act.""\n        )\n\n        def first_timestep():\n            assignment = self.has_previous.assign(\n                value=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')), read_value=False\n            )\n            with tf.control_dependencies(control_inputs=(assignment,)):\n                if self.concatenate:\n                    current = x\n                else:\n                    current = tf.expand_dims(input=x, axis=(self.axis + 1))\n                multiples = tuple(\n                    self.length if dims == self.axis + 1 else 1\n                    for dims in range(util.rank(x=current))\n                )\n                return tf.tile(input=current, multiples=multiples)\n\n        def other_timesteps():\n            if self.concatenate:\n                current = x\n            else:\n                current = tf.expand_dims(input=x, axis=(self.axis + 1))\n            return tf.concat(values=(self.previous, current), axis=(self.axis + 1))\n\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            xs = self.cond(\n                pred=self.has_previous, true_fn=other_timesteps, false_fn=first_timestep\n            )\n\n            if self.concatenate:\n                begin = tuple(\n                    self.input_spec[\'shape\'][dims - 1] if dims == self.axis + 1 else 0\n                    for dims in range(util.rank(x=xs))\n                )\n            else:\n                begin = tuple(1 if dims == self.axis + 1 else 0 for dims in range(util.rank(x=xs)))\n\n            assignment = self.previous.assign(\n                value=tf.slice(input_=xs, begin=begin, size=self.previous.shape), read_value=False\n            )\n\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            return util.identity_operation(x=xs)\n'"
tensorforce/core/layers/rnn.py,4,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError\nfrom tensorforce.core.layers import TransformationBase\n\n\nclass Rnn(TransformationBase):\n    """"""\n    Recurrent neural network layer (specification key: `rnn`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        cell (\'gru\' | \'lstm\'): The recurrent cell type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        return_final_state (bool): Whether to return the final state instead of the per-step\n            outputs (<span style=""color:#00C000""><b>default</b></span>: true).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: tanh).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments for Keras RNN layer, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/layers>`__.\n    """"""\n\n    def __init__(\n        self, name, cell, size, return_final_state=True, bias=True, activation=\'tanh\', dropout=0.0,\n        is_trainable=True, input_spec=None, summary_labels=None, l2_regularization=None, **kwargs\n    ):\n        self.cell = cell\n        self.return_final_state = return_final_state\n\n        if self.return_final_state and self.cell == \'lstm\':\n            assert size % 2 == 0\n            self.size = size // 2\n        else:\n            self.size = size\n\n        if self.cell == \'gru\':\n            self.rnn = tf.keras.layers.GRU(\n                units=self.size, return_sequences=True, return_state=True, name=\'rnn\',\n                input_shape=input_spec[\'shape\'], **kwargs  # , dtype=util.tf_dtype(dtype=\'float\')\n            )\n        elif self.cell == \'lstm\':\n            self.rnn = tf.keras.layers.LSTM(\n                units=self.size, return_sequences=True, return_state=True, name=\'rnn\',\n                input_shape=input_spec[\'shape\'], **kwargs  # , dtype=util.tf_dtype(dtype=\'float\')\n            )\n        else:\n            raise TensorforceError.unexpected()\n\n        super().__init__(\n            name=name, size=size, bias=bias, activation=activation, dropout=dropout,\n            is_trainable=is_trainable, input_spec=input_spec, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        if self.squeeze and self.return_final_state:\n            raise TensorforceError(\n                ""Invalid combination for Lstm layer: size=0 and return_final_state=True.""\n            )\n\n    def default_input_spec(self):\n        return dict(type=\'float\', shape=(-1, 0))\n\n    def get_output_spec(self, input_spec):\n        if self.return_final_state:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:-2] + (self.size,)\n        elif self.squeeze:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:-1]\n        else:\n            input_spec[\'shape\'] = input_spec[\'shape\'][:-1] + (self.size,)\n        input_spec.pop(\'min_value\', None)\n        input_spec.pop(\'max_value\', None)\n\n        return input_spec\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        self.rnn.build(input_shape=((None,) + self.input_spec[\'shape\']))\n\n        for variable in self.rnn.trainable_weights:\n            name = variable.name[variable.name.rindex(self.name + \'/\') + len(self.name) + 1: -2]\n            self.variables[name] = variable\n            if self.is_trainable:\n                self.trainable_variables[name] = variable\n        for variable in self.rnn.non_trainable_weights:\n            name = variable.name[variable.name.rindex(self.name + \'/\') + len(self.name) + 1: -2]\n            self.variables[name] = variable\n\n    def tf_regularize(self):\n        regularization_loss = super().tf_regularize()\n\n        if len(self.rnn.losses) > 0:\n            regularization_loss += tf.math.add_n(inputs=self.rnn.losses)\n\n        return regularization_loss\n\n    def tf_apply(self, x, sequence_length=None):\n        x = self.rnn(inputs=x, initial_state=None)\n\n        if self.return_final_state:\n            if self.cell == \'gru\':\n                x = x[1]\n            elif self.cell == \'lstm\':\n                x = tf.concat(values=(x[1], x[2]), axis=1)\n        else:\n            x = x[0]\n\n        return super().tf_apply(x=x)\n\n\nclass Gru(Rnn):\n    """"""\n    Gated recurrent unit layer (specification key: `gru`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        cell (\'gru\' | \'lstm\'): The recurrent cell type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        return_final_state (bool): Whether to return the final state instead of the per-step\n            outputs (<span style=""color:#00C000""><b>default</b></span>: true).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments for Keras GRU layer, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU>`__.\n    """"""\n\n    def __init__(\n        self, name, size, return_final_state=True, bias=False, activation=None, dropout=0.0,\n        is_trainable=True, input_spec=None, summary_labels=None, l2_regularization=None, **kwargs\n    ):\n        super().__init__(\n            name=name, cell=\'gru\', size=size, return_final_state=return_final_state, bias=bias,\n            activation=activation, dropout=dropout, input_spec=input_spec,\n            summary_labels=summary_labels, l2_regularization=l2_regularization, **kwargs\n        )\n\n\nclass Lstm(Rnn):\n    """"""\n    Long short-term memory layer (specification key: `lstm`).\n\n    Args:\n        name (string): Layer name\n            (<span style=""color:#00C000""><b>default</b></span>: internally chosen).\n        cell (\'gru\' | \'lstm\'): The recurrent cell type\n            (<span style=""color:#C00000""><b>required</b></span>).\n        size (int >= 0): Layer output size, 0 implies additionally removing the axis\n            (<span style=""color:#C00000""><b>required</b></span>).\n        return_final_state (bool): Whether to return the final state instead of the per-step\n            outputs (<span style=""color:#00C000""><b>default</b></span>: true).\n        bias (bool): Whether to add a trainable bias variable\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        activation (\'crelu\' | \'elu\' | \'leaky-relu\' | \'none\' | \'relu\' | \'selu\' | \'sigmoid\' |\n            \'softmax\' | \'softplus\' | \'softsign\' | \'swish\' | \'tanh\'): Activation nonlinearity\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        dropout (parameter, 0.0 <= float < 1.0): Dropout rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        is_trainable (bool): Whether layer variables are trainable\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#00C000""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments for Keras LSTM layer, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM>`__.\n    """"""\n\n    def __init__(\n        self, name, size, return_final_state=True, bias=False, activation=None, dropout=0.0,\n        is_trainable=True, input_spec=None, summary_labels=None, l2_regularization=None, **kwargs\n    ):\n        super().__init__(\n            name=name, cell=\'lstm\', size=size, return_final_state=return_final_state, bias=bias,\n            activation=activation, dropout=dropout, input_spec=input_spec,\n            summary_labels=summary_labels, l2_regularization=l2_regularization, **kwargs\n        )\n'"
tensorforce/core/memories/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.memories.memory import Memory\nfrom tensorforce.core.memories.queue import Queue\n\n# from tensorforce.core.memories.prioritized_replay import PrioritizedReplay\nfrom tensorforce.core.memories.recent import Recent\nfrom tensorforce.core.memories.replay import Replay\n\n\nmemory_modules = dict(default=Replay, recent=Recent, replay=Replay)\n\n\n__all__ = [\'Memory\', \'memory_modules\', \'Queue\', \'Recent\', \'Replay\']\n'"
tensorforce/core/memories/memory.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core import Module\n\n\nclass Memory(Module):\n    """"""\n    Base class for memories.\n\n    Args:\n        name (string): Memory name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        values_spec (specification): Values specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        min_capacity (int >= 0): Minimum memory capacity\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n    def __init__(\n        self, name, values_spec, min_capacity=0, device=None, summary_labels=None,\n        l2_regularization=None\n    ):\n        super().__init__(\n            name=name, device=device, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        self.values_spec = values_spec\n\n    def tf_enqueue(self, states, internals, actions, terminal, reward):\n        raise NotImplementedError\n\n    def tf_retrieve(self, indices, values=None):\n        raise NotImplementedError\n\n    def tf_successors(self, indices, horizon, sequence_values=(), final_values=()):\n        raise NotImplementedError\n\n    def tf_predecessors(self, indices, horizon, sequence_values=(), initial_values=()):\n        raise NotImplementedError\n\n    def tf_retrieve_timesteps(self, n, past_padding, future_padding):\n        raise NotImplementedError\n\n    def tf_retrieve_episodes(self, n):\n        raise NotImplementedError\n'"
tensorforce/core/memories/queue.py,154,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core.memories import Memory\n\n\nclass Queue(Memory):\n    """"""\n    Base class for memories organized as a queue / circular buffer.\n\n    Args:\n        name (string): Memory name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        values_spec (specification): Values specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        capacity (int > 0): Memory capacity\n            (<span style=""color:#00C000""><b>default</b></span>: minimum capacity).\n        min_capacity (int >= 0): Minimum memory capacity\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, capacity=None, values_spec=None, min_capacity=0, device=None,\n        summary_labels=None\n    ):\n        super().__init__(\n            name=name, values_spec=values_spec, min_capacity=min_capacity, device=device,\n            summary_labels=summary_labels\n        )\n\n        if capacity is None:\n            if min_capacity == 0:\n                raise TensorforceError.required(\n                    name=\'memory\', argument=\'capacity\', condition=\'unknown minimum capacity\'\n                )\n            else:\n                self.capacity = min_capacity\n        elif capacity < min_capacity:\n            raise TensorforceError.value(\n                name=\'memory\', argument=\'capacity\', value=capacity,\n                hint=(\'< minimum capacity \' + str(min_capacity))\n            )\n        else:\n            self.capacity = capacity\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        # Value buffers\n        self.buffers = OrderedDict()\n        for name, spec in self.values_spec.items():\n            if util.is_nested(name=name):\n                self.buffers[name] = OrderedDict()\n                for inner_name, spec in spec.items():\n                    shape = (self.capacity,) + spec[\'shape\']\n                    self.buffers[name][inner_name] = self.add_variable(\n                        name=(inner_name + \'-buffer\'), dtype=spec[\'type\'], shape=shape,\n                        is_trainable=False\n                    )\n            else:\n                shape = (self.capacity,) + spec[\'shape\']\n                if name == \'terminal\':\n                    # Terminal initialization has to agree with terminal_indices\n                    initializer = np.zeros(\n                        shape=(self.capacity,), dtype=util.np_dtype(dtype=\'long\')\n                    )\n                    initializer[-1] = 1\n                    self.buffers[name] = self.add_variable(\n                        name=(name + \'-buffer\'), dtype=spec[\'type\'], shape=shape,\n                        is_trainable=False, initializer=initializer\n                    )\n                else:\n                    self.buffers[name] = self.add_variable(\n                        name=(name + \'-buffer\'), dtype=spec[\'type\'], shape=shape, is_trainable=False\n                    )\n\n        # Buffer index (modulo capacity, next index to write to)\n        self.buffer_index = self.add_variable(\n            name=\'buffer-index\', dtype=\'long\', shape=(), is_trainable=False, initializer=\'zeros\'\n        )\n\n        # Terminal indices\n        # (oldest episode terminals first, initially the only terminal is last index)\n        initializer = np.zeros(shape=(self.capacity + 1,), dtype=util.np_dtype(dtype=\'long\'))\n        initializer[0] = self.capacity - 1\n        self.terminal_indices = self.add_variable(\n            name=\'terminal-indices\', dtype=\'long\', shape=(self.capacity + 1,), is_trainable=False,\n            initializer=initializer\n        )\n\n        # Episode count\n        self.episode_count = self.add_variable(\n            name=\'episode-count\', dtype=\'long\', shape=(), is_trainable=False, initializer=\'zeros\'\n        )\n\n    def tf_enqueue(self, states, internals, auxiliaries, actions, terminal, reward):\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        three = tf.constant(value=3, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n        if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n            num_timesteps = tf.shape(input=terminal, out_type=util.tf_dtype(dtype=\'long\'))[0]\n        else:\n            num_timesteps = tf.dtypes.cast(\n                x=tf.shape(input=terminal)[0], dtype=util.tf_dtype(dtype=\'long\')\n            )\n\n        # # Max capacity\n        # latest_terminal_index = self.terminal_indices[self.episode_count]\n        # max_capacity = self.buffer_index - latest_terminal_index - one\n        # max_capacity = capacity - (tf.math.mod(x=max_capacity, y=capacity) + one)\n\n        # Remove last observation terminal marker\n        last_index = tf.math.mod(x=(self.buffer_index - one), y=capacity)\n        last_terminal = tf.gather(params=self.buffers[\'terminal\'], indices=(last_index,))[0]\n        corrected_terminal = tf.where(\n            condition=tf.math.equal(x=last_terminal, y=three), x=zero, y=last_terminal\n        )\n        assignment = tf.compat.v1.assign(\n            ref=self.buffers[\'terminal\'][last_index], value=corrected_terminal\n        )\n\n        # Assertions\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            assertions = [\n                # check: number of timesteps fit into effectively available buffer\n                tf.debugging.assert_less_equal(\n                    x=num_timesteps, y=capacity, message=""Memory does not have enough capacity.""\n                ),\n                # at most one terminal\n                tf.debugging.assert_less_equal(\n                    x=tf.math.count_nonzero(input=terminal, dtype=util.tf_dtype(dtype=\'long\')),\n                    y=one, message=""Timesteps contain more than one terminal.""\n                ),\n                # if terminal, last timestep in batch\n                tf.debugging.assert_equal(\n                    x=tf.math.reduce_any(input_tensor=tf.math.greater(x=terminal, y=zero)),\n                    y=tf.math.greater(x=terminal[-1], y=zero),\n                    message=""Terminal is not the last timestep.""\n                ),\n                # general check: all terminal indices true\n                tf.debugging.assert_equal(\n                    x=tf.reduce_all(\n                        input_tensor=tf.gather(\n                            params=tf.math.greater(x=self.buffers[\'terminal\'], y=zero),\n                            indices=self.terminal_indices[:self.episode_count + one]\n                        )\n                    ),\n                    y=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')),\n                    message=""Memory consistency check.""\n                ),\n                # general check: only terminal indices true\n                tf.debugging.assert_equal(\n                    x=tf.math.count_nonzero(\n                        input=self.buffers[\'terminal\'], dtype=util.tf_dtype(dtype=\'long\')\n                    ),\n                    y=(self.episode_count + one), message=""Memory consistency check.""\n                )\n            ]\n\n        # Buffer indices to overwrite\n        with tf.control_dependencies(control_inputs=assertions):\n            overwritten_indices = tf.range(\n                start=self.buffer_index, limit=(self.buffer_index + num_timesteps)\n            )\n            overwritten_indices = tf.math.mod(x=overwritten_indices, y=capacity)\n\n            # Count number of overwritten episodes\n            num_episodes = tf.math.count_nonzero(\n                input=tf.gather(params=self.buffers[\'terminal\'], indices=overwritten_indices),\n                axis=0, dtype=util.tf_dtype(dtype=\'long\')\n            )\n\n            # Shift remaining terminal indices accordingly\n            limit_index = self.episode_count + one\n            assertion = tf.debugging.assert_greater_equal(\n                x=limit_index, y=num_episodes, message=""Memory episode overwriting check.""\n            )\n\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            assignment = tf.compat.v1.assign(\n                ref=self.terminal_indices[:limit_index - num_episodes],\n                value=self.terminal_indices[num_episodes: limit_index]\n            )\n\n        # Decrement episode count accordingly\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            assignment = self.episode_count.assign_sub(delta=num_episodes, read_value=False)\n\n        # Write new observations\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            indices = tf.range(start=self.buffer_index, limit=(self.buffer_index + num_timesteps))\n            indices = tf.math.mod(x=indices, y=capacity)\n            indices = tf.expand_dims(input=indices, axis=1)\n            values = dict(\n                states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n                terminal=terminal, reward=reward\n            )\n            assignments = list()\n            for name, buffer in self.buffers.items():\n                if util.is_nested(name=name):\n                    for inner_name, buffer in buffer.items():\n                        assignment = buffer.scatter_nd_update(\n                            indices=indices, updates=values[name][inner_name]\n                        )\n                        assignments.append(assignment)\n                else:\n                    if name == \'terminal\':\n                        # Add last observation terminal marker\n                        corrected_terminal = tf.where(\n                            condition=tf.math.equal(x=terminal[-1], y=zero), x=three, y=terminal[-1]\n                        )\n                        assignment = buffer.scatter_nd_update(\n                            indices=indices,\n                            updates=tf.concat(values=(terminal[:-1], (corrected_terminal,)), axis=0)\n                        )\n                    else:\n                        assignment = buffer.scatter_nd_update(indices=indices, updates=values[name])\n                    assignments.append(assignment)\n\n        # Increment buffer index\n        with tf.control_dependencies(control_inputs=assignments):\n            assignment = self.buffer_index.assign_add(delta=num_timesteps, read_value=False)\n\n        # Count number of new episodes\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            num_new_episodes = tf.math.count_nonzero(\n                input=terminal, dtype=util.tf_dtype(dtype=\'long\')\n            )\n\n            # Write new terminal indices\n            limit_index = self.episode_count + one\n            assignment = tf.compat.v1.assign(\n                ref=self.terminal_indices[limit_index: limit_index + num_new_episodes],\n                value=tf.boolean_mask(\n                    tensor=overwritten_indices, mask=tf.math.greater(x=terminal, y=zero)\n                )\n            )\n\n        # Increment episode count accordingly\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            assignment = self.episode_count.assign_add(delta=num_new_episodes, read_value=False)\n\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            return util.no_operation()\n\n    def tf_retrieve(self, indices, values):\n        # if values is None:\n        #     is_single_value = False\n        #     values = (\'states\', \'internals\', \'actions\', \'terminal\', \'reward\')\n        if isinstance(values, str):\n            is_single_value = True\n            values = [values]\n        else:\n            is_single_value = False\n            values = list(values)\n\n        # Retrieve values\n        for n, name in enumerate(values):\n            if util.is_nested(name=name):\n                value = OrderedDict()\n                for inner_name in self.values_spec[name]:\n                    value[inner_name] = tf.gather(\n                        params=self.buffers[name][inner_name], indices=indices\n                    )\n            else:\n                value = tf.gather(params=self.buffers[name], indices=indices)\n            values[n] = value\n\n        # # Stop gradients\n        # values = util.fmap(function=tf.stop_gradient, xs=values)\n\n        # Return values or single value\n        if is_single_value:\n            return values[0]\n        else:\n            return values\n\n    def tf_predecessors(self, indices, horizon, sequence_values=(), initial_values=()):\n        if sequence_values == () and initial_values == ():\n            raise TensorforceError.unexpected()\n\n        if isinstance(sequence_values, str):\n            is_single_sequence_value = True\n            sequence_values = [sequence_values]\n        else:\n            is_single_sequence_value = False\n            sequence_values = list(sequence_values)\n        if isinstance(initial_values, str):\n            is_single_initial_value = True\n            initial_values = [initial_values]\n        else:\n            is_single_initial_value = False\n            initial_values = list(initial_values)\n\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n\n        def body(lengths, predecessor_indices, mask):\n            previous_index = tf.math.mod(x=(predecessor_indices[:, :1] - one), y=capacity)\n            predecessor_indices = tf.concat(values=(previous_index, predecessor_indices), axis=1)\n            previous_terminal = self.retrieve(indices=previous_index, values=\'terminal\')\n            is_not_terminal = tf.math.logical_and(\n                x=tf.math.logical_not(x=tf.math.greater(x=previous_terminal, y=zero)),\n                y=mask[:, :1]\n            )\n            mask = tf.concat(values=(is_not_terminal, mask), axis=1)\n            is_not_terminal = tf.squeeze(input=is_not_terminal, axis=1)\n            zeros = tf.zeros_like(input=is_not_terminal, dtype=util.tf_dtype(dtype=\'long\'))\n            ones = tf.ones_like(input=is_not_terminal, dtype=util.tf_dtype(dtype=\'long\'))\n            lengths += tf.where(condition=is_not_terminal, x=ones, y=zeros)\n            return lengths, predecessor_indices, mask\n\n        lengths = tf.ones_like(input=indices, dtype=util.tf_dtype(dtype=\'long\'))\n        predecessor_indices = tf.expand_dims(input=indices, axis=1)\n        mask = tf.ones_like(input=predecessor_indices, dtype=util.tf_dtype(dtype=\'bool\'))\n        shape = tf.TensorShape(dims=((None, None)))\n\n        lengths, predecessor_indices, mask = self.while_loop(\n            cond=util.tf_always_true, body=body,\n            loop_vars=(lengths, predecessor_indices, mask),\n            shape_invariants=(lengths.get_shape(), shape, shape), back_prop=False,\n            maximum_iterations=horizon\n        )\n\n        predecessor_indices = tf.reshape(tensor=predecessor_indices, shape=(-1,))\n        mask = tf.reshape(tensor=mask, shape=(-1,))\n        predecessor_indices = tf.boolean_mask(tensor=predecessor_indices, mask=mask, axis=0)\n\n        assertion = tf.debugging.assert_greater_equal(\n            x=tf.math.mod(x=(predecessor_indices - self.buffer_index), y=capacity), y=zero,\n            message=""Predecessor check.""\n        )\n\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            starts = tf.math.cumsum(x=lengths, exclusive=True)\n            initial_indices = tf.gather(params=predecessor_indices, indices=starts)\n\n            for n, name in enumerate(sequence_values):\n                if util.is_nested(name=name):\n                    sequence_value = OrderedDict()\n                    for inner_name, spec in self.values_spec[name].items():\n                        sequence_value[inner_name] = tf.gather(\n                            params=self.buffers[name][inner_name], indices=predecessor_indices\n                        )\n                else:\n                    sequence_value = tf.gather(\n                        params=self.buffers[name], indices=predecessor_indices\n                    )\n                sequence_values[n] = sequence_value\n\n            for n, name in enumerate(initial_values):\n                if util.is_nested(name=name):\n                    initial_value = OrderedDict()\n                    for inner_name, spec in self.values_spec[name].items():\n                        initial_value[inner_name] = tf.gather(\n                            params=self.buffers[name][inner_name], indices=initial_indices\n                        )\n                else:\n                    initial_value = tf.gather(\n                        params=self.buffers[name], indices=initial_indices\n                    )\n                initial_values[n] = initial_value\n\n        # def body(lengths, sequence_values, initial_values):\n        #     # Retrieve previous indices\n        #     previous_indices = tf.math.mod(x=(indices - lengths), y=capacity)\n        #     previous_values = self.retrieve(\n        #         indices=previous_indices, values=(tuple(sequence_values) + tuple(initial_values))\n        #     )\n\n        #     # Overwrite initial values\n        #     for name in initial_values:\n        #         initial_values[name] = previous_values[name]\n\n        #     # Concatenate sequence values\n        #     for name, value, previous_value in util.zip_items(sequence_values, previous_values):\n        #         if util.is_nested(name=name):\n        #             for inner_name, value, previous_value in util.zip_items(value, previous_value):\n        #                 previous_value = tf.expand_dims(input=previous_value, axis=1)\n        #                 sequence_values[name][inner_name] = tf.concat(\n        #                     values=(previous_value, value), axis=1\n        #                 )\n        #         else:\n        #             previous_value = tf.expand_dims(input=previous_value, axis=1)\n        #             sequence_values[name] = tf.concat(values=(previous_value, value), axis=1)\n\n        #     # Increment lengths unless start of episode\n        #     with tf.control_dependencies(control_inputs=util.flatten(xs=previous_values)):\n        #         previous_indices = tf.math.mod(x=(previous_indices - one), y=capacity)\n        #         terminal = self.retrieve(indices=previous_indices, values=\'terminal\')\n        #         x = tf.zeros_like(input=terminal, dtype=util.tf_dtype(dtype=\'long\'))\n        #         y = tf.ones_like(input=terminal, dtype=util.tf_dtype(dtype=\'long\'))\n        #         lengths += tf.where(condition=terminal, x=x, y=y)\n\n        #     return lengths, sequence_values, initial_values\n\n        # # Sequence lengths\n        # lengths = tf.zeros_like(input=indices, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # # Shape invariants\n        # start_sequence_values = OrderedDict()\n        # sequence_shapes = OrderedDict()\n        # for name in sequence_values:\n        #     if util.is_nested(name=name):\n        #         start_sequence_values[name] = OrderedDict()\n        #         sequence_shapes[name] = OrderedDict()\n        #         for inner_name, spec in self.values_spec[name].items():\n        #             start_sequence_values[name][inner_name] = tf.zeros(shape=((0, tf.shape(indices)[0]) + spec[\'shape\']))\n        #             shape = tf.TensorShape(dims=((None, None) + spec[\'shape\']))\n        #             sequence_shapes[name][inner_name] = shape\n        #     else:\n        #         start_sequence_values[name] = tf.zeros(shape=((0, tf.shape(indices)[0]) + self.values_spec[name][\'shape\']))\n        #         shape = tf.TensorShape(dims=((None, None) + self.values_spec[name][\'shape\']))\n        #         sequence_shapes[name] = shape\n        # start_initial_values = OrderedDict()\n        # initial_shapes = OrderedDict()\n        # for name in initial_values:\n        #     if util.is_nested(name=name):\n        #         start_initial_values[name] = OrderedDict()\n        #         initial_shapes[name] = OrderedDict()\n        #         for inner_name, spec in self.values_spec[name].items():\n        #             start_initial_values[name][inner_name] = tf.zeros(shape=((tf.shape(indices)[0],) + spec[\'shape\']))\n        #             shape = tf.TensorShape(dims=((None,) + spec[\'shape\']))\n        #             initial_shapes[name][inner_name] = shape\n        #     else:\n        #         start_initial_values[name] = tf.zeros(shape=((tf.shape(indices)[0],) + self.values_spec[name][\'shape\']))\n        #         shape = tf.TensorShape(dims=((None,) + self.values_spec[name][\'shape\']))\n        #         initial_shapes[name] = shape\n\n        # # Retrieve predecessors\n        # lengths, sequence_values, initial_values = self.while_loop(\n        #     cond=util.tf_always_true, body=body,\n        #     loop_vars=(lengths, start_sequence_values, start_initial_values),\n        #     shape_invariants=(lengths.get_shape(), sequence_shapes, initial_shapes),\n        #     back_prop=False, maximum_iterations=horizon\n        # )\n\n        # # Stop gradients\n        # sequence_values = util.fmap(function=tf.stop_gradient, xs=sequence_values)\n        # initial_values = util.fmap(function=tf.stop_gradient, xs=initial_values)\n\n        if len(sequence_values) == 0:\n            if is_single_initial_value:\n                initial_values = initial_values[0]\n            return lengths, initial_values\n\n        elif len(initial_values) == 0:\n            if is_single_sequence_value:\n                sequence_values = sequence_values[0]\n            return starts, lengths, sequence_values\n\n        else:\n            if is_single_sequence_value:\n                sequence_values = sequence_values[0]\n            if is_single_initial_value:\n                initial_values = initial_values[0]\n            return starts, lengths, sequence_values, initial_values\n\n    def tf_successors(self, indices, horizon, sequence_values=(), final_values=()):\n        if sequence_values == () and final_values == ():\n            raise TensorforceError.unexpected()\n\n        if isinstance(sequence_values, str):\n            is_single_sequence_value = True\n            sequence_values = [sequence_values]\n        else:\n            is_single_sequence_value = False\n            sequence_values = list(sequence_values)\n        if isinstance(final_values, str):\n            is_single_final_value = True\n            final_values = [final_values]\n        else:\n            is_single_final_value = False\n            final_values = list(final_values)\n\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n\n        def body(lengths, successor_indices, mask):\n            current_index = successor_indices[:, -1:]\n            current_terminal = self.retrieve(indices=current_index, values=\'terminal\')\n            is_not_terminal = tf.math.logical_and(\n                x=tf.math.logical_not(x=tf.math.greater(x=current_terminal, y=zero)),\n                y=mask[:, -1:]\n            )\n            next_index = tf.math.mod(x=(current_index + one), y=capacity)\n            successor_indices = tf.concat(values=(successor_indices, next_index), axis=1)\n            mask = tf.concat(values=(mask, is_not_terminal), axis=1)\n            is_not_terminal = tf.squeeze(input=is_not_terminal, axis=1)\n            zeros = tf.zeros_like(input=is_not_terminal, dtype=util.tf_dtype(dtype=\'long\'))\n            ones = tf.ones_like(input=is_not_terminal, dtype=util.tf_dtype(dtype=\'long\'))\n            lengths += tf.where(condition=is_not_terminal, x=ones, y=zeros)\n            return lengths, successor_indices, mask\n\n        lengths = tf.ones_like(input=indices, dtype=util.tf_dtype(dtype=\'long\'))\n        successor_indices = tf.expand_dims(input=indices, axis=1)\n        mask = tf.ones_like(input=successor_indices, dtype=util.tf_dtype(dtype=\'bool\'))\n        shape = tf.TensorShape(dims=((None, None)))\n\n        lengths, successor_indices, mask = self.while_loop(\n            cond=util.tf_always_true, body=body, loop_vars=(lengths, successor_indices, mask),\n            shape_invariants=(lengths.get_shape(), shape, shape), back_prop=False,\n            maximum_iterations=horizon\n        )\n\n        successor_indices = tf.reshape(tensor=successor_indices, shape=(-1,))\n        mask = tf.reshape(tensor=mask, shape=(-1,))\n        successor_indices = tf.boolean_mask(tensor=successor_indices, mask=mask, axis=0)\n\n        assertion = tf.debugging.assert_greater_equal(\n            x=tf.math.mod(x=(self.buffer_index - one - successor_indices), y=capacity), y=zero,\n            message=""Successor check.""\n        )\n\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            starts = tf.math.cumsum(x=lengths, exclusive=True)\n            ends = tf.math.cumsum(x=lengths) - one\n            final_indices = tf.gather(params=successor_indices, indices=ends)\n\n            for n, name in enumerate(sequence_values):\n                if util.is_nested(name=name):\n                    sequence_value = OrderedDict()\n                    for inner_name, spec in self.values_spec[name].items():\n                        sequence_value[inner_name] = tf.gather(\n                            params=self.buffers[name][inner_name], indices=successor_indices\n                        )\n                else:\n                    sequence_value = tf.gather(\n                        params=self.buffers[name], indices=successor_indices\n                    )\n                sequence_values[n] = sequence_value\n\n            for n, name in enumerate(final_values):\n                if util.is_nested(name=name):\n                    final_value = OrderedDict()\n                    for inner_name, spec in self.values_spec[name].items():\n                        final_value[inner_name] = tf.gather(\n                            params=self.buffers[name][inner_name], indices=final_indices\n                        )\n                else:\n                    final_value = tf.gather(\n                        params=self.buffers[name], indices=final_indices\n                    )\n                final_values[n] = final_value\n\n        # def body(lengths, sequence_values, final_values):\n        #     # Retrieve next indices\n        #     next_indices = tf.math.mod(x=(indices - lengths), y=capacity)\n        #     next_values = self.retrieve(\n        #         indices=next_indices, values=(tuple(sequence_values) + tuple(final_values))\n        #     )\n\n        #     # Overwrite final values\n        #     for name in final_values:\n        #         final_values[name] = next_values[name]\n\n        #     # Concatenate sequence values\n        #     for name, value, next_value in util.zip_items(sequence_values, next_values):\n        #         if util.is_nested(name=name):\n        #             for inner_name, value, next_value in util.zip_items(value, next_value):\n        #                 next_value = tf.expand_dims(input=next_value, axis=1)\n        #                 sequence_values[name][inner_name] = tf.concat(\n        #                     values=(value, next_value), axis=1\n        #                 )\n        #         else:\n        #             next_value = tf.expand_dims(input=next_value, axis=1)\n        #             sequence_values[name] = tf.concat(values=(value, next_value), axis=1)\n\n        #     # Increment lengths unless start of episode\n        #     with tf.control_dependencies(control_inputs=util.flatten(xs=next_values)):\n        #         next_indices = tf.math.mod(x=(next_indices - one), y=capacity)\n        #         terminal = self.retrieve(indices=next_indices, values=\'terminal\')\n        #         x = tf.zeros_like(input=terminal, dtype=util.tf_dtype(dtype=\'long\'))\n        #         y = tf.ones_like(input=terminal, dtype=util.tf_dtype(dtype=\'long\'))\n        #         lengths += tf.where(condition=terminal, x=x, y=y)\n\n        #     return lengths, sequence_values, final_values\n\n        # # Sequence lengths\n        # lengths = tf.zeros_like(input=indices, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # # Shape invariants\n        # start_sequence_values = OrderedDict()\n        # sequence_shapes = OrderedDict()\n        # for name in sequence_values:\n        #     if util.is_nested(name=name):\n        #         start_sequence_values[name] = OrderedDict()\n        #         sequence_shapes[name] = OrderedDict()\n        #         for inner_name, spec in self.values_spec[name].items():\n        #             start_sequence_values[name][inner_name] = tf.zeros(shape=((0, tf.shape(indices)[0]) + spec[\'shape\']))\n        #             shape = tf.TensorShape(dims=((None, None) + spec[\'shape\']))\n        #             sequence_shapes[name][inner_name] = shape\n        #     else:\n        #         start_sequence_values[name] = tf.zeros(shape=((0, tf.shape(indices)[0]) + self.values_spec[name][\'shape\']))\n        #         shape = tf.TensorShape(dims=((None, None) + self.values_spec[name][\'shape\']))\n        #         sequence_shapes[name] = shape\n        # start_final_values = OrderedDict()\n        # final_shapes = OrderedDict()\n        # for name in final_values:\n        #     if util.is_nested(name=name):\n        #         start_final_values[name] = OrderedDict()\n        #         final_shapes[name] = OrderedDict()\n        #         for inner_name, spec in self.values_spec[name].items():\n        #             start_final_values[name][inner_name] = tf.zeros(shape=((tf.shape(indices)[0],) + spec[\'shape\']))\n        #             shape = tf.TensorShape(dims=((None,) + spec[\'shape\']))\n        #             final_shapes[name][inner_name] = shape\n        #     else:\n        #         start_final_values[name] = tf.zeros(shape=((tf.shape(indices)[0],) + self.values_spec[name][\'shape\']))\n        #         shape = tf.TensorShape(dims=((None,) + self.values_spec[name][\'shape\']))\n        #         final_shapes[name] = shape\n\n        # # Retrieve predecessors\n        # lengths, sequence_values, final_values = self.while_loop(\n        #     cond=util.tf_always_true, body=body,\n        #     loop_vars=(lengths, start_sequence_values, start_final_values),\n        #     shape_invariants=(lengths.get_shape(), sequence_shapes, final_shapes),\n        #     back_prop=False, maximum_iterations=horizon\n        # )\n\n        # # Stop gradients\n        # sequence_values = util.fmap(function=tf.stop_gradient, xs=sequence_values)\n        # final_values = util.fmap(function=tf.stop_gradient, xs=final_values)\n\n        if len(sequence_values) == 0:\n            if is_single_final_value:\n                final_values = final_values[0]\n            return lengths, final_values\n\n        elif len(final_values) == 0:\n            if is_single_sequence_value:\n                sequence_values = sequence_values[0]\n            return starts, lengths, sequence_values\n\n        else:\n            if is_single_sequence_value:\n                sequence_values = sequence_values[0]\n            if is_single_final_value:\n                final_values = final_values[0]\n            return starts, lengths, sequence_values, final_values\n'"
tensorforce/core/memories/recent.py,15,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.memories import Queue\n\n\nclass Recent(Queue):\n    """"""\n    Batching memory which always retrieves most recent experiences (specification key: `recent`).\n\n    Args:\n        name (string): Memory name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        capacity (int > 0): Memory capacity\n            (<span style=""color:#00C000""><b>default</b></span>: minimum capacity).\n        values_spec (specification): Values specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        min_capacity (int >= 0): Minimum memory capacity\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def tf_retrieve_timesteps(self, n, past_horizon, future_horizon):\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Check whether memory contains at least one valid timestep\n        num_timesteps = tf.minimum(x=self.buffer_index, y=capacity) - past_horizon - future_horizon\n        assertion = tf.debugging.assert_greater_equal(x=num_timesteps, y=one)\n\n        # Most recent timestep indices range\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            indices = tf.range(start=(self.buffer_index - n), limit=self.buffer_index)\n            indices = tf.math.mod(x=(indices - future_horizon), y=capacity)\n\n        return indices\n\n    def tf_retrieve_episodes(self, n):\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Check whether memory contains at least one episode\n        assertion = tf.debugging.assert_greater_equal(x=self.episode_count, y=one)\n\n        # Get start and limit index for most recent n episodes\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            start = self.terminal_indices[self.episode_count - n]\n            limit = self.terminal_indices[self.episode_count]\n            # Increment terminal of previous episode\n            start = start + one\n            limit = limit + one\n\n            # Correct limit index if smaller than start index\n            limit = limit + tf.where(condition=(limit < start), x=capacity, y=zero)\n\n            # Most recent episode indices range\n            indices = tf.range(start=start, limit=limit)\n            indices = tf.math.mod(x=indices, y=capacity)\n\n        return indices\n'"
tensorforce/core/memories/replay.py,26,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.memories import Queue\n\n\nclass Replay(Queue):\n    """"""\n    Replay memory which randomly retrieves experiences (specification key: `replay`).\n\n    Args:\n        name (string): Memory name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        capacity (int > 0): Memory capacity\n            (<span style=""color:#00C000""><b>default</b></span>: minimum capacity).\n        values_spec (specification): Values specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        min_capacity (int >= 0): Minimum memory capacity\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def tf_retrieve_timesteps(self, n, past_horizon, future_horizon):\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Check whether memory contains at least one valid timestep\n        num_timesteps = tf.minimum(x=self.buffer_index, y=capacity) - past_horizon - future_horizon\n        assertion = tf.debugging.assert_greater_equal(x=num_timesteps, y=one)\n\n        # Randomly sampled timestep indices\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            indices = tf.random.uniform(\n                shape=(n,), maxval=num_timesteps, dtype=util.tf_dtype(dtype=\'long\')\n            )\n            indices = tf.math.mod(\n                x=(self.buffer_index - one - indices - future_horizon), y=capacity\n            )\n\n        return indices\n\n    def tf_retrieve_episodes(self, n):\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n        capacity = tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Check whether memory contains at least one episode\n        assertion = tf.debugging.assert_greater_equal(x=self.episode_count, y=one)\n\n        # Get start and limit indices for randomly sampled n episodes\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            random_terminal_indices = tf.random.uniform(\n                shape=(n,), maxval=self.episode_count, dtype=util.tf_dtype(dtype=\'long\')\n            )\n            starts = tf.gather(params=self.terminal_indices, indices=random_terminal_indices)\n            limits = tf.gather(\n                params=self.terminal_indices, indices=(random_terminal_indices + one)\n            )\n            # Increment terminal of previous episode\n            starts = starts + one\n            limits = limits + one\n\n            # Correct limit indices if smaller than start indices\n            zero_array = tf.fill(\n                dims=(n,), value=tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n            )\n            capacity_array = tf.fill(\n                dims=(n,), value=tf.constant(value=self.capacity, dtype=util.tf_dtype(dtype=\'long\'))\n            )\n            limits = limits + tf.where(condition=(limits < starts), x=capacity_array, y=zero_array)\n\n            # Concatenate randomly sampled episode indices ranges\n            def cond(indices, i):\n                return tf.math.less(x=i, y=n)\n\n            def reduce_range_concat(indices, i):\n                episode_indices = tf.range(start=starts[i], limit=limits[i])\n                indices = tf.concat(values=(indices, episode_indices), axis=0)\n                i = i + one\n                return indices, i\n\n            indices = tf.zeros(shape=(0,), dtype=util.tf_dtype(dtype=\'long\'))\n            indices, _ = self.while_loop(\n                cond=cond, body=reduce_range_concat, loop_vars=(indices, zero),\n                shape_invariants=(tf.TensorShape(dims=(None,)), zero.get_shape()), back_prop=False\n            )\n            indices = tf.math.mod(x=indices, y=capacity)\n\n        return indices\n'"
tensorforce/core/models/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.models.model import Model\n\nfrom tensorforce.core.models.constant import ConstantModel\nfrom tensorforce.core.models.tensorforce import TensorforceModel\nfrom tensorforce.core.models.random import RandomModel\n\n\n__all__ = [\'ConstantModel\', \'Model\', \'RandomModel\', \'TensorforceModel\']\n'"
tensorforce/core/models/constant.py,21,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.models import Model\n\n\nclass ConstantModel(Model):\n    """"""\n    Utility class to return constant actions of a desired shape and with given bounds.\n    """"""\n\n    def __init__(\n        self,\n        # Model\n        name, device, parallel_interactions, buffer_observe, seed, summarizer, config, states,\n        actions,\n        # ConstantModel\n        action_values\n    ):\n        super().__init__(\n            # Model\n            name=name, device=None, parallel_interactions=parallel_interactions,\n            buffer_observe=buffer_observe, seed=seed, execution=None, saver=None,\n            summarizer=summarizer, config=config, states=states, internals=OrderedDict(),\n            actions=actions, preprocessing=None, exploration=0.0, variable_noise=0.0,\n            l2_regularization=0.0\n        )\n\n        # check values\n        self.action_values = action_values\n\n    def tf_core_act(self, states, internals, auxiliaries):\n        assert len(internals) == 0\n\n        actions = OrderedDict()\n        for name, spec in self.actions_spec.items():\n            some_state = next(iter(states.values()))\n            if util.tf_dtype(dtype=\'int\') in (tf.int32, tf.int64):\n                batch_size = tf.shape(input=some_state, out_type=util.tf_dtype(dtype=\'int\'))[0:1]\n            else:\n                batch_size = tf.dtypes.cast(\n                    x=tf.shape(input=some_state)[0:1], dtype=util.tf_dtype(dtype=\'int\')\n                )\n            shape = tf.constant(value=spec[\'shape\'], dtype=util.tf_dtype(dtype=\'int\'))\n            shape = tf.concat(values=(batch_size, shape), axis=0)\n            dtype = util.tf_dtype(dtype=spec[\'type\'])\n\n            if spec[\'type\'] == \'int\':\n                # Action choices\n                int_dtype = util.tf_dtype(dtype=\'int\')\n                choices = list(range(spec[\'num_values\']))\n                choices_tile = ((1,) + spec[\'shape\'] + (1,))\n                choices = np.tile(A=[choices], reps=choices_tile)\n                choices_shape = ((1,) + spec[\'shape\'] + (spec[\'num_values\'],))\n                choices = tf.constant(value=choices, dtype=int_dtype, shape=choices_shape)\n                ones = tf.ones(shape=(len(spec[\'shape\']) + 1,), dtype=int_dtype)\n                batch_size = tf.dtypes.cast(x=shape[0:1], dtype=int_dtype)\n                multiples = tf.concat(values=(batch_size, ones), axis=0)\n                choices = tf.tile(input=choices, multiples=multiples)\n\n                # First unmasked action\n                mask = auxiliaries[name + \'_mask\']\n                num_values = tf.math.count_nonzero(input=mask, axis=-1, dtype=int_dtype)\n                offset = tf.math.cumsum(x=num_values, axis=-1, exclusive=True)\n                if self.action_values is not None and name in self.action_values:\n                    action = self.action_values[name]\n                    num_values = tf.math.count_nonzero(\n                        input=mask[..., :action], axis=-1, dtype=int_dtype\n                    )\n                    action = tf.math.cumsum(x=num_values, axis=-1, exclusive=True)\n                else:\n                    action = tf.zeros_like(input=offset)\n                choices = tf.boolean_mask(tensor=choices, mask=mask)\n                actions[name] = tf.gather(params=choices, indices=(action + offset))\n\n            elif spec[\'type\'] == \'float\' and \'min_value\' in spec:\n                min_value = spec[\'min_value\']\n                max_value = spec[\'max_value\']\n                if self.action_values is not None and name in self.action_values:\n                    assert min_value <= self.action_values[name] <= max_value\n                    action = self.action_values[name]\n                else:\n                    action = min_value + 0.5 * (max_value - min_value)\n                actions[name] = tf.fill(dims=shape, value=tf.constant(value=action, dtype=dtype))\n\n            elif self.action_values is not None and name in self.action_values:\n                value = self.action_values[name]\n                actions[name] = tf.fill(dims=shape, value=tf.constant(value=value, dtype=dtype))\n\n            else:\n                actions[name] = tf.zeros(shape=shape, dtype=dtype)\n\n        return actions, OrderedDict()\n\n    def tf_core_observe(self, states, internals, auxiliaries, actions, terminal, reward):\n        return util.no_operation()\n'"
tensorforce/core/models/model.py,238,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\nfrom copy import deepcopy\nimport os\n\nimport h5py\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import Module, parameter_modules\nfrom tensorforce.core.networks import Preprocessor\n\n\nclass Model(Module):\n\n    def __init__(\n        self,\n        # Model\n        name, device, parallel_interactions, buffer_observe, seed, execution, saver, summarizer,\n        config, states, internals, actions, preprocessing, exploration, variable_noise,\n        l2_regularization\n    ):\n        if summarizer is None or summarizer.get(\'directory\') is None:\n            summary_labels = None\n        else:\n            summary_labels = summarizer.get(\'labels\', (\'graph\',))\n\n        super().__init__(\n            name=name, device=device, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        # Parallel interactions\n        assert isinstance(parallel_interactions, int) and parallel_interactions >= 1\n        self.parallel_interactions = parallel_interactions\n\n        # Buffer observe\n        assert isinstance(buffer_observe, int) and buffer_observe >= 1\n        self.buffer_observe = buffer_observe\n\n        # Seed\n        self.seed = seed\n\n        # Execution\n        self.execution = dict() if execution is None else execution\n        if \'session_config\' in self.execution:\n            session = self.execution[\'session_config\']\n            if \'cluster_def\' in session:\n                session[\'cluster_def\'] = tf.train.ClusterDef(job=[\n                    tf.train.JobDef(name=name, tasks=[\n                        tf.train.JobDef.TasksEntry(key=key, value=value)\n                        for key, value in tasks.items()\n                    ]) for name, tasks in session[\'cluster_def\'].items()\n                ])\n            if \'device_count\' in session:\n                session[\'device_count\'] = [\n                    tf.compat.v1.ConfigProto.DeviceCountEntry(key=value)\n                    for key, value in session[\'device_count\'].items()\n                ]\n            if \'experimental\' in session:\n                session[\'experimental\'] = tf.compat.v1.ConfigProto.Experimental(\n                    **session[\'experimental\']\n                )\n            if \'gpu_options\' in session:\n                if \'experimental\' in session[\'gpu_options\']:\n                    session[\'gpu_options\'][\'experimental\'] = tf.compat.v1.GPUOptions.Experimental(\n                        **session[\'gpu_options\'][\'experimental\']\n                    )\n                session[\'gpu_options\'] = tf.compat.v1.GPUOptions(**session[\'gpu_options\'])\n            if \'graph_options\' in session:\n                if \'optimizer_options\' in session[\'graph_options\']:\n                    session[\'graph_options\'][\'optimizer_options\'] = tf.compat.v1.OptimizerOptions(\n                        **session[\'graph_options\'][\'optimizer_options\']\n                    )\n                session[\'graph_options\'] = tf.compat.v1.GraphOptions(**session[\'graph_options\'])\n            self.execution[\'session_config\'] = tf.compat.v1.ConfigProto(**session)\n\n        # Saver\n        if saver is None:\n            self.saver_spec = None\n        elif not all(\n            key in (\'directory\', \'filename\', \'frequency\', \'load\', \'max-checkpoints\')\n            for key in saver\n        ):\n            raise TensorforceError.value(\n                name=\'agent\', argument=\'saver\', value=list(saver),\n                hint=\'not from {directory,filename,frequency,load,max-checkpoints}\'\n            )\n        elif saver.get(\'directory\') is None:\n            self.saver_spec = None\n        else:\n            self.saver_spec = dict(saver)\n\n        # Summarizer\n        if summarizer is None:\n            self.summarizer_spec = None\n        elif not all(\n            key in (\'custom\', \'directory\', \'flush\', \'frequency\', \'labels\', \'max-summaries\')\n            for key in summarizer\n        ):\n            raise TensorforceError.value(\n                name=\'agent\', argument=\'summarizer\', value=list(summarizer),\n                hint=\'not from {custom,directory,flush,frequency,labels,max-summaries}\'\n            )\n        elif summarizer.get(\'directory\') is None:\n            self.summarizer_spec = None\n        else:\n            self.summarizer_spec = dict(summarizer)\n\n        self.config = None if config is None else dict(config)\n\n        # States/internals/actions specifications\n        self.states_spec = OrderedDict(states)\n        self.internals_spec = OrderedDict() if internals is None else OrderedDict(internals)\n        self.internals_init = OrderedDict()\n        for name in self.internals_spec:\n            self.internals_init[name] = None\n            if name in self.states_spec:\n                raise TensorforceError.collision(\n                    name=\'name\', value=name, group1=\'states\', group2=\'internals\'\n                )\n        self.actions_spec = OrderedDict(actions)\n        for name in self.actions_spec:\n            if name in self.states_spec:\n                raise TensorforceError.collision(\n                    name=\'name\', value=name, group1=\'states\', group2=\'actions\'\n                )\n            if name in self.internals_spec:\n                raise TensorforceError.collision(\n                    name=\'name\', value=name, group1=\'actions\', group2=\'internals\'\n                )\n        self.auxiliaries_spec = OrderedDict()\n        for name, spec in self.actions_spec.items():\n            if spec[\'type\'] == \'int\':\n                name = name + \'_mask\'\n                if name in self.states_spec:\n                    raise TensorforceError.collision(\n                        name=\'name\', value=name, group1=\'states\', group2=\'action-masks\'\n                    )\n                if name in self.internals_spec:\n                    raise TensorforceError.collision(\n                        name=\'name\', value=name, group1=\'internals\', group2=\'action-masks\'\n                    )\n                if name in self.actions_spec:\n                    raise TensorforceError.collision(\n                        name=\'name\', value=name, group1=\'actions\', group2=\'action-masks\'\n                    )\n                self.auxiliaries_spec[name] = dict(\n                    type=\'bool\', shape=(spec[\'shape\'] + (spec[\'num_values\'],))\n                )\n\n        # Preprocessing\n        self.preprocessing = OrderedDict()\n        self.unprocessed_state_spec = dict()\n        for name, spec in self.states_spec.items():\n            if preprocessing is None:\n                layers = None\n            elif name in preprocessing:\n                layers = preprocessing[name]\n            elif spec[\'type\'] in preprocessing:\n                layers = preprocessing[spec[\'type\']]\n            else:\n                layers = None\n            if layers is not None:\n                self.unprocessed_state_spec[name] = spec\n                self.preprocessing[name] = self.add_module(\n                    name=(name + \'-preprocessing\'), module=Preprocessor, is_trainable=False,\n                    input_spec=spec, layers=layers\n                )\n                self.states_spec[name] = self.preprocessing[name].get_output_spec()\n        if preprocessing is not None and \'reward\' in preprocessing:\n            reward_spec = dict(type=\'float\', shape=())\n            self.preprocessing[\'reward\'] = self.add_module(\n                name=(\'reward-preprocessing\'), module=Preprocessor, is_trainable=False,\n                input_spec=reward_spec, layers=preprocessing[\'reward\']\n            )\n            if self.preprocessing[\'reward\'].get_output_spec() != reward_spec:\n                raise TensorforceError.mismatch(\n                    name=\'preprocessing\', argument=\'reward output spec\',\n                    value1=self.preprocessing[\'reward\'].get_output_spec(), value2=reward_spec\n                )\n\n        self.values_spec = OrderedDict(\n            states=self.states_spec, internals=self.internals_spec,\n            auxiliaries=self.auxiliaries_spec, actions=self.actions_spec,\n            terminal=dict(type=\'long\', shape=()), reward=dict(type=\'float\', shape=())\n        )\n\n        # Exploration\n        exploration = 0.0 if exploration is None else exploration\n        if isinstance(exploration, dict) and \\\n                all(name in self.actions_spec for name in exploration):\n            # Different exploration per action\n            self.exploration = OrderedDict()\n            for name in self.actions_spec:\n                if name in exploration:\n                    if self.actions_spec[name][\'type\'] in (\'bool\', \'int\'):\n                        self.exploration[name] = self.add_module(\n                            name=(name + \'-exploration\'), module=exploration[name],\n                            modules=parameter_modules, is_trainable=False, dtype=\'float\',\n                            min_value=0.0, max_value=1.0\n                        )\n                    else:\n                        self.exploration[name] = self.add_module(\n                            name=(name + \'-exploration\'), module=exploration[name],\n                            modules=parameter_modules, is_trainable=False, dtype=\'float\',\n                            min_value=0.0\n                        )\n        else:\n            # Same exploration for all actions\n            self.exploration = self.add_module(\n                name=\'exploration\', module=exploration, modules=parameter_modules,\n                is_trainable=False, dtype=\'float\', min_value=0.0\n            )\n\n        # Variable noise\n        assert variable_noise is None or isinstance(variable_noise, dict) or variable_noise >= 0.0\n        variable_noise = 0.0 if variable_noise is None else variable_noise\n        self.variable_noise = self.add_module(\n            name=\'variable-noise\', module=variable_noise, modules=parameter_modules,\n            is_trainable=False, dtype=\'float\', min_value=0.0\n        )\n\n        # Register global tensors\n        for name, spec in self.states_spec.items():\n            Module.register_tensor(name=name, spec=spec, batched=True)\n        for name, spec in self.internals_spec.items():\n            Module.register_tensor(name=name, spec=spec, batched=True)\n        for name, spec in self.actions_spec.items():\n            Module.register_tensor(name=name, spec=spec, batched=True)\n        Module.register_tensor(name=\'terminal\', spec=dict(type=\'long\', shape=()), batched=True)\n        Module.register_tensor(name=\'reward\', spec=dict(type=\'float\', shape=()), batched=True)\n        Module.register_tensor(name=\'independent\', spec=dict(type=\'bool\', shape=()), batched=False)\n        Module.register_tensor(\n            name=\'deterministic\', spec=dict(type=\'bool\', shape=()), batched=False\n        )\n        Module.register_tensor(name=\'timestep\', spec=dict(type=\'long\', shape=()), batched=False)\n        Module.register_tensor(name=\'episode\', spec=dict(type=\'long\', shape=()), batched=False)\n        Module.register_tensor(name=\'update\', spec=dict(type=\'long\', shape=()), batched=False)\n\n    def initialize(self):\n        """"""\n        Sets up the TensorFlow model graph, starts the servers (distributed mode), creates summarizers\n        and savers, initializes (and enters) the TensorFlow session.\n        """"""\n        tf.compat.v1.reset_default_graph()\n\n        # Create/get our graph, setup local model/global model links, set scope and device.\n        graph_default_context = self.setup_graph()\n\n        # Start a tf Server (in case of distributed setup). Only start once.\n        # if self.execution_type == ""distributed"" and self.server is None and self.is_local_model:\n        # if self.execution_spec is None or self.execution_type == \'local\' or not self.is_local_model:\n        if True:\n            self.server = None\n        else:\n            # Creates and stores a tf server (and optionally joins it if we are a parameter-server).\n            # Only relevant, if we are running in distributed mode.\n            self.server = tf.compat.v1.train.Server(\n                server_or_cluster_def=self.distributed_spec[""cluster_spec""],\n                job_name=self.distributed_spec[""job""],\n                task_index=self.distributed_spec[""task_index""],\n                protocol=self.distributed_spec.get(""protocol""),\n                config=self.distributed_spec.get(""session_config""),\n                start=True\n            )\n            if self.distributed_spec[""job""] == ""ps"":\n                self.server.join()\n                # This is unreachable?\n                quit()\n\n        super().initialize()\n\n        # If we are a global model -> return here.\n        # Saving, syncing, finalizing graph, session is done by local replica model.\n        # if self.execution_spec is not None and self.execution_type == ""distributed"" and not self.is_local_model:\n        #     return\n\n        # Saver/Summary -> Scaffold.\n        # Creates the tf.compat.v1.train.Saver object and stores it in self.saver.\n        # if self.execution_spec is None or self.execution_type == ""single"":\n        if True:\n            saved_variables = self.get_variables(only_saved=True)\n        else:\n            saved_variables = self.global_model.get_variables(only_saved=True)\n\n        # global_variables += [self.global_episode, self.global_timestep]\n\n        # for c in self.get_savable_components():\n        #     c.register_saver_ops()\n\n        # TensorFlow saver object\n        # TODO potentially make other options configurable via saver spec.\n\n        # possibility to turn off?\n        if self.saver_spec is None:\n            max_to_keep = 5\n        else:\n            max_to_keep = self.saver_spec.get(\'max-checkpoints\', 5)\n        self.saver = tf.compat.v1.train.Saver(\n            var_list=saved_variables,  # should be given?\n            reshape=False,\n            sharded=False,\n            max_to_keep=max_to_keep,\n            keep_checkpoint_every_n_hours=10000.0,\n            name=None,\n            restore_sequentially=False,\n            saver_def=None,\n            builder=None,\n            defer_build=False,\n            allow_empty=False,\n            write_version=tf.compat.v1.train.SaverDef.V2,\n            pad_step_number=False,\n            save_relative_paths=False,\n            filename=None\n        )\n\n        self.setup_scaffold()\n\n        # Create necessary hooks for the upcoming session.\n        hooks = self.setup_hooks()\n\n        # We are done constructing: Finalize our graph, create and enter the session.\n        self.setup_session(self.server, hooks, graph_default_context)\n\n        if self.saver_directory is not None:\n            self.save(\n                directory=self.saver_directory, filename=self.saver_filename, format=\'tensorflow\',\n                append=\'timesteps\', no_act_pb=True\n            )\n\n    def setup_graph(self):\n        """"""\n        Creates our Graph and figures out, which shared/global model to hook up to.\n        If we are in a global-model\'s setup procedure, we do not create\n        a new graph (return None as the context). We will instead use the already existing local replica graph\n        of the model.\n\n        Returns: None or the graph\'s as_default()-context.\n        """"""\n        graph_default_context = None\n\n        # Single (non-distributed) mode.\n        # if self.execution_spec is None or self.execution_type == \'single\':\n        if True:\n            self.graph = tf.Graph()\n            graph_default_context = self.graph.as_default()\n            graph_default_context.__enter__()\n            self.global_model = None\n\n        # Distributed tf\n        elif self.execution_type == \'distributed\':\n            # Parameter-server -> Do not build any graph.\n            if self.distributed_spec[""job""] == ""ps"":\n                return None\n\n            # worker -> construct the global (main) model; the one hosted on the ps,\n            elif self.distributed_spec[""job""] == ""worker"":\n                # The local replica model.\n                if self.is_local_model:\n                    graph = tf.Graph()\n                    graph_default_context = graph.as_default()\n                    graph_default_context.__enter__()\n\n                    # Now that the graph is created and entered -> deepcopoy ourselves and setup global model first,\n                    # then continue.\n                    self.global_model = deepcopy(self)\n                    # Switch on global construction/setup-mode for the pass to setup().\n                    self.global_model.is_local_model = False\n                    self.global_model.setup()\n\n                    self.graph = graph\n\n                    # self.as_local_model() for all optimizers:\n                    # self.optimizer_spec = dict(\n                    #     type=\'global_optimizer\',\n                    #     optimizer=self.optimizer_spec\n                    # )\n\n                    self.scope += \'-worker\' + str(self.distributed_spec[""task_index""])\n                # The global_model (whose Variables are hosted by the ps).\n                else:\n                    self.graph = tf.get_default_graph()  # lives in the same graph as local model\n                    self.global_model = None\n                    self.device = tf.compat.v1.train.replica_device_setter(\n                        # Place its Variables on the parameter server(s) (round robin).\n                        #ps_device=""/job:ps"",  # default\n                        # Train-ops for the global_model are hosted locally (on this worker\'s node).\n                        worker_device=self.device,\n                        cluster=self.distributed_spec[""cluster_spec""]\n                    )\n            else:\n                raise TensorforceError(""Unsupported job type: {}!"".format(self.distributed_spec[""job""]))\n        else:\n            raise TensorforceError(""Unsupported distributed type: {}!"".format(self.distributed_spec[""type""]))\n\n        if self.seed is not None:\n            tf.random.set_seed(seed=self.seed)\n\n        return graph_default_context\n\n    def setup_scaffold(self):\n        """"""\n        Creates the tf.compat.v1.train.Scaffold object and assigns it to self.scaffold.\n        Other fields of the Scaffold are generated automatically.\n        """"""\n        # if self.execution_spec is None or self.execution_type == ""single"":\n        if True:\n            global_variables = self.get_variables()\n            # global_variables += [self.global_episode, self.global_timestep]\n            init_op = tf.compat.v1.variables_initializer(var_list=global_variables)\n            if self.summarizer_spec is not None:\n                init_op = tf.group(init_op, self.summarizer_init)\n            if self.graph_summary is None:\n                ready_op = tf.compat.v1.report_uninitialized_variables(var_list=global_variables)\n                ready_for_local_init_op = None\n                local_init_op = None\n            else:\n                ready_op = None\n                ready_for_local_init_op = tf.compat.v1.report_uninitialized_variables(\n                    var_list=global_variables\n                )\n                local_init_op = self.graph_summary\n\n        else:\n            # Global and local variable initializers.\n            global_variables = self.global_model.get_variables()\n            # global_variables += [self.global_episode, self.global_timestep]\n            local_variables = self.get_variables()\n            init_op = tf.compat.v1.variables_initializer(var_list=global_variables)\n            if self.summarizer_spec is not None:\n                init_op = tf.group(init_op, self.summarizer_init)\n            ready_op = tf.compat.v1.report_uninitialized_variables(\n                var_list=(global_variables + local_variables)\n            )\n            ready_for_local_init_op = tf.compat.v1.report_uninitialized_variables(\n                var_list=global_variables\n            )\n            if self.graph_summary is None:\n                local_init_op = tf.group(\n                    tf.compat.v1.variables_initializer(var_list=local_variables),\n                    # Synchronize values of trainable variables.\n                    *(tf.assign(ref=local_var, value=global_var) for local_var, global_var in zip(\n                        self.get_variables(only_trainable=True),\n                        self.global_model.get_variables(only_trainable=True)\n                    ))\n                )\n            else:\n                local_init_op = tf.group(\n                    tf.compat.v1.variables_initializer(var_list=local_variables),\n                    self.graph_summary,\n                    # Synchronize values of trainable variables.\n                    *(tf.assign(ref=local_var, value=global_var) for local_var, global_var in zip(\n                        self.get_variables(only_trainable=True),\n                        self.global_model.get_variables(only_trainable=True)\n                    ))\n                )\n\n        def init_fn(scaffold, session):\n            if self.saver_spec is not None and self.saver_spec.get(\'load\', True):\n                directory = self.saver_spec[\'directory\']\n                load = self.saver_spec.get(\'load\')\n                if isinstance(load, str):\n                    save_path = os.path.join(directory, load)\n                else:\n                    save_path = tf.compat.v1.train.latest_checkpoint(\n                        checkpoint_dir=directory, latest_filename=None\n                    )\n                if save_path is not None:\n                    # global vs local model restored correctly?\n                    scaffold.saver.restore(sess=session, save_path=save_path)\n                    session.run(fetches=util.join_scopes(self.name + \'.reset\', \'timestep-output:0\'))\n\n        # TensorFlow scaffold object\n        # TODO explain what it does.\n        self.scaffold = tf.compat.v1.train.Scaffold(\n            init_op=init_op,\n            init_feed_dict=None,\n            init_fn=init_fn,\n            ready_op=ready_op,\n            ready_for_local_init_op=ready_for_local_init_op,\n            local_init_op=local_init_op,\n            summary_op=None,\n            saver=self.saver,\n            copy_from_scaffold=None\n        )\n\n    def setup_hooks(self):\n        """"""\n        Creates and returns a list of hooks to use in a session. Populates self.saver_directory.\n\n        Returns: List of hooks to use in a session.\n        """"""\n        hooks = list()\n\n        # Checkpoint saver hook\n        if self.saver_spec is not None:  # and (self.execution_type == \'single\' or self.distributed_spec[\'task_index\'] == 0):\n            self.saver_directory = self.saver_spec[\'directory\']\n            self.saver_filename = self.saver_spec.get(\'filename\', self.name)\n            frequency = self.saver_spec.get(\'frequency\', 600)\n            if frequency is not None:\n                hooks.append(tf.compat.v1.train.CheckpointSaverHook(\n                    checkpoint_dir=self.saver_directory, save_secs=frequency, save_steps=None,\n                    saver=None,  # None since given via \'scaffold\' argument.\n                    checkpoint_basename=self.saver_filename, scaffold=self.scaffold, listeners=None\n                ))\n        else:\n            self.saver_directory = None\n            self.saver_filename = self.name\n\n        # Stop at step hook\n        # hooks.append(tf.compat.v1.train.StopAtStepHook(\n        #     num_steps=???,  # This makes more sense, if load and continue training.\n        #     last_step=None  # Either one or the other has to be set.\n        # ))\n\n        # # Step counter hook\n        # hooks.append(tf.compat.v1.train.StepCounterHook(\n        #     every_n_steps=counter_config.get(\'steps\', 100),  # Either one or the other has to be set.\n        #     every_n_secs=counter_config.get(\'secs\'),  # Either one or the other has to be set.\n        #     output_dir=None,  # None since given via \'summary_writer\' argument.\n        #     summary_writer=summary_writer\n        # ))\n\n        # Other available hooks:\n        # tf.compat.v1.train.FinalOpsHook(final_ops, final_ops_feed_dict=None)\n        # tf.compat.v1.train.GlobalStepWaiterHook(wait_until_step)\n        # tf.compat.v1.train.LoggingTensorHook(tensors, every_n_iter=None, every_n_secs=None)\n        # tf.compat.v1.train.NanTensorHook(loss_tensor, fail_on_nan_loss=True)\n        # tf.compat.v1.train.ProfilerHook(save_steps=None, save_secs=None, output_dir=\'\', show_dataflow=True, show_memory=False)\n\n        return hooks\n\n    def setup_session(self, server, hooks, graph_default_context):\n        """"""\n        Creates and then enters the session for this model (finalizes the graph).\n\n        Args:\n            server (tf.compat.v1.train.Server): The tf.compat.v1.train.Server object to connect to (None for single execution).\n            hooks (list): A list of (saver, summary, etc..) hooks to be passed to the session.\n            graph_default_context: The graph as_default() context that we are currently in.\n        """"""\n        # if self.execution_spec is not None and self.execution_type == ""distributed"":\n        if False:\n            # if self.distributed_spec[\'task_index\'] == 0:\n            # TensorFlow chief session creator object\n            session_creator = tf.compat.v1.train.ChiefSessionCreator(\n                scaffold=self.scaffold,\n                master=server.target,\n                config=self.execution.get(\'session_config\'),\n                checkpoint_dir=None,\n                checkpoint_filename_with_path=None\n            )\n            # else:\n            #     # TensorFlow worker session creator object\n            #     session_creator = tf.compat.v1.train.WorkerSessionCreator(\n            #         scaffold=self.scaffold,\n            #         master=server.target,\n            #         config=self.execution_spec.get(\'session_config\'),\n            #     )\n\n            # TensorFlow monitored session object\n            self.monitored_session = tf.compat.v1.train.MonitoredSession(\n                session_creator=session_creator,\n                hooks=hooks,\n                stop_grace_period_secs=120  # Default value.\n            )\n            # from tensorflow.python.debug import DumpingDebugWrapperSession\n            # self.monitored_session = DumpingDebugWrapperSession(self.monitored_session, self.tf_session_dump_dir)\n\n        else:\n            # TensorFlow non-distributed monitored session object\n            self.monitored_session = tf.compat.v1.train.SingularMonitoredSession(\n                hooks=hooks,\n                scaffold=self.scaffold,\n                master=\'\',  # Default value.\n                config=self.execution.get(\'session_config\'),\n                checkpoint_dir=None\n            )\n\n        if graph_default_context:\n            graph_default_context.__exit__(None, None, None)\n        self.graph.finalize()\n\n        # enter the session to be ready for acting/learning\n        self.monitored_session.__enter__()\n        self.session = self.monitored_session._tf_sess()\n\n    def close(self):\n        if self.summarizer_spec is not None:\n            self.monitored_session.run(fetches=self.summarizer_close)\n        if self.saver_directory is not None:\n            self.save(\n                directory=self.saver_directory, filename=self.saver_filename, format=\'tensorflow\',\n                append=\'timesteps\', no_act_pb=True\n            )\n        self.monitored_session.__exit__(None, None, None)\n        tf.compat.v1.reset_default_graph()\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        # States\n        self.states_input = OrderedDict()\n        for name, state_spec in self.states_spec.items():\n            state_spec = self.unprocessed_state_spec.get(name, state_spec)\n            self.states_input[name] = self.add_placeholder(\n                name=name, dtype=state_spec[\'type\'], shape=state_spec[\'shape\'], batched=True\n            )\n\n        # Internals\n        self.internals_input = OrderedDict()\n        for name, internal_spec in self.internals_spec.items():\n            self.internals_input[name] = self.add_placeholder(\n                name=name, dtype=internal_spec[\'type\'], shape=internal_spec[\'shape\'], batched=True\n            )\n\n        # Auxiliaries\n        self.auxiliaries_input = OrderedDict()\n        # Categorical action masks\n        for name, action_spec in self.actions_spec.items():\n            if action_spec[\'type\'] == \'int\':\n                name = name + \'_mask\'\n                shape = action_spec[\'shape\'] + (action_spec[\'num_values\'],)\n                # default = tf.constant(\n                #     value=True, dtype=util.tf_dtype(dtype=\'bool\'), shape=((1,) + shape)\n                # )\n                if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                    batch_size = tf.shape(\n                        input=next(iter(self.states_input.values())),\n                        out_type=util.tf_dtype(dtype=\'long\')\n                    )[:1]\n                else:\n                    batch_size = tf.dtypes.cast(\n                        x=tf.shape(input=next(iter(self.states_input.values())))[:1],\n                        dtype=util.tf_dtype(dtype=\'long\')\n                    )\n                tf_shape = tf.constant(\n                    value=(action_spec[\'shape\'] + (action_spec[\'num_values\'],)),\n                    dtype=util.tf_dtype(dtype=\'long\')\n                )\n                tf_shape = tf.concat(values=(batch_size, tf_shape), axis=0)\n                default = tf.ones(shape=((1,) + shape), dtype=util.tf_dtype(dtype=\'bool\'))\n                self.auxiliaries_input[name] = self.add_placeholder(\n                    name=name, dtype=\'bool\', shape=shape, batched=True, default=default\n                )\n\n        # Terminal  (default: False?)\n        self.terminal_input = self.add_placeholder(\n            name=\'terminal\', dtype=\'long\', shape=(), batched=True\n        )\n\n        # Reward  (default: 0.0?)\n        self.reward_input = self.add_placeholder(\n            name=\'reward\', dtype=\'float\', shape=(), batched=True\n        )\n\n        # Deterministic flag\n        self.deterministic_input = self.add_placeholder(\n            name=\'deterministic\', dtype=\'bool\', shape=(), batched=False,\n            default=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n        )\n\n        # Parallel index\n        self.parallel_input = self.add_placeholder(\n            name=\'parallel\', dtype=\'long\', shape=(), batched=True,\n            default=tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'), shape=(1,))\n        )\n\n        # Local timestep\n        self.timestep = self.add_variable(\n            name=\'timestep\', dtype=\'long\', shape=(self.parallel_interactions,),\n            initializer=\'zeros\', is_trainable=False\n        )\n\n        # Local episode\n        self.episode = self.add_variable(\n            name=\'episode\', dtype=\'long\', shape=(self.parallel_interactions,), initializer=\'zeros\',\n            is_trainable=False\n        )\n\n        # Episode reward\n        self.episode_reward = self.add_variable(\n            name=\'episode-reward\', dtype=\'float\', shape=(self.parallel_interactions,),\n            initializer=\'zeros\', is_trainable=False\n        )\n\n        # States buffer variable\n        self.states_buffer = OrderedDict()\n        for name, spec in self.states_spec.items():\n            self.states_buffer[name] = self.add_variable(\n                name=(name + \'-buffer\'), dtype=spec[\'type\'],\n                shape=((self.parallel_interactions, self.buffer_observe) + spec[\'shape\']),\n                is_trainable=False, is_saved=False\n            )\n\n        # Internals buffer variable\n        self.internals_buffer = OrderedDict()\n        for name, spec in self.internals_spec.items():\n            shape = ((self.parallel_interactions, self.buffer_observe + 1) + spec[\'shape\'])\n            initializer = np.zeros(shape=shape, dtype=util.np_dtype(dtype=spec[\'type\']))\n            initializer[:, 0] = self.internals_init[name]\n            self.internals_buffer[name] = self.add_variable(\n                name=(name + \'-buffer\'), dtype=spec[\'type\'], shape=shape, is_trainable=False,\n                initializer=initializer, is_saved=False\n            )\n\n        # Auxiliaries buffer variable\n        self.auxiliaries_buffer = OrderedDict()\n        for name, spec in self.auxiliaries_spec.items():\n            self.auxiliaries_buffer[name] = self.add_variable(\n                name=(name + \'-buffer\'), dtype=spec[\'type\'],\n                shape=((self.parallel_interactions, self.buffer_observe) + spec[\'shape\']),\n                is_trainable=False, is_saved=False\n            )\n\n        # Actions buffer variable\n        self.actions_buffer = OrderedDict()\n        for name, spec in self.actions_spec.items():\n            self.actions_buffer[name] = self.add_variable(\n                name=(name + \'-buffer\'), dtype=spec[\'type\'],\n                shape=((self.parallel_interactions, self.buffer_observe) + spec[\'shape\']),\n                is_trainable=False, is_saved=False\n            )\n\n        # Buffer index\n        self.buffer_index = self.add_variable(\n            name=\'buffer-index\', dtype=\'long\', shape=(self.parallel_interactions,),\n            initializer=\'zeros\', is_trainable=False, is_saved=False\n        )\n\n    def api_reset(self):\n        assignment = self.buffer_index.assign(\n            value=tf.zeros(shape=(self.parallel_interactions,), dtype=util.tf_dtype(dtype=\'long\')),\n            read_value=False\n        )\n\n        # TODO: Synchronization initial sync?\n\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            timestep = util.identity_operation(\n                x=self.global_timestep, operation_name=\'timestep-output\'\n            )\n            episode = util.identity_operation(\n                x=self.global_episode, operation_name=\'episode-output\'\n            )\n            update = util.identity_operation(\n                x=self.global_update, operation_name=\'update-output\'\n            )\n\n        return timestep, episode, update\n\n    def api_independent_act(self):\n        # Inputs\n        states = OrderedDict(self.states_input)\n        internals = OrderedDict(self.internals_input)\n        auxiliaries = OrderedDict(self.auxiliaries_input)\n\n        true = tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n        zero_float = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n\n        batch_size = tf.shape(\n            input=next(iter(states.values())), out_type=util.tf_dtype(dtype=\'long\')\n        )[:1]\n\n        # Set global tensors\n        Module.update_tensors(\n            independent=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')),\n            deterministic=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')),\n            timestep=self.global_timestep, episode=self.global_episode, update=self.global_update\n        )\n\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Preprocessing states\n        if any(name in self.preprocessing for name in self.states_spec):\n            for name in self.states_spec:\n                if name in self.preprocessing:\n                    states[name] = self.preprocessing[name].apply(x=states[name])\n            dependencies = util.flatten(xs=states)\n        else:\n            dependencies = ()\n\n        # Variable noise\n        variables = self.get_variables(only_trainable=True)\n        variable_noise = self.variable_noise.final_value()\n        if len(variables) > 0 and variable_noise > 0.0:\n            variable_noise = tf.constant(\n                value=variable_noise, dtype=util.tf_dtype(dtype=self.variable_noise.dtype)\n            )\n            with tf.control_dependencies(control_inputs=dependencies):\n                dependencies = list()\n                variable_noise_tensors = list()\n                for variable in variables:\n                    if variable.dtype == util.tf_dtype(dtype=\'float\'):\n                        noise = tf.random.normal(\n                            shape=util.shape(variable), mean=0.0, stddev=variable_noise,\n                            dtype=util.tf_dtype(dtype=\'float\')\n                        )\n                    else:\n                        noise = tf.random.normal(\n                            shape=util.shape(variable), mean=0.0,\n                            stddev=tf.dtypes.cast(x=variable_noise, dtype=variable.dtype),\n                            dtype=variable.dtype\n                        )\n                    variable_noise_tensors.append(noise)\n                    dependencies.append(variable.assign_add(delta=noise, read_value=False))\n        else:\n            variable_noise = None\n\n        # Core act: retrieve actions and internals\n        with tf.control_dependencies(control_inputs=dependencies):\n            actions, internals = self.core_act(\n                states=states, internals=internals, auxiliaries=auxiliaries\n            )\n            dependencies = util.flatten(xs=actions) + util.flatten(xs=internals)\n\n        # Exploration\n        if not isinstance(self.exploration, dict):\n            exploration = self.exploration.final_value()\n            if exploration > 0.0:\n                exploration = tf.constant(\n                    value=exploration, dtype=util.tf_dtype(dtype=self.exploration.dtype)\n                )\n            else:\n                exploration = None\n\n        for name, spec in self.actions_spec.items():\n            if isinstance(self.exploration, dict):\n                if name in self.exploration:\n                    exploration = self.exploration[name].final_value()\n                    if exploration > 0.0:\n                        exploration = tf.constant(\n                            value=exploration,\n                            dtype=util.tf_dtype(dtype=self.exploration[name].dtype)\n                        )\n                    else:\n                        exploration = None\n                else:\n                    continue\n\n            if exploration is None:\n                continue\n\n            with tf.control_dependencies(control_inputs=dependencies):\n\n                float_dtype = util.tf_dtype(dtype=\'float\')\n                shape = tf.shape(input=actions[name])\n\n                if spec[\'type\'] == \'bool\':\n                    condition = tf.random.uniform(shape=shape, dtype=float_dtype) < exploration\n                    half = tf.constant(value=0.5, dtype=float_dtype)\n                    random_action = tf.random.uniform(shape=shape, dtype=float_dtype) < half\n                    actions[name] = tf.where(condition=condition, x=random_action, y=actions[name])\n\n                elif spec[\'type\'] == \'int\':\n                    int_dtype = util.tf_dtype(dtype=\'int\')\n\n                    # (Same code as for RandomModel)\n                    shape = tf.shape(input=actions[name])\n\n                    # Action choices\n                    choices = list(range(spec[\'num_values\']))\n                    choices_tile = ((1,) + spec[\'shape\'] + (1,))\n                    choices = np.tile(A=[choices], reps=choices_tile)\n                    choices_shape = ((1,) + spec[\'shape\'] + (spec[\'num_values\'],))\n                    choices = tf.constant(value=choices, dtype=int_dtype, shape=choices_shape)\n                    ones = tf.ones(shape=(len(spec[\'shape\']) + 1,), dtype=tf.dtypes.int32)\n                    batch_size = tf.dtypes.cast(x=shape[0:1], dtype=tf.dtypes.int32)\n                    multiples = tf.concat(values=(batch_size, ones), axis=0)\n                    choices = tf.tile(input=choices, multiples=multiples)\n\n                    # Random unmasked action\n                    mask = auxiliaries[name + \'_mask\']\n                    num_values = tf.math.count_nonzero(\n                        input=mask, axis=-1, dtype=tf.dtypes.int32\n                    )\n                    random_action = tf.random.uniform(shape=shape, dtype=float_dtype)\n                    random_action = tf.dtypes.cast(\n                        x=(random_action * tf.dtypes.cast(x=num_values, dtype=float_dtype)),\n                        dtype=tf.dtypes.int32\n                    )\n\n                    # Correct for masked actions\n                    choices = tf.boolean_mask(tensor=choices, mask=mask)\n                    offset = tf.math.cumsum(x=num_values, axis=-1, exclusive=True)\n                    random_action = tf.gather(params=choices, indices=(random_action + offset))\n\n                    # Random action\n                    condition = tf.random.uniform(shape=shape, dtype=float_dtype) < exploration\n                    actions[name] = tf.where(condition=condition, x=random_action, y=actions[name])\n\n                elif spec[\'type\'] == \'float\':\n                    if \'min_value\' in spec:\n                        noise = tf.random.normal(shape=shape, dtype=float_dtype) * exploration\n                        actions[name] = tf.clip_by_value(\n                            t=(actions[name] + noise), clip_value_min=spec[\'min_value\'],\n                            clip_value_max=spec[\'max_value\']\n                        )\n\n                    else:\n                        noise = tf.random.normal(shape=shape, dtype=float_dtype) * exploration\n                        actions[name] = actions[name] + noise\n\n                dependencies = util.flatten(xs=actions)\n\n        # Variable noise\n        if variable_noise is not None:\n            with tf.control_dependencies(control_inputs=dependencies):\n                dependencies = list()\n                for variable, noise in zip(variables, variable_noise_tensors):\n                    dependencies.append(variable.assign_sub(delta=noise, read_value=False))\n\n        # Return values\n        with tf.control_dependencies(control_inputs=dependencies):\n            # Function-level identity operation for retrieval (plus enforce dependency)\n            for name, spec in self.actions_spec.items():\n                actions[name] = util.identity_operation(\n                    x=actions[name], operation_name=(name + \'-output\')\n                )\n            for name, spec in self.internals_spec.items():\n                internals[name] = util.identity_operation(\n                    x=internals[name], operation_name=(name + \'-output\')\n                )\n\n        return actions, internals\n\n    def api_act(self):\n        # Inputs\n        states = OrderedDict(self.states_input)\n        auxiliaries = OrderedDict(self.auxiliaries_input)\n        parallel = self.parallel_input\n\n        true = tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n        zero_float = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n\n        if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n            parallel_shape = tf.shape(input=parallel, out_type=util.tf_dtype(dtype=\'long\'))\n        else:\n            parallel_shape = tf.dtypes.cast(\n                x=tf.shape(input=parallel), dtype=util.tf_dtype(dtype=\'long\')\n            )\n\n        # Assertions\n        assertions = list()\n        # states: type and shape\n        for name, spec in self.states_spec.items():\n            spec = self.unprocessed_state_spec.get(name, spec)\n            tf.debugging.assert_type(\n                tensor=states[name], tf_type=util.tf_dtype(dtype=spec[\'type\']),\n                message=""Agent.act: invalid type for {} state input."".format(name)\n            )\n            shape = tf.constant(value=spec[\'shape\'], dtype=util.tf_dtype(dtype=\'long\'))\n            if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                actual_shape = tf.shape(input=states[name], out_type=util.tf_dtype(dtype=\'long\'))\n            else:\n                actual_shape = tf.dtypes.cast(\n                    x=tf.shape(input=states[name]), dtype=util.tf_dtype(dtype=\'long\')\n                )\n            assertions.append(\n                tf.debugging.assert_equal(\n                    x=actual_shape, y=tf.concat(values=(parallel_shape, shape), axis=0),\n                    message=""Agent.act: invalid shape for {} state input."".format(name)\n                )\n            )\n        # action_masks: type and shape\n        for name, spec in self.actions_spec.items():\n            if spec[\'type\'] == \'int\':\n                name = name + \'_mask\'\n                tf.debugging.assert_type(\n                    tensor=auxiliaries[name], tf_type=util.tf_dtype(dtype=\'bool\'),\n                    message=""Agent.act: invalid type for {} action-mask input."".format(name)\n                )\n                shape = tf.constant(\n                    value=(spec[\'shape\'] + (spec[\'num_values\'],)),\n                    dtype=util.tf_dtype(dtype=\'long\')\n                )\n                if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n                    actual_shape = tf.shape(\n                        input=auxiliaries[name], out_type=util.tf_dtype(dtype=\'long\')\n                    )\n                else:\n                    actual_shape = tf.dtypes.cast(\n                        x=tf.shape(input=auxiliaries[name]), dtype=util.tf_dtype(dtype=\'long\')\n                    )\n                assertions.append(\n                    tf.debugging.assert_equal(\n                        x=actual_shape, y=tf.concat(values=(parallel_shape, shape), axis=0),\n                        message=""Agent.act: invalid shape for {} action-mask input."".format(name)\n                    )\n                )\n                assertions.append(\n                    tf.debugging.assert_equal(\n                        x=tf.reduce_all(\n                            input_tensor=tf.reduce_any(\n                                input_tensor=auxiliaries[name], axis=(len(spec[\'shape\']) + 1)\n                            ), axis=tuple(range(len(spec[\'shape\']) + 1))\n                        ),\n                        y=true, message=""Agent.act: at least one action has to be valid for {} ""\n                                        ""action-mask input."".format(name)\n                    )\n                )\n        # parallel: type, shape and value\n        tf.debugging.assert_type(\n            tensor=parallel, tf_type=util.tf_dtype(dtype=\'long\'),\n            message=""Agent.act: invalid type for parallel input.""\n        )\n        assertions.append(tf.debugging.assert_rank(\n            x=parallel, rank=1, message=""Agent.act: invalid shape for parallel input.""\n        ))\n        assertions.append(tf.debugging.assert_non_negative(\n            x=parallel, message=""Agent.act: parallel input has to be non-negative.""\n        ))\n        assertions.append(\n            tf.debugging.assert_less(\n                x=parallel,\n                y=tf.constant(value=self.parallel_interactions, dtype=util.tf_dtype(dtype=\'long\')),\n                message=""Agent.act: parallel input has to be less than parallel_interactions.""\n            )\n        )\n\n        # Set global tensors\n        Module.update_tensors(\n            independent=tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\')),\n            deterministic=tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\')),\n            timestep=self.global_timestep, episode=self.global_episode, update=self.global_update\n        )\n\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n\n        dependencies = assertions\n\n        # Preprocessing states\n        if any(name in self.preprocessing for name in self.states_spec):\n            with tf.control_dependencies(control_inputs=dependencies):\n                for name in self.states_spec:\n                    if name in self.preprocessing:\n                        states[name] = self.preprocessing[name].apply(x=states[name])\n            dependencies = util.flatten(xs=states)\n\n        # Variable noise\n        variables = self.get_variables(only_trainable=True)\n        if len(variables) > 0:\n            with tf.control_dependencies(control_inputs=dependencies):\n                dependencies = list()\n                variable_noise_tensors = list()\n                variable_noise = self.variable_noise.value()\n                for variable in variables:\n                    if variable.dtype == util.tf_dtype(dtype=\'float\'):\n                        noise = tf.random.normal(\n                            shape=util.shape(variable), mean=0.0, stddev=variable_noise,\n                            dtype=util.tf_dtype(dtype=\'float\')\n                        )\n                    else:\n                        noise = tf.random.normal(\n                            shape=util.shape(variable), mean=0.0,\n                            stddev=tf.dtypes.cast(x=variable_noise, dtype=variable.dtype),\n                            dtype=variable.dtype\n                        )\n                    variable_noise_tensors.append(noise)\n                    dependencies.append(variable.assign_add(delta=noise, read_value=False))\n\n        # Initialize or retrieve internals\n        internals = OrderedDict()\n        if len(self.internals_spec) > 0:\n            with tf.control_dependencies(control_inputs=dependencies):\n                for name, spec in self.internals_spec.items():\n                    buffer_index = tf.gather(params=self.buffer_index, indices=parallel)\n                    indices = tf.stack(values=(parallel, buffer_index), axis=1)\n                    internals[name] = tf.gather_nd(\n                        params=self.internals_buffer[name], indices=indices\n                    )\n\n                dependencies = util.flatten(xs=internals)\n\n        # Core act: retrieve actions and internals\n        with tf.control_dependencies(control_inputs=dependencies):\n            actions, internals = self.core_act(\n                states=states, internals=internals, auxiliaries=auxiliaries\n            )\n            dependencies = util.flatten(xs=actions) + util.flatten(xs=internals)\n\n        # Check action masks\n        # TODO: also check float bounds, move after exploration?\n        assertions = list()\n        for name, spec in self.actions_spec.items():\n            if spec[\'type\'] == \'int\':\n                indices = tf.dtypes.cast(x=actions[name], dtype=util.tf_dtype(dtype=\'long\'))\n                indices = tf.expand_dims(input=indices, axis=-1)\n                is_unmasked = tf.gather(\n                    params=auxiliaries[name + \'_mask\'], indices=indices, batch_dims=-1\n                )\n                assertions.append(tf.debugging.assert_equal(\n                    x=tf.math.reduce_all(input_tensor=is_unmasked), y=true,\n                    message=""Action mask check.""\n                ))\n        dependencies += assertions\n\n        # Exploration\n        with tf.control_dependencies(control_inputs=dependencies):\n            if not isinstance(self.exploration, dict):\n                exploration = self.exploration.value()\n\n            for name, spec in self.actions_spec.items():\n                if isinstance(self.exploration, dict):\n                    if name in self.exploration:\n                        exploration = self.exploration[name].value()\n                    else:\n                        continue\n\n                float_dtype = util.tf_dtype(dtype=\'float\')\n                shape = tf.shape(input=actions[name])\n\n                if spec[\'type\'] == \'bool\':\n                    condition = tf.random.uniform(shape=shape, dtype=float_dtype) < exploration\n                    half = tf.constant(value=0.5, dtype=float_dtype)\n                    random_action = tf.random.uniform(shape=shape, dtype=float_dtype) < half\n                    actions[name] = tf.where(condition=condition, x=random_action, y=actions[name])\n\n                elif spec[\'type\'] == \'int\':\n                    int_dtype = util.tf_dtype(dtype=\'int\')\n\n                    # (Same code as for RandomModel)\n                    shape = tf.shape(input=actions[name])\n\n                    # Action choices\n                    choices = list(range(spec[\'num_values\']))\n                    choices_tile = ((1,) + spec[\'shape\'] + (1,))\n                    choices = np.tile(A=[choices], reps=choices_tile)\n                    choices_shape = ((1,) + spec[\'shape\'] + (spec[\'num_values\'],))\n                    choices = tf.constant(value=choices, dtype=int_dtype, shape=choices_shape)\n                    ones = tf.ones(shape=(len(spec[\'shape\']) + 1,), dtype=tf.dtypes.int32)\n                    batch_size = tf.dtypes.cast(x=shape[0:1], dtype=tf.dtypes.int32)\n                    multiples = tf.concat(values=(batch_size, ones), axis=0)\n                    choices = tf.tile(input=choices, multiples=multiples)\n\n                    # Random unmasked action\n                    mask = auxiliaries[name + \'_mask\']\n                    num_values = tf.math.count_nonzero(\n                        input=mask, axis=-1, dtype=tf.dtypes.int32\n                    )\n                    random_action = tf.random.uniform(shape=shape, dtype=float_dtype)\n                    random_action = tf.dtypes.cast(\n                        x=(random_action * tf.dtypes.cast(x=num_values, dtype=float_dtype)),\n                        dtype=tf.dtypes.int32\n                    )\n\n                    # Correct for masked actions\n                    choices = tf.boolean_mask(tensor=choices, mask=mask)\n                    offset = tf.math.cumsum(x=num_values, axis=-1, exclusive=True)\n                    random_action = tf.gather(params=choices, indices=(random_action + offset))\n\n                    # Random action\n                    condition = tf.random.uniform(shape=shape, dtype=float_dtype) < exploration\n                    actions[name] = tf.where(condition=condition, x=random_action, y=actions[name])\n\n                elif spec[\'type\'] == \'float\':\n                    if \'min_value\' in spec:\n                        noise = tf.random.normal(shape=shape, dtype=float_dtype) * exploration\n                        actions[name] = tf.clip_by_value(\n                            t=(actions[name] + noise), clip_value_min=spec[\'min_value\'],\n                            clip_value_max=spec[\'max_value\']\n                        )\n\n                    else:\n                        noise = tf.random.normal(shape=shape, dtype=float_dtype) * exploration\n                        actions[name] = actions[name] + noise\n\n            dependencies = util.flatten(xs=actions)\n\n        # Variable noise\n        if len(variables) > 0:\n            with tf.control_dependencies(control_inputs=dependencies):\n                dependencies = list()\n                for variable, noise in zip(variables, variable_noise_tensors):\n                    dependencies.append(variable.assign_sub(delta=noise, read_value=False))\n\n        # Update states/internals/actions buffers\n        with tf.control_dependencies(control_inputs=dependencies):\n            operations = list()\n            buffer_index = tf.gather(params=self.buffer_index, indices=parallel)\n            indices = tf.stack(values=(parallel, buffer_index), axis=1)\n            for name in self.states_spec:\n                operations.append(\n                    self.states_buffer[name].scatter_nd_update(\n                        indices=indices, updates=states[name]\n                    )\n                )\n            for name in self.auxiliaries_spec:\n                operations.append(\n                    self.auxiliaries_buffer[name].scatter_nd_update(\n                        indices=indices, updates=auxiliaries[name]\n                    )\n                )\n            for name in self.actions_spec:\n                operations.append(\n                    self.actions_buffer[name].scatter_nd_update(\n                        indices=indices, updates=actions[name]\n                    )\n                )\n            indices = tf.stack(values=(parallel, buffer_index + one), axis=1)\n            for name in self.internals_spec:\n                operations.append(\n                    self.internals_buffer[name].scatter_nd_update(\n                        indices=indices, updates=internals[name]\n                    )\n                )\n\n            # Increment buffer index\n            with tf.control_dependencies(control_inputs=operations):\n                incremented_buffer_index = self.buffer_index.scatter_nd_add(\n                    indices=tf.expand_dims(input=parallel, axis=1),\n                    updates=tf.fill(dims=parallel_shape, value=one)\n                )\n\n        # Increment timestep\n        with tf.control_dependencies(control_inputs=(incremented_buffer_index,)):\n            assignments = list()\n            assignments.append(\n                self.timestep.scatter_nd_add(\n                    indices=tf.expand_dims(input=parallel, axis=1),\n                    updates=tf.fill(dims=parallel_shape, value=one)\n                )\n            )\n            assignments.append(self.global_timestep.assign_add(\n                delta=parallel_shape[0], read_value=False\n            ))\n\n        # Return timestep\n        with tf.control_dependencies(control_inputs=assignments):\n            # Function-level identity operation for retrieval (plus enforce dependency)\n            for name, spec in self.actions_spec.items():\n                actions[name] = util.identity_operation(\n                    x=actions[name], operation_name=(name + \'-output\')\n                )\n            timestep = util.identity_operation(\n                x=self.global_timestep, operation_name=\'timestep-output\'\n            )\n\n        return actions, timestep\n\n    def api_observe(self):\n        # Inputs\n        terminal = self.terminal_input\n        reward = self.reward_input\n        parallel = self.parallel_input\n\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n\n        buffer_index = tf.gather(params=self.buffer_index, indices=parallel)\n\n        # Assertions\n        assertions = list()\n        # terminal: type and shape\n        tf.debugging.assert_type(\n            tensor=terminal, tf_type=util.tf_dtype(dtype=\'long\'),\n            message=""Agent.observe: invalid type for terminal input.""\n        )\n        assertions.append(tf.debugging.assert_rank(\n            x=terminal, rank=1, message=""Agent.observe: invalid shape for terminal input.""\n        ))\n        # reward: type and shape\n        tf.debugging.assert_type(\n            tensor=reward, tf_type=util.tf_dtype(dtype=\'float\'),\n            message=""Agent.observe: invalid type for reward input.""\n        )\n        assertions.append(tf.debugging.assert_rank(\n            x=reward, rank=1, message=""Agent.observe: invalid shape for reward input.""\n        ))\n        # parallel: type, shape and value\n        tf.debugging.assert_type(\n            tensor=parallel, tf_type=util.tf_dtype(dtype=\'long\'),\n            message=""Agent.observe: invalid type for parallel input.""\n        )\n        tf.debugging.assert_scalar(\n            tensor=parallel[0], message=""Agent.observe: parallel input has to be a scalar.""\n        )\n        assertions.append(tf.debugging.assert_non_negative(\n            x=parallel, message=""Agent.observe: parallel input has to be non-negative.""\n        ))\n        assertions.append(tf.debugging.assert_less(\n            x=parallel[0],\n            y=tf.constant(value=self.parallel_interactions, dtype=util.tf_dtype(dtype=\'long\')),\n            message=""Agent.observe: parallel input has to be less than parallel_interactions.""\n        ))\n        # shape of terminal equals shape of reward\n        assertions.append(tf.debugging.assert_equal(\n            x=tf.shape(input=terminal), y=tf.shape(input=reward),\n            message=""Agent.observe: incompatible shapes of terminal and reward input.""\n        ))\n        # size of terminal equals buffer index\n        assertions.append(tf.debugging.assert_equal(\n            x=tf.shape(input=terminal, out_type=util.tf_dtype(dtype=\'long\'))[0],\n            y=tf.dtypes.cast(x=buffer_index, dtype=util.tf_dtype(dtype=\'long\')),\n            message=""Agent.observe: number of observe-timesteps has to be equal to number of ""\n                    ""buffered act-timesteps.""\n        ))\n        # at most one terminal\n        assertions.append(tf.debugging.assert_less_equal(\n            x=tf.math.count_nonzero(input=terminal, dtype=util.tf_dtype(dtype=\'long\')),\n            y=tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\')),\n            message=""Agent.observe: input contains more than one terminal.""\n        ))\n        # if terminal, last timestep in batch\n        assertions.append(tf.debugging.assert_equal(\n            x=tf.math.reduce_any(input_tensor=tf.math.greater(x=terminal, y=zero)),\n            y=tf.math.greater(x=terminal[-1], y=zero),\n            message=""Agent.observe: terminal is not the last input timestep.""\n        ))\n\n        # Set global tensors\n        Module.update_tensors(\n            independent=tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\')),\n            deterministic=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')),\n            timestep=self.global_timestep, episode=self.global_episode, update=self.global_update\n        )\n\n        with tf.control_dependencies(control_inputs=assertions):\n            reward = self.add_summary(\n                label=(\'timestep-reward\', \'rewards\'), name=\'timestep-reward\', tensor=reward\n            )\n            assignment = self.episode_reward.scatter_nd_add(\n                indices=tf.expand_dims(input=parallel, axis=1),\n                updates=[tf.math.reduce_sum(input_tensor=reward, axis=0)]\n            )\n\n        # Reset episode reward\n        def reset_episode_reward():\n            zero_float = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n            zero_float = self.add_summary(\n                label=(\'episode-reward\', \'rewards\'), name=\'episode-reward\',\n                tensor=tf.gather(params=self.episode_reward, indices=parallel),\n                pass_tensors=zero_float  # , step=\'episode\'\n            )\n            assignment = self.episode_reward.scatter_nd_update(\n                indices=tf.expand_dims(input=parallel, axis=1), updates=[zero_float]\n            )\n            with tf.control_dependencies(control_inputs=(assignment,)):\n                return util.no_operation()\n\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            reset_episode_rew = self.cond(\n                pred=(terminal[-1] > zero), true_fn=reset_episode_reward, false_fn=util.no_operation\n            )\n            dependencies = (reset_episode_rew,)\n\n        # Preprocessing reward\n        if \'reward\' in self.preprocessing:\n            with tf.control_dependencies(control_inputs=dependencies):\n                reward = self.preprocessing[\'reward\'].apply(x=reward)\n            dependencies = (reward,)\n\n        # Increment episode\n        def increment_episode():\n            assignments = list()\n            one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n            assignments.append(self.episode.scatter_nd_add(\n                indices=tf.expand_dims(input=parallel, axis=1), updates=[one])\n            )\n            assignments.append(self.global_episode.assign_add(delta=one, read_value=False))\n            with tf.control_dependencies(control_inputs=assignments):\n                return util.no_operation()\n\n        with tf.control_dependencies(control_inputs=dependencies):\n            incremented_episode = self.cond(\n                pred=(terminal[-1] > zero), true_fn=increment_episode, false_fn=util.no_operation\n            )\n            dependencies = (incremented_episode,)\n\n        # Core observe: retrieve observe operation\n        with tf.control_dependencies(control_inputs=dependencies):\n            states = OrderedDict()\n            for name in self.states_spec:\n                states[name] = self.states_buffer[name][parallel[0], :buffer_index[0]]\n            internals = OrderedDict()\n            for name in self.internals_spec:\n                internals[name] = self.internals_buffer[name][parallel[0], :buffer_index[0]]\n            auxiliaries = OrderedDict()\n            for name in self.auxiliaries_spec:\n                auxiliaries[name] = self.auxiliaries_buffer[name][parallel[0], :buffer_index[0]]\n            actions = OrderedDict()\n            for name in self.actions_spec:\n                actions[name] = self.actions_buffer[name][parallel[0], :buffer_index[0]]\n\n            reward = self.add_summary(\n                label=(\'raw-reward\', \'rewards\'), name=\'raw-reward\', tensor=reward\n            )\n\n            is_updated = self.core_observe(\n                states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n                terminal=terminal, reward=reward\n            )\n\n        # Reset buffer index\n        with tf.control_dependencies(control_inputs=(is_updated,)):\n            reset_buffer_index = self.buffer_index.scatter_nd_update(\n                indices=tf.expand_dims(input=parallel, axis=1), updates=[zero]\n            )\n            dependencies = (reset_buffer_index,)\n\n        if len(self.preprocessing) > 0:\n            with tf.control_dependencies(control_inputs=dependencies):\n\n                def reset_preprocessors():\n                    operations = list()\n                    for preprocessor in self.preprocessing.values():\n                        operations.append(preprocessor.reset())\n                    return tf.group(*operations)\n\n                preprocessors_reset = self.cond(\n                    pred=(terminal[-1] > zero), true_fn=reset_preprocessors,\n                    false_fn=util.no_operation\n                )\n                dependencies = (preprocessors_reset,)\n\n        # Return episode\n        with tf.control_dependencies(control_inputs=dependencies):\n            # Function-level identity operation for retrieval (plus enforce dependency)\n            updated = util.identity_operation(x=is_updated, operation_name=\'updated-output\')\n            episode = util.identity_operation(\n                x=self.global_episode, operation_name=\'episode-output\'\n            )\n            update = util.identity_operation(\n                x=self.global_update, operation_name=\'update-output\'\n            )\n\n        return updated, episode, update\n\n    def tf_core_act(self, states, internals, auxiliaries):\n        raise NotImplementedError\n\n    def tf_core_observe(self, states, internals, auxiliaries, actions, terminal, reward):\n        raise NotImplementedError\n\n    def tf_regularize(self, states, internals, auxiliaries):\n        return super().tf_regularize()\n\n    def get_variable(self, variable):\n        if not variable.startswith(self.name):\n            variable = util.join_scopes(self.name, variable)\n        fetches = variable + \'-output:0\'\n        return self.monitored_session.run(fetches=fetches)\n\n    def assign_variable(self, variable, value):\n        if variable.startswith(self.name + \'/\'):\n            variable = variable[len(self.name) + 1:]\n        module = self\n        scope = variable.split(\'/\')\n        for _ in range(len(scope) - 1):\n            module = module.modules[scope.pop(0)]\n        fetches = util.join_scopes(self.name, variable) + \'-assign\'\n        dtype = util.dtype(x=module.variables[scope[0]])\n        feed_dict = {util.join_scopes(self.name, \'assignment-\') + dtype + \'-input:0\': value}\n        self.monitored_session.run(fetches=fetches, feed_dict=feed_dict)\n\n    def summarize(self, summary, value, step=None):\n        fetches = util.join_scopes(self.name, summary, \'write_summary\', \'Const:0\')\n        feed_dict = {util.join_scopes(self.name, \'summarize-input:0\'): value}\n        if step is not None:\n            feed_dict[util.join_scopes(self.name, \'summarize-step-input:0\')] = step\n        self.monitored_session.run(fetches=fetches, feed_dict=feed_dict)\n\n    def save(self, directory, filename, format, append=None, no_act_pb=False):\n        path = os.path.join(directory, filename)\n\n        if append == \'timesteps\':\n            append = self.global_timestep\n        elif append == \'episodes\':\n            append = self.global_episode\n        elif append == \'updates\':\n            append = self.global_update\n        else:\n            assert append is None\n\n        if format == \'tensorflow\':\n            if self.summarizer_spec is not None:\n                self.monitored_session.run(fetches=self.summarizer_flush)\n            saver_path = self.saver.save(\n                sess=self.session, save_path=path, global_step=append,\n                # latest_filename=None,  # Defaults to \'checkpoint\'.\n                meta_graph_suffix=\'meta\', write_meta_graph=True, write_state=True\n            )\n            assert saver_path.startswith(path)\n            path = saver_path\n\n            if not no_act_pb:\n                graph_def = self.graph.as_graph_def()\n\n                # freeze_graph clear_devices option\n                for node in graph_def.node:\n                    node.device = \'\'\n\n                graph_def = tf.compat.v1.graph_util.remove_training_nodes(input_graph=graph_def)\n                output_node_names = [\n                    self.name + \'.independent_act/\' + name + \'-output\'\n                    for name in self.output_tensors[\'independent_act\']\n                ]\n                # implies tf.compat.v1.graph_util.extract_sub_graph\n                graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(\n                    sess=self.monitored_session, input_graph_def=graph_def,\n                    output_node_names=output_node_names\n                )\n                graph_path = tf.io.write_graph(\n                    graph_or_graph_def=graph_def, logdir=directory,\n                    name=(os.path.split(path)[1] + \'.pb\'), as_text=False\n                )\n                assert graph_path == path + \'.pb\'\n\n        elif format == \'numpy\':\n            if append is not None:\n                append = self.monitored_session.run(fetches=append)\n                path += \'-\' + str(append)\n            path += \'.npz\'\n            variables = dict()\n            for variable in self.get_variables(only_saved=True):\n                name = variable.name[len(self.name) + 1: -2]\n                variables[name] = self.get_variable(variable=name)\n            np.savez(file=path, **variables)\n\n        elif format == \'hdf5\':\n            if append is not None:\n                append = self.monitored_session.run(fetches=append)\n                path += \'-\' + str(append)\n            path += \'.hdf5\'\n            with h5py.File(name=path, mode=\'w\') as filehandle:\n                for variable in self.get_variables(only_saved=True):\n                    name = variable.name[len(self.name) + 1: -2]\n                    filehandle.create_dataset(name=name, data=self.get_variable(variable=name))\n\n        else:\n            assert False\n\n        return path\n\n    def restore(self, directory, filename, format):\n        path = os.path.join(directory, filename)\n\n        if format == \'tensorflow\':\n            self.saver.restore(sess=self.session, save_path=path)\n\n        elif format == \'numpy\':\n            variables = np.load(file=(path + \'.npz\'))\n            for variable in self.get_variables(only_saved=True):\n                name = variable.name[len(self.name) + 1: -2]\n                self.assign_variable(variable=name, value=variables[name])\n\n        elif format == \'hdf5\':\n            if os.path.isfile(path + \'.hdf5\'):\n                path = path + \'.hdf5\'\n            else:\n                path = path + \'.h5\'\n            with h5py.File(name=path, mode=\'r\') as filehandle:\n                for variable in self.get_variables(only_saved=True):\n                    name = variable.name[len(self.name) + 1: -2]\n                    self.assign_variable(variable=name, value=filehandle[name])\n\n        else:\n            assert False\n\n        fetches = (self.global_timestep, self.global_episode, self.global_update)\n        return self.monitored_session.run(fetches=fetches)\n'"
tensorforce/core/models/random.py,22,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core.models import Model\n\n\nclass RandomModel(Model):\n    """"""\n    Utility class to return random actions of a desired shape and with given bounds.\n    """"""\n\n    def __init__(\n        self,\n        # Model\n        name, device, parallel_interactions, buffer_observe, seed, summarizer, config, states,\n        actions\n    ):\n        super().__init__(\n            # Model\n            name=name, device=None, parallel_interactions=parallel_interactions,\n            buffer_observe=buffer_observe, seed=seed, execution=None, saver=None,\n            summarizer=summarizer, config=config, states=states, internals=OrderedDict(),\n            actions=actions, preprocessing=None, exploration=0.0, variable_noise=0.0,\n            l2_regularization=0.0\n        )\n\n    def tf_core_act(self, states, internals, auxiliaries):\n        if len(internals) > 0:\n            raise TensorforceError.unexpected()\n\n        actions = OrderedDict()\n        for name, spec in self.actions_spec.items():\n            batch_size = tf.shape(input=next(iter(states.values())))[0:1]\n            shape = tf.constant(value=spec[\'shape\'], dtype=tf.int32)\n            shape = tf.concat(values=(batch_size, shape), axis=0)\n            dtype = util.tf_dtype(dtype=spec[\'type\'])\n\n            if spec[\'type\'] == \'bool\':\n                actions[name] = tf.math.less(\n                    x=tf.random.uniform(shape=shape, dtype=util.tf_dtype(dtype=\'float\')),\n                    y=tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n                )\n\n            elif spec[\'type\'] == \'int\':\n                # (Same code as for Exploration)\n                int_dtype = util.tf_dtype(dtype=\'int\')\n                float_dtype = util.tf_dtype(dtype=\'float\')\n\n                # Action choices\n                choices = list(range(spec[\'num_values\']))\n                choices_tile = ((1,) + spec[\'shape\'] + (1,))\n                choices = np.tile(A=[choices], reps=choices_tile)\n                choices_shape = ((1,) + spec[\'shape\'] + (spec[\'num_values\'],))\n                choices = tf.constant(value=choices, dtype=int_dtype, shape=choices_shape)\n                ones = tf.ones(shape=(len(spec[\'shape\']) + 1,), dtype=int_dtype)\n                batch_size = tf.dtypes.cast(x=shape[0:1], dtype=int_dtype)\n                multiples = tf.concat(values=(batch_size, ones), axis=0)\n                choices = tf.tile(input=choices, multiples=multiples)\n\n                # Random unmasked action\n                mask = auxiliaries[name + \'_mask\']\n                num_values = tf.math.count_nonzero(input=mask, axis=-1, dtype=int_dtype)\n                action = tf.random.uniform(shape=shape, dtype=float_dtype)\n                action = tf.dtypes.cast(\n                    x=(action * tf.dtypes.cast(x=num_values, dtype=float_dtype)), dtype=int_dtype\n                )\n\n                # Correct for masked actions\n                choices = tf.boolean_mask(tensor=choices, mask=mask)\n                offset = tf.math.cumsum(x=num_values, axis=-1, exclusive=True)\n                actions[name] = tf.gather(params=choices, indices=(action + offset))\n\n            elif spec[\'type\'] == \'float\':\n                if \'min_value\' in spec:\n                    min_value = tf.constant(value=spec[\'min_value\'], dtype=dtype)\n                    max_value = tf.constant(value=spec[\'max_value\'], dtype=dtype)\n                    actions[name] = tf.random.uniform(\n                        shape=shape, minval=min_value, maxval=max_value, dtype=dtype\n                    )\n\n                else:\n                    actions[name] = tf.random.uniform(shape=shape, dtype=dtype)\n\n        return actions, OrderedDict()\n\n    def tf_core_observe(self, states, internals, auxiliaries, actions, terminal, reward):\n        return util.no_operation()\n'"
tensorforce/core/models/tensorforce.py,98,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import memory_modules, Module, optimizer_modules, parameter_modules\nfrom tensorforce.core.estimators import Estimator\nfrom tensorforce.core.models import Model\nfrom tensorforce.core.networks import Preprocessor\nfrom tensorforce.core.objectives import objective_modules\nfrom tensorforce.core.policies import policy_modules\n\n\nclass TensorforceModel(Model):\n\n    def __init__(\n        self,\n        # Model\n        name, device, parallel_interactions, buffer_observe, seed, execution, saver, summarizer,\n        config, states, actions, preprocessing, exploration, variable_noise,\n        l2_regularization,\n        # TensorforceModel\n        policy, memory, update, optimizer, objective, reward_estimation, baseline_policy,\n        baseline_optimizer, baseline_objective, entropy_regularization, max_episode_timesteps\n    ):\n        preprocessed_states = OrderedDict(states)\n        for state_name, state_spec in states.items():\n            if preprocessing is None:\n                layers = None\n            elif state_name in preprocessing:\n                layers = preprocessing[state_name]\n            elif state_spec[\'type\'] in preprocessing:\n                layers = preprocessing[state_spec[\'type\']]\n            else:\n                layers = None\n            if layers is not None:\n                preprocessed_states[state_name] = Preprocessor.output_spec(\n                    input_spec=state_spec, layers=layers\n                )\n\n        # Policy internals specification\n        policy_cls, first_arg, kwargs = Module.get_module_class_and_kwargs(\n            name=\'policy\', module=policy, modules=policy_modules, states_spec=preprocessed_states,\n            actions_spec=actions\n        )\n        if first_arg is None:\n            internals = policy_cls.internals_spec(name=\'policy\', **kwargs)\n        else:\n            internals = policy_cls.internals_spec(first_arg, name=\'policy\', **kwargs)\n        if any(internal.startswith(\'baseline-\') for internal in internals):\n            raise TensorforceError.value(\n                name=\'model\', argument=\'internals\', value=list(internals),\n                hint=\'starts with baseline-\'\n            )\n\n        # Baseline internals specification\n        if baseline_policy is None:\n            pass\n        else:\n            baseline_cls, first_arg, kwargs = Module.get_module_class_and_kwargs(\n                name=\'baseline\', module=baseline_policy, modules=policy_modules,\n                states_spec=preprocessed_states, actions_spec=actions\n            )\n            if first_arg is None:\n                baseline_internals = baseline_cls.internals_spec(name=\'baseline\', **kwargs)\n            else:\n                baseline_internals = baseline_cls.internals_spec(\n                    first_arg, name=\'baseline\', **kwargs\n                )\n            for internal, spec in baseline_internals.items():\n                if internal in internals:\n                    raise TensorforceError.collision(\n                        name=\'model\', value=\'internals\', group1=\'policy\', group2=\'baseline\'\n                    )\n                internals[internal] = spec\n\n        super().__init__(\n            # Model\n            name=name, device=device, parallel_interactions=parallel_interactions,\n            buffer_observe=buffer_observe, seed=seed, execution=execution, saver=saver,\n            summarizer=summarizer, config=config, states=states, internals=internals,\n            actions=actions, preprocessing=preprocessing, exploration=exploration,\n            variable_noise=variable_noise, l2_regularization=l2_regularization\n        )\n\n        # Policy\n        self.policy = self.add_module(\n            name=\'policy\', module=policy, modules=policy_modules, states_spec=self.states_spec,\n            actions_spec=self.actions_spec\n        )\n\n        # Update mode\n        if not all(key in (\'batch_size\', \'frequency\', \'start\', \'unit\') for key in update):\n            raise TensorforceError.value(\n                name=\'agent\', argument=\'update\', value=list(update),\n                hint=\'not from {batch_size,frequency,start,unit}\'\n            )\n        # update: unit\n        elif \'unit\' not in update:\n            raise TensorforceError.required(name=\'agent\', argument=\'update[unit]\')\n        elif update[\'unit\'] not in (\'timesteps\', \'episodes\'):\n            raise TensorforceError.value(\n                name=\'agent\', argument=\'update[unit]\', value=update[\'unit\'],\n                hint=\'not in {timesteps,episodes}\'\n            )\n        # update: batch_size\n        elif \'batch_size\' not in update:\n            raise TensorforceError.required(name=\'agent\', argument=\'update[batch_size]\')\n\n        self.update_unit = update[\'unit\']\n        self.update_batch_size = self.add_module(\n            name=\'update-batch-size\', module=update[\'batch_size\'], modules=parameter_modules,\n            is_trainable=False, dtype=\'long\', min_value=1\n        )\n        if \'frequency\' in update and update[\'frequency\'] == \'never\':\n            self.update_frequency = None\n        else:\n            self.update_frequency = self.add_module(\n                name=\'update-frequency\', module=update.get(\'frequency\', update[\'batch_size\']),\n                modules=parameter_modules, is_trainable=False, dtype=\'long\', min_value=1,\n                max_value=max(2, self.update_batch_size.max_value())\n            )\n            self.update_start = self.add_module(\n                name=\'update-start\', module=update.get(\'start\', 0), modules=parameter_modules,\n                is_trainable=False, dtype=\'long\', min_value=0\n            )\n\n        # Optimizer\n        self.optimizer = self.add_module(\n            name=\'optimizer\', module=optimizer, modules=optimizer_modules, is_trainable=False\n        )\n\n        # Objective\n        self.objective = self.add_module(\n            name=\'objective\', module=objective, modules=objective_modules, is_trainable=False\n        )\n\n        # Baseline optimization overview:\n        # Policy    Objective   Optimizer   Config\n        #   n         n           n           estimate_horizon=False\n        #   n         n           f           invalid!!!\n        #   n         n           y           invalid!!!\n        #   n         y           n           bl trainable, weighted 1.0\n        #   n         y           f           bl trainable, weighted\n        #   n         y           y           separate, use main policy\n        #   y         n           n           bl trainable, estimate_advantage=True, equal horizon\n        #   y         n           f           invalid!!!\n        #   y         n           y           separate, use main objective\n        #   y         y           n           bl trainable, weighted 1.0, equal horizon\n        #   y         y           f           bl trainable, weighted, equal horizon\n        #   y         y           y           separate\n\n        # Baseline objective\n        if baseline_objective is None:\n            self.baseline_objective = None\n        else:\n            self.baseline_objective = self.add_module(\n                name=\'baseline-objective\', module=baseline_objective, modules=objective_modules,\n                is_trainable=False, is_subscope=True\n            )\n\n        # Baseline optimizer\n        if baseline_optimizer is None:\n            self.baseline_optimizer = None\n            if self.baseline_objective is None:\n                self.baseline_loss_weight = None\n            else:\n                self.baseline_loss_weight = 1.0\n        elif isinstance(baseline_optimizer, float):\n            assert self.baseline_objective is not None\n            self.baseline_optimizer = None\n            self.baseline_loss_weight = baseline_optimizer\n        else:\n            assert self.baseline_objective is not None or baseline_policy is not None\n            self.baseline_optimizer = self.add_module(\n                name=\'baseline-optimizer\', module=baseline_optimizer, modules=optimizer_modules,\n                is_trainable=False, is_subscope=True\n            )\n            self.baseline_loss_weight = None\n\n        # Baseline\n        if (baseline_policy is not None or self.baseline_objective is not None) and \\\n                self.baseline_optimizer is None:\n            # since otherwise not part of training\n            assert self.baseline_objective is not None or \\\n                reward_estimation.get(\'estimate_advantage\', True)\n            is_trainable = True\n        else:\n            is_trainable = False\n        if baseline_policy is None:\n            self.baseline_policy = self.policy\n            self.separate_baseline_policy = False\n        else:\n            self.baseline_policy = self.add_module(\n                name=\'baseline\', module=baseline_policy, modules=policy_modules,\n                is_trainable=is_trainable, is_subscope=True, states_spec=self.states_spec,\n                actions_spec=self.actions_spec\n            )\n            self.separate_baseline_policy = True\n\n        # Estimator\n        if not all(key in (\n            \'discount\', \'estimate_actions\', \'estimate_advantage\', \'estimate_horizon\',\n            \'estimate_terminal\', \'horizon\'\n        ) for key in reward_estimation):\n            raise TensorforceError.value(\n                name=\'agent\', argument=\'reward_estimation\', value=reward_estimation,\n                hint=\'not from {discount,estimate_actions,estimate_advantage,estimate_horizon,\'\n                     \'estimate_terminal,horizon}\'\n            )\n        if not self.separate_baseline_policy and self.baseline_optimizer is None and \\\n                self.baseline_objective is None:\n            estimate_horizon = False\n        else:\n            estimate_horizon = \'late\'\n        if self.separate_baseline_policy and self.baseline_objective is None and \\\n                self.baseline_optimizer is None:\n            estimate_advantage = True\n        else:\n            estimate_advantage = False\n        self.estimator = self.add_module(\n            name=\'estimator\', module=Estimator, is_trainable=False, is_saved=False,\n            values_spec=self.values_spec, horizon=reward_estimation[\'horizon\'],\n            discount=reward_estimation.get(\'discount\', 1.0),\n            estimate_horizon=reward_estimation.get(\'estimate_horizon\', estimate_horizon),\n            estimate_actions=reward_estimation.get(\'estimate_actions\', False),\n            estimate_terminal=reward_estimation.get(\'estimate_terminal\', False),\n            estimate_advantage=reward_estimation.get(\'estimate_advantage\', estimate_advantage),\n            # capacity=reward_estimation[\'capacity\']\n            min_capacity=self.buffer_observe,\n            max_past_horizon=self.baseline_policy.max_past_horizon(is_optimization=False)\n        )\n\n        # Memory\n        if self.update_unit == \'timesteps\':\n            policy_horizon = self.policy.max_past_horizon(is_optimization=True)\n            baseline_horizon = self.baseline_policy.max_past_horizon(is_optimization=True) - \\\n                self.estimator.min_future_horizon()\n            min_capacity = self.update_batch_size.max_value() + 1 + \\\n                self.estimator.max_future_horizon() + max(policy_horizon, baseline_horizon)\n        elif self.update_unit == \'episodes\':\n            if max_episode_timesteps is None:\n                min_capacity = 0\n            else:\n                min_capacity = (self.update_batch_size.max_value() + 1) * max_episode_timesteps\n        else:\n            assert False\n\n        self.memory = self.add_module(\n            name=\'memory\', module=memory, modules=memory_modules, is_trainable=False,\n            values_spec=self.values_spec, min_capacity=min_capacity\n        )\n\n        # Entropy regularization\n        entropy_regularization = 0.0 if entropy_regularization is None else entropy_regularization\n        self.entropy_regularization = self.add_module(\n            name=\'entropy-regularization\', module=entropy_regularization,\n            modules=parameter_modules, is_trainable=False, dtype=\'float\', min_value=0.0\n        )\n\n        # Internals initialization\n        self.internals_init.update(self.policy.internals_init())\n        self.internals_init.update(self.baseline_policy.internals_init())\n        if any(internal_init is None for internal_init in self.internals_init.values()):\n            raise TensorforceError.required(name=\'model\', argument=\'internals_init\')\n\n        # Register global tensors\n        Module.register_tensor(name=\'update\', spec=dict(type=\'long\', shape=()), batched=False)\n        Module.register_tensor(\n            name=\'dependency_starts\', spec=dict(type=\'long\', shape=()), batched=True\n        )\n        Module.register_tensor(\n            name=\'dependency_lengths\', spec=dict(type=\'long\', shape=()), batched=True\n        )\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        # Actions\n        self.actions_input = OrderedDict()\n        for name, action_spec in self.actions_spec.items():\n            self.actions_input[name] = self.add_placeholder(\n                name=name, dtype=action_spec[\'type\'], shape=action_spec[\'shape\'], batched=True\n            )\n\n        # Last update\n        self.last_update = self.add_variable(\n            name=\'last-update\', dtype=\'long\', shape=(), initializer=-1, is_trainable=False\n        )\n\n    def api_experience(self):\n        # Inputs\n        states = OrderedDict(self.states_input)\n        internals = OrderedDict(self.internals_input)\n        auxiliaries = OrderedDict(self.auxiliaries_input)\n        actions = OrderedDict(self.actions_input)\n        terminal = self.terminal_input\n        reward = self.reward_input\n\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        true = tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n        batch_size = tf.shape(input=terminal)[:1]\n\n        # Assertions\n        assertions = list()\n        # terminal: type and shape\n        tf.debugging.assert_type(\n            tensor=terminal, tf_type=util.tf_dtype(dtype=\'long\'),\n            message=""Agent.experience: invalid type for terminal input.""\n        )\n        assertions.append(tf.debugging.assert_rank(\n            x=terminal, rank=1, message=""Agent.experience: invalid shape for terminal input.""\n        ))\n        # reward: type and shape\n        tf.debugging.assert_type(\n            tensor=reward, tf_type=util.tf_dtype(dtype=\'float\'),\n            message=""Agent.experience: invalid type for reward input.""\n        )\n        assertions.append(tf.debugging.assert_rank(\n            x=reward, rank=1, message=""Agent.experience: invalid shape for reward input.""\n        ))\n        # shape of terminal equals shape of reward\n        assertions.append(tf.debugging.assert_equal(\n            x=tf.shape(input=terminal), y=tf.shape(input=reward),\n            message=""Agent.experience: incompatible shapes of terminal and reward input.""\n        ))\n        # buffer index is zero\n        assertions.append(tf.debugging.assert_equal(\n            x=tf.math.reduce_sum(input_tensor=self.buffer_index, axis=0),\n            y=tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\')),\n            message=""Agent.experience: cannot be called mid-episode.""\n        ))\n        # at most one terminal\n        assertions.append(tf.debugging.assert_less_equal(\n            x=tf.math.count_nonzero(input=terminal, dtype=util.tf_dtype(dtype=\'long\')),\n            y=tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\')),\n            message=""Agent.experience: input contains more than one terminal.""\n        ))\n        # if terminal, last timestep in batch\n        assertions.append(tf.debugging.assert_equal(\n            x=tf.math.reduce_any(input_tensor=tf.math.greater(x=terminal, y=zero)),\n            y=tf.math.greater(x=terminal[-1], y=zero),\n            message=""Agent.experience: terminal is not the last input timestep.""\n        ))\n        # states: type and shape\n        for name, spec in self.states_spec.items():\n            spec = self.unprocessed_state_spec.get(name, spec)\n            tf.debugging.assert_type(\n                tensor=states[name], tf_type=util.tf_dtype(dtype=spec[\'type\']),\n                message=""Agent.experience: invalid type for {} state input."".format(name)\n            )\n            shape = tf.constant(value=spec[\'shape\'], dtype=util.tf_dtype(dtype=\'int\'))\n            assertions.append(\n                tf.debugging.assert_equal(\n                    x=tf.shape(input=states[name], out_type=util.tf_dtype(dtype=\'int\')),\n                    y=tf.concat(values=(batch_size, shape), axis=0),\n                    message=""Agent.experience: invalid shape for {} state input."".format(name)\n                )\n            )\n        # internals: type and shape\n        for name, spec in self.internals_spec.items():\n            tf.debugging.assert_type(\n                tensor=internals[name], tf_type=util.tf_dtype(dtype=spec[\'type\']),\n                message=""Agent.experience: invalid type for {} internal input."".format(name)\n            )\n            shape = tf.constant(value=spec[\'shape\'], dtype=util.tf_dtype(dtype=\'int\'))\n            assertions.append(\n                tf.debugging.assert_equal(\n                    x=tf.shape(input=internals[name], out_type=util.tf_dtype(dtype=\'int\')),\n                    y=tf.concat(values=(batch_size, shape), axis=0),\n                    message=""Agent.experience: invalid shape for {} internal input."".format(name)\n                )\n            )\n        # action_masks: type and shape\n        for name, spec in self.actions_spec.items():\n            if spec[\'type\'] == \'int\':\n                name = name + \'_mask\'\n                tf.debugging.assert_type(\n                    tensor=auxiliaries[name], tf_type=util.tf_dtype(dtype=\'bool\'),\n                    message=""Agent.experience: invalid type for {} action-mask input."".format(name)\n                )\n                shape = tf.constant(\n                    value=(spec[\'shape\'] + (spec[\'num_values\'],)), dtype=util.tf_dtype(dtype=\'int\')\n                )\n                assertions.append(\n                    tf.debugging.assert_equal(\n                        x=tf.shape(input=auxiliaries[name], out_type=util.tf_dtype(dtype=\'int\')),\n                        y=tf.concat(values=(batch_size, shape), axis=0),\n                        message=""Agent.experience: invalid shape for {} action-mask input."".format(\n                            name\n                        )\n                    )\n                )\n                assertions.append(\n                    tf.debugging.assert_equal(\n                        x=tf.reduce_all(\n                            input_tensor=tf.reduce_any(\n                                input_tensor=auxiliaries[name], axis=(len(spec[\'shape\']) + 1)\n                            ), axis=tuple(range(len(spec[\'shape\']) + 1))\n                        ),\n                        y=true, message=""Agent.experience: at least one action has to be valid ""\n                                        ""for {} action-mask input."".format(name)\n                    )\n                )\n        # actions: type and shape\n        for name, spec in self.actions_spec.items():\n            tf.debugging.assert_type(\n                tensor=actions[name], tf_type=util.tf_dtype(dtype=spec[\'type\']),\n                message=""Agent.experience: invalid type for {} action input."".format(name)\n            )\n            shape = tf.constant(value=spec[\'shape\'], dtype=util.tf_dtype(dtype=\'int\'))\n            assertions.append(\n                tf.debugging.assert_equal(\n                    x=tf.shape(input=actions[name], out_type=util.tf_dtype(dtype=\'int\')),\n                    y=tf.concat(values=(batch_size, shape), axis=0),\n                    message=""Agent.experience: invalid shape for {} action input."".format(name)\n                )\n            )\n\n        # Set global tensors\n        Module.update_tensors(\n            independent=tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\')),\n            deterministic=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')),\n            timestep=self.global_timestep, episode=self.global_episode, update=self.global_update\n        )\n\n        with tf.control_dependencies(control_inputs=assertions):\n            # Preprocessing states\n            if any(name in self.preprocessing for name in self.states_spec):\n                for name in self.states_spec:\n                    if name in self.preprocessing:\n                        states[name] = self.preprocessing[name].apply(x=states[name])\n\n            # Preprocessing reward\n            if \'reward\' in self.preprocessing:\n                reward = self.preprocessing[\'reward\'].apply(x=reward)\n\n            # Core experience: retrieve experience operation\n            experienced = self.core_experience(\n                states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n                terminal=terminal, reward=reward\n            )\n\n        with tf.control_dependencies(control_inputs=(experienced,)):\n            # Function-level identity operation for retrieval (plus enforce dependency)\n            timestep = util.identity_operation(\n                x=self.global_timestep, operation_name=\'timestep-output\'\n            )\n            episode = util.identity_operation(\n                x=self.global_episode, operation_name=\'episode-output\'\n            )\n            update = util.identity_operation(\n                x=self.global_update, operation_name=\'update-output\'\n            )\n\n        return timestep, episode, update\n\n    def api_update(self):\n        # Set global tensors\n        Module.update_tensors(\n            independent=tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\')),\n            deterministic=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\')),\n            timestep=self.global_timestep, episode=self.global_episode, update=self.global_update\n        )\n\n        # Core update: retrieve update operation\n        updated = self.core_update()\n\n        with tf.control_dependencies(control_inputs=(updated,)):\n            # Function-level identity operation for retrieval (plus enforce dependency)\n            timestep = util.identity_operation(\n                x=self.global_timestep, operation_name=\'timestep-output\'\n            )\n            episode = util.identity_operation(\n                x=self.global_episode, operation_name=\'episode-output\'\n            )\n            update = util.identity_operation(\n                x=self.global_update, operation_name=\'update-output\'\n            )\n\n        return timestep, episode, update\n\n    def tf_core_act(self, states, internals, auxiliaries):\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Dependency horizon\n        past_horizon = self.policy.past_horizon(is_optimization=False)\n        past_horizon = tf.math.maximum(\n            x=past_horizon, y=self.baseline_policy.past_horizon(is_optimization=False)\n        )\n\n        # TODO: handle arbitrary non-optimization horizons!\n        # assertion = tf.debugging.assert_equal(\n        #     x=past_horizon, y=zero,\n        #     message=""Temporary: policy and baseline cannot depend on previous states unless ""\n        #             ""optimization.""\n        # )\n        # with tf.control_dependencies(control_inputs=(assertion,)):\n        some_state = next(iter(states.values()))\n        if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n            batch_size = tf.shape(input=some_state, out_type=util.tf_dtype(dtype=\'long\'))[0]\n        else:\n            batch_size = tf.dtypes.cast(\n                x=tf.shape(input=some_state)[0], dtype=util.tf_dtype(dtype=\'long\')\n            )\n        starts = tf.range(start=batch_size, dtype=util.tf_dtype(dtype=\'long\'))\n        lengths = tf.ones(shape=(batch_size,), dtype=util.tf_dtype(dtype=\'long\'))\n        Module.update_tensors(dependency_starts=starts, dependency_lengths=lengths)\n\n        # Policy act\n        actions, next_internals = self.policy.act(\n            states=states, internals=internals, auxiliaries=auxiliaries, return_internals=True\n        )\n\n        if any(name not in next_internals for name in internals):\n            # Baseline policy act to retrieve next internals\n            _, baseline_internals = self.baseline_policy.act(\n                states=states, internals=internals, auxiliaries=auxiliaries, return_internals=True\n            )\n            assert all(name not in next_internals for name in baseline_internals)\n            next_internals.update(baseline_internals)\n\n        return actions, next_internals\n\n    def tf_core_observe(self, states, internals, auxiliaries, actions, terminal, reward):\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Experience\n        experienced = self.core_experience(\n            states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n            terminal=terminal, reward=reward\n        )\n\n        # If no periodic update\n        if self.update_frequency is None:\n            return experienced\n\n        # Periodic update\n        with tf.control_dependencies(control_inputs=(experienced,)):\n            batch_size = self.update_batch_size.value()\n            frequency = self.update_frequency.value()\n            start = self.update_start.value()\n\n            if self.update_unit == \'timesteps\':\n                # Timestep-based batch\n                policy_horizon = self.policy.past_horizon(is_optimization=True)\n                baseline_horizon = self.baseline_policy.past_horizon(is_optimization=True) - \\\n                    self.estimator.future_horizon()\n                past_horizon = tf.math.maximum(x=policy_horizon, y=baseline_horizon)\n                future_horizon = self.estimator.future_horizon()\n                start = tf.math.maximum(\n                    x=start, y=(frequency + past_horizon + future_horizon + one)\n                )\n                unit = Module.retrieve_tensor(name=\'timestep\')\n\n            elif self.update_unit == \'episodes\':\n                # Episode-based batch\n                start = tf.math.maximum(x=start, y=frequency)\n                unit = Module.retrieve_tensor(name=\'episode\')\n\n            unit = unit - start\n            is_frequency = tf.math.equal(x=tf.math.mod(x=unit, y=frequency), y=zero)\n            is_frequency = tf.math.logical_and(x=is_frequency, y=(unit > self.last_update))\n\n            def perform_update():\n                assignment = self.last_update.assign(value=unit, read_value=False)\n                with tf.control_dependencies(control_inputs=(assignment,)):\n                    return self.core_update()\n\n            is_updated = self.cond(\n                pred=is_frequency, true_fn=perform_update, false_fn=util.no_operation\n            )\n\n        return is_updated\n\n    def tf_core_experience(self, states, internals, auxiliaries, actions, terminal, reward):\n        zero = tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Enqueue experience for early reward estimation\n        any_overwritten, overwritten_values = self.estimator.enqueue(\n            baseline=self.baseline_policy, states=states, internals=internals,\n            auxiliaries=auxiliaries, actions=actions, terminal=terminal, reward=reward\n        )\n\n        # If terminal, store remaining values in memory\n\n        def true_fn():\n            reset_values = self.estimator.reset(baseline=self.baseline_policy)\n\n            new_overwritten_values = OrderedDict()\n            for name, value1, value2 in util.zip_items(overwritten_values, reset_values):\n                if util.is_nested(name=name):\n                    new_overwritten_values[name] = OrderedDict()\n                    for inner_name, value1, value2 in util.zip_items(value1, value2):\n                        new_overwritten_values[name][inner_name] = tf.concat(\n                            values=(value1, value2), axis=0\n                        )\n                else:\n                    new_overwritten_values[name] = tf.concat(values=(value1, value2), axis=0)\n            return new_overwritten_values\n\n        def false_fn():\n            return overwritten_values\n\n        with tf.control_dependencies(control_inputs=util.flatten(xs=overwritten_values)):\n            values = self.cond(pred=(terminal[-1] > zero), true_fn=true_fn, false_fn=false_fn)\n\n        # If any, store overwritten values\n        def store():\n            return self.memory.enqueue(**values)\n\n        terminal = values[\'terminal\']\n        if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n            num_values = tf.shape(input=terminal, out_type=util.tf_dtype(dtype=\'long\'))[0]\n        else:\n            num_values = tf.dtypes.cast(\n                x=tf.shape(input=terminal)[0], dtype=util.tf_dtype(dtype=\'long\')\n            )\n\n        stored = self.cond(pred=(num_values > zero), true_fn=store, false_fn=util.no_operation)\n\n        return stored\n\n    def tf_core_update(self):\n        Module.update_tensor(name=\'update\', tensor=self.global_update)\n\n        true = tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n\n        # Retrieve batch\n        batch_size = self.update_batch_size.value()\n        if self.update_unit == \'timesteps\':\n            # Timestep-based batch\n            # Dependency horizon\n            past_horizon = self.policy.past_horizon(is_optimization=True)\n            past_horizon = tf.math.maximum(\n                x=past_horizon, y=self.baseline_policy.past_horizon(is_optimization=True)\n            )\n            future_horizon = self.estimator.future_horizon()\n            indices = self.memory.retrieve_timesteps(\n                n=batch_size, past_horizon=past_horizon, future_horizon=future_horizon\n            )\n        elif self.update_unit == \'episodes\':\n            # Episode-based batch\n            indices = self.memory.retrieve_episodes(n=batch_size)\n\n        # Optimization\n        optimized = self.optimize(indices=indices)\n\n        # Increment update\n        with tf.control_dependencies(control_inputs=(optimized,)):\n            assignment = self.global_update.assign_add(delta=one, read_value=False)\n\n        with tf.control_dependencies(control_inputs=(assignment,)):\n            return util.identity_operation(x=true)\n\n    def tf_optimize(self, indices):\n        # Baseline optimization\n        if self.baseline_optimizer is not None:\n            optimized = self.optimize_baseline(indices=indices)\n            dependencies = (optimized,)\n        else:\n            dependencies = (indices,)\n\n        # Reward estimation\n        with tf.control_dependencies(control_inputs=dependencies):\n            reward = self.memory.retrieve(indices=indices, values=\'reward\')\n            reward = self.estimator.complete(\n                baseline=self.baseline_policy, memory=self.memory, indices=indices, reward=reward\n            )\n            reward = self.add_summary(\n                label=(\'empirical-reward\', \'rewards\'), name=\'empirical-reward\', tensor=reward\n            )\n            is_baseline_optimized = self.separate_baseline_policy and \\\n                self.baseline_optimizer is None and self.baseline_objective is None\n            reward = self.estimator.estimate(\n                baseline=self.baseline_policy, memory=self.memory, indices=indices, reward=reward,\n                is_baseline_optimized=is_baseline_optimized\n            )\n            reward = self.add_summary(\n                label=(\'estimated-reward\', \'rewards\'), name=\'estimated-reward\', tensor=reward\n            )\n\n        # Stop gradients of estimated rewards if separate baseline optimization\n        if not is_baseline_optimized:\n            reward = tf.stop_gradient(input=reward)\n\n        # Retrieve states, internals and actions\n        past_horizon = self.policy.past_horizon(is_optimization=True)\n        if self.separate_baseline_policy and self.baseline_optimizer is None:\n            assertion = tf.debugging.assert_equal(\n                x=past_horizon,\n                y=self.baseline_policy.past_horizon(is_optimization=True),\n                message=""Policy and baseline depend on a different number of previous states.""\n            )\n        else:\n            assertion = past_horizon\n\n        with tf.control_dependencies(control_inputs=(assertion,)):\n            # horizon change: see timestep-based batch sampling\n            starts, lengths, states, internals = self.memory.predecessors(\n                indices=indices, horizon=past_horizon, sequence_values=\'states\',\n                initial_values=\'internals\'\n            )\n            Module.update_tensors(dependency_starts=starts, dependency_lengths=lengths)\n            auxiliaries, actions = self.memory.retrieve(\n                indices=indices, values=(\'auxiliaries\', \'actions\')\n            )\n\n        # Optimizer arguments\n        independent = Module.update_tensor(\n            name=\'independent\', tensor=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n        )\n\n        variables = self.get_variables(only_trainable=True)\n\n        arguments = dict(\n            states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n            reward=reward\n        )\n\n        fn_loss = self.total_loss\n\n        def fn_kl_divergence(states, internals, auxiliaries, actions, reward, other=None):\n            kl_divergence = self.policy.kl_divergence(\n                states=states, internals=internals, auxiliaries=auxiliaries, other=other\n            )\n            if self.baseline_optimizer is None and self.baseline_objective is not None:\n                kl_divergence += self.baseline_policy.kl_divergence(\n                    states=states, internals=internals, auxiliaries=auxiliaries, other=other\n                )\n            return kl_divergence\n\n        if self.global_model is None:\n            global_variables = None\n        else:\n            global_variables = self.global_model.get_variables(only_trainable=True)\n\n        kwargs = self.objective.optimizer_arguments(\n            policy=self.policy, baseline=self.baseline_policy\n        )\n        if self.baseline_optimizer is None and self.baseline_objective is not None:\n            util.deep_disjoint_update(\n                target=kwargs,\n                source=self.baseline_objective.optimizer_arguments(policy=self.baseline_policy)\n            )\n\n        dependencies = util.flatten(xs=arguments)\n\n        # KL divergence before\n        if self.is_summary_logged(\n            label=(\'kl-divergence\', \'action-kl-divergences\', \'kl-divergences\')\n        ):\n            with tf.control_dependencies(control_inputs=dependencies):\n                kldiv_reference = self.policy.kldiv_reference(\n                    states=states, internals=internals, auxiliaries=auxiliaries\n                )\n                dependencies = util.flatten(xs=kldiv_reference)\n\n        # Optimization\n        with tf.control_dependencies(control_inputs=dependencies):\n            optimized = self.optimizer.minimize(\n                variables=variables, arguments=arguments, fn_loss=fn_loss,\n                fn_kl_divergence=fn_kl_divergence, global_variables=global_variables, **kwargs\n            )\n\n        with tf.control_dependencies(control_inputs=(optimized,)):\n            # Loss summaries\n            if self.is_summary_logged(label=(\'loss\', \'objective-loss\', \'losses\')):\n                objective_loss = self.objective.loss_per_instance(policy=self.policy, **arguments)\n                objective_loss = tf.math.reduce_mean(input_tensor=objective_loss, axis=0)\n            if self.is_summary_logged(label=(\'objective-loss\', \'losses\')):\n                optimized = self.add_summary(\n                    label=(\'objective-loss\', \'losses\'), name=\'objective-loss\',\n                    tensor=objective_loss, pass_tensors=optimized\n                )\n            if self.is_summary_logged(label=(\'loss\', \'regularization-loss\', \'losses\')):\n                regularization_loss = self.regularize(\n                    states=states, internals=internals, auxiliaries=auxiliaries\n                )\n            if self.is_summary_logged(label=(\'regularization-loss\', \'losses\')):\n                optimized = self.add_summary(\n                    label=(\'regularization-loss\', \'losses\'), name=\'regularization-loss\',\n                    tensor=regularization_loss, pass_tensors=optimized\n                )\n            if self.is_summary_logged(label=(\'loss\', \'losses\')):\n                loss = objective_loss + regularization_loss\n            if self.baseline_optimizer is None and self.baseline_objective is not None:\n                if self.is_summary_logged(label=(\'loss\', \'baseline-objective-loss\', \'losses\')):\n                    if self.baseline_objective is None:\n                        baseline_objective_loss = self.objective.loss_per_instance(\n                            policy=self.baseline_policy, **arguments\n                        )\n                    else:\n                        baseline_objective_loss = self.baseline_objective.loss_per_instance(\n                            policy=self.baseline_policy, **arguments\n                        )\n                    baseline_objective_loss = tf.math.reduce_mean(\n                        input_tensor=baseline_objective_loss, axis=0\n                    )\n                if self.is_summary_logged(label=(\'baseline-objective-loss\', \'losses\')):\n                    optimized = self.add_summary(\n                        label=(\'baseline-objective-loss\', \'losses\'),\n                        name=\'baseline-objective-loss\', tensor=baseline_objective_loss,\n                        pass_tensors=optimized\n                    )\n                if self.is_summary_logged(\n                    label=(\'loss\', \'baseline-regularization-loss\', \'losses\')\n                ):\n                    baseline_regularization_loss = self.baseline_policy.regularize()\n                if self.is_summary_logged(label=(\'baseline-regularization-loss\', \'losses\')):\n                    optimized = self.add_summary(\n                        label=(\'baseline-regularization-loss\', \'losses\'),\n                        name=\'baseline-regularization-loss\', tensor=baseline_regularization_loss,\n                        pass_tensors=optimized\n                    )\n                if self.is_summary_logged(label=(\'loss\', \'baseline-loss\', \'losses\')):\n                    baseline_loss = baseline_objective_loss + baseline_regularization_loss\n                if self.is_summary_logged(label=(\'baseline-loss\', \'losses\')):\n                    optimized = self.add_summary(\n                        label=(\'baseline-loss\', \'losses\'), name=\'baseline-loss\',\n                        tensor=baseline_loss, pass_tensors=optimized\n                    )\n                if self.is_summary_logged(label=(\'loss\', \'losses\')):\n                    loss += self.baseline_loss_weight * baseline_loss\n            if self.is_summary_logged(label=(\'loss\', \'losses\')):\n                optimized = self.add_summary(\n                    label=(\'loss\', \'losses\'), name=\'loss\', tensor=loss, pass_tensors=optimized\n                )\n\n            # Entropy summaries\n            if self.is_summary_logged(label=(\'entropy\', \'action-entropies\', \'entropies\')):\n                entropies = self.policy.entropy(\n                    states=states, internals=internals, auxiliaries=auxiliaries,\n                    include_per_action=(len(self.actions_spec) > 1)\n                )\n            if self.is_summary_logged(label=(\'entropy\', \'entropies\')):\n                if len(self.actions_spec) == 1:\n                    optimized = self.add_summary(\n                        label=(\'entropy\', \'entropies\'), name=\'entropy\', tensor=entropies,\n                        pass_tensors=optimized\n                    )\n                else:\n                    optimized = self.add_summary(\n                        label=(\'entropy\', \'entropies\'), name=\'entropy\', tensor=entropies[\'*\'],\n                        pass_tensors=optimized\n                    )\n            if len(self.actions_spec) > 1 and \\\n                    self.is_summary_logged(label=(\'action-entropies\', \'entropies\')):\n                for name in self.actions_spec:\n                    optimized = self.add_summary(\n                        label=(\'action-entropies\', \'entropies\'), name=(name + \'-entropy\'),\n                        tensor=entropies[name], pass_tensors=optimized\n                    )\n\n            # KL divergence summaries\n            if self.is_summary_logged(\n                label=(\'kl-divergence\', \'action-kl-divergences\', \'kl-divergences\')\n            ):\n                kl_divergences = self.policy.kl_divergence(\n                    states=states, internals=internals, auxiliaries=auxiliaries,\n                    other=kldiv_reference, include_per_action=(len(self.actions_spec) > 1)\n                )\n            if self.is_summary_logged(label=(\'kl-divergence\', \'kl-divergences\')):\n                if len(self.actions_spec) == 1:\n                    optimized = self.add_summary(\n                        label=(\'kl-divergence\', \'kl-divergences\'), name=\'kl-divergence\',\n                        tensor=kl_divergences, pass_tensors=optimized\n                    )\n                else:\n                    optimized = self.add_summary(\n                        label=(\'kl-divergence\', \'kl-divergences\'), name=\'kl-divergence\',\n                        tensor=kl_divergences[\'*\'], pass_tensors=optimized\n                    )\n            if len(self.actions_spec) > 1 and \\\n                    self.is_summary_logged(label=(\'action-kl-divergences\', \'kl-divergences\')):\n                for name in self.actions_spec:\n                    optimized = self.add_summary(\n                        label=(\'action-kl-divergences\', \'kl-divergences\'),\n                        name=(name + \'-kl-divergence\'), tensor=kl_divergences[name],\n                        pass_tensors=optimized\n                    )\n\n        Module.update_tensor(name=\'independent\', tensor=independent)\n\n        return optimized\n\n    def tf_total_loss(self, states, internals, auxiliaries, actions, reward, **kwargs):\n        # Loss per instance\n        loss = self.objective.loss_per_instance(\n            policy=self.policy, states=states, internals=internals, auxiliaries=auxiliaries,\n            actions=actions, reward=reward, **kwargs\n        )\n\n        # Objective loss\n        loss = tf.math.reduce_mean(input_tensor=loss, axis=0)\n\n        # Regularization losses\n        loss += self.regularize(\n            states=states, internals=internals, auxiliaries=auxiliaries\n        )\n\n        # Baseline loss\n        if self.baseline_optimizer is None and self.baseline_objective is not None:\n            loss += self.baseline_loss_weight * self.baseline_loss(\n                states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n                reward=reward\n            )\n        else:\n            assert self.baseline_loss_weight is None\n\n        return loss\n\n    def tf_regularize(self, states, internals, auxiliaries):\n        regularization_loss = super().tf_regularize(\n            states=states, internals=internals, auxiliaries=auxiliaries\n        )\n\n        # Entropy regularization\n        zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n        entropy_regularization = self.entropy_regularization.value()\n\n        def no_entropy_regularization():\n            return zero\n\n        def apply_entropy_regularization():\n            entropy = self.policy.entropy(\n                states=states, internals=internals, auxiliaries=auxiliaries\n            )\n            entropy = tf.math.reduce_mean(input_tensor=entropy, axis=0)\n            return -entropy_regularization * entropy\n\n        skip_entropy_regularization = tf.math.equal(x=entropy_regularization, y=zero)\n        regularization_loss += self.cond(\n            pred=skip_entropy_regularization, true_fn=no_entropy_regularization,\n            false_fn=apply_entropy_regularization\n        )\n\n        return regularization_loss\n\n    def tf_optimize_baseline(self, indices):\n        # Retrieve states, internals, actions and reward\n        past_horizon = self.baseline_policy.past_horizon(is_optimization=True)\n        # horizon change: see timestep-based batch sampling\n        starts, lengths, states, internals = self.memory.predecessors(\n            indices=indices, horizon=past_horizon, sequence_values=\'states\',\n            initial_values=\'internals\'\n        )\n        Module.update_tensors(dependency_starts=starts, dependency_lengths=lengths)\n        auxiliaries, actions, reward = self.memory.retrieve(\n            indices=indices, values=(\'auxiliaries\', \'actions\', \'reward\')\n        )\n\n        # Reward estimation (separate from main policy, so updated baseline is used there)\n        reward = self.memory.retrieve(indices=indices, values=\'reward\')\n        reward = self.estimator.complete(\n            baseline=self.baseline_policy, memory=self.memory, indices=indices, reward=reward\n        )\n\n        # Optimizer arguments\n        independent = Module.update_tensor(\n            name=\'independent\', tensor=tf.constant(value=True, dtype=util.tf_dtype(dtype=\'bool\'))\n        )\n\n        variables = self.baseline_policy.get_variables(only_trainable=True)\n\n        arguments = dict(\n            states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n            reward=reward\n        )\n\n        fn_loss = self.baseline_loss\n\n        def fn_kl_divergence(states, internals, auxiliaries, actions, reward, other=None):\n            return self.baseline_policy.kl_divergence(\n                states=states, internals=internals, auxiliaries=auxiliaries, other=other\n            )\n\n        source_variables = self.policy.get_variables(only_trainable=True)\n\n        if self.global_model is None:\n            global_variables = None\n        else:\n            global_variables = self.global_model.baseline_policy.get_variables(only_trainable=True)\n\n        if self.baseline_objective is None:\n            kwargs = self.objective.optimizer_arguments(policy=self.baseline_policy)\n        else:\n            kwargs = self.baseline_objective.optimizer_arguments(policy=self.baseline_policy)\n\n        # Optimization\n        optimized = self.baseline_optimizer.minimize(\n            variables=variables, arguments=arguments, fn_loss=fn_loss,\n            fn_kl_divergence=fn_kl_divergence, source_variables=source_variables,\n            global_variables=global_variables, **kwargs\n        )\n\n        with tf.control_dependencies(control_inputs=(optimized,)):\n            # Loss summaries\n            if self.is_summary_logged(\n                label=(\'baseline-loss\', \'baseline-objective-loss\', \'losses\')\n            ):\n                if self.baseline_objective is None:\n                    objective_loss = self.objective.loss_per_instance(\n                        policy=self.baseline_policy, **arguments\n                    )\n                else:\n                    objective_loss = self.baseline_objective.loss_per_instance(\n                        policy=self.baseline_policy, **arguments\n                    )\n                objective_loss = tf.math.reduce_mean(input_tensor=objective_loss, axis=0)\n            if self.is_summary_logged(label=(\'baseline-objective-loss\', \'losses\')):\n                optimized = self.add_summary(\n                    label=(\'baseline-objective-loss\', \'losses\'), name=\'baseline-objective-loss\',\n                    tensor=objective_loss, pass_tensors=optimized\n                )\n            if self.is_summary_logged(\n                label=(\'baseline-loss\', \'baseline-regularization-loss\', \'losses\')\n            ):\n                regularization_loss = self.baseline_policy.regularize()\n            if self.is_summary_logged(label=(\'baseline-regularization-loss\', \'losses\')):\n                optimized = self.add_summary(\n                    label=(\'baseline-regularization-loss\', \'losses\'),\n                    name=\'baseline-regularization-loss\', tensor=regularization_loss,\n                    pass_tensors=optimized\n                )\n            if self.is_summary_logged(label=(\'baseline-loss\', \'losses\')):\n                loss = objective_loss + regularization_loss\n                optimized = self.add_summary(\n                    label=(\'baseline-loss\', \'losses\'), name=\'baseline-loss\', tensor=loss,\n                    pass_tensors=optimized\n                )\n\n        independent = Module.update_tensor(name=\'independent\', tensor=independent)\n\n        return optimized\n\n    def tf_baseline_loss(self, states, internals, auxiliaries, actions, reward, **kwargs):\n        # Loss per instance\n        if self.baseline_objective is None:\n            loss = self.objective.loss_per_instance(\n                policy=self.baseline_policy, states=states, internals=internals,\n                auxiliaries=auxiliaries, actions=actions, reward=reward, **kwargs\n            )\n        else:\n            loss = self.baseline_objective.loss_per_instance(\n                policy=self.baseline_policy, states=states, internals=internals,\n                auxiliaries=auxiliaries, actions=actions, reward=reward, **kwargs\n            )\n\n        # Objective loss\n        loss = tf.math.reduce_mean(input_tensor=loss, axis=0)\n\n        # Regularization losses\n        loss += self.baseline_policy.regularize()\n\n        return loss\n'"
tensorforce/core/networks/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.networks.network import Network, LayerbasedNetwork\n\nfrom tensorforce.core.networks.auto import AutoNetwork\nfrom tensorforce.core.networks.layered import LayeredNetwork\nfrom tensorforce.core.networks.preprocessor import Preprocessor\n\n\nnetwork_modules = dict(\n    auto=AutoNetwork, custom=LayeredNetwork, default=LayeredNetwork, layered=LayeredNetwork\n)\n\n\n__all__ = [\n    \'AutoNetwork\', \'LayerbasedNetwork\', \'LayeredNetwork\', \'Network\', \'network_modules\',\n    \'Preprocessor\'\n]\n'"
tensorforce/core/networks/auto.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce import TensorforceError\nfrom tensorforce.core.layers import InternalLstm\nfrom tensorforce.core.networks import LayerbasedNetwork\n\n\nclass AutoNetwork(LayerbasedNetwork):\n    """"""\n    Network which is automatically configured based on its input tensors, offering high-level\n    customization (specification key: `auto`).\n\n    Args:\n        name (string): Network name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        inputs_spec (specification): Input tensors specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        size (int > 0): Layer size, before concatenation if multiple states\n            (<span style=""color:#00C000""><b>default</b></span>: 64).\n        depth (int > 0): Number of layers per state, before concatenation if multiple states\n            (<span style=""color:#00C000""><b>default</b></span>: 2).\n        final_size (int > 0): Layer size after concatenation if multiple states\n            (<span style=""color:#00C000""><b>default</b></span>: layer size).\n        final_depth (int > 0): Number of layers after concatenation if multiple states\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        internal_rnn (false | parameter, long >= 0): Whether to add an internal state LSTM cell\n            as last layer, and if so, horizon of the LSTM for truncated backpropagation through time\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, inputs_spec, size=64, depth=2, final_size=None, final_depth=1,\n        internal_rnn=False, device=None, summary_labels=None, l2_regularization=None\n    ):\n        # Some defaults require change in internals_spec\n        super().__init__(\n            name=name, inputs_spec=inputs_spec, device=device, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        self.size = size\n        self.depth = depth\n        self.final_size = size if final_size is None else final_size\n        self.final_depth = final_depth\n        self.internal_rnn = internal_rnn\n\n        # State-specific layers\n        self.state_specific_layers = OrderedDict()\n        # prefix = self.name + \'-\'\n        prefix = \'\'\n        for name, spec in inputs_spec.items():\n            layers = list()\n\n            # Retrieve state\n            layers.append(self.add_module(\n                name=(prefix + name + \'-retrieve\'), module=\'retrieve\', tensors=name\n            ))\n\n            # Embed bool and int states\n            if spec[\'type\'] in (\'bool\', \'int\'):\n                layers.append(self.add_module(\n                    name=(prefix + name + \'-embedding\'), module=\'embedding\', size=self.size\n                ))\n                embedding = 1\n            else:\n                embedding = 0\n\n            # Shape-specific layer type\n            if len(spec[\'shape\']) == 1 - embedding:\n                layer = \'dense\'\n            elif len(spec[\'shape\']) == 2 - embedding:\n                layer = \'conv1d\'\n            elif len(spec[\'shape\']) == 3 - embedding:\n                layer = \'conv2d\'\n            elif len(spec[\'shape\']) == 0:\n                layers.append(self.add_module(name=(prefix + name + \'-flatten\'), module=\'flatten\'))\n                layer = \'dense\'\n            else:\n                raise TensorforceError.unexpected()\n\n            # Repeat layer according to depth (one less if embedded)\n            for n in range(self.depth - embedding):\n                layers.append(self.add_module(\n                    name=(prefix + name + \'-\' + layer + str(n)), module=layer, size=self.size\n                ))\n\n            # Max pool if rank greater than one\n            if len(spec[\'shape\']) > 1 - embedding:\n                layers.append(self.add_module(\n                    name=(prefix + name + \'-pooling\'), module=\'pooling\', reduction=\'max\'\n                ))\n\n            # Register state-specific embedding\n            layers.append(self.add_module(\n                name=(prefix + name + \'-register\'), module=\'register\',\n                tensor=\'{}-{}-embedding\'.format(self.name, name)\n            ))\n\n            self.state_specific_layers[name] = layers\n\n        # Final combined layers\n        self.final_layers = list()\n\n        # Retrieve state-specific embeddings\n        self.final_layers.append(self.add_module(\n            name=(prefix + \'retrieve\'), module=\'retrieve\',\n            tensors=tuple(\'{}-{}-embedding\'.format(self.name, name) for name in inputs_spec),\n            aggregation=\'concat\'\n        ))\n\n        # Repeat layer according to depth\n        if len(inputs_spec) > 1:\n            for n in range(self.final_depth):\n                self.final_layers.append(self.add_module(\n                    name=(prefix + \'dense\' + str(n)), module=\'dense\', size=self.final_size\n                ))\n\n        # Internal Rnn\n        if self.internal_rnn is False:\n            self.internal_rnn = None\n        else:\n            self.internal_rnn = self.add_module(\n                name=(prefix + \'internal_lstm\'), module=\'internal_lstm\', size=self.final_size,\n                length=self.internal_rnn\n            )\n\n    @classmethod\n    def internals_spec(\n        cls, network=None, name=None, size=None, final_size=None, internal_rnn=None, **kwargs\n    ):\n        internals_spec = OrderedDict()\n\n        if network is None:\n            assert name is not None\n            if size is None:\n                size = 64\n            if internal_rnn is None:\n                internal_rnn = False\n\n            if internal_rnn > 0:\n                final_size = size if final_size is None else final_size\n                for internal_name, spec in InternalLstm.internals_spec(size=final_size).items():\n                    internals_spec[name + \'-\' + internal_name] = spec\n\n        else:\n            assert name is None and size is None and final_size is None and internal_rnn is None\n\n            if network.internal_rnn is not None:\n                for internal_name, spec in network.internal_rnn.__class__.internals_spec(\n                    layer=network.internal_rnn\n                ).items():\n                    internals_spec[network.name + \'-\' + internal_name] = spec\n\n        return internals_spec\n\n    def internals_init(self):\n        internals_init = OrderedDict()\n\n        if self.internal_rnn is not None:\n            for name, internal_init in self.internal_rnn.internals_init().items():\n                internals_init[self.name + \'-\' + name] = internal_init\n\n        return internals_init\n\n    def tf_apply(self, x, internals, return_internals=False):\n        super().tf_apply(x=x, internals=internals, return_internals=return_internals)\n\n        # State-specific layers\n        for name, layers in self.state_specific_layers.items():\n            tensor = x[name]\n            for layer in layers:\n                tensor = layer.apply(x=tensor)\n\n        # Final combined layers\n        for layer in self.final_layers:\n            tensor = layer.apply(x=tensor)\n\n        # Internal Rnn\n        next_internals = OrderedDict()\n        if self.internal_rnn is not None:\n            internals = {\n                name: internals[self.name + \'-\' + name]\n                for name in self.internal_rnn.__class__.internals_spec(layer=self.internal_rnn)\n            }\n            assert len(internals) > 0\n            tensor, internals = self.internal_rnn.apply(x=tensor, initial=internals)\n            for name, internal in internals.items():\n                next_internals[self.name + \'-\' + name] = internal\n\n        if return_internals:\n            return tensor, next_internals\n        else:\n            return tensor\n'"
tensorforce/core/networks/layered.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import Counter, OrderedDict\n\nfrom tensorforce import TensorforceError\nfrom tensorforce.core import Module\nfrom tensorforce.core.layers import layer_modules, StatefulLayer\nfrom tensorforce.core.networks import LayerbasedNetwork\n\n\nclass LayeredNetwork(LayerbasedNetwork):\n    """"""\n    Network consisting of Tensorforce layers, which can be specified as either a list of layer\n    specifications in the case of a standard sequential layer-stack architecture, or as a list of\n    list of layer specifications in the case of a more complex architecture consisting of multiple\n    sequential layer-stacks (specification key: `custom` or `layered`).\n\n    Args:\n        name (string): Network name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        layers (iter[specification] | iter[iter[specification]]): Layers configuration, see\n            [layers](../modules/layers.html)\n            (<span style=""color:#C00000""><b>required</b></span>).\n        inputs_spec (specification): Input tensors specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    # (requires layers as first argument)\n    def __init__(\n        self, name, layers, inputs_spec, device=None, summary_labels=None, l2_regularization=None\n    ):\n        super().__init__(\n            name=name, inputs_spec=inputs_spec, device=device, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        self.layers_spec = layers\n\n        self.parse_layers_spec(layers_spec=self.layers_spec, layer_counter=Counter())\n\n    def parse_layers_spec(self, layers_spec, layer_counter):\n        if isinstance(layers_spec, list):\n            for spec in layers_spec:\n                self.parse_layers_spec(layers_spec=spec, layer_counter=layer_counter)\n\n        else:\n            if \'name\' in layers_spec:\n                layers_spec = dict(layers_spec)\n                layer_name = layers_spec.pop(\'name\')\n            else:\n                if isinstance(layers_spec.get(\'type\'), str):\n                    layer_type = layers_spec[\'type\']\n                else:\n                    layer_type = \'layer\'\n                layer_name = layer_type + str(layer_counter[layer_type])\n                layer_counter[layer_type] += 1\n\n            # layer_name = self.name + \'-\' + layer_name\n            self.add_module(name=layer_name, module=layers_spec)\n\n    # (requires layers as first argument)\n    @classmethod\n    def internals_spec(cls, layers=None, network=None, name=None, **kwargs):\n        internals_spec = super().internals_spec(network=network)\n\n        if network is None:\n            assert layers is not None and name is not None\n\n            for internal_name, spec in cls.internals_from_layers_spec(\n                layers_spec=layers, layer_counter=Counter()\n            ):\n                internal_name = name + \'-\' + internal_name\n                if internal_name in internals_spec:\n                    raise TensorforceError.unexpected()\n                internals_spec[internal_name] = spec\n\n        else:\n            assert layers is None and name is None\n\n        return internals_spec\n\n    @classmethod\n    def internals_from_layers_spec(cls, layers_spec, layer_counter):\n        if isinstance(layers_spec, list):\n            for spec in layers_spec:\n                yield from cls.internals_from_layers_spec(\n                    layers_spec=spec, layer_counter=layer_counter\n                )\n\n        else:\n            if \'name\' in layers_spec:\n                layers_spec = dict(layers_spec)\n                layer_name = layers_spec.pop(\'name\')\n            else:\n                if isinstance(layers_spec.get(\'type\'), str):\n                    layer_type = layers_spec[\'type\']\n                else:\n                    layer_type = \'layer\'\n                layer_name = layer_type + str(layer_counter[layer_type])\n                layer_counter[layer_type] += 1\n\n            layer_cls, first_arg, kwargs = Module.get_module_class_and_kwargs(\n                name=layer_name, module=layers_spec, modules=layer_modules\n            )\n            if issubclass(layer_cls, StatefulLayer):\n                if first_arg is None:\n                    internals_spec = layer_cls.internals_spec(**kwargs)\n                else:\n                    internals_spec = layer_cls.internals_spec(first_arg, **kwargs)\n                for name, spec in internals_spec.items():\n                    name = \'{}-{}\'.format(layer_name, name)\n                    yield name, spec\n\n    def tf_apply(self, x, internals, return_internals=False):\n        super().tf_apply(x=x, internals=internals, return_internals=return_internals)\n\n        if isinstance(x, dict):\n            x = x[next(iter(x))]\n\n        next_internals = OrderedDict()\n        for layer in self.modules.values():\n            if isinstance(layer, StatefulLayer):\n                layer_internals = {\n                    name: internals[\'{}-{}-{}\'.format(self.name, layer.name, name)]\n                    for name in layer.__class__.internals_spec(layer=layer)\n                }\n                assert len(layer_internals) > 0\n                x, layer_internals = layer.apply(x=x, initial=layer_internals)\n                for name, internal in layer_internals.items():\n                    next_internals[\'{}-{}-{}\'.format(self.name, layer.name, name)] = internal\n\n            else:\n                x = layer.apply(x=x)\n\n        if return_internals:\n            return x, next_internals\n        else:\n            return x\n'"
tensorforce/core/networks/network.py,2,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import Module\nfrom tensorforce.core.layers import Layer, layer_modules, StatefulLayer, TemporalLayer\nfrom tensorforce.core.parameters import Parameter\n\n\nclass Network(Module):\n    """"""\n    Base class for neural networks.\n\n    Args:\n        name (string): Network name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        inputs_spec (specification): Input tensors specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, inputs_spec, device=None, summary_labels=None, l2_regularization=None\n    ):\n        super().__init__(\n            name=name, device=device, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        self.inputs_spec = inputs_spec\n\n    def get_output_spec(self):\n        raise NotImplementedError\n\n    @classmethod\n    def internals_spec(cls, network=None, **kwargs):\n        return OrderedDict()\n\n    def internals_init(self):\n        return OrderedDict()\n\n    def max_past_horizon(self, is_optimization=False):\n        raise NotImplementedError\n\n    def tf_past_horizon(self, is_optimization=False):\n        raise NotImplementedError\n\n    def tf_apply(self, x, internals, return_internals=False):\n        Module.update_tensors(**x)\n\n    def create_tf_function(self, name, tf_function):\n        if tf_function.__name__ != \'tf_apply\':\n            return super().create_tf_function(name=name, tf_function=tf_function)\n\n        def validated_tf_function(x, internals, return_internals=False):\n            if util.is_atomic_values_spec(values_spec=self.inputs_spec):\n                if not util.is_consistent_with_value_spec(value_spec=self.inputs_spec, x=x):\n                    raise TensorforceError(""Invalid input arguments for tf_apply."")\n            else:\n                if not all(\n                    util.is_consistent_with_value_spec(value_spec=spec, x=x[name])\n                    for name, spec in self.inputs_spec.items()\n                ):\n                    raise TensorforceError(""Invalid input arguments for tf_apply."")\n            if not all(\n                util.is_consistent_with_value_spec(value_spec=spec, x=internals[name])\n                for name, spec in self.__class__.internals_spec(network=self).items()\n            ):\n                raise TensorforceError(""Invalid input arguments for tf_apply."")\n\n            if return_internals:\n                x, internals = tf_function(x=x, internals=internals, return_internals=True)\n            else:\n                x = tf_function(x=x, internals=internals, return_internals=False)\n\n            if not util.is_consistent_with_value_spec(value_spec=self.get_output_spec(), x=x):\n                raise TensorforceError(""Invalid output arguments for tf_apply."")\n            if return_internals and not all(\n                util.is_consistent_with_value_spec(value_spec=spec, x=internals[name])\n                for name, spec in self.__class__.internals_spec(network=self).items()\n            ):\n                raise TensorforceError(""Invalid output arguments for tf_apply."")\n\n            if return_internals:\n                return x, internals\n            else:\n                return x\n\n        return super().create_tf_function(name=name, tf_function=validated_tf_function)\n\n\nclass LayerbasedNetwork(Network):\n    """"""\n    Base class for networks using Tensorforce layers.\n    """"""\n\n    def __init__(\n        self, name, inputs_spec, device=None, summary_labels=None, l2_regularization=None\n    ):\n        """"""\n        Layer-based network constructor.\n        """"""\n        super().__init__(\n            name=name, inputs_spec=inputs_spec, device=device, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        if len(inputs_spec) == 1:\n            self.output_spec = next(iter(inputs_spec.values()))\n        else:\n            self.output_spec = None\n\n    def get_output_spec(self):\n        return self.output_spec\n\n    @classmethod\n    def internals_spec(cls, network=None, **kwargs):\n        internals_spec = OrderedDict()\n\n        if network is not None:\n            for layer in network.modules.values():\n                if not isinstance(layer, StatefulLayer):\n                    continue\n                for name, spec in layer.__class__.internals_spec(layer=layer).items():\n                    name = \'{}-{}-{}\'.format(network.name, layer.name, name)\n                    if name in internals_spec:\n                        raise TensorforceError.unexpected()\n                    internals_spec[name] = spec\n\n        return internals_spec\n\n    def internals_init(self):\n        internals_init = OrderedDict()\n\n        for layer in self.modules.values():\n            if not isinstance(layer, StatefulLayer):\n                continue\n            for name, internal_init in layer.internals_init().items():\n                internals_init[\'{}-{}-{}\'.format(self.name, layer.name, name)] = internal_init\n\n        return internals_init\n\n    def add_module(self, *args, **kwargs):\n        # Default modules set: layer_modules\n        if len(args) < 3 and \'modules\' not in kwargs:\n            assert \'is_subscope\' not in kwargs\n            kwargs[\'modules\'] = layer_modules\n            kwargs[\'is_subscope\'] = True\n\n        # if \'input_spec\' in kwargs:\n        #     layer = super().add_module(*args, **kwargs)\n        #     self.output_spec = layer.output_spec\n\n        # else:\n        if self.output_spec is None:\n            if util.is_atomic_values_spec(values_spec=self.inputs_spec):\n                self.output_spec = self.inputs_spec\n            elif len(self.inputs_spec) == 1:\n                self.output_spec = next(iter(self.inputs_spec.values()))\n            else:\n                self.output_spec = None\n\n        if self.output_spec is not None:\n            if \'input_spec\' in kwargs:\n                kwargs[\'input_spec\'] = util.unify_value_specs(\n                    value_spec1=kwargs[\'input_spec\'], value_spec2=self.output_spec\n                )\n            else:\n                kwargs[\'input_spec\'] = self.output_spec\n\n        layer = super().add_module(*args, **kwargs)\n\n        self.output_spec = layer.output_spec\n\n        if not isinstance(layer, (Layer, Parameter)):\n            raise TensorforceError.type(\n                name=\'layer-based network\', argument=\'sub-module\', value=layer\n            )\n\n        return layer\n\n    def max_past_horizon(self, is_optimization):\n        past_horizons = [0]\n        for layer in self.modules.values():\n            if isinstance(layer, TemporalLayer):\n                if not isinstance(layer, StatefulLayer) or is_optimization:\n                    past_horizons.append(layer.dependency_horizon.max_value())\n        return max(past_horizons)\n\n    def tf_past_horizon(self, is_optimization):\n        past_horizons = [tf.constant(value=0, dtype=util.tf_dtype(dtype=\'long\'))]\n        for layer in self.modules.values():\n            if isinstance(layer, TemporalLayer):\n                if not isinstance(layer, StatefulLayer) or is_optimization:\n                    past_horizons.append(layer.dependency_horizon.value())\n        return tf.math.reduce_max(input_tensor=tf.stack(values=past_horizons, axis=0), axis=0)\n'"
tensorforce/core/networks/preprocessor.py,1,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import Counter, OrderedDict\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import Module\nfrom tensorforce.core.layers import layer_modules, PreprocessingLayer, StatefulLayer, TemporalLayer\nfrom tensorforce.core.networks import LayerbasedNetwork\n\n\nclass Preprocessor(LayerbasedNetwork):\n    """"""\n    Special preprocessor network following a sequential layer-stack architecture, which can be\n    specified as either a single or a list of layer specifications.\n\n    Args:\n        name (string): Network name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        input_spec (specification): Input tensor specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        layers (iter[specification] | iter[iter[specification]]): Layers configuration, see\n            [layers](../modules/layers.html)\n            (<span style=""color:#C00000""><b>required</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, input_spec, layers, device=None, summary_labels=None, l2_regularization=None\n    ):\n        super().__init__(\n            name=name, inputs_spec=input_spec, device=device, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        if isinstance(layers, (dict, str)):\n            layers = [layers]\n\n        layer_counter = Counter()\n        for layer_spec in layers:\n            if \'name\' in layer_spec:\n                layer_name = layer_spec[\'name\']\n            else:\n                if isinstance(layer_spec, dict) and isinstance(layer_spec.get(\'type\'), str):\n                    layer_type = layer_spec[\'type\']\n                else:\n                    layer_type = \'layer\'\n                layer_name = layer_type + str(layer_counter[layer_type])\n                layer_counter[layer_type] += 1\n\n            # layer_name = self.name + \'-\' + layer_name\n            self.add_module(name=layer_name, module=layer_spec)\n\n    @classmethod\n    def internals_spec(cls, network=None, **kwargs):\n        raise NotImplementedError\n\n    @classmethod\n    def output_spec(cls, input_spec, layers, **kwargs):\n        input_spec = dict(input_spec)\n        if isinstance(layers, (dict, str)):\n            layers = [layers]\n\n        layer_counter = Counter()\n        for layer_spec in layers:\n            if \'name\' in layer_spec:\n                layer_name = layer_spec[\'name\']\n            else:\n                if isinstance(layer_spec, dict) and isinstance(layer_spec.get(\'type\'), str):\n                    layer_type = layer_spec[\'type\']\n                else:\n                    layer_type = \'layer\'\n                layer_name = layer_type + str(layer_counter[layer_type])\n                layer_counter[layer_type] += 1\n\n            layer_cls, first_arg, kwargs = Module.get_module_class_and_kwargs(\n                name=layer_name, module=layer_spec, modules=layer_modules, input_spec=input_spec\n            )\n            if first_arg is None:\n                input_spec = layer_cls.output_spec(**kwargs)\n            else:\n                input_spec = layer_cls.output_spec(first_arg, **kwargs)\n\n        return input_spec\n\n    def internals_init(self):\n        raise NotImplementedError\n\n    def add_module(self, *args, **kwargs):\n        layer = super().add_module(*args, **kwargs)\n\n        if isinstance(layer, (TemporalLayer, StatefulLayer)):\n            raise TensorforceError.type(\n                name=\'preprocessor network\', argument=\'sub-module\', value=layer\n            )\n\n        return layer\n\n    def tf_reset(self):\n        operations = list()\n        for layer in self.modules.values():\n            if isinstance(layer, PreprocessingLayer):\n                operations.append(layer.reset())\n        return tf.group(*operations)\n\n    def tf_apply(self, x):\n        for layer in self.modules.values():\n            x = layer.apply(x=x)\n        return x\n\n    def create_tf_function(self, name, tf_function):\n        if tf_function.__name__ != \'tf_apply\':\n            return super().create_tf_function(name=name, tf_function=tf_function)\n\n        def validated_tf_function(x):\n            if util.is_atomic_values_spec(values_spec=self.inputs_spec):\n                if not util.is_consistent_with_value_spec(value_spec=self.inputs_spec, x=x):\n                    raise TensorforceError(""Invalid input arguments for tf_apply."")\n            else:\n                if not all(\n                    util.is_consistent_with_value_spec(value_spec=spec, x=x[name])\n                    for name, spec in self.inputs_spec.items()\n                ):\n                    raise TensorforceError(""Invalid input arguments for tf_apply."")\n\n            x = tf_function(x=x)\n\n            if not util.is_consistent_with_value_spec(value_spec=self.get_output_spec(), x=x):\n                raise TensorforceError(""Invalid output arguments for tf_apply."")\n\n            return x\n\n        return super().create_tf_function(name=name, tf_function=validated_tf_function)\n'"
tensorforce/core/objectives/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.objectives.objective import Objective\n\nfrom tensorforce.core.objectives.deterministic_policy_gradient import DeterministicPolicyGradient\nfrom tensorforce.core.objectives.plus import Plus\nfrom tensorforce.core.objectives.policy_gradient import PolicyGradient\nfrom tensorforce.core.objectives.value import Value\n\n\nobjective_modules = dict(\n    deterministic_policy_gradient=DeterministicPolicyGradient, plus=Plus,\n    policy_gradient=PolicyGradient, value=Value\n)\n\n\n__all__ = [\n    \'DeterministicPolicyGradient\', \'Objective\', \'objective_modules\', \'Plus\', \'PolicyGradient\',\n    \'Value\'\n]\n'"
tensorforce/core/objectives/deterministic_policy_gradient.py,6,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.objectives import Objective\n\n\nclass DeterministicPolicyGradient(Objective):\n    """"""\n    Deterministic policy gradient objective (specification key: `det_policy_gradient`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def tf_loss_per_instance(self, policy, states, internals, auxiliaries, actions, reward):\n        policy_actions = policy.act(\n            states=states, internals=internals, auxiliaries=auxiliaries, return_internals=False\n        )\n\n        summed_actions = list()\n        for name, action in policy_actions.items():\n            rank = len(policy.actions_spec[name][\'shape\'])\n            for n in range(rank):\n                action = tf.math.reduce_sum(input_tensor=action, axis=(rank - n))\n            summed_actions.append(action)\n        summed_actions = tf.math.add_n(inputs=summed_actions)\n        # mean? (will be mean later)\n        # tf.concat(values=, axis=1)\n        # tf.math.reduce_mean(input_tensor=, axis=1)\n\n        return summed_actions\n\n    def tf_initial_gradients(self, policy, baseline, states, internals, auxiliaries):\n        actions = policy.act(\n            states=states, internals=internals, auxiliaries=auxiliaries, return_internals=False\n        )\n        actions_value = baseline.actions_value(\n            states=states, internals=internals, auxiliaries=auxiliaries, actions=actions\n        )\n        assert len(actions) == 1\n        action = next(iter(actions.values()))\n        shape = util.shape(x=action)\n        if len(shape) == 1:\n            gradients = -tf.gradients(ys=actions_value, xs=[action])[0][0]\n        elif len(shape) == 2 and shape[1] == 1:\n            gradients = -tf.gradients(ys=actions_value, xs=[action])[0][0][0]\n        else:\n            assert False\n\n        return gradients\n\n    def optimizer_arguments(self, policy, baseline, **kwargs):\n        arguments = super().optimizer_arguments()\n\n        def fn_initial_gradients(states, internals, auxiliaries, actions, reward):\n            return self.initial_gradients(\n                policy=policy, baseline=baseline, states=states, internals=internals,\n                auxiliaries=auxiliaries\n            )\n\n        arguments[\'fn_initial_gradients\'] = fn_initial_gradients\n\n        return arguments\n'"
tensorforce/core/objectives/objective.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nfrom tensorforce.core import Module\n\n\nclass Objective(Module):\n    """"""\n    Base class for optimization objectives.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, summary_labels=None):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n    def tf_loss_per_instance(self, policy, states, internals, auxiliaries, actions, reward):\n        raise NotImplementedError\n\n    def optimizer_arguments(self, **kwargs):\n        return OrderedDict()\n'"
tensorforce/core/objectives/plus.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport tensorforce.core\nfrom tensorforce.core.objectives import Objective\n\n\nclass Plus(Objective):\n    """"""\n    Additive combination of two objectives (specification key: `plus`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        objective1 (specification): First objective configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        objective2 (specification): Second objective configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, objective1, objective2, summary_labels=None):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        self.objective1 = self.add_module(\n            name=\'first-objective\', module=objective1, modules=tensorforce.core.objective_modules\n        )\n        self.objective2 = self.add_module(\n            name=\'second-objective\', module=objective2, modules=tensorforce.core.objective_modules\n        )\n\n    def tf_loss_per_instance(\n        self, policy, states, internals, auxiliaries, actions, reward, **kwargs\n    ):\n        kwargs1 = OrderedDict()\n        kwargs2 = OrderedDict()\n        for key, value in kwargs.items():\n            assert len(value) == 2 and (value[0] is not None or value[1] is not None)\n            if value[0] is not None:\n                kwargs1[key] = value[0]\n            if value[1] is not None:\n                kwargs2[key] = value[1]\n\n        loss1 = self.objective1.loss_per_instance(\n            policy=policy, states=states, internals=internals, auxiliaries=auxiliaries,\n            actions=actions, reward=reward, **kwargs1\n        )\n\n        loss2 = self.objective2.loss_per_instance(\n            policy=policy, states=states, internals=internals, auxiliaries=auxiliaries,\n            actions=actions, reward=reward, **kwargs2\n        )\n\n        return loss1 + loss2\n\n    def optimizer_arguments(self, **kwargs):\n        arguments = super().optimizer_arguments()\n        arguments1 = self.objective1.optimizer_arguments(**kwargs)\n        arguments2 = self.objective1.optimizer_arguments(**kwargs)\n        for key, function in arguments1:\n            if key in arguments2:\n\n                def plus_function(states, internals, auxiliaries, actions, reward):\n                    value1 = function(\n                        states=states, internals=internals, auxiliaries=auxiliaries,\n                        actions=actions, reward=reward\n                    )\n                    value2 = arguments2[key](\n                        states=states, internals=internals, auxiliaries=auxiliaries,\n                        actions=actions, reward=reward\n                    )\n                    return (value1, value2)\n\n            else:\n\n                def plus_function(states, internals, auxiliaries, actions, reward):\n                    value1 = function(\n                        states=states, internals=internals, auxiliaries=auxiliaries,\n                        actions=actions, reward=reward\n                    )\n                    return (value1, None)\n\n            arguments[key] = plus_function\n\n        for key, function in arguments2:\n            if key not in arguments1:\n\n                def plus_function(states, internals, auxiliaries, actions, reward):\n                    value2 = function(\n                        states=states, internals=internals, auxiliaries=auxiliaries,\n                        actions=actions, reward=reward\n                    )\n                    return (None, value2)\n\n            arguments[key] = plus_function\n\n        return arguments\n'"
tensorforce/core/objectives/policy_gradient.py,8,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.objectives import Objective\n\n\nclass PolicyGradient(Objective):\n    """"""\n    Policy gradient objective, which maximizes the log-likelihood or likelihood-ratio scaled by the\n    target reward value (specification key: `policy_gradient`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        ratio_based (bool): Whether to scale the likelihood-ratio instead of the log-likelihood\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        clipping_value (parameter, float >= 0.0): Clipping threshold for the maximized value\n            (<span style=""color:#00C000""><b>default</b></span>: no clipping).\n        early_reduce (bool): Whether to compute objective for reduced likelihoods instead of per\n            likelihood (<span style=""color:#00C000""><b>default</b></span>: true).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, ratio_based=False, clipping_value=0.0, early_reduce=True,\n        summary_labels=None\n    ):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        self.ratio_based = ratio_based\n\n        clipping_value = 0.0 if clipping_value is None else clipping_value\n        self.clipping_value = self.add_module(\n            name=\'clipping-value\', module=clipping_value, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0\n        )\n\n        self.early_reduce = early_reduce\n\n    def tf_loss_per_instance(\n        self, policy, states, internals, auxiliaries, actions, reward, reference=None\n    ):\n        assert self.ratio_based or reference is None\n\n        log_probability = policy.log_probability(\n            states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n            reduced=self.early_reduce\n        )\n\n        zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n        one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n\n        clipping_value = self.clipping_value.value()\n\n        if self.ratio_based:\n            if reference is None:\n                reference = log_probability\n            scaling = tf.exp(x=(log_probability - tf.stop_gradient(input=reference)))\n            min_value = one / (one + clipping_value)\n            max_value = one + clipping_value\n\n        else:\n            scaling = log_probability\n            min_value = -clipping_value\n            max_value = log_probability + one\n\n        if not self.early_reduce:\n            reward = tf.expand_dims(input=reward, axis=1)\n\n        def no_clipping():\n            return scaling * reward\n\n        def apply_clipping():\n            clipped_scaling = tf.clip_by_value(\n                t=scaling, clip_value_min=min_value, clip_value_max=max_value\n            )\n            return tf.minimum(x=(scaling * reward), y=(clipped_scaling * reward))\n\n        skip_clipping = tf.math.equal(x=clipping_value, y=zero)\n        scaled = self.cond(pred=skip_clipping, true_fn=no_clipping, false_fn=apply_clipping)\n\n        loss = -scaled\n\n        if not self.early_reduce:\n            loss = tf.math.reduce_mean(input_tensor=loss, axis=1)\n\n        return loss\n\n    def tf_reference(self, policy, states, internals, auxiliaries, actions):\n        reference = policy.log_probability(\n            states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n            reduced=self.early_reduce\n        )\n\n        return reference\n\n    def optimizer_arguments(self, policy, **kwargs):\n        arguments = super().optimizer_arguments()\n\n        if self.ratio_based:\n\n            def fn_reference(states, internals, auxiliaries, actions, reward):\n                return self.reference(\n                    policy=policy, states=states, internals=internals, auxiliaries=auxiliaries,\n                    actions=actions\n                )\n\n            arguments[\'fn_reference\'] = fn_reference\n\n        return arguments\n'"
tensorforce/core/objectives/value.py,10,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.objectives import Objective\n\n\nclass Value(Objective):\n    """"""\n    Value approximation objective, which minimizes the L2-distance between the state-(action-)value\n    estimate and the target reward value (specification key: `value`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        value (""state"" | ""action""): Whether to approximate the state- or state-action-value\n            (<span style=""color:#00C000""><b>default</b></span>: ""state"").\n        huber_loss (parameter, float >= 0.0): Huber loss threshold\n            (<span style=""color:#00C000""><b>default</b></span>: no huber loss).\n        early_reduce (bool): Whether to compute objective for reduced values instead of value per\n            action (<span style=""color:#00C000""><b>default</b></span>: true).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, value=\'state\', huber_loss=0.0, early_reduce=True, summary_labels=None\n    ):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        assert value in (\'state\', \'action\')\n        self.value = value\n\n        huber_loss = 0.0 if huber_loss is None else huber_loss\n        self.huber_loss = self.add_module(\n            name=\'huber-loss\', module=huber_loss, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0\n        )\n\n        self.early_reduce = early_reduce\n\n    def tf_loss_per_instance(self, policy, states, internals, auxiliaries, actions, reward):\n        if not self.early_reduce:\n            reward = tf.expand_dims(input=reward, axis=1)\n\n        if self.value == \'state\':\n            value = policy.states_value(\n                states=states, internals=internals, auxiliaries=auxiliaries,\n                reduced=self.early_reduce\n            )\n        elif self.value == \'action\':\n            value = policy.actions_value(\n                states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n                reduced=self.early_reduce\n            )\n\n        difference = value - reward\n\n        zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n        half = tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n\n        huber_loss = self.huber_loss.value()\n        skip_huber_loss = tf.math.equal(x=huber_loss, y=zero)\n\n        def no_huber_loss():\n            return half * tf.square(x=difference)\n\n        def apply_huber_loss():\n            inside_huber_bounds = tf.math.less_equal(x=tf.abs(x=difference), y=huber_loss)\n            quadratic = half * tf.square(x=difference)\n            linear = huber_loss * (tf.abs(x=difference) - half * huber_loss)\n            return tf.where(condition=inside_huber_bounds, x=quadratic, y=linear)\n\n        loss = self.cond(pred=skip_huber_loss, true_fn=no_huber_loss, false_fn=apply_huber_loss)\n\n        if not self.early_reduce:\n            loss = tf.math.reduce_mean(input_tensor=loss, axis=1)\n\n        return loss\n'"
tensorforce/core/optimizers/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\nfrom functools import partial\r\n\r\nfrom tensorforce.core.optimizers.optimizer import Optimizer\r\n\r\nfrom tensorforce.core.optimizers.meta_optimizer import MetaOptimizer\r\n\r\nfrom tensorforce.core.optimizers.clipping_step import ClippingStep\r\nfrom tensorforce.core.optimizers.evolutionary import Evolutionary\r\nfrom tensorforce.core.optimizers.global_optimizer import GlobalOptimizer\r\nfrom tensorforce.core.optimizers.meta_optimizer_wrapper import MetaOptimizerWrapper\r\nfrom tensorforce.core.optimizers.multi_step import MultiStep\r\nfrom tensorforce.core.optimizers.natural_gradient import NaturalGradient\r\nfrom tensorforce.core.optimizers.optimizing_step import OptimizingStep\r\nfrom tensorforce.core.optimizers.plus import Plus\r\nfrom tensorforce.core.optimizers.subsampling_step import SubsamplingStep\r\nfrom tensorforce.core.optimizers.synchronization import Synchronization\r\nfrom tensorforce.core.optimizers.tf_optimizer import TFOptimizer, tensorflow_optimizers\r\n\r\n\r\noptimizer_modules = dict(\r\n    clipping_step=ClippingStep, default=MetaOptimizerWrapper, evolutionary=Evolutionary,\r\n    global_optimizer=GlobalOptimizer, meta_optimizer_wrapper=MetaOptimizerWrapper,\r\n    multi_step=MultiStep, natural_gradient=NaturalGradient, optimizing_step=OptimizingStep,\r\n    plus=Plus, subsampling_step=SubsamplingStep, synchronization=Synchronization,\r\n    tf_optimizer=TFOptimizer\r\n)\r\n\r\n\r\nfor name, optimizer in tensorflow_optimizers.items():\r\n    assert name not in optimizer_modules\r\n    optimizer_modules[name] = partial(TFOptimizer, optimizer=name)\r\n\r\n\r\n__all__ = [\r\n    \'ClippingStep\', \'Evolutionary\', \'GlobalOptimizer\', \'MetaOptimizer\', \'MetaOptimizerWrapper\',\r\n    \'MultiStep\', \'NaturalGradient\', \'OptimizingStep\', \'Optimizer\', \'optimizer_modules\', \'Plus\',\r\n    \'SubsamplingStep\', \'Synchronization\', \'TFOptimizer\'\r\n]\r\n'"
tensorforce/core/optimizers/clipping_step.py,6,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.optimizers import MetaOptimizer\n\n\nclass ClippingStep(MetaOptimizer):\n    """"""\n    Clipping-step meta optimizer, which clips the updates of the given optimizer (specification\n    key: `clipping_step`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (specification): Optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        threshold (parameter, float >= 0.0): Clipping threshold\n            (<span style=""color:#C00000""><b>required</b></span>).\n        mode (\'global_norm\' | \'norm\' | \'value\'): Clipping mode\n            (<span style=""color:#00C000""><b>default</b></span>: \'global_norm\').\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, optimizer, threshold, mode=\'global_norm\', summary_labels=None):\n        super().__init__(name=name, optimizer=optimizer, summary_labels=summary_labels)\n\n        self.threshold = self.add_module(\n            name=\'threshold\', module=threshold, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0\n        )\n\n        assert mode in (\'global_norm\', \'norm\', \'value\')\n        self.mode = mode\n\n    def tf_step(self, variables, **kwargs):\n        deltas = self.optimizer.step(variables=variables, **kwargs)\n\n        with tf.control_dependencies(control_inputs=deltas):\n            threshold = self.threshold.value()\n            if self.mode == \'global_norm\':\n                clipped_deltas, update_norm = tf.clip_by_global_norm(\n                    t_list=deltas, clip_norm=threshold\n                )\n            else:\n                update_norm = tf.linalg.global_norm(t_list=deltas)\n                clipped_deltas = list()\n                for delta in deltas:\n                    if self.mode == \'norm\':\n                        clipped_delta = tf.clip_by_norm(t=delta, clip_norm=threshold)\n                    elif self.mode == \'value\':\n                        clipped_delta = tf.clip_by_value(\n                            t=delta, clip_value_min=-threshold, clip_value_max=threshold\n                        )\n                    clipped_deltas.append(clipped_delta)\n\n            clipped_deltas = self.add_summary(\n                label=\'update-norm\', name=\'update-norm-unclipped\', tensor=update_norm,\n                pass_tensors=clipped_deltas\n            )\n\n            exceeding_deltas = list()\n            for delta, clipped_delta in zip(deltas, clipped_deltas):\n                exceeding_deltas.append(clipped_delta - delta)\n\n        applied = self.apply_step(variables=variables, deltas=exceeding_deltas)\n\n        with tf.control_dependencies(control_inputs=(applied,)):\n            return util.fmap(function=util.identity_operation, xs=clipped_deltas)\n'"
tensorforce/core/optimizers/evolutionary.py,13,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.optimizers import Optimizer\n\n\nclass Evolutionary(Optimizer):\n    """"""\n    Evolutionary optimizer, which samples random perturbations and applies them either as positive\n    or negative update depending on their improvement of the loss (specification key:\n    `evolutionary`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        learning_rate (parameter, float >= 0.0): Learning rate\n            (<span style=""color:#C00000""><b>required</b></span>).\n        num_samples (parameter, int >= 0): Number of sampled perturbations\n            (<span style=""color:#00C000""><b>default</b></span>: 1).\n        unroll_loop (bool): Whether to unroll the sampling loop\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, learning_rate, num_samples=1, unroll_loop=False, summary_labels=None\n    ):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        self.learning_rate = self.add_module(\n            name=\'learning-rate\', module=learning_rate, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0\n        )\n\n        assert isinstance(unroll_loop, bool)\n        self.unroll_loop = unroll_loop\n\n        if self.unroll_loop:\n            self.num_samples = num_samples\n        else:\n            self.num_samples = self.add_module(\n                name=\'num-samples\', module=num_samples, modules=parameter_modules, dtype=\'int\',\n                min_value=0\n            )\n\n    def tf_step(self, variables, arguments, fn_loss, **kwargs):\n        learning_rate = self.learning_rate.value()\n        unperturbed_loss = fn_loss(**arguments)\n\n        deltas = [tf.zeros_like(input=variable) for variable in variables]\n        previous_perturbations = [tf.zeros_like(input=variable) for variable in variables]\n\n        if self.unroll_loop:\n            # Unrolled for loop\n            for sample in range(self.num_samples):\n                with tf.control_dependencies(control_inputs=deltas):\n                    perturbations = [\n                        tf.random.normal(shape=util.shape(variable)) * learning_rate\n                        for variable in variables\n                    ]\n                    perturbation_deltas = [\n                        pert - prev_pert\n                        for pert, prev_pert in zip(perturbations, previous_perturbations)\n                    ]\n                    applied = self.apply_step(variables=variables, deltas=perturbation_deltas)\n                    previous_perturbations = perturbations\n\n                with tf.control_dependencies(control_inputs=(applied,)):\n                    perturbed_loss = fn_loss(**arguments)\n                    direction = tf.sign(x=(unperturbed_loss - perturbed_loss))\n                    deltas = [\n                        delta + direction * perturbation\n                        for delta, perturbation in zip(deltas, perturbations)\n                    ]\n\n        else:\n            # TensorFlow while loop\n            def body(deltas, previous_perturbations):\n                with tf.control_dependencies(control_inputs=deltas):\n                    perturbations = [\n                        learning_rate * tf.random.normal(\n                            shape=util.shape(x=variable), dtype=util.tf_dtype(dtype=\'float\')\n                        ) for variable in variables\n                    ]\n                    perturbation_deltas = [\n                        pert - prev_pert\n                        for pert, prev_pert in zip(perturbations, previous_perturbations)\n                    ]\n                    applied = self.apply_step(variables=variables, deltas=perturbation_deltas)\n\n                with tf.control_dependencies(control_inputs=(applied,)):\n                    perturbed_loss = fn_loss(**arguments)\n                    direction = tf.sign(x=(unperturbed_loss - perturbed_loss))\n                    deltas = [\n                        delta + direction * perturbation\n                        for delta, perturbation in zip(deltas, perturbations)\n                    ]\n\n                return deltas, perturbations\n\n            num_samples = self.num_samples.value()\n            deltas, perturbations = self.while_loop(\n                cond=util.tf_always_true, body=body, loop_vars=(deltas, previous_perturbations),\n                back_prop=False, maximum_iterations=num_samples\n            )\n\n        with tf.control_dependencies(control_inputs=deltas):\n            num_samples = tf.dtypes.cast(x=num_samples, dtype=util.tf_dtype(dtype=\'float\'))\n            deltas = [delta / num_samples for delta in deltas]\n            perturbation_deltas = [delta - pert for delta, pert in zip(deltas, perturbations)]\n            applied = self.apply_step(variables=variables, deltas=perturbation_deltas)\n\n        with tf.control_dependencies(control_inputs=(applied,)):\n            # Trivial operation to enforce control dependency\n            return util.fmap(function=util.identity_operation, xs=deltas)\n'"
tensorforce/core/optimizers/global_optimizer.py,3,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.optimizers import MetaOptimizer\n\n\nclass GlobalOptimizer(MetaOptimizer):\n    """"""\n    Global meta optimizer, which applies the given optimizer to the local variables, then applies\n    the update to a corresponding set of global variables, and subsequently updates the local\n    variables to the value of the global variables; will likely change in the future (specification\n    key: `global_optimizer`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (specification): Optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def tf_step(self, variables, **kwargs):\n        global_variables = kwargs[""global_variables""]\n\n        assert all(\n            util.shape(global_variable) == util.shape(local_variable)\n            for global_variable, local_variable in zip(global_variables, variables)\n        )\n\n        local_deltas = self.optimizer.step(variables=variables, **kwargs)\n\n        with tf.control_dependencies(control_inputs=local_deltas):\n            applied = self.optimizer.apply_step(variables=global_variables, deltas=local_deltas)\n\n        with tf.control_dependencies(control_inputs=(applied,)):\n            update_deltas = list()\n            for global_variable, local_variable in zip(global_variables, variables):\n                delta = global_variable - local_variable\n                update_deltas.append(delta)\n\n            applied = self.apply_step(variables=variables, deltas=update_deltas)\n\n            # TODO: Update time, episode, etc (like in Synchronization)?\n\n        with tf.control_dependencies(control_inputs=(applied,)):\n            return [\n                local_delta + update_delta\n                for local_delta, update_delta in zip(local_deltas, update_deltas)\n            ]\n'"
tensorforce/core/optimizers/meta_optimizer.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorforce.core\nfrom tensorforce.core.optimizers import Optimizer\n\n\nclass MetaOptimizer(Optimizer):\n    """"""\n    Meta optimizer, which takes the update mechanism implemented by another optimizer and modifies\n    it.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (specification): Optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, optimizer, summary_labels=None):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        self.optimizer = self.add_module(\n            name=\'inner-optimizer\', module=optimizer, modules=tensorforce.core.optimizer_modules\n        )\n'"
tensorforce/core/optimizers/meta_optimizer_wrapper.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.optimizers import MetaOptimizer\n\n\nclass MetaOptimizerWrapper(MetaOptimizer):\n    """"""\n    Meta optimizer wrapper (specification key: `meta_optimizer_wrapper`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (specification): Optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        multi_step (parameter, int > 0): Number of optimization steps\n            (<span style=""color:#00C000""><b>default</b></span>: single step).\n        subsampling_fraction (parameter, 0.0 < float <= 1.0): Fraction of batch timesteps to\n            subsample (<span style=""color:#00C000""><b>default</b></span>: no subsampling).\n        clipping_threshold (parameter, float > 0.0): Clipping threshold\n            (<span style=""color:#00C000""><b>default</b></span>: no clipping).\n        optimizing_iterations (parameter, int >= 0):  Maximum number of line search iterations\n            (<span style=""color:#00C000""><b>default</b></span>: no optimizing).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, optimizer, multi_step=1, subsampling_fraction=1.0, clipping_threshold=None,\n        optimizing_iterations=0, summary_labels=None, **kwargs\n    ):\n        optimizer = dict(type=optimizer)\n        optimizer.update(kwargs)\n        if optimizing_iterations > 0:\n            optimizer = dict(\n                type=\'optimizing_step\', optimizer=optimizer,\n                ls_max_iterations=optimizing_iterations\n            )\n        if clipping_threshold is not None:\n            optimizer = dict(\n                type=\'clipping_step\', optimizer=optimizer, threshold=clipping_threshold\n            )\n        if subsampling_fraction != 1.0:\n            optimizer = dict(\n                type=\'subsampling_step\', optimizer=optimizer, fraction=subsampling_fraction\n            )\n        if multi_step > 1:\n            optimizer = dict(type=\'multi_step\', optimizer=optimizer, num_steps=multi_step)\n\n        super().__init__(name=name, optimizer=optimizer, summary_labels=summary_labels)\n\n    def tf_step(self, variables, **kwargs):\n        return self.optimizer.step(variables=variables, **kwargs)\n'"
tensorforce/core/optimizers/multi_step.py,3,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.optimizers import MetaOptimizer\n\n\nclass MultiStep(MetaOptimizer):\n    """"""\n    Multi-step meta optimizer, which applies the given optimizer for a number of times\n    (specification key: `multi_step`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (specification): Optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        num_steps (parameter, int >= 0): Number of optimization steps\n            (<span style=""color:#C00000""><b>required</b></span>).\n        unroll_loop (bool): Whether to unroll the repetition loop\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, optimizer, num_steps, unroll_loop=False, summary_labels=None):\n        super().__init__(name=name, optimizer=optimizer, summary_labels=summary_labels)\n\n        assert isinstance(unroll_loop, bool)\n        self.unroll_loop = unroll_loop\n\n        if self.unroll_loop:\n            self.num_steps = num_steps\n        else:\n            self.num_steps = self.add_module(\n                name=\'num-steps\', module=num_steps, modules=parameter_modules, dtype=\'int\',\n                min_value=0\n            )\n\n    def tf_step(self, variables, arguments, fn_reference=None, **kwargs):\n        # Set reference to compare with at each optimization step, in case of a comparative loss.\n        if fn_reference is not None:\n            assert \'reference\' not in arguments\n            arguments[\'reference\'] = fn_reference(**arguments)\n\n        deltas = [tf.zeros_like(input=variable) for variable in variables]\n\n        if self.unroll_loop:\n            # Unrolled for loop\n            for _ in range(self.num_steps):\n                with tf.control_dependencies(control_inputs=deltas):\n                    step_deltas = self.optimizer.step(\n                        variables=variables, arguments=arguments, **kwargs\n                    )\n                    deltas = [delta1 + delta2 for delta1, delta2 in zip(deltas, step_deltas)]\n\n            return deltas\n\n        else:\n            # TensorFlow while loop\n            def body(deltas):\n                with tf.control_dependencies(control_inputs=deltas):\n                    step_deltas = self.optimizer.step(\n                        variables=variables, arguments=arguments, **kwargs\n                    )\n                    deltas = [delta1 + delta2 for delta1, delta2 in zip(deltas, step_deltas)]\n                return (deltas,)\n\n            num_steps = self.num_steps.value()\n            deltas = self.while_loop(\n                cond=util.tf_always_true, body=body, loop_vars=(deltas,), back_prop=False,\n                maximum_iterations=num_steps\n            )[0]\n\n            return deltas\n'"
tensorforce/core/optimizers/natural_gradient.py,19,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.optimizers import Optimizer\nfrom tensorforce.core.optimizers.solvers import solver_modules\n\n\nclass NaturalGradient(Optimizer):\n    """"""\n    Natural gradient optimizer (specification key: `natural_gradient`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        learning_rate (parameter, float >= 0.0): Learning rate as KL-divergence of distributions\n            between optimization steps (<span style=""color:#C00000""><b>required</b></span>).\n        cg_max_iterations (int >= 0): Maximum number of conjugate gradient iterations.\n            (<span style=""color:#00C000""><b>default</b></span>: 10).\n        cg_damping (0.0 <= float <= 1.0): Conjugate gradient damping factor.\n            (<span style=""color:#00C000""><b>default</b></span>: 1e-3).\n        cg_unroll_loop (bool): Whether to unroll the conjugate gradient loop\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, learning_rate, cg_max_iterations=10, cg_damping=1e-3, cg_unroll_loop=False,\n        summary_labels=None\n    ):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        self.learning_rate = self.add_module(\n            name=\'learning-rate\', module=learning_rate, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0\n        )\n\n        self.solver = self.add_module(\n            name=\'conjugate-gradient\', module=\'conjugate_gradient\', modules=solver_modules,\n            max_iterations=cg_max_iterations, damping=cg_damping, unroll_loop=cg_unroll_loop\n        )\n\n    def tf_step(\n        self, variables, arguments, fn_loss, fn_kl_divergence, return_estimated_improvement=False,\n        **kwargs\n    ):\n        # Optimize: argmin(w) loss(w + delta) such that kldiv(P(w) || P(w + delta)) = learning_rate\n        # For more details, see our blogpost:\n        # https://reinforce.io/blog/end-to-end-computation-graphs-for-reinforcement-learning/\n\n        # Calculates the product x * F of a given vector x with the fisher matrix F.\n        # Incorporating the product prevents having to calculate the entire matrix explicitly.\n        def fisher_matrix_product(deltas):\n            # Gradient is not propagated through solver.\n            deltas = [tf.stop_gradient(input=delta) for delta in deltas]\n\n            # kldiv\n            kldiv = fn_kl_divergence(**arguments)\n\n            # grad(kldiv)\n            kldiv_grads = tf.gradients(ys=kldiv, xs=variables)\n            num_grad_none = sum(grad is None for grad in kldiv_grads)\n            assert num_grad_none < len(kldiv_grads)\n            kldiv_grads = [\n                tf.zeros_like(input=var) if grad is None else tf.convert_to_tensor(value=grad)\n                for grad, var in zip(kldiv_grads, variables)\n            ]\n\n            # delta\' * grad(kldiv)\n            delta_kldiv_grads = tf.add_n(inputs=[\n                tf.reduce_sum(input_tensor=(delta * grad))\n                for delta, grad in zip(deltas, kldiv_grads)\n            ])\n\n            # [delta\' * F] = grad(delta\' * grad(kldiv))\n            delta_kldiv_grads2 = tf.gradients(ys=delta_kldiv_grads, xs=variables)\n            assert sum(grad is None for grad in delta_kldiv_grads2) == num_grad_none\n            return [\n                tf.zeros_like(input=var) if grad is None else tf.convert_to_tensor(value=grad)\n                for grad, var in zip(delta_kldiv_grads2, variables)\n            ]\n\n        # loss\n        arguments = util.fmap(function=tf.stop_gradient, xs=arguments)\n        loss = fn_loss(**arguments)\n\n        # grad(loss)\n        loss_gradients = tf.gradients(ys=loss, xs=variables)\n\n        # Solve the following system for delta\' via the conjugate gradient solver.\n        # [delta\' * F] * delta\' = -grad(loss)\n        # --> delta\'  (= lambda * delta)\n        deltas = self.solver.solve(\n            fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients]\n        )\n\n        # delta\' * F\n        delta_fisher_matrix_product = fisher_matrix_product(deltas=deltas)\n\n        # c\' = 0.5 * delta\' * F * delta\'  (= lambda * c)\n        # TODO: Why constant and hence KL-divergence sometimes negative?\n        half = tf.constant(value=0.5, dtype=util.tf_dtype(dtype=\'float\'))\n        constant = half * tf.add_n(inputs=[\n            tf.reduce_sum(input_tensor=(delta_F * delta))\n            for delta_F, delta in zip(delta_fisher_matrix_product, deltas)\n        ])\n\n        learning_rate = self.learning_rate.value()\n\n        # Zero step if constant <= 0\n        def no_step():\n            zero_deltas = [tf.zeros_like(input=delta) for delta in deltas]\n            if return_estimated_improvement:\n                return zero_deltas, tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n            else:\n                return zero_deltas\n\n        # Natural gradient step if constant > 0\n        def apply_step():\n            # lambda = sqrt(c\' / c)\n            lagrange_multiplier = tf.sqrt(x=(constant / learning_rate))\n\n            # delta = delta\' / lambda\n            estimated_deltas = [delta / lagrange_multiplier for delta in deltas]\n\n            # improvement = grad(loss) * delta  (= loss_new - loss_old)\n            estimated_improvement = tf.add_n(inputs=[\n                tf.reduce_sum(input_tensor=(grad * delta))\n                for grad, delta in zip(loss_gradients, estimated_deltas)\n            ])\n\n            # Apply natural gradient improvement.\n            applied = self.apply_step(variables=variables, deltas=estimated_deltas)\n\n            with tf.control_dependencies(control_inputs=(applied,)):\n                # Trivial operation to enforce control dependency\n                estimated_delta = util.fmap(function=util.identity_operation, xs=estimated_deltas)\n                if return_estimated_improvement:\n                    return estimated_delta, estimated_improvement\n                else:\n                    return estimated_delta\n\n        # Natural gradient step only works if constant > 0\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        skip_step = constant < (epsilon * learning_rate)\n        return self.cond(pred=skip_step, true_fn=no_step, false_fn=apply_step)\n'"
tensorforce/core/optimizers/optimizer.py,11,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import Module\n\n\nclass Optimizer(Module):\n    """"""\n    Base class for optimizers.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, summary_labels=None):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n    def tf_step(self, variables, **kwargs):\n        raise NotImplementedError\n\n    def tf_apply_step(self, variables, deltas):\n        if len(variables) != len(deltas):\n            raise TensorforceError(""Invalid variables and deltas lists."")\n\n        assignments = list()\n        for variable, delta in zip(variables, deltas):\n            assignments.append(variable.assign_add(delta=delta, read_value=False))\n\n        with tf.control_dependencies(control_inputs=assignments):\n            return util.no_operation()\n\n    def tf_minimize(self, variables, **kwargs):\n        if any(variable.dtype != util.tf_dtype(dtype=\'float\') for variable in variables):\n            raise TensorforceError.unexpected()\n\n        deltas = self.step(variables=variables, **kwargs)\n\n        update_norm = tf.linalg.global_norm(t_list=deltas)\n        deltas = self.add_summary(\n            label=\'update-norm\', name=\'update-norm\', tensor=update_norm, pass_tensors=deltas\n        )\n\n        for n in range(len(variables)):\n            name = variables[n].name\n            if name[-2:] != \':0\':\n                raise TensorforceError.unexpected()\n            deltas[n] = self.add_summary(\n                label=\'updates\', name=(\'update-\' + name[:-2]), tensor=deltas[n], mean_variance=True\n            )\n            deltas[n] = self.add_summary(\n                label=\'updates-histogram\', name=(\'update-\' + name[:-2]), tensor=deltas[n]\n            )\n\n        # TODO: experimental\n        # with tf.control_dependencies(control_inputs=deltas):\n        #     zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n        #     false = tf.constant(value=False, dtype=util.tf_dtype(dtype=\'bool\'))\n        #     deltas = [self.cond(\n        #         pred=tf.math.reduce_all(input_tensor=tf.math.equal(x=delta, y=zero)),\n        #         true_fn=(lambda: tf.Print(delta, (variable.name,))),\n        #         false_fn=(lambda: delta)) for delta, variable in zip(deltas, variables)\n        #     ]\n        #     assertions = [\n        #         tf.debugging.assert_equal(\n        #             x=tf.math.reduce_all(input_tensor=tf.math.equal(x=delta, y=zero)), y=false,\n        #             message=""Zero delta check.""\n        #         ) for delta, variable in zip(deltas, variables)\n        #         if util.product(xs=util.shape(x=delta)) > 4 and \'distribution\' not in variable.name\n        #     ]\n\n        # with tf.control_dependencies(control_inputs=assertions):\n        with tf.control_dependencies(control_inputs=deltas):\n            return util.no_operation()\n\n    def add_variable(self, name, dtype, shape, is_trainable=False, initializer=\'zeros\'):\n        if is_trainable:\n            raise TensorforceError(""Invalid trainable variable."")\n\n        return super().add_variable(\n            name=name, dtype=dtype, shape=shape, is_trainable=is_trainable, initializer=initializer\n        )\n'"
tensorforce/core/optimizers/optimizing_step.py,5,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError\nfrom tensorforce.core.optimizers import MetaOptimizer\nfrom tensorforce.core.optimizers.solvers import solver_modules\n\n\nclass OptimizingStep(MetaOptimizer):\n    """"""\n    Optimizing-step meta optimizer, which applies line search to the given optimizer to find a more\n    optimal step size (specification key: `optimizing_step`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (specification): Optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        ls_max_iterations (parameter, int >= 0): Maximum number of line search iterations\n            (<span style=""color:#00C000""><b>default</b></span>: 10).\n        ls_accept_ratio (parameter, 0.0 <= float <= 1.0): Line search acceptance ratio\n            (<span style=""color:#00C000""><b>default</b></span>: 0.9).\n        ls_mode (\'exponential\' | \'linear\'): Line search mode, see line search solver\n            (<span style=""color:#00C000""><b>default</b></span>: \'exponential\').\n        ls_parameter (parameter, 0.0 <= float <= 1.0): Line search parameter, see line search solver\n            (<span style=""color:#00C000""><b>default</b></span>: 0.5).\n        ls_unroll_loop (bool): Whether to unroll the line search loop\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, optimizer, ls_max_iterations=10, ls_accept_ratio=0.9, ls_mode=\'exponential\',\n        ls_parameter=0.5, ls_unroll_loop=False, summary_labels=None\n    ):\n        super().__init__(name=name, optimizer=optimizer)\n\n        self.solver = self.add_module(\n            name=\'line-search\', module=\'line_search\', modules=solver_modules,\n            max_iterations=ls_max_iterations, accept_ratio=ls_accept_ratio, mode=ls_mode,\n            parameter=ls_parameter, unroll_loop=ls_unroll_loop\n        )\n\n    def tf_step(self, variables, arguments, fn_loss, fn_reference=None, **kwargs):\n        augmented_arguments = dict(arguments)\n\n        if fn_reference is not None:\n            # Set reference to compare with at each step, in case of a comparative loss.\n            reference = fn_reference(**arguments)  # ?????????????????????????????????????????????\n\n            assert \'reference\' not in augmented_arguments\n            augmented_arguments[\'reference\'] = reference\n\n        # Negative value since line search maximizes.\n        loss_before = -fn_loss(**augmented_arguments)\n\n        with tf.control_dependencies(control_inputs=(loss_before,)):\n            deltas = self.optimizer.step(\n                variables=variables, arguments=arguments, fn_loss=fn_loss,  # no reference here?\n                return_estimated_improvement=True, **kwargs\n            )\n\n            if isinstance(deltas, tuple):\n                # If \'return_estimated_improvement\' argument exists.\n                if len(deltas) != 2:\n                    raise TensorforceError(""Unexpected output of internal optimizer."")\n                deltas, estimated_improvement = deltas\n                # Negative value since line search maximizes.\n                estimated_improvement = -estimated_improvement\n            else:\n                estimated_improvement = None\n\n        with tf.control_dependencies(control_inputs=deltas):\n            # Negative value since line search maximizes.\n            loss_step = -fn_loss(**augmented_arguments)\n\n        with tf.control_dependencies(control_inputs=(loss_step,)):\n\n            def evaluate_step(deltas):\n                with tf.control_dependencies(control_inputs=deltas):\n                    applied = self.apply_step(variables=variables, deltas=deltas)\n                with tf.control_dependencies(control_inputs=(applied,)):\n                    # Negative value since line search maximizes.\n                    return -fn_loss(**augmented_arguments)\n\n            return self.solver.solve(\n                fn_x=evaluate_step, x_init=deltas, base_value=loss_before, target_value=loss_step,\n                estimated_improvement=estimated_improvement\n            )\n'"
tensorforce/core/optimizers/plus.py,2,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nimport tensorforce.core\nfrom tensorforce.core.optimizers import Optimizer\n\n\nclass Plus(Optimizer):\n    """"""\n    Additive combination of two optimizers (specification key: `plus`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer1 (specification): First optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        optimizer2 (specification): Second optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, optimizer1, optimizer2, summary_labels=None):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        self.optimizer1 = self.add_module(\n            name=\'first-optimizer\', module=optimizer1, modules=tensorforce.core.optimizer_modules\n        )\n        self.optimizer2 = self.add_module(\n            name=\'second-optimizer\', module=optimizer2, modules=tensorforce.core.optimizer_modules\n        )\n\n    def tf_step(self, **kwargs):\n        deltas1 = self.optimizer1.step(**kwargs)\n\n        with tf.control_dependencies(control_inputs=deltas1):\n            deltas2 = self.optimizer2.step(**kwargs)\n\n        with tf.control_dependencies(control_inputs=deltas2):\n            return [delta1 + delta2 for delta1, delta2 in zip(deltas1, deltas2)]\n'"
tensorforce/core/optimizers/subsampling_step.py,20,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import Module, parameter_modules\nfrom tensorforce.core.optimizers import MetaOptimizer\n\n\nclass SubsamplingStep(MetaOptimizer):\n    """"""\n    Subsampling-step meta optimizer, which randomly samples a subset of batch instances before\n    applying the given optimizer (specification key: `subsampling_step`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (specification): Optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        fraction (parameter, 0.0 <= float <= 1.0): Fraction of batch timesteps to subsample\n            (<span style=""color:#C00000""><b>required</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, optimizer, fraction, summary_labels=None):\n        super().__init__(name=name, optimizer=optimizer, summary_labels=summary_labels)\n\n        self.fraction = self.add_module(\n            name=\'fraction\', module=fraction, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0, max_value=1.0\n        )\n\n    def tf_step(self, variables, arguments, **kwargs):\n        # # Get some (batched) argument to determine batch size.\n        # arguments_iter = iter(arguments.values())\n        # some_argument = next(arguments_iter)\n\n        # try:\n        #     while not isinstance(some_argument, tf.Tensor) or util.rank(x=some_argument) == 0:\n        #         if isinstance(some_argument, dict):\n        #             if some_argument:\n        #                 arguments_iter = iter(some_argument.values())\n        #             some_argument = next(arguments_iter)\n        #         elif isinstance(some_argument, list):\n        #             if some_argument:\n        #                 arguments_iter = iter(some_argument)\n        #             some_argument = next(arguments_iter)\n        #         elif some_argument is None or util.rank(x=some_argument) == 0:\n        #             # Non-batched argument\n        #             some_argument = next(arguments_iter)\n        #         else:\n        #             raise TensorforceError(""Invalid argument type."")\n        # except StopIteration:\n        #     raise TensorforceError(""Invalid argument type."")\n\n        some_argument = arguments[\'reward\']\n\n        if util.tf_dtype(dtype=\'long\') in (tf.int32, tf.int64):\n            batch_size = tf.shape(input=some_argument, out_type=util.tf_dtype(dtype=\'long\'))[0]\n        else:\n            batch_size = tf.dtypes.cast(\n                x=tf.shape(input=some_argument)[0], dtype=util.tf_dtype(dtype=\'long\')\n            )\n        fraction = self.fraction.value()\n        num_samples = fraction * tf.dtypes.cast(x=batch_size, dtype=util.tf_dtype(\'float\'))\n        num_samples = tf.dtypes.cast(x=num_samples, dtype=util.tf_dtype(\'long\'))\n        one = tf.constant(value=1, dtype=util.tf_dtype(\'long\'))\n        num_samples = tf.maximum(x=num_samples, y=one)\n        indices = tf.random.uniform(\n            shape=(num_samples,), maxval=batch_size, dtype=util.tf_dtype(dtype=\'long\')\n        )\n        function = (lambda x: tf.gather(params=x, indices=indices))\n        subsampled_arguments = util.fmap(function=function, xs=arguments)\n\n        dependency_starts = Module.retrieve_tensor(name=\'dependency_starts\')\n        dependency_lengths = Module.retrieve_tensor(name=\'dependency_lengths\')\n        subsampled_starts = tf.gather(params=dependency_starts, indices=indices)\n        subsampled_lengths = tf.gather(params=dependency_lengths, indices=indices)\n        trivial_dependencies = tf.reduce_all(\n            input_tensor=tf.math.equal(x=dependency_lengths, y=one), axis=0\n        )\n\n        def dependency_state_indices():\n            fold = (lambda acc, args: tf.concat(\n                values=(acc, tf.range(start=args[0], limit=(args[0] + args[1]))), axis=0\n            ))\n            return tf.foldl(\n                fn=fold, elems=(subsampled_starts, subsampled_lengths), initializer=indices[:0],\n                parallel_iterations=10, back_prop=False, swap_memory=False\n            )\n\n        states_indices = self.cond(\n            pred=trivial_dependencies, true_fn=(lambda: indices), false_fn=dependency_state_indices\n        )\n        function = (lambda x: tf.gather(params=x, indices=states_indices))\n        subsampled_arguments[\'states\'] = util.fmap(function=function, xs=arguments[\'states\'])\n\n        subsampled_starts = tf.math.cumsum(x=subsampled_lengths, exclusive=True)\n        Module.update_tensors(\n            dependency_starts=subsampled_starts, dependency_lengths=subsampled_lengths\n        )\n\n        deltas = self.optimizer.step(variables=variables, arguments=subsampled_arguments, **kwargs)\n\n        Module.update_tensors(\n            dependency_starts=dependency_starts, dependency_lengths=dependency_lengths\n        )\n\n        return deltas\n'"
tensorforce/core/optimizers/synchronization.py,6,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import Module, parameter_modules\nfrom tensorforce.core.optimizers import Optimizer\n\n\nclass Synchronization(Optimizer):\n    """"""\n    Synchronization optimizer, which updates variables periodically to the value of a corresponding  \n    set of source variables (specification key: `synchronization`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (specification): Optimizer configuration\n            (<span style=""color:#C00000""><b>required</b></span>).\n        sync_frequency (parameter, int >= 1): Interval between updates which also perform a\n            synchronization step (<span style=""color:#00C000""><b>default</b></span>: every update).\n        update_weight (parameter, 0.0 <= float <= 1.0): Update weight\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(self, name, sync_frequency=1, update_weight=1.0, summary_labels=None):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        self.sync_frequency = self.add_module(\n            name=\'sync-frequency\', module=sync_frequency, modules=parameter_modules, dtype=\'long\',\n            min_value=1\n        )\n\n        self.update_weight = self.add_module(\n            name=\'update-weight\', module=update_weight, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0, max_value=1.0\n        )\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        self.next_sync = self.add_variable(\n            name=\'next-sync\', dtype=\'long\', shape=(), is_trainable=False\n        )\n\n    def tf_step(self, variables, source_variables, **kwargs):\n        assert all(\n            util.shape(source) == util.shape(target)\n            for source, target in zip(source_variables, variables)\n        )\n\n        one = tf.constant(value=1, dtype=util.tf_dtype(dtype=\'long\'))\n\n        def apply_sync():\n            next_sync_updated = self.next_sync.assign(\n                value=self.sync_frequency.value(), read_value=False\n            )\n\n            with tf.control_dependencies(control_inputs=(next_sync_updated,)):\n                update_weight = self.update_weight.value()\n                deltas = list()\n                for source_variable, target_variable in zip(source_variables, variables):\n                    delta = update_weight * (source_variable - target_variable)\n                    deltas.append(delta)\n                applied = self.apply_step(variables=variables, deltas=deltas)\n\n            with tf.control_dependencies(control_inputs=(applied,)):\n                # Trivial operation to enforce control dependency\n                return util.fmap(function=util.identity_operation, xs=deltas)\n\n        def no_sync():\n            next_sync_updated = self.next_sync.assign_sub(delta=one, read_value=False)\n\n            with tf.control_dependencies(control_inputs=(next_sync_updated,)):\n                deltas = list()\n                for variable in variables:\n                    delta = tf.zeros(\n                        shape=util.shape(variable), dtype=util.tf_dtype(dtype=\'float\')\n                    )\n                    deltas.append(delta)\n                return deltas\n\n        skip_sync = tf.math.greater(x=self.next_sync, y=one)\n\n        return self.cond(pred=skip_sync, true_fn=no_sync, false_fn=apply_sync)\n'"
tensorforce/core/optimizers/tf_optimizer.py,18,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom functools import partial\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.optimizers import Optimizer\n\n\ntensorflow_optimizers = dict(\n    adadelta=tf.keras.optimizers.Adadelta,\n    adagrad=tf.keras.optimizers.Adagrad,\n    adam=tf.keras.optimizers.Adam,\n    adamax=tf.keras.optimizers.Adamax,\n    ftrl=tf.keras.optimizers.Ftrl,\n    nadam=tf.keras.optimizers.Nadam,\n    rmsprop=tf.keras.optimizers.RMSprop,\n    sgd=tf.keras.optimizers.SGD\n)\n\n\ntry:\n    import tensorflow_addons as tfa\n\n    tensorflow_optimizers[\'adamw\'] = tfa.optimizers.AdamW\n    tensorflow_optimizers[\'lazyadam\'] = tfa.optimizers.LazyAdam\n    tensorflow_optimizers[\'radam\'] = tfa.optimizers.RectifiedAdam\n    tensorflow_optimizers[\'ranger\'] = (lambda **kwargs: tfa.optimizers.Lookahead(\n        optimizer=tfa.optimizers.RectifiedAdam(**kwargs), name=kwargs[\'name\']\n    ))\n    tensorflow_optimizers[\'sgdw\'] = tfa.optimizers.SGDW\nexcept ModuleNotFoundError:\n    pass\n\n\nclass TFOptimizer(Optimizer):\n    """"""\n    TensorFlow optimizer (specification key: `tf_optimizer`, `adadelta`, `adagrad`, `adam`,\n    `adamax`, `adamw`, `ftrl`, `lazyadam`, `nadam`, `radam`, `ranger`, `rmsprop`, `sgd`, `sgdw`)\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        optimizer (`adadelta` | `adagrad` | `adam` | `adamax` | `adamw` | `ftrl` | `lazyadam` | `nadam` | `radam` | `ranger` | `rmsprop` | `sgd` | `sgdw`):\n            TensorFlow optimizer name, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/optimizers>`__\n            and `TensorFlow Addons docs\n            <https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers>`__\n            (<span style=""color:#C00000""><b>required</b></span> unless given by specification key).\n        learning_rate (parameter, float >= 0.0): Learning rate\n            (<span style=""color:#00C000""><b>default</b></span>: 3e-4).\n        gradient_norm_clipping (parameter, float >= 0.0): Clip gradients by the ratio of the sum\n            of their norms (<span style=""color:#00C000""><b>default</b></span>: 1.0).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Arguments for the TensorFlow optimizer, special values ""decoupled_weight_decay"",\n            ""lookahead"" and ""moving_average"", see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/keras/optimizers>`__\n            and `TensorFlow Addons docs\n            <https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers>`__.\n    """"""\n\n    def __init__(\n        self, name, optimizer, learning_rate=3e-4, gradient_norm_clipping=1.0, summary_labels=None,\n        **kwargs\n    ):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        assert optimizer in tensorflow_optimizers\n        self.optimizer = tensorflow_optimizers[optimizer]\n        self.learning_rate = self.add_module(\n            name=\'learning-rate\', module=learning_rate, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0\n        )\n        self.gradient_norm_clipping = self.add_module(\n            name=\'gradient-norm-clipping\', module=gradient_norm_clipping,\n            modules=parameter_modules, dtype=\'float\', min_value=0.0\n        )\n        self.optimizer_kwargs = kwargs\n\n        if \'decoupled_weight_decay\' in self.optimizer_kwargs:\n            decoupled_weight_decay = self.optimizer_kwargs.pop(\'decoupled_weight_decay\')\n            self.optimizer = partial(\n                tfa.optimizers.extend_with_decoupled_weight_decay(base_optimizer=self.optimizer),\n                weight_decay=decoupled_weight_decay\n            )\n        if \'lookahead\' in self.optimizer_kwargs:\n            lookahead = self.optimizer_kwargs.pop(\'lookahead\')\n            if isinstance(lookahead, dict) or lookahead is True:\n                if lookahead is True:\n                    lookahead = dict()\n                self.optimizer = util.compose(\n                    function1=partial(tfa.optimizers.Lookahead, name=self.name, **lookahead),\n                    function2=self.optimizer\n                )\n        if \'moving_average\' in self.optimizer_kwargs:\n            moving_avg = self.optimizer_kwargs.pop(\'moving_average\')\n            if isinstance(moving_avg, dict) or moving_avg is True:\n                if moving_avg is True:\n                    moving_avg = dict()\n                self.optimizer = util.compose(\n                    function1=partial(tfa.optimizers.MovingAverage, name=self.name, **moving_avg),\n                    function2=self.optimizer\n                )\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        self.optimizer = self.optimizer(\n            learning_rate=self.learning_rate.value, name=self.name, **self.optimizer_kwargs\n        )\n\n    def tf_step(self, variables, arguments, fn_loss, fn_initial_gradients=None, **kwargs):\n        arguments = util.fmap(function=tf.stop_gradient, xs=arguments)\n        loss = fn_loss(**arguments)\n\n        # Force loss value and attached control flow to be computed.\n        with tf.control_dependencies(control_inputs=(loss,)):\n            # Trivial operation to enforce control dependency\n            previous_variables = util.fmap(function=util.identity_operation, xs=variables)\n\n        # Get variables before update.\n        with tf.control_dependencies(control_inputs=previous_variables):\n            # applied = self.optimizer.minimize(loss=loss, var_list=variables)\n            # grads_and_vars = self.optimizer.compute_gradients(loss=loss, var_list=variables)\n            # gradients, variables = zip(*grads_and_vars)\n            if fn_initial_gradients is None:\n                initial_gradients = None\n            else:\n                initial_gradients = fn_initial_gradients(**arguments)\n                initial_gradients = tf.stop_gradient(input=initial_gradients)\n\n            gradients = tf.gradients(ys=loss, xs=variables, grad_ys=initial_gradients)\n            assertions = [\n                tf.debugging.assert_all_finite(x=gradient, message=""Finite gradients check."")\n                for gradient in gradients\n            ]\n\n        with tf.control_dependencies(control_inputs=assertions):\n            gradient_norm_clipping = self.gradient_norm_clipping.value()\n            gradients, gradient_norm = tf.clip_by_global_norm(\n                t_list=gradients, clip_norm=gradient_norm_clipping\n            )\n            gradients = self.add_summary(\n                label=\'update-norm\', name=\'gradient-norm-unclipped\', tensor=gradient_norm,\n                pass_tensors=gradients\n            )\n\n            applied = self.optimizer.apply_gradients(grads_and_vars=zip(gradients, variables))\n\n        # Return deltas after actually having change the variables.\n        with tf.control_dependencies(control_inputs=(applied,)):\n            return [\n                variable - previous_variable\n                for variable, previous_variable in zip(variables, previous_variables)\n            ]\n\n    def get_variables(self, only_trainable=False, only_saved=False):\n        optimizer = self.optimizer\n        while True:\n            for variable in optimizer.weights:\n                name = \'/\' + self.name + \'/\'\n                if name in variable.name:\n                    name = variable.name[variable.name.rindex(name) + len(name): -2]\n                else:\n                    name = variable.name[variable.name.rindex(\'/\') + 1: -2]\n                self.variables[name] = variable\n            for name, value in optimizer._hyper.items():\n                if isinstance(value, tf.Variable):\n                    self.variables[name] = value\n            if hasattr(optimizer, \'_ema\'):\n                for variable in optimizer._ema._averages.values():\n                    assert variable.name.startswith(\'agent/\') and \\\n                        variable.name.endswith(\'/ExponentialMovingAverage:0\')\n                    self.variables[variable.name[:-2]] = variable\n            if hasattr(optimizer, \'_optimizer\'):\n                optimizer = optimizer._optimizer\n            else:\n                break\n\n        variables = super().get_variables(only_trainable=only_trainable, only_saved=only_saved)\n\n        return variables\n'"
tensorforce/core/parameters/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.parameters.parameter import Parameter\n\nfrom tensorforce.core.parameters.constant import Constant\nfrom tensorforce.core.parameters.decaying import Decaying\nfrom tensorforce.core.parameters.ornstein_uhlenbeck import OrnsteinUhlenbeck\nfrom tensorforce.core.parameters.piecewise_constant import PiecewiseConstant\nfrom tensorforce.core.parameters.random import Random\n\n\nparameter_modules = dict(\n    constant=Constant, decaying=Decaying, default=Constant, ornstein_uhlenbeck=OrnsteinUhlenbeck,\n    piecewise_constant=PiecewiseConstant, random=Random\n)\n\n\n__all__ = [\n    \'Constant\', \'Decaying\', \'OrnsteinUhlenbeck\', \'Parameter\', \'parameter_modules\',\n    \'PiecewiseConstant\', \'Random\'\n]\n'"
tensorforce/core/parameters/constant.py,1,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core.parameters import Parameter\n\n\nclass Constant(Parameter):\n    """"""\n    Constant hyperparameter.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        value (dtype-dependent): Constant hyperparameter value\n            (<span style=""color:#C00000""><b>required</b></span>).\n        dtype (""bool"" | ""int"" | ""long"" | ""float""): Tensor type\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        min_value (dtype-compatible value): Lower parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        max_value (dtype-compatible value): Upper parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    # Argument \'value\' first for default specification\n    def __init__(self, name, value, dtype, min_value=None, max_value=None, summary_labels=None):\n        if isinstance(value, bool):\n            if dtype != \'bool\':\n                raise TensorforceError.unexpected()\n        elif isinstance(value, int):\n            if dtype not in (\'int\', \'long\'):\n                raise TensorforceError.unexpected()\n        elif isinstance(value, float):\n            if dtype != \'float\':\n                raise TensorforceError.unexpected()\n        else:\n            raise TensorforceError.unexpected()\n\n        self.constant_value = value\n\n        super().__init__(\n            name=name, dtype=dtype, min_value=min_value, max_value=max_value,\n            summary_labels=summary_labels\n        )\n\n    def min_value(self):\n        return self.constant_value\n\n    def max_value(self):\n        return self.constant_value\n\n    def final_value(self):\n        return self.constant_value\n\n    def parameter_value(self, step):\n        parameter = tf.constant(value=self.constant_value, dtype=util.tf_dtype(dtype=self.dtype))\n\n        return parameter\n'"
tensorforce/core/parameters/decaying.py,12,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import Module\nfrom tensorforce.core.parameters import Parameter\n\n\nclass Decaying(Parameter):\n    """"""\n    Decaying hyperparameter.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        dtype (""bool"" | ""int"" | ""long"" | ""float""): Tensor type\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        unit (""timesteps"" | ""episodes"" | ""updates""): Unit of decay schedule\n            (<span style=""color:#C00000""><b>required</b></span>).\n        decay (""cosine"" | ""cosine_restarts"" | ""exponential"" | ""inverse_time"" | ""linear_cosine"" | ""linear_cosine_noisy"" | ""polynomial""):\n            Decay type, see\n            `TensorFlow docs <https://www.tensorflow.org/api_docs/python/tf/train>`__\n            (<span style=""color:#C00000""><b>required</b></span>).\n        initial_value (float): Initial value\n            (<span style=""color:#C00000""><b>required</b></span>).\n        decay_steps (long): Number of decay steps\n            (<span style=""color:#C00000""><b>required</b></span>).\n        increasing (bool): Whether to subtract the decayed value from 1.0\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        inverse (bool): Whether to take the inverse of the decayed value\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        scale (float): Scaling factor for (inverse) decayed value\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).\n        min_value (dtype-compatible value): Lower parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        max_value (dtype-compatible value): Upper parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (""all"" | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments depend on decay mechanism.<br>\n            Cosine decay:\n            <ul>\n            <li><b>alpha</b> (<i>float</i>) &ndash; Minimum learning rate value as a fraction of\n            learning_rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).</li>\n            </ul>\n            Cosine decay with restarts:\n            <ul>\n            <li><b>t_mul</b> (<i>float</i>) &ndash; Used to derive the number of iterations in the\n            i-th period\n            (<span style=""color:#00C000""><b>default</b></span>: 2.0).</li>\n            <li><b>m_mul</b> (<i>float</i>) &ndash; Used to derive the initial learning rate of the\n            i-th period\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).</li>\n            <li><b>alpha</b> (<i>float</i>) &ndash; Minimum learning rate value as a fraction of\n            the learning_rate\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).</li>\n            </ul>\n            Exponential decay:\n            <ul>\n            <li><b>decay_rate</b> (<i>float</i>) &ndash; Decay rate\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>staircase</b> (<i>bool</i>) &ndash; Whether to apply decay in a discrete\n            staircase, as opposed to continuous, fashion.\n            (<span style=""color:#00C000""><b>default</b></span>: false).</li>\n            </ul>\n            Inverse time decay:\n            <ul>\n            <li><b>decay_rate</b> (<i>float</i>) &ndash; Decay rate\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>staircase</b> (<i>bool</i>) &ndash; Whether to apply decay in a discrete\n            staircase, as opposed to continuous, fashion.\n            (<span style=""color:#00C000""><b>default</b></span>: false).</li>\n            </ul>\n            Linear cosine decay:\n            <ul>\n            <li><b>num_periods</b> (<i>float</i>) &ndash; Number of periods in the cosine part of\n            the decay\n            (<span style=""color:#00C000""><b>default</b></span>: 0.5).</li>\n            <li><b>alpha</b> (<i>float</i>) &ndash; Alpha value\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).</li>\n            <li><b>beta</b> (<i>float</i>) &ndash; Beta value\n            (<span style=""color:#00C000""><b>default</b></span>: 0.001).</li>\n            </ul>\n            Natural exponential decay:\n            <ul>\n            <li><b>decay_rate</b> (<i>float</i>) &ndash; Decay rate\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>staircase</b> (<i>bool</i>) &ndash; Whether to apply decay in a discrete\n            staircase, as opposed to continuous, fashion.\n            (<span style=""color:#00C000""><b>default</b></span>: false).</li>\n            </ul>\n            Noisy linear cosine decay:\n            <ul>\n            <li><b>initial_variance</b> (<i>float</i>) &ndash; Initial variance for the noise\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).</li>\n            <li><b>variance_decay</b> (<i>float</i>) &ndash; Decay for the noise\'s variance\n            (<span style=""color:#00C000""><b>default</b></span>: 0.55).</li>\n            <li><b>num_periods</b> (<i>float</i>) &ndash; Number of periods in the cosine part of\n            the decay\n            (<span style=""color:#00C000""><b>default</b></span>: 0.5).</li>\n            <li><b>alpha</b> (<i>float</i>) &ndash; Alpha value\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).</li>\n            <li><b>beta</b> (<i>float</i>) &ndash; Beta value\n            (<span style=""color:#00C000""><b>default</b></span>: 0.001).</li>\n            </ul>\n            Polynomial decay:\n            <ul>\n            <li><b>final_value</b> (<i>float</i>) &ndash; Final value\n            (<span style=""color:#C00000""><b>required</b></span>).</li>\n            <li><b>power</b> (<i>float</i>) &ndash; Power of polynomial\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0, thus linear).</li>\n            <li><b>cycle</b> (<i>bool</i>) &ndash; Whether to cycle beyond decay_steps\n            (<span style=""color:#00C000""><b>default</b></span>: false).</li>\n            </ul>\n    """"""\n\n    def __init__(\n        self, name, dtype, unit, decay, initial_value, decay_steps, increasing=False,\n        inverse=False, scale=1.0, min_value=None, max_value=None, summary_labels=None, **kwargs\n    ):\n        assert unit in (\'timesteps\', \'episodes\', \'updates\')\n        assert decay in (\n            \'cosine\', \'cosine_restarts\', \'exponential\', \'inverse_time\', \'linear_cosine\',\n            \'linear_cosine_noisy\', \'polynomial\'\n        )\n        assert isinstance(initial_value, float)\n        assert isinstance(decay_steps, int) or decay_steps % 10.0 == 0.0\n\n        self.decay = decay\n        self.initial_value = initial_value\n        self.decay_steps = int(decay_steps)\n        self.increasing = increasing\n        self.inverse = inverse\n        self.scale = scale\n        self.kwargs = kwargs\n\n        super().__init__(\n            name=name, dtype=dtype, unit=unit, min_value=min_value, max_value=max_value,\n            summary_labels=summary_labels\n        )\n\n    def min_value(self):\n        if self.decay == \'cosine\' or self.decay == \'cosine_restarts\':\n            assert 0.0 <= self.kwargs.get(\'alpha\', 0.0) <= 1.0\n            if self.initial_value >= 0.0:\n                min_value = self.initial_value * self.kwargs.get(\'alpha\', 0.0)\n                max_value = self.initial_value\n            else:\n                min_value = self.initial_value\n                max_value = self.initial_value * self.kwargs.get(\'alpha\', 0.0)\n\n        elif self.decay == \'exponential\' or self.decay == \'inverse_time\':\n            assert 0.0 <= self.kwargs[\'decay_rate\'] <= 1.0\n            if self.kwargs[\'decay_rate\'] == 1.0:\n                min_value = max_value = self.initial_value\n            elif self.initial_value >= 0.0:\n                min_value = 0.0\n                max_value = self.initial_value\n            else:\n                min_value = self.initial_value\n                max_value = 0.0\n\n        elif self.decay == \'linear_cosine\' or self.decay == \'linear_cosine_noisy\':\n            assert 0.0 <= self.kwargs.get(\'alpha\', 0.0) <= 1.0\n            assert 0.0 <= self.kwargs.get(\'beta\', 0.0) <= 1.0\n            if self.initial_value >= 0.0:\n                min_value = self.initial_value * self.kwargs.get(\'beta\', 0.001)\n                max_value = self.initial_value * (\n                    1.0 + self.kwargs.get(\'alpha\', 0.0) + self.kwargs.get(\'beta\', 0.001)\n                )\n            else:\n                min_value = self.initial_value * (\n                    1.0 + self.kwargs.get(\'alpha\', 0.0) + self.kwargs.get(\'beta\', 0.001)\n                )\n                max_value = self.initial_value * self.kwargs.get(\'beta\', 0.001)\n\n        elif self.decay == \'polynomial\':\n            if self.kwargs.get(\'power\', 1.0) == 0.0:\n                min_value = max_value = self.initial_value\n            elif self.initial_value >= self.kwargs[\'final_value\']:\n                min_value = self.kwargs[\'final_value\']\n                max_value = self.initial_value\n            else:\n                min_value = self.initial_value\n                max_value = self.kwargs[\'final_value\']\n\n        assert min_value <= max_value\n\n        if self.increasing:\n            assert 0.0 <= min_value <= max_value <= 1.0\n            min_value, max_value = 1.0 - max_value, 1.0 - min_value\n\n        if self.inverse:\n            assert 0.0 < min_value <= max_value\n            min_value, max_value = 1.0 / max_value, 1.0 / min_value\n\n        if self.scale == 1.0:\n            pass\n        elif self.scale >= 0.0:\n            min_value, max_value = self.scale * min_value, self.scale * max_value\n        else:\n            min_value, max_value = self.scale * max_value, self.scale * min_value\n\n        return util.py_dtype(dtype=self.dtype)(min_value)\n\n    def max_value(self):\n        if self.decay == \'cosine\' or self.decay == \'cosine_restarts\':\n            assert 0.0 <= self.kwargs.get(\'alpha\', 0.0) <= 1.0\n            if self.initial_value >= 0.0:\n                min_value = self.initial_value * self.kwargs.get(\'alpha\', 0.0)\n                max_value = self.initial_value\n            else:\n                min_value = self.initial_value\n                max_value = self.initial_value * self.kwargs.get(\'alpha\', 0.0)\n\n        elif self.decay == \'exponential\' or self.decay == \'inverse_time\':\n            assert 0.0 <= self.kwargs[\'decay_rate\'] <= 1.0\n            if self.kwargs[\'decay_rate\'] == 1.0:\n                min_value = max_value = self.initial_value\n            elif self.initial_value >= 0.0:\n                min_value = 0.0\n                max_value = self.initial_value\n            else:\n                min_value = self.initial_value\n                max_value = 0.0\n\n        elif self.decay == \'linear_cosine\' or self.decay == \'linear_cosine_noisy\':\n            assert 0.0 <= self.kwargs.get(\'alpha\', 0.0) <= 1.0\n            assert 0.0 <= self.kwargs.get(\'beta\', 0.0) <= 1.0\n            if self.initial_value >= 0.0:\n                min_value = self.initial_value * self.kwargs.get(\'beta\', 0.001)\n                max_value = self.initial_value * (\n                    1.0 + self.kwargs.get(\'alpha\', 0.0) + self.kwargs.get(\'beta\', 0.001)\n                )\n            else:\n                min_value = self.initial_value * (\n                    1.0 + self.kwargs.get(\'alpha\', 0.0) + self.kwargs.get(\'beta\', 0.001)\n                )\n                max_value = self.initial_value * self.kwargs.get(\'beta\', 0.001)\n\n        elif self.decay == \'polynomial\':\n            if self.kwargs.get(\'power\', 1.0) == 0.0:\n                min_value = max_value = self.initial_value\n            elif self.initial_value >= self.kwargs[\'final_value\']:\n                min_value = self.kwargs[\'final_value\']\n                max_value = self.initial_value\n            else:\n                min_value = self.initial_value\n                max_value = self.kwargs[\'final_value\']\n\n        assert min_value <= max_value\n\n        if self.increasing:\n            assert 0.0 <= min_value <= max_value <= 1.0\n            min_value, max_value = 1.0 - max_value, 1.0 - min_value\n\n        if self.inverse:\n            assert 0.0 < min_value <= max_value\n            min_value, max_value = 1.0 / max_value, 1.0 / min_value\n\n        if self.scale == 1.0:\n            pass\n        elif self.scale >= 0.0:\n            min_value, max_value = self.scale * min_value, self.scale * max_value\n        else:\n            min_value, max_value = self.scale * max_value, self.scale * min_value\n\n        return util.py_dtype(dtype=self.dtype)(max_value)\n\n    def final_value(self):\n        if self.decay == \'cosine\' or self.decay == \'cosine_restarts\':\n            assert 0.0 <= self.kwargs[\'decay_rate\'] <= 1.0\n            value = self.initial_value * self.kwargs.get(\'alpha\', 0.0)\n\n        elif self.decay == \'exponential\' or self.decay == \'inverse_time\':\n            assert 0.0 <= self.kwargs[\'decay_rate\'] <= 1.0\n            if self.kwargs[\'decay_rate\'] == 1.0:\n                value = self.initial_value\n            else:\n                value = 0.0\n\n        elif self.decay == \'linear_cosine\' or self.decay == \'linear_cosine_noisy\':\n            assert 0.0 <= self.kwargs.get(\'alpha\', 0.0) <= 1.0\n            assert 0.0 <= self.kwargs.get(\'beta\', 0.0) <= 1.0\n            value = self.initial_value * self.kwargs.get(\'beta\', 0.001)\n\n        elif self.decay == \'polynomial\':\n            if self.kwargs.get(\'power\', 1.0) == 0.0:\n                value = self.initial_value\n            else:\n                value = self.kwargs[\'final_value\']\n\n        if self.increasing:\n            assert 0.0 <= value <= 1.0\n            value = 1.0 - value\n\n        if self.inverse:\n            assert value > 0.0\n            value = 1.0 / value\n\n        if self.scale != 1.0:\n            value = value * self.scale\n\n        return util.py_dtype(dtype=self.dtype)(value)\n\n\n    def parameter_value(self, step):\n        initial_value = tf.constant(value=self.initial_value, dtype=util.tf_dtype(dtype=\'float\'))\n\n        if self.decay == \'cosine\':\n            assert 0.0 <= self.kwargs.get(\'alpha\', 0.0) <= 1.0\n            parameter = tf.keras.experimental.CosineDecay(\n                initial_learning_rate=initial_value, decay_steps=self.decay_steps,\n                alpha=self.kwargs.get(\'alpha\', 0.0)\n            )(step=step)\n\n        elif self.decay == \'cosine_restarts\':\n            assert 0.0 <= self.kwargs.get(\'alpha\', 0.0) <= 1.0\n            parameter = tf.keras.experimental.CosineDecayRestarts(\n                initial_learning_rate=initial_value, first_decay_steps=self.decay_steps,\n                t_mul=self.kwargs.get(\'t_mul\', 2.0), m_mul=self.kwargs.get(\'m_mul\', 1.0),\n                alpha=self.kwargs.get(\'alpha\', 0.0)\n            )(step=step)\n\n        elif self.decay == \'exponential\':\n            assert self.kwargs[\'decay_rate\'] >= 0.0\n            parameter = tf.keras.optimizers.schedules.ExponentialDecay(\n                initial_learning_rate=initial_value, decay_steps=self.decay_steps,\n                decay_rate=self.kwargs[\'decay_rate\'], staircase=self.kwargs.get(\'staircase\', False)\n            )(step=step)\n\n        elif self.decay == \'inverse_time\':\n            assert self.kwargs[\'decay_rate\'] >= 0.0\n            parameter = tf.keras.optimizers.schedules.InverseTimeDecay(\n                initial_learning_rate=initial_value, decay_steps=self.decay_steps,\n                decay_rate=self.kwargs[\'decay_rate\'], staircase=self.kwargs.get(\'staircase\', False)\n            )(step=step)\n\n        elif self.decay == \'linear_cosine\':\n            assert self.kwargs.get(\'beta\', 0.001) >= 0.0\n            parameter = tf.keras.experimental.LinearCosineDecay(\n                initial_learning_rate=initial_value, decay_steps=self.decay_steps,\n                num_periods=self.kwargs.get(\'num_periods\', 0.5),\n                alpha=self.kwargs.get(\'alpha\', 0.0), beta=self.kwargs.get(\'beta\', 0.001)\n            )(step=step)\n\n        elif self.decay == \'linear_cosine_noisy\':\n            assert self.kwargs.get(\'beta\', 0.001) >= 0.0\n            parameter = tf.keras.experimental.NoisyLinearCosineDecay(\n                initial_learning_rate=initial_value, decay_steps=self.decay_steps,\n                initial_variance=self.kwargs.get(\'initial_variance\', 1.0),\n                variance_decay=self.kwargs.get(\'variance_decay\', 0.55),\n                num_periods=self.kwargs.get(\'num_periods\', 0.5),\n                alpha=self.kwargs.get(\'alpha\', 0.0), beta=self.kwargs.get(\'beta\', 0.001)\n            )(step=step)\n\n        elif self.decay == \'polynomial\':\n            assert self.kwargs[\'power\'] >= 0.0\n            parameter = tf.keras.optimizers.schedules.PolynomialDecay(\n                initial_learning_rate=initial_value, decay_steps=self.decay_steps,\n                end_learning_rate=self.kwargs[\'final_value\'], power=self.kwargs.get(\'power\', 1.0),\n                cycle=self.kwargs.get(\'cycle\', False)\n            )(step=step)\n\n        if self.increasing:\n            one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n            parameter = one - parameter\n\n        if self.inverse:\n            one = tf.constant(value=1.0, dtype=util.tf_dtype(dtype=\'float\'))\n            parameter = one / parameter\n\n        if self.scale != 1.0:\n            scale = tf.constant(value=self.scale, dtype=util.tf_dtype(dtype=\'float\'))\n            parameter = parameter * scale\n\n        if self.dtype != \'float\':\n            parameter = tf.dtypes.cast(x=parameter, dtype=util.tf_dtype(dtype=self.dtype))\n\n        return parameter\n'"
tensorforce/core/parameters/ornstein_uhlenbeck.py,4,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.parameters import Parameter\n\n\nclass OrnsteinUhlenbeck(Parameter):\n    """"""\n    Ornstein-Uhlenbeck process.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        dtype (""bool"" | ""int"" | ""long"" | ""float""): Tensor type\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        theta (float > 0.0): Theta value\n            (<span style=""color:#00C000""><b>default</b></span>: 0.15).\n        sigma (float > 0.0): Sigma value\n            (<span style=""color:#00C000""><b>default</b></span>: 0.3).\n        mu (float): Mu value\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        absolute (bool): Absolute value\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        min_value (dtype-compatible value): Lower parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        max_value (dtype-compatible value): Upper parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, dtype, theta=0.15, sigma=0.3, mu=0.0, absolute=False, min_value=None,\n        max_value=None, summary_labels=None\n    ):\n        self.theta = theta\n        self.mu = mu\n        self.sigma = sigma\n        self.absolute = absolute\n\n        super().__init__(\n            name=name, dtype=dtype, min_value=min_value, max_value=max_value,\n            summary_labels=summary_labels\n        )\n\n    def min_value(self):\n        if self.absolute:\n            return util.py_dtype(dtype=self.dtype)(0.0)\n        else:\n            super().min_value()\n\n    def final_value(self):\n        return util.py_dtype(dtype=self.dtype)(self.mu)\n\n    def parameter_value(self, step):\n        self.process = self.add_variable(\n            name=\'process\', dtype=\'float\', shape=(), is_trainable=False, initializer=self.mu\n        )\n\n        delta = self.theta * (self.mu - self.process) + self.sigma * tf.random.normal(shape=())\n        if self.absolute:\n            parameter = self.process.assign(value=tf.math.abs(x=(self.process + delta)))\n        else:\n            parameter = self.process.assign_add(delta=delta)\n\n        if self.dtype != \'float\':\n            parameter = tf.dtypes.cast(x=parameter, dtype=util.tf_dtype(dtype=self.dtype))\n        else:\n            parameter = tf.identity(input=parameter)\n\n        return parameter\n'"
tensorforce/core/parameters/parameter.py,1,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import Module\n\n\nclass Parameter(Module):\n    """"""\n    Base class for dynamic hyperparameters.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        dtype (""bool"" | ""int"" | ""long"" | ""float""): Tensor type\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        unit (""timesteps"" | ""episodes"" | ""updates""): Unit of parameter schedule\n            (<span style=""color:#00C000""><b>default</b></span>: none).\n        shape (iter[int > 0]): Tensor shape\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        min_value (dtype-compatible value): Lower parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        max_value (dtype-compatible value): Upper parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, dtype, unit=None, shape=(), min_value=None, max_value=None, summary_labels=None\n    ):\n        super().__init__(name=name, summary_labels=summary_labels)\n\n        assert unit in (None, \'timesteps\', \'episodes\', \'updates\')\n        self.unit = unit\n\n        spec = dict(type=dtype, shape=shape)\n        spec = util.valid_value_spec(value_spec=spec, return_normalized=True)\n        self.dtype = spec[\'type\']\n        self.shape = spec[\'shape\']\n\n        assert min_value is None or max_value is None or min_value < max_value\n        if self.dtype == \'bool\':\n            if min_value is not None or max_value is not None:\n                raise TensorforceError.unexpected()\n        elif self.dtype in (\'int\', \'long\'):\n            if (min_value is not None and not isinstance(min_value, int)) or \\\n                    (max_value is not None and not isinstance(max_value, int)):\n                raise TensorforceError.unexpected()\n        elif self.dtype == \'float\':\n            if (min_value is not None and not isinstance(min_value, float)) or \\\n                    (max_value is not None and not isinstance(max_value, float)):\n                raise TensorforceError.unexpected()\n        else:\n            assert False\n\n        assert self.min_value() is None or self.max_value() is None or \\\n            self.min_value() <= self.max_value()\n        if min_value is not None:\n            if self.min_value() is None:\n                raise TensorforceError.value(\n                    name=self.name, argument=\'lower bound\', value=self.min_value(),\n                    hint=(\'not >= \' + str(min_value))\n                )\n            elif self.min_value() < min_value:\n                raise TensorforceError.value(\n                    name=self.name, argument=\'lower bound\', value=self.min_value(),\n                    hint=(\'< \' + str(min_value))\n                )\n        if max_value is not None:\n            if self.max_value() is None:\n                raise TensorforceError.value(\n                    name=self.name, argument=\'upper bound\', value=self.max_value(),\n                    hint=(\'not <= \' + str(max_value))\n                )\n            elif self.max_value() > max_value:\n                raise TensorforceError.value(\n                    name=self.name, argument=\'upper bound\', value=self.max_value(),\n                    hint=(\'> \' + str(max_value))\n                )\n\n        Module.register_tensor(name=self.name, spec=spec, batched=False)\n\n    def min_value(self):\n        return None\n\n    def max_value(self):\n        return None\n\n    def final_value(self):\n        raise NotImplementedError\n\n    def parameter_value(self):\n        raise NotImplementedError\n\n    def tf_initialize(self):\n        super().tf_initialize()\n\n        if self.unit is None:\n            step = None\n        elif self.unit == \'timesteps\':\n            step = Module.retrieve_tensor(name=\'timestep\')\n        elif self.unit == \'episodes\':\n            step = Module.retrieve_tensor(name=\'episode\')\n        elif self.unit == \'updates\':\n            step = Module.retrieve_tensor(name=\'update\')\n\n        default = self.parameter_value(step=step)\n\n        # Temporarily leave module variable scope, otherwise placeholder name is unnecessarily long\n        if self.device is not None:\n            raise TensorforceError.unexpected()\n\n        self.scope.__exit__(None, None, None)\n\n        self.parameter_input = self.add_placeholder(\n            name=self.name, dtype=self.dtype, shape=self.shape, batched=False, default=default\n        )\n\n        self.scope.__enter__()\n\n    def tf_value(self):\n        parameter = tf.identity(input=self.parameter_input)\n\n        parameter = self.add_summary(label=\'parameters\', name=self.name, tensor=parameter)\n\n        # Required for TensorFlow optimizers learning_rate\n        if Module.global_tensors is not None:\n            Module.update_tensor(name=self.name, tensor=parameter)\n\n        return parameter\n'"
tensorforce/core/parameters/piecewise_constant.py,2,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import Module\nfrom tensorforce.core.parameters import Parameter\n\n\nclass PiecewiseConstant(Parameter):\n    """"""\n    Piecewise-constant hyperparameter.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        dtype (""bool"" | ""int"" | ""long"" | ""float""): Tensor type\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        unit (""timesteps"" | ""episodes"" | ""updates""): Unit of interval boundaries\n            (<span style=""color:#C00000""><b>required</b></span>).\n        boundaries (iter[long]): Strictly increasing interval boundaries for constant segments\n            (<span style=""color:#C00000""><b>required</b></span>).\n        values (iter[dtype-dependent]): Interval values of constant segments, one more than\n            (<span style=""color:#C00000""><b>required</b></span>).\n        min_value (dtype-compatible value): Lower parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        max_value (dtype-compatible value): Upper parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, dtype, unit, boundaries, values, min_value=None, max_value=None,\n        summary_labels=None\n    ):\n        if isinstance(values[0], bool):\n            if dtype != \'bool\':\n                raise TensorforceError.unexpected()\n        elif isinstance(values[0], int):\n            if dtype not in (\'int\', \'long\'):\n                raise TensorforceError.unexpected()\n        elif isinstance(values[0], float):\n            if dtype != \'float\':\n                raise TensorforceError.unexpected()\n        else:\n            raise TensorforceError.unexpected()\n\n        assert unit in (\'timesteps\', \'episodes\', \'updates\')\n        assert len(values) == len(boundaries) + 1\n        assert all(isinstance(value, type(values[0])) for value in values)\n\n        self.boundaries = boundaries\n        self.values = values\n\n        super().__init__(\n            name=name, dtype=dtype, unit=unit, min_value=min_value, max_value=max_value,\n            summary_labels=summary_labels\n        )\n\n    def min_value(self):\n        return min(self.values)\n\n    def max_value(self):\n        return max(self.values)\n\n    def final_value(self):\n        return self.values[-1]\n\n    def parameter_value(self, step):\n        parameter = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n            boundaries=self.boundaries, values=self.values\n        )(step=step)\n\n        if not util.is_dtype(x=parameter, dtype=self.dtype):\n            parameter = tf.dtypes.cast(x=parameter, dtype=util.tf_dtype(dtype=self.dtype))\n\n        return parameter\n'"
tensorforce/core/parameters/random.py,2,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.parameters import Parameter\n\n\nclass Random(Parameter):\n    """"""\n    Random hyperparameter.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        dtype (""bool"" | ""int"" | ""long"" | ""float""): Tensor type\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        distribution (""normal"" | ""uniform""): Distribution type for random hyperparameter value\n            (<span style=""color:#C00000""><b>required</b></span>).\n        shape (iter[int > 0]): Tensor shape\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        min_value (dtype-compatible value): Lower parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        max_value (dtype-compatible value): Upper parameter value bound\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        kwargs: Additional arguments dependent on distribution type.<br>\n            Normal distribution:\n            <ul>\n            <li><b>mean</b> (<i>float</i>) &ndash; Mean\n            (<span style=""color:#00C000""><b>default</b></span>: 0.0).</li>\n            <li><b>stddev</b> (<i>float > 0.0</i>) &ndash; Standard deviation\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0).</li>\n            </ul>\n            Uniform distribution:\n            <ul>\n            <li><b>minval</b> (<i>int / float</i>) &ndash; Lower bound\n            (<span style=""color:#00C000""><b>default</b></span>: 0 / 0.0).</li>\n            <li><b>maxval</b> (<i>float > minval</i>) &ndash; Upper bound\n            (<span style=""color:#00C000""><b>default</b></span>: 1.0 for float,\n            <span style=""color:#C00000""><b>required</b></span> for int).</li>\n            </ul>\n        """"""\n\n    def __init__(\n        self, name, dtype, distribution, shape=(), min_value=None, max_value=None,\n        summary_labels=None, **kwargs\n    ):\n        assert dtype in (\'int\', \'long\', \'float\')\n        assert distribution in (\'normal\', \'uniform\')\n\n        self.distribution = distribution\n        self.kwargs = kwargs\n\n        super().__init__(\n            name=name, dtype=dtype, shape=shape, min_value=min_value, max_value=max_value,\n            summary_labels=summary_labels\n        )\n\n    def min_value(self):\n        if self.distribution == \'uniform\':\n            if self.dtype == \'int\' or self.dtype == \'long\':\n                return int(self.kwargs.get(\'minval\', 0))\n            elif self.dtype == \'float\':\n                return int(self.kwargs.get(\'minval\', 0.0))\n        else:\n            return super().min_value()\n\n    def max_value(self):\n        if self.distribution == \'uniform\':\n            if self.dtype == \'int\' or self.dtype == \'long\':\n                return float(self.kwargs[\'maxval\'])\n            elif self.dtype == \'float\':\n                return float(self.kwargs.get(\'maxval\', 1.0))\n        else:\n            return super().max_value()\n\n    def final_value(self):\n        if self.distribution == \'normal\':\n            return util.py_dtype(dtype=self.dtype)(self.kwargs.get(\'mean\', 0.0))\n\n        elif self.distribution == \'uniform\':\n            if self.kwargs.get(\'maxval\', None) is None:\n                return 0.5\n\n            else:\n                return util.py_dtype(dtype=self.dtype)(\n                    self.kwargs[\'maxval\'] - self.kwargs.get(\'minval\', 0)\n                )\n\n        else:\n            assert False\n\n    def parameter_value(self, step):\n        if self.distribution == \'normal\':\n            parameter = tf.random.normal(\n                shape=self.shape, dtype=util.tf_dtype(dtype=self.dtype),\n                mean=self.kwargs.get(\'mean\', 0.0), stddev=self.kwargs.get(\'stddev\', 1.0)\n            )\n\n        elif self.distribution == \'uniform\':\n            parameter = tf.random.uniform(\n                shape=self.shape, dtype=util.tf_dtype(dtype=self.dtype),\n                minval=self.kwargs.get(\'minval\', 0), maxval=self.kwargs.get(\'maxval\', None)\n            )\n\n        return parameter\n'"
tensorforce/core/policies/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.policies.policy import Policy\n\nfrom tensorforce.core.policies.action_value import ActionValue\nfrom tensorforce.core.policies.stochastic import Stochastic\n\nfrom tensorforce.core.policies.parametrized_distributions import ParametrizedDistributions\n\n\npolicy_modules = dict(\n    default=ParametrizedDistributions, parametrized_distributions=ParametrizedDistributions\n)\n\n\n__all__ = [\'ActionValue\', \'ParametrizedDistributions\', \'Policy\', \'Stochastic\', \'ValueEstimator\']\n'"
tensorforce/core/policies/action_value.py,10,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core.policies import Policy\n\n\nclass ActionValue(Policy):\n    """"""\n    Base class for action-value-based policies.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        states_spec (specification): States specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        actions_spec (specification): Actions specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def tf_act(self, states, internals, auxiliaries, return_internals):\n        assert return_internals\n\n        actions_values = self.actions_values(\n            states=states, internals=internals, auxiliaries=auxiliaries\n        )\n\n        actions = OrderedDict()\n        for name, spec, action_values in util.zip_items(self.actions_spec, actions_values):\n            actions[name] = tf.math.argmax(\n                input=action_values, axis=-1, output_type=util.tf_dtype(spec[\'type\'])\n            )\n\n        return actions\n\n    def tf_states_value(\n        self, states, internals, auxiliaries, reduced=True, include_per_action=False\n    ):\n        states_values = self.states_values(\n            states=states, internals=internals, auxiliaries=auxiliaries\n        )\n\n        for name, spec, states_value in util.zip_items(self.actions_spec, states_values):\n            states_values[name] = tf.reshape(\n                tensor=states_value, shape=(-1, util.product(xs=spec[\'shape\']))\n            )\n\n        states_value = tf.concat(values=tuple(states_values.values()), axis=1)\n        if reduced:\n            states_value = tf.math.reduce_mean(input_tensor=states_value, axis=1)\n            if include_per_action:\n                for name in self.actions_spec:\n                    states_values[name] = tf.math.reduce_mean(\n                        input_tensor=states_values[name], axis=1\n                    )\n\n        if include_per_action:\n            states_values[\'*\'] = states_value\n            return states_values\n        else:\n            return states_value\n\n    def tf_actions_value(\n        self, states, internals, auxiliaries, actions, reduced=True, include_per_action=False\n    ):\n        actions_values = self.actions_values(\n            states=states, internals=internals, auxiliaries=auxiliaries, actions=actions\n        )\n\n        for name, spec, actions_value in util.zip_items(self.actions_spec, actions_values):\n            actions_values[name] = tf.reshape(\n                tensor=actions_value, shape=(-1, util.product(xs=spec[\'shape\']))\n            )\n\n        actions_value = tf.concat(values=tuple(actions_values.values()), axis=1)\n        if reduced:\n            actions_value = tf.math.reduce_mean(input_tensor=actions_value, axis=1)\n            if include_per_action:\n                for name in self.actions_spec:\n                    actions_values[name] = tf.math.reduce_mean(\n                        input_tensor=actions_values[name], axis=1\n                    )\n\n        if include_per_action:\n            actions_values[\'*\'] = actions_value\n            return actions_values\n        else:\n            return actions_value\n\n    def tf_states_values(self, states, internals, auxiliaries):\n        if not all(spec[\'type\'] == \'int\' for spec in self.actions_spec.values()):\n            raise NotImplementedError\n\n        actions_values = self.actions_values(\n            states=states, internals=internals, auxiliaries=auxiliaries\n        )\n\n        states_values = OrderedDict()\n        for name, spec, action_values in util.zip_items(self.actions_spec, actions_values):\n            states_values[name] = tf.math.reduce_max(input_tensor=action_values, axis=-1)\n\n        return states_values\n\n    def tf_actions_values(self, states, internals, auxiliaries, actions=None):\n        raise NotImplementedError\n'"
tensorforce/core/policies/parametrized_distributions.py,2,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import distribution_modules, layer_modules, Module, network_modules\nfrom tensorforce.core.networks import Network\nfrom tensorforce.core.policies import Stochastic, ActionValue\n\n\nclass ParametrizedDistributions(Stochastic, ActionValue):\n    """"""\n    Policy which parametrizes independent distributions per action conditioned on the output of a\n    central states-processing neural network (supports both stochastic and action-value-based\n    policy interface) (specification key: `parametrized_distributions`).\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        network (\'auto\' | specification): Policy network configuration, see\n            [networks](../modules/networks.html)\n            (<span style=""color:#00C000""><b>default</b></span>: \'auto\', automatically configured\n            network).\n        distributions (dict[specification]): Distributions configuration, see\n            [distributions](../modules/distributions.html), specified per\n            action-type or -name\n            (<span style=""color:#00C000""><b>default</b></span>: per action-type, Bernoulli\n            distribution for binary boolean actions, categorical distribution for discrete integer\n            actions, Gaussian distribution for unbounded continuous actions, Beta distribution for\n            bounded continuous actions).\n        temperature (parameter | dict[parameter], float >= 0.0): Sampling temperature, global or\n            per action (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        use_beta_distribution (bool): Whether to use the Beta distribution for bounded continuous\n            actions by default.\n            (<span style=""color:#00C000""><b>default</b></span>: true).\n        infer_state_value (False | ""action-values"" | ""distribution""): Whether to infer the state\n            value from either the action values or (experimental) the distribution parameters\n            (<span style=""color:#00C000""><b>default</b></span>: false).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        states_spec (specification): States specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        actions_spec (specification): Actions specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n    """"""\n\n    # Network first\n    def __init__(\n        self, name, network=\'auto\', distributions=None, temperature=0.0, use_beta_distribution=True,\n        infer_state_value=False, device=None, summary_labels=None, l2_regularization=None,\n        states_spec=None, actions_spec=None\n    ):\n        if isinstance(network, Network):\n            assert device is None\n            device = network.device\n            network.device = None\n\n        super().__init__(\n            name=name, states_spec=states_spec, actions_spec=actions_spec, temperature=temperature,\n            device=device, summary_labels=summary_labels, l2_regularization=l2_regularization\n        )\n\n        # Network\n        self.network = self.add_module(\n            name=(self.name + \'-network\'), module=network, modules=network_modules,\n            inputs_spec=self.states_spec\n        )\n        output_spec = self.network.get_output_spec()\n        if output_spec[\'type\'] != \'float\':\n            raise TensorforceError(\n                ""Invalid output type for network: {}."".format(output_spec[\'type\'])\n            )\n        Module.register_tensor(name=self.name, spec=output_spec, batched=True)\n        embedding_shape = output_spec[\'shape\']\n\n        # Distributions\n        self.distributions = OrderedDict()\n        for name, spec in self.actions_spec.items():\n            if spec[\'type\'] == \'bool\':\n                default_module = \'bernoulli\'\n            elif spec[\'type\'] == \'int\':\n                default_module = \'categorical\'\n            elif spec[\'type\'] == \'float\':\n                if use_beta_distribution and \'min_value\' in spec:\n                    default_module = \'beta\'\n                else:\n                    default_module = \'gaussian\'\n\n            if distributions is None:\n                module = None\n            else:\n                module = dict()\n                if spec[\'type\'] in distributions:\n                    if isinstance(distributions[spec[\'type\']], str):\n                        module = distributions[spec[\'type\']]\n                    else:\n                        module.update(distributions[spec[\'type\']])\n                if name in distributions:\n                    if isinstance(distributions[name], str):\n                        module = distributions[name]\n                    else:\n                        module.update(distributions[name])\n\n            self.distributions[name] = self.add_module(\n                name=(name + \'-distribution\'), module=module, modules=distribution_modules,\n                default_module=default_module, action_spec=spec, embedding_shape=embedding_shape\n            )\n\n        # State value\n        assert infer_state_value in (False, \'action-values\', \'distribution\')\n        self.infer_state_value = infer_state_value\n        if self.infer_state_value is False:\n            self.value = self.add_module(\n                name=\'states-value\', module=\'linear\', modules=layer_modules, size=0,\n                input_spec=output_spec\n            )\n\n    @classmethod\n    def internals_spec(cls, network=None, policy=None, name=None, states_spec=None, **kwargs):\n        if policy is None:\n            if network is None:\n                network = \'auto\'\n            assert name is not None and states_spec is not None\n\n            network_cls, first_arg, kwargs = Module.get_module_class_and_kwargs(\n                name=(name + \'-network\'), module=network, modules=network_modules,\n                inputs_spec=states_spec\n            )\n\n            if first_arg is None:\n                return network_cls.internals_spec(name=(name + \'-network\'), **kwargs)\n            else:\n                return network_cls.internals_spec(first_arg, name=(name + \'-network\'), **kwargs)\n\n        else:\n            assert network is None and name is None and states_spec is None\n            return policy.network.__class__.internals_spec(network=policy.network)\n\n    def internals_init(self):\n        return self.network.internals_init()\n\n    def max_past_horizon(self, is_optimization):\n        return self.network.max_past_horizon(is_optimization=is_optimization)\n\n    def tf_past_horizon(self, is_optimization):\n        return self.network.past_horizon(is_optimization=is_optimization)\n\n    def tf_act(self, states, internals, auxiliaries, return_internals):\n        return Stochastic.tf_act(\n            self=self, states=states, internals=internals, auxiliaries=auxiliaries,\n            return_internals=return_internals\n        )\n\n    def tf_sample_actions(self, states, internals, auxiliaries, temperature, return_internals):\n        if return_internals:\n            embedding, internals = self.network.apply(\n                x=states, internals=internals, return_internals=return_internals\n            )\n        else:\n            embedding = self.network.apply(\n                x=states, internals=internals, return_internals=return_internals\n            )\n\n        Module.update_tensor(name=self.name, tensor=embedding)\n\n        actions = OrderedDict()\n        for name, spec, distribution, temp in util.zip_items(\n            self.actions_spec, self.distributions, temperature\n        ):\n            if spec[\'type\'] == \'int\':\n                mask = auxiliaries[name + \'_mask\']\n                parameters = distribution.parametrize(x=embedding, mask=mask)\n            else:\n                parameters = distribution.parametrize(x=embedding)\n            actions[name] = distribution.sample(parameters=parameters, temperature=temp)\n\n        if return_internals:\n            return actions, internals\n        else:\n            return actions\n\n    def tf_log_probabilities(self, states, internals, auxiliaries, actions):\n        embedding = self.network.apply(x=states, internals=internals)\n        Module.update_tensor(name=self.name, tensor=embedding)\n\n        log_probabilities = OrderedDict()\n        for name, spec, distribution, action in util.zip_items(\n            self.actions_spec, self.distributions, actions\n        ):\n            if spec[\'type\'] == \'int\':\n                mask = auxiliaries[name + \'_mask\']\n                parameters = distribution.parametrize(x=embedding, mask=mask)\n            else:\n                parameters = distribution.parametrize(x=embedding)\n            log_probabilities[name] = distribution.log_probability(\n                parameters=parameters, action=action\n            )\n\n        return log_probabilities\n\n    def tf_entropies(self, states, internals, auxiliaries):\n        embedding = self.network.apply(x=states, internals=internals)\n        Module.update_tensor(name=self.name, tensor=embedding)\n\n        entropies = OrderedDict()\n        for name, spec, distribution in util.zip_items(self.actions_spec, self.distributions):\n            if spec[\'type\'] == \'int\':\n                mask = auxiliaries[name + \'_mask\']\n                parameters = distribution.parametrize(x=embedding, mask=mask)\n            else:\n                parameters = distribution.parametrize(x=embedding)\n            entropies[name] = distribution.entropy(parameters=parameters)\n\n        return entropies\n\n    def tf_kl_divergences(self, states, internals, auxiliaries, other=None):\n        parameters = self.kldiv_reference(\n            states=states, internals=internals, auxiliaries=auxiliaries\n        )\n\n        if other is None:\n            other = util.fmap(function=tf.stop_gradient, xs=parameters)\n        elif isinstance(other, ParametrizedDistributions):\n            other = other.kldiv_reference(\n                states=states, internals=internals, auxiliaries=auxiliaries\n            )\n            other = util.fmap(function=tf.stop_gradient, xs=other)\n        elif isinstance(other, dict):\n            if any(name not in other for name in self.actions_spec):\n                raise TensorforceError.unexpected()\n        else:\n            raise TensorforceError.unexpected()\n\n        kl_divergences = OrderedDict()\n        for name, distribution in self.distributions.items():\n            kl_divergences[name] = distribution.kl_divergence(\n                parameters1=parameters[name], parameters2=other[name]\n            )\n\n        return kl_divergences\n\n    def tf_kldiv_reference(self, states, internals, auxiliaries):\n        embedding = self.network.apply(x=states, internals=internals)\n\n        kldiv_reference = OrderedDict()\n        for name, spec, distribution in util.zip_items(self.actions_spec, self.distributions):\n            if spec[\'type\'] == \'int\':\n                mask = auxiliaries[name + \'_mask\']\n                kldiv_reference[name] = distribution.parametrize(x=embedding, mask=mask)\n            else:\n                kldiv_reference[name] = distribution.parametrize(x=embedding)\n\n        return kldiv_reference\n\n    def tf_states_values(self, states, internals, auxiliaries):\n        if self.infer_state_value == \'action-values\':\n            return ActionValue.tf_states_values(\n                self=self, states=states, internals=internals, auxiliaries=auxiliaries\n            )\n\n        else:\n            embedding = self.network.apply(x=states, internals=internals)\n            Module.update_tensor(name=self.name, tensor=embedding)\n\n            states_values = OrderedDict()\n            for name, spec, distribution in util.zip_items(self.actions_spec, self.distributions):\n                if spec[\'type\'] == \'int\':\n                    mask = auxiliaries[name + \'_mask\']\n                    parameters = distribution.parametrize(x=embedding, mask=mask)\n                else:\n                    parameters = distribution.parametrize(x=embedding)\n                states_values[name] = distribution.states_value(parameters=parameters)\n\n            return states_values\n\n    def tf_actions_values(self, states, internals, auxiliaries, actions=None):\n        embedding = self.network.apply(x=states, internals=internals)\n        Module.update_tensor(name=self.name, tensor=embedding)\n\n        actions_values = OrderedDict()\n        for name, spec, distribution in util.zip_items(self.actions_spec, self.distributions):\n            if spec[\'type\'] == \'int\':\n                mask = auxiliaries[name + \'_mask\']\n                parameters = distribution.parametrize(x=embedding, mask=mask)\n            else:\n                parameters = distribution.parametrize(x=embedding)\n            if actions is None:\n                action = None\n            else:\n                action = actions[name]\n            actions_values[name] = distribution.action_value(parameters=parameters, action=action)\n\n        return actions_values\n\n    def tf_states_value(\n        self, states, internals, auxiliaries, reduced=True, include_per_action=False\n    ):\n        if self.infer_state_value is False:\n            if not reduced or include_per_action:\n                raise TensorforceError.invalid(name=\'policy.states_value\', argument=\'reduced\')\n\n            embedding = self.network.apply(x=states, internals=internals)\n            Module.update_tensor(name=self.name, tensor=embedding)\n\n            states_value = self.value.apply(x=embedding)\n            return states_value\n\n        else:\n            return ActionValue.tf_states_value(\n                self=self, states=states, internals=internals, auxiliaries=auxiliaries,\n                reduced=reduced, include_per_action=include_per_action\n            )\n'"
tensorforce/core/policies/policy.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import Module\n\n\nclass Policy(Module):\n    """"""\n    Base class for decision policies.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        states_spec (specification): States specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        actions_spec (specification): Actions specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, states_spec, actions_spec, device=None, summary_labels=None,\n        l2_regularization=None\n    ):\n        super().__init__(\n            name=name, device=device, summary_labels=summary_labels,\n            l2_regularization=l2_regularization\n        )\n\n        self.states_spec = states_spec\n        self.actions_spec = actions_spec\n\n    @classmethod\n    def internals_spec(cls, policy=None, **kwargs):\n        raise NotImplementedError\n\n    def internals_init(self):\n        raise NotImplementedError\n\n    def max_past_horizon(self, is_optimization=False):\n        raise NotImplementedError\n\n    def tf_past_horizon(self, is_optimization=False):\n        raise NotImplementedError\n\n    def tf_act(self, states, internals, auxiliaries, return_internals):\n        raise NotImplementedError\n'"
tensorforce/core/policies/stochastic.py,15,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom collections import OrderedDict\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import Module, parameter_modules\nfrom tensorforce.core.policies import Policy\n\n\nclass Stochastic(Policy):\n    """"""\n    Base class for stochastic policies.\n\n    Args:\n        name (string): Module name\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        states_spec (specification): States specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        actions_spec (specification): Actions specification\n            (<span style=""color:#0000C0""><b>internal use</b></span>).\n        temperature (parameter | dict[parameter], float >= 0.0): Sampling temperature, global or\n            per action (<span style=""color:#00C000""><b>default</b></span>: 0.0).\n        device (string): Device name\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        summary_labels (\'all\' | iter[string]): Labels of summaries to record\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n        l2_regularization (float >= 0.0): Scalar controlling L2 regularization\n            (<span style=""color:#00C000""><b>default</b></span>: inherit value of parent module).\n    """"""\n\n    def __init__(\n        self, name, states_spec, actions_spec, temperature=0.0, device=None, summary_labels=None,\n        l2_regularization=None\n    ):\n        super().__init__(\n            name=name, states_spec=states_spec, actions_spec=actions_spec, device=device,\n            summary_labels=summary_labels, l2_regularization=l2_regularization\n        )\n\n        # Sampling temperature\n        if isinstance(temperature, dict) and \\\n                all(name in self.actions_spec for name in temperature):\n            # Different temperature per action\n            self.temperature = OrderedDict()\n            for name in self.actions_spec:\n                if name in temperature:\n                    self.temperature[name] = self.add_module(\n                        name=(name + \'-temperature\'), module=temperature[name],\n                        modules=parameter_modules, is_trainable=False, dtype=\'float\', min_value=0.0\n                    )\n        else:\n            # Same temperature for all actions\n            self.temperature = self.add_module(\n                name=\'temperature\', module=temperature, modules=parameter_modules,\n                is_trainable=False, dtype=\'float\', min_value=0.0\n            )\n\n    def tf_act(self, states, internals, auxiliaries, return_internals):\n        deterministic = Module.retrieve_tensor(name=\'deterministic\')\n\n        zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n        temperature = OrderedDict()\n        if isinstance(self.temperature, dict):\n            for name in self.actions_spec:\n                if name in self.temperature:\n                    temperature[name] = tf.where(\n                        condition=deterministic, x=zero, y=self.temperature[name].value()\n                    )\n                else:\n                    temperature[name] = zero\n        else:\n            value = tf.where(condition=deterministic, x=zero, y=self.temperature.value())\n            for name in self.actions_spec:\n                temperature[name] = value\n\n        return self.sample_actions(\n            states=states, internals=internals, auxiliaries=auxiliaries,\n            temperature=temperature, return_internals=return_internals\n        )\n\n    def tf_log_probability(\n        self, states, internals, auxiliaries, actions, reduced=True, include_per_action=False\n    ):\n        log_probabilities = self.log_probabilities(\n            states=states, internals=internals, auxiliaries=auxiliaries, actions=actions\n        )\n\n        for name, spec, log_probability in util.zip_items(self.actions_spec, log_probabilities):\n            log_probabilities[name] = tf.reshape(\n                tensor=log_probability, shape=(-1, util.product(xs=spec[\'shape\']))\n            )\n\n        log_probability = tf.concat(values=tuple(log_probabilities.values()), axis=1)\n        if reduced:\n            log_probability = tf.math.reduce_mean(input_tensor=log_probability, axis=1)\n            if include_per_action:\n                for name in self.actions_spec:\n                    log_probabilities[name] = tf.math.reduce_mean(\n                        input_tensor=log_probabilities[name], axis=1\n                    )\n\n        if include_per_action:\n            log_probabilities[\'*\'] = log_probability\n            return log_probabilities\n        else:\n            return log_probability\n\n    def tf_entropy(self, states, internals, auxiliaries, reduced=True, include_per_action=False):\n        entropies = self.entropies(states=states, internals=internals, auxiliaries=auxiliaries)\n\n        for name, spec, entropy in util.zip_items(self.actions_spec, entropies):\n            entropies[name] = tf.reshape(\n                tensor=entropy, shape=(-1, util.product(xs=spec[\'shape\']))\n            )\n\n        entropy = tf.concat(values=tuple(entropies.values()), axis=1)\n\n        if reduced:\n            entropy = tf.math.reduce_mean(input_tensor=entropy, axis=1)\n            if include_per_action:\n                for name in self.actions_spec:\n                    entropies[name] = tf.math.reduce_mean(input_tensor=entropies[name], axis=1)\n\n        if include_per_action:\n            entropies[\'*\'] = entropy\n            return entropies\n        else:\n            return entropy\n\n    def tf_kl_divergence(\n        self, states, internals, auxiliaries, other=None, reduced=True, include_per_action=False\n    ):\n        kl_divergences = self.kl_divergences(\n            states=states, internals=internals, auxiliaries=auxiliaries, other=other\n        )\n\n        for name, spec, kl_divergence in util.zip_items(self.actions_spec, kl_divergences):\n            kl_divergences[name] = tf.reshape(\n                tensor=kl_divergence, shape=(-1, util.product(xs=spec[\'shape\']))\n            )\n\n        kl_divergence = tf.concat(values=tuple(kl_divergences.values()), axis=1)\n        if reduced:\n            kl_divergence = tf.math.reduce_mean(input_tensor=kl_divergence, axis=1)\n            if include_per_action:\n                for name in self.actions_spec:\n                    kl_divergences[name] = tf.math.reduce_mean(\n                        input_tensor=kl_divergences[name], axis=1\n                    )\n\n        if include_per_action:\n            kl_divergences[\'*\'] = kl_divergence\n            return kl_divergences\n        else:\n            return kl_divergence\n\n    def tf_sample_actions(self, states, internals, auxiliaries, temperature, return_internals):\n        raise NotImplementedError\n\n    def tf_log_probabilities(self, states, internals, auxiliaries, actions):\n        raise NotImplementedError\n\n    def tf_entropies(self, states, internals, auxiliaries):\n        raise NotImplementedError\n\n    def tf_kl_divergences(self, states, internals, auxiliaries, other=None):\n        raise NotImplementedError\n\n    def tf_kldiv_reference(self, states, internals, auxiliaries):\n        raise NotImplementedError\n'"
tensorforce/environments/carla/env_utils.py,0,"b'""""""Utility functions for environment.py""""""\n\nimport os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport carla\nimport pygame\nimport threading\nimport datetime\n\nfrom typing import Union\nfrom tensorforce.agents import Agent\n\n\n# constants:\nepsilon = np.finfo(np.float32).eps\n\n# Use this dict to convert lanes objects to integers:\nWAYPOINT_DICT = dict(lane_change={carla.LaneChange.NONE: 0,\n                                  carla.LaneChange.Both: 1,\n                                  carla.LaneChange.Left: 2,\n                                  carla.LaneChange.Right: 3},\n                     lane_type={carla.LaneType.NONE: 0,\n                                carla.LaneType.Bidirectional: 1,\n                                carla.LaneType.Biking: 2,\n                                carla.LaneType.Border: 3,\n                                carla.LaneType.Driving: 4,\n                                carla.LaneType.Entry: 5,\n                                carla.LaneType.Exit: 6,\n                                carla.LaneType.Median: 7,\n                                carla.LaneType.OffRamp: 8,\n                                carla.LaneType.OnRamp: 9,\n                                carla.LaneType.Parking: 10,\n                                carla.LaneType.Rail: 11,\n                                carla.LaneType.Restricted: 12,\n                                carla.LaneType.RoadWorks: 13,\n                                carla.LaneType.Shoulder: 14,\n                                carla.LaneType.Sidewalk: 15,\n                                carla.LaneType.Special1: 16,\n                                carla.LaneType.Special2: 17,\n                                carla.LaneType.Special3: 18,\n                                carla.LaneType.Stop: 19,\n                                carla.LaneType.Tram: 20,\n                                carla.LaneType.Any: 21},\n                     lane_marking_type={carla.LaneMarkingType.NONE: 0,\n                                        carla.LaneMarkingType.BottsDots: 1,\n                                        carla.LaneMarkingType.Broken: 2,\n                                        carla.LaneMarkingType.BrokenBroken: 3,\n                                        carla.LaneMarkingType.BrokenSolid: 4,\n                                        carla.LaneMarkingType.Curb: 5,\n                                        carla.LaneMarkingType.Grass: 6,\n                                        carla.LaneMarkingType.Solid: 7,\n                                        carla.LaneMarkingType.SolidBroken: 8,\n                                        carla.LaneMarkingType.SolidSolid: 9,\n                                        carla.LaneMarkingType.Other: 10},\n                     traffic_light={carla.TrafficLightState.Green: 0,\n                                    carla.TrafficLightState.Red: 1,\n                                    carla.TrafficLightState.Yellow: 2,\n                                    carla.TrafficLightState.Off: 3,\n                                    carla.TrafficLightState.Unknown: 4}\n                     )\n\n\n# -------------------------------------------------------------------------------------------------\n# -- PyGame\n# -------------------------------------------------------------------------------------------------\n\ndef init_pygame():\n    if not pygame.get_init():\n        pygame.init()\n\n    if not pygame.font.get_init():\n        pygame.font.init()\n\n\ndef get_display(window_size, mode=pygame.HWSURFACE | pygame.DOUBLEBUF):\n    """"""Returns a display used to render images and text.\n        :param window_size: a tuple (width: int, height: int)\n        :param mode: pygame rendering mode. Default: pygame.HWSURFACE | pygame.DOUBLEBUF\n        :return: a pygame.display instance.\n    """"""\n    return pygame.display.set_mode(window_size, mode)\n\n\ndef get_font(size=14):\n    return pygame.font.Font(pygame.font.get_default_font(), size)\n\n\ndef display_image(display, image, window_size=(800, 600), blend=False):\n    """"""Displays the given image on a pygame window\n    :param blend: whether to blend or not the given image.\n    :param window_size: the size of the pygame\'s window. Default is (800, 600)\n    :param display: pygame.display\n    :param image: the image (numpy.array) to display/render on.\n    """"""\n    # Resize image if necessary\n    if (image.shape[1], image.shape[0]) != window_size:\n        image = resize(image, size=window_size)\n\n    image_surface = pygame.surfarray.make_surface(image.swapaxes(0, 1))\n\n    if blend:\n        image_surface.set_alpha(100)\n\n    display.blit(image_surface, (0, 0))\n\n\ndef display_text(display, font, text: [str], color=(255, 255, 255), origin=(0, 0), offset=(0, 2)):\n    position = origin\n\n    for line in text:\n        if isinstance(line, dict):\n            display.blit(font.render(line.get(\'text\'), True, line.get(\'color\', color)), position)\n        else:\n            display.blit(font.render(line, True, color), position)\n\n        position = (position[0] + offset[0], position[1] + offset[1])\n\n\ndef pygame_save(display, path: str, name: str = None):\n    if name is None:\n        name = \'image-\' + str(datetime.datetime.now()) + \'.jpg\'\n\n    thread = threading.Thread(target=lambda: pygame.image.save(display, os.path.join(path, name)))\n    thread.start()\n\n\n# -------------------------------------------------------------------------------------------------\n# -- CARLA\n# -------------------------------------------------------------------------------------------------\n\ndef get_client(address, port, timeout=2.0) -> carla.Client:\n    """"""Connects to the simulator.\n        @:returns a carla.Client instance if the CARLA simulator accepts the connection.\n    """"""\n    client: carla.Client = carla.Client(address, port)\n    client.set_timeout(timeout)\n    return client\n\n\ndef random_blueprint(world: carla.World, actor_filter=\'vehicle.*\', role_name=\'agent\') -> carla.ActorBlueprint:\n    """"""Retrieves a random blueprint.\n        :param world: a carla.World instance.\n        :param actor_filter: a string used to filter (select) blueprints. Default: \'vehicle.*\'\n        :param role_name: blueprint\'s role_name, Default: \'agent\'.\n        :return: a carla.ActorBlueprint instance.\n    """"""\n    blueprints = world.get_blueprint_library().filter(actor_filter)\n    blueprint: carla.ActorBlueprint = random.choice(blueprints)\n    blueprint.set_attribute(\'role_name\', role_name)\n\n    if blueprint.has_attribute(\'color\'):\n        color = random.choice(blueprint.get_attribute(\'color\').recommended_values)\n        blueprint.set_attribute(\'color\', color)\n\n    if blueprint.has_attribute(\'driver_id\'):\n        driver_id = random.choice(blueprint.get_attribute(\'driver_id\').recommended_values)\n        blueprint.set_attribute(\'driver_id\', driver_id)\n\n    if blueprint.has_attribute(\'is_invincible\'):\n        blueprint.set_attribute(\'is_invincible\', \'true\')\n\n    # set the max speed\n    if blueprint.has_attribute(\'speed\'):\n        float(blueprint.get_attribute(\'speed\').recommended_values[1])\n        float(blueprint.get_attribute(\'speed\').recommended_values[2])\n    else:\n        print(""No recommended values for \'speed\' attribute"")\n\n    return blueprint\n\n\ndef random_spawn_point(world_map: carla.Map, different_from: carla.Location = None) -> carla.Transform:\n    """"""Returns a random spawning location.\n        :param world_map: a carla.Map instance obtained by calling world.get_map()\n        :param different_from: ensures that the location of the random spawn point is different from the one specified here.\n        :return: a carla.Transform instance.\n    """"""\n    available_spawn_points = world_map.get_spawn_points()\n\n    if different_from is not None:\n        while True:\n            spawn_point = random.choice(available_spawn_points)\n\n            if spawn_point.location != different_from:\n                return spawn_point\n    else:\n        return random.choice(available_spawn_points)\n\n\ndef spawn_actor(world: carla.World, blueprint: carla.ActorBlueprint, spawn_point: carla.Transform,\n                attach_to: carla.Actor = None, attachment_type=carla.AttachmentType.Rigid) -> carla.Actor:\n    """"""Tries to spawn an actor in a CARLA simulator.\n        :param world: a carla.World instance.\n        :param blueprint: specifies which actor has to be spawned.\n        :param spawn_point: where to spawn the actor. A transform specifies the location and rotation.\n        :param attach_to: whether the spawned actor has to be attached (linked) to another one.\n        :param attachment_type: the kind of the attachment. Can be \'Rigid\' or \'SpringArm\'.\n        :return: a carla.Actor instance.\n    """"""\n    actor = world.try_spawn_actor(blueprint, spawn_point, attach_to, attachment_type)\n\n    if actor is None:\n        raise ValueError(f\'Cannot spawn actor. Try changing the spawn_point ({spawn_point}) to something else.\')\n\n    return actor\n\n\ndef get_blueprint(world: carla.World, actor_id: str) -> carla.ActorBlueprint:\n    return world.get_blueprint_library().find(actor_id)\n\n\ndef global_to_local(point: carla.Location, reference: Union[carla.Transform, carla.Location, carla.Rotation]):\n    """"""Translates a 3D point from global to local coordinates using the current transformation as reference""""""\n    if isinstance(reference, carla.Transform):\n        reference.transform(point)\n    elif isinstance(reference, carla.Location):\n        carla.Transform(reference, carla.Rotation()).transform(point)\n    elif isinstance(reference, carla.Rotation):\n        carla.Transform(carla.Location(), reference).transform(point)\n    else:\n        raise ValueError(\'Argument ""reference"" is none of carla.Transform or carla.Location or carla.Rotation!\')\n\n\n# -------------------------------------------------------------------------------------------------\n# -- Other\n# -------------------------------------------------------------------------------------------------\n\ndef resize(image, size: (int, int), interpolation=cv2.INTER_CUBIC):\n    """"""Resize the given image.\n        :param image: a numpy array with shape (height, width, channels).\n        :param size: (width, height) to resize the image to.\n        :param interpolation: Default: cv2.INTER_CUBIC.\n        :return: the reshaped image.\n    """"""\n    return cv2.resize(image, dsize=size, interpolation=interpolation)\n\n\ndef scale(num, from_interval=(-1.0, +1.0), to_interval=(0.0, 7.0)) -> float:\n    """"""Scales (interpolates) the given number to a given interval.\n        :param num: a number\n        :param from_interval: the interval the number is assumed to lie in.\n        :param to_interval: the target interval.\n        :return: the scaled/interpolated number.\n    """"""\n    x = np.interp(num, from_interval, to_interval)\n    return float(round(x))\n\n\ndef cv2_grayscale(image, is_bgr=True, depth=1):\n    """"""Convert a RGB or BGR image to grayscale using OpenCV (cv2).\n        :param image: input image, a numpy.ndarray.\n        :param is_bgr: tells whether the image is in BGR format. If False, RGB format is assumed.\n        :param depth: replicates the gray depth channel multiple times. E.g. useful to display grayscale images as rgb.\n    """"""\n    assert depth >= 1\n\n    if is_bgr:\n        grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        grayscale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    if depth > 1:\n        return np.stack((grayscale,) * depth, axis=-1)\n\n    return grayscale\n\n\ndef save_agent(agent: Agent, agent_name: str, directory: str, separate_dir=True) -> str:\n    if separate_dir:\n        save_path = os.path.join(directory, agent_name)\n        os.makedirs(save_path, exist_ok=True)\n    else:\n        save_path = directory\n\n    checkpoint_path = agent.save(directory=save_path, filename=agent_name)\n    return checkpoint_path\n\n\ndef get_record_path(base_dir: str, prefix=\'ep\', pattern=\'-\'):\n    dirs = sorted(os.listdir(base_dir))\n    count = 0\n\n    if len(dirs) > 0:\n        count = 1 + int(dirs[-1].split(pattern)[1])\n\n    record_path = os.path.join(base_dir, f\'{prefix}{pattern}{count}\')\n    os.mkdir(record_path)\n\n    return record_path\n\n\ndef replace_nans(data: dict, nan=0.0, pos_inf=0.0, neg_inf=0.0):\n    """"""In-place replacement of non-numerical values, i.e. NaNs and +/- infinity""""""\n    for key, value in data.items():\n        if np.isnan(value).any() or np.isinf(value).any():\n            data[key] = np.nan_to_num(value, nan=nan, posinf=pos_inf, neginf=neg_inf)\n\n    return data\n\n\n# -------------------------------------------------------------------------------------------------\n# -- Debug\n# -------------------------------------------------------------------------------------------------\n\nclass Colors(object):\n    """"""Wraps some carla.Color instances.""""""\n    red = carla.Color(255, 0, 0)\n    green = carla.Color(0, 255, 0)\n    blue = carla.Color(47, 210, 231)\n    cyan = carla.Color(0, 255, 255)\n    yellow = carla.Color(255, 255, 0)\n    orange = carla.Color(255, 162, 0)\n    white = carla.Color(255, 255, 255)\n    black = carla.Color(0, 0, 0)\n\n\ndef draw_transform(debug, trans, col=Colors.red, lt=-1):\n    yaw_in_rad = math.radians(trans.rotation.yaw)\n    pitch_in_rad = math.radians(trans.rotation.pitch)\n\n    p1 = carla.Location(x=trans.location.x + math.cos(pitch_in_rad) * math.cos(yaw_in_rad),\n                        y=trans.location.y + math.cos(pitch_in_rad) * math.sin(yaw_in_rad),\n                        z=trans.location.z + math.sin(pitch_in_rad))\n\n    debug.draw_arrow(trans.location, p1, thickness=0.05, arrow_size=0.1, color=col, life_time=lt)\n\n\ndef draw_radar_measurement(debug_helper: carla.DebugHelper, data: carla.RadarMeasurement, velocity_range=7.5,\n                           size=0.075, life_time=0.06):\n    """"""Code adapted from carla/PythonAPI/examples/manual_control.py:\n        - White: means static points.\n        - Red: indicates points moving towards the object.\n        - Blue: denoted points moving away.\n    """"""\n    radar_rotation = data.transform.rotation\n    for detection in data:\n        azimuth = math.degrees(detection.azimuth) + radar_rotation.yaw\n        altitude = math.degrees(detection.altitude) + radar_rotation.pitch\n\n        # move to local coordinates:\n        forward_vec = carla.Vector3D(x=detection.depth - 0.25)\n        global_to_local(forward_vec,\n                        reference=carla.Rotation(pitch=altitude, yaw=azimuth, roll=radar_rotation.roll))\n\n        # draw:\n        debug_helper.draw_point(data.transform.location + forward_vec, size=size, life_time=life_time,\n                                persistent_lines=False, color=carla.Color(255, 255, 255))\n\n\n# -------------------------------------------------------------------------------------------------\n# -- Math\n# -------------------------------------------------------------------------------------------------\n\ndef l2_norm(location1, location2):\n    """"""Computes the Euclidean distance between two carla.Location objects.""""""\n    dx = location1.x - location2.x\n    dy = location1.y - location2.y\n    dz = location1.z - location2.z\n    return math.sqrt(dx**2 + dy**2 + dz**2) + epsilon\n\n\ndef vector_norm(vec: carla.Vector3D) -> float:\n    """"""Returns the norm/magnitude (a scalar) of the given 3D vector.""""""\n    return math.sqrt(vec.x**2 + vec.y**2 + vec.z**2)\n\n\ndef speed(actor: carla.Actor) -> float:\n    """"""Returns the speed of the given actor in km/h.""""""\n    return 3.6 * vector_norm(actor.get_velocity())\n\n\ndef dot_product(a: carla.Vector3D, b: carla.Vector3D) -> float:\n    return a.x * b.x + a.y * b.y + a.z * b.z\n\n\ndef cosine_similarity(a: carla.Vector3D, b: carla.Vector3D) -> float:\n    """"""-1: opposite vectors (pointing in the opposite direction),\n        0: orthogonal,\n        1: exactly the same (pointing in the same direction)\n    """"""\n    return dot_product(a, b) / (vector_norm(a) * vector_norm(b))\n'"
tensorforce/environments/carla/sensors.py,0,"b'""""""A collection of sensors helpers.""""""\n\nimport math\nimport numpy as np\nimport carla\n\n\nclass Sensor(object):\n    """"""Base class for wrapping sensors.""""""\n    def __init__(self, parent_actor: carla.Actor, transform=carla.Transform(), attachment_type=None,\n                 attributes: dict = None):\n        self.parent = parent_actor\n        self.world = self.parent.get_world()\n        self.attributes = attributes or dict()\n        self.event_callbacks = []\n\n        # Look for callback(s)\n        if \'callback\' in self.attributes:\n            self.event_callbacks.append(self.attributes.pop(\'callback\'))\n\n        elif \'callbacks\' in self.attributes:\n            for callback in self.attributes.pop(\'callbacks\'):\n                self.event_callbacks.append(callback)\n\n        # detector-sensors retrieve data only when triggered (not at each tick!)\n        self.sensor, self.is_detector = self._spawn(transform, attachment_type)\n\n    @property\n    def name(self) -> str:\n        raise NotImplementedError\n\n    def set_parent_actor(self, actor: carla.Actor):\n        self.parent = actor\n\n    def add_callback(self, callback):\n        assert callable(callback)\n        self.event_callbacks.append(callback)\n\n    def clear_callbacks(self):\n        self.event_callbacks.clear()\n\n    @staticmethod\n    def create(sensor_type, **kwargs):\n        if sensor_type == \'sensor.other.collision\':\n            return CollisionDetector(**kwargs)\n\n        elif sensor_type == \'sensor.other.lane_invasion\':\n            return LaneInvasionSensor(**kwargs)\n\n        elif sensor_type == \'sensor.other.gnss\':\n            return GnssSensor(**kwargs)\n\n        elif sensor_type == \'sensor.other.imu\':\n            return IMUSensor(**kwargs)\n\n        elif sensor_type == \'sensor.camera.rgb\':\n            return RGBCameraSensor(**kwargs)\n\n        elif sensor_type == \'sensor.camera.semantic_segmentation\':\n            return SemanticCameraSensor(**kwargs)\n\n        elif sensor_type == \'sensor.camera.depth\':\n            return DepthCameraSensor(**kwargs)\n\n        elif sensor_type == \'sensor.other.obstacle\':\n            return ObstacleDetector(**kwargs)\n\n        elif sensor_type == \'sensor.lidar.ray_cast\':\n            return LidarSensor(**kwargs)\n\n        elif sensor_type == \'sensor.other.radar\':\n            return RadarSensor(**kwargs)\n        else:\n            raise ValueError(f\'String `{sensor_type}` does not denote a valid sensor!\')\n\n    def start(self):\n        """"""Start listening for events""""""\n        if not self.sensor.is_listening:\n            self.sensor.listen(self.on_event)\n        else:\n            print(f\'Sensor {self.name} is already been started!\')\n\n    def stop(self):\n        """"""Stop listening for events""""""\n        self.sensor.stop()\n\n    def _spawn(self, transform, attachment_type=None):\n        """"""Spawns itself within a carla.World.""""""\n        if attachment_type is None:\n            attachment_type = carla.AttachmentType.Rigid\n\n        sensor_bp: carla.ActorBlueprint = self.world.get_blueprint_library().find(self.name)\n\n        for attr, value in self.attributes.items():\n            if sensor_bp.has_attribute(attr):\n                sensor_bp.set_attribute(attr, str(value))\n            else:\n                print(f\'Sensor {self.name} has no attribute `{attr}`\')\n\n        sensor_actor = self.world.spawn_actor(sensor_bp, transform, self.parent, attachment_type)\n        is_detector = not sensor_bp.has_attribute(\'sensor_tick\')\n\n        return sensor_actor, is_detector\n\n    def on_event(self, event):\n        for callback in self.event_callbacks:\n            callback(event)\n\n    def destroy(self):\n        if self.sensor is not None:\n            self.sensor.stop()\n            self.sensor.destroy()\n            self.sensor = None\n\n        self.parent = None\n        self.world = None\n\n\n# -------------------------------------------------------------------------------------------------\n# -- Camera Sensors\n# -------------------------------------------------------------------------------------------------\n\nclass CameraSensor(Sensor):\n    def __init__(self, color_converter=carla.ColorConverter.Raw, **kwargs):\n        super().__init__(**kwargs)\n        self.color_converter = color_converter\n\n    @property\n    def name(self):\n        raise NotImplementedError\n\n    def convert_image(self, image: carla.Image, color_converter=None):\n        color_converter = color_converter or self.color_converter or carla.ColorConverter.Raw\n        image.convert(color_converter)\n\n        array = np.frombuffer(image.raw_data, dtype=np.uint8)\n        array = np.reshape(array, (image.height, image.width, 4))\n        array = array[:, :, :3]\n        array = array[:, :, ::-1]\n        return array\n\n    def save_to_disk(self, image: carla.Image, path: str):\n        """"""Saves the carla.Image to disk using its color_converter.""""""\n        assert isinstance(image, carla.Image)\n        assert isinstance(path, str)\n        image.save_to_disk(path, color_converter=self.color_converter)\n\n\nclass RGBCameraSensor(CameraSensor):\n    @property\n    def name(self):\n        return \'sensor.camera.rgb\'\n\n\nclass DepthCameraSensor(CameraSensor):\n    @property\n    def name(self):\n        return \'sensor.camera.depth\'\n\n\nclass SemanticCameraSensor(CameraSensor):\n    @property\n    def name(self):\n        return \'sensor.camera.semantic_segmentation\'\n\n\n# -------------------------------------------------------------------------------------------------\n# -- Detector Sensors\n# -------------------------------------------------------------------------------------------------\n\nclass CollisionDetector(Sensor):\n    def __init__(self, parent_actor, **kwargs):\n        super().__init__(parent_actor, **kwargs)\n\n    @property\n    def name(self):\n        return \'sensor.other.collision\'\n\n\nclass LaneInvasionSensor(Sensor):\n    def __init__(self, parent_actor, **kwargs):\n        super().__init__(parent_actor, **kwargs)\n\n    @property\n    def name(self):\n        return \'sensor.other.lane_invasion\'\n\n\nclass ObstacleDetector(Sensor):\n    def __init__(self, parent_actor, **kwargs):\n        super().__init__(parent_actor, **kwargs)\n\n    @property\n    def name(self):\n        return \'sensor.other.obstacle\'\n\n\n# -------------------------------------------------------------------------------------------------\n# -- Other Sensors\n# -------------------------------------------------------------------------------------------------\n\nclass LidarSensor(Sensor):\n    def __init__(self, parent_actor, **kwargs):\n        super().__init__(parent_actor, **kwargs)\n\n    @property\n    def name(self):\n        return \'sensor.lidar.ray_cast\'\n\n\nclass RadarSensor(Sensor):\n    def __init__(self, parent_actor, **kwargs):\n        super().__init__(parent_actor, **kwargs)\n\n    @property\n    def name(self):\n        return \'sensor.other.radar\'\n\n    @staticmethod\n    def convert(radar_measurement: carla.RadarMeasurement):\n        """"""Converts a carla.RadarMeasurement into a numpy array [[velocity, altitude, azimuth, depth]]""""""\n        points = np.frombuffer(radar_measurement.raw_data, dtype=np.dtype(\'f4\'))\n        points = np.reshape(points, (len(radar_measurement), 4))\n        return points\n\n\nclass GnssSensor(Sensor):\n    def __init__(self, parent_actor, transform=carla.Transform(carla.Location(x=1.0, z=2.8)), **kwargs):\n        super().__init__(parent_actor, transform=transform, **kwargs)\n        self.lat = 0.0\n        self.lon = 0.0\n\n    @property\n    def name(self):\n        return \'sensor.other.gnss\'\n\n    def on_event(self, event):\n        super().on_event(event)\n        self.lat = event.latitude\n        self.lon = event.longitude\n\n    def destroy(self):\n        super().destroy()\n        self.lat = None\n        self.lon = None\n\n\nclass IMUSensor(Sensor):\n    def __init__(self, parent_actor, **kwargs):\n        super().__init__(parent_actor, **kwargs)\n        self.accelerometer = (0.0, 0.0, 0.0)\n        self.gyroscope = (0.0, 0.0, 0.0)\n        self.compass = 0.0\n\n    @property\n    def name(self):\n        return \'sensor.other.imu\'\n\n    def on_event(self, event):\n        super().on_event(event)\n        limits = (-99.9, 99.9)\n\n        self.accelerometer = (\n            max(limits[0], min(limits[1], event.accelerometer.x)),\n            max(limits[0], min(limits[1], event.accelerometer.y)),\n            max(limits[0], min(limits[1], event.accelerometer.z)))\n\n        self.gyroscope = (\n            max(limits[0], min(limits[1], math.degrees(event.gyroscope.x))),\n            max(limits[0], min(limits[1], math.degrees(event.gyroscope.y))),\n            max(limits[0], min(limits[1], math.degrees(event.gyroscope.z))))\n\n        self.compass = math.degrees(event.compass)\n\n    def destroy(self):\n        super().destroy()\n        self.accelerometer = None\n        self.gyroscope = None\n        self.compass = None\n\n\n# -------------------------------------------------------------------------------------------------\n# -- Sensors specifications\n# -------------------------------------------------------------------------------------------------\n\nclass SensorSpecs(object):\n    ATTACHMENT_TYPE = {\'SpringArm\': carla.AttachmentType.SpringArm,\n                       \'Rigid\': carla.AttachmentType.Rigid,\n                       None: carla.AttachmentType.Rigid}\n\n    COLOR_CONVERTER = {\'Raw\': carla.ColorConverter.Raw,\n                       \'CityScapesPalette\': carla.ColorConverter.CityScapesPalette,\n                       \'Depth\': carla.ColorConverter.Depth,\n                       \'LogarithmicDepth\': carla.ColorConverter.LogarithmicDepth,\n                       None: carla.ColorConverter.Raw}\n\n    @staticmethod\n    def get_position(position: str = None) -> carla.Transform:\n        if position == \'top\':\n            return carla.Transform(carla.Location(x=-5.5, z=2.5), carla.Rotation(pitch=8.0))\n        elif position == \'top-view\':\n            return carla.Transform(carla.Location(x=-8.0, z=6.0), carla.Rotation(pitch=6.0))\n        elif position == \'front\':\n            return carla.Transform(carla.Location(x=1.5, z=1.8))\n        elif position == \'on-top\':\n            return carla.Transform(carla.Location(x=-0.9, y=0.0, z=2.2))\n        elif position == \'on-top2\':\n            return carla.Transform(carla.Location(x=0.0, y=0.0, z=2.2))\n        elif position == \'radar\':\n            return carla.Transform(carla.Location(x=2.8, z=1.0), carla.Rotation(pitch=5))\n        else:\n            return carla.Transform()\n\n    @staticmethod\n    def set(sensor_spec: dict, **kwargs):\n        for key, value in kwargs.items():\n            if key == \'position\':\n                sensor_spec[\'transform\'] = SensorSpecs.get_position(value)\n            elif key == \'attachment_type\':\n                sensor_spec[key] = SensorSpecs.ATTACHMENT_TYPE[value]\n            elif key == \'color_converter\':\n                sensor_spec[key] = SensorSpecs.COLOR_CONVERTER[value]\n\n    @staticmethod\n    def add_callback(sensor_spec: dict, callback):\n        assert callable(callback)\n        assert isinstance(sensor_spec, dict)\n\n        attributes = sensor_spec.get(\'attributes\', dict())\n\n        if \'callback\' in attributes:\n            attributes[\'callbacks\'] = [callback, attributes.pop(\'callback\')]\n\n        elif \'callbacks\' in attributes:\n            attributes[\'callbacks\'].append(callback)\n        else:\n            attributes[\'callback\'] = callback\n\n        sensor_spec[\'attributes\'] = attributes\n\n    @staticmethod\n    def set_color_converter(camera_spec: dict, color_converter: str = None):\n        camera_spec[\'color_converter\'] = SensorSpecs.COLOR_CONVERTER[color_converter]\n        return SensorSpecs\n\n    @staticmethod\n    def camera(kind: str, transform: carla.Transform = None, position: str = None, attachment_type=None,\n               color_converter=None, **kwargs) -> dict:\n        assert kind in [\'rgb\', \'depth\', \'semantic_segmentation\']\n        return dict(type=\'sensor.camera.\' + kind,\n                    transform=transform or SensorSpecs.get_position(position),\n                    attachment_type=SensorSpecs.ATTACHMENT_TYPE[attachment_type],\n                    color_converter=SensorSpecs.COLOR_CONVERTER[color_converter],\n                    attributes=kwargs)\n\n    @staticmethod\n    def rgb_camera(transform: carla.Transform = None, position: str = None, attachment_type=\'SpringArm\',\n                   color_converter=\'Raw\', **kwargs):\n        return SensorSpecs.camera(\'rgb\', transform, position, attachment_type, color_converter, **kwargs)\n\n    @staticmethod\n    def depth_camera(transform: carla.Transform = None, position: str = None, attachment_type=\'SpringArm\',\n                     color_converter=\'LogarithmicDepth\', **kwargs):\n        return SensorSpecs.camera(\'depth\', transform, position, attachment_type, color_converter, **kwargs)\n\n    @staticmethod\n    def segmentation_camera(transform: carla.Transform = None, position: str = None, attachment_type=\'SpringArm\',\n                            color_converter=\'CityScapesPalette\', **kwargs):\n        return SensorSpecs.camera(\'semantic_segmentation\', transform, position, attachment_type, color_converter, **kwargs)\n\n    @staticmethod\n    def detector(kind: str, transform: carla.Transform = None, position: str = None, attachment_type=None,\n                 **kwargs) -> dict:\n        assert kind in [\'collision\', \'lane_invasion\', \'obstacle\']\n        return dict(type=\'sensor.other.\' + kind,\n                    transform=transform or SensorSpecs.get_position(position),\n                    attachment_type=SensorSpecs.ATTACHMENT_TYPE[attachment_type],\n                    attributes=kwargs)\n\n    @staticmethod\n    def collision_detector(transform: carla.Transform = None, position: str = None, attachment_type=\'Rigid\', **kwargs):\n        return SensorSpecs.detector(\'collision\', transform, position, attachment_type, **kwargs)\n\n    @staticmethod\n    def lane_detector(transform: carla.Transform = None, position: str = None, attachment_type=\'Rigid\', **kwargs):\n        return SensorSpecs.detector(\'lane_invasion\', transform, position, attachment_type, **kwargs)\n\n    @staticmethod\n    def obstacle_detector(transform: carla.Transform = None, position: str = None, attachment_type=\'Rigid\', **kwargs):\n        return SensorSpecs.detector(\'obstacle\', transform, position, attachment_type, **kwargs)\n\n    @staticmethod\n    def other(kind: str, transform: carla.Transform = None, position: str = None, attachment_type=None, **kwargs) -> dict:\n        assert kind in [\'imu\', \'gnss\', \'radar\']\n        return dict(type=\'sensor.other.\' + kind,\n                    transform=transform or SensorSpecs.get_position(position),\n                    attachment_type=SensorSpecs.ATTACHMENT_TYPE[attachment_type],\n                    attributes=kwargs)\n\n    @staticmethod\n    def lidar(transform: carla.Transform = None, position: str = None, attachment_type=None, **kwargs) -> dict:\n        return dict(type=\'sensor.lidar.ray_cast\',\n                    transform=transform or SensorSpecs.get_position(position),\n                    attachment_type=SensorSpecs.ATTACHMENT_TYPE[attachment_type],\n                    attributes=kwargs)\n\n    @staticmethod\n    def radar(transform: carla.Transform = None, position: str = None, attachment_type=\'Rigid\', **kwargs):\n        return SensorSpecs.other(\'radar\', transform, position, attachment_type, **kwargs)\n\n    @staticmethod\n    def imu(transform: carla.Transform = None, position: str = None, attachment_type=\'Rigid\', **kwargs):\n        return SensorSpecs.other(\'imu\', transform, position, attachment_type, **kwargs)\n\n    @staticmethod\n    def gnss(transform: carla.Transform = None, position: str = None, attachment_type=\'Rigid\', **kwargs):\n        return SensorSpecs.other(\'imu\', transform, position, attachment_type, **kwargs)\n'"
tensorforce/environments/carla/synchronous_mode.py,0,"b'import carla\nimport queue\n\n\nclass CARLASyncContext(object):\n    """"""\n    Context manager to synchronize output from different sensors. Synchronous\n    mode is enabled as long as we are inside this context\n\n        with CARLASyncContext(world, sensors) as sync_mode:\n            while True:\n                data = sync_mode.tick(timeout=1.0)\n\n    This code is based on https://github.com/carla-simulator/carla/blob/master/PythonAPI/examples/synchronous_mode.py\n    """"""\n\n    def __init__(self, world, sensors: dict, fps=30):\n        self.world = world\n        self.sensors = sensors\n        self.frame = None\n        self.delta_seconds = 1.0 / fps\n        self._settings = None\n\n        # Make a queue for each sensor and for world:\n        self._queues = dict()\n        self._add_queue(\'world\', self.world.on_tick)\n\n        for name, sensor in self.sensors.items():\n            self._add_queue(name, sensor.add_callback)\n\n    def __enter__(self):\n        self._settings = self.world.get_settings()\n        self.frame = self.world.apply_settings(carla.WorldSettings(\n            no_rendering_mode=False,\n            fixed_delta_seconds=self.delta_seconds,\n            synchronous_mode=True))\n\n        for sensor in self.sensors.values():\n            sensor.start()\n\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.world.apply_settings(self._settings)\n\n        for sensor in self.sensors.values():\n            sensor.stop()\n\n    def tick(self, timeout):\n        self.frame = self.world.tick()\n\n        data = dict()\n        for name, q in self._queues.items():\n\n            if name != \'world\' and self.sensors[name].is_detector:\n                # Detectors retrieve data only when triggered so have to not wait\n                data[name] = self._get_detector_data(q)\n            else:\n                # Cameras + other are sensors that retrieve data at every simulation step\n                data[name] = self._get_sensor_data(q, timeout)\n\n        return data\n\n    def _add_queue(self, name, register_event):\n        """"""Registers an even on its own queue identified by name""""""\n        q = queue.Queue()\n        register_event(q.put)\n        self._queues[name] = q\n\n    @staticmethod\n    def _get_detector_data(sensor_queue: queue.Queue):\n        """"""Retrieves data for detector, the call is non-blocking thus doesn\'t wait for available data.""""""\n        data = []\n\n        while not sensor_queue.empty():\n            data.append(sensor_queue.get_nowait())\n\n        return data\n\n    def _get_sensor_data(self, sensor_queue: queue.Queue, timeout: float):\n        """"""Retrieves data for sensors (i.e. camera and other) it blocks waiting until timeout is expired.""""""\n        while True:\n            data = sensor_queue.get(timeout=timeout)\n\n            if data.frame == self.frame:\n                return data\n'"
tensorforce/core/optimizers/solvers/__init__.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core.optimizers.solvers.solver import Solver\nfrom tensorforce.core.optimizers.solvers.iterative import Iterative\n\nfrom tensorforce.core.optimizers.solvers.conjugate_gradient import ConjugateGradient\nfrom tensorforce.core.optimizers.solvers.line_search import LineSearch\n\n\nsolver_modules = dict(conjugate_gradient=ConjugateGradient, line_search=LineSearch)\n\n\n__all__ = [\'ConjugateGradient\', \'Iterative\', \'LineSearch\', \'Solver\', \'solver_modules\']\n'"
tensorforce/core/optimizers/solvers/conjugate_gradient.py,13,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.optimizers.solvers import Iterative\n\n\nclass ConjugateGradient(Iterative):\n    """"""\n    Conjugate gradient algorithm which iteratively finds a solution $x$ for a system of linear  \n    equations of the form $A x = b$, where $A x$ could be, for instance, a locally linear  \n    approximation of a high-dimensional function.\n\n    See below pseudo-code taken from  \n    [Wikipedia](https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm):\n\n    ```text\n    def conjgrad(A, b, x_0):\n        r_0 := b - A * x_0\n        c_0 := r_0\n        r_0^2 := r^T * r\n\n        for t in 0, ..., max_iterations - 1:\n            Ac := A * c_t\n            cAc := c_t^T * Ac\n            \\alpha := r_t^2 / cAc\n            x_{t+1} := x_t + \\alpha * c_t\n            r_{t+1} := r_t - \\alpha * Ac\n            r_{t+1}^2 := r_{t+1}^T * r_{t+1}\n            if r_{t+1} < \\epsilon:\n                break\n            \\beta = r_{t+1}^2 / r_t^2\n            c_{t+1} := r_{t+1} + \\beta * c_t\n\n        return x_{t+1}\n    ```\n\n    """"""\n\n    def __init__(self, name, max_iterations, damping, unroll_loop=False):\n        """"""\n        Creates a new conjugate gradient solver instance.\n\n        Args:\n            max_iterations (parameter, int >= 0): Maximum number of iterations before termination.\n            damping (parameter, 0.0 <= float <= 1.0): Damping factor.\n            unroll_loop: Unrolls the TensorFlow while loop if true.\n        """"""\n        super().__init__(name=name, max_iterations=max_iterations, unroll_loop=unroll_loop)\n\n        self.damping = self.add_module(\n            name=\'damping\', module=damping, modules=parameter_modules, dtype=\'float\', min_value=0.0,\n            max_value=1.0\n        )\n\n    def tf_solve(self, fn_x, x_init, b):\n        """"""\n        Iteratively solves the system of linear equations $A x = b$.\n\n        Args:\n            fn_x: A callable returning the left-hand side $A x$ of the system of linear equations.\n            x_init: Initial solution guess $x_0$, zero vector if None.\n            b: The right-hand side $b$ of the system of linear equations.\n\n        Returns:\n            A solution $x$ to the problem as given by the solver.\n        """"""\n        return super().tf_solve(fn_x, x_init, b)\n\n    def tf_step(self, x, conjugate, residual, squared_residual):\n        """"""\n        Iteration loop body of the conjugate gradient algorithm.\n\n        Args:\n            x: Current solution estimate $x_t$.\n            conjugate: Current conjugate $c_t$.\n            residual: Current residual $r_t$.\n            squared_residual: Current squared residual $r_t^2$.\n\n        Returns:\n            Updated arguments for next iteration.\n        """"""\n\n        # Ac := A * c_t\n        A_conjugate = self.fn_x(conjugate)\n\n        # TODO: reference?\n        damping = self.damping.value()\n\n        def no_damping():\n            return A_conjugate\n\n        def apply_damping():\n            return [A_conj + damping * conj for A_conj, conj in zip(A_conjugate, conjugate)]\n\n        zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype=\'float\'))\n        skip_damping = tf.math.equal(x=damping, y=zero)\n        A_conjugate = self.cond(pred=skip_damping, true_fn=no_damping, false_fn=apply_damping)\n\n        # cAc := c_t^T * Ac\n        conjugate_A_conjugate = tf.add_n(\n            inputs=[\n                tf.reduce_sum(input_tensor=(conj * A_conj))\n                for conj, A_conj in zip(conjugate, A_conjugate)\n            ]\n        )\n\n        # \\alpha := r_t^2 / cAc\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        alpha = squared_residual / tf.maximum(x=conjugate_A_conjugate, y=epsilon)\n\n        # x_{t+1} := x_t + \\alpha * c_t\n        next_x = [t + alpha * conj for t, conj in zip(x, conjugate)]\n\n        # r_{t+1} := r_t - \\alpha * Ac\n        next_residual = [res - alpha * A_conj for res, A_conj in zip(residual, A_conjugate)]\n\n        # r_{t+1}^2 := r_{t+1}^T * r_{t+1}\n        next_squared_residual = tf.add_n(\n            inputs=[tf.reduce_sum(input_tensor=(res * res)) for res in next_residual]\n        )\n\n        # \\beta = r_{t+1}^2 / r_t^2\n        beta = next_squared_residual / tf.maximum(x=squared_residual, y=epsilon)\n\n        # c_{t+1} := r_{t+1} + \\beta * c_t\n        next_conjugate = [res + beta * conj for res, conj in zip(next_residual, conjugate)]\n\n        return next_x, next_conjugate, next_residual, next_squared_residual\n\n    def tf_next_step(self, x, conjugate, residual, squared_residual):\n        """"""\n        Termination condition: max number of iterations, or residual sufficiently small.\n\n        Args:\n            x: Current solution estimate $x_t$.\n            conjugate: Current conjugate $c_t$.\n            residual: Current residual $r_t$.\n            squared_residual: Current squared residual $r_t^2$.\n\n        Returns:\n            True if another iteration should be performed.\n        """"""\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n\n        return squared_residual >= epsilon\n\n    def tf_start(self, x_init, b):\n        """"""\n        Initialization step preparing the arguments for the first iteration of the loop body:  \n        $x_0, 0, p_0, r_0, r_0^2$.\n\n        Args:\n            x_init: Initial solution guess $x_0$, zero vector if None.\n            b: The right-hand side $b$ of the system of linear equations.\n\n        Returns:\n            Initial arguments for tf_step.\n        """"""\n        if x_init is None:\n            # Initial guess is zero vector if not given.\n            x_init = [tf.zeros_like(input=t, dtype=util.tf_dtype(dtype=\'float\')) for t in b]\n\n        # r_0 := b - A * x_0\n        # c_0 := r_0\n        conjugate = residual = [t - fx for t, fx in zip(b, self.fn_x(x_init))]\n\n        # r_0^2 := r^T * r\n        squared_residual = tf.add_n(\n            inputs=[tf.reduce_sum(input_tensor=(res * res)) for res in residual]\n        )\n\n        return x_init, conjugate, residual, squared_residual\n'"
tensorforce/core/optimizers/solvers/iterative.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce import util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.optimizers.solvers import Solver\n\n\nclass Iterative(Solver):\n    """"""\n    Generic solver which *iteratively* solves an equation/optimization problem. Involves an  \n    initialization step, the iteration loop body and the termination condition.\n    """"""\n\n    def __init__(self, name, max_iterations, unroll_loop):\n        """"""\n        Creates a new iterative solver instance.\n\n        Args:\n            max_iterations (parameter, int >= 0): Maximum number of iterations before termination.\n            unroll_loop: Unrolls the TensorFlow while loop if true.\n        """"""\n        super().__init__(name=name)\n\n        assert isinstance(unroll_loop, bool)\n        self.unroll_loop = unroll_loop\n\n        if self.unroll_loop:\n            self.max_iterations = max_iterations\n        else:\n            self.max_iterations = self.add_module(\n                name=\'max-iterations\', module=max_iterations, modules=parameter_modules,\n                dtype=\'int\', min_value=0\n            )\n\n    def tf_solve(self, fn_x, x_init, *args):\n        """"""\n        Iteratively solves an equation/optimization for $x$ involving an expression $f(x)$.\n\n        Args:\n            fn_x: A callable returning an expression $f(x)$ given $x$.\n            x_init: Initial solution guess $x_0$.\n            *args: Additional solver-specific arguments.\n\n        Returns:\n            A solution $x$ to the problem as given by the solver.\n        """"""\n        self.fn_x = fn_x\n\n        # Initialization step\n        args = self.start(x_init, *args)\n\n        # Iteration loop with termination condition\n        if self.unroll_loop:\n            # Unrolled for loop\n            for _ in range(self.max_iterations):\n                next_step = self.next_step(*args)\n                step = (lambda: self.step(*args))\n                do_nothing = (lambda: args)\n                args = self.cond(pred=next_step, true_fn=step, false_fn=do_nothing)\n\n        else:\n            # TensorFlow while loop\n            max_iterations = self.max_iterations.value()\n            args = self.while_loop(\n                cond=self.next_step, body=self.step, loop_vars=args, back_prop=False,\n                maximum_iterations=max_iterations\n            )\n\n        solution = self.end(*args)\n\n        return solution\n\n    def tf_start(self, x_init, *args):\n        """"""\n        Initialization step preparing the arguments for the first iteration of the loop body.\n\n        Args:\n            x_init: Initial solution guess $x_0$.\n            *args: Additional solver-specific arguments.\n\n        Returns:\n            Initial arguments for tf_step.\n        """"""\n        return (x_init,) + args\n\n    def tf_step(self, x, *args):\n        """"""\n        Iteration loop body of the iterative solver.\n\n        Args:\n            x: Current solution estimate.\n            *args: Additional solver-specific arguments.\n\n        Returns:\n            Updated arguments for next iteration.\n        """"""\n        raise NotImplementedError\n\n    def tf_next_step(self, x, *args):\n        """"""\n        Termination condition (default: max number of iterations).\n\n        Args:\n            x: Current solution estimate.\n            *args: Additional solver-specific arguments.\n\n        Returns:\n            True if another iteration should be performed.\n        """"""\n        return util.tf_always_true()\n\n    def tf_end(self, x_final, *args):\n        """"""\n        Termination step preparing the return value.\n\n        Args:\n            x: Final solution estimate.\n            *args: Additional solver-specific arguments.\n\n        Returns:\n            Final solution.\n        """"""\n        return x_final\n'"
tensorforce/core/optimizers/solvers/line_search.py,9,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorforce import TensorforceError, util\nfrom tensorforce.core import parameter_modules\nfrom tensorforce.core.optimizers.solvers import Iterative\n\n\nclass LineSearch(Iterative):\n    """"""\n    Line search algorithm which iteratively optimizes the value $f(x)$ for $x$ on the line between  \n    $x\'$ and $x_0$ by optimistically taking the first acceptable $x$ starting from $x_0$ and  \n    moving towards $x\'$.\n    """"""\n\n    def __init__(\n        self, name, max_iterations, accept_ratio, mode, parameter, unroll_loop=False\n    ):\n        """"""\n        Creates a new line search solver instance.\n\n        Args:\n            max_iterations (parameter, int >= 0): Maximum number of iterations before termination.\n            accept_ratio (parameter, 0.0 <= float <= 1.0): Lower limit of what improvement ratio\n                over $x = x\'$ is acceptable (based either on a given estimated improvement or with\n                respect to the value at   $x = x\'$).\n            mode: Mode of movement between $x_0$ and $x\'$, either \'linear\' or \'exponential\'.\n            parameter (parameter, 0.0 <= float <= 1.0): Movement mode parameter, additive or\n                multiplicative, respectively.\n            unroll_loop: Unrolls the TensorFlow while loop if true.\n        """"""\n        super().__init__(name=name, max_iterations=max_iterations, unroll_loop=unroll_loop)\n\n        assert accept_ratio >= 0.0\n        self.accept_ratio = self.add_module(\n            name=\'accept-ratio\', module=accept_ratio, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0, max_value=1.0\n        )\n\n        # TODO: Implement such sequences more generally, also useful for learning rate decay or so.\n        if mode not in (\'linear\', \'exponential\'):\n            raise TensorforceError(\n                ""Invalid line search mode: {}, please choose one of \'linear\' or \'exponential\'"".format(mode)\n            )\n        self.mode = mode\n\n        self.parameter = self.add_module(\n            name=\'parameter\', module=parameter, modules=parameter_modules, dtype=\'float\',\n            min_value=0.0, max_value=1.0\n        )\n\n    def tf_solve(self, fn_x, x_init, base_value, target_value, estimated_improvement=None):\n        """"""\n        Iteratively optimizes $f(x)$ for $x$ on the line between $x\'$ and $x_0$.\n\n        Args:\n            fn_x: A callable returning the value $f(x)$ at $x$.\n            x_init: Initial solution guess $x_0$.\n            base_value: Value $f(x\')$ at $x = x\'$.\n            target_value: Value $f(x_0)$ at $x = x_0$.\n            estimated_improvement: Estimated improvement for $x = x_0$, $f(x\')$ if None.\n\n        Returns:\n            A solution $x$ to the problem as given by the solver.\n        """"""\n        return super().tf_solve(fn_x, x_init, base_value, target_value, estimated_improvement)\n\n    def tf_start(self, x_init, base_value, target_value, estimated_improvement):\n        """"""\n        Initialization step preparing the arguments for the first iteration of the loop body.\n\n        Args:\n            x_init: Initial solution guess $x_0$.\n            base_value: Value $f(x\')$ at $x = x\'$.\n            target_value: Value $f(x_0)$ at $x = x_0$.\n            estimated_improvement: Estimated value at $x = x_0$, $f(x\')$ if None.\n\n        Returns:\n            Initial arguments for tf_step.\n        """"""\n        self.base_value = base_value\n\n        if estimated_improvement is None:  # TODO: Is this a good alternative?\n            estimated_improvement = tf.abs(x=base_value)\n\n        difference = target_value - self.base_value\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        improvement = difference / tf.maximum(x=estimated_improvement, y=epsilon)\n\n        last_improvement = improvement - 1.0\n        parameter = self.parameter.value()\n\n        if self.mode == \'linear\':\n            deltas = [-t * parameter for t in x_init]\n            self.estimated_incr = -estimated_improvement * parameter\n\n        elif self.mode == \'exponential\':\n            deltas = [-t * parameter for t in x_init]\n\n        return x_init, deltas, improvement, last_improvement, estimated_improvement\n\n    def tf_step(self, x, deltas, improvement, last_improvement, estimated_improvement):\n        """"""\n        Iteration loop body of the line search algorithm.\n\n        Args:\n            x: Current solution estimate $x_t$.\n            deltas: Current difference $x_t - x\'$.\n            improvement: Current improvement $(f(x_t) - f(x\')) / v\'$.\n            last_improvement: Last improvement $(f(x_{t-1}) - f(x\')) / v\'$.\n            estimated_improvement: Current estimated value $v\'$.\n\n        Returns:\n            Updated arguments for next iteration.\n        """"""\n        next_x = [t + delta for t, delta in zip(x, deltas)]\n        parameter = self.parameter.value()\n\n        if self.mode == \'linear\':\n            next_deltas = deltas\n            next_estimated_improvement = estimated_improvement + self.estimated_incr\n\n        elif self.mode == \'exponential\':\n            next_deltas = [delta * parameter for delta in deltas]\n            next_estimated_improvement = estimated_improvement * parameter\n\n        target_value = self.fn_x(next_deltas)\n\n        difference = target_value - self.base_value\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        next_improvement = difference / tf.maximum(x=next_estimated_improvement, y=epsilon)\n\n        return next_x, next_deltas, next_improvement, improvement, next_estimated_improvement\n\n    def tf_next_step(self, x, deltas, improvement, last_improvement, estimated_improvement):\n        """"""\n        Termination condition: max number of iterations, or no improvement for last step, or  \n        improvement less than acceptable ratio, or estimated value not positive.\n\n        Args:\n            x: Current solution estimate $x_t$.\n            deltas: Current difference $x_t - x\'$.\n            improvement: Current improvement $(f(x_t) - f(x\')) / v\'$.\n            last_improvement: Last improvement $(f(x_{t-1}) - f(x\')) / v\'$.\n            estimated_improvement: Current estimated value $v\'$.\n\n        Returns:\n            True if another iteration should be performed.\n        """"""\n        improved = improvement > last_improvement\n        accept_ratio = self.accept_ratio.value()\n        next_step = tf.math.logical_and(x=improved, y=(improvement < accept_ratio))\n        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype=\'float\'))\n        return tf.math.logical_and(x=next_step, y=(estimated_improvement > epsilon))\n\n    def tf_end(self, x_final, deltas, improvement, last_improvement, estimated_improvement):\n        """"""\n        Termination step preparing the return value.\n\n        Args:\n            x_init: Final solution estimate $x_n$.\n            deltas: Current difference $x_n - x\'$.\n            improvement: Current improvement $(f(x_n) - f(x\')) / v\'$.\n            last_improvement: Last improvement $(f(x_{n-1}) - f(x\')) / v\'$.\n            estimated_improvement: Current estimated value $v\'$.\n\n        Returns:\n            Final solution.\n        """"""\n        def accept_deltas():\n            return [t + delta for t, delta in zip(x_final, deltas)]\n\n        def undo_deltas():\n            value = self.fn_x([-delta for delta in deltas])\n            with tf.control_dependencies(control_inputs=(value,)):\n                return util.fmap(function=util.identity_operation, xs=x_final)\n\n        skip_undo_deltas = improvement > last_improvement\n        x_final = self.cond(pred=skip_undo_deltas, true_fn=accept_deltas, false_fn=undo_deltas)\n        return x_final\n'"
tensorforce/core/optimizers/solvers/solver.py,0,"b'# Copyright 2018 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom tensorforce.core import Module\n\n\nclass Solver(Module):\n    """"""\n    Generic TensorFlow-based solver which solves a not yet further specified  \n    equation/optimization problem.\n    """"""\n\n    def tf_solve(self, fn_x, *args):\n        """"""\n        Solves an equation/optimization for $x$ involving an expression $f(x)$.\n\n        Args:\n            fn_x: A callable returning an expression $f(x)$ given $x$.\n            *args: Additional solver-specific arguments.\n\n        Returns:\n            A solution $x$ to the problem as given by the solver.\n        """"""\n        raise NotImplementedError\n'"
