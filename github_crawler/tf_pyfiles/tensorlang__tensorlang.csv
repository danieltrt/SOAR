file_path,api_count,code
core/native/initscript.py,0,"b'import os\nimport sys\nimport zipimport\n\nsys.frozen = True\n\nFILE_NAME = sys.executable\nDIR_NAME = os.path.dirname(sys.executable)\n\nTF_CPU_LIB = ""%s/lib/python3.5-cpu"" % os.path.dirname(DIR_NAME)\nTF_GPU_LIB = ""%s/lib/python3.5-gpu"" % os.path.dirname(DIR_NAME)\nif os.path.exists(""/usr/local/cuda""):\n    tf_lib = TF_GPU_LIB\nelse:\n    tf_lib = TF_CPU_LIB\nos.environ[""TF_LIBRARY_DIR""] = tf_lib\n\n# Setting the below to \'1\' or above filters out info. This messages like:\n# I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:865] OS X does not support NUMA - returning NUMA node zero\n# Setting the below to \'2\' or above filters out warnings. This hides messages like:\n# W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nsys.path = [\n    ""%s/tensorflow/python"" % tf_lib,\n    ""%s/tensorflow/contrib/tfprof/python/tools/tfprof"" % tf_lib,\n] + sys.path\n\nm = __import__(""__main__"")\nimporter = zipimport.zipimporter(os.path.dirname(os.__file__))\nname, ext = os.path.splitext(os.path.basename(os.path.normcase(FILE_NAME)))\nmoduleName = ""%s__main__"" % name\ncode = importer.get_code(moduleName)\nexec(code, m.__dict__)\n\nversionInfo = sys.version_info[:3]\nif versionInfo >= (2, 5, 0) and versionInfo <= (2, 6, 4):\n    module = sys.modules.get(""threading"")\n    if module is not None:\n        module._shutdown()\n'"
core/native/nao__init__.py,0,b'from nao import cli\n\ncli.main()\n'
core/native/setup.py,0,"b'#!/usr/bin/env python\n\nimport os\nimport sys\nimport glob\nimport importlib\n\nfrom cx_Freeze import setup, Executable\n\n_version = os.getenv(\'BUILD_COUNTER\', \'9\')\n\nbase = None\nif sys.platform == ""win32"":\n  base = ""Win32GUI""\n\n# Need to copy all external files too.\nexternal_dir = os.path.dirname(importlib.util.find_spec(\'external\').origin)\nexternal_resources = [f for f in glob.glob(os.path.join(external_dir, \'*\')) if f != \'__init__.py\' and f != \'__pycache__\']\n\nprint(""sys.path"", sys.path, file=sys.stderr)\n\nsetup(\n    name=""nao"",\n    version=""0.1"",\n    description=""Programming language for large scale computational networks"",\n    options={\n      ""build_exe"": {\n        ""replace_paths"": [(""%s/"" % p, ""//"") for p in sys.path],\n        ""bin_path_excludes"": [\n          ""bazel-out/"",\n          *os.environ[\'CXFREEZE_BIN_PATH_EXCLUDES\'].split(\':\'),\n          sys.prefix,\n        ],\n        ""bin_excludes"": [\n        ],\n        ""excludes"": [\n          ""setuptools"",\n          ""distutils"",\n        ],\n        ""includes"": [\n          ""pkg_resources._vendor.packaging.version"",\n          ""pkg_resources._vendor.packaging.specifiers"",\n          ""pkg_resources._vendor.packaging.requirements"",\n          ""pkg_resources._vendor.pyparsing"",\n          ""numpy.core._methods"",\n          ""numpy.lib.format"",\n\n          ""tensorflow.contrib.framework.python.ops.gen_variable_ops"",\n          ""tensorflow.contrib.layers.python.layers.utils"",\n\n        #   ""tensorflow.contrib.cudnn_rnn.python.ops._cudnn_rnn_ops"",\n        #   ""tensorflow.contrib.factorization.python.ops._clustering_ops"",\n        #   ""tensorflow.contrib.factorization.python.ops._factorization_ops"",\n        #   ""tensorflow.contrib.ffmpeg.ffmpeg"",\n        #   ""tensorflow.contrib.framework.python.ops._variable_ops"",\n        #   ""tensorflow.contrib.image.python.ops._image_ops"",\n        #   ""tensorflow.contrib.input_pipeline.python.ops._input_pipeline_ops"",\n        #   ""tensorflow.contrib.layers.python.ops._bucketization_op"",\n        #   ""tensorflow.contrib.layers.python.ops._sparse_feature_cross_op"",\n        #   ""tensorflow.contrib.nccl.python.ops._nccl_ops"",\n        #   ""tensorflow.contrib.rnn.python.ops._gru_ops"",\n        #   ""tensorflow.contrib.rnn.python.ops._lstm_ops"",\n        #   ""tensorflow.contrib.tensor_forest.hybrid.python.ops._training_ops"",\n        #   ""tensorflow.contrib.tensor_forest.python.ops._tensor_forest_ops"",\n          ""tensorflow""\n        ],\n        ""include_files"": [\n          *[(d, os.path.join(""lib"", ""python3.5"", ""external"", os.path.basename(d))) for d in external_resources]\n        ],\n        ""packages"": [\n          ""nao"",\n          ""nao_parser"",\n        ]\n      }\n    },\n    executables=[\n      Executable(\n          ""%s/nao__init__.py"" % os.path.dirname(__file__),\n          base=base,\n          initScript=""%s/initscript.py"" % os.path.realpath(os.path.dirname(__file__)),\n          targetName=""nao"")\n    ])\n'"
core/python/setup.py,0,"b'from glob import glob\nimport os\nfrom setuptools import setup, find_packages\n\n_version = os.getenv(\'BUILD_COUNTER\', \'9\')\n\ndef read(filename):\n  with open(os.path.join(os.path.dirname(__file__), filename)) as f:\n    return f.read()\n\nsetup(\n    name=\'nao\',\n    author=\'Adam Bouhenguel\',\n    author_email=\'adam@bouhenguel.com\',\n    url=\'https://github.com/ajbouh/nao\',\n    version=_version,\n    install_requires=[l for l in read(\'requirements.txt\').splitlines() if l and not l.startswith(""#"")],\n    packages=[*find_packages(\'src\'), *find_packages(\'gen\')],\n    package_dir={\'nao_parser\': \'gen/nao_parser\', \'\': \'src\'},\n    entry_points={\n      ""console_scripts"": [\n        ""nao = nao.cli:main""\n      ]\n    },\n    include_package_data=True,\n)\n'"
root/src/log.py,0,"b'from tensorflow.python.framework import dtypes\n\nimport sys\n\nfrom nao.run import graph_summary\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\ndef LogWithStep(summary_protobuf, global_step) -> dtypes.int64:\n  graph_summary.get_summary_writer().add_summary(summary_protobuf, global_step=global_step)\n  return 0\n\ndef Log(summary_protobuf) -> dtypes.int64:\n  eprint(""Writing summary!"")\n  graph_summary.get_summary_writer().add_summary(summary_protobuf)\n  return 0\n\ndef Debug(x) -> dtypes.int64:\n  eprint(""Debug %s"" % x)\n  return 0\n'"
core/native/vendor/cx_Freeze-5.0.1/setup.py,0,"b'""""""\nDistutils script for cx_Freeze.\n""""""\n\nimport cx_Freeze\nimport distutils.command.bdist_rpm\nimport distutils.command.build_ext\nimport distutils.command.install\nimport distutils.command.install_data\nimport distutils.sysconfig\nimport os\nimport sys\n\ntry:\n    from setuptools import setup, Extension\nexcept ImportError:\n    from distutils.core import setup\n    from distutils.extension import Extension\n\nif sys.platform == ""win32"":\n    import msilib\n    import distutils.command.bdist_msi\n\n    class bdist_msi(distutils.command.bdist_msi.bdist_msi):\n\n        def add_scripts(self):\n            distutils.command.bdist_msi.bdist_msi.add_scripts(self)\n            msilib.add_data(self.db, ""RemoveFile"",\n                    [(""cxFreezeBatch"", ""cx_Freeze"", ""cxfreeze*.bat"", ""Scripts"",\n                        2)])\n\n\nclass bdist_rpm(distutils.command.bdist_rpm.bdist_rpm):\n\n    # rpm automatically byte compiles all Python files in a package but we\n    # don\'t want that to happen for initscripts and samples so we tell it to\n    # ignore those files\n    def _make_spec_file(self):\n        specFile = distutils.command.bdist_rpm.bdist_rpm._make_spec_file(self)\n        specFile.insert(0, ""%define _unpackaged_files_terminate_build 0%{nil}"")\n        return specFile\n\n    def run(self):\n        distutils.command.bdist_rpm.bdist_rpm.run(self)\n        specFile = os.path.join(self.rpm_base, ""SPECS"",\n                ""%s.spec"" % self.distribution.get_name())\n        queryFormat = ""%{name}-%{version}-%{release}.%{arch}.rpm""\n        command = ""rpm -q --qf \'%s\' --specfile %s"" % (queryFormat, specFile)\n        origFileName = os.popen(command).read()\n        parts = origFileName.split(""-"")\n        parts.insert(2, ""py%s%s"" % sys.version_info[:2])\n        newFileName = ""-"".join(parts)\n        self.move_file(os.path.join(""dist"", origFileName),\n                os.path.join(""dist"", newFileName))\n\n\nclass build_ext(distutils.command.build_ext.build_ext):\n\n    def build_extension(self, ext):\n        if ""bases"" not in ext.name:\n            distutils.command.build_ext.build_ext.build_extension(self, ext)\n            return\n        if sys.platform == ""win32"" and self.compiler.compiler_type == ""mingw32"":\n            ext.sources.append(""source/bases/manifest.rc"")\n        os.environ[""LD_RUN_PATH""] = ""${ORIGIN}:${ORIGIN}/../lib""\n        objects = self.compiler.compile(ext.sources,\n                output_dir = self.build_temp,\n                include_dirs = ext.include_dirs,\n                debug = self.debug,\n                depends = ext.depends)\n        fileName = os.path.splitext(self.get_ext_filename(ext.name))[0]\n        fullName = os.path.join(self.build_lib, fileName)\n        libraryDirs = ext.library_dirs or []\n        libraries = self.get_libraries(ext)\n        extraArgs = ext.extra_link_args or []\n        if sys.platform == ""win32"":\n            compiler_type = self.compiler.compiler_type\n            if compiler_type == ""msvc"":\n                extraArgs.append(""/MANIFEST"")\n            elif compiler_type == ""mingw32"" and ""Win32GUI"" in ext.name:\n                extraArgs.append(""-mwindows"")\n        else:\n            vars = distutils.sysconfig.get_config_vars()\n            libraryDirs.append(vars[""LIBPL""])\n            abiflags = getattr(sys, ""abiflags"", """")\n            libraries.append(""python%s.%s%s"" % \\\n                    (sys.version_info[0], sys.version_info[1], abiflags))\n            if vars[""LINKFORSHARED""] and sys.platform != ""darwin"":\n                extraArgs.extend(vars[""LINKFORSHARED""].split())\n            if vars[""LIBS""]:\n                extraArgs.extend(vars[""LIBS""].split())\n            if vars[""LIBM""]:\n                extraArgs.append(vars[""LIBM""])\n            if vars[""BASEMODLIBS""]:\n                extraArgs.extend(vars[""BASEMODLIBS""].split())\n            if vars[""LOCALMODLIBS""]:\n                extraArgs.extend(vars[""LOCALMODLIBS""].split())\n            extraArgs.append(""-s"")\n        self.compiler.link_executable(objects, fullName,\n                libraries = libraries,\n                library_dirs = libraryDirs,\n                runtime_library_dirs = ext.runtime_library_dirs,\n                extra_postargs = extraArgs,\n                debug = self.debug)\n\n    def get_ext_filename(self, name):\n        fileName = distutils.command.build_ext.build_ext.get_ext_filename(self,\n                name)\n        if name.endswith(""util""):\n            return fileName\n        vars = distutils.sysconfig.get_config_vars()\n        soExt = vars.get(""EXT_SUFFIX"", vars.get(""SO""))\n        ext = self.compiler.exe_extension or """"\n        return fileName[:-len(soExt)] + ext\n\n\ndef find_cx_Logging():\n    dirName = os.path.dirname(os.getcwd())\n    loggingDir = os.path.join(dirName, ""cx_Logging"")\n    if not os.path.exists(loggingDir):\n        return\n    subDir = ""implib.%s-%s"" % (distutils.util.get_platform(), sys.version[:3])\n    importLibraryDir = os.path.join(loggingDir, ""build"", subDir)\n    if not os.path.exists(importLibraryDir):\n        return\n    return loggingDir, importLibraryDir\n\n\ncommandClasses = dict(\n        build_ext = build_ext,\n        bdist_rpm = bdist_rpm)\nif sys.platform == ""win32"":\n    commandClasses[""bdist_msi""] = bdist_msi\n\n# build utility module\nif sys.platform == ""win32"":\n    libraries = [""imagehlp"", ""Shlwapi""]\nelse:\n    libraries = []\nutilModule = Extension(""cx_Freeze.util"", [""source/util.c""],\n        libraries = libraries)\n\n# build base executables\ndocFiles = ""README.txt""\nscripts = [""cxfreeze"", ""cxfreeze-quickstart""]\noptions = dict(bdist_rpm = dict(doc_files = docFiles),\n        install = dict(optimize = 1))\ndepends = [""source/bases/Common.c""]\nextra_link_args = []\nif sys.platform == ""darwin"":\n    extra_link_args.append(\'-Wl,-rpath,@executable_path/../lib\')\nconsole = Extension(""cx_Freeze.bases.Console"", [""source/bases/Console.c""],\n        extra_link_args = extra_link_args,\n        depends = depends, libraries = libraries)\nextensions = [utilModule, console]\nif sys.platform == ""win32"":\n    scripts.append(""cxfreeze-postinstall"")\n    options[""bdist_msi""] = dict(install_script = ""cxfreeze-postinstall"")\n    gui = Extension(""cx_Freeze.bases.Win32GUI"", [""source/bases/Win32GUI.c""],\n            depends = depends, libraries = libraries + [""user32""])\n    extensions.append(gui)\n    moduleInfo = find_cx_Logging()\n    if moduleInfo is not None:\n        includeDir, libraryDir = moduleInfo\n        service = Extension(""cx_Freeze.bases.Win32Service"",\n                [""source/bases/Win32Service.c""], depends = depends,\n                library_dirs = [libraryDir],\n                libraries = libraries + [""advapi32"", ""cx_Logging""],\n                include_dirs = [includeDir])\n        extensions.append(service)\n\n# define package data\npackageData = []\nfor fileName in os.listdir(os.path.join(""cx_Freeze"", ""initscripts"")):\n    name, ext = os.path.splitext(fileName)\n    if ext != "".py"":\n        continue\n    packageData.append(""initscripts/%s"" % fileName)\nfor fileName in os.listdir(os.path.join(""cx_Freeze"", ""samples"")):\n    dirName = os.path.join(""cx_Freeze"", ""samples"", fileName)\n    if not os.path.isdir(dirName):\n        continue\n    packageData.append(""samples/%s/*.py"" % fileName)\n\nclassifiers = [\n        ""Development Status :: 5 - Production/Stable"",\n        ""Intended Audience :: Developers"",\n        ""License :: OSI Approved :: Python Software Foundation License"",\n        ""Natural Language :: English"",\n        ""Operating System :: OS Independent"",\n        ""Programming Language :: C"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 2"",\n        ""Programming Language :: Python :: 3"",\n        ""Topic :: Software Development :: Build Tools"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n        ""Topic :: System :: Software Distribution"",\n        ""Topic :: Utilities""\n]\n\nsetup(name = ""cx_Freeze"",\n        description = ""create standalone executables from Python scripts"",\n        long_description = ""create standalone executables from Python scripts"",\n        version = ""5.0.1"",\n        cmdclass = commandClasses,\n        options = options,\n        ext_modules = extensions,\n        packages = [\'cx_Freeze\'],\n        maintainer=""Anthony Tuininga"",\n        maintainer_email=""anthony.tuininga@gmail.com"",\n        url = ""http://cx-freeze.sourceforge.net"",\n        scripts = scripts,\n        classifiers = classifiers,\n        keywords = ""freeze"",\n        license = ""Python Software Foundation License"",\n        package_data = {""cx_Freeze"" : packageData })\n\n'"
core/python/src/nao/__init__.py,0,b''
core/python/src/nao/cli.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport datetime\n\nimport argparse\nimport json\nimport pprint\nimport re\nimport sys\nimport traceback\nimport gc\n\nfrom os import path\n\nfrom nao.compiler.compiler import Compiler\nfrom nao.compiler.asset import graph_assets\n\nfrom nao.structure import graph_io\nfrom nao.structure import graph_query\nfrom nao.structure import graph_xform\nfrom nao.run import graph_execution\nfrom nao.tool import graph_repl\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import meta_graph\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.core.framework import variable_pb2\nfrom tensorflow.core.protobuf import control_flow_pb2\n\nimport subprocess\n\ndef eprint(*args, **kwargs):\n    print(*args, file=sys.stderr, **kwargs)\n\npp = pprint.PrettyPrinter(indent=2, stream=sys.stderr).pprint\n\ndef main():\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(""package_names"", type=str, nargs=\'*\')\n  parser.add_argument(""--root"", metavar=\'DIR\', type=str,\n                      help=""""""Specify root directory to search for imports from"""""")\n  parser.add_argument(""--source"", type=str,\n                      help=""""""Specify source code instead of reading from file"""""")\n\n  parser.add_argument(""--reopen-stderr"", metavar=\'FILE\', type=argparse.FileType(\'a\', encoding=\'UTF-8\'))\n  parser.add_argument(""--reopen-stdout"", metavar=\'FILE\', type=argparse.FileType(\'a\', encoding=\'UTF-8\'))\n\n\n  parser.add_argument(""--assets-fetch"", default=False, action=\'store_const\', const=True,\n                      help=""""""Fetch any assets we don\'t already have."""""")\n  parser.add_argument(""--assets-root"", metavar=\'DIR\', type=str,\n                      help=""""""Specify root directory for assets."""""")\n\n  parser.add_argument(""--metagraphdef"", metavar=\'FILE\', type=str,\n                      help=""""""Graph file to load."""""")\n  parser.add_argument(""--binary-metagraphdef"", default=False, action=\'store_const\', const=True,\n                      help=""""""Whether or not input is binary."""""")\n  parser.add_argument(""--feed-constants"", metavar=\'FILE\', type=str,\n                      help=""""""Path to GraphDef protobuf with constants to feed"""""")\n  parser.add_argument(""--feed-constants-strip"", metavar=\'PREFIX\', type=str, default="""",\n                      help=""""""Prefix to filter for (and strip from) constants"""""")\n  parser.add_argument(""--feed-constants-prefix"", metavar=\'PREFIX\', type=str,\n                      help=""""""Prefix to add to constant names in feed"""""")\n  parser.add_argument(""--feed-constants-binary"", default=False, action=\'store_const\', const=True,\n                      help=""""""Whether or not feed constant protobuf is binary"""""")\n\n  parser.add_argument(""--run"", default=False, action=\'store_const\', const=True,\n                      help=""""""Run the graph with given (or default) --result* and --feed-* options"""""")\n  parser.add_argument(""--run-result-pattern"", metavar=\'PATTERN\', type=str, default=""^(${package}/Main)/outputs/(.*)$"",\n                      help=""""""Pattern to discover run results."""""")\n  parser.add_argument(""--result-binary"", default=False, action=\'store_const\', const=True,\n                      help=""""""Whether or not to result in binary."""""")\n  parser.add_argument(""--result"", metavar=\'FILE\', type=str, default=""/dev/stdout"")\n\n  parser.add_argument(""--test"", default=False, action=\'store_const\', const=True,\n                      help=""""""Run the tests graphs with given (or default) --test-* options"""""")\n  parser.add_argument(""--test-result-pattern"", metavar=\'PATTERN\', type=str, default=""^(${package}/Test[^/]*)/outputs/(.*)$"",\n                      help=""""""Pattern to discover test graph results."""""")\n\n  parser.add_argument(""--repl"", default=False, action=\'store_const\', const=True,\n                      help=""""""Start REPL"""""")\n\n  parser.add_argument(""--tensorboard"", nargs=\'?\', default="""", metavar=""IP:PORT"",\n                      help=""""""Start tensorboard server on the given address, with the given --log-root or --log-dir"""""")\n\n  parser.add_argument(""--jupyter-kernel"", nargs=\'?\', default="""", metavar=""CONFIG_FILE"",\n                      help=""""""Start Jupyter kernel with the given configuration file"""""")\n\n  parser.add_argument(""--train"", default=False, action=\'store_const\', const=True,\n                      help=""""""Run train graphs with given (or default) --train-* options"""""")\n  parser.add_argument(""--train-result-pattern"", metavar=\'PATTERN\', type=str, default=""^(${package}/Train[^/]*)/outputs/(.*)$"",\n                      help=""""""Pattern to discover train graph results."""""")\n\n  parser.add_argument(""--workspace"", metavar=\'DIR\', type=str,\n                      help=""""""Default value for workspace"""""")\n  parser.add_argument(""--log-root"", metavar=\'DIR\', type=str,\n                      help=""""""Which directory to calculate default log dir from."""""")\n  parser.add_argument(""--log-dir"", metavar=\'DIR\', type=str,\n                      help=""""""Which directory to put logs in."""""")\n\n  parser.add_argument(""--output"", default=False, action=\'store_const\', const=True,\n                      help=""""""Output graph"""""")\n  parser.add_argument(""--output-root"", metavar=\'DIR\', type=str,\n                      help=""""""When automatically constructing output path, use this as base"""""")\n  parser.add_argument(""--output-name"", metavar=\'NAME\', type=str,\n                      help=""""""Base name to use for output file name. Defaults to ${package} if there\'s only one."""""")\n  parser.add_argument(""--output-result-pattern"", metavar=\'PATTERN\', type=str, default=""^(${package}/[^/]*)(/outputs/[^/]*)?$"",\n                      help=""""""Pattern to discover outputs of graph to output."""""")\n  parser.add_argument(""--output-format"", metavar=\'FORMAT\', type=str, default=""metagraph"",\n                      help=""""""Defaults to metagraph"""""")\n  parser.add_argument(""--output-binary"", default=False, action=\'store_const\', const=True,\n                      help=""""""Whether or not to output in binary."""""")\n  parser.add_argument(""--output-file"", metavar=\'FILE\', type=str,\n                      help=""""""Path to write output to. Defaults to ${output-name}.${output-format}"""""")\n\n  FLAGS = parser.parse_args()\n\n  if FLAGS.reopen_stderr:\n    os.close(sys.stderr.fileno())\n    os.dup2(FLAGS.reopen_stderr.fileno(), sys.stderr.fileno())\n\n  if FLAGS.reopen_stdout:\n    os.close(sys.stdout.fileno())\n    os.dup2(FLAGS.reopen_stdout.fileno(), sys.stdout.fileno())\n\n  package_names = FLAGS.package_names\n\n  should_parse = len(package_names) > 0 or FLAGS.source\n  if not (should_parse or FLAGS.run or FLAGS.test or FLAGS.output):\n    if os.isatty(1):\n      FLAGS.repl = True\n\n  if should_parse and not (FLAGS.repl or FLAGS.run or FLAGS.test or FLAGS.output):\n    FLAGS.output = True\n\n  def search_upwards(startdir, filename):\n    curdir = startdir\n    while True:\n      if path.exists(path.join(curdir, filename)):\n        return curdir\n      lastdir = curdir\n      curdir = path.dirname(curdir)\n      if curdir == lastdir:\n        return None\n\n  if not FLAGS.workspace:\n    FLAGS.workspace = os.environ.get(""NAOPATH"", """")\n    if not FLAGS.workspace:\n      FLAGS.workspace = search_upwards(os.getcwd(), "".naoconfig"")\n    if not FLAGS.workspace:\n      FLAGS.workspace = "".""\n\n  if FLAGS.assets_root is None:\n    FLAGS.assets_root = path.join(FLAGS.workspace, ""assets"")\n\n  if FLAGS.output_root is None:\n    FLAGS.output_root = path.join(FLAGS.workspace, ""pkg"")\n\n  if FLAGS.root is None:\n    FLAGS.root = path.join(FLAGS.workspace, ""src"")\n\n  if FLAGS.log_root is None:\n    FLAGS.log_root = path.join(FLAGS.workspace, ""log"")\n\n  if FLAGS.tensorboard is None:\n    FLAGS.tensorboard = ""127.0.0.1:6006""\n\n  def log_dir_fn_fn(pkg_names):\n    if FLAGS.log_dir:\n      return lambda: FLAGS.log_dir\n\n    session_name = datetime.datetime.utcnow().strftime(""%F_%H-%M-%S"")\n    base_log_dir = path.join(FLAGS.log_root, pkg_names[0], session_name)\n    def log_dir_fn(run_id=None):\n      log_dir = base_log_dir\n      if run_id is not None:\n        log_dir = path.join(log_dir, ""%04d"" % run_id)\n      return log_dir\n\n    return log_dir_fn\n\n  def new_compiler():\n    return Compiler(\n        FLAGS.root,\n        FLAGS.output_root,\n        FLAGS.assets_root)\n\n  meta_graph_def = None\n\n  output_package_names = None\n\n  if should_parse:\n    p = new_compiler()\n    if FLAGS.source:\n      package_name = ""main""\n      package_names = [package_name]\n      p.put_source(package_name + "".nao"", FLAGS.source)\n      p.resolve_import_path(package_name)\n    else:\n      # Look for matching packages _train\n      if FLAGS.train:\n        output_package_names = package_names[:]\n        package_names.extend([pkg + ""_train"" for pkg in package_names])\n\n      for package_name in package_names:\n        p.resolve_import_path(package_name)\n\n    meta_graph_def = p.meta_graph_def()\n    p = None\n    # print(""parsed"", expressions)\n    # We need to do this so we clean up references to py_funcs. LAME.\n    gc.collect()\n\n  # Sometimes we want to output different packages than we\'re testing, training, etc.\n  if output_package_names == None:\n    output_package_names = package_names\n\n  if not FLAGS.output_name and len(output_package_names) == 1:\n    FLAGS.output_name = output_package_names[0]\n    if FLAGS.train:\n      FLAGS.output_name += ""_trained""\n\n  if FLAGS.metagraphdef:\n    package_names = [""[^/]+""]\n    meta_graph_def = graph_io.read_meta_graph_def(\n        FLAGS.metagraphdef,\n        FLAGS.binary_metagraphdef)\n\n  if FLAGS.output and FLAGS.output_name and not FLAGS.output_file:\n    output_suffix = ""."" + FLAGS.output_format + "".pb""\n    if not FLAGS.output_binary:\n      output_suffix += ""txt""\n    FLAGS.output_file = FLAGS.output_root + ""/"" + FLAGS.output_name + output_suffix\n\n  # Now that we know our package names, use them to target the proper results.\n  package_pattern = ""(?:"" + str.join(""|"", package_names) + "")""\n  FLAGS.test_result_pattern = FLAGS.test_result_pattern.replace(""${package}"", package_pattern)\n  FLAGS.train_result_pattern = FLAGS.train_result_pattern.replace(""${package}"", package_pattern)\n  FLAGS.run_result_pattern = FLAGS.run_result_pattern.replace(""${package}"", package_pattern)\n\n  output_package_pattern = ""(?:"" + str.join(""|"", output_package_names) + "")""\n  FLAGS.output_result_pattern = FLAGS.output_result_pattern.replace(""${package}"", output_package_pattern)\n  eprint(""FLAGS"", FLAGS)\n  eprint(""package_names"", package_names)\n\n  if FLAGS.tensorboard != """":\n    tb_host, tb_port = FLAGS.tensorboard.split(\':\', 1)\n    tb_logdir = FLAGS.log_dir or FLAGS.log_root\n    if tb_port is not None:\n      tb_port = int(tb_port)\n\n    from nao.tool import tensorboard_server\n    sys.exit(tensorboard_server.main(tb_logdir, tb_host=tb_host, tb_port=tb_port))\n\n  if FLAGS.jupyter_kernel != """":\n    jupyter_config_file = FLAGS.jupyter_kernel\n    from nao.tool import jupyter_kernel, jupyter_kernel_driver\n\n    if jupyter_config_file:\n      eprint(""Reading jupyter_config file \'%s\'..."" % jupyter_config_file)\n      jupyter_config = json.loads("""".join(open(jupyter_config_file).readlines()))\n    else:\n      import uuid\n      jupyter_config = {\n        \'control_port\'      : 0,\n        \'hb_port\'           : 0,\n        \'iopub_port\'        : 0,\n        \'ip\'                : \'127.0.0.1\',\n        \'key\'               : str(uuid.uuid4()),\n        \'shell_port\'        : 0,\n        \'signature_scheme\'  : \'hmac-sha256\',\n        \'stdin_port\'        : 0,\n        \'transport\'         : \'tcp\'\n      }\n\n    pallet_parser = new_compiler()\n    repl_session = graph_repl.ReplSession(pallet_parser, log_dir_fn_fn([""jupyter""]))\n    driver = jupyter_kernel_driver.Driver(repl_session)\n    sys.exit(jupyter_kernel.Kernel(jupyter_config, driver.info(), driver.do).run())\n\n  def feed_dict_fn():\n    feed_dict = {}\n    # Properly find and strip prefix of constants, loading them with given prefix to feed_dict\n    if FLAGS.feed_constants:\n      feed_graph_def = graph_io.read_graph_def(FLAGS.feed_constants, FLAGS.feed_constants_binary)\n      constants = graph_query.find_nodes_with_prefix(feed_graph_def, FLAGS.feed_constants_strip)\n      constants_dict = graph_xform.constants_as_dict(constants)\n      strip_prefix = FLAGS.feed_constants_strip\n      add_prefix = FLAGS.feed_constants_prefix\n      for name, value in constants_dict.items():\n        if strip_prefix != None:\n          if name.startswith(strip_prefix):\n            name = name[len(strip_prefix):]\n          else:\n            continue\n        feed_dict[add_prefix + name + "":0""] = value\n\n    asset_map = graph_assets.load_asset_map(tf.get_default_graph())\n    eprint(""asset_map"", asset_map)\n\n    assets_by_path = {}\n    missing_assets = {}\n    for asset_name, asset in asset_map.items():\n      asset_path = path.join(FLAGS.assets_root, asset_name)\n      assets_by_path[asset_path] = asset\n      feed_dict[asset[""placeholder""]] = asset_path\n      if not os.path.exists(asset_path):\n        missing_assets[asset_path] = asset\n\n    if len(missing_assets) > 0:\n      if not FLAGS.assets_fetch:\n        raise Exception(""Missing assets: %s"" % missing_assets)\n\n      for asset_path, asset in missing_assets.items():\n        graph_assets.maybe_download(asset_path, asset[""url""])\n\n    eprint(""feed_dict"", feed_dict)\n    return feed_dict\n\n  if FLAGS.train:\n    def post_train(session, result_scope_prefixes):\n      graph = session.graph\n\n      trained_var_name_bs = set()\n      for result_scope_prefix in result_scope_prefixes:\n        collection_name = ""%s:variable_names"" % result_scope_prefix\n        eprint(""collection_name"", collection_name)\n        for var_name_b in graph.get_collection_ref(collection_name):\n          trained_var_name_bs.add(var_name_b)\n\n      var_names = [b.decode(\'utf-8\') for b in trained_var_name_bs]\n      vars = graph_query.find_variables_by_name(\n          graph.get_collection_ref(""variables""),\n          var_names)\n      eprint(""saving vars"", var_names, vars)\n      graph_xform.replace_variable_initializers_with_current_values(\n          graph,\n          vars,\n          ""Trained"")\n\n    graph_execution.import_and_run_meta_graph(\n      meta_graph_def=meta_graph_def,\n      feed_dict_fn=feed_dict_fn,\n      result_pattern=re.compile(FLAGS.train_result_pattern),\n      finish_session_fn=post_train,\n      log_dir_fn=lambda x: log_dir_fn_fn(x)(),\n    )\n    meta_graph_def, _ = meta_graph.export_scoped_meta_graph()\n\n  if FLAGS.test:\n    graph_execution.import_and_run_meta_graph(\n      meta_graph_def=meta_graph_def,\n      feed_dict_fn=feed_dict_fn,\n      log_dir_fn=lambda x: log_dir_fn_fn(x)(),\n      result_pattern=re.compile(FLAGS.test_result_pattern),\n    )\n\n  if meta_graph_def and FLAGS.output_file:\n    eprint(""meta_graph_def"", [n.name for n in meta_graph_def.graph_def.node])\n    graph_def = meta_graph_def.graph_def\n    output_re = re.compile(FLAGS.output_result_pattern)\n    output_node_names = [\'py_funcs_json\'] # HACK(adamb) So that pyfuncs still work.\n    var_names = set()\n    for n in graph_def.node:\n      m = output_re.match(n.name)\n      if not m:\n        continue\n      output_node_names.append(n.name)\n\n      # If this isn\'t a function, then we\'re covered. Otherwise pick up needed\n      # variables.\n      if not m.group(2):\n        continue\n\n      # Look for collection of variable names referenced by this function.\n      collection_name = ""%s:variable_names"" % m.group(1)\n      eprint(""collection_name"", collection_name)\n      function_var_name_bs = meta_graph_def.collection_def[collection_name].bytes_list.value\n      for var_name_b in function_var_name_bs:\n        # Remember the name of each variable referenced.\n        var_names.add(var_name_b.decode(\'utf-8\'))\n\n    eprint(""var_names"", var_names)\n    eprint(""output_node_names"", output_node_names)\n    graph_xform.strip_meta_graph(meta_graph_def, output_node_names, var_names)\n\n  if FLAGS.output_file:\n    output_dirname = os.path.dirname(FLAGS.output_file)\n    if not os.path.exists(output_dirname):\n      os.makedirs(output_dirname)\n\n    if FLAGS.output_format == ""metagraph"":\n      graph_io.write_meta_graph_def(\n        meta_graph_def=meta_graph_def,\n        file=FLAGS.output_file,\n        binary=FLAGS.output_binary)\n    elif FLAGS.output_format == ""graph"":\n      # If we trained and we\'re outputting a graph_def, we\'ll need to modify it.\n      # We\'ll need to replace all the trained variables with the *constants* that\n      # their initializers refer to.\n      if FLAGS.train:\n        pass\n      graph_io.write_graph_def(\n        graph_def=meta_graph_def.graph_def,\n        file=FLAGS.output_file,\n        binary=FLAGS.output_binary)\n\n  if FLAGS.run:\n    results = graph_execution.import_and_run_meta_graph(\n      meta_graph_def=meta_graph_def,\n      feed_dict_fn=feed_dict_fn,\n      log_dir_fn=lambda x: log_dir_fn_fn(x)(),\n      result_pattern=re.compile(FLAGS.run_result_pattern),\n    )\n\n    graph_def = graph_xform.dict_as_graph_def(results)\n    graph_io.write_graph_def(\n      graph_def,\n      file=FLAGS.result,\n      binary=FLAGS.result_binary,\n    )\n\n  if FLAGS.repl:\n    graph_repl.run(new_compiler(), log_dir_fn_fn([""repl""]))\n\nif __name__ == \'__main__\':\n  try:\n    main()\n    # with tf.device(""/cpu:0""):\n    #   main()\n  except Exception as ex:\n    # TODO(adamb) Should do *real* error printing.\n    # NOTE(adamb) Need to correlate expressions with line and character numbers!\n    traceback.print_exc(file=sys.stderr)\n    sys.exit(1)\n'"
root/src/datasets/mnist/format.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for reading MNIST data.""""""\n\nimport gzip\nimport numpy\n\nfrom tensorflow.python.framework import dtypes\n\nimport sys\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\ndef ReadImages(filename) -> dtypes.float32:\n  with open(filename, \'rb\') as f:\n    images = extract_images(f)\n    images = images.astype(numpy.float32)\n    images = numpy.multiply(images, 1.0 / 255.0)\n    images = images.reshape(images.shape[0], images.shape[1] * images.shape[2])\n    return images\n\ndef ReadLabels(filename) -> dtypes.float64:\n  with open(filename, \'rb\') as f:\n    return extract_labels(f, one_hot=True)\n\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n\ndef extract_images(f):\n  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n  Args:\n    f: A file object that can be passed into a gzip reader.\n  Returns:\n    data: A 4D uint8 numpy array [index, y, x, depth].\n  Raises:\n    ValueError: If the bytestream does not start with 2051.\n  """"""\n  with gzip.GzipFile(fileobj=f) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\'Invalid magic number %d in MNIST image file: %s\' %\n                       (magic, f.name))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\n\ndef dense_to_one_hot(labels_dense, num_classes):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\n\ndef extract_labels(f, one_hot=False, num_classes=10):\n  """"""Extract the labels into a 1D uint8 numpy array [index].\n  Args:\n    f: A file object that can be passed into a gzip reader.\n    one_hot: Does one hot encoding for the result.\n    num_classes: Number of classes for the one hot encoding.\n  Returns:\n    labels: a 1D uint8 numpy array.\n  Raises:\n    ValueError: If the bystream doesn\'t start with 2049.\n  """"""\n  with gzip.GzipFile(fileobj=f) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\'Invalid magic number %d in MNIST label file: %s\' %\n                       (magic, f.name))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels, num_classes)\n    return labels\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/__init__.py,0,"b'version = ""5.0""\n__version__ = version\n\nimport sys\nfrom cx_Freeze.dist import *\nif sys.platform == ""win32"":\n    from cx_Freeze.windist import *\nelif sys.platform == ""darwin"":\n    from cx_Freeze.macdist import *\nfrom cx_Freeze.finder import *\nfrom cx_Freeze.freezer import *\nfrom cx_Freeze.main import *\n\ndel dist    # noqa: F821 undefined name\ndel finder  # noqa: F821 undefined name\ndel freezer # noqa: F821 undefined name\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/common.py,0,"b'""""""\nThis module contains utility functions shared between cx_Freeze modules.\n""""""\n\n\ndef normalize_to_list(value):\n    """"""\n    Takes the different formats of options containing multiple values and\n    returns the value as a list object.\n    """"""\n    if value is None:\n        normalizedValue = []\n    elif isinstance(value, str):\n        normalizedValue = value.split(\',\')\n    else:\n        normalizedValue = list(value)\n\n    return normalizedValue\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/dist.py,0,"b'import distutils.command.bdist_rpm\nimport distutils.command.build\nimport distutils.command.install\nimport distutils.core\nimport distutils.dir_util\nimport distutils.dist\nimport distutils.errors\nimport distutils.log\nimport distutils.util\nimport distutils.version\nimport os\nimport sys\n\nimport cx_Freeze\nfrom cx_Freeze.common import normalize_to_list\n\n__all__ = [ ""bdist_rpm"", ""build"", ""build_exe"", ""install"", ""install_exe"",\n            ""setup"" ]\n\nclass Distribution(distutils.dist.Distribution):\n\n    def __init__(self, attrs):\n        self.executables = []\n        distutils.dist.Distribution.__init__(self, attrs)\n\n\nclass bdist_rpm(distutils.command.bdist_rpm.bdist_rpm):\n\n    def finalize_options(self):\n        distutils.command.bdist_rpm.bdist_rpm.finalize_options(self)\n        self.use_rpm_opt_flags = 1\n\n    def _make_spec_file(self):\n        contents = distutils.command.bdist_rpm.bdist_rpm._make_spec_file(self)\n        contents.append(\'%define __prelink_undo_cmd %{nil}\')\n        return [c for c in contents if c != \'BuildArch: noarch\']\n\n\nclass build(distutils.command.build.build):\n    user_options = distutils.command.build.build.user_options + [\n        (\'build-exe=\', None, \'build directory for executables\')\n    ]\n\n    def get_sub_commands(self):\n        subCommands = distutils.command.build.build.get_sub_commands(self)\n        if self.distribution.executables:\n            subCommands.append(""build_exe"")\n        return subCommands\n\n    def initialize_options(self):\n        distutils.command.build.build.initialize_options(self)\n        self.build_exe = None\n\n    def finalize_options(self):\n        distutils.command.build.build.finalize_options(self)\n        if self.build_exe is None:\n            dirName = ""exe.%s-%s"" % \\\n                    (distutils.util.get_platform(), sys.version[0:3])\n            self.build_exe = os.path.join(self.build_base, dirName)\n\n\nclass build_exe(distutils.core.Command):\n    description = ""build executables from Python scripts""\n    user_options = [\n        (\'build-exe=\', \'b\',\n         \'directory for built executables\'),\n        (\'optimize=\', \'O\',\n         \'optimization level: -O1 for ""python -O"", \'\n         \'-O2 for ""python -OO"" and -O0 to disable [default: -O0]\'),\n        (\'excludes=\', \'e\',\n         \'comma-separated list of modules to exclude\'),\n        (\'includes=\', \'i\',\n         \'comma-separated list of modules to include\'),\n        (\'packages=\', \'p\',\n         \'comma-separated list of packages to include\'),\n        (\'namespace-packages=\', None,\n         \'comma-separated list of namespace packages to include\'),\n        (\'replace-paths=\', None,\n         \'comma-separated list of paths to replace in included modules\'),\n        (\'path=\', None,\n         \'comma-separated list of paths to search\'),\n        (\'no-compress\', None,\n         \'create a zipfile with no compression\'),\n        (\'constants=\', None,\n         \'comma-separated list of constants to include\'),\n        (\'include-files=\', \'f\',\n         \'list of tuples of additional files to include in distribution\'),\n        (\'include-msvcr=\', None,\n         \'include the Microsoft Visual C runtime files\'),\n        (\'zip-includes=\', None,\n         \'list of tuples of additional files to include in zip file\'),\n        (\'bin-includes\', None,\n         \'list of names of files to include when determining dependencies\'),\n        (\'bin-excludes\', None,\n         \'list of names of files to exclude when determining dependencies\'),\n        (\'bin-path-includes\', None,\n         \'list of paths from which to include files when determining \'\n         \'dependencies\'),\n        (\'bin-path-excludes\', None,\n         \'list of paths from which to exclude files when determining \'\n         \'dependencies\'),\n        (\'zip-include-packages=\', None,\n         \'comma-separated list of packages to include in the zip file \' \\\n                \'(or * for all) [default: none]\'),\n        (\'zip-exclude-packages=\', None,\n         \'comma-separated list of packages to exclude from the zip file \' \\\n                \'and place in the file system instead (or * for all) \' \\\n                \'[default: *]\'),\n        (\'silent\', \'s\',\n         \'suppress all output except warnings\')\n    ]\n    boolean_options = [""no-compress"", ""include_msvcr"", ""silent""]\n\n    def add_to_path(self, name):\n        sourceDir = getattr(self, name.lower())\n        if sourceDir is not None:\n            sys.path.insert(0, sourceDir)\n\n    def build_extension(self, name, moduleName = None):\n        if moduleName is None:\n            moduleName = name\n        sourceDir = getattr(self, name.lower())\n        if sourceDir is None:\n            return\n        origDir = os.getcwd()\n        scriptArgs = [""build""]\n        command = self.distribution.get_command_obj(""build"")\n        if command.compiler is not None:\n            scriptArgs.append(""--compiler=%s"" % command.compiler)\n        os.chdir(sourceDir)\n        distutils.log.info(""building \'%s\' extension in \'%s\'"", name, sourceDir)\n        distribution = distutils.core.run_setup(""setup.py"", scriptArgs)\n        modules = [m for m in distribution.ext_modules if m.name == moduleName]\n        if not modules:\n            messageFormat = ""no module named \'%s\' in \'%s\'""\n            raise distutils.errors.DistutilsSetupError(messageFormat %\n                    (moduleName, sourceDir))\n        command = distribution.get_command_obj(""build_ext"")\n        command.ensure_finalized()\n        if command.compiler is None:\n            command.run()\n        else:\n            command.build_extensions()\n        dirName = os.path.join(sourceDir, command.build_lib)\n        os.chdir(origDir)\n        if dirName not in sys.path:\n            sys.path.insert(0, dirName)\n        return os.path.join(sourceDir, command.build_lib,\n                command.get_ext_filename(moduleName))\n\n    def initialize_options(self):\n        self.list_options = [\n            \'excludes\',\n            \'includes\',\n            \'packages\',\n            \'namespace_packages\',\n            \'replace_paths\',\n            \'constants\',\n            \'include_files\',\n            \'zip_includes\',\n            \'bin_excludes\',\n            \'bin_includes\',\n            \'bin_path_includes\',\n            \'bin_path_excludes\',\n            \'zip_include_packages\',\n            \'zip_exclude_packages\',\n        ]\n\n        for option in self.list_options:\n            setattr(self, option, [])\n\n        self.zip_exclude_packages = ""*""\n        self.optimize = 0\n        self.build_exe = None\n        self.no_compress = False\n        self.path = None\n        self.include_msvcr = None\n        self.silent = None\n\n    def finalize_options(self):\n        self.set_undefined_options(\'build\', (\'build_exe\', \'build_exe\'))\n        self.optimize = int(self.optimize)\n\n        if self.silent is None:\n            self.silent = False\n\n        # Make sure all options of multiple values are lists\n        for option in self.list_options:\n            setattr(self, option, normalize_to_list(getattr(self, option)))\n\n    def run(self):\n        metadata = self.distribution.metadata\n        constantsModule = cx_Freeze.ConstantsModule(metadata.version)\n        for constant in self.constants:\n            parts = constant.split(""="")\n            if len(parts) == 1:\n                name = constant\n                value = None\n            else:\n                name, stringValue = parts\n                value = eval(stringValue)\n            constantsModule.values[name] = value\n        freezer = cx_Freeze.Freezer(self.distribution.executables,\n                [constantsModule], self.includes, self.excludes, self.packages,\n                self.replace_paths, (not self.no_compress), self.optimize,\n                self.path, self.build_exe,\n                includeMSVCR = self.include_msvcr,\n                includeFiles = self.include_files,\n                binIncludes = self.bin_includes,\n                binExcludes = self.bin_excludes,\n                zipIncludes = self.zip_includes,\n                silent = self.silent,\n                namespacePackages = self.namespace_packages,\n                binPathIncludes = self.bin_path_includes,\n                binPathExcludes = self.bin_path_excludes,\n                metadata = metadata,\n                zipIncludePackages = self.zip_include_packages,\n                zipExcludePackages = self.zip_exclude_packages)\n        freezer.Freeze()\n\n    def set_source_location(self, name, *pathParts):\n        envName = ""%s_BASE"" % name.upper()\n        attrName = name.lower()\n        sourceDir = getattr(self, attrName)\n        if sourceDir is None:\n            baseDir = os.environ.get(envName)\n            if baseDir is None:\n                return\n            sourceDir = os.path.join(baseDir, *pathParts)\n            if os.path.isdir(sourceDir):\n                setattr(self, attrName, sourceDir)\n\n\nclass install(distutils.command.install.install):\n    user_options = distutils.command.install.install.user_options + [\n            (\'install-exe=\', None,\n             \'installation directory for executables\')\n    ]\n\n    def expand_dirs(self):\n        distutils.command.install.install.expand_dirs(self)\n        self._expand_attrs([\'install_exe\'])\n\n    def get_sub_commands(self):\n        subCommands = distutils.command.install.install.get_sub_commands(self)\n        if self.distribution.executables:\n            subCommands.append(""install_exe"")\n        return [s for s in subCommands if s != ""install_egg_info""]\n\n    def initialize_options(self):\n        distutils.command.install.install.initialize_options(self)\n        self.install_exe = None\n\n    def finalize_options(self):\n        if self.prefix is None and sys.platform == ""win32"":\n            try:\n                import winreg\n            except:\n                import _winreg as winreg\n            key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE,\n                    r""Software\\Microsoft\\Windows\\CurrentVersion"")\n            prefix = str(winreg.QueryValueEx(key, ""ProgramFilesDir"")[0])\n            metadata = self.distribution.metadata\n            self.prefix = ""%s/%s"" % (prefix, metadata.name)\n        distutils.command.install.install.finalize_options(self)\n        self.convert_paths(\'exe\')\n        if self.root is not None:\n            self.change_roots(\'exe\')\n\n    def select_scheme(self, name):\n        distutils.command.install.install.select_scheme(self, name)\n        if self.install_exe is None:\n            if sys.platform == ""win32"":\n                self.install_exe = \'$base\'\n            else:\n                metadata = self.distribution.metadata\n                dirName = ""%s-%s"" % (metadata.name, metadata.version)\n                self.install_exe = \'$base/lib/%s\' % dirName\n\n\nclass install_exe(distutils.core.Command):\n    description = ""install executables built from Python scripts""\n    user_options = [\n        (\'install-dir=\', \'d\', \'directory to install executables to\'),\n        (\'build-dir=\', \'b\', \'build directory (where to install from)\'),\n        (\'force\', \'f\', \'force installation (overwrite existing files)\'),\n        (\'skip-build\', None, \'skip the build steps\')\n    ]\n\n    def initialize_options(self):\n        self.install_dir = None\n        self.force = 0\n        self.build_dir = None\n        self.skip_build = None\n\n    def finalize_options(self):\n        self.set_undefined_options(\'build\', (\'build_exe\', \'build_dir\'))\n        self.set_undefined_options(\'install\',\n                (\'install_exe\', \'install_dir\'),\n                (\'force\', \'force\'),\n                (\'skip_build\', \'skip_build\'))\n\n    def run(self):\n        if not self.skip_build:\n            self.run_command(\'build_exe\')\n        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)\n        if sys.platform != ""win32"":\n            baseDir = os.path.dirname(os.path.dirname(self.install_dir))\n            binDir = os.path.join(baseDir, ""bin"")\n            if not os.path.exists(binDir):\n                os.makedirs(binDir)\n            sourceDir = os.path.join("".."", self.install_dir[len(baseDir) + 1:])\n            for executable in self.distribution.executables:\n                name = os.path.basename(executable.targetName)\n                source = os.path.join(sourceDir, name)\n                target = os.path.join(binDir, name)\n                if os.path.exists(target):\n                    os.unlink(target)\n                os.symlink(source, target)\n                self.outfiles.append(target)\n\n    def get_inputs(self):\n        return self.distribution.executables or []\n\n    def get_outputs(self):\n        return self.outfiles or []\n\n\ndef _AddCommandClass(commandClasses, name, cls):\n    if name not in commandClasses:\n        commandClasses[name] = cls\n\n\ndef setup(**attrs):\n    attrs.setdefault(""distclass"", Distribution)\n    commandClasses = attrs.setdefault(""cmdclass"", {})\n    if sys.platform == ""win32"":\n        if sys.version_info[:2] >= (2, 5):\n            _AddCommandClass(commandClasses, ""bdist_msi"", cx_Freeze.bdist_msi)\n    elif sys.platform == ""darwin"":\n        _AddCommandClass(commandClasses, ""bdist_dmg"", cx_Freeze.bdist_dmg)\n        _AddCommandClass(commandClasses, ""bdist_mac"", cx_Freeze.bdist_mac)\n    else:\n        _AddCommandClass(commandClasses, ""bdist_rpm"", cx_Freeze.bdist_rpm)\n    _AddCommandClass(commandClasses, ""build"", build)\n    _AddCommandClass(commandClasses, ""build_exe"", build_exe)\n    _AddCommandClass(commandClasses, ""install"", install)\n    _AddCommandClass(commandClasses, ""install_exe"", install_exe)\n    distutils.core.setup(**attrs)\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/finder.py,0,"b'""""""\nBase class for finding modules.\n""""""\n\nimport dis\nimport imp\nimport logging\nimport marshal\nimport opcode\nimport os\nimport pkgutil\nimport re\nimport sys\nimport types\nimport zipfile\n\nimport cx_Freeze.hooks\n\nBUILD_LIST = opcode.opmap[""BUILD_LIST""]\nINPLACE_ADD = opcode.opmap[""INPLACE_ADD""]\nLOAD_CONST = opcode.opmap[""LOAD_CONST""]\nIMPORT_NAME = opcode.opmap[""IMPORT_NAME""]\nIMPORT_FROM = opcode.opmap[""IMPORT_FROM""]\nIMPORT_STAR = opcode.opmap[""IMPORT_STAR""]\nSTORE_FAST = opcode.opmap[""STORE_FAST""]\nSTORE_NAME = opcode.opmap[""STORE_NAME""]\nSTORE_GLOBAL = opcode.opmap[""STORE_GLOBAL""]\nSTORE_OPS = (STORE_NAME, STORE_GLOBAL)\n\n__all__ = [ ""Module"", ""ModuleFinder"" ]\n\ntry:\n    bytes   # Python >= 2.6\nexcept NameError:\n    bytes = str\n\ntry:\n    source_from_cache = imp.source_from_cache  # Python 3.2 and above\nexcept AttributeError:\n    def source_from_cache(path):  # Pre PEP 3147 - cache is just .pyc/.pyo\n        assert path.endswith((\'.pyc\', \'.pyo\'))\n        return path[:-1]\n\nclass ZipModulesCache(object):\n    """"""A cache of module and package locations within zip files.""""""\n    def __init__(self):\n        # filename -> None (used like a set)\n        self.files_seen = {}\n        # (path, modulename) -> module_details\n        self.loadable_modules = {}\n    \n    def find(self, path, modulename):\n        """"""Find a module in the given path.\n        \n        path should be a string referring to a zipfile or a directory in a\n        zip file. If it is outside a zip file, it will be ignored.\n        \n        modulename should be a string, with only the last part of the module\n        name, i.e. not containing any dots.\n        \n        If the module is found, this returns information in the same format\n        as :func:`imp.find_module`. Otherwise, it returns None.\n        """"""\n        try:\n            return self.retrieve_loadable_module(path, modulename)\n        except KeyError:\n            pass\n\n        if path in self.files_seen:\n            return None\n\n        # This is a marker that we\'ve seen it, whether or not it\'s a zip file.\n        self.files_seen[path] = None\n\n        if os.path.isfile(path) and zipfile.is_zipfile(path):\n            self.cache_zip_file(path)\n            try:\n                return self.retrieve_loadable_module(path, modulename)\n            except KeyError:\n                return None\n    \n    def retrieve_loadable_module(self, directory, modulename):\n        """"""Retrieve a module from the cache and translate its info into the\n        format returned by :func:`imp.find_module`.\n        \n        Raises KeyError if the module is not present.\n        """"""\n        zip, ideal_path, actual_path, ispkg = self.loadable_modules[directory, modulename]\n        # zip: zipfile.ZipFile object\n        # ideal_path: the path to the pkg directory or module .py file\n        # actual path: path to the .pyc file (None for pkg directories)\n        # ispkg: bool, True if this entry refers to a package.\n        full_path = os.path.join(zip.filename, ideal_path)\n        if ispkg:\n            return None, full_path, (\'\', \'\', imp.PKG_DIRECTORY)\n        else:                \n            fp = zip.read(actual_path)\n            info = ("".pyc"", ""rb"", imp.PY_COMPILED)\n            return fp, full_path, info\n    \n    def cache_zip_file(self, zip_path):\n        """"""Read a zip file and cache the modules and packages found inside it.\n        """"""\n        zip = zipfile.ZipFile(zip_path)\n        for archiveName in zip.namelist():\n            baseName, ext = os.path.splitext(archiveName)\n            if ext not in (\'.pyc\', \'.pyo\'):\n                continue\n            if \'__pycache__\' in baseName:\n                if sys.version_info[:2] < (3, 2) \\\n                        or not baseName.endswith(imp.get_tag()):\n                    continue\n                baseName = os.path.splitext(source_from_cache(archiveName))[0]\n            nameparts = baseName.split(""/"")\n            \n            if len(nameparts) > 1 and nameparts[-1] == \'__init__\':\n                # dir/__init__.pyc  -> dir is a package\n                self.record_loadable_module(nameparts[:-1], None, zip, True)\n\n            self.record_loadable_module(nameparts, archiveName, zip, False)\n\n    def record_loadable_module(self, nameparts, actual_path, zip, ispkg=False):\n        """"""Cache one module found in the zip file.""""""\n        parent_dir = os.path.normpath(os.path.join(zip.filename, ""/"".join(nameparts[:-1])))\n        modulename = nameparts[-1]\n        ideal_path = ""/"".join(nameparts) + ("""" if ispkg else "".py"")\n        if (parent_dir, modulename) not in self.loadable_modules:\n            self.loadable_modules[parent_dir, modulename] = (zip, ideal_path, actual_path, ispkg)\n\nclass ModuleFinder(object):\n\n    def __init__(self, includeFiles = None, excludes = [], path = None,\n            replacePaths = None):\n        self.includeFiles = includeFiles\n        if includeFiles is None:\n            self.includeFiles = []\n        self.excludes = dict.fromkeys(excludes)\n        self.replacePaths = replacePaths\n        if replacePaths is None:\n            self.replacePaths = []\n        self.path = path or sys.path\n        self.modules = []\n        self.aliases = {}\n        self._modules = dict.fromkeys(excludes)\n        self._builtinModules = dict.fromkeys(sys.builtin_module_names)\n        self._badModules = {}\n        self._zip_modules_cache = ZipModulesCache()\n        cx_Freeze.hooks.initialize(self)\n        initialExcludedModules = self.excludes.copy()\n        self._AddBaseModules()\n\n    def _AddBaseModules(self):\n        """"""Add the base modules to the finder. These are the modules that\n           Python imports itself during initialization and, if not found,\n           can result in behavior that differs from running from source;\n           also include modules used within the bootstrap code.\n\n           When cx_Freeze is built, these modules (and modules they load) are\n           included in the startup zip file.\n           """"""\n        self.IncludeModule(""traceback"")\n        self.IncludeModule(""warnings"")\n        self.IncludePackage(""encodings"")\n        if sys.version_info[0] >= 3:\n            self.IncludeModule(""io"")\n        self.IncludeModule(""os"")\n        self.IncludeModule(""sys"")\n        self.IncludeModule(""zlib"")\n        if sys.version_info[:2] >= (3, 4):\n            # We need this, because collections gets loaded (via traceback),\n            # and a partially frozen package causes problems.\n            self.IncludeModule(""collections.abc"")\n        if sys.version_info[:2] >= (3,5):\n            self.IncludeModule(""importlib.abc"")\n\n    def _AddModule(self, name):\n        """"""Add a module to the list of modules but if one is already found,\n           then return it instead; this is done so that packages can be\n           handled properly.""""""\n        module = self._modules.get(name)\n        if module is None:\n            module = self._modules[name] = Module(name)\n            self.modules.append(module)\n            if name in self._badModules:\n                logging.debug(""Removing module [%s] from list of bad modules"",\n                        name)\n                del self._badModules[name]\n        return module\n\n    def _DetermineParent(self, caller):\n        """"""Determine the parent to use when searching packages.""""""\n        if caller is not None:\n            if caller.path is not None:\n                return caller\n            return self._GetParentByName(caller.name)\n\n    def _EnsureFromList(self, caller, packageModule, fromList,\n            deferredImports):\n        """"""Ensure that the from list is satisfied. This is only necessary for\n           package modules. If the package module has not been completely\n           imported yet, defer the import until it has been completely imported\n           in order to avoid spurious errors about missing modules.""""""\n        if packageModule.inImport and caller is not packageModule:\n            deferredImports.append((caller, packageModule, fromList))\n        else:\n            for name in fromList:\n                if name in packageModule.globalNames:\n                    continue\n                subModuleName = ""%s.%s"" % (packageModule.name, name)\n                self._ImportModule(subModuleName, deferredImports, caller)\n\n    def _FindModule(self, name, path, namespace):\n        try:\n            # imp loads normal modules from the filesystem\n            return imp.find_module(name, path)\n        except ImportError:\n            if namespace and name in sys.modules:\n                # Namespace package (?)\n                module = sys.modules[name]\n                info = ("""", """", imp.PKG_DIRECTORY)\n                return None, module.__path__[0], info\n\n            # Check for modules in zip files.\n            # If a path is a subdirectory within a zip file, we must have\n            # already cached the contents of the zip file to find modules in it.\n            if path is None:\n                path = []\n            for location in path:\n                res = self._zip_modules_cache.find(location, name)\n                if res is not None:\n                    return res\n            raise\n\n    def _GetParentByName(self, name):\n        """"""Return the parent module given the name of a module.""""""\n        pos = name.rfind(""."")\n        if pos > 0:\n            parentName = name[:pos]\n            return self._modules[parentName]\n\n    def _ImportAllSubModules(self, module, deferredImports, recursive = True):\n        """"""Import all sub modules to the given package.""""""\n        suffixes = [s[0] for s in imp.get_suffixes()]\n\n        for path in module.path:\n            try:\n                fileNames = os.listdir(path)\n            except os.error:\n                continue\n\n            for fileName in fileNames:\n                fullName = os.path.join(path, fileName)\n                if os.path.isdir(fullName):\n                    initFile = os.path.join(fullName, ""__init__.py"")\n                    if not os.path.exists(initFile):\n                        continue\n                    name = fileName\n                else:\n                    # We need to run through these in order to correctly pick\n                    # up PEP 3149 library names (e.g. .cpython-32mu.so).\n                    for suffix in suffixes:\n                        if fileName.endswith(suffix):\n                            name = fileName[:-len(suffix)]\n\n                            # Skip modules whose names appear to contain \'.\',\n                            # as we may be using the wrong suffix, and even if\n                            # we\'re not, such module names will break the import\n                            # code.\n                            if ""."" not in name:\n                                break\n\n                    else:\n                        continue\n                    if name == ""__init__"":\n                        continue\n\n                subModuleName = ""%s.%s"" % (module.name, name)\n                subModule = self._InternalImportModule(subModuleName,\n                                deferredImports)\n                if subModule is None:\n                    if subModuleName not in self.excludes:\n                        raise ImportError(""No module named %r"" % subModuleName)\n                else:\n                    module.globalNames[name] = None\n                    if subModule.path and recursive:\n                        self._ImportAllSubModules(subModule, deferredImports,\n                                recursive)\n\n    def _ImportDeferredImports(self, deferredImports, skipInImport = False):\n        """"""Import any sub modules that were deferred, if applicable.""""""\n        while deferredImports:\n            newDeferredImports = []\n            for caller, packageModule, subModuleNames in deferredImports:\n                if packageModule.inImport and skipInImport:\n                    continue\n                self._EnsureFromList(caller, packageModule, subModuleNames,\n                        newDeferredImports)\n            deferredImports = newDeferredImports\n            skipInImport = True\n\n    def _ImportModule(self, name, deferredImports, caller = None,\n            relativeImportIndex = 0, namespace = False):\n        """"""Attempt to find the named module and return it or None if no module\n           by that name could be found.""""""\n\n        # absolute import (available in Python 2.5 and up)\n        # the name given is the only name that will be searched\n        if relativeImportIndex == 0:\n            module = self._InternalImportModule(name,\n                    deferredImports, namespace = namespace)\n\n        # old style relative import (regular \'import foo\' in Python 2)\n        # the name given is tried in the current package, and if\n        # no match is found, sys.path is searched for a top-level module/pockage\n        elif relativeImportIndex < 0:\n            parent = self._DetermineParent(caller)\n            if parent is not None:\n                fullName = ""%s.%s"" % (parent.name, name)\n                module = self._InternalImportModule(fullName,\n                        deferredImports, namespace = namespace)\n                if module is not None:\n                    parent.globalNames[name] = None\n                    return module\n\n            module = self._InternalImportModule(name,\n                    deferredImports, namespace = namespace)\n\n        # new style relative import (available in Python 2.5 and up)\n        # the index indicates how many levels to traverse and only that level\n        # is searched for the named module\n        elif relativeImportIndex > 0:\n            parent = caller\n            if parent.path is not None:\n                relativeImportIndex -= 1\n            while parent is not None and relativeImportIndex > 0:\n                parent = self._GetParentByName(parent.name)\n                relativeImportIndex -= 1\n            if parent is None:\n                module = None\n            elif not name:\n                module = parent\n            else:\n                name = ""%s.%s"" % (parent.name, name)\n                module = self._InternalImportModule(name,\n                        deferredImports, namespace = namespace)\n\n        # if module not found, track that fact\n        if module is None:\n            if caller is None:\n                raise ImportError(""No module named %r"" % name)\n            self._RunHook(""missing"", name, caller)\n            if name not in caller.ignoreNames:\n                callers = self._badModules.setdefault(name, {})\n                callers[caller.name] = None\n\n        return module\n\n    def _InternalImportModule(self, name, deferredImports, namespace = False):\n        """"""Internal method used for importing a module which assumes that the\n           name given is an absolute name. None is returned if the module\n           cannot be found.""""""\n        try:\n            # Check in module cache before trying to import it again.\n            return self._modules[name]\n        except KeyError:\n            pass\n\n        if name in self._builtinModules:\n            module = self._AddModule(name)\n            logging.debug(""Adding module [%s] [C_BUILTIN]"", name)\n            self._RunHook(""load"", module.name, module)\n            module.inImport = False\n            return module\n\n        pos = name.rfind(""."")\n        if pos < 0:  # Top-level module\n            path = self.path\n            searchName = name\n            parentModule = None\n        else:        # Dotted module name - look up the parent module\n            parentName = name[:pos]\n            parentModule = \\\n                    self._InternalImportModule(parentName, deferredImports,\n                            namespace = namespace)\n            if parentModule is None:\n                return None\n            if namespace:\n                parentModule.ExtendPath()\n            path = parentModule.path\n            searchName = name[pos + 1:]\n\n        if name in self.aliases:\n            actualName = self.aliases[name]\n            module = self._InternalImportModule(actualName, deferredImports)\n            self._modules[name] = module\n            return module\n\n        try:\n            fp, path, info = self._FindModule(searchName, path, namespace)\n            if info[-1] == imp.C_BUILTIN and parentModule is not None:\n                return None\n            module = self._LoadModule(name, fp, path, info, deferredImports,\n                    parentModule, namespace)\n        except ImportError:\n            logging.debug(""Module [%s] cannot be imported"", name)\n            self._modules[name] = None\n            return None\n        return module\n\n    def _LoadModule(self, name, fp, path, info, deferredImports,\n            parent = None, namespace = False):\n        """"""Load the module, given the information acquired by the finder.""""""\n        suffix, mode, type = info\n        if type == imp.PKG_DIRECTORY:\n            return self._LoadPackage(name, path, parent, deferredImports,\n                    namespace)\n        module = self._AddModule(name)\n        module.file = path\n        module.parent = parent\n\n        if type == imp.PY_SOURCE:\n            logging.debug(""Adding module [%s] [PY_SOURCE]"", name)\n            # Load & compile Python source code\n            if sys.version_info[0] >= 3:\n                # For Python 3, read the file with the correct encoding\n                import tokenize\n                fp = open(path, ""rb"")\n                encoding, lines = tokenize.detect_encoding(fp.readline)\n                fp = open(path, ""U"", encoding = encoding)\n            codeString = fp.read()\n            if codeString and codeString[-1] != ""\\n"":\n                codeString = codeString + ""\\n""\n            try:\n                module.code = compile(codeString, path, ""exec"")\n            except SyntaxError:\n                raise ImportError(""Invalid syntax in %s"" % path)\n        \n        elif type == imp.PY_COMPILED:\n            logging.debug(""Adding module [%s] [PY_COMPILED]"", name)\n            # Load Python bytecode\n            if isinstance(fp, bytes):\n                magic = fp[:4]\n            else:\n                magic = fp.read(4)\n            if magic != imp.get_magic():\n                raise ImportError(""Bad magic number in %s"" % path)\n            skip_bytes = 8 if (sys.version_info[:2] >= (3,3)) else 4\n            if isinstance(fp, bytes):\n                module.code = marshal.loads(fp[skip_bytes+4:])\n                module.inZipFile = True\n            else:\n                fp.read(skip_bytes)\n                module.code = marshal.load(fp)\n        \n        elif type == imp.C_EXTENSION:\n            logging.debug(""Adding module [%s] [C_EXTENSION]"", name)\n            if parent is None:\n                # Our extension loader (see the freezer module) uses imp to\n                # load compiled extensions.\n                self.IncludeModule(""imp"")\n\n        # If there\'s a custom hook for this module, run it.\n        self._RunHook(""load"", module.name, module)\n        \n        if module.code is not None:\n            if self.replacePaths:\n                topLevelModule = module\n                while topLevelModule.parent is not None:\n                    topLevelModule = topLevelModule.parent\n                module.code = self._ReplacePathsInCode(topLevelModule,\n                        module.code)\n            \n            # Scan the module code for import statements\n            self._ScanCode(module.code, module, deferredImports)\n        \n        module.inImport = False\n        return module\n\n    def _LoadPackage(self, name, path, parent, deferredImports, namespace):\n        """"""Load the package, given its name and path.""""""\n        module = self._AddModule(name)\n        module.path = [path]\n        try:\n            fp, path, info = self._FindModule(""__init__"", module.path, False)\n            self._LoadModule(name, fp, path, info, deferredImports, parent)\n            logging.debug(""Adding module [%s] [PKG_DIRECTORY]"", name)\n        except ImportError:\n            if not namespace:\n                raise\n            fileName = os.path.join(path, ""__init__.py"")\n            module.code = compile("""", fileName, ""exec"")\n            logging.debug(""Adding module [%s] [PKG_NAMESPACE_DIRECTORY]"", name)\n        return module\n\n    def _ReplacePathsInCode(self, topLevelModule, co):\n        """"""Replace paths in the code as directed, returning a new code object\n           with the modified paths in place.""""""\n        # Prepare the new filename.\n        origFileName = newFileName = os.path.normpath(co.co_filename)\n        for searchValue, replaceValue in self.replacePaths:\n            if searchValue == ""*"":\n                searchValue = os.path.dirname(topLevelModule.file)\n                if topLevelModule.path:\n                    searchValue = os.path.dirname(searchValue)\n                if searchValue:\n                    searchValue = searchValue + os.path.sep\n            if not origFileName.startswith(searchValue):\n                continue\n            newFileName = replaceValue + origFileName[len(searchValue):]\n            break\n        \n        # Run on subordinate code objects from function & class definitions.\n        constants = list(co.co_consts)\n        for i, value in enumerate(constants):\n            if isinstance(value, type(co)):\n                constants[i] = self._ReplacePathsInCode(topLevelModule, value)\n        \n        # Build the new code object.\n        if sys.version_info[0] < 3:\n            return types.CodeType(co.co_argcount, co.co_nlocals,\n                    co.co_stacksize, co.co_flags, co.co_code, tuple(constants),\n                    co.co_names, co.co_varnames, newFileName, co.co_name,\n                    co.co_firstlineno, co.co_lnotab, co.co_freevars,\n                    co.co_cellvars)\n        return types.CodeType(co.co_argcount, co.co_kwonlyargcount,\n                co.co_nlocals, co.co_stacksize, co.co_flags, co.co_code,\n                tuple(constants), co.co_names, co.co_varnames, newFileName,\n                co.co_name, co.co_firstlineno, co.co_lnotab, co.co_freevars,\n                co.co_cellvars)\n\n    def _RunHook(self, hookName, moduleName, *args):\n        """"""Run hook for the given module if one is present.""""""\n        name = ""%s_%s"" % (hookName, moduleName.replace(""."", ""_""))\n        method = getattr(cx_Freeze.hooks, name, None)\n        if method is not None:\n            method(self, *args)\n\n    def _ScanCode(self, co, module, deferredImports, topLevel = True):\n        """"""Scan code, looking for imported modules and keeping track of the\n           constants that have been created in order to better tell which\n           modules are truly missing.""""""\n        arguments = []\n        importedModule = None\n        method = dis._unpack_opargs if sys.version_info[:2] >= (3, 5) \\\n                else self._UnpackOpArgs\n        for opIndex, op, opArg in method(co.co_code):\n\n            # keep track of constants (these are used for importing)\n            # immediately restart loop so arguments are retained\n            if op == LOAD_CONST:\n                arguments.append(co.co_consts[opArg])\n                continue\n\n            # import statement: attempt to import module\n            elif op == IMPORT_NAME:\n                name = co.co_names[opArg]\n                if len(arguments) >= 2:\n                    relativeImportIndex, fromList = arguments[-2:]\n                else:\n                    relativeImportIndex = -1\n                    fromList, = arguments\n                if name not in module.excludeNames:\n                    importedModule = self._ImportModule(name, deferredImports,\n                            module, relativeImportIndex)\n                    if importedModule is not None:\n                        if fromList and fromList != (""*"",) \\\n                                and importedModule.path is not None:\n                            self._EnsureFromList(module, importedModule,\n                                    fromList, deferredImports)\n\n            # import * statement: copy all global names\n            elif op == IMPORT_STAR and topLevel and importedModule is not None:\n                module.globalNames.update(importedModule.globalNames)\n\n            # store operation: track only top level\n            elif topLevel and op in STORE_OPS:\n                name = co.co_names[opArg]\n                module.globalNames[name] = None\n\n            # reset arguments; these are only needed for import statements so\n            # ignore them in all other cases!\n            arguments = []\n\n        # Scan the code objects from function & class definitions\n        for constant in co.co_consts:\n            if isinstance(constant, type(co)):\n                self._ScanCode(constant, module, deferredImports,\n                        topLevel = False)\n\n    def _UnpackOpArgs(self, code):\n        """"""Unpack the operations and arguments from the byte code. From Python\n           3.5 onwards this is found in the private method _unpack_opargs\n           but for earlier releases this wasn\'t available as a separate\n           method.""""""\n        opIndex = 0\n        numOps = len(code)\n        is3 = sys.version_info[0] >= 3\n        while opIndex < numOps:\n            offset = opIndex\n            if is3:\n                op = code[opIndex]\n            else:\n                op = ord(code[opIndex])\n            opIndex += 1\n            arg = None\n            if op >= dis.HAVE_ARGUMENT:\n                if is3:\n                    arg = code[opIndex] + code[opIndex + 1] * 256\n                else:\n                    arg = ord(code[opIndex]) + ord(code[opIndex + 1]) * 256\n                opIndex += 2\n            yield (offset, op, arg)\n\n    def AddAlias(self, name, aliasFor):\n        """"""Add an alias for a particular module; when an attempt is made to\n           import a module using the alias name, import the actual name\n           instead.""""""\n        self.aliases[name] = aliasFor\n\n    def ExcludeModule(self, name):\n        """"""Exclude the named module from the resulting frozen executable.""""""\n        self.excludes[name] = None\n        self._modules[name] = None\n\n    def IncludeFile(self, path, moduleName = None):\n        """"""Include the named file as a module in the frozen executable.""""""\n        name, ext = os.path.splitext(os.path.basename(path))\n        if moduleName is None:\n            moduleName = name\n        info = (ext, ""r"", imp.PY_SOURCE)\n        deferredImports = []\n        module = self._LoadModule(moduleName, open(path, ""U""), path, info,\n                deferredImports)\n        self._ImportDeferredImports(deferredImports)\n        return module\n\n    def IncludeFiles(self, sourcePath, targetPath):\n        """"""Include the files in the given directory in the target build.""""""\n        self.includeFiles.append((sourcePath, targetPath))\n\n    def IncludeModule(self, name, namespace = False):\n        """"""Include the named module in the frozen executable.""""""\n        deferredImports = []\n        module = self._ImportModule(name, deferredImports,\n                namespace = namespace)\n        self._ImportDeferredImports(deferredImports, skipInImport = True)\n        return module\n\n    def IncludePackage(self, name):\n        """"""Include the named package and any submodules in the frozen\n           executable.""""""\n        deferredImports = []\n        module = self._ImportModule(name, deferredImports)\n        if module.path:\n            self._ImportAllSubModules(module, deferredImports)\n        self._ImportDeferredImports(deferredImports, skipInImport = True)\n        return module\n\n    def ReportMissingModules(self):\n        """"""Display a list of modules that weren\'t found.""""""\n        if self._badModules:\n            sys.stdout.write(""Missing modules:\\n"")\n            names = list(self._badModules.keys())\n            names.sort()\n            for name in names:\n                callers = list(self._badModules[name].keys())\n                callers.sort()\n                sys.stdout.write(""? %s imported from %s\\n"" % \\\n                        (name, "", "".join(callers)))\n            sys.stdout.write(""This is not necessarily a problem - the modules ""\n                             ""may not be needed on this platform.\\n"")\n            sys.stdout.write(""\\n"")\n\n\nclass Module(object):\n\n    def __init__(self, name):\n        self.name = name\n        self.file = None\n        self.path = None\n        self.code = None\n        self.parent = None\n        self.globalNames = {}\n        self.excludeNames = {}\n        self.ignoreNames = {}\n        self.inZipFile = False\n        self.inImport = True\n\n    def __repr__(self):\n        parts = [""name=%s"" % repr(self.name)]\n        if self.file is not None:\n            parts.append(""file=%s"" % repr(self.file))\n        if self.path is not None:\n            parts.append(""path=%s"" % repr(self.path))\n        return ""<Module %s>"" % "", "".join(parts)\n\n    def AddGlobalName(self, name):\n        self.globalNames[name] = None\n\n    def ExcludeName(self, name):\n        self.excludeNames[name] = None\n\n    def ExtendPath(self):\n        self.path = pkgutil.extend_path(self.path, self.name)\n        if self.parent is not None:\n            self.parent.ExtendPath()\n\n    def IgnoreName(self, name):\n        self.ignoreNames[name] = None\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/freezer.py,0,"b'""""""\nBase class for freezing scripts into executables.\n""""""\n\nfrom __future__ import print_function\n\nimport datetime\nimport distutils.sysconfig\nimport imp\nimport marshal\nimport os\nimport shutil\nimport socket\nimport stat\nimport struct\nimport sys\nimport time\nimport zipfile\n\nimport cx_Freeze\n\n__all__ = [ ""ConfigError"", ""ConstantsModule"", ""Executable"", ""Freezer"" ]\n\nEXTENSION_LOADER_SOURCE = \\\n""""""\ndef __bootstrap__():\n    import imp, os, sys\n    global __bootstrap__, __loader__\n    __loader__ = None; del __bootstrap__, __loader__\n\n    found = False\n    for p in sys.path:\n        if not os.path.isdir(p):\n            continue\n        f = os.path.join(p, ""%s"")\n        if not os.path.exists(f):\n            continue\n        m = imp.load_dynamic(__name__, f)\n        import sys\n        sys.modules[__name__] = m\n        found = True\n        break\n    if not found:\n        del sys.modules[__name__]\n        raise ImportError(""No module named %%s"" %% __name__)\n__bootstrap__()\n""""""\n\n\nMSVCR_MANIFEST_TEMPLATE = """"""\n<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?>\n<assembly xmlns=""urn:schemas-microsoft-com:asm.v1"" manifestVersion=""1.0"">\n<noInheritable/>\n<assemblyIdentity\n    type=""win32""\n    name=""Microsoft.VC90.CRT""\n    version=""9.0.21022.8""\n    processorArchitecture=""{PROC_ARCH}""\n    publicKeyToken=""1fc8b3b9a1e18e3b""/>\n<file name=""MSVCR90.DLL""/>\n<file name=""MSVCM90.DLL""/>\n<file name=""MSVCP90.DLL""/>\n</assembly>\n""""""\n\ndef process_path_specs(specs):\n    """"""Prepare paths specified as config.\n    \n    The input is a list of either strings, or 2-tuples (source, target).\n    Where single strings are supplied, the basenames are used as targets.\n    Where targets are given explicitly, they must not be absolute paths.\n    \n    Returns a list of 2-tuples, or throws ConfigError if something is wrong\n    in the input.\n    """"""\n    processedSpecs = []\n    for spec in specs:\n        if not isinstance(spec, (list, tuple)):\n            source = spec\n            target = None\n        elif len(spec) != 2:\n            raise ConfigError(""path spec must be a list or tuple of ""\n                    ""length two"")\n        else:\n            source, target = spec\n        source = os.path.normpath(source)\n        if not target:\n            target = os.path.basename(source)\n        elif os.path.isabs(target):\n            raise ConfigError(""target path for include file may not be ""\n                    ""an absolute path"")\n        processedSpecs.append((source, target))\n    return processedSpecs\n\ndef get_resource_file_path(dirName, name, ext):\n    """"""Return the path to a resource file shipped with cx_Freeze.\n    \n    This is used to find our base executables and initscripts when they are\n    just specified by name.\n    """"""\n    if os.path.isabs(name):\n        return name\n    name = os.path.normcase(name)\n    fullDir = os.path.join(os.path.dirname(cx_Freeze.__file__), dirName)\n    if os.path.isdir(fullDir):\n        for fileName in os.listdir(fullDir):\n            checkName, checkExt = \\\n                    os.path.splitext(os.path.normcase(fileName))\n            if name == checkName and ext == checkExt:\n                return os.path.join(fullDir, fileName)\n\n\nclass Freezer(object):\n\n    def __init__(self, executables, constantsModules = [], includes = [],\n            excludes = [], packages = [], replacePaths = [], compress = True,\n            optimizeFlag = 0, path = None,\n            targetDir = None, binIncludes = [], binExcludes = [],\n            binPathIncludes = [], binPathExcludes = [],\n            includeFiles = [], zipIncludes = [], silent = False,\n            namespacePackages = [], metadata = None,\n            includeMSVCR = False, zipIncludePackages = [],\n            zipExcludePackages = [""*""]):\n        self.executables = list(executables)\n        self.constantsModules = list(constantsModules)\n        self.includes = list(includes)\n        self.excludes = list(excludes)\n        self.packages = list(packages)\n        self.namespacePackages = list(namespacePackages)\n        self.replacePaths = list(replacePaths)\n        self.compress = compress\n        self.optimizeFlag = optimizeFlag\n        self.path = path\n        self.includeMSVCR = includeMSVCR\n        self.targetDir = targetDir\n        self.binIncludes = [os.path.normcase(n) \\\n                for n in self._GetDefaultBinIncludes() + binIncludes]\n        self.binExcludes = [os.path.normcase(n) \\\n                for n in self._GetDefaultBinExcludes() + binExcludes]\n        self.binPathIncludes = [os.path.normcase(n) for n in binPathIncludes]\n        self.binPathExcludes = [os.path.normcase(n) \\\n                for n in self._GetDefaultBinPathExcludes() + binPathExcludes]\n        self.includeFiles = process_path_specs(includeFiles)\n        self.zipIncludes = process_path_specs(zipIncludes)\n        self.silent = silent\n        self.metadata = metadata\n        self.zipIncludePackages = list(zipIncludePackages)\n        self.zipExcludePackages = list(zipExcludePackages)\n        self._VerifyConfiguration()\n\n    def _AddVersionResource(self, exe):\n        try:\n            from win32verstamp import stamp\n        except:\n            print(""*** WARNING *** unable to create version resource"")\n            print(""install pywin32 extensions first"")\n            return\n        fileName = exe.targetName\n        versionInfo = VersionInfo(self.metadata.version,\n                comments = self.metadata.long_description,\n                description = self.metadata.description,\n                company = self.metadata.author,\n                product = self.metadata.name,\n                copyright = exe.copyright,\n                trademarks = exe.trademarks)\n        stamp(fileName, versionInfo)\n\n    def _CopyFile(self, source, target, copyDependentFiles,\n            includeMode = False):\n        normalizedSource = os.path.normcase(os.path.normpath(source))\n        normalizedTarget = os.path.normcase(os.path.normpath(target))\n        if normalizedTarget in self.filesCopied:\n            return\n        if normalizedSource == normalizedTarget:\n            return\n        self._RemoveFile(target)\n        targetDir = os.path.dirname(target)\n        self._CreateDirectory(targetDir)\n        if not self.silent:\n            sys.stdout.write(""copying %s -> %s\\n"" % (source, target))\n        shutil.copyfile(source, target)\n        shutil.copystat(source, target)\n        if includeMode:\n            shutil.copymode(source, target)\n        self.filesCopied[normalizedTarget] = None\n        if copyDependentFiles:\n            changedDependencies = []\n            for depSource in self._GetDependentFiles(source):\n                depTarget = os.path.join(self.targetDir, ""lib"", os.path.basename(depSource))\n                self._CopyFile(depSource, depTarget, copyDependentFiles)\n                newName = self._UpdateLibraryName(depTarget)\n                if newName:\n                    changedDependencies.append((depSource, newName))\n\n            if changedDependencies:\n                self._UpdateDependendentFiles(target, changedDependencies)\n\n    def _CreateDirectory(self, path):\n        if not os.path.isdir(path):\n            if not self.silent:\n                sys.stdout.write(""creating directory %s\\n"" % path)\n            os.makedirs(path)\n\n    def _FreezeExecutable(self, exe):\n        finder = self.finder\n        finder.IncludeFile(exe.script, exe.moduleName)\n        finder.IncludeFile(exe.initScript, exe.initModuleName)\n        startupModule = get_resource_file_path(""initscripts"", ""__startup__"",\n                "".py"")\n        finder.IncludeFile(startupModule)\n\n        self._CopyFile(exe.base, exe.targetName, copyDependentFiles = True,\n                includeMode = True)\n        if self.includeMSVCR:\n            self._IncludeMSVCR(exe)\n\n        # Copy icon\n        if exe.icon is not None:\n            if sys.platform == ""win32"":\n                import cx_Freeze.util\n                cx_Freeze.util.AddIcon(exe.targetName, exe.icon)\n            else:\n                targetName = os.path.join(os.path.dirname(exe.targetName),\n                        os.path.basename(exe.icon))\n                self._CopyFile(exe.icon, targetName,\n                        copyDependentFiles = False)\n\n        if not os.access(exe.targetName, os.W_OK):\n            mode = os.stat(exe.targetName).st_mode\n            os.chmod(exe.targetName, mode | stat.S_IWUSR)\n        if self.metadata is not None and sys.platform == ""win32"":\n            self._AddVersionResource(exe)\n\n    def _GetDefaultBinExcludes(self):\n        """"""Return the file names of libraries that need not be included because\n           they would normally be expected to be found on the target system or\n           because they are part of a package which requires independent\n           installation anyway.""""""\n        if sys.platform == ""win32"":\n            return [""comctl32.dll"", ""oci.dll"", ""cx_Logging.pyd""]\n        else:\n            return [""libclntsh.so"", ""libwtc9.so""]\n\n    def _GetDefaultBinIncludes(self):\n        """"""Return the file names of libraries which must be included for the\n           frozen executable to work.""""""\n        if sys.platform == ""win32"":\n            pythonDll = ""python%s%s.dll"" % sys.version_info[:2]\n            return [pythonDll, ""gdiplus.dll"", ""mfc71.dll"", ""msvcp71.dll"",\n                    ""msvcr71.dll""]\n        else:\n            soName = distutils.sysconfig.get_config_var(""INSTSONAME"")\n            if soName is None:\n                return []\n            pythonSharedLib = self._RemoveVersionNumbers(soName)\n            return [pythonSharedLib]\n\n    def _GetDefaultBinPathExcludes(self):\n        """"""Return the paths of directories which contain files that should not\n           be included, generally because they contain standard system\n           libraries.""""""\n        if sys.platform == ""win32"":\n            import cx_Freeze.util\n            systemDir = cx_Freeze.util.GetSystemDir()\n            windowsDir = cx_Freeze.util.GetWindowsDir()\n            return [windowsDir, systemDir, os.path.join(windowsDir, ""WinSxS"")]\n        elif sys.platform == ""darwin"":\n            return [""/lib"", ""/usr/lib"", ""/System/Library/Frameworks""]\n        else:\n            return [""/lib"", ""/lib32"", ""/lib64"", ""/usr/lib"", ""/usr/lib32"",\n                    ""/usr/lib64""]\n\n    def _UpdateDependendentFiles(self, path, changedDependencies):\n        if sys.platform == ""darwin"":\n            if not path.endswith("".so"") and not path.endswith("".dylib""):\n                return\n            s = os.stat(path)\n            os.chmod(path, stat.S_IWRITE | s.st_mode)\n            command_fragments = [\'install_name_tool\']\n            for old, new in changedDependencies:\n                command_fragments.extend([""-change"", ""\'%s\'"" % old, ""\'%s\'"" % new])\n            command_fragments.append(""\'%s\'"" % path)\n            command = "" "".join(command_fragments)\n            for line in os.popen(command):\n                continue\n            os.chmod(path, s.st_mode)\n\n    def _UpdateLibraryName(self, path):\n        if sys.platform == ""darwin"":\n            name = ""@rpath/%s"" % os.path.basename(path)\n            if not path.endswith("".so"") and not path.endswith("".dylib""):\n                return\n            s = os.stat(path)\n            os.chmod(path, stat.S_IWRITE | s.st_mode)\n            command = \'install_name_tool -id ""%s"" ""%s""\' % (name, path)\n            for line in os.popen(command):\n                continue\n            os.chmod(path, s.st_mode)\n            return name\n\n    def _GetDependentFiles(self, path):\n        """"""Return the file\'s dependencies using platform-specific tools (the\n           imagehlp library on Windows, otool on Mac OS X and ldd on Linux);\n           limit this list by the exclusion lists as needed""""""\n        dependentFiles = self.dependentFiles.get(path)\n        if dependentFiles is None:\n            if sys.platform == ""win32"":\n                origPath = os.environ[""PATH""]\n                os.environ[""PATH""] = origPath + os.pathsep + \\\n                        os.pathsep.join(sys.path)\n                import cx_Freeze.util\n                try:\n                    dependentFiles = cx_Freeze.util.GetDependentFiles(path)\n                except cx_Freeze.util.BindError:\n                    # Sometimes this gets called when path is not actually a library\n                    # See issue 88\n                    dependentFiles = []\n                os.environ[""PATH""] = origPath\n            else:\n                dependentFiles = []\n                if sys.platform == ""darwin"":\n                    command = \'otool -L ""%s""\' % path\n                    splitString = "" (compatibility""\n                    dependentFileIndex = 0\n                else:\n                    command = \'ldd ""%s""\' % path\n                    splitString = "" => ""\n                    dependentFileIndex = 1\n                for line in os.popen(command):\n                    parts = line.expandtabs().strip().split(splitString)\n                    if len(parts) != 2:\n                        continue\n                    dependentFile = parts[dependentFileIndex].strip()\n                    if dependentFile == os.path.basename(path):\n                        continue\n                    if dependentFile in (""not found"", ""(file not found)""):\n                        fileName = parts[0]\n                        if fileName not in self.linkerWarnings:\n                            self.linkerWarnings[fileName] = None\n                            message = ""WARNING: cannot find %s\\n"" % fileName\n                            sys.stdout.write(message)\n                        continue\n                    if dependentFile.startswith(""(""):\n                        continue\n                    pos = dependentFile.find("" ("")\n                    if pos >= 0:\n                        dependentFile = dependentFile[:pos].strip()\n                    if dependentFile:\n                        dependentFiles.append(dependentFile)\n                if sys.platform == ""darwin"":\n                    # Make library paths absolute. This is needed to use\n                    # cx_Freeze on OSX in e.g. a conda-based distribution.\n                    # Note that with @rpath we just assume Python\'s lib dir,\n                    # which should work in most cases.\n                    dirname = os.path.dirname(path)\n                    dependentFiles = [p.replace(\'@loader_path\', dirname)\n                                      for p in dependentFiles]\n                    dependentFiles = [p.replace(\'@rpath\', sys.prefix + \'/lib\')\n                                      for p in dependentFiles]\n            dependentFiles = self.dependentFiles[path] = \\\n                    [f for f in dependentFiles if self._ShouldCopyFile(f)]\n        return dependentFiles\n\n    def _GetModuleFinder(self, argsSource = None):\n        if argsSource is None:\n            argsSource = self\n        finder = cx_Freeze.ModuleFinder(self.includeFiles, self.excludes,\n                self.path, self.replacePaths)\n        for name in self.namespacePackages:\n            package = finder.IncludeModule(name, namespace = True)\n            package.ExtendPath()\n        for name in self.includes:\n            finder.IncludeModule(name)\n        for name in self.packages:\n            finder.IncludePackage(name)\n        return finder\n\n    def _IncludeMSVCR(self, exe):\n        msvcRuntimeDll = None\n        targetDir = os.path.dirname(exe.targetName)\n        for fullName in self.filesCopied:\n            path, name = os.path.split(os.path.normcase(fullName))\n            if name.startswith(""msvcr"") and name.endswith("".dll""):\n                msvcRuntimeDll = name\n                for otherName in [name.replace(""r"", c) for c in ""mp""]:\n                    sourceName = os.path.join(self.msvcRuntimeDir, otherName)\n                    if not os.path.exists(sourceName):\n                        continue\n                    targetName = os.path.join(targetDir, otherName)\n                    self._CopyFile(sourceName, targetName,\n                            copyDependentFiles = False)\n                break\n\n        if msvcRuntimeDll is not None and msvcRuntimeDll == ""msvcr90.dll"":\n            if struct.calcsize(""P"") == 4:\n                arch = ""x86""\n            else:\n                arch = ""amd64""\n            manifest = MSVCR_MANIFEST_TEMPLATE.strip().replace(""{PROC_ARCH}"",\n                    arch)\n            fileName = os.path.join(targetDir, ""Microsoft.VC90.CRT.manifest"")\n            sys.stdout.write(""creating %s\\n"" % fileName)\n            open(fileName, ""w"").write(manifest)\n\n    def _PrintReport(self, fileName, modules):\n        sys.stdout.write(""writing zip file %s\\n\\n"" % fileName)\n        sys.stdout.write(""  %-25s %s\\n"" % (""Name"", ""File""))\n        sys.stdout.write(""  %-25s %s\\n"" % (""----"", ""----""))\n        for module in modules:\n            if module.path:\n                sys.stdout.write(""P"")\n            else:\n                sys.stdout.write(""m"")\n            sys.stdout.write("" %-25s %s\\n"" % (module.name, module.file or """"))\n        sys.stdout.write(""\\n"")\n\n    def _RemoveFile(self, path):\n        if os.path.exists(path):\n            os.chmod(path, stat.S_IWRITE)\n            os.remove(path)\n\n    def _RemoveVersionNumbers(self, libName):\n        tweaked = False\n        parts = libName.split(""."")\n        while parts:\n            if not parts[-1].isdigit():\n                break\n            parts.pop(-1)\n            tweaked = True\n        if tweaked:\n            libName = ""."".join(parts)\n        return libName\n\n    def _ShouldCopyFile(self, path):\n        """"""Return true if the file should be copied to the target machine. This\n           is done by checking the binPathIncludes, binPathExcludes,\n           binIncludes and binExcludes configuration variables using first the\n           full file name, then just the base file name, then the file name\n           without any version numbers.\n           \n           Files are included unless specifically excluded but inclusions take\n           precedence over exclusions.""""""\n\n        # check for C runtime, if desired\n        path = os.path.normcase(path)\n        dirName, fileName = os.path.split(path)\n        if fileName.startswith(""msvcr"") and fileName.endswith("".dll""):\n            self.msvcRuntimeDir = dirName\n            return self.includeMSVCR\n\n        # check the full path\n        if path in self.binIncludes:\n            return True\n        if path in self.binExcludes:\n            return False\n\n        # check the file name by itself (with any included version numbers)\n        if fileName in self.binIncludes:\n            return True\n        if fileName in self.binExcludes:\n            return False\n\n        # check the file name by itself (version numbers removed)\n        name = self._RemoveVersionNumbers(fileName)\n        if name in self.binIncludes:\n            return True\n        if name in self.binExcludes:\n            return False\n\n        # check the path for inclusion/exclusion\n        for path in self.binPathIncludes:\n            if dirName.startswith(path):\n                return True\n        for path in self.binPathExcludes:\n            if dirName.startswith(path):\n                return False\n\n        return True\n\n    def _ShouldIncludeInFileSystem(self, module):\n        if module.parent is not None:\n            return self._ShouldIncludeInFileSystem(module.parent)\n        if module.path is None or module.file is None:\n            return False\n        if self.zipIncludeAllPackages \\\n                and module.name not in self.zipExcludePackages \\\n                or module.name in self.zipIncludePackages:\n            return False\n        return True\n\n    def _VerifyConfiguration(self):\n        if self.compress is None:\n            self.compress = True\n        if self.targetDir is None:\n            self.targetDir = os.path.abspath(""dist"")\n        if self.path is None:\n            self.path = sys.path\n\n        for sourceFileName, targetFileName in \\\n                self.includeFiles + self.zipIncludes:\n            if not os.path.exists(sourceFileName):\n                raise ConfigError(""cannot find file/directory named %s"",\n                        sourceFileName)\n            if os.path.isabs(targetFileName):\n                raise ConfigError(""target file/directory cannot be absolute"")\n\n        self.zipExcludeAllPackages = ""*"" in self.zipExcludePackages\n        self.zipIncludeAllPackages = ""*"" in self.zipIncludePackages\n        if self.zipExcludeAllPackages and self.zipIncludeAllPackages:\n            raise ConfigError(""all packages cannot be included and excluded "" \\\n                    ""from the zip file at the same time"")\n        for name in self.zipIncludePackages:\n            if name in self.zipExcludePackages:\n                raise ConfigError(""package %s cannot be both included and "" \\\n                        ""excluded from zip file"", name)\n\n        for executable in self.executables:\n            executable._VerifyConfiguration(self)\n\n    def _WriteModules(self, fileName, finder):\n        for module in self.constantsModules:\n            module.Create(finder)\n        modules = [m for m in finder.modules \\\n                if m.name not in self.excludeModules]\n        modules.sort(key = lambda m: m.name)\n\n        if not self.silent:\n            self._PrintReport(fileName, modules)\n        finder.ReportMissingModules()\n\n        targetDir = os.path.dirname(fileName)\n        if sys.platform != ""win32"":\n            if os.path.basename(targetDir) == ""lib64"":\n                linkName = os.path.join(os.path.dirname(targetDir), ""lib"")\n                if not os.path.exists(linkName):\n                    os.symlink(""lib64"", linkName)\n            targetDir = os.path.join(targetDir,\n                    ""python%s.%s"" % sys.version_info[:2])\n        self._CreateDirectory(targetDir)\n\n        # Prepare zip file\n        outFile = zipfile.PyZipFile(fileName, ""w"", zipfile.ZIP_DEFLATED)\n\n        filesToCopy = []\n        magic = imp.get_magic()\n        ignorePatterns = shutil.ignore_patterns(""*.py"", ""*.pyc"", ""*.pyo"",\n                ""__pycache__"")\n        for module in modules:\n\n            # determine if the module should be written to the file system;\n            # a number of packages make the assumption that files that they\n            # require will be found in a location relative to where\n            # they are located on disk; these packages will fail with strange\n            # errors when they are written to a zip file instead\n            includeInFileSystem = self._ShouldIncludeInFileSystem(module)\n\n            # if the module refers to a package, check to see if this package\n            # should be included in the zip file or should be written to the\n            # file system; if the package should be written to the file system,\n            # any non-Python files are copied at this point if the target\n            # directory does not already exist\n            if module.path is not None and includeInFileSystem:\n                parts = module.name.split(""."")\n                targetPackageDir = os.path.join(targetDir, *parts)\n                sourcePackageDir = os.path.dirname(module.file)\n                if not os.path.exists(targetPackageDir):\n                    print(""Copying data from package"", module.name + ""..."")\n                    shutil.copytree(sourcePackageDir, targetPackageDir,\n                            ignore = ignorePatterns)\n\n            # if an extension module is found in a package that is to be\n            # included in a zip file, save a Python loader in the zip file and\n            # copy the actual file to the build directory because shared\n            # libraries cannot be loaded from a zip file\n            if module.code is None and module.file is not None \\\n                    and not includeInFileSystem:\n                fileName = os.path.basename(module.file)\n                if ""."" in module.name:\n                    baseFileName, ext = os.path.splitext(fileName)\n                    fileName = module.name + ext\n                    generatedFileName = ""ExtensionLoader_%s.py"" % \\\n                            module.name.replace(""."", ""_"")\n                    module.code = compile(EXTENSION_LOADER_SOURCE % fileName,\n                            generatedFileName, ""exec"")\n                target = os.path.join(targetDir, fileName)\n                filesToCopy.append((module, target))\n\n            # starting with Python 3.3 the pyc file format contains the source\n            # size; it is not actually used for anything except determining if\n            # the file is up to date so we can safely set this value to zero\n            if module.code is not None:\n                if module.file is not None and os.path.exists(module.file):\n                    mtime = os.stat(module.file).st_mtime\n                else:\n                    mtime = time.time()\n                if sys.version_info[:2] < (3, 3):\n                    header = magic + struct.pack(""<i"", int(mtime))\n                else:\n                    header = magic + struct.pack(""<ii"", int(mtime), 0)\n                data = header + marshal.dumps(module.code)\n\n            # if the module should be written to the file system, do so\n            if includeInFileSystem:\n                parts = module.name.split(""."")\n                if module.code is None:\n                    parts.pop()\n                    parts.append(os.path.basename(module.file))\n                    targetName = os.path.join(targetDir, *parts)\n                    self._CopyFile(module.file, targetName,\n                            copyDependentFiles = True)\n                else:\n                    if module.path is not None:\n                        parts.append(""__init__"")\n                    targetName = os.path.join(targetDir, *parts) + "".pyc""\n\n                    # Make target directory writeable if we\'re modifying it.\n                    targetDirname = os.path.dirname(targetName)\n                    if not os.access(targetDirname, os.W_OK):\n                        targetDirStat = os.stat(targetDirname)\n                        os.chmod(targetDirname, stat.S_IWRITE | targetDirStat.st_mode)\n\n                    open(targetName, ""wb"").write(data)\n\n\n            # otherwise, write to the zip file\n            elif module.code is not None:\n                # Don\'t accept timestamps before 1980. They aren\'t zip-safe.\n                zipTime = time.localtime(max(315561601, mtime))[:6]\n                fileName = ""/"".join(module.name.split("".""))\n                if module.path:\n                    fileName += ""/__init__""\n                zinfo = zipfile.ZipInfo(fileName + "".pyc"", zipTime)\n                if self.compress:\n                    zinfo.compress_type = zipfile.ZIP_DEFLATED\n                outFile.writestr(zinfo, data)\n\n        # write any files to the zip file that were requested specially\n        for sourceFileName, targetFileName in self.zipIncludes:\n            outFile.write(sourceFileName, targetFileName)\n\n        outFile.close()\n\n        # Copy Python extension modules from the list built above.\n        origPath = os.environ[""PATH""]\n        for module, target in filesToCopy:\n            try:\n                if module.parent is not None:\n                    path = os.pathsep.join([origPath] + module.parent.path)\n                    os.environ[""PATH""] = path\n                self._CopyFile(module.file, target, copyDependentFiles = True)\n            finally:\n                os.environ[""PATH""] = origPath\n\n    def Freeze(self):\n        self.finder = None\n        self.excludeModules = {}\n        self.dependentFiles = {}\n        self.filesCopied = {}\n        self.linkerWarnings = {}\n        self.msvcRuntimeDir = None\n        import cx_Freeze.util\n        cx_Freeze.util.SetOptimizeFlag(self.optimizeFlag)\n\n        self.finder = self._GetModuleFinder()\n        for executable in self.executables:\n            self._FreezeExecutable(executable)\n        targetDir = zipTargetDir = self.targetDir\n        rawLibDir = distutils.sysconfig.get_config_var(""LIBDIR"")\n        if rawLibDir:\n            zipTargetDir = os.path.join(targetDir, os.path.basename(rawLibDir))\n        fileName = os.path.join(zipTargetDir,\n                ""python%s%s.zip"" % sys.version_info[:2])\n        self._RemoveFile(fileName)\n        self._WriteModules(fileName, self.finder)\n\n        for sourceFileName, targetFileName in self.includeFiles:\n            if os.path.isdir(sourceFileName):\n                # Copy directories by recursing into them.\n                # Can\'t use shutil.copytree because we may need dependencies\n                for path, dirNames, fileNames in os.walk(sourceFileName):\n                    shortPath = path[len(sourceFileName) + 1:]\n                    if "".svn"" in dirNames:\n                        dirNames.remove("".svn"")\n                    if ""CVS"" in dirNames:\n                        dirNames.remove(""CVS"")\n                    fullTargetDir = os.path.join(targetDir,\n                            targetFileName, shortPath)\n                    self._CreateDirectory(fullTargetDir)\n                    for fileName in fileNames:\n                        fullSourceName = os.path.join(path, fileName)\n                        fullTargetName = os.path.join(fullTargetDir, fileName)\n                        self._CopyFile(fullSourceName, fullTargetName,\n                                copyDependentFiles = True)\n            else:\n                # Copy regular files.\n                fullName = os.path.join(targetDir, targetFileName)\n                self._CopyFile(sourceFileName, fullName,\n                        copyDependentFiles = True)\n\n\nclass ConfigError(Exception):\n\n    def __init__(self, format, *args):\n        self.what = format % args\n\n    def __str__(self):\n        return self.what\n\n\nclass Executable(object):\n\n    def __init__(self, script, initScript = None, base = None,\n            targetName = None, icon = None, shortcutName = None, \n            shortcutDir = None, copyright = None, trademarks = None):\n        self.script = script\n        self.initScript = initScript or ""Console""\n        self.base = base or ""Console""\n        self.targetName = os.path.join(""bin"", targetName)\n        self.icon = icon\n        self.shortcutName = shortcutName\n        self.shortcutDir = shortcutDir\n        self.copyright = copyright\n        self.trademarks = trademarks\n\n    def __repr__(self):\n        return ""<Executable script=%s>"" % self.script\n\n    def _VerifyConfiguration(self, freezer):\n        self._GetInitScriptFileName()\n        self._GetBaseFileName()\n        if self.targetName is None:\n            name, ext = os.path.splitext(os.path.basename(self.script))\n            baseName, ext = os.path.splitext(self.base)\n            self.targetName = name + ext\n        name, ext = os.path.splitext(os.path.basename(self.targetName))\n        self.moduleName = ""%s__main__"" % os.path.normcase(name)\n        self.initModuleName = ""%s__init__"" % os.path.normcase(name)\n        self.targetName = os.path.join(freezer.targetDir, self.targetName)\n\n    def _GetBaseFileName(self):\n        name = self.base\n        ext = "".exe"" if sys.platform == ""win32"" else """"\n        self.base = get_resource_file_path(""bases"", name, ext)\n        if self.base is None:\n            raise ConfigError(""no base named %s"", name)\n\n    def _GetInitScriptFileName(self):\n        name = self.initScript\n        self.initScript = get_resource_file_path(""initscripts"", name, "".py"")\n        if self.initScript is None:\n            raise ConfigError(""no initscript named %s"", name)\n\n\nclass ConstantsModule(object):\n\n    def __init__(self, releaseString = None, copyright = None,\n            moduleName = ""BUILD_CONSTANTS"", timeFormat = ""%B %d, %Y %H:%M:%S""):\n        self.moduleName = moduleName\n        self.timeFormat = timeFormat\n        self.values = {}\n        self.values[""BUILD_RELEASE_STRING""] = releaseString\n        self.values[""BUILD_COPYRIGHT""] = copyright\n\n    def Create(self, finder):\n        """"""Create the module which consists of declaration statements for each\n           of the values.""""""\n        today = datetime.datetime.today()\n        sourceTimestamp = 0\n        for module in finder.modules:\n            if module.file is None:\n                continue\n            if module.inZipFile:\n                continue\n            if not os.path.exists(module.file):\n                raise ConfigError(""no file named %s (for module %s)"",\n                        module.file, module.name)\n            timestamp = os.stat(module.file).st_mtime\n            sourceTimestamp = max(sourceTimestamp, timestamp)\n        sourceTimestamp = datetime.datetime.fromtimestamp(sourceTimestamp)\n        self.values[""BUILD_TIMESTAMP""] = today.strftime(self.timeFormat)\n        self.values[""BUILD_HOST""] = socket.gethostname().split(""."")[0]\n        self.values[""SOURCE_TIMESTAMP""] = \\\n                sourceTimestamp.strftime(self.timeFormat)\n        module = finder._AddModule(self.moduleName)\n        sourceParts = []\n        names = list(self.values.keys())\n        names.sort()\n        for name in names:\n            value = self.values[name]\n            sourceParts.append(""%s = %r"" % (name, value))\n        source = ""\\n"".join(sourceParts)\n        module.code = compile(source, ""%s.py"" % self.moduleName, ""exec"")\n        return module\n\n\nclass VersionInfo(object):\n\n    def __init__(self, version, internalName = None, originalFileName = None,\n            comments = None, company = None, description = None,\n            copyright = None, trademarks = None, product = None, dll = False,\n            debug = False, verbose = True):\n        parts = version.split(""."")\n        while len(parts) < 4:\n            parts.append(""0"")\n        self.version = ""."".join(parts)\n        self.internal_name = internalName\n        self.original_filename = originalFileName\n        self.comments = comments\n        self.company = company\n        self.description = description\n        self.copyright = copyright\n        self.trademarks = trademarks\n        self.product = product\n        self.dll = dll\n        self.debug = debug\n        self.verbose = verbose\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/hooks.py,0,"b'import os\nimport sys\n\ndef initialize(finder):\n    """"""upon initialization of the finder, this routine is called to set up some\n       automatic exclusions for various platforms.""""""\n    finder.ExcludeModule(""FCNTL"")\n    finder.ExcludeModule(""os.path"")\n    if os.name == ""nt"":\n        finder.ExcludeModule(""fcntl"")\n        finder.ExcludeModule(""grp"")\n        finder.ExcludeModule(""pwd"")\n        finder.ExcludeModule(""termios"")\n    else:\n        finder.ExcludeModule(""_subprocess"")\n        finder.ExcludeModule(""_winreg"")\n        finder.ExcludeModule(""msilib"")\n        finder.ExcludeModule(""msvcrt"")\n        finder.ExcludeModule(""multiprocessing._multiprocessing"")\n        finder.ExcludeModule(""nt"")\n        finder.ExcludeModule(""nturl2path"")\n        finder.ExcludeModule(""pyHook"")\n        finder.ExcludeModule(""pythoncom"")\n        finder.ExcludeModule(""pywintypes"")\n        finder.ExcludeModule(""winerror"")\n        finder.ExcludeModule(""winsound"")\n        finder.ExcludeModule(""win32api"")\n        finder.ExcludeModule(""win32con"")\n        finder.ExcludeModule(""win32gui"")\n        finder.ExcludeModule(""win32event"")\n        finder.ExcludeModule(""win32evtlog"")\n        finder.ExcludeModule(""win32evtlogutil"")\n        finder.ExcludeModule(""win32file"")\n        finder.ExcludeModule(""win32pdh"")\n        finder.ExcludeModule(""win32pipe"")\n        finder.ExcludeModule(""win32process"")\n        finder.ExcludeModule(""win32security"")\n        finder.ExcludeModule(""win32service"")\n        finder.ExcludeModule(""wx.activex"")\n    if os.name != ""posix"":\n        finder.ExcludeModule(""posix"")\n    if sys.platform != ""darwin"":\n        finder.ExcludeModule(""Carbon"")\n        finder.ExcludeModule(""gestalt"")\n        finder.ExcludeModule(""ic"")\n        finder.ExcludeModule(""mac"")\n        finder.ExcludeModule(""MacOS"")\n        finder.ExcludeModule(""macostools"")\n        finder.ExcludeModule(""macpath"")\n        finder.ExcludeModule(""macurl2path"")\n        finder.ExcludeModule(""_scproxy"")\n        if os.name != ""nt"":\n            finder.ExcludeModule(""EasyDialogs"")\n    if os.name != ""os2"":\n        finder.ExcludeModule(""os2"")\n        finder.ExcludeModule(""os2emxpath"")\n        finder.ExcludeModule(""_emx_link"")\n    if os.name != ""ce"":\n        finder.ExcludeModule(""ce"")\n    if os.name != ""riscos"":\n        finder.ExcludeModule(""riscos"")\n        finder.ExcludeModule(""riscosenviron"")\n        finder.ExcludeModule(""riscospath"")\n        finder.ExcludeModule(""rourl2path"")\n    if sys.platform[:4] != ""java"":\n        finder.ExcludeModule(""java.lang"")\n        finder.ExcludeModule(""org.python.core"")\n    if sys.platform[:4] != ""OpenVMS"":\n        finder.ExcludeModule(""vms_lib"")\n    if sys.version_info[0] >= 3:\n        finder.ExcludeModule(""new"")\n        finder.ExcludeModule(""Tkinter"")\n    else:\n        finder.ExcludeModule(""tkinter"")\n\n\ndef load_cElementTree(finder, module):\n    """"""the cElementTree module implicitly loads the elementtree.ElementTree\n       module; make sure this happens.""""""\n    finder.IncludeModule(""elementtree.ElementTree"")\n\n\ndef load_ceODBC(finder, module):\n    """"""the ceODBC module implicitly imports both datetime and decimal; make\n       sure this happens.""""""\n    finder.IncludeModule(""datetime"")\n    finder.IncludeModule(""decimal"")\n\n\ndef load_cx_Oracle(finder, module):\n    """"""the cx_Oracle module implicitly imports datetime; make sure this\n       happens.""""""\n    finder.IncludeModule(""datetime"")\n    try:\n        finder.IncludeModule(""decimal"")\n    except ImportError:\n        pass\n\n\ndef load_datetime(finder, module):\n    """"""the datetime module implicitly imports time; make sure this happens.""""""\n    finder.IncludeModule(""time"")\n\n\ndef load_docutils_frontend(finder, module):\n    """"""The optik module is the old name for the optparse module; ignore the\n       module if it cannot be found.""""""\n    module.IgnoreName(""optik"")\n\n\ndef load_dummy_threading(finder, module):\n    """"""the dummy_threading module plays games with the name of the threading\n       module for its own purposes; ignore that here""""""\n    finder.ExcludeModule(""_dummy_threading"")\n\n\ndef load_email(finder, module):\n    """"""the email package has a bunch of aliases as the submodule names were\n       all changed to lowercase in Python 2.5; mimic that here.""""""\n    if sys.version_info[:2] >= (2, 5):\n        for name in (""Charset"", ""Encoders"", ""Errors"", ""FeedParser"",\n                ""Generator"", ""Header"", ""Iterators"", ""Message"", ""Parser"",\n                ""Utils"", ""base64MIME"", ""quopriMIME""):\n            finder.AddAlias(""email.%s"" % name, ""email.%s"" % name.lower())\n\n\ndef load_ftplib(finder, module):\n    """"""the ftplib module attempts to import the SOCKS module; ignore this\n       module if it cannot be found""""""\n    module.IgnoreName(""SOCKS"")\n\n\ndef load_GifImagePlugin(finder, module):\n    """"""The GifImagePlugin module optionally imports the _imaging_gif module""""""\n    module.IgnoreName(""_imaging_gif"")\n\n\ndef load_glib(finder, module):\n    """"""Ignore globals that are imported.""""""\n    module.AddGlobalName(""GError"")\n    module.AddGlobalName(""IOChannel"")\n    module.AddGlobalName(""IO_ERR"")\n    module.AddGlobalName(""IO_FLAG_APPEND"")\n    module.AddGlobalName(""IO_FLAG_GET_MASK"")\n    module.AddGlobalName(""IO_FLAG_IS_READABLE"")\n    module.AddGlobalName(""IO_FLAG_IS_SEEKABLE"")\n    module.AddGlobalName(""IO_FLAG_IS_WRITEABLE"")\n    module.AddGlobalName(""IO_FLAG_MASK"")\n    module.AddGlobalName(""IO_FLAG_NONBLOCK"")\n    module.AddGlobalName(""IO_FLAG_SET_MASK"")\n    module.AddGlobalName(""IO_HUP"")\n    module.AddGlobalName(""IO_IN"")\n    module.AddGlobalName(""IO_NVAL"")\n    module.AddGlobalName(""IO_OUT"")\n    module.AddGlobalName(""IO_PRI"")\n    module.AddGlobalName(""IO_STATUS_AGAIN"")\n    module.AddGlobalName(""IO_STATUS_EOF"")\n    module.AddGlobalName(""IO_STATUS_ERROR"")\n    module.AddGlobalName(""IO_STATUS_NORMAL"")\n    module.AddGlobalName(""Idle"")\n    module.AddGlobalName(""MainContext"")\n    module.AddGlobalName(""MainLoop"")\n    module.AddGlobalName(""OPTION_ERROR"")\n    module.AddGlobalName(""OPTION_ERROR_BAD_VALUE"")\n    module.AddGlobalName(""OPTION_ERROR_FAILED"")\n    module.AddGlobalName(""OPTION_ERROR_UNKNOWN_OPTION"")\n    module.AddGlobalName(""OPTION_FLAG_FILENAME"")\n    module.AddGlobalName(""OPTION_FLAG_HIDDEN"")\n    module.AddGlobalName(""OPTION_FLAG_IN_MAIN"")\n    module.AddGlobalName(""OPTION_FLAG_NOALIAS"")\n    module.AddGlobalName(""OPTION_FLAG_NO_ARG"")\n    module.AddGlobalName(""OPTION_FLAG_OPTIONAL_ARG"")\n    module.AddGlobalName(""OPTION_FLAG_REVERSE"")\n    module.AddGlobalName(""OPTION_REMAINING"")\n    module.AddGlobalName(""OptionContext"")\n    module.AddGlobalName(""OptionGroup"")\n    module.AddGlobalName(""PRIORITY_DEFAULT"")\n    module.AddGlobalName(""PRIORITY_DEFAULT_IDLE"")\n    module.AddGlobalName(""PRIORITY_HIGH"")\n    module.AddGlobalName(""PRIORITY_HIGH_IDLE"")\n    module.AddGlobalName(""PRIORITY_LOW"")\n    module.AddGlobalName(""Pid"")\n    module.AddGlobalName(""PollFD"")\n    module.AddGlobalName(""SPAWN_CHILD_INHERITS_STDIN"")\n    module.AddGlobalName(""SPAWN_DO_NOT_REAP_CHILD"")\n    module.AddGlobalName(""SPAWN_FILE_AND_ARGV_ZERO"")\n    module.AddGlobalName(""SPAWN_LEAVE_DESCRIPTORS_OPEN"")\n    module.AddGlobalName(""SPAWN_SEARCH_PATH"")\n    module.AddGlobalName(""SPAWN_STDERR_TO_DEV_NULL"")\n    module.AddGlobalName(""SPAWN_STDOUT_TO_DEV_NULL"")\n    module.AddGlobalName(""Source"")\n    module.AddGlobalName(""Timeout"")\n    module.AddGlobalName(""child_watch_add"")\n    module.AddGlobalName(""filename_display_basename"")\n    module.AddGlobalName(""filename_display_name"")\n    module.AddGlobalName(""filename_from_utf8"")\n    module.AddGlobalName(""get_application_name"")\n    module.AddGlobalName(""get_current_time"")\n    module.AddGlobalName(""get_prgname"")\n    module.AddGlobalName(""glib_version"")\n    module.AddGlobalName(""idle_add"")\n    module.AddGlobalName(""io_add_watch"")\n    module.AddGlobalName(""main_context_default"")\n    module.AddGlobalName(""main_depth"")\n    module.AddGlobalName(""markup_escape_text"")\n    module.AddGlobalName(""set_application_name"")\n    module.AddGlobalName(""set_prgname"")\n    module.AddGlobalName(""source_remove"")\n    module.AddGlobalName(""spawn_async"")\n    module.AddGlobalName(""timeout_add"")\n    module.AddGlobalName(""timeout_add_seconds"")\n\n\ndef load_gtk__gtk(finder, module):\n    """"""the gtk._gtk module has a number of implicit imports""""""\n    finder.IncludeModule(""atk"")\n    finder.IncludeModule(""cairo"")\n    finder.IncludeModule(""gio"")\n    finder.IncludeModule(""pango"")\n    finder.IncludeModule(""pangocairo"")\n\n\ndef load_hashlib(finder, module):\n    """"""hashlib\'s fallback modules don\'t exist if the equivalent OpenSSL\n    algorithms are loaded from _hashlib, so we can ignore the error.""""""\n    module.IgnoreName(""_md5"")\n    module.IgnoreName(""_sha"")\n    module.IgnoreName(""_sha256"")\n    module.IgnoreName(""_sha512"")\n\n\ndef load_h5py(finder, module):\n    """"""h5py module has a number of implicit imports""""""\n    finder.IncludeModule(\'h5py.defs\')\n    finder.IncludeModule(\'h5py.utils\')\n    finder.IncludeModule(\'h5py._proxy\')\n    try:\n        import h5py.api_gen\n        finder.IncludeModule(\'h5py.api_gen\')\n    except ImportError:\n        pass\n    finder.IncludeModule(\'h5py._errors\')\n    finder.IncludeModule(\'h5py.h5ac\')\n\n\ndef load_matplotlib(finder, module):\n    """"""the matplotlib module requires data to be found in mpl-data in the\n       same directory as the frozen executable so oblige it""""""\n    dir = os.path.join(module.path[0], ""mpl-data"")\n    finder.IncludeFiles(dir, ""mpl-data"")\n\n\ndef load_matplotlib_numerix(finder, module):\n    """"""the numpy.numerix module loads a number of modules dynamically""""""\n    for name in (""ma"", ""fft"", ""linear_algebra"", ""random_array"", ""mlab""):\n        finder.IncludeModule(""%s.%s"" % (module.name, name))\n\n\ndef load_Numeric(finder, module):\n    """"""the Numeric module optionally loads the dotblas module; ignore the error\n       if this modules does not exist.""""""\n    module.IgnoreName(""dotblas"")\n\n\ndef load_numpy_core_multiarray(finder, module):\n    """"""the numpy.core.multiarray module is an extension module and the numpy\n       module imports * from this module; define the list of global names\n       available to this module in order to avoid spurious errors about missing\n       modules""""""\n    module.AddGlobalName(""arange"")\n\n\ndef load_numpy_core_numerictypes(finder, module):\n    """"""the numpy.core.numerictypes module adds a number of items to itself\n       dynamically; define these to avoid spurious errors about missing\n       modules""""""\n    module.AddGlobalName(""bool_"")\n    module.AddGlobalName(""cdouble"")\n    module.AddGlobalName(""complexfloating"")\n    module.AddGlobalName(""csingle"")\n    module.AddGlobalName(""double"")\n    module.AddGlobalName(""float64"")\n    module.AddGlobalName(""float_"")\n    module.AddGlobalName(""inexact"")\n    module.AddGlobalName(""intc"")\n    module.AddGlobalName(""int32"")\n    module.AddGlobalName(""number"")\n    module.AddGlobalName(""single"")\n\n\ndef load_numpy_core_umath(finder, module):\n    """"""the numpy.core.umath module is an extension module and the numpy module\n       imports * from this module; define the list of global names available\n       to this module in order to avoid spurious errors about missing\n       modules""""""\n    module.AddGlobalName(""add"")\n    module.AddGlobalName(""absolute"")\n    module.AddGlobalName(""arccos"")\n    module.AddGlobalName(""arccosh"")\n    module.AddGlobalName(""arcsin"")\n    module.AddGlobalName(""arcsinh"")\n    module.AddGlobalName(""arctan"")\n    module.AddGlobalName(""arctanh"")\n    module.AddGlobalName(""bitwise_and"")\n    module.AddGlobalName(""bitwise_or"")\n    module.AddGlobalName(""bitwise_xor"")\n    module.AddGlobalName(""ceil"")\n    module.AddGlobalName(""conj"")\n    module.AddGlobalName(""conjugate"")\n    module.AddGlobalName(""cosh"")\n    module.AddGlobalName(""divide"")\n    module.AddGlobalName(""fabs"")\n    module.AddGlobalName(""floor"")\n    module.AddGlobalName(""floor_divide"")\n    module.AddGlobalName(""fmod"")\n    module.AddGlobalName(""greater"")\n    module.AddGlobalName(""hypot"")\n    module.AddGlobalName(""invert"")\n    module.AddGlobalName(""isfinite"")\n    module.AddGlobalName(""isinf"")\n    module.AddGlobalName(""isnan"")\n    module.AddGlobalName(""less"")\n    module.AddGlobalName(""left_shift"")\n    module.AddGlobalName(""log"")\n    module.AddGlobalName(""logical_and"")\n    module.AddGlobalName(""logical_not"")\n    module.AddGlobalName(""logical_or"")\n    module.AddGlobalName(""logical_xor"")\n    module.AddGlobalName(""maximum"")\n    module.AddGlobalName(""minimum"")\n    module.AddGlobalName(""multiply"")\n    module.AddGlobalName(""negative"")\n    module.AddGlobalName(""not_equal"")\n    module.AddGlobalName(""power"")\n    module.AddGlobalName(""remainder"")\n    module.AddGlobalName(""right_shift"")\n    module.AddGlobalName(""sign"")\n    module.AddGlobalName(""sinh"")\n    module.AddGlobalName(""sqrt"")\n    module.AddGlobalName(""tan"")\n    module.AddGlobalName(""tanh"")\n    module.AddGlobalName(""true_divide"")\n\n\ndef load_numpy_distutils_command_scons(finder, module):\n    """"""the numpy.distutils.command.scons module optionally imports the numscons\n       module; ignore the error if the module cannot be found.""""""\n    module.IgnoreName(""numscons"")\n\n\ndef load_numpy_distutils_misc_util(finder, module):\n    """"""the numpy.distutils.misc_util module optionally imports the numscons\n       module; ignore the error if the module cannot be found.""""""\n    module.IgnoreName(""numscons"")\n\n\ndef load_numpy_distutils_system_info(finder, module):\n    """"""the numpy.distutils.system_info module optionally imports the Numeric\n       module; ignore the error if the module cannot be found.""""""\n    module.IgnoreName(""Numeric"")\n\n\ndef load_numpy_f2py___version__(finder, module):\n    """"""the numpy.f2py.__version__ module optionally imports the __svn_version__\n       module; ignore the error if the module cannot be found.""""""\n    module.IgnoreName(""__svn_version__"")\n\n\ndef load_numpy_linalg(finder, module):\n    """"""the numpy.linalg module implicitly loads the lapack_lite module; make\n       sure this happens""""""\n    finder.IncludeModule(""numpy.linalg.lapack_lite"")\n\n\ndef load_numpy_random_mtrand(finder, module):\n    """"""the numpy.random.mtrand module is an extension module and the numpy\n       module imports * from this module; define the list of global names\n       available to this module in order to avoid spurious errors about missing\n       modules""""""\n    module.AddGlobalName(""rand"")\n    module.AddGlobalName(""randn"")\n\n\ndef load_postgresql_lib(finder, module):\n    """"""the postgresql.lib module requires the libsys.sql file to be included\n       so make sure that file is included""""""\n    fileName = os.path.join(module.path[0], ""libsys.sql"")\n    finder.IncludeFiles(fileName, os.path.basename(fileName))\n\n\ndef load_pty(finder, module):\n    """"""The sgi module is not needed for this module to function.""""""\n    module.IgnoreName(""sgi"")\n\n\ndef load_pydoc(finder, module):\n    """"""The pydoc module will work without the Tkinter module so ignore the\n       error if that module cannot be found.""""""\n    module.IgnoreName(""Tkinter"")\n\n\ndef load_pythoncom(finder, module):\n    """"""the pythoncom module is actually contained in a DLL but since those\n       cannot be loaded directly in Python 2.5 and higher a special module is\n       used to perform that task; simply use that technique directly to\n       determine the name of the DLL and ensure it is included as a file in\n       the target directory.""""""\n    import pythoncom\n    finder.IncludeFiles(pythoncom.__file__,\n            os.path.basename(pythoncom.__file__))\n\n\ndef load_pywintypes(finder, module):\n    """"""the pywintypes module is actually contained in a DLL but since those\n       cannot be loaded directly in Python 2.5 and higher a special module is\n       used to perform that task; simply use that technique directly to\n       determine the name of the DLL and ensure it is included as a file in the\n       target directory.""""""\n    import pywintypes\n    finder.IncludeFiles(pywintypes.__file__,\n            os.path.basename(pywintypes.__file__))\n\n\n# PyQt5 and PyQt4 can\'t both be loaded in the same process, so we cache the\n# QtCore module so we can still return something sensible if we try to load\n# both.\n_qtcore = None\ndef _qt_implementation(module):\n    """"""Helper function to get name (PyQt4, PyQt5, PySide) and the QtCore module\n    """"""\n    global _qtcore\n    name = module.name.split(\'.\')[0]\n    try:\n        _qtcore = __import__(name, fromlist=[\'QtCore\']).QtCore\n    except RuntimeError:\n        print(""WARNING: Tried to load multiple incompatible Qt wrappers. ""\n              ""Some incorrect files may be copied."")\n    return name, _qtcore\n\ndef copy_qt_plugins(plugins, finder, QtCore):\n    """"""Helper function to find and copy Qt plugins.""""""\n    \n    # Qt Plugins can either be in a plugins directory next to the Qt libraries,\n    # or in other locations listed by QCoreApplication.libraryPaths()\n    dir0 = os.path.join(os.path.dirname(QtCore.__file__), ""plugins"")\n    for libpath in QtCore.QCoreApplication.libraryPaths() + [dir0]:\n        sourcepath = os.path.join(str(libpath), plugins)\n        if os.path.exists(sourcepath):\n            finder.IncludeFiles(sourcepath, plugins)\n\n\ndef load_PyQt4_phonon(finder, module):\n    """"""In Windows, phonon4.dll requires an additional dll phonon_ds94.dll to\n       be present in the build directory inside a folder phonon_backend.""""""\n    name, QtCore = _qt_implementation(module)\n    if sys.platform == ""win32"":\n        copy_qt_plugins(""phonon_backend"", finder, QtCore)\n\nload_PySide_phonon = load_PyQt5_phonon = load_PyQt4_phonon\n\ndef load_PyQt4_QtCore(finder, module):\n    """"""the PyQt4.QtCore module implicitly imports the sip module and,\n       depending on configuration, the PyQt4._qt module.""""""\n    name, QtCore = _qt_implementation(module)\n    finder.IncludeModule(""sip"")\n    try:\n        finder.IncludeModule(""%s._qt"" % name)\n    except ImportError:\n        pass\n\nload_PyQt5_QtCore = load_PyQt4_QtCore\n\ndef load_PySide_QtCore(finder, module):\n    """"""PySide.QtCore dynamically loads the stdlib atexit module.""""""\n    finder.IncludeModule(""atexit"")\n\ndef load_PyQt4_Qt(finder, module):\n    """"""the PyQt4.Qt module is an extension module which imports a number of\n       other modules and injects their namespace into its own. It seems a\n       foolish way of doing things but perhaps there is some hidden advantage\n       to this technique over pure Python; ignore the absence of some of\n       the modules since not every installation includes all of them.""""""\n    name, QtCore = _qt_implementation(module)\n    finder.IncludeModule(""%s.QtCore"" % name)\n    finder.IncludeModule(""%s.QtGui"" % name)\n    for mod in (""_qt"", ""QtSvg"", ""Qsci"", ""QtAssistant"", ""QtNetwork"", ""QtOpenGL"",\n                ""QtScript"", ""QtSql"", ""QtSvg"", ""QtTest"", ""QtXml""):\n        try:\n            finder.IncludeModule(name + \'.\' + mod)\n        except ImportError:\n            pass\n\nload_PyQt5_Qt = load_PyQt4_Qt\n\ndef load_PyQt4_uic(finder, module):\n    """"""The uic module makes use of ""plugins"" that need to be read directly and\n       cannot be frozen; the PyQt4.QtWebKit and PyQt4.QtNetwork modules are\n       also implicity loaded.""""""\n    name, QtCore = _qt_implementation(module)\n    dir = os.path.join(module.path[0], ""widget-plugins"")\n    finder.IncludeFiles(dir, ""%s.uic.widget-plugins"" % name)\n    finder.IncludeModule(""%s.QtNetwork"" % name)\n    try:\n        finder.IncludeModule(""%s.QtWebKit"" % name)\n    except ImportError:\n        pass\n\nload_PyQt5_uic = load_PyQt4_uic\n\ndef _QtGui(finder, module, version_str):\n    name, QtCore = _qt_implementation(module)\n    finder.IncludeModule(""%s.QtCore"" % name)\n    copy_qt_plugins(""imageformats"", finder, QtCore)\n    if version_str >= \'5\':\n        # On Qt5, we need the platform plugins. For simplicity, we just copy any\n        # that are installed.\n        copy_qt_plugins(""platforms"", finder, QtCore)\n\ndef load_PyQt4_QtGui(finder, module):\n    """"""There is a chance that GUI will use some image formats\n    add the image format plugins\n    """"""\n    name, QtCore = _qt_implementation(module)\n    _QtGui(finder, module, QtCore.QT_VERSION_STR)\n\nload_PyQt5_QtGui = load_PyQt4_QtGui\n\ndef load_PySide_QtGui(finder, module):\n    """"""There is a chance that GUI will use some image formats\n    add the image format plugins\n    """"""\n    from PySide import QtCore\n    # Pyside.__version* is PySide version, PySide.QtCore.__version* is Qt version\n    _QtGui(finder, module, QtCore.__version__)\n\ndef load_PyQt5_QtWidgets(finder, module):\n    finder.IncludeModule(\'PyQt5.QtGui\')\n\ndef load_PyQt4_QtWebKit(finder, module):\n    name, QtCore = _qt_implementation(module)\n    finder.IncludeModule(""%s.QtNetwork"" % name)\n    finder.IncludeModule(""%s.QtGui"" % name)\n\nload_PyQt5_QtWebKit = load_PySide_QtWebKit = load_PyQt4_QtWebKit\n\ndef load_PyQt5_QtMultimedia(finder, module):\n    name, QtCore = _qt_implementation(module)\n    finder.IncludeModule(""%s.QtCore"" % name)\n    finder.IncludeModule(""%s.QtMultimediaWidgets"" % name)\n    copy_qt_plugins(""mediaservice"", finder, QtCore)\n\n\ndef load_reportlab(finder, module):\n    """"""the reportlab module loads a submodule rl_settings via exec so force\n       its inclusion here""""""\n    finder.IncludeModule(""reportlab.rl_settings"")\n\n\ndef load_scipy(finder, module):\n    """"""the scipy module loads items within itself in a way that causes\n       problems without the entire package and a number of other subpackages\n       being present.""""""\n    finder.IncludePackage(""scipy.lib"")\n    finder.IncludePackage(""scipy.misc"")\n\n\ndef load_scipy_linalg(finder, module):\n    """"""the scipy.linalg module loads items within itself in a way that causes\n       problems without the entire package being present.""""""\n    module.AddGlobalName(""norm"")\n    finder.IncludePackage(""scipy.linalg"")\n\n\ndef load_scipy_linalg_interface_gen(finder, module):\n    """"""the scipy.linalg.interface_gen module optionally imports the pre module;\n       ignore the error if this module cannot be found""""""\n    module.IgnoreName(""pre"")\n\n\ndef load_scipy_sparse_linalg_dsolve_linsolve(finder, module):\n    """"""the scipy.linalg.dsolve.linsolve optionally loads scikits.umfpack""""""\n    module.IgnoreName(""scikits.umfpack"")\n\n\ndef load_scipy_special__cephes(finder, module):\n    """"""the scipy.special._cephes is an extension module and the scipy module\n       imports * from it in places; advertise the global names that are used\n       in order to avoid spurious errors about missing modules.""""""\n    module.AddGlobalName(""gammaln"")\n\n\ndef load_setuptools_extension(finder, module):\n    """"""the setuptools.extension module optionally loads\n       Pyrex.Distutils.build_ext but its absence is not considered an error.""""""\n    module.IgnoreName(""Pyrex.Distutils.build_ext"")\n\n\ndef load_site(finder, module):\n    """"""the site module optionally loads the sitecustomize and usercustomize\n       modules; ignore the error if these modules do not exist.""""""\n    module.IgnoreName(""sitecustomize"")\n    module.IgnoreName(""usercustomize"")\n\n\ndef load_tkinter(finder, module):\n    """"""the tkinter module has data files that are required to be loaded so\n       ensure that they are copied into the directory that is expected at\n       runtime.""""""\n    if sys.platform == ""win32"":\n        import tkinter\n        import _tkinter\n        tclSourceDir = os.environ[""TCL_LIBRARY""]\n        tkSourceDir = os.environ[""TK_LIBRARY""]\n        finder.IncludeFiles(tclSourceDir, ""tcl"")\n        finder.IncludeFiles(tkSourceDir, ""tk"")\n\n\ndef load_Tkinter(finder, module):\n    """"""the Tkinter module has data files that are required to be loaded so\n       ensure that they are copied into the directory that is expected at\n       runtime.""""""\n    import Tkinter\n    import _tkinter\n    tk = _tkinter.create()\n    tclDir = os.path.dirname(tk.call(""info"", ""library""))\n    # on OS X, Tcl and Tk are organized in frameworks, different layout\n    if sys.platform == \'darwin\' and tk.call(\'tk\', \'windowingsystem\') == \'aqua\':\n        tclSourceDir=os.path.join(os.path.split(tclDir)[0], \'Tcl\')\n        tkSourceDir = tclSourceDir.replace(\'Tcl\', \'Tk\')\n    else:\n        tclSourceDir = os.path.join(tclDir, ""tcl%s"" % _tkinter.TCL_VERSION)\n        tkSourceDir = os.path.join(tclDir, ""tk%s"" % _tkinter.TK_VERSION)\n    finder.IncludeFiles(tclSourceDir, ""tcl"")\n    finder.IncludeFiles(tkSourceDir, ""tk"")\n\n\ndef load_tempfile(finder, module):\n    """"""the tempfile module attempts to load the fcntl and thread modules but\n       continues if these modules cannot be found; ignore these modules if they\n       cannot be found.""""""\n    module.IgnoreName(""fcntl"")\n    module.IgnoreName(""thread"")\n\n\ndef load_time(finder, module):\n    """"""the time module implicitly loads _strptime; make sure this happens.""""""\n    finder.IncludeModule(""_strptime"")\n\n\ndef load_twisted_conch_ssh_transport(finder, module):\n    """"""the twisted.conch.ssh.transport module uses __import__ builtin to\n       dynamically load different ciphers at runtime.""""""\n    finder.IncludePackage(""Crypto.Cipher"")\n\n\ndef load_twitter(finder, module):\n    """"""the twitter module tries to load the simplejson, json and django.utils\n       module in an attempt to locate any module that will implement the\n       necessary protocol; ignore these modules if they cannot be found.""""""\n    module.IgnoreName(""json"")\n    module.IgnoreName(""simplejson"")\n    module.IgnoreName(""django.utils"")\n\n\ndef load_win32api(finder, module):\n    """"""the win32api module implicitly loads the pywintypes module; make sure\n       this happens.""""""\n    finder.IncludeModule(""pywintypes"")\n\n\ndef load_win32com(finder, module):\n    """"""the win32com package manipulates its search path at runtime to include\n       the sibling directory called win32comext; simulate that by changing the\n       search path in a similar fashion here.""""""\n    baseDir = os.path.dirname(os.path.dirname(module.file))\n    module.path.append(os.path.join(baseDir, ""win32comext""))\n\n\ndef load_win32file(finder, module):\n    """"""the win32api module implicitly loads the pywintypes module; make sure\n       this happens.""""""\n    finder.IncludeModule(""pywintypes"")\n\n\ndef load_wx_lib_pubsub_core(finder, module):\n    """"""the wx.lib.pubsub.core module modifies the search path which cannot\n       be done in a frozen application in the same way; modify the module\n       search path here instead so that the right modules are found; note\n       that this only works if the import of wx.lib.pubsub.setupkwargs\n       occurs first.""""""\n    dirName = os.path.dirname(module.file)\n    module.path.insert(0, os.path.join(dirName, ""kwargs""))\n\n\ndef load_Xlib_display(finder, module):\n    """"""the Xlib.display module implicitly loads a number of extension modules;\n       make sure this happens.""""""\n    finder.IncludeModule(""Xlib.ext.xtest"")\n    finder.IncludeModule(""Xlib.ext.shape"")\n    finder.IncludeModule(""Xlib.ext.xinerama"")\n    finder.IncludeModule(""Xlib.ext.record"")\n    finder.IncludeModule(""Xlib.ext.composite"")\n    finder.IncludeModule(""Xlib.ext.randr"")\n\n\ndef load_Xlib_support_connect(finder, module):\n    """"""the Xlib.support.connect module implicitly loads a platform specific\n       module; make sure this happens.""""""\n    if sys.platform.split(""-"")[0] == ""OpenVMS"":\n        moduleName = ""vms_connect""\n    else:\n        moduleName = ""unix_connect""\n    finder.IncludeModule(""Xlib.support.%s"" % moduleName)\n\n\ndef load_Xlib_XK(finder, module):\n    """"""the Xlib.XK module implicitly loads some keysymdef modules; make sure\n       this happens.""""""\n    finder.IncludeModule(""Xlib.keysymdef.miscellany"")\n    finder.IncludeModule(""Xlib.keysymdef.latin1"")\n\n\ndef load_xml(finder, module):\n    """"""the builtin xml package attempts to load the _xmlplus module to see if\n       that module should take its role instead; ignore the failure to find\n       this module, though.""""""\n    module.IgnoreName(""_xmlplus"")\n\n\ndef load_xml_etree_cElementTree(finder, module):\n    """"""the xml.etree.cElementTree module implicitly loads the\n       xml.etree.ElementTree module; make sure this happens.""""""\n    finder.IncludeModule(""xml.etree.ElementTree"")\n\n\ndef load_xmlrpclib(finder, module):\n    """"""the xmlrpclib optionally imports the _xmlrpclib and sgmlop modules;\n       ignore the error if these modules cannot be found.""""""\n    module.IgnoreName(""_xmlrpclib"")\n    module.IgnoreName(""sgmlop"")\n\n\ndef load_zope(finder, module):\n    """"""the zope package is distributed in multiple packages and they need to be\n       stitched back together again.""""""\n    module.ExtendPath()\n\n\ndef load_zope_component(finder, module):\n    """"""the zope.component package requires the presence of the pkg_resources\n       module but it uses a dynamic, not static import to do its work.""""""\n    finder.IncludeModule(""pkg_resources"")\n\n\ndef missing_cElementTree(finder, caller):\n    """"""the cElementTree has been incorporated into the standard library in\n       Python 2.5 so ignore its absence if it cannot found.""""""\n    if sys.version_info[:2] >= (2, 5):\n        caller.IgnoreName(""cElementTree"")\n\n\ndef missing_EasyDialogs(finder, caller):\n    """"""the EasyDialogs module is not normally present on Windows but it also\n       may be so instead of excluding it completely, ignore it if it can\'t be\n       found""""""\n    if sys.platform == ""win32"":\n        caller.IgnoreName(""EasyDialogs"")\n\n\ndef missing_gdk(finder, caller):\n    """"""the gdk module is buried inside gtk so there is no need to concern\n       ourselves with an error saying that it cannot be found""""""\n    caller.IgnoreName(""gdk"")\n\n\ndef missing_ltihooks(finder, caller):\n    """"""this module is not necessairly present so ignore it when it cannot be\n       found""""""\n    caller.IgnoreName(""ltihooks"")\n\n\ndef missing_readline(finder, caller):\n    """"""the readline module is not normally present on Windows but it also may\n       be so instead of excluding it completely, ignore it if it can\'t be\n       found""""""\n    if sys.platform == ""win32"":\n        caller.IgnoreName(""readline"")\n\n\ndef missing_xml_etree(finder, caller):\n    """"""the xml.etree package is new for Python 2.5 but it is common practice\n       to use a try..except.. block in order to support versions earlier than\n       Python 2.5 transparently; ignore the absence of the package in this\n       situation.""""""\n    if sys.version_info[:2] < (2, 5):\n        caller.IgnoreName(""xml.etree"")\n\n\ndef load_zmq(finder, module):\n    """"""the zmq package loads zmq.backend.cython dynamically and links \n    dynamically to zmq.libzmq.""""""\n    finder.IncludePackage(""zmq.backend.cython"")\n    if sys.platform == ""win32"":\n        # Not sure yet if this is cross platform\n        import zmq.libzmq\n        srcFileName = os.path.basename(zmq.libzmq.__file__)\n        finder.IncludeFiles(\n            os.path.join(module.path[0], srcFileName), srcFileName)\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/macdist.py,0,"b'from distutils.core import Command\nimport os\nimport plistlib\nimport stat\nimport subprocess\n\nfrom cx_Freeze.common import normalize_to_list\n\n__all__ = [""bdist_dmg"", ""bdist_mac""]\n\n\nclass bdist_dmg(Command):\n    description = ""create a Mac DMG disk image containing the Mac "" \\\n        ""application bundle""\n    user_options = [\n        (\'volume-label=\', None, \'Volume label of the DMG disk image\'),\n        (\'applications-shortcut=\', None, \'Boolean for whether to include \'\n            \'shortcut to Applications in the DMG disk image\'),\n    ]\n\n    def initialize_options(self):\n        self.volume_label = self.distribution.get_fullname()\n        self.applications_shortcut = False\n\n    def finalize_options(self):\n        pass\n\n    def buildDMG(self):\n        # Remove DMG if it already exists\n        if os.path.exists(self.dmgName):\n            os.unlink(self.dmgName)\n\n        createargs = [\n            \'hdiutil\', \'create\', \'-fs\', \'HFSX\', \'-format\', \'UDZO\',\n            self.dmgName, \'-imagekey\', \'zlib-level=9\', \'-srcfolder\',\n            self.bundleDir, \'-volname\', self.volume_label\n        ]\n\n        if self.applications_shortcut:\n            scriptargs = [\n                \'osascript\', \'-e\', \'tell application ""Finder"" to make alias \\\n                file to POSIX file ""/Applications"" at POSIX file ""%s""\' %\n                os.path.realpath(self.buildDir)\n            ]\n\n            if os.spawnvp(os.P_WAIT, \'osascript\', scriptargs) != 0:\n                raise OSError(\'creation of Applications shortcut failed\')\n\n            createargs.append(\'-srcfolder\')\n            createargs.append(self.buildDir + \'/Applications\')\n\n        # Create the dmg\n        if os.spawnvp(os.P_WAIT, \'hdiutil\', createargs) != 0:\n            raise OSError(\'creation of the dmg failed\')\n\n    def run(self):\n        # Create the application bundle\n        self.run_command(\'bdist_mac\')\n\n        # Find the location of the application bundle and the build dir\n        self.bundleDir = self.get_finalized_command(\'bdist_mac\').bundleDir\n        self.buildDir = self.get_finalized_command(\'build\').build_base\n\n        # Set the file name of the DMG to be built\n        self.dmgName = os.path.join(self.buildDir,\n                                    self.volume_label + \'.dmg\')\n\n        self.execute(self.buildDMG, ())\n\n\nclass bdist_mac(Command):\n    description = ""create a Mac application bundle""\n\n    user_options = [\n        (\'iconfile=\', None, \'Path to an icns icon file for the application.\'),\n        (\'qt-menu-nib=\', None, \'Location of qt_menu.nib folder for Qt \'\n            \'applications. Will be auto-detected by default.\'),\n        (\'bundle-name=\', None, \'File name for the bundle application \'\n            \'without the .app extension.\'),\n        (\'custom-info-plist=\', None, \'File to be used as the Info.plist in \'\n            \'the app bundle. A basic one will be generated by default.\'),\n        (\'include-frameworks=\', None, \'A comma separated list of Framework \'\n            \'directories to include in the app bundle.\'),\n        (\'codesign-identity=\', None, \'The identity of the key to be used to \'\n            \'sign the app bundle.\'),\n        (\'codesign-entitlements=\', None, \'The path to an entitlements file \'\n            \'to use for your application\\\'s code signature.\'),\n        (\'codesign-deep=\', None, \'Boolean for whether to codesign using the \' \\\n            \'--deep option.\'),\n        (\'codesign-resource-rules\', None, \'Plist file to be passed to \' \\\n            \'codesign\\\'s --resource-rules option.\'),\n    ]\n\n    def initialize_options(self):\n        self.iconfile = None\n        self.qt_menu_nib = False\n        self.bundle_name = self.distribution.get_fullname()\n        self.custom_info_plist = None\n        self.include_frameworks = []\n        self.codesign_identity = None\n        self.codesign_entitlements = None\n        self.codesign_deep = None\n        self.codesign_resource_rules = None\n\n    def finalize_options(self):\n        self.include_frameworks = normalize_to_list(self.include_frameworks)\n\n    def create_plist(self):\n        """"""Create the Contents/Info.plist file""""""\n        # Use custom plist if supplied, otherwise create a simple default.\n        if self.custom_info_plist:\n            contents = plistlib.readPlist(self.custom_info_plist)\n        else:\n            contents = {\n                \'CFBundleIconFile\': \'icon.icns\',\n                \'CFBundleDevelopmentRegion\': \'English\',\n            }\n\n        # Ensure CFBundleExecutable is set correctly\n        contents[\'CFBundleExecutable\'] = self.bundle_executable\n\n        plist = open(os.path.join(self.contentsDir, \'Info.plist\'), \'wb\')\n        plistlib.writePlist(contents, plist)\n        plist.close()\n\n    def setRelativeReferencePaths(self):\n        """""" For all files in Contents/MacOS, check if they are binaries\n            with references to other files in that dir. If so, make those\n            references relative. The appropriate commands are applied to all\n            files; they will just fail for files on which they do not apply.""""""\n        files = []\n        for root, dirs, dir_files in os.walk(self.binDir):\n            files.extend([os.path.join(root, f).replace(self.binDir + ""/"", """")\n                          for f in dir_files])\n        for fileName in files:\n\n            # install_name_tool can\'t handle zip files or directories\n            filePath = os.path.join(self.binDir, fileName)\n            if fileName.endswith(\'.zip\'):\n                continue\n\n            # ensure write permissions\n            mode = os.stat(filePath).st_mode\n            if not (mode & stat.S_IWUSR):\n                os.chmod(filePath, mode | stat.S_IWUSR)\n\n            # let the file itself know its place\n            subprocess.call((\'install_name_tool\', \'-id\',\n                             \'@executable_path/\' + fileName, filePath))\n\n            # find the references: call otool -L on the file\n            otool = subprocess.Popen((\'otool\', \'-L\', filePath),\n                                     stdout=subprocess.PIPE)\n            references = otool.stdout.readlines()[1:]\n\n            for reference in references:\n\n                # find the actual referenced file name\n                referencedFile = reference.decode().strip().split()[0]\n\n                if referencedFile.startswith(\'@executable_path\'):\n                    # the referencedFile is already a relative path (to the executable)\n                    continue\n\n                path, name = os.path.split(referencedFile)\n\n                #some referenced files have not previously been copied to the\n                #executable directory - the assumption is that you don\'t need\n                #to copy anything fro /usr or /System, just from folders like\n                #/opt this fix should probably be elsewhere though\n                if (name not in files and not path.startswith(\'/usr\') and not\n                        path.startswith(\'/System\')):\n                    print(referencedFile)\n                    self.copy_file(referencedFile,\n                                   os.path.join(self.binDir, name))\n                    files.append(name)\n\n                # see if we provide the referenced file;\n                # if so, change the reference\n                if name in files:\n                    newReference = \'@executable_path/\' + name\n                    subprocess.call((\'install_name_tool\', \'-change\',\n                                    referencedFile, newReference, filePath))\n\n    def find_qt_menu_nib(self):\n        """"""Returns a location of a qt_menu.nib folder, or None if this is not\n           a Qt application.""""""\n        if self.qt_menu_nib:\n            return self.qt_menu_nib\n        elif any(n.startswith(""PyQt4.QtCore"")\n                 for n in os.listdir(self.binDir)):\n            from PyQt4 import QtCore\n        elif any(n.startswith(""PySide.QtCore"")\n                 for n in os.listdir(self.binDir)):\n            from PySide import QtCore\n        else:\n            return None\n\n        libpath = str(QtCore.QLibraryInfo.location(\n                      QtCore.QLibraryInfo.LibrariesPath))\n        for subpath in [\'QtGui.framework/Resources/qt_menu.nib\',\n                        \'Resources/qt_menu.nib\']:\n            path = os.path.join(libpath, subpath)\n            if os.path.exists(path):\n                return path\n\n        # Last resort: fixed paths (macports)\n        for path in [\'/opt/local/Library/Frameworks/QtGui.framework/Versions/\'\n                     \'4/Resources/qt_menu.nib\']:\n            if os.path.exists(path):\n                return path\n\n        print (""Could not find qt_menu.nib"")\n        raise IOError(""Could not find qt_menu.nib"")\n\n    def prepare_qt_app(self):\n        """"""Add resource files for a Qt application. Should do nothing if the\n           application does not use QtCore.""""""\n        nib_locn = self.find_qt_menu_nib()\n        if nib_locn is None:\n            return\n\n        # Copy qt_menu.nib\n        self.copy_tree(nib_locn, os.path.join(self.resourcesDir,\n                       \'qt_menu.nib\'))\n\n        # qt.conf needs to exist, but needn\'t have any content\n        f = open(os.path.join(self.resourcesDir, \'qt.conf\'), ""w"")\n        f.close()\n\n    def run(self):\n        self.run_command(\'build\')\n        build = self.get_finalized_command(\'build\')\n\n        # Define the paths within the application bundle\n        self.bundleDir = os.path.join(build.build_base,\n                                      self.bundle_name + "".app"")\n        self.contentsDir = os.path.join(self.bundleDir, \'Contents\')\n        self.resourcesDir = os.path.join(self.contentsDir, \'Resources\')\n        self.binDir = os.path.join(self.contentsDir, \'MacOS\')\n        self.frameworksDir = os.path.join(self.contentsDir, \'Frameworks\')\n\n        #Find the executable name\n        executable = self.distribution.executables[0].targetName\n        _, self.bundle_executable = os.path.split(executable)\n\n        # Build the app directory structure\n        self.mkpath(self.resourcesDir)\n        self.mkpath(self.binDir)\n        self.mkpath(self.frameworksDir)\n\n        self.copy_tree(build.build_exe, self.binDir)\n\n        # Copy the icon\n        if self.iconfile:\n            self.copy_file(self.iconfile, os.path.join(self.resourcesDir,\n                                                       \'icon.icns\'))\n\n        # Copy in Frameworks\n        for framework in self.include_frameworks:\n            self.copy_tree(framework, self.frameworksDir + \'/\' +\n                           os.path.basename(framework))\n\n        # Create the Info.plist file\n        self.execute(self.create_plist, ())\n\n        # Make all references to libraries relative\n        self.execute(self.setRelativeReferencePaths, ())\n\n        # For a Qt application, run some tweaks\n        self.execute(self.prepare_qt_app, ())\n\n        # Sign the app bundle if a key is specified\n        if self.codesign_identity:\n            signargs = [\n                \'codesign\', \'-s\', self.codesign_identity\n            ]\n\n            if self.codesign_entitlements:\n                signargs.append(\'--entitlements\')\n                signargs.append(self.codesign_entitlements)\n\n            if self.codesign_deep:\n                signargs.insert(1, \'--deep\')\n\n            if self.codesign_resource_rules:\n                signargs.insert(1, \'--resource-rules=\' +\n                                self.codesign_resource_rules)\n\n            signargs.append(self.bundleDir)\n\n            if os.spawnvp(os.P_WAIT, \'codesign\', signargs) != 0:\n                raise OSError(\'Code signing of app bundle failed\')\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/main.py,0,"b'import optparse\nimport os\nimport shutil\nimport stat\nimport sys\n\nimport cx_Freeze\n\n__all__ = [""main""]\n\nUSAGE = \\\n""""""\n%prog [options] [SCRIPT]\n\nFreeze a Python script and all of its referenced modules to a base\nexecutable which can then be distributed without requiring a Python\ninstallation.""""""\n\nVERSION = \\\n""""""\n%%prog %s\nCopyright (c) 2007-2017 Anthony Tuininga. All rights reserved.\nCopyright (c) 2001-2006 Computronix Corporation. All rights reserved."""""" % \\\n        cx_Freeze.version\n\n\ndef ParseCommandLine():\n    parser = optparse.OptionParser(version = VERSION.strip(),\n            usage = USAGE.strip())\n    parser.add_option(""-O"",\n            action = ""count"",\n            default = 0,\n            dest = ""optimized"",\n            help = ""optimize generated bytecode as per PYTHONOPTIMIZE; ""\n                   ""use -OO in order to remove doc strings"")\n    parser.add_option(""-c"", ""--compress"",\n            action = ""store_true"",\n            dest = ""compress"",\n            help = ""compress byte code in zip files"")\n    parser.add_option(""-s"", ""--silent"",\n            action = ""store_true"",\n            dest = ""silent"",\n            help = ""suppress all output except warnings and errors"")\n    parser.add_option(""--base-name"",\n            dest = ""baseName"",\n            metavar = ""NAME"",\n            help = ""file on which to base the target file; if the name of the ""\n                   ""file is not an absolute file name, the subdirectory bases ""\n                   ""(rooted in the directory in which the freezer is found) ""\n                   ""will be searched for a file matching the name"")\n    parser.add_option(""--init-script"",\n            dest = ""initScript"",\n            metavar = ""NAME"",\n            help = ""script which will be executed upon startup; if the name ""\n                   ""of the file is not an absolute file name, the ""\n                   ""subdirectory initscripts (rooted in the directory in ""\n                   ""which the cx_Freeze package is found) will be searched ""\n                   ""for a file matching the name"")\n    parser.add_option(""--target-dir"", ""--install-dir"",\n            dest = ""targetDir"",\n            metavar = ""DIR"",\n            help = ""the directory in which to place the target file and ""\n                   ""any dependent files"")\n    parser.add_option(""--target-name"",\n            dest = ""targetName"",\n            metavar = ""NAME"",\n            help = ""the name of the file to create instead of the base name ""\n                   ""of the script and the extension of the base binary"")\n    parser.add_option(""--default-path"",\n            action = ""append"",\n            dest = ""defaultPath"",\n            metavar = ""DIRS"",\n            help = ""list of paths separated by the standard path separator ""\n                   ""for the platform which will be used to initialize ""\n                   ""sys.path prior to running the module finder"")\n    parser.add_option(""--include-path"",\n            action = ""append"",\n            dest = ""includePath"",\n            metavar = ""DIRS"",\n            help = ""list of paths separated by the standard path separator ""\n                   ""for the platform which will be used to modify sys.path ""\n                   ""prior to running the module finder"")\n    parser.add_option(""--replace-paths"",\n            dest = ""replacePaths"",\n            metavar = ""DIRECTIVES"",\n            help = ""replace all the paths in modules found in the given paths ""\n                   ""with the given replacement string; multiple values are ""\n                   ""separated by the standard path separator and each value ""\n                   ""is of the form path=replacement_string; path can be * ""\n                   ""which means all paths not already specified"")\n    parser.add_option(""--include-modules"",\n            dest = ""includeModules"",\n            metavar = ""NAMES"",\n            help = ""comma separated list of modules to include"")\n    parser.add_option(""--exclude-modules"",\n            dest = ""excludeModules"",\n            metavar = ""NAMES"",\n            help = ""comma separated list of modules to exclude"")\n    parser.add_option(""--ext-list-file"",\n            dest = ""extListFile"",\n            metavar = ""NAME"",\n            help = ""name of file in which to place the list of dependent files ""\n                   ""which were copied into the target directory"")\n    parser.add_option(""-z"", ""--zip-include"",\n            dest = ""zipIncludes"",\n            action = ""append"",\n            default = [],\n            metavar = ""SPEC"",\n            help = ""name of file to add to the zip file or a specification of ""\n                   ""the form name=arcname which will specify the archive name ""\n                   ""to use; multiple --zip-include arguments can be used"")\n    parser.add_option(""--icon"",\n            dest = ""icon"",\n            help = ""name of the icon file for the application"")\n    options, args = parser.parse_args()\n    if len(args) == 0:\n        options.script = None\n    elif len(args) == 1:\n        options.script, = args\n    else:\n        parser.error(""only one script can be specified"")\n    if not args and options.includeModules is None:\n        parser.error(""script or a list of modules must be specified"")\n    if not args and options.targetName is None:\n        parser.error(""script or a target name must be specified"")\n    if options.excludeModules:\n        options.excludeModules = options.excludeModules.split("","")\n    else:\n        options.excludeModules = []\n    if options.includeModules:\n        options.includeModules = options.includeModules.split("","")\n    else:\n        options.includeModules = []\n    replacePaths = []\n    if options.replacePaths:\n        for directive in options.replacePaths.split(os.pathsep):\n            fromPath, replacement = directive.split(""="")\n            replacePaths.append((fromPath, replacement))\n    options.replacePaths = replacePaths\n    if options.defaultPath is not None:\n        sys.path = [p for mp in options.defaultPath \\\n                for p in mp.split(os.pathsep)]\n    if options.includePath is not None:\n        paths = [p for mp in options.includePath for p in mp.split(os.pathsep)]\n        sys.path = paths + sys.path\n    if options.script is not None:\n        sys.path.insert(0, os.path.dirname(options.script))\n    zipIncludes = []\n    if options.zipIncludes:\n        for spec in options.zipIncludes:\n            if \'=\' in spec:\n                zipIncludes.append(spec.split(\'=\', 1))\n            else:\n                zipIncludes.append(spec)\n    options.zipIncludes = zipIncludes\n    return options\n\n\ndef main():\n    options = ParseCommandLine()\n    executables = [cx_Freeze.Executable(options.script,\n                                        initScript = options.initScript,\n                                        base = options.baseName,\n                                        icon = options.icon,\n                                        targetName = options.targetName)\n                  ]\n    freezer = cx_Freeze.Freezer(executables,\n            includes = options.includeModules,\n            excludes = options.excludeModules,\n            replacePaths = options.replacePaths,\n            compress = options.compress,\n            optimizeFlag = options.optimized,\n            path = None,\n            targetDir = options.targetDir,\n            zipIncludes = options.zipIncludes,\n            silent = options.silent)\n    freezer.Freeze()\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/setupwriter.py,0,"b'import os\nimport sys\nimport subprocess\n\ntry:\n    input = raw_input  # Python 2\nexcept NameError:\n    pass               # Python 3\n\nclass SetupWriter(object):\n    bases = {\n        ""C"" : ""Console"",\n        ""G"" : ""Win32GUI"",\n        ""S"" : ""Win32Service""\n    }\n\n    @property\n    def base(self):\n        return self.bases[self.baseCode]\n\n    @property\n    def defaultExecutableName(self):\n        name, ext = os.path.splitext(self.script)\n        return name\n\n    def __init__(self):\n        self.name = self.description = self.script = """"\n        self.executableName = self.defaultExecutableName\n        self.setupFileName = ""setup.py""\n        self.version = ""1.0""\n        self.baseCode = ""C""\n\n    def GetBooleanValue(self, label, default = False):\n        defaultResponse = default and ""y"" or ""n""\n        while True:\n            response = self.GetValue(label, defaultResponse,\n                    separator = ""? "").lower()\n            if response in (""y"", ""n"", ""yes"", ""no""):\n                break\n        return response in (""y"", ""yes"")\n\n    def GetValue(self, label, default = """", separator = "": ""):\n        if default:\n            label += "" [%s]"" % default\n        return input(label + separator).strip() or default\n\n    def PopulateFromCommandLine(self):\n        self.name = self.GetValue(""Project name"", self.name)\n        self.version = self.GetValue(""Version"", self.version)\n        self.description = self.GetValue(""Description"", self.description)\n        self.script = self.GetValue(""Python file to make executable from"",\n                self.script)\n        self.executableName = self.GetValue(""Executable file name"",\n                self.defaultExecutableName)\n        basesPrompt = ""(C)onsole application, (G)UI application, or (S)ervice""\n        while True:\n            self.baseCode = self.GetValue(basesPrompt, ""C"")\n            if self.baseCode in self.bases:\n                break\n        while True:\n            self.setupFileName = self.GetValue(""Save setup script to"",\n                    self.setupFileName)\n            if not os.path.exists(self.setupFileName):\n                break\n            if self.GetBooleanValue(""Overwrite %s"" % self.setupFileName):\n                break\n\n    def Write(self):\n        output = open(self.setupFileName, ""w"")\n        w = lambda s: output.write(s + ""\\n"")\n\n        w(""from cx_Freeze import setup, Executable"")\n        w("""")\n        \n        w(""# Dependencies are automatically detected, but it might need"")\n        w(""# fine tuning."")\n        w(""buildOptions = dict(packages = [], excludes = [])"")\n        w("""")\n        \n        if self.base.startswith(\'Win32\'):\n            w(""import sys"")\n            w(""base = %r if sys.platform==\'win32\' else None"" % self.base)\n        else:\n            w(""base = %r"" % self.base)\n        w("""")\n\n        w(""executables = ["")\n        if self.executableName != self.defaultExecutableName:\n            w(""    Executable(%r, base=base, targetName = %r)"" % \\\n                    (self.script, self.executableName))\n        else:\n            w(""    Executable(%r, base=base)"" % self.script)\n        w(""]"")\n        w("""")\n \n        w((""setup(name=%r,\\n""\n           ""      version = %r,\\n""\n           ""      description = %r,\\n""\n           ""      options = dict(build_exe = buildOptions),\\n""\n           ""      executables = executables)"") % \\\n           (self.name, self.version, self.description))\n\ndef main():\n    writer = SetupWriter()\n    writer.PopulateFromCommandLine()\n    writer.Write()\n    print("""")\n    print(""Setup script written to %s; run it as:"" % writer.setupFileName)\n    print(""    python %s build"" % writer.setupFileName)\n    if writer.GetBooleanValue(""Run this now""):\n        subprocess.call([""python"", writer.setupFileName, ""build""])\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/windist.py,0,"b'import distutils.command.bdist_msi\nimport distutils.errors\nimport distutils.util\nimport msilib\nimport os\n\n__all__ = [ ""bdist_msi"" ]\n\n# force the remove existing products action to happen first since Windows\n# installer appears to be braindead and doesn\'t handle files shared between\n# different ""products"" very well\nsequence = msilib.sequence.InstallExecuteSequence\nfor index, info in enumerate(sequence):\n    if info[0] == \'RemoveExistingProducts\':\n        sequence[index] = (info[0], info[1], 1450)\n\n\nclass bdist_msi(distutils.command.bdist_msi.bdist_msi):\n    user_options = distutils.command.bdist_msi.bdist_msi.user_options + [\n        (\'add-to-path=\', None, \'add target dir to PATH environment variable\'),\n        (\'upgrade-code=\', None, \'upgrade code to use\'),\n        (\'initial-target-dir=\', None, \'initial target directory\'),\n        (\'target-name=\', None, \'name of the file to create\'),\n        (\'directories=\', None, \'list of 3-tuples of directories to create\'),\n        (\'data=\', None, \'dictionary of data indexed by table name\'),\n        (\'product-code=\', None, \'product code to use\')\n    ]\n    x = y = 50\n    width = 370\n    height = 300\n    title = ""[ProductName] Setup""\n    modeless = 1\n    modal = 3\n\n    def add_config(self, fullname):\n        if self.add_to_path:\n            msilib.add_data(self.db, \'Environment\',\n                    [(""E_PATH"", ""=-*Path"", r""[~];[TARGETDIR]"", ""TARGETDIR"")])\n        if self.directories:\n            msilib.add_data(self.db, ""Directory"", self.directories)\n        msilib.add_data(self.db, \'CustomAction\',\n                [(""A_SET_TARGET_DIR"", 256 + 51, ""TARGETDIR"",\n                        self.initial_target_dir)])\n        msilib.add_data(self.db, \'InstallExecuteSequence\',\n                [(""A_SET_TARGET_DIR"", \'TARGETDIR=""""\', 401)])\n        msilib.add_data(self.db, \'InstallUISequence\',\n                [(""PrepareDlg"", None, 140),\n                 (""A_SET_TARGET_DIR"", \'TARGETDIR=""""\', 401),\n                 (""SelectDirectoryDlg"", ""not Installed"", 1230),\n                 (""MaintenanceTypeDlg"",\n                        ""Installed and not Resume and not Preselected"", 1250),\n                 (""ProgressDlg"", None, 1280)\n                ])\n        for index, executable in enumerate(self.distribution.executables):\n            if executable.shortcutName is not None \\\n                    and executable.shortcutDir is not None:\n                baseName = os.path.basename(executable.targetName)\n                msilib.add_data(self.db, ""Shortcut"",\n                        [(""S_APP_%s"" % index, executable.shortcutDir,\n                                executable.shortcutName, ""TARGETDIR"",\n                                ""[TARGETDIR]%s"" % baseName, None, None, None,\n                                None, None, None, None)])\n        for tableName, data in self.data.items():\n            msilib.add_data(self.db, tableName, data)\n\n    def add_cancel_dialog(self):\n        dialog = msilib.Dialog(self.db, ""CancelDlg"", 50, 10, 260, 85, 3,\n                self.title, ""No"", ""No"", ""No"")\n        dialog.text(""Text"", 48, 15, 194, 30, 3,\n                ""Are you sure you want to cancel [ProductName] installation?"")\n        button = dialog.pushbutton(""Yes"", 72, 57, 56, 17, 3, ""Yes"", ""No"")\n        button.event(""EndDialog"", ""Exit"")\n        button = dialog.pushbutton(""No"", 132, 57, 56, 17, 3, ""No"", ""Yes"")\n        button.event(""EndDialog"", ""Return"")\n\n    def add_error_dialog(self):\n        dialog = msilib.Dialog(self.db, ""ErrorDlg"", 50, 10, 330, 101, 65543,\n                self.title, ""ErrorText"", None, None)\n        dialog.text(""ErrorText"", 50, 9, 280, 48, 3, """")\n        for text, x in [(""No"", 120), (""Yes"", 240), (""Abort"", 0),\n                (""Cancel"", 42), (""Ignore"", 81), (""Ok"", 159), (""Retry"", 198)]:\n            button = dialog.pushbutton(text[0], x, 72, 81, 21, 3, text, None)\n            button.event(""EndDialog"", ""Error%s"" % text)\n\n    def add_exit_dialog(self):\n        dialog = distutils.command.bdist_msi.PyDialog(self.db, ""ExitDialog"",\n                self.x, self.y, self.width, self.height, self.modal,\n                self.title, ""Finish"", ""Finish"", ""Finish"")\n        dialog.title(""Completing the [ProductName] installer"")\n        dialog.back(""< Back"", ""Finish"", active = False)\n        dialog.cancel(""Cancel"", ""Back"", active = False)\n        dialog.text(""Description"", 15, 235, 320, 20, 0x30003,\n                ""Click the Finish button to exit the installer."")\n        button = dialog.next(""Finish"", ""Cancel"", name = ""Finish"")\n        button.event(""EndDialog"", ""Return"")\n\n    def add_fatal_error_dialog(self):\n        dialog = distutils.command.bdist_msi.PyDialog(self.db, ""FatalError"",\n                self.x, self.y, self.width, self.height, self.modal,\n                self.title, ""Finish"", ""Finish"", ""Finish"")\n        dialog.title(""[ProductName] installer ended prematurely"")\n        dialog.back(""< Back"", ""Finish"", active = False)\n        dialog.cancel(""Cancel"", ""Back"", active = False)\n        dialog.text(""Description1"", 15, 70, 320, 80, 0x30003,\n                ""[ProductName] setup ended prematurely because of an error. ""\n                ""Your system has not been modified. To install this program ""\n                ""at a later time, please run the installation again."")\n        dialog.text(""Description2"", 15, 155, 320, 20, 0x30003,\n                ""Click the Finish button to exit the installer."")\n        button = dialog.next(""Finish"", ""Cancel"", name = ""Finish"")\n        button.event(""EndDialog"", ""Exit"")\n\n    def add_files(self):\n        db = self.db\n        cab = msilib.CAB(""distfiles"")\n        f = msilib.Feature(db, ""default"", ""Default Feature"", ""Everything"", 1,\n                directory=""TARGETDIR"")\n        f.set_current()\n        rootdir = os.path.abspath(self.bdist_dir)\n        root = msilib.Directory(db, cab, None, rootdir, ""TARGETDIR"",\n                ""SourceDir"")\n        db.Commit()\n        todo = [root]\n        while todo:\n            dir = todo.pop()\n            for file in os.listdir(dir.absolute):\n                if os.path.isdir(os.path.join(dir.absolute, file)):\n                    newDir = msilib.Directory(db, cab, dir, file, file,\n                            ""%s|%s"" % (dir.make_short(file), file))\n                    todo.append(newDir)\n                else:\n                    dir.add_file(file)\n        cab.commit(db)\n\n    def add_files_in_use_dialog(self):\n        dialog = distutils.command.bdist_msi.PyDialog(self.db, ""FilesInUse"",\n                self.x, self.y, self.width, self.height, 19, self.title,\n                ""Retry"", ""Retry"", ""Retry"", bitmap = False)\n        dialog.text(""Title"", 15, 6, 200, 15, 0x30003,\n                r""{\\DlgFontBold8}Files in Use"")\n        dialog.text(""Description"", 20, 23, 280, 20, 0x30003,\n                ""Some files that need to be updated are currently in use."")\n        dialog.text(""Text"", 20, 55, 330, 50, 3,\n                ""The following applications are using files that need to be ""\n                ""updated by this setup. Close these applications and then ""\n                ""click Retry to continue the installation or Cancel to exit ""\n                ""it."")\n        dialog.control(""List"", ""ListBox"", 20, 107, 330, 130, 7,\n                ""FileInUseProcess"", None, None, None)\n        button = dialog.back(""Exit"", ""Ignore"", name = ""Exit"")\n        button.event(""EndDialog"", ""Exit"")\n        button = dialog.next(""Ignore"", ""Retry"", name = ""Ignore"")\n        button.event(""EndDialog"", ""Ignore"")\n        button = dialog.cancel(""Retry"", ""Exit"", name = ""Retry"")\n        button.event(""EndDialog"", ""Retry"")\n\n    def add_maintenance_type_dialog(self):\n        dialog = distutils.command.bdist_msi.PyDialog(self.db,\n                ""MaintenanceTypeDlg"", self.x, self.y, self.width, self.height,\n                self.modal, self.title, ""Next"", ""Next"", ""Cancel"")\n        dialog.title(""Welcome to the [ProductName] Setup Wizard"")\n        dialog.text(""BodyText"", 15, 63, 330, 42, 3,\n                ""Select whether you want to repair or remove [ProductName]."")\n        group = dialog.radiogroup(""RepairRadioGroup"", 15, 108, 330, 60, 3,\n                ""MaintenanceForm_Action"", """", ""Next"")\n        group.add(""Repair"", 0, 18, 300, 17, ""&Repair [ProductName]"")\n        group.add(""Remove"", 0, 36, 300, 17, ""Re&move [ProductName]"")\n        dialog.back(""< Back"", None, active = False)\n        button = dialog.next(""Finish"", ""Cancel"")\n        button.event(""[REINSTALL]"", ""ALL"",\n                \'MaintenanceForm_Action=""Repair""\', 5)\n        button.event(""[Progress1]"", ""Repairing"",\n                \'MaintenanceForm_Action=""Repair""\', 6)\n        button.event(""[Progress2]"", ""repairs"",\n                \'MaintenanceForm_Action=""Repair""\', 7)\n        button.event(""Reinstall"", ""ALL"",\n                \'MaintenanceForm_Action=""Repair""\', 8)\n        button.event(""[REMOVE]"", ""ALL"",\n                \'MaintenanceForm_Action=""Remove""\', 11)\n        button.event(""[Progress1]"", ""Removing"",\n                \'MaintenanceForm_Action=""Remove""\', 12)\n        button.event(""[Progress2]"", ""removes"",\n                \'MaintenanceForm_Action=""Remove""\', 13)\n        button.event(""Remove"", ""ALL"",\n                \'MaintenanceForm_Action=""Remove""\', 14)\n        button.event(""EndDialog"", ""Return"",\n                \'MaintenanceForm_Action<>""Change""\', 20)\n        button = dialog.cancel(""Cancel"", ""RepairRadioGroup"")\n        button.event(""SpawnDialog"", ""CancelDlg"")\n\n    def add_prepare_dialog(self):\n        dialog = distutils.command.bdist_msi.PyDialog(self.db, ""PrepareDlg"",\n                self.x, self.y, self.width, self.height, self.modeless,\n                self.title, ""Cancel"", ""Cancel"", ""Cancel"")\n        dialog.text(""Description"", 15, 70, 320, 40, 0x30003,\n                ""Please wait while the installer prepares to guide you through ""\n                ""the installation."")\n        dialog.title(""Welcome to the [ProductName] installer"")\n        text = dialog.text(""ActionText"", 15, 110, 320, 20, 0x30003,\n                ""Pondering..."")\n        text.mapping(""ActionText"", ""Text"")\n        text = dialog.text(""ActionData"", 15, 135, 320, 30, 0x30003, None)\n        text.mapping(""ActionData"", ""Text"")\n        dialog.back(""Back"", None, active = False)\n        dialog.next(""Next"", None, active = False)\n        button = dialog.cancel(""Cancel"", None)\n        button.event(""SpawnDialog"", ""CancelDlg"")\n\n    def add_progress_dialog(self):\n        dialog = distutils.command.bdist_msi.PyDialog(self.db, ""ProgressDlg"",\n                self.x, self.y, self.width, self.height, self.modeless,\n                self.title, ""Cancel"", ""Cancel"", ""Cancel"", bitmap = False)\n        dialog.text(""Title"", 20, 15, 200, 15, 0x30003,\n                r""{\\DlgFontBold8}[Progress1] [ProductName]"")\n        dialog.text(""Text"", 35, 65, 300, 30, 3,\n                ""Please wait while the installer [Progress2] [ProductName]."")\n        dialog.text(""StatusLabel"", 35, 100 ,35, 20, 3, ""Status:"")\n        text = dialog.text(""ActionText"", 70, 100, self.width - 70, 20, 3,\n                ""Pondering..."")\n        text.mapping(""ActionText"", ""Text"")\n        control = dialog.control(""ProgressBar"", ""ProgressBar"", 35, 120, 300,\n                10, 65537, None, ""Progress done"", None, None)\n        control.mapping(""SetProgress"", ""Progress"")\n        dialog.back(""< Back"", ""Next"", active = False)\n        dialog.next(""Next >"", ""Cancel"", active = False)\n        button = dialog.cancel(""Cancel"", ""Back"")\n        button.event(""SpawnDialog"", ""CancelDlg"")\n\n    def add_properties(self):\n        metadata = self.distribution.metadata\n        props = [\n                (\'DistVersion\', metadata.get_version()),\n                (\'DefaultUIFont\', \'DlgFont8\'),\n                (\'ErrorDialog\', \'ErrorDlg\'),\n                (\'Progress1\', \'Install\'),\n                (\'Progress2\', \'installs\'),\n                (\'MaintenanceForm_Action\', \'Repair\'),\n                (\'ALLUSERS\', \'1\')\n        ]\n        email = metadata.author_email or metadata.maintainer_email\n        if email:\n            props.append((""ARPCONTACT"", email))\n        if metadata.url:\n            props.append((""ARPURLINFOABOUT"", metadata.url))\n        if self.upgrade_code is not None:\n            props.append((""UpgradeCode"", self.upgrade_code))\n        msilib.add_data(self.db, \'Property\', props)\n\n    def add_select_directory_dialog(self):\n        dialog = distutils.command.bdist_msi.PyDialog(self.db,\n                ""SelectDirectoryDlg"", self.x, self.y, self.width, self.height,\n                self.modal, self.title, ""Next"", ""Next"", ""Cancel"")\n        dialog.title(""Select destination directory"")\n        dialog.back(""< Back"", None, active = False)\n        button = dialog.next(""Next >"", ""Cancel"")\n        button.event(""SetTargetPath"", ""TARGETDIR"", ordering = 1)\n        button.event(""SpawnWaitDialog"", ""WaitForCostingDlg"", ordering = 2)\n        button.event(""EndDialog"", ""Return"", ordering = 3)\n        button = dialog.cancel(""Cancel"", ""DirectoryCombo"")\n        button.event(""SpawnDialog"", ""CancelDlg"")\n        dialog.control(""DirectoryCombo"", ""DirectoryCombo"", 15, 70, 272, 80,\n                393219, ""TARGETDIR"", None, ""DirectoryList"", None)\n        dialog.control(""DirectoryList"", ""DirectoryList"", 15, 90, 308, 136, 3,\n                ""TARGETDIR"", None, ""PathEdit"", None)\n        dialog.control(""PathEdit"", ""PathEdit"", 15, 230, 306, 16, 3,\n                ""TARGETDIR"", None, ""Next"", None)\n        button = dialog.pushbutton(""Up"", 306, 70, 18, 18, 3, ""Up"", None)\n        button.event(""DirectoryListUp"", ""0"")\n        button = dialog.pushbutton(""NewDir"", 324, 70, 30, 18, 3, ""New"", None)\n        button.event(""DirectoryListNew"", ""0"")\n\n    def add_text_styles(self):\n        msilib.add_data(self.db, \'TextStyle\',\n                [(""DlgFont8"", ""Tahoma"", 9, None, 0),\n                 (""DlgFontBold8"", ""Tahoma"", 8, None, 1),\n                 (""VerdanaBold10"", ""Verdana"", 10, None, 1),\n                 (""VerdanaRed9"", ""Verdana"", 9, 255, 0)\n                ])\n\n    def add_ui(self):\n        self.add_text_styles()\n        self.add_error_dialog()\n        self.add_fatal_error_dialog()\n        self.add_cancel_dialog()\n        self.add_exit_dialog()\n        self.add_user_exit_dialog()\n        self.add_files_in_use_dialog()\n        self.add_wait_for_costing_dialog()\n        self.add_prepare_dialog()\n        self.add_select_directory_dialog()\n        self.add_progress_dialog()\n        self.add_maintenance_type_dialog()\n\n    def add_upgrade_config(self, sversion):\n        if self.upgrade_code is not None:\n            msilib.add_data(self.db, \'Upgrade\',\n                    [(self.upgrade_code, None, sversion, None, 513, None,\n                            ""REMOVEOLDVERSION""),\n                     (self.upgrade_code, sversion, None, None, 257, None,\n                            ""REMOVENEWVERSION"")\n                    ])\n\n    def add_user_exit_dialog(self):\n        dialog = distutils.command.bdist_msi.PyDialog(self.db, ""UserExit"",\n                self.x, self.y, self.width, self.height, self.modal,\n                self.title, ""Finish"", ""Finish"", ""Finish"")\n        dialog.title(""[ProductName] installer was interrupted"")\n        dialog.back(""< Back"", ""Finish"", active = False)\n        dialog.cancel(""Cancel"", ""Back"", active = False)\n        dialog.text(""Description1"", 15, 70, 320, 80, 0x30003,\n                ""[ProductName] setup was interrupted. Your system has not ""\n                ""been modified. To install this program at a later time, ""\n                ""please run the installation again."")\n        dialog.text(""Description2"", 15, 155, 320, 20, 0x30003,\n                ""Click the Finish button to exit the installer."")\n        button = dialog.next(""Finish"", ""Cancel"", name = ""Finish"")\n        button.event(""EndDialog"", ""Exit"")\n\n    def add_wait_for_costing_dialog(self):\n        dialog = msilib.Dialog(self.db, ""WaitForCostingDlg"", 50, 10, 260, 85,\n                self.modal, self.title, ""Return"", ""Return"", ""Return"")\n        dialog.text(""Text"", 48, 15, 194, 30, 3,\n                ""Please wait while the installer finishes determining your ""\n                ""disk space requirements."")\n        button = dialog.pushbutton(""Return"", 102, 57, 56, 17, 3, ""Return"",\n                None)\n        button.event(""EndDialog"", ""Exit"")\n\n    def finalize_options(self):\n        distutils.command.bdist_msi.bdist_msi.finalize_options(self)\n        name = self.distribution.get_name()\n        fullname = self.distribution.get_fullname()\n        if self.initial_target_dir is None:\n            if distutils.util.get_platform() == ""win-amd64"":\n                programFilesFolder = ""ProgramFiles64Folder""\n            else:\n                programFilesFolder = ""ProgramFilesFolder""\n            self.initial_target_dir = r""[%s]\\%s"" % (programFilesFolder, name)\n        if self.add_to_path is None:\n            self.add_to_path = False\n        if self.target_name is None:\n            self.target_name = fullname\n        if not self.target_name.lower().endswith("".msi""):\n            platform = distutils.util.get_platform().replace(""win-"", """")\n            self.target_name = ""%s-%s.msi"" % (self.target_name, platform)\n        if not os.path.isabs(self.target_name):\n            self.target_name = os.path.join(self.dist_dir, self.target_name)\n        if self.directories is None:\n            self.directories = []\n        if self.data is None:\n            self.data = {}\n\n    def initialize_options(self):\n        distutils.command.bdist_msi.bdist_msi.initialize_options(self)\n        self.upgrade_code = None\n        self.product_code = None\n        self.add_to_path = None\n        self.initial_target_dir = None\n        self.target_name = None\n        self.directories = None\n        self.data = None\n\n    def run(self):\n        if not self.skip_build:\n            self.run_command(\'build\')\n        install = self.reinitialize_command(\'install\', reinit_subcommands = 1)\n        install.prefix = self.bdist_dir\n        install.skip_build = self.skip_build\n        install.warn_dir = 0\n        distutils.log.info(""installing to %s"", self.bdist_dir)\n        install.ensure_finalized()\n        install.run()\n        self.mkpath(self.dist_dir)\n        fullname = self.distribution.get_fullname()\n        if os.path.exists(self.target_name):\n            os.unlink(self.target_name)\n        metadata = self.distribution.metadata\n        author = metadata.author or metadata.maintainer or ""UNKNOWN""\n        version = metadata.get_version()\n        sversion = ""%d.%d.%d"" % \\\n                distutils.version.StrictVersion(version).version\n        if self.product_code is None:\n            self.product_code = msilib.gen_uuid()\n        self.db = msilib.init_database(self.target_name, msilib.schema,\n                self.distribution.metadata.name, self.product_code, sversion,\n                author)\n        msilib.add_tables(self.db, msilib.sequence)\n        self.add_properties()\n        self.add_config(fullname)\n        self.add_upgrade_config(sversion)\n        self.add_ui()\n        self.add_files()\n        self.db.Commit()\n        self.distribution.dist_files.append((\'bdist_msi\', sversion or \'any\', self.target_name))\n        \n        if not self.keep_temp:\n            distutils.dir_util.remove_tree(self.bdist_dir,\n                    dry_run = self.dry_run)\n'"
core/native/vendor/cx_Freeze-5.0.1/test/test_finder.py,0,"b'try:\n    from unittest import mock  # Python >= 3.3\nexcept ImportError:\n    import mock\n\nimport os.path\nimport sys\n\ntest_dir = os.path.dirname(__file__)\n\nfrom cx_Freeze.finder import ModuleFinder\n\nany3 = (mock.ANY,)*3\n\ndef test_ScanCode():\n    mf = ModuleFinder()\n    with mock.patch.object(mf, \'_ImportModule\') as _ImportModule_mock:\n        _ImportModule_mock.return_value = None\n        mf.IncludeFile(os.path.join(test_dir, \'imports_sample.py\'))\n        _ImportModule_mock.assert_has_calls([mock.call(\'moda\', *any3),\n                                             mock.call(\'modb\', *any3),\n                                             mock.call(\'\', *any3),\n                                             mock.call(\'modd\', *any3),\n                                             mock.call(\'mode\', *any3),\n                                             mock.call(\'modf\', *any3),\n                                             mock.call(\'modg.submod\', *any3),\n                                             mock.call(\'modh\', *any3),\n                                            ])\n\ndef test_not_import_invalid_module_name():\n    """"""testpkg1 contains not.importable.py, which shouldn\'t be included.""""""\n    mf = ModuleFinder()\n    mf.path.insert(0, os.path.join(test_dir, \'samples\'))\n\n    try:\n        module = mf.IncludePackage(\'testpkg1\')  # Threw ImportError before the bug was fixed\n    except ImportError:\n        assert False, ""submodules with names containing \'.\' should not be included""\n\n    assert \'invalid-identifier\' in module.globalNames, \\\n            \'submodules whose names contain invalid identifiers should still be imported\'\n\ndef test_invalid_syntax():\n    """"""Invalid syntax (e.g. Py2 or Py3 only code) should not break freezing.""""""\n    mf = ModuleFinder(path=[os.path.join(test_dir, \'samples\')]+sys.path)\n    try:\n        mf.IncludeModule(\'invalid_syntax\')  # Threw SyntaxError before the bug was fixed\n    except ImportError:\n        pass\n    else:\n        assert False, ""Expected ImportError, but no error was raised""\n'"
core/native/vendor/cx_Freeze-5.0.1/test/test_misc.py,0,"b'from os.path import join as pjoin\nimport sys\n\nfrom nose.tools import assert_raises\n\nfrom cx_Freeze.freezer import process_path_specs, ConfigError\n\nrootdir = ""C:\\\\"" if sys.platform==\'win32\' else \'/\'\n\ndef test_process_path_specs():\n    inp = [pjoin(rootdir, \'foo\', \'bar\'),\n           (pjoin(rootdir, \'foo\', \'qux\'), pjoin(\'baz\', \'xyz\'))]\n    outp = process_path_specs(inp)\n    assert outp == [(pjoin(rootdir, \'foo\', \'bar\'), \'bar\'),\n                    (pjoin(rootdir, \'foo\', \'qux\'), pjoin(\'baz\', \'xyz\'))]\n\ndef test_process_path_specs_bad():\n    with assert_raises(ConfigError):\n        process_path_specs([(pjoin(rootdir, \'foo\'), pjoin(rootdir, \'bar\'))])\n    \n    with assert_raises(ConfigError):\n        process_path_specs([(\'a\', \'b\', \'c\')])'"
core/native/vendor/cx_Freeze-5.0.1/test/test_zip_packages.py,0,"b""import imp\nimport os\nimport shutil\nimport tempfile\nimport zipfile\n\ntest_dir = os.path.dirname(__file__)\nsamples_dir = os.path.join(test_dir, 'samples')\n\nfrom cx_Freeze.finder import ZipModulesCache, ModuleFinder, Module\n\ndef clean_pyc_files():    \n    for dirpath, dirnames, filenames in os.walk(samples_dir):\n        for filename in filenames:\n            if filename.endswith(('.pyc', '.pyo')):\n                os.unlink(os.path.join(dirpath, filename))\n        if '__pycache__' in dirnames:\n            dirnames.remove('__pycache__')\n            shutil.rmtree(os.path.join(dirpath, '__pycache__'))\n\ndef prepare_zip_file():\n    clean_pyc_files()\n    tmpd = tempfile.mkdtemp()\n    egg = os.path.join(tmpd, 'testpkg1.egg')\n    eggzip = zipfile.PyZipFile(egg, 'w', zipfile.ZIP_DEFLATED)\n    eggzip.writepy(os.path.join(samples_dir, 'testmod1.py'))\n    eggzip.writepy(os.path.join(samples_dir, 'testpkg1'))\n    eggzip.close()\n    return egg\n\ndef test_ZipModulesCache():\n    egg = prepare_zip_file()\n    try:\n        zmc = ZipModulesCache()\n        \n        mod = zmc.find(egg, 'testmod1')\n        assert mod is not None\n        assert mod[2][2] == imp.PY_COMPILED\n        \n        pkg = zmc.find(egg, 'testpkg1')\n        assert pkg is not None\n        assert pkg[2][2] == imp.PKG_DIRECTORY\n        \n        # This needs to be called after zmc.find(egg, *)\n        submod = zmc.find(os.path.join(egg, 'testpkg1'), 'submod')\n        assert submod is not None\n        assert submod[2][2] == imp.PY_COMPILED\n    finally:\n        os.unlink(egg)\n\ndef test_FindModule_from_zip():\n    egg = prepare_zip_file()\n    try:\n        mf = ModuleFinder()\n        mf.path = [egg]\n        mod = mf._InternalImportModule('testpkg1.submod', deferredImports=[])\n        assert isinstance(mod, Module)\n    finally:\n        os.unlink(egg)"""
core/python/src/nao/compiler/__init__.py,0,b''
core/python/src/nao/compiler/compiler.py,5,"b'import sys\n\nfrom os import path\n\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import meta_graph\n\nfrom nao.compiler.asset import compiler as asset_compiler\nfrom nao.compiler.nao import compiler as nao_compiler\nfrom nao.compiler.py import compiler as py_compiler\nfrom nao.compiler.metagraph_pbtxt import compiler as metagraph_pbtxt_compiler\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\nclass Workspace:\n  def __init__(self, src_root, pkg_root, asset_root):\n    self._src_root = src_root\n    self._pkg_root = pkg_root\n    self._asset_root = asset_root\n    self.clear()\n\n  def clear(self):\n    self._source_cache = {}\n\n  def put_src(self, filename, source):\n    self._source_cache[filename] = source\n\n  def read_src(self, filename):\n    if filename in self._source_cache:\n      return self._source_cache[filename]\n\n    filepath = path.join(self._src_root, filename)\n    if not path.exists(filepath):\n      return None\n\n    with open(filepath) as f:\n      return f.read()\n\n  def find_asset_path(self, name):\n    return path.join(self._asset_root, name)\n\n  def find_pkg_path(self, filename):\n    filepath = path.join(self._pkg_root, filename)\n    if not path.exists(filepath):\n      return None\n    return filepath\n\nclass Compiler:\n  def __init__(self, src_root, pkg_root, asset_root):\n    self._g = tf.Graph()\n    self._device = None\n    self._workspace = Workspace(src_root, pkg_root, asset_root)\n    self._import_cache = {}\n    self._import_cache_tags = {}\n    self._compilers = [\n      asset_compiler,\n      nao_compiler,\n      py_compiler,\n      metagraph_pbtxt_compiler,\n    ]\n    self.clear()\n\n  def clear(self):\n    self._workspace.clear()\n\n  def put_source(self, filename, source):\n    self._workspace.put_src(filename, source)\n\n  def asset_path(self, name):\n    return self._workspace.find_asset_path(name)\n\n  def set_default_device(self, device):\n    self._device = device\n\n  def new_session(self):\n    config = tf.ConfigProto(\n      operation_timeout_in_ms=20000,\n      # allow_soft_placement=True,\n      # log_device_placement=True,\n    )\n\n    # Don\'t allocate everything we think we\'ll need ahead of time.\n    config.gpu_options.allow_growth = True\n\n    return tf.Session(\n      config=config,\n      graph=self._g,\n    )\n\n  def meta_graph_def(self):\n    meta_graph_def = None\n    with self._g.as_default():\n      with tf.device(self._device):\n        for compiler in self._compilers:\n          if hasattr(compiler, ""finish""):\n            compiler.finish()\n\n      meta_graph_def, _ = meta_graph.export_scoped_meta_graph()\n    return meta_graph_def\n\n  def resolve_import_path(self, import_path, tags=None, reimport=False):\n    pkg = None\n    if import_path in self._import_cache:\n      pkg = self._import_cache[import_path]\n      prev_tags = self._import_cache_tags[import_path]\n      if tags is not None:\n        if prev_tags is None or sorted(prev_tags.items()) != sorted(tags.items()):\n          raise Exception(""A subsequent resolution of %s used different tags. Before: %s vs Now %s"" % (import_path, prev_tags, tags))\n      if not reimport:\n        return pkg\n\n    needed_imports, compile_fn = self._resolve_import_path(import_path, tags)\n\n    imports = {}\n    for imported_path, imported_tags in needed_imports:\n      imports[imported_path] = self.resolve_import_path(imported_path, imported_tags)\n\n    with self._g.as_default():\n      with tf.device(self._device):\n        pkg = compile_fn(imports, pkg)\n\n    self._import_cache[import_path] = pkg\n    self._import_cache_tags[import_path] = tags\n\n    return pkg\n\n  def _resolve_import_path(self, import_path, tags):\n    for compiler in self._compilers:\n      resolved = compiler.make_compile_fn(self._workspace, import_path, tags or {})\n      if resolved:\n        return resolved\n\n    raise Exception(""No such import path: "" + import_path)\n'"
core/python/src/nao/compiler/primitive_function.py,0,"b'import inspect\nfrom functools import partial\n\nclass PrimitiveFunction:\n  def __init__(self, fn, prepend_with_context=False):\n    self._prepend_with_context = prepend_with_context\n    self._fn = fn\n    sig = inspect.signature(fn)\n    self._params = sig.parameters\n\n    self._name_is_kwdarg = False\n    self._name_is_posarg = False\n    if \'name\' in self._params:\n      name_param = self._params[\'name\']\n      kind = name_param.kind\n      if kind == inspect.Parameter.KEYWORD_ONLY:\n        self._name_is_kwdarg = True\n      if kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n        self._name_is_kwdarg = True\n        self._name_is_posarg = True\n      else:\n        self._name_is_posarg = True\n\n  def _name(self):\n    return self._fn.__name__\n\n  def apply_attrs(self, visitor, attrs):\n    return PrimitiveFunction(partial(self._fn, **attrs))\n\n  def apply_kw(self, visitor, ctx, name, attrs, kwargs):\n    if kwargs == None:\n      kwargs = {}\n\n    if name != None:\n      kwargs = dict(kwargs)\n      kwargs[\'name\'] = name\n\n    args = []\n    if self._prepend_with_context:\n      args.append(ctx)\n\n    try:\n      return self._fn(*args, **kwargs)\n    except:\n      raise Exception(""Tried to call %s with args %s and kwargs %s""  % (self._fn, args, kwargs))\n\n  def apply(self, visitor, ctx, name, attrs, args):\n    if attrs == None:\n      attrs = {}\n\n    name_is_kwdarg = self._name_is_kwdarg\n\n    if self._name_is_posarg:\n      new_args = []\n      arg_ix = 0\n      nargs = len(args)\n      for param_name, param in self._params.items():\n        if param_name == \'name\':\n          new_args.append(name)\n          name_is_kwdarg = False\n        else:\n          if arg_ix >= nargs:\n            break\n          new_args.append(args[arg_ix])\n          arg_ix += 1\n      args = new_args\n\n    if name_is_kwdarg:\n      attrs = dict(attrs)\n      attrs[\'name\'] = name\n\n    if self._prepend_with_context:\n      args = [ctx, *args]\n\n    # eprint(""Applying"", ""call %s with name %s args %s and kwargs %s""  % (self._fn, name, args, attrs))\n\n    return ctx.call(self._fn, args, attrs)\n'"
core/python/src/nao/compiler/python_package.py,0,"b'from nao.compiler.primitive_function import PrimitiveFunction\n\nclass PythonPackage:\n  def __init__(self, mod, prepend_with_context=False):\n    self._mod = mod\n    self._prepend_with_context = prepend_with_context\n\n  def apply(self, visitor, ctx, name, attrs, args):\n    # eprint(""applying %s to args %s"" % (self, args))\n    n, *_ = args\n    if n.startswith(\'__\'):\n      raise Exception(""Tried to use non-exported namespace entry named: %s"" % n)\n\n    val = getattr(self._mod, n)\n    if callable(val):\n      return PrimitiveFunction(val, self._prepend_with_context)\n\n    return val\n'"
core/python/src/nao/compiler/retvalbag.py,1,"b'import tensorflow as tf\n\ndef _graph_for(v):\n  if isinstance(v, (tf.Operation, tf.Tensor, tf.Variable)):\n    return v.graph\n  return None\n\ndef unwrap_bag(v):\n  if type(v) == RetvalBag:\n    v = v.get(None)\n  return v\n\n\nclass RetvalBag:\n  def __init__(self, a_dict, fn=None):\n    self._d = {}\n    self.graph = None\n\n    for k, v in a_dict.items():\n      if type(v) == RetvalBag:\n        v = v.get(None)\n      if fn:\n        v = fn(v)\n      if self.graph is None:\n        self.graph = _graph_for(v)\n      self._d[k] = v\n\n    for k, v in self._d.items():\n      g = _graph_for(v)\n      if g is not None and self.graph != g:\n        graphs = [(k, v, _graph_for(v)) for k, v in self._d.items()]\n        raise Exception(""RetvalBag can only contain elements from a single graph. Got elements from multiple graphs: %s"" % graphs)\n\n  def get(self, key):\n    if key == None:\n      key = self._default_key()\n    return self._d[key]\n\n  def values(self):\n    return self._d.values()\n\n  def wrap(self, fn):\n    return RetvalBag(self._d, fn=fn)\n\n  def len(self):\n    return len(self._d)\n\n  def __str__(self):\n    return ""RetvalBag(%s)"" % self._d\n\n  def _default_key(self):\n    l = len(self._d)\n    if l == 0:\n      raise Exception(""Can\'t get default retval for an empty RetvalBag"")\n    if l > 1:\n      raise Exception(""Can\'t get default retval a RetvalBag with more than one entry: %s"" % self._d)\n    return list(self._d.keys())[0]\n'"
core/python/src/nao/run/__init__.py,0,b''
core/python/src/nao/run/graph_execution.py,12,"b'import json\n\nfrom nao.structure import graph_query\nfrom nao.structure import graph_xform\nfrom nao.structure import graph_ffi\n\nfrom nao.run import graph_summary\n\nfrom tensorflow.python.framework import meta_graph\n\nimport tensorflow as tf\nimport sys\n\nfrom tensorflow.python.client import timeline\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\ndef create_session():\n  config = tf.ConfigProto(\n    # log_device_placement=True,\n    operation_timeout_in_ms=600000,\n    inter_op_parallelism_threads=2,\n  )\n\n  return tf.Session(config=config)\n\ndef run_session(\n    sess,\n    result_pattern,\n    feed_dict,\n    log_dir_fn,\n    finish_session_fn=None):\n\n  prefixes, result_names, ops = graph_query.find_results(sess.graph, result_pattern)\n  log_dir = log_dir_fn(prefixes)\n  graph_summary.set_summary_writer(tf.summary.FileWriter(log_dir, sess.graph))\n\n  eprint(tf.GraphKeys.QUEUE_RUNNERS, tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS))\n\n  tf.global_variables_initializer().run()\n\n  coord = tf.train.Coordinator()\n\n  run_options = tf.RunOptions(\n    # trace_level=tf.RunOptions.FULL_TRACE\n  )\n  run_metadata = tf.RunMetadata()\n\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  try:\n    result_tensors = sess.run(\n      fetches=ops,\n      feed_dict=feed_dict,\n      options=run_options,\n      run_metadata=run_metadata,\n    )\n\n    if finish_session_fn:\n      finish_session_fn(sess, prefixes)\n\n    return dict(zip(result_names, result_tensors))\n  finally:\n    # Create the Timeline object, and write it to a json\n    # tl = timeline.Timeline(run_metadata.step_stats)\n    # ctf = tl.generate_chrome_trace_format()\n    # with open(\'timeline.json\', \'w\') as f:\n    #     f.write(ctf)\n\n    coord.request_stop()\n    coord.join(threads)\n    graph_summary.set_summary_writer(None)\n\nfrom tensorflow.python.ops import script_ops\n\ndef import_and_run_meta_graph(\n    meta_graph_def,\n    result_pattern,\n    feed_dict_fn,\n    log_dir_fn,\n    finish_session_fn=None):\n  # TODO(adamb) Carefully find the asset map and replace any asset py funcs appropriately with input_map.\n  with create_session() as sess:\n    try:\n      meta_graph.import_scoped_meta_graph(\n        meta_graph_def,\n        input_map=None,\n      )\n    except KeyError as e:\n      nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n      nodes.sort()\n      eprint(\'error, but got nodes\', nodes)\n      raise e\n\n    # NOTE(adamb) Could also store files to copy out in assets_collection\n    js_py_func_data_tensor = None\n    try:\n      js_py_func_data_tensor = sess.graph.get_tensor_by_name(""py_funcs_json:0"")\n    except KeyError as e:\n      pass\n\n    if js_py_func_data_tensor is not None:\n      js_py_func_data = js_py_func_data_tensor.eval().decode(\'utf-8\')\n      py_func_data = json.loads(js_py_func_data)\n      # eprint(\'loaded py_func_data\', py_func_data)\n      py_importer = graph_ffi.PythonImporter()\n      py_importer.restore_py_funcs(script_ops._py_funcs, py_func_data)\n\n    try:\n      return run_session(sess, result_pattern, feed_dict_fn(), log_dir_fn, finish_session_fn=finish_session_fn)\n    finally:\n      sess.close()\n\n\ndef run_imported_graph(graph_def, result_pattern, feed_dict_fn, log_dir_fn):\n  with create_session() as sess:\n    tf.import_graph_def(\n      graph_def,\n      name=""""\n    )\n\n    try:\n      return run_session(sess, result_pattern, feed_dict_fn(), log_dir_fn)\n    finally:\n      sess.close()\n'"
core/python/src/nao/run/graph_summary.py,0,"b'\n_summary_writer = None\n\nclass Multiplexer:\n  def __init__(self, targets):\n    self._targets = targets\n\n  def add_target(self, target):\n    self._targets.append(target)\n\n  def remove_target(self, target):\n    self._targets.remove(target)\n\n  def add_event(self, event):\n    for target in self._targets:\n      target.add_event(event)\n\n  def flush(self):\n    for target in self._targets:\n      target.flush()\n\n  def add_graph(self, graph, global_step=None):\n    for target in self._targets:\n      target.add_graph(graph, global_step)\n\n  def add_meta_graph(self, meta_graph_def, global_step=None):\n    for target in self._targets:\n      target.add_meta_graph(meta_graph_def, global_step)\n\n  def add_run_metadata(self, run_metadata, tag, global_step=None):\n    for target in self._targets:\n      target.add_run_metadata(run_metadata, tag, global_step)\n\n  def add_summary(self, summary, global_step=None):\n    for target in self._targets:\n      target.add_summary(summary, global_step)\n\n  def add_session_log(self, session_log, global_step=None):\n    for target in self._targets:\n      target.add_session_log(session_log, global_step)\n\nclass Delegate:\n  def __init__(self, fn):\n    self._fn = fn\n\n  def flush(self):\n    pass\n\n  def add_event(self, event):\n    self._fn([""event"", event])\n\n  def add_graph(self, graph, global_step=None):\n    self._fn([""graph"", graph, global_step])\n\n  def add_meta_graph(self, meta_graph_def, global_step=None):\n    self._fn([""meta_graph"", meta_graph_def, global_step])\n\n  def add_run_metadata(self, run_metadata, tag, global_step=None):\n    self._fn([""run_metadata"", run_metadata, tag, global_step])\n\n  def add_summary(self, summary, global_step=None):\n    self._fn([""summary"", summary, global_step])\n\n  def add_session_log(self, session_log, global_step=None):\n    self._fn([""session_log"", session_log, global_step])\n\ndef set_summary_writer(summary_writer):\n  global _summary_writer\n  _summary_writer = summary_writer\n\ndef get_summary_writer():\n  global _summary_writer\n  return _summary_writer\n'"
core/python/src/nao/structure/__init__.py,0,b''
core/python/src/nao/structure/graph_constants.py,1,"b'import json\nimport sys\nimport tensorflow as tf\nimport numpy\n\nfrom tensorflow.python.framework import tensor_util\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\ndef load_json(graph, const_name):\n  json_tensor = None\n  try:\n    json_tensor = graph.get_tensor_by_name(""%s:0"" % const_name)\n  except KeyError as e:\n    pass\n\n  if json_tensor is None:\n    return None\n\n  json_str = str(tensor_util.constant_value(json_tensor).astype(\'U\'))\n  return json.loads(json_str)\n\ndef store_json(const_name, value):\n  json_str = json.dumps(value).encode(\'UTF-8\')\n  return tf.constant(json_str, name=const_name)\n'"
core/python/src/nao/structure/graph_ffi.py,0,"b'import ast\nimport imp\nimport sys\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\nclass PythonImporter:\n  def __init__(self):\n    self._module_sources = {} # module_name -> module_source\n    self._imported_functions = {} # fn -> (module_name, fn_name)\n\n  def import_module(self, module_name, module_src):\n    # Documentation here http://greentreesnakes.readthedocs.io/en/latest/nodes.html\n    inner_module_ast = ast.parse(module_src)\n    fn_asts = [f for f in inner_module_ast.body if isinstance(f, ast.FunctionDef)]\n\n    fn_names = [fn_ast.name for fn_ast in fn_asts]\n    return self._load_module_functions(module_name, module_src, fn_names)\n\n  def dump_py_funcs(self, py_funcs):\n    with py_funcs._lock:\n      return {\n        \'unique_id\': py_funcs._unique_id,\n        \'modules\': self._dump_modules(py_funcs._funcs)\n      }\n\n  def restore_py_funcs(self, py_funcs, data):\n    unique_id = data[\'unique_id\']\n    modules = data[\'modules\']\n\n    # eprint(\'py_funcs._funcs\', py_funcs._funcs)\n    with py_funcs._lock:\n      if len(py_funcs._funcs) > 0:\n        raise Exception(\n            ""py_funcs is not pristine (len(py_funcs._funcs) is %d). Aborting restore."" % len(py_funcs._funcs))\n\n      py_funcs._unique_id = unique_id\n      py_funcs._funcs = self._load_funcs(modules)\n\n  def _dump_modules(self, fn_by_token_dict):\n    modules = {}\n    for token, fn in fn_by_token_dict.items():\n      if fn not in self._imported_functions:\n        continue\n      module_name, fn_name = self._imported_functions[fn]\n      # eprint(""dumping %s -> (%s, %s)"" % (fn, module_name, fn_name))\n      if module_name not in modules:\n        modules[module_name] = {\n          ""source"": self._module_sources[module_name],\n          ""fn_name_by_token"": {},\n        }\n      modules[module_name][""fn_name_by_token""][token] = fn_name\n\n    return modules\n\n  def _load_funcs(self, modules):\n    fn_by_token = {}\n    for module_name, data in modules.items():\n      source, fn_name_by_token = data[\'source\'], data[\'fn_name_by_token\']\n      fn_names = []\n      for token, fn_name in fn_name_by_token.items():\n        fn_names.append(fn_name)\n\n      fns = self._load_module_functions(module_name, source, fn_names)\n      for token, fn_name in fn_name_by_token.items():\n        fn_by_token[token] = fns[fn_name]\n\n    return fn_by_token\n\n  def _load_module_functions(self, name, source, fn_names):\n    m = imp.new_module(name)\n    exec(source, m.__dict__)\n\n    d = {}\n    for fn_name in fn_names:\n      fn = getattr(m, fn_name)\n      self._imported_functions[fn] = (name, fn_name)\n      d[fn_name] = fn\n\n    self._module_sources[name] = source\n\n    # eprint(\'_load_module_functions\', d)\n    return d\n'"
core/python/src/nao/structure/graph_io.py,0,"b'from tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.protobuf import meta_graph_pb2\nfrom tensorflow.python.framework import meta_graph\n\nfrom google.protobuf import text_format\n\ndef _write_pb(pb, file, binary):\n  if binary:\n    with open(file, ""wb"") as f:\n      f.write(pb.SerializeToString())\n  else:\n    with open(file, ""w"") as f:\n      data = text_format.MessageToString(pb)\n      f.write(data)\n\ndef _parse_pb(pb, data, binary):\n  if binary:\n    pb.ParseFromString(data)\n  else:\n    text_format.Merge(data, pb)\n  return pb\n\ndef _read_pb(pb, file, binary):\n  mode = ""r""\n  if binary:\n    mode += ""b""\n\n  with open(file, mode) as f:\n    return _parse_pb(pb, f.read(), binary)\n\ndef read_graph_def(file, binary):\n  return _read_pb(graph_pb2.GraphDef(), file, binary)\n\ndef write_graph_def(graph_def, file, binary):\n  _write_pb(graph_def, file, binary)\n\ndef read_meta_graph_def(file, binary):\n  return _read_pb(meta_graph_pb2.MetaGraphDef(), file, binary)\n\ndef write_meta_graph_def(meta_graph_def, file, binary):\n  _write_pb(meta_graph_def, file, binary)\n'"
core/python/src/nao/structure/graph_query.py,0,"b'def find_variables_by_name(var_collection, var_names):\n  vars_by_name = {}\n  for var in var_collection:\n    vars_by_name[var.name] = var\n\n  return [vars_by_name[var_name] for var_name in var_names]\n\ndef find_nodes_with_pattern(graph, pattern):\n  node_matches = []\n  for n in graph.get_operations():\n    m = pattern.match(n.name)\n    if m:\n      node_matches.append((n, m))\n\n  if len(node_matches) == 0:\n    raise Exception(\n        ""No nodes match pattern %s. Considered: %s"" % (pattern, [n.name for n in graph.get_operations()]))\n\n  return node_matches\n\ndef find_results(graph_def, result_pattern):\n  node_matches = find_nodes_with_pattern(graph_def, result_pattern)\n  ops = [n.name + "":0"" for n, m in node_matches]\n  result_names = [m.group(2) for n, m in node_matches]\n  prefix_set = set()\n  for n, m in node_matches:\n    prefix_set.add(m.group(1))\n\n  return (list(prefix_set), result_names, ops)\n'"
core/python/src/nao/structure/graph_xform.py,12,"b'import sys\n\nimport tensorflow as tf\n\nfrom tensorflow.core.framework import variable_pb2\nfrom tensorflow.core.protobuf import control_flow_pb2\nfrom tensorflow.python.framework import graph_util\n\ndef eprint(*args, **kwargs):\n    print(*args, file=sys.stderr, **kwargs)\n\ndef constants_as_dict(constants):\n  d = {}\n  for node in constants:\n    name = node.name\n    tensor = node.attr[\'value\'].tensor\n    value = None\n    dtype = tensor.dtype\n    if dtype == tf.bool:\n      value = tensor.bool_val\n    elif dtype == tf.float16:\n      value = tensor.half_val\n    elif dtype == tf.float32:\n      value = tensor.float_val\n    if dtype == tf.float64:\n      value = tensor.double_val\n    elif dtype == tf.complex64:\n      value = tensor.scomplex_val\n    elif dtype == tf.complex128:\n      value = tensor.dcomplex_val\n    elif dtype == tf.int64:\n      value = tensor.int64_val\n    elif dtype == tf.string:\n      value = tensor.string_val\n\n    d[name] = value\n\n  return d\n\ndef dict_as_graph_def(constants_dict):\n  with tf.Graph().as_default() as g:\n    for name, value in constants_dict.items():\n      tf.constant(value, name=name)\n\n    return g.as_graph_def()\n\ndef replace_variable_initializers_with_current_values(graph, vars, value_suffix):\n  with graph.as_default():\n    for var in vars:\n      var_op_name = var.op.name\n      var_value = var.value().eval()\n      var_init_op = tf.assign(\n        var,\n        tf.constant(var_value, name=""%s/%s"" % (var_op_name, value_suffix)),\n        name=""%s/Assign%s"" % (var_op_name, value_suffix)).op\n      var._initializer_op = var_init_op\n      eprint(""Resetting initializer for var"", var)\n\ndef strip_meta_graph(meta_graph_def, node_names, var_names):\n  node_names = node_names[:]\n  collections = meta_graph_def.collection_def\n\n  # Look for matching variable names and initializers and keep them too.\n  var_def = variable_pb2.VariableDef()\n  for var_col_name in [""variables"", ""trainable_variables""]:\n    var_def_bs = collections[var_col_name].bytes_list.value\n    for var_def_b in var_def_bs:\n      var_def.ParseFromString(var_def_b)\n      if var_def.variable_name not in var_names:\n        # TODO(adamb) Should remove variable from collection.\n        continue\n      node_names.append(var_def.initializer_name)\n\n  wc_def = control_flow_pb2.WhileContextDef()\n  wc_values = collections[""while_context""].bytes_list.value\n  for wc_ix in range(len(wc_values) - 1, -1, -1):\n    wc_bytes = wc_values[wc_ix]\n    wc_def.ParseFromString(wc_bytes)\n    unused = True\n    wc_pivot_name = wc_def.pivot_name\n    for name in node_names:\n      if name.startswith(wc_pivot_name):\n        unused = False\n        break\n\n    if unused:\n      del wc_values[wc_ix]\n\n  graph_def = meta_graph_def.graph_def\n  eprint(""only keeping"", node_names, ""from"", [n.name for n in graph_def.node])\n  graph_def = graph_util.extract_sub_graph(graph_def, node_names)\n  meta_graph_def.graph_def.CopyFrom(graph_def)\n'"
core/python/src/nao/tool/__init__.py,0,b''
core/python/src/nao/tool/graph_repl.py,13,"b'import traceback\nimport sys\nimport tensorflow as tf\n\nfrom nao.run import graph_summary\n\nfrom nao.compiler.retvalbag import RetvalBag\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\nclass ReplSession:\n  def __init__(self, compiler, log_dir_fn):\n    self._log_dir_fn = log_dir_fn\n    self._suffix = "".nao""\n    self._compiler = compiler\n    self._session = compiler.new_session()\n    self._graph = self._session.graph\n    self._previous_queue_runners = frozenset()\n    self._previous_vars = frozenset()\n    self._threads = []\n    self._coord = tf.train.Coordinator()\n    self._next_run_id = 0\n    self._summary_writer = None\n\n  def _vars(self):\n    with self._graph.as_default():\n      return tf.global_variables()\n\n  def _queue_runners(self):\n    with self._graph.as_default():\n      return tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\n\n  def _init_new_vars(self, new_vars):\n    if len(new_vars) == 0:\n      return\n\n    print(""New variables"", new_vars)\n    tf.variables_initializer(new_vars).run(session=self._session)\n\n  def _init_new_queue_runners(self, new_queue_runners):\n    if len(new_queue_runners) == 0:\n      return\n\n    print(""new_queue_runners"", new_queue_runners)\n    for qr in new_queue_runners:\n      threads = qr.create_threads(self._session, coord=self._coord, daemon=True, start=True)\n      print(""started"", threads)\n      self._threads.extend(threads)\n\n  def run(self, src, summary_fn=None):\n    run_id = self._next_run_id\n    self._next_run_id = self._next_run_id + 1\n    if self._summary_writer is None:\n      self._summary_writer = tf.summary.FileWriter(\n          self._log_dir_fn())\n\n    try:\n      multiplexer = graph_summary.Multiplexer([self._summary_writer])\n      if summary_fn is not None:\n        multiplexer.add_target(graph_summary.Delegate(summary_fn))\n\n      graph_summary.set_summary_writer(multiplexer)\n      return self._run(multiplexer, run_id, src)\n    finally:\n      graph_summary.set_summary_writer(None)\n      self._summary_writer.flush()\n\n  def _run(self, summary_writer, run_id, src):\n    self._compiler.clear()\n    self._compiler.put_source(""main%s"" % self._suffix, src)\n    self._compiler.set_default_device(""/cpu:0"")\n\n    above = None\n    pkg = self._compiler.resolve_import_path(""main"", reimport=True)\n    above = pkg.ctx().get_above()\n\n    # Write graph once we\'ve generated it.\n    summary_writer.add_graph(self._session.graph, run_id)\n    summary_writer.flush()\n\n    vars = frozenset(self._vars())\n    self._init_new_vars(vars - self._previous_vars)\n    self._previous_vars = vars\n\n    queue_runners = frozenset(self._queue_runners())\n    self._init_new_queue_runners(queue_runners - self._previous_queue_runners)\n    self._previous_queue_runners = queue_runners\n\n    if isinstance(above, RetvalBag):\n      above = above.get(None)\n\n    if isinstance(above, (tf.Tensor, tf.Variable, tf.Operation)):\n      run_metadata = tf.RunMetadata()\n      above = self._session.run(above, run_metadata=run_metadata)\n      summary_writer.add_run_metadata(run_metadata, ""repl-%04d"" % run_id, run_id)\n\n    return above\n\n  def __del__(self):\n    # Shutdown threads, if any.\n    if self._summary_writer is not None:\n      self._summary_writer.close()\n    self._coord.request_stop()\n    self._coord.join(self._threads)\n\n    self._session.close()\n\nimport atexit\nimport os\nimport readline\n\nHISTORY_BASENAME = \'.nao_history\'\n\ndef run(parser, log_fn):\n  histfile = os.path.join(os.path.expanduser(""~""), HISTORY_BASENAME)\n\n  try:\n    readline.read_history_file(histfile)\n    h_len = readline.get_history_length()\n  except FileNotFoundError:\n    open(histfile, \'wb\').close()\n    h_len = 0\n\n  def save(prev_h_len, histfile):\n    new_h_len = readline.get_history_length()\n    readline.set_history_length(1000)\n    readline.append_history_file(new_h_len - prev_h_len, histfile)\n\n  repl_session = ReplSession(parser, log_fn)\n  while True:\n    try:\n      src = input(""> "")\n      if src == """":\n        continue\n    except KeyboardInterrupt:\n      print(""^C"")\n      continue\n    except EOFError:\n      print()\n      break\n\n    try:\n      result = repl_session.run(src)\n      print(result)\n    except Exception as e:\n      print("""".join(traceback.format_exception(None, e, e.__traceback__)),\n            file=sys.stdout, flush=True)\n\n\n  #\n  #\n  # """"""A training helper that checkpoints models and computes summaries.\n  #\n  # The Supervisor is a small wrapper around a `Coordinator`, a `Saver`,\n  # and a `SessionManager` that takes care of common needs of TensorFlow\n  # training programs.\n  #\n  # #### Use for a single program\n  #\n  # ```python\n  # with tf.Graph().as_default():\n  #   ...add operations to the graph...\n  #   # Create a Supervisor that will checkpoint the model in \'/tmp/mydir\'.\n  #   sv = Supervisor(logdir=\'/tmp/mydir\')\n  #   # Get a TensorFlow session managed by the supervisor.\n  #   with sv.managed_session(FLAGS.master) as sess:\n  #     # Use the session to train the graph.\n  #     while not sv.should_stop():\n  #       sess.run(<my_train_op>)\n  # ```\n  #\n  # Within the `with sv.managed_session()` block all variables in the graph have\n  # been initialized.  In addition, a few services have been started to\n  # checkpoint the model and add summaries to the event log.\n  #\n  # If the program crashes and is restarted, the managed session automatically\n  # reinitialize variables from the most recent checkpoint.\n  #\n  # The supervisor is notified of any exception raised by one of the services.\n  # After an exception is raised, `should_stop()` returns `True`.  In that case\n  # the training loop should also stop.  This is why the training loop has to\n  # check for `sv.should_stop()`.\n  #\n  # Exceptions that indicate that the training inputs have been exhausted,\n  # `tf.errors.OutOfRangeError`, also cause `sv.should_stop()` to return `True`\n  # but are not re-raised from the `with` block: they indicate a normal\n  # termination.\n  #\n  # #### What `master` string to use\n  #\n  # Whether you are running on your machine or in the cluster you can use the\n  # following values for the --master flag:\n  #\n  # * Specifying `\'\'` requests an in-process session that does not use RPC.\n  #\n  # * Specifying `\'local\'` requests a session that uses the RPC-based\n  #   ""Master interface"" to run TensorFlow programs. See\n  #   [`tf.train.Server.create_local_server()`](#Server.create_local_server) for\n  #   details.\n  #\n  # #### Advanced use\n  #\n  # ##### Launching additional services\n  #\n  # `managed_session()` launches the Checkpoint and Summary services (threads).\n  # If you need more services to run you can simply launch them in the block\n  # controlled by `managed_session()`.\n  #\n  # Example: Start a thread to print losses.  We want this thread to run\n  # every 60 seconds, so we launch it with `sv.loop()`.\n  #\n  #   ```python\n  #   ...\n  #   sv = Supervisor(logdir=\'/tmp/mydir\')\n  #   with sv.managed_session(FLAGS.master) as sess:\n  #     sv.loop(60, print_loss, (sess, ))\n  #     while not sv.should_stop():\n  #       sess.run(my_train_op)\n  #   ```\n  #\n  # ##### Launching fewer services\n  #\n  # `managed_session()` launches the ""summary"" and ""checkpoint"" threads which use\n  # either the optionally `summary_op` and `saver` passed to the constructor, or\n  # default ones created automatically by the supervisor.  If you want to run\n  # your own summary and checkpointing logic, disable these services by passing\n  # `None` to the `summary_op` and `saver` parameters.\n  #\n  # Example: Create summaries manually every 100 steps in the chief.\n  #\n  #   ```python\n  #   # Create a Supervisor with no automatic summaries.\n  #   sv = Supervisor(logdir=\'/tmp/mydir\', is_chief=is_chief, summary_op=None)\n  #   # As summary_op was None, managed_session() does not start the\n  #   # summary thread.\n  #   with sv.managed_session(FLAGS.master) as sess:\n  #     for step in xrange(1000000):\n  #       if sv.should_stop():\n  #         break\n  #       if is_chief and step % 100 == 0:\n  #         # Create the summary every 100 chief steps.\n  #         sv.summary_computed(sess, sess.run(my_summary_op))\n  #       else:\n  #         # Train normally\n  #         sess.run(my_train_op)\n  #   ```\n  #\n  # ##### Custom model initialization\n  #\n  # `managed_session()` only supports initializing the model by running an\n  # `init_op` or restoring from the latest checkpoint.  If you have special\n  # initialization needs, see how to specify a `local_init_op` when creating the\n  # supervisor.  You can also use the `SessionManager` directly to create a\n  # session and check if it could be initialized automatically.\n  #\n  #   """"""Create a `Supervisor`.\n  #\n  #   Args:\n  #     graph: A `Graph`.  The graph that the model will use.  Defaults to the\n  #       default `Graph`.  The supervisor may add operations to the graph before\n  #       creating a session, but the graph should not be modified by the caller\n  #       after passing it to the supervisor.\n  #     ready_op: 1-D string `Tensor`.  This tensor is evaluated by supervisors in\n  #       `prepare_or_wait_for_session()` to check if the model is ready to use.\n  #       The model is considered ready if it returns an empty array.  Defaults to\n  #       the tensor returned from `tf.report_uninitialized_variables()`  If\n  #       `None`, the model is not checked for readiness.\n  #     ready_for_local_init_op: 1-D string `Tensor`.  This tensor is evaluated by\n  #       supervisors in `prepare_or_wait_for_session()` to check if the model is\n  #       ready to run the local_init_op.\n  #       The model is considered ready if it returns an empty array.  Defaults to\n  #       the tensor returned from\n  #       `tf.report_uninitialized_variables(tf.global_variables())`. If `None`,\n  #       the model is not checked for readiness before running local_init_op.\n  #     is_chief: If True, create a chief supervisor in charge of initializing\n  #       and restoring the model.  If False, create a supervisor that relies\n  #       on a chief supervisor for inits and restore.\n  #     init_op: `Operation`.  Used by chief supervisors to initialize the model\n  #       when it can not be recovered.  Defaults to an `Operation` that\n  #       initializes all variables.  If `None`, no initialization is done\n  #       automatically unless you pass a value for `init_fn`, see below.\n  #     init_feed_dict: A dictionary that maps `Tensor` objects to feed values.\n  #       This feed dictionary will be used when `init_op` is evaluated.\n  #     local_init_op: `Operation`. Used by all supervisors to run initializations\n  #       that should run for every new supervisor instance. By default these\n  #       are table initializers and initializers for local variables.\n  #       If `None`, no further per supervisor-instance initialization is\n  #       done automatically.\n  #     logdir: A string.  Optional path to a directory where to checkpoint the\n  #       model and log events for the visualizer.  Used by chief supervisors.\n  #       The directory will be created if it does not exist.\n  #     summary_op: An `Operation` that returns a Summary for the event logs.\n  #       Used by chief supervisors if a `logdir` was specified.  Defaults to the\n  #       operation returned from summary.merge_all().  If `None`, summaries are\n  #       not computed automatically.\n  #     saver: A Saver object.  Used by chief supervisors if a `logdir` was\n  #       specified.  Defaults to the saved returned by Saver().\n  #       If `None`, the model is not saved automatically.\n  #     global_step: An integer Tensor of size 1 that counts steps.  The value\n  #       from \'global_step\' is used in summaries and checkpoint filenames.\n  #       Default to the op named \'global_step\' in the graph if it exists, is of\n  #       rank 1, size 1, and of type tf.int32 or tf.int64.  If `None` the global\n  #       step is not recorded in summaries and checkpoint files.  Used by chief\n  #       supervisors if a `logdir` was specified.\n  #     save_summaries_secs: Number of seconds between the computation of\n  #       summaries for the event log.  Defaults to 120 seconds.  Pass 0 to\n  #       disable summaries.\n  #     save_model_secs: Number of seconds between the creation of model\n  #       checkpoints.  Defaults to 600 seconds.  Pass 0 to disable checkpoints.\n  #     recovery_wait_secs: Number of seconds between checks that the model\n  #       is ready.  Used by supervisors when waiting for a chief supervisor\n  #       to initialize or restore the model.  Defaults to 30 seconds.\n  #     stop_grace_secs: Grace period, in seconds, given to running threads to\n  #       stop when `stop()` is called.  Defaults to 120 seconds.\n  #     checkpoint_basename: The basename for checkpoint saving.\n  #     session_manager: `SessionManager`, which manages Session creation and\n  #       recovery. If it is `None`, a default `SessionManager` will be created\n  #       with the set of arguments passed in for backwards compatibility.\n  #     summary_writer: `SummaryWriter` to use or `USE_DEFAULT`.  Can be `None`\n  #       to indicate that no summaries should be written.\n  #     init_fn: Optional callable used to initialize the model. Called\n  #       after the optional `init_op` is called.  The callable must accept one\n  #       argument, the session being initialized.\n  #\n  #   Returns:\n  #     A `Supervisor`.\n  #   """"""\n'"
core/python/src/nao/tool/json_util.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A module providing a function for serializing JSON values with Infinity.\n\nPython provides no way to override how json.dumps serializes\nInfinity/-Infinity/NaN; if allow_nan is true, it encodes them as\nInfinity/-Infinity/NaN, in violation of the JSON spec and in violation of what\nJSON.parse accepts. If it\'s false, it throws a ValueError, Neither subclassing\nJSONEncoder nor passing a function in the |default| keyword argument overrides\nthis.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom tensorflow.python.util import compat\n\n_INFINITY = float(\'inf\')\n_NEGATIVE_INFINITY = float(\'-inf\')\n\n\ndef Cleanse(obj, encoding=\'utf-8\'):\n  """"""Makes Python object appropriate for JSON serialization.\n\n  - Replaces instances of Infinity/-Infinity/NaN with strings.\n  - Turns byte strings into unicode strings.\n  - Turns sets into sorted lists.\n  - Turns tuples into lists.\n\n  Args:\n    obj: Python data structure.\n    encoding: Charset used to decode byte strings.\n\n  Returns:\n    Unicode JSON data structure.\n  """"""\n  if isinstance(obj, int):\n    return obj\n  elif isinstance(obj, float):\n    if obj == _INFINITY:\n      return \'Infinity\'\n    elif obj == _NEGATIVE_INFINITY:\n      return \'-Infinity\'\n    elif math.isnan(obj):\n      return \'NaN\'\n    else:\n      return obj\n  elif isinstance(obj, bytes):\n    return compat.as_text(obj, encoding)\n  elif isinstance(obj, list) or isinstance(obj, tuple):\n    return [Cleanse(i, encoding) for i in obj]\n  elif isinstance(obj, set):\n    return [Cleanse(i, encoding) for i in sorted(obj)]\n  elif isinstance(obj, dict):\n    return {Cleanse(k, encoding): Cleanse(v, encoding) for k, v in obj.items()}\n  else:\n    return obj\n'"
core/python/src/nao/tool/jupyter_kernel.py,0,"b'# Originally based on simple_kernel.py\n# by Doug Blank <doug.blank@gmail.com>\n#\n# To adjust debug output, set debug_level to:\n#  0 - show no debugging information\n#  1 - shows basic running information\n#  2 - also shows loop details\n#  3 - also shows message details\n#\nfrom __future__ import print_function\n\n## General Python imports:\nimport sys\nimport os\n\nimport json\nimport hmac\nimport uuid\nimport errno\nimport hashlib\nimport datetime\nimport threading\nfrom pprint import pformat\n\n# zmq specific imports:\nimport zmq\nfrom zmq.eventloop import ioloop, zmqstream\nfrom zmq.error import ZMQError\n\nPYTHON3 = sys.version_info.major == 3\n\n#Globals:\nDELIM = b""<IDS|MSG>""\n\ndebug_level = 1 # 0 (none) to 3 (all) for various levels of detail\ndef dprint(level, *args, **kwargs):\n  """""" Show debug information """"""\n  if level <= debug_level:\n    print(""DEBUG:"", *args, file=sys.stderr, **kwargs)\n    sys.stderr.flush()\n\nclass WireProtocol:\n  def __init__(self, engine_id, secure_key, signature_scheme):\n    self._engine_id = engine_id\n    signature_schemes = {""hmac-sha256"": hashlib.sha256}\n    self._auth = hmac.HMAC(\n      self._str_to_bytes(secure_key),\n      digestmod=signature_schemes[signature_scheme])\n\n  def _str_to_bytes(self, s):\n    return s.encode(\'ascii\') if PYTHON3 else bytes(s)\n\n  def _msg_id(self):\n    """""" Return a new uuid for message id """"""\n    return str(uuid.uuid4())\n\n  def _new_header(self, msg_type):\n    """"""make a new header""""""\n    return {\n        ""date"": datetime.datetime.now().isoformat(),\n        ""msg_id"": self._msg_id(),\n        ""username"": ""kernel"",\n        ""session"": self._engine_id,\n        ""msg_type"": msg_type,\n        ""version"": ""5.0"",\n      }\n\n  def sign(self, msg_lst):\n    """"""\n    Sign a message with a secure signature.\n    """"""\n    h = self._auth.copy()\n    for m in msg_lst:\n      h.update(m)\n    return self._str_to_bytes(h.hexdigest())\n\n  def serialize_wire_msg(self, msg_type, content=None, parent_header=None, metadata=None, identities=None):\n    header = self._new_header(msg_type)\n    if content is None:\n      content = {}\n    if parent_header is None:\n      parent_header = {}\n    if metadata is None:\n      metadata = {}\n\n    def encode(msg):\n      return self._str_to_bytes(json.dumps(msg))\n\n    msg_lst = [\n      encode(header),\n      encode(parent_header),\n      encode(metadata),\n      encode(content),\n    ]\n    signature = self.sign(msg_lst)\n    parts = [DELIM,\n         signature,\n         msg_lst[0],\n         msg_lst[1],\n         msg_lst[2],\n         msg_lst[3]]\n    if identities:\n      parts = identities + parts\n\n    return parts\n\n  def deserialize_wire_msg(self, wire_msg):\n    """"""split the routing prefix and message frames from a message on the wire""""""\n    delim_idx = wire_msg.index(DELIM)\n    identities = wire_msg[:delim_idx]\n    m_signature = wire_msg[delim_idx + 1]\n    msg_frames = wire_msg[delim_idx + 2:]\n\n    def decode(msg):\n      dprint(1, ""decode"", msg)\n      return json.loads(msg.decode(\'ascii\') if PYTHON3 else msg)\n\n    m = {}\n    m[\'header\']        = decode(msg_frames[0])\n    m[\'parent_header\'] = decode(msg_frames[1])\n    m[\'metadata\']      = decode(msg_frames[2])\n    m[\'content\']       = decode(msg_frames[3])\n    dprint(1, ""will sign"", m)\n    check_sig = self.sign(msg_frames)\n    if check_sig != m_signature:\n      dprint(1, check_sig ,""!="", m_signature)\n      raise ValueError(""Signatures do not match"")\n\n    dprint(1, ""m"", m)\n    dprint(1, ""identities"", identities)\n    return identities, m\n\nclass OutgoingStream:\n  def __init__(self, wire, stream):\n    self._wire = wire\n    self._stream = stream\n\n  def send(self, msg_type, content=None, parent_header=None, metadata=None, identities=None):\n    parts = self._wire.serialize_wire_msg(msg_type, content=content, parent_header=parent_header, metadata=metadata, identities=identities)\n    dprint(3, ""send parts:"", parts)\n    self._stream.send_multipart(parts)\n    self._stream.flush()\n\nclass ShellHandler:\n  def __init__(self, engine_id, iopub, shell, driver_info, driver):\n    self._driver_info = driver_info\n    self._driver = driver\n    self._engine_id = engine_id\n    self._iopub = iopub\n    self._shell = shell\n    self._execution_count = 1\n    self._pending_execute_requests = []\n    self._pending_execute_request = False\n\n  def _begin(self, identities, msg, on_done):\n    execution_count = self._execution_count\n    started = datetime.datetime.now().isoformat()\n    parent_header = msg[\'header\']\n    code = msg[\'content\'][""code""]\n\n    self._iopub.send(\'status\', {\'execution_state\': ""busy""}, parent_header=parent_header)\n\n    self._execution_count += 1\n    content = {\n      \'execution_count\': execution_count,\n      \'code\': code,\n    }\n    self._iopub.send(\'execute_input\', content, parent_header=parent_header)\n\n    def _done(result_data, result_metadata=None):\n      if result_metadata is None:\n        result_metadata = {}\n\n      self._iopub.send(\'status\', {\'execution_state\': ""idle""}, parent_header=parent_header)\n\n      content = {\n        \'execution_count\': execution_count,\n        \'data\': result_data,\n        \'metadata\': result_metadata\n      }\n      self._iopub.send(\'execute_result\', content, parent_header=parent_header)\n\n      metadata = {\n        ""dependencies_met"": True,\n        ""engine"": self._engine_id,\n        ""status"": ""ok"",\n        ""started"": started,\n      }\n      content = {\n        ""status"": ""ok"",\n        ""execution_count"": execution_count,\n        ""user_variables"": {},\n        ""payload"": [],\n        ""user_expressions"": {},\n      }\n      self._shell.send(\'execute_reply\', content, metadata=metadata,\n        parent_header=parent_header, identities=identities)\n\n      on_done()\n\n    return _done\n\n  def execute_request(self, identities, msg):\n    def schedule_next():\n      print(""schedule_next"", self._pending_execute_request, self._pending_execute_requests)\n\n      if len(self._pending_execute_requests) == 0:\n        self._pending_execute_request = False\n      else:\n        identities2, msg2 = self._pending_execute_requests.pop(0)\n        self._execute_request(schedule_next, identities2, msg2)\n\n    if self._pending_execute_request:\n      self._pending_execute_requests.append((identities, msg))\n    else:\n      self._execute_request(schedule_next, identities, msg)\n\n\n  def _execute_request(self, on_done, identities, msg):\n    on_result = self._begin(identities, msg, on_done)\n\n    code = msg[\'content\'][""code""]\n\n    has_displayed = set()\n    def on_display(display_id, data, metadata):\n      content = {\n        ""data"": data,\n        ""metadata"": metadata,\n        ""transient"": {\n          ""display_id"": display_id,\n        },\n      }\n\n      display_message_type = \'update_display_data\'\n      if display_id not in has_displayed:\n        display_message_type = \'display_data\'\n        has_displayed.add(display_id)\n      self._iopub.send(display_message_type, content, parent_header=msg[\'header\'])\n\n    def on_stdout(text):\n      content = {\n        \'name\': ""stdout"",\n        \'text\': text,\n      }\n      self._iopub.send(\'stream\', content, parent_header=msg[\'header\'])\n\n    self._driver(code, on_stdout, on_display, on_result)\n\n  def kernel_info_request(self, identities, msg):\n    content = {}\n    content.update(self._driver_info)\n    content.update({\n      ""protocol_version"": ""5.0"",\n      ""ipython_version"": [1, 1, 0, """"],\n    })\n    self._shell.send(\'kernel_info_reply\', content, parent_header=msg[\'header\'], identities=identities)\n\n  def __call__(self, identities, msg):\n    dprint(1, ""shell received:"", identities, msg)\n\n    # process request:\n    msg_type = msg[\'header\'][""msg_type""]\n    if msg_type == ""execute_request"":\n      self.execute_request(identities, msg)\n    elif msg_type == ""kernel_info_request"":\n      self.kernel_info_request(identities, msg)\n    elif msg_type == ""history_request"":\n      dprint(1, ""unhandled history request"")\n    else:\n      dprint(1, ""unknown msg_type:"", msg_type)\n\nclass Kernel:\n  def __init__(self, config, driver_info, driver):\n    # Clone config so we can update it.\n    config = json.loads(json.dumps(config))\n\n    self._config = config\n    self._exiting = False\n    self._engine_id = str(uuid.uuid4())\n    self._wire = WireProtocol(self._engine_id, config[""key""], config[""signature_scheme""])\n\n    connection = config[""transport""] + ""://"" + config[""ip""]\n\n    def bind(socket, port):\n      if port <= 0:\n        return socket.bind_to_random_port(connection)\n      else:\n        socket.bind(""%s:%s"" % (connection, port))\n      return port\n\n    def wrap_with_deserialization(fn):\n      def accept(wire_msg):\n        return fn(*self._wire.deserialize_wire_msg(wire_msg))\n\n      return accept\n\n    ## Initialize:\n    ioloop.install()\n\n    ctx = zmq.Context()\n    self._heartbeat_socket = ctx.socket(zmq.REP)\n    config[""hb_port""] = bind(self._heartbeat_socket, config[""hb_port""])\n\n    # IOPub/Sub: also called SubSocketChannel in IPython sources\n    self._iopub_socket = ctx.socket(zmq.PUB)\n    config[""iopub_port""] = bind(self._iopub_socket, config[""iopub_port""])\n    iopub_stream = zmqstream.ZMQStream(self._iopub_socket)\n    iopub_stream.on_recv(wrap_with_deserialization(self._iopub_handler))\n    iopub = OutgoingStream(self._wire, iopub_stream)\n\n    self._control_socket = ctx.socket(zmq.ROUTER)\n    config[""control_port""] = bind(self._control_socket, config[""control_port""])\n    control_stream = zmqstream.ZMQStream(self._control_socket)\n    control_stream.on_recv(wrap_with_deserialization(self._control_handler))\n\n    self._stdin_socket = ctx.socket(zmq.ROUTER)\n    config[""stdin_port""] = bind(self._stdin_socket, config[""stdin_port""])\n    stdin_stream = zmqstream.ZMQStream(self._stdin_socket)\n    stdin_stream.on_recv(wrap_with_deserialization(self._stdin_handler))\n\n    self._shell_socket = ctx.socket(zmq.ROUTER)\n    config[""shell_port""] = bind(self._shell_socket, config[""shell_port""])\n    shell_stream = zmqstream.ZMQStream(self._shell_socket)\n    shell = OutgoingStream(self._wire, shell_stream)\n    shell_stream.on_recv(wrap_with_deserialization(self._shell_handler))\n\n    self._shell_handler_impl = ShellHandler(self._engine_id, iopub, shell, driver_info, driver)\n\n  def _control_handler(self, identities, msg):\n    dprint(1, ""control received:"", identities, msg)\n    msg_type = msg[\'header\'][""msg_type""]\n    if msg_type == ""shutdown_request"":\n      self._shutdown_request(identities, msg)\n\n  def _shell_handler(self, identities, msg):\n    msg_type = msg[\'header\'][""msg_type""]\n    if msg_type == ""shutdown_request"":\n      self._shutdown_request(identities, msg)\n    else:\n      self._shell_handler_impl(identities, msg)\n\n  def _shutdown_request(self, identities, msg):\n    self.shutdown()\n\n  def _iopub_handler(self, identities, msg):\n    dprint(1, ""iopub received:"", identities, msg)\n\n  def _stdin_handler(self, identities, msg):\n    dprint(1, ""stdin received:"", identities, msg)\n\n  # Utility functions:\n  def shutdown(self):\n    self._exiting = True\n    ioloop.IOLoop.instance().stop()\n\n  def run(self):\n    dprint(1, ""Config:"", json.dumps(self._config))\n    dprint(1, ""Starting loops..."")\n\n    def heartbeat_loop():\n      dprint(2, ""Starting loop for \'Heartbeat\'..."")\n      while not self._exiting:\n        dprint(3, ""."", end="""")\n        try:\n          zmq.device(zmq.FORWARDER, self._heartbeat_socket, self._heartbeat_socket)\n        except zmq.ZMQError as e:\n          if e.errno == errno.EINTR:\n            continue\n          else:\n            raise\n        else:\n          break\n    hb_thread = threading.Thread(target=heartbeat_loop)\n    hb_thread.daemon = True\n    hb_thread.start()\n\n    dprint(1, ""Ready! Listening..."")\n\n    ioloop.IOLoop.instance().start()\n'"
core/python/src/nao/tool/jupyter_kernel_driver.py,0,"b'import base64\nimport imghdr\nimport json\nimport queue\nimport sys\nimport threading\nimport time\n\nfrom nao.tool import summary_format, json_util\n\n_IMGHDR_TO_MIMETYPE = {\n    \'bmp\': \'image/bmp\',\n    \'gif\': \'image/gif\',\n    \'jpeg\': \'image/jpeg\',\n    \'png\': \'image/png\'\n}\n_DEFAULT_IMAGE_MIMETYPE = \'application/octet-stream\'\n\ndef _content_type_for_image(encoded_image_string):\n  image_type = imghdr.what(None, encoded_image_string)\n  return _IMGHDR_TO_MIMETYPE.get(image_type, _DEFAULT_IMAGE_MIMETYPE)\n\nclass Driver:\n  def __init__(self, repl_session):\n    self._repl_session = repl_session\n    self._id = 0\n    self._display_queue = queue.Queue()\n    self._display_thread = None\n\n  def info(self):\n    return {\n      ""language_version"": [0, 0, 1],\n      ""language"": ""nao"",\n      ""implementation"": ""nao"",\n      ""implementation_version"": ""0.0.1"",\n      ""language_info"": {\n        ""name"": ""nao"",\n        ""version"": ""1.0"",\n        \'mimetype\': """",\n        \'file_extension\': "".nao"",\n        \'pygments_lexer\': """",\n        \'codemirror_mode\': """",\n        \'nbconvert_exporter\': """",\n      },\n      ""banner"": """"\n    }\n\n  def _emit_summary_pb(self, on_display, protobuf, wall_time, step):\n    def display_summary(summary_type, summary):\n      self._id = self._id + 1\n\n      data = {}\n      metadata = {}\n      if summary_type == ""image"":\n        encoded_image_string = summary[""encoded_image_string""]\n        content_type = _content_type_for_image(encoded_image_string)\n        data[content_type] = base64.b64encode(encoded_image_string).decode(\'utf-8\')\n        metadata[content_type] = {\n          ""width"": summary[""width""],\n          ""height"": summary[""height""],\n        }\n      else:\n        content_type = ""application/json""\n        data[content_type] = json_util.Cleanse(summary)\n\n      on_display(str(self._id), data, metadata)\n\n    summary_format.parse(protobuf, wall_time, int(step), display_summary)\n\n  def _run_display_thread(self):\n    q = self._display_queue\n    while True:\n      display_data = q.get()\n      try:\n        if display_data is None:\n          break\n        self._emit_summary_pb(*display_data)\n      except Exception as e:\n        print(""ERROR in _run_display_thread"", e, file=sys.stderr)\n      finally:\n        q.task_done()\n\n  def start(self):\n    if self._display_thread:\n      return\n\n    self._display_thread = threading.Thread(target=self._run_display_thread)\n    self._display_thread.start()\n\n  def do(self, code, on_stdout, on_display, on_result):\n    self.start()\n\n    q = self._display_queue\n    def on_summary_protobuf(args):\n      if args[0] is not ""summary"":\n        return\n\n      protobuf, step = args[1], args[2]\n      q.put((on_display, protobuf, time.time(), step))\n\n    try:\n      result = self._repl_session.run(code, summary_fn=on_summary_protobuf)\n    except Exception as e:\n      result = e\n    on_result({""text/plain"": str(result)})\n'"
core/python/src/nao/tool/summary_format.py,0,"b'# Taken from https://github.com/tensorflow/tensorboard/blob/master/tensorboard/backend/event_processing/event_accumulator.py\n\nfrom collections import namedtuple\n\nimport numpy as np\n\nfrom tensorflow.core.framework import summary_pb2\nfrom tensorflow.core.util import event_pb2\n\n# The tag that values containing health pills have. Health pill data is stored\n# in tensors. In order to distinguish health pill values from scalar values, we\n# rely on how health pill values have this special tag value.\nHEALTH_PILL_EVENT_TAG = \'__health_pill__\'\n\ndef _CompressHistogram(histo_ev, bps):\n  """"""Creates fixed size histogram by adding compression to accumulated state.\n\n  This routine transforms a histogram at a particular step by linearly\n  interpolating its variable number of buckets to represent their cumulative\n  weight at a constant number of compression points. This significantly reduces\n  the size of the histogram and makes it suitable for a two-dimensional area\n  plot where the output of this routine constitutes the ranges for a single x\n  coordinate.\n\n  Args:\n    histo_ev: A HistogramEvent namedtuple.\n    bps: Compression points represented in basis points, 1/100ths of a percent.\n\n  Returns:\n    CompressedHistogramEvent namedtuple.\n  """"""\n  # See also: Histogram::Percentile() in core/lib/histogram/histogram.cc\n  histo = histo_ev.histogram_value\n  if not histo.num:\n    return {\n      ""wall_time"": histo_ev.wall_time,\n      ""step"": histo_ev.step,\n      ""compressed_histogram_values"": [{""basis_point"": b, ""value"": 0.0} for b in bps]\n    }\n  bucket = np.array(histo.bucket)\n  weights = (bucket * bps[-1] / (bucket.sum() or 1.0)).cumsum()\n  values = []\n  j = 0\n  while j < len(bps):\n    i = np.searchsorted(weights, bps[j], side=\'right\')\n    while i < len(weights):\n      cumsum = weights[i]\n      cumsum_prev = weights[i - 1] if i > 0 else 0.0\n      if cumsum == cumsum_prev:  # prevent remap divide by zero\n        i += 1\n        continue\n      if not i or not cumsum_prev:\n        lhs = histo.min\n      else:\n        lhs = max(histo.bucket_limit[i - 1], histo.min)\n      rhs = min(histo.bucket_limit[i], histo.max)\n      weight = _Remap(bps[j], cumsum_prev, cumsum, lhs, rhs)\n      values.append({""basis_point"": bps[j], ""value"": weight})\n      j += 1\n      break\n    else:\n      break\n  while j < len(bps):\n    values.append({""basis_point"": bps[j], ""value"": histo.max})\n    j += 1\n  return {""wall_time"": histo_ev.wall_time, ""step"": histo_ev.step, ""compressed_histogram_values"": values}\n\n\ndef _Remap(x, x0, x1, y0, y1):\n  """"""Linearly map from [x0, x1] unto [y0, y1].""""""\n  return y0 + (x - x0) * float(y1 - y0) / (x1 - x0)\n\ndef _ConvertHistogramProtoToTuple(histo):\n  return {\n    ""min"": histo.min,\n    ""max"": histo.max,\n    ""num"": histo.num,\n    ""sum"": histo.sum,\n    ""sum_squares"": histo.sum_squares,\n    ""bucket_limit"": list(histo.bucket_limit),\n    ""bucket"": list(histo.bucket),\n  }\n\n## Normal CDF for std_devs: (-Inf, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, Inf)\n## naturally gives bands around median of width 1 std dev, 2 std dev, 3 std dev,\n## and then the long tail.\nNORMAL_HISTOGRAM_BPS = (0, 668, 1587, 3085, 5000, 6915, 8413, 9332, 10000)\n\ndef _ParseHistogram(tag, wall_time, step, histo):\n  histo = _ConvertHistogramProtoToTuple(histo)\n  histo_ev = {""wall_time"": wall_time, ""step"": step, ""histogram_value"": histo}\n  return [histo_ev, lambda x: _CompressHistogram(x, NORMAL_HISTOGRAM_BPS)]\n\ndef _ParseImage(tag, wall_time, step, image):\n  return {\n    ""wall_time"": wall_time,\n    ""step"": step,\n    ""encoded_image_string"": image.encoded_image_string,\n    ""width"": image.width,\n    ""height"": image.height,\n  }\n\ndef _ParseAudio(tag, wall_time, step, audio):\n  return {\n    ""wall_time"": wall_time,\n    ""step"": step,\n    ""encoded_audio_string"": audio.encoded_audio_string,\n    ""content_type"": audio.content_type,\n    ""sample_rate"": audio.sample_rate,\n    ""length_frames"": audio.length_frames,\n  }\n\ndef _ParseScalar(tag, wall_time, step, scalar):\n  return {\n    ""wall_time"": wall_time,\n    ""step"": step,\n    ""value"": scalar,\n  }\n\ndef _ParseTensor(tag, wall_time, step, tensor):\n  return {\n    ""wall_time"": wall_time,\n    ""step"": step,\n    ""tensor_proto"": tensor,\n  }\n\n## Different types of summary events handled by the event_accumulator\n_SUMMARY_TYPES = {\n  \'simple_value\': _ParseScalar,\n  \'histo\': _ParseHistogram,\n  \'image\': _ParseImage,\n  \'audio\': _ParseAudio,\n  \'tensor\': _ParseTensor,\n}\n\ndef parse(bytes, wall_time, step, visit):\n  summary = summary_pb2.Summary()\n  summary.ParseFromString(bytes)\n\n  for value in summary.value:\n    if value.HasField(\'tensor\') and value.tag == HEALTH_PILL_EVENT_TAG:\n      continue\n\n    for summary_type, summary_func in _SUMMARY_TYPES.items():\n      if value.HasField(summary_type):\n        datum = getattr(value, summary_type)\n        tag = value.node_name if summary_type == \'tensor\' else value.tag\n        parsed = summary_func(tag, wall_time, step, datum)\n        visit(summary_type, parsed)\n'"
core/python/src/nao/tool/tensorboard_server.py,0,"b'import logging as base_logging\nimport os\nimport socket\nimport sys\nfrom werkzeug import serving\n\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import resource_loader\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.tensorboard.backend import application\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\n\ndef make_simple_server(tb_app, host, port):\n  """"""Create an HTTP server for TensorBoard.\n\n  Args:\n    tb_app: The TensorBoard WSGI application to create a server for.\n    host: Indicates the interfaces to bind to (\'::\' or \'0.0.0.0\' for all\n        interfaces, \'::1\' or \'127.0.0.1\' for localhost). A blank value (\'\')\n        indicates protocol-agnostic all interfaces.\n    port: The port to bind to (0 indicates an unused port selected by the\n        operating system).\n  Returns:\n    A tuple of (server, url):\n      server: An HTTP server object configured to host TensorBoard.\n      url: A best guess at a URL where TensorBoard will be accessible once the\n        server has been started.\n  Raises:\n    socket.error: If a server could not be constructed with the host and port\n      specified. Also logs an error message.\n  """"""\n  # Mute the werkzeug logging.\n  base_logging.getLogger(\'werkzeug\').setLevel(base_logging.WARNING)\n\n  try:\n    if host:\n      # The user gave us an explicit host\n      server = serving.make_server(host, port, tb_app, threaded=True)\n      if \':\' in host and not host.startswith(\'[\'):\n        # Display IPv6 addresses as [::1]:80 rather than ::1:80\n        final_host = \'[{}]\'.format(host)\n      else:\n        final_host = host\n    else:\n      # We\'ve promised to bind to all interfaces on this host. However, we\'re\n      # not sure whether that means IPv4 or IPv6 interfaces.\n      try:\n        # First try passing in a blank host (meaning all interfaces). This,\n        # unfortunately, defaults to IPv4 even if no IPv4 interface is available\n        # (yielding a socket.error).\n        server = serving.make_server(host, port, tb_app, threaded=True)\n      except socket.error:\n        # If a blank host didn\'t work, we explicitly request IPv6 interfaces.\n        server = serving.make_server(\'::\', port, tb_app, threaded=True)\n      final_host = socket.gethostname()\n    server.daemon_threads = True\n  except socket.error as socket_error:\n    if port == 0:\n      msg = \'TensorBoard unable to find any open port\'\n    else:\n      msg = (\n          \'TensorBoard attempted to bind to port %d, but it was already in use\'\n          % port)\n    logging.error(msg)\n    print(msg)\n    raise socket_error\n\n  final_port = server.socket.getsockname()[1]\n  tensorboard_url = \'http://%s:%d\' % (final_host, final_port)\n  return server, tensorboard_url\n\n\ndef run_simple_server(tb_app, host, port):\n  """"""Run a TensorBoard HTTP server, and print some messages to the console.""""""\n  try:\n    server, url = make_simple_server(tb_app, host, port)\n  except socket.error:\n    # An error message was already logged\n    exit(-1)\n  msg = \'Starting TensorBoard %s at %s\' % (tb_app.tag, url)\n  print(msg)\n  logging.info(msg)\n  print(\'(Press CTRL+C to quit)\')\n  sys.stdout.flush()\n\n  server.serve_forever()\n\ndef main(tb_logdir, tb_host=None, tb_port=None, tb_debug=True, tb_purge_orphaned_data=False, tb_reload_interval=5):\n  if tb_host is None:\n    tb_host = \'127.0.0.1\'\n\n  if tb_port is None:\n    tb_port = 6006\n\n  tb_port = int(tb_port or \'6006\')\n\n  tb = application.standard_tensorboard_wsgi(\n      logdir=tb_logdir,\n      purge_orphaned_data=tb_purge_orphaned_data,\n      reload_interval=tb_reload_interval)\n\n  run_simple_server(tb, host=tb_host, port=tb_port)\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/initscripts/Console.py,0,"b'#------------------------------------------------------------------------------\n# Console.py\n#   Initialization script for cx_Freeze. Sets the attribute sys.frozen so that\n# modules that expect it behave as they should.\n#------------------------------------------------------------------------------\n\nimport os\nimport sys\nimport zipimport\n\nsys.frozen = True\n\nFILE_NAME = sys.executable\nDIR_NAME = os.path.dirname(sys.executable)\n\nos.environ[""TCL_LIBRARY""] = os.path.join(DIR_NAME, ""tcl"")\nos.environ[""TK_LIBRARY""] = os.path.join(DIR_NAME, ""tk"")\n\nm = __import__(""__main__"")\nimporter = zipimport.zipimporter(os.path.dirname(os.__file__))\nname, ext = os.path.splitext(os.path.basename(os.path.normcase(FILE_NAME)))\nmoduleName = ""%s__main__"" % name\ncode = importer.get_code(moduleName)\nexec(code, m.__dict__)\n\nversionInfo = sys.version_info[:3]\nif versionInfo >= (2, 5, 0) and versionInfo <= (2, 6, 4):\n    module = sys.modules.get(""threading"")\n    if module is not None:\n        module._shutdown()\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/initscripts/ConsoleSetLibPath.py,0,"b'#------------------------------------------------------------------------------\n# ConsoleSetLibPath.py\n#   Initialization script for cx_Freeze which manipulates the path so that the\n# directory in which the executable is found is searched for extensions but\n# no other directory is searched. The environment variable LD_LIBRARY_PATH is\n# manipulated first, however, to ensure that shared libraries found in the\n# target directory are found. This requires a restart of the executable because\n# the environment variable LD_LIBRARY_PATH is only checked at startup.\n#------------------------------------------------------------------------------\n\nimport os\nimport sys\nimport zipimport\n\nFILE_NAME = sys.executable\nDIR_NAME = os.path.dirname(sys.executable)\n\npaths = os.environ.get(""LD_LIBRARY_PATH"", """").split(os.pathsep)\n\nif DIR_NAME not in paths:\n    paths.insert(0, DIR_NAME)\n    os.environ[""LD_LIBRARY_PATH""] = os.pathsep.join(paths)\n    os.execv(sys.executable, sys.argv)\n\nsys.frozen = True\nsys.path = sys.path[:4]\n\nos.environ[""TCL_LIBRARY""] = os.path.join(DIR_NAME, ""tcl"")\nos.environ[""TK_LIBRARY""] = os.path.join(DIR_NAME, ""tk"")\n\nm = __import__(""__main__"")\nimporter = zipimport.zipimporter(os.path.dirname(os.__file__))\nname, ext = os.path.splitext(os.path.basename(os.path.normcase(FILE_NAME)))\nmoduleName = ""%s__main__"" % name\ncode = importer.get_code(moduleName)\nexec(code, m.__dict__)\n\nversionInfo = sys.version_info[:3]\nif versionInfo >= (2, 5, 0) and versionInfo <= (2, 6, 4):\n    module = sys.modules.get(""threading"")\n    if module is not None:\n        module._shutdown()\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/initscripts/SharedLib.py,0,"b'#------------------------------------------------------------------------------\n# SharedLib.py\n#   Initialization script for cx_Freeze which behaves similarly to the one for\n# console based applications but must handle the case where Python has already\n# been initialized and another DLL of this kind has been loaded. As such it\n# does not block the path unless sys.frozen is not already set.\n#------------------------------------------------------------------------------\n\nimport encodings\nimport os\nimport sys\nimport warnings\n\nDIR_NAME = os.path.dirname(sys.executable)\n\nif not hasattr(sys, ""frozen""):\n    sys.frozen = True\n    sys.path = sys.path[:4]\n\nos.environ[""TCL_LIBRARY""] = os.path.join(DIR_NAME, ""tcl"")\nos.environ[""TK_LIBRARY""] = os.path.join(DIR_NAME, ""tk"")\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/initscripts/SharedLibSource.py,0,"b'#------------------------------------------------------------------------------\n# SharedLibSource.py\n#   Initialization script for cx_Freeze which imports the site module (as per\n# normal processing of a Python script) and then searches for a file with the\n# same name as the shared library but with the extension .pth. The entries in\n# this file are used to modify the path to use for subsequent imports.\n#------------------------------------------------------------------------------\n\nimport os\nimport sys\nimport warnings\n\n# the site module must be imported for normal behavior to take place; it is\n# done dynamically so that cx_Freeze will not add all modules referenced by\n# the site module to the frozen executable\n__import__(""site"")\n\nFILE_NAME = sys.executable\n\n# now locate the pth file to modify the path appropriately\nbaseName, ext = os.path.splitext(FILE_NAME)\npathFileName = baseName + "".pth""\nsys.path = [s.strip() for s in open(pathFileName).read().splitlines()] + \\\n        sys.path\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/initscripts/__startup__.py,0,"b'#------------------------------------------------------------------------------\n# __startup__.py\n#   This is the first script that is run when cx_Freeze starts up. It simply\n# determines the name of the initscript that is to be executed.\n#------------------------------------------------------------------------------\n\nimport os\nimport sys\n\nbaseName = os.path.normcase(os.path.basename(sys.executable))\nname, ext = os.path.splitext(baseName)\n__import__(name + ""__init__"")\n\n'"
core/python/src/nao/compiler/asset/__init__.py,0,b''
core/python/src/nao/compiler/asset/compiler.py,3,"b'import tensorflow as tf\n\nfrom nao.compiler.asset import graph_assets\n\n_ASSETS = []\n\ndef finish():\n  asset_map = graph_assets.consolidate_to_asset_map(_ASSETS)\n  graph_assets.store_asset_map(asset_map)\n\ndef asset_map():\n  return graph_assets.consolidate_to_asset_map(_ASSETS)\n\ndef make_compile_fn(workspace, import_path, tags):\n  if not tags.get(""asset"", False):\n    return None\n\n  url = tags[""url""]\n  sha256 = tags.get(""sha256"", None)\n  asset_path = workspace.find_asset_path(import_path)\n\n  def maybe_download(path, url):\n    print(""maybe_download"", ""path"", path, type(path), ""url"", url, type(url))\n\n    return graph_assets.maybe_download(\n        path.decode(\'utf-8\'),\n        url.decode(\'utf-8\')).encode(\'utf-8\')\n\n  def compile(resolved_imports, previous):\n    placeholder_name = import_path.split(""/"")[-1].replace(""-"", ""_"").replace(""."", ""_"")\n    # placeholder = tf.placeholder(tf.string, tf.TensorShape([]), placeholder_name)\n    placeholder = tf.py_func(\n        maybe_download,\n        [asset_path, url],\n        tf.string,\n        name=placeholder_name)\n\n    _ASSETS.append(\n      {\n        ""placeholder"": placeholder.name,\n        ""name"": import_path,\n        ""url"": url,\n        ""sha256"": sha256,\n      }\n    )\n    return placeholder\n\n  return ([], compile)\n'"
core/python/src/nao/compiler/asset/graph_assets.py,0,"b'import tensorflow as tf\n\nfrom six.moves import urllib\n\nfrom os import path\nfrom tensorflow.python.platform import gfile\n\nfrom nao.compiler.asset.retry import retry\nfrom nao.structure import graph_constants\n\n_ASSET_MAP_JSON_KEY = ""asset_map_json""\n\n# Returns {""asset_name"": {""url"", ""digest""}}\ndef load_asset_map(graph):\n  asset_map = graph_constants.load_json(graph, _ASSET_MAP_JSON_KEY)\n  if asset_map is None:\n    return {}\n  return asset_map\n\n# asset_map = {""asset_name"": {""url"", ""digest""}}\ndef store_asset_map(asset_map):\n  return graph_constants.store_json(_ASSET_MAP_JSON_KEY, asset_map)\n\n# assets = [{""name"", ""digest"", ""url""}]\n# Raise if two assets have the same name but different urls or different digests\ndef consolidate_to_asset_map(assets):\n  result = {} # ""name"" -> asset\n  collisions = {} # {""name"" -> [asset]}\n  for asset in assets:\n    name = asset[""name""]\n    if name in result:\n      collisions[name] = [result[name]]\n      del result[name]\n    elif name in collisions:\n      collisions[name].append(asset)\n    else:\n      result[name] = asset\n\n  if len(collisions) > 0:\n    raise Exception(""Found asset collisions: %s"" % collisions)\n\n  return result\n\n\n_RETRIABLE_ERRNOS = {\n    110,  # Connection timed out [socket.py]\n}\n\n\ndef _is_retriable(e):\n  return isinstance(e, IOError) and e.errno in _RETRIABLE_ERRNOS\n\n@retry(initial_delay=1.0, max_delay=16.0, is_retriable=_is_retriable)\ndef _urlretrieve_with_retry(url, filename=None):\n  return urllib.request.urlretrieve(url, filename)\n\ndef maybe_download(filepath, source_url):\n  """"""Download the data from source url, unless it\'s already here.\n  Args:\n      basename: string, name of the file in the directory.\n      target_dir: string, path to working directory.\n      source_url: url to download from if file doesn\'t exist.\n  Returns:\n      Path to resulting file.\n  """"""\n  target_dir = path.dirname(filepath)\n  if not gfile.Exists(target_dir):\n    gfile.MakeDirs(target_dir)\n  if not gfile.Exists(filepath):\n    print(\'Downloading\', source_url, \'to\', filepath)\n    temp_file_name, _ = _urlretrieve_with_retry(source_url)\n    gfile.Copy(temp_file_name, filepath)\n    with gfile.GFile(filepath) as f:\n      size = f.size()\n    print(\'Successfully downloaded\', filepath, size, \'bytes.\')\n  return filepath\n'"
core/python/src/nao/compiler/asset/retry.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base utilities for loading datasets.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\nimport time\n\nfrom os import path\n\ndef retry(initial_delay,\n          max_delay,\n          factor=2.0,\n          jitter=0.25,\n          is_retriable=None):\n  """"""Simple decorator for wrapping retriable functions.\n  Args:\n    initial_delay: the initial delay.\n    factor: each subsequent retry, the delay is multiplied by this value.\n        (must be >= 1).\n    jitter: to avoid lockstep, the returned delay is multiplied by a random\n        number between (1-jitter) and (1+jitter). To add a 20% jitter, set\n        jitter = 0.2. Must be < 1.\n    max_delay: the maximum delay allowed (actual max is\n        max_delay * (1 + jitter).\n    is_retriable: (optional) a function that takes an Exception as an argument\n        and returns true if retry should be applied.\n  """"""\n  if factor < 1:\n    raise ValueError(\'factor must be >= 1; was %f\' % (factor,))\n\n  if jitter >= 1:\n    raise ValueError(\'jitter must be < 1; was %f\' % (jitter,))\n\n  # Generator to compute the individual delays\n  def delays():\n    delay = initial_delay\n    while delay <= max_delay:\n      yield delay * random.uniform(1 - jitter,  1 + jitter)\n      delay *= factor\n\n  def wrap(fn):\n    """"""Wrapper function factory invoked by decorator magic.""""""\n\n    def wrapped_fn(*args, **kwargs):\n      """"""The actual wrapper function that applies the retry logic.""""""\n      for delay in delays():\n        try:\n          return fn(*args, **kwargs)\n        except Exception as e:  # pylint: disable=broad-except)\n          if is_retriable is None:\n            continue\n\n          if is_retriable(e):\n            time.sleep(delay)\n          else:\n            raise\n      return fn(*args, **kwargs)\n    return wrapped_fn\n  return wrap\n'"
core/python/src/nao/compiler/metagraph_pbtxt/__init__.py,0,b''
core/python/src/nao/compiler/metagraph_pbtxt/compiler.py,11,"b'import tensorflow as tf\n\nimport re\nimport sys\nimport tensorflow as tf\n\nfrom tensorflow.contrib.graph_editor import make_view\nimport tensorflow.contrib.graph_editor.transform as transform\n\nfrom nao.structure import graph_io\n\nfrom nao.compiler.retvalbag import RetvalBag\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\nclass SubGraphViewFunction:\n  def __init__(self, name, sgv, source_scope, inputs, output_names):\n    eprint(""sgv.graph"", sgv.graph)\n    self._nam = name\n    self._inputs = inputs\n    self._src_scope = source_scope\n    self._output_names = output_names\n    self._sgv = sgv\n\n  def _name(self):\n    return self._nam\n\n  def apply(self, visitor, ctx, name, attrs, args):\n    # We\'ll replace the inputs with the given args and copy all operations\n    # (other than vars, which we leave as is) between outputs and inputs.\n\n    g = tf.get_default_graph()\n    full_scope = g.unique_name(self._name() or ""sgv"", False)\n    scope_name = full_scope.split(""/"")[-1]\n    eprint(""full_scope"", full_scope)\n    eprint(""scope_name"", scope_name)\n    replacements_ts = dict(zip(self._inputs, args))\n    eprint(""replacements_ts"", replacements_ts)\n\n    with tf.name_scope(None):\n      self._copy_with_input_replacements(replacements_ts, full_scope)\n\n    output_items = [(n, ""%s/outputs/%s"" % (full_scope, n)) for n in self._output_names]\n    eprint(""output_items"", output_items)\n    try:\n      return RetvalBag(dict([(n, g.get_operation_by_name(p).outputs[0]) for n, p in output_items]))\n    except KeyError as e:\n      nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n      nodes.sort()\n      eprint(\'error, but got nodes\', nodes)\n      raise e\n\n  def _copy_with_input_replacements(self, replacement_ts, dst_scope):\n    """"""Copy a subgraph, replacing some of its inputs.\n    Note a replacement only happens if the tensor to be replaced\n    is an input of the given subgraph. The inputs of a subgraph can\n    be queried using sgv.inputs.\n    Args:\n      sgv: the source subgraph-view. This argument is converted to a subgraph\n        using the same rules as the function subgraph.make_view.\n      replacement_ts: dictionary mapping from original tensors to the\n        replaced one.\n      dst_scope: the destination scope.\n      reuse_dst_scope: if True the dst_scope is re-used if it already exists.\n        Otherwise, the scope is given a unique name based on the one given\n        by appending an underscore followed by a digit (default).\n    Raises:\n      TypeError: if dst_graph is not a tf.Graph.\n      StandardError: if sgv cannot be converted to a SubGraphView using\n        the same rules as the function subgraph.make_view.\n    """"""\n    copier = transform.Transformer()\n    # Replace tensor if possible.\n    def replace_t_with_replacement_handler(info, t):\n      if t in replacement_ts:\n        eprint(""Did find"", t, ""in"", replacement_ts)\n        return replacement_ts[t]\n      else:\n        eprint(""Did not find"", t, ""in"", replacement_ts)\n        return transform.keep_t_if_possible_handler(info, t)\n    copier.transform_external_input_handler = replace_t_with_replacement_handler\n\n    orig_transform_op = copier.transform_op_handler\n    def transform_op(info, op, copy_shape=True):\n      eprint(""transform_op"", op.name)\n      if isinstance(op, tf.Variable):\n        eprint(""Won\'t copy variable"", op)\n        return op\n\n      return orig_transform_op(info, op, copy_shape)\n    copier.transform_op_handler = transform_op\n    dst_graph = tf.get_default_graph()\n    eprint(""dst_graph"", dst_graph)\n\n    return copier(\n        self._sgv, dst_graph, dst_scope, self._src_scope, reuse_dst_scope=False)\n\nclass MetaGraphDefPackage:\n  def __init__(self, meta_graph_def, import_path, internal_scope):\n    eprint(""import_path, internal_scope"", import_path, internal_scope)\n\n    internal_scope = internal_scope or import_path\n\n    self._meta_graph_def = meta_graph_def\n    self._internal_scope = internal_scope\n\n    import_scope = import_path\n    tensors = []\n    functions = {} # name -> (scope, inputs, outputs)\n    pattern = re.compile(""%s/([^/]+)(?:$|(/inputs/|/outputs/).)"" % internal_scope)\n    graph_def = meta_graph_def.graph_def\n    for n in graph_def.node:\n      m = pattern.match(n.name)\n      if not m:\n        continue\n\n      name = m.group(1)\n      function_component = m.group(2)\n      if not function_component:\n        tensors.append(n.name)\n        continue\n\n      if name not in functions:\n        function = (name, [], [])\n        functions[name] = function\n      else:\n        function = functions[name]\n\n      if function_component.startswith(""/inputs/""):\n        function[1].append(n.name)\n      else:\n        output_prefix_len = len(internal_scope) + len(name) + len(function_component) + 1\n        function[2].append(n.name[output_prefix_len:])\n\n    try:\n      with tf.name_scope(None):\n        tf.train.import_meta_graph(meta_graph_def, import_scope=import_path)\n    except KeyError as e:\n      nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n      nodes.sort()\n      eprint(\'error, but got nodes\', nodes)\n      raise e\n\n    nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n    nodes.sort()\n    eprint(\'no error, but got nodes\', nodes)\n\n    g = tf.get_default_graph()\n    exports = {} # name -> tensor | function\n    for name in tensors:\n      eprint(""tensor"", name, ""%s/%s"" % (import_scope, name))\n      exports[name] = g.get_operation_by_name(""%s/%s"" % (import_scope, name))\n\n    for name, (scope, full_input_names, output_names) in functions.items():\n      # inputs = [g.get_tensor_by_name(""%s/%s:0"" % (import_scope, full_input_name)) for full_input_name in full_input_names]\n      inputs = [g.get_tensor_by_name(""%s/%s:0"" % (import_scope, full_input_name)) for full_input_name in full_input_names]\n      source_scope = ""%s/%s/%s"" % (import_path, internal_scope, scope)\n      eprint(""sgv_scope"", ""%s/%s/%s"" % (import_path, internal_scope, scope), g)\n      source_pattern = ""%s/(?:_|outputs)/.*"" % source_scope\n      sgv = make_view(source_pattern, graph=g)\n      eprint(""sgv.inputs"", list(sgv.inputs))\n      exports[name] = SubGraphViewFunction(name, sgv, source_scope, inputs, output_names)\n\n    self._exports = exports\n\n  def apply(self, visitor, ctx, name, attrs, args):\n    n, *_ = args\n\n    try:\n      return self._exports[n]\n    except KeyError as e:\n      eprint(""only have exports"", list(self._exports.keys()))\n      raise e\n\ndef make_compile_fn(workspace, import_path, tags):\n  basename, scope_name = (import_path + "":"").split("":"", 1)\n  scope_name = scope_name[:-1]\n\n  filepath = workspace.find_pkg_path(basename + "".metagraph.pbtxt"")\n  binary = False\n  if filepath is None:\n    return None\n\n  def compile(resolved_imports, previous):\n    eprint(""_sf_tf_metagraph_package"", import_path, scope_name)\n    # TODO(adamb) how do we handle the fact that there may be multiple packages\n    #     within the given file. Should we only parse out the one we want?\n    meta_graph_def = graph_io.read_meta_graph_def(filepath, binary)\n    return MetaGraphDefPackage(meta_graph_def, basename, scope_name)\n\n  return ([], compile)\n'"
core/python/src/nao/compiler/nao/__init__.py,0,b''
core/python/src/nao/compiler/nao/compiler.py,0,"b'import pprint\nimport sys\nimport pkgutil\n\nimport tensorflow as tf\n\nfrom py_mini_racer import py_mini_racer\n\nfrom nao.compiler.nao import graph_gen\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\npp = pprint.PrettyPrinter(indent=2, stream=sys.stderr).pprint\n\n_js_ctx = py_mini_racer.MiniRacer()\n_js_ctx.eval(pkgutil.get_data(""nao_parser"", ""parse.js""))\n\ndef _parse_import_tag(import_tag):\n  if not import_tag:\n    return None\n\n  def split_tag(s):\n    result = s.split("":"", 1)\n    if len(result) == 1:\n      result = [result[0], True]\n    return result\n\n  return dict([split_tag(frag) for frag in import_tag.split("","")])\n\ndef _enumerate_imports(exprs):\n  imported = []\n  for expr in exprs:\n    if expr[0] != ""_sf_import"":\n      continue\n\n    for import_name, import_path, import_tag in expr[1]:\n      imported.append((import_path, _parse_import_tag(import_tag)))\n\n  return imported\n\ndef make_compile_fn(workspace, import_path, tags):\n  source = workspace.read_src(import_path + "".nao"")\n  if source is None:\n    return None\n\n  exprs = _js_ctx.call(""parse.parseExpressions"", source)\n  # pp(exprs)\n\n  imported = []\n  for imported_path, imported_tags in _enumerate_imports(exprs):\n    # Skip imports that provide direct access to TensorFlow internals.\n    if imported_path.startswith(""tensorflow:""):\n      continue\n\n    if imported_tags and imported_tags.get(""asset"", False):\n      imported_tags[""url""] = imported_path\n      imported_path = imported_path.split(""://"", 1)[1]\n\n    imported.append((imported_path, imported_tags))\n\n  def compile(resolved_imports, previous):\n    return graph_gen.TopLevel()._sf_package(resolved_imports, previous, import_path, *exprs)\n\n  return (imported, compile)\n'"
core/python/src/nao/compiler/nao/graph_context.py,13,"b'import copy\nimport sys\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import state_ops\n\nfrom collections import OrderedDict\n\nfrom nao.compiler.primitive_function import PrimitiveFunction\nfrom nao.compiler.retvalbag import RetvalBag\nfrom nao.compiler.nao import graph_function\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\nclass SentinelContextDelegate:\n  def __init__(self):\n    self._delegate = None\n\n  def local_items(self):\n    return []\n\n  def attr_items(self):\n    return []\n\n  def call(self, fn, args, kwargs):\n    try:\n      return fn(*args, **kwargs)\n    except:\n      raise Exception(""Tried to call %s with args %s and kwargs %s""  % (fn, args, kwargs))\n\n  def get_index(self, target, index):\n    # eprint(""get_index"", target, index, type(index))\n\n    if type(target) == RetvalBag:\n      return target.get(index)\n\n    if isinstance(index, int):\n      if hasattr(target, \'__getitem__\'):\n        return target[index]\n\n    # HACK(adamb) This will return an wrapped python function.\n    # We\'re using it at the moment to get access to op-producing\n    # instance methods like those on QueueBase. 8[\n    v = getattr(target, index)\n    if callable(v):\n      return PrimitiveFunction(v)\n\n    return v\n\n  def fully_qualified_package(self, name):\n    raise Exception(""Package not available: %s"" % name)\n\n  def imported_package(self, name):\n    raise Exception(""Package not imported: %s"" % name)\n\n  def get_attr(self, name):\n    raise Exception(""No such attribute: %s"" % name)\n\n  def update_local(self, name, rhs):\n    raise Exception(""No such local or function: %s"" % name)\n\n  def get_local(self, name):\n    raise Exception(""No such local or function: %s"" % name)\n\n  def eliminate_leaf(self, op):\n    pass\n\n  def duplicate_for(self, other):\n    return other.duplicate()\n\n  def leaves(self):\n    return frozenset([])\n\nclass Context:\n  def __init__(self, delegate, proxy=None):\n    self._proxy = proxy\n    self._delegate = delegate\n    self._fully_qualified_packages = {}\n    self._imported_packages = {}\n    self._wrap_locals_in_vars = False\n    self._attrs = {}\n    self._locals = {}\n    self._leaves = set()\n    self._above = None\n\n  def wrap_locals_in_vars(self):\n    self._wrap_locals_in_vars = True\n\n  def duplicate_for(self, other):\n    ctx = other.duplicate()\n    ctx._proxy = self._proxy\n    return ctx\n\n  def call(self, fn, args, kwargs):\n    return self._maybe_proxy(self._delegate.call(fn, args, kwargs))\n\n  def get_index(self, target, index):\n    return self._maybe_proxy(self._delegate.get_index(target, index))\n\n  def subcontext(self):\n    return Context(self, proxy=self._proxy)\n\n  def local_items(self):\n    l = self._delegate.local_items()\n    l.extend(self._locals.items())\n    return l\n\n  def attr_items(self):\n    l = self._delegate.attr_items()\n    l.extend(self._attrs.items())\n    return l\n\n  def resolve_fully_qualified_package(self, name):\n    if name in self._fully_qualified_packages:\n      return self._fully_qualified_packages[name]\n\n    subctx = self.subcontext()\n    pkg = graph_function.Package(subctx)\n    self.define_fully_qualified_package(name, pkg)\n    return pkg\n\n  def define_fully_qualified_package(self, name, pkg):\n    if name in self._fully_qualified_packages:\n      raise Exception(""Already defined package: %s"" % name)\n\n    eprint(""Defining package"", name)\n    self._fully_qualified_packages[name] = pkg\n\n  def fully_qualified_package(self, name):\n    if name in self._fully_qualified_packages:\n      return self._fully_qualified_packages[name]\n\n    return self._delegate.fully_qualified_package(name)\n\n  def import_package(self, name, pkg):\n    if name in self._imported_packages:\n      raise Exception(""Already imported package: %s"" % name)\n\n    eprint(""Importing package"", name)\n    self._imported_packages[name] = pkg\n\n  def imported_package(self, name):\n    if name in self._imported_packages:\n      return self._imported_packages[name]\n\n    return self._delegate.imported_package(name)\n\n  def duplicate(self):\n    ctx = copy.copy(self)\n    ctx._attrs = copy.copy(ctx._attrs)\n    ctx._locals = copy.copy(ctx._locals)\n    ctx._leaves = copy.copy(ctx._leaves)\n    return ctx\n\n  def get_above(self):\n    return self._maybe_proxy(self._above)\n\n  def set_above(self, value):\n    self._above = value\n\n  def _maybe_proxy(self, v):\n    if self._proxy is None:\n      return v\n\n    return self._proxy(v)\n\n  def update_local(self, name, rhs):\n    if name in self._locals:\n      v = self._locals[name]\n      if not isinstance(v, tf.Variable) and not (isinstance(v, tf.Tensor) and v.dtype._is_ref_dtype):\n        raise Exception(""%s not a variable: %s"" % (name, v))\n      eprint(""updating local"", name, ""from"", v, ""to"", rhs)\n      v = tf.assign(v, rhs)\n      eprint(""updated local"", name, ""is"", v)\n      self._locals[name] = v\n      return v\n\n    return self._delegate.update_local(name, rhs)\n\n  def define_local(self, name, value):\n    if name in self._locals:\n      raise Exception(""Local already defined: %s"" % name)\n\n    should_wrap_in_var = False\n    if self._wrap_locals_in_vars:\n      if isinstance(value, tf.Tensor):\n        should_wrap_in_var = True\n\n      # HACK(adamb) Unwrapping in here really isn\'t great, since auto-unwrapping can create unexpected behavior.\n      if isinstance(value, RetvalBag) and value.len() == 1:\n        if isinstance(value.get(None), tf.Tensor):\n          should_wrap_in_var = True\n          value = value.get(None)\n\n    if should_wrap_in_var:\n      variable = state_ops.variable_op_v2(\n          value.get_shape(),\n          value.dtype.base_dtype)\n\n      with tf.control_dependencies(None):\n        value = tf.identity(\n          tf.cond(\n            tf.is_variable_initialized(variable),\n            lambda: variable,\n            lambda: tf.assign(variable, value)\n          )\n        )\n      print(""value"", value)\n\n    self._locals[name] = value\n    return value\n\n  def has_attr(self, name):\n    return name in self._attrs\n\n  def define_attr(self, name, value):\n    if self.has_attr(name):\n      raise Exception(""Attribute already defined: %s"" % name)\n\n    if name in self._locals:\n      raise Exception(""Can\'t define attribute. Local exists with name: %s"" % name)\n\n    self._attrs[name] = value\n\n  def get_attr(self, name):\n    if name in self._attrs:\n      return self._maybe_proxy(self._attrs[name])\n\n    # If a local is *completely constant*, we can treat it like an attr.\n    if name in self._locals:\n      local = self._locals[name]\n      if isinstance(local, tf.Tensor):\n        try:\n          return self._maybe_proxy(tf.contrib.util.constant_value(local))\n        except TypeError as te:\n          raise Exception(""Local is not usable as an attribute: "" + name)\n      return self._maybe_proxy(local)\n\n    return self._maybe_proxy(self._delegate.get_attr(name))\n\n  def get_local(self, name):\n    if type(name) != str:\n      raise Exception(""Tried to look up local with non-string name: %s"" % name)\n\n    if name == \'^\':\n      return self._maybe_proxy(self._above)\n\n    if name in self._locals:\n      return self._maybe_proxy(self._locals[name])\n\n    if name in self._attrs:\n      return self._maybe_proxy(self._attrs[name])\n\n    if name in self._imported_packages:\n      return self._maybe_proxy(self._imported_packages[name])\n\n    return self._maybe_proxy(self._delegate.get_local(name))\n\n  def get_local_strict(self, name):\n    if name in self._locals:\n      return self._maybe_proxy(self._locals[name])\n\n    raise Exception(""No such entry: %s. Have: %s"" % (name, self._locals))\n\n  def possible_leaf(self, v):\n    if isinstance(v, (tf.Tensor, tf.Operation, tf.Variable)):\n      self._leaves.add(v)\n\n    if isinstance(v, RetvalBag):\n      for v_ in v.values():\n        self.possible_leaf(v_)\n\n  def eliminate_leaf(self, v):\n    if isinstance(v, (tf.Tensor, tf.Operation, tf.Variable)):\n      self._leaves.discard(v)\n\n    if isinstance(v, RetvalBag):\n      for v_ in v.values():\n        self.eliminate_leaf(v_)\n        return\n\n    return self._delegate.eliminate_leaf(v)\n\n  def leaves(self):\n    l = frozenset(self._leaves)\n    # if self._delegate:\n    #   l = l | self._delegate.leaves()\n    return l\n\n  def __str__(self):\n    return ""%s -> %s"" % (self._locals.items(), self._delegate)\n'"
core/python/src/nao/compiler/nao/graph_function.py,11,"b'import inspect\nimport re\nimport sys\n\nimport tensorflow as tf\nfrom tensorflow.contrib.graph_editor import make_view\nimport tensorflow.contrib.graph_editor.transform as transform\n\nfrom nao.compiler.retvalbag import RetvalBag\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\nclass Package:\n  def __init__(self, ctx):\n    self._ctx = ctx\n\n  def ctx(self):\n    return self._ctx\n\n  def apply(self, visitor, ctx, name, attrs, args):\n    n, *_ = args\n    if self._ctx.has_attr(n):\n      value = self._ctx.get_attr(n)\n    else:\n      value = self._ctx.get_local_strict(n)\n\n    if not n[0].isupper():\n      raise Exception(""Tried to use non-exported value named: %s"" % n)\n\n    if name and isinstance(value, tf.Tensor):\n      return tf.identity(value, name=name)\n\n    return value\n\n# HACK(adamb) We need to be able to pass in our own custom instances of\n\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.framework import ops\n\nclass _TensorRefWrapper(optimizer._OptimizableVariable):\n  def __init__(self, ref):\n    self._ref = ref\n\n  def target(self):\n    return self._ref\n\n  def update_op(self, optimizer, g):\n    if isinstance(g, ops.Tensor):\n      return optimizer._apply_dense(g, self._ref)  # pylint: disable=protected-access\n    else:\n      assert isinstance(g, ops.IndexedSlices), (""Gradient "", g, "" is neither a ""\n                                                ""tensor nor IndexedSlices."")\n      # pylint: disable=protected-access\n      return optimizer._apply_sparse_duplicate_indices(g, self._ref)\n\nprev_get_processor = optimizer._get_processor\ndef _get_processor(v):\n  if isinstance(v, tf.Tensor):\n    return _TensorRefWrapper(v)\n  return prev_get_processor(v)\noptimizer._get_processor = _get_processor\n\nclass TransformedFunction:\n  def __init__(self, name, fn, macro):\n    self._nam = name\n    self._fn = fn\n    self._macro = macro\n\n  def _name(self):\n    return self._nam\n\n  def _apply(self, impl, visitor):\n    vars = []\n    def var_reference(var):\n      vars.append(var)\n\n    visitor.add_variable_listener(var_reference)\n    try:\n      retvals = impl()\n    finally:\n      visitor.remove_variable_listener(var_reference)\n\n    output = retvals.get(None)\n\n    macro_attrs = {\n      ""output"": output,\n      ""trainable"": vars,\n    }\n\n    # eprint(""macro_attrs"", macro_attrs)\n\n    out = self._macro.apply_attrs(visitor, macro_attrs)\n\n    with tf.control_dependencies([out.get(None)]):\n      return tf.identity(output)\n\n\n  def apply_kw(self, visitor, ctx, scope_name, attrs, kwargs):\n    def _apply_kw():\n      return self._fn.apply_kw(visitor, ctx, scope_name, None, kwargs)\n\n    return self._apply(_apply_kw, visitor)\n\n  def apply(self, visitor, ctx, scope_name, attrs, args):\n    def _apply():\n      return self._fn.apply(visitor, ctx, scope_name, None, args)\n\n    return self._apply(_apply, visitor)\n\nclass DeclaredMacro:\n  def __init__(self, ctx, expr):\n    self._ctx = ctx\n    self._expr = expr\n\n  def _name(self):\n    return self._expr[0]\n\n  def _attr_specs(self):\n    return self._expr[1];\n\n  def _retval_specs(self):\n    return self._expr[2]\n\n  def _retval_argnames(self):\n    return [name for (_, name) in self._retval_specs()]\n\n  def _body(self):\n    return self._expr[3:]\n\n  def _do_macro_apply(self, visitor, ctx):\n    returned = {}\n\n    g = tf.get_default_graph()\n    scope_name = g.unique_name(self._name() or ""macro"", False).split(""/"")[-1]\n\n    with tf.variable_scope(scope_name):\n      # Need to visit expressions\n      visitor._visit_exprs(ctx, self._body())\n\n    for retval_name, retval_argname in self._retval_specs():\n      returned[retval_name] = ctx.get_local(retval_argname)\n\n    # eprint(""returned"", returned)\n\n    return RetvalBag(returned)\n\n  # Update given attrs and return a new function.\n  # (add separate data structure for tracking pre-specified and now unoverrideable values).\n  def apply_attrs(self, visitor, attrs):\n    ctx = self._ctx.duplicate()\n\n    has_ellipsis = False\n    for name, value in attrs.items():\n      if name == \'_ellipsis\':\n        has_ellipsis = True\n        continue\n      ctx.define_attr(name, value)\n\n    missing_attributes = []\n    for name, _, _ in self._attr_specs():\n      if name not in attrs and not ctx.has_attr(name):\n        missing_attributes.append(name)\n\n    if has_ellipsis:\n      if len(missing_attributes) == 0:\n        raise Exception(""Saw ... given, but not missing attributes"")\n\n      return DeclaredMacro(ctx, self._expr)\n    else:\n      if len(missing_attributes) > 0:\n        raise Exception(""No ... given and missing attributes: %s"" % missing_attributes)\n\n      return self._do_macro_apply(visitor, ctx)\n\n\n  # If we see syntax like: foo(a: ?, b: ?) then it\'s a partial application.\n  # For these, bind the values we have return a new function where these values are unoverrideable.\n  def apply_partial():\n    pass\n\n\nclass DeclaredFunction:\n  def __init__(self, ctx, expr):\n    self._ctx = ctx\n    self._expr = expr\n\n  def clone(self):\n    return DeclaredFunction(self._ctx, self._expr)\n\n  def rename(self, name):\n    self._expr[0] = name\n\n  def _name(self):\n    return self._expr[0]\n\n  def _attr_specs(self):\n    return self._expr[1];\n\n  def _arg_specs(self):\n    return self._expr[2]\n\n  def _arg_names(self):\n    return [name for (name, shape, dtype) in self._arg_specs()]\n\n  def _retval_specs(self):\n    return self._expr[3]\n\n  def _retval_argnames(self):\n    return [name for (_, name) in self._retval_specs()]\n\n  def _body(self):\n    return self._expr[4:]\n\n  def has_attrs(self):\n    attr_specs = self._attr_specs()\n    return attr_specs is not None and len(attr_specs) > 0\n\n  # Update given attrs and return a new function.\n  # (add separate data structure for tracking pre-specified and now unoverrideable values).\n  def apply_attrs(self, visitor, attrs):\n    ctx = self._ctx.duplicate()\n\n    has_ellipsis = False\n    for name, value in attrs.items():\n      if name == \'_ellipsis\':\n        has_ellipsis = True\n        continue\n      ctx.define_attr(name, value)\n\n    missing_attributes = []\n    for name, _, _ in self._attr_specs():\n      if name not in attrs and not ctx.has_attr(name):\n        missing_attributes.append(name)\n\n    if has_ellipsis:\n      if len(missing_attributes) == 0:\n        raise Exception(""Saw ... given, but not missing attributes"")\n    else:\n      if len(missing_attributes) > 0:\n        raise Exception(""No ... given and missing attributes: %s"" % missing_attributes)\n\n    return DeclaredFunction(ctx, self._expr)\n\n  # If we see syntax like: foo(a: ?, b: ?) then it\'s a partial application.\n  # For these, bind the values we have return a new function where these values are unoverrideable.\n  def apply_partial():\n    pass\n\n  def _do_apply(self, visitor, ctx, scope_name, attrs, bind_args):\n    returned = {}\n    new_ctx = ctx.duplicate_for(self._ctx)\n    if attrs != None:\n      for n, v in attrs.items():\n        new_ctx.define_attr(n, v)\n\n    g = tf.get_default_graph()\n\n    # preload locals with references to input operations\n    bind_args(new_ctx)\n\n    with tf.variable_scope(scope_name):\n      # Need to visit expressions\n      visitor._visit_exprs(new_ctx, self._body())\n\n    for retval_name, retval_argname in self._retval_specs():\n      returned[retval_name] = new_ctx.get_local(retval_argname)\n\n    result = RetvalBag(returned)\n    # if name:\n    #   # HACK(adamb) The tf.identity call below just demands that the result is a Tensor.\n    #   if len(returned) == 1:\n    #     result = tf.identity(result.get(None), name=name)\n\n    return result\n\n  def apply_kw(self, visitor, ctx, scope_name, attrs, kwargs):\n    def bind_args_by_name(new_ctx):\n      for arg_name, arg in kwargs.items():\n        new_ctx.define_local(arg_name, arg)\n\n    return self._do_apply(visitor, ctx, scope_name, attrs, bind_args_by_name)\n\n  def apply(self, visitor, ctx, scope_name, attrs, args):\n    def bind_args_by_pos(new_ctx):\n      for arg_name, arg in zip(self._arg_names(), args):\n        new_ctx.define_local(arg_name, arg)\n\n    return self._do_apply(visitor, ctx, scope_name, attrs, bind_args_by_pos)\n'"
core/python/src/nao/compiler/nao/graph_gen.py,46,"b'import sys\nimport json\n\nfrom functools import reduce\nfrom functools import partial\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import gen_data_flow_ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.util import compat\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import variable_pb2\nfrom tensorflow.core.protobuf import control_flow_pb2\n\nfrom nao.structure import graph_ffi\nfrom nao.structure import graph_io\n\nfrom nao.compiler.nao import graph_context\nfrom nao.compiler.nao import graph_function\nfrom nao.compiler.nao.graph_loop import _sf_while_loop\n\nfrom nao.compiler.retvalbag import RetvalBag, unwrap_bag\n\nfrom nao.compiler.python_package import PythonPackage\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\n\nclass Nao:\n  def __init__(self, visitor):\n    self._visitor = visitor\n\n  def map(self, ctx, elems, fn=None, dtype=None, name=None):\n    eprint(""map of"", elems, ""with"", fn, dtype, ""named"", name)\n    with tf.control_dependencies([]):\n      try:\n        def some_fn(elem):\n          return fn.apply(self._visitor, ctx, None, None, [elem]).get(None)\n\n        result = tf.map_fn(\n          fn=some_fn,\n          elems=elems,\n          dtype=dtype,\n          parallel_iterations=1,\n          back_prop=False,\n          swap_memory=False,\n          infer_shape=True)\n\n        return tf.identity(result, name=name)\n\n      except KeyError as ke:\n        eprint(\'error, but got graph\', tf.get_default_graph().as_graph_def())\n        nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n        nodes.sort()\n        eprint(\'error, but got nodes\', nodes)\n        raise ke\n\n  def enqueue_many(self, ctx, queue_ref, components, name=None):\n    if name is None:\n      name = tf.get_default_graph().unique_name(""EnqueueMany"", False).split(""/"")[-1]\n\n    ret = gen_data_flow_ops._queue_enqueue_many_v2(\n        queue_ref, components=components, name=name)\n\n    # NOTE(mrry): Not using a shape function because we need access to\n    # the Queue object.\n    # op = ret[0].op\n    # batch_dim = tensor_shape.Dimension(tensor_util.constant_value(op.inputs[1]))\n    # for output, shape in zip(op.values(), shapes):\n    #   output.set_shape(tensor_shape.TensorShape([batch_dim]).concatenate(shape))\n\n    return ret\n\n  def dequeue_many(self, ctx, queue_ref, n, component_types=None, name=None):\n    if name is None:\n      name = tf.get_default_graph().unique_name(""DequeueMany"", False).split(""/"")[-1]\n\n    ret = gen_data_flow_ops._queue_dequeue_many_v2(\n        queue_ref, n=n, component_types=component_types, name=name)\n\n    # NOTE(mrry): Not using a shape function because we need access to\n    # the Queue object.\n    # op = ret[0].op\n    # batch_dim = tensor_shape.Dimension(tensor_util.constant_value(op.inputs[1]))\n    # for output, shape in zip(op.values(), shapes):\n    #   output.set_shape(tensor_shape.TensorShape([batch_dim]).concatenate(shape))\n\n    return ret\n\n  def dequeue(self, ctx, queue_ref, component_types=None, name=None):\n    if name is None:\n      name = tf.get_default_graph().unique_name(""Dequeue"", False).split(""/"")[-1]\n\n    ret = gen_data_flow_ops._queue_dequeue_v2(\n        queue_ref, component_types=component_types, name=name)\n\n    # NOTE(mrry): Not using a shape function because we need access to\n    # the Queue object.\n    # op = ret[0].op\n    # batch_dim = tensor_shape.Dimension(tensor_util.constant_value(op.inputs[1]))\n    # for output, shape in zip(op.values(), shapes):\n    #   output.set_shape(tensor_shape.TensorShape([batch_dim]).concatenate(shape))\n\n    return ret\n\n  def var_transform(self, ctx, fn, macro, name=None):\n    return graph_function.TransformedFunction(name, fn, macro)\n\nclass TopLevel:\n  TYPES = {\n\t  ""half"": tf.float16,\n\t  ""float"": tf.float32,\n\t  ""double"": tf.float64,\n\t  ""int8"": tf.int8,\n\t  ""int16"": tf.int16,\n\t  ""int32"": tf.int32,\n\t  ""int64"": tf.int64,\n\t  ""uint8"": tf.uint8,\n\t  ""uint16"": tf.uint16,\n\t  ""resource"": tf.resource,\n\t  ""string"": tf.string,\n\t  ""bool"": tf.bool,\n\t  ""complex64"": tf.complex64,\n\t  ""complex128"": tf.complex128,\n\t  ""qint8"": tf.qint8,\n\t  ""qint32"": tf.qint32,\n\t  ""quint"": tf.quint8,\n  }\n\n  def __init__(self):\n    self.nesting_level = 0\n    self._variable_listeners = []\n\n  def add_variable_listener(self, listener):\n    self._variable_listeners.append(listener)\n\n  def remove_variable_listener(self, listener):\n    self._variable_listeners.remove(listener)\n\n  # ""primitive"" values\n  def _sf_type(self, ctx, name):\n    return TopLevel.TYPES[name]\n\n  def shape(self, ctx, *dims):\n    return tf.TensorShape(dims)\n\n  def _sf_whole(self, ctx, digits):\n    return int(digits)\n\n  def _sf_fraction(self, ctx, decimal):\n    return float(decimal)\n\n  def _named_define_local(self, ctx, name, value):\n    return ctx.define_local(name, value)\n\n  def _named_define_attr(self, ctx, name, value):\n    ctx.define_attr(name, value)\n    return value\n\n  def _named_tensor(self, ctx, name, shape, dtype, value):\n    op = None\n    try:\n      op = tf.constant(value, shape=shape, dtype=dtype, name=name)\n    except TypeError as e:\n      eprint(""tf.constant(%s, shape=%s, dtype=%s, name=%s)"" % (value, shape, dtype, name))\n      raise e\n    ctx.possible_leaf(op)\n\n    if name != None:\n      return ctx.define_local(name, op)\n\n    return op\n\n  def __named_apply_prep(self, ctx, name, fn, attrs, do_apply):\n    if attrs and \'_ellipsis\' in attrs:\n      raise Exception(""Can\'t use attribute ellipsis in function apply"")\n\n    fn = unwrap_bag(fn)\n\n    base_scope_name = None\n    if hasattr(fn, \'_name\'):\n      base_scope_name = fn._name()\n\n    if not base_scope_name:\n      base_scope_name = \'fnc\'\n\n    g = tf.get_default_graph()\n    scope_name = ""q___%s"" % g.unique_name(base_scope_name, False).split(""/"")[-1]\n\n    if not hasattr(fn, \'apply\') and hasattr(fn, \'apply_attrs\'):\n      fn = fn.apply_attrs(self, attrs)\n      fn = unwrap_bag(fn)\n      attrs = None\n\n    result = do_apply(fn, scope_name)\n\n    ctx.possible_leaf(result)\n\n    if name != None:\n      result = ctx.define_local(name, result)\n\n      # HACK(adamb) The tf.identity call below just demands that the result is a Tensor.\n      if isinstance(result, RetvalBag) and result.len() == 1:\n        result = result.get(None)\n      if isinstance(result, tf.Tensor):\n        tf.identity(result, name=name)\n\n    return result\n\n  def _named_apply_keywords(self, ctx, name, fn, attrs, kwargs):\n    def keyword_apply(unwrapped_fn, scope_name):\n      unwrapped_kwargs = {}\n      for key, value in kwargs.items():\n        value = unwrap_bag(value)\n        ctx.eliminate_leaf(value)\n        unwrapped_kwargs[key] = value\n\n      return unwrapped_fn.apply_kw(self, ctx, name, attrs, unwrapped_kwargs)\n    return self.__named_apply_prep(ctx, name, fn, attrs, keyword_apply)\n\n  def _named_apply(self, ctx, name, fn, attrs, *args):\n    def positonal_apply(unwrapped_fn, scope_name):\n      unwrapped_args = []\n      for arg in args:\n        arg = unwrap_bag(arg)\n        ctx.eliminate_leaf(arg)\n        unwrapped_args.append(arg)\n      if hasattr(unwrapped_fn, \'apply\'):\n        return unwrapped_fn.apply(self, ctx, scope_name, attrs, unwrapped_args)\n      else:\n        raise Exception(""Can\'t apply non-function %s with unwrapped args %s"" % (unwrapped_fn, unwrapped_args))\n    return self.__named_apply_prep(ctx, name, fn, attrs, positonal_apply)\n\n  # TODO(adamb) Should take a name\n  def _sf_cond(self, ctx, cond_expr, then_expr, else_expr):\n    return tf.cond(\n      pred=self.visit(ctx, cond_expr),\n      fn1=lambda: self.visit(ctx.subcontext(), then_expr),\n      fn2=lambda: self.visit(ctx.subcontext(), else_expr),\n    )\n\n  # TODO(adamb) Should take a list of targets. If targets corresponds to \'*\'\n  # syntax, use leaves. Otherwise, use specific entries.\n  def _sf_after_leaves(self, ctx, *exprs):\n    leaves = ctx.leaves()\n    # eprint(""_sf_after_leaves"", leaves)\n    with tf.control_dependencies(list(leaves)):\n      return self._visit_exprs(ctx, exprs)\n\n  def _visit_exprs(self, ctx, exprs):\n    result = None\n    for expr in exprs:\n      result = self.visit(ctx, expr)\n      ctx.possible_leaf(result)\n      ctx.set_above(result)\n    return result\n\n  # TODO(adamb) Consider a ""name"" register, which specifies what the name of\n  #     the *next* node defined should be. Upon definition, the name register\n  #      should be cleared. This would simplify the current contracts.\n  #\n  # TODO(adamb) Should have name\n\n  def _sf_while_loop(self, ctx, cond_expr, body_exprs, body_retvals, init_exprs):\n    return _sf_while_loop(self, ctx, cond_expr, body_exprs, body_retvals, init_exprs)\n\n  def _sf_local(self, ctx, name):\n    # eprint(ctx)\n    return ctx.get_local(name)\n\n  def _sf_package_lookup(self, ctx, pkg_name):\n    return ctx.imported_package(pkg_name)\n\n  def list(self, ctx, *entries):\n    return [unwrap_bag(e) for e in entries]\n\n  def apply_attrs(self, ctx, function, attrs):\n    return unwrap_bag(function).apply_attrs(self, attrs)\n\n  def _sf_map(self, ctx, *entry_exprs):\n    d = {}\n    for name, value_expr in entry_exprs:\n      if name in d:\n        raise Exception(""Already have keyword %s"" % name)\n      d[name] = self.visit(ctx, value_expr)\n      # d[name] = unwrap_bag(self.visit(ctx, value_expr))\n    return d\n\n  def _sf_attr(self, ctx, name):\n    # eprint(ctx)\n    return ctx.get_attr(name)\n\n  def _named_var_update(self, ctx, name, rhs):\n    return ctx.update_local(name, rhs)\n\n  def _named_var(self, ctx, name, shape, dtype, initializer):\n    if initializer != None and not callable(initializer):\n      # TODO(adamb) Check the shape of initializer\n      shape = None\n\n    v = tf.Variable(\n        name=name,\n        initial_value=initializer,\n        expected_shape=shape,\n        dtype=dtype)\n    # eprint(""named var"", v, type(v))\n\n    return ctx.define_local(name, v)\n\n  def assert_type(self, ctx, dtype, val):\n    # TODO(adamb) Actually check type\n    return val\n\n  def assert_shape(self, ctx, shape, val):\n    # TODO(adamb) Actually check shape\n    # eprint(\'%s.set_shape(%s)\' % (val, shape))\n    val.set_shape(shape)\n    # eprint(\'shape is now %s\' % (val))\n    return val\n\n  def _sf_index(self, ctx, expr, index_expr):\n    index = self.visit(ctx, index_expr)\n    target = self.visit(ctx, expr)\n\n    return ctx.get_index(target, index)\n\n  def _sf_function(self, ctx, name, attr_spec_exprs, arg_spec_exprs, retval_specs, *body_expr):\n    attr_specs = attr_spec_exprs and [(name, self._visit(ctx, shape), self._visit(ctx, type)) for (name, shape, type) in attr_spec_exprs]\n    arg_specs = [(name, self._visit(ctx, shape), self._visit(ctx, type)) for (name, shape, type) in arg_spec_exprs]\n\n    fn = graph_function.DeclaredFunction(ctx.subcontext(), [name, attr_specs, arg_specs, retval_specs, *body_expr])\n\n    return fn\n\n  def _sf_macro(self, ctx, name, *rest):\n    return graph_function.DeclaredMacro(ctx.subcontext(), [name, *rest])\n\n  def _sf_import(self, ctx, name_triples):\n    # HACK(adamb) At the moment, assume that we\'re only talking about python)\n    for name, import_path, tag in name_triples:\n      pkg = None\n      if import_path.startswith(""tensorflow:""):\n        package_path, scope = import_path.split("":"", 1)\n        py_module = tf\n        if scope:\n          parts = scope.split(""/"")\n          py_module = reduce(lambda p, n: getattr(p, n), parts, py_module)\n        pkg = PythonPackage(py_module)\n      else:\n        if ""://"" in import_path:\n          import_path = import_path.split(""://"")[1]\n        pkg = ctx.fully_qualified_package(import_path)\n\n      ctx.import_package(name, pkg)\n\n  # HACK(adamb) For now we manually export declared functions with initial capital letters.\n  #     When functions are emitted as FunctionDefs, this can be removed.\n  def _maybe_export_function(self, package_name, subctx, name, value):\n    if not name[0].isupper():\n      eprint(""not capitalized"", name)\n      return\n\n    value = unwrap_bag(value)\n    eprint(""considering"", name)\n\n    if not isinstance(value, graph_function.DeclaredFunction):\n      eprint(""isn\'t a declared function"", type(value))\n      return\n\n    fn = value\n\n    if fn.has_attrs():\n      eprint(""has attributes, skipping."")\n      return\n\n    g = tf.get_default_graph()\n    var_collection_name = ""%s:variable_names"" % (g.unique_name(name, False))\n    var_set = set()\n    def on_var(var):\n      var_set.add(var.name)\n    self.add_variable_listener(on_var)\n\n    eprint(""exporting"", name, fn)\n    with tf.variable_scope(name):\n      with tf.variable_scope(""inputs""):\n        args = [tf.placeholder(arg_dtype, arg_shape, arg_name) for (arg_name, arg_shape, arg_dtype) in fn._arg_specs()]\n\n      subctx2 = subctx.subcontext()\n      fn.apply(self, subctx2, ""_"", None, args)\n\n      with tf.variable_scope(""outputs""):\n        g = tf.get_default_graph()\n        for (retval_name, retval_inner_name) in fn._retval_specs():\n          tensor_prefix = ""%s/%s"" % (package_name, name)\n          try:\n            returned_tensor = g.get_tensor_by_name(""%s/_/%s:0"" % (tensor_prefix, retval_inner_name))\n          except KeyError as ke:\n            eprint(""repeating lookup of %s in prefix %s for retval %s"" % (retval_inner_name, tensor_prefix, retval_name))\n            # If we fail to find the tensor above, perhaps it was just an input.\n            try:\n              returned_tensor = g.get_tensor_by_name(""%s/inputs/%s:0"" % (tensor_prefix, retval_inner_name))\n            except KeyError:\n              nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n              nodes.sort()\n              eprint(\'error, but got nodes\', nodes)\n              raise ke\n\n          tf.identity(returned_tensor, name=retval_name)\n\n    for var_name in var_set:\n      g.add_to_collection(var_collection_name, var_name)\n\n    self.remove_variable_listener(on_var)\n\n  def _sf_package(self, imports, pkg, name, *exprs):\n    if pkg is None:\n      superctx = graph_context.Context(graph_context.SentinelContextDelegate())\n      superctx.import_package(""tf"", PythonPackage(tf))\n      superctx.import_package(""nao"", PythonPackage(Nao(self), prepend_with_context=True))\n      pkg = superctx.resolve_fully_qualified_package(name)\n\n    ctx = pkg.ctx()\n    ctx.wrap_locals_in_vars()\n    prev_local_items = ctx.local_items()\n    prev_attr_items = ctx.attr_items()\n\n    for import_name, import_pkg in imports.items():\n      ctx.define_fully_qualified_package(import_name, import_pkg)\n\n    with tf.variable_scope(name):\n      self._visit_exprs(ctx, exprs)\n\n      for local_name, local_value in ctx.local_items():\n        if (local_name, local_value) in prev_local_items:\n          continue\n\n        self._maybe_export_function(name, ctx, local_name, local_value)\n\n      for attr_name, attr_value in ctx.attr_items():\n        if (attr_name, attr_value) in prev_attr_items:\n          continue\n\n        self._maybe_export_function(name, ctx, attr_name, attr_value)\n\n      # eprint(""%sctx: %s"" % (\'  \' * self.nesting_level, ctx))\n      return pkg\n\n  def visit(self, ctx, expr):\n    return self._visit_result(self._visit(ctx, expr))\n\n  def _visit_result(self, result):\n    # TODO(adamb) What about retval bags that contain variables?\n    is_tensor_ref = isinstance(result, tf.Tensor) and result.dtype._is_ref_dtype\n    is_variable = isinstance(result, tf.Variable)\n    if is_variable or is_tensor_ref:\n      # if is_tensor_ref:\n      #   eprint(""saw tensor ref"", result, result.op)\n      # else:\n      #   eprint(""saw variable"", result)\n\n      for listener in self._variable_listeners:\n        listener(result)\n\n    return result\n\n  def _visit(self, ctx, expr):\n    self.nesting_level = self.nesting_level + 1\n    # eprint(""%s%s"" % (\'  \' * self.nesting_level, expr))\n\n    if type(expr) == list:\n      expr_type = expr[0]\n      if not isinstance(expr_type, str):\n        raise Exception(""Expression type isn\'t a string. Expression: %s"" % expr)\n      attr = getattr(self, expr_type)\n\n      if expr_type.startswith(""_sf_""): # Special form\n        result = attr(ctx, *expr[1:])\n      elif expr_type.startswith(""_named_""): # name, then expressions\n        # eprint(""%sctx: %s"" % (\'  \' * self.nesting_level, ctx))\n        result = attr(ctx, expr[1], *[self.visit(ctx, subexpr) for subexpr in expr[2:]])\n      else: # just expressions\n        result = attr(ctx, *[self.visit(ctx, subexpr) for subexpr in expr[1:]])\n\n      # eprint(""visited %s expr %s => %s; ctx: %s"" % (expr_type, expr, result, ctx))\n    #   eprint(""%s=> %s"" % (\'  \' * self.nesting_level, result))\n      self.nesting_level = self.nesting_level - 1\n      return result\n    else:\n      # eprint(""visiting primitive %s ctx: %s"" % (expr, ctx))\n      self.nesting_level = self.nesting_level - 1\n      return expr\n'"
core/python/src/nao/compiler/nao/graph_loop.py,29,"b'import sys\n\nimport tensorflow as tf\nfrom tensorflow.core.protobuf import control_flow_pb2\n\nfrom nao.compiler.retvalbag import RetvalBag, unwrap_bag\n\nfrom collections import OrderedDict\n\ndef eprint(*args, **kwargs):\n  print(*args, file=sys.stderr, **kwargs)\n\ndef zero_value_for_dtype(dtype):\n  value = 0\n  if dtype.base_dtype == tf.resource:\n    value = None\n  elif dtype.base_dtype == tf.string:\n    value = """"\n  elif dtype.base_dtype == tf.bool:\n    value = False\n  elif dtype.base_dtype == tf.float16 or dtype.base_dtype == tf.float32 or dtype.base_dtype == tf.float64:\n    value = 0.0\n  return value\n\ndef _while_prune(meta_graph_def, prune_names):\n  graph_def = meta_graph_def.graph_def\n  nodes = graph_def.node\n  for ix in range(len(nodes) - 1, -1, -1):\n    n = nodes[ix]\n    if n.name in prune_names:\n      del nodes[ix]\n      # eprint(""n.name removed from graph_def!!"", n.name)\n    else:\n      # eprint(""n.name not in proxy_names"", n.name)\n      pass\n\ndef _while_fix_context_scope(meta_graph_def, import_scope):\n  col_defs = meta_graph_def.collection_def\n\n  if ""while_context"" not in col_defs:\n    return\n\n  wc_values = col_defs[""while_context""].bytes_list.value\n  for wcd_ix in range(0, len(wc_values)):\n    while_context_bytes = wc_values[wcd_ix]\n    while_context_def = control_flow_pb2.WhileContextDef()\n    while_context_def.ParseFromString(while_context_bytes)\n    values_def = while_context_def.values_def\n    values = values_def.values\n    # for v_ix in range(0, len(values)):\n    #   values[v_ix] = import_scope + ""/"" + values[v_ix]\n\n    external_values = values_def.external_values\n    # for k in list(external_values.keys()):\n    #   external_values[k] = import_scope + ""/"" + external_values[k]\n\n    # eprint(""while_context_def"", while_context_def, while_context_def.SerializeToString())\n    wc_values[wcd_ix] = while_context_def.SerializeToString()\n\ndef _while_fix_colocations(meta_graph_def, proxy_cruft):\n  graph_def = meta_graph_def.graph_def\n  for node in graph_def.node:\n    # Rewrite the colocation attributes in the graph, since the\n    # names of new ops may have changed.\n    for key, value in node.attr.items():\n      if key != \'_class\':\n        continue\n\n      class_values = value.list.s\n      for ix in range(len(class_values) - 1, -1, -1):\n        class_value = class_values[ix]\n        if class_value.startswith(b\'loc:@\'):\n          op_name = class_value[5:].decode()\n          if op_name not in proxy_cruft and ""%s:0"" % op_name not in proxy_cruft:\n            # eprint(""Skipping replacement of"", op_name, ""not in"", list(proxy_cruft))\n            continue\n          # replacement_name = proxy_cruft[op_name].name\n          # replacement = compat.as_bytes(""loc:@"" + replacement_name)\n          # eprint(""Replacing"", class_value, ""with"", replacement)\n          # class_values[ix] = replacement\n          # HACK(adamb) It would be much, much better to just do the replacement\n          #     commented out above, but we apparently can\'t replace a location\n          #     with a value pointing to the existing graph. Strange.\n          eprint(""HACK(adamb) Removing"", class_value, ""for op_name"", op_name, ""node name"", node.name)\n          del class_values[ix]\n\ndef _sf_while_inner(use_device, visitor_class, ctx, exprs):\n  with tf.Graph().as_default() as g:\n    with tf.device(use_device):\n      visitor = visitor_class()\n      final_tensor = visitor._visit_exprs(ctx, exprs)\n\n      # We do not want to include shapes, since inferred shapes will cause problems\n      # for shape inference upon import and re-export.\n\n      # HACK(adamb) Since TensorFlow uses __del__ to clean up py_funcs, we need to copy them.\n      cleanup_py_funcs_used_in_graph = []\n      if hasattr(g, ""_cleanup_py_funcs_used_in_graph""):\n        cleanup_py_funcs_used_in_graph = g._cleanup_py_funcs_used_in_graph[:]\n\n      return (\n        tf.train.export_meta_graph(),\n        cleanup_py_funcs_used_in_graph,\n        unwrap_bag(final_tensor).name\n      )\n\ndef _sf_while_embed(import_scope, input_map, retval_names, meta_graph_def, cleanup_py_funcs):\n  g = tf.get_default_graph()\n\n  try:\n    with tf.name_scope(None):\n      # with tf.control_dependencies(None):\n        try:\n          # eprint(\'while embed\', import_scope, input_map, retval_names)\n          imported_vars = tf.train.import_meta_graph(\n              meta_graph_def,\n              import_scope=import_scope,\n              input_map=input_map)\n          # eprint(\'imported_vars\', import_scope, imported_vars)\n        except ValueError as ve:\n          # HACK(adamb) We don\'t want to error on unused input_map values.\n          pass\n\n    if not hasattr(g, ""_cleanup_py_funcs_used_in_graph""):\n      g._cleanup_py_funcs_used_in_graph = []\n\n    g._cleanup_py_funcs_used_in_graph.extend(cleanup_py_funcs)\n\n    # eprint(""have graph"", import_scope, g.as_graph_def(add_shapes=True))\n\n    return [g.get_tensor_by_name(""%s/%s"" % (import_scope, n)) for n in retval_names]\n  except KeyError as ke:\n    eprint(\'error, but got graph\', tf.get_default_graph().as_graph_def())\n    nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n    nodes.sort()\n    eprint(\'error, but got nodes\', nodes)\n    raise ke\n\ndef _sf_while_loop(visitor, ctx, cond_expr, body_exprs, body_retvals, init_exprs):\n  # Need to evaluate body_exprs first, looking for all variables that will be created\n  # internally. Roll up into nested variable contexts. Unroll these contexts to be\n  # passed as part of var_list. Within def body(*a), repackage these variables into\n  # variable contexts. Use these contexts *instead of* creating variables directly.\n  # So we want to be able to indicate whether or not contexts should be allowed to\n  # create variables on the fly. Within a while cond/body, they should not. Otherwise,\n  # the can (e.g. when compiling a graph { ... } expression)\n\n  # track these so we can eventually remove them.\n  proxy_cruft = set()\n  proxied_placeholder_names = OrderedDict()\n  proxied_placeholders = OrderedDict()\n\n  # track replacements. focus on external entity to internal name.\n  def proxy(v):\n    # eprint(""proxy"", v)\n\n    if isinstance(v, RetvalBag):\n      if v.graph is None:\n        return v\n\n      if v.graph == tf.get_default_graph():\n        return v\n\n      return v.wrap(proxy)\n\n    if not isinstance(v, (tf.Operation, tf.Tensor, tf.Variable)):\n      return v\n\n    if v.graph == tf.get_default_graph():\n      return v\n\n    if v.name in proxied_placeholders:\n      return proxied_placeholders[v.name]\n\n    if v.name in proxied_placeholder_names:\n      placeholder_name = proxied_placeholder_names[v.name]\n    else:\n      placeholder_name = ""Proxy_%d"" % len(proxied_placeholder_names)\n\n    p = None\n    with tf.name_scope(None):\n      with tf.control_dependencies(None):\n        p_name = None\n        if isinstance(v, tf.Tensor) and v.dtype._is_ref_dtype:\n          p = tf.Variable(\n              initial_value=zero_value_for_dtype(v.dtype),\n              trainable=False,\n              collections=[],\n              name=placeholder_name,\n              dtype=v.dtype.base_dtype,\n              validate_shape=False)\n          p.set_shape(v.get_shape())\n          p_name = ""%s"" % p.op.name\n          proxy_cruft.add(p_name)\n          proxy_cruft.add(""%s/read"" % p.op.name)\n          proxy_cruft.add(""%s/Assign"" % p.op.name)\n          proxy_cruft.add(""%s/initial_value"" % p.op.name)\n        elif isinstance(v, tf.Variable):\n          p = tf.Variable(\n              initial_value=zero_value_for_dtype(v.dtype),\n              trainable=False,\n              collections=[],\n              name=placeholder_name,\n              dtype=v.dtype.base_dtype,\n              validate_shape=False)\n          p.set_shape(v.get_shape())\n          p_name = ""%s:0"" % p.op.name\n          p = tf.get_default_graph().get_tensor_by_name(p_name)\n          v = v.graph.get_tensor_by_name(""%s:0"" % v.op.name)\n          proxy_cruft.add(p_name)\n          proxy_cruft.add(""%s/read"" % p.op.name)\n          proxy_cruft.add(""%s/Assign"" % p.op.name)\n          proxy_cruft.add(""%s/initial_value"" % p.op.name)\n        else:\n          p = tf.placeholder(\n              v.dtype,\n              shape=v.get_shape(),\n              name=placeholder_name)\n          p_name = p.op.name\n          proxy_cruft.add(p_name)\n\n    proxied_placeholders[v.name] = p\n    proxied_placeholder_names[v.name] = p_name\n\n    if placeholder_name and placeholder_name != p.op.name:\n      raise Exception(""Created placeholder with unexpected name: %s vs %s"" % (placeholder_name, p.op.name))\n\n    return p\n\n  g = tf.get_default_graph()\n  while_loop_name = g.unique_name(""while"", False)\n\n  # eprint(\'init_exprs\', init_exprs)\n  initial_value_ctx = ctx.subcontext()\n  initial_value_ctx._proxy = proxy\n  initial_tensor_list = None\n  initial_local_names = None\n  with tf.variable_scope(\'%s_init\' % while_loop_name):\n    initial_tensor_list = [unwrap_bag(visitor.visit(initial_value_ctx, expr)) for expr in init_exprs]\n    initial_local_names = [define[1] for define in init_exprs]\n\n  local_name_by_tensor_name = dict(zip([t.name for t in initial_tensor_list], initial_local_names))\n\n  device_stack = g._device_function_stack\n  use_device = None\n  if len(device_stack) > 0:\n    use_device = device_stack[-1]\n\n  eprint(""Will use device"", use_device)\n\n  # Ensure we have a placeholder for every initial value.\n  with tf.Graph().as_default():\n    with tf.device(use_device):\n      for local_name in initial_local_names:\n        initial_value_ctx.get_local(local_name)\n\n  # Don\'t let cached placeholders from init_exprs infect our graph.\n  proxied_placeholders = OrderedDict()\n  cond_ctx = initial_value_ctx.subcontext()\n  cond_meta_graph_def, cond_cleanup_funcs, cond_retval_name = _sf_while_inner(use_device, type(visitor), cond_ctx, [cond_expr])\n\n  # Don\'t let cached placeholders from cond_exprs infect our graph.\n  proxied_placeholders = OrderedDict()\n  body_ctx = initial_value_ctx.subcontext()\n  body_meta_graph_def, body_cleanup_funcs, _ = _sf_while_inner(use_device, type(visitor), body_ctx, body_exprs)\n\n  # HACK(adamb) Don\'t actually import any nodes that are only proxies.\n  #     This should probably be done automatically by the TF import\n  #     logic, but empirically this is not the case.\n  _while_prune(cond_meta_graph_def, proxy_cruft)\n  _while_fix_colocations(cond_meta_graph_def, proxy_cruft)\n\n  _while_prune(body_meta_graph_def, proxy_cruft)\n  _while_fix_colocations(body_meta_graph_def, proxy_cruft)\n\n  body_retval_dict = dict(body_retvals)\n  body_retval_names = []\n  next_value_ixs = []\n\n  loop_vars = [g.get_tensor_by_name(v_name) for v_name in proxied_placeholder_names.keys()]\n\n  ix = -1\n  for t in loop_vars:\n    ix += 1\n    # if it\'s in initial_tensor_list, then look up its init_local_name\n    # if we have a retval for this init_local_name, then use the inner_retval\n    # otherwise pass through.\n    if t.name in local_name_by_tensor_name:\n      local_name = local_name_by_tensor_name[t.name]\n      if local_name in body_retval_dict:\n        # eprint(""while next vals"", ix, t.get_shape(), t.name, local_name, body_retval_dict[local_name])\n        body_retval_names.append(""%s:0"" % body_retval_dict[local_name])\n        next_value_ixs.append(ix)\n      else:\n        # eprint(""while next vals skipped"", ix, local_name)\n        pass\n    else:\n      # eprint(""while next vals t.name"", ix, t.name)\n      pass\n\n  # eprint(""while initial_local_names"", initial_local_names)\n  # eprint(""while initial_tensor_list"", initial_tensor_list)\n  # eprint(""while proxied_placeholder_names"", proxied_placeholder_names)\n  # eprint(""while local_name_by_tensor_name"", local_name_by_tensor_name)\n\n  def cond(*a):\n    # We use a variable_scope because name_scope has a strange\n    # only-sometimes-present trailing / that messes with everything.\n    cond_import_scope = \'%s_cond\' % while_loop_name\n\n    _while_fix_context_scope(cond_meta_graph_def, cond_import_scope)\n\n    return _sf_while_embed(\n        cond_import_scope,\n        dict(zip(proxied_placeholder_names.values(), a)),\n        [cond_retval_name],\n        cond_meta_graph_def,\n        cond_cleanup_funcs)[0]\n\n  def body(*a):\n    body_input_map = dict(zip(proxied_placeholder_names.values(), a))\n    # eprint(""while body"", body_input_map)\n\n    # We use a variable_scope because name_scope has a strange\n    # only-sometimes-present trailing / that messes with everything.\n    body_import_scope = \'%s_body\' % while_loop_name\n\n    _while_fix_context_scope(body_meta_graph_def, body_import_scope)\n\n    next_values = _sf_while_embed(\n        body_import_scope,\n        body_input_map,\n        body_retval_names,\n        body_meta_graph_def,\n        body_cleanup_funcs)\n\n    body_results = list(a)\n    for ix, val in zip(next_value_ixs, next_values):\n      val.set_shape(a[ix].get_shape())\n      # eprint(\'while shape\', ix, a[ix], a[ix].get_shape(), val, val.get_shape())\n      # val.set_shape(val.get_shape())\n      body_results[ix] = val\n\n    # eprint(\'while body_results\', body_results)\n    return body_results\n\n  # If we\'re referencing variables, we need to alert listeners.\n  for v in loop_vars:\n    visitor._visit_result(v)\n\n  results = None\n  results = tf.while_loop(\n    cond=cond,\n    body=body,\n    loop_vars=loop_vars,\n    parallel_iterations=1,\n    back_prop=False,\n    name=while_loop_name.split(""/"")[-1],\n  )\n\n  if type(results) != list:\n    results = [results]\n\n  r = {}\n  for k_name, v in zip(proxied_placeholder_names.keys(), results):\n    if k_name in local_name_by_tensor_name:\n      r[local_name_by_tensor_name[k_name]] = v\n\n  return RetvalBag(r)\n'"
core/python/src/nao/compiler/py/__init__.py,0,b''
core/python/src/nao/compiler/py/compiler.py,2,"b'import imp\nimport inspect\nimport json\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import script_ops\n\nfrom nao.structure import graph_ffi\nfrom nao.compiler.primitive_function import PrimitiveFunction\nfrom nao.compiler.python_package import PythonPackage\n\nclass ImportedPythonFunction:\n  def __init__(self, fn):\n    self._fn = fn\n    sig = inspect.signature(fn)\n    self._Tout = sig.return_annotation\n    self._argnames = sig.parameters.keys()\n    self.__name__ = self._fn.__name__\n\n  def __call__(self, *args):\n    return tf.py_func(\n        func=self._fn,\n        inp=args,\n        Tout=self._Tout,\n        stateful=True, # TODO\n        name=None)\n\n\n_python_importer = graph_ffi.PythonImporter()\n\ndef finish():\n  py_func_data = _python_importer.dump_py_funcs(script_ops._py_funcs)\n  tf.constant(json.dumps(py_func_data), name=""py_funcs_json"")\n\ndef make_compile_fn(workspace, import_path, tags):\n  source = workspace.read_src(import_path + "".py"")\n  if source is None:\n    return None\n\n  def compile(resolved_imports, previous):\n    outer_module = imp.new_module(""%s$wrapper"" % import_path)\n    all_fns = _python_importer.import_module(import_path, source)\n    for fn_name, fn in all_fns.items():\n      imported_py_func = ImportedPythonFunction(fn)\n      setattr(outer_module, fn_name, imported_py_func)\n\n    return PythonPackage(outer_module)\n\n  # TODO(adamb) Considering returning all the above logic as ""imports"",\n  #     so we can hide use of _python_importer\n  return ([], compile)\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/PyQt4/PyQt4app.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nfrom PyQt4.QtGui import QApplication, QDialog\n\napp = QApplication(sys.argv)\nform = QDialog()\nform.show()\napp.exec_()\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/PyQt4/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# A simple setup script to create an executable using PyQt4. This also\n# demonstrates the method for creating a Windows executable that does not have\n# an associated console.\n#\n# PyQt4app.py is a very simple type of PyQt4 application\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the application\n\nimport sys\nfrom cx_Freeze import setup, Executable\n\nbase = None\nif sys.platform == 'win32':\n    base = 'Win32GUI'\n\noptions = {\n    'build_exe': {\n        'includes': 'atexit'\n    }\n}\n\nexecutables = [\n    Executable('PyQt4app.py', base=base)\n]\n\nsetup(name='simple_PyQt4',\n      version='0.1',\n      description='Sample cx_Freeze PyQt4 script',\n      options=options,\n      executables=executables\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/Tkinter/SimpleTkApp.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\ntry:\n    from tkinter import Tk, Label, Button, BOTTOM\nexcept ImportError:\n    from Tkinter import Tk, Label, Button, BOTTOM\n\nroot = Tk()\nroot.title('Button')\nLabel(text='I am a button').pack(pady=15)\nButton(text='Button').pack(side=BOTTOM)\nroot.mainloop()\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/Tkinter/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# A simple setup script to create an executable using Tkinter. This also\n# demonstrates the method for creating a Windows executable that does not have\n# an associated console.\n#\n# SimpleTkApp.py is a very simple type of Tkinter application\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the application\n\nimport sys\nfrom cx_Freeze import setup, Executable\n\nbase = None\nif sys.platform == 'win32':\n    base = 'Win32GUI'\n\nexecutables = [\n    Executable('SimpleTkApp.py', base=base)\n]\n\nsetup(name='simple_Tkinter',\n      version='0.1',\n      description='Sample cx_Freeze Tkinter script',\n      executables=executables\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/advanced/advanced_1.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('Hello from cx_Freeze Advanced #1\\n\\n')\n\nmodule = __import__('testfreeze_1')\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/advanced/advanced_2.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('Hello from cx_Freeze Advanced #2\\n\\n')\n\nmodule = __import__('testfreeze_2')\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/advanced/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# An advanced setup script to create multiple executables and demonstrate a few\n# of the features available to setup scripts\n#\n# hello.py is a very simple 'Hello, world' type script which also displays the\n# environment in which the script runs\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the script without Python\n\nimport sys\nfrom cx_Freeze import setup, Executable\n\noptions = {\n    'build_exe': {\n        'includes': [\n            'testfreeze_1',\n            'testfreeze_2'\n        ],\n        'path': sys.path + ['modules']\n    }\n}\n\nexecutables = [\n    Executable('advanced_1.py'),\n    Executable('advanced_2.py')\n]\n\nsetup(name='advanced_cx_Freeze_sample',\n      version='0.1',\n      description='Advanced sample cx_Freeze script',\n      options=options,\n      executables=executables\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/asmodule/asmodule.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport datetime\nimport sys\n\ndef SayHello():\n    sys.stdout.write('Hello from cx_Freeze\\n')\n    sys.stdout.write('The current date is %s\\n\\n' %\n            datetime.datetime.today().strftime('%B %d, %Y %H:%M:%S'))\n\n    sys.stdout.write('Executable: %r\\n' % sys.executable)\n    sys.stdout.write('Prefix: %r\\n' % sys.prefix)\n    sys.stdout.write('File system encoding: %r\\n\\n' % \\\n            sys.getfilesystemencoding())\n\n    sys.stdout.write('ARGUMENTS:\\n')\n    for a in sys.argv:\n        sys.stdout.write('%s\\n' % a)\n    sys.stdout.write('\\n')\n\n    sys.stdout.write('PATH:\\n')\n    for p in sys.path:\n        sys.stdout.write('%s\\n' % p)\n    sys.stdout.write('\\n')\n\nif __name__ == '__main__':\n    SayHello()\n\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/asmodule/setup.py,0,"b'# -*- coding: utf-8 -*-\n\n# A very simple setup script to create a single executable built from a module\n# which includes an executable section protected by ""if __name__ == \'__main__\'\n#\n# Run the build process by running the command \'python setup.py build\'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the script without Python\n\nfrom cx_Freeze import setup, Executable\n\nexecutables = [\n    Executable(\'asmodule.py\')\n]\n\nsetup(name=\'asmodule\',\n      version=\'0.1\',\n      description=\'Sample cx_Freeze script\',\n      executables=executables\n      )\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/matplotlib/matplotlib_eg.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom numpy import arange, sin, pi\nimport matplotlib\nmatplotlib.use(\'WXAgg\')\nfrom matplotlib.backends.backend_wxagg import FigureCanvasWxAgg as FigureCanvas\nfrom matplotlib.backends.backend_wx import NavigationToolbar2Wx\nfrom matplotlib.figure import Figure\nimport sys\nimport wx\n\n\nclass CanvasFrame(wx.Frame):\n    def __init__(self):\n        wx.Frame.__init__(self, None, -1, \'CanvasFrame\', size=(550, 350))\n        if sys.version_info[0] == 3:\n            color = wx.Colour(""WHITE"")\n        else:\n            color = wx.NamedColour(""WHITE"")\n        self.SetBackgroundColour(color)\n        self.figure = Figure()\n        self.axes = self.figure.add_subplot(111)\n        t = arange(0.0, 3.0, 0.01)\n        s = sin(2 * pi * t)\n        self.axes.plot(t, s)\n        self.canvas = FigureCanvas(self, -1, self.figure)\n        self.sizer = wx.BoxSizer(wx.VERTICAL)\n        self.sizer.Add(self.canvas, 1, wx.LEFT | wx.TOP | wx.GROW)\n        self.SetSizerAndFit(self.sizer)\n        self.add_toolbar()\n\n    def add_toolbar(self):\n        self.toolbar = NavigationToolbar2Wx(self.canvas)\n        self.toolbar.Realize()\n        if wx.Platform == \'__WXMAC__\':\n            self.SetToolBar(self.toolbar)\n        else:\n            tw, th = self.toolbar.GetSize()\n            fw, fh = self.canvas.GetSize()\n            self.toolbar.SetSize(wx.Size(fw, th))\n            self.sizer.Add(self.toolbar, 0, wx.LEFT | wx.EXPAND)\n        self.toolbar.update()\n\n    def OnPaint(self, event):\n        self.canvas.draw()\n\n\nclass App(wx.App):\n    def OnInit(self):\n        \'\'\'Create the main window and insert the custom frame\'\'\'\n        frame = CanvasFrame()\n        frame.Show(True)\n        return True\n\napp = App(0)\napp.MainLoop()\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/matplotlib/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# A simple setup script to create an executable using matplotlib.\n#\n# matplotlib_eg.py is a very simple matplotlib application that demonstrates\n# its use.\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the application\n\nimport sys\nfrom cx_Freeze import setup, Executable\n\nbase = 'Console'\nif sys.platform == 'win32':\n    base = 'Win32GUI'\n\noptions = {\n    'build_exe': {\n\n        # Sometimes a little fine-tuning is needed\n        # exclude all backends except wx\n        'excludes': ['gtk', 'PyQt4', 'PyQt5', 'Tkinter']\n    }\n}\n\nexecutables = [\n    Executable('matplotlib_eg.py', base=base)\n]\n\nsetup(name='matplotlib_eg',\n      version='0.1',\n      description='Sample matplotlib script',\n      executables=executables,\n      options=options\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/openpyxl/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# A very simple setup script to create a single executable that makes use of\n# the openpyxl package. This package, like a number of others, makes the\n# assumption that it is found in the file system, and so fails miserably if\n# it is included in a zip file.\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the script without Python\n\nfrom cx_Freeze import setup, Executable\n\nexecutables = [\n    Executable('test_openpyxl.py')\n]\n\nsetup(name='test_openpyxl',\n      version='0.1',\n      description='Sample cx_Freeze script testing the use of openpyxl',\n      executables=executables\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/openpyxl/test_openpyxl.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# NOTE: this code is the sample code found in the openpyxl documentation which\n# can be found at https://openpyxl.readthedocs.io/en/default.\n\nfrom openpyxl import Workbook\nwb = Workbook()\n\n# grab the active worksheet\nws = wb.active\n\n# Data can be assigned directly to cells\nws[\'A1\'] = 42\n\n# Rows can also be appended\nws.append([1, 2, 3])\n\n# Python types will automatically be converted\nimport datetime\nws[\'A2\'] = datetime.datetime.now()\n\n# Save the file\nwb.save(""sample.xlsx"")\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/relimport.py,0,b'# -*- coding: utf-8 -*-\n\nimport pkg1\n'
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# relimport.py is a very simple script that tests importing using relative\n# imports (available in Python 2.5 and up)\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the script without Python\n\nfrom cx_Freeze import setup, Executable\n\nexecutables = [\n    Executable('relimport.py')\n]\n\nsetup(name='relimport',\n      version='0.1',\n      description='Sample cx_Freeze script for relative imports',\n      executables=executables\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/service/Config.py,0,"b""# -*- coding: utf-8 -*-\n\n#------------------------------------------------------------------------------\n# Config.py\n#   This file defines information about the service. The first four\n# attributes are expected to be defined and if they are not an exception will\n# be thrown when attempting to create the service:\n#\n#   NAME\n#       the name to call the service with one %s place holder that will be used\n#       to identify the service further.\n#\n#   DISPLAY_NAME\n#       the value to use as the display name for the service with one %s place\n#       holder that will be used to identify the service further.\n#\n#   MODULE_NAME\n#       the name of the module implementing the service.\n#\n#   CLASS_NAME\n#       the name of the class within the module implementing the service. This\n#       class should accept no parameters in the constructor. It should have a\n#       method called 'Initialize' which will accept the configuration file\n#       name. It should also have a method called 'Run' which will be called\n#       with no parameters when the service is started. It should also have a\n#       method called 'Stop' which will be called with no parameters when the\n#       service is stopped using the service control GUI.\n#\n#   DESCRIPTION\n#       the description of the service (optional)\n#\n#   AUTO_START\n#       whether the service should be started automatically (optional)\n#\n#   SESSION_CHANGES\n#       whether the service should monitor session changes (optional). If\n#       True, session changes will call the method 'SessionChanged' with the\n#       parameters sessionId and eventTypeId.\n#------------------------------------------------------------------------------\n\nNAME = 'cx_FreezeSampleService%s'\nDISPLAY_NAME = 'cx_Freeze Sample Service - %s'\nMODULE_NAME = 'ServiceHandler'\nCLASS_NAME = 'Handler'\nDESCRIPTION = 'Sample service description'\nAUTO_START = False\nSESSION_CHANGES = False\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/service/ServiceHandler.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nImplements a simple service using cx_Freeze.\n\nSee below for more information on what methods must be implemented and how they\nare called.\n""""""\n\nimport threading\n\nclass Handler(object):\n\n    # no parameters are permitted; all configuration should be placed in the\n    # configuration file and handled in the Initialize() method\n    def __init__(self):\n        self.stopEvent = threading.Event()\n        self.stopRequestedEvent = threading.Event()\n\n    # called when the service is starting\n    def Initialize(self, configFileName):\n        pass\n\n    # called when the service is starting immediately after Initialize()\n    # use this to perform the work of the service; don\'t forget to set or check\n    # for the stop event or the service GUI will not respond to requests to\n    # stop the service\n    def Run(self):\n        self.stopRequestedEvent.wait()\n        self.stopEvent.set()\n\n    # called when the service is being stopped by the service manager GUI\n    def Stop(self):\n        self.stopRequestedEvent.set()\n        self.stopEvent.wait()\n\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/service/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# A simple setup script for creating a Windows service. See the comments in the\n# Config.py and ServiceHandler.py files for more information on how to set this\n# up.\n#\n# Installing the service is done with the option --install <Name> and\n# uninstalling the service is done with the option --uninstall <Name>. The\n# value for <Name> is intended to differentiate between different invocations\n# of the same service code -- for example for accessing different databases or\n# using different configuration files.\n\nfrom cx_Freeze import setup, Executable\n\noptions = {\n    'build_exe': {\n        'includes': ['ServiceHandler', 'cx_Logging']\n    }\n}\n\nexecutables = [\n    Executable('Config.py', base='Win32Service',\n               targetName='cx_FreezeSampleService.exe')\n]\n\nsetup(name='cx_FreezeSampleService',\n      version='0.1',\n      description='Sample cx_Freeze Windows serice',\n      executables=executables,\n      options=options\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/simple/hello.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom datetime import datetime\nimport sys\nfrom sys import stdout\n\nstdout.write('Hello from cx_Freeze\\n')\nstdout.write('The current date is %s\\n\\n' %\n             datetime.today().strftime('%B %d, %Y %H:%M:%S'))\n\nstdout.write('Executable: %r\\n' % sys.executable)\nstdout.write('Prefix: %r\\n' % sys.prefix)\nstdout.write('Default encoding: %r\\n' % sys.getdefaultencoding())\nstdout.write('File system encoding: %r\\n\\n' % sys.getfilesystemencoding())\n\nstdout.write('ARGUMENTS:\\n')\nfor a in sys.argv:\n    stdout.write('%s\\n' % a)\nstdout.write('\\n')\n\nstdout.write('PATH:\\n')\nfor p in sys.path:\n    stdout.write('%s\\n' % p)\nstdout.write('\\n')\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/simple/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# A very simple setup script to create a single executable\n#\n# hello.py is a very simple 'Hello, world' type script which also displays the\n# environment in which the script runs\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the script without Python\n\nfrom cx_Freeze import setup, Executable\n\nexecutables = [\n    Executable('hello.py')\n]\n\nsetup(name='hello',\n      version='0.1',\n      description='Sample cx_Freeze script',\n      executables=executables\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/wx/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# A simple setup script to create an executable running wxPython. This also\n# demonstrates the method for creating a Windows executable that does not have\n# an associated console.\n#\n# wxapp.py is a very simple 'Hello, world' type wxPython application\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the application\n\nimport sys\nfrom cx_Freeze import setup, Executable\n\nbase = None\nif sys.platform == 'win32':\n    base = 'Win32GUI'\n\nexecutables = [\n    Executable('wxapp.py', base=base)\n]\n\nsetup(name='hello',\n      version='0.1',\n      description='Sample cx_Freeze wxPython script',\n      executables=executables\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/wx/wxapp.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport wx\n\n\nclass Frame(wx.Frame):\n    def __init__(self):\n        wx.Frame.__init__(self, parent=None, title='Hello from cx_Freeze')\n        panel = wx.Panel(self)\n        closeMeButton = wx.Button(panel, -1, 'Close Me')\n        self.Connect(closeMeButton.GetId(), -1, wx.EVT_BUTTON.typeId,\n                self.OnCloseMe)\n        self.Connect(self.GetId(), -1, wx.EVT_CLOSE.typeId, self.OnCloseWindow)\n        pushMeButton = wx.Button(panel, -1, 'Push Me')\n        self.Connect(pushMeButton.GetId(), -1, wx.EVT_BUTTON.typeId,\n                self.OnPushMe)\n        sizer = wx.BoxSizer(wx.HORIZONTAL)\n        sizer.Add(closeMeButton, flag=wx.ALL, border=20)\n        sizer.Add(pushMeButton, flag=wx.ALL, border=20)\n        panel.SetSizer(sizer)\n        topSizer = wx.BoxSizer(wx.VERTICAL)\n        topSizer.Add(panel, flag=wx.ALL | wx.EXPAND)\n        topSizer.Fit(self)\n\n    def OnCloseMe(self, event):\n        self.Close(True)\n\n    def OnPushMe(self, event):\n        wx.MessageBox('I was pushed!', 'Informational message')\n\n    def OnCloseWindow(self, event):\n        self.Destroy()\n\n\nclass App(wx.App):\n    def OnInit(self):\n        frame = Frame()\n        frame.Show(True)\n        self.SetTopWindow(frame)\n        return True\n\n\napp = App(1)\napp.MainLoop()\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/zope/qotd.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nA simple Quote of the Day server\n""""""\n\nfrom twisted.internet.protocol import Protocol, Factory\nfrom twisted.internet import reactor\n\n\nclass QOTD(Protocol):\n    def connectionMade(self):\n        self.transport.write(b\'An apple a day keeps the doctor away\\r\\n\')\n        self.transport.loseConnection()\n\n# Next lines are magic:\nfactory = Factory()\nfactory.protocol = QOTD\n\n# 8007 is the port you want to run under. Choose something >1024\nreactor.listenTCP(8007, factory)\nreactor.run()\n'"
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/zope/setup.py,0,"b""# -*- coding: utf-8 -*-\n\n# A simple setup script to create an executable using Zope which demonstrates\n# the use of namespace packages.\n#\n# qotd.py is a very simple type of Zope application\n#\n# Run the build process by running the command 'python setup.py build'\n#\n# If everything works well you should find a subdirectory in the build\n# subdirectory that contains the files needed to run the application\n\nimport sys\nfrom cx_Freeze import setup, Executable\n\noptions = {\n    'build_exe': {\n        'namespace_packages': ['zope']\n    }\n}\n\nexecutables = [\n    Executable('qotd.py')\n]\n\nsetup(name='QOTD sample',\n      version='1.0',\n      description='QOTD sample for demonstrating use of namespace packages',\n      options=options,\n      executables=executables\n      )\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/advanced/modules/testfreeze_1.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('Test freeze module #1\\n')\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/advanced/modules/testfreeze_2.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('Test freeze module #2\\n')\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/pkg1/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('importing pkg1\\n')\n\nfrom . import sub1\nfrom . import pkg2\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/pkg1/sub1.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('importing pkg1.sub1\\n')\n\nfrom . import sub2\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/pkg1/sub2.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('importing pkg1.sub2\\n')\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/pkg1/sub4.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('importing pkg1.sub4\\n')\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/pkg1/sub6.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('importing pkg1.sub6\\n')\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/pkg1/pkg2/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('importing pkg1.pkg2\\n')\n\nfrom . import sub3\nfrom .. import sub4\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/pkg1/pkg2/sub3.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('importing pkg1.pkg2.sub3\\n')\n\nfrom . import sub5\nfrom .. import sub6\n"""
core/native/vendor/cx_Freeze-5.0.1/cx_Freeze/samples/relimport/pkg1/pkg2/sub5.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\n\nsys.stdout.write('importing pkg1.pkg2.sub5\\n')\n"""
