file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\n# Read requirements.txt, ignore comments\ntry:\n    REQUIRES = list()\n    f = open(""requirements.txt"", ""rb"")\n    for line in f.read().decode(""utf-8"").split(""\\n""):\n        line = line.strip()\n        if ""#"" in line:\n            line = line[:line.find(""#"")].strip()\n        if line:\n            REQUIRES.append(line)\nexcept:\n    print(""\'requirements.txt\' not found!"")\n    REQUIRES = list()\n\nsetup(\n    name = ""rlzoo"",\n    version = ""1.0.3"",\n    include_package_data=True,\n    author=\'Zihan Ding, Tianyang Yu, Yanhua Huang, Hongming Zhang, Hao Dong\',\n    author_email=\'zhding@mail.ustc.edu.cn\',\n    url = ""https://github.com/tensorlayer/RLzoo"" ,\n    license = ""apache"" ,\n    packages = find_packages(),\n    install_requires=REQUIRES,\n    description = ""A collection of reinforcement learning algorithms with hierarchical code structure and convenient APIs."",\n    keywords = ""Reinforcment Learning"",\n    platform=[\'any\'],\n    python_requires=\'>=3.5\',\n)\n'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(""../""))  # Important\nsys.path.insert(0, os.path.abspath(os.path.join("".."", ""rlzoo"")))  # Important\n\n# from rlzoo.algorithms import *\nimport sphinx_rtd_theme\n\n# -- Project information -----------------------------------------------------\n\nproject = \'RLzoo\'\ncopyright = \'2020, Zihan Ding, Tianyang Yu, Yanhua Huang, Hongming Zhang, Hao Dong\'\nauthor = \'Zihan Ding, Tianyang Yu, Yanhua Huang, Hongming Zhang, Hao Dong\'\n\n# The full version, including alpha/beta/rc tags\nrelease = \'1.0.3\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.imgmath\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    # \'sphinxcontrib.bibtex\',\n    \'recommonmark\'\n]\n\nautodoc_mock_imports = [\n    \'cv2\',\n    \'hyperdash\',\n    \'gridfs\',\n    \'horovod\',\n    \'hyperdash\',\n    \'imageio\',\n    \'lxml\',\n    \'matplotlib\',\n    \'nltk\',\n    # \'numpy\',\n    \'PIL\',\n    \'progressbar\',\n    \'pymongo\',\n    \'scipy\',\n    \'skimage\',\n    \'sklearn\',\n    # \'tensorflow\',\n    \'tqdm\',\n    \'h5py\',\n    # \'tensorlayer.third_party.roi_pooling.roi_pooling.roi_pooling_ops\',  # TL C++ Packages\n]\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\nsource_suffix = [\'.rst\', \'.md\']\nmaster_doc = \'index\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\nhtml_logo = \'./img/rlzoo-logo.png\'\n\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n'"
rlzoo/__init__.py,0,b''
rlzoo/run_rlzoo.py,0,"b""from rlzoo.common.env_wrappers import *\nfrom rlzoo.common.utils import *\nfrom rlzoo.algorithms import *\n\n# EnvName = 'PongNoFrameskip-v4'\n# EnvType = 'atari'\n\n# EnvName = 'CartPole-v0'\nEnvName = 'Pendulum-v0'\nEnvType = 'classic_control'\n\n# EnvName = 'BipedalWalker-v2'\n# EnvType = 'box2d'\n\n# EnvName = 'Ant-v2'\n# EnvType = 'mujoco'\n\n# EnvName = 'FetchPush-v1'\n# EnvType = 'robotics'\n\n# EnvName = 'FishSwim-v0'\n# EnvType = 'dm_control'\n\n# EnvName = 'ReachTarget'\n# EnvType = 'rlbench'\n# env = build_env(EnvName, EnvType, state_type='vision')\n\nAlgName = 'SAC'\nenv = build_env(EnvName, EnvType)\nalg_params, learn_params = call_default_params(env, EnvType, AlgName)\nalg = eval(AlgName+'(**alg_params)')\nalg.learn(env=env, mode='train', render=False, **learn_params)\nalg.learn(env=env, mode='test', render=True, **learn_params)\n\n# AlgName = 'DPPO'\n# number_workers = 2  # need to specify number of parallel workers in parallel algorithms like A3C and DPPO\n# env = build_env(EnvName, EnvType, nenv=number_workers)\n# alg_params, learn_params = call_default_params(env, EnvType, AlgName)\n# alg_params['method'] = 'clip'    # specify 'clip' or 'penalty' method for different version of PPO and DPPO\n# alg = eval(AlgName+'(**alg_params)')\n# alg.learn(env=env,  mode='train', render=False, **learn_params)\n# alg.learn(env=env,  mode='test', render=True, **learn_params)\n\n# AlgName = 'PPO'\n# env = build_env(EnvName, EnvType)\n# alg_params, learn_params = call_default_params(env, EnvType, AlgName)\n# alg_params['method'] = 'clip'    # specify 'clip' or 'penalty' method for different version of PPO and DPPO\n# alg = eval(AlgName+'(**alg_params)')\n# alg.learn(env=env,  mode='train', render=False, **learn_params)\n# alg.learn(env=env,  mode='test', render=True, **learn_params)\n\n# AlgName = 'A3C'\n# number_workers = 2  # need to specify number of parallel workers\n# env = build_env(EnvName, EnvType, nenv=number_workers)\n# alg_params, learn_params = call_default_params(env, EnvType, 'A3C')\n# alg = eval(AlgName+'(**alg_params)')\n# alg.learn(env=env,  mode='train', render=False, **learn_params)\n# alg.learn(env=env,  mode='test', render=True, **learn_params)\n\nenv.close()\n"""
rlzoo/algorithms/__init__.py,0,b'from .ac.ac import AC\nfrom .pg.pg import PG\nfrom .dqn.dqn import DQN\nfrom .a3c.a3c import A3C\nfrom .ddpg.ddpg import DDPG\nfrom .td3.td3 import TD3\nfrom .sac.sac import SAC\nfrom .ppo.ppo import PPO\nfrom .ppo_penalty.ppo_penalty import PPO_PENALTY\nfrom .ppo_clip.ppo_clip import PPO_CLIP\nfrom .dppo.dppo import DPPO\nfrom .dppo_penalty.dppo_penalty import DPPO_PENALTY\nfrom .dppo_clip.dppo_clip import DPPO_CLIP\nfrom .trpo.trpo import TRPO\n'
rlzoo/common/__init__.py,0,b''
rlzoo/common/basic_nets.py,17,"b'""""""Basic neural networks""""""\nimport tensorflow as tf\nimport tensorlayer as tl\nfrom tensorlayer.layers import Dense, Input\nfrom gym import spaces\nfrom collections import OrderedDict\n\n\ndef MLP(input_dim, hidden_dim_list, w_init=tf.initializers.Orthogonal(0.2),\n        activation=tf.nn.relu, *args, **kwargs):\n    """"""Multiple fully-connected layers for approximation\n\n    :param input_dim: (int) size of input tensor\n    :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n    :param w_init: (callable) initialization method for weights\n    :param activation: (callable) activation function of hidden layers\n\n    Return:\n        input tensor, output tensor\n    """"""\n\n    l = inputs = Input([None, input_dim])\n    for i in range(len(hidden_dim_list)):\n        l = Dense(n_units=hidden_dim_list[i], act=activation, W_init=w_init)(l)\n    outputs = l\n\n    return inputs, outputs\n\n\ndef MLPModel(input_dim, hidden_dim_list, w_init=tf.initializers.Orthogonal(0.2),\n             activation=tf.nn.relu, *args, **kwargs):\n    """"""Multiple fully-connected layers for approximation\n\n    :param input_dim: (int) size of input tensor\n    :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n    :param w_init: (callable) initialization method for weights\n    :param activation: (callable) activation function of hidden layers\n\n    Return:\n        input tensor, output tensor\n    """"""\n    l = inputs = Input([None, input_dim], name=\'Input_Layer\')\n    for i in range(len(hidden_dim_list)):\n        l = Dense(n_units=hidden_dim_list[i], act=activation, W_init=w_init, name=\'Hidden_Layer%d\' % (i + 1))(l)\n    outputs = l\n\n    return tl.models.Model(inputs=inputs, outputs=outputs)\n\n\ndef CNN(input_shape, conv_kwargs=None):\n    """"""Multiple convolutional layers for approximation\n    Default setting is equal to architecture used in DQN\n\n    :param input_shape: (tuple[int]) (H, W, C)\n    :param conv_kwargs: (list[param]) list of conv parameters for tl.layers.Conv2d\n\n    Return:\n        input tensor, output tensor\n    """"""\n    if not conv_kwargs:\n        in_channels = input_shape[-1]\n        conv_kwargs = [\n            {\n                \'in_channels\': in_channels, \'n_filter\': 32, \'act\': tf.nn.relu,\n                \'filter_size\': (8, 8), \'strides\': (4, 4), \'padding\': \'VALID\',\n                \'W_init\': tf.initializers.GlorotUniform()\n            },\n            {\n                \'in_channels\': 32, \'n_filter\': 64, \'act\': tf.nn.relu,\n                \'filter_size\': (4, 4), \'strides\': (2, 2), \'padding\': \'VALID\',\n                \'W_init\': tf.initializers.GlorotUniform()\n            },\n            {\n                \'in_channels\': 64, \'n_filter\': 64, \'act\': tf.nn.relu,\n                \'filter_size\': (3, 3), \'strides\': (1, 1), \'padding\': \'VALID\',\n                \'W_init\': tf.initializers.GlorotUniform()\n            }\n        ]\n    l = inputs = tl.layers.Input((1,) + input_shape)\n\n    for i, kwargs in enumerate(conv_kwargs):\n        # kwargs[\'name\'] = kwargs.get(\'name\', \'cnn_layer{}\'.format(i + 1))\n        l = tl.layers.Conv2d(**kwargs)(l)\n    outputs = tl.layers.Flatten()(l)\n\n    return inputs, outputs\n\n\ndef CNNModel(input_shape, conv_kwargs=None):\n    """"""Multiple convolutional layers for approximation\n    Default setting is equal to architecture used in DQN\n\n    :param input_shape: (tuple[int]) (H, W, C)\n    :param conv_kwargs: (list[param]) list of conv parameters for tl.layers.Conv2d\n\n    Return:\n        tl.model.Model\n    """"""\n    if not conv_kwargs:\n        in_channels = input_shape[-1]\n        conv_kwargs = [\n            {\n                \'in_channels\': in_channels, \'n_filter\': 32, \'act\': tf.nn.relu,\n                \'filter_size\': (8, 8), \'strides\': (4, 4), \'padding\': \'VALID\',\n                \'W_init\': tf.initializers.GlorotUniform()\n            },\n            {\n                \'in_channels\': 32, \'n_filter\': 64, \'act\': tf.nn.relu,\n                \'filter_size\': (4, 4), \'strides\': (2, 2), \'padding\': \'VALID\',\n                \'W_init\': tf.initializers.GlorotUniform()\n            },\n            {\n                \'in_channels\': 64, \'n_filter\': 64, \'act\': tf.nn.relu,\n                \'filter_size\': (3, 3), \'strides\': (1, 1), \'padding\': \'VALID\',\n                \'W_init\': tf.initializers.GlorotUniform()\n            }\n        ]\n\n    ni = tl.layers.Input((1,) + input_shape, name=\'CNN_Input\')\n    hi = ni\n\n    for i, kwargs in enumerate(conv_kwargs):\n        kwargs[\'name\'] = kwargs.get(\'name\', \'CNN_Layer{}\'.format(i + 1))\n        hi = tl.layers.Conv2d(**kwargs)(hi)\n    no = tl.layers.Flatten(name=\'Flatten_Layer\')(hi)\n\n    return tl.models.Model(inputs=ni, outputs=no)\n\n\ndef CreateInputLayer(state_space, conv_kwargs=None):\n    def CreateSingleInput(single_state_space):\n        single_state_shape = single_state_space.shape\n        # build structure\n        if len(single_state_shape) == 1:\n            l = inputs = Input((None,) + single_state_shape, name=\'input_layer\')\n        else:\n            with tf.name_scope(\'CNN\'):\n                inputs, l = CNN(single_state_shape, conv_kwargs=conv_kwargs)\n        return inputs, l, single_state_shape\n\n    if isinstance(state_space, spaces.Dict):\n        input_dict, layer_dict, shape_dict = OrderedDict(), OrderedDict(), OrderedDict()\n        for k, v in state_space.spaces.items():\n            input_dict[k], layer_dict[k], shape_dict[k] = CreateSingleInput(v)\n        return input_dict, layer_dict, shape_dict\n    if isinstance(state_space, spaces.Space):\n        return CreateSingleInput(state_space)\n    else:\n        raise ValueError(\'state space error\')\n'"
rlzoo/common/buffer.py,0,"b'""""""\nFunctions for utilization.\n\n# Requirements\ntensorflow==2.0.0a0\ntensorlayer==2.0.1\n\n""""""\nimport inspect\nimport operator\nimport random\n\nimport numpy as np\n\n\nclass ReplayBuffer(object):\n    """"""A standard ring buffer for storing transitions and sampling for training""""""\n    def __init__(self, capacity):\n        self.capacity = capacity  # mamimum number of samples\n        self.buffer = []\n        self.position = 0  # pointer\n\n    def push(self, state, action, reward, next_state, done):\n        if len(self.buffer) < self.capacity:\n            self.buffer.append(None)\n        self.buffer[self.position] = (state, action, reward, next_state, done)\n        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n\n    def sample(self, batch_size):\n        indexes = range(len(self))\n        # sample with replacement\n        idxes = [random.choice(indexes) for _ in range(batch_size)]\n        return self._encode_sample(idxes)\n\n    def _encode_sample(self, idxes):\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n        for i in idxes:\n            state, action, reward, next_state, done = self.buffer[i]\n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n            next_states.append(next_state)\n            dones.append(done)\n        return (\n            np.stack(states),\n            np.stack(actions),\n            np.stack(rewards),\n            np.stack(next_states),\n            np.stack(dones),\n        )\n\n    def __len__(self):\n        return len(self.buffer)\n\n\nclass SegmentTree(object):\n    def __init__(self, capacity, operation, neutral_element):\n        """"""Build a Segment Tree data structure.\n\n        https://en.wikipedia.org/wiki/Segment_tree\n\n        Can be used as regular array, but with two\n        important differences:\n\n            a) setting item\'s value is slightly slower.\n               It is O(lg capacity) instead of O(1).\n            b) user has access to an efficient ( O(log segment size) )\n               `reduce` operation which reduces `operation` over\n               a contiguous subsequence of items in the array.\n\n        :param apacity: (int)\n            Total size of the array - must be a power of two.\n        :param operation: (lambda obj, obj -> obj)\n            and operation for combining elements (eg. sum, max)\n            must form a mathematical group together with the set of\n            possible values for array elements (i.e. be associative)\n        :param neutral_element: (obj)\n            neutral element for the operation above. eg. float(\'-inf\')\n            for max and 0 for sum.\n        """"""\n        assert capacity > 0 and capacity & (capacity - 1) == 0, \\\n            ""capacity must be positive and a power of 2.""\n        self._capacity = capacity\n        self._value = [neutral_element for _ in range(2 * capacity)]\n        self._operation = operation\n\n    def _reduce_helper(self, start, end, node, node_start, node_end):\n        if start == node_start and end == node_end:\n            return self._value[node]\n        mid = (node_start + node_end) // 2\n        if end <= mid:\n            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n        else:\n            if mid + 1 <= start:\n                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n            else:\n                return self._operation(\n                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n                )\n\n    def reduce(self, start=0, end=None):\n        """"""Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n\n        :param start: (int) beginning of the subsequence\n        :param end: (int) end of the subsequences\n\n        Returns:\n            reduced: (obj) result of reducing self.operation over the specified range of array.\n        """"""\n        if end is None:\n            end = self._capacity\n        if end < 0:\n            end += self._capacity\n        end -= 1\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n\n    def __setitem__(self, idx, val):\n        # index of the leaf\n        idx += self._capacity\n        self._value[idx] = val\n        idx //= 2\n        while idx >= 1:\n            self._value[idx] = self._operation(self._value[2 * idx], self._value[2 * idx + 1])\n            idx //= 2\n\n    def __getitem__(self, idx):\n        assert 0 <= idx < self._capacity\n        return self._value[self._capacity + idx]\n\n\nclass SumSegmentTree(SegmentTree):\n\n    def __init__(self, capacity):\n        super(SumSegmentTree, self).__init__(capacity=capacity, operation=operator.add, neutral_element=0.0)\n\n    def sum(self, start=0, end=None):\n        """"""Returns arr[start] + ... + arr[end]""""""\n        return super(SumSegmentTree, self).reduce(start, end)\n\n    def find_prefixsum_idx(self, prefixsum):\n        """"""Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n\n        :param perfixsum: (float)\n            upperbound on the sum of array prefix\n\n        Returns:\n            idx: (int)\n                highest index satisfying the prefixsum constraint\n        """"""\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        idx = 1\n        while idx < self._capacity:  # while non-leaf\n            if self._value[2 * idx] > prefixsum:\n                idx = 2 * idx\n            else:\n                prefixsum -= self._value[2 * idx]\n                idx = 2 * idx + 1\n        return idx - self._capacity\n\n\nclass MinSegmentTree(SegmentTree):\n\n    def __init__(self, capacity):\n        super(MinSegmentTree, self).__init__(capacity=capacity, operation=min, neutral_element=float(\'inf\'))\n\n    def min(self, start=0, end=None):\n        """"""Returns min(arr[start], ...,  arr[end])""""""\n\n        return super(MinSegmentTree, self).reduce(start, end)\n\n\nclass PrioritizedReplayBuffer(ReplayBuffer):  # is it succeed from the ReplayBuffer above?\n    def __init__(self, capacity, alpha, beta):\n        """"""Create Prioritized Replay buffer.\n\n        :param capacity: (int)\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        :param alpha: (float)\n            how much prioritization is used\n            (0 - no prioritization, 1 - full prioritization)\n\n        See Also:\n            ReplayBuffer.__init__\n        """"""\n        super(PrioritizedReplayBuffer, self).__init__(capacity)\n        assert alpha >= 0\n        self._alpha = alpha\n\n        it_capacity = 1\n        while it_capacity < capacity:\n            it_capacity *= 2\n\n        self._it_sum = SumSegmentTree(it_capacity)\n        self._it_min = MinSegmentTree(it_capacity)\n        self._max_priority = 1.0\n        self.beta = beta\n\n    def push(self, *args):\n        """"""See ReplayBuffer.store_effect""""""\n        idx = self.position\n        super().push(*args)\n        self._it_sum[idx] = self._max_priority ** self._alpha\n        self._it_min[idx] = self._max_priority ** self._alpha\n\n    def _sample_proportional(self, batch_size):\n        res = []\n        p_total = self._it_sum.sum(0, len(self.buffer) - 1)\n        every_range_len = p_total / batch_size\n        for i in range(batch_size):\n            mass = random.random() * every_range_len + i * every_range_len\n            idx = self._it_sum.find_prefixsum_idx(mass)\n            res.append(idx)\n        return res\n\n    def sample(self, batch_size):\n        """"""Sample a batch of experiences""""""\n        idxes = self._sample_proportional(batch_size)\n\n        it_sum = self._it_sum.sum()\n        p_min = self._it_min.min() / it_sum\n        max_weight = (p_min * len(self.buffer))**(-self.beta)\n\n        p_samples = np.asarray([self._it_sum[idx] for idx in idxes]) / it_sum\n        weights = (p_samples * len(self.buffer)) ** (-self.beta) / max_weight\n        encoded_sample = self._encode_sample(idxes)\n        return encoded_sample + (weights, idxes)\n\n    def update_priorities(self, idxes, priorities):\n        """"""Update priorities of sampled transitions""""""\n        assert len(idxes) == len(priorities)\n        for idx, priority in zip(idxes, priorities):\n            assert priority > 0\n            assert 0 <= idx < len(self.buffer)\n            self._it_sum[idx] = priority ** self._alpha\n            self._it_min[idx] = priority ** self._alpha\n\n            self._max_priority = max(self._max_priority, priority)\n\n\nclass HindsightReplayBuffer(ReplayBuffer):\n    """"""Hindsight Experience Replay\n    In this buffer, state is a tuple consists of (observation, goal)\n    """"""\n    GOAL_FUTURE = \'future\'\n    GOAL_EPISODE = \'episode\'\n    GOAL_RANDOM = \'random\'\n\n    def __init__(self, capacity, hindsight_freq, goal_type, reward_func, done_func):\n        """"""\n        :param hindsight_freq (int): How many hindsight transitions will be generated for each real transition\n        :param goal_type (str): The generatation method of hindsight goals. Should be HER_GOAL_*\n        :param reward_func (callable): goal (np.array) X next_state (np.array) -> reward (float)\n        :param done_func (callable): goal (np.array) X next_state (np.array) -> done_flag (bool)\n        """"""\n        super().__init__(capacity)\n        self.hindsight_freq = hindsight_freq\n        self.goal_type = goal_type\n        self.reward_func = reward_func\n        self.done_func = done_func\n\n    def _sample_goals(self, episode, t):\n        goals = []\n        episode_len = len(episode)\n        for _ in range(self.hindsight_freq):\n            if self.goal_type == HindsightReplayBuffer.GOAL_FUTURE:\n                index = random.choice(range(t + 1, episode_len))\n                source = episode\n            elif self.goal_type == HindsightReplayBuffer.GOAL_EPISODE:\n                index = random.choice(range(episode_len))\n                source = episode\n            elif self.goal_type == HindsightReplayBuffer.GOAL_RANDOM:\n                index = random.choice(range(len(self)))\n                source = self.buffer\n            else:\n                raise ValueError(""Invalid goal type %s"" % self.goal_type)\n            goals.append(source[index][0][0])  # return the observation\n        return goals\n\n    def push(self, *args, **kwargs):\n        if inspect.stack()[1][3] != \'push_episode\':\n            raise ValueError(""Please use `push_episode` methods in HER"")\n        else:\n            super().push(*args, **kwargs)\n\n    def push_episode(self, states, actions, rewards, next_states, dones):\n        episode = list(zip(states, actions, rewards, next_states, dones))\n        episode_len = len(states)\n        for t, (state, action, reward, next_state, done) in enumerate(episode):\n            self.push(state, action, reward, next_state, done)\n            if self.goal_type == HindsightReplayBuffer.GOAL_FUTURE and t == episode_len - 1:\n                break\n            for goal in self._sample_goals(episode, t):\n                s = (state[0], goal)\n                a = action\n                r = self.reward_func(goal, next_state[0])\n                s_ = (next_state[0], goal)\n                d = self.done_func(goal, next_state[0])\n                self.push(s, a, r, s_, d)\n'"
rlzoo/common/build_rlbench_env.py,0,"b'import sys\nfrom collections import OrderedDict\n\nimport numpy as np\nfrom gym import spaces\n\nfrom pyrep.const import RenderMode\nfrom pyrep.objects.dummy import Dummy\nfrom pyrep.objects.vision_sensor import VisionSensor\nfrom rlbench.environment import Environment\nfrom rlbench.action_modes import ArmActionMode, ActionMode\nfrom rlbench.observation_config import ObservationConfig\nfrom rlbench.tasks import *\n\n\n# Don\'t forget to add: export PYTHONPATH=PATH_TO_YOUR_LOCAL_RLBENCH_REPO\n\n# list of state types\nstate_types = [\'left_shoulder_rgb\',\n               \'left_shoulder_depth\',\n               \'left_shoulder_mask\',\n               \'right_shoulder_rgb\',\n               \'right_shoulder_depth\',\n               \'right_shoulder_mask\',\n               \'wrist_rgb\',\n               \'wrist_depth\',\n               \'wrist_mask\',\n               \'joint_velocities\',\n               \'joint_velocities_noise\',\n               \'joint_positions\',\n               \'joint_positions_noise\',\n               \'joint_forces\',\n               \'joint_forces_noise\',\n               \'gripper_pose\',\n               \'gripper_touch_forces\',\n               \'task_low_dim_state\']\n\n\nclass RLBenchEnv():\n    """""" make RLBench env to have same interfaces as openai.gym """"""\n\n    def __init__(self, task_name: str, state_type: list = \'state\', ):\n        # render_mode=None):\n        """"""\n        create RL Bench environment\n        :param task_name: task names can be found in rlbench.tasks\n        :param state_type: state or vision or a sub list of state_types list like [\'left_shoulder_rgb\']\n        """"""\n        if state_type == \'state\' or state_type == \'vision\' or isinstance(state_type, list):\n            self._state_type = state_type\n        else:\n            raise ValueError(\'State type value error, your value is {}\'.format(state_type))\n        # self._render_mode = render_mode\n        self._render_mode = None\n        obs_config = ObservationConfig()\n        obs_config.set_all(True)\n        action_mode = ActionMode(ArmActionMode.ABS_JOINT_VELOCITY)\n        self.env = Environment(\n            action_mode, obs_config=obs_config, headless=True)\n        self.env.launch()\n        try:\n            self.task = self.env.get_task(getattr(sys.modules[__name__], task_name))\n        except:\n            raise NotImplementedError\n\n        _, obs = self.task.reset()\n        self.spec = Spec(task_name)\n\n        if self._state_type == \'state\':\n            self.observation_space = spaces.Box(\n                low=-np.inf, high=np.inf, shape=obs.get_low_dim_data().shape)\n        elif self._state_type == \'vision\':\n            space_dict = OrderedDict()\n            space_dict[""state""] = spaces.Box(\n                low=-np.inf, high=np.inf, shape=obs.get_low_dim_data().shape)\n            for i in [""left_shoulder_rgb"", ""right_shoulder_rgb"", ""wrist_rgb"", ""front_rgb""]:\n                space_dict[i] = spaces.Box(\n                    low=0, high=1, shape=getattr(obs, i).shape)\n            self.observation_space = spaces.Dict(space_dict)\n        else:\n            space_dict = OrderedDict()\n            for name in self._state_type:\n                if name.split(\'_\')[-1] in (\'rgb\', \'depth\', \'mask\'):\n                    space_dict[name] = spaces.Box(\n                        low=0, high=1, shape=getattr(obs, name).shape)\n                else:\n                    space_dict[name] = spaces.Box(\n                        low=-np.inf, high=np.inf,\n                        shape=getattr(obs, name).shape)\n                self.observation_space = spaces.Dict(space_dict)\n        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.env.action_size,), dtype=np.float32)\n\n        # if render_mode is not None:\n        #     # Add the camera to the scene\n        #     cam_placeholder = Dummy(\'cam_cinematic_placeholder\')\n        #     self._gym_cam = VisionSensor.create([640, 360])\n        #     self._gym_cam.set_pose(cam_placeholder.get_pose())\n        #     if render_mode == \'human\':\n        #         self._gym_cam.set_render_mode(RenderMode.OPENGL3_WINDOWED)\n        #     else:\n        #         self._gym_cam.set_render_mode(RenderMode.OPENGL3)\n\n    def _extract_obs(self, obs):\n        if self._state_type == \'state\':\n            return np.array(obs.get_low_dim_data(), np.float32)\n        elif self._state_type == \'vision\':\n            return np.array([np.array(obs.get_low_dim_data(), np.float32),\n                    np.array(obs.left_shoulder_rgb, np.float32),\n                    np.array(obs.right_shoulder_rgb, np.float32),\n                    np.array(obs.wrist_rgb, np.float32),\n                    np.array(obs.front_rgb, np.float32), ])\n        else:\n            result = [\'tag\']\n            for name in self._state_type:\n                result.append(np.array(getattr(obs, name), np.float32))\n            return np.delete(np.array(result,), 0, 0)\n\n    def seed(self, seed_value):\n        # set seed as in openai.gym env\n        pass\n\n    def render(self, mode=\'human\'):\n        # todo render available at any time\n        if self._render_mode is None:\n            self._render_mode = mode\n            # Add the camera to the scene\n            cam_placeholder = Dummy(\'cam_cinematic_placeholder\')\n            self._gym_cam = VisionSensor.create([640, 360])\n            self._gym_cam.set_pose(cam_placeholder.get_pose())\n            if mode == \'human\':\n                self._gym_cam.set_render_mode(RenderMode.OPENGL3_WINDOWED)\n            else:\n                self._gym_cam.set_render_mode(RenderMode.OPENGL3)\n\n        if mode != self._render_mode:\n            raise ValueError(\n                \'The render mode must match the render mode selected in the \'\n                \'constructor. \\nI.e. if you want ""human"" render mode, then \'\n                \'create the env by calling: \'\n                \'gym.make(""reach_target-state-v0"", render_mode=""human"").\\n\'\n                \'You passed in mode %s, but expected %s.\' % (\n                    mode, self._render_mode))\n        if mode == \'rgb_array\':\n            return self._gym_cam.capture_rgb()\n\n    def reset(self):\n        descriptions, obs = self.task.reset()\n        return self._extract_obs(obs)\n\n    def step(self, action):\n        obs, reward, terminate = self.task.step(action)\n        return self._extract_obs(obs), reward, terminate, None\n\n    def close(self):\n        self.env.shutdown()\n\n\nclass Spec():\n    """""" a fake spec """"""\n\n    def __init__(self, id_name):\n        self.id = id_name\n'"
rlzoo/common/distributions.py,26,"b'""""""Definition of parametrized distributions. Adapted from openai/baselines""""""\nimport copy\nfrom functools import wraps\n\nimport numpy as np\nimport tensorflow as tf\nfrom gym import spaces\n\n\ndef expand_dims(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        result = tf.expand_dims(result, axis=-1)\n        return result\n\n    return wrapper\n\n\nclass Distribution(object):\n    """"""A particular probability distribution""""""\n\n    def set_param(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def sample(self, *args, **kwargs):\n        """"""Sampling from distribution. Allow explore parameters.""""""\n        raise NotImplementedError\n\n    def logp(self, x):\n        """"""Calculate log probability of a sample.""""""\n        return -self.neglogp(x)\n\n    def neglogp(self, x):\n        """"""Calculate negative log probability of a sample.""""""\n        raise NotImplementedError\n\n    def kl(self, *parameters):\n        """"""Calculate Kullback\xe2\x80\x93Leibler divergence""""""\n        raise NotImplementedError\n\n    def entropy(self):\n        """"""Calculate the entropy of distribution.""""""\n        raise NotImplementedError\n\n\nclass Categorical(Distribution):\n    """"""Creates a categorical distribution""""""\n\n    def __init__(self, ndim, logits=None):\n        """"""\n        Args:\n            ndim (int): total number of actions\n            logits (tensor): logits variables\n        """"""\n        self._ndim = ndim\n        self._logits = logits\n        self.param = self._logits\n\n    @property\n    def ndim(self):\n        return copy.copy(self._ndim)\n\n    def set_param(self, logits):\n        """"""\n        Args:\n            logits (tensor): logits variables to set\n        """"""\n        self._logits = logits\n        self.param = self._logits\n\n    def get_param(self):\n        return copy.deepcopy(self._logits)\n\n    def sample(self):\n        """""" Sample actions from distribution, using the Gumbel-Softmax trick """"""\n        u = np.array(np.random.uniform(0, 1, size=np.shape(self._logits)), dtype=np.float32)\n        res = tf.argmax(self._logits - tf.math.log(-tf.math.log(u)), axis=-1)\n        return res\n\n    def greedy_sample(self):\n        """""" Get actions greedily """"""\n        _probs = tf.nn.softmax(self._logits)\n        return tf.argmax(_probs, axis=-1)\n\n    def logp(self, x):\n        return -self.neglogp(x)\n\n    @expand_dims\n    def neglogp(self, x):\n        x = np.array(x)\n        if np.any(x % 1):\n            raise ValueError(\'Input float actions in discrete action space\')\n        x = tf.convert_to_tensor(x, tf.int32)\n        x = tf.one_hot(x, self._ndim, axis=-1)\n        return tf.nn.softmax_cross_entropy_with_logits(x, self._logits)\n\n    @expand_dims\n    def kl(self, logits):\n        """"""\n        Args:\n            logits (tensor): logits variables of another distribution\n        """"""\n        a0 = self._logits - tf.reduce_max(self._logits, axis=-1, keepdims=True)\n        a1 = logits - tf.reduce_max(logits, axis=-1, keepdims=True)\n        ea0 = tf.exp(a0)\n        ea1 = tf.exp(a1)\n        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n        z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(\n            p0 * (a0 - tf.math.log(z0) - a1 + tf.math.log(z1)), axis=-1)\n\n    @expand_dims\n    def entropy(self):\n        a0 = self._logits - tf.reduce_max(self._logits, axis=-1, keepdims=True)\n        ea0 = tf.exp(a0)\n        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (tf.math.log(z0) - a0), axis=-1)\n\n\nclass DiagGaussian(Distribution):\n    """"""Creates a diagonal Gaussian distribution """"""\n\n    def __init__(self, ndim, mean_logstd=None):\n        """"""\n        Args:\n            ndim (int): the dimenstion of actions\n            mean_logstd (tensor): mean and logstd stacked on the last axis\n        """"""\n        self._ndim = ndim\n        self.mean = None\n        self.logstd = None\n        self.std = None\n        self.action_mean = None\n        self.action_scale = None\n        self.param = self.mean, self.logstd\n        if mean_logstd is not None:\n            self.set_param(mean_logstd)\n\n    @property\n    def ndim(self):\n        return copy.copy(self._ndim)\n\n    def set_param(self, mean_logstd):\n        """"""\n        Args:\n            mean_logstd (tensor): mean and log std\n        """"""\n        self.mean, self.logstd = mean_logstd\n        self.std = tf.math.exp(self.logstd)\n        self.param = self.mean, self.logstd\n\n    def get_param(self):\n        """""" Get parameters """"""\n        return copy.deepcopy(self.mean), copy.deepcopy(self.logstd)\n\n    def sample(self):\n        """""" Get actions in deterministic or stochastic manner """"""\n        return self.mean, self.std * np.random.normal(0, 1, np.shape(self.mean))\n\n    def greedy_sample(self):\n        """""" Get actions greedily/deterministically """"""\n        return self.mean\n\n    def logp(self, x):\n        return -self.neglogp(x)\n\n    @expand_dims\n    def neglogp(self, x):\n        # here we reverse the action normalization to make the computation of negative log probability correct\n        x = (x - self.action_mean)/self.action_scale\n\n        return 0.5 * tf.reduce_sum(tf.square((x - self.mean) / self.std), axis=-1) \\\n                 + 0.5 * np.log(2.0 * np.pi) * float(self._ndim) + tf.reduce_sum(self.logstd, axis=-1)\n\n    @expand_dims\n    def kl(self, mean_logstd):\n        """"""\n        Args:\n            mean_logstd (tensor): mean and logstd of another distribution\n        """"""\n        mean, logstd = mean_logstd\n        return tf.reduce_sum(\n            logstd - self.logstd +\n            (tf.square(self.std) + tf.square(self.mean - mean))\n            / (2.0 * tf.square(tf.math.exp(logstd))) - 0.5, axis=-1)\n\n    @expand_dims\n    def entropy(self):\n        return tf.reduce_sum(\n            self.logstd + 0.5 * np.log(2.0 * np.pi * np.e), axis=-1)\n\n\ndef make_dist(ac_space):\n    """"""Get distribution based on action space\n\n    :param ac_space: gym.spaces.Space\n    """"""\n    if isinstance(ac_space, spaces.Discrete):\n        return Categorical(ac_space.n)\n    elif isinstance(ac_space, spaces.Box):\n        assert len(ac_space.shape) == 1\n        return DiagGaussian(ac_space.shape[0])\n    else:\n        raise NotImplementedError\n'"
rlzoo/common/env_list.py,0,"b'def get_envlist(env_type):\n    """""" get list of env names wrt the type of env """"""\n    try:\n        l = all_env_list[env_type]\n    except:\n        print(\'Env Type {:s} Not Found!\'.format(env_type))\n    return l\n\n\nall_env_list = {\n    ## Gym\n    # Atari\n    \'atari\': [\'AirRaid-v0\',\n              \'AirRaid-v4\',\n              \'AirRaidDeterministic-v0\',\n              \'AirRaidDeterministic-v4\',\n              \'AirRaidNoFrameskip-v0\',\n              \'AirRaidNoFrameskip-v4\',\n              \'AirRaid-ram-v0\',\n              \'AirRaid-ram-v4\',\n              \'AirRaid-ramDeterministic-v0\',\n              \'AirRaid-ramDeterministic-v4\',\n              \'AirRaid-ramNoFrameskip-v0\',\n              \'AirRaid-ramNoFrameskip-v4\',\n              \'Alien-v0\',\n              \'Alien-v4\',\n              \'AlienDeterministic-v0\',\n              \'AlienDeterministic-v4\',\n              \'AlienNoFrameskip-v0\',\n              \'AlienNoFrameskip-v4\',\n              \'Alien-ram-v0\',\n              \'Alien-ram-v4\',\n              \'Alien-ramDeterministic-v0\',\n              \'Alien-ramDeterministic-v4\',\n              \'Alien-ramNoFrameskip-v0\',\n              \'Alien-ramNoFrameskip-v4\',\n              \'Amidar-v0\',\n              \'Amidar-v4\',\n              \'AmidarDeterministic-v0\',\n              \'AmidarDeterministic-v4\',\n              \'AmidarNoFrameskip-v0\',\n              \'AmidarNoFrameskip-v4\',\n              \'Amidar-ram-v0\',\n              \'Amidar-ram-v4\',\n              \'Amidar-ramDeterministic-v0\',\n              \'Amidar-ramDeterministic-v4\',\n              \'Amidar-ramNoFrameskip-v0\',\n              \'Amidar-ramNoFrameskip-v4\',\n              \'Assault-v0\',\n              \'Assault-v4\',\n              \'AssaultDeterministic-v0\',\n              \'AssaultDeterministic-v4\',\n              \'AssaultNoFrameskip-v0\',\n              \'AssaultNoFrameskip-v4\',\n              \'Assault-ram-v0\',\n              \'Assault-ram-v4\',\n              \'Assault-ramDeterministic-v0\',\n              \'Assault-ramDeterministic-v4\',\n              \'Assault-ramNoFrameskip-v0\',\n              \'Assault-ramNoFrameskip-v4\',\n              \'Asterix-v0\',\n              \'Asterix-v4\',\n              \'AsterixDeterministic-v0\',\n              \'AsterixDeterministic-v4\',\n              \'AsterixNoFrameskip-v0\',\n              \'AsterixNoFrameskip-v4\',\n              \'Asterix-ram-v0\',\n              \'Asterix-ram-v4\',\n              \'Asterix-ramDeterministic-v0\',\n              \'Asterix-ramDeterministic-v4\',\n              \'Asterix-ramNoFrameskip-v0\',\n              \'Asterix-ramNoFrameskip-v4\',\n              \'Asteroids-v0\',\n              \'Asteroids-v4\',\n              \'AsteroidsDeterministic-v0\',\n              \'AsteroidsDeterministic-v4\',\n              \'AsteroidsNoFrameskip-v0\',\n              \'AsteroidsNoFrameskip-v4\',\n              \'Asteroids-ram-v0\',\n              \'Asteroids-ram-v4\',\n              \'Asteroids-ramDeterministic-v0\',\n              \'Asteroids-ramDeterministic-v4\',\n              \'Asteroids-ramNoFrameskip-v0\',\n              \'Asteroids-ramNoFrameskip-v4\',\n              \'Atlantis-v0\',\n              \'Atlantis-v4\',\n              \'AtlantisDeterministic-v0\',\n              \'AtlantisDeterministic-v4\',\n              \'AtlantisNoFrameskip-v0\',\n              \'AtlantisNoFrameskip-v4\',\n              \'Atlantis-ram-v0\',\n              \'Atlantis-ram-v4\',\n              \'Atlantis-ramDeterministic-v0\',\n              \'Atlantis-ramDeterministic-v4\',\n              \'Atlantis-ramNoFrameskip-v0\',\n              \'Atlantis-ramNoFrameskip-v4\',\n              \'BankHeist-v0\',\n              \'BankHeist-v4\',\n              \'BankHeistDeterministic-v0\',\n              \'BankHeistDeterministic-v4\',\n              \'BankHeistNoFrameskip-v0\',\n              \'BankHeistNoFrameskip-v4\',\n              \'BankHeist-ram-v0\',\n              \'BankHeist-ram-v4\',\n              \'BankHeist-ramDeterministic-v0\',\n              \'BankHeist-ramDeterministic-v4\',\n              \'BankHeist-ramNoFrameskip-v0\',\n              \'BankHeist-ramNoFrameskip-v4\',\n              \'BattleZone-v0\',\n              \'BattleZone-v4\',\n              \'BattleZoneDeterministic-v0\',\n              \'BattleZoneDeterministic-v4\',\n              \'BattleZoneNoFrameskip-v0\',\n              \'BattleZoneNoFrameskip-v4\',\n              \'BattleZone-ram-v0\',\n              \'BattleZone-ram-v4\',\n              \'BattleZone-ramDeterministic-v0\',\n              \'BattleZone-ramDeterministic-v4\',\n              \'BattleZone-ramNoFrameskip-v0\',\n              \'BattleZone-ramNoFrameskip-v4\',\n              \'BeamRider-v0\',\n              \'BeamRider-v4\',\n              \'BeamRiderDeterministic-v0\',\n              \'BeamRiderDeterministic-v4\',\n              \'BeamRiderNoFrameskip-v0\',\n              \'BeamRiderNoFrameskip-v4\',\n              \'BeamRider-ram-v0\',\n              \'BeamRider-ram-v4\',\n              \'BeamRider-ramDeterministic-v0\',\n              \'BeamRider-ramDeterministic-v4\',\n              \'BeamRider-ramNoFrameskip-v0\',\n              \'BeamRider-ramNoFrameskip-v4\',\n              \'Berzerk-v0\',\n              \'Berzerk-v4\',\n              \'BerzerkDeterministic-v0\',\n              \'BerzerkDeterministic-v4\',\n              \'BerzerkNoFrameskip-v0\',\n              \'BerzerkNoFrameskip-v4\',\n              \'Berzerk-ram-v0\',\n              \'Berzerk-ram-v4\',\n              \'Berzerk-ramDeterministic-v0\',\n              \'Berzerk-ramDeterministic-v4\',\n              \'Berzerk-ramNoFrameskip-v0\',\n              \'Berzerk-ramNoFrameskip-v4\',\n              \'Bowling-v0\',\n              \'Bowling-v4\',\n              \'BowlingDeterministic-v0\',\n              \'BowlingDeterministic-v4\',\n              \'BowlingNoFrameskip-v0\',\n              \'BowlingNoFrameskip-v4\',\n              \'Bowling-ram-v0\',\n              \'Bowling-ram-v4\',\n              \'Bowling-ramDeterministic-v0\',\n              \'Bowling-ramDeterministic-v4\',\n              \'Bowling-ramNoFrameskip-v0\',\n              \'Bowling-ramNoFrameskip-v4\',\n              \'Boxing-v0\',\n              \'Boxing-v4\',\n              \'BoxingDeterministic-v0\',\n              \'BoxingDeterministic-v4\',\n              \'BoxingNoFrameskip-v0\',\n              \'BoxingNoFrameskip-v4\',\n              \'Boxing-ram-v0\',\n              \'Boxing-ram-v4\',\n              \'Boxing-ramDeterministic-v0\',\n              \'Boxing-ramDeterministic-v4\',\n              \'Boxing-ramNoFrameskip-v0\',\n              \'Boxing-ramNoFrameskip-v4\',\n              \'Breakout-v0\',\n              \'Breakout-v4\',\n              \'BreakoutDeterministic-v0\',\n              \'BreakoutDeterministic-v4\',\n              \'BreakoutNoFrameskip-v0\',\n              \'BreakoutNoFrameskip-v4\',\n              \'Breakout-ram-v0\',\n              \'Breakout-ram-v4\',\n              \'Breakout-ramDeterministic-v0\',\n              \'Breakout-ramDeterministic-v4\',\n              \'Breakout-ramNoFrameskip-v0\',\n              \'Breakout-ramNoFrameskip-v4\',\n              \'Carnival-v0\',\n              \'Carnival-v4\',\n              \'CarnivalDeterministic-v0\',\n              \'CarnivalDeterministic-v4\',\n              \'CarnivalNoFrameskip-v0\',\n              \'CarnivalNoFrameskip-v4\',\n              \'Carnival-ram-v0\',\n              \'Carnival-ram-v4\',\n              \'Carnival-ramDeterministic-v0\',\n              \'Carnival-ramDeterministic-v4\',\n              \'Carnival-ramNoFrameskip-v0\',\n              \'Carnival-ramNoFrameskip-v4\',\n              \'Centipede-v0\',\n              \'Centipede-v4\',\n              \'CentipedeDeterministic-v0\',\n              \'CentipedeDeterministic-v4\',\n              \'CentipedeNoFrameskip-v0\',\n              \'CentipedeNoFrameskip-v4\',\n              \'Centipede-ram-v0\',\n              \'Centipede-ram-v4\',\n              \'Centipede-ramDeterministic-v0\',\n              \'Centipede-ramDeterministic-v4\',\n              \'Centipede-ramNoFrameskip-v0\',\n              \'Centipede-ramNoFrameskip-v4\',\n              \'ChopperCommand-v0\',\n              \'ChopperCommand-v4\',\n              \'ChopperCommandDeterministic-v0\',\n              \'ChopperCommandDeterministic-v4\',\n              \'ChopperCommandNoFrameskip-v0\',\n              \'ChopperCommandNoFrameskip-v4\',\n              \'ChopperCommand-ram-v0\',\n              \'ChopperCommand-ram-v4\',\n              \'ChopperCommand-ramDeterministic-v0\',\n              \'ChopperCommand-ramDeterministic-v4\',\n              \'ChopperCommand-ramNoFrameskip-v0\',\n              \'ChopperCommand-ramNoFrameskip-v4\',\n              \'CrazyClimber-v0\',\n              \'CrazyClimber-v4\',\n              \'CrazyClimberDeterministic-v0\',\n              \'CrazyClimberDeterministic-v4\',\n              \'CrazyClimberNoFrameskip-v0\',\n              \'CrazyClimberNoFrameskip-v4\',\n              \'CrazyClimber-ram-v0\',\n              \'CrazyClimber-ram-v4\',\n              \'CrazyClimber-ramDeterministic-v0\',\n              \'CrazyClimber-ramDeterministic-v4\',\n              \'CrazyClimber-ramNoFrameskip-v0\',\n              \'CrazyClimber-ramNoFrameskip-v4\',\n              \'DemonAttack-v0\',\n              \'DemonAttack-v4\',\n              \'DemonAttackDeterministic-v0\',\n              \'DemonAttackDeterministic-v4\',\n              \'DemonAttackNoFrameskip-v0\',\n              \'DemonAttackNoFrameskip-v4\',\n              \'DemonAttack-ram-v0\',\n              \'DemonAttack-ram-v4\',\n              \'DemonAttack-ramDeterministic-v0\',\n              \'DemonAttack-ramDeterministic-v4\',\n              \'DemonAttack-ramNoFrameskip-v0\',\n              \'DemonAttack-ramNoFrameskip-v4\',\n              \'DoubleDunk-v0\',\n              \'DoubleDunk-v4\',\n              \'DoubleDunkDeterministic-v0\',\n              \'DoubleDunkDeterministic-v4\',\n              \'DoubleDunkNoFrameskip-v0\',\n              \'DoubleDunkNoFrameskip-v4\',\n              \'DoubleDunk-ram-v0\',\n              \'DoubleDunk-ram-v4\',\n              \'DoubleDunk-ramDeterministic-v0\',\n              \'DoubleDunk-ramDeterministic-v4\',\n              \'DoubleDunk-ramNoFrameskip-v0\',\n              \'DoubleDunk-ramNoFrameskip-v4\',\n              \'ElevatorAction-v0\',\n              \'ElevatorAction-v4\',\n              \'ElevatorActionDeterministic-v0\',\n              \'ElevatorActionDeterministic-v4\',\n              \'ElevatorActionNoFrameskip-v0\',\n              \'ElevatorActionNoFrameskip-v4\',\n              \'ElevatorAction-ram-v0\',\n              \'ElevatorAction-ram-v4\',\n              \'ElevatorAction-ramDeterministic-v0\',\n              \'ElevatorAction-ramDeterministic-v4\',\n              \'ElevatorAction-ramNoFrameskip-v0\',\n              \'ElevatorAction-ramNoFrameskip-v4\',\n              \'Enduro-v0\',\n              \'Enduro-v4\',\n              \'EnduroDeterministic-v0\',\n              \'EnduroDeterministic-v4\',\n              \'EnduroNoFrameskip-v0\',\n              \'EnduroNoFrameskip-v4\',\n              \'Enduro-ram-v0\',\n              \'Enduro-ram-v4\',\n              \'Enduro-ramDeterministic-v0\',\n              \'Enduro-ramDeterministic-v4\',\n              \'Enduro-ramNoFrameskip-v0\',\n              \'Enduro-ramNoFrameskip-v4\',\n              \'FishingDerby-v0\',\n              \'FishingDerby-v4\',\n              \'FishingDerbyDeterministic-v0\',\n              \'FishingDerbyDeterministic-v4\',\n              \'FishingDerbyNoFrameskip-v0\',\n              \'FishingDerbyNoFrameskip-v4\',\n              \'FishingDerby-ram-v0\',\n              \'FishingDerby-ram-v4\',\n              \'FishingDerby-ramDeterministic-v0\',\n              \'FishingDerby-ramDeterministic-v4\',\n              \'FishingDerby-ramNoFrameskip-v0\',\n              \'FishingDerby-ramNoFrameskip-v4\',\n              \'Freeway-v0\',\n              \'Freeway-v4\',\n              \'FreewayDeterministic-v0\',\n              \'FreewayDeterministic-v4\',\n              \'FreewayNoFrameskip-v0\',\n              \'FreewayNoFrameskip-v4\',\n              \'Freeway-ram-v0\',\n              \'Freeway-ram-v4\',\n              \'Freeway-ramDeterministic-v0\',\n              \'Freeway-ramDeterministic-v4\',\n              \'Freeway-ramNoFrameskip-v0\',\n              \'Freeway-ramNoFrameskip-v4\',\n              \'Frostbite-v0\',\n              \'Frostbite-v4\',\n              \'FrostbiteDeterministic-v0\',\n              \'FrostbiteDeterministic-v4\',\n              \'FrostbiteNoFrameskip-v0\',\n              \'FrostbiteNoFrameskip-v4\',\n              \'Frostbite-ram-v0\',\n              \'Frostbite-ram-v4\',\n              \'Frostbite-ramDeterministic-v0\',\n              \'Frostbite-ramDeterministic-v4\',\n              \'Frostbite-ramNoFrameskip-v0\',\n              \'Frostbite-ramNoFrameskip-v4\',\n              \'Gopher-v0\',\n              \'Gopher-v4\',\n              \'GopherDeterministic-v0\',\n              \'GopherDeterministic-v4\',\n              \'GopherNoFrameskip-v0\',\n              \'GopherNoFrameskip-v4\',\n              \'Gopher-ram-v0\',\n              \'Gopher-ram-v4\',\n              \'Gopher-ramDeterministic-v0\',\n              \'Gopher-ramDeterministic-v4\',\n              \'Gopher-ramNoFrameskip-v0\',\n              \'Gopher-ramNoFrameskip-v4\',\n              \'Gravitar-v0\',\n              \'Gravitar-v4\',\n              \'GravitarDeterministic-v0\',\n              \'GravitarDeterministic-v4\',\n              \'GravitarNoFrameskip-v0\',\n              \'GravitarNoFrameskip-v4\',\n              \'Gravitar-ram-v0\',\n              \'Gravitar-ram-v4\',\n              \'Gravitar-ramDeterministic-v0\',\n              \'Gravitar-ramDeterministic-v4\',\n              \'Gravitar-ramNoFrameskip-v0\',\n              \'Gravitar-ramNoFrameskip-v4\',\n              \'Hero-v0\',\n              \'Hero-v4\',\n              \'HeroDeterministic-v0\',\n              \'HeroDeterministic-v4\',\n              \'HeroNoFrameskip-v0\',\n              \'HeroNoFrameskip-v4\',\n              \'Hero-ram-v0\',\n              \'Hero-ram-v4\',\n              \'Hero-ramDeterministic-v0\',\n              \'Hero-ramDeterministic-v4\',\n              \'Hero-ramNoFrameskip-v0\',\n              \'Hero-ramNoFrameskip-v4\',\n              \'IceHockey-v0\',\n              \'IceHockey-v4\',\n              \'IceHockeyDeterministic-v0\',\n              \'IceHockeyDeterministic-v4\',\n              \'IceHockeyNoFrameskip-v0\',\n              \'IceHockeyNoFrameskip-v4\',\n              \'IceHockey-ram-v0\',\n              \'IceHockey-ram-v4\',\n              \'IceHockey-ramDeterministic-v0\',\n              \'IceHockey-ramDeterministic-v4\',\n              \'IceHockey-ramNoFrameskip-v0\',\n              \'IceHockey-ramNoFrameskip-v4\',\n              \'Jamesbond-v0\',\n              \'Jamesbond-v4\',\n              \'JamesbondDeterministic-v0\',\n              \'JamesbondDeterministic-v4\',\n              \'JamesbondNoFrameskip-v0\',\n              \'JamesbondNoFrameskip-v4\',\n              \'Jamesbond-ram-v0\',\n              \'Jamesbond-ram-v4\',\n              \'Jamesbond-ramDeterministic-v0\',\n              \'Jamesbond-ramDeterministic-v4\',\n              \'Jamesbond-ramNoFrameskip-v0\',\n              \'Jamesbond-ramNoFrameskip-v4\',\n              \'JourneyEscape-v0\',\n              \'JourneyEscape-v4\',\n              \'JourneyEscapeDeterministic-v0\',\n              \'JourneyEscapeDeterministic-v4\',\n              \'JourneyEscapeNoFrameskip-v0\',\n              \'JourneyEscapeNoFrameskip-v4\',\n              \'JourneyEscape-ram-v0\',\n              \'JourneyEscape-ram-v4\',\n              \'JourneyEscape-ramDeterministic-v0\',\n              \'JourneyEscape-ramDeterministic-v4\',\n              \'JourneyEscape-ramNoFrameskip-v0\',\n              \'JourneyEscape-ramNoFrameskip-v4\',\n              \'Kangaroo-v0\',\n              \'Kangaroo-v4\',\n              \'KangarooDeterministic-v0\',\n              \'KangarooDeterministic-v4\',\n              \'KangarooNoFrameskip-v0\',\n              \'KangarooNoFrameskip-v4\',\n              \'Kangaroo-ram-v0\',\n              \'Kangaroo-ram-v4\',\n              \'Kangaroo-ramDeterministic-v0\',\n              \'Kangaroo-ramDeterministic-v4\',\n              \'Kangaroo-ramNoFrameskip-v0\',\n              \'Kangaroo-ramNoFrameskip-v4\',\n              \'Krull-v0\',\n              \'Krull-v4\',\n              \'KrullDeterministic-v0\',\n              \'KrullDeterministic-v4\',\n              \'KrullNoFrameskip-v0\',\n              \'KrullNoFrameskip-v4\',\n              \'Krull-ram-v0\',\n              \'Krull-ram-v4\',\n              \'Krull-ramDeterministic-v0\',\n              \'Krull-ramDeterministic-v4\',\n              \'Krull-ramNoFrameskip-v0\',\n              \'Krull-ramNoFrameskip-v4\',\n              \'KungFuMaster-v0\',\n              \'KungFuMaster-v4\',\n              \'KungFuMasterDeterministic-v0\',\n              \'KungFuMasterDeterministic-v4\',\n              \'KungFuMasterNoFrameskip-v0\',\n              \'KungFuMasterNoFrameskip-v4\',\n              \'KungFuMaster-ram-v0\',\n              \'KungFuMaster-ram-v4\',\n              \'KungFuMaster-ramDeterministic-v0\',\n              \'KungFuMaster-ramDeterministic-v4\',\n              \'KungFuMaster-ramNoFrameskip-v0\',\n              \'KungFuMaster-ramNoFrameskip-v4\',\n              \'MontezumaRevenge-v0\',\n              \'MontezumaRevenge-v4\',\n              \'MontezumaRevengeDeterministic-v0\',\n              \'MontezumaRevengeDeterministic-v4\',\n              \'MontezumaRevengeNoFrameskip-v0\',\n              \'MontezumaRevengeNoFrameskip-v4\',\n              \'MontezumaRevenge-ram-v0\',\n              \'MontezumaRevenge-ram-v4\',\n              \'MontezumaRevenge-ramDeterministic-v0\',\n              \'MontezumaRevenge-ramDeterministic-v4\',\n              \'MontezumaRevenge-ramNoFrameskip-v0\',\n              \'MontezumaRevenge-ramNoFrameskip-v4\',\n              \'MsPacman-v0\',\n              \'MsPacman-v4\',\n              \'MsPacmanDeterministic-v0\',\n              \'MsPacmanDeterministic-v4\',\n              \'MsPacmanNoFrameskip-v0\',\n              \'MsPacmanNoFrameskip-v4\',\n              \'MsPacman-ram-v0\',\n              \'MsPacman-ram-v4\',\n              \'MsPacman-ramDeterministic-v0\',\n              \'MsPacman-ramDeterministic-v4\',\n              \'MsPacman-ramNoFrameskip-v0\',\n              \'MsPacman-ramNoFrameskip-v4\',\n              \'NameThisGame-v0\',\n              \'NameThisGame-v4\',\n              \'NameThisGameDeterministic-v0\',\n              \'NameThisGameDeterministic-v4\',\n              \'NameThisGameNoFrameskip-v0\',\n              \'NameThisGameNoFrameskip-v4\',\n              \'NameThisGame-ram-v0\',\n              \'NameThisGame-ram-v4\',\n              \'NameThisGame-ramDeterministic-v0\',\n              \'NameThisGame-ramDeterministic-v4\',\n              \'NameThisGame-ramNoFrameskip-v0\',\n              \'NameThisGame-ramNoFrameskip-v4\',\n              \'Phoenix-v0\',\n              \'Phoenix-v4\',\n              \'PhoenixDeterministic-v0\',\n              \'PhoenixDeterministic-v4\',\n              \'PhoenixNoFrameskip-v0\',\n              \'PhoenixNoFrameskip-v4\',\n              \'Phoenix-ram-v0\',\n              \'Phoenix-ram-v4\',\n              \'Phoenix-ramDeterministic-v0\',\n              \'Phoenix-ramDeterministic-v4\',\n              \'Phoenix-ramNoFrameskip-v0\',\n              \'Phoenix-ramNoFrameskip-v4\',\n              \'Pitfall-v0\',\n              \'Pitfall-v4\',\n              \'PitfallDeterministic-v0\',\n              \'PitfallDeterministic-v4\',\n              \'PitfallNoFrameskip-v0\',\n              \'PitfallNoFrameskip-v4\',\n              \'Pitfall-ram-v0\',\n              \'Pitfall-ram-v4\',\n              \'Pitfall-ramDeterministic-v0\',\n              \'Pitfall-ramDeterministic-v4\',\n              \'Pitfall-ramNoFrameskip-v0\',\n              \'Pitfall-ramNoFrameskip-v4\',\n              \'Pong-v0\',\n              \'Pong-v4\',\n              \'PongDeterministic-v0\',\n              \'PongDeterministic-v4\',\n              \'PongNoFrameskip-v0\',\n              \'PongNoFrameskip-v4\',\n              \'Pong-ram-v0\',\n              \'Pong-ram-v4\',\n              \'Pong-ramDeterministic-v0\',\n              \'Pong-ramDeterministic-v4\',\n              \'Pong-ramNoFrameskip-v0\',\n              \'Pong-ramNoFrameskip-v4\',\n              \'Pooyan-v0\',\n              \'Pooyan-v4\',\n              \'PooyanDeterministic-v0\',\n              \'PooyanDeterministic-v4\',\n              \'PooyanNoFrameskip-v0\',\n              \'PooyanNoFrameskip-v4\',\n              \'Pooyan-ram-v0\',\n              \'Pooyan-ram-v4\',\n              \'Pooyan-ramDeterministic-v0\',\n              \'Pooyan-ramDeterministic-v4\',\n              \'Pooyan-ramNoFrameskip-v0\',\n              \'Pooyan-ramNoFrameskip-v4\',\n              \'PrivateEye-v0\',\n              \'PrivateEye-v4\',\n              \'PrivateEyeDeterministic-v0\',\n              \'PrivateEyeDeterministic-v4\',\n              \'PrivateEyeNoFrameskip-v0\',\n              \'PrivateEyeNoFrameskip-v4\',\n              \'PrivateEye-ram-v0\',\n              \'PrivateEye-ram-v4\',\n              \'PrivateEye-ramDeterministic-v0\',\n              \'PrivateEye-ramDeterministic-v4\',\n              \'PrivateEye-ramNoFrameskip-v0\',\n              \'PrivateEye-ramNoFrameskip-v4\',\n              \'Qbert-v0\',\n              \'Qbert-v4\',\n              \'QbertDeterministic-v0\',\n              \'QbertDeterministic-v4\',\n              \'QbertNoFrameskip-v0\',\n              \'QbertNoFrameskip-v4\',\n              \'Qbert-ram-v0\',\n              \'Qbert-ram-v4\',\n              \'Qbert-ramDeterministic-v0\',\n              \'Qbert-ramDeterministic-v4\',\n              \'Qbert-ramNoFrameskip-v0\',\n              \'Qbert-ramNoFrameskip-v4\',\n              \'Riverraid-v0\',\n              \'Riverraid-v4\',\n              \'RiverraidDeterministic-v0\',\n              \'RiverraidDeterministic-v4\',\n              \'RiverraidNoFrameskip-v0\',\n              \'RiverraidNoFrameskip-v4\',\n              \'Riverraid-ram-v0\',\n              \'Riverraid-ram-v4\',\n              \'Riverraid-ramDeterministic-v0\',\n              \'Riverraid-ramDeterministic-v4\',\n              \'Riverraid-ramNoFrameskip-v0\',\n              \'Riverraid-ramNoFrameskip-v4\',\n              \'RoadRunner-v0\',\n              \'RoadRunner-v4\',\n              \'RoadRunnerDeterministic-v0\',\n              \'RoadRunnerDeterministic-v4\',\n              \'RoadRunnerNoFrameskip-v0\',\n              \'RoadRunnerNoFrameskip-v4\',\n              \'RoadRunner-ram-v0\',\n              \'RoadRunner-ram-v4\',\n              \'RoadRunner-ramDeterministic-v0\',\n              \'RoadRunner-ramDeterministic-v4\',\n              \'RoadRunner-ramNoFrameskip-v0\',\n              \'RoadRunner-ramNoFrameskip-v4\',\n              \'Robotank-v0\',\n              \'Robotank-v4\',\n              \'RobotankDeterministic-v0\',\n              \'RobotankDeterministic-v4\',\n              \'RobotankNoFrameskip-v0\',\n              \'RobotankNoFrameskip-v4\',\n              \'Robotank-ram-v0\',\n              \'Robotank-ram-v4\',\n              \'Robotank-ramDeterministic-v0\',\n              \'Robotank-ramDeterministic-v4\',\n              \'Robotank-ramNoFrameskip-v0\',\n              \'Robotank-ramNoFrameskip-v4\',\n              \'Seaquest-v0\',\n              \'Seaquest-v4\',\n              \'SeaquestDeterministic-v0\',\n              \'SeaquestDeterministic-v4\',\n              \'SeaquestNoFrameskip-v0\',\n              \'SeaquestNoFrameskip-v4\',\n              \'Seaquest-ram-v0\',\n              \'Seaquest-ram-v4\',\n              \'Seaquest-ramDeterministic-v0\',\n              \'Seaquest-ramDeterministic-v4\',\n              \'Seaquest-ramNoFrameskip-v0\',\n              \'Seaquest-ramNoFrameskip-v4\',\n              \'Skiing-v0\',\n              \'Skiing-v4\',\n              \'SkiingDeterministic-v0\',\n              \'SkiingDeterministic-v4\',\n              \'SkiingNoFrameskip-v0\',\n              \'SkiingNoFrameskip-v4\',\n              \'Skiing-ram-v0\',\n              \'Skiing-ram-v4\',\n              \'Skiing-ramDeterministic-v0\',\n              \'Skiing-ramDeterministic-v4\',\n              \'Skiing-ramNoFrameskip-v0\',\n              \'Skiing-ramNoFrameskip-v4\',\n              \'Solaris-v0\',\n              \'Solaris-v4\',\n              \'SolarisDeterministic-v0\',\n              \'SolarisDeterministic-v4\',\n              \'SolarisNoFrameskip-v0\',\n              \'SolarisNoFrameskip-v4\',\n              \'Solaris-ram-v0\',\n              \'Solaris-ram-v4\',\n              \'Solaris-ramDeterministic-v0\',\n              \'Solaris-ramDeterministic-v4\',\n              \'Solaris-ramNoFrameskip-v0\',\n              \'Solaris-ramNoFrameskip-v4\',\n              \'SpaceInvaders-v0\',\n              \'SpaceInvaders-v4\',\n              \'SpaceInvadersDeterministic-v0\',\n              \'SpaceInvadersDeterministic-v4\',\n              \'SpaceInvadersNoFrameskip-v0\',\n              \'SpaceInvadersNoFrameskip-v4\',\n              \'SpaceInvaders-ram-v0\',\n              \'SpaceInvaders-ram-v4\',\n              \'SpaceInvaders-ramDeterministic-v0\',\n              \'SpaceInvaders-ramDeterministic-v4\',\n              \'SpaceInvaders-ramNoFrameskip-v0\',\n              \'SpaceInvaders-ramNoFrameskip-v4\',\n              \'StarGunner-v0\',\n              \'StarGunner-v4\',\n              \'StarGunnerDeterministic-v0\',\n              \'StarGunnerDeterministic-v4\',\n              \'StarGunnerNoFrameskip-v0\',\n              \'StarGunnerNoFrameskip-v4\',\n              \'StarGunner-ram-v0\',\n              \'StarGunner-ram-v4\',\n              \'StarGunner-ramDeterministic-v0\',\n              \'StarGunner-ramDeterministic-v4\',\n              \'StarGunner-ramNoFrameskip-v0\',\n              \'StarGunner-ramNoFrameskip-v4\',\n              \'Tennis-v0\',\n              \'Tennis-v4\',\n              \'TennisDeterministic-v0\',\n              \'TennisDeterministic-v4\',\n              \'TennisNoFrameskip-v0\',\n              \'TennisNoFrameskip-v4\',\n              \'Tennis-ram-v0\',\n              \'Tennis-ram-v4\',\n              \'Tennis-ramDeterministic-v0\',\n              \'Tennis-ramDeterministic-v4\',\n              \'Tennis-ramNoFrameskip-v0\',\n              \'Tennis-ramNoFrameskip-v4\',\n              \'TimePilot-v0\',\n              \'TimePilot-v4\',\n              \'TimePilotDeterministic-v0\',\n              \'TimePilotDeterministic-v4\',\n              \'TimePilotNoFrameskip-v0\',\n              \'TimePilotNoFrameskip-v4\',\n              \'TimePilot-ram-v0\',\n              \'TimePilot-ram-v4\',\n              \'TimePilot-ramDeterministic-v0\',\n              \'TimePilot-ramDeterministic-v4\',\n              \'TimePilot-ramNoFrameskip-v0\',\n              \'TimePilot-ramNoFrameskip-v4\',\n              \'Tutankham-v0\',\n              \'Tutankham-v4\',\n              \'TutankhamDeterministic-v0\',\n              \'TutankhamDeterministic-v4\',\n              \'TutankhamNoFrameskip-v0\',\n              \'TutankhamNoFrameskip-v4\',\n              \'Tutankham-ram-v0\',\n              \'Tutankham-ram-v4\',\n              \'Tutankham-ramDeterministic-v0\',\n              \'Tutankham-ramDeterministic-v4\',\n              \'Tutankham-ramNoFrameskip-v0\',\n              \'Tutankham-ramNoFrameskip-v4\',\n              \'UpNDown-v0\',\n              \'UpNDown-v4\',\n              \'UpNDownDeterministic-v0\',\n              \'UpNDownDeterministic-v4\',\n              \'UpNDownNoFrameskip-v0\',\n              \'UpNDownNoFrameskip-v4\',\n              \'UpNDown-ram-v0\',\n              \'UpNDown-ram-v4\',\n              \'UpNDown-ramDeterministic-v0\',\n              \'UpNDown-ramDeterministic-v4\',\n              \'UpNDown-ramNoFrameskip-v0\',\n              \'UpNDown-ramNoFrameskip-v4\',\n              \'Venture-v0\',\n              \'Venture-v4\',\n              \'VentureDeterministic-v0\',\n              \'VentureDeterministic-v4\',\n              \'VentureNoFrameskip-v0\',\n              \'VentureNoFrameskip-v4\',\n              \'Venture-ram-v0\',\n              \'Venture-ram-v4\',\n              \'Venture-ramDeterministic-v0\',\n              \'Venture-ramDeterministic-v4\',\n              \'Venture-ramNoFrameskip-v0\',\n              \'Venture-ramNoFrameskip-v4\',\n              \'VideoPinball-v0\',\n              \'VideoPinball-v4\',\n              \'VideoPinballDeterministic-v0\',\n              \'VideoPinballDeterministic-v4\',\n              \'VideoPinballNoFrameskip-v0\',\n              \'VideoPinballNoFrameskip-v4\',\n              \'VideoPinball-ram-v0\',\n              \'VideoPinball-ram-v4\',\n              \'VideoPinball-ramDeterministic-v0\',\n              \'VideoPinball-ramDeterministic-v4\',\n              \'VideoPinball-ramNoFrameskip-v0\',\n              \'VideoPinball-ramNoFrameskip-v4\',\n              \'WizardOfWor-v0\',\n              \'WizardOfWor-v4\',\n              \'WizardOfWorDeterministic-v0\',\n              \'WizardOfWorDeterministic-v4\',\n              \'WizardOfWorNoFrameskip-v0\',\n              \'WizardOfWorNoFrameskip-v4\',\n              \'WizardOfWor-ram-v0\',\n              \'WizardOfWor-ram-v4\',\n              \'WizardOfWor-ramDeterministic-v0\',\n              \'WizardOfWor-ramDeterministic-v4\',\n              \'WizardOfWor-ramNoFrameskip-v0\',\n              \'WizardOfWor-ramNoFrameskip-v4\',\n              \'YarsRevenge-v0\',\n              \'YarsRevenge-v4\',\n              \'YarsRevengeDeterministic-v0\',\n              \'YarsRevengeDeterministic-v4\',\n              \'YarsRevengeNoFrameskip-v0\',\n              \'YarsRevengeNoFrameskip-v4\',\n              \'YarsRevenge-ram-v0\',\n              \'YarsRevenge-ram-v4\',\n              \'YarsRevenge-ramDeterministic-v0\',\n              \'YarsRevenge-ramDeterministic-v4\',\n              \'YarsRevenge-ramNoFrameskip-v0\',\n              \'YarsRevenge-ramNoFrameskip-v4\',\n              \'Zaxxon-v0\',\n              \'Zaxxon-v4\',\n              \'ZaxxonDeterministic-v0\',\n              \'ZaxxonDeterministic-v4\',\n              \'ZaxxonNoFrameskip-v0\',\n              \'ZaxxonNoFrameskip-v4\',\n              \'Zaxxon-ram-v0\',\n              \'Zaxxon-ram-v4\',\n              \'Zaxxon-ramDeterministic-v0\',\n              \'Zaxxon-ramDeterministic-v4\',\n              \'Zaxxon-ramNoFrameskip-v0\',\n              \'Zaxxon-ramNoFrameskip-v4\'],\n\n    # Classic control\n    \'classic_control\': [\n        \'Acrobot-v1\',\n        \'CartPole-v1\',\n        \'CartPole-v0\',\n        \'MountainCar-v0\',\n        \'MountainCarContinuous-v0\',\n        \'Pendulum-v0\'\n    ],\n\n    # Box2D\n    \'box2d\': [\n        \'BipedalWalker-v2\',\n        \'BipedalWalkerHardcore-v2\',\n        \'CarRacing-v0\',\n        \'LunarLander-v2\',\n        \'LunarLanderContinuous-v2\'\n    ],\n\n    # MuJoCo\n    \'mujoco\': [\n        \'Ant-v2\',\n        \'HalfCheetah-v2\',\n        \'Hopper-v2\',\n        \'Humanoid-v2\',\n        \'HumanoidStandup-v2\',\n        \'InvertedDoublePendulum-v2\',\n        \'InvertedPendulum-v2\',\n        \'Reacher-v2\',\n        \'Swimmer-v2\',\n        \'Walker2d-v2\'\n    ],\n\n    # Robotics\n    \'robotics\': [\n        \'FetchPickAndPlace-v1\',\n        \'FetchPush-v1\',\n        \'FetchReach-v1\',\n        \'FetchSlide-v1\',\n        \'HandManipulateBlock-v0\',\n        \'HandManipulateEgg-v0\',\n        \'HandManipulatePen-v0\',\n        \'HandReach-v0\'\n    ],\n\n    ## Deepmind Control Suite  (need check!)\n    \'dm_control\': [\n        \'AcrobotSparse-v0\',\n        \'BallincupCatch-v0\',\n        \'CartpoleSwingup-v0\',\n        \'FingerTurn-v0\',\n        \'FishSwim-v0\',\n        \'CheetahRun-v0\',\n        \'HopperHop-v0\',\n        \'HumanoidStand-v0\',\n        \'HumanoidWalk-v0\',\n        \'HumanoidRun-v0\',\n        \'ManipulatorBringball-v0\',\n        \'PendulumSwingup-v0\',\n        \'Pointmass-v0\',\n        \'ReacherHard-v0\',\n        \'Swimmer-v0\',\n        \'WalkerRun-v0\'\n    ],\n\n    ## RLBench\n    \'rlbench\': [\n        \'BeatTheBuzz\',\n        \'BlockPyramid\',\n        \'ChangeChannel\',\n        \'ChangeClock\',\n        \'CloseBox\',\n        \'CloseDoor\',\n        \'CloseDrawer\',\n        \'CloseFridge\',\n        \'CloseGrill\',\n        \'CloseJar\',\n        \'CloseLaptopLid\',\n        \'CloseMicrowave\',\n        \'EmptyContainer\',\n        \'EmptyDishwasher\',\n        \'GetIceFromFridge\',\n        \'HangFrameOnHanger\',\n        \'HannoiSquare\',\n        \'HitBallWithQueue\',\n        \'Hockey\',\n        \'InsertUsbInComputer\',\n        \'LampOff\',\n        \'LampOn\',\n        \'LightBulbIn\',\n        \'LightBulbOut\',\n        \'MeatOffGrill\',\n        \'MeatOnGrill\',\n        \'MoveHanger\',\n        \'OpenBox\',\n        \'OpenDoor\',\n        \'OpenDrawer\',\n        \'OpenFridge\',\n        \'OpenGrill\',\n        \'OpenJar\',\n        \'OpenMicrowave\',\n        \'OpenOven\',\n        \'OpenWindow\',\n        \'OpenWineBottle\',\n        \'PhoneOnBase\',\n        \'PickAndLift\',\n        \'PickUpCup\',\n        \'PlaceCups\',\n        \'PlaceHangerOnRack\',\n        \'PlaceShapeInShapeSorter\',\n        \'PlayJenga\',\n        \'PlugChargerInPowerSupply\',\n        \'PourFromCupToCup\',\n        \'PressSwitch\',\n        \'PushButton\',\n        \'PushButtons\',\n        \'PutBooksOnBookshelf\',\n        \'PutBottleInFridge\',\n        \'PutGroceriesInCupboard\',\n        \'PutItemInDrawer\',\n        \'PutKnifeInKnifeBlock\',\n        \'PutKnifeOnChoppingBoard\',\n        \'PutMoneyInSafe\',\n        \'PutPlateInColoredDishRack\',\n        \'PutRubbishInBin\',\n        \'PutShoesInBox\',\n        \'PutToiletRollOnStand\',\n        \'PutTrayInOven\',\n        \'PutUmbrellaInUmbrellaStand\',\n        \'ReachAndDrag\',\n        \'ReachTarget\',\n        \'RemoveCups\',\n        \'ScoopWithSpatula\',\n        \'ScrewNail\',\n        \'SetTheTable\',\n        \'SetupCheckers\',\n        \'SlideBlockToTarget\',\n        \'SlideCabinetOpen\',\n        \'SlideCabinetOpenAndPlaceCups\',\n        \'SolvePuzzle\',\n        \'StackBlocks\',\n        \'StackCups\',\n        \'StackWine\',\n        \'StraightenRope\',\n        \'SweepToDustpan\',\n        \'TakeCupOutFromCabinet\',\n        \'TakeFrameOffHanger\',\n        \'TakeItemOutOfDrawer\',\n        \'TakeLidOffSaucepan\',\n        \'TakeMoneyOutSafe\',\n        \'TakeOffWeighingScales\',\n        \'TakePlateOffColoredDishRack\',\n        \'TakeShoesOutOfBox\',\n        \'TakeToiletRollOffStand\',\n        \'TakeTrayOutOfOven\',\n        \'TakeUmbrellaOutOfUmbrellaStand\',\n        \'TakeUsbOutOfComputer\',\n        \'ToiletSeatDown\',\n        \'ToiletSeatUp\',\n        \'TurnOvenOn\',\n        \'TurnTap\',\n        \'TvOff\',\n        \'TvOn\',\n        \'UnplugCharger\',\n        \'WaterPlants\',\n        \'WeighingScales\',\n        \'WipeDesk\'\n    ]\n}\n'"
rlzoo/common/env_wrappers.py,0,"b'""""""Env wrappers\nMost common wrappers can be checked from following links for usage: \n\n`https://pypi.org/project/gym-vec-env`\n\n`https://github.com/openai/baselines/blob/master/baselines/common/wrappers.py`\n""""""\nfrom collections import deque\nfrom functools import partial\nfrom multiprocessing import Pipe, Process, cpu_count\nfrom sys import platform\n\nimport cv2\nimport gym\nimport numpy as np\nfrom gym import spaces\nfrom gym.wrappers import FlattenDictWrapper\n\nfrom rlzoo.common.env_list import get_envlist\n\n__all__ = (\n    \'build_env\',  # build env\n    \'TimeLimit\',  # Time limit wrapper\n    \'NoopResetEnv\',  # Run random number of no-ops on reset\n    \'FireResetEnv\',  # Reset wrapper for envs with fire action\n    \'EpisodicLifeEnv\',  # end-of-life == end-of-episode wrapper\n    \'MaxAndSkipEnv\',  # skip frame wrapper\n    \'ClipRewardEnv\',  # clip reward wrapper\n    \'WarpFrame\',  # warp observation wrapper\n    \'FrameStack\',  # stack frame wrapper\n    \'LazyFrames\',  # lazy store wrapper\n    \'RewardShaping\',  # reward shaping\n    \'SubprocVecEnv\',  # vectorized env wrapper\n    \'VecFrameStack\',  # stack frames in vectorized env\n    \'Monitor\',  # Episode reward and length monitor\n    \'NormalizedActions\',  # normalized action to actual space\n    \'DmObsTrans\',  # translate observations in dm_control environments\n)\ncv2.ocl.setUseOpenCL(False)\n\n\ndef build_env(env_id, env_type, vectorized=False,\n              seed=0, reward_shaping=None, nenv=1, **kwargs):\n    """"""\n    Build env based on options\n\n    :param env_id: (str) environment id\n    :param env_type: (str) atari, classic_control, box2d\n    :param vectorized: (bool) whether sampling parrallel\n    :param seed: (int) random seed for env\n    :param reward_shaping: (callable) callable function for reward shaping\n    :param nenv: (int) how many processes will be used in sampling\n    :param kwargs: (dict)\n    :param max_episode_steps: (int) the maximum episode steps\n    """"""\n    nenv = nenv or cpu_count() // (1 + (platform == \'darwin\'))\n    stack = env_type == \'atari\'\n    if nenv > 1:\n        if vectorized:\n            env = _make_vec_env(env_id, env_type, nenv, seed,\n                                reward_shaping, stack, **kwargs)\n        else:\n            env = []\n            for _ in range(nenv):\n                single_env = _make_env(env_id, env_type, seed,\n                                       reward_shaping, stack, **kwargs)\n                env.append(single_env)  # get env as a list of same single env\n\n    else:\n        env = _make_env(env_id, env_type, seed,\n                        reward_shaping, stack, **kwargs)\n\n    return env\n\n\ndef check_name_in_list(env_id, env_type):\n    """""" Check if env_id exists in the env_type list """"""\n    env_list = get_envlist(env_type)\n    if env_id not in env_list:\n        print(\'Env ID {:s} Not Found In {:s}!\'.format(env_id, env_type))\n    else:\n        print(\'Env ID {:s} Exists!\'.format(env_id))\n\n\ndef _make_env(env_id, env_type, seed, reward_shaping, frame_stack, **kwargs):\n    """"""Make single env""""""\n    check_name_in_list(env_id, env_type)  # check existence of env_id in env_type\n    if env_type == \'atari\':\n        env = gym.make(env_id)\n        env = NoopResetEnv(env, noop_max=30)\n        if \'NoFrameskip\' in env.spec.id:\n            env = MaxAndSkipEnv(env, skip=4)\n        env = Monitor(env)\n        # deepmind wrap\n        env = EpisodicLifeEnv(env)\n        if \'FIRE\' in env.unwrapped.get_action_meanings():\n            env = FireResetEnv(env)\n        env = WarpFrame(env)\n        env = ClipRewardEnv(env)\n        if frame_stack:\n            env = FrameStack(env, 4)\n    elif env_type in [\'classic_control\', \'box2d\', \'mujoco\']:\n        env = gym.make(env_id).unwrapped\n        max_episode_steps = kwargs.get(\'max_episode_steps\')\n        if max_episode_steps is not None:\n            env = TimeLimit(env.unwrapped, max_episode_steps)\n        env = Monitor(env)\n    elif env_type == \'robotics\':\n        env = gym.make(env_id)\n        env = FlattenDictWrapper(env, [\'observation\', \'desired_goal\'])\n        env = Monitor(env, info_keywords=(\'is_success\',))\n    elif env_type == \'dm_control\':\n        env = gym.make(\'dm2gym:\' + env_id, environment_kwargs={\'flat_observation\': True})\n        env = DmObsTrans(env)\n    elif env_type == \'rlbench\':\n        from rlzoo.common.build_rlbench_env import RLBenchEnv\n        state_type = kwargs.get(\'state_type\')\n        env = RLBenchEnv(env_id) if state_type is None else RLBenchEnv(env_id, state_type)\n    else:\n        raise NotImplementedError\n\n    if reward_shaping is not None:\n        if callable(reward_shaping):\n            env = RewardShaping(env, reward_shaping)\n        else:\n            raise ValueError(\'reward_shaping parameter must be callable\')\n    env.seed(seed)\n    return env\n\n\ndef _make_vec_env(env_id, env_type, nenv, seed,\n                  reward_shaping, frame_stack, **kwargs):\n    """"""Make vectorized env""""""\n    env = SubprocVecEnv([partial(\n        _make_env, env_id, env_type, seed + i, reward_shaping, False, **kwargs\n    ) for i in range(nenv)])\n    if frame_stack:\n        env = VecFrameStack(env, 4)\n    return env\n\n\nclass DmObsTrans(gym.Wrapper):\n    """""" Observation process for DeepMind Control Suite environments """"""\n\n    def __init__(self, env):\n        self.env = env\n        super(DmObsTrans, self).__init__(env)\n        self.__need_trans = False\n        if isinstance(self.observation_space, gym.spaces.dict.Dict):\n            self.observation_space = self.observation_space[\'observations\']\n            self.__need_trans = True\n\n    def step(self, ac):\n        observation, reward, done, info = self.env.step(ac)\n        if self.__need_trans:\n            observation = observation[\'observations\']\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        observation = self.env.reset(**kwargs)\n        if self.__need_trans:\n            observation = observation[\'observations\']\n        return observation\n\n\nclass TimeLimit(gym.Wrapper):\n\n    def __init__(self, env, max_episode_steps=None):\n        self.env = env\n        super(TimeLimit, self).__init__(env)\n        self._max_episode_steps = max_episode_steps\n        self._elapsed_steps = 0\n\n    def step(self, ac):\n        observation, reward, done, info = self.env.step(ac)\n        self._elapsed_steps += 1\n        if self._elapsed_steps >= self._max_episode_steps:\n            done = True\n            info[\'TimeLimit.truncated\'] = True\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        self._elapsed_steps = 0\n        return self.env.reset(**kwargs)\n\n\nclass NoopResetEnv(gym.Wrapper):\n\n    def __init__(self, env, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        super(NoopResetEnv, self).__init__(env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass FireResetEnv(gym.Wrapper):\n\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        super(FireResetEnv, self).__init__(env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        super(EpisodicLifeEnv, self).__init__(env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if 0 < lives < self.lives:\n            # for Qbert sometimes we stay in lives == 0 condition for a few\n            # frames so it\'s important to keep lives > 0, so that we only reset\n            # once the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        super(MaxAndSkipEnv, self).__init__(env)\n        # most recent raw observations (for max pooling across time steps)\n        shape = (2,) + env.observation_space.shape\n        self._obs_buffer = np.zeros(shape, dtype=np.uint8)\n        self._skip = skip\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = info = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n\n    def __init__(self, env):\n        super(ClipRewardEnv, self).__init__(env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\n\nclass WarpFrame(gym.ObservationWrapper):\n\n    def __init__(self, env, width=84, height=84, grayscale=True):\n        """"""Warp frames to 84x84 as done in the Nature paper and later work.""""""\n        super(WarpFrame, self).__init__(env)\n        self.width = width\n        self.height = height\n        self.grayscale = grayscale\n        shape = (self.height, self.width, 1 if self.grayscale else 3)\n        self.observation_space = spaces.Box(low=0, high=255, shape=shape, dtype=np.uint8)\n\n    def observation(self, frame):\n        if self.grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        size = (self.width, self.height)\n        frame = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n        if self.grayscale:\n            frame = np.expand_dims(frame, -1)\n        return frame\n\n\nclass FrameStack(gym.Wrapper):\n\n    def __init__(self, env, k):\n        """"""Stack k last frames.\n        Returns lazy array, which is much more memory efficient.\n        See Also `LazyFrames`\n        """"""\n        super(FrameStack, self).__init__(env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        shape = shp[:-1] + (shp[-1] * k,)\n        self.observation_space = spaces.Box(low=0, high=255, shape=shape, dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return np.asarray(self._get_ob())\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return np.asarray(self._get_ob()), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\nclass LazyFrames(object):\n\n    def __init__(self, frames):\n        """"""This object ensures that common frames between the observations are\n        only stored once. It exists purely to optimize memory usage which can be\n        huge for DQN\'s 1M frames replay buffers.\n\n        This object should only be converted to numpy array before being passed\n        to the model. You\'d not believe how complex the previous solution was.\n        """"""\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\n\nclass RewardShaping(gym.RewardWrapper):\n    """"""Shaping the reward\n    For reward scale, func can be `lambda r: r * scale`\n    """"""\n\n    def __init__(self, env, func):\n        super(RewardShaping, self).__init__(env)\n        self.func = func\n\n    def reward(self, reward):\n        return self.func(reward)\n\n\nclass VecFrameStack(object):\n\n    def __init__(self, env, k):\n        self.env = env\n        self.k = k\n        self.action_space = env.action_space\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        shape = shp[:-1] + (shp[-1] * k,)\n        self.observation_space = spaces.Box(low=0, high=255, shape=shape, dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return np.asarray(self._get_ob())\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return np.asarray(self._get_ob()), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\ndef _worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.x()\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            remote.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            remote.send(ob)\n        elif cmd == \'reset_task\':\n            ob = env._reset_task()\n            remote.send(ob)\n        elif cmd == \'close\':\n            remote.close()\n            break\n        elif cmd == \'get_spaces\':\n            remote.send((env.observation_space, env.action_space))\n        else:\n            raise NotImplementedError\n\n\nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents\n    """"""\n\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n\n\nclass SubprocVecEnv(object):\n\n    def __init__(self, env_fns):\n        """"""\n        envs: list of gym environments to run in subprocesses\n        """"""\n        self.num_envs = len(env_fns)\n\n        self.waiting = False\n        self.closed = False\n        nenvs = len(env_fns)\n        self.nenvs = nenvs\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n        zipped_args = zip(self.work_remotes, self.remotes, env_fns)\n        self.ps = [\n            Process(target=_worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n            for (work_remote, remote, env_fn) in zipped_args\n        ]\n\n        for p in self.ps:\n            # if the main process crashes, we should not cause things to hang\n            p.daemon = True\n            p.start()\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        observation_space, action_space = self.remotes[0].recv()\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    def _step_async(self, actions):\n        """"""\n            Tell all the environments to start taking a step\n            with the given actions.\n            Call step_wait() to get the results of the step.\n            You should not call this if a step_async run is\n            already pending.\n            """"""\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def _step_wait(self):\n        """"""\n            Wait for the step taken with step_async().\n            Returns (obs, rews, dones, infos):\n             - obs: an array of observations, or a tuple of\n                    arrays of observations.\n             - rews: an array of rewards\n             - dones: an array of ""episode done"" booleans\n             - infos: a sequence of info objects\n            """"""\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        """"""\n            Reset all the environments and return an array of\n            observations, or a tuple of observation arrays.\n            If step_async is still doing work, that work will\n            be cancelled and step_wait() should not be called\n            until step_async() is invoked again.\n            """"""\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def _reset_task(self):\n        for remote in self.remotes:\n            remote.send((\'reset_task\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def close(self):\n        if self.closed:\n            return\n        if self.waiting:\n            for remote in self.remotes:\n                remote.recv()\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n            self.closed = True\n\n    def __len__(self):\n        return self.nenvs\n\n    def step(self, actions):\n        self._step_async(actions)\n        return self._step_wait()\n\n\nclass Monitor(gym.Wrapper):\n\n    def __init__(self, env, info_keywords=None):\n        super(Monitor, self).__init__(env)\n        self._monitor_rewards = None\n        self._info_keywords = info_keywords or []\n\n    def reset(self, **kwargs):\n        self._monitor_rewards = []\n        return self.env.reset(**kwargs)\n\n    def step(self, action):\n        o_, r, done, info = self.env.step(action)\n        self._monitor_rewards.append(r)\n        if done:\n            info[\'episode\'] = {\n                \'r\': sum(self._monitor_rewards),\n                \'l\': len(self._monitor_rewards)\n            }\n            for keyword in self._info_keywords:\n                info[\'episode\'][keyword] = info[keyword]\n        return o_, r, done, info\n\n\nclass NormalizedActions(gym.ActionWrapper):\n\n    def _action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = low + (action + 1.0) * 0.5 * (high - low)\n        action = np.clip(action, low, high)\n\n        return action\n\n    def _reverse_action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = 2 * (action - low) / (high - low) - 1\n        action = np.clip(action, low, high)\n\n        return action\n\n\ndef close_env(env):\n    """"""\n    close environment or environment list\n    """"""\n    try:\n        env.close()\n    except:\n        pass\n    try:\n        for e in env:\n            e.close()\n    except:\n        pass\n'"
rlzoo/common/math_utils.py,0,"b'""""""\nFunctions for mathematics utilization.\n\n# Requirements\ntensorflow==2.0.0a0\ntensorlayer==2.0.1\n\n""""""\n\n\ndef flatten_dims(shapes):  # will be moved to common\n    dim = 1\n    for s in shapes:\n        dim *= s\n    return dim\n'"
rlzoo/common/policy_networks.py,26,"b'""""""\nFunctions for utilization.\n\n# Requirements\ntensorflow==2.0.0a0\ntensorlayer==2.0.1\n\n""""""\nimport copy\nimport numpy as np\nimport tensorlayer as tl\nfrom tensorlayer.models import Model\n\nfrom rlzoo.common.basic_nets import *\nfrom rlzoo.common.distributions import make_dist\n\n\nclass StochasticContinuousPolicyNetwork(Model):\n    def __init__(self, state_shape, action_shape, hidden_dim_list, w_init=tf.keras.initializers.glorot_normal(),\n                 activation=tf.nn.relu, output_activation=None, log_std_min=-20, log_std_max=2, trainable=True):\n        """""" \n        Stochastic continuous policy network with multiple fully-connected layers or convolutional layers (according to state shape)\n\n        :param state_shape: (tuple[int]) shape of the state, for example, (state_dim, ) for single-dimensional state\n        :param action_shape: (tuple[int]) shape of the action, for example, (action_dim, ) for single-dimensional action\n        :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n        :param w_init: (callable) weights initialization\n        :param activation: (callable) activation function\n        :param output_activation: (callable or None) output activation function\n        :param log_std_min: (float) lower bound of standard deviation of action\n        :param log_std_max: (float) upper bound of standard deviation of action\n        :param trainable: (bool) set training and evaluation mode\n        """"""\n\n        action_dim = action_shape[0]\n        if len(state_shape) == 1:\n            with tf.name_scope(\'MLP\'):\n                state_dim = state_shape[0]\n                inputs, l = MLP(state_dim, hidden_dim_list, w_init, activation)\n        else:\n            with tf.name_scope(\'CNN\'):\n                inputs, l = CNN(state_shape, conv_kwargs=None)\n        with tf.name_scope(\'Output_Mean\'):\n            mean_linear = Dense(n_units=action_dim, act=output_activation, W_init=w_init)(l)\n        with tf.name_scope(\'Output_Std\'):\n            log_std_linear = Dense(n_units=action_dim, act=output_activation, W_init=w_init)(l)\n            log_std_linear = tl.layers.Lambda(lambda x: tf.clip_by_value(x, log_std_min, log_std_max), name=\'Lambda\')(\n                log_std_linear)\n\n        super().__init__(inputs=inputs, outputs=[mean_linear, log_std_linear])\n        if trainable:\n            self.train()\n        else:\n            self.eval()\n\n\nclass DeterministicContinuousPolicyNetwork(Model):\n    def __init__(self, state_shape, action_shape, hidden_dim_list, w_init=tf.keras.initializers.glorot_normal(), \\\n                 activation=tf.nn.relu, output_activation=tf.nn.tanh, trainable=True):\n        """""" \n        Deterministic continuous policy network with multiple fully-connected layers or convolutional layers (according to state shape)\n        \n        :param state_shape: (tuple[int]) shape of the state, for example, (state_dim, ) for single-dimensional state\n        :param action_shape: (tuple[int]) shape of the action, for example, (action_dim, ) for single-dimensional action\n        :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n        :param w_init: (callable) weights initialization\n        :param activation: (callable) activation function\n        :param output_activation: (callable or None) output activation function\n        :param trainable: (bool) set training and evaluation mode\n        """"""\n\n        action_dim = action_shape[0]\n\n        if len(state_shape) == 1:\n            with tf.name_scope(\'MLP\'):\n                state_dim = state_shape[0]\n                inputs, l = MLP(state_dim, hidden_dim_list, w_init, activation)\n        else:\n            with tf.name_scope(\'CNN\'):\n                inputs, l = CNN(state_shape, conv_kwargs=None)\n\n        with tf.name_scope(\'Output\'):\n            outputs = Dense(n_units=action_dim, act=output_activation, W_init=w_init)(l)\n\n        super().__init__(inputs=inputs, outputs=outputs)\n        if trainable:\n            self.train()\n        else:\n            self.eval()\n\n\nclass DeterministicPolicyNetwork(Model):\n    def __init__(self, state_space, action_space, hidden_dim_list, w_init=tf.keras.initializers.glorot_normal(),\n                 activation=tf.nn.relu, output_activation=tf.nn.tanh, trainable=True, name=None):\n        """""" \n        Deterministic continuous/discrete policy network with multiple fully-connected layers\n\n        :param state_space: (gym.spaces) space of the state from gym environments\n        :param action_space: (gym.spaces) space of the action from gym environments\n        :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n        :param w_init: (callable) weights initialization\n        :param activation: (callable) activation function\n        :param output_activation: (callable or None) output activation function\n        :param trainable: (bool) set training and evaluation mode\n        """"""\n        self._state_space, self._action_space = state_space, action_space\n\n        if isinstance(self._action_space, spaces.Discrete):\n            self._action_shape = self._action_space.n,\n\n        elif isinstance(self._action_space, spaces.Box):\n            assert len(self._action_space.shape) == 1\n            self._action_shape = self._action_space.shape\n\n            assert all(self._action_space.low < self._action_space.high)\n            action_bounds = [self._action_space.low, self._action_space.high]\n            self._action_mean = np.mean(action_bounds, 0)\n            self._action_scale = action_bounds[1] - self._action_mean\n        else:\n            raise NotImplementedError\n\n        obs_inputs, current_layer, self._state_shape = CreateInputLayer(state_space)\n\n        if isinstance(state_space, spaces.Dict):\n            assert isinstance(obs_inputs, dict)\n            assert isinstance(current_layer, dict)\n            self.input_dict = obs_inputs\n            obs_inputs = list(obs_inputs.values())\n            current_layer = tl.layers.Concat(-1)(list(current_layer.values()))\n\n        with tf.name_scope(\'MLP\'):\n            for i, dim in enumerate(hidden_dim_list):\n                current_layer = Dense(n_units=dim, act=activation, W_init=w_init, name=\'hidden_layer%d\' % (i + 1))(current_layer)\n\n        with tf.name_scope(\'Output\'):\n            outputs = Dense(n_units=self._action_shape[0], act=output_activation, W_init=w_init, name=\'outputs\')(current_layer)\n\n            if isinstance(self._action_space, spaces.Discrete):\n                outputs = tl.layers.Lambda(lambda x: tf.argmax(tf.nn.softmax(x), axis=-1))(outputs)\n            elif isinstance(self._action_space, spaces.Box):\n                outputs = tl.layers.Lambda(lambda x: x * self._action_scale + self._action_mean)(outputs)\n                outputs = tl.layers.Lambda(lambda x: tf.clip_by_value(x, self._action_space.low,\n                                                                      self._action_space.high))(outputs)\n\n        # make model\n        super().__init__(inputs=obs_inputs, outputs=outputs, name=name)\n        print(\'Policy network created\')\n        if trainable:\n            self.train()\n        else:\n            self.eval()\n\n    def __call__(self, states, *args, **kwargs):\n        if isinstance(self._state_space, spaces.Dict):\n            states = np.array(states).transpose([1, 0]).tolist()\n        else:\n            if np.shape(states)[1:] != self.state_shape:\n                raise ValueError(\n                    \'Input state shape error. shape can be {} but your shape is {}\'.format((None,) + self.state_shape,\n                                                                                           np.shape(states)))\n            states = np.array(states, dtype=np.float32)\n        return super().__call__(states, *args, **kwargs)\n\n    def random_sample(self):\n        """""" generate random actions for exploration """"""\n\n        if isinstance(self._action_space, spaces.Discrete):\n            return np.random.choice(self._action_space.n, 1)[0]\n        else:\n            return np.random.uniform(self._action_space.low, self._action_space.high, self._action_shape)\n\n    @property\n    def state_space(self):\n        return copy.deepcopy(self._state_space)\n\n    @property\n    def action_space(self):\n        return copy.deepcopy(self._action_space)\n\n    @property\n    def state_shape(self):\n        return copy.deepcopy(self._state_shape)\n\n    @property\n    def action_shape(self):\n        return copy.deepcopy(self._action_shape)\n\n\nclass StochasticPolicyNetwork(Model):\n    def __init__(self, state_space, action_space, hidden_dim_list, w_init=tf.keras.initializers.glorot_normal(),\n                 activation=tf.nn.relu, output_activation=tf.nn.tanh, log_std_min=-20, log_std_max=2, trainable=True,\n                 name=None, state_conditioned=False):\n        """""" \n        Stochastic continuous/discrete policy network with multiple fully-connected layers \n        \n        :param state_space: (gym.spaces) space of the state from gym environments\n        :param action_space: (gym.spaces) space of the action from gym environments\n        :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n        :param w_init: (callable) weights initialization\n        :param activation: (callable) activation function\n        :param output_activation: (callable or None) output activation function\n        :param log_std_min: (float) lower bound of standard deviation of action\n        :param log_std_max: (float) upper bound of standard deviation of action\n        :param trainable: (bool) set training and evaluation mode\n\n        Tips: We recommend to use tf.nn.tanh for output_activation, especially for continuous action space, \\\n            to ensure the final action range is exactly the same as declared in action space after action normalization.\n        """"""\n        self._state_space, self._action_space = state_space, action_space\n\n        if isinstance(self._action_space, spaces.Discrete):\n            self._action_shape = self._action_space.n,\n            self.policy_dist = make_dist(self._action_space)  # create action distribution\n        elif isinstance(self._action_space, spaces.Box):  # normalize action\n            assert len(self._action_space.shape) == 1\n            self._action_shape = self._action_space.shape\n\n            assert all(self._action_space.low < self._action_space.high)\n            action_bounds = [self._action_space.low, self._action_space.high]\n            self._action_mean = np.mean(action_bounds, 0)\n            self._action_scale = action_bounds[1] - self._action_mean\n\n            self.policy_dist = make_dist(self._action_space)  # create action distribution\n            self.policy_dist.action_mean = self._action_mean\n            self.policy_dist.action_scale = self._action_scale\n        else:\n            raise NotImplementedError\n\n        self._state_conditioned = state_conditioned\n\n        obs_inputs, current_layer, self._state_shape = CreateInputLayer(state_space)\n\n        # build structure\n        if isinstance(state_space, spaces.Dict):\n            assert isinstance(obs_inputs, dict)\n            assert isinstance(current_layer, dict)\n            self.input_dict = obs_inputs\n            obs_inputs = list(obs_inputs.values())\n            current_layer = tl.layers.Concat(-1)(list(current_layer.values()))\n\n        with tf.name_scope(\'MLP\'):\n            for i, dim in enumerate(hidden_dim_list):\n                current_layer = Dense(n_units=dim, act=activation,\n                                      W_init=w_init, name=\'hidden_layer%d\' % (i + 1))(current_layer)\n\n        with tf.name_scope(\'Output\'):\n            if isinstance(action_space, spaces.Discrete):\n                outputs = Dense(n_units=self.policy_dist.ndim, act=output_activation, W_init=w_init)(current_layer)\n            elif isinstance(action_space, spaces.Box):\n                mu = Dense(n_units=self.policy_dist.ndim, act=output_activation, W_init=w_init)(current_layer)\n\n                if self._state_conditioned:\n                    log_sigma = Dense(n_units=self.policy_dist.ndim, act=None, W_init=w_init)(current_layer)\n                    log_sigma = tl.layers.Lambda(lambda x: tf.clip_by_value(x, log_std_min, log_std_max))(log_sigma)\n                    outputs = [mu, log_sigma]\n                else:\n                    outputs = mu\n                    self._log_sigma = tf.Variable(np.zeros(self.policy_dist.ndim, dtype=np.float32))\n            else:\n                raise NotImplementedError\n\n        # make model\n        super().__init__(inputs=obs_inputs, outputs=outputs, name=name)\n        if isinstance(self._action_space, spaces.Box) and not self._state_conditioned:\n            self.trainable_weights.append(self._log_sigma)\n\n        if trainable:\n            self.train()\n        else:\n            self.eval()\n\n    def __call__(self, states, *args, greedy=False, **kwargs):\n        if isinstance(self._state_space, spaces.Dict):\n            states = np.array(states).transpose([1, 0]).tolist()\n        else:\n            if np.shape(states)[1:] != self.state_shape:\n                raise ValueError(\n                    \'Input state shape error. Shape should be {} but your shape is {}\'.format((None,) + self.state_shape,\n                                                                                              np.shape(states)))\n            states = np.array(states, dtype=np.float32)\n        params = super().__call__(states, *args, **kwargs)\n        if isinstance(self._action_space, spaces.Box) and not self._state_conditioned:\n            params = params, self._log_sigma\n        self.policy_dist.set_param(params)\n        if greedy:\n            result = self.policy_dist.greedy_sample()\n        else:\n            result = self.policy_dist.sample()\n\n        if isinstance(self._action_space, spaces.Box):  # normalize action\n            if greedy:\n                result = result * self._action_scale + self._action_mean\n            else:\n                result, explore = result\n                result = result * self._action_scale + self._action_mean + explore\n\n            result = tf.clip_by_value(result, self._action_space.low, self._action_space.high)\n        return result\n\n    def random_sample(self):\n        """""" generate random actions for exploration """"""\n\n        if isinstance(self._action_space, spaces.Discrete):\n            return np.random.choice(self._action_space.n, 1)[0]\n        else:\n            return np.random.uniform(self._action_space.low, self._action_space.high, self._action_shape)\n\n    @property\n    def state_space(self):\n        return copy.deepcopy(self._state_space)\n\n    @property\n    def action_space(self):\n        return copy.deepcopy(self._action_space)\n\n    @property\n    def state_shape(self):\n        return copy.deepcopy(self._state_shape)\n\n    @property\n    def action_shape(self):\n        return copy.deepcopy(self._action_shape)\n\n\nif __name__ == \'__main__\':\n    import gym\n    from rlzoo.common.env_wrappers import *\n    from rlzoo.common.value_networks import *\n    # EnvName = \'PongNoFrameskip-v4\'\n    # EnvName = \'Pong-v4\'\n    # EnvType = \'atari\'\n\n    EnvName = \'CartPole-v0\'\n    # EnvName = \'Pendulum-v0\'\n    EnvType = \'classic_control\'\n\n    # EnvName = \'BipedalWalker-v2\'\n    # EnvType = \'box2d\'\n\n    # EnvName = \'Ant-v2\'\n    # EnvType = \'mujoco\'\n\n    # EnvName = \'FetchPush-v1\'\n    # EnvType = \'robotics\'\n\n    # EnvName = \'FishSwim-v0\'\n    # EnvType = \'dm_control\'\n\n    # EnvName = \'ReachTarget\'\n    # EnvType = \'rlbench\'\n    # env = build_env(EnvName, EnvType, nenv=2)\n\n    # env = build_env(EnvName, EnvType, state_type=\'vision\', nenv=2)\n    # env = build_env(EnvName, EnvType, state_type=\'vision\')\n    env = build_env(EnvName, EnvType)\n    s = env.reset()\n    print(s)\n\n    # policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space, [64, 64])\n    policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space, [64, 64])\n    a = policy_net([s, s])\n    print(a)\n    # q_net = QNetwork(env.observation_space, env.action_space, [64, 64], state_only=False, dueling=False)\n    # q = q_net([[s], a])\n    print(\'-\'*100)\n    # print(q)\n'"
rlzoo/common/utils.py,1,"b'""""""\nFunctions for utilization.\n\n# Requirements\ntensorflow==2.0.0a0\ntensorlayer==2.0.1\n\n""""""\nimport os\nimport re\n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorlayer as tl\nimport tensorflow as tf\nfrom importlib import import_module\n\n\ndef plot(episode_rewards, algorithm_name, env_name):\n    """"""\n    plot the learning curve, saved as ./img/algorithm_name-env_name.png\n\n    :param episode_rewards: array of floats\n    :param algorithm_name: string\n    :param env_name: string\n    """"""\n    path = os.path.join(\'.\', \'img\')\n    name = algorithm_name + \'-\' + env_name\n    plt.figure(figsize=(10, 5))\n    plt.title(name)\n    plt.plot(np.arange(len(episode_rewards)), episode_rewards)\n    plt.xlabel(\'Episode\')\n    plt.ylabel(\'Episode Reward\')\n    if not os.path.exists(path):\n        os.makedirs(path)\n    plt.savefig(os.path.join(path, name + \'.png\'))\n    plt.close()\n\n\ndef plot_save_log(episode_rewards, algorithm_name, env_name):\n    """"""\n    plot the learning curve, saved as ./img/algorithm_name-env_name.png,\n    and save the rewards log as ./log/algorithm_name-env_name.npy\n\n    :param episode_rewards: array of floats\n    :param algorithm_name: string\n    :param env_name: string\n    """"""\n    path = os.path.join(\'.\', \'log\')\n    name = algorithm_name + \'-\' + env_name\n    plot(episode_rewards, algorithm_name, env_name)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    np.save(os.path.join(path, name), episode_rewards)\n\n\ndef save_model(model, model_name, algorithm_name, env_name):\n    """"""\n    save trained neural network model\n\n    :param model: tensorlayer.models.Model\n    :param model_name: string, e.g. \'model_sac_q1\'\n    :param algorithm_name: string, e.g. \'SAC\'\n    """"""\n    name = algorithm_name + \'-\' + env_name\n    path = os.path.join(\'.\', \'model\', name)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    tl.files.save_npz(model.trainable_weights, os.path.join(path, model_name))\n\n\ndef load_model(model, model_name, algorithm_name, env_name):\n    """"""\n    load saved neural network model\n\n    :param model: tensorlayer.models.Model\n    :param model_name: string, e.g. \'model_sac_q1\'\n    :param algorithm_name: string, e.g. \'SAC\'\n    """"""\n    name = algorithm_name + \'-\' + env_name\n    path = os.path.join(\'.\', \'model\', name)\n    try:\n        param = tl.files.load_npz(path, model_name + \'.npz\')\n        for p0, p1 in zip(model.trainable_weights, param):\n            p0.assign(p1)\n    except Exception as e:\n        print(\'Load Model Fails!\')\n        raise e\n\n\ndef parse_all_args(parser):\n    """""" Parse known and unknown args """"""\n    common_options, other_args = parser.parse_known_args()\n    other_options = dict()\n    index = 0\n    n = len(other_args)\n    float_pattern = re.compile(r\'^[-+]?[-0-9]\\d*\\.\\d*|[-+]?\\.?[0-9]\\d*$\')\n    while index < n:  # only str, int and float type will be parsed\n        if other_args[index].startswith(\'--\'):\n            if other_args[index].__contains__(\'=\'):\n                key, value = other_args[index].split(\'=\')\n                index += 1\n            else:\n                key, value = other_args[index:index + 2]\n                index += 2\n            if re.match(float_pattern, value):\n                value = float(value)\n                if value.is_integer():\n                    value = int(value)\n            other_options[key[2:]] = value\n    return common_options, other_options\n\n\ndef make_env(env_id):\n    env = gym.make(env_id).unwrapped\n    """""" add env wrappers here """"""\n    return env\n\n\ndef get_algorithm_module(algorithm, submodule):\n    """""" Get algorithm module in the corresponding folder """"""\n    return import_module(\'.\'.join([\'rlzoo\', \'algorithms\', algorithm, submodule]))\n\n\ndef call_default_params(env, envtype, alg, default_seed=True):\n    """""" Get the default parameters for training from the default script """"""\n    alg = alg.lower()\n    default = import_module(\'.\'.join([\'rlzoo\', \'algorithms\', alg, \'default\']))\n    params = getattr(default, envtype)(env,\n                                       default_seed)  # need manually set seed in the main script if default_seed = False\n    return params\n\n\ndef set_seed(seed, env=None):\n    """""" set random seed for reproduciblity """"""\n    if isinstance(env, list):\n        assert isinstance(seed, list)\n        for i in range(len(env)):\n            env[i].seed(seed[i])\n        seed = seed[0]  # pick one seed for np and tf\n    elif env is not None:\n        env.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n'"
rlzoo/common/value_networks.py,27,"b'""""""\nFunctions for utilization.\n\n# Requirements\ntensorflow==2.0.0a0\ntensorlayer==2.0.1\n\n""""""\nimport copy\n\nimport numpy as np\nimport tensorlayer as tl\nfrom tensorlayer.layers import BatchNorm, Dense, Input\nfrom tensorlayer.models import Model\n\nfrom rlzoo.common.basic_nets import *\n\n\nclass ValueNetwork(Model):\n    def __init__(self, state_space, hidden_dim_list, w_init=tf.keras.initializers.glorot_normal(),\n                 activation=tf.nn.relu, output_activation=None, trainable=True, name=None):\n        """""" \n        Value network with multiple fully-connected layers or convolutional layers (according to state shape)\n        \n        :param state_space: (gym.spaces) space of the state from gym environments\n        :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n        :param w_init: (callable) weights initialization\n        :param activation: (callable) activation function\n        :param output_activation: (callable or None) output activation function\n        :param trainable: (bool) set training and evaluation mode\n        """"""\n        self._state_space = state_space\n\n        obs_inputs, current_layer, self._state_shape = CreateInputLayer(state_space)\n\n        if isinstance(state_space, spaces.Dict):\n            assert isinstance(obs_inputs, OrderedDict)\n            assert isinstance(current_layer, OrderedDict)\n            self.input_dict = obs_inputs\n            obs_inputs = list(obs_inputs.values())\n            current_layer = tl.layers.Concat(-1)(list(current_layer.values()))\n\n        with tf.name_scope(\'MLP\'):\n            for i, dim in enumerate(hidden_dim_list):\n                current_layer = Dense(n_units=dim, act=activation, W_init=w_init, name=\'hidden_layer%d\' % (i + 1))(\n                    current_layer)\n\n        with tf.name_scope(\'Output\'):\n            outputs = Dense(n_units=1, act=output_activation, W_init=w_init)(current_layer)\n\n        super().__init__(inputs=obs_inputs, outputs=outputs, name=name)\n        if trainable:\n            self.train()\n        else:\n            self.eval()\n\n    def __call__(self, states, *args, **kwargs):\n        if isinstance(self._state_space, spaces.Dict):\n            states = np.array(states).transpose([1, 0]).tolist()\n        else:\n            if np.shape(states)[1:] != self.state_shape:\n                raise ValueError(\n                    \'Input state shape error. Shape can be {} but your shape is {}\'.format((None,) + self.state_shape,\n                                                                                           np.shape(states)))\n            states = np.array(states, dtype=np.float32)\n        return super().__call__(states, *args, **kwargs)\n\n    @property\n    def state_space(self):\n        return copy.deepcopy(self._state_space)\n\n    @property\n    def state_shape(self):\n        return copy.deepcopy(self._state_shape)\n\n\nclass MlpQNetwork(Model):\n    def __init__(self, state_shape, action_shape, hidden_dim_list, \\\n                 w_init=tf.keras.initializers.glorot_normal(), activation=tf.nn.relu, output_activation=None,\n                 trainable=True):\n        """""" \n        Q-value network with multiple fully-connected layers \n\n        Inputs: (state tensor, action tensor)\n\n        :param state_shape: (tuple[int]) shape of the state, for example, (state_dim, ) for single-dimensional state\n        :param action_shape: (tuple[int]) shape of the action, for example, (action_dim, ) for single-dimensional action\n        :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n        :param w_init: (callable) weights initialization\n        :param activation: (callable) activation function\n        :param output_activation: (callable or None) output activation function\n        :param trainable: (bool) set training and evaluation mode\n        """"""\n\n        input_shape = tuple(map(sum, zip(action_shape, state_shape)))\n        input_dim = input_shape[0]\n\n        assert len(state_shape) == 1\n        with tf.name_scope(\'MLP\'):\n            inputs, l = MLP(input_dim, hidden_dim_list, w_init, activation)\n\n        with tf.name_scope(\'Output\'):\n            outputs = Dense(n_units=1, act=output_activation, W_init=w_init)(l)\n\n        super().__init__(inputs=inputs, outputs=outputs)\n        if trainable:\n            self.train()\n        else:\n            self.eval()\n\n\nclass QNetwork(Model):\n    def __init__(self, state_space, action_space, hidden_dim_list,\n                 w_init=tf.keras.initializers.glorot_normal(), activation=tf.nn.relu, output_activation=None,\n                 trainable=True, name=None, state_only=False, dueling=False):\n        """""" Q-value network with multiple fully-connected layers or convolutional layers (according to state shape)\n\n        :param state_space: (gym.spaces) space of the state from gym environments\n        :param action_space: (gym.spaces) space of the action from gym environments\n        :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n        :param w_init: (callable) weights initialization\n        :param activation: (callable) activation function\n        :param output_activation: (callable or None) output activation function\n        :param trainable: (bool) set training and evaluation mode\n        :param name: (str) name the model\n        :param state_only: (bool) only input state or not, available in discrete action space\n        :param dueling: (bool) whether use the dueling output or not, available in discrete action space\n        """"""\n        self._state_space, self._action_space = state_space, action_space\n        self.state_only = state_only\n        self.dueling = dueling\n\n        # create state input layer\n        obs_inputs, current_layer, self._state_shape = CreateInputLayer(state_space)\n\n        # create action input layer\n        if isinstance(self._action_space, spaces.Discrete):\n            self._action_shape = self._action_space.n,\n            if not self.state_only:\n                act_inputs = Input((None,), name=\'Act_Input_Layer\', dtype=tf.int64)\n        elif isinstance(self._action_space, spaces.Box):\n            self._action_shape = self._action_space.shape\n            assert len(self._action_shape) == 1\n            act_inputs = Input((None,) + self._action_shape, name=\'Act_Input_Layer\')\n        else:\n            raise NotImplementedError\n\n        # concat multi-head state\n        if isinstance(state_space, spaces.Dict):\n            assert isinstance(obs_inputs, dict)\n            assert isinstance(current_layer, dict)\n            self.input_dict = obs_inputs\n            obs_inputs = list(obs_inputs.values())\n            current_layer = tl.layers.Concat(-1)(list(current_layer.values()))\n\n        if isinstance(self._action_space, spaces.Box):\n            current_layer = tl.layers.Concat(-1)([current_layer, act_inputs])\n\n        with tf.name_scope(\'QNet_MLP\'):\n            for i, dim in enumerate(hidden_dim_list):\n                current_layer = Dense(n_units=dim, act=activation, W_init=w_init,\n                                      name=\'mlp_hidden_layer%d\' % (i + 1))(current_layer)\n\n        with tf.name_scope(\'Outputs\'):\n            if isinstance(self._action_space, spaces.Discrete):\n                if self.dueling:\n                    v = Dense(1, None, tf.initializers.Orthogonal(1.0))(current_layer)\n                    q = Dense(n_units=self._action_shape[0], act=output_activation, W_init=w_init)(\n                        current_layer)\n                    mean_q = tl.layers.Lambda(lambda x: tf.reduce_mean(x, 1, True))(q)\n                    current_layer = tl.layers.Lambda(lambda x: x[0] + x[1] - x[2])((v, q, mean_q))\n                else:\n                    current_layer = Dense(n_units=self._action_shape[0], act=output_activation, W_init=w_init)(\n                        current_layer)\n\n                if not self.state_only:\n                    act_one_hot = tl.layers.OneHot(depth=self._action_shape[0], axis=1)(\n                        act_inputs)  # discrete action choice to one-hot vector\n                    outputs = tl.layers.Lambda(\n                        lambda x: tf.reduce_sum(tf.reduce_prod(x, axis=0), axis=1))((current_layer, act_one_hot))\n                else:\n                    outputs = current_layer\n\n            elif isinstance(self._action_space, spaces.Box):\n                outputs = Dense(n_units=1, act=output_activation, W_init=w_init)(current_layer)\n            else:\n                raise ValueError(""State Shape Not Accepted!"")\n\n        if isinstance(state_space, spaces.Dict):\n            if self.state_only:\n                super().__init__(inputs=obs_inputs, outputs=outputs, name=name)\n            else:\n                super().__init__(inputs=obs_inputs + [act_inputs], outputs=outputs, name=name)\n        else:\n            if self.state_only:\n                super().__init__(inputs=obs_inputs, outputs=outputs, name=name)\n            else:\n                super().__init__(inputs=[obs_inputs, act_inputs], outputs=outputs, name=name)\n        if trainable:\n            self.train()\n        else:\n            self.eval()\n\n    def __call__(self, inputs, *args, **kwargs):\n        if self.state_only:\n            states = inputs\n        else:\n            states, actions = inputs\n\n        # states and actions must have the same length\n        if not self.state_only and len(states) != len(actions):\n            raise ValueError(\n                \'Length of states and actions not match. States length is {} but actions length is {}\'.format(\n                    len(states),\n                    len(actions)))\n\n        if isinstance(self._state_space, spaces.Dict):\n            states = np.array(states).transpose([1, 0]).tolist()  # batch states to multi-head\n            ssv = list(self._state_shape.values())\n            # check state shape\n            for i, each_head in enumerate(states):\n                if np.shape(each_head)[1:] != ssv[i]:\n                    raise ValueError(\'Input state shape error.\')\n\n        else:\n            if np.shape(states)[1:] != self.state_shape:\n                raise ValueError(\n                    \'Input state shape error. Shape can be {} but your shape is {}\'.format((None,) + self.state_shape,\n                                                                                           np.shape(states)))\n            states = np.array(states, dtype=np.float32)\n\n        if not self.state_only:\n            if isinstance(self._action_space, spaces.Discrete) and np.any(actions % 1):\n                raise ValueError(\'Input float actions in discrete action space\')\n            if isinstance(self._action_space, spaces.Discrete):\n                actions = tf.convert_to_tensor(actions, dtype=tf.int64)\n            elif isinstance(self._action_space, spaces.Box):\n                actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n            if isinstance(self._state_space, spaces.Dict):\n                return super().__call__(states + [actions], *args, **kwargs)\n            else:\n                return super().__call__([states, actions], *args, **kwargs)\n        else:\n            return super().__call__(states, *args, **kwargs)\n\n\n    @property\n    def state_space(self):\n        return copy.deepcopy(self._state_space)\n\n    @property\n    def action_space(self):\n        return copy.deepcopy(self._action_space)\n\n    @property\n    def state_shape(self):\n        return copy.deepcopy(self._state_shape)\n\n    @property\n    def action_shape(self):\n        return copy.deepcopy(self._action_shape)\n\n\nclass NAFLayer(tl.layers.Layer):\n    def __init__(self, action_dim, name=None):\n        super(NAFLayer, self).__init__(name)\n        self.action_dim = action_dim\n\n    def forward(self, inputs):\n        L, u, mu, value = inputs\n        pivot = 0\n        rows = []\n        for idx in range(self.action_dim):\n            offset = self.action_dim - idx\n            diag = tf.exp(tf.slice(L, (0, pivot), (-1, 1)))\n            nondiag = tf.slice(L, (0, pivot + 1), (-1, offset - 1))\n            row = tf.pad(tf.concat([diag, nondiag], 1), ((0, 0), (idx, 0)))\n            pivot += offset\n            rows.append(row)\n        L_T = tf.stack(rows, axis=1)\n        P = tf.matmul(tf.transpose(L_T, (0, 2, 1)), L_T)  # L L^T\n        temp = tf.expand_dims(u - mu, -1)\n        adv = tf.squeeze(-0.5 * tf.matmul(tf.transpose(temp, [0, 2, 1]), tf.matmul(P, temp)), -1)\n        return adv + value\n\n    def build(self, inputs_shape=None):\n        pass\n\n\nclass NAFQNetwork(Model):\n    def __init__(self, state_space, action_space, hidden_dim_list,\n                 w_init=tf.keras.initializers.glorot_normal(), activation=tf.nn.tanh, trainable=True, name=None):\n        """""" NAF Q-value network with multiple fully-connected layers\n\n        :param state_space: (gym.spaces) space of the state from gym environments\n        :param action_space: (gym.spaces) space of the action from gym environments\n        :param hidden_dim_list: (list[int]) a list of dimensions of hidden layers\n        :param w_init: (callable) weights initialization\n        :param activation: (callable) activation function\n        :param trainable: (bool) set training and evaluation mode\n        :param name: (str) name the model\n        """"""\n        assert isinstance(action_space, spaces.Box)\n        self._state_space, self._action_space = state_space, action_space\n        self._action_shape = self._action_space.shape\n        assert len(self._action_shape) == 1\n        act_inputs = Input((None,) + self._action_shape, name=\'Act_Input_Layer\')\n\n        # create state input layer\n        obs_inputs, current_layer, self._state_shape = CreateInputLayer(state_space)\n\n        # concat multi-head state\n        if isinstance(state_space, spaces.Dict):\n            assert isinstance(obs_inputs, dict)\n            assert isinstance(current_layer, dict)\n            self.input_dict = obs_inputs\n            obs_inputs = list(obs_inputs.values())\n            current_layer = tl.layers.Concat(-1)(list(current_layer.values()))\n\n        # calculate value\n        current_layer = BatchNorm()(current_layer)\n        with tf.name_scope(\'NAF_VALUE_MLP\'):\n            for i, dim in enumerate(hidden_dim_list):\n                current_layer = Dense(n_units=dim, act=activation, W_init=w_init,\n                                      name=\'mlp_hidden_layer%d\' % (i + 1))(current_layer)\n            value = Dense(n_units=1, W_init=w_init, name=\'naf_value_mlp_output\')(current_layer)\n\n        # calculate advantange and Q-value\n        dim = self._action_shape[0]\n        with tf.name_scope(\'NAF_ADVANTAGE\'):\n            mu = Dense(n_units=dim, act=activation, W_init=w_init, name=\'mu\')(current_layer)\n            L = Dense(n_units=int((dim * (dim + 1)) / 2), W_init=w_init, name=\'L\')(current_layer)\n            qvalue = NAFLayer(dim)([L, act_inputs, mu, value])\n\n        super().__init__(inputs=[obs_inputs, act_inputs], outputs=qvalue, name=name)\n        if trainable:\n            self.train()\n        else:\n            self.eval()\n\n    def __call__(self, inputs, *args, **kwargs):\n        states, actions = inputs\n\n        # states and actions must have the same length\n        if len(states) != len(actions):\n            raise ValueError(\n                \'Length of states and actions not match. States length is {} but actions length is {}\'.format(\n                    len(states),\n                    len(actions)))\n\n        if isinstance(self._state_space, spaces.Dict):\n            states = np.array(states).transpose([1, 0]).tolist()  # batch states to multi-head\n            ssv = list(self._state_shape.values())\n            # check state shape\n            for i, each_head in enumerate(states):\n                if np.shape(each_head)[1:] != ssv[i]:\n                    raise ValueError(\'Input state shape error.\')\n\n        else:\n            if np.shape(states)[1:] != self.state_shape:\n                raise ValueError(\n                    \'Input state shape error. Shape can be {} but your shape is {}\'.format((None,) + self.state_shape,\n                                                                                           np.shape(states)))\n            states = np.array(states, dtype=np.float32)\n\n        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n        if isinstance(self._state_space, spaces.Dict):\n            return super().__call__(states + [actions], *args, **kwargs)\n        else:\n            return super().__call__([states, actions], *args, **kwargs)\n\n    @property\n    def state_space(self):\n        return copy.deepcopy(self._state_space)\n\n    @property\n    def action_space(self):\n        return copy.deepcopy(self._action_space)\n\n    @property\n    def state_shape(self):\n        return copy.deepcopy(self._state_shape)\n\n    @property\n    def action_shape(self):\n        return copy.deepcopy(self._action_shape)\n'"
rlzoo/interactive/common.py,0,"b'import decimal\n\nimport ipywidgets as widgets\nimport numpy as np\n\nborder_list = [None, \'hidden\', \'dotted\', \'dashed\', \'solid\', \'double\',\n               \'groove\', \'ridge\', \'inset\', \'outset\', \'inherit\']\n\n\nclass NumInput(widgets.HBox):\n\n    def __init__(self, init_value, step=None, range_min=None, range_max=None):\n        self.range = [range_min, range_max]\n        range_min = 0 if range_min is None else range_min\n        range_max = init_value * 2 if range_max is None else range_max\n        self.range_size = max([range_max - init_value, init_value - range_min])\n        if step is None:\n            fs = decimal.Decimal(str(init_value)).as_tuple().exponent\n            self.decimals = -fs\n            step = np.round(np.power(0.1, self.decimals), self.decimals)\n        else:\n            fs = decimal.Decimal(str(step)).as_tuple().exponent\n            fv = decimal.Decimal(str(init_value)).as_tuple().exponent\n            self.decimals = -min(fs, fv)\n\n        self.step = step\n\n        self.slider = widgets.FloatSlider(\n            value=init_value,\n            min=range_min,\n            max=range_max,\n            step=step,\n            description=\'Slider input:\',\n            disabled=False,\n            continuous_update=False,\n            orientation=\'horizontal\',\n            readout=True,\n            readout_format=\'.\' + str(self.decimals) + \'f\'\n        )\n\n        self.text = widgets.FloatText(\n            value=self.slider.value,\n            description=\'Manual input:\',\n            disabled=False\n        )\n\n        def __extend_max(change):\n            num_new = np.around(change[\'new\'], decimals=self.decimals)\n            num_old = change[\'old\']\n            if num_new > num_old:\n                if num_new - num_old > (self.slider.max - num_old) / 2:\n                    self.range_size *= 2\n                else:\n                    self.range_size *= 0.5\n            else:\n                if num_old - num_new > (num_old - self.slider.min) / 2:\n                    self.range_size *= 2\n                else:\n                    self.range_size *= 0.5\n\n            if self.range_size < self.step * 10:\n                self.range_size = self.step * 10\n\n            self.slider.min = num_new - self.range_size if self.range[0] is None else self.range[0]\n            self.slider.max = num_new + self.range_size if self.range[1] is None else self.range[1]\n            self.slider.value = num_new\n            self.text.value = num_new\n\n        self.slider.observe(__extend_max, names=\'value\')\n        self.text.observe(__extend_max, names=\'value\')\n        box_layout = widgets.Layout(display=\'flex\',\n                                    align_items=\'stretch\',\n                                    justify_content=\'center\', )\n        #         self.frame = widgets.HBox([self.slider, self.text], layout=box_layout)\n        super().__init__([self.slider, self.text], layout=box_layout)\n        self._int_type = False\n        if (isinstance(init_value, int) or isinstance(init_value, np.int16) \\\n            or isinstance(init_value, np.int32) or isinstance(init_value, np.int64)) \\\n                and step % 1 == 0:\n            self._int_type = True\n\n    @property\n    def value(self):\n        result = self.slider.value\n        if self._int_type:\n            result = int(result)\n        return result\n\n\nclass Border:\n    def __init__(self, element_list, description=None, size=5, style=0):\n        if not isinstance(element_list, list):\n            element_list = [element_list]\n\n        box_layout = widgets.Layout(display=\'flex\',\n                                    flex_flow=\'column\',\n                                    align_items=\'flex-start\',\n                                    align_content=\'flex-start\',\n                                    #                                     justify_content=\'center\',\n                                    justify_content=\'space-around\',\n                                    border=border_list[2]\n                                    )\n        frame = widgets.Box(children=element_list, layout=box_layout)\n\n        if description is not None:\n            caption = widgets.HTML(value=""<font size=""+str(size)+""><B>""+description+""</B>"")\n            children = [caption, frame]\n        else:\n            children = [frame]\n\n        box_layout = widgets.Layout(display=\'flex\',\n                                    flex_flow=\'column\',\n                                    align_items=\'center\',\n                                    justify_content=\'center\',\n                                    border=border_list[style], )\n        self.frame = widgets.Box(children=children, layout=box_layout)\n\n\nclass InfoDisplay:\n    def __init__(self, description, detail):\n        label = widgets.Label(description)\n        self.data = widgets.Label(detail)\n        self.frame = widgets.HBox([label, self.data], layout=widgets.Layout(justify_content=\'flex-start\', ))\n#                                                                           border=border_list[2]))\n'"
rlzoo/interactive/components.py,0,"b'from __future__ import print_function\nimport copy\nfrom collections import OrderedDict\n\nfrom ipywidgets import Layout\nfrom ipywidgets import GridspecLayout\n\nfrom IPython.display import clear_output\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom gym import spaces\n\nfrom rlzoo.common.env_list import all_env_list\nfrom rlzoo.common.utils import *\nfrom rlzoo.interactive.common import *\n\nall_env_list = OrderedDict(sorted(all_env_list.items()))\n\n\nclass EnvironmentSelector(widgets.VBox):\n    def __init__(self):\n        env_list = all_env_list\n        # al = list(env_list.keys())\n        al = [\'atari\', \'classic_control\', \'box2d\', \'mujoco\', \'robotics\', \'dm_control\', \'rlbench\']\n        description = \'Environment Selector\'\n        caption = widgets.HTML(value=""<font size=4><B>"" + description + ""</B>"")\n\n        text_0 = widgets.Label(""Choose your environment"")\n\n        self.env_type = widgets.Dropdown(\n            options=al,\n            value=al[0],\n            description=\'env type:\',\n            disabled=False,\n        )\n\n        self.env_name = widgets.Dropdown(\n            options=env_list[al[0]],\n            value=env_list[al[0]][0],\n            description=\'env name:\',\n            disabled=False,\n        )\n        env_select_box = widgets.VBox([text_0, self.env_type, self.env_name])\n\n        text_1 = widgets.Label(value=""Environment settings"")\n\n        self.env_num = widgets.IntText(\n            value=1,\n            description=\'multi envs:\',\n            disabled=False,\n            min=1,\n            #             layout=Layout(width=\'150px\')\n        )\n\n        self.env_state = widgets.Dropdown(\n            options=[\'default\'],\n            value=\'default\',\n            description=\'state type:\',\n            disabled=False,\n        )\n\n        #         self.create_button = widgets.Button(\n        #             description=\'Create!\',\n        #             disabled=False,\n        #             tooltip=\'Create\',\n        #             icon=\'check\'\n        #         )\n\n        #         multi_box = widgets.HBox([self.env_multi, self.env_num], layout=Layout(justify_content=\'flex-start\'))\n        env_setting_box = widgets.VBox([text_1, self.env_num, self.env_state])\n\n        select_box = widgets.HBox([env_select_box, env_setting_box],\n                                  layout=Layout(justify_content=\'Center\'))\n\n        #         self.frame = widgets.VBox([select_box, widgets.Box([self.create_button],\n        #                                                            layout=Layout(justify_content=\'Center\'))])\n        #         self.frame = widgets.AppLayout(left_sidebar=select_box, center=info_border.frame)\n\n        def env_type_change(change):\n            d = env_list[self.env_type.value]\n            self.env_name.options = d\n            self.env_name.value = d[0]\n            if self.env_type.value == \'rlbench\':\n                self.env_state.options = [\'state\', \'vision\']\n                self.env_state.value = \'state\'\n                self.env_num.value = 1\n                self.env_num.disabled = True\n            else:\n                self.env_state.options = [\'default\']\n                self.env_state.value = \'default\'\n                self.env_num.disabled = False\n\n        #         def create_env(c):  # todo the program will be blocked if rlbench env is created here\n        #             if self.env_type.value == \'rlbench\':\n        #                 print(self.env_name.value, self.env_type.value, self.env_num.value, self.env_state.value)\n        #                 self._env = build_env(self.env_name.value, self.env_type.value,\n        #                                      nenv=self.env_num.value, state_type=self.env_state.value)\n        #             self._env = build_env(self.env_name.value, self.env_type.value, nenv=self.env_num.value)\n        #             print(\'Environment created successfully!\')\n\n        def change_nenv(c):\n            if self.env_num.value < 1:\n                self.env_num.value = 1\n\n        self.env_num.observe(change_nenv, names=\'value\')\n        self.env_type.observe(env_type_change, names=\'value\')\n\n        #         self.create_button.on_click(create_env)\n\n        super().__init__([caption, select_box], layout=widgets.Layout(align_items=\'center\', ))\n\n    @property\n    def value(self):\n        return {\'env_id\': self.env_name.value,\n                \'env_type\': self.env_type.value,\n                \'nenv\': self.env_num.value,\n                \'state_type\': self.env_state.value}\n\n\n#     @property\n#     def env(self):\n#         return self._env\n\nclass SpaceInfoViewer(widgets.Box):\n    def __init__(self, sp):\n        assert isinstance(sp, spaces.Space)\n        if isinstance(sp, spaces.Dict):\n            it = list(sp.spaces.items())\n            info = GridspecLayout(len(it), 2)\n            for i, v in enumerate(it):\n                info[i, 0], info[i, 1] = widgets.Label(v[0]), widgets.Label(str(v[1]))\n        else:\n            info = widgets.Label(str(sp))\n        super().__init__([info])\n\n\nclass EnvInfoViewer(widgets.VBox):\n    def __init__(self, env):\n        if isinstance(env, list):\n            env = env[0]\n        env_obs = SpaceInfoViewer(env.observation_space)\n        env_act = SpaceInfoViewer(env.action_space)\n        tips = None\n        if isinstance(env.action_space, gym.spaces.Discrete):\n            tips = \'The action space is discrete.\'\n        elif isinstance(env.action_space, gym.spaces.Box):\n            tips = \'The action space is continuous.\'\n\n        description = \'Environment Information\'\n        caption = widgets.HTML(value=""<font size=4><B>"" + description + ""</B>"")\n\n        a00, a01 = widgets.Label(\'Environment name:\'), widgets.Label(env.spec.id)\n        a10, a11 = widgets.Label(\'Observation space:\'), env_obs\n        a20, a21 = widgets.Label(\'Action space:\'), env_act\n\n        if tips is None:\n            # use GirdBox instead of GridspecLayout to ensure each row has a different height\n            info = widgets.GridBox(children=[a00, a01, a10, a11, a20, a21],\n                                   layout=Layout(grid_template_areas=""""""\n                                               ""a00 a01""\n                                               ""a10 a11""\n                                               ""a20 a21""\n                                               """"""))\n        else:\n            t0 = widgets.Label(\'Tips:\')\n            t1 = widgets.Label(tips)\n            info = widgets.GridBox(children=[a00, a01, a10, a11, a20, a21, t0, t1],\n                                   layout=Layout(grid_template_areas=""""""\n                                               ""a00 a01""\n                                               ""a10 a11""\n                                               ""a20 a21""\n                                               ""t0 t1""\n                                               """"""))\n\n        super().__init__([caption, info], layout=widgets.Layout(align_items=\'center\', ))\n\n\nall_alg_list = [\'A3C\', \'AC\', \'DDPG\', \'DPPO\', \'DQN\', \'PG\', \'PPO\', \'SAC\', \'TD3\', \'TRPO\']\nall_alg_dict = {\'discrete_action_space\': [\'AC\', \'DQN\', \'PG\', \'PPO\', \'TRPO\'],\n                \'continuous_action_space\': [\'AC\', \'DDPG\', \'PG\', \'PPO\', \'SAC\', \'TD3\', \'TRPO\'],\n                \'multi_env\': [\'A3C\', \'DPPO\']\n                }\n\n\nclass AlgorithmSelector(widgets.VBox):\n    def __init__(self, env):\n        description = \'Algorithm Selector\'\n        caption = widgets.HTML(value=""<font size=4><B>"" + description + ""</B>"")\n        info = \'Supported algorithms are shown below\'\n        if isinstance(env, list):\n            #             info = \'Distributed algorithms are shown below\'\n            table = all_alg_dict[\'multi_env\']\n            self.env_id = env[0].spec.id\n        elif isinstance(env.action_space, gym.spaces.Discrete):\n            #             info = \'Algorithms which support discrete action space are shown below\'\n            table = all_alg_dict[\'discrete_action_space\']\n            self.env_id = env.spec.id\n        elif isinstance(env.action_space, gym.spaces.Box):\n            #             info = \'Algorithms which support continuous action space are shown below\'\n            table = all_alg_dict[\'continuous_action_space\']\n            self.env_id = env.spec.id\n        else:\n            raise ValueError(\'Unsupported environment\')\n\n        self.algo_name = widgets.Dropdown(\n            options=table,\n            value=table[0],\n            description=\'Algorithms:\',\n            disabled=False,\n        )\n\n        super().__init__([caption, widgets.Label(info), self.algo_name],\n                         layout=widgets.Layout(align_items=\'center\', ))\n\n    @property\n    def value(self):\n        return self.algo_name.value\n\n\ndef TransInput(value):\n    if isinstance(value, bool):\n        return widgets.Checkbox(value=value, description=\'\', disabled=False, indent=False)\n    elif isinstance(value, int) or isinstance(value, float) \\\n            or isinstance(value, np.int16) or isinstance(value, np.float16) \\\n            or isinstance(value, np.int32) or isinstance(value, np.float32) \\\n            or isinstance(value, np.int64) or isinstance(value, np.float64) \\\n            or isinstance(value, np.float128):\n        return NumInput(value)\n    else:\n        return widgets.Label(value)\n\n\nclass AlgoInfoViewer(widgets.VBox):\n    def __init__(self, alg_selector, org_alg_params, org_learn_params):\n        alg_params, learn_params = copy.deepcopy(org_alg_params), copy.deepcopy(org_learn_params)\n\n        # ---------------- alg_params --------------- #\n        description = \'Algorithm Parameters\'\n        alg_caption = widgets.HTML(value=""<font size=4><B>"" + description + ""</B>"")\n        net_label = widgets.Label(\'Network information:\')\n        show_net = lambda net: widgets.VBox([widgets.Label(str(layer)) for layer in net.all_layers])\n\n        n = np.ndim(alg_params[\'net_list\'])\n        if n == 1:\n            model_net = alg_params[\'net_list\']\n        elif n == 2:\n            model_net = alg_params[\'net_list\'][0]\n\n        net_info = widgets.VBox([widgets.VBox([widgets.Label(str(net.__class__.__name__)),\n                                               show_net(net), ],\n                                              layout=widgets.Layout(border=border_list[2],\n                                                                    align_items=\'center\',\n                                                                    align_content=\'center\'\n                                                                    )\n                                              ) for net in model_net])\n        self._net_list = alg_params[\'net_list\']\n        del alg_params[\'net_list\']\n\n        opt_label = widgets.Label(\'Optimizer information:\')\n\n        def show_params(params):\n            params = copy.deepcopy(params)\n            n = len(params)\n            frame = widgets.GridspecLayout(n, 2, layout=widgets.Layout(border=border_list[2], ))\n            show_info = lambda k: [widgets.Label(str(k)), widgets.Label(str(params[k]))]\n            frame[0, 0], frame[0, 1] = show_info(\'name\')\n            frame[1, 0], frame[1, 1] = show_info(\'learning_rate\')\n            del params[\'name\']\n            del params[\'learning_rate\']\n            for i, k in enumerate(sorted(params.keys())):\n                if k != \'name\' and k != \'learning_rate\':\n                    frame[2 + i, 0], frame[2 + i, 1] = show_info(k)\n            return frame\n\n        opt_info = widgets.VBox([show_params(n.get_config()) for n in alg_params[\'optimizers_list\']])\n        self._optimizers_list = alg_params[\'optimizers_list\']\n        del alg_params[\'optimizers_list\']\n\n        stu_frame = widgets.GridBox(children=[net_label, net_info, opt_label, opt_info],\n                                    layout=Layout(grid_template_areas=""""""\n                                           ""net_label net_info""\n                                           ""opt_label opt_info""\n                                           """"""))\n\n        alg_sel_dict = dict()\n        sk = sorted(alg_params.keys())\n        n = len(sk) + 1\n        alg_param_sel = widgets.GridspecLayout(n, 2)\n        b = 0\n        if \'method\' in sk:\n            module = widgets.RadioButtons(options=[\'penalty\', \'clip\'], disabled=False)\n            alg_param_sel[0, 0], alg_param_sel[0, 1] = widgets.Label(\'method\'), module\n            alg_sel_dict[\'method\'] = module\n            sk.remove(\'method\')\n            b += 1\n\n        for i, k in enumerate(sk):\n            module = TransInput(alg_params[k])\n            alg_sel_dict[k] = module\n            if k == \'dueling\':\n                module.disabled = True\n            alg_param_sel[i + b, 0], alg_param_sel[i + b, 1] = widgets.Label(k), module\n\n        alg_param_box = widgets.VBox([alg_caption, stu_frame, alg_param_sel], )\n        name = alg_selector.value + \'-\' + alg_selector.env_id\n        path = os.path.join(\'.\', \'model\', name)\n        alg_param_sel[n - 1, 0] = widgets.Label(\'model save path\')\n        alg_param_sel[n - 1, 1] = widgets.Label(path)\n\n        self.alg_sel_dict = alg_sel_dict\n        # ================== alg_params ================= #\n\n        # ----------------- learn_params ---------------- #\n        description = \'Learn Parameters\'\n        learn_caption = widgets.HTML(value=""<font size=4><B>"" + description + ""</B>"")\n\n        learn_sel_dict = dict()\n        sk = sorted(learn_params.keys())\n\n        n = len(sk)\n        if \'mode\' not in sk: n += 1\n        if \'render\' not in sk: n += 1\n        learn_param_sel = widgets.GridspecLayout(n, 2)\n\n        module = widgets.RadioButtons(options=[\'train\', \'test\'], disabled=False)\n        learn_param_sel[0, 0], learn_param_sel[0, 1] = widgets.Label(\'mode\'), module\n        learn_sel_dict[\'mode\'] = module\n        try:\n            sk.remove(\'mode\')\n        except:\n            pass\n\n        module = widgets.Checkbox(value=False, description=\'\', disabled=False, indent=False)\n        learn_param_sel[1, 0], learn_param_sel[1, 1] = widgets.Label(\'render\'), module\n        learn_sel_dict[\'render\'] = module\n        try:\n            sk.remove(\'render\')\n        except:\n            pass\n\n        for i, k in enumerate(sk):\n            module = TransInput(learn_params[k])\n            learn_sel_dict[k] = module\n            learn_param_sel[i + 2, 0], learn_param_sel[i + 2, 1] = widgets.Label(k), module\n        learn_param_box = widgets.VBox([learn_caption, learn_param_sel],\n                                       #                                      layout=Layout(align_items=\'center\',)\n                                       )\n        self.learn_sel_dict = learn_sel_dict\n        # ================= learn_params ================ #\n\n        b = widgets.Output(layout=widgets.Layout(border=\'solid\'))\n\n        self.smooth_factor_slider = widgets.FloatSlider(\n            value=0.8,\n            min=0,\n            max=1,\n            step=0.01,\n            description=\'learning curve smooth factor\',\n            disabled=False,\n            continuous_update=False,\n            orientation=\'horizontal\',\n            readout=True,\n            readout_format=\'.2f\',\n            style={\'description_width\': \'initial\'},\n        )\n        super().__init__([alg_param_box, b, learn_param_box, b, self.smooth_factor_slider])\n\n    @property\n    def alg_params(self):\n        result = {\'net_list\': self._net_list, \'optimizers_list\': self._optimizers_list}\n        for k in self.alg_sel_dict.keys():\n            result[k] = self.alg_sel_dict[k].value\n        return result\n\n    @property\n    def smooth_factor(self):\n        return self.smooth_factor_slider.value\n\n    @property\n    def learn_params(self):\n        result = dict()\n        for k in self.learn_sel_dict.keys():\n            result[k] = self.learn_sel_dict[k].value\n        return result\n\n\nclass RevOutput(widgets.Output):\n    def _append_stream_output(self, text, stream_name):\n        """"""Append a stream output.""""""\n        self.outputs = (\n                           {\'output_type\': \'stream\', \'name\': stream_name, \'text\': text},\n                       ) + self.outputs\n\n    def append_display_data(self, display_object):\n        """"""Append a display object as an output.\n\n        Parameters\n        ----------\n        display_object : IPython.core.display.DisplayObject\n            The object to display (e.g., an instance of\n            `IPython.display.Markdown` or `IPython.display.Image`).\n        """"""\n        fmt = InteractiveShell.instance().display_formatter.format\n        data, metadata = fmt(display_object)\n        self.outputs = (\n                           {\n                               \'output_type\': \'display_data\',\n                               \'data\': data,\n                               \'metadata\': metadata\n                           },\n                       ) + self.outputs\n\n\nclass OutputMonitor(widgets.HBox):\n    def __init__(self, learn_params, smooth_factor):\n        max_num = learn_params[\'train_episodes\'] if learn_params[\'mode\'] == \'train\' else learn_params[\'test_episodes\']\n        self.progress = widgets.FloatProgress(value=0.0, min=0.0, max=max_num, description=\'Progress\')\n\n        self.plot_out = widgets.Output(layout=widgets.Layout(width=\'350px\',\n                                                             height=\'250px\', ))\n        self.smooth_factor = smooth_factor\n        # self.smooth_factor = widgets.FloatSlider(\n        #     value=self.sf,\n        #     min=0,\n        #     max=1,\n        #     step=0.01,\n        #     description=\'smooth factor\',\n        #     disabled=False,\n        #     continuous_update=False,\n        #     orientation=\'horizontal\',\n        #     readout=True,\n        #     readout_format=\'.2f\'\n        # )\n\n        # def link(c):\n        #     self.sf = self.smooth_factor.value\n\n        # self.smooth_factor.observe(link, \'value\')\n        # plot_out = widgets.VBox([widgets.Label(\'Learning curve\'), self.plot_out, self.smooth_factor])\n        plot_out = widgets.VBox([widgets.Label(\'Learning curve\'), self.plot_out])\n\n        self.print_out = RevOutput(layout=widgets.Layout(overflow=\'scroll\',\n                                                         width=\'60%\',\n                                                         height=\'300px\',\n                                                         # display=\'flex\',\n                                                         # positioning=\'bottom\',\n                                                         border=\'1px solid black\',\n                                                         ))\n        self.plot_func([])\n        super().__init__([widgets.VBox([plot_out, self.progress]), self.print_out])\n\n    def plot_func(self, datas):\n        # datas = signal.lfilter([1 - self.smooth_factor], [1, -self.smooth_factor], datas, axis=0)\n        if datas:\n            disD = [datas[0]]\n            for d in datas[1:]:\n                disD.append(disD[-1] * self.smooth_factor + d * (1 - self.smooth_factor))\n        else:\n            disD = datas\n        with self.plot_out:\n            self.progress.value = len(disD)\n            plt.plot(disD)\n            clear_output(wait=True)\n            plt.show()\n'"
rlzoo/algorithms/a3c/__init__.py,0,b''
rlzoo/algorithms/a3c/a3c.py,9,"b'""""""\nAsynchronous Advantage Actor Critic (A3C) with Continuous Action Space.\n\nActor Critic History\n----------------------\nA3C > DDPG (for continuous action space) > AC\n\nAdvantage\n----------\nTrain faster and more stable than AC.\n\nDisadvantage\n-------------\nHave bias.\n\nReference\n----------\nOriginal Paper: https://arxiv.org/pdf/1602.01783.pdf\nMorvanZhou\'s tutorial: https://morvanzhou.github.io/tutorials/\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/\nEnvironment\n-----------\nBipedalWalker-v2 : https://gym.openai.com/envs/BipedalWalker-v2\n\nReward is given for moving forward, total 300+ points up to the far end.\nIf the robot falls, it gets -100. Applying motor torque costs a small amount of\npoints, more optimal agent will get better score. State consists of hull angle\nspeed, angular velocity, horizontal speed, vertical speed, position of joints\nand joints angular speed, legs contact with ground, and 10 lidar rangefinder\nmeasurements. There\'s no coordinates in the state vector.\n\nPrerequisites\n--------------\ntensorflow 2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer 2.0.0\n&&\npip install box2d box2d-kengz --user\n\n""""""\n\nimport multiprocessing\nimport threading\nimport time\n\nfrom rlzoo.common.utils import *\nfrom rlzoo.common.buffer import *\n\n\n# tl.logging.set_verbosity(tl.logging.DEBUG)\n###################  Asynchronous Advantage Actor Critic (A3C)  ####################################\nclass ACNet(object):\n\n    def __init__(self, net_list, scope, entropy_beta):\n        self.ENTROPY_BETA = entropy_beta\n        self.actor, self.critic = net_list\n\n    # @tf.function  # shouldn\'t use here!\n    def update_global(\n            self, buffer_s, buffer_a, buffer_v_target, globalAC\n    ):  # refer to the global Actor-Crtic network for updating it with samples\n        """""" update the global critic """"""\n        with tf.GradientTape() as tape:\n            self.v = self.critic(buffer_s)\n            self.v_target = buffer_v_target\n            td = tf.subtract(self.v_target, self.v, name=\'TD_error\')\n            self.c_loss = tf.reduce_mean(tf.square(td))\n        self.c_grads = tape.gradient(self.c_loss, self.critic.trainable_weights)\n        OPT_C.apply_gradients(zip(self.c_grads, globalAC.critic.trainable_weights))  # local grads applies to global net\n        del tape  # Drop the reference to the tape\n        """""" update the global actor """"""\n        with tf.GradientTape() as tape:\n            self.actor(buffer_s)\n            self.a_his = buffer_a  # float32\n            log_prob = self.actor.policy_dist.logp(self.a_his)\n            exp_v = log_prob * td  # td is from the critic part, no gradients for it\n            entropy = self.actor.policy_dist.entropy()  # encourage exploration\n            self.exp_v = self.ENTROPY_BETA * entropy + exp_v\n            self.a_loss = tf.reduce_mean(-self.exp_v)\n        self.a_grads = tape.gradient(self.a_loss, self.actor.trainable_weights)\n        OPT_A.apply_gradients(zip(self.a_grads, globalAC.actor.trainable_weights))  # local grads applies to global net\n        del tape  # Drop the reference to the tape\n\n    # @tf.function\n    def pull_global(self, globalAC):  # run by a local, pull weights from the global nets\n        for l_p, g_p in zip(self.actor.trainable_weights, globalAC.actor.trainable_weights):\n            l_p.assign(g_p)\n        for l_p, g_p in zip(self.critic.trainable_weights, globalAC.critic.trainable_weights):\n            l_p.assign(g_p)\n\n    def get_action(self, s):  # run by a local\n        return self.actor(np.array([s])).numpy()[0]\n\n    def get_action_greedy(self, s):\n        return self.actor(np.array([s]), greedy=True)[0].numpy()\n\n    def save_ckpt(self, env_name):  # save trained weights\n        save_model(self.actor, \'model_actor\', \'A3C\', env_name)\n        save_model(self.critic, \'model_critic\', \'A3C\', env_name)\n\n    def load_ckpt(self, env_name):  # load trained weights\n        load_model(self.actor, \'model_actor\', \'A3C\', env_name)\n        load_model(self.critic, \'model_critic\', \'A3C\', env_name)\n\n\nclass Worker(object):\n    def __init__(self, env, net_list, name, train_episodes, max_steps, gamma, update_itr, entropy_beta,\n                 render, plot_func):\n        self.name = name\n        self.AC = ACNet(net_list, name, entropy_beta)\n        self.MAX_GLOBAL_EP = train_episodes\n        self.UPDATE_GLOBAL_ITER = update_itr\n        self.GAMMA = gamma\n        self.env = env\n        self.max_steps = max_steps\n        self.render = render\n        self.plot_func = plot_func\n\n    def work(self, globalAC):\n        global COORD, GLOBAL_RUNNING_R, GLOBAL_EP, OPT_A, OPT_C, t0, SAVE_INTERVAL\n        total_step = 1\n        save_cnt = 1\n        buffer_s, buffer_a, buffer_r = [], [], []\n        while not COORD.should_stop() and GLOBAL_EP < self.MAX_GLOBAL_EP:\n            s = self.env.reset()\n            ep_r = 0\n            for epi_step in range(self.max_steps):\n                # visualize Worker_0 during training\n                if self.name == \'Worker_0\' and total_step % 30 == 0 and self.render:\n                    self.env.render()\n                s = s.astype(\'float32\')  # double to float\n                a = self.AC.get_action(s)\n                s_, r, done, _info = self.env.step(a)\n\n                s_ = s_.astype(\'float32\')  # double to float\n\n                ep_r += r\n                buffer_s.append(s)\n                buffer_a.append(a)\n                buffer_r.append(r)\n\n                if total_step % self.UPDATE_GLOBAL_ITER == 0 or done:  # update global and assign to local net\n\n                    if done:\n                        v_s_ = 0  # terminal\n                    else:\n                        v_s_ = self.AC.critic(s_[np.newaxis, :])[0, 0]  # reduce dim from 2 to 0\n\n                    buffer_v_target = []\n\n                    for r in buffer_r[::-1]:  # reverse buffer r\n                        v_s_ = r + self.GAMMA * v_s_\n                        buffer_v_target.append(v_s_)\n\n                    buffer_v_target.reverse()\n                    buffer_s = buffer_s if len(buffer_s[0].shape) > 1 else np.vstack(\n                        buffer_s)  # no vstack for raw-pixel input\n                    buffer_a, buffer_v_target = (\n                        np.vstack(buffer_a), np.vstack(buffer_v_target)\n                    )\n\n                    # update gradients on global network\n                    self.AC.update_global(buffer_s, buffer_a, buffer_v_target.astype(\'float32\'), globalAC)\n                    buffer_s, buffer_a, buffer_r = [], [], []\n\n                    # update local network from global network\n                    self.AC.pull_global(globalAC)\n\n                s = s_\n                total_step += 1\n                if self.name == \'Worker_0\' and GLOBAL_EP >= save_cnt * SAVE_INTERVAL:\n                    plot_save_log(GLOBAL_RUNNING_R, algorithm_name=self.name, env_name=self.env.spec.id)\n                    globalAC.save_ckpt(env_name=self.env.spec.id)\n                    save_cnt += 1\n                if done:\n                    break\n\n            GLOBAL_RUNNING_R.append(ep_r)\n            if self.name == \'Worker_0\' and self.plot_func is not None:\n                self.plot_func(GLOBAL_RUNNING_R)\n            print(\'{}, Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\' \\\n                  .format(self.name, GLOBAL_EP, self.MAX_GLOBAL_EP, ep_r, time.time() - t0))\n            GLOBAL_EP += 1\n\n\nclass A3C():\n    def __init__(self, net_list, optimizers_list, entropy_beta=0.005):\n        """"""\n        :param entropy_beta: factor for entropy boosted exploration\n        """"""\n        self.net_list = net_list\n        self.optimizers_list = optimizers_list\n        self.GLOBAL_AC = ACNet(self.net_list[0], \'global\', entropy_beta)  # we only need its params\n        self.entropy_beta = entropy_beta\n        self.name = \'A3C\'\n\n    def learn(self, env, train_episodes=1000, test_episodes=10, max_steps=150, render=False, n_workers=1, update_itr=10,\n              gamma=0.99, save_interval=500, mode=\'train\', plot_func=None):\n\n        """"""\n        :param env: a list of same learning environments\n        :param train_episodes:  total number of episodes for training\n        :param test_episodes:  total number of episodes for testing\n        :param max_steps:  maximum number of steps for one episode\n        :param render: render or not\n        :param n_workers: manually set number of workers\n        :param update_itr: update global policy after several episodes\n        :param gamma: reward discount factor\n        :param save_interval: timesteps for saving the weights and plotting the results\n        :param mode: train or test\n        :param plot_func: additional function for interactive module\n        """"""\n        global COORD, GLOBAL_RUNNING_R, GLOBAL_EP, OPT_A, OPT_C, t0, SAVE_INTERVAL\n        SAVE_INTERVAL = save_interval\n        COORD = tf.train.Coordinator()\n        GLOBAL_RUNNING_R = []\n        GLOBAL_EP = 0  # will increase during training, stop training when it >= MAX_GLOBAL_EP\n        N_WORKERS = n_workers if n_workers > 0 else multiprocessing.cpu_count()\n\n        self.plot_func = plot_func\n        if mode == \'train\':\n            # ============================= TRAINING ===============================\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env[0].spec.id))\n            t0 = time.time()\n            with tf.device(""/cpu:0""):\n                [OPT_A, OPT_C] = self.optimizers_list\n\n                workers = []\n                # Create worker\n                for i in range(N_WORKERS):\n                    i_name = \'Worker_%i\' % i  # worker name\n                    workers.append(\n                        Worker(env[i], self.net_list[i + 1], i_name, train_episodes, max_steps, gamma,\n                               update_itr, self.entropy_beta, render, plot_func))\n\n            # start TF threading\n            worker_threads = []\n            for worker in workers:\n                # t = threading.Thread(target=worker.work)\n                job = lambda: worker.work(self.GLOBAL_AC)\n                t = threading.Thread(target=job)\n                t.start()\n                worker_threads.append(t)\n\n            COORD.join(worker_threads)\n\n            plot_save_log(GLOBAL_RUNNING_R, algorithm_name=self.name, env_name=env[0].spec.id)\n            self.GLOBAL_AC.save_ckpt(env_name=env[0].spec.id)\n\n        elif mode == \'test\':\n            # ============================= EVALUATION =============================\n            env = env[0]  # only need one env for test\n            self.GLOBAL_AC.load_ckpt(env_name=env.spec.id)\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            frame_idx = 0\n            for eps in range(test_episodes):\n                s = env.reset()\n                rall = 0\n                for step in range(max_steps):\n                    env.render()\n                    frame_idx += 1\n                    s = s.astype(\'float32\')  # double to float\n                    a = self.GLOBAL_AC.get_action_greedy(s)\n                    s, r, d, _ = env.step(a)\n                    if render:\n                        env.render()\n                    rall += r\n                    if d:\n                        break\n\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    eps, test_episodes, rall, time.time() - t0))\n\n        elif mode is not \'test\':\n            print(\'unknow mode type\')\n'"
rlzoo/algorithms/a3c/default.py,35,"b'from rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nentropy_beta: factor for entropy boosted exploration\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ntrain_episodes:  total number of episodes for training\ntest_episodes:  total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nn_workers: manually set number of workers\nupdate_itr: update global policy after several episodes\ngamma: reward discount factor\nsave_interval: timesteps for saving the weights and plotting the results\nmode: train or test\n------------------------------------------------\n""""""\n\n\ndef atari(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(\n        entropy_beta=0.005\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 4  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        net_list2 = []  # networks list of networks list, each item for single thread/process\n        for _ in range(num_env + 1):  # additional one for global\n            with tf.name_scope(\'AC\'):\n                with tf.name_scope(\'Critic\'):\n                    critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n                with tf.name_scope(\'Actor\'):\n                    actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                    hidden_dim_list=num_hidden_layer * [hidden_dim])\n            net_list = [actor, critic]\n            net_list2.append(net_list)\n        alg_params[\'net_list\'] = net_list2\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-3, 1e-3  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.RMSprop(a_lr, name=\'RMS_optimizer_actor\')\n        c_optimizer = tf.optimizers.RMSprop(c_lr, name=\'RMS_optimizer_critic\')\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=100,\n        gamma=0.9,\n        train_episodes=1000,\n        test_episodes=10,\n        save_interval=100,\n        update_itr=10,\n        n_workers=num_env\n    )\n\n    return alg_params, learn_params\n\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(\n        entropy_beta=0.005\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 4  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        net_list2 = []  # networks list of networks list, each item for single thread/process\n        for _ in range(num_env + 1):  # additional one for global\n            with tf.name_scope(\'AC\'):\n                with tf.name_scope(\'Critic\'):\n                    critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n                with tf.name_scope(\'Actor\'):\n                    actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                    hidden_dim_list=num_hidden_layer * [hidden_dim])\n            net_list = [actor, critic]\n            net_list2.append(net_list)\n        alg_params[\'net_list\'] = net_list2\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-3, 1e-3  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.RMSprop(a_lr, name=\'RMS_optimizer_actor\')\n        c_optimizer = tf.optimizers.RMSprop(c_lr, name=\'RMS_optimizer_critic\')\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=100,\n        gamma=0.9,\n        train_episodes=1000,\n        test_episodes=10,\n        save_interval=100,\n        update_itr=10,\n        n_workers=num_env\n    )\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(\n        entropy_beta=0.005\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 4  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        net_list2 = []  # networks list of networks list, each item for single thread/process\n        for _ in range(num_env + 1):  # additional one for global\n            with tf.name_scope(\'AC\'):\n                with tf.name_scope(\'Critic\'):\n                    critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n                with tf.name_scope(\'Actor\'):\n                    actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                    hidden_dim_list=num_hidden_layer * [hidden_dim])\n            net_list = [actor, critic]\n            net_list2.append(net_list)\n        alg_params[\'net_list\'] = net_list2\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-3, 1e-3  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.RMSprop(a_lr, name=\'RMS_optimizer_actor\')\n        c_optimizer = tf.optimizers.RMSprop(c_lr, name=\'RMS_optimizer_critic\')\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=20000,\n        gamma=0.9,\n        train_episodes=20000,\n        test_episodes=10,\n        save_interval=500,\n        update_itr=10,\n        n_workers=num_env\n    )\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(\n        entropy_beta=0.005\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 4  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        net_list2 = []  # networks list of networks list, each item for single thread/process\n        for _ in range(num_env + 1):  # additional one for global\n            with tf.name_scope(\'AC\'):\n                with tf.name_scope(\'Critic\'):\n                    critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n                with tf.name_scope(\'Actor\'):\n                    actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                    hidden_dim_list=num_hidden_layer * [hidden_dim])\n            net_list = [actor, critic]\n            net_list2.append(net_list)\n        alg_params[\'net_list\'] = net_list2\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-3, 1e-3  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.RMSprop(a_lr, name=\'RMS_optimizer_actor\')\n        c_optimizer = tf.optimizers.RMSprop(c_lr, name=\'RMS_optimizer_critic\')\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=100,\n        gamma=0.9,\n        train_episodes=1000,\n        test_episodes=10,\n        save_interval=100,\n        update_itr=10,\n        n_workers=num_env\n    )\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(\n        entropy_beta=0.005\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 4  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        net_list2 = []  # networks list of networks list, each item for single thread/process\n        for _ in range(num_env + 1):  # additional one for global\n            with tf.name_scope(\'AC\'):\n                with tf.name_scope(\'Critic\'):\n                    critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n                with tf.name_scope(\'Actor\'):\n                    actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                    hidden_dim_list=num_hidden_layer * [hidden_dim])\n            net_list = [actor, critic]\n            net_list2.append(net_list)\n        alg_params[\'net_list\'] = net_list2\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-3, 1e-3  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.RMSprop(a_lr, name=\'RMS_optimizer_actor\')\n        c_optimizer = tf.optimizers.RMSprop(c_lr, name=\'RMS_optimizer_critic\')\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=100,\n        gamma=0.9,\n        train_episodes=1000,\n        test_episodes=10,\n        save_interval=100,\n        update_itr=10,\n        n_workers=num_env\n\n    )\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(\n        entropy_beta=0.005\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 4  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        net_list2 = []  # networks list of networks list, each item for single thread/process\n        for _ in range(num_env + 1):  # additional one for global\n            with tf.name_scope(\'AC\'):\n                with tf.name_scope(\'Critic\'):\n                    critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n                with tf.name_scope(\'Actor\'):\n                    actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                    hidden_dim_list=num_hidden_layer * [hidden_dim])\n            net_list = [actor, critic]\n            net_list2.append(net_list)\n        alg_params[\'net_list\'] = net_list2\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-3, 1e-3  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.RMSprop(a_lr, name=\'RMS_optimizer_actor\')\n        c_optimizer = tf.optimizers.RMSprop(c_lr, name=\'RMS_optimizer_critic\')\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=100,\n        gamma=0.9,\n        train_episodes=1000,\n        test_episodes=10,\n        save_interval=100,\n        update_itr=10,\n        n_workers=num_env\n\n    )\n\n    return alg_params, learn_params\n\n\ndef rlbench(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(\n        entropy_beta=0.005\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 4  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        net_list2 = []  # networks list of networks list, each item for single thread/process\n        for _ in range(num_env + 1):  # additional one for global\n            with tf.name_scope(\'AC\'):\n                with tf.name_scope(\'Critic\'):\n                    critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n                with tf.name_scope(\'Actor\'):\n                    actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                    hidden_dim_list=num_hidden_layer * [hidden_dim])\n            net_list = [actor, critic]\n            net_list2.append(net_list)\n        alg_params[\'net_list\'] = net_list2\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-3, 1e-3  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.RMSprop(a_lr, name=\'RMS_optimizer_actor\')\n        c_optimizer = tf.optimizers.RMSprop(c_lr, name=\'RMS_optimizer_critic\')\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=100,\n        gamma=0.9,\n        train_episodes=1000,\n        test_episodes=10,\n        save_interval=100,\n        update_itr=10,\n        n_workers=num_env\n\n    )\n\n    return alg_params, learn_params'"
rlzoo/algorithms/a3c/run_a3c.py,6,"b'from rlzoo.algorithms.a3c.a3c import A3C\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nimport gym\n\n"""""" load environment """"""\nenv_id = \'BipedalWalker-v2\'\nenv = gym.make(env_id).unwrapped\n# env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized/wrapped environment to run\naction_shape = env.action_space.shape\nstate_shape = env.observation_space.shape\n# reproducible\nseed = 2\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nenv.seed(seed)\n\n"""""" build networks for the algorithm """"""\nnum_hidden_layer = 4  # number of hidden layers for the networks\nhidden_dim = 64  # dimension of hidden layers for the networks\nnum_workers = 2\nnet_list2 = []\nfor i in range(num_workers + 1):\n    with tf.name_scope(\'A3C\'):\n        with tf.name_scope(\'Actor\'):\n            actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                            hidden_dim_list=num_hidden_layer * [hidden_dim])\n        with tf.name_scope(\'Critic\'):\n            critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n    net_list = [actor, critic]\n    net_list2.append(net_list)\n\n"""""" choose optimizers """"""\nactor_lr, critic_lr = 5e-5, 1e-4  # learning rate\na_optimizer = tf.optimizers.RMSprop(actor_lr)\nc_optimizer = tf.optimizers.RMSprop(critic_lr)\noptimizers_list = [a_optimizer, c_optimizer]\n\nmodel = A3C(net_list2, optimizers_list, entropy_beta=0.005)\n"""""" \nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nentropy_beta: factor for entropy boosted exploration\n""""""\n\nenv_list = []\nfor i in range(num_workers):\n    env_list.append(gym.make(env_id).unwrapped)\nmodel.learn(env_list, train_episodes=20000, test_episodes=100, max_steps=20000, n_workers=num_workers, update_itr=10,\n            gamma=0.99, save_interval=500, mode=\'train\')\n"""""" \nfull list of parameters for training\n---------------------------------------\nenv_list: a list of same learning environments\ntrain_episodes:  total number of episodes for training\ntest_episodes:  total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nn_workers: manually set number of workers\nupdate_itr: update global policy after several episodes\ngamma: reward discount factor\nsave_interval: timesteps for saving the weights and plotting the results\nmode: train or test\n""""""\n# test\nmodel.learn(env_list, test_episodes=100, max_steps=20000, mode=\'test\', render=True)\n'"
rlzoo/algorithms/ac/__init__.py,0,b''
rlzoo/algorithms/ac/ac.py,4,"b'""""""\nActor-Critic \n-------------\nIt uses TD-error as the Advantage.\n\nActor Critic History\n----------------------\nA3C > DDPG > AC\n\nAdvantage\n----------\nAC converge faster than Policy Gradient.\n\nDisadvantage (IMPORTANT)\n------------------------\nThe Policy is oscillated (difficult to converge), DDPG can solve\nthis problem using advantage of DQN.\n\nReference\n----------\npaper: https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf\nView more on MorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials/\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/\n\nEnvironment\n------------\nCartPole-v0: https://gym.openai.com/envs/CartPole-v0\n\nA pole is attached by an un-actuated joint to a cart, which moves along a\nfrictionless track. The system is controlled by applying a force of +1 or -1\nto the cart. The pendulum starts upright, and the goal is to prevent it from\nfalling over.\n\nA reward of +1 is provided for every timestep that the pole remains upright.\nThe episode ends when the pole is more than 15 degrees from vertical, or the\ncart moves more than 2.4 units from the center.\n\n\nPrerequisites\n--------------\ntensorflow >=2.0.0a0\ntensorlayer >=2.0.0\n\n""""""\nimport time\n\nimport tensorlayer as tl\n\nfrom rlzoo.common.utils import *\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.policy_networks import *\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n\n###############################  Actor-Critic  ####################################\nclass AC:\n    def __init__(self, net_list, optimizers_list, gamma=0.9):\n        assert len(net_list) == 2\n        assert len(optimizers_list) == 2\n        self.name = \'AC\'\n        self.actor, self.critic = net_list\n        assert isinstance(self.critic, ValueNetwork)\n        assert isinstance(self.actor, StochasticPolicyNetwork)\n        self.a_optimizer, self.c_optimizer = optimizers_list\n        self.GAMMA = gamma\n\n    def update(self, s, a, r, s_):\n        # critic update\n        v_ = self.critic(np.array([s_]))\n        with tf.GradientTape() as tape:\n            v = self.critic(np.array([s]))\n            td_error = r + self.GAMMA * v_ - v  # TD_error = r + lambd * V(newS) - V(S)\n            loss = tf.square(td_error)\n        grad = tape.gradient(loss, self.critic.trainable_weights)\n        self.c_optimizer.apply_gradients(zip(grad, self.critic.trainable_weights))\n\n        # actor update\n        with tf.GradientTape() as tape:\n            # _logits = self.actor(np.array([s]))\n            ## cross-entropy loss weighted by td-error (advantage),\n            # the cross-entropy mearsures the difference of two probability distributions: the predicted logits and sampled action distribution,\n            # then weighted by the td-error: small difference of real and predict actions for large td-error (advantage); and vice versa.\n\n            _ = self.actor(np.array([s]))\n            neg_log_prob = self.actor.policy_dist.neglogp([a])\n            _exp_v = tf.reduce_mean(neg_log_prob * td_error)\n        grad = tape.gradient(_exp_v, self.actor.trainable_weights)\n        self.a_optimizer.apply_gradients(zip(grad, self.actor.trainable_weights))\n        return _exp_v\n\n    def get_action(self, s):\n        return self.actor(np.array([s]))[0].numpy()\n\n    def get_action_greedy(self, s):\n        return self.actor(np.array([s]), greedy=True)[0].numpy()\n\n    def save_ckpt(self, env_name):  # save trained weights\n        save_model(self.actor, \'model_actor\', self.name, env_name)\n        save_model(self.critic, \'model_critic\', self.name, env_name)\n\n    def load_ckpt(self, env_name):  # load trained weights\n        load_model(self.actor, \'model_actor\', self.name, env_name)\n        load_model(self.critic, \'model_critic\', self.name, env_name)\n\n    def learn(self, env, train_episodes=1000, test_episodes=500, max_steps=200,\n              save_interval=100, mode=\'train\', render=False, plot_func=None):\n        """"""\n        :param env: learning environment\n        :param train_episodes:  total number of episodes for training\n        :param test_episodes:  total number of episodes for testing\n        :param max_steps:  maximum number of steps for one episode\n        :param save_interval: time steps for saving the weights and plotting the results\n        :param mode: \'train\' or \'test\'\n        :param render:  if true, visualize the environment\n        :param plot_func: additional function for interactive module\n        """"""\n\n        t0 = time.time()\n        if mode == \'train\':\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            reward_buffer = []\n            for i_episode in range(train_episodes):\n                s = env.reset()\n                ep_rs_sum = 0  # rewards of all steps\n\n                for step in range(max_steps):\n\n                    if render:\n                        env.render()\n\n                    a = self.get_action(s)\n                    s_new, r, done, info = env.step(a)\n                    ep_rs_sum += r\n\n                    try:\n                        self.update(s, a, r, s_new)  # learn Policy : true_gradient = grad[logPi(s, a) * td_error]\n                    except KeyboardInterrupt:  # if Ctrl+C at running actor.learn(), then save model, or exit if not at actor.learn()\n                        self.save_ckpt(env_name=env.spec.id)\n                        plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\n\n                    s = s_new\n\n                    if done:\n                        break\n\n                reward_buffer.append(ep_rs_sum)\n                if plot_func is not None:\n                    plot_func(reward_buffer)\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\' \\\n                      .format(i_episode, train_episodes, ep_rs_sum, time.time() - t0))\n\n                if i_episode % save_interval == 0:\n                    self.save_ckpt(env_name=env.spec.id)\n                    plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\n\n            self.save_ckpt(env_name=env.spec.id)\n            plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\n\n        elif mode == \'test\':\n            self.load_ckpt(env_name=env.spec.id)\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n\n            reward_buffer = []\n            for i_episode in range(test_episodes):\n                s = env.reset()\n                ep_rs_sum = 0  # rewards of all steps\n                for step in range(max_steps):\n                    if render: env.render()\n                    a = self.get_action_greedy(s)\n                    s_new, r, done, info = env.step(a)\n                    s_new = s_new\n\n                    ep_rs_sum += r\n                    s = s_new\n\n                    if done:\n                        break\n\n                reward_buffer.append(ep_rs_sum)\n                if plot_func:\n                    plot_func(reward_buffer)\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    i_episode, test_episodes, ep_rs_sum, time.time() - t0))\n\n        elif mode is not \'test\':\n            print(\'unknow mode type\')\n'"
rlzoo/algorithms/ac/default.py,42,"b'import tensorflow as tf\nimport tensorlayer as tl\n\nfrom rlzoo.common import math_utils\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.policy_networks import *\nfrom gym import spaces\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\ngamma: discounted factor of reward\naction_range: scale of action values\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\nenv: learning environment\ntrain_episodes:  total number of episodes for training\ntest_episodes:  total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nsave_interval: time steps for saving the weights and plotting the results\nmode: \'train\' or \'test\'\nrender:  if true, visualize the environment\n------------------------------------------------\n""""""\n\n\ndef atari(env, default_seed=True):\n    if default_seed:\n        seed = 1\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        gamma=0.9,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'AC\'):\n            with tf.name_scope(\'Critic\'):\n                critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Actor\'):\n                actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                output_activation=tf.nn.tanh)\n        net_list = [actor, critic]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-4, 2e-4  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.Adam(a_lr)\n        c_optimizer = tf.optimizers.Adam(c_lr)\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=200,\n        train_episodes=500,\n        test_episodes=100,\n        save_interval=50,\n    )\n\n    return alg_params, learn_params\n\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        seed = 1\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        gamma=0.9,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'AC\'):\n            with tf.name_scope(\'Critic\'):\n                critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Actor\'):\n                actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                output_activation=tf.nn.tanh)\n        net_list = [actor, critic]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-4, 2e-4  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.Adam(a_lr)\n        c_optimizer = tf.optimizers.Adam(c_lr)\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=200,\n        train_episodes=500,\n        test_episodes=100,\n        save_interval=50,\n    )\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        seed = 1\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        gamma=0.9,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'AC\'):\n            with tf.name_scope(\'Critic\'):\n                critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Actor\'):\n                actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                output_activation=tf.nn.tanh)\n        net_list = [actor, critic]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-4, 2e-4  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.Adam(a_lr)\n        c_optimizer = tf.optimizers.Adam(c_lr)\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=200,\n        train_episodes=500,\n        test_episodes=100,\n        save_interval=50,\n    )\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        seed = 1\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        gamma=0.9,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'AC\'):\n            with tf.name_scope(\'Critic\'):\n                critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Actor\'):\n                actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                output_activation=tf.nn.tanh)\n        net_list = [actor, critic]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-4, 2e-4  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.Adam(a_lr)\n        c_optimizer = tf.optimizers.Adam(c_lr)\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=200,\n        train_episodes=500,\n        test_episodes=100,\n        save_interval=50,\n    )\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        seed = 1\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        gamma=0.9,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'AC\'):\n            with tf.name_scope(\'Critic\'):\n                critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Actor\'):\n                actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                output_activation=tf.nn.tanh)\n        net_list = [actor, critic]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-4, 2e-4  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.Adam(a_lr)\n        c_optimizer = tf.optimizers.Adam(c_lr)\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=200,\n        train_episodes=500,\n        test_episodes=100,\n        save_interval=50,\n    )\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        seed = 1\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        gamma=0.9,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'AC\'):\n            with tf.name_scope(\'Critic\'):\n                critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Actor\'):\n                actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                output_activation=tf.nn.tanh)\n        net_list = [actor, critic]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-4, 2e-4  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.Adam(a_lr)\n        c_optimizer = tf.optimizers.Adam(c_lr)\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=200,\n        train_episodes=500,\n        test_episodes=100,\n        save_interval=50,\n    )\n\n    return alg_params, learn_params\n\n\ndef rlbench(env, default_seed=True):\n    if default_seed:\n        seed = 1\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        gamma=0.9,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'AC\'):\n            with tf.name_scope(\'Critic\'):\n                critic = ValueNetwork(env.observation_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Actor\'):\n                actor = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                output_activation=tf.nn.tanh)\n        net_list = [actor, critic]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        a_lr, c_lr = 1e-4, 2e-4  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\n        a_optimizer = tf.optimizers.Adam(a_lr)\n        c_optimizer = tf.optimizers.Adam(c_lr)\n        optimizers_list = [a_optimizer, c_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=200,\n        train_episodes=500,\n        test_episodes=100,\n        save_interval=50,\n    )\n\n    return alg_params, learn_params\n'"
rlzoo/algorithms/ac/run_ac.py,6,"b'from rlzoo.common.utils import set_seed\nfrom rlzoo.algorithms.ac.ac import AC\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.policy_networks import *\nimport gym\n\n"""""" load environment """"""\n# env = gym.make(\'CartPole-v0\').unwrapped\nenv = gym.make(\'Pendulum-v0\').unwrapped\nobs_space = env.observation_space\nact_space = env.action_space\n# reproducible\nseed = 1\nset_seed(seed, env)\n\n# env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized/wrapped environment to run\n\n\n"""""" build networks for the algorithm """"""\nnum_hidden_layer = 2  # number of hidden layers for the networks\nhidden_dim = 64  # dimension of hidden layers for the networks\nwith tf.name_scope(\'AC\'):\n    with tf.name_scope(\'Critic\'):\n        critic = ValueNetwork(obs_space, hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Actor\'):\n        actor = StochasticPolicyNetwork(obs_space, act_space, hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                        output_activation=tf.nn.tanh)\nnet_list = [actor, critic]\n\n"""""" choose optimizers """"""\na_lr, c_lr = 1e-4, 2e-4  # a_lr: learning rate of the actor; c_lr: learning rate of the critic\na_optimizer = tf.optimizers.Adam(a_lr)\nc_optimizer = tf.optimizers.Adam(c_lr)\noptimizers_list = [a_optimizer, c_optimizer]\n\nmodel = AC(net_list, optimizers_list)\n"""""" \nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\ngamma: discounted factor of reward\naction_range: scale of action values\n""""""\n\nmodel.learn(env, train_episodes=500,  max_steps=200,\n            save_interval=50, mode=\'train\', render=False)\n"""""" \nfull list of parameters for training\n---------------------------------------\nenv: learning environment\ntrain_episodes:  total number of episodes for training\ntest_episodes:  total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nsave_interval: time steps for saving the weights and plotting the results\nmode: \'train\' or \'test\'\nrender:  if true, visualize the environment\n""""""\nmodel.learn(env, test_episodes=100, max_steps=200,  mode=\'test\', render=True)\n'"
rlzoo/algorithms/ddpg/__init__.py,0,b''
rlzoo/algorithms/ddpg/ddpg.py,6,"b'""""""\nDeep Deterministic Policy Gradient (DDPG)\n-----------------------------------------\nAn algorithm concurrently learns a Q-function and a policy.\nIt uses off-policy data and the Bellman equation to learn the Q-function,\nand uses the Q-function to learn the policy.\nReference\n---------\nDeterministic Policy Gradient Algorithms, Silver et al. 2014\nContinuous Control With Deep Reinforcement Learning, Lillicrap et al. 2016\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials/\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/\n\nPrerequisites\n-------------\ntensorflow >=2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer >=2.0.0\n""""""\n\nimport time\n\nfrom rlzoo.common.utils import *\nfrom rlzoo.common.buffer import *\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\n\n\n###############################  DDPG  ####################################\n\n\nclass DDPG(object):\n    """"""\n    DDPG class\n    """"""\n\n    def __init__(self, net_list, optimizers_list, replay_buffer_size, action_range=1., tau=0.01):\n        """"""\n        :param net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\n        :param optimizers_list: a list of optimizers for all networks and differentiable variables\n        :param replay_buffer_size: the size of buffer for storing explored samples\n        :param tau: soft update factor\n        """"""\n        assert len(net_list) == 4\n        assert len(optimizers_list) == 2\n        self.name = \'DDPG\'\n\n        self.critic, self.critic_target, self.actor, self.actor_target = net_list\n\n        assert isinstance(self.critic, QNetwork)\n        assert isinstance(self.critic_target, QNetwork)\n        assert isinstance(self.actor, DeterministicPolicyNetwork)\n        assert isinstance(self.actor_target, DeterministicPolicyNetwork)\n        assert isinstance(self.actor.action_space, gym.spaces.Box)\n\n        def copy_para(from_model, to_model):\n            for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):\n                j.assign(i)\n\n        copy_para(self.actor, self.actor_target)\n        copy_para(self.critic, self.critic_target)\n\n        self.replay_buffer_size = replay_buffer_size\n        self.buffer = ReplayBuffer(replay_buffer_size)\n\n        self.ema = tf.train.ExponentialMovingAverage(decay=1 - tau)  # soft replacement\n        self.action_range = action_range\n\n        self.critic_opt, self.actor_opt = optimizers_list\n\n    def ema_update(self):\n        """"""\n        Soft updating by exponential smoothing\n        \n        :return: None\n        """"""\n        paras = self.actor.trainable_weights + self.critic.trainable_weights\n        self.ema.apply(paras)\n        for i, j in zip(self.actor_target.trainable_weights + self.critic_target.trainable_weights, paras):\n            i.assign(self.ema.average(j))\n\n    def sample_action(self):\n        """""" generate random actions for exploration """"""\n        a = tf.random.uniform(self.actor.action_space.shape, self.actor.action_space.low, self.actor.action_space.high)\n        return a\n\n    def get_action(self, s, noise_scale):\n        """"""\n        Choose action with exploration\n\n        :param s: state\n\n        :return: action\n        """"""\n        a = self.actor([s])[0].numpy()*self.action_range\n\n        # add randomness to action selection for exploration\n        noise = np.random.normal(0, 1, a.shape) * noise_scale\n        a += noise\n        a = np.clip(a, self.actor.action_space.low, self.actor.action_space.high)\n\n        return a\n\n    def get_action_greedy(self, s):\n        """"""\n        Choose action\n\n        :param s: state\n\n        :return: action\n        """"""\n        return self.actor([s])[0].numpy()*self.action_range\n\n    def update(self, batch_size, gamma):\n        """"""\n        Update parameters\n\n        :param batch_size: update batch size\n        :param gamma: reward decay factor\n\n        :return:\n        """"""\n        bs, ba, br, bs_, bd = self.buffer.sample(batch_size)\n\n        ba_ = self.actor_target(bs_)*self.action_range\n\n        q_ = self.critic_target([bs_, ba_])\n        y = br + (1 - bd) * gamma * q_\n        with tf.GradientTape() as tape:\n            q = self.critic([bs, ba])\n            td_error = tf.losses.mean_squared_error(y, q)\n        c_grads = tape.gradient(td_error, self.critic.trainable_weights)\n        self.critic_opt.apply_gradients(zip(c_grads, self.critic.trainable_weights))\n\n        with tf.GradientTape() as tape:\n            a = self.actor(bs)*self.action_range\n            q = self.critic([bs, a])\n            a_loss = - tf.reduce_mean(q)  # maximize the q\n        a_grads = tape.gradient(a_loss, self.actor.trainable_weights)\n        self.actor_opt.apply_gradients(zip(a_grads, self.actor.trainable_weights))\n        self.ema_update()\n\n    def store_transition(self, s, a, r, s_, d):\n        """"""\n        Store data in data buffer\n\n        :param s: state\n        :param a: act\n        :param r: reward\n        :param s_: next state\n\n        :return: None\n        """"""\n        d = 1 if d else 0\n\n        self.buffer.push(s, a, [r], s_, d)\n\n    def save_ckpt(self, env_name):\n        """"""\n        save trained weights\n\n        :return: None\n        """"""\n        save_model(self.actor, \'model_policy_net\', self.name, env_name)\n        save_model(self.actor_target, \'model_target_policy_net\', self.name, env_name)\n        save_model(self.critic, \'model_q_net\', self.name, env_name)\n        save_model(self.critic_target, \'model_target_q_net\', self.name, env_name)\n\n    def load_ckpt(self, env_name):\n        """"""\n        load trained weights\n\n        :return: None\n        """"""\n        load_model(self.actor, \'model_policy_net\', self.name, env_name)\n        load_model(self.actor_target, \'model_target_policy_net\', self.name, env_name)\n        load_model(self.critic, \'model_q_net\', self.name, env_name)\n        load_model(self.critic_target, \'model_target_q_net\', self.name, env_name)\n\n    def learn(self, env, train_episodes=200, test_episodes=100, max_steps=200, save_interval=10, explore_steps=500,\n              mode=\'train\', render=False, batch_size=32, gamma=0.9, noise_scale=1., noise_scale_decay=0.995,\n              plot_func=None):\n        """"""\n        learn function\n\n        :param env: learning environment\n        :param train_episodes: total number of episodes for training\n        :param test_episodes: total number of episodes for testing\n        :param max_steps: maximum number of steps for one episode\n        :param save_interval: time steps for saving\n        :param explore_steps: for random action sampling in the beginning of training\n        :param mode: train or test mode\n        :param render: render each step\n        :param batch_size: update batch size\n        :param gamma: reward decay factor\n        :param noise_scale: range of action noise for exploration\n        :param noise_scale_decay: noise scale decay factor\n        :param plot_func: additional function for interactive module\n        :return: None\n        """"""\n\n        t0 = time.time()\n\n        if mode == \'train\':  # train\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            reward_buffer = []\n            frame_idx = 0\n            for i in range(1, train_episodes + 1):\n                s = env.reset()\n                ep_reward = 0\n\n                for j in range(max_steps):\n                    if render:\n                        env.render()\n                    # Add exploration noise\n                    if frame_idx > explore_steps:\n                        a = self.get_action(s, noise_scale)\n                    else:\n                        a = self.sample_action()\n                        frame_idx += 1\n\n                    s_, r, done, info = env.step(a)\n\n                    self.store_transition(s, a, r, s_, done)\n                    if len(self.buffer) >= self.replay_buffer_size:\n                        self.update(batch_size, gamma)\n                        noise_scale *= noise_scale_decay\n                    s = s_\n                    ep_reward += r\n\n                    if done:\n                        break\n\n                print(\n                    \'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                        i, train_episodes, ep_reward,\n                        time.time() - t0\n                    )\n                )\n\n                reward_buffer.append(ep_reward)\n                if plot_func is not None:\n                    plot_func(reward_buffer)\n                if i and not i % save_interval:\n                    self.save_ckpt(env_name=env.spec.id)\n                    plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\n\n            self.save_ckpt(env_name=env.spec.id)\n            plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\n\n        # test\n        elif mode == \'test\':\n            self.load_ckpt(env_name=env.spec.id)\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            reward_buffer = []\n            for eps in range(1, test_episodes+1):\n                ep_rs_sum = 0\n                s = env.reset()\n                for step in range(max_steps):\n                    if render:\n                        env.render()\n                    action = self.get_action_greedy(s)\n                    s, reward, done, info = env.step(action)\n                    ep_rs_sum += reward\n                    if done:\n                        break\n\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    eps, test_episodes, ep_rs_sum, time.time() - t0)\n                )\n            reward_buffer.append(ep_rs_sum)\n            if plot_func:\n                plot_func(reward_buffer)\n        else:\n            print(\'unknown mode type\')'"
rlzoo/algorithms/ddpg/default.py,36,"b'from rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nreplay_buffer_size: the size of buffer for storing explored samples\ntau: soft update factor\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: time steps for saving\nexplore_steps: for random action sampling in the beginning of training\nmode: train or test mode\nrender: render each step\nbatch_size: update batch size\ngamma: reward decay factor\nnoise_scale: range of action noise for exploration\nnoise_scale_decay: noise scale decay factor\n-----------------------------------------------\n""""""\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        replay_buffer_size=10000,\n        tau=0.01,\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DDPG\'):\n            with tf.name_scope(\'Q_Net\'):\n                q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net\'):\n                target_q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               num_hidden_layer * [hidden_dim])\n\n        net_list = [q_net, target_q_net, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-3\n        critic_lr = 2e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=100,\n        test_episodes=10,\n        max_steps=200,\n        save_interval=10,\n        explore_steps=500,\n        batch_size=32,\n        gamma=0.9,\n        noise_scale=1.,\n        noise_scale_decay=0.995\n    )\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        replay_buffer_size=10000,\n        tau=0.01,\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DDPG\'):\n            with tf.name_scope(\'Q_Net\'):\n                q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net\'):\n                target_q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               num_hidden_layer * [hidden_dim])\n\n        net_list = [q_net, target_q_net, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-3\n        critic_lr = 2e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=100,\n        test_episodes=10,\n        max_steps=200,\n        save_interval=10,\n        explore_steps=500,\n        batch_size=32,\n        gamma=0.9,\n        noise_scale=1.,\n        noise_scale_decay=0.995\n    )\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        replay_buffer_size=10000,\n        tau=0.01,\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DDPG\'):\n            with tf.name_scope(\'Q_Net\'):\n                q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net\'):\n                target_q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               num_hidden_layer * [hidden_dim])\n\n        net_list = [q_net, target_q_net, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-3\n        critic_lr = 2e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=100,\n        test_episodes=10,\n        max_steps=200,\n        save_interval=10,\n        explore_steps=500,\n        batch_size=32,\n        gamma=0.9,\n        noise_scale=1.,\n        noise_scale_decay=0.995\n    )\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        replay_buffer_size=10000,\n        tau=0.01,\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DDPG\'):\n            with tf.name_scope(\'Q_Net\'):\n                q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net\'):\n                target_q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               num_hidden_layer * [hidden_dim])\n\n        net_list = [q_net, target_q_net, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-3\n        critic_lr = 2e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=100,\n        test_episodes=10,\n        max_steps=200,\n        save_interval=10,\n        explore_steps=500,\n        batch_size=32,\n        gamma=0.9,\n        noise_scale=1.,\n        noise_scale_decay=0.995\n    )\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        replay_buffer_size=10000,\n        tau=0.01,\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DDPG\'):\n            with tf.name_scope(\'Q_Net\'):\n                q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net\'):\n                target_q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               num_hidden_layer * [hidden_dim])\n\n        net_list = [q_net, target_q_net, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-3\n        critic_lr = 2e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=100,\n        test_episodes=10,\n        max_steps=200,\n        save_interval=10,\n        explore_steps=500,\n        batch_size=32,\n        gamma=0.9,\n        noise_scale=1.,\n        noise_scale_decay=0.995\n    )\n\n    return alg_params, learn_params\n\n\ndef rlbench(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        replay_buffer_size=1000,\n        tau=0.01,\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DDPG\'):\n            with tf.name_scope(\'Q_Net\'):\n                q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net\'):\n                target_q_net = QNetwork(env.observation_space, env.action_space, num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               num_hidden_layer * [hidden_dim])\n\n        net_list = [q_net, target_q_net, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-3\n        critic_lr = 2e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=100,\n        test_episodes=10,\n        max_steps=200,\n        save_interval=10,\n        explore_steps=500,\n        batch_size=32,\n        gamma=0.9,\n        noise_scale=1.,\n        noise_scale_decay=0.995\n    )\n\n    return alg_params, learn_params'"
rlzoo/algorithms/ddpg/run_ddpg.py,1,"b'from rlzoo.common.utils import make_env, set_seed\nfrom rlzoo.algorithms.ddpg.ddpg import DDPG\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nimport gym\n\n"""""" load environment """"""\nenv = gym.make(\'Pendulum-v0\').unwrapped\n\nobs_space = env.observation_space\nact_space = env.action_space\n\n# reproducible\nseed = 2\nset_seed(seed, env)\n\n"""""" build networks for the algorithm """"""\nname = \'DDPG\'\nnum_hidden_layer = 2  # number of hidden layers for the networks\nhidden_dim = 64  # dimension of hidden layers for the networks\n\nactor = DeterministicPolicyNetwork(obs_space, act_space, [hidden_dim] * num_hidden_layer)\ncritic = QNetwork(obs_space, act_space, [hidden_dim] * num_hidden_layer)\n\nactor_target = DeterministicPolicyNetwork(obs_space, act_space, [hidden_dim] * num_hidden_layer, trainable=False)\n\ncritic_target = QNetwork(obs_space, act_space, [hidden_dim] * num_hidden_layer, trainable=False)\n\nnet_list = [critic, critic_target, actor, actor_target]\n\n"""""" create model """"""\nactor_lr = 1e-3\ncritic_lr = 2e-3\noptimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\nreplay_buffer_size = 10000\nmodel = DDPG(net_list, optimizers_list, replay_buffer_size)\n"""""" \nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nreplay_buffer_size: the size of buffer for storing explored samples\ntau: soft update factor\n""""""\n\nmodel.learn(env, train_episodes=100, max_steps=200, save_interval=10,\n            mode=\'train\', render=False, batch_size=32, gamma=0.9, noise_scale=1., noise_scale_decay=0.995)\n""""""\nfull list of parameters for training\n---------------------------------------\nenv: learning environment\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: time steps for saving\nexplore_steps: for random action sampling in the beginning of training\nmode: train or test mode\nrender: render each step\nbatch_size: update batch size\ngamma: reward decay factor\nnoise_scale: range of action noise for exploration\nnoise_scale_decay: noise scale decay factor\n""""""\n\nmodel.learn(env, test_episodes=10, max_steps=200, mode=\'test\', render=True)\n\n'"
rlzoo/algorithms/dppo/__init__.py,0,b''
rlzoo/algorithms/dppo/default.py,24,"b'from rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nepsilon: clip parameter  (for method \'clip\')\nkl_target: controls bounds of policy update and adaptive lambda  (for method \'penalty\')\nlam:  KL-regularization coefficient   (for method \'penalty\')\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nsave_interval: time steps for saving\ngamma: reward discount factor\nmode: train or test\nbatch_size: update batch size\na_update_steps: actor update iteration steps\nc_update_steps: critic update iteration steps\nn_worker: number of workers\n-----------------------------------------------\n""""""\n\n\ndef atari(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5  # for method \'penalty\'\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DPPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer)\n\n        net_list = v_net, policy_net\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        a_update_steps=10,\n                        c_update_steps=10,\n                        n_workers=num_env,\n                        batch_size=32)\n\n    return alg_params, learn_params\n\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5  # for method \'penalty\'\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DPPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer)\n\n        net_list = v_net, policy_net\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        a_update_steps=10,\n                        c_update_steps=10,\n                        n_workers=num_env,\n                        batch_size=32)\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5  # for method \'penalty\'\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DPPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer)\n\n        net_list = v_net, policy_net\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        a_update_steps=10,\n                        c_update_steps=10,\n                        n_workers=num_env,\n                        batch_size=32)\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5  # for method \'penalty\'\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DPPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer)\n\n        net_list = v_net, policy_net\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        a_update_steps=10,\n                        c_update_steps=10,\n                        n_workers=num_env,\n                        batch_size=32)\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5  # for method \'penalty\'\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DPPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer)\n\n        net_list = v_net, policy_net\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        a_update_steps=10,\n                        c_update_steps=10,\n                        n_workers=num_env,\n                        batch_size=32)\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        assert isinstance(env, list)\n        seed = np.arange(len(env)).tolist()  # a list of seeds for each env\n        set_seed(seed, env)  # reproducible\n\n    # for multi-threading\n    if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\n        num_env = len(env)  # number of envs passed in\n        env = env[0]  # take one of the env as they are all the same\n    else:\n        num_env = 1\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5  # for method \'penalty\'\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'DPPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer)\n\n        net_list = v_net, policy_net\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        a_update_steps=10,\n                        c_update_steps=10,\n                        n_workers=num_env,\n                        batch_size=32)\n\n    return alg_params, learn_params\n'"
rlzoo/algorithms/dppo/dppo.py,0,"b""from rlzoo.algorithms.dppo_penalty.dppo_penalty import DPPO_PENALTY\r\nfrom rlzoo.algorithms.dppo_clip.dppo_clip import DPPO_CLIP\r\n\r\n\r\ndef DPPO(**alg_params):\r\n    method = alg_params['method']\r\n    if method == 'penalty':\r\n        del alg_params['epsilon']\r\n        algo = DPPO_PENALTY\r\n    elif method == 'clip':\r\n        del alg_params['kl_target']\r\n        del alg_params['lam']\r\n        algo = DPPO_CLIP\r\n    else:\r\n        raise ValueError('Method input error. Method can only be penalty or clip')\r\n    del alg_params['method']\r\n    return algo(**alg_params)\r\n"""
rlzoo/algorithms/dppo_clip/__init__.py,0,b''
rlzoo/algorithms/dppo_clip/dppo_clip.py,9,"b'""""""\r\nDistributed Proximal Policy Optimization (DPPO)\r\n----------------------------\r\nA distributed version of OpenAI\'s Proximal Policy Optimization (PPO).\r\nWorkers in parallel to collect data, then stop worker\'s roll-out and train PPO on collected data.\r\nRestart workers once PPO is updated.\r\n\r\nReference\r\n---------\r\nEmergence of Locomotion Behaviours in Rich Environments, Heess et al. 2017\r\nProximal Policy Optimization Algorithms, Schulman et al. 2017\r\nHigh Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016\r\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials\r\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/\r\n\r\nPrerequisites\r\n--------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-probability 0.6.0\r\ntensorlayer >=2.0.0\r\n\r\n""""""\r\n\r\nimport queue\r\nimport threading\r\nimport time\r\n\r\nfrom rlzoo.common.utils import *\r\nfrom rlzoo.common.policy_networks import *\r\nfrom rlzoo.common.value_networks import *\r\n\r\nEPS = 1e-8  # epsilon\r\n\r\n\r\nclass DPPO_CLIP(object):\r\n    """"""\r\n    PPO class\r\n    """"""\r\n\r\n    def __init__(self, net_list, optimizers_list, epsilon=0.2):\r\n        """"""\r\n        :param net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\r\n        :param optimizers_list: a list of optimizers for all networks and differentiable variables\r\n        :param state_dim: dimension of action for the environment\r\n        :param action_dim: dimension of state for the environment\r\n        :param a_bounds: a list of [min_action, max_action] action bounds for the environment\r\n        :param epsilon: clip parameter\r\n        """"""\r\n        assert len(net_list) == 2\r\n        assert len(optimizers_list) == 2\r\n        self.name = \'DPPO_CLIP\'\r\n\r\n        self.epsilon = epsilon\r\n\r\n        self.critic, self.actor = net_list\r\n\r\n        assert isinstance(self.critic, ValueNetwork)\r\n        assert isinstance(self.actor, StochasticPolicyNetwork)\r\n\r\n        self.critic_opt, self.actor_opt = optimizers_list\r\n        self.last_update_epoch = 0\r\n\r\n    def a_train(self, tfs, tfa, tfadv, oldpi_prob):\r\n        """"""\r\n        Update policy network\r\n\r\n        :param tfs: state\r\n        :param tfa: act\r\n        :param tfadv: advantage\r\n        :param oldpi_prob: old pi probability of a in s\r\n\r\n        :return:\r\n        """"""\r\n        with tf.GradientTape() as tape:\r\n            _ = self.actor(tfs)\r\n            pi_prob = tf.exp(self.actor.policy_dist.logp(tfa))\r\n            ratio = pi_prob / (oldpi_prob + EPS)\r\n\r\n            surr = ratio * tfadv\r\n            aloss = -tf.reduce_mean(\r\n                tf.minimum(surr, tf.clip_by_value(ratio, 1. - self.epsilon, 1. + self.epsilon) * tfadv))\r\n        a_gard = tape.gradient(aloss, self.actor.trainable_weights)\r\n        self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\r\n\r\n    def c_train(self, tfdc_r, s):\r\n        """"""\r\n        Update actor network\r\n\r\n        :param tfdc_r: cumulative reward\r\n        :param s: state\r\n\r\n        :return: None\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        with tf.GradientTape() as tape:\r\n            v = self.critic(s)\r\n            advantage = tfdc_r - v\r\n            closs = tf.reduce_mean(tf.square(advantage))\r\n        grad = tape.gradient(closs, self.critic.trainable_weights)\r\n        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))\r\n\r\n    def cal_adv(self, tfs, tfdc_r):\r\n        """"""\r\n        Calculate advantage\r\n\r\n        :param tfs: state\r\n        :param tfdc_r: cumulative reward\r\n\r\n        :return: advantage\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        advantage = tfdc_r - self.critic(tfs)\r\n        return advantage.numpy()\r\n\r\n    def update(self, a_update_steps, c_update_steps, save_interval, env):\r\n        """"""\r\n        Update\r\n\r\n        :param a_update_steps: actor update steps\r\n        :param c_update_steps: critic update steps\r\n        :param save_interval: save interval\r\n        :param env: environment\r\n\r\n        :return: None\r\n        """"""\r\n        global GLOBAL_UPDATE_COUNTER\r\n        while not COORD.should_stop():\r\n            if GLOBAL_EP < EP_MAX:\r\n                UPDATE_EVENT.wait()  # wait until get batch of data\r\n                data = [QUEUE.get() for _ in range(QUEUE.qsize())]  # collect data from all workers\r\n\r\n                s, a, r = zip(*data)\r\n                s, a, r = np.vstack(s), np.vstack(a), np.vstack(r)\r\n                s, a, r = np.array(s), np.array(a, np.float32), np.array(r, np.float32)\r\n\r\n                adv = self.cal_adv(s, r)\r\n                # adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful\r\n\r\n                _ = self.actor(s)\r\n                oldpi_prob = tf.exp(self.actor.policy_dist.logp(a))\r\n                oldpi_prob = tf.stop_gradient(oldpi_prob)\r\n\r\n                # update actor\r\n                for _ in range(a_update_steps):\r\n                    self.a_train(s, a, adv, oldpi_prob)\r\n\r\n                # update critic\r\n                for _ in range(c_update_steps):\r\n                    self.c_train(r, s)\r\n\r\n                UPDATE_EVENT.clear()  # updating finished\r\n                GLOBAL_UPDATE_COUNTER = 0  # reset counter\r\n                ROLLING_EVENT.set()  # set roll-out available\r\n\r\n                if (not GLOBAL_EP % save_interval) and GLOBAL_EP != self.last_update_epoch:\r\n                    self.save_ckpt(env_name=env.spec.id)\r\n                    self.last_update_epoch = GLOBAL_EP\r\n                    plot_save_log(GLOBAL_RUNNING_R, algorithm_name=self.name, env_name=env.spec.id)\r\n\r\n    def get_action(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s])[0].numpy()\r\n\r\n    def get_action_greedy(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s], greedy=True)[0].numpy()\r\n\r\n    def get_v(self, s):\r\n        """"""\r\n        Compute value\r\n\r\n        :param s: state\r\n\r\n        :return: value\r\n        """"""\r\n        s = s.astype(np.float32)\r\n        if s.ndim < 2: s = s[np.newaxis, :]\r\n        return self.critic(s)[0, 0]\r\n\r\n    def save_ckpt(self, env_name):\r\n        """"""\r\n        save trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        save_model(self.actor, \'actor\', self.name, env_name)\r\n        save_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def load_ckpt(self, env_name):\r\n        """"""\r\n        load trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        load_model(self.actor, \'actor\', self.name, env_name)\r\n        load_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def learn(self, env, train_episodes=200, test_episodes=100, max_steps=200, save_interval=10, gamma=0.9,\r\n              mode=\'train\', render=False, batch_size=32, a_update_steps=10, c_update_steps=10, n_workers=4,\r\n              plot_func=None):\r\n        """"""\r\n        learn function\r\n\r\n        :param env: learning environment\r\n        :param train_episodes: total number of episodes for training\r\n        :param test_episodes: total number of episodes for testing\r\n        :param max_steps:  maximum number of steps for one episode\r\n        :param save_interval: time steps for saving\r\n        :param gamma: reward discount factor\r\n        :param mode: train or test\r\n        :param render: render each step\r\n        :param batch_size: update batch size\r\n        :param a_update_steps: actor update iteration steps\r\n        :param c_update_steps: critic update iteration steps\r\n        :param n_workers: number of workers\r\n        :param plot_func: additional function for interactive module\r\n        :return: None\r\n        """"""\r\n        t0 = time.time()\r\n        global GLOBAL_PPO, UPDATE_EVENT, ROLLING_EVENT, GLOBAL_UPDATE_COUNTER, GLOBAL_EP, GLOBAL_RUNNING_R, COORD, QUEUE\r\n        global EP_LEN, MIN_BATCH_SIZE, GAMMA, EP_MAX, RENDER\r\n        EP_LEN, MIN_BATCH_SIZE, GAMMA, EP_MAX, RENDER = max_steps, batch_size, gamma, train_episodes, render\r\n        GLOBAL_PPO = self\r\n        if mode == \'train\':  # train\r\n            if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\r\n                assert len(env) == n_workers\r\n            else:\r\n                assert n_workers == 1\r\n                env = env,\r\n\r\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env[0].spec.id))\r\n            UPDATE_EVENT, ROLLING_EVENT = threading.Event(), threading.Event()\r\n            UPDATE_EVENT.clear()  # not update now\r\n            ROLLING_EVENT.set()  # start to roll out\r\n            workers = [Worker(wid=i, env=env[i], plot_func=plot_func) for i in range(n_workers)]\r\n\r\n            GLOBAL_UPDATE_COUNTER, GLOBAL_EP = 0, 0\r\n            GLOBAL_RUNNING_R = []\r\n            COORD = tf.train.Coordinator()\r\n            QUEUE = queue.Queue()  # workers putting data in this queue\r\n            threads = []\r\n            for worker in workers:  # worker threads\r\n                t = threading.Thread(target=worker.work, args=())\r\n                t.start()  # training\r\n                threads.append(t)\r\n            # add a PPO updating thread\r\n            threads.append(threading.Thread(target=self.update, args=(a_update_steps, c_update_steps,\r\n                                                                      save_interval, env[0])))\r\n            threads[-1].start()\r\n            COORD.join(threads)\r\n\r\n            plot_save_log(GLOBAL_RUNNING_R, algorithm_name=self.name, env_name=env[0].spec.id)\r\n            self.save_ckpt(env_name=env[0].spec.id)\r\n\r\n        # test\r\n        elif mode == \'test\':\r\n            if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\r\n                env = env[0]\r\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\r\n            self.load_ckpt(env_name=env.spec.id)\r\n            reward_buffer = []\r\n            for eps in range(test_episodes):\r\n                ep_rs_sum = 0\r\n                s = env.reset()\r\n                for step in range(max_steps):\r\n                    if RENDER:\r\n                        env.render()\r\n                    action = self.get_action_greedy(s)\r\n                    s, reward, done, info = env.step(action)\r\n                    ep_rs_sum += reward\r\n                    if done:\r\n                        break\r\n\r\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    eps, test_episodes, ep_rs_sum, time.time() - t0)\r\n                )\r\n\r\n                reward_buffer.append(ep_rs_sum)\r\n                if plot_func is not None:\r\n                    plot_func(reward_buffer)\r\n        else:\r\n            print(\'unknown mode type\')\r\n\r\n\r\nclass Worker(object):\r\n    """"""\r\n    Worker class for distributional running\r\n    """"""\r\n\r\n    def __init__(self, wid, env, plot_func):\r\n        self.wid = wid\r\n        self.env = env\r\n        # self.env.seed(wid * 100 + RANDOMSEED)\r\n        global GLOBAL_PPO\r\n        self.ppo = GLOBAL_PPO\r\n        self.plot_func = plot_func\r\n\r\n    def work(self):\r\n        """"""\r\n        Define a worker\r\n        \r\n        :return: None\r\n        """"""\r\n        t0 = time.time()\r\n        global GLOBAL_EP, GLOBAL_RUNNING_R, GLOBAL_UPDATE_COUNTER, REWARD_SHAPING, RENDER\r\n        while not COORD.should_stop():\r\n            s = self.env.reset()\r\n            ep_r = 0\r\n            buffer_s, buffer_a, buffer_r = [], [], []\r\n            for t in range(1, EP_LEN + 1):\r\n                if not ROLLING_EVENT.is_set():  # while global PPO is updating\r\n                    ROLLING_EVENT.wait()  # wait until PPO is updated\r\n                    buffer_s, buffer_a, buffer_r = [], [], []  # clear history buffer, use new policy to collect data\r\n                a = self.ppo.get_action(s)\r\n                for step in range(EP_LEN):\r\n                    if RENDER:\r\n                        self.env.render()\r\n                s_, r, done, _ = self.env.step(a)\r\n                buffer_s.append(s)\r\n                buffer_a.append(a)\r\n                buffer_r.append(r)\r\n                s = s_\r\n                ep_r += r\r\n\r\n                GLOBAL_UPDATE_COUNTER += 1  # count to minimum batch size, no need to wait other workers\r\n                if t == EP_LEN - 1 or GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE or done:\r\n                    if done:\r\n                        v_s_ = 0\r\n                    else:\r\n                        try:\r\n                            v_s_ = self.ppo.get_v(s_)\r\n                        except:\r\n                            v_s_ = self.ppo.get_v(s_[np.newaxis, :])  # for raw-pixel input\r\n                    discounted_r = []  # compute discounted reward\r\n                    for r in buffer_r[::-1]:\r\n                        v_s_ = r + GAMMA * v_s_\r\n                        discounted_r.append(v_s_)\r\n                    discounted_r.reverse()\r\n                    bs = buffer_s\r\n                    ba, br = buffer_a, np.array(discounted_r)[:, np.newaxis]\r\n                    buffer_s, buffer_a, buffer_r = [], [], []\r\n                    QUEUE.put((bs, ba, br))  # put data in the queue\r\n                    if GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE:\r\n                        ROLLING_EVENT.clear()  # stop collecting data\r\n                        UPDATE_EVENT.set()  # globalPPO update\r\n\r\n                    if GLOBAL_EP >= EP_MAX:  # stop training\r\n                        COORD.request_stop()\r\n                        break\r\n                if done:\r\n                    break\r\n\r\n            GLOBAL_RUNNING_R.append(ep_r)\r\n            GLOBAL_EP += 1\r\n\r\n            print(\r\n                \'Episode: {}/{}  | Worker: {} | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    GLOBAL_EP, EP_MAX, self.wid, ep_r,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n            if self.wid == 0 and self.plot_func is not None:\r\n                self.plot_func(GLOBAL_RUNNING_R)\r\n'"
rlzoo/algorithms/dppo_clip/run_dppo_clip.py,1,"b'from rlzoo.common.utils import set_seed\r\nfrom rlzoo.algorithms.dppo_clip.dppo_clip import DPPO_CLIP\r\nfrom rlzoo.common.policy_networks import *\r\nfrom rlzoo.common.value_networks import *\r\nimport gym\r\n\r\nn_workers = 4\r\n"""""" load environment """"""\r\nenv = [gym.make(\'Pendulum-v0\').unwrapped for i in range(n_workers)]\r\n\r\n# reproducible\r\nseed = 2\r\nset_seed(seed)\r\n\r\n"""""" build networks for the algorithm """"""\r\nname = \'DPPO_CLIP\'\r\nhidden_dim = 64\r\nnum_hidden_layer = 2\r\ncritic = ValueNetwork(env[0].observation_space, [hidden_dim] * num_hidden_layer, name=name + \'_value\')\r\n\r\nactor = StochasticPolicyNetwork(env[0].observation_space, env[0].action_space,\r\n                                [hidden_dim] * num_hidden_layer,\r\n                                trainable=True,\r\n                                name=name + \'_policy\')\r\nnet_list = critic, actor\r\n\r\n"""""" create model """"""\r\nactor_lr = 1e-4\r\ncritic_lr = 2e-4\r\noptimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\r\nmodel = DPPO_CLIP(net_list, optimizers_list)\r\n""""""\r\nfull list of arguments for the algorithm\r\n----------------------------------------\r\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\r\noptimizers_list: a list of optimizers for all networks and differentiable variables\r\nepsilon: clip parameter\r\n""""""\r\n\r\nmodel.learn(env, train_episodes=1000, max_steps=200, save_interval=50, gamma=0.9,\r\n            mode=\'train\', render=False, batch_size=32, a_update_steps=10, c_update_steps=10, n_workers=n_workers)\r\n\r\n""""""\r\nfull list of parameters for training\r\n---------------------------------------\r\nenv: learning environment\r\ntrain_episodes: total number of episodes for training\r\ntest_episodes: total number of episodes for testing\r\nmax_steps:  maximum number of steps for one episode\r\nsave_interval: time steps for saving\r\ngamma: reward discount factor\r\nmode: train or test\r\nbatch_size: update batch size\r\na_update_steps: actor update iteration steps\r\nc_update_steps: critic update iteration steps\r\nn_workers: number of workers\r\n:return: None\r\n""""""\r\nmodel.learn(env, test_episodes=100, max_steps=200, mode=\'test\', render=True)\r\n'"
rlzoo/algorithms/dppo_penalty/__init__.py,0,b''
rlzoo/algorithms/dppo_penalty/dppo_penalty.py,9,"b'""""""\r\nDistributed Proximal Policy Optimization (DPPO)\r\n----------------------------\r\nA distributed version of OpenAI\'s Proximal Policy Optimization (PPO).\r\nWorkers in parallel to collect data, then stop worker\'s roll-out and train PPO on collected data.\r\nRestart workers once PPO is updated.\r\n\r\nReference\r\n---------\r\nEmergence of Locomotion Behaviours in Rich Environments, Heess et al. 2017\r\nProximal Policy Optimization Algorithms, Schulman et al. 2017\r\nHigh Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016\r\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials\r\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/\r\n\r\nPrerequisites\r\n--------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-probability 0.6.0\r\ntensorlayer >=2.0.0\r\n\r\n""""""\r\n\r\nimport queue\r\nimport threading\r\nimport time\r\n\r\nfrom rlzoo.common.utils import *\r\nfrom rlzoo.common.policy_networks import *\r\nfrom rlzoo.common.value_networks import *\r\n\r\nEPS = 1e-8  # epsilon\r\n\r\n\r\nclass DPPO_PENALTY(object):\r\n    """"""\r\n    PPO class\r\n    """"""\r\n\r\n    def __init__(self, net_list, optimizers_list, kl_target=0.01, lam=0.5):\r\n        """"""\r\n        :param net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\r\n        :param optimizers_list: a list of optimizers for all networks and differentiable variables\r\n        :param kl_target: controls bounds of policy update and adaptive lambda\r\n        :param lam:  KL-regularization coefficient\r\n        """"""\r\n        assert len(net_list) == 2\r\n        assert len(optimizers_list) == 2\r\n        self.name = \'DPPO_PENALTY\'\r\n        self.kl_target = kl_target\r\n        self.lam = lam\r\n\r\n        self.critic, self.actor = net_list\r\n\r\n        assert isinstance(self.critic, ValueNetwork)\r\n        assert isinstance(self.actor, StochasticPolicyNetwork)\r\n\r\n        self.critic_opt, self.actor_opt = optimizers_list\r\n        self.old_dist = make_dist(self.actor.action_space)\r\n        self.last_update_epoch = 0\r\n\r\n    def a_train(self, tfs, tfa, tfadv, oldpi_prob):\r\n        """"""\r\n        Update policy network\r\n        \r\n        :param tfs: state\r\n        :param tfa: act\r\n        :param tfadv: advantage\r\n\r\n        :return:\r\n        """"""\r\n        tfs = np.array(tfs)\r\n        tfa = np.array(tfa, np.float32)\r\n        tfadv = np.array(tfadv, np.float32)\r\n        with tf.GradientTape() as tape:\r\n            _ = self.actor(tfs)\r\n            pi_prob = tf.exp(self.actor.policy_dist.logp(tfa))\r\n            ratio = pi_prob / (oldpi_prob + EPS)\r\n\r\n            surr = ratio * tfadv\r\n            kl = self.old_dist.kl(self.actor.policy_dist.get_param())\r\n            # kl = tfp.distributions.kl_divergence(oldpi, pi)\r\n            kl_mean = tf.reduce_mean(kl)\r\n            aloss = -(tf.reduce_mean(surr - self.lam * kl))\r\n        a_gard = tape.gradient(aloss, self.actor.trainable_weights)\r\n        self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\r\n\r\n        return kl_mean\r\n\r\n    def c_train(self, tfdc_r, s):\r\n        """"""\r\n        Update actor network\r\n\r\n        :param tfdc_r: cumulative reward\r\n        :param s: state\r\n\r\n        :return: None\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        with tf.GradientTape() as tape:\r\n            v = self.critic(s)\r\n            advantage = tfdc_r - v\r\n            closs = tf.reduce_mean(tf.square(advantage))\r\n        grad = tape.gradient(closs, self.critic.trainable_weights)\r\n        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))\r\n\r\n    def cal_adv(self, tfs, tfdc_r):\r\n        """"""\r\n        Calculate advantage\r\n\r\n        :param tfs: state\r\n        :param tfdc_r: cumulative reward\r\n\r\n        :return: advantage\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        advantage = tfdc_r - self.critic(tfs)\r\n        return advantage.numpy()\r\n\r\n    def update(self, a_update_steps, c_update_steps, save_interval, env):\r\n        """"""\r\n        Update\r\n\r\n        :param a_update_steps: actor update steps\r\n        :param c_update_steps: critic update steps\r\n        :param save_interval: save interval\r\n        :param env: environment\r\n\r\n        :return: None\r\n        """"""\r\n        global GLOBAL_UPDATE_COUNTER\r\n        while not COORD.should_stop():\r\n            if GLOBAL_EP < EP_MAX:\r\n                UPDATE_EVENT.wait()  # wait until get batch of data\r\n                data = [QUEUE.get() for _ in range(QUEUE.qsize())]  # collect data from all workers\r\n                s, a, r = zip(*data)\r\n                s, a, r = np.vstack(s), np.vstack(a), np.vstack(r)\r\n                s, a, r = np.array(s), np.array(a, np.float32), np.array(r, np.float32)\r\n\r\n                adv = self.cal_adv(s, r)\r\n                # adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful\r\n\r\n                _ = self.actor(s)\r\n                oldpi_prob = tf.exp(self.actor.policy_dist.logp(a))\r\n                oldpi_prob = tf.stop_gradient(oldpi_prob)\r\n\r\n                oldpi_param = self.actor.policy_dist.get_param()\r\n                self.old_dist.set_param(oldpi_param)\r\n\r\n                # update actor\r\n                for _ in range(a_update_steps):\r\n                    kl = self.a_train(s, a, adv, oldpi_prob)\r\n                    if kl > 4 * self.kl_target:  # this in in google\'s paper\r\n                        break\r\n                if kl < self.kl_target / 1.5:  # adaptive lambda, this is in OpenAI\'s paper\r\n                    self.lam /= 2\r\n                elif kl > self.kl_target * 1.5:\r\n                    self.lam *= 2\r\n                self.lam = np.clip(self.lam, 1e-4, 10)  # sometimes explode, this clipping is MorvanZhou\'s solution\r\n\r\n                # update critic\r\n                for _ in range(c_update_steps):\r\n                    self.c_train(r, s)\r\n\r\n                UPDATE_EVENT.clear()  # updating finished\r\n                GLOBAL_UPDATE_COUNTER = 0  # reset counter\r\n                ROLLING_EVENT.set()  # set roll-out available\r\n\r\n                if (not GLOBAL_EP % save_interval) and GLOBAL_EP != self.last_update_epoch:\r\n                    self.save_ckpt(env_name=env.spec.id)\r\n                    self.last_update_epoch = GLOBAL_EP\r\n                    plot_save_log(GLOBAL_RUNNING_R, algorithm_name=self.name, env_name=env.spec.id)\r\n\r\n    def get_action(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s])[0].numpy()\r\n\r\n    def get_action_greedy(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s], greedy=True)[0].numpy()\r\n\r\n    def get_v(self, s):\r\n        """"""\r\n        Compute value\r\n\r\n        :param s: state\r\n\r\n        :return: value\r\n        """"""\r\n        if s.ndim < 2: s = s[np.newaxis, :]\r\n        return self.critic(s)[0, 0]\r\n\r\n    def save_ckpt(self, env_name):\r\n        """"""\r\n        save trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        save_model(self.actor, \'actor\', self.name, env_name)\r\n        save_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def load_ckpt(self, env_name):\r\n        """"""\r\n        load trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        load_model(self.actor, \'actor\', self.name, env_name)\r\n        load_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def learn(self, env, train_episodes=200, test_episodes=100, max_steps=200, save_interval=10, gamma=0.9,\r\n              mode=\'train\', render=False, batch_size=32, a_update_steps=10, c_update_steps=10, n_workers=4,\r\n              plot_func=None):\r\n        """"""\r\n        learn function\r\n\r\n        :param env: learning environment\r\n        :param train_episodes: total number of episodes for training\r\n        :param test_episodes: total number of episodes for testing\r\n        :param max_steps:  maximum number of steps for one episode\r\n        :param save_interval: time steps for saving\r\n        :param gamma: reward discount factor\r\n        :param mode: train or test\r\n        :param render: render each step\r\n        :param batch_size: update batch size\r\n        :param a_update_steps: actor update iteration steps\r\n        :param c_update_steps: critic update iteration steps\r\n        :param n_workers: number of workers\r\n        :param plot_func: additional function for interactive module\r\n        :return: None\r\n        """"""\r\n        t0 = time.time()\r\n        global GLOBAL_PPO, UPDATE_EVENT, ROLLING_EVENT, GLOBAL_UPDATE_COUNTER, GLOBAL_EP, GLOBAL_RUNNING_R, COORD, QUEUE\r\n        global EP_LEN, MIN_BATCH_SIZE, GAMMA, EP_MAX, RENDER\r\n        EP_LEN, MIN_BATCH_SIZE, GAMMA, EP_MAX, RENDER = max_steps, batch_size, gamma, train_episodes, render\r\n        GLOBAL_PPO = self\r\n        if mode == \'train\':  # train\r\n            if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\r\n                assert len(env) == n_workers\r\n            else:\r\n                assert n_workers == 1\r\n                env = env,\r\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env[0].spec.id))\r\n            UPDATE_EVENT, ROLLING_EVENT = threading.Event(), threading.Event()\r\n            UPDATE_EVENT.clear()  # not update now\r\n            ROLLING_EVENT.set()  # start to roll out\r\n            workers = [Worker(wid=i, env=env[i], plot_func=plot_func) for i in range(n_workers)]\r\n\r\n            GLOBAL_UPDATE_COUNTER, GLOBAL_EP = 0, 0\r\n            GLOBAL_RUNNING_R = []\r\n            COORD = tf.train.Coordinator()\r\n            QUEUE = queue.Queue()  # workers putting data in this queue\r\n            threads = []\r\n            for worker in workers:  # worker threads\r\n                t = threading.Thread(target=worker.work, args=())\r\n                t.start()  # training\r\n                threads.append(t)\r\n            # add a PPO updating thread\r\n            threads.append(threading.Thread(target=self.update, args=(a_update_steps, c_update_steps,\r\n                                                                      save_interval, env[0])))\r\n            threads[-1].start()\r\n            COORD.join(threads)\r\n\r\n            self.save_ckpt(env_name=env[0].spec.id)\r\n            plot_save_log(GLOBAL_RUNNING_R, algorithm_name=self.name, env_name=env[0].spec.id)\r\n\r\n        # test\r\n        elif mode == \'test\':\r\n            if isinstance(env, list):  # judge if multiple envs are passed in for parallel computing\r\n                env = env[0]\r\n            else:\r\n                env = env\r\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\r\n            self.load_ckpt(env_name=env.spec.id)\r\n            for eps in range(test_episodes):\r\n                ep_rs_sum = 0\r\n                s = env.reset()\r\n                for step in range(max_steps):\r\n                    if RENDER:\r\n                        env.render()\r\n                    action = self.get_action_greedy(s)\r\n                    s, reward, done, info = env.step(action)\r\n                    ep_rs_sum += reward\r\n                    if done:\r\n                        break\r\n\r\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    eps, test_episodes, ep_rs_sum, time.time() - t0)\r\n                )\r\n        else:\r\n            print(\'unknown mode type\')\r\n\r\n\r\nclass Worker(object):\r\n    """"""\r\n    Worker class for distributional running\r\n    """"""\r\n\r\n    def __init__(self, wid, env, plot_func):\r\n        self.wid = wid\r\n        self.env = env\r\n        global GLOBAL_PPO\r\n        self.ppo = GLOBAL_PPO\r\n        self.plot_func = plot_func\r\n\r\n    def work(self):\r\n        """"""\r\n        Define a worker\r\n\r\n        :return: None\r\n        """"""\r\n        t0 = time.time()\r\n        global GLOBAL_EP, GLOBAL_RUNNING_R, GLOBAL_UPDATE_COUNTER, REWARD_SHAPING, RENDER\r\n        while not COORD.should_stop():\r\n            s = self.env.reset()\r\n            ep_r = 0\r\n            buffer_s, buffer_a, buffer_r = [], [], []\r\n            for t in range(1, EP_LEN + 1):\r\n                if not ROLLING_EVENT.is_set():  # while global PPO is updating\r\n                    ROLLING_EVENT.wait()  # wait until PPO is updated\r\n                    buffer_s, buffer_a, buffer_r = [], [], []  # clear history buffer, use new policy to collect data\r\n                a = self.ppo.get_action(s)\r\n                for step in range(EP_LEN):\r\n                    if RENDER:\r\n                        self.env.render()\r\n                s_, r, done, _ = self.env.step(a)\r\n                buffer_s.append(s)\r\n                buffer_a.append(a)\r\n                buffer_r.append(r)\r\n                s = s_\r\n                ep_r += r\r\n\r\n                GLOBAL_UPDATE_COUNTER += 1  # count to minimum batch size, no need to wait other workers\r\n                if t == EP_LEN - 1 or GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE or done:\r\n                    if done:\r\n                        v_s_ = 0\r\n                    else:\r\n                        try:\r\n                            v_s_ = self.ppo.get_v(s_)\r\n                        except:\r\n                            v_s_ = self.ppo.get_v(s_[np.newaxis, :])  # for raw-pixel input\r\n                    discounted_r = []  # compute discounted reward\r\n                    for r in buffer_r[::-1]:\r\n                        v_s_ = r + GAMMA * v_s_\r\n                        discounted_r.append(v_s_)\r\n                    discounted_r.reverse()\r\n\r\n                    bs = buffer_s\r\n                    ba, br = np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\r\n                    buffer_s, buffer_a, buffer_r = [], [], []\r\n                    QUEUE.put((bs, ba, br))  # put data in the queue\r\n                    if GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE:\r\n                        ROLLING_EVENT.clear()  # stop collecting data\r\n                        UPDATE_EVENT.set()  # globalPPO update\r\n\r\n                    if GLOBAL_EP >= EP_MAX:  # stop training\r\n                        COORD.request_stop()\r\n                        break\r\n                if done:\r\n                    break\r\n\r\n            GLOBAL_RUNNING_R.append(ep_r)\r\n            GLOBAL_EP += 1\r\n\r\n            print(\r\n                \'Episode: {}/{}  | Worker: {} | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    GLOBAL_EP, EP_MAX, self.wid, ep_r,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n            if self.wid == 0 and self.plot_func is not None:\r\n                self.plot_func(GLOBAL_RUNNING_R)\r\n'"
rlzoo/algorithms/dppo_penalty/run_dppo_penalty.py,1,"b'from rlzoo.common.utils import set_seed\r\nfrom rlzoo.algorithms.dppo_penalty.dppo_penalty import DPPO_PENALTY\r\nfrom rlzoo.common.policy_networks import *\r\nfrom rlzoo.common.value_networks import *\r\nimport gym\r\n\r\n\r\nn_workers = 4\r\n"""""" load environment """"""\r\nenv = [gym.make(\'Pendulum-v0\').unwrapped for i in range(n_workers)]\r\n\r\n# reproducible\r\nseed = 2\r\nset_seed(seed)\r\n\r\n\r\n"""""" build networks for the algorithm """"""\r\nname = \'DPPO_PENALTY\'\r\nhidden_dim = 64\r\nnum_hidden_layer = 2\r\ncritic = ValueNetwork(env[0].observation_space, [hidden_dim] * num_hidden_layer, name=name + \'_value\')\r\n\r\nactor = StochasticPolicyNetwork(env[0].observation_space, env[0].action_space,\r\n                                [hidden_dim] * num_hidden_layer, trainable=True,\r\n                                name=name + \'_policy\')\r\nnet_list = critic, actor\r\n\r\n"""""" create model """"""\r\nactor_lr = 1e-4\r\ncritic_lr = 2e-4\r\noptimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\r\nmodel = DPPO_PENALTY(net_list, optimizers_list)\r\n""""""\r\nfull list of arguments for the algorithm\r\n----------------------------------------\r\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\r\noptimizers_list: a list of optimizers for all networks and differentiable variables\r\nkl_target: controls bounds of policy update and adaptive lambda\r\nlam:  KL-regularization coefficient\r\n""""""\r\n\r\nmodel.learn(env, train_episodes=1000, max_steps=200, save_interval=50, gamma=0.9,\r\n            mode=\'train\', render=False, batch_size=32, a_update_steps=10, c_update_steps=10, n_workers=4)\r\n\r\n""""""\r\nfull list of parameters for training\r\n---------------------------------------\r\nenv: learning environment\r\ntrain_episodes: total number of episodes for training\r\ntest_episodes: total number of episodes for testing\r\nmax_steps:  maximum number of steps for one episode\r\nsave_interval: time steps for saving\r\ngamma: reward discount factor\r\nmode: train or test\r\nbatch_size: update batch size\r\na_update_steps: actor update iteration steps\r\nc_update_steps: critic update iteration steps\r\nn_workers: number of workers\r\n:return: None\r\n""""""\r\nmodel.learn(env, test_episodes=100, max_steps=200, mode=\'test\', render=True)\r\n'"
rlzoo/algorithms/dqn/__init__.py,0,b''
rlzoo/algorithms/dqn/default.py,25,"b'from gym.spaces import Discrete\n\nfrom rlzoo.common.utils import set_seed\nfrom rlzoo.common.value_networks import *\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ndouble_q (bool): if True double DQN will be used\ndueling (bool): if True dueling value estimation will be used\nexploration_rate (float): fraction of entire training period over\n    which the exploration rate is annealed\nexploration_final_eps (float): final value of random action probability\nbatch_size (int): size of a batched sampled from replay buffer for training\ntrain_freq (int): update the model every `train_freq` steps\nlearning_starts (int): how many steps of the model to collect transitions\n                        for before learning starts\ntarget_network_update_freq (int): update the target network every\n                                    `target_network_update_freq` steps\nbuffer_size (int): size of the replay buffer\nprioritized_replay (bool): if True prioritized replay buffer will be used.\nprioritized_alpha (float): alpha parameter for prioritized replay\nprioritized_beta0 (float): beta parameter for prioritized replay\nmode (str): train or test\n-----------------------------------------------\n""""""\n\n\ndef atari(env, default_seed=False, **kwargs):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    assert isinstance(env.action_space, Discrete)\n\n    alg_params = dict(\n        dueling=True,\n        double_q=True,\n        buffer_size=1000,\n        prioritized_replay=True,\n        prioritized_alpha=0.6,\n        prioritized_beta0=0.4,\n    )\n    alg_params.update(kwargs)\n    if alg_params.get(\'net_list\') is None:\n        alg_params[\'net_list\'] = [QNetwork(env.observation_space, env.action_space, [64],\n                                           state_only=True, dueling=alg_params[\'dueling\'])]\n\n    if alg_params.get(\'optimizers_list\') is None:\n        alg_params[\'optimizers_list\'] = tf.optimizers.Adam(1e-4, epsilon=1e-5, clipnorm=10),\n\n    learn_params = dict(\n        train_episodes=int(1e5),\n        test_episodes=10,\n        max_steps=200,\n        save_interval=1e4,\n        batch_size=32,\n        exploration_rate=0.1,\n        exploration_final_eps=0.01,\n        train_freq=4,\n        learning_starts=10000,\n        target_network_update_freq=1000,\n        gamma=0.99,\n    )\n\n    return alg_params, learn_params\n\n\ndef classic_control(env, default_seed=False, **kwargs):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    assert isinstance(env.action_space, Discrete)\n\n    alg_params = dict(\n        dueling=True,\n        double_q=True,\n        buffer_size=1000,\n        prioritized_replay=False,\n        prioritized_alpha=0.6,\n        prioritized_beta0=0.4,\n    )\n    alg_params.update(kwargs)\n    if alg_params.get(\'net_list\') is None:\n        alg_params[\'net_list\'] = [QNetwork(env.observation_space, env.action_space, [64], activation=tf.nn.tanh,\n                                           state_only=True, dueling=alg_params[\'dueling\'])]\n\n    if alg_params.get(\'optimizers_list\') is None:\n        alg_params[\'optimizers_list\'] = tf.optimizers.Adam(5e-3, epsilon=1e-5),\n\n    learn_params = dict(\n        train_episodes=int(1e3),\n        test_episodes=10,\n        max_steps=200,\n        save_interval=1e3,\n        batch_size=32,\n        exploration_rate=0.2,\n        exploration_final_eps=0.01,\n        train_freq=4,\n        learning_starts=200,\n        target_network_update_freq=50,\n        gamma=0.99,\n    )\n\n    return alg_params, learn_params\n\n\n# class CNNQNet(tl.models.Model):\n#     def __init__(self, in_dim, act_dim, dueling):\n#         super().__init__()\n#         self._state_shape = in_dim\n#         self._action_shape = act_dim,\n#         self.dueling = dueling\n#         with tf.name_scope(\'DQN\'):\n#             with tf.name_scope(\'CNN\'):\n#                 self.cnn = basic_nets.CNNModel(in_dim)\n#             mlp_in_shape = self.cnn.outputs[0].shape[0]\n#             with tf.name_scope(\'QValue\'):\n#                 hidden_dim = 256\n#                 self.preq = tl.layers.Dense(\n#                     hidden_dim, tf.nn.relu,\n#                     tf.initializers.Orthogonal(1.0),\n#                     in_channels=mlp_in_shape\n#                 )\n#                 self.qout = tl.layers.Dense(\n#                     act_dim, None,\n#                     tf.initializers.Orthogonal(1.0),\n#                     in_channels=hidden_dim\n#                 )\n#             if dueling:\n#                 with tf.name_scope(\'Value\'):\n#                     hidden_dim = 256\n#                     self.prev = tl.layers.Dense(\n#                         hidden_dim, tf.nn.relu,\n#                         tf.initializers.Orthogonal(1.0),\n#                         in_channels=mlp_in_shape\n#                     )\n#                     self.vout = tl.layers.Dense(\n#                         1, None,\n#                         tf.initializers.Orthogonal(1.0),\n#                         in_channels=hidden_dim\n#                     )\n#\n#     def forward(self, obv):\n#         obv = tf.cast(obv, tf.float32) / 255.0\n#         mlp_in = tl.layers.flatten_reshape(self.cnn(obv))\n#         q_out = self.qout(self.preq(mlp_in))\n#         if self.dueling:\n#             v_out = self.vout(self.prev(mlp_in))\n#             q_out = v_out + q_out - tf.reduce_mean(q_out, 1, True)\n#         return q_out\n#\n#     @property\n#     def state_shape(self):\n#         return copy.deepcopy(self._state_shape)\n#\n#     @property\n#     def action_shape(self):\n#         return copy.deepcopy(self._action_shape)\n#\n#\n# class MLPQNet(tl.models.Model):\n#     def __init__(self, in_dim, act_dim, dueling):\n#         super().__init__()\n#         self._state_shape = in_dim,\n#         self._action_shape = act_dim,\n#         self.dueling = dueling\n#         hidden_dim = 64\n#         with tf.name_scope(\'DQN\'):\n#             with tf.name_scope(\'MLP\'):\n#                 self.mlp = tl.layers.Dense(\n#                     hidden_dim, tf.nn.tanh,\n#                     tf.initializers.Orthogonal(1.0),\n#                     in_channels=in_dim\n#                 )\n#             with tf.name_scope(\'QValue\'):\n#                 self.qmlp = tl.layers.Dense(\n#                     act_dim, None,\n#                     tf.initializers.Orthogonal(1.0),\n#                     in_channels=hidden_dim\n#                 )\n#             if dueling:\n#                 with tf.name_scope(\'Value\'):\n#                     self.vmlp = tl.layers.Dense(\n#                         1, None,\n#                         tf.initializers.Orthogonal(1.0),\n#                         in_channels=hidden_dim\n#                     )\n#\n#     def forward(self, obv):\n#         obv = tf.cast(obv, tf.float32)\n#         latent = self.mlp(obv)\n#         q_out = self.qmlp(latent)\n#         if self.dueling:\n#             v_out = self.vmlp(latent)\n#             q_out = v_out + q_out - tf.reduce_mean(q_out, 1, True)\n#         return q_out\n#\n#     @property\n#     def state_shape(self):\n#         return copy.deepcopy(self._state_shape)\n#\n#     @property\n#     def action_shape(self):\n#         return copy.deepcopy(self._action_shape)\n'"
rlzoo/algorithms/dqn/dqn.py,18,"b'""""""\nDeep Q Network\n""""""\nimport random\nfrom copy import deepcopy\n\nfrom rlzoo.common.utils import *\nfrom rlzoo.common.buffer import ReplayBuffer, PrioritizedReplayBuffer\nfrom rlzoo.common.value_networks import *\n\n\nclass DQN(object):\n    """"""\n    Papers:\n\n    Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep\n    reinforcement learning[J]. Nature, 2015, 518(7540): 529.\n\n    Hessel M, Modayil J, Van Hasselt H, et al. Rainbow: Combining Improvements\n    in Deep Reinforcement Learning[J]. 2017.\n    """"""\n\n    def __init__(self, net_list, optimizers_list, double_q, dueling, buffer_size,\n                 prioritized_replay, prioritized_alpha, prioritized_beta0, ):\n        """"""\n        Parameters:\n        ----------\n        :param net_list (list): a list of networks (value and policy) used in the algorithm, from common functions or customization\n        :param optimizers_list (list): a list of optimizers for all networks and differentiable variables\n        :param double_q (bool): if True double DQN will be used\n        :param dueling (bool): if True dueling value estimation will be used\n        :param buffer_size (int): size of the replay buffer\n        :param prioritized_replay (bool): if True prioritized replay buffer will be used.\n        :param prioritized_alpha (float): alpha parameter for prioritized replay\n        :param prioritized_beta0 (float): beta parameter for prioritized replay\n        """"""\n        assert isinstance(net_list[0], QNetwork)\n        self.name = \'DQN\'\n        if prioritized_replay:\n            self.buffer = PrioritizedReplayBuffer(\n                buffer_size, prioritized_alpha, prioritized_beta0)\n        else:\n            self.buffer = ReplayBuffer(buffer_size)\n\n        self.network = net_list[0]\n        self.target_network = deepcopy(net_list[0])\n        self.network.train()\n        self.target_network.infer()\n        self.optimizer = optimizers_list[0]\n        self.double_q = double_q\n        self.prioritized_replay = prioritized_replay\n        self.dueling = dueling\n\n    def get_action(self, obv, eps=0.2):\n        out_dim = self.network.action_shape[0]\n        if random.random() < eps:\n            return int(random.random() * out_dim)\n        else:\n            obv = np.expand_dims(obv, 0).astype(\'float32\')\n            return self.network(obv).numpy().argmax(1)[0]\n\n    def get_action_greedy(self, obv):\n        obv = np.expand_dims(obv, 0).astype(\'float32\')\n        return self.network(obv).numpy().argmax(1)[0]\n\n    def sync(self):\n        """"""Copy q network to target q network""""""\n\n        for var, var_tar in zip(self.network.trainable_weights,\n                                self.target_network.trainable_weights):\n            var_tar.assign(var)\n\n    def save_ckpt(self, env_name):\n        """"""\n        save trained weights\n        :return: None\n        """"""\n        save_model(self.network, \'qnet\', \'DQN\', env_name)\n\n    def load_ckpt(self, env_name):\n        """"""\n        load trained weights\n        :return: None\n        """"""\n        load_model(self.network, \'qnet\', \'DQN\', env_name)\n\n    # @tf.function\n    def _td_error(self, transitions, reward_gamma):\n        b_o, b_a, b_r, b_o_, b_d = transitions\n        b_d = tf.cast(b_d, tf.float32)\n        b_a = tf.cast(b_a, tf.int64)\n        b_r = tf.cast(b_r, tf.float32)\n        if self.double_q:\n            b_a_ = tf.one_hot(tf.argmax(self.network(b_o_), 1), self.network.action_shape[0])\n            b_q_ = (1 - b_d) * tf.reduce_sum(self.target_network(b_o_) * b_a_, 1)\n        else:\n            b_q_ = (1 - b_d) * tf.reduce_max(self.target_network(b_o_), 1)\n\n        b_q = tf.reduce_sum(self.network(b_o) * tf.one_hot(b_a, self.network.action_shape[0]), 1)\n        return b_q - (b_r + reward_gamma * b_q_)\n\n    def store_transition(self, s, a, r, s_, d):\n        self.buffer.push(s, a, r, s_, d)\n\n    def update(self, batch_size, gamma):\n        if self.prioritized_replay:\n            # sample from prioritized replay buffer\n            *transitions, b_w, idxs = self.buffer.sample(batch_size)\n            # calculate weighted huber loss\n            with tf.GradientTape() as tape:\n                priorities = self._td_error(transitions, gamma)\n                huber_loss = tf.where(tf.abs(priorities) < 1,\n                                      tf.square(priorities) * 0.5,\n                                      tf.abs(priorities) - 0.5)\n                loss = tf.reduce_mean(huber_loss * b_w)\n            # backpropagate\n            grad = tape.gradient(loss, self.network.trainable_weights)\n            self.optimizer.apply_gradients(zip(grad, self.network.trainable_weights))\n            # update priorities\n            priorities = np.clip(np.abs(priorities), 1e-6, None)\n            self.buffer.update_priorities(idxs, priorities)\n        else:\n            # sample from prioritized replay buffer\n            transitions = self.buffer.sample(batch_size)\n            # calculate huber loss\n            with tf.GradientTape() as tape:\n                td_errors = self._td_error(transitions, gamma)\n                huber_loss = tf.where(tf.abs(td_errors) < 1,\n                                      tf.square(td_errors) * 0.5,\n                                      tf.abs(td_errors) - 0.5)\n                loss = tf.reduce_mean(huber_loss)\n            # backpropagate\n            grad = tape.gradient(loss, self.network.trainable_weights)\n            self.optimizer.apply_gradients(zip(grad, self.network.trainable_weights))\n\n    def learn(\n            self, env, mode=\'train\', render=False,\n            train_episodes=1000, test_episodes=10, max_steps=200,\n            save_interval=1000, gamma=0.99,\n            exploration_rate=0.2, exploration_final_eps=0.01,\n            target_network_update_freq=50,\n            batch_size=32, train_freq=4, learning_starts=200,\n            plot_func=None\n    ):\n\n        """"""\n        :param env: learning environment\n        :param mode: train or test\n        :param render: render each step\n        :param train_episodes: total number of episodes for training\n        :param test_episodes: total number of episodes for testing\n        :param max_steps: maximum number of steps for one episode\n        :param save_interval: time steps for saving\n        :param gamma: reward decay factor\n        :param exploration_rate (float): fraction of entire training period over\n            which the exploration rate is annealed\n        :param exploration_final_eps (float): final value of random action probability\n        :param target_network_update_freq (int): update the target network every\n                                          `target_network_update_freq` steps\n        :param batch_size (int): size of a batched sampled from replay buffer for training\n        :param train_freq (int): update the model every `train_freq` steps\n        :param learning_starts (int): how many steps of the model to collect transitions\n                               for before learning starts\n        :param plot_func: additional function for interactive module\n\n        """"""\n        if mode == \'train\':\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            reward_buffer = []\n            i = 0\n            for episode in range(1, train_episodes + 1):\n                o = env.reset()\n                ep_reward = 0\n                for step in range(1, max_steps + 1):\n                    i += 1\n                    if render:\n                        env.render()\n                    eps = 1 - (1 - exploration_final_eps) * \\\n                          min(1, i / exploration_rate * (train_episodes * max_steps))\n                    a = self.get_action(o, eps)\n\n                    # execute action and feed to replay buffer\n                    # note that `_` tail in var name means next\n                    o_, r, done, info = env.step(a)\n                    self.store_transition(o, a, r, o_, done)\n                    ep_reward += r\n\n                    # update networks\n                    if i >= learning_starts and i % train_freq == 0:\n                        self.update(batch_size, gamma)\n\n                    if i % target_network_update_freq == 0:\n                        self.sync()\n\n                    # reset current observation\n                    if done:\n                        break\n                    else:\n                        o = o_\n\n                    # saving model\n                    if i % save_interval == 0:\n                        self.save_ckpt(env.spec.id)\n                print(\n                    \'Time steps so far: {}, episode so far: {}, \'\n                    \'episode reward: {:.4f}, episode length: {}\'\n                        .format(i, episode, ep_reward, step)\n                )\n                reward_buffer.append(ep_reward)\n                if plot_func is not None:\n                    plot_func(reward_buffer)\n\n        elif mode == \'test\':\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n\n            self.load_ckpt(env.spec.id)\n            self.network.infer()\n\n            reward_buffer = []\n            for episode in range(1, test_episodes + 1):\n                o = env.reset()\n                ep_reward = 0\n                for step in range(1, max_steps + 1):\n                    if render:\n                        env.render()\n                    a = self.get_action_greedy(o)\n\n                    # execute action\n                    # note that `_` tail in var name means next\n                    o_, r, done, info = env.step(a)\n                    ep_reward += r\n\n                    if done:\n                        break\n                    else:\n                        o = o_\n\n                print(\n                    \'episode so far: {}, \'\n                    \'episode reward: {:.4f}, episode length: {}\'\n                        .format(episode, ep_reward, step)\n                )\n                reward_buffer.append(ep_reward)\n                if plot_func is not None:\n                    plot_func(reward_buffer)\n\n        else:\n            print(\'unknown mode type\')\n'"
rlzoo/algorithms/dqn/run_dqn.py,2,"b'import gym\n\nfrom rlzoo.algorithms.dqn.dqn import DQN\nfrom rlzoo.algorithms.dqn.default import *\nfrom rlzoo.common.value_networks import *\nimport gym\n\n"""""" load environment """"""\nenv = gym.make(\'CartPole-v0\').unwrapped\n\nobs_space = env.observation_space\nact_space = env.action_space\n\n# reproducible\nseed = 2\nset_seed(seed, env)\n\nin_dim = env.observation_space.shape[0]\nact_dim = env.action_space.n\n"""""" build networks for the algorithm """"""\nname = \'DQN\'\nQ_net = QNetwork(env.observation_space, env.action_space, [64], activation=tf.nn.tanh,\n                 state_only=True, dueling=True)\nnet_list = [Q_net]\n\n"""""" create model """"""\noptimizer = tf.optimizers.Adam(5e-3, epsilon=1e-5)\noptimizers_list = [optimizer]\nmodel = DQN(net_list, optimizers_list,\n            double_q=True,\n            dueling=True,\n            buffer_size=10000,\n            prioritized_replay=False,\n            prioritized_alpha=0.6,\n            prioritized_beta0=0.4)\n"""""" \nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nreplay_buffer_size: the size of buffer for storing explored samples\ntau: soft update factor\n""""""\n\nmodel.learn(env, mode=\'train\', render=False,\n            train_episodes=1000,\n            test_episodes=10,\n            max_steps=200,\n            save_interval=1e3,\n            batch_size=32,\n            exploration_rate=0.2,\n            exploration_final_eps=0.01,\n            train_freq=4,\n            learning_starts=200,\n            target_network_update_freq=50,\n            gamma=0.99, )\n""""""\nfull list of parameters for training\n---------------------------------------\nenv: learning environment\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: time steps for saving\nexplore_steps: for random action sampling in the beginning of training\nmode: train or test mode\nrender: render each step\nbatch_size: update batch size\ngamma: reward decay factor\nnoise_scale: range of action noise for exploration\nnoise_scale_decay: noise scale decay factor\n""""""\n\nmodel.learn(env, mode=\'test\', render=True,\n            test_episodes=10,\n            batch_size=32,\n            exploration_rate=0.2,\n            exploration_final_eps=0.01,\n            train_freq=4,\n            learning_starts=200,\n            target_network_update_freq=50,\n            gamma=0.99, )\n'"
rlzoo/algorithms/pg/__init__.py,0,b''
rlzoo/algorithms/pg/default.py,21,"b'from rlzoo.common.policy_networks import *\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: time steps for saving\nmode: train or test\nrender: render each step\ngamma: reward decay\n-----------------------------------------------\n""""""\n\n\ndef atari(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict()\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 1  # number of hidden layers for the networks\n        hidden_dim = 32  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PG\'):\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     num_hidden_layer * [hidden_dim])\n        net_list = [policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        learning_rate = 0.02\n        policy_optimizer = tf.optimizers.Adam(learning_rate)\n        optimizers_list = [policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=200,\n        test_episodes=100,\n        max_steps=200,\n        save_interval=20,\n        gamma=0.95\n    )\n\n    return alg_params, learn_params\n\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict()\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 1  # number of hidden layers for the networks\n        hidden_dim = 32  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PG\'):\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     num_hidden_layer * [hidden_dim])\n        net_list = [policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        learning_rate = 0.02\n        policy_optimizer = tf.optimizers.Adam(learning_rate)\n        optimizers_list = [policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=200,\n        test_episodes=100,\n        max_steps=200,\n        save_interval=20,\n        gamma=0.95\n    )\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict()\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 1  # number of hidden layers for the networks\n        hidden_dim = 32  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PG\'):\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     num_hidden_layer * [hidden_dim])\n        net_list = [policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        learning_rate = 0.02\n        policy_optimizer = tf.optimizers.Adam(learning_rate)\n        optimizers_list = [policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=200,\n        test_episodes=100,\n        max_steps=200,\n        save_interval=20,\n        gamma=0.95\n    )\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict()\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 1  # number of hidden layers for the networks\n        hidden_dim = 32  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PG\'):\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     num_hidden_layer * [hidden_dim])\n        net_list = [policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        learning_rate = 0.02\n        policy_optimizer = tf.optimizers.Adam(learning_rate)\n        optimizers_list = [policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=200,\n        test_episodes=100,\n        max_steps=200,\n        save_interval=20,\n        gamma=0.95\n    )\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict()\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 1  # number of hidden layers for the networks\n        hidden_dim = 32  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PG\'):\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     num_hidden_layer * [hidden_dim])\n        net_list = [policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        learning_rate = 0.02\n        policy_optimizer = tf.optimizers.Adam(learning_rate)\n        optimizers_list = [policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=200,\n        test_episodes=100,\n        max_steps=200,\n        save_interval=20,\n        gamma=0.95\n    )\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict()\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 1  # number of hidden layers for the networks\n        hidden_dim = 32  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PG\'):\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     num_hidden_layer * [hidden_dim])\n        net_list = [policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        learning_rate = 0.02\n        policy_optimizer = tf.optimizers.Adam(learning_rate)\n        optimizers_list = [policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=200,\n        test_episodes=100,\n        max_steps=200,\n        save_interval=20,\n        gamma=0.95\n    )\n\n    return alg_params, learn_params\n\n\ndef rlbench(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict()\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 1  # number of hidden layers for the networks\n        hidden_dim = 32  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PG\'):\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     num_hidden_layer * [hidden_dim])\n        net_list = [policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        learning_rate = 0.02\n        policy_optimizer = tf.optimizers.Adam(learning_rate)\n        optimizers_list = [policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        train_episodes=200,\n        test_episodes=100,\n        max_steps=200,\n        save_interval=20,\n        gamma=0.95\n    )\n\n    return alg_params, learn_params\n'"
rlzoo/algorithms/pg/pg.py,2,"b'""""""\nVanilla Policy Gradient(VPG or REINFORCE)\n-----------------------------------------\nThe policy gradient algorithm works by updating policy parameters via stochastic gradient ascent on policy performance.\nIt\'s an on-policy algorithm can be used for environments with either discrete or continuous action spaces.\nHere is an example on discrete action space game CartPole-v0.\nTo apply it on continuous action space, you need to change the last softmax layer and the get_action function.\n\nReference\n---------\nCookbook: Barto A G, Sutton R S. Reinforcement Learning: An Introduction[J]. 1998.\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials/\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/\n\nPrerequisites\n--------------\ntensorflow >=2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer >=2.0.0\n\n""""""\nimport time\n\nfrom rlzoo.common.utils import *\nfrom rlzoo.common.policy_networks import *\n\n\n###############################  PG  ####################################\n\n\nclass PG:\n    """"""\n    PG class\n    """"""\n\n    def __init__(self, net_list, optimizers_list):\n        """"""\n        :param net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\n        :param optimizers_list: a list of optimizers for all networks and differentiable variables\n\n        """"""\n        assert len(net_list) == 1\n        assert len(optimizers_list) == 1\n        self.name = \'PG\'\n        self.model = net_list[0]\n        assert isinstance(self.model, StochasticPolicyNetwork)\n        self.buffer = []\n        print(\'Policy Network\', self.model)\n        self.optimizer = optimizers_list[0]\n\n    def get_action(self, s):\n        """"""\n        choose action with probabilities.\n\n        :param s: state\n\n        :return: act\n        """"""\n        return self.model([s])[0].numpy()\n\n    def get_action_greedy(self, s):\n        """"""\n        choose action with greedy policy\n\n        :param s: state\n\n        :return: act\n        """"""\n        return self.model([s], greedy=True).numpy()[0]\n\n    def store_transition(self, s, a, r):\n        """"""\n        store data in memory buffer\n\n        :param s: state\n        :param a: act\n        :param r: reward\n\n        :return:\n        """"""\n        self.buffer.append([s, np.array(a, np.float32), np.array(r, np.float32)])\n\n    def update(self, gamma):\n        """"""\n        update policy parameters via stochastic gradient ascent\n\n        :return: None\n        """"""\n        # discount and normalize episode reward\n        s, a, r = zip(*self.buffer)\n        s, a, r = np.array(s), np.array(a), np.array(r).flatten()\n        discounted_ep_rs_norm = self._discount_and_norm_rewards(r, gamma)\n\n        with tf.GradientTape() as tape:\n            self.model(s)\n            neg_log_prob = self.model.policy_dist.neglogp(a)\n            loss = tf.reduce_mean(neg_log_prob * discounted_ep_rs_norm)  # reward guided loss\n\n        grad = tape.gradient(loss, self.model.trainable_weights)\n        self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n\n        self.buffer = []\n        return discounted_ep_rs_norm\n\n    def _discount_and_norm_rewards(self, reward_list, gamma):\n        """"""\n        compute discount_and_norm_rewards\n\n        :return: discount_and_norm_rewards\n        """"""\n        # discount episode rewards\n        discounted_ep_rs = np.zeros_like(reward_list)\n        running_add = 0\n        for t in reversed(range(0, len(reward_list))):\n            running_add = running_add * gamma + reward_list[t]\n            discounted_ep_rs[t] = running_add\n\n        # normalize episode rewards\n        discounted_ep_rs -= np.mean(discounted_ep_rs)\n        std = np.std(discounted_ep_rs)\n        if std != 0:\n            discounted_ep_rs /= np.std(discounted_ep_rs)\n        discounted_ep_rs = discounted_ep_rs[:, np.newaxis]\n        return discounted_ep_rs\n\n    def save_ckpt(self, env_name):\n        """"""\n        save trained weights\n\n        :return: None\n        """"""\n        save_model(self.model, \'model_policy\', self.name, env_name)\n\n    def load_ckpt(self, env_name):\n        """"""\n        load trained weights\n\n        :return: None\n        """"""\n        load_model(self.model, \'model_policy\', self.name, env_name)\n\n    def learn(self, env, train_episodes=200, test_episodes=100, max_steps=200, save_interval=100,\n              mode=\'train\', render=False, gamma=0.95, plot_func=None):\n        """"""\n        :param env: learning environment\n        :param train_episodes: total number of episodes for training\n        :param test_episodes: total number of episodes for testing\n        :param max_steps: maximum number of steps for one episode\n        :param save_interval: time steps for saving\n        :param mode: train or test\n        :param render: render each step\n        :param gamma: reward decay\n        :param plot_func: additional function for interactive module\n        :return: None\n        """"""\n\n        if mode == \'train\':\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            reward_buffer = []\n            t0 = time.time()\n\n            for i_episode in range(1, train_episodes + 1):\n\n                observation = env.reset()\n\n                ep_rs_sum = 0\n                for step in range(max_steps):\n                    if render:\n                        env.render()\n                    action = self.get_action(observation)\n                    observation_, reward, done, info = env.step(action)\n                    self.store_transition(observation, action, reward)\n\n                    ep_rs_sum += reward\n                    observation = observation_\n\n                    if done:\n                        break\n\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    i_episode, train_episodes, ep_rs_sum, time.time() - t0)\n                )\n                reward_buffer.append(ep_rs_sum)\n                if plot_func is not None:\n                    plot_func(reward_buffer)\n\n                self.update(gamma)\n\n                if i_episode and i_episode % save_interval == 0:\n                    self.save_ckpt(env_name=env.spec.id)\n                    plot_save_log(reward_buffer, algorithm_name=\'PG\', env_name=env.spec.id)\n\n            self.save_ckpt(env_name=env.spec.id)\n            plot_save_log(reward_buffer, algorithm_name=\'PG\', env_name=env.spec.id)\n\n        elif mode == \'test\':\n            # test\n            self.load_ckpt(env_name=env.spec.id)\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            t0 = time.time()\n            for eps in range(test_episodes):\n                observation = env.reset()\n                ep_rs_sum = 0\n                for step in range(max_steps):\n                    if render:\n                        env.render()\n                    action = self.get_action_greedy(observation)\n                    observation, reward, done, info = env.step(action)\n                    ep_rs_sum += reward\n                    if done:\n                        break\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    eps, test_episodes, ep_rs_sum, time.time() - t0)\n                )\n\n        else:\n            print(\'unknown mode type\')\n'"
rlzoo/algorithms/pg/run_pg.py,2,"b'from rlzoo.algorithms.pg.pg import PG\r\nfrom rlzoo.common.policy_networks import *\r\nimport gym\r\n\r\n"""""" load environment """"""\r\nenv = gym.make(\'CartPole-v0\').unwrapped\r\n# env = gym.make(\'Pendulum-v0\').unwrapped\r\n# env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized/wrapped environment to run\r\nobs_space = env.observation_space\r\nact_space = env.action_space\r\n\r\n# reproducible\r\nseed = 2\r\nnp.random.seed(seed)\r\ntf.random.set_seed(seed)\r\nenv.seed(seed)\r\n\r\n"""""" build networks for the algorithm """"""\r\nname = \'pg\'\r\nnum_hidden_layer = 1  # number of hidden layers for the networks\r\nhidden_dim = 32  # dimension of hidden layers for the networks\r\n\r\npolicy_net = StochasticPolicyNetwork(obs_space, act_space, num_hidden_layer * [hidden_dim])\r\nnet_list = [policy_net]\r\n\r\n"""""" choose optimizers """"""\r\nlearning_rate = 0.02\r\npolicy_optimizer = tf.optimizers.Adam(learning_rate)\r\noptimizers_list = [policy_optimizer]\r\n\r\nmodel = PG(net_list, optimizers_list)\r\n"""""" \r\nfull list of arguments for the algorithm\r\n----------------------------------------\r\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\r\noptimizers_list: a list of optimizers for all networks and differentiable variables\r\n""""""\r\n\r\nmodel.learn(env, train_episodes=200, max_steps=200, save_interval=20, mode=\'train\', render=False, gamma=0.95)\r\n""""""\r\nfull list of parameters for training\r\n---------------------------------------\r\nenv: learning environment\r\ntrain_episodes: total number of episodes for training\r\ntest_episodes: total number of episodes for testing\r\nmax_steps: maximum number of steps for one episode\r\nsave_interval: time steps for saving\r\nmode: train or test\r\nrender: render each step\r\ngamma: reward decay\r\n""""""\r\n\r\n# test\r\nmodel.learn(env, test_episodes=100, max_steps=200, mode=\'test\', render=True)\r\n'"
rlzoo/algorithms/ppo/__init__.py,0,b''
rlzoo/algorithms/ppo/default.py,35,"b'from rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nepsilon: clip parameter  (for method \'clip\')\nkl_target: controls bounds of policy update and adaptive lambda  (for method \'penalty\')\nlam:  KL-regularization coefficient   (for method \'penalty\')\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: time steps for saving\ngamma: reward discount factor\nmode: train or test\nrender: render each step\nbatch_size: UPDATE batch size\na_update_steps: actor update iteration steps\nc_update_steps: critic update iteration steps\n-----------------------------------------------\n""""""\n\n\ndef atari(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 1\n        set_seed(seed, env)\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5,)  # for method \'penalty\'\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer,\n                                                     output_activation=tf.nn.tanh, trainable=True)\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        batch_size=32,\n                        a_update_steps=10,\n                        c_update_steps=10)\n\n    return alg_params, learn_params\n\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 1\n        set_seed(seed, env)\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5,)  # for method \'penalty\'\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer,\n                                                     output_activation=tf.nn.tanh, trainable=True)\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        batch_size=32,\n                        a_update_steps=10,\n                        c_update_steps=10)\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 1\n        set_seed(seed, env)\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5,)  # for method \'penalty\'\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer,\n                                                     output_activation=tf.nn.tanh, trainable=True)\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        batch_size=32,\n                        a_update_steps=10,\n                        c_update_steps=10)\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 1\n        set_seed(seed, env)\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5,)  # for method \'penalty\'\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer,\n                                                     output_activation=tf.nn.tanh, trainable=True)\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        batch_size=32,\n                        a_update_steps=10,\n                        c_update_steps=10)\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 1\n        set_seed(seed, env)\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5,)  # for method \'penalty\'\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer,\n                                                     output_activation=tf.nn.tanh, trainable=True)\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        batch_size=32,\n                        a_update_steps=10,\n                        c_update_steps=10)\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 1\n        set_seed(seed, env)\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5,)  # for method \'penalty\'\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer,\n                                                     output_activation=tf.nn.tanh, trainable=True)\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        batch_size=32,\n                        a_update_steps=10,\n                        c_update_steps=10)\n\n    return alg_params, learn_params\n\n\ndef rlbench(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 1\n        set_seed(seed, env)\n\n    alg_params = dict(method=\'clip\',  # method can be clip or penalty\n                      epsilon=0.2,  # for method \'clip\'\n                      kl_target=0.01,  # for method \'penalty\'\n                      lam=0.5,)  # for method \'penalty\'\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'PPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer,\n                                                     output_activation=tf.nn.tanh, trainable=True)\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        actor_lr = 1e-4\n        critic_lr = 2e-4\n        optimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=1000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=50,\n                        gamma=0.9,\n                        batch_size=32,\n                        a_update_steps=10,\n                        c_update_steps=10)\n\n    return alg_params, learn_params\n'"
rlzoo/algorithms/ppo/ppo.py,0,"b""from rlzoo.algorithms.ppo_penalty.ppo_penalty import PPO_PENALTY\r\nfrom rlzoo.algorithms.ppo_clip.ppo_clip import PPO_CLIP\r\n\r\n\r\ndef PPO(**alg_params):\r\n    method = alg_params['method']\r\n    if method == 'penalty':\r\n        del alg_params['epsilon']\r\n        algo = PPO_PENALTY\r\n    elif method == 'clip':\r\n        del alg_params['kl_target']\r\n        del alg_params['lam']\r\n        algo = PPO_CLIP\r\n    else:\r\n        raise ValueError('Method input error. Method can only be penalty or clip')\r\n    del alg_params['method']\r\n    return algo(**alg_params)\r\n"""
rlzoo/algorithms/ppo_clip/__init__.py,0,b''
rlzoo/algorithms/ppo_clip/ppo_clip.py,8,"b'""""""\r\nProximal Policy Optimization (PPO)\r\n----------------------------\r\nA simple version of Proximal Policy Optimization (PPO) using single thread.\r\nPPO is a family of first-order methods that use a few other tricks to keep new policies close to old.\r\nPPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO.\r\n\r\nReference\r\n---------\r\nProximal Policy Optimization Algorithms, Schulman et al. 2017\r\nHigh Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016\r\nEmergence of Locomotion Behaviours in Rich Environments, Heess et al. 2017\r\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials\r\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/\r\n\r\nPrerequisites\r\n--------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-probability 0.6.0\r\ntensorlayer >=2.0.0\r\n\r\n""""""\r\nimport time\r\n\r\nfrom rlzoo.common.utils import *\r\nfrom rlzoo.common.policy_networks import *\r\nfrom rlzoo.common.value_networks import *\r\n\r\n\r\nEPS = 1e-8  # epsilon\r\n\r\n\r\n###############################  PPO  ####################################\r\n\r\nclass PPO_CLIP(object):\r\n    """"""\r\n    PPO class\r\n    """"""\r\n\r\n    def __init__(self, net_list, optimizers_list, epsilon=0.2):\r\n        """"""\r\n        :param net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\r\n        :param optimizers_list: a list of optimizers for all networks and differentiable variables\r\n        :param epsilon: clip parameter\r\n        """"""\r\n        assert len(net_list) == 2\r\n        assert len(optimizers_list) == 2\r\n\r\n        self.name = \'PPO_CLIP\'\r\n        self.epsilon = epsilon\r\n\r\n        self.critic, self.actor = net_list\r\n\r\n        assert isinstance(self.critic, ValueNetwork)\r\n        assert isinstance(self.actor, StochasticPolicyNetwork)\r\n\r\n        self.critic_opt, self.actor_opt = optimizers_list\r\n\r\n    def a_train(self, tfs, tfa, tfadv, oldpi_prob):\r\n        """"""\r\n        Update policy network\r\n\r\n        :param tfs: state\r\n        :param tfa: act\r\n        :param tfadv: advantage\r\n        :param oldpi_prob: old policy distribution\r\n\r\n        :return: None\r\n        """"""\r\n        try:\r\n            tfs = np.array(tfs, np.float32)\r\n            tfa = np.array(tfa, np.float32)\r\n            tfadv = np.array(tfadv, np.float32)\r\n        except:\r\n            pass\r\n\r\n        with tf.GradientTape() as tape:\r\n            _ = self.actor(tfs)\r\n            pi_prob = tf.exp(self.actor.policy_dist.logp(tfa))\r\n            ratio = pi_prob / (oldpi_prob + EPS)\r\n\r\n            surr = ratio * tfadv\r\n            aloss = -tf.reduce_mean(\r\n                tf.minimum(surr, tf.clip_by_value(ratio, 1. - self.epsilon, 1. + self.epsilon) * tfadv))\r\n        a_gard = tape.gradient(aloss, self.actor.trainable_weights)\r\n        self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\r\n\r\n    def c_train(self, tfdc_r, s):\r\n        """"""\r\n        Update actor network\r\n\r\n        :param tfdc_r: cumulative reward\r\n        :param s: state\r\n\r\n        :return: None\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        with tf.GradientTape() as tape:\r\n            v = self.critic(s)\r\n            advantage = tfdc_r - v\r\n            closs = tf.reduce_mean(tf.square(advantage))\r\n        grad = tape.gradient(closs, self.critic.trainable_weights)\r\n        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))\r\n\r\n    def cal_adv(self, tfs, tfdc_r):\r\n        """"""\r\n        Calculate advantage\r\n\r\n        :param tfs: state\r\n        :param tfdc_r: cumulative reward\r\n\r\n        :return: advantage\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        advantage = tfdc_r - self.critic(tfs)\r\n        return advantage.numpy()\r\n\r\n    def update(self, s, a, r, a_update_steps, c_update_steps):\r\n        """"""\r\n        Update parameter with the constraint of KL divergent\r\n\r\n        :param s: state\r\n        :param a: act\r\n        :param r: reward\r\n\r\n        :return: None\r\n        """"""\r\n        adv = self.cal_adv(s, r)\r\n        # adv = (adv - adv.mean())/(adv.std()+1e-6)  # adv norm, sometimes helpful\r\n\r\n        _ = self.actor(s)\r\n        oldpi_prob = tf.exp(self.actor.policy_dist.logp(a))\r\n        oldpi_prob = tf.stop_gradient(oldpi_prob)\r\n\r\n        # update actor\r\n        for _ in range(a_update_steps):\r\n            self.a_train(s, a, adv, oldpi_prob)\r\n\r\n        # update critic\r\n        for _ in range(c_update_steps):\r\n            self.c_train(r, s)\r\n\r\n    def get_action(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s])[0].numpy()\r\n\r\n    def get_action_greedy(self, s):\r\n        """"""\r\n        Choose action\r\n        :param s: state\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s], greedy=True)[0].numpy()\r\n\r\n    def get_v(self, s):\r\n        """"""\r\n        Compute value\r\n\r\n        :param s: state\r\n\r\n        :return: value\r\n        """"""\r\n        try:\r\n            s = s.astype(np.float32)\r\n            if s.ndim < 2: s = s[np.newaxis, :]\r\n        except:\r\n            pass\r\n        res = self.critic(s)[0, 0]\r\n        return res\r\n\r\n    def save_ckpt(self, env_name):\r\n        """"""\r\n        save trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        save_model(self.actor, \'actor\', self.name, env_name)\r\n        save_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def load_ckpt(self, env_name):\r\n        """"""\r\n        load trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        load_model(self.actor, \'actor\', self.name, env_name)\r\n        load_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def learn(self, env, train_episodes=200, test_episodes=100, max_steps=200, save_interval=10,\r\n              gamma=0.9, mode=\'train\', render=False, batch_size=32, a_update_steps=10, c_update_steps=10,\r\n              plot_func=None):\r\n        """"""\r\n        learn function\r\n\r\n        :param env: learning environment\r\n        :param train_episodes: total number of episodes for training\r\n        :param test_episodes: total number of episodes for testing\r\n        :param max_steps: maximum number of steps for one episode\r\n        :param save_interval: timesteps for saving\r\n        :param gamma: reward discount factor\r\n        :param mode: train or test\r\n        :param render: render each step\r\n        :param batch_size: udpate batchsize\r\n        :param a_update_steps: actor update iteration steps\r\n        :param c_update_steps: critic update iteration steps\r\n        :param plot_func: additional function for interactive module\r\n        :return: None\r\n        """"""\r\n\r\n        t0 = time.time()\r\n\r\n        if mode == \'train\':\r\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\r\n            reward_buffer = []\r\n            for ep in range(1, train_episodes + 1):\r\n                s = env.reset()\r\n                buffer_s, buffer_a, buffer_r = [], [], []\r\n                ep_rs_sum = 0\r\n                for t in range(max_steps):  # in one episode\r\n                    if render:\r\n                        env.render()\r\n                    a = self.get_action(s)\r\n\r\n                    s_, r, done, _ = env.step(a)\r\n                    buffer_s.append(s)\r\n                    buffer_a.append(a)\r\n                    buffer_r.append(r)\r\n                    s = s_\r\n                    ep_rs_sum += r\r\n\r\n                    # update ppo\r\n                    if (t + 1) % batch_size == 0 or t == max_steps - 1 or done:\r\n                        if done:\r\n                            v_s_ = 0\r\n                        else:\r\n                            try:\r\n                                v_s_ = self.get_v(s_)\r\n                            except:\r\n                                v_s_ = self.get_v([s_])   # for raw-pixel input\r\n\r\n                        discounted_r = []\r\n                        for r in buffer_r[::-1]:\r\n                            v_s_ = r + gamma * v_s_\r\n                            discounted_r.append(v_s_)\r\n                        discounted_r.reverse()\r\n                        # bs = buffer_s if len(buffer_s[0].shape)>1 else np.vstack(buffer_s) # no vstack for raw-pixel input\r\n                        bs = buffer_s\r\n                        ba, br = np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\r\n                        buffer_s, buffer_a, buffer_r = [], [], []\r\n\r\n                        self.update(bs, ba, br, a_update_steps, c_update_steps)\r\n                    if done:\r\n                        break\r\n\r\n                print(\r\n                    \'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                        ep, train_episodes, ep_rs_sum,\r\n                        time.time() - t0\r\n                    )\r\n                )\r\n\r\n                reward_buffer.append(ep_rs_sum)\r\n                if plot_func is not None:\r\n                    plot_func(reward_buffer)\r\n                if ep and not ep % save_interval:\r\n                    self.save_ckpt(env_name=env.spec.id)\r\n                    plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\r\n\r\n            self.save_ckpt(env_name=env.spec.id)\r\n            plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\r\n\r\n        # test\r\n        elif mode == \'test\':\r\n            self.load_ckpt(env_name=env.spec.id)\r\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\r\n            reward_buffer = []\r\n            for eps in range(test_episodes):\r\n                ep_rs_sum = 0\r\n                s = env.reset()\r\n                for step in range(max_steps):\r\n                    if render:\r\n                        env.render()\r\n                    action = self.get_action_greedy(s)\r\n                    s, reward, done, info = env.step(action)\r\n                    ep_rs_sum += reward\r\n                    if done:\r\n                        break\r\n\r\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    eps, test_episodes, ep_rs_sum, time.time() - t0)\r\n                )\r\n                reward_buffer.append(ep_rs_sum)\r\n                if plot_func:\r\n                    plot_func(reward_buffer)\r\n        else:\r\n            print(\'unknown mode type\')\r\n'"
rlzoo/algorithms/ppo_clip/run_ppo_clip.py,2,"b'from rlzoo.common.utils import make_env, set_seed\nfrom rlzoo.algorithms.ppo_clip.ppo_clip import PPO_CLIP\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nimport gym\n\n\n"""""" load environment """"""\nenv = gym.make(\'Pendulum-v0\').unwrapped\n\n# reproducible\nseed = 1\nset_seed(seed, env)\n\n"""""" build networks for the algorithm """"""\nname = \'PPO_CLIP\'\nhidden_dim = 64\nnum_hidden_layer = 2\ncritic = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer, name=name + \'_value\')\n\nactor = StochasticPolicyNetwork(env.observation_space, env.action_space, [hidden_dim] * num_hidden_layer,\n                                output_activation=tf.nn.tanh, name=name + \'_policy\')\nnet_list = critic, actor\n\n"""""" create model """"""\nactor_lr = 1e-4\ncritic_lr = 2e-4\noptimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n\nmodel = PPO_CLIP(net_list, optimizers_list,)\n""""""\nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nepsilon: clip parameter\n""""""\n\nmodel.learn(env, train_episodes=500, max_steps=200, save_interval=50, gamma=0.9,\n            mode=\'train\', render=False, batch_size=32, a_update_steps=10, c_update_steps=10)\n\n""""""\nfull list of parameters for training\n---------------------------------------\nenv: learning environment\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: time steps for saving\ngamma: reward discount factor\nmode: train or test\nrender: render each step\nbatch_size: UPDATE batch size\na_update_steps: actor update iteration steps\nc_update_steps: critic update iteration steps\n:return: None\n""""""\nmodel.learn(env, test_episodes=100, max_steps=200, mode=\'test\', render=True)\n\n'"
rlzoo/algorithms/ppo_penalty/__init__.py,0,b''
rlzoo/algorithms/ppo_penalty/ppo_penalty.py,8,"b'""""""\r\nProximal Policy Optimization (PPO)\r\n----------------------------\r\nA simple version of Proximal Policy Optimization (PPO) using single thread.\r\nPPO is a family of first-order methods that use a few other tricks to keep new policies close to old.\r\nPPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO.\r\n\r\nReference\r\n---------\r\nProximal Policy Optimization Algorithms, Schulman et al. 2017\r\nHigh Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016\r\nEmergence of Locomotion Behaviours in Rich Environments, Heess et al. 2017\r\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials\r\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/\r\n\r\nPrerequisites\r\n--------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-probability 0.6.0\r\ntensorlayer >=2.0.0\r\n\r\n""""""\r\nimport time\r\n\r\nfrom rlzoo.common.utils import *\r\nfrom rlzoo.common.policy_networks import *\r\nfrom rlzoo.common.value_networks import *\r\n\r\n\r\nEPS = 1e-8  # epsilon\r\n\r\n\r\nclass PPO_PENALTY(object):\r\n    """"""\r\n    PPO class\r\n    """"""\r\n\r\n    def __init__(self, net_list, optimizers_list, kl_target=0.01, lam=0.5):\r\n        """"""\r\n        :param net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\r\n        :param optimizers_list: a list of optimizers for all networks and differentiable variables\r\n        :param kl_target: controls bounds of policy update and adaptive lambda\r\n        :param lam:  KL-regularization coefficient\r\n        """"""\r\n        assert len(net_list) == 2\r\n        assert len(optimizers_list) == 2\r\n\r\n        self.name = \'PPO_PENALTY\'\r\n\r\n        self.critic, self.actor = net_list\r\n\r\n        assert isinstance(self.critic, ValueNetwork)\r\n        assert isinstance(self.actor, StochasticPolicyNetwork)\r\n\r\n        self.kl_target = kl_target\r\n        self.lam = lam\r\n\r\n        self.critic_opt, self.actor_opt = optimizers_list\r\n        self.old_dist = make_dist(self.actor.action_space)\r\n\r\n    def a_train(self, tfs, tfa, tfadv, oldpi_prob):\r\n        """"""\r\n        Update policy network\r\n\r\n        :param tfs: state\r\n        :param tfa: act\r\n        :param tfadv: advantage\r\n\r\n        :return:\r\n        """"""\r\n        tfs = np.array(tfs)\r\n        tfa = np.array(tfa, np.float32)\r\n        tfadv = np.array(tfadv, np.float32)\r\n\r\n        with tf.GradientTape() as tape:\r\n            _ = self.actor(tfs)\r\n            pi_prob = tf.exp(self.actor.policy_dist.logp(tfa))\r\n            ratio = pi_prob / (oldpi_prob + EPS)\r\n\r\n            surr = ratio * tfadv\r\n            kl = self.old_dist.kl(self.actor.policy_dist.param)\r\n            # kl = tfp.distributions.kl_divergence(oldpi, pi)\r\n            kl_mean = tf.reduce_mean(kl)\r\n            aloss = -(tf.reduce_mean(surr - self.lam * kl))\r\n        a_gard = tape.gradient(aloss, self.actor.trainable_weights)\r\n        self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\r\n\r\n        return kl_mean\r\n\r\n    def c_train(self, tfdc_r, s):\r\n        """"""\r\n        Update actor network\r\n\r\n        :param tfdc_r: cumulative reward\r\n        :param s: state\r\n\r\n        :return: None\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        with tf.GradientTape() as tape:\r\n            v = self.critic(s)\r\n            advantage = tfdc_r - v\r\n            closs = tf.reduce_mean(tf.square(advantage))\r\n        grad = tape.gradient(closs, self.critic.trainable_weights)\r\n        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))\r\n\r\n    def cal_adv(self, tfs, tfdc_r):\r\n        """"""\r\n        Calculate advantage\r\n\r\n        :param tfs: state\r\n        :param tfdc_r: cumulative reward\r\n\r\n        :return: advantage\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        advantage = tfdc_r - self.critic(tfs)\r\n        return advantage.numpy()\r\n\r\n    def update(self, s, a, r, a_update_steps, c_update_steps):\r\n        """"""\r\n        Update parameter with the constraint of KL divergent\r\n\r\n        :param s: state\r\n        :param a: act\r\n        :param r: reward\r\n\r\n        :return: None\r\n        """"""\r\n        adv = self.cal_adv(s, r)\r\n        # adv = (adv - adv.mean())/(adv.std()+1e-6)  # normalize advantage, sometimes helpful\r\n\r\n        _ = self.actor(s)\r\n        oldpi_prob = tf.exp(self.actor.policy_dist.logp(a))\r\n        oldpi_prob = tf.stop_gradient(oldpi_prob)\r\n\r\n        oldpi_param = self.actor.policy_dist.get_param()\r\n        self.old_dist.set_param(oldpi_param)\r\n\r\n        for _ in range(a_update_steps):\r\n            kl = self.a_train(s, a, adv, oldpi_prob)\r\n            if kl > 4 * self.kl_target:  # this in in google\'s paper\r\n                break\r\n        if kl < self.kl_target / 1.5:  # adaptive lambda, this is in OpenAI\'s paper\r\n            self.lam /= 2\r\n        elif kl > self.kl_target * 1.5:\r\n            self.lam *= 2\r\n        self.lam = np.clip(self.lam, 1e-4, 10)  # sometimes explode, this clipping is MorvanZhou\'s solution\r\n\r\n        # update critic\r\n        for _ in range(c_update_steps):\r\n            self.c_train(r, s)\r\n\r\n    def get_action(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s])[0].numpy()\r\n\r\n    def get_action_greedy(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s], greedy=True)[0].numpy()\r\n\r\n    def get_v(self, s):\r\n        """"""\r\n        Compute value\r\n\r\n        :param s: state\r\n\r\n        :return: value\r\n        """"""\r\n        if s.ndim < 2: s = s[np.newaxis, :]\r\n        return self.critic(s)[0, 0]\r\n\r\n    def save_ckpt(self, env_name):\r\n        """"""\r\n        save trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        save_model(self.actor, \'actor\', self.name, env_name)\r\n        save_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def load_ckpt(self, env_name):\r\n        """"""\r\n        load trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        load_model(self.actor, \'actor\', self.name, env_name)\r\n        load_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def learn(self, env, train_episodes=1000, test_episodes=10, max_steps=200, save_interval=10, gamma=0.9,\r\n              mode=\'train\', render=False, batch_size=32, a_update_steps=10, c_update_steps=10,\r\n              plot_func=None):\r\n        """"""\r\n        learn function\r\n\r\n        :param env: learning environment\r\n        :param train_episodes: total number of episodes for training\r\n        :param test_episodes: total number of episodes for testing\r\n        :param max_steps: maximum number of steps for one episode\r\n        :param save_interval: time steps for saving\r\n        :param gamma: reward discount factor\r\n        :param mode: train or test\r\n        :param render: render each step\r\n        :param batch_size: update batch size\r\n        :param a_update_steps: actor update iteration steps\r\n        :param c_update_steps: critic update iteration steps\r\n        :param plot_func: additional function for interactive module\r\n        :return: None\r\n        """"""\r\n\r\n        t0 = time.time()\r\n        if mode == \'train\':\r\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\r\n            reward_buffer = []\r\n            for ep in range(1, train_episodes + 1):\r\n                s = env.reset()\r\n                buffer_s, buffer_a, buffer_r = [], [], []\r\n                ep_r = 0\r\n                for t in range(max_steps):  # in one episode\r\n                    if render:\r\n                        env.render()\r\n                    a = self.get_action(s)\r\n                    s_, r, done, _ = env.step(a)\r\n                    buffer_s.append(s)\r\n                    buffer_a.append(a)\r\n                    buffer_r.append(r)  # normalize reward, find to be useful\r\n                    s = s_\r\n                    ep_r += r\r\n\r\n                    # update ppo\r\n                    if (t + 1) % batch_size == 0 or t == max_steps - 1:\r\n                        if done:\r\n                            v_s_ = 0\r\n                        else:\r\n                            try:\r\n                                v_s_ = self.get_v(s_)\r\n                            except:\r\n                                v_s_ = self.get_v(s_[np.newaxis, :])   # for raw-pixel input\r\n                        discounted_r = []\r\n                        for r in buffer_r[::-1]:\r\n                            v_s_ = r + gamma * v_s_\r\n                            discounted_r.append(v_s_)\r\n                        discounted_r.reverse()\r\n\r\n                        bs = buffer_s\r\n                        ba, br = np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\r\n                        buffer_s, buffer_a, buffer_r = [], [], []\r\n                        self.update(bs, ba, br, a_update_steps, c_update_steps)\r\n\r\n                    if done:\r\n                        break\r\n\r\n                print(\r\n                    \'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                        ep, train_episodes, ep_r,\r\n                        time.time() - t0\r\n                    )\r\n                )\r\n\r\n                reward_buffer.append(ep_r)\r\n                if plot_func is not None:\r\n                    plot_func(reward_buffer)\r\n                if ep and not ep % save_interval:\r\n                    self.save_ckpt(env_name=env.spec.id)\r\n                    plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\r\n\r\n            self.save_ckpt(env_name=env.spec.id)\r\n            plot_save_log(reward_buffer, algorithm_name=self.name, env_name=env.spec.id)\r\n\r\n        # test\r\n        elif mode == \'test\':\r\n            self.load_ckpt(env_name=env.spec.id)\r\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\r\n            reward_buffer = []\r\n            for eps in range(test_episodes):\r\n                ep_rs_sum = 0\r\n                s = env.reset()\r\n                for step in range(max_steps):\r\n                    if render:\r\n                        env.render()\r\n                    action = self.get_action_greedy(s)\r\n                    s, reward, done, info = env.step(action)\r\n                    ep_rs_sum += reward\r\n                    if done:\r\n                        break\r\n\r\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    eps, test_episodes, ep_rs_sum, time.time() - t0)\r\n                )\r\n            reward_buffer.append(ep_rs_sum)\r\n            if plot_func:\r\n                plot_func(reward_buffer)\r\n        else:\r\n            print(\'unknown mode type\')\r\n\r\n'"
rlzoo/algorithms/ppo_penalty/run_ppo_penalty.py,2,"b'from rlzoo.common.utils import make_env, set_seed\nfrom rlzoo.algorithms.ppo_penalty.ppo_penalty import PPO_PENALTY\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nimport gym\n\n\n"""""" load environment """"""\nenv = gym.make(\'Pendulum-v0\').unwrapped\n\n# reproducible\nseed = 1\nset_seed(seed, env)\n\n"""""" build networks for the algorithm """"""\nname = \'PPO_PENALTY\'\nhidden_dim = 64\nnum_hidden_layer = 2\ncritic = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer, name=name + \'_value\')\n\nactor = StochasticPolicyNetwork(env.observation_space, env.action_space, [hidden_dim] * num_hidden_layer,\n                                output_activation=tf.nn.tanh, name=name + \'_policy\')\nnet_list = critic, actor\n\n"""""" create model """"""\nactor_lr = 1e-4\ncritic_lr = 2e-4\noptimizers_list = [tf.optimizers.Adam(critic_lr), tf.optimizers.Adam(actor_lr)]\n\nmodel = PPO_PENALTY(net_list, optimizers_list,)\n""""""\nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nkl_target: controls bounds of policy update and adaptive lambda\nlam:  KL-regularization coefficient\n""""""\n\nmodel.learn(env, train_episodes=500, max_steps=200, save_interval=50, gamma=0.9,\n            mode=\'train\', render=False, batch_size=32, a_update_steps=10, c_update_steps=10)\n\n""""""\nfull list of parameters for training\n---------------------------------------\nenv: learning environment\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: times teps for saving\ngamma: reward discount factor\nmode: train or test\nrender: render each step\nbatch_size: update batch size\na_update_steps: actor update iteration steps\nc_update_steps: critic update iteration steps\n:return: None\n""""""\n\nmodel.learn(env, test_episodes=100, max_steps=200, mode=\'test\', render=True)\n'"
rlzoo/algorithms/sac/__init__.py,0,b''
rlzoo/algorithms/sac/default.py,60,"b'from rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nreplay_buffer_capacity: the size of buffer for storing explored samples\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ntrain_episodes:  total number of episodes for training\ntest_episodes:  total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nbatch_size:  udpate batchsize\nexplore_steps:  for random action sampling in the beginning of training\nupdate_itr: repeated updates for single step\npolicy_target_update_interval: delayed update for the policy network and target networks\nreward_scale: value range of reward\nsave_interval: timesteps for saving the weights and plotting the results\nmode: \'train\'  or \'test\'\nAUTO_ENTROPY: automatically udpating variable alpha for entropy\nrender: if true, visualize the environment\n-----------------------------------------------\n""""""\n\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks, default as the same for each layer here\n        with tf.name_scope(\'SAC\'):\n            with tf.name_scope(\'Q_Net1\'):\n                soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                     output_activation=None,\n                                                     state_conditioned=True)\n        net_list = [soft_q_net1, soft_q_net2, target_soft_q_net1, target_soft_q_net2, policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        soft_q_lr, policy_lr, alpha_lr = 3e-4, 3e-4, 3e-4  # soft_q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network; alpha_lr: learning rate of the variable alpha\n        soft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)\n        soft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        alpha_optimizer = tf.optimizers.Adam(alpha_lr)\n        optimizers_list = [soft_q_optimizer1, soft_q_optimizer2, policy_optimizer, alpha_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=200,\n        update_itr=3,\n        policy_target_update_interval=3,\n        reward_scale=1.,\n        AUTO_ENTROPY=True,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks, default as the same for each layer here\n        with tf.name_scope(\'SAC\'):\n            with tf.name_scope(\'Q_Net1\'):\n                soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                     output_activation=None,\n                                                     state_conditioned=True)\n        net_list = [soft_q_net1, soft_q_net2, target_soft_q_net1, target_soft_q_net2, policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        soft_q_lr, policy_lr, alpha_lr = 3e-4, 3e-4, 3e-4  # soft_q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network; alpha_lr: learning rate of the variable alpha\n        soft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)\n        soft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        alpha_optimizer = tf.optimizers.Adam(alpha_lr)\n        optimizers_list = [soft_q_optimizer1, soft_q_optimizer2, policy_optimizer, alpha_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=200,\n        update_itr=3,\n        policy_target_update_interval=3,\n        reward_scale=1.,\n        AUTO_ENTROPY=True,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks, default as the same for each layer here\n        with tf.name_scope(\'SAC\'):\n            with tf.name_scope(\'Q_Net1\'):\n                soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                     output_activation=None,\n                                                     state_conditioned=True)\n        net_list = [soft_q_net1, soft_q_net2, target_soft_q_net1, target_soft_q_net2, policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        soft_q_lr, policy_lr, alpha_lr = 3e-4, 3e-4, 3e-4  # soft_q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network; alpha_lr: learning rate of the variable alpha\n        soft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)\n        soft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        alpha_optimizer = tf.optimizers.Adam(alpha_lr)\n        optimizers_list = [soft_q_optimizer1, soft_q_optimizer2, policy_optimizer, alpha_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=200,\n        update_itr=3,\n        policy_target_update_interval=3,\n        reward_scale=1.,\n        AUTO_ENTROPY=True,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks, default as the same for each layer here\n        with tf.name_scope(\'SAC\'):\n            with tf.name_scope(\'Q_Net1\'):\n                soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                     output_activation=None,\n                                                     state_conditioned=True)\n        net_list = [soft_q_net1, soft_q_net2, target_soft_q_net1, target_soft_q_net2, policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        soft_q_lr, policy_lr, alpha_lr = 3e-4, 3e-4, 3e-4  # soft_q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network; alpha_lr: learning rate of the variable alpha\n        soft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)\n        soft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        alpha_optimizer = tf.optimizers.Adam(alpha_lr)\n        optimizers_list = [soft_q_optimizer1, soft_q_optimizer2, policy_optimizer, alpha_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=200,\n        update_itr=3,\n        policy_target_update_interval=3,\n        reward_scale=1.,\n        AUTO_ENTROPY=True,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks, default as the same for each layer here\n        with tf.name_scope(\'SAC\'):\n            with tf.name_scope(\'Q_Net1\'):\n                soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                     output_activation=None,\n                                                     state_conditioned=True)\n        net_list = [soft_q_net1, soft_q_net2, target_soft_q_net1, target_soft_q_net2, policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        soft_q_lr, policy_lr, alpha_lr = 3e-4, 3e-4, 3e-4  # soft_q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network; alpha_lr: learning rate of the variable alpha\n        soft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)\n        soft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        alpha_optimizer = tf.optimizers.Adam(alpha_lr)\n        optimizers_list = [soft_q_optimizer1, soft_q_optimizer2, policy_optimizer, alpha_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=200,\n        update_itr=3,\n        policy_target_update_interval=3,\n        reward_scale=1.,\n        AUTO_ENTROPY=True,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef rlbench(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks, default as the same for each layer here\n        with tf.name_scope(\'SAC\'):\n            with tf.name_scope(\'Q_Net1\'):\n                soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                              hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     hidden_dim_list=num_hidden_layer * [hidden_dim],\n                                                     output_activation=None,\n                                                     state_conditioned=True)\n        net_list = [soft_q_net1, soft_q_net2, target_soft_q_net1, target_soft_q_net2, policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        soft_q_lr, policy_lr, alpha_lr = 3e-4, 3e-4, 3e-4  # soft_q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network; alpha_lr: learning rate of the variable alpha\n        soft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)\n        soft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        alpha_optimizer = tf.optimizers.Adam(alpha_lr)\n        optimizers_list = [soft_q_optimizer1, soft_q_optimizer2, policy_optimizer, alpha_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=200,\n        update_itr=3,\n        policy_target_update_interval=3,\n        reward_scale=1.,\n        AUTO_ENTROPY=True,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n'"
rlzoo/algorithms/sac/run_sac.py,11,"b'from rlzoo.algorithms.sac.sac import SAC\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nimport gym\n\n"""""" load environment """"""\nenv = gym.make(\'Pendulum-v0\').unwrapped\n# env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized/wrapped environment to run\naction_shape = env.action_space.shape\nstate_shape = env.observation_space.shape\n# reproducible\nseed = 2\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nenv.seed(seed)\n\n"""""" build networks for the algorithm """"""\nnum_hidden_layer = 2  # number of hidden layers for the networks\nhidden_dim = 64  # dimension of hidden layers for the networks, default as the same for each layer here\nwith tf.name_scope(\'SAC\'):\n    with tf.name_scope(\'Q_Net1\'):\n        soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                               hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Q_Net2\'):\n        soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                               hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Target_Q_Net1\'):\n        target_soft_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                      hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Target_Q_Net2\'):\n        target_soft_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                      hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Policy\'):\n        policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                             hidden_dim_list=num_hidden_layer * [hidden_dim], \n                                             output_activation=None,\n                                             state_conditioned=True)\nnet_list = [soft_q_net1, soft_q_net2, target_soft_q_net1, target_soft_q_net2, policy_net]\n\n"""""" choose optimizers """"""\nsoft_q_lr, policy_lr, alpha_lr = 3e-4, 3e-4, 3e-4  # soft_q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network; alpha_lr: learning rate of the variable alpha\nsoft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)\nsoft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)\npolicy_optimizer = tf.optimizers.Adam(policy_lr)\nalpha_optimizer = tf.optimizers.Adam(alpha_lr)\noptimizers_list = [soft_q_optimizer1, soft_q_optimizer2, policy_optimizer, alpha_optimizer]\n\nmodel = SAC(net_list, optimizers_list)\n"""""" \nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nstate_dim: dimension of state for the environment\naction_dim: dimension of action for the environment\nreplay_buffer_capacity: the size of buffer for storing explored samples\naction_range: value of each action in [-action_range, action_range]\n""""""\n\nmodel.learn(env, train_episodes=100, max_steps=150, batch_size=64, explore_steps=500, \\\n            update_itr=3, policy_target_update_interval=3, reward_scale=1., save_interval=10, \\\n            mode=\'train\', AUTO_ENTROPY=True, render=False)\n"""""" \nfull list of parameters for training\n---------------------------------------\nenv: learning environment\ntrain_episodes:  total number of episodes for training\ntest_episodes:  total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nbatch_size:  udpate batchsize\nexplore_steps:  for random action sampling in the beginning of training\nupdate_itr: repeated updates for single step\npolicy_target_update_interval: delayed update for the policy network and target networks\nreward_scale: value range of reward\nsave_interval: timesteps for saving the weights and plotting the results\nmode: \'train\'  or \'test\'\nAUTO_ENTROPY: automatically udpating variable alpha for entropy\nDETERMINISTIC: stochastic action policy if False, otherwise deterministic\nrender: if true, visualize the environment\n""""""\n# test\nmodel.learn(env, test_episodes=10, max_steps=150, mode=\'test\', render=True)\n'"
rlzoo/algorithms/sac/sac.py,18,"b'""""""\nSoft Actor-Critic\nusing target Q instead of V net: 2 Q net, 2 target Q net, 1 policy net\nadding alpha loss\npaper: https://arxiv.org/pdf/1812.05905.pdf\nActor policy is stochastic.\nEnv: Openai Gym Pendulum-v0, continuous action space\ntensorflow 2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer 2.0.0\n&&\npip install box2d box2d-kengz --user\n""""""\n\nimport time\n\nimport tensorflow_probability as tfp\nimport tensorlayer as tl\nfrom rlzoo.common.utils import *\nfrom rlzoo.common.buffer import *\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\n\ntfd = tfp.distributions\nNormal = tfd.Normal\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n\nclass SAC():\n    """""" Soft Actor-Critic """"""\n\n    def __init__(self, net_list, optimizers_list, replay_buffer_capacity=5e5):\n        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n        self.name = \'SAC\'\n\n        # get all networks\n        [self.soft_q_net1, self.soft_q_net2, self.target_soft_q_net1, self.target_soft_q_net2,\n         self.policy_net] = net_list\n\n        assert isinstance(self.soft_q_net1, QNetwork)\n        assert isinstance(self.soft_q_net2, QNetwork)\n        assert isinstance(self.target_soft_q_net1, QNetwork)\n        assert isinstance(self.target_soft_q_net2, QNetwork)\n        assert isinstance(self.policy_net, StochasticPolicyNetwork)\n        assert isinstance(self.policy_net.action_space, gym.spaces.Box)\n\n        self.action_dim = self.policy_net.action_shape[0]\n\n        self.log_alpha = tf.Variable(0, dtype=np.float32, name=\'log_alpha\')\n        self.alpha = tf.math.exp(self.log_alpha)\n        print(\'Soft Q Network (1,2): \', self.soft_q_net1)\n        print(\'Policy Network: \', self.policy_net)\n\n        # initialize weights of target networks\n        self.target_soft_q_net1 = self.target_ini(self.soft_q_net1, self.target_soft_q_net1)\n        self.target_soft_q_net2 = self.target_ini(self.soft_q_net2, self.target_soft_q_net2)\n\n        [self.soft_q_optimizer1, self.soft_q_optimizer2, self.policy_optimizer, self.alpha_optimizer] = optimizers_list\n\n    def evaluate(self, state, epsilon=1e-6):\n        """""" generate action with state for calculating gradients """"""\n        _ = self.policy_net(state)\n        mean, log_std = self.policy_net.policy_dist.get_param()  # as SAC uses TanhNorm instead of normal distribution, need original mean_std\n        std = tf.math.exp(log_std)  # no clip in evaluation, clip affects gradients flow\n\n        normal = Normal(0, 1)\n        z = normal.sample(mean.shape)\n        action_0 = tf.math.tanh(mean + std * z)  # TanhNormal distribution as actions; reparameterization trick\n        # according to original paper, with an extra last term for normalizing different action range\n        log_prob = Normal(mean, std).log_prob(mean + std * z) - tf.math.log(1. - action_0 ** 2 + epsilon)\n        # both dims of normal.log_prob and -log(1-a**2) are (N,dim_of_action);\n        # the Normal.log_prob outputs the same dim of input features instead of 1 dim probability,\n        # needs sum up across the dim of actions to get 1 dim probability; or else use Multivariate Normal.\n        log_prob = tf.reduce_sum(log_prob, axis=1)[:, np.newaxis]  # expand dim as reduce_sum causes 1 dim reduced\n\n        action = action_0 * self.policy_net.policy_dist.action_scale + self.policy_net.policy_dist.action_mean\n\n        return action, log_prob, z, mean, log_std\n\n    def get_action(self, state):\n        """""" generate action with state for interaction with envronment """"""\n        action, _, _, _, _ = self.evaluate(np.array([state]))\n        return action.numpy()[0]\n\n    def get_action_greedy(self, state):\n        """""" generate action with state for interaction with envronment """"""\n        mean = self.policy_net(np.array([state]), greedy=True).numpy()[0]\n        action = tf.math.tanh(mean) * self.policy_net.policy_dist.action_scale + self.policy_net.policy_dist.action_mean\n        return action\n\n    def sample_action(self, ):\n        """""" generate random actions for exploration """"""\n        return self.policy_net.random_sample()\n\n    def target_ini(self, net, target_net):\n        """""" hard-copy update for initializing target networks """"""\n        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n            target_param.assign(param)\n        return target_net\n\n    def target_soft_update(self, net, target_net, soft_tau):\n        """""" soft update the target net with Polyak averaging """"""\n        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n            target_param.assign(  # copy weight value into target parameters\n                target_param * (1.0 - soft_tau) + param * soft_tau\n            )\n        return target_net\n\n    def update(self, batch_size, reward_scale=10., auto_entropy=True, target_entropy=-2, gamma=0.99, soft_tau=1e-2):\n        """""" update all networks in SAC """"""\n        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n\n        reward = reward[:, np.newaxis]  # expand dim\n        done = done[:, np.newaxis]\n\n        reward = reward_scale * (reward -\n                                 np.mean(reward, axis=0)) / (\n                         np.std(reward, axis=0) + 1e-6)  # normalize with batch mean and std\n\n        # Training Q Function\n        new_next_action, next_log_prob, _, _, _ = self.evaluate(next_state)\n        target_q_min = tf.minimum(\n            self.target_soft_q_net1([next_state, new_next_action]),\n            self.target_soft_q_net2([next_state, new_next_action])\n        ) - self.alpha * next_log_prob\n        target_q_value = reward + (1 - done) * gamma * target_q_min  # if done==1, only reward\n\n        with tf.GradientTape() as q1_tape:\n            predicted_q_value1 = self.soft_q_net1([state, action])\n            q_value_loss1 = tf.reduce_mean(tf.losses.mean_squared_error(predicted_q_value1, target_q_value))\n        q1_grad = q1_tape.gradient(q_value_loss1, self.soft_q_net1.trainable_weights)\n        self.soft_q_optimizer1.apply_gradients(zip(q1_grad, self.soft_q_net1.trainable_weights))\n\n        with tf.GradientTape() as q2_tape:\n            predicted_q_value2 = self.soft_q_net2([state, action])\n            q_value_loss2 = tf.reduce_mean(tf.losses.mean_squared_error(predicted_q_value2, target_q_value))\n        q2_grad = q2_tape.gradient(q_value_loss2, self.soft_q_net2.trainable_weights)\n        self.soft_q_optimizer2.apply_gradients(zip(q2_grad, self.soft_q_net2.trainable_weights))\n\n        # Training Policy Function\n        with tf.GradientTape() as p_tape:\n            new_action, log_prob, z, mean, log_std = self.evaluate(state)\n            """""" implementation 1 """"""\n            predicted_new_q_value = tf.minimum(self.soft_q_net1([state, new_action]),\n                                               self.soft_q_net2([state, new_action]))\n            """""" implementation 2 """"""\n            # predicted_new_q_value = self.soft_q_net1([state, new_action])\n            policy_loss = tf.reduce_mean(self.alpha * log_prob - predicted_new_q_value)\n        p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)\n        self.policy_optimizer.apply_gradients(zip(p_grad, self.policy_net.trainable_weights))\n\n        # Updating alpha w.r.t entropy\n        # alpha: trade-off between exploration (max entropy) and exploitation (max Q)\n        if auto_entropy is True:\n            with tf.GradientTape() as alpha_tape:\n                alpha_loss = -tf.reduce_mean((self.log_alpha * (log_prob + target_entropy)))\n            alpha_grad = alpha_tape.gradient(alpha_loss, [self.log_alpha])\n            self.alpha_optimizer.apply_gradients(zip(alpha_grad, [self.log_alpha]))\n            self.alpha = tf.math.exp(self.log_alpha)\n        else:  # fixed alpha\n            self.alpha = 1.\n            alpha_loss = 0\n\n        # Soft update the target value nets\n        self.target_soft_q_net1 = self.target_soft_update(self.soft_q_net1, self.target_soft_q_net1, soft_tau)\n        self.target_soft_q_net2 = self.target_soft_update(self.soft_q_net2, self.target_soft_q_net2, soft_tau)\n\n    def save_ckpt(self, env_name):\n        """""" save trained weights """"""\n        save_model(self.soft_q_net1, \'model_q_net1\', self.name, env_name)\n        save_model(self.soft_q_net2, \'model_q_net2\', self.name, env_name)\n        save_model(self.target_soft_q_net1, \'model_target_q_net1\', self.name, env_name)\n        save_model(self.target_soft_q_net2, \'model_target_q_net2\', self.name, env_name)\n        save_model(self.policy_net, \'model_policy_net\', self.name, env_name)\n\n    def load_ckpt(self, env_name):\n        """""" load trained weights """"""\n        load_model(self.soft_q_net1, \'model_q_net1\', self.name, env_name)\n        load_model(self.soft_q_net2, \'model_q_net2\', self.name, env_name)\n        load_model(self.target_soft_q_net1, \'model_target_q_net1\', self.name, env_name)\n        load_model(self.target_soft_q_net2, \'model_target_q_net2\', self.name, env_name)\n        load_model(self.policy_net, \'model_policy_net\', self.name, env_name)\n\n    def learn(self, env, train_episodes=1000, test_episodes=1000, max_steps=150, batch_size=64, explore_steps=500,\n              update_itr=3, policy_target_update_interval=3, reward_scale=1., save_interval=20,\n              mode=\'train\', AUTO_ENTROPY=True, render=False, plot_func=None):\n        """"""\n        :param env: learning environment\n        :param train_episodes:  total number of episodes for training\n        :param test_episodes:  total number of episodes for testing\n        :param max_steps:  maximum number of steps for one episode\n        :param batch_size:  udpate batchsize\n        :param explore_steps:  for random action sampling in the beginning of training\n        :param update_itr: repeated updates for single step\n        :param policy_target_update_interval: delayed update for the policy network and target networks\n        :param reward_scale: value range of reward\n        :param save_interval: timesteps for saving the weights and plotting the results\n        :param mode: \'train\' or \'test\'\n        :param AUTO_ENTROPY: automatically updating variable alpha for entropy\n        :param render: if true, visualize the environment\n        :param plot_func: additional function for interactive module\n        """"""\n\n        # training loop\n        if mode == \'train\':\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            frame_idx = 0\n            rewards = []\n            t0 = time.time()\n            for eps in range(train_episodes):\n                state = env.reset()\n                episode_reward = 0\n\n                for step in range(max_steps):\n                    if frame_idx > explore_steps:\n                        action = self.get_action(state)\n                    else:\n                        action = self.sample_action()\n\n                    next_state, reward, done, _ = env.step(action)\n                    if render: env.render()\n                    done = 1 if done == True else 0\n\n                    self.replay_buffer.push(state, action, reward, next_state, done)\n\n                    state = next_state\n                    episode_reward += reward\n                    frame_idx += 1\n\n                    if len(self.replay_buffer) > batch_size:\n                        for i in range(update_itr):\n                            self.update(\n                                batch_size, reward_scale=reward_scale, auto_entropy=AUTO_ENTROPY,\n                                target_entropy=-1. * self.action_dim\n                            )\n\n                    if done:\n                        break\n                if eps % int(save_interval) == 0:\n                    plot_save_log(rewards, algorithm_name=self.name, env_name=env.spec.id)\n                    self.save_ckpt(env_name=env.spec.id)\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\' \\\n                      .format(eps, train_episodes, episode_reward, time.time() - t0))\n                rewards.append(episode_reward)\n                if plot_func is not None:\n                    plot_func(rewards)\n            plot_save_log(rewards, algorithm_name=self.name, env_name=env.spec.id)\n            self.save_ckpt(env_name=env.spec.id)\n\n        elif mode == \'test\':\n            frame_idx = 0\n            rewards = []\n            t0 = time.time()\n            self.load_ckpt(env_name=env.spec.id)\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            # set test mode\n            self.soft_q_net1.eval()\n            self.soft_q_net2.eval()\n            self.target_soft_q_net1.eval()\n            self.target_soft_q_net2.eval()\n            self.policy_net.eval()\n\n            for eps in range(test_episodes):\n                state = env.reset()\n                episode_reward = 0\n\n                for step in range(max_steps):\n                    action = self.get_action_greedy(state)\n                    next_state, reward, done, _ = env.step(action)\n                    if render: env.render()\n                    done = 1 if done == True else 0\n\n                    state = next_state\n                    episode_reward += reward\n                    frame_idx += 1\n                    if done:\n                        break\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\' \\\n                      .format(eps, test_episodes, episode_reward, time.time() - t0))\n                rewards.append(episode_reward)\n            if plot_func:\n                plot_func(rewards)\n\n        else:\n            print(\'unknow mode type\')\n'"
rlzoo/algorithms/td3/__init__.py,0,b''
rlzoo/algorithms/td3/default.py,60,"b'from rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nreplay_buffer_capacity: the size of buffer for storing explored samples\npolicy_target_update_interval: delayed interval for updating the target policy\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ntrain_episodes:  total number of episodes for training\ntest_episodes:  total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nbatch_size:  udpate batchsize\nexplore_steps:  for random action sampling in the beginning of training\nupdate_itr: repeated updates for single step\nreward_scale: value range of reward\nsave_interval: timesteps for saving the weights and plotting the results\nexplore_noise_scale: range of action noise for exploration\neval_noise_scale: range of action noise for evaluation of action value\nmode: \'train\' or \'test\'\nrender: if true, visualize the environment\n-----------------------------------------------\n""""""\n\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n        policy_target_update_interval=5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TD3\'):\n            with tf.name_scope(\'Q_Net1\'):\n                q_net1 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                q_net2 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               hidden_dim_list=num_hidden_layer * [hidden_dim])\n        net_list = [q_net1, q_net2, target_q_net1, target_q_net2, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        q_lr, policy_lr = 3e-4, 3e-4  # q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network\n        q_optimizer1 = tf.optimizers.Adam(q_lr)\n        q_optimizer2 = tf.optimizers.Adam(q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        optimizers_list = [q_optimizer1, q_optimizer2, policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=500,\n        update_itr=3,\n        reward_scale=1.,\n        explore_noise_scale=1.0,\n        eval_noise_scale=0.5,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n        policy_target_update_interval=5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TD3\'):\n            with tf.name_scope(\'Q_Net1\'):\n                q_net1 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                q_net2 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               hidden_dim_list=num_hidden_layer * [hidden_dim])\n        net_list = [q_net1, q_net2, target_q_net1, target_q_net2, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        q_lr, policy_lr = 3e-4, 3e-4  # q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network\n        q_optimizer1 = tf.optimizers.Adam(q_lr)\n        q_optimizer2 = tf.optimizers.Adam(q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        optimizers_list = [q_optimizer1, q_optimizer2, policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=500,\n        update_itr=3,\n        reward_scale=1.,\n        explore_noise_scale=1.0,\n        eval_noise_scale=0.5,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n        policy_target_update_interval=5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TD3\'):\n            with tf.name_scope(\'Q_Net1\'):\n                q_net1 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                q_net2 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               hidden_dim_list=num_hidden_layer * [hidden_dim])\n        net_list = [q_net1, q_net2, target_q_net1, target_q_net2, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        q_lr, policy_lr = 3e-4, 3e-4  # q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network\n        q_optimizer1 = tf.optimizers.Adam(q_lr)\n        q_optimizer2 = tf.optimizers.Adam(q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        optimizers_list = [q_optimizer1, q_optimizer2, policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=500,\n        update_itr=3,\n        reward_scale=1.,\n        explore_noise_scale=1.0,\n        eval_noise_scale=0.5,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n        policy_target_update_interval=5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TD3\'):\n            with tf.name_scope(\'Q_Net1\'):\n                q_net1 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                q_net2 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               hidden_dim_list=num_hidden_layer * [hidden_dim])\n        net_list = [q_net1, q_net2, target_q_net1, target_q_net2, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        q_lr, policy_lr = 3e-4, 3e-4  # q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network\n        q_optimizer1 = tf.optimizers.Adam(q_lr)\n        q_optimizer2 = tf.optimizers.Adam(q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        optimizers_list = [q_optimizer1, q_optimizer2, policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=500,\n        update_itr=3,\n        reward_scale=1.,\n        explore_noise_scale=1.0,\n        eval_noise_scale=0.5,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n        policy_target_update_interval=5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TD3\'):\n            with tf.name_scope(\'Q_Net1\'):\n                q_net1 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                q_net2 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               hidden_dim_list=num_hidden_layer * [hidden_dim])\n        net_list = [q_net1, q_net2, target_q_net1, target_q_net2, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        q_lr, policy_lr = 3e-4, 3e-4  # q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network\n        q_optimizer1 = tf.optimizers.Adam(q_lr)\n        q_optimizer2 = tf.optimizers.Adam(q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        optimizers_list = [q_optimizer1, q_optimizer2, policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=500,\n        update_itr=3,\n        reward_scale=1.,\n        explore_noise_scale=1.0,\n        eval_noise_scale=0.5,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n\n\ndef rlbench(env, default_seed=True):\n    if default_seed:\n        seed = 2\n        set_seed(seed, env)  # reproducible\n\n    alg_params = dict(\n        replay_buffer_capacity=5e5,\n        policy_target_update_interval=5,\n    )\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TD3\'):\n            with tf.name_scope(\'Q_Net1\'):\n                q_net1 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Q_Net2\'):\n                q_net2 = QNetwork(env.observation_space, env.action_space,\n                                  hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net1\'):\n                target_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Q_Net2\'):\n                target_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                         hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Policy\'):\n                policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                        hidden_dim_list=num_hidden_layer * [hidden_dim])\n            with tf.name_scope(\'Target_Policy\'):\n                target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                               hidden_dim_list=num_hidden_layer * [hidden_dim])\n        net_list = [q_net1, q_net2, target_q_net1, target_q_net2, policy_net, target_policy_net]\n        alg_params[\'net_list\'] = net_list\n    if alg_params.get(\'optimizers_list\') is None:\n        q_lr, policy_lr = 3e-4, 3e-4  # q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network\n        q_optimizer1 = tf.optimizers.Adam(q_lr)\n        q_optimizer2 = tf.optimizers.Adam(q_lr)\n        policy_optimizer = tf.optimizers.Adam(policy_lr)\n        optimizers_list = [q_optimizer1, q_optimizer2, policy_optimizer]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(\n        max_steps=150,\n        batch_size=64,\n        explore_steps=500,\n        update_itr=3,\n        reward_scale=1.,\n        explore_noise_scale=1.0,\n        eval_noise_scale=0.5,\n        train_episodes=100,\n        test_episodes=10,\n        save_interval=10,\n    )\n\n    return alg_params, learn_params\n'"
rlzoo/algorithms/td3/run_td3.py,11,"b'from rlzoo.algorithms.td3.td3 import TD3\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nimport gym\n\n"""""" load environment """"""\nenv = gym.make(\'Pendulum-v0\').unwrapped\n# env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized/wrapped environment to run\naction_shape = env.action_space.shape\nstate_shape = env.observation_space.shape\n# reproducible\nseed = 2\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nenv.seed(seed)\n\n"""""" build networks for the algorithm """"""\nnum_hidden_layer = 2  # number of hidden layers for the networks\nhidden_dim = 64  # dimension of hidden layers for the networks\nwith tf.name_scope(\'TD3\'):\n    with tf.name_scope(\'Q_Net1\'):\n        q_net1 = QNetwork(env.observation_space, env.action_space,\n                          hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Q_Net2\'):\n        q_net2 = QNetwork(env.observation_space, env.action_space,\n                          hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Target_Q_Net1\'):\n        target_q_net1 = QNetwork(env.observation_space, env.action_space,\n                                 hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Target_Q_Net2\'):\n        target_q_net2 = QNetwork(env.observation_space, env.action_space,\n                                 hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Policy\'):\n        policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                hidden_dim_list=num_hidden_layer * [hidden_dim])\n    with tf.name_scope(\'Target_Policy\'):\n        target_policy_net = DeterministicPolicyNetwork(env.observation_space, env.action_space,\n                                                       hidden_dim_list=num_hidden_layer * [hidden_dim])\nnet_list = [q_net1, q_net2, target_q_net1, target_q_net2, policy_net, target_policy_net]\n\n"""""" choose optimizers """"""\nq_lr, policy_lr = 3e-4, 3e-4  # q_lr: learning rate of the Q network; policy_lr: learning rate of the policy network\nq_optimizer1 = tf.optimizers.Adam(q_lr)\nq_optimizer2 = tf.optimizers.Adam(q_lr)\npolicy_optimizer = tf.optimizers.Adam(policy_lr)\noptimizers_list = [q_optimizer1, q_optimizer2, policy_optimizer]\n\nmodel = TD3(net_list, optimizers_list)\n"""""" \nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\nstate_dim: dimension of state for the environment\naction_dim: dimension of action for the environment\nreplay_buffer_capacity: the size of buffer for storing explored samples\npolicy_target_update_interval: delayed interval for updating the target policy\naction_range: value of each action in [-action_range, action_range]\n""""""\n\nmodel.learn(env, train_episodes=100, max_steps=150, batch_size=64, explore_steps=500, update_itr=3,\n            reward_scale=1., save_interval=10, explore_noise_scale=1.0, eval_noise_scale=0.5, mode=\'train\',\n            render=False)\n"""""" \nfull list of parameters for training\n---------------------------------------\nenv: learning environment\ntrain_episodes:  total number of episodes for training\ntest_episodes:  total number of episodes for testing\nmax_steps:  maximum number of steps for one episode\nbatch_size:  udpate batchsize\nexplore_steps:  for random action sampling in the beginning of training\nupdate_itr: repeated updates for single step\nreward_scale: value range of reward\nsave_interval: timesteps for saving the weights and plotting the results\nexplore_noise_scale: range of action noise for exploration\neval_noise_scale: range of action noise for evaluation of action value\nmode: \'train\' or \'test\'\nrender: if true, visualize the environment\n\n""""""\n# test\nmodel.learn(env, test_episodes=10, max_steps=150, mode=\'test\', render=True)\n'"
rlzoo/algorithms/td3/td3.py,9,"b'""""""\nTwin Delayed DDPG (TD3)\n------------------------\nDDPG suffers from problems like overestimate of Q-values and sensitivity to hyper-parameters.\nTwin Delayed DDPG (TD3) is a variant of DDPG with several tricks:\n* Trick One: Clipped Double-Q Learning. TD3 learns two Q-functions instead of one (hence \xe2\x80\x9ctwin\xe2\x80\x9d), \nand uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.\n\n* Trick Two: \xe2\x80\x9cDelayed\xe2\x80\x9d Policy Updates. TD3 updates the policy (and target networks) less frequently \nthan the Q-function. \n\n* Trick Three: Target Policy Smoothing. TD3 adds noise to the target action, to make it harder for \nthe policy to exploit Q-function errors by smoothing out Q along changes in action.\n\nThe implementation of TD3 includes 6 networks: 2 Q-net, 2 target Q-net, 1 policy net, 1 target policy net\nActor policy in TD3 is deterministic, with Gaussian exploration noise.\n\nReference\n---------\noriginal paper: https://arxiv.org/pdf/1802.09477.pdf\n\n\nEnvironment\n---\nOpenai Gym Pendulum-v0, continuous action space\nhttps://gym.openai.com/envs/Pendulum-v0/\n\nPrerequisites\n---\ntensorflow >=2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer >=2.0.0\n\n&&\npip install box2d box2d-kengz --user\n\n\n""""""\nimport time\n\nimport tensorflow_probability as tfp\nimport tensorlayer as tl\nfrom rlzoo.common.utils import *\nfrom rlzoo.common.buffer import *\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\n\ntfd = tfp.distributions\nNormal = tfd.Normal\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n\n###############################  TD3  ####################################\n\n\nclass TD3():\n    """""" twin-delayed ddpg """"""\n\n    def __init__(self, net_list, optimizers_list, replay_buffer_capacity=5e5, policy_target_update_interval=5):\n        self.name = \'TD3\'\n        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n\n        # get all networks\n        [self.q_net1, self.q_net2, self.target_q_net1, self.target_q_net2, self.policy_net,\n         self.target_policy_net] = net_list\n\n        assert isinstance(self.q_net1, QNetwork)\n        assert isinstance(self.q_net2, QNetwork)\n        assert isinstance(self.target_q_net1, QNetwork)\n        assert isinstance(self.target_q_net2, QNetwork)\n        assert isinstance(self.policy_net, DeterministicPolicyNetwork)\n        assert isinstance(self.target_policy_net, DeterministicPolicyNetwork)\n        assert isinstance(self.policy_net.action_space, gym.spaces.Box)\n\n        print(\'Q Network (1,2): \', self.q_net1)\n        print(\'Policy Network: \', self.policy_net)\n\n        # initialize weights of target networks\n        self.target_q_net1 = self.target_ini(self.q_net1, self.target_q_net1)\n        self.target_q_net2 = self.target_ini(self.q_net2, self.target_q_net2)\n        self.target_policy_net = self.target_ini(self.policy_net, self.target_policy_net)\n\n        self.update_cnt = 0\n        self.policy_target_update_interval = policy_target_update_interval\n\n        [self.q_optimizer1, self.q_optimizer2, self.policy_optimizer] = optimizers_list\n\n    def evaluate(self, state, eval_noise_scale, target=False):\n        """"""\n        generate action with state for calculating gradients;\n\n        :param eval_noise_scale: as the trick of target policy smoothing, for generating noisy actions.\n        """"""\n        if target:\n            action = self.target_policy_net(state)\n        else:\n            action = self.policy_net(state)\n        # add noise\n        normal = Normal(0, 1)\n        eval_noise_clip = 2 * eval_noise_scale\n        noise = normal.sample(action.shape) * eval_noise_scale\n        noise = tf.clip_by_value(noise, -eval_noise_clip, eval_noise_clip)\n        action = action + noise\n\n        return action\n\n    def get_action(self, state, explore_noise_scale):\n        """""" generate action with state for interaction with envronment """"""\n        action = self.policy_net(np.array([state]))\n        action = action.numpy()[0]\n\n        # add noise\n        normal = Normal(0, 1)\n        noise = normal.sample(action.shape) * explore_noise_scale\n        action = action + noise\n\n        return action.numpy()\n\n    def get_action_greedy(self, state):\n        """""" generate action with state for interaction with envronment """"""\n        return self.policy_net(np.array([state])).numpy()[0]\n\n    def sample_action(self):\n        """""" generate random actions for exploration """"""\n        return self.policy_net.random_sample()\n\n    def target_ini(self, net, target_net):\n        """""" hard-copy update for initializing target networks """"""\n        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n            target_param.assign(param)\n        return target_net\n\n    def target_soft_update(self, net, target_net, soft_tau):\n        """""" soft update the target net with Polyak averaging """"""\n        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n            target_param.assign(  # copy weight value into target parameters\n                target_param * (1.0 - soft_tau) + param * soft_tau\n            )\n        return target_net\n\n    def update(self, batch_size, eval_noise_scale, reward_scale=1., gamma=0.9, soft_tau=1e-2):\n        """""" update all networks in TD3 """"""\n        self.update_cnt += 1\n        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n\n        reward = reward[:, np.newaxis]  # expand dim\n        done = done[:, np.newaxis]\n\n        new_next_action = self.evaluate(\n            next_state, eval_noise_scale=eval_noise_scale, target=True\n        )  # clipped normal noise\n        reward = reward_scale * (reward -\n                                 np.mean(reward, axis=0)) / (np.std(reward,\n                                                                    axis=0) + 1e-6)  # normalize with batch mean and std; plus a small number to prevent numerical problem\n\n        # Training Q Function\n        target_q_min = tf.minimum(self.target_q_net1([next_state, new_next_action]),\n                                  self.target_q_net2([next_state, new_next_action]))\n\n        target_q_value = reward + (1 - done) * gamma * target_q_min  # if done==1, only reward\n\n        with tf.GradientTape() as q1_tape:\n            predicted_q_value1 = self.q_net1([state, action])\n            q_value_loss1 = tf.reduce_mean(tf.square(predicted_q_value1 - target_q_value))\n        q1_grad = q1_tape.gradient(q_value_loss1, self.q_net1.trainable_weights)\n        self.q_optimizer1.apply_gradients(zip(q1_grad, self.q_net1.trainable_weights))\n\n        with tf.GradientTape() as q2_tape:\n            predicted_q_value2 = self.q_net2([state, action])\n            q_value_loss2 = tf.reduce_mean(tf.square(predicted_q_value2 - target_q_value))\n        q2_grad = q2_tape.gradient(q_value_loss2, self.q_net2.trainable_weights)\n        self.q_optimizer2.apply_gradients(zip(q2_grad, self.q_net2.trainable_weights))\n\n        # Training Policy Function\n        if self.update_cnt % self.policy_target_update_interval == 0:\n            with tf.GradientTape() as p_tape:\n                new_action = self.evaluate(\n                    state, eval_noise_scale=0.0, target=False\n                )  # no noise, deterministic policy gradients\n                # """""" implementation 1 """"""\n                # predicted_new_q_value = tf.minimum(self.q_net1([state, new_action]),self.q_net2([state, new_action]))\n                """""" implementation 2 """"""\n                predicted_new_q_value = self.q_net1([state, new_action])\n                policy_loss = -tf.reduce_mean(predicted_new_q_value)\n            p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)\n            self.policy_optimizer.apply_gradients(zip(p_grad, self.policy_net.trainable_weights))\n\n            # Soft update the target nets\n            self.target_q_net1 = self.target_soft_update(self.q_net1, self.target_q_net1, soft_tau)\n            self.target_q_net2 = self.target_soft_update(self.q_net2, self.target_q_net2, soft_tau)\n            self.target_policy_net = self.target_soft_update(self.policy_net, self.target_policy_net, soft_tau)\n\n    def save_ckpt(self, env_name):  # save trained weights\n        save_model(self.q_net1, \'model_q_net1\', self.name, env_name)\n        save_model(self.q_net2, \'model_q_net2\', self.name, env_name)\n        save_model(self.target_q_net1, \'model_target_q_net1\', self.name, env_name)\n        save_model(self.target_q_net2, \'model_target_q_net2\', self.name, env_name)\n        save_model(self.policy_net, \'model_policy_net\', self.name, env_name)\n        save_model(self.target_policy_net, \'model_target_policy_net\', self.name, env_name)\n\n    def load_ckpt(self, env_name):  # load trained weights\n        load_model(self.q_net1, \'model_q_net1\', self.name, env_name)\n        load_model(self.q_net2, \'model_q_net2\', self.name, env_name)\n        load_model(self.target_q_net1, \'model_target_q_net1\', self.name, env_name)\n        load_model(self.target_q_net2, \'model_target_q_net2\', self.name, env_name)\n        load_model(self.policy_net, \'model_policy_net\', self.name, env_name)\n        load_model(self.target_policy_net, \'model_target_policy_net\', self.name, env_name)\n\n    def learn(self, env, train_episodes=1000, test_episodes=1000, max_steps=150, batch_size=64, explore_steps=500,\n              update_itr=3,\n              reward_scale=1., save_interval=10, explore_noise_scale=1.0, eval_noise_scale=0.5, mode=\'train\',\n              render=False, plot_func=None):\n        """"""\n        :param env: learning environment\n        :param train_episodes:  total number of episodes for training\n        :param test_episodes:  total number of episodes for testing\n        :param max_steps:  maximum number of steps for one episode\n        :param batch_size:  udpate batchsize\n        :param explore_steps:  for random action sampling in the beginning of training\n        :param update_itr: repeated updates for single step\n        :param reward_scale: value range of reward\n        :param save_interval: timesteps for saving the weights and plotting the results\n        :param explore_noise_scale: range of action noise for exploration\n        :param eval_noise_scale: range of action noise for evaluation of action value\n        :param mode: \'train\' or \'test\'\n        :param render: if true, visualize the environment\n        :param plot_func: additional function for interactive module\n        """"""\n\n        # training loop\n        if mode == \'train\':\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            frame_idx = 0\n            rewards = []\n            t0 = time.time()\n            for eps in range(train_episodes):\n                state = env.reset()\n                episode_reward = 0\n\n                for step in range(max_steps):\n                    if frame_idx > explore_steps:\n                        action = self.get_action(state, explore_noise_scale=explore_noise_scale)\n                    else:\n                        action = self.sample_action()\n\n                    next_state, reward, done, _ = env.step(action)\n                    if render: env.render()\n                    done = 1 if done == True else 0\n\n                    self.replay_buffer.push(state, action, reward, next_state, done)\n\n                    state = next_state\n                    episode_reward += reward\n                    frame_idx += 1\n\n                    if len(self.replay_buffer) > batch_size:\n                        for i in range(update_itr):\n                            self.update(batch_size, eval_noise_scale=eval_noise_scale, reward_scale=reward_scale)\n\n                    if done:\n                        break\n\n                if eps % int(save_interval) == 0:\n                    plot_save_log(rewards, algorithm_name=self.name, env_name=env.spec.id)\n                    self.save_ckpt(env_name=env.spec.id)\n\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\' \\\n                      .format(eps, train_episodes, episode_reward, time.time() - t0))\n                rewards.append(episode_reward)\n                if plot_func is not None:\n                    plot_func(rewards)\n            plot_save_log(rewards, algorithm_name=self.name, env_name=env.spec.id)\n            self.save_ckpt(env_name=env.spec.id)\n\n        elif mode == \'test\':\n            frame_idx = 0\n            rewards = []\n            t0 = time.time()\n\n            self.load_ckpt(env_name=env.spec.id)\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\n            # set test mode\n            self.q_net1.eval()\n            self.q_net2.eval()\n            self.target_q_net1.eval()\n            self.target_q_net2.eval()\n            self.policy_net.eval()\n            self.target_policy_net.eval()\n\n            for eps in range(test_episodes):\n                state = env.reset()\n                episode_reward = 0\n\n                for step in range(max_steps):\n                    action = self.get_action_greedy(state)\n                    next_state, reward, done, _ = env.step(action)\n                    if render: env.render()\n                    done = 1 if done == True else 0\n\n                    state = next_state\n                    episode_reward += reward\n                    frame_idx += 1\n\n                    if done:\n                        break\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\' \\\n                      .format(eps, test_episodes, episode_reward, time.time() - t0))\n                rewards.append(episode_reward)\n                if plot_func is not None:\n                    plot_func(rewards)\n\n        else:\n            print(\'unknow mode type, activate test mode as default\')\n'"
rlzoo/algorithms/trpo/__init__.py,0,b''
rlzoo/algorithms/trpo/default.py,35,"b'from rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nfrom rlzoo.common.utils import set_seed\n\n"""""" \nfull list of algorithm parameters (alg_params)\n-----------------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\ndamping_coeff: Artifact for numerical stability\ncg_iters: Number of iterations of conjugate gradient to perform\ndelta: KL-divergence limit for TRPO update.\n-----------------------------------------------\n\nfull list of learning parameters (learn_params)\n-----------------------------------------------\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: time steps for saving\ngamma: reward discount factor\nmode: train or test\nrender: render each step\nbatch_size: update batch size\nbacktrack_iters: Maximum number of steps allowed in the backtracking line search\nbacktrack_coeff: How far back to step during backtracking line search\ntrain_critic_iters: critic update iteration steps\n-----------------------------------------------\n""""""\n\n\ndef atari(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        damping_coeff=0.1,\n        cg_iters=10,\n        delta=0.01\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TRPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer, output_activation=tf.nn.tanh)\n\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        critic_lr = 1e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=2000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=100,\n                        gamma=0.9,\n                        batch_size=256,\n                        backtrack_iters=10,\n                        backtrack_coeff=0.8,\n                        train_critic_iters=80)\n\n    return alg_params, learn_params\n\n\ndef classic_control(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        damping_coeff=0.1,\n        cg_iters=10,\n        delta=0.01\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TRPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer, output_activation=tf.nn.tanh)\n\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        critic_lr = 1e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=2000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=100,\n                        gamma=0.9,\n                        batch_size=256,\n                        backtrack_iters=10,\n                        backtrack_coeff=0.8,\n                        train_critic_iters=80)\n\n    return alg_params, learn_params\n\n\ndef box2d(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        damping_coeff=0.1,\n        cg_iters=10,\n        delta=0.01\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TRPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer, output_activation=tf.nn.tanh)\n\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        critic_lr = 1e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=2000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=100,\n                        gamma=0.9,\n                        batch_size=256,\n                        backtrack_iters=10,\n                        backtrack_coeff=0.8,\n                        train_critic_iters=80)\n\n    return alg_params, learn_params\n\n\ndef mujoco(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        damping_coeff=0.1,\n        cg_iters=10,\n        delta=0.01\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TRPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer, output_activation=tf.nn.tanh)\n\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        critic_lr = 1e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=2000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=100,\n                        gamma=0.9,\n                        batch_size=256,\n                        backtrack_iters=10,\n                        backtrack_coeff=0.8,\n                        train_critic_iters=80)\n\n    return alg_params, learn_params\n\n\ndef robotics(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        damping_coeff=0.1,\n        cg_iters=10,\n        delta=0.01\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TRPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer, output_activation=tf.nn.tanh)\n\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        critic_lr = 1e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=2000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=100,\n                        gamma=0.9,\n                        batch_size=256,\n                        backtrack_iters=10,\n                        backtrack_coeff=0.8,\n                        train_critic_iters=80)\n\n    return alg_params, learn_params\n\n\ndef dm_control(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        damping_coeff=0.1,\n        cg_iters=10,\n        delta=0.01\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TRPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer, output_activation=tf.nn.tanh)\n\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        critic_lr = 1e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=2000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=100,\n                        gamma=0.9,\n                        batch_size=256,\n                        backtrack_iters=10,\n                        backtrack_coeff=0.8,\n                        train_critic_iters=80)\n\n    return alg_params, learn_params\n\n\ndef rlbench(env, default_seed=True):\n    if default_seed:\n        # reproducible\n        seed = 2\n        set_seed(seed, env)\n\n    alg_params = dict(\n        damping_coeff=0.1,\n        cg_iters=10,\n        delta=0.01\n    )\n\n    if alg_params.get(\'net_list\') is None:\n        num_hidden_layer = 2  # number of hidden layers for the networks\n        hidden_dim = 64  # dimension of hidden layers for the networks\n        with tf.name_scope(\'TRPO\'):\n            with tf.name_scope(\'V_Net\'):\n                v_net = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer)\n            with tf.name_scope(\'Policy\'):\n                policy_net = StochasticPolicyNetwork(env.observation_space, env.action_space,\n                                                     [hidden_dim] * num_hidden_layer, output_activation=tf.nn.tanh)\n\n        net_list = [v_net, policy_net]\n        alg_params[\'net_list\'] = net_list\n\n    if alg_params.get(\'optimizers_list\') is None:\n        critic_lr = 1e-3\n        optimizers_list = [tf.optimizers.Adam(critic_lr)]\n        alg_params[\'optimizers_list\'] = optimizers_list\n\n    learn_params = dict(train_episodes=2000,\n                        test_episodes=100,\n                        max_steps=200,\n                        save_interval=100,\n                        gamma=0.9,\n                        batch_size=256,\n                        backtrack_iters=10,\n                        backtrack_coeff=0.8,\n                        train_critic_iters=80)\n\n    return alg_params, learn_params\n'"
rlzoo/algorithms/trpo/run_trpo.py,2,"b'from rlzoo.common.utils import set_seed\nfrom rlzoo.algorithms.trpo.trpo import TRPO\nfrom rlzoo.common.policy_networks import *\nfrom rlzoo.common.value_networks import *\nimport gym\n\n"""""" load environment """"""\nenv = gym.make(\'Pendulum-v0\').unwrapped\n\n# reproducible\nseed = 2\nset_seed(seed, env)\n\n"""""" build networks for the algorithm """"""\nname = \'TRPO\'\nhidden_dim = 64\nnum_hidden_layer = 2\ncritic = ValueNetwork(env.observation_space, [hidden_dim] * num_hidden_layer, name=name + \'_value\')\n\nactor = StochasticPolicyNetwork(env.observation_space, env.action_space, [hidden_dim] * num_hidden_layer,\n                                output_activation=tf.nn.tanh, name=name + \'_policy\')\nnet_list = critic, actor\n\ncritic_lr = 1e-3\noptimizers_list = [tf.optimizers.Adam(critic_lr)]\n\n"""""" create model """"""\nmodel = TRPO(net_list, optimizers_list, damping_coeff=0.1, cg_iters=10, delta=0.01)\n""""""\nfull list of arguments for the algorithm\n----------------------------------------\nnet_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\noptimizers_list: a list of optimizers for all networks and differentiable variables\ndamping_coeff: Artifact for numerical stability\ncg_iters: Number of iterations of conjugate gradient to perform\ndelta: KL-divergence limit for TRPO update.\n""""""\n\nmodel.learn(env, mode=\'train\', render=False, train_episodes=2000, max_steps=200, save_interval=100,\n            gamma=0.9, batch_size=256, backtrack_iters=10, backtrack_coeff=0.8, train_critic_iters=80)\n""""""\nfull list of parameters for training\n---------------------------------------\nenv: learning environment\ntrain_episodes: total number of episodes for training\ntest_episodes: total number of episodes for testing\nmax_steps: maximum number of steps for one episode\nsave_interval: time steps for saving\ngamma: reward discount factor\nmode: train or test\nrender: render each step\nbatch_size: update batch size\nbacktrack_iters: Maximum number of steps allowed in the backtracking line search\nbacktrack_coeff: How far back to step during backtracking line search\ntrain_critic_iters: critic update iteration steps\n""""""\n\nmodel.learn(env, test_episodes=100, max_steps=200, mode=\'test\', render=True)\n'"
rlzoo/algorithms/trpo/trpo.py,18,"b'""""""\r\nTrust Region Policy Optimization (TRPO)\r\n---------------------------------------\r\nPG method with a large step can collapse the policy performance,\r\neven with a small step can lead a large differences in policy.\r\nTRPO constraint the step in policy space using KL divergence (rather than in parameter space),\r\nwhich can monotonically improve performance and avoid a collapsed update.\r\n\r\nReference\r\n---------\r\nTrust Region Policy Optimization, Schulman et al. 2015\r\nHigh Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016\r\nApproximately Optimal Approximate Reinforcement Learning, Kakade and Langford 2002\r\nopenai/spinningup : http://spinningup.openai.com/en/latest/algorithms/trpo.html\r\n\r\nPrerequisites\r\n--------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-probability 0.6.0\r\ntensorlayer >=2.0.0\r\n\r\n""""""\r\nimport time\r\n\r\nfrom rlzoo.common.utils import *\r\nfrom rlzoo.common.policy_networks import *\r\nfrom rlzoo.common.value_networks import *\r\n\r\nEPS = 1e-8  # epsilon\r\n\r\n""""""\r\n\r\nTrust Region Policy Optimization \r\n\r\n(with support for Natural Policy Gradient)\r\n\r\n""""""\r\n\r\n\r\nclass TRPO:\r\n    """"""\r\n    trpo class\r\n    """"""\r\n\r\n    def __init__(self, net_list, optimizers_list, damping_coeff=0.1, cg_iters=10, delta=0.01):\r\n        """"""\r\n        :param net_list: a list of networks (value and policy) used in the algorithm, from common functions or customization\r\n        :param optimizers_list: a list of optimizers for all networks and differentiable variables\r\n        :param damping_coeff: Artifact for numerical stability\r\n        :param cg_iters: Number of iterations of conjugate gradient to perform\r\n        :param delta: KL-divergence limit for TRPO update.\r\n        """"""\r\n\r\n        assert len(net_list) == 2\r\n        assert len(optimizers_list) == 1\r\n\r\n        self.name = \'TRPO\'\r\n\r\n        self.critic, self.actor = net_list\r\n\r\n        assert isinstance(self.critic, ValueNetwork)\r\n        assert isinstance(self.actor, StochasticPolicyNetwork)\r\n\r\n        self.damping_coeff, self.cg_iters = damping_coeff, cg_iters\r\n        self.delta = delta\r\n\r\n        # Optimizer for value function\r\n        self.critic_opt, = optimizers_list\r\n        self.old_dist = make_dist(self.actor.action_space)\r\n\r\n    @staticmethod\r\n    def flat_concat(xs):\r\n        """"""\r\n        flat concat input\r\n\r\n        :param xs: a list of tensor\r\n\r\n        :return: flat tensor\r\n        """"""\r\n        return tf.concat([tf.reshape(x, (-1,)) for x in xs], axis=0)\r\n\r\n    @staticmethod\r\n    def assign_params_from_flat(x, params):\r\n        """"""\r\n        assign params from flat input\r\n\r\n        :param x:\r\n        :param params:\r\n\r\n        :return: group\r\n        """"""\r\n        flat_size = lambda p: int(np.prod(p.shape.as_list()))  # the \'int\' is important for scalars\r\n        splits = tf.split(x, [flat_size(p) for p in params])\r\n        new_params = [tf.reshape(p_new, p.shape) for p, p_new in zip(params, splits)]\r\n        return tf.group([p.assign(p_new) for p, p_new in zip(params, new_params)])\r\n\r\n    def get_action(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s])[0].numpy()\r\n\r\n    def get_action_greedy(self, s):\r\n        """"""\r\n        Choose action\r\n\r\n        :param s: state\r\n\r\n        :return: clipped act\r\n        """"""\r\n        return self.actor([s], greedy=True)[0].numpy()\r\n\r\n    def cal_adv(self, tfs, tfdc_r):\r\n        """"""\r\n        Calculate advantage\r\n\r\n        :param tfs: state\r\n        :param tfdc_r: cumulative reward\r\n\r\n        :return: advantage\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        advantage = tfdc_r - self.critic(tfs)\r\n        return advantage.numpy()\r\n\r\n    def get_v(self, s):\r\n        """"""\r\n        Compute value\r\n\r\n        :param s: state\r\n\r\n        :return: value\r\n        """"""\r\n        if s.ndim < 2: s = s[np.newaxis, :]\r\n        res = self.critic(s)[0, 0]\r\n        return res\r\n\r\n    def c_train(self, tfdc_r, s):\r\n        """"""\r\n        Update actor network\r\n\r\n        :param tfdc_r: cumulative reward\r\n        :param s: state\r\n\r\n        :return: None\r\n        """"""\r\n        tfdc_r = np.array(tfdc_r, dtype=np.float32)\r\n        with tf.GradientTape() as tape:\r\n            v = self.critic(s)\r\n            advantage = tfdc_r - v\r\n            closs = tf.reduce_mean(tf.square(advantage))\r\n        grad = tape.gradient(closs, self.critic.trainable_weights)\r\n        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))\r\n\r\n    def save_ckpt(self, env_name):\r\n        """"""\r\n        save trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        save_model(self.actor, \'actor\', self.name, env_name)\r\n        save_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    def load_ckpt(self, env_name):\r\n        """"""\r\n        load trained weights\r\n\r\n        :return: None\r\n        """"""\r\n        load_model(self.actor, \'actor\', self.name, env_name)\r\n        load_model(self.critic, \'critic\', self.name, env_name)\r\n\r\n    # TRPO losses\r\n    def pi_loss(self, inputs):\r\n        """"""\r\n        calculate pi loss\r\n\r\n        :param inputs: a list of x_ph, a_ph, adv_ph, ret_ph, logp_old_ph and other inputs\r\n\r\n        :return: pi loss\r\n        """"""\r\n        x_ph, a_ph, adv_ph, ret_ph, logp_old_ph, *info_values = inputs\r\n\r\n        pi, logp, logp_pi, info, info_phs, d_kl = self.actor.cal_outputs_1(x_ph, a_ph, *info_values)\r\n        ratio = tf.exp(logp - logp_old_ph)  # pi(a|s) / pi_old(a|s)\r\n        pi_loss = -tf.reduce_mean(ratio * adv_ph)\r\n        return pi_loss\r\n\r\n    # Symbols needed for CG solver\r\n    def gradient(self, inputs):\r\n        """"""\r\n        pi gradients\r\n\r\n        :param inputs: a list of x_ph, a_ph, adv_ph, ret_ph, logp_old_ph and other inputs\r\n\r\n        :return: gradient\r\n        """"""\r\n        pi_params = self.actor.trainable_weights\r\n        with tf.GradientTape() as tape:\r\n            loss = self.pi_loss(inputs)\r\n        grad = tape.gradient(loss, pi_params)\r\n        grad = self.flat_concat(grad)\r\n        return grad\r\n\r\n    # Symbols for getting and setting params\r\n    def get_pi_params(self):\r\n        """"""\r\n        get actor trainable parameters\r\n\r\n        :return: flat actor trainable parameters\r\n        """"""\r\n        pi_params = self.actor.trainable_weights\r\n        return self.flat_concat(pi_params)\r\n\r\n    def set_pi_params(self, v_ph):\r\n        """"""\r\n        set actor trainable parameters\r\n\r\n        :param v_ph: inputs\r\n\r\n        :return: None\r\n        """"""\r\n        pi_params = self.actor.trainable_weights\r\n        self.assign_params_from_flat(v_ph, pi_params)\r\n\r\n    def cg(self, Ax, b):\r\n        """"""\r\n        Conjugate gradient algorithm\r\n        (see https://en.wikipedia.org/wiki/Conjugate_gradient_method)\r\n        """"""\r\n        x = np.zeros_like(b)\r\n        r = copy.deepcopy(b)  # Note: should be \'b - Ax(x)\', but for x=0, Ax(x)=0. Change if doing warm start.\r\n        p = copy.deepcopy(r)\r\n        r_dot_old = np.dot(r, r)\r\n\r\n        for _ in range(self.cg_iters):\r\n            z = Ax(p)\r\n            alpha = r_dot_old / (np.dot(p, z) + EPS)\r\n            x += alpha * p\r\n            r -= alpha * z\r\n            r_dot_new = np.dot(r, r)\r\n            p = r + (r_dot_new / r_dot_old) * p\r\n            r_dot_old = r_dot_new\r\n        return x\r\n\r\n    def eval(self, bs, ba, badv, oldpi_prob):\r\n        _ = self.actor(bs)\r\n        pi_prob = tf.exp(self.actor.policy_dist.logp(ba))\r\n        ratio = pi_prob / (oldpi_prob + EPS)\r\n\r\n        surr = ratio * badv\r\n        aloss = -tf.reduce_mean(surr)\r\n        kl = self.old_dist.kl(self.actor.policy_dist.param)\r\n        # kl = tfp.distributions.kl_divergence(oldpi, pi)\r\n        kl = tf.reduce_mean(kl)\r\n        return aloss, kl\r\n\r\n    def a_train(self, s, a, adv, oldpi_prob, backtrack_iters, backtrack_coeff):\r\n        s = np.array(s)\r\n        a = np.array(a, np.float32)\r\n        adv = np.array(adv, np.float32)\r\n\r\n        with tf.GradientTape() as tape:\r\n            aloss, kl = self.eval(s, a, adv, oldpi_prob)\r\n        a_grad = tape.gradient(aloss, self.actor.trainable_weights)\r\n        # print(a_grad)\r\n        a_grad = self.flat_concat(a_grad)\r\n        pi_l_old = aloss\r\n        # print(a_grad)\r\n\r\n        Hx = lambda x: self.hessian_vector_product(s, a, adv, oldpi_prob, x)\r\n        x = self.cg(Hx, a_grad)\r\n        alpha = np.sqrt(2 * self.delta / (np.dot(x, Hx(x)) + EPS))\r\n\r\n        old_params = self.flat_concat(self.actor.trainable_weights)\r\n\r\n        for j in range(backtrack_iters):\r\n            self.set_pi_params(old_params - alpha * x * backtrack_coeff ** j)\r\n            kl, pi_l_new = self.eval(s, a, adv, oldpi_prob)\r\n            if kl <= self.delta and pi_l_new <= pi_l_old:\r\n                # Accepting new params at step j of line search.\r\n                break\r\n\r\n            if j == backtrack_iters - 1:\r\n                # Line search failed! Keeping old params.\r\n                self.set_pi_params(old_params)\r\n\r\n    def hessian_vector_product(self, s, a, adv, oldpi_prob, v_ph):\r\n        # for H = grad**2 f, compute Hx\r\n        params = self.actor.trainable_weights\r\n\r\n        with tf.GradientTape() as tape1:\r\n            with tf.GradientTape() as tape0:\r\n                aloss, kl = self.eval(s, a, adv, oldpi_prob)\r\n            g = tape0.gradient(kl, params)\r\n            g = self.flat_concat(g)\r\n            assert v_ph.shape == g.shape\r\n            v = tf.reduce_sum(g * v_ph)\r\n        grad = tape1.gradient(v, params)\r\n        hvp = self.flat_concat(grad)\r\n\r\n        if self.damping_coeff > 0:\r\n            hvp += self.damping_coeff * v_ph\r\n        return hvp\r\n\r\n    def update(self, bs, ba, br, train_critic_iters, backtrack_iters, backtrack_coeff):\r\n        """"""\r\n        update trpo\r\n\r\n        :return: None\r\n        """"""\r\n        adv = self.cal_adv(bs, br)\r\n        _ = self.actor(bs)\r\n        oldpi_prob = tf.exp(self.actor.policy_dist.logp(ba))\r\n        oldpi_prob = tf.stop_gradient(oldpi_prob)\r\n\r\n        oldpi_param = self.actor.policy_dist.get_param()\r\n        self.old_dist.set_param(oldpi_param)\r\n\r\n        self.a_train(bs, ba, adv, oldpi_prob, backtrack_iters, backtrack_coeff)\r\n\r\n        for _ in range(train_critic_iters):\r\n            self.c_train(br, bs)\r\n\r\n    def learn(self, env, train_episodes=200, test_episodes=100, max_steps=200, save_interval=10,\r\n              gamma=0.9, mode=\'train\', render=False, batch_size=32, backtrack_iters=10, backtrack_coeff=0.8,\r\n              train_critic_iters=80, plot_func=None):\r\n        """"""\r\n        learn function\r\n\r\n        :param env: learning environment\r\n        :param train_episodes: total number of episodes for training\r\n        :param test_episodes: total number of episodes for testing\r\n        :param max_steps: maximum number of steps for one episode\r\n        :param save_interval: time steps for saving\r\n        :param gamma: reward discount factor\r\n        :param mode: train or test\r\n        :param render: render each step\r\n        :param batch_size: update batch size\r\n        :param backtrack_iters: Maximum number of steps allowed in the backtracking line search\r\n        :param backtrack_coeff: How far back to step during backtracking line search\r\n        :param train_critic_iters: critic update iteration steps\r\n        \r\n        :return: None\r\n        """"""\r\n\r\n        t0 = time.time()\r\n\r\n        if mode == \'train\':\r\n            print(\'Training...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\r\n            reward_buffer = []\r\n            for ep in range(1, train_episodes + 1):\r\n                s = env.reset()\r\n                buffer_s, buffer_a, buffer_r = [], [], []\r\n                ep_rs_sum = 0\r\n                for t in range(max_steps):  # in one episode\r\n                    if render:\r\n                        env.render()\r\n                    a = self.get_action(s)\r\n\r\n                    s_, r, done, _ = env.step(a)\r\n                    buffer_s.append(s)\r\n                    buffer_a.append(a)\r\n                    buffer_r.append(r)\r\n                    s = s_\r\n                    ep_rs_sum += r\r\n\r\n                    # update ppo\r\n                    if (t + 1) % batch_size == 0 or t == max_steps - 1 or done:\r\n                        if done:\r\n                            v_s_ = 0\r\n                        else:\r\n                            try:\r\n                                v_s_ = self.get_v(s_)\r\n                            except:\r\n                                v_s_ = self.get_v(s_[np.newaxis, :])  # for raw-pixel input\r\n                        discounted_r = []\r\n                        for r in buffer_r[::-1]:\r\n                            v_s_ = r + gamma * v_s_\r\n                            discounted_r.append(v_s_)\r\n                        discounted_r.reverse()\r\n                        bs = buffer_s\r\n                        ba, br = buffer_a, np.array(discounted_r)[:, np.newaxis]\r\n                        buffer_s, buffer_a, buffer_r = [], [], []\r\n                        self.update(bs, ba, br, train_critic_iters, backtrack_iters, backtrack_coeff)\r\n                    if done:\r\n                        break\r\n\r\n                print(\r\n                    \'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                        ep, train_episodes, ep_rs_sum,\r\n                        time.time() - t0\r\n                    )\r\n                )\r\n\r\n                reward_buffer.append(ep_rs_sum)\r\n                if plot_func is not None:\r\n                    plot_func(reward_buffer)\r\n                if ep and not ep % save_interval:\r\n                    self.save_ckpt(env_name=env.spec.id)\r\n                    plot_save_log(reward_buffer, self.name, env.spec.id)\r\n\r\n            self.save_ckpt(env_name=env.spec.id)\r\n            plot_save_log(reward_buffer, self.name, env.spec.id)\r\n\r\n        # test\r\n        elif mode == \'test\':\r\n            self.load_ckpt(env_name=env.spec.id)\r\n            print(\'Testing...  | Algorithm: {}  | Environment: {}\'.format(self.name, env.spec.id))\r\n            reward_buffer = []\r\n            for eps in range(test_episodes):\r\n                ep_rs_sum = 0\r\n                s = env.reset()\r\n                for step in range(max_steps):\r\n                    if render:\r\n                        env.render()\r\n                    action = self.get_action_greedy(s)\r\n                    s, reward, done, info = env.step(action)\r\n                    ep_rs_sum += reward\r\n                    if done:\r\n                        break\r\n\r\n                print(\'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    eps, test_episodes, ep_rs_sum, time.time() - t0)\r\n                )\r\n            reward_buffer.append(ep_rs_sum)\r\n            if plot_func:\r\n                plot_func(reward_buffer)\r\n        else:\r\n            print(\'unknown mode type\')\r\n'"
