file_path,api_count,code
eval.py,0,"b'#!/usr/bin/env python3\r\n\r\nimport os\r\nimport cv2\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport argparse\r\nimport matplotlib\r\nimport multiprocessing\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorlayer as tl\r\nfrom Hyperpose import Config,Model,Dataset\r\n\r\nif __name__ == \'__main__\':\r\n    parser = argparse.ArgumentParser(description=\'FastPose.\')\r\n    parser.add_argument(""--model_type"",\r\n                        type=str,\r\n                        default=""Openpose"",\r\n                        help=""human pose estimation model type, available options: Openpose, LightweightOpenpose ,PoseProposal"")\r\n    parser.add_argument(""--model_backbone"",\r\n                        type=str,\r\n                        default=""Default"",\r\n                        help=""model backbone, available options: Mobilenet, Vgg19, Resnet18, Resnet50"")\r\n    parser.add_argument(""--model_name"",\r\n                        type=str,\r\n                        default=""default_name"",\r\n                        help=""model name,to distinguish model and determine model dir"")\r\n    parser.add_argument(""--dataset_type"",\r\n                        type=str,\r\n                        default=""MSCOCO"",\r\n                        help=""dataset name,to determine which dataset to use, available options: coco "")\r\n    parser.add_argument(""--dataset_path"",\r\n                        type=str,\r\n                        default=""data"",\r\n                        help=""dataset path,to determine the path to load the dataset"")\r\n    parser.add_argument(\'--train_type\',\r\n                        type=str,\r\n                        default=""Single_train"",\r\n                        help=\'train type, available options: Single_train, Parallel_train\')\r\n    parser.add_argument(\'--kf_optimizer\',\r\n                        type=str,\r\n                        default=\'Pair_avg\',\r\n                        help=\'kung fu parallel optimizor,available options: Sync_sgd, Sync_avg, Pair_avg\')\r\n    parser.add_argument(\'--eval_num\',\r\n                        type=int,\r\n                        default=10000,\r\n                        help=\'number of evaluation\')\r\n                        \r\n\r\n    args=parser.parse_args()\r\n    Config.set_model_name(args.model_name)\r\n    Config.set_model_type(Config.MODEL[args.model_type])\r\n    Config.set_model_backbone(Config.BACKBONE[args.model_backbone])\r\n    Config.set_dataset_type(Config.DATA[args.dataset_type])\r\n    Config.set_dataset_path(args.dataset_path)\r\n    \r\n    config=Config.get_config()\r\n    model=Model.get_model(config)\r\n    evaluate=Model.get_evaluate(config)\r\n    dataset=Dataset.get_dataset(config)\r\n\r\n    evaluate(model,dataset,vis_num=30,total_eval_num=args.eval_num)\r\n'"
export_pb.py,3,"b'#!/usr/bin/env python3\n\nimport os\nimport argparse\nimport tensorflow as tf\nimport tensorlayer as tl\nfrom framework import Config,Model,Dataset\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'export fastpose models to pb format.\')\n    parser.add_argument(""--model_type"",\n                        type=str,\n                        default=""Openpose"",\n                        help=""human pose estimation model type, available options: Openpose, LightweightOpenpose ,PoseProposal"")\n    parser.add_argument(""--model_backbone"",\n                        type=str,\n                        default=""Default"",\n                        help=""model backbone, available options: Mobilenet, Vgg19, Resnet18, Resnet50"")\n    parser.add_argument(""--model_name"",\n                        type=str,\n                        default=""default_name"",\n                        help=""model name,to distinguish model and determine model dir"")\n    parser.add_argument(""--dataset_type"",\n                        type=str,\n                        default=""MSCOCO"",\n                        help=""dataset name,to determine which dataset to use, available options: coco "")\n    parser.add_argument(""--dataset_path"",\n                        type=str,\n                        default=""data"",\n                        help=""dataset path,to determine the path to load the dataset"")\n    parser.add_argument(\'--train_type\',\n                        type=str,\n                        default=""Single_train"",\n                        help=\'train type, available options: Single_train, Parallel_train\')\n    parser.add_argument(\'--kf_optimizer\',\n                        type=str,\n                        default=\'Sma\',\n                        help=\'kung fu parallel optimizor,available options: Sync_sgd, Async_sgd, Sma\')\n    parser.add_argument(""--output_dir"",\n                        type=str,\n                        default=""save_dir"",\n                        help=""which dir to output the exported pb model"")\n    \n    \n    args=parser.parse_args()    \n    Config.set_model_name(args.model_name)\n    Config.set_model_type(Config.MODEL[args.model_type])\n    Config.set_model_backbone(Config.BACKBONE[args.model_backbone])\n    config=Config.get_config()\n    export_model=Model.get_model(config)\n\n    input_path=f""{config.model.model_dir}/newest_model.npz""\n    output_dir=f""{args.output_dir}/{config.model.model_name}""\n    output_path=f""{output_dir}/frozen_{config.model.model_name}.pb""\n    print(f""exporting model {config.model.model_name} from {input_path}..."")\n    if(not os.path.exists(output_dir)):\n        print(""creating output_dir..."")\n        os.mkdir(output_dir)\n    if(not os.path.exists(input_path)):\n        print(""input model file doesn\'t exist!"")\n        print(""conversion aborted!"")\n    else:\n        export_model.load_weights(input_path)\n        export_model.eval()\n        if(export_model.data_format==""channels_last""):\n            input_signature=tf.TensorSpec(shape=(None,export_model.hin,export_model.win,3))\n        else:\n            input_signature=tf.TensorSpec(shape=(None,3,export_model.hin,export_model.win))\n        concrete_function=export_model.infer.get_concrete_function(x=input_signature)\n        frozen_graph=convert_variables_to_constants_v2(concrete_function)\n        frozen_graph_def=frozen_graph.graph.as_graph_def()\n        tf.io.write_graph(graph_or_graph_def=frozen_graph_def,logdir=output_dir,name=f""frozen_{args.model_name}.pb"",\\\n            as_text=False)\n        print(f""exporting pb file finished! output file: {output_path}"")\n\n'"
train.py,0,"b'#!/usr/bin/env python3\n\nimport os\nimport cv2\nimport sys\nimport math\nimport json\nimport time\nimport argparse\nimport matplotlib\nimport multiprocessing\nimport numpy as np\nimport tensorflow as tf\nimport tensorlayer as tl\nfrom Hyperpose import Config,Model,Dataset\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'FastPose.\')\n    parser.add_argument(""--model_type"",\n                        type=str,\n                        default=""Openpose"",\n                        help=""human pose estimation model type, available options: Openpose, LightweightOpenpose ,MobilenetThinOpenpose, PoseProposal"")\n    parser.add_argument(""--model_backbone"",\n                        type=str,\n                        default=""Default"",\n                        help=""model backbone, available options: Mobilenet, Vggtiny, Vgg19, Resnet18, Resnet50"")\n    parser.add_argument(""--model_name"",\n                        type=str,\n                        default=""default_name"",\n                        help=""model name,to distinguish model and determine model dir"")\n    parser.add_argument(""--dataset_type"",\n                        type=str,\n                        default=""MSCOCO"",\n                        help=""dataset name,to determine which dataset to use, available options: MSCOCO, MPII "")\n    parser.add_argument(""--dataset_path"",\n                        type=str,\n                        default=""data"",\n                        help=""dataset path,to determine the path to load the dataset"")\n    parser.add_argument(\'--train_type\',\n                        type=str,\n                        default=""Single_train"",\n                        help=\'train type, available options: Single_train, Parallel_train\')\n    parser.add_argument(\'--learning_rate\',\n                        type=float,\n                        default=1e-4,\n                        help=\'learning rate\')\n    parser.add_argument(\'--batch_size\',\n                        type=int,\n                        default=8,\n                        help=\'batch_size\')\n    parser.add_argument(\'--kf_optimizer\',\n                        type=str,\n                        default=\'Sync_avg\',\n                        help=\'kung fu parallel optimizor,available options: Sync_sgd, Sync_avg, Pair_avg\')\n                        \n    args=parser.parse_args()\n    #config model\n    Config.set_model_name(args.model_name)\n    Config.set_model_type(Config.MODEL[args.model_type])\n    Config.set_model_backbone(Config.BACKBONE[args.model_backbone])\n    #config train\n    Config.set_train_type(Config.TRAIN[args.train_type])\n    Config.set_learning_rate(args.learning_rate)\n    Config.set_batch_size(args.batch_size)\n    Config.set_kungfu_option(Config.KUNGFU[args.kf_optimizer])\n    #config dataset\n    Config.set_dataset_type(Config.DATA[args.dataset_type])\n    Config.set_dataset_path(args.dataset_path)\n    \n    #train\n    config=Config.get_config()\n    model=Model.get_model(config)\n    train=Model.get_train(config)\n    dataset=Dataset.get_dataset(config)\n    train(model,dataset)\n\n    #eval\n    config=Config.get_config()\n    model=Model.getModel(config)\n    evaluate=Model.get_evaluate_by_config(config)\n    dataset=Dataset.get_dataset(config)\n    evaluate(model,dataset,vis_num=30,total_eval_num=10000)\n\n    #user pipeline\n    Openpose=Model.get_model(Config.MODEL.Openpose)\n    model=Openpose(n_pos=12,hin=384,win=384)\n    Cocodataset=Dataset.generateTrainDataset()\n    train_dataset.map'"
Hyperpose/__init__.py,0,b''
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\ndir_path = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.abspath(\'../\'))\nsys.path.insert(0, os.path.abspath(\'../Hyperpose\'))\n\n# -- Project information -----------------------------------------------------\n\nproject = \'HyperPose\'\ncopyright = \'2020, Jiawei Liu, Yixiao Guo, Luo Mai, Guo Li, Hao Dong\'\nauthor = \'Jiawei Liu, Yixiao Guo, Luo Mai, Guo Li, Hao Dong\'\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx_markdown_tables\',\n    \'recommonmark\',\n    \'numpydoc\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\nsource_suffix = {\n    \'.rst\': \'restructuredtext\',\n    \'.md\': \'markdown\',\n}\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nmaster_doc = \'index\'\n\n# Do doxygen\nimport subprocess\n\nsubprocess.call(\'cd doxygen; doxygen Doxyfile\', shell=True)\nsubprocess.call(\'mkdir -p _build/html\', shell=True)\nsubprocess.call(\'cp -r doxygen/build/html _build//html/cpp\', shell=True)\n'"
Hyperpose/Config/__init__.py,5,"b'import os\nimport logging\nimport tensorflow as tf\nimport tensorlayer as tl\nimport matplotlib\nmatplotlib.use(""Agg"")\nfrom easydict import EasyDict as edict\nfrom .define import *\ntl.layers.BatchNorm2d\nupdate_config,update_train,update_eval,update_model,update_data,update_log=edict(),edict(),edict(),edict(),edict(),edict()\nupdate_model.model_type=MODEL.Openpose\n\n#get configure api\ndef get_config():\n    \'\'\'get the config object with all the configuration information\n\n    get the config object based on the previous setting functions, \n    the config object will be passed to the functions of Model and Dataset module to\n    construct the system.\n\n    only the setting functions called before this get_config function is valid, thus\n    use this function after all configuration done.\n    \n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    config object\n        an edict object contains all the configuration information.\n\n    \'\'\'\n    #import basic configurations\n    if(update_model.model_type==MODEL.Openpose):\n        from .config_opps import model,train,eval,data,log\n    elif(update_model.model_type==MODEL.LightweightOpenpose):\n        from .config_lopps import model,train,eval,data,log\n    elif(update_model.model_type==MODEL.MobilenetThinOpenpose):\n        from .config_mbtopps import model,train,eval,data,log\n    elif(update_model.model_type==MODEL.PoseProposal):\n        from .config_ppn import model,train,eval,data,log\n    #merge settings with basic configurations\n    model.update(update_model)\n    train.update(update_train)\n    eval.update(update_eval)\n    data.update(update_data)\n    log.update(update_log)\n    #assemble configure\n    config=edict()\n    config.model=model\n    config.train=train\n    config.eval=eval\n    config.data=data\n    config.log=log\n    #path configure\n    tl.files.exists_or_mkdir(config.model.model_dir, verbose=True)  # to save model files \n    tl.files.exists_or_mkdir(config.train.vis_dir, verbose=True)  # to save visualization results\n    tl.files.exists_or_mkdir(config.eval.vis_dir, verbose=True)  # to save visualization results\n    tl.files.exists_or_mkdir(config.data.vis_dir, verbose=True)  # to save visualization results\n    #device configure\n    #FIXME: replace experimental tf functions when in tf 2.1 version\n    tf.debugging.set_log_device_placement(False)\n    tf.config.set_soft_device_placement(True)\n    for gpu in tf.config.experimental.get_visible_devices(""GPU""):\n        tf.config.experimental.set_memory_growth(gpu,True)\n    #logging configure\n    tl.files.exists_or_mkdir(os.path.dirname(config.log.log_path),verbose=True)\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n    tl.logging.set_verbosity(tl.logging.INFO)\n    return config\n\n#set configure api\n#model configure api\ndef set_model_arch(model_arch):\n    \'\'\'set user defined model architecture\n\n    replace default model architecture with user-defined model architecture, use it in the following training and evaluation \n    \n    Parameters\n    ----------\n    arg1 : tensorlayer.models.MODEL\n        An object of a model class inherit from tensorlayer.models.MODEL class,\n        should implement forward function and cal_loss function to make it compatible with the existing pipeline\n\n        The forward funtion should follow the signature below:\n|           openpose models: def forward(self,x,is_train=False) ,return conf_map,paf_map,stage_confs,stage_pafs\n|           poseproposal models: def forward(self,x,is_train=False), return pc,pi,px,py,pw,ph,pe\n\n        The cal_loss function should follow the signature below:\n\n|           openpose models: def cal_loss(self,stage_confs,stage_pafs,gt_conf,gt_paf,mask), return loss,loss_confs,loss_pafs\n|           poseproposal models: def cal_loss(self,tc,tx,ty,tw,th,te,te_mask,pc,pi,px,py,pw,ph,pe):\n         return loss_rsp,loss_iou,loss_coor,loss_size,loss_limb\n    \n    Returns\n    -------\n    None\n\n    \'\'\'\n    update_model.model_arch=model_arch\n\ndef set_model_type(model_type):\n    \'\'\'set preset model architecture \n    \n    configure the model architecture as one of the desired preset model architectures\n\n    Parameters\n    ----------\n    arg1 : Config.MODEL\n        a enum value of enum class Config.MODEL, available options:\n|            Config.MODEL.Openpose (original Openpose)\n|            Config.MODEL.LightweightOpenpose (lightweight variant version of Openpose,real-time on cpu)\n|            Config.MODEL.PoseProposal (pose proposal network)\n|            Config.MODEL.MobilenetThinOpenpose (lightweight variant version of openpose)\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_model.model_type=model_type\n\n\ndef set_model_backbone(model_backbone):\n    \'\'\'set preset model backbones \n    \n    set current model backbone to other common backbones \n    different backbones have different computation complexity\n    this enable dynamicly adapt the model architecture to approriate size.\n\n    Parameters\n    ----------\n    arg1 : Config.BACKBONE\n        a enum value of enum class Config.BACKBONE\n        available options:\n|           Config.BACKBONE.DEFUALT (default backbone of the architecture)\n|           Config.BACKBONE.MobilenetV1\n|           Config.BACKBONE.MobilenetV2\n|           Config.BACKBONE.Vggtiny\n|           Config.BACKBONE.Vgg16\n|           Config.BACKBONE.Vgg19\n|           Config.BACKBONE.Resnet18\n|           Config.BACKBONE.Resnet50\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_model.model_backbone=model_backbone\n\ndef set_data_format(data_format):\n    \'\'\'set model dataformat\n\n    set the channel order of current model:\n\n|       ""channels_first"" dataformat is faster in deployment\n|       ""channels_last"" dataformat is more common\n    the integrated pipeline will automaticly adapt to the chosen data format\n\n    Parameters\n    ----------\n    arg1 : string\n        available input:\n|           \'channels_first\': data_shape N*C*H*W\n|           \'channels_last\': data_shape N*H*W*C  \n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_model.data_format=data_format\n\ndef set_model_name(model_name):\n    \'\'\'set the name of model\n\n    the models are distinguished by their names,so it is necessary to set model\'s name when train multiple models at the same time.\n    each model\'s ckpt data and log are saved on the \'save_dir/model_name\' directory, the following directory are determined:\n\n|       directory to save model                      ./save_dir/model_name/model_dir\n|       directory to save train result               ./save_dir/model_name/train_vis_dir\n|       directory to save evaluate result            ./save_dir/model_name/eval_vis_dir\n|       directory to save dataset visualize result   ./save_dir/model_name/data_vis_dir\n|       file path to save train log                  ./save_dir/model_name/log.txt\n\n    Parameters\n    ----------\n    arg1 : string\n        name of the model\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_model.model_name=model_name\n    update_model.model_dir = f""./save_dir/{update_model.model_name}/model_dir""\n    update_train.vis_dir = f""./save_dir/{update_model.model_name}/train_vis_dir""\n    update_eval.vis_dir=f""./save_dir/{update_model.model_name}/eval_vis_dir""\n    update_data.vis_dir=f""./save_dir/{update_model.model_name}/data_vis_dir""\n    update_log.log_path= f""./save_dir/{update_model.model_name}/log.txt""\n\n#train configure api\ndef set_train_type(train_type):\n    \'\'\'set single_train or parallel train\n\n    default using single train, which train the model on one GPU.\n    set parallel train will use Kungfu library to accelerate training on multiple GPU.\n\n    to use parallel train better, it is also allow to set parallel training optimizor by set_kungfu_option.\n\n    Parameters\n    ----------\n    arg1 : Config.TRAIN\n        a enum value of enum class Config.TRAIN,available options:\n|           Config.TRAIN.Single_train\n|           Config.TRAIN.Parallel_train\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_train.train_type=train_type\n\ndef set_learning_rate(learning_rate):\n    \'\'\'set the learning rate in training\n\n    Parameters\n    ----------\n    arg1 : float\n        learning rate\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_train.lr_init=learning_rate\n\ndef set_batch_size(batch_size):\n    \'\'\'set the batch size in training\n\n    Parameters\n    ----------\n    arg1 : int\n        batch_size\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_train.batch_size=batch_size\n\n\ndef set_kungfu_option(kungfu_option):\n    \'\'\'set the optimizor of parallel training\n\n    kungfu distribute training library needs to wrap tensorflow optimizor in\n    kungfu optimizor, this function is to choose kungfu optimizor wrap type\n\n    Parameters\n    ----------\n    arg1 : Config.KUNGFU\n        a enum value of enum class Config.KUNGFU\n        available options:\n|           Config.KUNGFU.Sync_sgd (SynchronousSGDOptimizer, hyper-parameter-robus)\n|           Config.KUNGFU.Sync_avg (SynchronousAveragingOptimizer)\n|           Config.KUNGFU.Pair_avg (PairAveragingOptimizer, communication-efficient)\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_train.kungfu_option=kungfu_option\n\n#data configure api\ndef set_dataset_type(dataset_type):\n    \'\'\'set the dataset for train and evaluate\n\n    set which dataset to use, the process of downlaoding, decoding, reformatting of different type\n    of dataset is automatic.\n    the evaluation metric of different dataset follows their official metric,\n    for COCO is MAP, for MPII is MPCH.\n    \n    This API also receive user-defined dataset class, which should implement the following functions\n|       __init__: take the config object with all configuration to init the dataset\n|       get_parts: return a enum class which defines the key point definition of the dataset\n|       get_limbs: return a [2*num_limbs] array which defines the limb definition of the dataset\n|       get_colors: return a list which defines the visualization color of the limbs\n|       get_train_dataset: return a tensorflow dataset which contains elements for training. each element should contains an image path and a target dict decoded in bytes by _pickle\n|       get_eval_dataset: return a tensorflow dataset which contains elements for evaluating. each element should contains an image path and an image id\n|       official_eval: if want to evaluate on this user-defined dataset, evalutation function should be implemented.\n    one can refer the Dataset.mpii_dataset and Dataset.mscoco_dataset for detailed information.\n\n    Parameters\n    ----------\n    arg1 : Config.DATA\n        a enum value of enum class Config.DATA or user-defined dataset\n        available options:\n|           Config.DATA.MSCOCO\n|           Config.DATA.MPII\n|           user-defined dataset\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_data.dataset_type=dataset_type\n\n\ndef set_dataset_path(dataset_path):\n    \'\'\'set the path of the dataset\n\n    set the path of the directory where dataset is,if the dataset doesn\'t exist in this directory, \n    then it will be automaticly download in this directory and decoded. \n\n    Parameters\n    ----------\n    arg1 : String\n        a string indicates the path of the dataset,\n        default: ./data \n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_data.dataset_path=dataset_path\n\ndef set_dataset_filter(dataset_filter):\n    \'\'\'set the user defined dataset filter\n\n    set the dataset filter as the input function.\n    to uniformly format different dataset, \n    Hyperpose organize the annotations of one image in one dataset in the similiar meta classes.\n    for COCO dataset, it is COCOMeta; for MPII dataset, it is MPIIMeta.\n    Meta classes will have some common information such as image_id, joint_list etc,\n    they also have some dataset-specific imformation, such as mask, is_crowd, headbbx_list etc.\n    \n    the dataset_fiter will perform on the Meta objects of the corresponding dataset, if \n    it returns True, the image and annotaions the Meta object related will be kept,\n    otherwise it will be filtered out. Please refer the Dataset.xxxMeta classes for better use.\n\n    Parameters\n    ----------\n    arg1 : function\n        a function receive a meta object as input, return a bool value indicates whether \n        the meta should be kept or filtered out. return Ture for keeping and False for depricating the object.\n        default: None\n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_data.dataset_filter=dataset_filter\n\n#log configure api\ndef set_log_freq(log_freq):\n    \'\'\'set the frequency of logging\n\n    set the how many iteration intervals between two log information\n\n    Parameters\n    ----------\n    arg1 : Int\n        a int value indicates the iteration number bwteen two logs\n        default: 1 \n    \n    Returns\n    -------\n    None\n    \'\'\'\n    update_log.log_freq=log_freq'"
Hyperpose/Config/config_lopps.py,0,"b'import os\r\nfrom .define import MODEL,DATA,TRAIN\r\nfrom easydict import EasyDict as edict\r\n\r\n#model configuration\r\nmodel = edict()\r\n# number of keypoints + 1 for background\r\nmodel.n_pos = 19  \r\nmodel.num_channels=128\r\n# input size during training , 240\r\nmodel.hin = 368  \r\nmodel.win = 432\r\n# output size during training (default 46)\r\nmodel.hout = 46 \r\nmodel.wout = 54\r\nmodel.model_type = MODEL.Openpose\r\nmodel.model_name = ""default_name""\r\nmodel.model_backbone=""Default""\r\nmodel.data_format = ""channels_first""\r\n# save directory\r\nmodel.model_dir = f""./save_dir/{model.model_name}/model_dir""\r\n\r\n#train configuration\r\ntrain=edict()\r\ntrain.batch_size = 8\r\ntrain.save_interval = 5000\r\n# total number of step\r\ntrain.n_step = 1000000\r\n# initial learning rate  \r\ntrain.lr_init = 4e-5  \r\n# evey number of step to decay lr\r\ntrain.lr_decay_every_step = 136120  \r\n# decay lr factor\r\ntrain.lr_decay_factor = 0.666\r\ntrain.weight_decay_factor = 5e-4\r\ntrain.train_type=TRAIN.Single_train\r\ntrain.vis_dir=f""./save_dir/{model.model_name}/train_vis_dir""\r\n\r\n#eval configuration\r\neval =edict()\r\neval.batch_size=22\r\neval.vis_dir= f""./save_dir/{model.model_name}/eval_vis_dir""\r\n\r\n#data configuration\r\ndata = edict()\r\ndata.dataset_type = DATA.MSCOCO  # coco, custom, coco_and_custom\r\ndata.dataset_version = ""2017""  # MSCOCO version 2014 or 2017\r\ndata.dataset_path = ""./data""\r\ndata.dataset_filter=None\r\ndata.vis_dir=f""./save_dir/{model.model_name}/data_vis_dir""\r\n\r\n#log configuration\r\nlog = edict()\r\nlog.log_interval = 1\r\nlog.log_path= f""./save_dir/{model.model_name}/log.txt""\r\n'"
Hyperpose/Config/config_mbtopps.py,0,"b'import os\r\nfrom .define import MODEL,DATA,TRAIN\r\nfrom easydict import EasyDict as edict\r\n\r\n#model configuration\r\nmodel = edict()\r\n# number of keypoints + 1 for background\r\nmodel.n_pos = 19  \r\nmodel.num_channels=128\r\n# input size during training , 240\r\nmodel.hin = 368  \r\nmodel.win = 432\r\n# output size during training (default 46)\r\nmodel.hout = 46 \r\nmodel.wout = 54\r\nmodel.model_type = MODEL.Openpose\r\nmodel.model_name = ""default_name""\r\nmodel.model_backbone=""Default""\r\nmodel.data_format = ""channels_first""\r\n# save directory\r\nmodel.model_dir = f""./save_dir/{model.model_name}/model_dir""\r\n\r\n#train configuration\r\ntrain=edict()\r\ntrain.batch_size = 8\r\ntrain.save_interval = 5000\r\n# total number of step\r\ntrain.n_step = 1000000\r\n# initial learning rate  \r\ntrain.lr_init = 4e-5  \r\n# evey number of step to decay lr\r\ntrain.lr_decay_every_step = 136120  \r\n# decay lr factor\r\ntrain.lr_decay_factor = 0.666\r\ntrain.weight_decay_factor = 5e-4\r\ntrain.train_type=TRAIN.Single_train\r\ntrain.vis_dir=f""./save_dir/{model.model_name}/train_vis_dir""\r\n\r\n#eval configuration\r\neval =edict()\r\neval.batch_size=22\r\neval.vis_dir= f""./save_dir/{model.model_name}/eval_vis_dir""\r\n\r\n#data configuration\r\ndata = edict()\r\ndata.dataset_type = DATA.MSCOCO  # coco, custom, coco_and_custom\r\ndata.dataset_version = ""2017""  # MSCOCO version 2014 or 2017\r\ndata.dataset_path = ""./data""\r\ndata.dataset_filter=None\r\ndata.vis_dir=f""./save_dir/{model.model_name}/data_vis_dir""\r\n\r\n#log configuration\r\nlog = edict()\r\nlog.log_interval = 1\r\nlog.log_path= f""./save_dir/{model.model_name}/log.txt""\r\n'"
Hyperpose/Config/config_opps.py,0,"b'import os\nfrom .define import MODEL,DATA,TRAIN\nfrom easydict import EasyDict as edict\n\n#model configuration\nmodel = edict()\n# number of keypoints + 1 for background\nmodel.n_pos = 19  \nmodel.num_channels=128\n# input size during training , 240\nmodel.hin = 368  \nmodel.win = 432\n# output size during training (default 46)\nmodel.hout = 46 \nmodel.wout = 54\nmodel.model_type = MODEL.Openpose\nmodel.model_name = ""default_name""\nmodel.model_backbone=""Default""\nmodel.data_format = ""channels_first""\n# save directory\nmodel.model_dir = f""./save_dir/{model.model_name}/model_dir""\n\n#train configuration\ntrain=edict()\ntrain.batch_size = 8\ntrain.save_interval = 5000\n# total number of step\ntrain.n_step = 1000000\n# initial learning rate  \ntrain.lr_init = 4e-5  \n# evey number of step to decay lr\ntrain.lr_decay_every_step = 136120  \n# decay lr factor\ntrain.lr_decay_factor = 0.666\ntrain.weight_decay_factor = 4e-4\ntrain.train_type=TRAIN.Single_train\ntrain.vis_dir=f""./save_dir/{model.model_name}/train_vis_dir""\n\n#eval configuration\neval =edict()\neval.batch_size=22\neval.vis_dir= f""./save_dir/{model.model_name}/eval_vis_dir""\n\n#data configuration\ndata = edict()\ndata.dataset_type = DATA.MSCOCO  # coco, custom, coco_and_custom\ndata.dataset_version = ""2017""  # MSCOCO version 2014 or 2017\ndata.dataset_path = ""./data""\ndata.dataset_filter=None\ndata.vis_dir=f""./save_dir/{model.model_name}/data_vis_dir""\n\n#log configuration\nlog = edict()\nlog.log_interval = 1\nlog.log_path= f""./save_dir/{model.model_name}/log.txt""\n'"
Hyperpose/Config/config_ppn.py,0,"b'import os\nfrom .define import MODEL,DATA,TRAIN\nfrom easydict import EasyDict as edict\n\n#model configuration\nmodel = edict()\n # number of keypoints, including instance\nmodel.K_size = 18 \nmodel.L_size = 17\n# input size during training\nmodel.hin = 384  \nmodel.win = 384\n# output size during training\nmodel.hout = 12 \nmodel.wout = 12\n#neibor size during training\nmodel.hnei=9\nmodel.wnei=9\n#loss weights\nmodel.lmd_rsp=0.25\nmodel.lmd_iou=1\nmodel.lmd_coor=5.0\nmodel.lmd_size=5.0\nmodel.lmd_limb=0.5\nmodel.model_type = MODEL.PoseProposal \nmodel.model_name = ""default_name""\nmodel.model_backbone=""Default""\nmodel.data_format = ""channels_first""\n#save directory\nmodel.model_dir= f""./save_dir/{model.model_name}/model_dir"" \n\n#train configuration\ntrain = edict()\ntrain.batch_size = 22\ntrain.save_interval = 5000\ntrain.n_step = 1500000  \ntrain.lr_init = 1e-4  # initial learning rate\ntrain.lr_decay_factor=0.3\ntrain.weight_decay_factor = 5e-4\ntrain.train_type=TRAIN.Single_train\ntrain.vis_dir = f""./save_dir/{model.model_name}/train_vis_dir""\n\n#eval configuration\neval =edict()\neval.batch_size=22\neval.vis_dir= f""./save_dir/{model.model_name}/eval_vis_dir""\n\n#data configuration\ndata = edict()\ndata.dataset_type = DATA.MSCOCO\ndata.dataset_version = ""2017"" \ndata.dataset_path = ""./data""\ndata.dataset_filter=None\ndata.vis_dir=f""./save_dir/{model.model_name}/data_vis_dir""\n\n#log configuration\nlog = edict()\nlog.log_interval = 1\nlog.log_path = f""./save_dir/{model.model_name}/log.txt""'"
Hyperpose/Config/define.py,0,b'from enum import Enum\r\n\r\nclass BACKBONE(Enum):\r\n    Default=0\r\n    Mobilenetv1=1\r\n    Vgg19=2\r\n    Resnet18=3\r\n    Resnet50=4\r\n    Vggtiny=5\r\n    Mobilenetv2=6\r\n    Vgg16=7\r\n\r\nclass MODEL(Enum):\r\n    Openpose=0\r\n    LightweightOpenpose=1\r\n    PoseProposal=2\r\n    MobilenetThinOpenpose=3\r\n\r\nclass DATA(Enum):\r\n    MSCOCO=0\r\n    MPII=1\r\n\r\nclass TRAIN(Enum):\r\n    Single_train=0\r\n    Parallel_train=1\r\n\r\nclass KUNGFU(Enum):\r\n    Sync_sgd=0\r\n    Sync_avg=1\r\n    Pair_avg=2\r\n'
Hyperpose/Dataset/__init__.py,0,"b'import os\nfrom .common import TRAIN,MODEL,DATA\nfrom .mpii_dataset.dataset import MPII_dataset\nfrom .mscoco_dataset.dataset import MSCOCO_dataset\n\ndef get_dataset(config):\n    \'\'\'get dataset object based on the config object\n\n    consturct and return a dataset object based on the config.\n    No matter what the bottom dataset type is, the APIs of the returned dataset object are uniform, they are \n    the following APIs:\n\n        visualize: visualize annotations of the train dataset and save it in ""data_vir_dir""\n        get_dataset_type: return the type of the bottom dataset.\n        get_train_dataset: return a uniform tensorflow dataset object for training.\n        get_val_dataset: return a uniform tensorflow dataset object for evaluating.\n        official_eval: perform official evaluation on this dataset.\n\n\n    The construction pipeline of this dataset object is below:\n\n        1.check whether the dataset file(official zip or mat) is under data_path,\n        if it isn\'t, download it from official website automaticly\n\n        2.decode the official dataset file, organize the annotations in corresponding Meta classes,\n        conveniet for processing.\n\n        3.based on annotation, split train and evaluat part for furthur use.\n\n    if user defined thier own dataset_filter, it will be executed in the train dataset or evaluate dataset generating procedure.\n\n    use the APIs of this returned dataset object, the difference of different dataset is minimized.\n\n    Parameters\n    ----------\n    arg1 : config object\n        the config object return by Config.get_config() function, which includes all the configuration information.\n    \n    Returns\n    -------\n    dataset\n        a dataset object with unifrom APIs:\n        visualize, get_dataset_type, get_train_dataset, get_val_dataset,official_eval\n    \'\'\'\n\n    dataset_type=config.data.dataset_type\n    if(dataset_type==DATA.MSCOCO):\n        print(""using Mscoco dataset!"")\n        dataset=MSCOCO_dataset(config)\n        dataset.prepare_dataset()\n    elif(dataset_type==DATA.MPII):\n        print(""using Mpii dataset!"")\n        dataset=MPII_dataset(config)\n        dataset.prepare_dataset()\n    else:\n        print(""using user-defined dataset!"")\n        user_dataset=dataset_type\n        dataset=user_dataset(config)\n    return dataset\n'"
Hyperpose/Dataset/common.py,0,"b""import zipfile\r\nfrom enum import Enum\r\nfrom ..Config.define import TRAIN,MODEL,DATA,KUNGFU\r\n\r\ndef unzip(path_to_zip_file, directory_to_extract_to):\r\n    zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\r\n    zip_ref.extractall(directory_to_extract_to)\r\n    zip_ref.close()\r\n"""
Hyperpose/Model/__init__.py,0,"b'import tensorflow as tf\nfrom functools import partial\nfrom .human import Human\nfrom .common import rename_tensor\nfrom .common import TRAIN,MODEL,DATA,KUNGFU,BACKBONE\nfrom .openpose import OpenPose,LightWeightOpenPose,MobilenetThinOpenpose\nfrom .pose_proposal import PoseProposal\nfrom .backbones import MobilenetV1_backbone,MobilenetV2_backbone,vgg16_backbone,vgg19_backbone,vggtiny_backbone\nfrom .backbones import Resnet18_backbone,Resnet50_backbone\n\ndef get_model(config):\n    \'\'\'get model based on config object\n\n    construct and return a model based on the configured model_type and model_backbone.\n    each preset model architecture has a default backbone, replace it with chosen common model_backbones allow user to\n    change model computation complexity to adapt to application scene.\n\n    Parameters\n    ----------\n    arg1 : config object\n        the config object return by Config.get_config() function, which includes all the configuration information.\n    \n    Returns\n    -------\n    tensorlayer.models.MODEL\n        a model object inherited from tensorlayer.models.MODEL class, has configured model architecture and chosen \n        model backbone. can be user defined architecture by using Config.set_model_architecture() function.\n    \'\'\'\n    model=config.model\n    #user configure model arch themselves\n    if(""model_arch"" in model):\n        print(""using user self-defined model arch!"")\n        ret_model=model.model_arch(config)\n    #using defualt arch\n    else:\n        backbone=None\n        if(""model_backbone"" in model):\n            model_backbone=model.model_backbone\n            if(model_backbone==BACKBONE.Default):\n                print(f""using default backbone!"")\n            elif(model_backbone==BACKBONE.Mobilenetv1):\n                backbone=MobilenetV1_backbone\n                print(f""setting MobilnetV1_backbone!"")\n            elif(model_backbone==BACKBONE.Vgg19):\n                backbone=vgg19_backbone\n                print(f""setting Vgg19_backbone!"")\n            elif(model_backbone==BACKBONE.Resnet18):\n                backbone=Resnet18_backbone\n                print(f""setting Resnet18_backbone!"")\n            elif(model_backbone==BACKBONE.Resnet50):\n                backbone=Resnet50_backbone\n                print(f""setting Resnet50_backbone!"")\n            elif(model_backbone==BACKBONE.Vggtiny):\n                backbone=vggtiny_backbone\n                print(f""setting Vggtiny_backbone"")\n            elif(model_backbone==BACKBONE.Mobilenetv2):\n                backbone=MobilenetV2_backbone\n                print(f""setting MobilenetV2_backbone"")\n            elif(model_backbone==BACKBONE.Vgg16):\n                backbone=vgg16_backbone\n                print(f""setting Vgg16_backbone"")\n            else:\n                raise NotImplementedError(f""unknown model backbone {model_backbone}"")\n\n        model_type=model.model_type\n        dataset_type=config.data.dataset_type\n        if(model_type == MODEL.Openpose or model_type == MODEL.LightweightOpenpose or model_type==MODEL.MobilenetThinOpenpose):\n            from .openpose.utils import get_parts\n            from .openpose.utils import get_limbs\n            model.n_pos=len(get_parts(dataset_type))\n            model.n_limbs=len(get_limbs(dataset_type))\n        elif(model_type == MODEL.PoseProposal):\n            from .pose_proposal.utils import get_parts\n            from .pose_proposal.utils import get_limbs\n            model.K_size=len(get_parts(dataset_type))\n            model.L_size=len(get_limbs(dataset_type))\n        \n        #set model\n        if model_type == MODEL.Openpose:\n            from .openpose import OpenPose as model_arch\n            ret_model=model_arch(n_pos=model.n_pos,n_limbs=model.n_limbs,num_channels=model.num_channels,hin=model.hin,win=model.win,\\\n                hout=model.hout,wout=model.wout,backbone=backbone,data_format=model.data_format)\n        elif model_type == MODEL.LightweightOpenpose:\n            from .openpose import LightWeightOpenPose as model_arch\n            ret_model=model_arch(n_pos=model.n_pos,n_limbs=model.n_limbs,num_channels=model.num_channels,hin=model.hin,win=model.win,\\\n                hout=model.hout,wout=model.wout,backbone=backbone,data_format=model.data_format)\n        elif model_type == MODEL.MobilenetThinOpenpose:\n            from .openpose import MobilenetThinOpenpose as model_arch\n            ret_model=model_arch(n_pos=model.n_pos,n_limbs=model.n_limbs,num_channels=model.num_channels,hin=model.hin,win=model.win,\\\n                hout=model.hout,wout=model.wout,backbone=backbone,data_format=model.data_format)\n        elif model_type == MODEL.PoseProposal:\n            from .pose_proposal import PoseProposal as model_arch\n            ret_model=model_arch(K_size=model.K_size,L_size=model.L_size,hnei=model.hnei,wnei=model.wnei,lmd_rsp=model.lmd_rsp,\\\n                lmd_iou=model.lmd_iou,lmd_coor=model.lmd_coor,lmd_size=model.lmd_size,lmd_limb=model.lmd_limb,backbone=backbone,\\\n                data_format=model.data_format)\n        else:\n            raise RuntimeError(f\'unknown model type {model_type}\')\n        print(f""using {model_type.name} model arch!"")\n    return ret_model\n\ndef get_train(config):\n    \'\'\'get train pipeline based on config object\n\n    construct train pipeline based on the chosen model_type and dataset_type,\n    default is single train pipeline performed on single GPU,\n    can be parallel train pipeline use function Config.set_train_type()\n\n    the returned train pipeline can be easily used by train(model,dataset),\n    where model is obtained by Model.get_model(), dataset is obtained by Dataset.get_dataset()\n\n    the train pipeline will:\n    1.store and restore ckpt in directory ./save_dir/model_name/model_dir\n    2.log loss information in directory ./save_dir/model_name/log.txt\n    3.visualize model output periodly during training in directory ./save_dir/model_name/train_vis_dir\n    the newest model is at path ./save_dir/model_name/model_dir/newest_model.npz\n\n    Parameters\n    ----------\n    arg1 : config object\n        the config object return by Config.get_config() function, which includes all the configuration information.\n    \n    Returns\n    -------\n    function\n        a train pipeline function which takes model and dataset as input, can be either single train or parallel train pipeline.\n    \n    \'\'\'\n    # determine train process\n    model_type=config.model.model_type\n    if model_type == MODEL.Openpose or model_type == MODEL.LightweightOpenpose or model_type==MODEL.MobilenetThinOpenpose:\n        from .openpose import single_train,parallel_train\n    elif model_type == MODEL.PoseProposal:\n        from .pose_proposal import single_train,parallel_train\n    else:\n        raise RuntimeError(f\'unknown model type {model_type}\')\n    print(f""training {model_type.name} model..."")\n\n    #determine train type\n    train_type=config.train.train_type\n    if(train_type==TRAIN.Single_train):\n        train=partial(single_train,config=config)\n    elif(train_type==TRAIN.Parallel_train):\n        #set defualt kungfu opt type\n        if(""kungfu_option"" not in config.train):\n            config.train.kungfu_option=KUNGFU.Sma\n        train=partial(parallel_train,config=config)\n    print(f""using {train_type.name}..."")\n    return train\n\ndef get_evaluate(config):\n    \'\'\'get evaluate pipeline based on config object\n\n    construct evaluate pipeline based on the chosen model_type and dataset_type,\n    the evaluation metric fellows the official metrics of the chosen dataset.\n\n    the returned evaluate pipeline can be easily used by evaluate(model,dataset),\n    where model is obtained by Model.get_model(), dataset is obtained by Dataset.get_dataset()\n\n    the evaluate pipeline will:\n    1.loading newest model at path ./save_dir/model_name/model_dir/newest_model.npz\n    2.perform inference and parsing over the chosen evaluate dataset\n    3.visualize model output in evaluation in directory ./save_dir/model_name/eval_vis_dir\n    4.output model metrics by calling dataset.official_eval()\n\n    Parameters\n    ----------\n    arg1 : config object\n        the config object return by Config.get_config() function, which includes all the configuration information.\n    \n    Returns\n    -------\n    function\n        a evaluate pipeline function which takes model and dataset as input, and output model metrics\n    \n    \'\'\'\n    model_type=config.model.model_type\n    if model_type == MODEL.Openpose or model_type == MODEL.LightweightOpenpose or model_type==MODEL.MobilenetThinOpenpose:\n        from .openpose import evaluate\n    elif model_type == MODEL.PoseProposal:\n        from .pose_proposal import evaluate\n    else:\n        raise RuntimeError(f\'unknown model type {model_type}\')\n    evaluate=partial(evaluate,config=config)\n    print(f""evaluating {model_type.name} model..."")\n    return evaluate\n\ndef get_preprocess(model_type):\n    \'\'\'get preprocess function based model_type\n\n    get the preprocess function of the specified kind of model to help user construct thier own train\n    and evaluate pipeline rather than using the integrated train or evaluate pipeline directly when in need.\n\n    the preprocess function is able to convert the image and annotation to the model output format for training\n    or evaluation.\n\n    Parameters\n    ----------\n    arg1 : Config.MODEL\n        a enum value of enum class Config.MODEL\n    \n    Returns\n    -------\n    function\n        a preprocess function of the specified kind of model\n    \'\'\'\n\n    if model_type == MODEL.Openpose or model_type == MODEL.LightweightOpenpose or model_type==MODEL.MobilenetThinOpenpose:\n        from .openpose.utils import preprocess\n    elif model_type == MODEL.PoseProposal:\n        from .pose_proposal.utils import preprocess\n    return preprocess\n\ndef get_postprocess(model_type):\n    \'\'\'get postprocess function based model_type\n\n    get the postprocess function of the specified kind of model to help user construct thier own \n    evaluate pipeline rather than using the integrated train or evaluate pipeline directly when in need\n\n    the postprocess function is able to parse the model output feature map and output parsed human objects of Human class,\n    which contains all dectected keypoints.\n\n    Parameters\n    ----------\n    arg1 : Config.MODEL\n        a enum value of enum class Config.MODEL\n    \n    Returns\n    -------\n    function\n        a postprocess function of the specified kind of model\n    \'\'\'\n    if model_type == MODEL.Openpose or model_type == MODEL.LightweightOpenpose or model_type==MODEL.MobilenetThinOpenpose:\n        from .openpose.utils import postprocess\n    elif model_type == MODEL.PoseProposal:\n        from .pose_proposal.utils import postprocess\n    return postprocess\n\ndef get_visualize(model_type):\n    \'\'\'get visualize function based model_type\n\n    get the visualize function of the specified kind of model to help user construct thier own \n    evaluate pipeline rather than using the integrated train or evaluate pipeline directly when in need\n\n    the visualize function is able to visualize model\'s output feature map, which is helpful for\n    training and evaluation analysis.\n\n    Parameters\n    ----------\n    arg1 : Config.MODEL\n        a enum value of enum class Config.MODEL\n    \n    Returns\n    -------\n    function\n        a visualize function of the specified kind of model\n    \'\'\'\n    if model_type == MODEL.Openpose or model_type == MODEL.LightweightOpenpose or model_type==MODEL.MobilenetThinOpenpose:\n        from .openpose.utils import visualize\n    elif model_type == MODEL.PoseProposal:\n        from .pose_proposal.utils import visualize\n    return visualize'"
Hyperpose/Model/backbones.py,44,"b'import tensorflow as tf\r\nimport tensorlayer as tl\r\nfrom tensorlayer import layers\r\nfrom tensorlayer.models import Model\r\nfrom tensorlayer.layers import BatchNorm2d, Conv2d, DepthwiseConv2d, LayerList, MaxPool2d, SeparableConv2d\r\n\r\nclass MobilenetV1_backbone(Model):\r\n    def __init__(self,scale_size=8,data_format=""channel_last""):\r\n        super().__init__()\r\n        self.data_format=data_format\r\n        self.scale_size=scale_size\r\n        if(self.scale_size==8):\r\n            strides=(1,1)\r\n        else:\r\n            strides=(2,2)\r\n        self.out_channels=1024\r\n        self.layer_list=[]\r\n        self.layer_list+=self.conv_block(n_filter=32,in_channels=3,filter_size=(3,3),strides=(2,2))\r\n        self.layer_list+=self.separable_conv_block(n_filter=64,in_channels=32,filter_size=(3,3),strides=(1,1))\r\n        self.layer_list+=self.separable_conv_block(n_filter=128,in_channels=64,filter_size=(3,3),strides=(2,2))\r\n        self.layer_list+=self.separable_conv_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1))\r\n        self.layer_list+=self.separable_conv_block(n_filter=256,in_channels=128,filter_size=(3,3),strides=(2,2))\r\n        self.layer_list+=self.separable_conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1))\r\n        self.layer_list+=self.separable_conv_block(n_filter=512,in_channels=256,filter_size=(3,3),strides=strides)\r\n        self.layer_list+=self.separable_conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1))\r\n        self.layer_list+=self.separable_conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1))\r\n        self.layer_list+=self.separable_conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1))\r\n        self.layer_list+=self.separable_conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1))\r\n        self.layer_list+=self.separable_conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1))\r\n        self.layer_list+=self.separable_conv_block(n_filter=1024,in_channels=512,filter_size=(3,3),strides=strides)\r\n        self.layer_list+=self.separable_conv_block(n_filter=1024,in_channels=1024,filter_size=(3,3),strides=(1,1))\r\n        self.main_block=LayerList(self.layer_list)\r\n    \r\n    def forward(self,x):\r\n        return self.main_block.forward(x)\r\n\r\n    def conv_block(self,n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),padding=""SAME""):\r\n        layer_list=[]\r\n        layer_list.append(Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=filter_size,strides=strides,\\\r\n            data_format=self.data_format,padding=padding))\r\n        layer_list.append(BatchNorm2d(num_features=n_filter,is_train=True,act=tf.nn.relu,data_format=self.data_format))\r\n        return LayerList(layer_list)\r\n    \r\n    def separable_conv_block(self,n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1)):\r\n        layer_list=[]\r\n        layer_list.append(DepthwiseConv2d(in_channels=in_channels,filter_size=filter_size,strides=strides,data_format=self.data_format))\r\n        layer_list.append(BatchNorm2d(num_features=in_channels,is_train=True,act=tf.nn.relu,data_format=self.data_format))\r\n        layer_list.append(Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=(1,1),strides=(1,1),data_format=self.data_format))\r\n        layer_list.append(BatchNorm2d(num_features=n_filter,is_train=True,act=tf.nn.relu,data_format=self.data_format))\r\n        return LayerList(layer_list)\r\n\r\nclass MobilenetV2_backbone(Model):\r\n    def __init__(self,scale_size=8,data_format=""channels_last""):\r\n        super().__init__()\r\n        self.data_format=data_format\r\n        self.scale_size=scale_size\r\n        self.out_channels=320\r\n        if(self.scale_size==8):\r\n            strides=(1,1)\r\n        else:\r\n            strides=(2,2)\r\n        #block_1 n=1\r\n        self.block_1_1=Conv2d(n_filter=32,in_channels=3,filter_size=(3,3),strides=(2,2),data_format=self.data_format)\r\n        self.block_1_2=BatchNorm2d(num_features=32,is_train=True,act=tf.nn.relu6,data_format=self.data_format)\r\n        #block_2 n=1\r\n        self.block_2_1=self.InvertedResidual(n_filter=16,in_channels=32,strides=(1,1),exp_ratio=1,data_format=self.data_format)\r\n        #block_3 n=2\r\n        self.block_3_1=self.InvertedResidual(n_filter=24,in_channels=16,strides=(2,2),exp_ratio=6,data_format=self.data_format)\r\n        self.block_3_2=self.InvertedResidual(n_filter=24,in_channels=24,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        #block_4 n=3\r\n        self.block_4_1=self.InvertedResidual(n_filter=32,in_channels=24,strides=(2,2),exp_ratio=6,data_format=self.data_format)\r\n        self.block_4_2=self.InvertedResidual(n_filter=32,in_channels=32,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        self.block_4_3=self.InvertedResidual(n_filter=32,in_channels=32,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        #block_5 n=4\r\n        self.block_5_1=self.InvertedResidual(n_filter=64,in_channels=32,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        self.block_5_2=self.InvertedResidual(n_filter=64,in_channels=64,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        self.block_5_3=self.InvertedResidual(n_filter=64,in_channels=64,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        self.block_5_4=self.InvertedResidual(n_filter=64,in_channels=64,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        #block_6 n=3\r\n        self.block_6_1=self.InvertedResidual(n_filter=96,in_channels=64,strides=strides,exp_ratio=6,data_format=self.data_format)\r\n        self.block_6_2=self.InvertedResidual(n_filter=96,in_channels=96,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        self.block_6_3=self.InvertedResidual(n_filter=96,in_channels=96,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        #block_7 n=3\r\n        self.block_7_1=self.InvertedResidual(n_filter=160,in_channels=96,strides=strides,exp_ratio=6,data_format=self.data_format)\r\n        self.block_7_2=self.InvertedResidual(n_filter=160,in_channels=160,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        self.block_7_3=self.InvertedResidual(n_filter=160,in_channels=160,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        #block_8 n=1\r\n        self.block_8=self.InvertedResidual(n_filter=320,in_channels=160,strides=(1,1),exp_ratio=6,data_format=self.data_format)\r\n        \r\n    def forward(self,x):\r\n        x=self.block_1_1.forward(x)\r\n        x=self.block_1_2.forward(x)\r\n        x=self.block_2_1.forward(x)\r\n        x=self.block_3_1.forward(x)\r\n        x=self.block_3_2.forward(x)\r\n        x=self.block_4_1.forward(x)\r\n        x=self.block_4_2.forward(x)\r\n        x=self.block_4_3.forward(x)\r\n        x=self.block_5_1.forward(x)\r\n        x=self.block_5_2.forward(x)\r\n        x=self.block_5_3.forward(x)\r\n        x=self.block_5_4.forward(x)\r\n        x=self.block_6_1.forward(x)\r\n        x=self.block_6_2.forward(x)\r\n        x=self.block_6_3.forward(x)\r\n        x=self.block_7_1.forward(x)\r\n        x=self.block_7_2.forward(x)\r\n        x=self.block_7_3.forward(x)\r\n        x=self.block_8.forward(x)\r\n        return x\r\n\r\n    class InvertedResidual(Model):\r\n        def __init__(self,n_filter=128,in_channels=128,strides=(1,1),exp_ratio=6,data_format=""channels_first""):\r\n            super().__init__()\r\n            self.n_filter=n_filter\r\n            self.in_channels=in_channels\r\n            self.strides=strides\r\n            self.exp_ratio=exp_ratio\r\n            self.data_format=data_format\r\n            self.hidden_dim=self.exp_ratio*self.in_channels\r\n            self.identity=False\r\n            if(self.strides==(1,1) and self.in_channels==self.n_filter):\r\n                self.identity=True\r\n            if(self.exp_ratio==1):\r\n                self.main_block=LayerList([\r\n                    DepthwiseConv2d(in_channels=self.hidden_dim,filter_size=(3,3),strides=self.strides,\\\r\n                        b_init=None,data_format=self.data_format),\r\n                    BatchNorm2d(num_features=self.hidden_dim,is_train=True,act=tf.nn.relu6,data_format=self.data_format),\r\n                    Conv2d(n_filter=self.n_filter,in_channels=self.hidden_dim,filter_size=(1,1),strides=(1,1),b_init=None,data_format=self.data_format),\r\n                    BatchNorm2d(num_features=self.n_filter,is_train=True,act=None,data_format=self.data_format)\r\n                ])\r\n            else:\r\n                self.main_block=LayerList([\r\n                    Conv2d(n_filter=self.hidden_dim,in_channels=self.in_channels,filter_size=(1,1),strides=(1,1),b_init=None,data_format=self.data_format),\r\n                    BatchNorm2d(num_features=self.hidden_dim,is_train=True,act=tf.nn.relu6,data_format=self.data_format),\r\n                    DepthwiseConv2d(in_channels=self.hidden_dim,filter_size=(3,3),strides=self.strides,\\\r\n                        b_init=None,data_format=self.data_format),\r\n                    BatchNorm2d(num_features=self.hidden_dim,is_train=True,act=tf.nn.relu6,data_format=self.data_format),\r\n                    Conv2d(n_filter=self.n_filter,in_channels=self.hidden_dim,filter_size=(1,1),strides=(1,1),b_init=None,data_format=self.data_format)\r\n                ])\r\n\r\n        def forward(self,x):\r\n            if(self.identity):\r\n                return x+self.main_block.forward(x)\r\n            else:\r\n                return self.main_block.forward(x)\r\n\r\nclass vggtiny_backbone(Model):\r\n    def __init__(self,in_channels=3,scale_size=8,data_format=""channels_first""):\r\n        super().__init__()\r\n        self.in_channels=in_channels\r\n        self.data_format=data_format\r\n        self.scale_size=scale_size\r\n        if(self.scale_size==8):\r\n            strides=(1,1)\r\n        else:\r\n            strides=(2,2)\r\n        self.out_channels=384\r\n        self.main_block=layers.LayerList([\r\n            self.conv_block(n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1)),\r\n            self.conv_block(n_filter=64,in_channels=32,filter_size=(3,3),strides=(1,1)),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),padding=""SAME"",data_format=self.data_format),\r\n            self.conv_block(n_filter=128,in_channels=64,filter_size=(3,3),strides=(1,1)),\r\n            self.conv_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1)),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),padding=""SAME"",data_format=self.data_format),\r\n            self.conv_block(n_filter=200,in_channels=128,filter_size=(3,3),strides=(1,1)),\r\n            self.conv_block(n_filter=200,in_channels=200,filter_size=(3,3),strides=strides),\r\n            self.conv_block(n_filter=200,in_channels=200,filter_size=(3,3),strides=(1,1)),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),padding=""SAME"",data_format=self.data_format),\r\n            self.conv_block(n_filter=384,in_channels=200,filter_size=(3,3),strides=(1,1)),\r\n            self.conv_block(n_filter=384,in_channels=384,filter_size=(3,3),strides=strides)\r\n        ])\r\n    \r\n    def conv_block(self,n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,padding=""SAME""):\r\n        layer_list=[]\r\n        layer_list.append(Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=filter_size,strides=strides,\\\r\n            act=None,data_format=self.data_format,padding=padding))\r\n        layer_list.append(BatchNorm2d(num_features=n_filter,act=act,is_train=True,data_format=self.data_format))\r\n        return LayerList(layer_list)\r\n    \r\n    def forward(self,x):\r\n        return self.main_block.forward(x)\r\n\r\nclass vgg16_backbone(Model):\r\n    def __init__(self,in_channels=3,scale_size=8,data_format=""channels_first""):\r\n        super().__init__()\r\n        self.in_channels=in_channels\r\n        self.data_format=data_format\r\n        self.scale_size=scale_size\r\n        if(self.scale_size==8):\r\n            strides=(1,1)\r\n        elif(self.scale_size==32):\r\n            strides=(2,2)\r\n        self.out_channels=512\r\n        self.layer_list=[\r\n            self.conv_block(n_filter=64,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=64,in_channels=64,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n            self.conv_block(n_filter=128,in_channels=64,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n            self.conv_block(n_filter=256,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n            self.conv_block(n_filter=512,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu)\r\n        ]\r\n        if(self.scale_size==32):\r\n            self.layer_list+=[MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format)]\r\n        self.layer_list+=[\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=strides,act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu)\r\n        ]\r\n        self.main_block=LayerList(self.layer_list)\r\n\r\n    def forward(self,x):\r\n        return self.main_block.forward(x)\r\n    \r\n    def conv_block(self,n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),act=None,padding=""SAME""):\r\n        return Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=filter_size,strides=strides,\\\r\n            act=act,data_format=self.data_format,padding=padding)\r\n\r\nclass vgg19_backbone(Model):\r\n    def __init__(self,in_channels=3,scale_size=8,data_format=""channels_first""):\r\n        super().__init__()\r\n        self.in_channels=in_channels\r\n        self.data_format=data_format\r\n        self.scale_size=scale_size\r\n        if(self.scale_size==8):\r\n            strides=(1,1)\r\n        else:\r\n            strides=(2,2)\r\n        self.out_channels=512\r\n        self.layer_list=[\r\n            self.conv_block(n_filter=64,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=64,in_channels=64,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n            self.conv_block(n_filter=128,in_channels=64,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n            self.conv_block(n_filter=256,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n            self.conv_block(n_filter=512,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu)\r\n        ]\r\n        if(self.scale_size==32):\r\n            self.layer_list+=[MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format)]\r\n        self.layer_list+=[\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=strides,act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n            self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu)\r\n        ]\r\n        self.main_block=LayerList(self.layer_list)\r\n    \r\n    \r\n    def forward(self,x):\r\n        return self.main_block.forward(x)\r\n    \r\n    def conv_block(self,n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),act=None,padding=""SAME""):\r\n        return Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=filter_size,strides=strides,\\\r\n            act=act,data_format=self.data_format,padding=padding)\r\n\r\n\r\nclass Resnet18_backbone(Model):\r\n    def __init__(self,in_channels=3,n_filter=64,scale_size=8,data_format=""channels_first""):\r\n        super().__init__()\r\n        self.in_channels=in_channels\r\n        self.n_filter=n_filter\r\n        self.scale_size=scale_size\r\n        self.data_format=data_format\r\n        self.out_channels=512\r\n        self.conv1=Conv2d(n_filter=64,in_channels=self.in_channels,filter_size=(7,7),strides=(2,2),padding=""SAME"",b_init=None,data_format=self.data_format)\r\n        self.bn1=BatchNorm2d(is_train=True,num_features=64,data_format=self.data_format,act=tf.nn.relu)\r\n        self.maxpool1=MaxPool2d(filter_size=(3,3),strides=(2,2),data_format=self.data_format)\r\n        self.layer1=self.Basic_block(in_channels=64,n_filter=64,strides=(1,1),data_format=self.data_format)\r\n        self.layer2=self.Basic_block(in_channels=64,n_filter=128,strides=(2,2),data_format=self.data_format)\r\n        if(self.scale_size==8):\r\n            strides=(1,1)\r\n        else:\r\n            strides=(2,2)\r\n        self.layer3=self.Basic_block(in_channels=128,n_filter=256,strides=strides,data_format=self.data_format)\r\n        self.layer4=self.Basic_block(in_channels=256,n_filter=512,strides=strides,data_format=self.data_format)\r\n    \r\n    def forward(self,x):\r\n        x=self.conv1.forward(x)\r\n        x=self.bn1.forward(x)\r\n        x=self.maxpool1.forward(x)\r\n        x=self.layer1.forward(x)\r\n        x=self.layer2.forward(x)\r\n        x=self.layer3.forward(x)\r\n        x=self.layer4.forward(x)\r\n        return x\r\n    \r\n    class Basic_block(Model):\r\n        def __init__(self,in_channels=64,n_filter=64,strides=(1,1),data_format=""channels_first""):\r\n            super().__init__()\r\n            self.in_channels=in_channels\r\n            self.n_filter=n_filter\r\n            self.strides=strides\r\n            self.data_format=data_format\r\n            self.downsample=None\r\n            if(self.strides!=(1,1) or self.in_channels!=self.n_filter):\r\n                self.downsample=LayerList([\r\n                    Conv2d(n_filter=self.n_filter,in_channels=self.in_channels,filter_size=(1,1),strides=self.strides,b_init=None,\\\r\n                        data_format=self.data_format),\r\n                    BatchNorm2d(is_train=True,num_features=self.n_filter,data_format=self.data_format)\r\n                    ])\r\n            self.main_block=LayerList([\r\n                Conv2d(n_filter=self.n_filter,in_channels=self.in_channels,filter_size=(3,3),strides=self.strides,b_init=None,data_format=self.data_format),\r\n                BatchNorm2d(is_train=True,num_features=self.n_filter,act=tf.nn.relu,data_format=self.data_format),\r\n                Conv2d(n_filter=self.n_filter,in_channels=self.n_filter,filter_size=(3,3),b_init=None,data_format=self.data_format),\r\n                BatchNorm2d(is_train=True,num_features=self.n_filter,data_format=self.data_format)\r\n            ])\r\n        \r\n        def forward(self,x):\r\n            res=x\r\n            x=self.main_block.forward(x)\r\n            if(self.downsample!=None):\r\n                res=self.downsample.forward(res)\r\n            return tf.nn.relu(res+x)    \r\n\r\nclass Resnet50_backbone(Model):\r\n    def __init__(self,in_channels=3,n_filter=64,scale_size=8,data_format=""channels_first""):\r\n        super().__init__()\r\n        self.in_channels=in_channels\r\n        self.n_filter=n_filter\r\n        self.scale_size=scale_size\r\n        self.data_format=data_format\r\n        self.out_channels=2048\r\n        self.conv1=Conv2d(n_filter=64,in_channels=self.in_channels,filter_size=(7,7),strides=(2,2),padding=""SAME"",b_init=None,data_format=self.data_format)\r\n        self.bn1=BatchNorm2d(is_train=True,num_features=64,data_format=self.data_format,act=tf.nn.relu)\r\n        self.maxpool1=MaxPool2d(filter_size=(3,3),strides=(2,2),data_format=self.data_format)\r\n        self.layer1=self.Basic_block(in_channels=64,n_filter=64,strides=(1,1),data_format=self.data_format)\r\n        self.layer2=self.Basic_block(in_channels=256,n_filter=128,strides=(2,2),data_format=self.data_format)\r\n        if(self.scale_size==8):\r\n            strides=(1,1)\r\n        else:\r\n            strides=(2,2)\r\n        self.layer3=self.Basic_block(in_channels=512,n_filter=256,strides=strides,data_format=self.data_format)\r\n        self.layer4=self.Basic_block(in_channels=1024,n_filter=512,strides=strides,data_format=self.data_format)\r\n    \r\n    def forward(self,x):\r\n        x=self.conv1.forward(x)\r\n        x=self.bn1.forward(x)\r\n        x=self.maxpool1.forward(x)\r\n        x=self.layer1.forward(x)\r\n        x=self.layer2.forward(x)\r\n        x=self.layer3.forward(x)\r\n        x=self.layer4.forward(x)\r\n        return x\r\n\r\n    class Basic_block(Model):\r\n        def __init__(self,in_channels=64,n_filter=64,strides=(1,1),data_format=""channels_first""):\r\n            super().__init__()\r\n            self.in_channels=in_channels\r\n            self.n_filter=n_filter\r\n            self.strides=strides\r\n            self.data_format=data_format\r\n            self.downsample=None\r\n            if(self.strides!=(1,1) or self.in_channels!=4*self.n_filter):\r\n                self.downsample=LayerList([\r\n                    Conv2d(n_filter=4*self.n_filter,in_channels=self.in_channels,filter_size=(1,1),strides=self.strides,b_init=None,\\\r\n                        data_format=self.data_format),\r\n                    BatchNorm2d(is_train=True,num_features=4*self.n_filter,data_format=self.data_format)\r\n                    ])\r\n            self.main_block=LayerList([\r\n                Conv2d(n_filter=self.n_filter,in_channels=self.in_channels,filter_size=(1,1),strides=(1,1),b_init=None,data_format=self.data_format),\r\n                BatchNorm2d(is_train=True,num_features=self.n_filter,act=tf.nn.relu,data_format=self.data_format),\r\n                Conv2d(n_filter=self.n_filter,in_channels=self.n_filter,filter_size=(3,3),strides=self.strides,b_init=None,data_format=self.data_format),\r\n                BatchNorm2d(is_train=True,num_features=self.n_filter,act=tf.nn.relu,data_format=self.data_format),\r\n                Conv2d(n_filter=4*self.n_filter,in_channels=self.n_filter,filter_size=(1,1),strides=(1,1),b_init=None,data_format=self.data_format),\r\n                BatchNorm2d(is_train=True,num_features=4*self.n_filter,data_format=self.data_format,)\r\n            ])\r\n        \r\n        def forward(self,x):\r\n            res=x\r\n            x=self.main_block.forward(x)\r\n            if(self.downsample!=None):\r\n                res=self.downsample.forward(res)\r\n            return  tf.nn.relu(x+res)'"
Hyperpose/Model/common.py,9,"b'import logging\nfrom enum import Enum\nimport time\nfrom distutils.dir_util import mkpath\n\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nfrom ..Config.define import MODEL,TRAIN,DATA,BACKBONE,KUNGFU\n\nregularizer_conv = 0.004\nregularizer_dsconv = 0.0004\nbatchnorm_fused = True\nactivation_fn = tf.nn.relu\n\nclass MPIIPart(Enum):\n    RAnkle = 0\n    RKnee = 1\n    RHip = 2\n    LHip = 3\n    LKnee = 4\n    LAnkle = 5\n    RWrist = 6\n    RElbow = 7\n    RShoulder = 8\n    LShoulder = 9\n    LElbow = 10\n    LWrist = 11\n    Neck = 12\n    Head = 13\n\n    @staticmethod\n    def from_coco(human):\n        # t = {\n        #     MPIIPart.RAnkle: CocoPart.RAnkle,\n        #     MPIIPart.RKnee: CocoPart.RKnee,\n        #     MPIIPart.RHip: CocoPart.RHip,\n        #     MPIIPart.LHip: CocoPart.LHip,\n        #     MPIIPart.LKnee: CocoPart.LKnee,\n        #     MPIIPart.LAnkle: CocoPart.LAnkle,\n        #     MPIIPart.RWrist: CocoPart.RWrist,\n        #     MPIIPart.RElbow: CocoPart.RElbow,\n        #     MPIIPart.RShoulder: CocoPart.RShoulder,\n        #     MPIIPart.LShoulder: CocoPart.LShoulder,\n        #     MPIIPart.LElbow: CocoPart.LElbow,\n        #     MPIIPart.LWrist: CocoPart.LWrist,\n        #     MPIIPart.Neck: CocoPart.Neck,\n        #     MPIIPart.Nose: CocoPart.Nose,\n        # }\n        \n        from ..Dataset.mscoco_dataset.define import CocoPart\n        t = [\n            (MPIIPart.Head, CocoPart.Nose),\n            (MPIIPart.Neck, CocoPart.Neck),\n            (MPIIPart.RShoulder, CocoPart.RShoulder),\n            (MPIIPart.RElbow, CocoPart.RElbow),\n            (MPIIPart.RWrist, CocoPart.RWrist),\n            (MPIIPart.LShoulder, CocoPart.LShoulder),\n            (MPIIPart.LElbow, CocoPart.LElbow),\n            (MPIIPart.LWrist, CocoPart.LWrist),\n            (MPIIPart.RHip, CocoPart.RHip),\n            (MPIIPart.RKnee, CocoPart.RKnee),\n            (MPIIPart.RAnkle, CocoPart.RAnkle),\n            (MPIIPart.LHip, CocoPart.LHip),\n            (MPIIPart.LKnee, CocoPart.LKnee),\n            (MPIIPart.LAnkle, CocoPart.LAnkle),\n        ]\n\n        pose_2d_mpii = []\n        visibilty = []\n        for mpi, coco in t:\n            if coco.value not in human.body_parts.keys():\n                pose_2d_mpii.append((0, 0))\n                visibilty.append(False)\n                continue\n            pose_2d_mpii.append((human.body_parts[coco.value].x, human.body_parts[coco.value].y))\n            visibilty.append(True)\n        return pose_2d_mpii, visibilty\n\n\ndef read_imgfile(path, width, height, data_format=\'channels_last\'):\n    """"""Read image file and resize to network input size.""""""\n    val_image = cv2.imread(path, cv2.IMREAD_COLOR)\n    val_image = val_image[:,:,::-1]\n    if width is not None and height is not None:\n        val_image = cv2.resize(val_image, (width, height))\n    if data_format == \'channels_first\':\n        val_image = val_image.transpose([2, 0, 1])\n    return val_image / 255.0\n\n\ndef get_sample_images(w, h):\n    val_image = [\n        read_imgfile(\'./images/p1.jpg\', w, h),\n        read_imgfile(\'./images/p2.jpg\', w, h),\n        read_imgfile(\'./images/p3.jpg\', w, h),\n        read_imgfile(\'./images/golf.jpg\', w, h),\n        read_imgfile(\'./images/hand1.jpg\', w, h),\n        read_imgfile(\'./images/hand2.jpg\', w, h),\n        read_imgfile(\'./images/apink1_crop.jpg\', w, h),\n        read_imgfile(\'./images/ski.jpg\', w, h),\n        read_imgfile(\'./images/apink2.jpg\', w, h),\n        read_imgfile(\'./images/apink3.jpg\', w, h),\n        read_imgfile(\'./images/handsup1.jpg\', w, h),\n        read_imgfile(\'./images/p3_dance.png\', w, h),\n    ]\n    return val_image\n\n\ndef load_graph(model_file):\n    """"""Load a freezed graph from file.""""""\n    graph_def = tf.GraphDef()\n    with open(model_file, ""rb"") as f:\n        graph_def.ParseFromString(f.read())\n\n    graph = tf.Graph()\n    with graph.as_default():\n        tf.import_graph_def(graph_def)\n    return graph\n\n\ndef get_op(graph, name):\n    return graph.get_operation_by_name(\'import/%s\' % name).outputs[0]\n\n\nclass Profiler(object):\n\n    def __init__(self):\n        self.count = dict()\n        self.total = dict()\n\n    def __del__(self):\n        if self.count:\n            self.report()\n\n    def report(self):\n        sorted_costs = sorted([(t, name) for name, t in self.total.items()])\n        sorted_costs.reverse()\n        names = [name for _, name in sorted_costs]\n        hr = \'-\' * 80\n        print(hr)\n        print(\'%-12s %-12s %-12s %s\' % (\'tot (s)\', \'count\', \'mean (ms)\', \'name\'))\n        print(hr)\n        for name in names:\n            tot, cnt = self.total[name], self.count[name]\n            mean = tot / cnt\n            print(\'%-12f %-12d %-12f %s\' % (tot, cnt, mean * 1000, name))\n\n    def __call__(self, name, duration):\n        if name in self.count:\n            self.count[name] += 1\n            self.total[name] += duration\n        else:\n            self.count[name] = 1\n            self.total[name] = duration\n\n\n_default_profiler = Profiler()\n\n\ndef measure(f, name=None):\n    if not name:\n        name = f.__name__\n    t0 = time.time()\n    result = f()\n    duration = time.time() - t0\n    _default_profiler(name, duration)\n    return result\n\n\ndef draw_humans(npimg, humans):\n    npimg = np.copy(npimg)\n    image_h, image_w = npimg.shape[:2]\n    centers = {}\n    for human in humans:\n        # draw point\n        for i in range(CocoPart.Background.value):\n            if i not in human.body_parts.keys():\n                continue\n\n            body_part = human.body_parts[i]\n            center = (int(body_part.x * image_w + 0.5), int(body_part.y * image_h + 0.5))\n            centers[i] = center\n            cv2.circle(npimg, center, 3, CocoColors[i], thickness=3, lineType=8, shift=0)\n\n        # draw line\n        for pair_order, pair in enumerate(CocoPairsRender):\n            if pair[0] not in human.body_parts.keys() or pair[1] not in human.body_parts.keys():\n                continue\n            cv2.line(npimg, centers[pair[0]], centers[pair[1]], CocoColors[pair_order], 3)\n\n    return npimg\n\n\ndef plot_humans(image, heatMat, pafMat, humans, name):\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    a = fig.add_subplot(2, 3, 1)\n\n    plt.imshow(draw_humans(image, humans))\n\n    a = fig.add_subplot(2, 3, 2)\n    tmp = np.amax(heatMat[:, :, :-1], axis=2)\n    plt.imshow(tmp, cmap=plt.cm.gray, alpha=0.5)\n    plt.colorbar()\n\n    tmp2 = pafMat.transpose((2, 0, 1))\n    tmp2_odd = np.amax(np.absolute(tmp2[::2, :, :]), axis=0)\n    tmp2_even = np.amax(np.absolute(tmp2[1::2, :, :]), axis=0)\n\n    a = fig.add_subplot(2, 3, 4)\n    a.set_title(\'Vectormap-x\')\n    plt.imshow(tmp2_odd, cmap=plt.cm.gray, alpha=0.5)\n    plt.colorbar()\n\n    a = fig.add_subplot(2, 3, 5)\n    a.set_title(\'Vectormap-y\')\n    plt.imshow(tmp2_even, cmap=plt.cm.gray, alpha=0.5)\n    plt.colorbar()\n    mkpath(\'vis\')\n    plt.savefig(\'vis/result-%s.png\' % name)\n\n\ndef rename_tensor(x, name):\n    # FIXME: use tf.identity(x, name=name) doesn\'t work\n    new_shape = []\n    for d in x.shape:\n        try:\n            d = int(d)\n        except:\n            d = -1\n        new_shape.append(d)\n    return tf.reshape(x, new_shape, name=name)\n\ndef tf_repeat(tensor, repeats):\n    """"""\n    Args:\n\n    input: A Tensor. 1-D or higher.\n    repeats: A list. Number of repeat for each dimension, length must be the same as the number of dimensions in input\n\n    Returns:\n\n    A Tensor. Has the same type as input. Has the shape of tensor.shape * repeats\n    """"""\n    expanded_tensor = tf.expand_dims(tensor, -1)\n    multiples = [1] + repeats\n    tiled_tensor = tf.tile(expanded_tensor, multiples=multiples)\n    repeated_tesnor = tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n    return repeated_tesnor\n\ndef init_log(config):\n    logging.basicConfig(filename=config.log.log_path,filemode=""a"",level=logging.INFO)\n\ndef log(msg):\n    logging.log(level=logging.INFO,msg=msg)\n    print(msg)\n'"
Hyperpose/Model/human.py,0,"b'import cv2\r\n\r\nclass Human:\r\n    """"""\r\n    body_parts: list of BodyPart\r\n    """"""\r\n\r\n    def __init__(self,parts,limbs,colors):\r\n        self.local_id=-1\r\n        self.global_id=-1\r\n        self.parts=parts\r\n        self.limbs=limbs\r\n        self.colors=colors\r\n        self.body_parts = {}\r\n        self.score = 0.0\r\n        self.bbx=None\r\n        self.area=None\r\n    \r\n    def get_global_id(self):\r\n        return int(self.global_id)\r\n    \r\n    def get_score(self):\r\n        for part_idx in self.body_parts.keys():\r\n            body_part=self.body_parts[part_idx]\r\n            self.score+=body_part.score\r\n        self.score=self.score/len(self.body_parts.keys())\r\n        return float(self.score)\r\n    \r\n    def get_partnum(self):\r\n        return len(self.body_parts.keys())\r\n    \r\n    def get_bbx(self):\r\n        min_x,min_y=10000,10000\r\n        max_x,max_y=-1,-1\r\n        for body_part_idx in self.body_parts.keys():\r\n            body_part=self.body_parts[body_part_idx]\r\n            x=body_part.x\r\n            y=body_part.y\r\n            min_x=min(x,min_x)\r\n            min_y=min(y,min_y)\r\n            max_x=max(x,max_x)\r\n            max_y=max(y,max_y)\r\n        center_x=(min_x+max_x)/2\r\n        center_y=(min_y+max_y)/2\r\n        h=max_y-min_y\r\n        w=max_x-min_x\r\n        self.bbx=[center_x,center_y,w,h]\r\n        return [center_x,center_y,w,h]\r\n    \r\n    def get_area(self):\r\n        bbx=self.get_bbx()\r\n        self.area=float(bbx[2]*bbx[3])\r\n        return self.area \r\n    \r\n    def scale(self,scale_w,scale_h):\r\n        for part_idx in self.body_parts.keys():\r\n            body_part=self.body_parts[part_idx]\r\n            body_part.x=body_part.x*scale_w\r\n            body_part.y=body_part.y*scale_h\r\n            body_part.w=body_part.w*scale_w\r\n            body_part.h=body_part.h*scale_h\r\n    \r\n    def draw_human(self,img):\r\n        for part_idx in self.body_parts.keys():\r\n            body_part=self.body_parts[part_idx]\r\n            x=body_part.x\r\n            y=body_part.y\r\n            color=self.colors[part_idx]\r\n            img=cv2.circle(img,(int(x),int(y)),radius=6,color=color,thickness=-1)\r\n        line_color=(255,0,0)\r\n        for limb in self.limbs:\r\n            src_part_idx,dst_part_idx=limb\r\n            if(src_part_idx==2 and dst_part_idx==16):\r\n                continue\r\n            elif(src_part_idx==5 and dst_part_idx==17):\r\n                continue\r\n            if((src_part_idx in self.body_parts) and (dst_part_idx in self.body_parts)):\r\n                src_body_part=self.body_parts[src_part_idx]\r\n                src_x,src_y=int(src_body_part.x),int(src_body_part.y)\r\n                dst_body_part=self.body_parts[dst_part_idx]\r\n                dst_x,dst_y=int(dst_body_part.x),int(dst_body_part.y)\r\n                img=cv2.line(img,(src_x,src_y),(dst_x,dst_y),color=line_color,thickness=3)\r\n        return img\r\n    \r\n    def print(self):\r\n        for part_idx in self.body_parts.keys():\r\n            body_part=self.body_parts[part_idx]\r\n            print(f""body-part:{self.parts(part_idx)} x:{body_part.x} y:{body_part.y} score:{body_part.score}"")\r\n        print()\r\n\r\n    def __str__(self):\r\n        return \' \'.join([str(x) for x in self.body_parts.values()])\r\n\r\n    def __repr__(self):\r\n        return self.__str__()\r\n\r\nclass BodyPart:\r\n    """"""\r\n    part_idx : part index(eg. 0 for nose)\r\n    x, y: coordinate of body part\r\n    score : confidence score\r\n    """"""\r\n\r\n    def __init__(self, parts, u_idx, part_idx, x, y, score, w=-1, h=-1 ):\r\n        self.parts=parts\r\n        self.u_idx=u_idx\r\n        self.part_idx = part_idx\r\n        self.x, self.y = x, y\r\n        self.w, self.h = w, h\r\n        self.score = score\r\n\r\n    def get_part_name(self):\r\n        return self.parts(self.part_idx)\r\n    \r\n    def get_x(self):\r\n        return float(self.x)\r\n    \r\n    def get_y(self):\r\n        return float(self.y)\r\n\r\n    def __str__(self):\r\n        return \'BodyPart:%d-(%.2f, %.2f) score=%.2f\' % (self.part_idx, self.x, self.y, self.score)\r\n\r\n    def __repr__(self):\r\n        return self.__str__()'"
Hyperpose/Dataset/mpii_dataset/__init__.py,0,b'from .dataset import init_dataset'
Hyperpose/Dataset/mpii_dataset/dataset.py,2,"b'import os\r\nimport cv2\r\nimport json\r\nimport numpy as np\r\nimport _pickle as cPickle\r\n\r\nfrom .define import MpiiPart,MpiiColor\r\nfrom .format import PoseInfo\r\nfrom .prepare import prepare_dataset\r\nfrom .visualize import visualize\r\nfrom .generate import get_train_dataset,get_eval_dataset\r\n\r\ndef init_dataset(config):\r\n    dataset=MPII_dataset(config)\r\n    return dataset\r\n\r\nclass MPII_dataset:\r\n    \'\'\'a dataset class specified for mpii dataset, provides uniform APIs\'\'\'\r\n    def __init__(self,config):\r\n        self.dataset_type=config.data.dataset_type\r\n        self.dataset_path=config.data.dataset_path\r\n        self.dataset_filter=config.data.dataset_filter\r\n        self.vis_dir=config.data.vis_dir\r\n        self.annos_path=None\r\n        self.images_path=None\r\n        self.parts=MpiiPart\r\n        self.colors=MpiiColor\r\n    \r\n    def visualize(self,vis_num=10):\r\n        \'\'\'visualize annotations of the train dataset\r\n\r\n        visualize the annotation points in the image to help understand and check annotation \r\n    \tthe visualized image will be saved in the ""data_vis_dir"" of the corresponding model directory(specified by model name).\r\n        the visualized annotations are from the train dataset.\r\n\r\n        Parameters\r\n        ----------\r\n        arg1 : Int\r\n            An integer indicates how many images with their annotations are going to be visualized.\r\n        \r\n        Returns\r\n        -------\r\n        None\r\n        \'\'\'\r\n        train_dataset=self.get_train_dataset()\r\n        visualize(self.vis_dir,vis_num,train_dataset,self.parts,self.colors)\r\n    \r\n    def get_parts(self):\r\n        return self.parts\r\n    \r\n    def get_colors(self):\r\n        return self.colors\r\n    \r\n    def get_dataset_type(self):\r\n        return self.dataset_type\r\n    \r\n    def prepare_dataset(self):\r\n        \'\'\'download,extract, and reformat the dataset\r\n        the official dataset is in .mat format, format it into json format automaticly.\r\n\r\n        Parameters\r\n        ----------\r\n        None\r\n\r\n        Returns\r\n        -------\r\n        None\r\n        \'\'\'\r\n        self.train_annos_path,self.val_annos_path,self.images_path=prepare_dataset(self.dataset_path)\r\n\r\n    def get_train_dataset(self):\r\n        \'\'\'provide uniform tensorflow dataset for training\r\n\r\n        return a tensorflow dataset based on MPII dataset, each iter contains two following object\r\n\r\n        1.image_path\r\n            a image path string encoded in utf-8 mode\r\n\r\n        2.target\r\n            bytes of a dict object encoded by _pickle, should be decode by ""_pickle.loads(target.numpy())""\r\n            the dict contains the following key-value pair:\r\n\r\n            2.1 key: ""obj"" \r\n                value: a list of keypoint annotations, each annotation corresponds to a person and is a list of\r\n                keypoints of the person, each keypoint is represent in the [x,y,v] mode, v=0 is unvisible and unanotated,\r\n                v=1 is unvisible but annotated, v=2 is visible and annotated.\r\n            2.2 key: ""mask"" \r\n                value: None(MPII doesn\'t provide any mask information)\r\n            2.3 key: ""bbx""\r\n                value: a list of bbx annotation of the image, each bbx is in the [x,y,w,h] form.\r\n\r\n        example use\r\n            1.use tensorflow map function to convert the target format\r\n            map_function(image_path,target):\r\n\r\n                image = tf.io.read_file(image_path)\r\n                image, target, mask=tf.py_function(defined_pyfunction, [image, target], [tf.float32, tf.float32, tf.float32])\r\n            2.process the target to your own format when in need in defined_pyfunction\r\n            defined_pyfunction(image, target):\r\n\r\n                target = _pickle.loads(target.numpy())\r\n                annos = target[""obj""]\r\n                mask = target[""mask""]\r\n                bbxs = target[""bbxs""]\r\n                processing\r\n            3. for image,target in train_dataset  \r\n\r\n        for more detail use, one can refer the training pipeline of models.\r\n\r\n        Parameters\r\n        ----------\r\n        None\r\n\r\n        Returns\r\n        -------\r\n        tensorflow dataset object \r\n            a unifrom formated tensorflow dataset object for training\r\n        \'\'\'\r\n        return get_train_dataset(self.images_path,self.train_annos_path,self.dataset_filter)\r\n    \r\n    def get_eval_dataset(self):\r\n        \'\'\'provide uniform tensorflow dataset for evaluating\r\n\r\n        return a tensorflow dataset based on MPII dataset, each iter contains two following object\r\n\r\n        1.image_path: \r\n            a image path string encoded in utf-8 mode\r\n        \r\n        2.image_id: \r\n            a image id string encoded in utf-8 mode\r\n        \r\n        example use:\r\n            for image_path,image_id in eval_dataset\r\n\r\n        for more detail use, one can refer the evaluating pipeline of models.\r\n\r\n        Parameters\r\n        ----------\r\n        None\r\n        \r\n        Returns\r\n        -------\r\n        tensorflow dataset object \r\n            a unifrom formated tensorflow dataset object for evaluating\r\n        \'\'\'\r\n        return get_eval_dataset(self.images_path,self.val_annos_path,self.dataset_filter)\r\n        \r\n    def official_eval(self,pd_json,eval_dir=f""./eval_dir""):\r\n        \'\'\'providing official evaluation of MPII dataset\r\n\r\n        output model metrics of PCHs on mpii evaluation dataset(split automaticly)\r\n\r\n        Parameters\r\n        ----------\r\n        arg1 : String\r\n            A string path of the json file in the same format of cocoeval annotation file(person_keypoints_val2017.json) \r\n            which contains predicted results. one can refer the evaluation pipeline of models for generation procedure of this json file.\r\n        arg2 : String\r\n            A string path indicates where the result json file which contains MPII PCH metrics of various keypoint saves.\r\n\r\n        Returns\r\n        -------\r\n        None\r\n        \'\'\'\r\n        #format result\r\n        pd_anns=pd_json[""annotations""]\r\n        pd_dict={}\r\n        for pd_ann in pd_anns:\r\n            image_id=pd_ann[""image_id""]\r\n            kpt_list=np.array(pd_ann[""keypoints""])\r\n            x=kpt_list[0::3][np.newaxis,...]\r\n            y=kpt_list[1::3][np.newaxis,...]\r\n            pd_ann[""keypoints""]=np.concatenate([x,y],axis=0)\r\n            if(image_id not in pd_dict):\r\n                pd_dict[image_id]=[]\r\n            pd_dict[image_id].append(pd_ann)\r\n        #format ground truth\r\n        metas=PoseInfo(self.images_path,self.val_annos_path,dataset_filter=self.dataset_filter).metas\r\n        gt_dict={}\r\n        for meta in metas:\r\n            gt_ann_list=meta.to_anns_list()\r\n            for gt_ann in gt_ann_list:\r\n                kpt_list=np.array(gt_ann[""keypoints""])\r\n                x=kpt_list[0::3][np.newaxis,...]\r\n                y=kpt_list[1::3][np.newaxis,...]\r\n                gt_ann[""keypoints""]=np.concatenate([x,y],axis=0)\r\n            gt_dict[meta.image_id]=gt_ann_list\r\n\r\n        all_pd_kpts=[]\r\n        all_gt_kpts=[]\r\n        all_gt_vis=[]\r\n        #match kpt into order for PCK calculation\r\n        for image_id in pd_dict.keys():\r\n            #sort pd_anns by score\r\n            pd_img_anns=np.array(pd_dict[image_id])\r\n            sort_idx=np.argsort([-pd_img_ann[""score""] for pd_img_ann in pd_img_anns])\r\n            pd_img_anns=pd_img_anns[sort_idx]\r\n            gt_img_anns=gt_dict[image_id]\r\n            #start to match pd and gt anns\r\n            match_pd_ids=np.full(shape=len(gt_img_anns),fill_value=-1)\r\n            for pd_id,pd_img_ann in enumerate(pd_img_anns):\r\n                pd_kpts=pd_img_ann[""keypoints""]\r\n                match_id=-1\r\n                match_dist=np.inf\r\n                for gt_id,gt_img_ann in enumerate(gt_img_anns):\r\n                    #gt person already matched\r\n                    if(match_pd_ids[gt_id]!=-1):\r\n                        continue\r\n                    gt_kpts=gt_img_ann[""keypoints""]\r\n                    gt_vis=gt_img_ann[""vis""]\r\n                    dist=np.mean(np.linalg.norm((pd_kpts-gt_kpts)*gt_vis,axis=0))\r\n                    if(dist<match_dist):\r\n                        match_dist=dist\r\n                        match_id=gt_id\r\n                if(match_id!=-1):\r\n                    match_pd_ids[match_id]=pd_id\r\n            #add kpts to the list by the matched order \r\n            for gt_id,gt_img_ann in enumerate(gt_img_anns):\r\n                all_gt_kpts.append(gt_img_ann[""keypoints""])\r\n                all_gt_vis.append(gt_img_ann[""vis""])\r\n                match_pd_id=match_pd_ids[gt_id]\r\n                if(match_pd_id!=-1):\r\n                    all_pd_kpts.append(pd_img_anns[match_pd_id][""keypoints""])\r\n                #not detected\r\n                else:\r\n                    all_pd_kpts.append(np.zeros_like(all_gt_kpts[-1]))\r\n        #calculate pchk\r\n        #shape kpts 2*n_pos*val_num\r\n        #shape vis n_pos*val_num\r\n        #shape all_dist n_pos*val_num\r\n        #shape headsize val_num\r\n        all_pd_kpts=np.array(all_pd_kpts).transpose([1,2,0])\r\n        all_gt_kpts=np.array(all_gt_kpts).transpose([1,2,0])\r\n        all_gt_vis=np.array(all_gt_vis).transpose([1,0])\r\n        all_gt_headsize=np.linalg.norm(all_gt_kpts[:,MpiiPart.Headtop.value,:]-all_gt_kpts[:,MpiiPart.UpperNeck.value,:],axis=0)\r\n        all_dist=np.linalg.norm(all_pd_kpts-all_gt_kpts,axis=0)/all_gt_headsize\r\n        jnt_vis_num=np.sum(all_gt_vis,axis=1)\r\n        PCKh=100.0*np.sum(all_dist<=0.5,axis=1)/jnt_vis_num\r\n        #calculate pchk_all\r\n        rng = np.arange(0, 0.5+0.01, 0.01)\r\n        pckAll = np.zeros((len(rng), 16))\r\n        for r in range(0,len(rng)):\r\n            threshold=rng[r]\r\n            pckAll[r]=100.0*np.sum(all_dist<=threshold,axis=1)/jnt_vis_num\r\n        #calculate mean\r\n        PCKh_mask = np.ma.array(PCKh, mask=False)\r\n        PCKh_mask.mask[6:8] = True\r\n\r\n        jnt_count = np.ma.array(jnt_vis_num, mask=False)\r\n        jnt_count.mask[6:8] = True\r\n        jnt_ratio = jnt_count / np.sum(jnt_count).astype(np.float64)\r\n        \r\n        result_dict={\r\n            ""Head"":     PCKh[MpiiPart.Headtop.value],\r\n            ""Shoulder"": 0.5*(PCKh[MpiiPart.LShoulder.value]+PCKh[MpiiPart.RShoulder.value]),\r\n            ""Elbow"":    0.5*(PCKh[MpiiPart.LElbow.value]+PCKh[MpiiPart.RElbow.value]),\r\n            ""Wrist"":    0.5*(PCKh[MpiiPart.LWrist.value]+PCKh[MpiiPart.RWrist.value]),\r\n            ""Hip"":      0.5*(PCKh[MpiiPart.LHip.value]+PCKh[MpiiPart.RHip.value]),\r\n            ""Knee"":     0.5*(PCKh[MpiiPart.LKnee.value]+PCKh[MpiiPart.RKnee.value]),\r\n            ""Ankle"":    0.5*(PCKh[MpiiPart.LAnkle.value]+PCKh[MpiiPart.RAnkle.value]),\r\n            ""Mean"":     np.sum(PCKh_mask*jnt_ratio),\r\n            ""Mean@0.1"": np.sum(pckAll[11,:]*jnt_ratio)\r\n        }\r\n        print(""evaluation result-PCKh:"")\r\n        for key in result_dict.keys():\r\n            print(f""{key}: {result_dict[key]}"")\r\n        result_path=os.path.join(eval_dir,""result.json"")\r\n        json.dump(result_dict,open(result_path,""w""))\r\n        return result_dict\r\n\r\n\r\n\r\n    \r\n'"
Hyperpose/Dataset/mpii_dataset/define.py,0,"b'from enum import Enum\r\n\r\nclass MpiiPart(Enum):\r\n    RAnkle = 0\r\n    RKnee = 1\r\n    RHip = 2\r\n    LHip = 3\r\n    LKnee = 4\r\n    LAnkle = 5\r\n    Pelvis=6\r\n    Thorax=7\r\n    UpperNeck=8\r\n    Headtop=9\r\n    RWrist =10\r\n    RElbow =11\r\n    RShoulder =12\r\n    LShoulder =13\r\n    LElbow = 14\r\n    LWrist = 15\r\n\r\n    @staticmethod\r\n    def from_coco(human):\r\n        from ..mscoco_dataset.define import CocoPart\r\n        t = [\r\n            (MpiiPart.Head, CocoPart.Nose),\r\n            (MpiiPart.Neck, CocoPart.Neck),\r\n            (MpiiPart.RShoulder, CocoPart.RShoulder),\r\n            (MpiiPart.RElbow, CocoPart.RElbow),\r\n            (MpiiPart.RWrist, CocoPart.RWrist),\r\n            (MpiiPart.LShoulder, CocoPart.LShoulder),\r\n            (MpiiPart.LElbow, CocoPart.LElbow),\r\n            (MpiiPart.LWrist, CocoPart.LWrist),\r\n            (MpiiPart.RHip, CocoPart.RHip),\r\n            (MpiiPart.RKnee, CocoPart.RKnee),\r\n            (MpiiPart.RAnkle, CocoPart.RAnkle),\r\n            (MpiiPart.LHip, CocoPart.LHip),\r\n            (MpiiPart.LKnee, CocoPart.LKnee),\r\n            (MpiiPart.LAnkle, CocoPart.LAnkle),\r\n        ]\r\n\r\n        pose_2d_mpii = []\r\n        visibilty = []\r\n        for _, coco in t:\r\n            if coco.value not in human.body_parts.keys():\r\n                pose_2d_mpii.append((0, 0))\r\n                visibilty.append(False)\r\n                continue\r\n            pose_2d_mpii.append((human.body_parts[coco.value].x, human.body_parts[coco.value].y))\r\n            visibilty.append(True)\r\n        return pose_2d_mpii, visibilty\r\n\r\nMpiiColor=[[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\r\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\r\n              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]'"
Hyperpose/Dataset/mpii_dataset/format.py,0,"b'import os\r\nimport json\r\nimport numpy as np\r\nimport scipy\r\nfrom scipy import io\r\n\r\nclass MPIIMeta:\r\n    def __init__(self,image_path,annos_list):\r\n        #print(f""test get meta with img_path:{image_path} annos_list:{annos_list}\\n\\n"")\r\n        image_name=os.path.basename(image_path)\r\n        self.image_id=int(image_name[:image_name.index(""."")])\r\n        self.image_path=image_path\r\n        self.n_pos=16\r\n        self.annos_list=annos_list\r\n        self.headbbx_list=[]\r\n        self.scale_list=[]\r\n        self.center_list=[]\r\n        self.joint_list=[]\r\n        for anno in self.annos_list:\r\n            #head bbx\r\n            x1,y1,x2,y2=anno[""x1""],anno[""y1""],anno[""x2""],anno[""y2""]\r\n            center_x=(x1+x2)/2\r\n            center_y=(y1+y2)/2\r\n            w,h=x2-x1,y2-y1\r\n            headbbx=np.array([center_x,center_y,w,h]).astype(np.float32)\r\n            self.headbbx_list.append(headbbx)\r\n            #scale\r\n            scale=anno[""scale""]\r\n            self.scale_list.append(np.float32(scale*200.0))\r\n            #pos\r\n            pos_x,pos_y=anno[""pos_x""],anno[""pos_y""]\r\n            pos=np.array([pos_x,pos_y]).astype(np.float32)\r\n            self.center_list.append(pos)\r\n            #kpts\r\n            kpts=[]\r\n            for kpt_id in range(0,self.n_pos):\r\n                x,y,v=anno[""kpts""][str(kpt_id)]\r\n                kpts+=[x,y,v]\r\n            self.joint_list.append(np.array(kpts))\r\n    \r\n    def to_anns_list(self):\r\n        anns_list=[]\r\n        for headbbx,scale,center,joints in zip(self.headbbx_list,self.scale_list,self.center_list,self.joint_list):\r\n            ann_dict={}\r\n            ann_dict[""headbbxs""]=headbbx\r\n            ann_dict[""scales""]=scale\r\n            ann_dict[""centers""]=center\r\n            vis=joints[2::3]\r\n            ann_dict[""keypoints""]=joints\r\n            ann_dict[""vis""]=vis\r\n            anns_list.append(ann_dict)\r\n        return anns_list\r\n\r\nclass PoseInfo:\r\n    def __init__(self,image_dir,annos_path,dataset_filter=None):\r\n        self.metas=[]\r\n        self.n_pos=16\r\n        self.image_dir=image_dir\r\n        self.annos_path=annos_path\r\n        self.get_image_annos()\r\n        if(dataset_filter!=None):\r\n            filter_metas=[]\r\n            for meta in self.metas:\r\n                if(dataset_filter(meta)==True):\r\n                    filter_metas.append(meta)\r\n            self.metas=filter_metas\r\n    \r\n    def get_image_annos(self):\r\n        json_dict=json.load(open(self.annos_path,""r""))\r\n        for image_path in json_dict.keys():\r\n            annos_list=json_dict[image_path]\r\n            self.metas.append(MPIIMeta(os.path.join(self.image_dir,image_path),annos_list))\r\n\r\n    def get_image_id_list(self):\r\n        image_id_list=[]\r\n        for meta in self.metas:\r\n            image_id_list.append(meta.image_id)\r\n        return image_id_list\r\n\r\n    def get_image_list(self):\r\n        image_list=[]\r\n        for meta in self.metas:\r\n            image_list.append(meta.image_path)\r\n        return image_list\r\n    \r\n    def get_headbbx_list(self):\r\n        headbbx_list=[]\r\n        for meta in self.metas:\r\n            headbbx_list.append(meta.headbbx_list)\r\n        return headbbx_list\r\n    \r\n    def get_scale_list(self):\r\n        scale_list=[]\r\n        for meta in self.metas:\r\n            scale_list.append(meta.scale_list)\r\n        return scale_list\r\n    \r\n    def get_center_list(self):\r\n        pos_list=[]\r\n        for meta in self.metas:\r\n            pos_list.append(meta.center_list)\r\n        return pos_list \r\n\r\n    def get_joint_list(self):\r\n        joint_list=[]\r\n        for meta in self.metas:\r\n            joint_list.append(meta.joint_list)\r\n        return joint_list\r\n\r\n\r\n\r\ndef generate_json(mat_path,is_test=False):\r\n    #utils\r\n    def check_exist(mat_obj,field_name):\r\n        if(field_name in mat_obj._fieldnames):\r\n            if(0 not in mat_obj.__dict__[field_name].shape):\r\n                return True\r\n        return False\r\n\r\n    #init anno_dict in case for lost annotation\r\n    def get_init_dict():\r\n        anno_dict={}\r\n        #init person info\r\n        anno_names=[""x1"",""y1"",""x2"",""y2"",""scale"",""pos_x"",""pos_y""]\r\n        for anno_name in anno_names:\r\n            anno_dict[anno_name]=-1.0\r\n        #init kpts\r\n        anno_dict[""kpts""]={}\r\n        for kpt_id in range(0,16):\r\n            anno_dict[""kpts""][kpt_id]=[-1000.0,-1000.0,-1]\r\n        return anno_dict\r\n\r\n    json_dict={}\r\n    mat=io.loadmat(mat_path,struct_as_record=False)\r\n    data_obj=mat[""RELEASE""][0][0]\r\n    anno_list=data_obj.__dict__[""annolist""][0]\r\n    train_marks=data_obj.__dict__[""img_train""][0]\r\n    if(is_test):\r\n        target_idx=np.where(train_marks==0)\r\n    else:\r\n        target_idx=np.where(train_marks==1)\r\n    target_anno_list=anno_list[target_idx]\r\n    for anno in target_anno_list:\r\n        #get image path\r\n        image_obj=anno.__dict__[""image""][0][0]\r\n        image_path=image_obj.__dict__[""name""][0]\r\n        #get annotations\r\n        annos_list=[]\r\n        if(len(anno.__dict__[""annorect""])==0):\r\n            continue\r\n        anno_objs=anno.__dict__[""annorect""][0]\r\n        #handle each person annotation\r\n        for anno_obj in anno_objs:\r\n            anno_dict=get_init_dict()\r\n            #get bbx and scale\r\n            for exp_name in [""x1"",""y1"",""x2"",""y2"",""scale""]:\r\n                if(check_exist(anno_obj,exp_name)):\r\n                    anno_dict[exp_name]=float(anno_obj.__dict__[exp_name][0][0])\r\n            #get pose\r\n            if(check_exist(anno_obj,""objpos"")):\r\n                pos_obj=anno_obj.__dict__[""objpos""][0][0]\r\n                if(check_exist(pos_obj,""x"")):\r\n                    anno_dict[""pos_x""]=float(pos_obj.__dict__[""x""][0][0])\r\n                if(check_exist(pos_obj,""y"")):\r\n                    anno_dict[""pos_y""]=float(pos_obj.__dict__[""y""][0][0])   \r\n            #get kpts\r\n            if(check_exist(anno_obj,""annopoints"")):\r\n                anno_points_obj=anno_obj.__dict__[""annopoints""][0][0]\r\n                if(check_exist(anno_points_obj,""point"")):\r\n                    kpt_objs=anno_points_obj.__dict__[""point""][0]\r\n                    for kpt_obj in kpt_objs:\r\n                        if(check_exist(kpt_obj,""id"")):\r\n                            kpt_id=int(kpt_obj.__dict__[""id""][0][0])\r\n                        if(check_exist(kpt_obj,""x"")):\r\n                            kpt_x=float(kpt_obj.__dict__[""x""][0][0])\r\n                        if(check_exist(kpt_obj,""y"")):\r\n                            kpt_y=float(kpt_obj.__dict__[""y""][0][0])\r\n                        if(check_exist(kpt_obj,""is_visible"")):\r\n                            kpt_v=kpt_obj.__dict__[""is_visible""]\r\n                        if(type(kpt_v)==float):\r\n                            kpt_v=float(kpt_v)\r\n                        elif(type(kpt_v)==np.ndarray):\r\n                            if(len(kpt_v.shape)==1):\r\n                                kpt_v=float(kpt_v[0])\r\n                            elif(len(kpt_v.shape)==2):\r\n                                kpt_v=float(kpt_v[0][0])\r\n                        #plus 1 for the difference between visible definition of coco and mpii\r\n                        anno_dict[""kpts""][int(kpt_id)]=[kpt_x,kpt_y,kpt_v+1]\r\n            annos_list.append(anno_dict)\r\n        if(image_path not in json_dict):\r\n            json_dict[image_path]=[]\r\n        json_dict[image_path]+=annos_list\r\n    return json_dict'"
Hyperpose/Dataset/mpii_dataset/generate.py,2,"b'import tensorflow as tf\r\nimport tensorlayer as tl\r\nimport _pickle as cPickle\r\nfrom .format import MPIIMeta,PoseInfo\r\n\r\ndef get_train_dataset(train_images_path,train_annos_path,dataset_filter=None):\r\n    #prepare data\r\n    mpii_data=PoseInfo(train_images_path,train_annos_path,dataset_filter=dataset_filter)\r\n    img_file_list=mpii_data.get_image_list()\r\n    objs_list=mpii_data.get_joint_list()\r\n    bbx_list=mpii_data.get_headbbx_list()\r\n    #assemble data\r\n    train_img_file_list=img_file_list\r\n    train_target_list=[]\r\n    for objs,bbx in zip(objs_list,bbx_list):\r\n        train_target_list.append({\r\n            ""obj"":objs,\r\n            ""mask"":None,\r\n            ""bbx"":bbx\r\n        })\r\n    #tensorflow data pipeline\r\n    def generator():\r\n        """"""TF Dataset generator.""""""\r\n        assert len(train_img_file_list) == len(train_target_list)\r\n        for _input, _target in zip(train_img_file_list, train_target_list):\r\n            yield _input.encode(\'utf-8\'), cPickle.dumps(_target)\r\n\r\n    train_dataset = tf.data.Dataset.from_generator(generator, output_types=(tf.string, tf.string))\r\n    return train_dataset\r\n\r\ndef get_eval_dataset(eval_images_path,eval_annos_path,dataset_filter=None):\r\n    #prepare data\r\n    mpii_data=PoseInfo(eval_images_path,eval_annos_path,dataset_filter=dataset_filter)\r\n    img_file_list=mpii_data.get_image_list()\r\n    img_id_list=mpii_data.get_image_id_list()\r\n    #assemble data\r\n    test_img_file_list=img_file_list\r\n    test_img_id_list=img_id_list\r\n    #tensorflow data pipeline\r\n    def generator():\r\n        """"""TF Dataset generator.""""""\r\n        assert len(test_img_file_list)==len(test_img_id_list)\r\n        for img_file,img_id in zip(test_img_file_list,test_img_id_list):\r\n            yield img_file.encode(""utf-8""),img_id\r\n\r\n    test_dataset = tf.data.Dataset.from_generator(generator,output_types=(tf.string,tf.int32))\r\n    return test_dataset\r\n'"
Hyperpose/Dataset/mpii_dataset/prepare.py,0,"b'\r\nimport os\r\nimport json\r\nimport tensorlayer as tl\r\nfrom tensorlayer import logging\r\nfrom tensorlayer.files.utils import (del_file, folder_exists, maybe_download_and_extract)\r\nfrom ..common import unzip\r\nfrom .format import generate_json\r\n\r\ndef prepare_dataset(dataset_path):\r\n    path=os.path.join(dataset_path,""mpii"")\r\n    #prepare annotation\r\n    annos_dir=os.path.join(path,""mpii_human_pose_v1_u12_2"")\r\n    mat_annos_path=os.path.join(annos_dir,""mpii_human_pose_v1_u12_1.mat"")\r\n    if(os.path.exists(mat_annos_path) is False):\r\n        logging.info(""    downloading annotations"")\r\n        os.system(f""wget https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12_2.zip -P {path}"")\r\n        unzip(os.path.join(path,""mpii_human_pose_v1_u12_2.zip""),path)\r\n        del_file(os.path.join(path,""mpii_human_pose_v1_u12_2.zip""))\r\n    else:\r\n        logging.info(""    annotations exist"")\r\n    #prepare json annotation\r\n    train_annos_path=os.path.join(annos_dir,""mpii_human_pose_train.json"")\r\n    val_annos_path=os.path.join(annos_dir,""mpii_human_pose_val.json"")\r\n    if((not os.path.exists(train_annos_path)) or (not os.path.exists(val_annos_path))):\r\n        print(""    json annotation doesn\'t exist, generaing json annotations..."")\r\n        json_annos=generate_json(mat_annos_path)\r\n        #left 3000 images for val\r\n        split_val_ids=None\r\n        split_path=os.path.join(annos_dir,""split_val.json"")\r\n        if(os.path.exists(split_path)):\r\n            print(""using preset split val set"")\r\n            split_val_ids=json.load(open(split_path,""r""))\r\n        else:\r\n            print(""using the first 3000 annotations as default val dataset"")\r\n        train_annos={}\r\n        val_annos={}\r\n        for annos_num,image_path in enumerate(json_annos.keys()):\r\n            if(split_val_ids!=None):\r\n                if(image_path in split_val_ids):\r\n                    val_annos[image_path]=json_annos[image_path]\r\n                else:\r\n                    train_annos[image_path]=json_annos[image_path]\r\n            else:\r\n                if(annos_num<3000):\r\n                    val_annos[image_path]=json_annos[image_path]\r\n                else:\r\n                    train_annos[image_path]=json_annos[image_path]\r\n        print(f""generated train annos:{len(train_annos.keys())}"")\r\n        print(f""generated val annos:{len(val_annos.keys())}"")\r\n        train_file=open(train_annos_path,""w"")\r\n        json.dump(train_annos,train_file)\r\n        train_file.close()\r\n        val_file=open(val_annos_path,""w"")\r\n        json.dump(val_annos,val_file)\r\n        val_file.close()\r\n        print(""    json annotation generation finished!"")\r\n\r\n    #prepare image\r\n    images_path=os.path.join(path,""images"")\r\n    if(os.path.exists(images_path) is False):\r\n        logging.info(""    downloading images"")\r\n        os.system(f""wget https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz -P {path}"")\r\n        os.system(f""tar -xvf {os.path.join(path,\'mpii_human_pose_v1.tar.gz\')}"")\r\n        del_file(os.path.join(path,""mpii_human_pose_v1.tar.gz""))\r\n    else:\r\n        logging.info(""    images exist"")\r\n    return train_annos_path,val_annos_path,images_path'"
Hyperpose/Dataset/mpii_dataset/utils.py,0,"b'import cv2\r\nimport numpy as np\r\n\r\ndef get_affine_transform(center,\r\n                         scale,\r\n                         rot,\r\n                         output_size,\r\n                         shift=np.array([0, 0], dtype=np.float32),\r\n                         inv=0):\r\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\r\n        print(scale)\r\n        scale = np.array([scale, scale])\r\n\r\n    scale_tmp = scale * 200.0\r\n    src_w = scale_tmp[0]\r\n    dst_w = output_size[0]\r\n    dst_h = output_size[1]\r\n\r\n    rot_rad = np.pi * rot / 180\r\n    src_dir = get_dir([0, src_w * -0.5], rot_rad)\r\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\r\n\r\n    src = np.zeros((3, 2), dtype=np.float32)\r\n    dst = np.zeros((3, 2), dtype=np.float32)\r\n    src[0, :] = center + scale_tmp * shift\r\n    src[1, :] = center + src_dir + scale_tmp * shift\r\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\r\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\r\n\r\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\r\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\r\n\r\n    if inv:\r\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\r\n    else:\r\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\r\n\r\n    return trans\r\n\r\n\r\ndef affine_transform(pt, t):\r\n    new_pt = np.array([pt[0], pt[1], 1.]).T\r\n    new_pt = np.dot(t, new_pt)\r\n    return new_pt[:2]\r\n\r\n\r\ndef get_3rd_point(a, b):\r\n    direct = a - b\r\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\r\n\r\n\r\ndef get_dir(src_point, rot_rad):\r\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\r\n\r\n    src_result = [0, 0]\r\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\r\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\r\n\r\n    return src_result'"
Hyperpose/Dataset/mpii_dataset/visualize.py,0,"b'import cv2\r\nimport numpy as np\r\nimport _pickle as cpickle\r\nimport matplotlib.pyplot as plt\r\n\r\ndef visualize(vis_dir,vis_num,dataset,parts,colors):\r\n    for vis_id,(img_file,annos) in enumerate(dataset,start=1):\r\n        if(vis_id>=vis_num):\r\n            break\r\n        image=cv2.cvtColor(cv2.imread(img_file.numpy().decode(""utf-8""),cv2.IMREAD_COLOR),cv2.COLOR_BGR2RGB)\r\n        annos=cpickle.loads(annos.numpy())\r\n        ori_img=image\r\n        vis_img=image.copy()\r\n        kpts_list=annos[0]\r\n        for kpts in kpts_list:\r\n            x,y,v=kpts[0::3],kpts[1::3],kpts[2::3]\r\n            for part_idx in range(0,len(parts)):\r\n                if(x[part_idx]<0 or y[part_idx]<0):\r\n                    continue\r\n                color=colors[part_idx]\r\n                vis_img=cv2.circle(vis_img,(int(x[part_idx]),int(y[part_idx])),radius=6,color=color,thickness=-1)\r\n        fig=plt.figure(figsize=(8,8))\r\n        a=fig.add_subplot(1,2,1)\r\n        a.set_title(""original image"")\r\n        plt.imshow(ori_img)\r\n        a=fig.add_subplot(1,2,2)\r\n        a.set_title(""visualized image"")\r\n        plt.imshow(vis_img)\r\n        plt.savefig(f""{vis_dir}/{vis_id}_vis_mpii.png"")\r\n        plt.close(\'all\')'"
Hyperpose/Dataset/mscoco_dataset/__init__.py,0,b'from .dataset import init_dataset'
Hyperpose/Dataset/mscoco_dataset/dataset.py,2,"b'import os\nimport cv2\nimport math\nimport json\nimport numpy as np\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom ..common import unzip\nfrom .prepare import prepare_dataset\nfrom .visualize import visualize\nfrom .format import CocoMeta,PoseInfo\nfrom .define import CocoPart,CocoColor\nfrom .generate import get_train_dataset, get_eval_dataset\n\ndef init_dataset(config):\n    dataset=MSCOCO_dataset(config)\n    return dataset\n\nclass MSCOCO_dataset:\n    \'\'\'a dataset class specified for coco dataset, provides uniform APIs\'\'\'\n    def __init__(self,config):\n        self.dataset_type=config.data.dataset_type\n        self.dataset_version=config.data.dataset_version\n        self.dataset_path=config.data.dataset_path\n        self.dataset_filter=config.data.dataset_filter\n        self.vis_dir=config.data.vis_dir\n        self.train_imgs_path,self.train_anns_path=None,None\n        self.val_imgs_path,self.val_anns_path=None,None\n        self.test_imgs_path,self.test_anns_path=None,None\n        self.parts=CocoPart\n        self.colors=CocoColor\n    \n    def visualize(self,vis_num):\n        \'\'\'visualize annotations of the train dataset\n\n        visualize the annotation points in the image to help understand and check annotation \n    \tthe visualized image will be saved in the ""data_vis_dir"" of the corresponding model directory(specified by model name).\n        the visualized annotations are from the train dataset.\n\n        Parameters\n        ----------\n        arg1 : Int\n            An integer indicates how many images with their annotations are going to be visualized.\n        \n        Returns\n        -------\n        None\n        \'\'\'\n        \n        train_dataset=self.get_train_dataset()\n        visualize(self.vis_dir,vis_num,train_dataset,self.parts,self.colors)\n\n    def get_parts(self):\n        return self.parts\n    \n    def get_colors(self):\n        return self.colors\n    \n    def prepare_dataset(self):\n        \'\'\'download,extract, and reformat the dataset\n        the official format is in zip format, extract it into json files and image files. \n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        \'\'\'\n        self.train_imgs_path,self.train_anns_path,\\\n            self.val_imgs_path,self.val_anns_path,\\\n                self.test_imgs_path,self.test_anns_path=prepare_dataset(self.dataset_path,self.dataset_version)\n    \n    def get_dataset_type(self):\n        return self.dataset_type\n\n    def get_train_dataset(self):\n        \'\'\'provide uniform tensorflow dataset for training\n\n        return a tensorflow dataset based on COCO dataset, each iter contains two following object\n\n        1.image_path\n            a image path string encoded in utf-8 mode\n\n        2.target\n            bytes of a dict object encoded by _pickle, should be decode by ""_pickle.loads(target.numpy())""\n            the dict contains the following key-value pair:\n\n            2.1 key: ""obj"" \n                value: a list of keypoint annotations, each annotation corresponds to a person and is a list of\n                keypoints of the person, each keypoint is represent in the [x,y,v] mode, v=0 is unvisible and unanotated,\n                v=1 is unvisible but annotated, v=2 is visible and annotated.\n\n            2.2 key: ""mask"" \n                value: the mask object of the coco dataset, can be docoded in to binary map array by \n                pycocotools.coco.maskUtils.decode() function\n\n            2.3 key: ""bbx""\n                value: a list of bbx annotation of the image, each bbx is in the [x,y,w,h] form.\n\n        example use\n            1.use tensorflow map function to convert the target format\n            map_function(image_path,target):\n\n                image = tf.io.read_file(image_path)\n                image, target, mask=tf.py_function(defined_pyfunction, [image, target], [tf.float32, tf.float32, tf.float32])\n\n            2.process the target to your own format when in need in defined_pyfunction\n\n            defined_pyfunction(image, target)\n                target = _pickle.loads(target.numpy())\n                annos = target[""obj""]\n                mask = target[""mask""]\n                bbxs = target[""bbxs""]\n                processing\n            3. for image,target in train_dataset  \n\n        for more detail use, one can refer the training pipeline of models.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        tensorflow dataset object \n            a unifrom formated tensorflow dataset object for training\n        \'\'\'\n        return get_train_dataset(self.train_imgs_path,self.train_anns_path)\n\n    def get_eval_dataset(self):\n        \'\'\'provide uniform tensorflow dataset for evaluating\n\n        return a tensorflow dataset based on COCO dataset, each iter contains two following object\n\n        1.image_path\n            a image path string encoded in utf-8 mode\n        \n        2.image_id\n            a image id string encoded in utf-8 mode\n        \n        example use\n            for image_path,image_id in eval_dataset\n\n        for more detail use, one can refer the evaluating pipeline of models.\n\n        Parameters\n        ----------\n        None\n        \n        Returns\n        -------\n        tensorflow dataset object \n            a unifrom formated tensorflow dataset object for evaluating\n        \'\'\'\n        return get_eval_dataset(self.val_imgs_path,self.val_anns_path)\n\n    def official_eval(self,pd_json,eval_dir=f""./eval_dir""):\n        \'\'\'providing official evaluation of COCO dataset\n\n        using pycocotool.cocoeval class to perform official evaluation.\n        output model metrics of MAPs on coco evaluation dataset\n\n        Parameters\n        ----------\n        arg1 : String\n            A string path of the json file in the same format of cocoeval annotation file(person_keypoints_val2017.json) \n            which contains predicted results. one can refer the evaluation pipeline of models for generation procedure of this json file.\n        arg2 : String\n            A string path indicates where the json files of filtered intersection part of predict results and ground truth\n            the filtered prediction file is stored in eval_dir/pd_ann.json\n            the filtered ground truth file is stored in eval_dir/gt_ann.json\n\n        Returns\n        -------\n        None\n        \'\'\'\n\n        all_gt_json=json.load(open(self.val_anns_path,""r""))\n        all_gt_coco=COCO(self.val_anns_path)\n        gt_json={}\n        #filter the gt annos\n        image_ids=[]\n        category_ids=[]\n        pd_anns=pd_json[""annotations""]\n        for pd_ann in pd_anns:\n            image_ids.append(pd_ann[""image_id""])\n            category_ids.append(pd_ann[""category_id""])\n        image_ids=list(np.unique(image_ids))\n        category_ids=list(np.unique(category_ids))\n        gt_json[""info""]=all_gt_json[""info""]\n        gt_json[""licenses""]=all_gt_json[""licenses""]\n        gt_json[""categories""]=all_gt_json[""categories""]\n        gt_json[""images""]=all_gt_coco.loadImgs(ids=image_ids)\n        gt_json[""annotations""]=all_gt_coco.loadAnns(all_gt_coco.getAnnIds(imgIds=image_ids,catIds=category_ids))\n\n        #save result in json form\n        os.makedirs(eval_dir,exist_ok=True)\n\n        gt_json_path=f""{eval_dir}/gt_ann.json""\n        gt_json_file=open(gt_json_path,""w"")\n        json.dump(gt_json,gt_json_file)\n        gt_json_file.close()\n\n        pd_json_path=f""{eval_dir}/pd_ann.json""\n        pd_json_file=open(pd_json_path,""w"")\n        json.dump(pd_anns,pd_json_file)\n        pd_json_file.close()\n        #evaluating \n        print(f""evluating on total {len(image_ids)} images..."")\n        gt_coco=COCO(gt_json_path)\n        pd_coco=gt_coco.loadRes(pd_json_path)\n\n        \'\'\'\n        #debug\n        print(f""test result compare!:"")\n        for image_id in image_ids:\n            print(f""test image_{image_id}:"")\n            pd_anns=pd_coco.loadAnns(pd_coco.getAnnIds(imgIds=image_id))\n            print(f""pd_kpts:{np.array(pd_anns[0][\'keypoints\']).astype(np.int32)}"")\n            gt_anns=gt_coco.loadAnns(gt_coco.getAnnIds(imgIds=image_id))\n            print(f""gt_kpts:{np.array(gt_anns[0][\'keypoints\']).astype(np.int32)}"")\n            \n            print(f""test all_info_gt:"")\n            for gt_ann in gt_anns:\n                print(f""kpst:{gt_ann[\'keypoints\']}"")\n                print(f""bbxs:{gt_ann[\'bbox\']}"")\n                print()\n        \'\'\'\n\n        std_eval=COCOeval(cocoGt=gt_coco,cocoDt=pd_coco,iouType=""keypoints"")\n        std_eval.evaluate()\n        std_eval.accumulate()\n        std_eval.summarize()'"
Hyperpose/Dataset/mscoco_dataset/define.py,0,"b'from enum import Enum\r\nclass CocoPart(Enum):\r\n    Nose = 0\r\n    Leye= 1\r\n    Reye= 2\r\n    LEar=3\r\n    REar=4\r\n    LShoulder=5\r\n    RShoulder=6\r\n    LElbow=7\r\n    RElbow=8\r\n    LWrist=9\r\n    RWrist=10\r\n    LHip=11\r\n    RHip=12\r\n    LKnee=13\r\n    RKnee=14\r\n    LAnkle=15\r\n    RAnkle=16\r\n\r\nCocoColor = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\r\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\r\n              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]'"
Hyperpose/Dataset/mscoco_dataset/format.py,0,"b'import os\r\nimport numpy as np\r\nfrom pycocotools.coco import COCO\r\nfrom scipy.spatial.distance import cdist\r\n\r\n## read coco data\r\nclass CocoMeta:\r\n    """""" Be used in PoseInfo. """"""\r\n    def __init__(self, image_id, img_url, img_meta, kpts_infos, masks, bbxs, is_crowd):\r\n        self.image_id = image_id\r\n        self.img_url = img_url\r\n        self.img = None\r\n        self.height = int(img_meta[\'height\'])\r\n        self.width = int(img_meta[\'width\'])\r\n        self.masks = masks\r\n        self.bbx_list=bbxs\r\n        self.is_crowd=is_crowd\r\n\r\n        self.joint_list=[]\r\n        for kpts_info in kpts_infos:\r\n            if kpts_info.get(\'num_keypoints\', 0) == 0:\r\n                continue\r\n            kpts = np.array(kpts_info[\'keypoints\'])\r\n            self.joint_list.append(kpts)\r\n\r\nclass PoseInfo:\r\n    """""" Use COCO for pose estimation, returns images with people only. """"""\r\n\r\n    def __init__(self, image_base_dir, anno_path, with_mask=True,dataset_filter=None, eval=False):\r\n        self.metas = []\r\n        # self.data_dir = data_dir\r\n        # self.data_type = data_type\r\n        self.eval=eval\r\n        self.image_base_dir = image_base_dir\r\n        self.anno_path = anno_path\r\n        self.with_mask = with_mask\r\n        self.coco = COCO(self.anno_path)\r\n        self.get_image_annos()\r\n        self.image_list = os.listdir(self.image_base_dir)\r\n        if(dataset_filter!=None):\r\n            filter_metas=[]\r\n            for meta in self.metas:\r\n                if(dataset_filter(meta)==True):\r\n                    filter_metas.append(meta)\r\n            self.metas=filter_metas\r\n\r\n    @staticmethod\r\n    def get_keypoints(annos_info):\r\n        annolist = []\r\n        for anno in annos_info:\r\n            adjust_anno = {\'keypoints\': anno[\'keypoints\'], \'num_keypoints\': anno[\'num_keypoints\']}\r\n            annolist.append(adjust_anno)\r\n        return annolist\r\n\r\n    @staticmethod    \r\n    def get_bbxs(annos_info):\r\n        bbxlist=[]\r\n        for anno in annos_info:\r\n            bbxlist.append(anno[""bbox""])\r\n        return bbxlist\r\n\r\n    def get_image_annos(self):\r\n        """"""Read JSON file, and get and check the image list.\r\n        Skip missing images.\r\n        """"""\r\n        images_ids = self.coco.getImgIds()\r\n        len_imgs = len(images_ids)\r\n        for idx in range(len_imgs):\r\n\r\n            image_info = self.coco.loadImgs(images_ids[idx])[0]\r\n            image_path = os.path.join(self.image_base_dir, image_info[\'file_name\'])\r\n            # filter that some images might not in the list\r\n            if not os.path.exists(image_path):\r\n                print(""[skip] json annotation found, but cannot found image: {}"".format(image_path))\r\n                continue\r\n\r\n            annos_ids = self.coco.getAnnIds(imgIds=images_ids[idx])\r\n            annos_info = self.coco.loadAnns(annos_ids)\r\n            kpts_info = self.get_keypoints(annos_info)\r\n            bbxs=self.get_bbxs(annos_info)\r\n\r\n            #############################################################################\r\n            anns = annos_info\r\n            prev_center = []\r\n            masks = []\r\n            #check for crowd\r\n            is_crowd=False\r\n            for ann in anns:\r\n                if(""iscrowd"" in ann and ann[""iscrowd""]):\r\n                    is_crowd=True\r\n\r\n            # sort from the biggest person to the smallest one\r\n            if self.with_mask:\r\n                persons_ids = np.argsort([-a[\'area\'] for a in anns], kind=\'mergesort\')\r\n\r\n                for p_id in list(persons_ids):\r\n                    person_meta = anns[p_id]\r\n\r\n                    if person_meta[""iscrowd""]:\r\n                        is_crowd=True\r\n                        masks.append(self.coco.annToRLE(person_meta))\r\n                        continue\r\n\r\n                    # skip this person if parts number is too low or if\r\n                    # segmentation area is too small\r\n                    if person_meta[""num_keypoints""] < 5 or person_meta[""area""] < 32 * 32:\r\n                        masks.append(self.coco.annToRLE(person_meta))\r\n                        continue\r\n\r\n                    person_center = [\r\n                        person_meta[""bbox""][0] + person_meta[""bbox""][2] / 2,\r\n                        person_meta[""bbox""][1] + person_meta[""bbox""][3] / 2\r\n                    ]\r\n\r\n                    # skip this person if the distance to existing person is too small\r\n                    too_close = False\r\n                    for pc in prev_center:\r\n                        a = np.expand_dims(pc[:2], axis=0)\r\n                        b = np.expand_dims(person_center, axis=0)\r\n                        dist = cdist(a, b)[0]\r\n                        if dist < pc[2] * 0.3:\r\n                            too_close = True\r\n                            break\r\n\r\n                    if too_close:\r\n                        # add mask of this person. we don\'t want to show the network\r\n                        # unlabeled people\r\n                        masks.append(self.coco.annToRLE(person_meta))\r\n                        continue\r\n\r\n            ############################################################################\r\n            #eval accept all images\r\n            if(self.eval):\r\n                meta = CocoMeta(images_ids[idx], image_path, image_info, kpts_info, masks, bbxs, is_crowd)\r\n                self.metas.append(meta)\r\n            #train filter images\r\n            else:\r\n                total_keypoints = sum([ann.get(\'num_keypoints\', 0) for ann in annos_info])\r\n                if total_keypoints > 0:\r\n                    meta = CocoMeta(images_ids[idx], image_path, image_info, kpts_info, masks, bbxs, is_crowd)\r\n                    self.metas.append(meta)\r\n\r\n        print(""Overall get {} valid pose images from {} and {}"".format(\r\n            len(self.metas), self.image_base_dir, self.anno_path))\r\n\r\n    def load_images(self):\r\n        pass\r\n\r\n    def get_image_id_list(self):\r\n        img_id_list=[]\r\n        for meta in self.metas:\r\n            img_id_list.append(meta.image_id)\r\n        return img_id_list\r\n\r\n    def get_image_list(self):\r\n        img_list = []\r\n        for meta in self.metas:\r\n            img_list.append(meta.img_url)\r\n        return img_list\r\n\r\n    def get_joint_list(self):\r\n        joint_list = []\r\n        for meta in self.metas:\r\n            joint_list.append(meta.joint_list)\r\n        return joint_list\r\n\r\n    def get_mask(self):\r\n        mask_list = []\r\n        for meta in self.metas:\r\n            mask_list.append(meta.masks)\r\n        return mask_list\r\n    \r\n    def get_bbx_list(self):\r\n        bbx_list=[]\r\n        for meta in self.metas:\r\n            bbx_list.append(meta.bbx_list)\r\n        return bbx_list\r\n'"
Hyperpose/Dataset/mscoco_dataset/generate.py,2,"b'import tensorflow as tf\r\nimport _pickle as cPickle\r\nfrom .format import CocoMeta,PoseInfo\r\n\r\ndef get_pose_data_list(im_path, ann_path,dataset_filter=None):\r\n    """"""\r\n    train_im_path : image folder name\r\n    train_ann_path : coco json file name\r\n    """"""\r\n    print(""[x] Get pose data from {}"".format(im_path))\r\n    data = PoseInfo(im_path, ann_path, with_mask=True, dataset_filter=dataset_filter)\r\n    imgs_file_list = data.get_image_list()\r\n    objs_info_list = data.get_joint_list()\r\n    mask_list = data.get_mask()\r\n    bbx_list=data.get_bbx_list()\r\n    target_list=[]\r\n    for objs,mask,bbx in zip(objs_info_list,mask_list,bbx_list):\r\n        target_list.append({\r\n            ""obj"":objs,\r\n            ""mask"":mask,\r\n            ""bbx"":bbx\r\n        })\r\n    if len(imgs_file_list) != len(objs_info_list):\r\n        raise Exception(""number of images and annotations do not match"")\r\n    else:\r\n        print(""{} has {} images"".format(im_path, len(imgs_file_list)))\r\n    return imgs_file_list, target_list\r\n\r\ndef get_train_dataset(train_imgs_path,train_anns_path,dataset_filter=None):\r\n    # read coco training images contains valid people\r\n    train_imgs_file_list,train_target_list =get_pose_data_list(train_imgs_path, train_anns_path, dataset_filter=dataset_filter)\r\n    #tensorflow data pipeline\r\n    def generator():\r\n        """"""TF Dataset generator.""""""\r\n        assert len(train_imgs_file_list) == len(train_target_list)\r\n        for _input, _target in zip(train_imgs_file_list, train_target_list):\r\n            yield _input.encode(\'utf-8\'), cPickle.dumps(_target)\r\n\r\n    train_dataset = tf.data.Dataset.from_generator(generator, output_types=(tf.string, tf.string))\r\n    return train_dataset\r\n\r\ndef get_eval_dataset(val_imgs_path,val_anns_path,dataset_filter=None):\r\n    # read coco training images contains valid people\r\n    coco_data=PoseInfo(val_imgs_path,val_anns_path,with_mask=False, dataset_filter=dataset_filter, eval=True)\r\n    img_file_list,img_id_list=coco_data.get_image_list(),coco_data.get_image_id_list()\r\n    #tensorflow data pipeline\r\n    def generator():\r\n        """"""TF Dataset generator.""""""\r\n        assert len(img_id_list)==len(img_file_list)\r\n        for img_file,img_id in zip(img_file_list,img_id_list):\r\n            yield img_file.encode(""utf-8""),img_id\r\n\r\n    eval_dataset = tf.data.Dataset.from_generator(generator,output_types=(tf.string,tf.int32))\r\n    return eval_dataset'"
Hyperpose/Dataset/mscoco_dataset/prepare.py,0,"b'\r\nimport os\r\nimport tensorlayer as tl\r\nfrom tensorlayer import logging\r\nfrom tensorlayer.files.utils import (del_file, folder_exists, maybe_download_and_extract)\r\n\r\nfrom ..common import unzip\r\n\r\ndef prepare_dataset(data_path=""./data"",version=""2017"",task=""person""):\r\n        """"""Download MSCOCO Dataset.\r\n        Both 2014 and 2017 dataset have train, validate and test sets, but 2017 version put less data into the validation set (115k train, 5k validate) i.e. has more training data.\r\n\r\n        Parameters\r\n        -----------\r\n        path : str\r\n            The path that the data is downloaded to, defaults is ``data/mscoco...``.\r\n        dataset : str\r\n            The MSCOCO dataset version, `2014` or `2017`.\r\n        task : str\r\n            person for pose estimation, caption for image captioning, instance for segmentation.\r\n\r\n        Returns\r\n        ---------\r\n        train_im_path : str\r\n            Folder path of all training images.\r\n        train_ann_path : str\r\n            File path of training annotations.\r\n        val_im_path : str\r\n            Folder path of all validating images.\r\n        val_ann_path : str\r\n            File path of validating annotations.\r\n        test_im_path : str\r\n            Folder path of all testing images.\r\n        test_ann_path : None\r\n            File path of testing annotations, but as the test sets of MSCOCO 2014 and 2017 do not have annotation, returns None.\r\n\r\n        Examples\r\n        ----------\r\n        >>> train_im_path, train_ann_path, val_im_path, val_ann_path, _, _ = \\\r\n        ...    tl.files.load_mscoco_dataset(\'data\', \'2017\')\r\n\r\n        References\r\n        -------------\r\n        - `MSCOCO <http://mscoco.org>`__.\r\n\r\n        """"""\r\n\r\n        if version == ""2014"":\r\n            logging.info(""    [============= MSCOCO 2014 =============]"")\r\n            path = os.path.join(data_path, \'mscoco2014\')\r\n\r\n            if folder_exists(os.path.join(path, ""annotations"")) is False:\r\n                logging.info(""    downloading annotations"")\r\n                os.system(""wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P {}"".format(path))\r\n                unzip(os.path.join(path, ""annotations_trainval2014.zip""), path)\r\n                del_file(os.path.join(path, ""annotations_trainval2014.zip""))\r\n            else:\r\n                logging.info(""    annotations exists"")\r\n\r\n            if folder_exists(os.path.join(path, ""val2014"")) is False:\r\n                logging.info(""    downloading validating images"")\r\n                os.system(""wget http://images.cocodataset.org/zips/val2014.zip -P {}"".format(path))\r\n                unzip(os.path.join(path, ""val2014.zip""), path)\r\n                del_file(os.path.join(path, ""val2014.zip""))\r\n            else:\r\n                logging.info(""    validating images exists"")\r\n\r\n            if folder_exists(os.path.join(path, ""train2014"")) is False:\r\n                logging.info(""    downloading training images"")\r\n                os.system(""wget http://images.cocodataset.org/zips/train2014.zip -P {}"".format(path))\r\n                unzip(os.path.join(path, ""train2014.zip""), path)\r\n                del_file(os.path.join(path, ""train2014.zip""))\r\n            else:\r\n                logging.info(""    training images exists"")\r\n\r\n            if folder_exists(os.path.join(path, ""test2014"")) is False:\r\n                logging.info(""    downloading testing images"")\r\n                os.system(""wget http://images.cocodataset.org/zips/test2014.zip -P {}"".format(path))\r\n                unzip(os.path.join(path, ""test2014.zip""), path)\r\n                del_file(os.path.join(path, ""test2014.zip""))\r\n            else:\r\n                logging.info(""    testing images exists"")\r\n        elif version == ""2017"":\r\n            # 11.5w train, 0.5w valid, test (no annotation)\r\n            path = os.path.join(data_path, \'mscoco2017\')\r\n\r\n            if folder_exists(os.path.join(path, ""annotations"")) is False:\r\n                logging.info(""    downloading annotations"")\r\n                os.system(""wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P {}"".format(path))\r\n                unzip(os.path.join(path, ""annotations_trainval2017.zip""), path)\r\n                del_file(os.path.join(path, ""annotations_trainval2017.zip""))\r\n            else:\r\n                logging.info(""    annotations exists"")\r\n\r\n            if folder_exists(os.path.join(path, ""val2017"")) is False:\r\n                logging.info(""    downloading validating images"")\r\n                os.system(""wget http://images.cocodataset.org/zips/val2017.zip -P {}"".format(path))\r\n                unzip(os.path.join(path, ""val2017.zip""), path)\r\n                del_file(os.path.join(path, ""val2017.zip""))\r\n            else:\r\n                logging.info(""    validating images exists"")\r\n            \r\n            if folder_exists(os.path.join(path, ""train2017"")) is False:\r\n                logging.info(""    downloading training images"")\r\n                os.system(""wget http://images.cocodataset.org/zips/train2017.zip -P {}"".format(path))\r\n                unzip(os.path.join(path, ""train2017.zip""), path)\r\n                del_file(os.path.join(path, ""train2017.zip""))\r\n            else:\r\n                logging.info(""    training images exists"")\r\n            \r\n            \'\'\'\r\n            if folder_exists(os.path.join(path, ""test2017"")) is False:\r\n                logging.info(""    downloading testing images"")\r\n                os.system(""wget http://images.cocodataset.org/zips/test2017.zip -P {}"".format(path))\r\n                unzip(os.path.join(path, ""test2017.zip""), path)\r\n                del_file(os.path.join(path, ""test2017.zip""))\r\n            \r\n            else:\r\n                logging.info(""    testing images exists"")\r\n            \'\'\'\r\n            print(""temply ignore test dataset"")\r\n\r\n        else:\r\n            raise Exception(""dataset can only be 2014 and 2017, see MSCOCO website for more details."")\r\n\r\n        if version == ""2014"":\r\n            train_images_path = os.path.join(path, ""train2014"")\r\n            if task == ""person"":\r\n                train_annotations_file_path = os.path.join(path, ""annotations"", ""person_keypoints_train2014.json"")\r\n            elif task == ""caption"":\r\n                train_annotations_file_path = os.path.join(path, ""annotations"", ""captions_train2014.json"")\r\n            elif task == ""instance"":\r\n                train_annotations_file_path = os.path.join(path, ""annotations"", ""instances_train2014.json"")\r\n            else:\r\n                raise Exception(""unknown task"")\r\n            val_images_path = os.path.join(path, ""val2014"")\r\n            if task == ""person"":\r\n                val_annotations_file_path = os.path.join(path, ""annotations"", ""person_keypoints_val2014.json"")\r\n            elif task == ""caption"":\r\n                val_annotations_file_path = os.path.join(path, ""annotations"", ""captions_val2014.json"")\r\n            elif task == ""instance"":\r\n                val_annotations_file_path = os.path.join(path, ""annotations"", ""instances_val2014.json"")\r\n            test_images_path = os.path.join(path, ""test2014"")\r\n            test_annotations_file_path = None #os.path.join(path, ""annotations"", ""person_keypoints_test2014.json"")\r\n            \r\n        elif version == ""2017"":\r\n            train_images_path = os.path.join(path, ""train2017"")\r\n            if task == ""person"":\r\n                train_annotations_file_path = os.path.join(path, ""annotations"", ""person_keypoints_train2017.json"")\r\n            elif task == ""caption"":\r\n                train_annotations_file_path = os.path.join(path, ""annotations"", ""captions_train2017.json"")\r\n            elif task == ""instance"":\r\n                train_annotations_file_path = os.path.join(path, ""annotations"", ""instances_train2017.json"")\r\n            else:\r\n                raise Exception(""unknown task"")\r\n            val_images_path = os.path.join(path, ""val2017"")\r\n            if task == ""person"":\r\n                val_annotations_file_path = os.path.join(path, ""annotations"", ""person_keypoints_val2017.json"")\r\n            elif task == ""caption"":\r\n                val_annotations_file_path = os.path.join(path, ""annotations"", ""captions_val2017.json"")\r\n            elif task == ""instance"":\r\n                val_annotations_file_path = os.path.join(path, ""annotations"", ""instances_val2017.json"")\r\n            test_images_path = os.path.join(path, ""test2017"")\r\n            test_annotations_file_path =None # os.path.join(path, ""annotations"", ""person_keypoints_test2017.json"")\r\n\r\n        return train_images_path,train_annotations_file_path,\\\r\n                     val_images_path,val_annotations_file_path,\\\r\n                        test_images_path,test_annotations_file_path\r\n        '"
Hyperpose/Dataset/mscoco_dataset/visualize.py,0,"b'import cv2\r\nimport numpy as np\r\nimport _pickle as cpickle\r\nimport matplotlib.pyplot as plt\r\n\r\ndef visualize(vis_dir,vis_num,dataset,parts,colors):\r\n    for vis_id,(img_file,annos) in enumerate(dataset,start=1):\r\n        if(vis_id>=vis_num):\r\n            break\r\n        image=cv2.cvtColor(cv2.imread(img_file.numpy().decode(""utf-8""),cv2.IMREAD_COLOR),cv2.COLOR_BGR2RGB)\r\n        annos=cpickle.loads(annos.numpy())\r\n        ori_img=image\r\n        vis_img=image.copy()\r\n        kpts_list=annos[0]\r\n        print(f""visualizing image:{vis_id} with {len(kpts_list)} humans..."")\r\n        for kpts in kpts_list:\r\n            x,y,v=kpts[0::3],kpts[1::3],kpts[2::3]\r\n            for part_idx in range(0,len(parts)):\r\n                color=colors[part_idx]\r\n                vis_img=cv2.circle(vis_img,(int(x[part_idx]),int(y[part_idx])),radius=6,color=color,thickness=-1)\r\n        fig=plt.figure(figsize=(8,8))\r\n        a=fig.add_subplot(1,2,1)\r\n        a.set_title(""original image"")\r\n        plt.imshow(ori_img)\r\n        a=fig.add_subplot(1,2,2)\r\n        a.set_title(""visualized image"")\r\n        plt.imshow(vis_img)\r\n        plt.savefig(f""{vis_dir}/{vis_id}_vis_coco.png"")\r\n        plt.close(\'all\')'"
Hyperpose/Model/openpose/__init__.py,0,b'from .model import *\nfrom .train import *\nfrom .eval import *'
Hyperpose/Model/openpose/define.py,0,"b'import numpy as np\r\nfrom enum import Enum\r\n#specialized for coco\r\nclass CocoPart(Enum):\r\n    Nose = 0\r\n    Neck = 1\r\n    RShoulder = 2\r\n    RElbow = 3\r\n    RWrist = 4\r\n    LShoulder = 5\r\n    LElbow = 6\r\n    LWrist = 7\r\n    RHip = 8\r\n    RKnee = 9\r\n    RAnkle = 10\r\n    LHip = 11\r\n    LKnee = 12\r\n    LAnkle = 13\r\n    REye = 14\r\n    LEye = 15\r\n    REar = 16\r\n    LEar = 17\r\n    Background = 18\r\n\r\nCocoLimb=list(zip([1, 8, 9,  1,  11, 12, 1, 2, 3,  2, 1, 5, 6, 5,  1,  0,  0,  14, 15],\r\n                  [8, 9, 10, 11, 12, 13, 2, 3, 4, 16, 5, 6, 7, 17, 0, 14, 15,  16, 17]))\r\n\r\nCocoColor = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\r\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\r\n              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\r\n\r\nto_coco_converter={0:0, 2:6, 3:8, 4:10, 5:5, 6:7, 7:9, 8:12, 9:14, 10:16, 11:11, 12:13, 13:15, 14:2, 15:1, 16:4, 17:3}\r\n\r\nfrom_coco_converter={0:0, 1:15, 2:14, 3:17, 4:16, 5:5, 6:2, 7:6, 8:3, 9:7, 10:4, 11:11, 12:8, 13:12, 14:9, 15:13, 16:10}\r\n\r\ndef coco_input_converter(coco_kpts):\r\n    cvt_kpts=np.zeros(shape=[len(CocoPart),2])\r\n    transform = np.array(\r\n            list(zip([0, 5, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3],\r\n                     [0, 6, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3])))\r\n    xs = coco_kpts[0::3]\r\n    ys = coco_kpts[1::3]\r\n    vs = coco_kpts[2::3]\r\n    lost_idx=np.where(vs<=0)[0]\r\n    xs[lost_idx]=-1000\r\n    ys[lost_idx]=-1000\r\n    cvt_xs=(xs[transform[:,0]]+xs[transform[:,1]])/2\r\n    cvt_ys=(ys[transform[:,0]]+ys[transform[:,1]])/2\r\n    cvt_kpts[:-1,:]=np.array([cvt_xs,cvt_ys]).transpose()\r\n    #adding background point\r\n    cvt_kpts[-1:,:]=-1000\r\n    return cvt_kpts\r\n\r\ndef coco_output_converter(kpt_list):\r\n    kpts=[]\r\n    for coco_idx in list(from_coco_converter.keys()):\r\n        model_idx=from_coco_converter[coco_idx]\r\n        x,y=kpt_list[model_idx]\r\n        if(x<0 or y<0):\r\n            kpts+=[0.0,0.0,0.0]\r\n        else:\r\n            kpts+=[x,y,2.0]\r\n    return kpts\r\n\r\ndef get_coco_flip_list():\r\n    flip_list=[]\r\n    for part_idx,part in enumerate(CocoPart):\r\n        #eye flip\r\n        if(part==CocoPart.REye):\r\n            flip_list.append(CocoPart.LEye.value)\r\n        elif(part==CocoPart.LEye):\r\n            flip_list.append(CocoPart.REye.value)\r\n        #ear flip\r\n        elif(part==CocoPart.REar):\r\n            flip_list.append(CocoPart.LEar.value)\r\n        elif(part==CocoPart.LEar):\r\n            flip_list.append(CocoPart.REar.value)\r\n        #shoulder flip\r\n        elif(part==CocoPart.RShoulder):\r\n            flip_list.append(CocoPart.LShoulder.value)\r\n        elif(part==CocoPart.LShoulder):\r\n            flip_list.append(CocoPart.RShoulder.value)\r\n        #elbow flip\r\n        elif(part==CocoPart.RElbow):\r\n            flip_list.append(CocoPart.LElbow.value)\r\n        elif(part==CocoPart.LElbow):\r\n            flip_list.append(CocoPart.RElbow.value)\r\n        #wrist flip\r\n        elif(part==CocoPart.RWrist):\r\n            flip_list.append(CocoPart.LWrist.value)\r\n        elif(part==CocoPart.LWrist):\r\n            flip_list.append(CocoPart.RWrist.value)\r\n        #hip flip\r\n        elif(part==CocoPart.RHip):\r\n            flip_list.append(CocoPart.LHip.value)\r\n        elif(part==CocoPart.LHip):\r\n            flip_list.append(CocoPart.RHip.value)\r\n        #knee flip\r\n        elif(part==CocoPart.RKnee):\r\n            flip_list.append(CocoPart.LKnee.value)\r\n        elif(part==CocoPart.LKnee):\r\n            flip_list.append(CocoPart.RKnee.value)\r\n        #ankle flip\r\n        elif(part==CocoPart.RAnkle):\r\n            flip_list.append(CocoPart.LAnkle.value)\r\n        elif(part==CocoPart.LAnkle):\r\n            flip_list.append(CocoPart.RAnkle.value)\r\n        #others\r\n        else:\r\n            flip_list.append(part.value)\r\n    return flip_list\r\n\r\nCoco_fliplist=get_coco_flip_list()\r\n\r\n#specialized for mpii\r\nclass MpiiPart(Enum):\r\n    Headtop=0\r\n    Neck=1\r\n    RShoulder=2\r\n    RElbow=3\r\n    RWrist=4\r\n    LShoulder=5\r\n    LElbow=6\r\n    LWrist=7\r\n    RHip=8\r\n    RKnee=9\r\n    RAnkle=10\r\n    LHip=11\r\n    LKnee=12\r\n    LAnkle=13\r\n    Center=14\r\n    Background=15\r\n\r\nMpiiLimb=list(zip([0, 1, 2, 3, 1, 5, 6, 1,  14,  8, 9,  14, 11, 12],\r\n                  [1, 2, 3, 4, 5, 6, 7, 14,  8,  9, 10, 11, 12, 13]))\r\n\r\nMpiiColor = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\r\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\r\n              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\r\n\r\nto_mpii_converter={0:9, 1:8, 2:12, 3:11, 4:10, 5:13, 6:14, 7:15, 8:2, 9:1, 10:0, 11:3, 12:4, 13:5}\r\n\r\nfrom_mpii_converter={0:10, 1:9, 2:8, 3:11, 4:12, 5:13, 8:1, 9:0, 10:4, 11:3, 12:2, 13:5, 14:6, 15:7}\r\n\r\ndef mpii_input_converter(mpii_kpts):\r\n    cvt_kpts=np.zeros(shape=[len(MpiiPart),2])\r\n    transform = np.array([9,8,12,11,10,13,14,15,2,1,0,3,4,5])\r\n    xs = mpii_kpts[0::3]\r\n    ys = mpii_kpts[1::3]\r\n    vs = mpii_kpts[2::3]\r\n    lost_idx=np.where(vs<=0)[0]\r\n    xs[lost_idx]=-1000\r\n    ys[lost_idx]=-1000\r\n    cvt_xs=xs[transform]\r\n    cvt_ys=ys[transform]\r\n    cvt_kpts[:-2,:]=np.array([cvt_xs,cvt_ys]).transpose()\r\n    if(xs[2]<=0 or xs[3]<=0 or xs[12]<=0 or xs[13]<=0 or ys[2]<=0 or ys[3]<=0 or ys[12]<=0 or ys[13]<=0):\r\n        center_x=-1000\r\n        center_y=-1000\r\n    else:\r\n        center_x=(xs[2]+xs[3]+xs[12]+xs[13])/4\r\n        center_y=(ys[2]+ys[3]+ys[12]+ys[13])/4\r\n    cvt_kpts[14,:]=np.array([center_x,center_y])\r\n    #adding background point\r\n    cvt_kpts[-1:,:]=-1000\r\n    return cvt_kpts\r\n\r\ndef mpii_output_converter(kpt_list):\r\n    kpts=[]\r\n    mpii_keys=from_mpii_converter.keys()\r\n    for mpii_idx in range(0,16):\r\n        if(mpii_idx in mpii_keys):\r\n            model_idx=from_mpii_converter[mpii_idx]\r\n            x,y=kpt_list[model_idx]\r\n            if(x<0 or y<0):\r\n                kpts+=[0.0,0.0,-1.0]\r\n            else:\r\n                kpts+=[x,y,1.0]\r\n        else:\r\n            kpts+=[0.0,0.0,-1.0]\r\n    return kpts\r\n\r\ndef get_mpii_flip_list():\r\n    flip_list=[]\r\n    for part_idx,part in enumerate(MpiiPart):\r\n        #shoulder flip\r\n        if(part==MpiiPart.RShoulder):\r\n            flip_list.append(MpiiPart.LShoulder.value)\r\n        elif(part==MpiiPart.LShoulder):\r\n            flip_list.append(MpiiPart.RShoulder.value)\r\n        #elbow flip\r\n        elif(part==MpiiPart.RElbow):\r\n            flip_list.append(MpiiPart.LElbow.value)\r\n        elif(part==MpiiPart.LElbow):\r\n            flip_list.append(MpiiPart.RElbow.value)\r\n        #wrist flip\r\n        elif(part==MpiiPart.RWrist):\r\n            flip_list.append(MpiiPart.LWrist.value)\r\n        elif(part==MpiiPart.LWrist):\r\n            flip_list.append(MpiiPart.RWrist.value)\r\n        #hip flip\r\n        elif(part==MpiiPart.RHip):\r\n            flip_list.append(MpiiPart.LHip.value)\r\n        elif(part==MpiiPart.LHip):\r\n            flip_list.append(MpiiPart.RHip.value)\r\n        #knee flip\r\n        elif(part==MpiiPart.RKnee):\r\n            flip_list.append(MpiiPart.LKnee.value)\r\n        elif(part==MpiiPart.LKnee):\r\n            flip_list.append(MpiiPart.RKnee.value)\r\n        #ankle flip\r\n        elif(part==MpiiPart.RAnkle):\r\n            flip_list.append(MpiiPart.LAnkle.value)\r\n        elif(part==MpiiPart.LAnkle):\r\n            flip_list.append(MpiiPart.RAnkle.value)\r\n        #others\r\n        else:\r\n            flip_list.append(part.value)\r\n    return flip_list\r\n\r\nMpii_flip_list=get_mpii_flip_list()'"
Hyperpose/Model/openpose/eval.py,3,"b'import os\nimport cv2\nimport json\nimport numpy as np\nimport tensorflow as tf\nimport scipy.stats as st\nfrom functools import partial\nimport multiprocessing\n\nimport matplotlib.pyplot as plt\nfrom .infer import Post_Processor\nfrom .utils import get_parts,get_limbs,get_colors,get_output_kptcvter\nfrom .utils import draw_results\n\ndef infer_one_img(model,post_processor,img,img_id=-1,is_visual=False,save_dir=""./vis_dir""):\n    img=img.numpy()\n    img_id=img_id.numpy()\n    img_h,img_w,_=img.shape\n    data_format=model.data_format\n    input_img=cv2.resize(img.copy(),(model.win,model.hin))[np.newaxis,:,:,:]\n    if(data_format==""channels_first""):\n        input_img=input_img.transpose([0,3,1,2])\n    conf_map,paf_map=model.forward(input_img,is_train=False)\n    if(data_format==""channels_last""):\n        conf_map=np.transpose(conf_map,[0,3,1,2])\n        paf_map=np.transpose(paf_map,[0,3,1,2])\n    conf_map=conf_map.numpy()\n    paf_map=paf_map.numpy()\n    humans=post_processor.process(conf_map,paf_map,img_h,img_w)\n    if(is_visual): \n        draw_conf_map=cv2.resize(conf_map[0].transpose([1,2,0]),(img_w,img_h)).transpose([2,0,1])\n        draw_paf_map=cv2.resize(paf_map[0].transpose([1,2,0]),(img_w,img_h)).transpose([2,0,1])\n        visualize(img,img_id,humans,draw_conf_map,draw_paf_map,save_dir)\n    return humans\n\ndef visualize(img,img_id,humans,conf_map,paf_map,save_dir):\n    print(f""{len(humans)} human found!"")\n    print(""visualizing..."")\n    os.makedirs(save_dir,exist_ok=True)\n    ori_img=np.clip(img*255.0,0.0,255.0).astype(np.uint8)\n    vis_img=ori_img.copy()\n    for human in humans:\n        vis_img=human.draw_human(vis_img)\n    fig=plt.figure(figsize=(8,8))\n    #show input image\n    a=fig.add_subplot(2,2,1)\n    a.set_title(""input image"")\n    plt.imshow(ori_img)\n    #show output result\n    a=fig.add_subplot(2,2,2)\n    a.set_title(""output result"")\n    plt.imshow(vis_img)\n    #show conf_map\n    show_conf_map=np.amax(np.abs(conf_map[:-1,:,:]),axis=0)\n    a=fig.add_subplot(2,2,3)\n    a.set_title(""conf_map"")\n    plt.imshow(show_conf_map)\n    #show paf_map\n    show_paf_map=np.amax(np.abs(paf_map[:,:,:]),axis=0)\n    a=fig.add_subplot(2,2,4)\n    a.set_title(""paf_map"")\n    plt.imshow(show_paf_map)\n    #save\n    plt.savefig(f""{save_dir}/{img_id}_visualize.png"")\n    plt.close(\'all\')\n\ndef _map_fn(image_file,image_id,hin,win):\n    #load data\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image, channels=3)  # get RGB with 0~1\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image,image_id\n\ndef evaluate(model,dataset,config,vis_num=30,total_eval_num=30):\n    \'\'\'evaluate pipeline of Openpose class models\n\n    input model and dataset, the evaluate pipeline will start automaticly\n    the evaluate pipeline will:\n    1.loading newest model at path ./save_dir/model_name/model_dir/newest_model.npz\n    2.perform inference and parsing over the chosen evaluate dataset\n    3.visualize model output in evaluation in directory ./save_dir/model_name/eval_vis_dir\n    4.output model metrics by calling dataset.official_eval()\n\n    Parameters\n    ----------\n    arg1 : tensorlayer.models.MODEL\n        a preset or user defined model object, obtained by Model.get_model() function\n    \n    arg2 : dataset\n        a constructed dataset object, obtained by Dataset.get_dataset() function\n    \n    arg3 : Int\n        an Integer indicates how many model output should be visualized\n    \n    arg4 : Int\n        an Integer indicates how many images should be evaluated\n\n    Returns\n    -------\n    None\n    \'\'\'\n    model.load_weights(os.path.join(config.model.model_dir,""newest_model.npz""))\n    model.eval()\n    pd_anns=[]\n    vis_dir=config.eval.vis_dir\n    dataset_type=dataset.get_dataset_type()\n    kpt_converter=get_output_kptcvter(dataset_type)\n    post_processor=Post_Processor(get_parts(dataset_type),get_limbs(dataset_type),get_colors(dataset_type))\n    \n    eval_dataset=dataset.get_eval_dataset()\n    paramed_map_fn=partial(_map_fn,hin=model.hin,win=model.win)\n    eval_dataset=eval_dataset.map(paramed_map_fn,num_parallel_calls=max(multiprocessing.cpu_count()//2,1))\n    for eval_num,(img,img_id) in enumerate(eval_dataset):\n        if(eval_num>=total_eval_num):\n            break\n        if(eval_num<=vis_num):\n            humans=infer_one_img(model,post_processor,img,img_id=img_id,is_visual=True,save_dir=vis_dir)\n        else:\n            humans=infer_one_img(model,post_processor,img,img_id=img_id,is_visual=False,save_dir=vis_dir)\n        for human in humans:\n            ann={}\n            ann[""category_id""]=1\n            ann[""image_id""]=int(img_id.numpy())\n            ann[""id""]=human.get_global_id()\n            ann[""area""]=human.get_area()\n            ann[""score""]=human.get_score()\n            kpt_list=[]\n            for part_idx in range(0,model.n_pos):\n                if(part_idx not in human.body_parts):\n                    kpt_list.append([-1000,-1000])\n                else:\n                    body_part=human.body_parts[part_idx]\n                    kpt_list.append([body_part.get_x(),body_part.get_y()])\n            ann[""keypoints""]=kpt_converter(kpt_list)\n            pd_anns.append(ann)   \n        if(eval_num%100==0):\n            print(f""evaluating {eval_num}/{len(list(eval_dataset))}"")\n            \n    result_dic={""annotations"":pd_anns}\n    dataset.official_eval(result_dic,vis_dir)\n'"
Hyperpose/Model/openpose/infer.py,2,"b'\r\nimport os\r\nimport cv2\r\nimport json\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport scipy.stats as st\r\nfrom ..human import Human,BodyPart\r\n\r\nclass Post_Processor:\r\n    def __init__(self,parts,limbs,colors):\r\n        self.cur_id=0\r\n        self.parts=parts\r\n        self.limbs=limbs\r\n        self.colors=colors\r\n        self.n_pos=len(self.parts)\r\n        self.n_limb=len(self.limbs)\r\n        self.thres_conf=0.1\r\n        self.thres_vec=0.1\r\n        self.thres_vec_cnt=6\r\n        self.thres_criterion2=0.05\r\n        self.thres_part_cnt=8\r\n        self.thres_human_score=0.55\r\n        self.step_paf=10\r\n        self.n_limb=len(self.limbs)\r\n    \r\n    def get_peak_map(self,conf_map):\r\n        def _gauss_smooth(origin):\r\n            sigma=3.0\r\n            kernel_size=17\r\n            smoothed=np.zeros(shape=origin.shape)\r\n            channel_num=origin.shape[-1]\r\n            for channel_idx in range(0,channel_num):\r\n                smoothed[0,:,:,channel_idx]=cv2.GaussianBlur(origin[0,:,:,channel_idx],\\\r\n                    ksize=(kernel_size,kernel_size),sigmaX=sigma,sigmaY=sigma)\r\n            return smoothed\r\n\r\n        smoothed = _gauss_smooth(conf_map)\r\n        max_pooled = tf.nn.pool(smoothed, window_shape=(3, 3), pooling_type=\'MAX\', padding=\'SAME\')\r\n        return tf.where(tf.equal(smoothed, max_pooled), smoothed, tf.zeros_like(conf_map)).numpy()\r\n    \r\n    def process(self,conf_map,paf_map,img_h,img_w):\r\n        conf_map=cv2.resize(conf_map[0].transpose([1,2,0]),(img_w,img_h))[np.newaxis,:,:,:]\r\n        paf_map=cv2.resize(paf_map[0].transpose([1,2,0]),(img_w,img_h))[np.newaxis,:,:,:]\r\n        peak_map=self.get_peak_map(conf_map)\r\n        humans=self.process_paf(peak_map[0],conf_map[0],paf_map[0])\r\n        return humans\r\n    \r\n    def process_paf(self,peak_map,conf_map,paf_map):\r\n        #filter valid peaks\r\n        peaks=[[] for part_idx in range(0,self.n_pos)]\r\n        all_peaks=[]\r\n        peak_ys,peak_xs,part_idxs=np.where(peak_map>self.thres_conf)\r\n        for peak_idx,(part_idx,peak_y,peak_x) in enumerate(zip(part_idxs,peak_ys,peak_xs)):\r\n            peak_score=conf_map[peak_y,peak_x,part_idx]\r\n            peaks[part_idx].append(Peak(peak_idx,part_idx,peak_y,peak_x,peak_score))\r\n            all_peaks.append(Peak(peak_idx,part_idx,peak_y,peak_x,peak_score))\r\n        #start candidate connection\r\n        \'\'\'\r\n        for part_idx in range(0,self.n_pos):\r\n            print(f""found peak_{part_idx}:{len(peaks[part_idx])}"")\r\n        \'\'\'\r\n        candidate_limbs=[[] for limb_idx in range(0,len(self.limbs))]\r\n        for limb_idx,limb in enumerate(self.limbs):\r\n            src_idx,dst_idx=limb\r\n            peak_src_list=peaks[src_idx]\r\n            peak_dst_list=peaks[dst_idx]\r\n            if((len(peak_src_list)==0) or(len(peak_dst_list)==0)):\r\n                continue\r\n            #print(f""candidating: src:{self.parts(src_idx)} dst:{self.parts(dst_idx)}"")\r\n            for peak_src in peak_src_list:\r\n                for peak_dst in peak_dst_list:\r\n                    #calculate paf vector\r\n                    vec_src=np.array([peak_src.y,peak_src.x])\r\n                    vec_dst=np.array([peak_dst.y,peak_dst.x])\r\n                    vec_limb=vec_dst-vec_src\r\n                    lenth=np.sqrt(np.sum(vec_limb**2))\r\n                    if(lenth<1e-12):\r\n                        continue\r\n                    vec_limb=vec_limb/lenth\r\n                    paf_vectors=self.get_paf_vectors(limb_idx,vec_src,vec_dst,paf_map)\r\n                    #calculate criterion\r\n                    criterion1=0\r\n                    scores=0.0\r\n                    for step in range(self.step_paf):\r\n                        score=np.sum(vec_limb*paf_vectors[step])\r\n                        if(score>=self.thres_vec):\r\n                            criterion1+=1\r\n                        scores+=score\r\n                    criterion2=scores/self.step_paf+min(0.0,0.5*conf_map.shape[0]/lenth-1.0)\r\n                    #filter candidate limbs\r\n                    #print(f""test start:{vec_src} end:{vec_dst} c1:{criterion1} c2:{criterion2}"")\r\n                    if(criterion1>self.thres_vec_cnt and criterion2>self.thres_criterion2):\r\n                        candidate_limbs[limb_idx].append(Connection(peak_src.idx,peak_dst.idx,criterion2))\r\n        #filter chosen connection\r\n        all_chosen_limbs=[[] for limb_idx in range(0,len(self.limbs))]\r\n        for limb_idx in range(0,len(self.limbs)):\r\n            sort_candidates=candidate_limbs[limb_idx]\r\n            sort_candidates.sort(reverse=True)\r\n            chosen_limbs=all_chosen_limbs[limb_idx]\r\n            for candidate in sort_candidates:\r\n                assigned=False\r\n                for chosen_limb in chosen_limbs:\r\n                    if(chosen_limb.peak_src_id==candidate.peak_src_id):\r\n                        assigned=True\r\n                    if(chosen_limb.peak_dst_id==candidate.peak_dst_id):\r\n                        assigned=True\r\n                    if(assigned):\r\n                        break\r\n                if(assigned):\r\n                    continue\r\n                chosen_limbs.append(candidate)\r\n        #assemble human\r\n        humans=[]\r\n        for limb_idx,limb in enumerate(self.limbs):\r\n            src_part_idx,dst_part_idx=limb\r\n            chosen_limbs=all_chosen_limbs[limb_idx]\r\n            for chosen_limb in chosen_limbs:\r\n                peak_src_id,peak_dst_id=chosen_limb.peak_src_id,chosen_limb.peak_dst_id\r\n                #print(f""assigning chosen_limb score:{chosen_limb.score}"")\r\n                touched_ids=[]\r\n                for human_id,human in enumerate(humans):\r\n                    if((human[src_part_idx]==peak_src_id) or (human[dst_part_idx]==peak_dst_id)):\r\n                        touched_ids.append(human_id)\r\n                if(len(touched_ids)==1):\r\n                    human=humans[touched_ids[0]]\r\n                    if(human[dst_part_idx]!=peak_dst_id):\r\n                        human[dst_part_idx]=peak_dst_id\r\n                        human[19]+=1\r\n                        human[18]+=all_peaks[peak_dst_id].score+chosen_limb.score\r\n                elif(len(touched_ids)>=2):\r\n                    membership=0\r\n                    human_1=humans[touched_ids[0]]\r\n                    human_2=humans[touched_ids[1]]\r\n                    for part_idx in range(0,18):\r\n                        if(human_1[part_idx]>=0 and human_2[part_idx]>=0):\r\n                            membership=2\r\n                    if(membership==0):\r\n                        human_1[0:18]+=human_2[0:18]+1\r\n                        human_1[18]+=human_2[18]+chosen_limb.score\r\n                        human_1[19]+=human_2[19]\r\n                        humans.pop(touched_ids[1])\r\n                    elif(membership==2):\r\n                        human_1[dst_part_idx]=peak_dst_id\r\n                        human_1[19]+=1\r\n                        human_1[18]+=all_peaks[peak_dst_id].score+chosen_limb.score\r\n                elif(len(touched_ids)==0 and limb_idx<17):\r\n                    human=np.zeros(shape=[20]).astype(np.float32)\r\n                    human[:]=-1\r\n                    human[src_part_idx]=peak_src_id\r\n                    human[dst_part_idx]=peak_dst_id\r\n                    human[18]=all_peaks[peak_src_id].score+all_peaks[peak_dst_id].score+chosen_limb.score\r\n                    human[19]=2\r\n                    humans.append(human)\r\n        #return assembled human\r\n        #print(f""test candidate human:{len(humans)}"")\r\n        ret_humans=[]\r\n        for human_id,human in enumerate(humans):\r\n            #print(f""test human filter score:{human[18]/human[19]}  part_num:{human[19]}"")\r\n            #print(f""test candidate"")\r\n            for i in range(0,18):\r\n                if(human[i]!=-1):\r\n                    peak=all_peaks[int(human[i])]\r\n                    #print(f""part:{self.parts(i)} loc_y:{peak.y} loc_x:{peak.x} socre:{peak.score}"")\r\n            if((human[18]/human[19]>=self.thres_human_score) and (human[19]>=self.thres_part_cnt)):\r\n                ret_human=Human(self.parts,self.limbs,self.colors)\r\n                ret_human.local_id=human_id\r\n                ret_human.score=human[18]/human[19]\r\n                for part_idx in range(0,self.n_pos-1):\r\n                    if(human[part_idx]!=-1):\r\n                        peak=all_peaks[int(human[part_idx])]\r\n                        x,y,score=peak.x,peak.y,peak.score\r\n                        ret_human.body_parts[part_idx]=BodyPart(parts=self.parts,u_idx=human[part_idx],part_idx=part_idx,\\\r\n                            x=x,y=y,score=score)\r\n                ret_human.global_id=self.cur_id\r\n                ret_humans.append(ret_human)\r\n                self.cur_id+=1\r\n        return ret_humans\r\n\r\n    def get_paf_vectors(self,limb_id,vec_src,vec_dst,paf_map):\r\n        def round(x):\r\n            sign_x=np.where(x>0,1,-1)\r\n            return (x+0.5*sign_x).astype(np.int)\r\n        paf_vectors=np.zeros(shape=(self.step_paf,2))\r\n        vec_limb=vec_dst-vec_src\r\n        for step in range(0,self.step_paf):\r\n            vec_loc_y,vec_loc_x=round(vec_src+vec_limb*step/self.step_paf)\r\n            vec_paf_x=paf_map[vec_loc_y][vec_loc_x][limb_id*2]\r\n            vec_paf_y=paf_map[vec_loc_y][vec_loc_x][limb_id*2+1]\r\n            paf_vectors[step][0]=vec_paf_y\r\n            paf_vectors[step][1]=vec_paf_x\r\n        return paf_vectors\r\n        \r\nclass Peak:\r\n    def __init__(self,peak_idx,part_idx,y,x,score):\r\n        self.idx=peak_idx\r\n        self.part_idx=part_idx\r\n        self.y=y\r\n        self.x=x\r\n        self.score=score\r\n\r\nclass Connection:\r\n    def __init__(self,peak_src_id,peak_dst_id,score):\r\n        self.peak_src_id=peak_src_id\r\n        self.peak_dst_id=peak_dst_id\r\n        self.score=score\r\n    \r\n    def __lt__(self,other):\r\n        return self.score<other.score\r\n    \r\n    def __eq__(self,other):\r\n        return self.score==other.score\r\n\r\n\r\n'"
Hyperpose/Model/openpose/train.py,22,"b'#!/usr/bin/env python3\n\nimport math\nimport multiprocessing\nimport os\nimport cv2\nimport time\nimport sys\nimport json\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport tensorflow as tf\nimport tensorlayer as tl\nfrom pycocotools.coco import maskUtils\nimport _pickle as cPickle\nfrom functools import partial\nfrom .utils import tf_repeat, get_heatmap, get_vectormap, draw_results \nfrom .utils import get_parts,get_limbs,get_input_kptcvter,get_flip_list\nfrom ..common import init_log,log,KUNGFU\n\ndef regulize_loss(target_model,weight_decay_factor):\n    re_loss=0\n    regularizer=tf.keras.regularizers.l2(l=weight_decay_factor)\n    for trainable_weight in target_model.trainable_weights:\n        re_loss+=regularizer(trainable_weight)\n    return re_loss\n\ndef _data_aug_fn(image, ground_truth, hin, hout, win, wout, parts, limbs , kpt_cvter, flip_list=None, data_format=""channels_first""):\n    """"""Data augmentation function.""""""\n    #restore data\n    concat_dim=0 if data_format==""channels_first"" else -1\n    ground_truth = cPickle.loads(ground_truth.numpy())\n    image=image.numpy()\n    annos = ground_truth[""obj""]\n    mask = ground_truth[""mask""]\n    #convert input keypoints\n    for anno_idx in range(0,len(annos)):\n        cvt_kpts=kpt_cvter(annos[anno_idx])\n        annos[anno_idx]=cvt_kpts\n\n    # decode mask\n    h_mask, w_mask, _ = np.shape(image)\n    mask_miss = np.ones((h_mask, w_mask), dtype=np.uint8)\n    if(mask!=None):\n        for seg in mask:\n            bin_mask = maskUtils.decode(seg)\n            bin_mask = np.logical_not(bin_mask)\n            mask_miss = np.bitwise_and(mask_miss, bin_mask)\n    \n    #debug no augmentation\n    #get transform matrix\n    M_rotate = tl.prepro.affine_rotation_matrix(angle=(-30, 30))  # original paper: -40~40\n    M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=(0.5, 0.8))  # original paper: 0.5~1.1\n    M_combined = M_rotate.dot(M_zoom)\n    h, w, _ = image.shape\n    transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, x=w, y=h)\n    \n    #apply data augmentation\n    image = tl.prepro.affine_transform_cv2(image, transform_matrix)\n    mask_miss = tl.prepro.affine_transform_cv2(mask_miss, transform_matrix, border_mode=\'replicate\')\n    annos = tl.prepro.affine_transform_keypoints(annos, transform_matrix)\n    \'\'\'\n    if(flip_list!=None):\n        image, annos, mask_miss = tl.prepro.keypoint_random_flip(image,annos, mask_miss, prob=0.5, flip_list=flip_list)\n    \'\'\'\n    image, annos, mask_miss = tl.prepro.keypoint_resize_random_crop(image, annos, mask_miss, size=(hin, win)) # hao add\n\n    # generate result which include keypoints heatmap and vectormap\n    height, width, _ = image.shape\n    heatmap = get_heatmap(annos, height, width, hout, wout, parts, limbs)\n    vectormap = get_vectormap(annos, height, width, hout, wout, parts, limbs)\n    resultmap = np.concatenate((heatmap, vectormap), axis=concat_dim)\n\n    image=cv2.resize(image,(win,hin))\n    mask_miss=cv2.resize(mask_miss,(win,hin))\n    img_mask=mask_miss\n    \n    #generate output masked image, result map and maskes\n    img_mask = mask_miss.reshape(hin, win, 1)\n    image = image * np.repeat(img_mask, 3, 2)\n    resultmap = np.array(resultmap, dtype=np.float32)\n    mask_miss = np.array(cv2.resize(mask_miss, (wout, hout), interpolation=cv2.INTER_AREA),dtype=np.float32)[:,:,np.newaxis]\n    if(data_format==""channels_first""):\n        image=np.transpose(image,[2,0,1])\n        mask_miss=np.transpose(mask_miss,[2,0,1])\n    return image, resultmap, mask_miss\n\n\ndef _map_fn(img_list, annos ,data_aug_fn, hin, win, hout, wout, parts, limbs):\n    """"""TF Dataset pipeline.""""""\n    #load data\n    image = tf.io.read_file(img_list)\n    image = tf.image.decode_jpeg(image, channels=3)  # get RGB with 0~1\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    #data augmentation using affine transform and get paf maps\n    image, resultmap, mask = tf.py_function(data_aug_fn, [image, annos], [tf.float32, tf.float32, tf.float32])\n    #data augmentaion using tf\n    image = tf.image.random_brightness(image, max_delta=45./255.)   # 64./255. 32./255.)  caffe -30~50\n    image = tf.image.random_contrast(image, lower=0.5, upper=1.5)   # lower=0.2, upper=1.8)  caffe 0.3~1.5\n    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n    return image, resultmap, mask\n\ndef get_paramed_map_fn(hin,win,hout,wout,parts,limbs,kpt_cvter,flip_list=None,data_format=""channels_first""):\n    paramed_data_aug_fn=partial(_data_aug_fn,hin=hin,win=win,hout=hout,wout=wout,parts=parts,limbs=limbs,kpt_cvter=kpt_cvter,\\\n        flip_list=flip_list,data_format=data_format)\n    paramed_map_fn=partial(_map_fn,data_aug_fn=paramed_data_aug_fn, hin=hin, win=win, hout=hout, wout=wout ,parts=parts, limbs=limbs)\n    return paramed_map_fn\n\ndef single_train(train_model,dataset,config):\n    \'\'\'Single train pipeline of Openpose class models\n\n    input model and dataset, the train pipeline will start automaticly\n    the train pipeline will:\n    1.store and restore ckpt in directory ./save_dir/model_name/model_dir\n    2.log loss information in directory ./save_dir/model_name/log.txt\n    3.visualize model output periodly during training in directory ./save_dir/model_name/train_vis_dir\n    the newest model is at path ./save_dir/model_name/model_dir/newest_model.npz\n\n    Parameters\n    ----------\n    arg1 : tensorlayer.models.MODEL\n        a preset or user defined model object, obtained by Model.get_model() function\n    \n    arg2 : dataset\n        a constructed dataset object, obtained by Dataset.get_dataset() function\n    \n    \n    Returns\n    -------\n    None\n    \'\'\'\n\n    init_log(config)\n    #train hyper params\n    #dataset params\n    n_step = config.train.n_step\n    batch_size = config.train.batch_size\n    #learning rate params\n    lr_init = config.train.lr_init\n    lr_decay_factor = config.train.lr_decay_factor\n    lr_decay_every_step = config.train.lr_decay_every_step\n    weight_decay_factor = config.train.weight_decay_factor\n    #log and checkpoint params\n    log_interval=config.log.log_interval\n    save_interval=config.train.save_interval\n    vis_dir=config.train.vis_dir\n    \n    #model hyper params\n    n_pos = train_model.n_pos\n    hin = train_model.hin\n    win = train_model.win\n    hout = train_model.hout\n    wout = train_model.wout\n    data_format=train_model.data_format\n    model_dir = config.model.model_dir\n\n\n    print(f""single training using learning rate:{lr_init} batch_size:{batch_size}"")\n    #training dataset configure with shuffle,augmentation,and prefetch\n    train_dataset=dataset.get_train_dataset()\n    dataset_type=dataset.get_dataset_type()\n    parts,limbs,kpt_cvter=get_parts(dataset_type),get_limbs(dataset_type),get_input_kptcvter(dataset_type)\n    flip_list=get_flip_list(dataset_type)\n    paramed_map_fn=get_paramed_map_fn(hin,win,hout,wout,parts,limbs,kpt_cvter,flip_list=flip_list,data_format=data_format)\n    train_dataset = train_dataset.shuffle(buffer_size=4096).repeat()\n    train_dataset = train_dataset.map(paramed_map_fn,num_parallel_calls=max(multiprocessing.cpu_count()//2,1))\n    train_dataset = train_dataset.batch(config.train.batch_size)  \n    train_dataset = train_dataset.prefetch(64)\n    \n    #train configure\n    step=tf.Variable(1, trainable=False)\n    lr=tf.Variable(lr_init,trainable=False)\n    opt=tf.keras.optimizers.Adam(learning_rate=lr)\n    ckpt=tf.train.Checkpoint(step=step,optimizer=opt,lr=lr)\n    ckpt_manager=tf.train.CheckpointManager(ckpt,model_dir,max_to_keep=3)\n    \n    #load from ckpt\n    try:\n        ckpt.restore(ckpt_manager.latest_checkpoint)\n    except:\n        log(""ckpt_path doesn\'t exist, step and optimizer are initialized"")\n    try:\n        train_model.load_weights(os.path.join(model_dir,""newest_model.npz""))\n    except:\n        log(""model_path doesn\'t exist, model parameters are initialized"")\n        \n    #optimize one step\n    @tf.function\n    def one_step(image,gt_label,mask,train_model):\n        step.assign_add(1)\n        with tf.GradientTape() as tape:\n            gt_conf=gt_label[:,:n_pos,:,:]\n            gt_paf=gt_label[:,n_pos:,:,:]\n            pd_conf,pd_paf,stage_confs,stage_pafs=train_model.forward(image,is_train=True)\n\n            pd_loss,loss_confs,loss_pafs=train_model.cal_loss(gt_conf,gt_paf,mask,stage_confs,stage_pafs)\n            re_loss=regulize_loss(train_model,weight_decay_factor)\n            total_loss=pd_loss+re_loss\n        \n        gradients=tape.gradient(total_loss,train_model.trainable_weights)\n        opt.apply_gradients(zip(gradients,train_model.trainable_weights))\n        return gt_conf,gt_paf,pd_conf,pd_paf,total_loss,re_loss,loss_confs,loss_pafs\n\n    #train each step\n    tic=time.time()\n    train_model.train()\n    log(\'Start - n_step: {} batch_size: {} lr_init: {} lr_decay_every_step: {}\'.format(\n            n_step, batch_size, lr_init, lr_decay_every_step))\n    for image,gt_label,mask in train_dataset:\n        #learning rate decay\n        if(step % lr_decay_every_step==0):\n            new_lr_decay = lr_decay_factor**(step / lr_decay_every_step) \n            lr=lr_init*new_lr_decay\n        #optimize one step\n        gt_conf,gt_paf,pd_conf,pd_paf,total_loss,re_loss,loss_confs,loss_pafs=one_step(image.numpy(),gt_label.numpy(),mask.numpy(),train_model)\n        #debug\n        #for stage_id,(loss_conf,loss_paf) in enumerate(zip(loss_confs,loss_pafs)):\n        #    print(f""test stage:{stage_id} loss_conf:{loss_conf} loss_paf:{loss_paf}"")\n\n        #save log info periodly\n        if((step!=0) and (step%log_interval)==0):\n            tic=time.time()\n            log(\'Total Loss at iteration {} / {} is: {} Learning rate {} l2_loss {} time:{}\'.format(\n                step.numpy(), n_step, total_loss, lr.numpy(), re_loss,time.time()-tic))\n\n        #save result and ckpt periodly\n        if((step!=0) and (step%save_interval)==0):\n            log(""saving model ckpt and result..."")\n            draw_results(image.numpy(), gt_conf.numpy(), pd_conf.numpy(), gt_paf.numpy(), pd_paf.numpy(), mask.numpy(),\\\n                 vis_dir,\'train_%d_\' % step)\n            ckpt_save_path=ckpt_manager.save()\n            log(f""ckpt save_path:{ckpt_save_path} saved!\\n"")\n            model_save_path=os.path.join(model_dir,""newest_model.npz"")\n            train_model.save_weights(model_save_path)\n            log(f""model save_path:{model_save_path} saved!\\n"")\n\n        #training finished\n        if(step==n_step):\n            break\n\ndef parallel_train(train_model,dataset,config):\n    \'\'\'Parallel train pipeline of openpose class models\n\n    input model and dataset, the train pipeline will start automaticly\n    the train pipeline will:\n    1.store and restore ckpt in directory ./save_dir/model_name/model_dir\n    2.log loss information in directory ./save_dir/model_name/log.txt\n    3.visualize model output periodly during training in directory ./save_dir/model_name/train_vis_dir\n    the newest model is at path ./save_dir/model_name/model_dir/newest_model.npz\n\n    Parameters\n    ----------\n    arg1 : tensorlayer.models.MODEL\n        a preset or user defined model object, obtained by Model.get_model() function\n    \n    arg2 : dataset\n        a constructed dataset object, obtained by Dataset.get_dataset() function\n    \n    \n    Returns\n    -------\n    None\n    \'\'\'\n    init_log(config)\n    #train hyper params\n    #dataset params\n    n_step = config.train.n_step\n    batch_size = config.train.batch_size\n    #learning rate params\n    lr_init = config.train.lr_init\n    lr_decay_factor = config.train.lr_decay_factor\n    lr_decay_every_step = config.train.lr_decay_every_step\n    weight_decay_factor = config.train.weight_decay_factor\n    #log and checkpoint params\n    log_interval=config.log.log_interval\n    save_interval=config.train.save_interval\n    vis_dir=config.train.vis_dir\n    \n    #model hyper params\n    n_pos = train_model.n_pos\n    hin = train_model.hin\n    win = train_model.win\n    hout = train_model.hout\n    wout = train_model.wout\n    data_format = train_model.data_format\n    model_dir = config.model.model_dir\n\n    #import kungfu\n    from kungfu import current_cluster_size, current_rank\n    from kungfu.tensorflow.initializer import broadcast_variables\n    from kungfu.tensorflow.optimizers import SynchronousSGDOptimizer, SynchronousAveragingOptimizer, PairAveragingOptimizer\n    \n\n    print(f""parallel training using learning rate:{lr_init} batch_size:{batch_size}"")\n    #training dataset configure with shuffle,augmentation,and prefetch\n    train_dataset=dataset.get_train_dataset()\n    dataset_type=dataset.get_dataset_type()\n    parts,limbs,kpt_cvter=get_parts(dataset_type),get_limbs(dataset_type),get_input_kptcvter(dataset_type)\n    flip_list=get_flip_list(dataset_type)\n    paramed_map_fn=get_paramed_map_fn(hin,win,hout,wout,parts,limbs,kpt_cvter,flip_list=flip_list,data_format=data_format)\n    train_dataset = train_dataset.shuffle(buffer_size=4096)\n    train_dataset = train_dataset.shard(num_shards=current_cluster_size(),index=current_rank())\n    train_dataset = train_dataset.repeat()\n    train_dataset = train_dataset.map(paramed_map_fn, num_parallel_calls=4)\n    train_dataset = train_dataset.batch(batch_size)  \n    train_dataset = train_dataset.prefetch(64)\n\n    #train model configure  \n    step=tf.Variable(1, trainable=False)\n    lr=tf.Variable(lr_init,trainable=False)\n    opt=tf.keras.optimizers.SGD(learning_rate=lr,momentum=0.9)\n    ckpt=tf.train.Checkpoint(step=step,optimizer=opt,lr=lr)\n    ckpt_manager=tf.train.CheckpointManager(ckpt,model_dir,max_to_keep=3)\n\n    #load from ckpt\n    try:\n        ckpt.restore(ckpt_manager.latest_checkpoint)\n    except:\n        log(""ckpt_path doesn\'t exist, step and optimizer are initialized"")\n    try:\n        train_model.load_weights(os.path.join(model_dir,""newest_model.npz""))\n    except:\n        log(""model_path doesn\'t exist, model parameters are initialized"")\n\n    # KungFu configure\n    kungfu_option=config.train.kungfu_option\n    if kungfu_option == KUNGFU.Sync_sgd:\n        print(""using Kungfu.SynchronousSGDOptimizer!"")\n        opt = SynchronousSGDOptimizer(opt)\n    elif kungfu_option == KUNGFU.Sync_avg:\n        print(""using Kungfu.SynchronousAveragingOptimize!"")\n        opt = SynchronousAveragingOptimizer(opt)\n    elif kungfu_option == KUNGFU.Pair_avg:\n        print(""using Kungfu.PairAveragingOptimizer!"")\n        opt=PairAveragingOptimizer(opt)\n    \n\n    n_step = n_step // current_cluster_size() + 1  # KungFu\n    lr_decay_every_step = lr_decay_every_step // current_cluster_size() + 1  # KungFu\n    \n    #optimize one step\n    @tf.function\n    def one_step(image,gt_label,mask,train_model,is_first_batch=False):\n        step.assign_add(1)\n        with tf.GradientTape() as tape:\n            gt_conf=gt_label[:,:n_pos,:,:]\n            gt_paf=gt_label[:,n_pos:,:,:]\n            pd_conf,pd_paf,stage_confs,stage_pafs=train_model.forward(image,is_train=True)\n\n            pd_loss,loss_confs,loss_pafs=train_model.cal_loss(gt_conf,gt_paf,mask,stage_confs,stage_pafs)\n            re_loss=regulize_loss(train_model,weight_decay_factor)\n            total_loss=pd_loss+re_loss\n        \n        gradients=tape.gradient(total_loss,train_model.trainable_weights)\n        opt.apply_gradients(zip(gradients,train_model.trainable_weights))\n        #Kung fu\n        if(is_first_batch):\n            broadcast_variables(train_model.all_weights)\n            broadcast_variables(opt.variables())\n        return gt_conf,gt_paf,pd_conf,pd_paf,total_loss,re_loss\n\n    #train each step\n    tic=time.time()\n    train_model.train()\n    log(f""Worker {current_rank()}: Initialized"")\n    log(\'Start - n_step: {} batch_size: {} lr_init: {} lr_decay_every_step: {}\'.format(\n            n_step, batch_size, lr_init, lr_decay_every_step))\n    for image,gt_label,mask in train_dataset:\n        #learning rate decay\n        if(step % lr_decay_every_step==0):\n            new_lr_decay = lr_decay_factor**(step // lr_decay_every_step)\n            lr=lr_init*new_lr_decay\n        #optimize one step\n        gt_conf,gt_paf,pd_conf,pd_paf,total_loss,re_loss=one_step(image.numpy(),gt_label.numpy(),mask.numpy(),\\\n            train_model,step==0)\n        #save log info periodly\n        if((step!=0) and (step%log_interval)==0):\n            tic=time.time()\n            log(\'Total Loss at iteration {} / {} is: {} Learning rate {} l2_loss {} time:{}\'.format(\n                step.numpy(), n_step, total_loss, lr.numpy(), re_loss,time.time()-tic))\n\n        #save result and ckpt periodly\n        if((step!=0) and (step%save_interval)==0 and current_rank()==0):\n            log(""saving model ckpt and result..."")\n            draw_results(image.numpy(), gt_conf.numpy(), pd_conf.numpy(), gt_paf.numpy(), pd_paf.numpy(), mask.numpy(),\\\n                 vis_dir,\'train_%d_\' % step)\n            ckpt_save_path=ckpt_manager.save()\n            log(f""ckpt save_path:{ckpt_save_path} saved!\\n"")\n            model_save_path=os.path.join(model_dir,""newest_model.npz"")\n            train_model.save_weights(model_save_path)\n            log(f""model save_path:{model_save_path} saved!\\n"")\n\n        #training finished\n        if(step==n_step):\n            break\n'"
Hyperpose/Model/openpose/utils.py,0,"b'# -*- coding: utf-8 -*-\n\n## xxx\n\nimport os\nimport cv2\nimport math\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import (del_file, folder_exists, maybe_download_and_extract)\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom distutils.dir_util import mkpath\nfrom scipy.spatial.distance import cdist\nfrom .infer import Post_Processor\nfrom ..human import Human\nfrom ..common import tf_repeat,TRAIN,MODEL,DATA\n\n\ndef preprocess(annos,img_height,img_width,model_hout,model_wout,dataset_type,data_format=""channels_first""):\n    \'\'\'preprocess function of openpose class models\n    \n    take keypoints annotations, image height and width, model input height and width, and dataset type,\n    return the constructed conf_map and paf_map\n\n    Parameters\n    ----------\n    arg1 : list\n        a list of annotation, each annotation is a list of keypoints that belongs to a person, each keypoint follows the\n        format (x,y), and x<0 or y<0 if the keypoint is not visible or not annotated.\n        the annotations must from a known dataset_type, other wise the keypoint and limbs order will not be correct.\n    \n    arg2 : Int\n        height of the input image, need this to make use of keypoint annotation\n    \n    arg3 : Int\n        width of the input image, need this to make use of keypoint annotation\n\n    arg4 : Int\n        height of the model output, will be the height of the generated maps\n    \n    arg5 : Int\n        width of the model output, will be the width of the generated maps\n\n    arg6 : Config.DATA\n        a enum value of enum class Config.DATA\n        dataset_type where the input annotation list from, because to generate correct\n        conf_map and paf_map, the order of keypoints and limbs should be awared.\n\n    arg7 : string\n        data format speficied for channel order\n        available input:\n        \'channels_first\': data_shape C*H*W\n        \'channels_last\': data_shape H*W*C  \n\n    Returns\n    -------\n    list\n        including two element\n        conf_map: heatmaps of keypoints, shape C*H*W(channels_first) or H*W*C(channels_last)\n        paf_map: heatmaps of limbs, shape C*H*W(channels_first) or H*W*C(channels_last)\n    \'\'\'\n    parts=get_parts(dataset_type)\n    limbs=get_limbs(dataset_type)\n    heatmap=get_heatmap(annos,img_height,img_width,model_hout,model_wout,parts,limbs,data_format)\n    vectormap=get_vectormap(annos,img_height,img_width,model_hout,model_wout,parts,limbs,data_format)\n    return heatmap,vectormap\n\ndef postprocess(conf_map,paf_map,dataset_type,data_format=""channels_first""):\n    \'\'\'postprocess function of openpose class models\n    \n    take model predicted feature maps, output parsed human objects, each one contains all detected keypoints of the person\n\n    Parameters\n    ----------\n    arg1 : numpy array\n        model predicted conf_map, heatmaps of keypoints, shape C*H*W(channels_first) or H*W*C(channels_last)\n    \n    arg1 : numpy array\n        model predicted paf_map, heatmaps of limbs, shape C*H*W(channels_first) or H*W*C(channels_last)\n    \n    arg3 : Config.DATA\n        an enum value of enum class Config.DATA\n        width of the model output, will be the width of the generated maps\n\n    arg4 : string\n        data format speficied for channel order\n        available input:\n        \'channels_first\': data_shape C*H*W\n        \'channels_last\': data_shape H*W*C \n\n    Returns\n    -------\n    list\n        contain object of humans,see Model.Human for detail information of Human object\n    \'\'\'\n    parts=get_parts(dataset_type)\n    limbs=get_limbs(dataset_type)\n    colors=get_colors(dataset_type)\n    if(dataset_type==""channels_last""):\n        conf_map=np.transpose(conf_map,[2,0,1])\n        paf_map=np.transpose(paf_map,[2,0,1])\n    _,img_h,img_w=conf_map.shape\n    conf_map=conf_map[np.newaxis,:,:,:]\n    paf_map=paf_map[np.newaxis,:,:,:]\n    post_processor=Post_Processor(parts,limbs,colors)\n    humans=post_processor.process(conf_map,paf_map,img_h,img_w)\n    return humans\n\ndef visualize(img,conf_map,paf_map,save_name=""maps"",save_dir=""./save_dir/vis_dir"",data_format=""channels_first""):\n    \'\'\'visualize function of openpose class models\n    \n    take model predict feature maps, output visualized image.\n    the image will be saved at \'save_dir\'/\'save_name\'_visualize.png\n\n    Parameters\n    ----------\n    arg1 : numpy array\n        image\n\n    arg2 : numpy array\n        model output conf_map, heatmaps of keypoints, shape C*H*W(channels_first) or H*W*C(channels_last)\n    \n    arg3 : numpy array\n        model output paf_map, heatmaps of limbs, shape C*H*W(channels_first) or H*W*C(channels_last)\n    \n    arg4 : String\n        specify output image name to distinguish.\n\n    arg5 : String\n        specify which directory to save the visualized image.\n\n    arg6 : string\n        data format speficied for channel order\n        available input:\n        \'channels_first\': data_shape C*H*W\n        \'channels_last\': data_shape H*W*C  \n\n    Returns\n    -------\n    None\n    \'\'\'\n    if(data_format==""channels_last""):\n        conf_map=np.transpose(conf_map,[2,0,1])\n        paf_map=np.tranpose(paf_map,[2,0,1])\n    os.makedirs(save_dir,exist_ok=True)\n    ori_img=np.clip(img*255.0,0.0,255.0).astype(np.uint8)\n    vis_img=ori_img.copy()\n    fig=plt.figure(figsize=(8,8))\n    #show input image\n    a=fig.add_subplot(2,2,1)\n    a.set_title(""input image"")\n    plt.imshow(vis_img)\n    #show conf_map\n    show_conf_map=np.amax(np.abs(conf_map[:-1,:,:]),axis=0)\n    a=fig.add_subplot(2,2,3)\n    a.set_title(""conf_map"")\n    plt.imshow(show_conf_map)\n    #show paf_map\n    show_paf_map=np.amax(np.abs(paf_map[:,:,:]),axis=0)\n    a=fig.add_subplot(2,2,4)\n    a.set_title(""paf_map"")\n    plt.imshow(show_paf_map)\n    #save\n    plt.savefig(f""{save_dir}/{save_name}_visualize.png"")\n    plt.close(\'all\')\n\ndef get_heatmap(annos, height, width, hout, wout, parts, limbs, data_format=""channels_first""):\n    """"""\n\n    Parameters\n    -----------\n\n\n    Returns\n    --------\n\n    """"""\n    # n_pos is 19 for coco, 15 for MPII\n    # the heatmap for every joints takes the maximum over all people\n    n_pos=len(parts)\n    joints_heatmap = np.zeros((n_pos, height, width), dtype=np.float32)\n    scale=min(height/hout,width/wout)\n    # among all people\n    for joint in annos:\n        # generate heatmap for every keypoints\n        # loop through all people and keep the maximum\n\n        for i, points in enumerate(joint):\n            if points[0] < 0 or points[1] < 0:\n                continue\n            joints_heatmap = put_heatmap(joints_heatmap, i, points, 8.0)\n\n    # 0: joint index, 1:y, 2:x\n    joints_heatmap = joints_heatmap.transpose((1, 2, 0))\n\n    # background\n    joints_heatmap[:, :, -1] = np.clip(1 - np.amax(joints_heatmap, axis=2), 0.0, 1.0)\n\n    #resize\n    if(data_format==""channels_first""):\n        resized_joints_heatmap=np.zeros((n_pos,hout,wout),dtype=np.float32)\n        for i in range(0, n_pos):\n            resized_joints_heatmap[i,:,:] = cv2.resize(np.array(joints_heatmap[:, :, i]), (wout, hout), interpolation=cv2.INTER_AREA)\n    elif(data_format==""channels_last""):\n        resized_joints_heatmap=np.zeros((hout,wout,n_pos),dtype=np.float32)\n        for i in range(0, n_pos):\n            resized_joints_heatmap[:,:,i] = cv2.resize(np.array(joints_heatmap[:, :, i]), (wout, hout), interpolation=cv2.INTER_AREA)\n    return resized_joints_heatmap\n\ndef put_heatmap(heatmap, plane_idx, center, sigma):\n    """"""\n\n    Parameters\n    -----------\n\n\n    Returns\n    --------\n\n\n    """"""\n    center_x, center_y = center\n    _, height, width = heatmap.shape[:3]\n\n    th = 4.6052\n    delta = math.sqrt(th * 2)\n\n    x0 = int(max(0, center_x - delta * sigma + 0.5))\n    y0 = int(max(0, center_y - delta * sigma + 0.5))\n\n    x1 = int(min(width - 1, center_x + delta * sigma + 0.5))\n    y1 = int(min(height - 1, center_y + delta * sigma + 0.5))\n\n    exp_factor = 1 / 2.0 / sigma / sigma\n\n    ## fast - vectorize\n    arr_heatmap = heatmap[plane_idx, y0:y1 + 1, x0:x1 + 1]\n    y_vec = (np.arange(y0, y1 + 1) - center_y)**2  # y1 included\n    x_vec = (np.arange(x0, x1 + 1) - center_x)**2\n    xv, yv = np.meshgrid(x_vec, y_vec)\n    arr_sum = exp_factor * (xv + yv)\n    arr_exp = np.exp(-arr_sum)\n    arr_exp[arr_sum > th] = 0\n    heatmap[plane_idx, y0:y1 + 1, x0:x1 + 1] = np.maximum(arr_heatmap, arr_exp)\n    return heatmap\n\n\ndef get_vectormap(annos, height, width , hout, wout, parts, limbs, data_format=""channels_first""):\n    """"""\n\n    Parameters\n    -----------\n\n\n    Returns\n    --------\n\n\n    """"""\n    n_limbs=len(limbs)\n    vectormap = np.zeros((2*n_limbs, height, width), dtype=np.float32)\n    counter = np.zeros((n_limbs, height, width), dtype=np.int16)\n\n    for joint in annos:\n        for i, (a, b) in enumerate(limbs):\n            v_start = joint[a]\n            v_end = joint[b]\n            # exclude invisible or unmarked point\n            if v_start[0] < -100 or v_start[1] < -100 or v_end[0] < -100 or v_end[1] < -100:\n                continue\n            vectormap = cal_vectormap_fast(vectormap, counter, i, v_start, v_end)\n\n    # normalize the PAF (otherwise longer limb gives stronger absolute strength)\n    for i in range(0,n_limbs):\n        filter_counter=np.where(counter[i]<=0,1,0)\n        div_counter=filter_counter+(1-filter_counter)*counter[i]\n        vectormap[i*2+0]/=div_counter\n        vectormap[i*2+1]/=div_counter\n\n    #resize\n    if(data_format==""channels_first""):\n        resized_vectormap=np.zeros((n_limbs*2,hout,wout),dtype= np.float32)\n        for i in range(0, n_limbs * 2):\n            resized_vectormap[i,:,:] = cv2.resize(np.array(vectormap[i, :, :]), (wout, hout), interpolation=cv2.INTER_AREA)\n    elif(data_format==""channels_last""):\n        resized_vectormap=np.zeros((hout,wout,n_limbs*2),dtype= np.float32)\n        for i in range(0, n_limbs * 2):\n            resized_vectormap[:,:,i] = cv2.resize(np.array(vectormap[i, :, :]), (wout, hout), interpolation=cv2.INTER_AREA)\n    return resized_vectormap\n\ndef cal_vectormap_ori(vectormap, countmap, i, v_start, v_end):\n    """"""\n\n    Parameters\n    -----------\n\n\n    Returns\n    --------\n\n\n    """"""\n    _, height, width = vectormap.shape[:3]\n\n    threshold = 8\n    vector_x = v_end[0] - v_start[0]\n    vector_y = v_end[1] - v_start[1]\n    length = math.sqrt(vector_x**2 + vector_y**2)\n    if length == 0:\n        return vectormap\n\n    min_x = max(0, int(min(v_start[0], v_end[0]) - threshold))\n    min_y = max(0, int(min(v_start[1], v_end[1]) - threshold))\n\n    max_x = min(width, int(max(v_start[0], v_end[0]) + threshold))\n    max_y = min(height, int(max(v_start[1], v_end[1]) + threshold))\n\n    norm_x = vector_x / length\n    norm_y = vector_y / length\n\n    for y in range(min_y, max_y):\n        for x in range(min_x, max_x):\n            bec_x = x - v_start[0]\n            bec_y = y - v_start[1]\n            dist = abs(bec_x * norm_y - bec_y * norm_x)\n\n            # orthogonal distance is < then threshold\n            if dist > threshold:\n                continue\n            countmap[i][y][x] += 1\n            vectormap[i * 2 + 0][y][x] = norm_x\n            vectormap[i * 2 + 1][y][x] = norm_y\n\n    return vectormap\n\n\ndef cal_vectormap_fast(vectormap, countmap, i, v_start, v_end):\n    """"""\n\n    Parameters\n    -----------\n\n\n    Returns\n    --------\n\n\n    """"""\n    _, height, width = vectormap.shape[:3]\n    _, height, width = vectormap.shape[:3]\n\n    threshold = 8\n    vector_x = v_end[0] - v_start[0]\n    vector_y = v_end[1] - v_start[1]\n\n    length = math.sqrt(vector_x**2 + vector_y**2)\n    if length == 0:\n        return vectormap\n\n    min_x = max(0, int(min(v_start[0], v_end[0]) - threshold))\n    min_y = max(0, int(min(v_start[1], v_end[1]) - threshold))\n\n    max_x = min(width, int(max(v_start[0], v_end[0]) + threshold))\n    max_y = min(height, int(max(v_start[1], v_end[1]) + threshold))\n\n    norm_x = vector_x / length\n    norm_y = vector_y / length\n\n    x_vec = (np.arange(min_x, max_x) - v_start[0]) * norm_y\n    y_vec = (np.arange(min_y, max_y) - v_start[1]) * norm_x\n\n    xv, yv = np.meshgrid(x_vec, y_vec)\n\n    dist_matrix = abs(xv - yv)\n    filter_matrix = np.where(dist_matrix > threshold, 0, 1)\n    countmap[i, min_y:max_y, min_x:max_x] += filter_matrix\n\n    vectormap[i*2+0,min_y:max_y,min_x:max_x]+=norm_x*filter_matrix\n    vectormap[i*2+1,min_y:max_y,min_x:max_x]+=norm_y*filter_matrix\n    return vectormap\n\n\ndef draw_results(images, heats_ground, heats_result, pafs_ground, pafs_result, masks, save_dir ,name=\'\', data_format=""channels_first""):\n    """"""Save results for debugging.\n\n    Parameters\n    -----------\n    images : a list of RGB images\n    heats_ground : a list of keypoint heat maps or None\n    heats_result : a list of keypoint heat maps or None\n    pafs_ground : a list of paf vector maps or None\n    pafs_result : a list of paf vector maps or None\n    masks : a list of mask for people\n    """"""\n    # interval = len(images)\n    if(data_format==""channnels_last""):\n        images=np.transpose(images,[0,3,1,2])\n        heats_ground=np.transpose(heats_ground,[0,3,1,2])\n        heats_result=np.transpose(heats_result,[0,3,1,2])\n        pafs_ground=np.transpose(pafs_ground,[0,3,1,2])\n        pafs_result=np.transpose(pafs_result,[0,3,1,2])\n        masks=np.transpose(masks,[0,3,1,2])\n\n    _,n_confmaps,_,_=heats_ground.shape\n    _,n_pafmaps,_,_=pafs_ground.shape\n    for i in range(len(images)):\n        if heats_ground is not None:\n            heat_ground = heats_ground[i]\n        if heats_result is not None:\n            heat_result = heats_result[i]\n        if pafs_ground is not None:\n            paf_ground = pafs_ground[i]\n        if pafs_result is not None:\n            paf_result = pafs_result[i]\n        if masks is not None:\n            # print(masks.shape)\n            mask = masks[i, 0, :, :]\n            # print(mask.shape)\n            mask = mask[np.newaxis, :, :]\n            # mask = masks[:,:,:,0]\n            # mask = mask.reshape(hout, wout, 1)\n            mask1 = np.repeat(mask, n_confmaps, 0)\n            mask2 = np.repeat(mask, n_pafmaps, 0)\n            # print(mask1.shape, mask2.shape)\n        image = images[i]\n\n        image=image.transpose([1,2,0])\n        img_h,img_w,img_c=image.shape\n        fig = plt.figure(figsize=(8, 8))\n        a = fig.add_subplot(2, 3, 1)\n        plt.imshow(image)\n        cv2.imwrite(f""./test_dir/test_{name}_{i}_input_image.jpg.jpg"",cv2.cvtColor(np.clip(image*255.0,0.,255.).astype(np.uint8),cv2.COLOR_RGB2BGR))\n\n        if pafs_ground is not None:\n            a = fig.add_subplot(2, 3, 2)\n            a.set_title(\'Vectormap_ground\')\n            vectormap=paf_ground\n            if(masks is not None):\n                vectormap = paf_ground * mask2\n            tmp2 = vectormap\n            tmp2_odd = np.amax(np.absolute(tmp2[::2, :, :]), axis=0)\n            tmp2_even = np.amax(np.absolute(tmp2[1::2, :, :]), axis=0)\n\n            # tmp2_odd = tmp2_odd * 255\n            # tmp2_odd = tmp2_odd.astype(np.int)\n            plt.imshow(tmp2_odd, alpha=0.3)\n            cv2.imwrite(f""./test_dir/test_{name}_{i}_paf_odd_gt.jpg"",np.clip(cv2.resize(tmp2_odd,(img_w,img_h))*255.0,0.,255.).astype(np.uint8))\n            cv2.imwrite(f""./test_dir/test_{name}_{i}_paf_even_gt.jpg"",np.clip(cv2.resize(tmp2_even,(img_w,img_h))*255.0,0.,255.).astype(np.uint8))\n            # tmp2_even = tmp2_even * 255\n            # tmp2_even = tmp2_even.astype(np.int)\n            plt.colorbar()\n            plt.imshow(tmp2_even, alpha=0.3)\n\n        if pafs_result is not None:\n            a = fig.add_subplot(2, 3, 3)\n            a.set_title(\'Vectormap result\')\n            if masks is not None:\n                vectormap = paf_result * mask2\n            else:\n                vectormap = paf_result\n            tmp2 = vectormap\n            tmp2_odd = np.amax(np.absolute(tmp2[::2, :, :]), axis=0)\n            tmp2_even = np.amax(np.absolute(tmp2[1::2, :, :]), axis=0)\n            plt.imshow(tmp2_odd, alpha=0.3)\n\n            cv2.imwrite(f""./test_dir/test_{name}_{i}_paf_odd_rs.jpg"",np.clip(cv2.resize(tmp2_odd,(img_w,img_h))*255.0,0.,255.).astype(np.uint8))\n            cv2.imwrite(f""./test_dir/test_{name}_{i}_paf_even_rs.jpg"",np.clip(cv2.resize(tmp2_even,(img_w,img_h))*255.0,0.,255.).astype(np.uint8))\n\n            plt.colorbar()\n            plt.imshow(tmp2_even, alpha=0.3)\n\n        if heats_result is not None:\n            a = fig.add_subplot(2, 3, 4)\n            a.set_title(\'Heatmap result\')\n            if masks is not None:\n                heatmap = heat_result * mask1\n            else:\n                heatmap = heat_result\n            tmp = heatmap\n            tmp = np.amax(heatmap[:-1, :, :], axis=0)\n            cv2.imwrite(f""./test_dir/test_heatmap_{name}_{i}_rs.jpg"",np.clip(cv2.resize(tmp,(img_w,img_h))*255.0,0.,255.).astype(np.uint8))\n\n            plt.imshow(tmp, alpha=0.3)\n            plt.colorbar()\n\n        if heats_ground is not None:\n            a = fig.add_subplot(2, 3, 5)\n            a.set_title(\'Heatmap ground truth\')\n            if masks is not None:\n                heatmap = heat_ground * mask1\n            else:\n                heatmap = heat_ground\n            tmp = heatmap\n            tmp = np.amax(heatmap[:-1, :, :], axis=0)\n            cv2.imwrite(f""./test_dir/test_heatmap_{name}_{i}_gt.jpg"",np.clip(cv2.resize(tmp,(img_w,img_h))*255.0,0.,255.).astype(np.uint8))\n\n            plt.imshow(tmp, alpha=0.3)\n            plt.colorbar()\n\n        if masks is not None:\n            a = fig.add_subplot(2, 3, 6)\n            a.set_title(\'Mask\')\n            # print(mask.shape, tmp.shape)\n            plt.imshow(mask[0, :, :], alpha=0.3)\n            plt.colorbar()\n        # plt.savefig(str(i)+\'.png\',dpi=300)\n        # plt.show()\n        os.makedirs(save_dir,exist_ok=True)\n        plt.savefig(os.path.join(save_dir, \'%s_%d.png\' % (name, i)), dpi=300)\n        plt.close()\n\n\n\ndef vis_annos(image, annos, save_dir ,name=\'\'):\n    """"""Save results for debugging.\n\n    Parameters\n    -----------\n    images : single RGB image\n    annos  : annotation, list of lists\n    """"""\n\n    fig = plt.figure(figsize=(8, 8))\n    a = fig.add_subplot(1, 1, 1)\n\n    plt.imshow(image)\n    for people in annos:\n        for idx, jo in enumerate(people):\n            if jo[0] > 0 and jo[1] > 0:\n                plt.plot(jo[0], jo[1], \'*\')\n\n    plt.savefig(os.path.join(save_dir, \'keypoints%s%d.png\' % (name, i)), dpi=300)\n\n\n\nfrom .define import CocoPart,CocoLimb,CocoColor,coco_input_converter,coco_output_converter,Coco_fliplist\nfrom .define import MpiiPart,MpiiLimb,MpiiColor,mpii_input_converter,mpii_output_converter,Mpii_flip_list\n\ndef get_parts(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return CocoPart\n    elif(dataset_type==DATA.MPII):\n        return MpiiPart\n\ndef get_limbs(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return CocoLimb\n    elif(dataset_type==DATA.MPII):\n        return MpiiLimb\n\ndef get_colors(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return CocoColor\n    elif(dataset_type==DATA.MPII):\n        return MpiiColor\n\ndef get_input_kptcvter(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return coco_input_converter\n    elif(dataset_type==DATA.MPII):\n        return mpii_input_converter\n\ndef get_output_kptcvter(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return coco_output_converter\n    elif(dataset_type==DATA.MPII):\n        return mpii_output_converter\n\ndef get_flip_list(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return Coco_fliplist\n    elif(dataset_type==DATA.MPII):\n        return Mpii_flip_list'"
Hyperpose/Model/pose_proposal/__init__.py,0,b'from .model import *\nfrom .train import *\nfrom .eval import *'
Hyperpose/Model/pose_proposal/define.py,0,"b'import numpy as np\r\nfrom enum import Enum\r\n#specialized for coco\r\nclass CocoPart(Enum):\r\n    Nose = 0\r\n    Instance = 1\r\n    RShoulder = 2\r\n    RElbow = 3\r\n    RWrist = 4\r\n    LShoulder = 5\r\n    LElbow = 6\r\n    LWrist = 7\r\n    RHip = 8\r\n    RKnee = 9\r\n    RAnkle = 10\r\n    LHip = 11\r\n    LKnee = 12\r\n    LAnkle = 13\r\n    REye = 14\r\n    LEye = 15\r\n    REar = 16\r\n    LEar = 17\r\n\r\nCocoLimb=list(zip([1, 8, 9,  1,  11, 12, 1, 2, 3, 1, 5, 6, 1, 0,  0,  14, 15],\r\n                  [8, 9, 10, 11, 12, 13, 2, 3, 4, 5, 6, 7, 0, 14, 15, 16, 17]))\r\n\r\nCocoColor = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\r\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\r\n              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\r\n\r\n\r\nto_coco_converter={0:0, 2:6, 3:8, 4:10, 5:5, 6:7, 7:9, 8:12, 9:14, 10:16, 11:11, 12:13, 13:15, 14:2, 15:1, 16:4, 17:3}\r\n\r\nfrom_coco_converter={0:0, 1:15, 2:14, 3:17, 4:16, 5:5, 6:2, 7:6, 8:3, 9:7, 10:4, 11:11, 12:8, 13:12, 14:9, 15:13, 16:10}\r\n\r\ndef coco_input_converter(coco_kpts):\r\n    transform = np.array(\r\n            list(zip([0, 5, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3],\r\n                     [0, 6, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3])))\r\n    xs = coco_kpts[0::3]\r\n    ys = coco_kpts[1::3]\r\n    vs = coco_kpts[2::3]\r\n    lost_idx=np.where(vs<=0)[0]\r\n    xs[lost_idx]=-1000\r\n    ys[lost_idx]=-1000\r\n    cvt_xs=(xs[transform[:,0]]+xs[transform[:,1]])/2\r\n    cvt_ys=(ys[transform[:,0]]+ys[transform[:,1]])/2\r\n    cvt_kpts=np.array([cvt_xs,cvt_ys]).transpose()\r\n    return cvt_kpts\r\n\r\ndef coco_output_converter(kpt_list):\r\n    kpts=[]\r\n    for coco_idx in list(from_coco_converter.keys()):\r\n        model_idx=from_coco_converter[coco_idx]\r\n        x,y=kpt_list[model_idx]\r\n        if(x<0 or y<0):\r\n            kpts+=[0.0,0.0,0.0]\r\n        else:\r\n            kpts+=[x,y,1.0]\r\n    return kpts\r\n\r\ndef get_coco_flip_list():\r\n    flip_list=[]\r\n    for part_idx,part in enumerate(CocoPart):\r\n        #eye flip\r\n        if(part==CocoPart.REye):\r\n            flip_list.append(CocoPart.LEye.value)\r\n        elif(part==CocoPart.LEye):\r\n            flip_list.append(CocoPart.REye.value)\r\n        #ear flip\r\n        elif(part==CocoPart.REar):\r\n            flip_list.append(CocoPart.LEar.value)\r\n        elif(part==CocoPart.LEar):\r\n            flip_list.append(CocoPart.REar.value)\r\n        #shoulder flip\r\n        elif(part==CocoPart.RShoulder):\r\n            flip_list.append(CocoPart.LShoulder.value)\r\n        elif(part==CocoPart.LShoulder):\r\n            flip_list.append(CocoPart.RShoulder.value)\r\n        #elbow flip\r\n        elif(part==CocoPart.RElbow):\r\n            flip_list.append(CocoPart.LElbow.value)\r\n        elif(part==CocoPart.LElbow):\r\n            flip_list.append(CocoPart.RElbow.value)\r\n        #wrist flip\r\n        elif(part==CocoPart.RWrist):\r\n            flip_list.append(CocoPart.LWrist.value)\r\n        elif(part==CocoPart.LWrist):\r\n            flip_list.append(CocoPart.RWrist.value)\r\n        #hip flip\r\n        elif(part==CocoPart.RHip):\r\n            flip_list.append(CocoPart.LHip.value)\r\n        elif(part==CocoPart.LHip):\r\n            flip_list.append(CocoPart.RHip.value)\r\n        #knee flip\r\n        elif(part==CocoPart.RKnee):\r\n            flip_list.append(CocoPart.LKnee.value)\r\n        elif(part==CocoPart.LKnee):\r\n            flip_list.append(CocoPart.RKnee.value)\r\n        #ankle flip\r\n        elif(part==CocoPart.RAnkle):\r\n            flip_list.append(CocoPart.LAnkle.value)\r\n        elif(part==CocoPart.LAnkle):\r\n            flip_list.append(CocoPart.RAnkle.value)\r\n        #others\r\n        else:\r\n            flip_list.append(part.value)\r\n    return flip_list\r\n\r\nCoco_fliplist=get_coco_flip_list()\r\n\r\n#specialized for mpii\r\nclass MpiiPart(Enum):\r\n    Headtop=0\r\n    Neck=1\r\n    RShoulder=2\r\n    RElbow=3\r\n    RWrist=4\r\n    LShoulder=5\r\n    LElbow=6\r\n    LWrist=7\r\n    RHip=8\r\n    RKnee=9\r\n    RAnkle=10\r\n    LHip=11\r\n    LKnee=12\r\n    LAnkle=13\r\n    Center=14\r\n    Instance=15\r\n\r\nMpiiLimb=list(zip([15, 15, 1, 2, 3, 1, 5, 6, 1,  14,  8, 9,  14, 11, 12],\r\n                  [ 0, 1,  2, 3, 4, 5, 6, 7, 14,  8,  9, 10, 11, 12, 13]))\r\n\r\nMpiiColor = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\r\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\r\n              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\r\n\r\nto_mpii_converter={0:9, 1:8, 2:12, 3:11, 4:10, 5:13, 6:14, 7:15, 8:2, 9:1, 10:0, 11:3, 12:4, 13:5}\r\n\r\nfrom_mpii_converter={0:10, 1:9, 2:8, 3:11, 4:12, 5:13, 8:1, 9:0, 10:4, 11:3, 12:2, 13:5, 14:6, 15:7}\r\n\r\ndef mpii_input_converter(coco_kpts):\r\n    cvt_kpts=np.zeros(shape=[len(MpiiPart),2])\r\n    transform = np.array([9,8,12,11,10,13,14,15,2,1,0,3,4,5])\r\n    xs = coco_kpts[0::3]\r\n    ys = coco_kpts[1::3]\r\n    vs = coco_kpts[2::3]\r\n    lost_idx=np.where(vs<=0)[0]\r\n    xs[lost_idx]=-1000\r\n    ys[lost_idx]=-1000\r\n    cvt_xs=xs[transform]\r\n    cvt_ys=ys[transform]\r\n    cvt_kpts[:-2,:]=np.array([cvt_xs,cvt_ys]).transpose()\r\n    center_x=(xs[2]+xs[3]+xs[12]+xs[13])/4\r\n    center_y=(ys[2]+ys[3]+ys[12]+ys[13])/4\r\n    cvt_kpts[14,:]=np.array([center_x,center_y])\r\n    #adding background point\r\n    cvt_kpts[15,:]=(cvt_kpts[0,:]+cvt_kpts[1,:])/2\r\n    return cvt_kpts\r\n\r\ndef mpii_output_converter(kpt_list):\r\n    kpts=[]\r\n    mpii_keys=from_mpii_converter.keys()\r\n    for mpii_idx in range(0,16):\r\n        if(mpii_idx in mpii_keys):\r\n            model_idx=from_mpii_converter[mpii_idx]\r\n            x,y=kpt_list[model_idx]\r\n            if(x<0 or y<0):\r\n                kpts+=[0.0,0.0,-1.0]\r\n            else:\r\n                kpts+=[x,y,1.0]\r\n        else:\r\n            kpts+=[0.0,0.0,-1.0]\r\n    return kpts\r\n\r\ndef get_mpii_flip_list():\r\n    flip_list=[]\r\n    for part_idx,part in enumerate(MpiiPart):\r\n        #shoulder flip\r\n        if(part==MpiiPart.RShoulder):\r\n            flip_list.append(MpiiPart.LShoulder.value)\r\n        elif(part==MpiiPart.LShoulder):\r\n            flip_list.append(MpiiPart.RShoulder.value)\r\n        #elbow flip\r\n        elif(part==MpiiPart.RElbow):\r\n            flip_list.append(MpiiPart.LElbow.value)\r\n        elif(part==MpiiPart.LElbow):\r\n            flip_list.append(MpiiPart.RElbow.value)\r\n        #wrist flip\r\n        elif(part==MpiiPart.RWrist):\r\n            flip_list.append(MpiiPart.LWrist.value)\r\n        elif(part==MpiiPart.LWrist):\r\n            flip_list.append(MpiiPart.RWrist.value)\r\n        #hip flip\r\n        elif(part==MpiiPart.RHip):\r\n            flip_list.append(MpiiPart.LHip.value)\r\n        elif(part==MpiiPart.LHip):\r\n            flip_list.append(MpiiPart.RHip.value)\r\n        #knee flip\r\n        elif(part==MpiiPart.RKnee):\r\n            flip_list.append(MpiiPart.LKnee.value)\r\n        elif(part==MpiiPart.LKnee):\r\n            flip_list.append(MpiiPart.RKnee.value)\r\n        #ankle flip\r\n        elif(part==MpiiPart.RAnkle):\r\n            flip_list.append(MpiiPart.LAnkle.value)\r\n        elif(part==MpiiPart.LAnkle):\r\n            flip_list.append(MpiiPart.RAnkle.value)\r\n        #others\r\n        else:\r\n            flip_list.append(part.value)\r\n    return flip_list\r\n\r\nMpii_flip_list=get_mpii_flip_list()'"
Hyperpose/Model/pose_proposal/eval.py,3,"b'import os\r\nimport cv2\r\nimport json\r\nimport scipy\r\nimport numpy as np\r\nimport multiprocessing\r\nimport tensorflow as tf\r\n\r\nimport matplotlib.pyplot as plt\r\nfrom functools import partial\r\nfrom .infer import Post_Processor\r\nfrom .utils import get_parts,get_limbs,get_colors,get_output_kptcvter\r\nfrom .utils import draw_bbx,draw_edge\r\n\r\ndef infer_one_img(model,post_processor,img,img_id=-1,is_visual=False,save_dir=""./vis_dir/pose_proposal""):\r\n    img=img.numpy()\r\n    img_id=img_id.numpy()\r\n    img_h,img_w,_=img.shape\r\n    data_format=model.data_format\r\n    input_img=cv2.resize(img,(model.win,model.hin))[np.newaxis,:,:,:]\r\n    if(data_format==""channels_first""):\r\n        input_img=np.transpose(input_img,[0,3,1,2])\r\n    pc,pi,px,py,pw,ph,pe=model.forward(input_img,is_train=False)\r\n    if(data_format==""channels_last""):\r\n        pc=np.transpose(pc,[0,3,1,2])\r\n        pi=np.transpose(pi,[0,3,1,2])\r\n        px=np.transpose(px,[0,3,1,2])\r\n        py=np.transpose(py,[0,3,1,2])\r\n        pw=np.transpose(pw,[0,3,1,2])\r\n        ph=np.transpose(ph,[0,3,1,2])\r\n        pe=np.transpose(pe,[0,5,1,2,3,4])\r\n    humans=post_processor.process(pc[0].numpy(),pi[0].numpy(),px[0].numpy(),py[0].numpy(),\\\r\n        pw[0].numpy(),ph[0].numpy(),pe[0].numpy())\r\n    #resize output\r\n    scale_w=img_w/model.win\r\n    scale_h=img_h/model.hin\r\n    for human in humans:\r\n        human.scale(scale_w=scale_w,scale_h=scale_h)\r\n    if(is_visual):\r\n        predicts=(pc[0],px[0]*scale_w,py[0]*scale_h,pw[0]*scale_w,ph[0]*scale_h,pe[0])\r\n        visualize(img,img_id,humans,predicts,model.hnei,model.wnei,model.hout,model.wout,post_processor.limbs,save_dir)\r\n    return humans\r\n\r\ndef visualize(img,img_id,humans,predicts,hnei,wnei,hout,wout,limbs,save_dir):\r\n    print(f""{len(humans)} human found!"")\r\n    print(""visualizing..."")\r\n    os.makedirs(save_dir,exist_ok=True)\r\n    img_w,img_h,_=img.shape\r\n    pc,px,py,pw,ph,pe=predicts\r\n    ori_img=np.clip(img*255.0,0.0,255.0).astype(np.uint8)\r\n    #show input image\r\n    fig=plt.figure(figsize=(8,8))\r\n    a=fig.add_subplot(2,2,1)\r\n    a.set_title(""input image"")\r\n    plt.imshow(ori_img)\r\n    #show output image\r\n    vis_img=ori_img.copy()\r\n    for human in humans:\r\n        human.print()\r\n        vis_img=human.draw_human(vis_img)\r\n    a=fig.add_subplot(2,2,2)\r\n    a.set_title(""output result"")\r\n    plt.imshow(vis_img)\r\n    #show parts and edges\r\n    vis_img=ori_img.copy()\r\n    vis_img=draw_bbx(vis_img,pc,px,py,pw,ph,threshold=0.7)\r\n    vis_img=draw_edge(vis_img,pe,px,py,pw,ph,hnei,wnei,hout,wout,limbs,threshold=0.7)\r\n    a=fig.add_subplot(2,2,3)\r\n    a.set_title(""bbxs and edges"")\r\n    plt.imshow(vis_img)\r\n    #save result\r\n    plt.savefig(f""{save_dir}/{img_id}_visualize.png"")\r\n    plt.close()\r\n\r\ndef _map_fn(image_file,image_id):\r\n    #load data\r\n    image = tf.io.read_file(image_file)\r\n    image = tf.image.decode_jpeg(image, channels=3)  # get RGB with 0~1\r\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\r\n    return image,image_id\r\n\r\ndef evaluate(model,dataset,config,vis_num=30,total_eval_num=30):\r\n    \'\'\'evaluate pipeline of poseProposal class models\r\n\r\n    input model and dataset, the evaluate pipeline will start automaticly\r\n    the evaluate pipeline will:\r\n    1.loading newest model at path ./save_dir/model_name/model_dir/newest_model.npz\r\n    2.perform inference and parsing over the chosen evaluate dataset\r\n    3.visualize model output in evaluation in directory ./save_dir/model_name/eval_vis_dir\r\n    4.output model metrics by calling dataset.official_eval()\r\n\r\n    Parameters\r\n    ----------\r\n    arg1 : tensorlayer.models.MODEL\r\n        a preset or user defined model object, obtained by Model.get_model() function\r\n    \r\n    arg2 : dataset\r\n        a constructed dataset object, obtained by Dataset.get_dataset() function\r\n    \r\n    arg3 : Int\r\n        an Integer indicates how many model output should be visualized\r\n    \r\n    arg4 : Int\r\n        an Integer indicates how many images should be evaluated\r\n\r\n    Returns\r\n    -------\r\n    None\r\n    \'\'\'\r\n    model.load_weights(os.path.join(config.model.model_dir,""newest_model.npz""))\r\n    pd_anns=[]\r\n    vis_dir=config.eval.vis_dir\r\n    dataset_type=dataset.get_dataset_type()\r\n    kpt_converter=get_output_kptcvter(dataset_type)\r\n    post_processor=Post_Processor(get_parts(dataset_type),get_limbs(dataset_type),get_colors(dataset_type))\r\n    \r\n    eval_dataset=dataset.get_eval_dataset()\r\n    paramed_map_fn=partial(_map_fn)\r\n    eval_dataset=eval_dataset.map(paramed_map_fn,num_parallel_calls=max(multiprocessing.cpu_count()//2,1))\r\n    for eval_num,(img,img_id) in enumerate(eval_dataset):\r\n        if(eval_num>=total_eval_num):\r\n            break\r\n        if(eval_num<=vis_num):\r\n            humans=infer_one_img(model,post_processor,img,img_id,is_visual=True,save_dir=vis_dir)\r\n        else:\r\n            humans=infer_one_img(model,post_processor,img,img_id,is_visual=False,save_dir=vis_dir)\r\n        for human in humans:\r\n            ann={}\r\n            ann[""category_id""]=1\r\n            ann[""image_id""]=int(img_id.numpy())\r\n            ann[""id""]=human.get_global_id()\r\n            ann[""area""]=human.get_area()\r\n            ann[""score""]=human.get_score()\r\n            kpt_list=[]\r\n            for part_idx in range(0,len(post_processor.parts)):\r\n                if(part_idx not in human.body_parts):\r\n                    kpt_list.append([-1000,-1000])\r\n                else:\r\n                    body_part=human.body_parts[part_idx]\r\n                    kpt_list.append([body_part.get_x(),body_part.get_y()])\r\n            ann[""keypoints""]=kpt_converter(kpt_list)\r\n            pd_anns.append(ann)   \r\n        #debug\r\n        if(eval_num%10==0):\r\n            print(f""evaluaing {eval_num}/{len(list(eval_dataset))}..."")\r\n    result_dic={""annotations"":pd_anns}\r\n    dataset.official_eval(result_dic,vis_dir)\r\n\r\n\r\n            \r\n\r\n        \r\n\r\n'"
Hyperpose/Model/pose_proposal/infer.py,0,"b'import os\r\nimport cv2\r\nimport json\r\nimport numpy as np\r\nfrom scipy import optimize\r\nfrom ..human import Human,BodyPart\r\nfrom .utils import non_maximium_supress\r\n\r\nclass Post_Processor:\r\n    def __init__(self,parts,limbs,colors,thresh_hold=0.3,eps=1e-8):\r\n        self.parts=parts\r\n        self.limbs=limbs\r\n        self.colors=colors\r\n        self.n_pos=len(self.parts)\r\n        self.n_limb=len(self.limbs)\r\n        self.eps=eps\r\n        self.cur_id=1\r\n        self.instance_id=1\r\n        self.thres_part_score=0.15\r\n        self.thres_edge_score=0.1\r\n        self.thres_nms=0.3\r\n        self.thres_part_cnt=4\r\n        self.thres_human_score=0.1\r\n\r\n    def process(self,pc,pi,px,py,pw,ph,pe):\r\n        def get_loc(idx,h,w):\r\n            y=idx//w\r\n            x=idx%w\r\n            return y,x\r\n\r\n        #reform output\r\n        _,hout,wout=pc.shape\r\n        L,hnei,wnei,_,_=pe.shape\r\n        bipart_num=hout*wout\r\n        pc=np.clip(pc,0.0,np.inf)\r\n        pi=np.clip(pi,0.0,np.inf)\r\n        pe=np.clip(pe,0.0,np.inf)\r\n        pd_score=(pc).reshape([self.n_pos,bipart_num])\r\n        e_score=np.zeros(shape=(L,bipart_num,bipart_num))\r\n        px=px.reshape([self.n_pos,bipart_num])\r\n        py=py.reshape([self.n_pos,bipart_num])\r\n        pw=pw.reshape([self.n_pos,bipart_num])\r\n        ph=ph.reshape([self.n_pos,bipart_num])\r\n        #construct bbxs\r\n        bbxs_list=[]\r\n        scores_list=[]\r\n        bbxids_list=[]\r\n        assems_list=[]\r\n        for part_idx in range(0,self.n_pos):\r\n            x,y=px[part_idx],py[part_idx]\r\n            w,h=pw[part_idx],ph[part_idx]\r\n            bbxs=np.array([x,y,w,h]).transpose()\r\n            scores=pd_score[part_idx]\r\n            #filte bbxs by score\r\n            filter_ids=np.where(scores>self.thres_part_score)[0]\r\n            filter_bbxs=bbxs[filter_ids]\r\n            filter_scores=scores[filter_ids]\r\n\r\n            #non-maximium supress\r\n            left_bbxids=non_maximium_supress(filter_bbxs,filter_scores,self.thres_nms)\r\n            #print(f""test filter_len:{len(filter_ids)} left_len:{len(left_bbxids)}"")\r\n            #print(f""test filter_ids:{filter_ids} left_bbxids:{left_bbxids} final_ids:{filter_ids[left_bbxids]}"")\r\n            bbxs_list.append(filter_bbxs[left_bbxids])\r\n            scores_list.append(filter_scores[left_bbxids])\r\n            bbxids_list.append(filter_ids[left_bbxids])\r\n            assems_list.append(np.full_like(scores_list[-1],-1))\r\n\r\n            #print(f""test nms:\\n part:{self.parts(part_idx)}\\n chosen_idxs:{bbxids_list[-1]}\\n"")\r\n        #new assemble\r\n        #init egde score\r\n        for l,limb in enumerate(self.limbs):\r\n            for src_id in range(0,bipart_num):\r\n                src_y,src_x=get_loc(src_id,hout,wout)\r\n                for dst_id in range(0,bipart_num):\r\n                    dst_y,dst_x=get_loc(dst_id,hout,wout)\r\n                    delta_y=dst_y-src_y\r\n                    delta_x=dst_x-src_x\r\n                    if((abs(delta_y)>hnei//2) or (abs(delta_x)>wnei//2)):\r\n                        continue \r\n                    e_score[l][src_id][dst_id]=pe[l][delta_y+hnei//2][delta_x+wnei//2][src_y][src_x]\r\n        e_score=e_score*np.where(e_score>=self.thres_edge_score,1,0)\r\n        #init instance id\r\n        for p_id in range(0,len(bbxs_list[self.instance_id])):\r\n            assems_list[self.instance_id][p_id]=p_id\r\n        #assemble limbs\r\n        for l,limb in enumerate(self.limbs):\r\n            src_part_idx,dst_part_idx=limb\r\n            src_score_list=scores_list[src_part_idx]\r\n            src_bbxid_list=bbxids_list[src_part_idx]\r\n            dst_score_list=scores_list[dst_part_idx]\r\n            dst_bbxid_list=bbxids_list[dst_part_idx]\r\n            match_score=np.zeros(shape=(len(src_score_list),len(dst_score_list)))\r\n            for i,(src_score,src_id) in enumerate(zip(src_score_list,src_bbxid_list)):\r\n                for j,(dst_score,dst_id) in enumerate(zip(dst_score_list,dst_bbxid_list)):\r\n                    match_score[i][j]=src_score*e_score[l][src_id][dst_id]*dst_score\r\n            num_conn=min(len(src_score_list),len(dst_score_list))\r\n            conn_list=[]\r\n            for _ in range(0,num_conn):\r\n                max_score=np.max(match_score)\r\n                if(max_score==0):\r\n                    break\r\n                src_ids, dst_ids=np.nonzero(match_score == max_score)\r\n                conn_list.append((src_ids[0],dst_ids[0],max_score))\r\n                src_id=src_ids[0]\r\n                src_score=src_score_list[src_id]\r\n                src_bbx=bbxs_list[src_part_idx][src_id]\r\n                \r\n                dst_id=dst_ids[0]\r\n                dst_score=dst_score_list[dst_id]\r\n                dst_bbx=bbxs_list[dst_part_idx][dst_id]\r\n\r\n                match_score[src_ids[0],:]=0\r\n                match_score[:,dst_ids[0]]=0\r\n            for conn in conn_list:\r\n                src_id,dst_id,conn_score=conn\r\n                assems_list[dst_part_idx][dst_id]=assems_list[src_part_idx][src_id]\r\n                #update score\r\n                scores_list[dst_part_idx][dst_id]=conn_score\r\n        #assemble humans\r\n        humans=[]\r\n        for _ in range(0,len(bbxs_list[self.instance_id])):\r\n            humans.append(Human(self.parts,self.limbs,self.colors))\r\n        for part_idx in range(0,self.n_pos):\r\n            bbxs,scores,bbx_ids,assem_ids=bbxs_list[part_idx],scores_list[part_idx],bbxids_list[part_idx],assems_list[part_idx]\r\n            for bbx,score,bbx_id,assem_id in zip(bbxs,scores,bbx_ids,assem_ids):\r\n                if(assem_id==-1):\r\n                    continue\r\n                loc_y,loc_x=get_loc(bbx_id,hout,wout)\r\n                x,y,w,h=bbx\r\n                humans[assem_id.astype(np.int)].body_parts[part_idx]=BodyPart(parts=self.parts,u_idx=f""{loc_y}-{loc_x}"",part_idx=part_idx,\\\r\n                    x=x,y=y,score=score,w=w,h=h)\r\n        return humans\r\n\r\n'"
Hyperpose/Model/pose_proposal/model.py,15,"b'import numpy as np\r\nimport tensorflow as tf\r\nimport tensorlayer as tl\r\nfrom tensorlayer import layers\r\nfrom tensorlayer.layers import BatchNorm2d, Conv2d, DepthwiseConv2d, LayerList, MaxPool2d\r\nfrom tensorlayer.models import Model\r\n\r\nclass PoseProposal(Model):\r\n    def __init__(self,K_size=18,L_size=17,win=384,hin=384,wout=12,hout=12,wnei=9,hnei=9\\\r\n        ,lmd_rsp=0.25,lmd_iou=1,lmd_coor=5,lmd_size=5,lmd_limb=0.5,backbone=None,data_format=""channels_first""):\r\n        super().__init__()\r\n        #construct params\r\n        self.K=K_size\r\n        self.L=L_size\r\n        self.win=win\r\n        self.hin=hin\r\n        self.wout=wout\r\n        self.hout=hout\r\n        self.hnei=hnei\r\n        self.wnei=wnei\r\n        self.n_pos=K_size\r\n        self.lmd_rsp=lmd_rsp\r\n        self.lmd_iou=lmd_iou\r\n        self.lmd_coor=lmd_coor\r\n        self.lmd_size=lmd_size\r\n        self.lmd_limb=lmd_limb\r\n        self.data_format=data_format\r\n        \r\n        self.output_dim=6*self.K+self.hnei*self.wnei*self.L\r\n        #construct networks\r\n        if(backbone==None):\r\n            self.backbone=self.Resnet_18(n_filter=512,in_channels=3,data_format=data_format)\r\n        else:\r\n            self.backbone=backbone(scale_size=32,data_format=self.data_format)\r\n        self.add_layer_1=LayerList([\r\n            Conv2d(n_filter=512,in_channels=self.backbone.out_channels,filter_size=(3,3),strides=(1,1),data_format=self.data_format),\r\n            BatchNorm2d(decay=0.9,act=lambda x:tl.act.leaky_relu(x,alpha=0.1),is_train=True,num_features=512,data_format=self.data_format)\r\n        ])\r\n        self.add_layer_2=LayerList([\r\n            Conv2d(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),data_format=self.data_format),\r\n            BatchNorm2d(decay=0.9,act=lambda x:tl.act.leaky_relu(x,alpha=0.1),is_train=True,num_features=512,data_format=self.data_format)\r\n        ])\r\n        self.add_layer_3=Conv2d(n_filter=self.output_dim,in_channels=512,filter_size=(1,1),strides=(1,1),data_format=self.data_format)\r\n\r\n    @tf.function\r\n    def forward(self,x,is_train=False):\r\n        x=self.backbone.forward(x)\r\n        x=self.add_layer_1.forward(x)\r\n        x=self.add_layer_2.forward(x)\r\n        x=self.add_layer_3.forward(x)\r\n        if(self.data_format==""channels_first""):\r\n            pc=x[:,0:self.K,:,:]\r\n            pi=x[:,self.K:2*self.K,:,:]\r\n            px=x[:,2*self.K:3*self.K,:,:]\r\n            py=x[:,3*self.K:4*self.K,:,:]\r\n            pw=x[:,4*self.K:5*self.K,:,:]\r\n            ph=x[:,5*self.K:6*self.K,:,:]\r\n            pe=tf.reshape(x[:,6*self.K:,:,:],[-1,self.L,self.wnei,self.hnei,self.wout,self.hout])\r\n        else:\r\n            pc=x[:,:,:,0:self.K]\r\n            pi=x[:,:,:,self.K:2*self.K]\r\n            px=x[:,:,:,2*self.K:3*self.K]\r\n            py=x[:,:,:,3*self.K:4*self.K]\r\n            pw=x[:,:,:,4*self.K:5*self.K]\r\n            ph=x[:,:,:,5*self.K:6*self.K]\r\n            pe=tf.reshape(x[:,:,:,6*self.K:],[-1,self.wnei,self.hnei,self.wout,self.hout,self.L])\r\n        if(is_train==False):\r\n            px,py,pw,ph=self.restore_coor(px,py,pw,ph)\r\n        return pc,pi,px,py,pw,ph,pe\r\n    \r\n    @tf.function\r\n    def infer(self,x):\r\n        pc,pi,px,py,pw,ph,pe=self.forward(x,is_train=False)\r\n        return pc,pi,px,py,pw,ph,pe\r\n    \r\n    def restore_coor(self,x,y,w,h):\r\n        grid_size_x=self.win/self.wout\r\n        grid_size_y=self.hin/self.hout\r\n        grid_x,grid_y=tf.meshgrid(np.arange(self.wout).astype(np.float32),np.arange(self.hout).astype(np.float32))\r\n        if(self.data_format==""channels_last""):\r\n            grid_size_x=grid_size_x[:,:,np.newaxis]\r\n            grid_size_y=grid_size_y[:,:,np.newaxis]\r\n        rx=(x+grid_x)*grid_size_x\r\n        ry=(y+grid_y)*grid_size_y\r\n        rw=(w**2)*self.win\r\n        rh=(h**2)*self.hin\r\n        return rx,ry,rw,rh\r\n    \r\n    def cal_iou(self,bbx1,bbx2):\r\n        #input x,y are the center of bbx\r\n        x1,y1,w1,h1=bbx1\r\n        x2,y2,w2,h2=bbx2\r\n        area1=w1*h1\r\n        area2=w2*h2\r\n        inter_x=tf.nn.relu(tf.minimum(x1+w1/2,x2+w2/2)-tf.maximum(x1-w1/2,x2-w2/2))\r\n        inter_y=tf.nn.relu(tf.minimum(y1+h1/2,y2+h2/2)-tf.maximum(y1-h1/2,y2-h2/2))\r\n        inter_area=inter_x*inter_y\r\n        union_area=area1+area2-inter_area+1e-6\r\n        return inter_area/union_area\r\n\r\n    def cal_loss(self,delta,tx,ty,tw,th,te,te_mask,pc,pi,px,py,pw,ph,pe):\r\n        rtx,rty,rtw,rth=self.restore_coor(tx,ty,tw,th)\r\n        rx,ry,rw,rh=self.restore_coor(px,py,pw,ph)\r\n        ti=self.cal_iou((rtx,rty,rtw,rth),(rx,ry,rw,rh))\r\n        loss_rsp=self.lmd_rsp*tf.reduce_mean(tf.reduce_sum((delta-pc)**2,axis=[1,2,3]))\r\n        loss_iou=self.lmd_iou*tf.reduce_mean(tf.reduce_sum(delta*((ti-pi)**2),axis=[1,2,3]))\r\n        loss_coor=self.lmd_coor*tf.reduce_mean(tf.reduce_sum(delta*((tx-px)**2+(ty-py)**2),axis=[1,2,3]))\r\n        loss_size=self.lmd_size*tf.reduce_mean(tf.reduce_sum(delta*((tw-pw)**2+(th-ph)**2),axis=[1,2,3]))\r\n        loss_limb=self.lmd_limb*tf.reduce_mean(tf.reduce_sum(te_mask*((te-pe)**2),axis=[1,2,3,4,5]))\r\n        return loss_rsp,loss_iou,loss_coor,loss_size,loss_limb\r\n    \r\n    class Resnet_18(Model):\r\n        def __init__(self,n_filter=512,in_channels=3,data_format=""channels_first""):\r\n            super().__init__()\r\n            self.data_format=data_format\r\n            self.out_channels=n_filter\r\n            self.conv1=Conv2d(n_filter=64,in_channels=in_channels,filter_size=(7,7),strides=(2,2),b_init=None,data_format=self.data_format)\r\n            self.bn1=BatchNorm2d(decay=0.9,act=tf.nn.relu,is_train=True,num_features=64,data_format=self.data_format)\r\n            self.maxpool=MaxPool2d(filter_size=(3,3),strides=(2,2),data_format=self.data_format)\r\n            self.res_block_2_1=self.Res_block(n_filter=64,in_channels=64,strides=(1,1),is_down_sample=False,data_format=self.data_format)\r\n            self.res_block_2_2=self.Res_block(n_filter=64,in_channels=64,strides=(1,1),is_down_sample=False,data_format=self.data_format)\r\n            self.res_block_3_1=self.Res_block(n_filter=128,in_channels=64,strides=(2,2),is_down_sample=True,data_format=self.data_format)\r\n            self.res_block_3_2=self.Res_block(n_filter=128,in_channels=128,strides=(1,1),is_down_sample=False,data_format=self.data_format)\r\n            self.res_block_4_1=self.Res_block(n_filter=256,in_channels=128,strides=(2,2),is_down_sample=True,data_format=self.data_format)\r\n            self.res_block_4_2=self.Res_block(n_filter=256,in_channels=256,strides=(1,1),is_down_sample=False,data_format=self.data_format)\r\n            self.res_block_5_1=self.Res_block(n_filter=n_filter,in_channels=256,strides=(2,2),is_down_sample=True,data_format=self.data_format)\r\n        \r\n        def forward(self,x):\r\n            x=self.conv1.forward(x)\r\n            x=self.bn1.forward(x)\r\n            x=self.maxpool.forward(x)\r\n            x=self.res_block_2_1.forward(x)\r\n            x=self.res_block_2_2.forward(x)\r\n            x=self.res_block_3_1.forward(x)\r\n            x=self.res_block_3_2.forward(x)\r\n            x=self.res_block_4_1.forward(x)\r\n            x=self.res_block_4_2.forward(x)\r\n            x=self.res_block_5_1.forward(x)\r\n            return x\r\n\r\n        class Res_block(Model):\r\n            def __init__(self,n_filter,in_channels,strides=(1,1),is_down_sample=False,data_format=""channels_first""):\r\n                super().__init__()\r\n                self.data_format=data_format\r\n                self.is_down_sample=is_down_sample\r\n                self.main_block=LayerList([\r\n                Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=(3,3),strides=strides,b_init=None,data_format=self.data_format),\r\n                BatchNorm2d(decay=0.9,act=tf.nn.relu,is_train=True,num_features=n_filter,data_format=self.data_format),\r\n                Conv2d(n_filter=n_filter,in_channels=n_filter,filter_size=(3,3),strides=(1,1),b_init=None,data_format=self.data_format),\r\n                BatchNorm2d(decay=0.9,is_train=True,num_features=n_filter,data_format=self.data_format),\r\n                ])\r\n                if(self.is_down_sample):\r\n                    self.down_sample=LayerList([\r\n                        Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=(3,3),strides=strides,b_init=None,data_format=self.data_format),\r\n                        BatchNorm2d(decay=0.9,is_train=True,num_features=n_filter,data_format=self.data_format)\r\n                    ])\r\n\r\n            def forward(self,x):\r\n                res=x\r\n                x=self.main_block.forward(x)\r\n                if(self.is_down_sample):\r\n                    res=self.down_sample.forward(res)\r\n                return tf.nn.relu(x+res)\r\n'"
Hyperpose/Model/pose_proposal/train.py,23,"b'#!/usr/bin/env python3\n\nimport math\nimport multiprocessing\nimport os\nimport cv2\nimport time\nimport sys\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport numpy as np\nimport tensorflow as tf\nimport tensorlayer as tl\nfrom functools import partial\nfrom pycocotools.coco import maskUtils\nimport _pickle as cPickle\nfrom .utils import  tf_repeat, draw_results, get_pose_proposals\nfrom .utils import get_parts,get_limbs,get_input_kptcvter\nfrom ..common import init_log,log,KUNGFU\n\ndef regulize_loss(target_model,weight_decay_factor):\n    re_loss=0\n    regularizer=tf.keras.regularizers.l2(l=weight_decay_factor)\n    for trainable_weight in target_model.trainable_weights:\n        re_loss+=regularizer(trainable_weight)\n    return re_loss\n\ndef _data_aug_fn(image, ground_truth, hin, win, hout, wout, hnei, wnei, parts, limbs, kpt_cvter, data_format=""channels_first""):\n    """"""Data augmentation function.""""""\n    #restore data\n    ground_truth = cPickle.loads(ground_truth.numpy())\n    image=image.numpy()\n    annos = ground_truth[""obj""]\n    mask = ground_truth[""mask""]\n    bbxs = ground_truth[""bbx""]\n    #kepoint transform\n    img_h,img_w,_=image.shape\n    for anno_idx in range(0,len(annos)):\n        annos[anno_idx]=kpt_cvter(annos[anno_idx])\n    annos=np.array(annos).astype(np.float32)\n    bbxs=np.array(bbxs).astype(np.float32)\n    scale_w=np.float32(win/img_w)\n    scale_h=np.float32(hin/img_h)\n    annos[:,:,0]*=scale_w\n    annos[:,:,1]*=scale_h\n    #bbx transform\n    bbxs[:,0]*=scale_w\n    bbxs[:,1]*=scale_h\n    bbxs[:,2]*=scale_w\n    bbxs[:,3]*=scale_h\n    # decode mask\n    h_mask, w_mask, _ = np.shape(image)\n    mask_miss = np.ones((h_mask, w_mask), dtype=np.uint8)\n    if(mask!=None):\n        for seg in mask:\n            bin_mask = maskUtils.decode(seg)\n            bin_mask = np.logical_not(bin_mask)\n            mask_miss = np.bitwise_and(mask_miss, bin_mask)\n    #image transform\n    image=cv2.resize(image,dsize=(win,hin))\n    mask_miss=cv2.resize(mask_miss,dsize=(win,hin))\n    \n    # generate result which include proposal region x,y,w,h,edges\n    delta,tx,ty,tw,th,te,te_mask=get_pose_proposals(annos,bbxs,hin,win,hout,wout,hnei,wnei,parts,limbs,mask_miss,data_format)\n\n    #generate output masked image, result map and maskes\n    img_mask = mask_miss[:,:,np.newaxis]\n    image = image * np.repeat(img_mask, 3, 2)\n    if(data_format==""channels_first""):\n        image=np.transpose(image,[2,0,1])\n    return image,delta,tx,ty,tw,th,te,te_mask\n\ndef _map_fn(img_list, annos, data_aug_fn, hin, win):\n    """"""TF Dataset pipeline.""""""\n    #load data\n    image = tf.io.read_file(img_list)\n    image = tf.image.decode_jpeg(image, channels=3)  # get RGB with 0~1\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    #data augmentation using affine transform and get paf maps\n    image,delta,tx,ty,tw,th,te,te_mask= tf.py_function(data_aug_fn, [image, annos], [tf.float32, tf.float32,\\\n         tf.float32,tf.float32, tf.float32, tf.float32,tf.float32, tf.float32])\n    #data augmentaion using tf\n    image = tf.image.random_brightness(image, max_delta=45./255.)   # 64./255. 32./255.)  caffe -30~50\n    image = tf.image.random_contrast(image, lower=0.5, upper=1.5)   # lower=0.2, upper=1.8)  caffe 0.3~1.5\n    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n    return image,(delta,tx,ty,tw,th,te,te_mask)\n\ndef get_paramed_map_fn(hin,win,hout,wout,hnei,wnei,parts,limbs,kpt_cvter,data_format=""channels_first""):\n    paramed_data_aug_fn=partial(_data_aug_fn,hin=hin,win=win,hout=hout,wout=wout,hnei=hnei,wnei=wnei,\\\n        parts=parts,limbs=limbs,kpt_cvter=kpt_cvter,data_format=data_format)\n    paramed_map_fn=partial(_map_fn,data_aug_fn=paramed_data_aug_fn,hin=hin, win=win)\n    return paramed_map_fn\n\ndef single_train(train_model,dataset,config):\n    \'\'\'Single train pipeline of PoseProposal class models\n\n    input model and dataset, the train pipeline will start automaticly\n    the train pipeline will:\n    1.store and restore ckpt in directory ./save_dir/model_name/model_dir\n    2.log loss information in directory ./save_dir/model_name/log.txt\n    3.visualize model output periodly during training in directory ./save_dir/model_name/train_vis_dir\n    the newest model is at path ./save_dir/model_name/model_dir/newest_model.npz\n\n    Parameters\n    ----------\n    arg1 : tensorlayer.models.MODEL\n        a preset or user defined model object, obtained by Model.get_model() function\n    \n    arg2 : dataset\n        a constructed dataset object, obtained by Dataset.get_dataset() function\n    \n    \n    Returns\n    -------\n    None\n    \'\'\'\n    init_log(config)\n    #train hyper paramss\n    #dataset params\n    n_step = config.train.n_step\n    batch_size = config.train.batch_size\n    #learning rate params\n    lr_init = config.train.lr_init\n    lr_decay_factor=config.train.lr_decay_factor\n    weight_decay_factor = config.train.weight_decay_factor\n    #log and checkpoint params\n    log_interval=config.log.log_interval\n    save_interval=config.train.save_interval\n    vis_dir=config.train.vis_dir\n    \n    #model hyper params\n    hin = train_model.hin\n    win = train_model.win\n    hout = train_model.hout\n    wout = train_model.wout\n    hnei = train_model.hnei\n    wnei = train_model.wnei\n    model_dir = config.model.model_dir\n\n    \n    print(f""single training using learning rate:{lr_init} batch_size:{batch_size}"")\n    #training dataset configure with shuffle,augmentation,and prefetch\n    train_dataset=dataset.get_train_dataset()\n    dataset_type=dataset.get_dataset_type()\n    parts,limbs,kpt_cvter=get_parts(dataset_type),get_limbs(dataset_type),get_input_kptcvter(dataset_type)\n    data_format=train_model.data_format\n    paramed_map_fn=get_paramed_map_fn(hin,win,hout,wout,hnei,wnei,parts,limbs,kpt_cvter,data_format)\n    train_dataset = train_dataset.shuffle(buffer_size=4096).repeat()\n    train_dataset = train_dataset.map(paramed_map_fn, num_parallel_calls=max(multiprocessing.cpu_count()//2,1))\n    train_dataset = train_dataset.batch(batch_size)  \n    train_dataset = train_dataset.prefetch(64)\n    \n    #train params configure\n    step=tf.Variable(1, trainable=False)\n    lr=tf.Variable(lr_init,trainable=False)\n    opt=tf.keras.optimizers.Adam(learning_rate=lr)\n    ckpt=tf.train.Checkpoint(step=step,optimizer=opt,lr=lr)\n    ckpt_manager=tf.train.CheckpointManager(ckpt,model_dir,max_to_keep=3)\n    \n    #load from ckpt\n    try:\n        ckpt.restore(ckpt_manager.latest_checkpoint)\n    except:\n        log(""ckpt_path doesn\'t exist, step and optimizer are initialized"")\n    try:\n        train_model.load_weights(os.path.join(model_dir,""newest_model.npz""))\n    except:\n        log(""model_path doesn\'t exist, model parameters are initialized"")\n        \n    #optimize one step\n    @tf.function\n    def one_step(image,targets,train_model):\n        step.assign_add(1)\n        with tf.GradientTape() as tape:\n            delta,tx,ty,tw,th,te,te_mask=targets\n            pc,pi,px,py,pw,ph,pe=train_model.forward(image,is_train=True)\n            loss_rsp,loss_iou,loss_coor,loss_size,loss_limb=\\\n                train_model.cal_loss(delta,tx,ty,tw,th,te,te_mask,pc,pi,px,py,pw,ph,pe)\n            pd_loss=loss_rsp+loss_iou+loss_coor+loss_size+loss_limb\n            re_loss=regulize_loss(train_model,weight_decay_factor)\n            total_loss=pd_loss+re_loss\n        \n        gradients=tape.gradient(total_loss,train_model.trainable_weights)\n        opt.apply_gradients(zip(gradients,train_model.trainable_weights))\n        predicts=(pc,px,py,pw,ph,pe)\n        return predicts,targets,pd_loss,re_loss, loss_rsp,loss_iou,loss_coor,loss_size,loss_limb\n\n    #train each step\n    tic=time.time()\n    train_model.train()\n    log(f\'Start - n_step: {n_step} batch_size: {batch_size} lr_init: {lr_init} lr_decay_factor: {lr_decay_factor}\')\n    avg_loss_rsp,avg_loss_iou,avg_loss_coor,avg_loss_size,avg_loss_limb,avg_pd_loss,avg_re_loss=0.,0.,0.,0.,0.,0.,0.\n    for image,targets in train_dataset:\n        #learning rate decay\n        lr=lr_init*(1-step/n_step*lr_decay_factor)\n        #optimize one step\n        predicts,targets,pd_loss,re_loss,loss_rsp,loss_iou,loss_coor,loss_size,loss_limb\\\n            =one_step(image,targets,train_model)\n        \n        avg_loss_rsp+=loss_rsp/log_interval\n        avg_loss_iou+=loss_iou/log_interval\n        avg_loss_coor+=loss_coor/log_interval\n        avg_loss_size+=loss_size/log_interval\n        avg_loss_limb+=loss_limb/log_interval\n        avg_pd_loss+=pd_loss/log_interval\n        avg_re_loss+=re_loss/log_interval\n        #save log info periodly\n        if((step!=0) and (step%log_interval)==0):\n            tic=time.time()\n            log(f""Train iteration {step.numpy()}/{n_step}, learning rate:{lr.numpy()},loss_rsp:{avg_loss_rsp},""+\\\n                    f""loss_iou:{avg_loss_iou},loss_coor:{avg_loss_coor},loss_size:{avg_loss_size},loss_limb:{avg_loss_limb},""+\\\n                        f""loss_pd:{avg_pd_loss},loss_re:{avg_re_loss} ,time:{time.time()-tic}"")\n            avg_loss_rsp,avg_loss_iou,avg_loss_coor,avg_loss_size,avg_loss_limb,avg_pd_loss,avg_re_loss=0.,0.,0.,0.,0.,0.,0.\n            \n        #save result and ckpt periodly\n        if((step!=0) and (step%save_interval)==0):\n            log(""saving model ckpt and result..."")\n            draw_results(image.numpy(),predicts,targets,parts,limbs,save_dir=vis_dir,name=f""ppn_step_{step.numpy()}"")\n            ckpt_save_path=ckpt_manager.save()\n            log(f""ckpt save_path:{ckpt_save_path} saved!\\n"")\n            model_save_path=os.path.join(model_dir,""newest_model.npz"")\n            train_model.save_weights(model_save_path)\n            model_save_path=os.path.join(model_dir,f""step_{step.numpy()}_model.npz"")\n            train_model.save_weights(model_save_path)\n            log(f""model save_path:{model_save_path} saved!\\n"")\n\n        #training finished\n        if(step==n_step):\n            break\n\ndef parallel_train(train_model,dataset,config):\n    \'\'\'Parallel train pipeline of PoseProposal class models\n\n    input model and dataset, the train pipeline will start automaticly\n    the train pipeline will:\n    1.store and restore ckpt in directory ./save_dir/model_name/model_dir\n    2.log loss information in directory ./save_dir/model_name/log.txt\n    3.visualize model output periodly during training in directory ./save_dir/model_name/train_vis_dir\n    the newest model is at path ./save_dir/model_name/model_dir/newest_model.npz\n\n    Parameters\n    ----------\n    arg1 : tensorlayer.models.MODEL\n        a preset or user defined model object, obtained by Model.get_model() function\n    \n    arg2 : dataset\n        a constructed dataset object, obtained by Dataset.get_dataset() function\n    \n    \n    Returns\n    -------\n    None\n    \'\'\'\n    init_log(config)\n    #train hyper params\n    #dataset params\n    n_step = config.train.n_step\n    batch_size = config.train.batch_size\n    #learning rate params\n    lr_init = config.train.lr_init\n    lr_decay_factor=config.train.lr_decay_factor\n    weight_decay_factor = config.train.weight_decay_factor\n    #log and checkpoint params\n    log_interval=config.log.log_interval\n    save_interval=config.train.save_interval\n    vis_dir=config.train.vis_dir\n    \n    #model hyper params\n    hin = train_model.hin\n    win = train_model.win\n    hout = train_model.hout\n    wout = train_model.wout\n    hnei = train_model.hnei\n    wnei = train_model.wnei\n    model_dir = config.model.model_dir\n    \n    #import kungfu\n    from kungfu import current_cluster_size, current_rank\n    from kungfu.tensorflow.initializer import broadcast_variables\n    from kungfu.tensorflow.optimizers import SynchronousSGDOptimizer, SynchronousAveragingOptimizer, PairAveragingOptimizer\n\n    \n    print(f""parallel training using learning rate:{lr_init} batch_size:{batch_size}"")\n    #training dataset configure with shuffle,augmentation,and prefetch\n    train_dataset=dataset.get_train_dataset()\n    dataset_type=dataset.get_dataset_type()\n    parts,limbs,kpt_cvter=get_parts(dataset_type),get_limbs(dataset_type),get_input_kptcvter(dataset_type)\n    data_format=train_model.data_format\n    paramed_map_fn=get_paramed_map_fn(hin,win,hout,wout,hnei,wnei,parts,limbs,kpt_cvter,data_format)\n    train_dataset = train_dataset.shuffle(buffer_size=4096)\n    train_dataset = train_dataset.shard(num_shards=current_cluster_size(),index=current_rank())\n    train_dataset = train_dataset.repeat()\n    train_dataset = train_dataset.map(paramed_map_fn, num_parallel_calls=4)\n    train_dataset = train_dataset.batch(batch_size)  \n    train_dataset = train_dataset.prefetch(buffer_size=2)\n\n    #train model configure\n    step=tf.Variable(1, trainable=False)\n    lr=tf.Variable(lr_init,trainable=False)\n    opt=tf.keras.optimizers.SGD(learning_rate=lr,momentum=0.9)\n    ckpt=tf.train.Checkpoint(step=step,optimizer=opt,lr=lr)\n    ckpt_manager=tf.train.CheckpointManager(ckpt,model_dir,max_to_keep=3)\n    \n\n    #load from ckpt\n    try:\n        ckpt.restore(ckpt_manager.latest_checkpoint)\n    except:\n        log(""ckpt_path doesn\'t exist, step and optimizer are initialized"")\n    try:\n        train_model.load_weights(os.path.join(model_dir,""newest_model.npz""))\n    except:\n        log(""model_path doesn\'t exist, model parameters are initialized"")\n    \n    #Kungfu configure\n    kungfu_option=config.train.kungfu_option\n    if kungfu_option == KUNGFU.Sync_sgd:\n        print(""using Kungfu.SynchronousSGDOptimizer!"")\n        opt = SynchronousSGDOptimizer(opt)\n    elif kungfu_option == KUNGFU.Sync_avg:\n        print(""using Kungfu.SynchronousAveragingOptimize!"")\n        opt = SynchronousAveragingOptimizer(opt)\n    elif kungfu_option == KUNGFU.Pair_avg:\n        print(""using Kungfu.PairAveragingOptimizer!"")\n        opt=PairAveragingOptimizer(opt)\n\n    n_step = n_step // current_cluster_size() + 1  # KungFu\n\n    #optimize one step\n    @tf.function\n    def one_step(image,targets,train_model,is_first_batch=False):\n        step.assign_add(1)\n        with tf.GradientTape() as tape:\n            delta,tx,ty,tw,th,te,te_mask=targets\n            pc,pi,px,py,pw,ph,pe=train_model.forward(image,is_train=True)\n            loss_rsp,loss_iou,loss_coor,loss_size,loss_limb=\\\n                train_model.cal_loss(delta,tx,ty,tw,th,te,te_mask,pc,pi,px,py,pw,ph,pe)\n            pd_loss=loss_rsp+loss_iou+loss_coor+loss_size+loss_limb\n            re_loss=regulize_loss(train_model,weight_decay_factor)\n            total_loss=pd_loss+re_loss\n        \n        gradients=tape.gradient(total_loss,train_model.trainable_weights)\n        opt.apply_gradients(zip(gradients,train_model.trainable_weights))\n        #Kung fu\n        if(is_first_batch):\n            broadcast_variables(train_model.all_weights)\n            broadcast_variables(opt.variables())\n        predicts=(pc,px,py,pw,ph,pe)\n        return predicts,targets,pd_loss,re_loss,loss_rsp,loss_iou,loss_coor,loss_size,loss_limb\n\n    #train each step\n    tic=time.time()\n    train_model.train()\n    log(f""Worker {current_rank()}: Initialized"")\n    log(f\'Start - n_step: {n_step} batch_size: {batch_size} lr_init: {lr_init} lr_decay_factor: {lr_decay_factor}\')\n    avg_loss_rsp,avg_loss_iou,avg_loss_coor,avg_loss_size,avg_loss_limb,avg_pd_loss,avg_re_loss=0.,0.,0.,0.,0.,0.,0.\n    for image,targets in train_dataset:\n        #learning rate decay\n        lr=lr_init*(1-step/n_step*lr_decay_factor)\n        #optimize one step\n        predicts,targets,pd_loss,re_loss,loss_rsp,loss_iou,loss_coor,loss_size,loss_limb=one_step(image,targets,train_model)\n        \n        avg_loss_rsp+=loss_rsp/log_interval\n        avg_loss_iou+=loss_iou/log_interval\n        avg_loss_coor+=loss_coor/log_interval\n        avg_loss_size+=loss_size/log_interval\n        avg_loss_limb+=loss_limb/log_interval\n        avg_pd_loss+=pd_loss/log_interval\n        avg_re_loss+=re_loss/log_interval\n        \n        #save log info periodly\n        if((step!=0) and (step%log_interval)==0):\n            tic=time.time()\n            log(f""worker:{current_rank()} Train iteration {step.numpy()}/{n_step}, learning rate:{lr.numpy()},""+\\\n                    f""loss_rsp:{avg_loss_rsp},loss_iou:{avg_loss_iou},loss_coor:{avg_loss_coor},loss_size:{avg_loss_size},""+\\\n                        f""loss_limb:{avg_loss_limb},loss_pd:{avg_pd_loss},loss_re:{avg_re_loss} ,time:{time.time()-tic}"")\n            avg_loss_rsp,avg_loss_iou,avg_loss_coor,avg_loss_size,avg_loss_limb,avg_pd_loss,avg_re_loss=0.,0.,0.,0.,0.,0.,0.\n\n        #save result and ckpt periodly\n        if((step!=0) and (step%save_interval)==0):\n            log(""saving model ckpt and result..."")\n            draw_results(image.numpy(),predicts,targets,parts,limbs,save_dir=vis_dir,name=f""ppn_step_{step.numpy()}"")\n            ckpt_save_path=ckpt_manager.save()\n            log(f""ckpt save_path:{ckpt_save_path} saved!\\n"")\n            model_save_path=os.path.join(model_dir,""newest_model.npz"")\n            train_model.save_weights(model_save_path)\n            log(f""model save_path:{model_save_path} saved!\\n"")\n\n        #training finished\n        if(step==n_step):\n            break\n\n'"
Hyperpose/Model/pose_proposal/utils.py,3,"b'import os\nimport cv2\nimport math\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import (del_file, folder_exists, maybe_download_and_extract)\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom distutils.dir_util import mkpath\nfrom scipy.spatial.distance import cdist\nfrom pycocotools.coco import COCO, maskUtils\n\ndef get_pose_proposals(kpts_list,bbxs,hin,win,hout,wout,hnei,wnei,parts,limbs,img_mask=None,data_format=""channels_first""):\n    K,L=len(parts),len(limbs)\n    grid_x=win/wout\n    grid_y=hin/hout\n    delta=np.zeros(shape=(K,hout,wout))\n    tx=np.zeros(shape=(K,hout,wout))\n    ty=np.zeros(shape=(K,hout,wout))\n    tw=np.zeros(shape=(K,hout,wout))\n    th=np.zeros(shape=(K,hout,wout))\n    te=np.zeros(shape=(L,hnei,wnei,hout,wout))\n    te_mask=np.zeros(shape=(L,hnei,wnei,hout,wout))\n    aux_delta=np.zeros(shape=(hout+hnei-1,wout+wnei-1,K,2))\n    #generate pose proposals for each labels person\n    for human_idx,(kpts,bbx) in enumerate(zip(kpts_list,bbxs)):\n        #change the background keypoint to the added human instance keypoint\n        _,_,ins_w,ins_h=bbx\n        part_size=int(max(ins_w,ins_h)/8)\n        instance_size=int(max(ins_w,ins_h)/4)\n        for k,kpt in enumerate(kpts):\n            x,y=kpt[0],kpt[1]\n            if(x<0 or y<0 or x>=win or y>=hin):\n                continue\n            if(img_mask.all()!=None):\n                #joints are masked\n                if(img_mask[int(x),int(y)]==0):\n                    continue\n            cx,cy=x/grid_x,y/grid_y\n            ix,iy=int(cx),int(cy)\n            delta[k,iy,ix]=1\n            aux_delta[iy+(hnei//2),ix+(wnei//2),k,0]=1\n            aux_delta[iy+(hnei//2),ix+(wnei//2),k,1]=human_idx\n            tx[k,iy,ix]=cx-ix\n            ty[k,iy,ix]=cy-iy\n            if(k==parts.Instance.value):\n                size=instance_size\n            else:\n                size=part_size\n            tw[k,iy,ix]=(size/win)**0.5\n            th[k,iy,ix]=(size/hin)**0.5\n                \n    #generate te and mask\n    np_limbs=np.array(limbs)\n    limbs_start=np_limbs[:,0]\n    limbs_end=np_limbs[:,1]\n    for iy in range(0,hout):\n        for ix in range(0,wout):\n            start=aux_delta[iy+(hnei//2),ix+(wnei//2),limbs_start,:]\n            end=aux_delta[iy:iy+(hnei//2)*2+1,ix:ix+(wnei//2)*2+1,limbs_end,:]\n            te_mask[:,:,:,iy,ix]=(np.maximum(start[:,0],end[:,:,:,0])).transpose(2,0,1)\n            condition=np.logical_and((start[:,0]*end[:,:,:,0]==1),start[:,1]==end[:,:,:,1])\n            te[:,:,:,iy,ix]=(np.where(condition,1,0)).transpose(2,0,1)\n    \n    if(data_format==""channels_last""):\n        delta=np.transpose(delta,[1,2,0])\n        tx=np.transpose(tx,[1,2,0])\n        ty=np.transpose(ty,[1,2,0])\n        tw=np.transpose(tw,[1,2,0])\n        th=np.transpose(th,[1,2,0])\n        te=np.transpose(te,[1,2,3,4,0])\n        te_mask=np.transpose(te_mask,[1,2,3,4,0])\n\n    return delta,tx,ty,tw,th,te,te_mask\n\ndef draw_bbx(img,img_pc,rx,ry,rw,rh,threshold=0.7):\n    color=(0,255,0)\n    valid_idxs=np.where(img_pc>=threshold,1,0)\n    ks,iys,ixs=np.nonzero(valid_idxs)\n    for k,iy,ix in zip(ks,iys,ixs):\n        x=rx[k][iy][ix]\n        y=ry[k][iy][ix]\n        w=rw[k][iy][ix]\n        h=rh[k][iy][ix]\n        img=cv2.circle(img,(int(x),int(y)),radius=2,color=color,thickness=-1)\n        img=cv2.rectangle(img,(int(x-w//2),int(y-h//2)),(int(x+w//2),int(y+h//2)),color,1)\n    return img\n\ndef draw_edge(img,img_e,rx,ry,rw,rh,hnei,wnei,hout,wout,limbs,threshold=0.7):\n    color=(255,0,0)\n    valid_idxs=np.where(img_e>=threshold,1,0)\n    ls,niys,nixs,iys,ixs=np.nonzero(valid_idxs)\n    for l,niy,nix,iy,ix in zip(ls,niys,nixs,iys,ixs):\n        s_id=limbs[l][0]\n        d_id=limbs[l][1]\n        #get src point\n        src_ix=ix\n        src_iy=iy\n        src_x=rx[s_id][src_iy][src_ix]\n        src_y=ry[s_id][src_iy][src_ix]\n        #get dst point\n        dst_ix=ix-wnei//2+nix\n        dst_iy=iy-hnei//2+niy\n        if(dst_ix<0 or dst_ix>=wout or dst_iy<0 or dst_iy>=hout):\n            continue\n        dst_x=rx[d_id][dst_iy][dst_ix]\n        dst_y=ry[d_id][dst_iy][dst_ix]\n        #draw line\n        img=cv2.line(img,(int(src_x),int(src_y)),(int(dst_x),int(dst_y)),color,2)\n    return img\n\ndef draw_results(img,predicts,targets,parts,limbs,save_dir,threshold=0.3,name="""",is_train=True,data_format=""channels_first""):\n    pc,px,py,pw,ph,pe=predicts\n    if(data_format==""channels_last""):\n        pc=np.transpose(pc,[0,3,1,2])\n        px=np.transpose(px,[0,3,1,2])\n        py=np.transpose(py,[0,3,1,2])\n        pw=np.transpose(pw,[0,3,1,2])\n        ph=np.transpose(ph,[0,3,1,2])\n        pe=np.transpose(pe,[0,5,1,2,3,4])\n    else:\n        img=np.transpose(img,[0,2,3,1])\n    if(is_train):\n        tc,tx,ty,tw,th,te,_=targets\n        if(data_format==""channels_last""):\n            tc=np.transpose(tc,[0,3,1,2])\n            tx=np.transpose(tx,[0,3,1,2])\n            ty=np.transpose(ty,[0,3,1,2])\n            tw=np.transpose(tw,[0,3,1,2])\n            th=np.transpose(th,[0,3,1,2])\n            te=np.transpose(te,[0,5,1,2,3,4])\n\n    img=np.clip(img*255.0,0,255).astype(np.uint8)\n    batch_size,hin,win,_=img.shape\n    batch_size,K,hout,wout=px.shape\n    batch_size,L,hnei,wnei,hout,wout=pe.shape\n    if(is_train):\n        rtx,rty,rtw,rth=restore_coor(tx,ty,tw,th,win,hin,wout,hout)\n        rpx,rpy,rpw,rph=restore_coor(px,py,pw,ph,win,hin,wout,hout)\n        for b_idx in range(0,batch_size):\n            fig=plt.figure(figsize=(8,8))\n            #draw original image\n            sub_plot=fig.add_subplot(2,2,1)\n            sub_plot.set_title(""Originl image"")\n            sub_plot.imshow(img[b_idx].copy())\n            #draw predict image\n            sub_plot=fig.add_subplot(2,2,2)\n            sub_plot.set_title(""Predict image"")\n            img_pd=img[b_idx].copy()\n            img_pd=draw_bbx(img_pd,pc[b_idx],rpx[b_idx],rpy[b_idx],rpw[b_idx],rph[b_idx],threshold)\n            img_pd=draw_edge(img_pd,pe[b_idx],rpx[b_idx],rpy[b_idx],rpw[b_idx],rph[b_idx],hnei,wnei,hout,wout,limbs,threshold)\n            sub_plot.imshow(img_pd)\n            #draw ground truth image\n            sub_plot=fig.add_subplot(2,2,3)\n            sub_plot.set_title(""True image"")\n            img_gt=img[b_idx].copy()\n            img_gt=draw_bbx(img_gt,tc[b_idx],rtx[b_idx],rty[b_idx],rtw[b_idx],rth[b_idx],threshold)\n            img_gt=draw_edge(img_gt,te[b_idx],rtx[b_idx],rty[b_idx],rtw[b_idx],rth[b_idx],hnei,wnei,hout,wout,limbs,threshold)\n            sub_plot.imshow(img_gt)\n            #save figure\n            plt.savefig(os.path.join(save_dir, \'%s_%d.png\' % (name, b_idx)), dpi=300)\n            plt.close()\n    else:\n        rpx,rpy,rpw,rph=px,py,pw,ph\n        for b_idx in range(0,batch_size):\n            fig=plt.figure(figsize=(8,8))\n            #draw original image\n            sub_plot=fig.add_subplot(1,2,1)\n            sub_plot.set_title(""Originl image"")\n            sub_plot.imshow(img[b_idx].copy())\n            #draw predict image\n            sub_plot=fig.add_subplot(1,2,2)\n            sub_plot.set_title(""Predict image"")\n            img_pd=img[b_idx].copy()\n            img_pd=draw_bbx(img_pd,pc[b_idx],rpx[b_idx],rpy[b_idx],rpw[b_idx],rph[b_idx],threshold)\n            img_pd=draw_edge(img_pd,pe[b_idx],rpx[b_idx],rpy[b_idx],rpw[b_idx],rph[b_idx],hnei,wnei,hout,wout,limbs,threshold)\n            sub_plot.imshow(img_pd)\n            #save figure\n            plt.savefig(os.path.join(save_dir, \'%s_%d.png\' % (name, b_idx)), dpi=300)\n            plt.close()\n\ndef restore_coor(x,y,w,h,win,hin,wout,hout,data_format=""channels_first""):\n        grid_size_x=win/wout\n        grid_size_y=hin/hout\n        grid_x,grid_y=tf.meshgrid(np.arange(wout).astype(np.float32),np.arange(hout).astype(np.float32))\n        if(data_format==""channels_last""):\n            grid_x=grid_x[...,np.newaxis]\n            grid_y=grid_y[...,np.newaxis]\n        rx=(x+grid_x)*grid_size_x\n        ry=(y+grid_y)*grid_size_y\n        rw=(w**2)*win\n        rh=(h**2)*hin\n        return rx,ry,rw,rh\n\ndef cal_iou(bbx1,bbx2):\n    x1,y1,w1,h1=bbx1\n    x2,y2,w2,h2=bbx2\n    area1=w1*h1\n    area2=w2*h2\n    inter_x=tf.nn.relu(tf.minimum(x1+w1/2,x2+w2/2)-tf.maximum(x1-w1/2,x2-w2/2))\n    inter_y=tf.nn.relu(tf.minimum(y1+h1/2,y2+h2/2)-tf.maximum(y1-h1/2,y2-h2/2))\n    inter_area=inter_x*inter_y\n    union_area=area1+area2-inter_area\n    return inter_area/union_area\n\ndef non_maximium_supress(bbxs,scores,thres):\n    # bbxs=[4,bipartnum]\n    bbxs_num=bbxs.shape[0]\n    idx=np.linspace(start=0,stop=bbxs_num-1,num=bbxs_num).astype(np.int)[:,np.newaxis]\n    idxed_bbxs=np.concatenate([bbxs,idx],axis=1)\n    chosen_idxs=[]\n    left_bbxs=idxed_bbxs\n    left_scores=scores\n    for _ in range(0,bbxs_num):\n        #sort most convinced bbx\n        sort_idx=np.argsort(-left_scores,axis=0)\n        left_scores=left_scores[sort_idx]\n        left_bbxs=left_bbxs[sort_idx,:]\n        maxconf_bbx=left_bbxs[0]\n        #print(f""max_score:{left_scores[0]} chosen_idx:{maxconf_bbx[4]}\\n"")\n        chosen_idxs.append(maxconf_bbx[4])\n        #calculate iou with other bbxs\n        #ious is the size [left_bbxnum]\n        #print(f""test maxconf_bbx:{maxconf_bbx} lef_bbxs[0]:{left_bbxs[:,0:4].transpose()[:,0]}"")\n        ious=cal_iou(maxconf_bbx[0:4],left_bbxs[:,0:4].transpose())\n        print(f""test iter one ious:{ious}"")\n        left_idx=np.where(ious<thres)[0]\n        #print(f""ious:{ious} left_idx:{left_idx} len_left_idx:{len(left_idx)}"")\n        if(len(left_idx)==0):\n            break\n        else:\n            left_scores=left_scores[left_idx]\n            left_bbxs=left_bbxs[left_idx,:]\n    chosen_idxs=np.array(chosen_idxs).astype(np.int)\n    #print(f""test chosen_idxs"")\n    return chosen_idxs\n\n\ndef vis_annos(image, annos, save_dir ,name=\'\'):\n    """"""Save results for debugging.\n\n    Parameters\n    -----------\n    images : single RGB image\n    annos  : annotation, list of lists\n    """"""\n\n    fig = plt.figure(figsize=(8, 8))\n    a = fig.add_subplot(1, 1, 1)\n\n    plt.imshow(image)\n    for people in annos:\n        for idx, jo in enumerate(people):\n            if jo[0] > 0 and jo[1] > 0:\n                plt.plot(jo[0], jo[1], \'*\')\n\n    plt.savefig(os.path.join(save_dir, \'keypoints%s%d.png\' % (name, i)), dpi=300)\n\n\nfrom .define import CocoPart,CocoLimb,CocoColor,coco_input_converter,coco_output_converter,Coco_fliplist\nfrom .define import MpiiPart,MpiiLimb,MpiiColor,mpii_input_converter,mpii_output_converter,Mpii_flip_list\n\ndef get_parts(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return CocoPart\n    elif(dataset_type==DATA.MPII):\n        return MpiiPart\n\ndef get_limbs(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return CocoLimb\n    elif(dataset_type==DATA.MPII):\n        return MpiiLimb\n\ndef get_colors(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return CocoColor\n    elif(dataset_type==DATA.MPII):\n        return MpiiColor\n\ndef get_input_kptcvter(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return coco_input_converter\n    elif(dataset_type==DATA.MPII):\n        return mpii_input_converter\n\ndef get_output_kptcvter(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return coco_output_converter\n    elif(dataset_type==DATA.MPII):\n        return mpii_output_converter\n\ndef get_flip_list(dataset_type):\n    if(dataset_type==DATA.MSCOCO):\n        return Coco_fliplist\n    elif(dataset_type==DATA.MPII):\n        return Mpii_flip_list\n\nfrom .infer import Post_Processor\nfrom ..common import tf_repeat,TRAIN,MODEL,DATA\n\n\ndef preprocess(annos,bbxs,model_hin,modeL_win,model_hout,model_wout,model_hnei,model_wnei,dataset_type,data_format=""channels_first""):\n    \'\'\'preprocess function of poseproposal class models\n\n    take keypoints annotations, bounding boxs annotatiosn, model input height and width, model limbs neighbor area height,\n    model limbs neighbor area width and dataset type\n    return the constructed targets of delta,tx,ty,tw,th,te,te_mask\n\n    Parameters\n    ----------\n    arg1 : list\n        a list of keypoint annotations, each annotation is a list of keypoints that belongs to a person, each keypoint follows the\n        format (x,y), and x<0 or y<0 if the keypoint is not visible or not annotated.\n        the annotations must from a known dataset_type, other wise the keypoint and limbs order will not be correct.\n\n    arg2 : list\n        a list of bounding box annotations, each bounding box is of format [x,y,w,h]\n\n    arg3 : Int\n        height of the model input\n\n    arg4 : Int\n        width of the model input\n\n    arg5 : Int\n        height of the model output\n\n    arg6 : Int\n        width of the model output\n\n    arg7 : Int\n        model limbs neighbor area height, determine the neighbor area to macth limbs,\n        see pose propsal paper for detail information\n\n    arg8 : Int\n        model limbs neighbor area width, determine the neighbor area to macth limbs,\n        see pose propsal paper for detail information\n\n    arg9 : Config.DATA\n        a enum value of enum class Config.DATA\n        dataset_type where the input annotation list from, because to generate correct\n        conf_map and paf_map, the order of keypoints and limbs should be awared.\n\n    arg10 : string\n        data format speficied for channel order\n        available input:\n        \'channels_first\': data_shape C*H*W\n        \'channels_last\': data_shape H*W*C\n\n    Returns\n    -------\n    list\n        including 7 elements\n        delta: keypoint confidence feature map, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        tx: keypoints bbx center x coordinates, divided by gridsize, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        ty: keypoints bbx center y coordinates, divided by gridsize, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        tw: keypoints bbxs width w, divided by image width, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        th: keypoints bbxs height h, divided by image width, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        te: edge confidence feature map,  shape [C,H,W,Hnei,Wnei](channels_first) or [H,W,Hnei,Wnei,C](channels_last)\n        te_mask: mask of edge confidence feature map, used for loss caculation,\n        shape [C,H,W,Hnei,Wnei](channels_first) or [H,W,Hnei,Wnei,C](channels_last)\n    \'\'\'\n    parts=get_parts(dataset_type)\n    limbs=get_limbs(dataset_type)\n    delta,tx,ty,tw,th,te,te_mask=get_pose_proposals(annos,bbxs,model_hin,modeL_win,model_hout,\\\n        model_wout,model_hnei,model_wnei,parts,limbs,img_mask=None,data_format=data_format)\n    return delta,tx,ty,tw,th,te,te_mask\n\ndef postprocess(predicts,dataset_type,data_format=""channels_first""):\n    \'\'\'postprocess function of poseproposal class models\n\n    take model predicted feature maps of delta,tx,ty,tw,th,te,te_mask,\n    output parsed human objects, each one contains all detected keypoints of the person\n\n    Parameters\n    ----------\n    arg1 : list\n        a list of model output: delta,tx,ty,tw,th,te,te_mask\n        delta: keypoint confidence feature map, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        tx: keypoints bbx center x coordinates, divided by gridsize, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        ty: keypoints bbx center y coordinates, divided by gridsize, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        tw: keypoints bbxs width w, divided by image width, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        th: keypoints bbxs height h, divided by image width, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        te: edge confidence feature map,  shape [C,H,W,Hnei,Wnei](channels_first) or [H,W,Hnei,Wnei,C](channels_last)\n        te_mask: mask of edge confidence feature map, used for loss caculation,\n        shape [C,H,W,Hnei,Wnei](channels_first) or [H,W,Hnei,Wnei,C](channels_last)\n\n    arg2: Config.DATA\n        a enum value of enum class Config.DATA\n        dataset_type where the input annotation list from, because to generate correct\n        conf_map and paf_map, the order of keypoints and limbs should be awared.\n\n    arg3 : string\n        data format speficied for channel order\n        available input:\n        \'channels_first\': data_shape C*H*W\n        \'channels_last\': data_shape H*W*C\n\n    Returns\n    -------\n    list\n        contain object of humans,see Model.Human for detail information of Human object\n    \'\'\'\n    pc,pi,px,py,pw,ph,pe=predicts\n    parts=get_parts(dataset_type)\n    limbs=get_limbs(dataset_type)\n    colors=get_colors(dataset_type)\n    post_processor=Post_Processor(parts,limbs,colors)\n    if(data_format==""channels_last""):\n        pc=np.transpose(pc,[2,0,1])\n        pi=np.transpose(pi,[2,0,1])\n        px=np.transpose(px,[2,0,1])\n        py=np.transpose(py,[2,0,1])\n        pw=np.transpose(pw,[2,0,1])\n        ph=np.transpose(ph,[2,0,1])\n        pe=np.transpose(pe,[4,0,1,2,3])\n    humans=post_processor.process(pc,pi,px,py,pw,ph,pe)\n    return humans\n\ndef visualize(img,predicts,dataset_type,save_name=""bbxs"",save_dir=""./save_dir/vis_dir"",data_format=""channels_first""):\n    \'\'\'visualize function of poseproposal class models\n\n    take model predicted feature maps of delta,tx,ty,tw,th,te,te_mask, output visualized image.\n    the image will be saved at \'save_dir\'/\'save_name\'_visualize.png\n\n    Parameters\n    ----------\n    arg1 : numpy array\n        image\n\n    arg2 : list\n        a list of model output: delta,tx,ty,tw,th,te,te_mask\n        delta: keypoint confidence feature map, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        tx: keypoints bbx center x coordinates, divided by gridsize, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        ty: keypoints bbx center y coordinates, divided by gridsize, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        tw: keypoints bbxs width w, divided by image width, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        th: keypoints bbxs height h, divided by image width, shape [C,H,W](channels_first) or [H,W,C](channels_last)\n        te: edge confidence feature map,  shape [C,H,W,Hnei,Wnei](channels_first) or [H,W,Hnei,Wnei,C](channels_last)\n        te_mask: mask of edge confidence feature map, used for loss caculation,\n        shape [C,H,W,Hnei,Wnei](channels_first) or [H,W,Hnei,Wnei,C](channels_last)\n\n    arg3: Config.DATA\n        a enum value of enum class Config.DATA\n        dataset_type where the input annotation list from\n\n    arg4 : String\n        specify output image name to distinguish.\n\n    arg5 : String\n        specify which directory to save the visualized image.\n\n    arg6 : string\n        data format speficied for channel order\n        available input:\n        \'channels_first\': data_shape C*H*W\n        \'channels_last\': data_shape H*W*C\n\n    Returns\n    -------\n    None\n    \'\'\'\n    pc,pi,px,py,pw,ph,pe=predicts\n    if(data_format==""channels_last""):\n        pc=np.transpose(pc,[2,0,1])\n        pi=np.transpose(pi,[2,0,1])\n        px=np.transpose(px,[2,0,1])\n        py=np.transpose(py,[2,0,1])\n        pw=np.transpose(pw,[2,0,1])\n        ph=np.transpose(ph,[2,0,1])\n        pe=np.transpose(pe,[4,0,1,2,3])\n    _,model_hnei,model_wnei,model_hout,model_wout=pe.shape\n    limbs=get_limbs(dataset_type)\n    os.makedirs(save_dir,exist_ok=True)\n    ori_img=np.clip(img*255.0,0.0,255.0).astype(np.uint8)\n    #show input image\n    fig=plt.figure(figsize=(8,8))\n    a=fig.add_subplot(2,2,1)\n    a.set_title(""input image"")\n    plt.imshow(ori_img)\n    #show parts and edges\n    vis_img=ori_img.copy()\n    vis_img=draw_bbx(vis_img,pc,px,py,pw,ph,threshold=0.7)\n    vis_img=draw_edge(vis_img,pe,px,py,pw,ph,model_hnei,model_wnei,model_hout,model_wout,limbs,threshold=0.7)\n    a=fig.add_subplot(2,2,2)\n    a.set_title(""visualized result"")\n    plt.imshow(vis_img)\n    #save result\n    plt.savefig(f""{save_dir}/{save_name}_visualize.png"")\n    plt.close()'"
Hyperpose/Model/openpose/model/__init__.py,0,b'from .openpose import OpenPose\r\nfrom .lw_openpose import LightWeightOpenPose\r\nfrom .mbv2_th_openpose import MobilenetThinOpenpose'
Hyperpose/Model/openpose/model/lw_openpose.py,21,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorlayer import layers\nfrom tensorlayer.models import Model\nfrom tensorlayer.layers import BatchNorm2d, Conv2d, DepthwiseConv2d, LayerList, MaxPool2d\nfrom ..utils import tf_repeat\ninitializer=tl.initializers.truncated_normal(stddev=0.005)\n\nclass LightWeightOpenPose(Model):\n    def __init__(self,n_pos=19,n_limbs=19,num_channels=128,hin=368,win=368,hout=46,wout=46,backbone=None,data_format=""channels_first""):\n        super().__init__()\n        self.num_channels=num_channels\n        self.n_pos=n_pos\n        self.n_limbs=n_limbs\n        self.n_confmaps=n_pos\n        self.n_pafmaps=2*n_limbs\n        self.hin=hin\n        self.win=win\n        self.hout=hout\n        self.wout=wout\n        self.data_format=data_format\n        if(self.data_format==""channels_first""):\n            self.concat_dim=1\n        else:\n            self.concat_dim=-1\n        #dilated mobilenetv1 backbone\n        if(backbone==None):\n            self.backbone=self.Dilated_mobilenet(data_format=self.data_format)\n        else:\n            self.backbone=backbone(scale_size=8,data_format=self.data_format)\n        #cpm stage to cutdown dimension\n        self.cpm_stage=self.Cpm_stage(n_filter=self.num_channels,in_channels=self.backbone.out_channels,data_format=self.data_format)\n        #init stage\n        self.init_stage=self.Init_stage(n_filter=self.num_channels,n_confmaps=self.n_confmaps,\\\n            n_pafmaps=self.n_pafmaps,data_format=self.data_format)\n        #one refinemnet stage\n        self.refine_stage1=self.Refinement_stage(n_filter=self.num_channels,n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,\\\n            in_channels=self.num_channels+self.n_confmaps+self.n_pafmaps,data_format=self.data_format)\n    @tf.function\n    def forward(self,x,is_train=False):\n        conf_list=[]\n        paf_list=[]\n        #backbone feature extract\n        backbone_features=self.backbone(x)\n        cpm_features=self.cpm_stage(backbone_features)\n        #init stage\n        init_conf,init_paf=self.init_stage(cpm_features)\n        conf_list.append(init_conf)\n        paf_list.append(init_paf)\n        x=tf.concat([cpm_features,init_conf,init_paf],self.concat_dim)\n        #refinement\n        ref_conf1,ref_paf1=self.refine_stage1(x)\n        conf_list.append(ref_conf1)\n        paf_list.append(ref_paf1)\n        if(is_train):\n            return conf_list[-1],paf_list[-1],conf_list,paf_list\n        else:\n            return conf_list[-1],paf_list[-1]\n    @tf.function\n    def infer(self,x):\n        conf_map,paf_map=self.forward(x,is_train=False)\n        return conf_map,paf_map\n    def cal_loss(self,gt_conf,gt_paf,mask,stage_confs,stage_pafs):\n        stage_losses=[]\n        batch_size=gt_conf.shape[0]\n        if(self.concat_dim==1):\n            mask_conf=tf_repeat(mask, [1,self.n_confmaps ,1,1])\n            mask_paf=tf_repeat(mask,[1,self.n_pafmaps ,1,1])\n        elif(self.concat_dim==-1):\n            mask_conf=tf_repeat(mask, [1,1,1,self.n_confmaps])\n            mask_paf=tf_repeat(mask,[1,1,1,self.n_pafmaps])\n        loss_confs,loss_pafs=[],[]\n        for stage_conf,stage_paf in zip(stage_confs,stage_pafs):\n            loss_conf=tf.nn.l2_loss((gt_conf-stage_conf)*mask_conf)\n            loss_paf=tf.nn.l2_loss((gt_paf-stage_paf)*mask_paf)\n            stage_losses.append(loss_conf)\n            stage_losses.append(loss_paf)\n            loss_confs.append(loss_conf)\n            loss_pafs.append(loss_paf)\n        pd_loss=tf.reduce_mean(stage_losses)/batch_size\n        return pd_loss,loss_confs,loss_pafs\n    class Dilated_mobilenet(Model):\n        def __init__(self,data_format=""channels_first""):\n            super().__init__()\n            self.data_format=data_format\n            self.out_channels=512\n            self.main_block=layers.LayerList([\n            conv_block(n_filter=32,in_channels=3,data_format=self.data_format,strides=(2,2)),\n            dw_conv_block(n_filter=64,in_channels=32,data_format=self.data_format),\n            dw_conv_block(n_filter=128,in_channels=64,data_format=self.data_format,strides=(2,2)),\n            dw_conv_block(n_filter=128,in_channels=128,data_format=self.data_format),\n            dw_conv_block(n_filter=256,in_channels=128,data_format=self.data_format,strides=(2,2)),\n            dw_conv_block(n_filter=256,in_channels=256,data_format=self.data_format),\n            dw_conv_block(n_filter=512,in_channels=256,data_format=self.data_format),\n            dw_conv_block(n_filter=512,in_channels=512,data_format=self.data_format,dilation_rate=(2,2)),\n            dw_conv_block(n_filter=512,in_channels=512,data_format=self.data_format),\n            dw_conv_block(n_filter=512,in_channels=512,data_format=self.data_format),\n            dw_conv_block(n_filter=512,in_channels=512,data_format=self.data_format),\n            dw_conv_block(n_filter=512,in_channels=512,data_format=self.data_format)\n            ])\n        def forward(self,x):\n            return self.main_block.forward(x)\n    class Cpm_stage(Model):\n        def __init__(self,n_filter=128,in_channels=512,data_format=""channels_first""):\n            super().__init__()\n            self.data_format=data_format\n            self.init_layer=Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=(1,1),act=tf.nn.relu,data_format=self.data_format)\n            self.main_block=layers.LayerList([\n                conv_block(n_filter=n_filter,in_channels=n_filter,data_format=self.data_format),\n                conv_block(n_filter=n_filter,in_channels=n_filter,data_format=self.data_format),\n                conv_block(n_filter=n_filter,in_channels=n_filter,data_format=self.data_format),\n            ])\n            self.end_layer=Conv2d(n_filter=n_filter,in_channels=n_filter,filter_size=(3,3),act=tf.nn.relu,data_format=self.data_format)\n        \n        def forward(self,x):\n            x=self.init_layer.forward(x)\n            x=x+self.main_block.forward(x)\n            return self.end_layer.forward(x)\n\n    class Init_stage(Model):\n        def __init__(self,n_filter=128,n_confmaps=19,n_pafmaps=38,data_format=""channels_first""):\n            super().__init__()\n            self.data_format=data_format\n            self.main_block=layers.LayerList([\n            Conv2d(n_filter=n_filter,in_channels=n_filter,act=tf.nn.relu,data_format=self.data_format),\n            Conv2d(n_filter=n_filter,in_channels=n_filter,act=tf.nn.relu,data_format=self.data_format),\n            Conv2d(n_filter=n_filter,in_channels=n_filter,act=tf.nn.relu,data_format=self.data_format)\n            ])\n            self.conf_block=layers.LayerList([\n            Conv2d(n_filter=512,in_channels=n_filter,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,W_init=initializer,\\\n                    b_init=initializer,data_format=self.data_format),\n            Conv2d(n_filter=n_confmaps,in_channels=512,filter_size=(1,1),strides=(1,1),W_init=initializer,\\\n                    b_init=initializer,data_format=self.data_format)\n            ])\n            self.paf_block=layers.LayerList([\n            Conv2d(n_filter=512,in_channels=n_filter,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,W_init=initializer,\\\n                b_init=initializer,data_format=self.data_format),\n            Conv2d(n_filter=n_pafmaps,in_channels=512,filter_size=(1,1),strides=(1,1),W_init=initializer,\\\n                b_init=initializer,data_format=self.data_format)\n            ])\n        def forward(self,x):\n            x=self.main_block.forward(x)\n            conf_map=self.conf_block.forward(x)\n            paf_map=self.paf_block.forward(x)\n            return conf_map,paf_map\n    class Refinement_stage(Model):\n        def __init__(self,n_filter=128,in_channels=185,n_confmaps=19,n_pafmaps=38,data_format=""channels_first""):\n            super().__init__()\n            self.data_format=data_format\n            self.block_1=self.Refinement_block(n_filter=n_filter,in_channels=in_channels,data_format=self.data_format)\n            self.block_2=self.Refinement_block(n_filter=n_filter,in_channels=n_filter,data_format=self.data_format)\n            self.block_3=self.Refinement_block(n_filter=n_filter,in_channels=n_filter,data_format=self.data_format)\n            self.block_4=self.Refinement_block(n_filter=n_filter,in_channels=n_filter,data_format=self.data_format)\n            self.block_5=self.Refinement_block(n_filter=n_filter,in_channels=n_filter,data_format=self.data_format)\n            self.conf_block=layers.LayerList([\n            Conv2d(n_filter=512,in_channels=n_filter,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,W_init=initializer,b_init=initializer,\\\n                data_format=self.data_format),\n            Conv2d(n_filter=n_confmaps,in_channels=512,filter_size=(1,1),strides=(1,1),W_init=initializer,b_init=initializer,\\\n                data_format=self.data_format)\n            ])\n            self.paf_block=layers.LayerList([\n            Conv2d(n_filter=512,in_channels=n_filter,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,W_init=initializer,b_init=initializer,\\\n                data_format=self.data_format),\n            Conv2d(n_filter=n_pafmaps,in_channels=512,filter_size=(1,1),strides=(1,1),W_init=initializer,b_init=initializer,\\\n                data_format=self.data_format)\n            ])\n        def forward(self,x):\n            x=self.block_1(x)\n            x=self.block_2(x)\n            x=self.block_3(x)\n            x=self.block_4(x)\n            x=self.block_5(x)\n            conf_map=self.conf_block.forward(x)\n            paf_map=self.paf_block.forward(x)\n            return conf_map,paf_map        \n        class Refinement_block(Model):\n            def __init__(self,n_filter,in_channels,data_format=""channels_first""):\n                super().__init__()\n                self.data_format=data_format\n                self.init_layer=Conv2d(n_filter=n_filter,filter_size=(1,1),in_channels=in_channels,act=tf.nn.relu,data_format=self.data_format)\n                self.main_block=layers.LayerList([\n                conv_block(n_filter=n_filter,in_channels=n_filter,data_format=self.data_format),\n                conv_block(n_filter=n_filter,in_channels=n_filter,dilation_rate=(1,1),data_format=self.data_format)\n                ])\n            def forward(self,x):\n                x=self.init_layer.forward(x)\n                return x+self.main_block.forward(x)\n\ndef conv_block(n_filter,in_channels,filter_size=(3,3),strides=(1,1),dilation_rate=(1,1),W_init=initializer,b_init=initializer,padding=""SAME"",data_format=""channels_first""):\n    layer_list=[]\n    layer_list.append(Conv2d(n_filter=n_filter,filter_size=filter_size,strides=strides,in_channels=in_channels,\\\n        dilation_rate=dilation_rate,padding=padding,W_init=initializer,b_init=initializer,data_format=data_format))\n    layer_list.append(BatchNorm2d(decay=0.99, act=tf.nn.relu,num_features=n_filter,data_format=data_format,is_train=True))\n    return layers.LayerList(layer_list)\n\ndef dw_conv_block(n_filter,in_channels,filter_size=(3,3),strides=(1,1),dilation_rate=(1,1),W_init=initializer,b_init=initializer,data_format=""channels_first""):\n    layer_list=[]\n    layer_list.append(DepthwiseConv2d(filter_size=filter_size,strides=strides,in_channels=in_channels,\n        dilation_rate=dilation_rate,W_init=initializer,b_init=None,data_format=data_format))\n    layer_list.append(BatchNorm2d(decay=0.99,act=tf.nn.relu,num_features=in_channels,data_format=data_format,is_train=True))\n    layer_list.append(Conv2d(n_filter=n_filter,filter_size=(1,1),strides=(1,1),in_channels=in_channels,W_init=initializer,b_init=None,data_format=data_format))\n    layer_list.append(BatchNorm2d(decay=0.99,act=tf.nn.relu,num_features=n_filter,data_format=data_format,is_train=True))\n    return layers.LayerList(layer_list)\n\ndef nobn_dw_conv_block(n_filter,in_channels,filter_size=(3,3),strides=(1,1),W_init=initializer,b_init=initializer,data_format=""channels_first""):\n    layer_list=[]\n    layer_list.append(DepthwiseConv2d(filter_size=filter_size,strides=strides,in_channels=in_channels,\n        act=tf.nn.relu,W_init=initializer,b_init=None,data_format=data_format))\n    layer_list.append(Conv2d(n_filter=n_filter,filter_size=(1, 1),strides=(1, 1),in_channels=in_channels,\n        act=tf.nn.relu,W_init=initializer,b_init=None,data_format=data_format))\n    return layers.LayerList(layer_list)\n'"
Hyperpose/Model/openpose/model/mbv2_sm_openpose.py,33,"b'import tensorflow as tf\r\nimport tensorlayer as tl\r\nfrom tensorlayer import layers\r\nfrom tensorlayer.models import Model\r\nfrom tensorlayer.layers import BatchNorm2d, Conv2d, DepthwiseConv2d, LayerList, MaxPool2d ,SeparableConv2d, UpSampling2d\r\nfrom ..utils import tf_repeat\r\n\r\nclass Mobilenetv2_small_Openpose(Model):\r\n    def __init__(self,n_pos=19,n_limbs=19,num_channels=128,hin=368,win=368,hout=46,wout=46,data_format=""channels_first""):\r\n        super().__init__()\r\n        self.num_channels=num_channels\r\n        self.n_pos=n_pos\r\n        self.n_limbs=n_limbs\r\n        self.n_confmaps=n_pos\r\n        self.n_pafmaps=2*n_limbs\r\n        self.hin=hin\r\n        self.win=win\r\n        self.hout=hout\r\n        self.wout=wout\r\n        self.data_format=data_format\r\n        if(self.data_format==""channels_first""):\r\n            self.concat_dim=1\r\n        else:\r\n            self.concat_dim=-1\r\n        \r\n        self.backbone=self.Mobilenetv2_variant(data_format=self.data_format)\r\n        self.init_stage=self.Init_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels,data_format=self.data_format)\r\n        self.refinement_stage_1=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+3*self.n_confmaps,data_format=self.data_format)\r\n        self.refinement_stage_2=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+3*self.n_confmaps,data_format=self.data_format)\r\n        self.refinement_stage_3=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+3*self.n_confmaps,data_format=self.data_format)\r\n        self.refinement_stage_4=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+3*self.n_confmaps,data_format=self.data_format)\r\n    \r\n    @tf.function\r\n    def forward(self,x,mask_conf=None,mask_paf=None,is_train=False):\r\n        conf_list=[]\r\n        paf_list=[] \r\n        backbone_features=self.backbone.forward(x)\r\n        conf_map,paf_map=self.init_stage.forward(backbone_features)\r\n        conf_list.append(conf_map)\r\n        paf_list.append(paf_map)\r\n        for refinement_stage_idx in range(1,6):\r\n            x=tf.concat([backbone_features,conf_list[-1],paf_list[-1]],self.concat_dim)\r\n            conf_map,paf_map=eval(f""self.refinement_stage_{refinement_stage_idx}.forward(x)"")\r\n            conf_list.append(conf_map)\r\n            paf_list.append(paf_map)\r\n        if(is_train):\r\n            return conf_list[-1],paf_list[-1],conf_list,paf_list\r\n        else:\r\n            return conf_list[-1],paf_list[-1]\r\n    \r\n    @tf.function\r\n    def infer(self,x):\r\n        conf_map,paf_map=self.forward(x,is_train=False)\r\n        return conf_map,paf_map\r\n    \r\n    def cal_loss(self,gt_conf,gt_paf,mask,stage_confs,stage_pafs):\r\n        stage_losses=[]\r\n        batch_size=gt_conf.shape[0]\r\n        mask_conf=tf_repeat(mask, [1,self.n_confmaps ,1,1])\r\n        mask_paf=tf_repeat(mask,[1,self.n_pafmaps ,1,1])\r\n        loss_confs,loss_pafs=[],[]\r\n        for stage_conf,stage_paf in zip(stage_confs,stage_pafs):\r\n            loss_conf=tf.nn.l2_loss((gt_conf-stage_conf)*mask_conf)\r\n            loss_paf=tf.nn.l2_loss((gt_paf-stage_paf)*mask_paf)\r\n            stage_losses.append(loss_conf)\r\n            stage_losses.append(loss_paf)\r\n            loss_confs.append(loss_conf)\r\n            loss_pafs.append(loss_paf)\r\n        pd_loss=tf.reduce_mean(stage_losses)/batch_size\r\n        return pd_loss,loss_confs,loss_pafs\r\n\r\n    class Mobilenetv2_variant(Model):\r\n        def __init__(self,data_format=""channels_first""):\r\n            super().__init__()\r\n            self.data_format=data_format\r\n            if(self.data_format==""channels_first""):\r\n                self.concat_dim=1\r\n            else:\r\n                self.concat_dim=-1\r\n            self.out_channels=704\r\n            self.convblock_0=conv_block(n_filter=32,in_channels=3,filter_size=(3,3),strides=(2,2),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_1=separable_block(n_filter=64,in_channels=32,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_2=separable_block(n_filter=128,in_channels=64,filter_size=(3,3),strides=(2,2),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_3=separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_4=separable_block(n_filter=256,in_channels=128,filter_size=(3,3),strides=(2,2),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_5=separable_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_6=separable_block(n_filter=512,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_7=separable_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.maxpool=MaxPool2d(filter_size=(2,2),strides=(2,2),padding=""SAME"",data_format=self.data_format)\r\n            self.upsample=UpSampling2d(scale=2,data_format=self.data_format)\r\n            \r\n\r\n        def forward(self,x):\r\n            concat_list=[]\r\n            x=self.convblock_0.forward(x)\r\n            x=self.convblock_1.forward(x)\r\n            concat_list.append(self.maxpool.forward(x))\r\n            x=self.convblock_2.forward(x)\r\n            x=self.convblock_3.forward(x)\r\n            concat_list.append(x)\r\n            x=self.convblock_4.forward(x)\r\n            x=self.convblock_5.forward(x)\r\n            x=self.convblock_6.forward(x)\r\n            x=self.convblock_7.forward(x)\r\n            concat_list.append(self.upsample.forward(x))\r\n            x=tf.concat(concat_list,self.concat_dim)\r\n            return x\r\n        \r\n    class Init_stage(Model):\r\n        def __init__(self,n_confmaps=19,n_pafmaps=38,in_channels=704,data_format=""channels_first""):\r\n            self.n_confmaps=n_confmaps\r\n            self.n_pafmaps=n_pafmaps\r\n            self.in_channels=in_channels\r\n            self.data_format=data_format\r\n            #conf block\r\n            self.conf_block=LayerList([\r\n                separable_block(n_filter=128,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=512,in_channels=128,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=self.n_confmaps,in_channels=512,filter_size=(1,1),strides=(1,1),act=None,data_format=self.data_format)\r\n            ])\r\n            #paf block\r\n            self.paf_block=LayerList([\r\n                separable_block(n_filter=128,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=512,in_channels=128,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=self.n_pafmaps,in_channels=512,filter_size=(1,1),strides=(1,1),act=None,data_format=self.data_format)\r\n            ])\r\n        \r\n        def forward(self,x):\r\n            conf_map=self.conf_block.forward(x)\r\n            paf_map=self.paf_block.forward(x)\r\n            return conf_map,paf_map\r\n        \r\n    class Refinement_stage(Model):\r\n        def __init__(self,n_confmaps=19,n_pafmaps=38,in_channels=19+38+704,data_format=""channels_first""):\r\n            self.n_confmaps=n_confmaps\r\n            self.n_pafmaps=n_pafmaps\r\n            self.in_channels=in_channels\r\n            self.data_format=data_format\r\n            #conf_block\r\n            self.conf_block=LayerList([\r\n                separable_block(n_filter=128,in_channels=self.in_channels,filter_size=(7,7),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=self.n_confmaps,in_channels=128,filter_size=(1,1),strides=(1,1),act=None,data_format=self.data_format),\r\n            ])\r\n            #paf_block\r\n            self.conf_block=LayerList([\r\n                separable_block(n_filter=128,in_channels=self.in_channels,filter_size=(7,7),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=self.n_pafmaps,in_channels=128,filter_size=(1,1),strides=(1,1),act=None,data_format=self.data_format),\r\n            ])\r\n        \r\n        def forward(self,x):\r\n            conf_map=self.conf_block.forward(x)\r\n            paf_map=self.paf_block.forward(x)\r\n            return conf_map,paf_map\r\n\r\ndef conv_block(n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,padding=""SAME"",data_format=""channels_first""):\r\n    layer_list=[]\r\n    layer_list.append(Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=filter_size,strides=strides,act=act,\\\r\n        data_format=data_format,padding=padding))\r\n    layer_list.append(BatchNorm2d(num_features=n_filter,decay=0.999,is_train=True,act=act,data_format=data_format))\r\n    return LayerList(layer_list)\r\n\r\ndef separable_block(n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,padding=""SAME"",data_format=""channels_first""):\r\n    layer_list=[]\r\n    layer_list.append(SeparableConv2d(n_filter=n_filter,in_channels=in_channels,filter_size=filter_size,strides=strides,act=act,\\\r\n        data_format=data_format,padding=padding))\r\n    layer_list.append(BatchNorm2d(num_features=n_filter,decay=0.999,is_train=True,act=act,data_format=data_format))\r\n    return LayerList(layer_list)'"
Hyperpose/Model/openpose/model/mbv2_th_openpose.py,37,"b'import tensorflow as tf\r\nimport tensorlayer as tl\r\nfrom tensorlayer import layers\r\nfrom tensorlayer.models import Model\r\nfrom tensorlayer.layers import BatchNorm2d, Conv2d, DepthwiseConv2d, LayerList, MaxPool2d\r\nfrom ..utils import tf_repeat\r\n\r\ninitial_w=tl.initializers.random_normal(stddev=0.01)\r\ninitial_b=tl.initializers.constant(value=0.0)\r\n\r\nclass MobilenetThinOpenpose(Model):\r\n    def __init__(self,n_pos=19,n_limbs=19,num_channels=128,hin=368,win=368,hout=46,wout=46,backbone=None,data_format=""channels_first""):\r\n        super().__init__()\r\n        self.num_channels=num_channels\r\n        self.n_pos=n_pos\r\n        self.n_limbs=n_limbs\r\n        self.n_confmaps=n_pos\r\n        self.n_pafmaps=2*n_limbs\r\n        self.hin=hin\r\n        self.win=win\r\n        self.hout=hout\r\n        self.wout=wout\r\n        self.data_format=data_format\r\n        if(self.data_format==""channels_first""):\r\n            self.concat_dim=1\r\n        else:\r\n            self.concat_dim=-1\r\n        if(backbone==None):\r\n            self.backbone=self.Mobilenetv2_variant(data_format=self.data_format)\r\n        else:\r\n            self.backbone=backbone(scale_size=8,data_format=self.data_format)\r\n        self.init_stage=self.Init_stage(n_confmaps=self.n_confmaps,in_channels=self.backbone.out_channels,data_format=self.data_format)\r\n        self.refinement_stage_1=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+self.n_confmaps+self.n_pafmaps,data_format=self.data_format)\r\n        self.refinement_stage_2=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+self.n_confmaps+self.n_pafmaps,data_format=self.data_format)\r\n        self.refinement_stage_3=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+self.n_confmaps+self.n_pafmaps,data_format=self.data_format)\r\n        self.refinement_stage_4=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+self.n_confmaps+self.n_pafmaps,data_format=self.data_format)\r\n        self.refinement_stage_5=self.Refinement_stage(n_confmaps=self.n_confmaps,n_pafmaps=self.n_pafmaps,in_channels=self.backbone.out_channels+self.n_confmaps+self.n_pafmaps,data_format=self.data_format)\r\n    \r\n    @tf.function\r\n    def forward(self,x,is_train=False):\r\n        conf_list=[]\r\n        paf_list=[] \r\n        backbone_features=self.backbone.forward(x)\r\n        conf_map,paf_map=self.init_stage.forward(backbone_features)\r\n        conf_list.append(conf_map)\r\n        paf_list.append(paf_map)\r\n        for refinement_stage_idx in range(1,6):\r\n            x=tf.concat([backbone_features,conf_list[-1],paf_list[-1]],self.concat_dim)\r\n            conf_map,paf_map=eval(f""self.refinement_stage_{refinement_stage_idx}.forward(x)"")\r\n            conf_list.append(conf_map)\r\n            paf_list.append(paf_map)\r\n        if(is_train):\r\n            return conf_list[-1],paf_list[-1],conf_list,paf_list\r\n        else:\r\n            return conf_list[-1],paf_list[-1]\r\n    \r\n    @tf.function\r\n    def infer(self,x):\r\n        conf_map,paf_map=self.forward(x,is_train=False)\r\n        return conf_map,paf_map\r\n    \r\n    def cal_loss(self,gt_conf,gt_paf,mask,stage_confs,stage_pafs):\r\n        stage_losses=[]\r\n        batch_size=gt_conf.shape[0]\r\n        mask_conf=tf_repeat(mask, [1,self.n_confmaps ,1,1])\r\n        mask_paf=tf_repeat(mask,[1,self.n_pafmaps ,1,1])\r\n        loss_confs,loss_pafs=[],[]\r\n        for stage_conf,stage_paf in zip(stage_confs,stage_pafs):\r\n            loss_conf=tf.nn.l2_loss((gt_conf-stage_conf)*mask_conf)\r\n            loss_paf=tf.nn.l2_loss((gt_paf-stage_paf)*mask_paf)\r\n            stage_losses.append(loss_conf)\r\n            stage_losses.append(loss_paf)\r\n            loss_confs.append(loss_conf)\r\n            loss_pafs.append(loss_paf)\r\n        pd_loss=tf.reduce_mean(stage_losses)/batch_size\r\n        return pd_loss,loss_confs,loss_pafs\r\n\r\n    class Mobilenetv2_variant(Model):\r\n        def __init__(self,data_format=""channels_first""):\r\n            super().__init__()\r\n            self.data_format=data_format\r\n            if(self.data_format==""channels_first""):\r\n                self.concat_dim=1\r\n            else:\r\n                self.concat_dim=-1\r\n            self.out_channels=1152\r\n            self.convblock_0=conv_block(n_filter=32,in_channels=3,filter_size=(3,3),strides=(2,2),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_1=separable_block(n_filter=64,in_channels=32,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_2=separable_block(n_filter=128,in_channels=64,filter_size=(3,3),strides=(2,2),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_3=separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_4=separable_block(n_filter=256,in_channels=128,filter_size=(3,3),strides=(2,2),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_5=separable_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_6=separable_block(n_filter=512,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_7=separable_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_8=separable_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_9=separable_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_10=separable_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.convblock_11=separable_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format)\r\n            self.maxpool=MaxPool2d(filter_size=(2,2),strides=(2,2),padding=""SAME"",data_format=self.data_format)\r\n            \r\n        \r\n        def forward(self,x):\r\n            concat_list=[]\r\n            x=self.convblock_0.forward(x)\r\n            x=self.convblock_1.forward(x)\r\n            x=self.convblock_2.forward(x)\r\n            x=self.convblock_3.forward(x)\r\n            concat_list.append(self.maxpool.forward(x))\r\n            x=self.convblock_4.forward(x)\r\n            x=self.convblock_5.forward(x)\r\n            x=self.convblock_6.forward(x)\r\n            x=self.convblock_7.forward(x)\r\n            concat_list.append(x)\r\n            x=self.convblock_8.forward(x)\r\n            x=self.convblock_9.forward(x)\r\n            x=self.convblock_10.forward(x)\r\n            x=self.convblock_11.forward(x)\r\n            concat_list.append(x)\r\n            x=tf.concat(concat_list,self.concat_dim)\r\n            return x\r\n        \r\n    class Init_stage(Model):\r\n        def __init__(self,n_confmaps=19,n_pafmaps=38,in_channels=1152,data_format=""channels_first""):\r\n            super().__init__()\r\n            self.n_confmaps=n_confmaps\r\n            self.n_pafmaps=n_pafmaps\r\n            self.in_channels=in_channels\r\n            self.data_format=data_format\r\n            #conf block\r\n            self.conf_block=LayerList([\r\n                separable_block(n_filter=128,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=512,in_channels=128,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=self.n_confmaps,in_channels=512,filter_size=(1,1),strides=(1,1),act=None,data_format=self.data_format)\r\n            ])\r\n            #paf block\r\n            self.paf_block=LayerList([\r\n                separable_block(n_filter=128,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=512,in_channels=128,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=self.n_pafmaps,in_channels=512,filter_size=(1,1),strides=(1,1),act=None,data_format=self.data_format)\r\n            ])\r\n        \r\n        def forward(self,x):\r\n            conf_map=self.conf_block.forward(x)\r\n            paf_map=self.paf_block.forward(x)\r\n            return conf_map,paf_map\r\n        \r\n    class Refinement_stage(Model):\r\n        def __init__(self,n_confmaps=19,n_pafmaps=38,in_channels=19+38+1152,data_format=""channels_first""):\r\n            super().__init__()\r\n            self.n_confmaps=n_confmaps\r\n            self.n_pafmaps=n_pafmaps\r\n            self.in_channels=in_channels\r\n            self.data_format=data_format\r\n            #conf_block\r\n            self.conf_block=LayerList([\r\n                separable_block(n_filter=128,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=self.n_confmaps,in_channels=128,filter_size=(1,1),strides=(1,1),act=None,data_format=self.data_format),\r\n            ])\r\n            #paf_block\r\n            self.paf_block=LayerList([\r\n                separable_block(n_filter=128,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=128,in_channels=128,filter_size=(1,1),strides=(1,1),act=tf.nn.relu,data_format=self.data_format),\r\n                separable_block(n_filter=self.n_pafmaps,in_channels=128,filter_size=(1,1),strides=(1,1),act=None,data_format=self.data_format),\r\n            ])\r\n        \r\n        def forward(self,x):\r\n            conf_map=self.conf_block.forward(x)\r\n            paf_map=self.paf_block.forward(x)\r\n            return conf_map,paf_map\r\n\r\ndef conv_block(n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),act=tf.nn.relu,padding=""SAME"",data_format=""channels_first""):\r\n    layer_list=[]\r\n    layer_list.append(Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=filter_size,strides=strides,act=act,\\\r\n        W_init=initial_w,b_init=initial_b,data_format=data_format,padding=padding))\r\n    layer_list.append(BatchNorm2d(num_features=n_filter,decay=0.999,is_train=True,act=act,data_format=data_format))\r\n    return LayerList(layer_list)\r\n\r\ndef separable_block(n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),dilation_rate=(1,1),act=tf.nn.relu,data_format=""channels_first""):\r\n    layer_list=[]\r\n    layer_list.append(DepthwiseConv2d(filter_size=filter_size,strides=strides,in_channels=in_channels,\r\n        dilation_rate=dilation_rate,W_init=initial_w,b_init=None,data_format=data_format))\r\n    layer_list.append(BatchNorm2d(decay=0.99,act=act,num_features=in_channels,data_format=data_format,is_train=True))\r\n    layer_list.append(Conv2d(n_filter=n_filter,filter_size=(1,1),strides=(1,1),in_channels=in_channels,W_init=initial_w,b_init=None,data_format=data_format))\r\n    layer_list.append(BatchNorm2d(decay=0.99,act=act,num_features=n_filter,data_format=data_format,is_train=True))\r\n    return layers.LayerList(layer_list)'"
Hyperpose/Model/openpose/model/openpose.py,44,"b'import tensorflow as tf\r\nimport tensorlayer as tl\r\nfrom tensorlayer import layers\r\nfrom tensorlayer.models import Model\r\nfrom tensorlayer.layers import BatchNorm2d, Conv2d, DepthwiseConv2d, LayerList, MaxPool2d\r\nfrom ..utils import tf_repeat\r\ninitial_w=tl.initializers.random_normal(stddev=0.01)\r\ninitial_b=tl.initializers.constant(value=0.0)\r\n\r\nclass OpenPose(Model):\r\n    def __init__(self,n_pos=19,n_limbs=19,num_channels=128,hin=368,win=368,hout=46,wout=46,backbone=None,pretrained_backbone=True,data_format=""channels_first""):\r\n        super().__init__()\r\n        self.num_channels=num_channels\r\n        self.n_pos=n_pos\r\n        self.n_limbs=n_limbs\r\n        self.n_confmaps=n_pos\r\n        self.n_pafmaps=2*n_limbs\r\n        self.hin=hin\r\n        self.win=win\r\n        self.hout=hout\r\n        self.wout=wout\r\n        self.data_format=data_format\r\n        self.pretrained_backbone=pretrained_backbone\r\n        self.concat_dim=1 if self.data_format==""channels_first"" else -1\r\n        #back bone configure\r\n        if(backbone==None):\r\n            self.backbone=self.vgg19(in_channels=3,pretrained=self.pretrained_backbone,data_format=self.data_format)\r\n        else:\r\n            self.backbone=backbone(scale_size=8,data_format=self.data_format)\r\n        self.cpm_stage=LayerList([\r\n            Conv2d(n_filter=256,in_channels=self.backbone.out_channels,filter_size=(3,3),strides=(1,1),padding=""SAME"",act=tf.nn.relu,data_format=self.data_format),\r\n            Conv2d(n_filter=128,in_channels=256,filter_size=(3,3),strides=(1,1),padding=""SAME"",act=tf.nn.relu,data_format=self.data_format)\r\n        ])\r\n        #init stage\r\n        self.init_stage=self.Init_stage(n_confmaps=self.n_confmaps, n_pafmaps=self.n_pafmaps,in_channels=128,data_format=self.data_format)\r\n        #one refinemnet stage\r\n        self.refinement_stage_1=self.Refinement_stage(n_confmaps=self.n_confmaps, n_pafmaps=self.n_pafmaps, in_channels=self.n_confmaps+self.n_pafmaps+128,data_format=self.data_format)\r\n        self.refinement_stage_2=self.Refinement_stage(n_confmaps=self.n_confmaps, n_pafmaps=self.n_pafmaps, in_channels=self.n_confmaps+self.n_pafmaps+128,data_format=self.data_format)\r\n        self.refinement_stage_3=self.Refinement_stage(n_confmaps=self.n_confmaps, n_pafmaps=self.n_pafmaps, in_channels=self.n_confmaps+self.n_pafmaps+128,data_format=self.data_format)\r\n        self.refinement_stage_4=self.Refinement_stage(n_confmaps=self.n_confmaps, n_pafmaps=self.n_pafmaps, in_channels=self.n_confmaps+self.n_pafmaps+128,data_format=self.data_format)\r\n        self.refinement_stage_5=self.Refinement_stage(n_confmaps=self.n_confmaps, n_pafmaps=self.n_pafmaps, in_channels=self.n_confmaps+self.n_pafmaps+128,data_format=self.data_format)\r\n        \r\n\r\n    @tf.function\r\n    def forward(self,x,is_train=False):\r\n        conf_list=[]\r\n        paf_list=[]\r\n        #backbone feature extract\r\n        vgg_features=self.backbone.forward(x)\r\n        vgg_features=self.cpm_stage.forward(vgg_features)\r\n        #init stage\r\n        init_conf,init_paf=self.init_stage.forward(vgg_features)\r\n        conf_list.append(init_conf)\r\n        paf_list.append(init_paf)\r\n        #refinement stages\r\n        for refine_stage_idx in range(1,6):\r\n            ref_x=tf.concat([vgg_features,conf_list[-1],paf_list[-1]],self.concat_dim)\r\n            ref_conf,ref_paf=eval(f""self.refinement_stage_{refine_stage_idx}.forward(ref_x)"")\r\n            conf_list.append(ref_conf)\r\n            paf_list.append(ref_paf)\r\n        if(is_train):\r\n            return conf_list[-1],paf_list[-1],conf_list,paf_list\r\n        else:\r\n            return conf_list[-1],paf_list[-1]\r\n    \r\n    @tf.function\r\n    def infer(self,x):\r\n        conf_map,paf_map=self.forward(x,is_train=False)\r\n        return conf_map,paf_map\r\n    \r\n    def cal_loss(self,gt_conf,gt_paf,mask,stage_confs,stage_pafs):\r\n        stage_losses=[]\r\n        batch_size=gt_conf.shape[0]\r\n        if(self.concat_dim==1):\r\n            mask_conf=tf_repeat(mask, [1,self.n_confmaps ,1,1])\r\n            mask_paf=tf_repeat(mask,[1,self.n_pafmaps ,1,1])\r\n        elif(self.concat_dim==-1):\r\n            mask_conf=tf_repeat(mask, [1,1,1,self.n_confmaps])\r\n            mask_paf=tf_repeat(mask,[1,1,1,self.n_pafmaps])\r\n        loss_confs,loss_pafs=[],[]\r\n        for stage_id,(stage_conf,stage_paf) in enumerate(zip(stage_confs,stage_pafs)):\r\n            loss_conf=tf.nn.l2_loss((gt_conf-stage_conf)*mask_conf)\r\n            loss_paf=tf.nn.l2_loss((gt_paf-stage_paf)*mask_paf)\r\n            stage_losses.append(loss_conf)\r\n            stage_losses.append(loss_paf)\r\n            loss_confs.append(loss_conf)\r\n            loss_pafs.append(loss_paf)\r\n        pd_loss=tf.reduce_mean(stage_losses)/batch_size\r\n        return pd_loss,loss_confs,loss_pafs\r\n     \r\n    class vgg19(Model):\r\n        def __init__(self,in_channels=3,data_format=""channels_first"",pretrained=True):\r\n            super().__init__()\r\n            self.data_format=data_format\r\n            self.pretrained=pretrained\r\n            self.transpose=False\r\n            self.out_channels=512\r\n            if(self.data_format==""channel_last""):\r\n                self.main_block=tl.models.vgg19(pretrained=self.pretrained,end_with=""conv4_2"")\r\n            else:\r\n                if(self.pretrained):\r\n                    print(""only channels_last pretrained vgg19 available, adding transpose"")\r\n                    self.main_block=tl.models.vgg19(pretrained=self.pretrained,end_with=""conv4_2"")\r\n                    self.transpose=True\r\n                else:\r\n                    self.main_block=layers.LayerList([\r\n                    self.conv_block(n_filter=64,in_channels=3,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    self.conv_block(n_filter=64,in_channels=64,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n                    self.conv_block(n_filter=128,in_channels=64,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    self.conv_block(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n                    self.conv_block(n_filter=256,in_channels=128,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    self.conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    self.conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    self.conv_block(n_filter=256,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    MaxPool2d(filter_size=(2,2),strides=(2,2),data_format=self.data_format),\r\n                    self.conv_block(n_filter=512,in_channels=256,filter_size=(3,3),strides=(1,1),act=tf.nn.relu),\r\n                    self.conv_block(n_filter=512,in_channels=512,filter_size=(3,3),strides=(1,1),act=tf.nn.relu)\r\n                    ])\r\n\r\n        def conv_block(self,n_filter=32,in_channels=3,filter_size=(3,3),strides=(1,1),act=None,padding=""SAME""):\r\n            return Conv2d(n_filter=n_filter,in_channels=in_channels,filter_size=filter_size,strides=strides,\\\r\n                act=act,data_format=self.data_format,padding=padding)\r\n\r\n        def forward(self,x):\r\n            if(self.transpose):\r\n                x=tf.transpose(x,[0,2,3,1])\r\n            x=self.main_block.forward(x)\r\n            if(self.transpose):\r\n                x=tf.transpose(x,[0,3,1,2])\r\n            return x\r\n    \r\n    class Init_stage(Model):\r\n        def __init__(self,n_confmaps=19,n_pafmaps=38,in_channels=128,data_format=""channels_first""):\r\n            super().__init__()\r\n            self.n_confmaps=n_confmaps\r\n            self.n_pafmaps=n_pafmaps\r\n            self.in_channels=in_channels\r\n            self.data_format=data_format\r\n            self.conf_block=layers.LayerList([\r\n                Conv2d(n_filter=128,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=512,in_channels=128,filter_size=(1,1),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=self.n_confmaps,in_channels=512,filter_size=(1,1),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format)\r\n            ])\r\n            self.paf_block=layers.LayerList([\r\n                Conv2d(n_filter=128,in_channels=self.in_channels,filter_size=(3,3),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(3,3),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=512,in_channels=128,filter_size=(1,1),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=self.n_pafmaps,in_channels=512,filter_size=(1,1),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format)\r\n            ])\r\n        \r\n        def forward(self,x):\r\n            conf_map=self.conf_block.forward(x)\r\n            paf_map=self.paf_block.forward(x)\r\n            return conf_map,paf_map\r\n    \r\n    class Refinement_stage(Model):\r\n        def __init__(self,n_confmaps=19,n_pafmaps=38,in_channels=185,data_format=""channels_first""):\r\n            super().__init__()\r\n            self.n_confmaps=n_confmaps\r\n            self.n_pafmaps=n_pafmaps\r\n            self.in_channels=in_channels\r\n            self.data_format=data_format\r\n            self.conf_block=layers.LayerList([\r\n                Conv2d(n_filter=128,in_channels=self.in_channels,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(1,1),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=self.n_confmaps,in_channels=128,filter_size=(1,1),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format)\r\n            ])\r\n            self.paf_block=layers.LayerList([\r\n                Conv2d(n_filter=128,in_channels=self.in_channels,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(7,7),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=128,in_channels=128,filter_size=(1,1),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format),\r\n                Conv2d(n_filter=self.n_pafmaps,in_channels=128,filter_size=(1,1),strides=(1,1),padding=""SAME"",act=tf.nn.relu,W_init=initial_w,b_init=initial_b,data_format=self.data_format)\r\n            ])\r\n        \r\n        def forward(self,x):\r\n            conf_map=self.conf_block.forward(x)\r\n            paf_map=self.paf_block.forward(x)\r\n            return conf_map,paf_map'"
