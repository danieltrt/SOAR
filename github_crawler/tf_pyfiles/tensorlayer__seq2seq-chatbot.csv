file_path,api_count,code
main.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport tensorlayer as tl\nimport numpy as np\nfrom tensorlayer.cost import cross_entropy_seq, cross_entropy_seq_with_mask\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom data.twitter import data\nfrom tensorlayer.models.seq2seq import Seq2seq\nfrom tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention\nimport os\n\n\ndef initial_setup(data_corpus):\n    metadata, idx_q, idx_a = data.load_data(PATH=\'data/{}/\'.format(data_corpus))\n    (trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(idx_q, idx_a)\n    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())\n    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())\n    testX = tl.prepro.remove_pad_sequences(testX.tolist())\n    testY = tl.prepro.remove_pad_sequences(testY.tolist())\n    validX = tl.prepro.remove_pad_sequences(validX.tolist())\n    validY = tl.prepro.remove_pad_sequences(validY.tolist())\n    return metadata, trainX, trainY, testX, testY, validX, validY\n\n\n\nif __name__ == ""__main__"":\n    data_corpus = ""twitter""\n\n    #data preprocessing\n    metadata, trainX, trainY, testX, testY, validX, validY = initial_setup(data_corpus)\n\n    # Parameters\n    src_len = len(trainX)\n    tgt_len = len(trainY)\n\n    assert src_len == tgt_len\n\n    batch_size = 32\n    n_step = src_len // batch_size\n    src_vocab_size = len(metadata[\'idx2w\']) # 8002 (0~8001)\n    emb_dim = 1024\n\n    word2idx = metadata[\'w2idx\']   # dict  word 2 index\n    idx2word = metadata[\'idx2w\']   # list index 2 word\n\n    unk_id = word2idx[\'unk\']   # 1\n    pad_id = word2idx[\'_\']     # 0\n\n    start_id = src_vocab_size  # 8002\n    end_id = src_vocab_size + 1  # 8003\n\n    word2idx.update({\'start_id\': start_id})\n    word2idx.update({\'end_id\': end_id})\n    idx2word = idx2word + [\'start_id\', \'end_id\']\n\n    src_vocab_size = tgt_vocab_size = src_vocab_size + 2\n\n    num_epochs = 50\n    vocabulary_size = src_vocab_size\n    \n\n\n    def inference(seed, top_n):\n        model_.eval()\n        seed_id = [word2idx.get(w, unk_id) for w in seed.split("" "")]\n        sentence_id = model_(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n        sentence = []\n        for w_id in sentence_id[0]:\n            w = idx2word[w_id]\n            if w == \'end_id\':\n                break\n            sentence = sentence + [w]\n        return sentence\n\n    decoder_seq_length = 20\n    model_ = Seq2seq(\n        decoder_seq_length = decoder_seq_length,\n        cell_enc=tf.keras.layers.GRUCell,\n        cell_dec=tf.keras.layers.GRUCell,\n        n_layer=3,\n        n_units=256,\n        embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim),\n        )\n    \n\n    # Uncomment below statements if you have already saved the model\n\n    # load_weights = tl.files.load_npz(name=\'model.npz\')\n    # tl.files.assign_weights(load_weights, model_)\n\n    optimizer = tf.optimizers.Adam(learning_rate=0.001)\n    model_.train()\n\n    seeds = [""happy birthday have a nice day"",\n                 ""donald trump won last nights presidential debate according to snap online polls""]\n    for epoch in range(num_epochs):\n        model_.train()\n        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n        total_loss, n_iter = 0, 0\n        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n                        total=n_step, desc=\'Epoch[{}/{}]\'.format(epoch + 1, num_epochs), leave=False):\n\n            X = tl.prepro.pad_sequences(X)\n            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n            _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=decoder_seq_length)\n            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=decoder_seq_length)\n            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n\n            with tf.GradientTape() as tape:\n                ## compute outputs\n                output = model_(inputs = [X, _decode_seqs])\n                \n                output = tf.reshape(output, [-1, vocabulary_size])\n                ## compute loss and update model\n                loss = cross_entropy_seq_with_mask(logits=output, target_seqs=_target_seqs, input_mask=_target_mask)\n\n                grad = tape.gradient(loss, model_.all_weights)\n                optimizer.apply_gradients(zip(grad, model_.all_weights))\n            \n            total_loss += loss\n            n_iter += 1\n\n        # printing average loss after every epoch\n        print(\'Epoch [{}/{}]: loss {:.4f}\'.format(epoch + 1, num_epochs, total_loss / n_iter))\n\n        for seed in seeds:\n            print(""Query >"", seed)\n            top_n = 3\n            for i in range(top_n):\n                sentence = inference(seed, top_n)\n                print("" >"", \' \'.join(sentence))\n\n        tl.files.save_npz(model_.all_weights, name=\'model.npz\')\n\n\n        \n    \n    \n'"
data/__init__.py,0,b'\nfrom __future__ import absolute_import\n\n# from . import twitter\n# from . import imagenet_classes\n# from . import\n'
data/cornell_corpus/data.py,0,"b'EN_WHITELIST = \'0123456789abcdefghijklmnopqrstuvwxyz \' # space is included in whitelist\nEN_BLACKLIST = \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\\'\'\n\nlimit = {\n        \'maxq\' : 25,\n        \'minq\' : 2,\n        \'maxa\' : 25,\n        \'mina\' : 2\n        }\n\nUNK = \'unk\'\nVOCAB_SIZE = 8000\n\n\nimport random\n\nimport nltk\nimport itertools\nfrom collections import defaultdict\n\nimport numpy as np\n\nimport pickle\n\n\n\n\'\'\'\n    1. Read from \'movie-lines.txt\'\n    2. Create a dictionary with ( key = line_id, value = text )\n\'\'\'\ndef get_id2line():\n    lines=open(\'raw_data/movie_lines.txt\', encoding=\'utf-8\', errors=\'ignore\').read().split(\'\\n\')\n    id2line = {}\n    for line in lines:\n        _line = line.split(\' +++$+++ \')\n        if len(_line) == 5:\n            id2line[_line[0]] = _line[4]\n    return id2line\n\n\'\'\'\n    1. Read from \'movie_conversations.txt\'\n    2. Create a list of [list of line_id\'s]\n\'\'\'\ndef get_conversations():\n    conv_lines = open(\'raw_data/movie_conversations.txt\', encoding=\'utf-8\', errors=\'ignore\').read().split(\'\\n\')\n    convs = [ ]\n    for line in conv_lines[:-1]:\n        _line = line.split(\' +++$+++ \')[-1][1:-1].replace(""\'"","""").replace("" "","""")\n        convs.append(_line.split(\',\'))\n    return convs\n\n\'\'\'\n    1. Get each conversation\n    2. Get each line from conversation\n    3. Save each conversation to file\n\'\'\'\ndef extract_conversations(convs,id2line,path=\'\'):\n    idx = 0\n    for conv in convs:\n        f_conv = open(path + str(idx)+\'.txt\', \'w\')\n        for line_id in conv:\n            f_conv.write(id2line[line_id])\n            f_conv.write(\'\\n\')\n        f_conv.close()\n        idx += 1\n\n\'\'\'\n    Get lists of all conversations as Questions and Answers\n    1. [questions]\n    2. [answers]\n\'\'\'\ndef gather_dataset(convs, id2line):\n    questions = []; answers = []\n\n    for conv in convs:\n        if len(conv) %2 != 0:\n            conv = conv[:-1]\n        for i in range(len(conv)):\n            if i%2 == 0:\n                questions.append(id2line[conv[i]])\n            else:\n                answers.append(id2line[conv[i]])\n\n    return questions, answers\n\n\n\'\'\'\n    We need 4 files\n    1. train.enc : Encoder input for training\n    2. train.dec : Decoder input for training\n    3. test.enc  : Encoder input for testing\n    4. test.dec  : Decoder input for testing\n\'\'\'\ndef prepare_seq2seq_files(questions, answers, path=\'\',TESTSET_SIZE = 30000):\n\n    # open files\n    train_enc = open(path + \'train.enc\',\'w\')\n    train_dec = open(path + \'train.dec\',\'w\')\n    test_enc  = open(path + \'test.enc\', \'w\')\n    test_dec  = open(path + \'test.dec\', \'w\')\n\n    # choose 30,000 (TESTSET_SIZE) items to put into testset\n    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n\n    for i in range(len(questions)):\n        if i in test_ids:\n            test_enc.write(questions[i]+\'\\n\')\n            test_dec.write(answers[i]+ \'\\n\' )\n        else:\n            train_enc.write(questions[i]+\'\\n\')\n            train_dec.write(answers[i]+ \'\\n\' )\n        if i%10000 == 0:\n            print(\'\\n>> written {} lines\'.format(i))\n\n    # close files\n    train_enc.close()\n    train_dec.close()\n    test_enc.close()\n    test_dec.close()\n\n\n\n\'\'\'\n remove anything that isn\'t in the vocabulary\n    return str(pure en)\n\n\'\'\'\ndef filter_line(line, whitelist):\n    return \'\'.join([ ch for ch in line if ch in whitelist ])\n\n\n\n\'\'\'\n filter too long and too short sequences\n    return tuple( filtered_ta, filtered_en )\n\n\'\'\'\ndef filter_data(qseq, aseq):\n    filtered_q, filtered_a = [], []\n    raw_data_len = len(qseq)\n\n    assert len(qseq) == len(aseq)\n\n    for i in range(raw_data_len):\n        qlen, alen = len(qseq[i].split(\' \')), len(aseq[i].split(\' \'))\n        if qlen >= limit[\'minq\'] and qlen <= limit[\'maxq\']:\n            if alen >= limit[\'mina\'] and alen <= limit[\'maxa\']:\n                filtered_q.append(qseq[i])\n                filtered_a.append(aseq[i])\n\n    # print the fraction of the original data, filtered\n    filt_data_len = len(filtered_q)\n    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n    print(str(filtered) + \'% filtered from original data\')\n\n    return filtered_q, filtered_a\n\n\n\'\'\'\n read list of words, create index to word,\n  word to index dictionaries\n    return tuple( vocab->(word, count), idx2w, w2idx )\n\n\'\'\'\ndef index_(tokenized_sentences, vocab_size):\n    # get frequency distribution\n    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n    # get vocabulary of \'vocab_size\' most used words\n    vocab = freq_dist.most_common(vocab_size)\n    # index2word\n    index2word = [\'_\'] + [UNK] + [ x[0] for x in vocab ]\n    # word2index\n    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n    return index2word, word2index, freq_dist\n\n\'\'\'\n filter based on number of unknowns (words not in vocabulary)\n  filter out the worst sentences\n\n\'\'\'\ndef filter_unk(qtokenized, atokenized, w2idx):\n    data_len = len(qtokenized)\n\n    filtered_q, filtered_a = [], []\n\n    for qline, aline in zip(qtokenized, atokenized):\n        unk_count_q = len([ w for w in qline if w not in w2idx ])\n        unk_count_a = len([ w for w in aline if w not in w2idx ])\n        if unk_count_a <= 2:\n            if unk_count_q > 0:\n                if unk_count_q/len(qline) > 0.2:\n                    pass\n            filtered_q.append(qline)\n            filtered_a.append(aline)\n\n    # print the fraction of the original data, filtered\n    filt_data_len = len(filtered_q)\n    filtered = int((data_len - filt_data_len)*100/data_len)\n    print(str(filtered) + \'% filtered from original data\')\n\n    return filtered_q, filtered_a\n\n\n\n\n\'\'\'\n create the final dataset :\n  - convert list of items to arrays of indices\n  - add zero padding\n      return ( [array_en([indices]), array_ta([indices]) )\n\n\'\'\'\ndef zero_pad(qtokenized, atokenized, w2idx):\n    # num of rows\n    data_len = len(qtokenized)\n\n    # numpy arrays to store indices\n    idx_q = np.zeros([data_len, limit[\'maxq\']], dtype=np.int32)\n    idx_a = np.zeros([data_len, limit[\'maxa\']], dtype=np.int32)\n\n    for i in range(data_len):\n        q_indices = pad_seq(qtokenized[i], w2idx, limit[\'maxq\'])\n        a_indices = pad_seq(atokenized[i], w2idx, limit[\'maxa\'])\n\n        #print(len(idx_q[i]), len(q_indices))\n        #print(len(idx_a[i]), len(a_indices))\n        idx_q[i] = np.array(q_indices)\n        idx_a[i] = np.array(a_indices)\n\n    return idx_q, idx_a\n\n\n\'\'\'\n replace words with indices in a sequence\n  replace with unknown if word not in lookup\n    return [list of indices]\n\n\'\'\'\ndef pad_seq(seq, lookup, maxlen):\n    indices = []\n    for word in seq:\n        if word in lookup:\n            indices.append(lookup[word])\n        else:\n            indices.append(lookup[UNK])\n    return indices + [0]*(maxlen - len(seq))\n\n\n\n\n\ndef process_data():\n\n    id2line = get_id2line()\n    print(\'>> gathered id2line dictionary.\\n\')\n    convs = get_conversations()\n    print(convs[121:125])\n    print(\'>> gathered conversations.\\n\')\n    questions, answers = gather_dataset(convs,id2line)\n\n    # change to lower case (just for en)\n    questions = [ line.lower() for line in questions ]\n    answers = [ line.lower() for line in answers ]\n\n    # filter out unnecessary characters\n    print(\'\\n>> Filter lines\')\n    questions = [ filter_line(line, EN_WHITELIST) for line in questions ]\n    answers = [ filter_line(line, EN_WHITELIST) for line in answers ]\n\n    # filter out too long or too short sequences\n    print(\'\\n>> 2nd layer of filtering\')\n    qlines, alines = filter_data(questions, answers)\n\n    for q,a in zip(qlines[141:145], alines[141:145]):\n        print(\'q : [{0}]; a : [{1}]\'.format(q,a))\n\n    # convert list of [lines of text] into list of [list of words ]\n    print(\'\\n>> Segment lines into words\')\n    qtokenized = [ [w.strip() for w in wordlist.split(\' \') if w] for wordlist in qlines ]\n    atokenized = [ [w.strip() for w in wordlist.split(\' \') if w] for wordlist in alines ]\n    print(\'\\n:: Sample from segmented list of words\')\n\n    for q,a in zip(qtokenized[141:145], atokenized[141:145]):\n        print(\'q : [{0}]; a : [{1}]\'.format(q,a))\n\n    # indexing -> idx2w, w2idx\n    print(\'\\n >> Index words\')\n    idx2w, w2idx, freq_dist = index_( qtokenized + atokenized, vocab_size=VOCAB_SIZE)\n\n    # filter out sentences with too many unknowns\n    print(\'\\n >> Filter Unknowns\')\n    qtokenized, atokenized = filter_unk(qtokenized, atokenized, w2idx)\n    print(\'\\n Final dataset len : \' + str(len(qtokenized)))\n\n\n    print(\'\\n >> Zero Padding\')\n    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n\n    print(\'\\n >> Save numpy arrays to disk\')\n    # save them\n    np.save(\'idx_q.npy\', idx_q)\n    np.save(\'idx_a.npy\', idx_a)\n\n    # let us now save the necessary dictionaries\n    metadata = {\n            \'w2idx\' : w2idx,\n            \'idx2w\' : idx2w,\n            \'limit\' : limit,\n            \'freq_dist\' : freq_dist\n                }\n\n    # write to disk : data control dictionaries\n    with open(\'metadata.pkl\', \'wb\') as f:\n        pickle.dump(metadata, f)\n\n    # count of unknowns\n    unk_count = (idx_q == 1).sum() + (idx_a == 1).sum()\n    # count of words\n    word_count = (idx_q > 1).sum() + (idx_a > 1).sum()\n\n    print(\'% unknown : {0}\'.format(100 * (unk_count/word_count)))\n    print(\'Dataset count : \' + str(idx_q.shape[0]))\n\n\n    #print \'>> gathered questions and answers.\\n\'\n    #prepare_seq2seq_files(questions,answers)\n\n\nimport numpy as np\nfrom random import sample\n\n\'\'\'\n split data into train (70%), test (15%) and valid(15%)\n    return tuple( (trainX, trainY), (testX,testY), (validX,validY) )\n\n\'\'\'\ndef split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n    # number of examples\n    data_len = len(x)\n    lens = [ int(data_len*item) for item in ratio ]\n\n    trainX, trainY = x[:lens[0]], y[:lens[0]]\n    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n\n    return (trainX,trainY), (testX,testY), (validX,validY)\n\n\n\'\'\'\n generate batches from dataset\n    yield (x_gen, y_gen)\n\n    TODO : fix needed\n\n\'\'\'\ndef batch_gen(x, y, batch_size):\n    # infinite while\n    while True:\n        for i in range(0, len(x), batch_size):\n            if (i+1)*batch_size < len(x):\n                yield x[i : (i+1)*batch_size ].T, y[i : (i+1)*batch_size ].T\n\n\'\'\'\n generate batches, by random sampling a bunch of items\n    yield (x_gen, y_gen)\n\n\'\'\'\ndef rand_batch_gen(x, y, batch_size):\n    while True:\n        sample_idx = sample(list(np.arange(len(x))), batch_size)\n        yield x[sample_idx].T, y[sample_idx].T\n\n#\'\'\'\n# convert indices of alphabets into a string (word)\n#    return str(word)\n#\n#\'\'\'\n#def decode_word(alpha_seq, idx2alpha):\n#    return \'\'.join([ idx2alpha[alpha] for alpha in alpha_seq if alpha ])\n#\n#\n#\'\'\'\n# convert indices of phonemes into list of phonemes (as string)\n#    return str(phoneme_list)\n#\n#\'\'\'\n#def decode_phonemes(pho_seq, idx2pho):\n#    return \' \'.join( [ idx2pho[pho] for pho in pho_seq if pho ])\n\n\n\'\'\'\n a generic decode function\n    inputs : sequence, lookup\n\n\'\'\'\ndef decode(sequence, lookup, separator=\'\'): # 0 used for padding, is ignored\n    return separator.join([ lookup[element] for element in sequence if element ])\n\n\n\nif __name__ == \'__main__\':\n    process_data()\n\n\ndef load_data(PATH=\'\'):\n    # read data control dictionaries\n    with open(PATH + \'metadata.pkl\', \'rb\') as f:\n        metadata = pickle.load(f)\n    # read numpy arrays\n    idx_q = np.load(PATH + \'idx_q.npy\')\n    idx_a = np.load(PATH + \'idx_a.npy\')\n    return metadata, idx_q, idx_a\n'"
data/twitter/data.py,0,"b'EN_WHITELIST = \'0123456789abcdefghijklmnopqrstuvwxyz \' # space is included in whitelist\nEN_BLACKLIST = \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\\'\'\n\nFILENAME = \'data/chat.txt\'\n\nlimit = {\n        \'maxq\' : 20,\n        \'minq\' : 0,\n        \'maxa\' : 20,\n        \'mina\' : 3\n        }\n\nUNK = \'unk\'\nVOCAB_SIZE = 6000\n\nimport random\nimport sys\n\nimport nltk\nimport itertools\nfrom collections import defaultdict\n\nimport numpy as np\n\nimport pickle\n\n\ndef ddefault():\n    return 1\n\n\'\'\'\n read lines from file\n     return [list of lines]\n\n\'\'\'\ndef read_lines(filename):\n    return open(filename).read().split(\'\\n\')[:-1]\n\n\n\'\'\'\n split sentences in one line\n  into multiple lines\n    return [list of lines]\n\n\'\'\'\ndef split_line(line):\n    return line.split(\'.\')\n\n\n\'\'\'\n remove anything that isn\'t in the vocabulary\n    return str(pure ta/en)\n\n\'\'\'\ndef filter_line(line, whitelist):\n    return \'\'.join([ ch for ch in line if ch in whitelist ])\n\n\n\'\'\'\n read list of words, create index to word,\n  word to index dictionaries\n    return tuple( vocab->(word, count), idx2w, w2idx )\n\n\'\'\'\ndef index_(tokenized_sentences, vocab_size):\n    # get frequency distribution\n    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n    # get vocabulary of \'vocab_size\' most used words\n    vocab = freq_dist.most_common(vocab_size)\n    # index2word\n    index2word = [\'_\'] + [UNK] + [ x[0] for x in vocab ]\n    # word2index\n    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n    return index2word, word2index, freq_dist\n\n\n\'\'\'\n filter too long and too short sequences\n    return tuple( filtered_ta, filtered_en )\n\n\'\'\'\ndef filter_data(sequences):\n    filtered_q, filtered_a = [], []\n    raw_data_len = len(sequences)//2\n\n    for i in range(0, len(sequences), 2):\n        qlen, alen = len(sequences[i].split(\' \')), len(sequences[i+1].split(\' \'))\n        if qlen >= limit[\'minq\'] and qlen <= limit[\'maxq\']:\n            if alen >= limit[\'mina\'] and alen <= limit[\'maxa\']:\n                filtered_q.append(sequences[i])\n                filtered_a.append(sequences[i+1])\n\n    # print the fraction of the original data, filtered\n    filt_data_len = len(filtered_q)\n    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n    print(str(filtered) + \'% filtered from original data\')\n\n    return filtered_q, filtered_a\n\n\n\n\n\n\'\'\'\n create the final dataset :\n  - convert list of items to arrays of indices\n  - add zero padding\n      return ( [array_en([indices]), array_ta([indices]) )\n\n\'\'\'\ndef zero_pad(qtokenized, atokenized, w2idx):\n    # num of rows\n    data_len = len(qtokenized)\n\n    # numpy arrays to store indices\n    idx_q = np.zeros([data_len, limit[\'maxq\']], dtype=np.int32)\n    idx_a = np.zeros([data_len, limit[\'maxa\']], dtype=np.int32)\n\n    for i in range(data_len):\n        q_indices = pad_seq(qtokenized[i], w2idx, limit[\'maxq\'])\n        a_indices = pad_seq(atokenized[i], w2idx, limit[\'maxa\'])\n\n        #print(len(idx_q[i]), len(q_indices))\n        #print(len(idx_a[i]), len(a_indices))\n        idx_q[i] = np.array(q_indices)\n        idx_a[i] = np.array(a_indices)\n\n    return idx_q, idx_a\n\n\n\'\'\'\n replace words with indices in a sequence\n  replace with unknown if word not in lookup\n    return [list of indices]\n\n\'\'\'\ndef pad_seq(seq, lookup, maxlen):\n    indices = []\n    for word in seq:\n        if word in lookup:\n            indices.append(lookup[word])\n        else:\n            indices.append(lookup[UNK])\n    return indices + [0]*(maxlen - len(seq))\n\n\ndef process_data():\n\n    print(\'\\n>> Read lines from file\')\n    lines = read_lines(filename=FILENAME)\n\n    # change to lower case (just for en)\n    lines = [ line.lower() for line in lines ]\n\n    print(\'\\n:: Sample from read(p) lines\')\n    print(lines[121:125])\n\n    # filter out unnecessary characters\n    print(\'\\n>> Filter lines\')\n    lines = [ filter_line(line, EN_WHITELIST) for line in lines ]\n    print(lines[121:125])\n\n    # filter out too long or too short sequences\n    print(\'\\n>> 2nd layer of filtering\')\n    qlines, alines = filter_data(lines)\n    print(\'\\nq : {0} ; a : {1}\'.format(qlines[60], alines[60]))\n    print(\'\\nq : {0} ; a : {1}\'.format(qlines[61], alines[61]))\n\n\n    # convert list of [lines of text] into list of [list of words ]\n    print(\'\\n>> Segment lines into words\')\n    qtokenized = [ wordlist.split(\' \') for wordlist in qlines ]\n    atokenized = [ wordlist.split(\' \') for wordlist in alines ]\n    print(\'\\n:: Sample from segmented list of words\')\n    print(\'\\nq : {0} ; a : {1}\'.format(qtokenized[60], atokenized[60]))\n    print(\'\\nq : {0} ; a : {1}\'.format(qtokenized[61], atokenized[61]))\n\n\n    # indexing -> idx2w, w2idx : en/ta\n    print(\'\\n >> Index words\')\n    idx2w, w2idx, freq_dist = index_( qtokenized + atokenized, vocab_size=VOCAB_SIZE)\n\n    print(\'\\n >> Zero Padding\')\n    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n\n    print(\'\\n >> Save numpy arrays to disk\')\n    # save them\n    np.save(\'idx_q.npy\', idx_q)\n    np.save(\'idx_a.npy\', idx_a)\n\n    # let us now save the necessary dictionaries\n    metadata = {\n            \'w2idx\' : w2idx,\n            \'idx2w\' : idx2w,\n            \'limit\' : limit,\n            \'freq_dist\' : freq_dist\n                }\n\n    # write to disk : data control dictionaries\n    with open(\'metadata.pkl\', \'wb\') as f:\n        pickle.dump(metadata, f)\n\ndef load_data(PATH=\'\'):\n    # read data control dictionaries\n    try:\n        with open(PATH + \'metadata.pkl\', \'rb\') as f:\n            metadata = pickle.load(f)\n    except:\n        metadata = None\n    # read numpy arrays\n    idx_q = np.load(PATH + \'idx_q.npy\')\n    idx_a = np.load(PATH + \'idx_a.npy\')\n    return metadata, idx_q, idx_a\n\nimport numpy as np\nfrom random import sample\n\n\'\'\'\n split data into train (70%), test (15%) and valid(15%)\n    return tuple( (trainX, trainY), (testX,testY), (validX,validY) )\n\n\'\'\'\ndef split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n    # number of examples\n    data_len = len(x)\n    lens = [ int(data_len*item) for item in ratio ]\n\n    trainX, trainY = x[:lens[0]], y[:lens[0]]\n    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n\n    return (trainX,trainY), (testX,testY), (validX,validY)\n\n\n\'\'\'\n generate batches from dataset\n    yield (x_gen, y_gen)\n\n    TODO : fix needed\n\n\'\'\'\ndef batch_gen(x, y, batch_size):\n    # infinite while\n    while True:\n        for i in range(0, len(x), batch_size):\n            if (i+1)*batch_size < len(x):\n                yield x[i : (i+1)*batch_size ].T, y[i : (i+1)*batch_size ].T\n\n\'\'\'\n generate batches, by random sampling a bunch of items\n    yield (x_gen, y_gen)\n\n\'\'\'\ndef rand_batch_gen(x, y, batch_size):\n    while True:\n        sample_idx = sample(list(np.arange(len(x))), batch_size)\n        yield x[sample_idx].T, y[sample_idx].T\n\n#\'\'\'\n# convert indices of alphabets into a string (word)\n#    return str(word)\n#\n#\'\'\'\n#def decode_word(alpha_seq, idx2alpha):\n#    return \'\'.join([ idx2alpha[alpha] for alpha in alpha_seq if alpha ])\n#\n#\n#\'\'\'\n# convert indices of phonemes into list of phonemes (as string)\n#    return str(phoneme_list)\n#\n#\'\'\'\n#def decode_phonemes(pho_seq, idx2pho):\n#    return \' \'.join( [ idx2pho[pho] for pho in pho_seq if pho ])\n\n\n\'\'\'\n a generic decode function\n    inputs : sequence, lookup\n\n\'\'\'\ndef decode(sequence, lookup, separator=\'\'): # 0 used for padding, is ignored\n    return separator.join([ lookup[element] for element in sequence if element ])\n\n\n\nif __name__ == \'__main__\':\n    process_data()\n'"
