file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\nimport codecs\nimport os\nimport sys\n\nos.environ[\'TENSORLAYER_PACKAGE_BUILDING\'] = \'True\'\n\ntry:\n    from setuptools import find_packages, setup, Extension\n    from setuptools.command.build_ext import build_ext\n\nexcept ImportError:\n    from distutils.core import (setup, find_packages)\n\nfrom tensorlayer import (\n    __contact_emails__, __contact_names__, __description__, __download_url__, __homepage__, __keywords__, __license__,\n    __package_name__, __repository_url__, __version__\n)\n\n# =================== Reading Readme file as TXT files ===================\n\nif os.path.exists(\'README.rst\'):\n    # codec is used for consistent encoding\n    long_description = codecs.open(\n        os.path.join(os.path.abspath(os.path.dirname(__file__)), \'README.rst\'), \'r\', \'utf-8\'\n    ).read()\n\nelse:\n    long_description = \'See \' + __homepage__\n\n# ======================= Reading Requirements files as TXT files =======================\n\n\ndef req_file(filename, folder=""requirements""):\n    with open(os.path.join(folder, filename)) as f:\n        content = f.readlines()\n    # you may also want to remove whitespace characters\n    # Example: `\\n` at the end of each line\n    return [x.strip() for x in content]\n\n\n# ======================= Defining the requirements var =======================\n\ninstall_requires = req_file(""requirements.txt"")\n\nextras_require = {\n    # User packages\n    \'tf_cpu\': req_file(""requirements_tf_cpu.txt""),\n    \'tf_gpu\': req_file(""requirements_tf_gpu.txt""),\n    \'extra\': req_file(""requirements_extra.txt""),\n\n    # Contrib Packages\n    \'contrib_loggers\': req_file(""requirements_contrib_loggers.txt""),\n\n    # Dev Packages\n    \'test\': req_file(""requirements_test.txt""),\n    \'dev\': req_file(""requirements_dev.txt""),\n    \'doc\': req_file(""requirements_doc.txt""),\n    \'db\': req_file(""requirements_db.txt""),\n}\n\nextras_require[\'all\'] = sum([extras_require.get(key) for key in [\'extra\', \'contrib_loggers\']], list())\n\nextras_require[\'all_cpu\'] = sum([extras_require.get(key) for key in [\'all\', \'tf_cpu\']], list())\nextras_require[\'all_gpu\'] = sum([extras_require.get(key) for key in [\'all\', \'tf_gpu\']], list())\n\nextras_require[\'all_dev\'] = sum([extras_require.get(key) for key in [\'all\', \'db\', \'dev\', \'doc\', \'test\']], list())\nextras_require[\'all_cpu_dev\'] = sum([extras_require.get(key) for key in [\'all_dev\', \'tf_cpu\']], list())\nextras_require[\'all_gpu_dev\'] = sum([extras_require.get(key) for key in [\'all_dev\', \'tf_gpu\']], list())\n\ncmdclass = dict()\next_modules = []\n\n# Readthedocs requires TF 1.5.0 to build properly\nif \'READTHEDOCS\' in os.environ:\n    ext_modules = [\n        Extension(\'install_requirements_for_rtd\', []),\n    ]\n\n    class custom_build_ext(build_ext):\n\n        def build_extensions(self):\n            os.system(\'./scripts/install-requirements-for-rtd.sh %s\' % os.path.dirname(sys.executable))\n\n    cmdclass = {\'build_ext\': custom_build_ext}\n\n# ======================= Define the package setup =======================\n\nsetup(\n    name=__package_name__,\n\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=__version__,\n    description=__description__,\n    long_description=long_description,\n\n    # The project\'s main homepage.\n    url=__repository_url__,\n    download_url=__download_url__,\n\n    # Author details\n    author=__contact_names__,\n    author_email=__contact_emails__,\n\n    # maintainer Details\n    maintainer=__contact_names__,\n    maintainer_email=__contact_emails__,\n\n    # The licence under which the project is released\n    license=__license__,\n    classifiers=[\n        # How mature is this project? Common values are\n        #  1 - Planning\n        #  2 - Pre-Alpha\n        #  3 - Alpha\n        #  4 - Beta\n        #  5 - Production/Stable\n        #  6 - Mature\n        #  7 - Inactive\n        \'Development Status :: 5 - Production/Stable\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n        \'Intended Audience :: Information Technology\',\n\n        # Indicate what your project relates to\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Image Recognition\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Utilities\',\n\n        # Pick your license as you wish (should match ""license"" above)\n        \'License :: OSI Approved :: Apache Software License\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n\n        # Additionnal Settings\n        \'Environment :: Console\',\n        \'Natural Language :: English\',\n        \'Operating System :: OS Independent\',\n    ],\n    keywords=__keywords__,\n    packages=find_packages(),\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=install_requires,\n    cmdclass=cmdclass,\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # $ pip install -e .[test]\n    extras_require=extras_require,\n    ext_modules=ext_modules,\n    scripts=[\n        \'tl\',\n    ],\n)\n'"
docker/pypi_list.py,0,"b'import argparse\nimport requests\nimport logging\n\nimport pip._internal\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser(description=\'Get the nth version of a given package\')\n    parser.add_argument(\'--package\', type=str, required=True, help=\'The PyPI you want to inspect\')\n    parser.add_argument(\'--nth_last_version\', type=int, default=1, help=\'The nth last package will be retrieved\')\n    parser.add_argument(\'--prerelease\', help=\'Get PreRelease Package Version\', action=\'store_true\')\n    parser.add_argument(\'--debug\', help=\'Print debug information\', action=\'store_true\')\n\n    args = parser.parse_args()\n\n    # create logger\n    logger = logging.getLogger(""PyPI_CLI"")\n\n    formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    if args.debug:\n        logger.setLevel(logging.DEBUG)\n\n    logger.debug(""Package: %s"" % args.package)\n    logger.debug(""nth_last_version: %s"" % args.nth_last_version)\n    logger.debug(""prerelease: %s"" % args.prerelease)\n    logger.debug(""debug: %s"" % args.debug)\n\n    finder = pip._internal.index.PackageFinder([], [\'https://pypi.python.org/simple\'], session=requests.Session())\n    results = finder.find_all_candidates(args.package)\n    tmp_versions = [str(p.version) for p in results]\n\n    logger.debug(""%s"" % tmp_versions)\n\n    versions = list()\n    for el in tmp_versions:\n        if el not in versions:\n            versions.append(el)\n\n    pos = -1\n    nth_version = 1\n\n    while True:\n        fetched_version = versions[pos]\n\n        logger.debug(""Version: %s"" % fetched_version)\n\n        if nth_version == args.nth_last_version:\n            if args.prerelease or not (""rc"" in fetched_version or ""a"" in fetched_version or ""b"" in fetched_version):\n                break\n            else:\n                pos -= 1\n                continue\n\n        pos -= 1\n        nth_version += 1\n\n    print(fetched_version)\n'"
docker/version_prefix.py,0,"b'import argparse\nimport logging\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser(description=\'Determine the version prefix to apply depending on the version name\')\n\n    parser.add_argument(\n        \'--version\', type=str, required=True, help=\'The Package Version to be installed in the container\'\n    )\n\n    parser.add_argument(\'--debug\', help=\'Print debug information\', action=\'store_true\')\n\n    args = parser.parse_args()\n\n    # create logger\n    logger = logging.getLogger(""VERSION_PREFIX_CLI"")\n\n    formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    if args.debug:\n        logger.setLevel(logging.DEBUG)\n\n    logger.debug(""Package Version: %s"" % args.version)\n\n    if ""rc"" in args.version or ""a"" in args.version or ""b"" in args.version:\n        print(""latest-dev"")\n    else:\n        print(""latest"")\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# TensorLayer documentation build configuration file, created by\n# sphinx-quickstart on Tue Aug  2 15:30:55 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os, sys, datetime\nsys.path.insert(0, os.path.abspath(""../""))  # Important\nsys.path.insert(0, os.path.abspath(os.path.join("".."", ""tensorlayer"")))  # Important\n\nfrom package_info import __shortversion__\nfrom package_info import __version__\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\n# extensions = [\n#    \'sphinx.ext.coverage\',\n#    \'sphinx.ext.githubpages\',\n#    \'numpydoc\',\n# ]\n\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.inheritance_diagram\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.viewcode\',\n]\n\nautodoc_mock_imports = [\n    \'cv2\',\n    \'gridfs\',\n    \'horovod\',\n    \'hyperdash\',\n    \'imageio\',\n    \'lxml\',\n    \'matplotlib\',\n    \'nltk\',\n    \'numpy\',\n    \'PIL\',\n    \'progressbar\',\n    \'pymongo\',\n    \'scipy\',\n    \'skimage\',\n    \'sklearn\',\n    \'tensorflow\',\n    \'tqdm\',\n    \'h5py\',\n\n    # TL C++ Packages\n    \'tensorlayer.third_party.roi_pooling.roi_pooling.roi_pooling_ops\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'TensorLayer\'\ncopyright = \'2016~%s, TensorLayer Contributors\' % (str(datetime.datetime.now().year))\nauthor = \'TensorLayer Contributors\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = __shortversion__\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#\n# today = \'\'\n#\n# Else, today_fmt is used as the format for a strftime call.\n#\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n#\n# html_title = \'TensorLayer\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#\n# html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#\n# html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#\n# html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n#\n# html_domain_indices = True\n\n# If false, no index is generated.\n#\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'h\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'r\', \'sv\', \'tr\', \'zh\'\n#\n# html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#\n# html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#\n# html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'TensorLayerdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'TensorLayer.tex\', \'TensorLayer Documentation\', \'TensorLayer contributors\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n#\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#\n# latex_appendices = []\n\n# If false, no module index is generated.\n#\n# latex_domain_indices = True\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \'tensorlayer\', \'TensorLayer Documentation\', [author], 1)]\n\n# If true, show URL addresses after external links.\n#\n# man_show_urls = False\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc, \'TensorLayer\', \'TensorLayer Documentation\', author, \'TensorLayer\',\n        \'Deep learning and Reinforcement learning library for Researchers and Engineers.\', \'Miscellaneous\'\n    ),\n]\n\n# Documents to append as an appendix to all manuals.\n#\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n#\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#\n# texinfo_no_detailmenu = False\n\n# -- Options for Epub output ----------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\nepub_author = author\nepub_publisher = author\nepub_copyright = copyright\n\n# The basename for the epub file. It defaults to the project name.\n# epub_basename = project\n\n# The HTML theme for the epub output. Since the default themes are not\n# optimized for small screen space, using the same theme for HTML and epub\n# output is usually not wise. This defaults to \'epub\', a theme designed to save\n# visual space.\n#\n# epub_theme = \'epub\'\n\n# The language of the text. It defaults to the language option\n# or \'en\' if the language is not set.\n#\n# epub_language = \'\'\n\n# The scheme of the identifier. Typical schemes are ISBN or URL.\n# epub_scheme = \'\'\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A tuple containing the cover image and cover page html template filenames.\n#\n# epub_cover = ()\n\n# A sequence of (type, uri, title) tuples for the guide element of content.opf.\n#\n# epub_guide = ()\n\n# HTML files that should be inserted before the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n#\n# epub_pre_files = []\n\n# HTML files that should be inserted after the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n#\n# epub_post_files = []\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n# The depth of the table of contents in toc.ncx.\n#\n# epub_tocdepth = 3\n\n# Allow duplicate toc entries.\n#\n# epub_tocdup = True\n\n# Choose between \'default\' and \'includehidden\'.\n#\n# epub_tocscope = \'default\'\n\n# Fix unsupported image types using the Pillow.\n#\n# epub_fix_images = False\n\n# Scale large images.\n#\n# epub_max_image_width = 0\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#\n# epub_show_urls = \'inline\'\n\n# If false, no index is generated.\n#\n# epub_use_index = True\n\npygments_style = \'sphinx\'\n\nhtml_theme = ""sphinx_rtd_theme""\n\nhtml_theme_path = []\n'"
examples/tutorial_work_with_onnx.py,22,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\nr""""""\nPlay with ONNX models in TensorLayer.\n\nThis tutorial is corresponding to the onnx-tf tutorial:\nhttps://github.com/onnx/tutorials/blob/7b549ae622ff8d74a5f5e0c32e109267f4c9ccae/tutorials/OnnxTensorflowExport.ipynb\n\nIntroduction\n----------------\nONNX is an open-source specification for neural models. It has the following components:\n- A definition of an extensible computation graph model.\n- Definitions of standard data types.\n- Definitions of built-in operators\nCaffe2, PyTorch, Microsoft Cognitive Toolkit, Apache MXNet and other tools are developing ONNX support. Enabling interoperability between different frameworks and streamlining the path from research to production will increase the speed of innovation in the AI community.\n\nTo run this script, you shall have the following pre-requisites:\n----------------------------\n- Install ONNX and onnx-tf package:\n>>> pip install onnx\n>>> pip install onnx-tf\nNote: When installing in a non-Anaconda environment, make sure to install the Protobuf compiler before running the pip installation of onnx. For example, on Ubuntu:\n>>>sudo apt-get install protobuf-compiler libprotoc-dev\n>>>pip install onnx\nMore details please go to ONNX official website: https://github.com/onnx/onnx\n\n- Testing environment configuration\xef\xbc\x9a\nUbuntu:16.04.4 LTS\nPython:3.6.5\nTensorLayer:1.8.6rc2\nTensorFlow-gpu:1.8.0\nonnx:1.2.2\nonnx-tf:1.1.2\n\nTutorial structure\n------------------\n\n1.Training\n----------\nFirstly, we can initiate the training script by issuing the command on your terminal.\n>>>python tutorial_work_with_onnx.py\n Shortly, we should obtain a trained MNIST model. The training process needs no special instrumentation. However, to successfully convert the trained model, onnx-tensorflow requires three pieces of information, all of which can be obtained after training is complete:\n\n- Graph definition:\nYou need to obtain information about the graph definition in the form of GraphProto. The easiest way to achieve this is to use the following snippet of code as shown in the example training script:\n>>>with open(""graph.proto"", ""wb"") as file:\n>>> graph = tf.get_default_graph().as_graph_def(add_shapes=True)\n>>> file.write(graph.SerializeToString())\nThis code is under the code where you call your architecture in your function\n\n- Shape information: By default, as_graph_def does not serialize any information about the shapes of the intermediate tensor and such information is required by onnx-tensorflow. Thus we request Tensorflow to serialize the shape information by adding the keyword argument add_shapes=True as demonstrated above.\n\n- Checkpoint: Tensorflow checkpoint files contain information about the obtained weight; thus they are needed to convert the trained model to ONNX format.\n\n2.Graph Freezing\n----------------\nSecondly, we freeze the graph. Thus here we build the free_graph tool in TensorLayer source folder and execute it with the information about where the GraphProto is, where the checkpoint file is and where to put the freozen graph.\n>>>python3 -m tensorflow.python.tools.freeze_graph \\\n    --input_graph=/root/graph.proto \\\n    --input_checkpoint=/root/model/model.ckpt \\\n    --output_graph=/root/frozen_graph.pb \\\n    --output_node_names=output/bias_add\\\n    --input_binary=True\n\nnote:\ninput_graph is the path of your proto file\ninput_checkpoint is the path of your checkpoint file\noutput_graph is the path where you want to put\noutput_node is the output node you want to put into your graph:\nyou can try this code to print and find the node what you want:\n>>>print([n.name for n in tf.get_default_graph().as_graph_def().node])\n\nNote that now we have obtained the frozen_graph.pb with graph definition as well as weight information in one file.\n\n3.Model Conversion\n-----------------\nThirdly, we convert the model to ONNX format using onnx-tensorflow. Using tensorflow_graph_to_onnx_model from onnx-tensorflow API (documentation available at https://github.com/onnx/onnx-tensorflow/blob/master/onnx_tf/doc/API.md).\n>>>import tensorflow as tf\n>>>from onnx_tf.frontend import tensorflow_graph_to_onnx_model\n\n>>>with tf.gfile.GFile(""frozen_graph.pb"", ""rb"") as f:\n>>>    graph_def = tf.GraphDef()\n>>>    graph_def.ParseFromString(f.read())\n>>>    onnx_model = tensorflow_graph_to_onnx_model(graph_def,\n>>>                                     ""output/bias_add"",\n>>>                                     opset=6)\n\n>>>    file = open(""mnist.onnx"", ""wb"")\n>>>    file.write(onnx_model.SerializeToString())\n>>>    file.close()\n\nThen you will get thr first node info:\n>>>input: ""cnn1/kernel""\n>>>output: ""cnn1/kernel/read""\n>>>name: ""cnn1/kernel/read""\n>>>op_type: ""Identity""\n\n4.Inference using Backend(This part onnx-tf is under implementation!!!)\n-------------------------------------------------------------------\nIn this tutorial, we continue our demonstration by performing inference using this obtained ONNX model. Here, we exported an image representing a handwritten 7 and stored the numpy array as image.npz. Using onnx-tf backend, we will classify this image using the converted ONNX model.\n>>>import onnx\n>>>import numpy as np\n>>>from onnx_tf.backend import prepare\n\n>>>model = onnx.load(\'mnist.onnx\')\n>>>tf_rep = prepare(model)\n>>>#Image Path\n>>>img = np.load(""./assets/image.npz"", allow_pickle=True)\n>>>output = tf_rep.run(img.reshape([1, 784]))\n>>>print ""The digit is classified as "", np.argmax(output)\n\nYou will get the information in your console:\n>>>The digit is classified as  7\n\n""""""\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.tools.freeze_graph import freeze_graph as _freeze_graph\n\nimport onnx\nimport tensorlayer as tl\nfrom onnx_tf.backend import prepare\nfrom onnx_tf.frontend import tensorflow_graph_to_onnx_model\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n\ndef generate_graph_and_checkpoint(graph_output_path, checkpoint_output_path):\n    """"""\n    Reimplementation of the TensorFlow official MNIST CNN tutorials and generate the graph and checkpoint for this model:\n    - https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros/index.html\n    - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py\n\n    - For simplified CNN layer see ""Convolutional layer (Simplified)""\n\n    Parameters\n    -----------\n    graph_output_path : string\n        the path of the graph where you want to save.\n    checkpoint_output_path : string\n        the path of the checkpoint where you want to save.\n\n    References\n    -----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`__\n\n    """"""\n    X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n\n    sess = tf.InteractiveSession()\n\n    batch_size = 128\n    x = tf.placeholder(tf.float32, shape=[batch_size, 28, 28, 1])  # [batch_size, height, width, channels]\n    y_ = tf.placeholder(tf.int64, shape=[batch_size])\n\n    net = tl.layers.InputLayer(x, name=\'input\')\n\n    # Simplified conv API (the same with the above layers)\n    net = tl.layers.Conv2d(net, 32, (5, 5), (1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'cnn1\')\n    net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding=\'SAME\', name=\'pool1\')\n    net = tl.layers.Conv2d(net, 64, (5, 5), (1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'cnn2\')\n    net = tl.layers.MaxPool2d(net, (2, 2), (2, 2), padding=\'SAME\', name=\'pool2\')\n    # end of conv\n    net = tl.layers.FlattenLayer(net, name=\'flatten\')\n    net = tl.layers.DropoutLayer(net, keep=0.5, name=\'drop1\')\n    net = tl.layers.DenseLayer(net, 256, act=tf.nn.relu, name=\'relu1\')\n    net = tl.layers.DropoutLayer(net, keep=0.5, name=\'drop2\')\n    net = tl.layers.DenseLayer(net, 10, act=None, name=\'output\')\n\n    y = net.outputs\n\n    print([n.name for n in tf.get_default_graph().as_graph_def().node])\n\n    # To string Graph\n    with open(graph_output_path, ""wb"") as file:\n        graph = tf.get_default_graph().as_graph_def(add_shapes=True)\n        file.write(graph.SerializeToString())\n\n    cost = tl.cost.cross_entropy(y, y_, \'cost\')\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    # train\n    n_epoch = 200\n    learning_rate = 0.0001\n    print_freq = 10\n\n    train_params = net.all_params\n    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, var_list=train_params)\n\n    tl.layers.initialize_global_variables(sess)\n    net.print_params()\n    net.print_layers()\n\n    print(\'   learning_rate: %f\' % learning_rate)\n    print(\'   batch_size: %d\' % batch_size)\n\n    for epoch in range(n_epoch):\n        start_time = time.time()\n        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n            feed_dict = {x: X_train_a, y_: y_train_a}\n            feed_dict.update(net.all_drop)  # enable noise layers\n            sess.run(train_op, feed_dict=feed_dict)\n        # Save the checkpoint every 10 eopchs\n        if epoch % 10 == 0:\n            tl.files.save_ckpt(sess, mode_name=\'model.ckpt\', save_dir=checkpoint_output_path, printable=True)\n        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n            print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n            train_loss, train_acc, n_batch = 0, 0, 0\n            for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n                dp_dict = tl.utils.dict_to_one(net.all_drop)  # disable noise layers\n                feed_dict = {x: X_train_a, y_: y_train_a}\n                feed_dict.update(dp_dict)\n                err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n                train_loss += err\n                train_acc += ac\n                n_batch += 1\n            print(""   train loss: %f"" % (train_loss / n_batch))\n            print(""   train acc: %f"" % (train_acc / n_batch))\n            val_loss, val_acc, n_batch = 0, 0, 0\n            for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n                dp_dict = tl.utils.dict_to_one(net.all_drop)  # disable noise layers\n                feed_dict = {x: X_val_a, y_: y_val_a}\n                feed_dict.update(dp_dict)\n                err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n                val_loss += err\n                val_acc += ac\n                n_batch += 1\n            print(""   val loss: %f"" % (val_loss / n_batch))\n            print(""   val acc: %f"" % (val_acc / n_batch))\n\n    # Evaluation\n    print(\'Evaluation\')\n    test_loss, test_acc, n_batch = 0, 0, 0\n    for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n        dp_dict = tl.utils.dict_to_one(net.all_drop)  # disable noise layers\n        feed_dict = {x: X_test_a, y_: y_test_a}\n        feed_dict.update(dp_dict)\n        err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n        test_loss += err\n        test_acc += ac\n        n_batch += 1\n    print(""   test loss: %f"" % (test_loss / n_batch))\n    print(""   test acc: %f"" % (test_acc / n_batch))\n\n\ndef freeze_graph(graph_path, checkpoint_path, output_path, end_node_names, is_binary_graph):\n    """"""Reimplementation of the TensorFlow official freeze_graph function to freeze the graph and checkpoint together:\n\n    Parameters\n    -----------\n    graph_path : string\n        the path where your graph file save.\n    checkpoint_output_path : string\n        the path where your checkpoint save.\n    output_path : string\n        the path where you want to save the output proto buff\n    end_node_names : string\n        the name of the end node in your graph you want to get in your proto buff\n    is_binary_graph : boolean\n        declare your file whether is a binary graph\n\n    References\n    ----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`__\n    - `tensorflow freeze_graph <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py>`\n    """"""\n    _freeze_graph(\n        input_graph=graph_path, input_saver=\'\', input_binary=is_binary_graph, input_checkpoint=checkpoint_path,\n        output_graph=output_path, output_node_names=end_node_names, restore_op_name=\'save/restore_all\',\n        filename_tensor_name=\'save/Const:0\', clear_devices=True, initializer_nodes=None\n    )\n\n\ndef convert_model_to_onnx(frozen_graph_path, end_node_names, onnx_output_path):\n    """"""Reimplementation of the TensorFlow-onnx official tutorial convert the proto buff to onnx file:\n\n    Parameters\n    -----------\n    frozen_graph_path : string\n        the path where your frozen graph file save.\n    end_node_names : string\n        the name of the end node in your graph you want to get in your proto buff\n    onnx_output_path : string\n        the path where you want to save the onnx file.\n\n    References\n    -----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`\n    """"""\n    with tf.gfile.GFile(frozen_graph_path, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        onnx_model = tensorflow_graph_to_onnx_model(graph_def, end_node_names, opset=6)\n        file = open(onnx_output_path, ""wb"")\n        file.write(onnx_model.SerializeToString())\n        file.close()\n\n\ndef convert_onnx_to_model(onnx_input_path):\n    """"""Reimplementation of the TensorFlow-onnx official tutorial convert the onnx file to specific: model\n\n    Parameters\n    -----------\n    onnx_input_path : string\n    the path where you save the onnx file.\n\n    References\n    -----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`__\n    """"""\n    model = onnx.load(onnx_input_path)\n    tf_rep = prepare(model)\n    # Image Path\n    img = np.load(""./assets/image.npz"", allow_pickle=True)\n    output = tf_rep.run(img.reshape([1, 784]))\n    print(""The digit is classified as "", np.argmax(output))\n\n\nif __name__ == \'__main__\':\n\n    # 1. Train the CNN network and output the graph and checkpoints\n    generate_graph_and_checkpoint(graph_output_path=\'graph.proto\', checkpoint_output_path=\'./\')\n\n    # 2. Freeze the graph with checkpoints\n    freeze_graph(\n        graph_path=\'graph.proto\', is_binary_graph=True, checkpoint_path=\'model.ckpt\', output_path=\'frozen_graph.pb\',\n        end_node_names=\'output/bias_add\'\n    )\n\n    # 3. Convert the tensorflow protobuf file to ONNX file\n    convert_model_to_onnx(\n        frozen_graph_path=\'frozen_graph.pb\', end_node_names=\'output/bias_add\', onnx_output_path=\'mnist.onnx\'\n    )\n\n    # 4. Convert thr ONNX file to specific model\n    # the following step is not working by far as the tensorflow-onnx project has a bug at the time of writing.\n    # convert_onnx_to_model(onnx_input_path=\'mnist.onnx\')\n'"
tensorlayer/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""Deep learning and Reinforcement learning library for Researchers and Engineers""""""\n\nimport os\nfrom distutils.version import LooseVersion\n\nfrom tensorlayer.package_info import (\n    VERSION, __contact_emails__, __contact_names__, __description__, __download_url__, __homepage__, __keywords__,\n    __license__, __package_name__, __repository_url__, __shortversion__, __version__\n)\n\nif \'TENSORLAYER_PACKAGE_BUILDING\' not in os.environ:\n\n    try:\n        import tensorflow\n    except Exception as e:\n        raise ImportError(\n            ""Tensorflow is not installed, please install it with the one of the following commands:\\n""\n            "" - `pip install --upgrade tensorflow`\\n""\n            "" - `pip install --upgrade tensorflow-gpu`""\n        )\n\n    if (""SPHINXBUILD"" not in os.environ and ""READTHEDOCS"" not in os.environ and\n            LooseVersion(tensorflow.__version__) < LooseVersion(""2.0.0"")):\n        raise RuntimeError(\n            ""TensorLayer does not support Tensorflow version older than 2.0.0.\\n""\n            ""Please update Tensorflow with:\\n""\n            "" - `pip install --upgrade tensorflow`\\n""\n            "" - `pip install --upgrade tensorflow-gpu`""\n        )\n\n    from tensorlayer import activation\n    from tensorlayer import array_ops\n    from tensorlayer import cost\n    from tensorlayer import decorators\n    from tensorlayer import files\n    from tensorlayer import initializers\n    from tensorlayer import iterate\n    from tensorlayer import layers\n    from tensorlayer import lazy_imports\n    from tensorlayer import logging\n    from tensorlayer import models\n    from tensorlayer import optimizers\n    from tensorlayer import rein\n    from tensorlayer import utils\n\n    from tensorlayer.lazy_imports import LazyImport\n\n    # Lazy Imports\n    db = LazyImport(""tensorlayer.db"")\n    distributed = LazyImport(""tensorlayer.distributed"")\n    nlp = LazyImport(""tensorlayer.nlp"")\n    prepro = LazyImport(""tensorlayer.prepro"")\n    utils = LazyImport(""tensorlayer.utils"")\n    visualize = LazyImport(""tensorlayer.visualize"")\n\n    # alias\n    act = activation\n    vis = visualize\n\n    alphas = array_ops.alphas\n    alphas_like = array_ops.alphas_like\n\n    # global vars\n    global_flag = {}\n    global_dict = {}\n'"
tensorlayer/activation.py,38,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""A file containing various activation functions.""""""\n\nimport tensorflow as tf\n\nfrom tensorlayer.decorators import deprecated\n\n__all__ = [\n    \'leaky_relu\',\n    \'leaky_relu6\',\n    \'leaky_twice_relu6\',\n    \'lrelu\',\n    \'lrelu6\',\n    \'ltrelu6\',\n    \'ramp\',\n    \'swish\',\n    \'sign\',\n    \'htanh\',\n    \'hard_tanh\',\n    \'pixel_wise_softmax\',\n    \'mish\',\n]\n\n\ndef ramp(x, v_min=0, v_max=1, name=None):\n    """"""Ramp activation function.\n\n    Reference: [tf.clip_by_value]<https://www.tensorflow.org/api_docs/python/tf/clip_by_value>\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n    v_min : float\n        cap input to v_min as a lower bound.\n    v_max : float\n        cap input to v_max as a upper bound.\n    name : str\n        The function name (optional).\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    """"""\n    return tf.clip_by_value(x, clip_value_min=v_min, clip_value_max=v_max, name=name)\n\n\n# @deprecated(date=""2018-09-30"", instructions=""This API is deprecated. Please use as `tf.nn.leaky_relu`"")\ndef leaky_relu(x, alpha=0.2, name=""leaky_relu""):\n    """"""leaky_relu can be used through its shortcut: :func:`tl.act.lrelu`.\n\n    This function is a modified version of ReLU, introducing a nonzero gradient for negative input. Introduced by the paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x >= 0: ``f(x) = x``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha : float\n        Slope.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.Input([10, 200])\n    >>> net = tl.layers.Dense(n_units=100, act=lambda x : tl.act.lrelu(x, 0.2), name=\'dense\')(net)\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    """"""\n    if not (0 < alpha <= 1):\n        raise ValueError(""`alpha` value must be in [0, 1]`"")\n\n    with tf.name_scope(name) as name_scope:\n        x = tf.convert_to_tensor(x, name=""features"")\n        return tf.maximum(x, alpha * x, name=name_scope)\n\n\ndef leaky_relu6(x, alpha=0.2, name=""leaky_relu6""):\n    """""":func:`leaky_relu6` can be used through its shortcut: :func:`tl.act.lrelu6`.\n\n    This activation function is a modified version :func:`leaky_relu` introduced by the following paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    This activation function also follows the behaviour of the activation function :func:`tf.nn.relu6` introduced by the following paper:\n    `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x in [0, 6]: ``f(x) = x``.\n      - When x > 6: ``f(x) = 6``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha : float\n        Slope.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.Input([10, 200])\n    >>> net = tl.layers.Dense(n_units=100, act=lambda x : tl.act.leaky_relu6(x, 0.2), name=\'dense\')(net)\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n    """"""\n    if not isinstance(alpha, tf.Tensor) and not (0 < alpha <= 1):\n        raise ValueError(""`alpha` value must be in [0, 1]`"")\n\n    with tf.name_scope(name) as name_scope:\n        x = tf.convert_to_tensor(x, name=""features"")\n        return tf.minimum(tf.maximum(x, alpha * x), 6, name=name_scope)\n\n\ndef leaky_twice_relu6(x, alpha_low=0.2, alpha_high=0.2, name=""leaky_relu6""):\n    """""":func:`leaky_twice_relu6` can be used through its shortcut: :func:`:func:`tl.act.ltrelu6`.\n\n    This activation function is a modified version :func:`leaky_relu` introduced by the following paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    This activation function also follows the behaviour of the activation function :func:`tf.nn.relu6` introduced by the following paper:\n    `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    This function push further the logic by adding `leaky` behaviour both below zero and above six.\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x in [0, 6]: ``f(x) = x``.\n      - When x > 6: ``f(x) = 6 + (alpha_high * (x-6))``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha_low : float\n        Slope for x < 0: ``f(x) = alpha_low * x``.\n    alpha_high : float\n        Slope for x < 6: ``f(x) = 6 (alpha_high * (x-6))``.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.Input([10, 200])\n    >>> net = tl.layers.Dense(n_units=100, act=lambda x : tl.act.leaky_twice_relu6(x, 0.2, 0.2), name=\'dense\')(net)\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    """"""\n    if not isinstance(alpha_high, tf.Tensor) and not (0 < alpha_high <= 1):\n        raise ValueError(""`alpha_high` value must be in [0, 1]`"")\n\n    if not isinstance(alpha_low, tf.Tensor) and not (0 < alpha_low <= 1):\n        raise ValueError(""`alpha_low` value must be in [0, 1]`"")\n\n    with tf.name_scope(name) as name_scope:\n        x = tf.convert_to_tensor(x, name=""features"")\n\n        x_is_above_0 = tf.minimum(x, 6 * (1 - alpha_high) + alpha_high * x)\n        x_is_below_0 = tf.minimum(alpha_low * x, 0)\n\n        return tf.maximum(x_is_above_0, x_is_below_0, name=name_scope)\n\n\ndef swish(x, name=\'swish\'):\n    """"""Swish function.\n\n     See `Swish: a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941>`__.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n    name: str\n        function name (optional).\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    """"""\n    # TODO: in this case, the beta = 1, but the beta can either be a constant or a trainable parameter\n    with tf.name_scope(name):\n        x = tf.nn.sigmoid(x) * x\n    return x\n\n\n# @tf.RegisterGradient(""QuantizeGrad"")\n# def _sign_grad(unused_op, grad):\n#     return tf.clip_by_value(grad, -1, 1)\n\n\n@tf.custom_gradient\ndef sign(x):\n    """"""Sign function.\n\n    Clip and binarize tensor using the straight through estimator (STE) for the gradient, usually be used for\n    quantizing values in `Binarized Neural Networks`: https://arxiv.org/abs/1602.02830.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models, Maas et al. (2013)`\n       http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf\n\n    - `BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. (2016)`\n       https://arxiv.org/abs/1602.02830\n\n    """"""\n\n    def grad(dy):\n        return tf.clip_by_value(dy, -1, 1)\n\n    return tf.sign(x, name=\'sign\'), grad\n\n\n# if tf.__version__ > ""1.7"":\n#     @tf.custom_gradient\n#     def sign(x):  # https://www.tensorflow.org/versions/master/api_docs/python/tf/custom_gradient?hl=ES#top_of_page\n#         """"""Differentiable sign function using sigmoid as the derivation function,\n#         see `tf.sign <https://www.tensorflow.org/api_docs/python/tf/sign>`__ and `tf.custom_gradient\n#         <https://www.tensorflow.org/versions/master/api_docs/python/tf/custom_gradient?hl=ES#top_of_page>`__.\n#\n#         Parameters\n#         ----------\n#         x : Tensor\n#             input.\n#\n#         Returns\n#         -------\n#         Tensor\n#             A ``Tensor`` in the same type as ``x``.\n#\n#         """"""\n#         tao = tf.nn.sigmoid(x)\n#         def grad():\n#             return tao * (1 - tao)\n#         return tf.sign(x), grad\n\n\ndef hard_tanh(x, name=\'htanh\'):\n    """"""Hard tanh activation function.\n\n    Which is a ramp function with low bound of -1 and upper bound of 1, shortcut is `htanh`.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n    name : str\n        The function name (optional).\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    """"""\n    # with tf.variable_scope(""hard_tanh""):\n    return tf.clip_by_value(x, -1, 1, name=name)\n\n\n@deprecated(date=""2018-06-30"", instructions=""This API will be deprecated soon as tf.nn.softmax can do the same thing"")\ndef pixel_wise_softmax(x, name=\'pixel_wise_softmax\'):\n    """"""Return the softmax outputs of images, every pixels have multiple label, the sum of a pixel is 1.\n\n    Usually be used for image segmentation.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n            - For 2d image, 4D tensor (batch_size, height, weight, channel), where channel >= 2.\n            - For 3d image, 5D tensor (batch_size, depth, height, weight, channel), where channel >= 2.\n    name : str\n        function name (optional)\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    Examples\n    --------\n    >>> outputs = pixel_wise_softmax(network.outputs)\n    >>> dice_loss = 1 - dice_coe(outputs, y_, epsilon=1e-5)\n\n    References\n    ----------\n    - `tf.reverse <https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse>`__\n\n    """"""\n    with tf.name_scope(name):\n        return tf.nn.softmax(x)\n\n\ndef mish(x):\n    """"""Mish activation function.\n\n    Reference: [Mish: A Self Regularized Non-Monotonic Neural Activation Function .Diganta Misra, 2019]<https://arxiv.org/abs/1908.08681>\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    """"""\n    return x * tf.math.tanh(tf.math.softplus(x))\n\n\n# Alias\nlrelu = leaky_relu\nlrelu6 = leaky_relu6\nltrelu6 = leaky_twice_relu6\nhtanh = hard_tanh\n'"
tensorlayer/array_ops.py,3,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""A file containing functions related to array manipulation.""""""\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op, dtypes, ops, tensor_shape\nfrom tensorflow.python.framework.constant_op import constant\nfrom tensorflow.python.framework.ops import convert_to_tensor\nfrom tensorflow.python.ops.array_ops import shape_internal\nfrom tensorflow.python.ops.gen_array_ops import fill, reshape\n\n__all__ = [\'alphas\', \'alphas_like\']\n\n\ndef alphas(shape, alpha_value, name=None):\n    """"""Creates a tensor with all elements set to `alpha_value`.\n    This operation returns a tensor of type `dtype` with shape `shape` and all\n    elements set to alpha.\n\n    Parameters\n    ----------\n    shape: A list of integers, a tuple of integers, or a 1-D `Tensor` of type `int32`.\n        The shape of the desired tensor\n    alpha_value: `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, int32`, `int64`\n        The value used to fill the resulting `Tensor`.\n    name: str\n        A name for the operation (optional).\n\n    Returns\n    -------\n    A `Tensor` with all elements set to alpha.\n\n    Examples\n    --------\n    >>> tl.alphas([2, 3], tf.int32)  # [[alpha, alpha, alpha], [alpha, alpha, alpha]]\n    """"""\n    with ops.name_scope(name, ""alphas"", [shape]) as name:\n\n        alpha_tensor = convert_to_tensor(alpha_value)\n        alpha_dtype = dtypes.as_dtype(alpha_tensor.dtype).base_dtype\n\n        if not isinstance(shape, ops.Tensor):\n            try:\n                shape = constant_op._tensor_shape_tensor_conversion_function(tensor_shape.TensorShape(shape))\n            except (TypeError, ValueError):\n                shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\n\n        if not shape._shape_tuple():\n            shape = reshape(shape, [-1])  # Ensure it\'s a vector\n\n        try:\n            output = constant(alpha_value, shape=shape, dtype=alpha_dtype, name=name)\n\n        except (TypeError, ValueError):\n            output = fill(shape, constant(alpha_value, dtype=alpha_dtype), name=name)\n\n        if output.dtype.base_dtype != alpha_dtype:\n            raise AssertionError(""Dtypes do not corresponds: %s and %s"" % (output.dtype.base_dtype, alpha_dtype))\n\n        return output\n\n\ndef alphas_like(tensor, alpha_value, name=None, optimize=True):\n    """"""Creates a tensor with all elements set to `alpha_value`.\n    Given a single tensor (`tensor`), this operation returns a tensor of the same\n    type and shape as `tensor` with all elements set to `alpha_value`.\n\n    Parameters\n    ----------\n    tensor: tf.Tensor\n        The Tensorflow Tensor that will be used as a template.\n    alpha_value: `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, int32`, `int64`\n        The value used to fill the resulting `Tensor`.\n    name: str\n        A name for the operation (optional).\n    optimize: bool\n        if true, attempt to statically determine the shape of \'tensor\' and encode it as a constant.\n\n    Returns\n    -------\n    A `Tensor` with all elements set to `alpha_value`.\n\n    Examples\n    --------\n    >>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n    >>> tl.alphas_like(tensor, 0.5)  # [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]\n    """"""\n    with ops.name_scope(name, ""alphas_like"", [tensor]) as name:\n        tensor = ops.convert_to_tensor(tensor, name=""tensor"")\n\n        if context.in_eager_mode():  # and dtype is not None and dtype != tensor.dtype:\n            ret = alphas(shape_internal(tensor, optimize=optimize), alpha_value=alpha_value, name=name)\n\n        else:  # if context.in_graph_mode():\n\n            # For now, variant types must be created via zeros_like; as we need to\n            # pass the input variant object to the proper zeros callback.\n\n            if (optimize and tensor.shape.is_fully_defined()):\n                # We can produce a zeros tensor independent of the value of \'tensor\',\n                # since the shape is known statically.\n                ret = alphas(tensor.shape, alpha_value=alpha_value, name=name)\n\n            # elif dtype is not None and dtype != tensor.dtype and dtype != dtypes.variant:\n            else:\n                ret = alphas(shape_internal(tensor, optimize=optimize), alpha_value=alpha_value, name=name)\n\n            ret.set_shape(tensor.get_shape())\n\n        return ret\n'"
tensorlayer/cost.py,95,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numbers\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops, math_ops, nn_ops, standard_ops\n\nfrom tensorlayer import logging\n\n__all__ = [\n    \'cross_entropy\',\n    \'sigmoid_cross_entropy\',\n    \'binary_cross_entropy\',\n    \'mean_squared_error\',\n    \'normalized_mean_square_error\',\n    \'absolute_difference_error\',\n    \'dice_coe\',\n    \'dice_hard_coe\',\n    \'iou_coe\',\n    \'cross_entropy_seq\',\n    \'cross_entropy_seq_with_mask\',\n    \'cosine_similarity\',\n    \'li_regularizer\',\n    \'lo_regularizer\',\n    \'maxnorm_regularizer\',\n    \'maxnorm_o_regularizer\',\n    \'maxnorm_i_regularizer\',\n]\n\n\ndef cross_entropy(output, target, name=None):\n    """"""Softmax cross-entropy operation, returns the TensorFlow expression of cross-entropy for two distributions,\n    it implements softmax internally. See ``tf.nn.sparse_softmax_cross_entropy_with_logits``.\n\n    Parameters\n    ----------\n    output : Tensor\n        A batch of distribution with shape: [batch_size, num of classes].\n    target : Tensor\n        A batch of index with shape: [batch_size, ].\n    name : string\n        Name of this loss.\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> ce = tl.cost.cross_entropy(y_logits, y_target_logits, \'my_loss\')\n\n    References\n    -----------\n    - About cross-entropy: `<https://en.wikipedia.org/wiki/Cross_entropy>`__.\n    - The code is borrowed from: `<https://en.wikipedia.org/wiki/Cross_entropy>`__.\n\n    """"""\n    # if name is None:\n    #     raise Exception(""Please give a unique name to tl.cost.cross_entropy for TF1.0+"")\n    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output), name=name)\n\n\ndef sigmoid_cross_entropy(output, target, name=None):\n    """"""Sigmoid cross-entropy operation, see ``tf.nn.sigmoid_cross_entropy_with_logits``.\n\n    Parameters\n    ----------\n    output : Tensor\n        A batch of distribution with shape: [batch_size, num of classes].\n    target : Tensor\n        A batch of index with shape: [batch_size, ].\n    name : string\n        Name of this loss.\n\n    """"""\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output), name=name)\n\n\ndef binary_cross_entropy(output, target, epsilon=1e-8, name=\'bce_loss\'):\n    """"""Binary cross entropy operation.\n\n    Parameters\n    ----------\n    output : Tensor\n        Tensor with type of `float32` or `float64`.\n    target : Tensor\n        The target distribution, format the same with `output`.\n    epsilon : float\n        A small value to avoid output to be zero.\n    name : str\n        An optional name to attach to this function.\n\n    References\n    -----------\n    - `ericjang-DRAW <https://github.com/ericjang/draw/blob/master/draw.py#L73>`__\n\n    """"""\n    #     with ops.op_scope([output, target], name, ""bce_loss"") as name:\n    #         output = ops.convert_to_tensor(output, name=""preds"")\n    #         target = ops.convert_to_tensor(targets, name=""target"")\n\n    # with tf.name_scope(name):\n    return tf.reduce_mean(\n        tf.reduce_sum(\n            -(target * tf.math.log(output + epsilon) + (1. - target) * tf.math.log(1. - output + epsilon)), axis=1\n        ), name=name\n    )\n\n    # For brevity, let `x = output`, `z = target`.  The binary cross entropy loss is\n    #\n    #     loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n\n\ndef mean_squared_error(output, target, is_mean=False, axis=-1, name=""mean_squared_error""):\n    """"""Return the TensorFlow expression of mean-square-error (L2) of two batch of data.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    is_mean : boolean\n        Whether compute the mean or sum for each example.\n            - If True, use ``tf.reduce_mean`` to compute the loss between one target and predict data.\n            - If False, use ``tf.reduce_sum`` (default).\n    axis : int or list of int\n        The dimensions to reduce.\n    name : str\n        An optional name to attach to this function.\n\n    References\n    ------------\n    - `Wiki Mean Squared Error <https://en.wikipedia.org/wiki/Mean_squared_error>`__\n\n    """"""\n    # with tf.name_scope(name):\n    # if len(output.shape) == 2:  # [batch_size, n_feature]\n    #     axis = 1\n    # elif len(output.shape) == 3:  # [batch_size, w, h]\n    #     axis = [1, 2]\n    # elif len(output.shape) == 4:  # [batch_size, w, h, c]\n    #     axis = [1, 2, 3]\n    # else:\n    #     raise Exception(""Unknow dimension"")\n\n    if is_mean:\n        mse = tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(output, target), axis), name=name)\n    else:\n        mse = tf.reduce_mean(tf.reduce_sum(tf.math.squared_difference(output, target), axis), name=name)\n    return mse\n\n\ndef normalized_mean_square_error(output, target, axis=-1, name=""normalized_mean_squared_error_loss""):\n    """"""Return the TensorFlow expression of normalized mean-square-error of two distributions.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    axis : int or list of int\n        The dimensions to reduce.\n    name : str\n        An optional name to attach to this function.\n\n    """"""\n    with tf.name_scope(""normalized_mean_squared_error_loss""):\n        # if len(output.shape) == 2:  # [batch_size, n_feature]\n        #     axis = 1\n        # elif len(output.shape) == 3:  # [batch_size, w, h]\n        #     axis = [1, 2]\n        # elif len(output.shape) == 4:  # [batch_size, w, h, c]\n        #     axis = [1, 2, 3]\n        nmse_a = tf.sqrt(tf.reduce_sum(tf.math.squared_difference(output, target), axis=axis))\n        nmse_b = tf.sqrt(tf.reduce_sum(tf.square(target), axis=axis))\n        nmse = tf.reduce_mean(nmse_a / nmse_b, name=name)\n    return nmse\n\n\ndef absolute_difference_error(output, target, is_mean=False, axis=-1, name=""absolute_difference_error_loss""):\n    """"""Return the TensorFlow expression of absolute difference error (L1) of two batch of data.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    is_mean : boolean\n        Whether compute the mean or sum for each example.\n            - If True, use ``tf.reduce_mean`` to compute the loss between one target and predict data.\n            - If False, use ``tf.reduce_sum`` (default).\n    axis : int or list of int\n        The dimensions to reduce.\n    name : str\n        An optional name to attach to this function.\n\n    """"""\n    # # with tf.name_scope(""absolute_difference_error_loss""):\n    # if len(output.shape) == 2:  # [batch_size, n_feature]\n    #     axis = 1\n    # elif len(output.shape) == 3:  # [batch_size, w, h]\n    #     axis = [1, 2]\n    # elif len(output.shape) == 4:  # [batch_size, w, h, c]\n    #     axis = [1, 2, 3]\n    # else:\n    #     raise Exception(""Unknow dimension"")\n    if is_mean:\n        loss = tf.reduce_mean(tf.reduce_mean(tf.abs(output - target), axis), name=name)\n    else:\n        loss = tf.reduce_mean(tf.reduce_sum(tf.abs(output - target), axis), name=name)\n    return loss\n\n\ndef dice_coe(output, target, loss_type=\'jaccard\', axis=(1, 2, 3), smooth=1e-5):\n    """"""Soft dice (S\xc3\xb8rensen or Jaccard) coefficient for comparing the similarity\n    of two batch of data, usually be used for binary image segmentation\n    i.e. labels are binary. The coefficient between 0 to 1, 1 means totally match.\n\n    Parameters\n    -----------\n    output : Tensor\n        A distribution with shape: [batch_size, ....], (any dimensions).\n    target : Tensor\n        The target distribution, format the same with `output`.\n    loss_type : str\n        ``jaccard`` or ``sorensen``, default is ``jaccard``.\n    axis : tuple of int\n        All dimensions are reduced, default ``[1,2,3]``.\n    smooth : float\n        This small value will be added to the numerator and denominator.\n            - If both output and target are empty, it makes sure dice is 1.\n            - If either output or target are empty (all pixels are background), dice = ```smooth/(small_value + smooth)``, then if smooth is very small, dice close to 0 (even the image values lower than the threshold), so in this case, higher smooth can have a higher dice.\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> outputs = tl.act.pixel_wise_softmax(outputs)\n    >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_)\n\n    References\n    -----------\n    - `Wiki-Dice <https://en.wikipedia.org/wiki/S\xc3\xb8rensen\xe2\x80\x93Dice_coefficient>`__\n\n    """"""\n    inse = tf.reduce_sum(output * target, axis=axis)\n    if loss_type == \'jaccard\':\n        l = tf.reduce_sum(output * output, axis=axis)\n        r = tf.reduce_sum(target * target, axis=axis)\n    elif loss_type == \'sorensen\':\n        l = tf.reduce_sum(output, axis=axis)\n        r = tf.reduce_sum(target, axis=axis)\n    else:\n        raise Exception(""Unknow loss_type"")\n    # old axis=[0,1,2,3]\n    # dice = 2 * (inse) / (l + r)\n    # epsilon = 1e-5\n    # dice = tf.clip_by_value(dice, 0, 1.0-epsilon) # if all empty, dice = 1\n    # new haodong\n    dice = (2. * inse + smooth) / (l + r + smooth)\n    ##\n    dice = tf.reduce_mean(dice, name=\'dice_coe\')\n    return dice\n\n\ndef dice_hard_coe(output, target, threshold=0.5, axis=(1, 2, 3), smooth=1e-5):\n    """"""Non-differentiable S\xc3\xb8rensen\xe2\x80\x93Dice coefficient for comparing the similarity\n    of two batch of data, usually be used for binary image segmentation i.e. labels are binary.\n    The coefficient between 0 to 1, 1 if totally match.\n\n    Parameters\n    -----------\n    output : tensor\n        A distribution with shape: [batch_size, ....], (any dimensions).\n    target : tensor\n        The target distribution, format the same with `output`.\n    threshold : float\n        The threshold value to be true.\n    axis : tuple of integer\n        All dimensions are reduced, default ``(1,2,3)``.\n    smooth : float\n        This small value will be added to the numerator and denominator, see ``dice_coe``.\n\n    References\n    -----------\n    - `Wiki-Dice <https://en.wikipedia.org/wiki/S\xc3\xb8rensen\xe2\x80\x93Dice_coefficient>`__\n\n    """"""\n    output = tf.cast(output > threshold, dtype=tf.float32)\n    target = tf.cast(target > threshold, dtype=tf.float32)\n    inse = tf.reduce_sum(tf.multiply(output, target), axis=axis)\n    l = tf.reduce_sum(output, axis=axis)\n    r = tf.reduce_sum(target, axis=axis)\n    # old axis=[0,1,2,3]\n    # hard_dice = 2 * (inse) / (l + r)\n    # epsilon = 1e-5\n    # hard_dice = tf.clip_by_value(hard_dice, 0, 1.0-epsilon)\n    # new haodong\n    hard_dice = (2. * inse + smooth) / (l + r + smooth)\n    ##\n    hard_dice = tf.reduce_mean(hard_dice, name=\'hard_dice\')\n    return hard_dice\n\n\ndef iou_coe(output, target, threshold=0.5, axis=(1, 2, 3), smooth=1e-5):\n    """"""Non-differentiable Intersection over Union (IoU) for comparing the\n    similarity of two batch of data, usually be used for evaluating binary image segmentation.\n    The coefficient between 0 to 1, and 1 means totally match.\n\n    Parameters\n    -----------\n    output : tensor\n        A batch of distribution with shape: [batch_size, ....], (any dimensions).\n    target : tensor\n        The target distribution, format the same with `output`.\n    threshold : float\n        The threshold value to be true.\n    axis : tuple of integer\n        All dimensions are reduced, default ``(1,2,3)``.\n    smooth : float\n        This small value will be added to the numerator and denominator, see ``dice_coe``.\n\n    Notes\n    ------\n    - IoU cannot be used as training loss, people usually use dice coefficient for training, IoU and hard-dice for evaluating.\n\n    """"""\n    pre = tf.cast(output > threshold, dtype=tf.float32)\n    truth = tf.cast(target > threshold, dtype=tf.float32)\n    inse = tf.reduce_sum(tf.multiply(pre, truth), axis=axis)  # AND\n    union = tf.reduce_sum(tf.cast(tf.add(pre, truth) >= 1, dtype=tf.float32), axis=axis)  # OR\n    # old axis=[0,1,2,3]\n    # epsilon = 1e-5\n    # batch_iou = inse / (union + epsilon)\n    # new haodong\n    batch_iou = (inse + smooth) / (union + smooth)\n    iou = tf.reduce_mean(batch_iou, name=\'iou_coe\')\n    return iou  # , pre, truth, inse, union\n\n\n# ## test soft/hard dice and iou\n# import numpy as np\n# y = np.zeros((1,10,10,1))\n# # y[0,0:5,0:5]=1.0\n# o = np.zeros((1,10,10,1))\n# # o[:,:,:,:] = 0            # what we want: dice=0   iou=0  OK\n# # o[0,0:2,0:2]=0.3          # what we want: dice larger iou=0  OK\n# # o[0,0:2,0:2]=0.6          # what we want: dice larger  iou small  OK\n# # o[0,0:3,0:3]=0.6          # what we want: dice larger iou larger OK\n# # o[0,0:3,0:3]=1            # what we want: dice larger iou same OK\n# # o[0,0:5,0:5]=1            # what we want: dice=1 iou=1  OK\n# # o[0,0:5,0:5]=0.3          # what we want: dice smaller  iou=0  OK\n# # o[0,0:5,0:5]=1e-2           # what we want: dice\xe2\x89\x880 iou=0  OK\n# # o[0,8:10,8:10]=1.0        # what we want: dice=0 iou=0  OK\n# # o[0,8:10,8:10]=1e-10        # what we want: dice=0 iou=0  OK\n# # y[:,:,:,:] = o[:,:,:,:] = 0 # what we want: dice=1 iou=1  OK\n# ## why in u-net, dice=1 hard-dice=1 iou=1 exist?? print bug?\n#\n# d = dice_coe(o, y, \'jaccard\', smooth=1.)\n# hd = dice_hard_coe(o, y, smooth=1e-5)\n# i = iou_coe(o, y, smooth=1e-5)\n# sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n# # sess.run(tf.local_variables_initializer())\n# print(sess.run([d,hd,i]))\n# # p, t, i, u = sess.run([pre, truth, inse, union])\n# # import pprint\n# # pprint.pprint(((y>0.5)*(o>0.5)).astype(int).tolist())\n# # pprint.pprint(p.tolist())\n# # pprint.pprint(t.tolist())\n# # pprint.pprint(i)\n# # pprint.pprint(u)\n# exit()\n\n\ndef sequence_loss_by_example(\n    logits, targets, weights, average_across_timesteps=True, softmax_loss_function=None, name=None\n):\n    """"""Weighted cross-entropy loss for a sequence of logits (per example). see original tensorflow code :\n    <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L1057>\n\n    Parameters\n    ----------\n    logits: List\n        List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List\n        List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List\n        List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: Boolean\n        If set, divide the returned cost by the total label weight.\n    softmax_loss_function: None or Function\n        Function (labels, logits) -> loss-batch to be used instead of the standard softmax (the default if this is None).\n        **Note that to avoid confusion, it is required for the function to accept named arguments.**\n    name: None or str\n        Optional name for this operation, default: ""sequence_loss_by_example"".\n\n    Returns\n    -------\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\n\n    Raises\n    ------\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n\n    """"""\n    if len(targets) != len(logits) or len(weights) != len(logits):\n        raise ValueError(\n            ""Lengths of logits, weights, and targets must be the same ""\n            ""%d, %d, %d."" % (len(logits), len(weights), len(targets))\n        )\n    with ops.name_scope(name, ""sequence_loss_by_example"", logits + targets + weights):\n        log_perp_list = []\n        for logit, target, weight in zip(logits, targets, weights):\n            if softmax_loss_function is None:\n                # TODO(irving,ebrevdo): This reshape is needed because\n                # sequence_loss_by_example is called with scalars sometimes, which\n                # violates our general scalar strictness policy.\n                target = array_ops.reshape(target, [-1])\n                crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(labels=target, logits=logit)\n            else:\n                crossent = softmax_loss_function(labels=target, logits=logit)\n            log_perp_list.append(crossent * weight)\n    log_perps = math_ops.add_n(log_perp_list)\n    if average_across_timesteps:\n        total_size = math_ops.add_n(weights)\n        total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n        log_perps /= total_size\n    return log_perps\n\n\ndef cross_entropy_seq(logits, target_seqs, batch_size=None):\n    """"""Returns the expression of cross-entropy of two sequences, implement\n    softmax internally. Normally be used for fixed length RNN outputs, see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm.py>`__.\n\n    Parameters\n    ----------\n    logits : Tensor\n        2D tensor with shape of `[batch_size * n_steps, n_classes]`.\n    target_seqs : Tensor\n        The target sequence, 2D tensor `[batch_size, n_steps]`, if the number of step is dynamic, please use ``tl.cost.cross_entropy_seq_with_mask`` instead.\n    batch_size : None or int.\n        Whether to divide the cost by batch size.\n            - If integer, the return cost will be divided by `batch_size`.\n            - If None (default), the return cost will not be divided by anything.\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> # see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm.py>`__.for more details\n    >>> # outputs shape : (batch_size * n_steps, n_classes)\n    >>> # targets shape : (batch_size, n_steps)\n    >>> cost = tl.cost.cross_entropy_seq(outputs, targets)\n\n    """"""\n    sequence_loss_by_example_fn = sequence_loss_by_example\n\n    loss = sequence_loss_by_example_fn(\n        [logits], [tf.reshape(target_seqs, [-1])], [tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)]\n    )\n    # [tf.ones([batch_size * num_steps])])\n    cost = tf.reduce_sum(loss)  # / batch_size\n    if batch_size is not None:\n        cost = cost / batch_size\n    return cost\n\n\ndef cross_entropy_seq_with_mask(logits, target_seqs, input_mask, return_details=False, name=None):\n    """"""Returns the expression of cross-entropy of two sequences, implement\n    softmax internally. Normally be used for Dynamic RNN with Synced sequence input and output.\n\n    Parameters\n    -----------\n    logits : Tensor\n        2D tensor with shape of [batch_size * ?, n_classes], `?` means dynamic IDs for each example.\n        - Can be get from `DynamicRNNLayer` by setting ``return_seq_2d`` to `True`.\n    target_seqs : Tensor\n        int of tensor, like word ID. [batch_size, ?], `?` means dynamic IDs for each example.\n    input_mask : Tensor\n        The mask to compute loss, it has the same size with `target_seqs`, normally 0 or 1.\n    return_details : boolean\n        Whether to return detailed losses.\n            - If False (default), only returns the loss.\n            - If True, returns the loss, losses, weights and targets (see source code).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> import tensorflow as tf\n    >>> import numpy as np\n    >>> batch_size = 64\n    >>> vocab_size = 10000\n    >>> embedding_size = 256\n    >>> ni = tl.layers.Input([batch_size, None], dtype=tf.int64)\n    >>> net = tl.layers.Embedding(\n    ...         vocabulary_size = vocab_size,\n    ...         embedding_size = embedding_size,\n    ...         name = \'seq_embedding\')(ni)\n    >>> net = tl.layers.RNN(\n    ...         cell =tf.keras.layers.LSTMCell(units=embedding_size, dropout=0.1),\n    ...         return_seq_2d = True,\n    ...         name = \'dynamicrnn\')(net)\n    >>> net = tl.layers.Dense(n_units=vocab_size, name=""output"")(net)\n    >>> model = tl.models.Model(inputs=ni, outputs=net)\n    >>> input_seqs = np.random.randint(0, 10, size=(batch_size, 10), dtype=np.int64)\n    >>> target_seqs = np.random.randint(0, 10, size=(batch_size, 10), dtype=np.int64)\n    >>> input_mask = np.random.randint(0, 2, size=(batch_size, 10), dtype=np.int64)\n    >>> outputs = model(input_seqs, is_train=True)\n    >>> loss = tl.cost.cross_entropy_seq_with_mask(outputs, target_seqs, input_mask)\n\n    """"""\n    targets = tf.reshape(target_seqs, [-1])  # to one vector\n    weights = tf.cast(tf.reshape(input_mask, [-1]), dtype=tf.float32)  # to one vector like targets\n    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets, name=name) * weights\n    # losses = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets, name=name)) # for TF1.0 and others\n\n    loss = tf.divide(\n        tf.reduce_sum(losses),  # loss from mask. reduce_sum before element-wise mul with mask !!\n        tf.reduce_sum(weights),\n        name=""seq_loss_with_mask""\n    )\n\n    if return_details:\n        return loss, losses, weights, targets\n    else:\n        return loss\n\n\ndef cosine_similarity(v1, v2):\n    """"""Cosine similarity [-1, 1].\n\n    Parameters\n    ----------\n    v1, v2 : Tensor\n        Tensor with the same shape [batch_size, n_feature].\n\n    References\n    ----------\n    - `Wiki <https://en.wikipedia.org/wiki/Cosine_similarity>`__.\n\n    """"""\n\n    return tf.reduce_sum(tf.multiply(v1, v2), 1) / \\\n        (tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) *\n         tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)))\n\n\n# Regularization Functions\ndef li_regularizer(scale, scope=None):\n    """"""Li regularization removes the neurons of previous layer. The `i` represents `inputs`.\n    Returns a function that can be used to apply group li regularization to weights.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n    scope: str\n        An optional scope name for this function.\n\n    Returns\n    --------\n    A function with signature `li(weights, name=None)` that apply Li regularization.\n\n    Raises\n    ------\n    ValueError : if scale is outside of the range [0.0, 1.0] or if scale is not a float.\n\n    """"""\n    if isinstance(scale, numbers.Integral):\n        raise ValueError(\'scale cannot be an integer: %s\' % scale)\n    if isinstance(scale, numbers.Real):\n        if scale < 0.:\n            raise ValueError(\'Setting a scale less than 0 on a regularizer: %g\' % scale)\n        if scale >= 1.:\n            raise ValueError(\'Setting a scale greater than 1 on a regularizer: %g\' % scale)\n        if scale == 0.:\n            logging.info(\'Scale of 0 disables regularizer.\')\n            return lambda _, name=None: None\n\n    def li(weights):\n        """"""Applies li regularization to weights.""""""\n        with tf.name_scope(\'li_regularizer\') as scope:\n            my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name=\'scale\')\n            # if tf.__version__ <= \'0.12\':\n            #     standard_ops_fn = standard_ops.mul\n            # else:\n            standard_ops_fn = standard_ops.multiply\n            return standard_ops_fn(\n                my_scale, standard_ops.reduce_sum(standard_ops.sqrt(standard_ops.reduce_sum(tf.square(weights), 1))),\n                name=scope\n            )\n\n    return li\n\n\ndef lo_regularizer(scale):\n    """"""Lo regularization removes the neurons of current layer. The `o` represents `outputs`\n    Returns a function that can be used to apply group lo regularization to weights.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n\n    Returns\n    -------\n    A function with signature `lo(weights, name=None)` that apply Lo regularization.\n\n    Raises\n    ------\n    ValueError : If scale is outside of the range [0.0, 1.0] or if scale is not a float.\n\n    """"""\n    if isinstance(scale, numbers.Integral):\n        raise ValueError(\'scale cannot be an integer: %s\' % scale)\n\n    if isinstance(scale, numbers.Real):\n        if scale < 0.:\n            raise ValueError(\'Setting a scale less than 0 on a regularizer: %g\' % scale)\n        if scale >= 1.:\n            raise ValueError(\'Setting a scale greater than 1 on a regularizer: %g\' % scale)\n        if scale == 0.:\n            logging.info(\'Scale of 0 disables regularizer.\')\n            return lambda _, name=None: None\n\n    def lo(weights, name=\'lo_regularizer\'):\n        """"""Applies group column regularization to weights.""""""\n        with tf.name_scope(name) as scope:\n            my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name=\'scale\')\n            # if tf.__version__ <= \'0.12\':\n            #     standard_ops_fn = standard_ops.mul\n            # else:\n            standard_ops_fn = standard_ops.multiply\n            return standard_ops_fn(\n                my_scale, standard_ops.reduce_sum(standard_ops.sqrt(standard_ops.reduce_sum(tf.square(weights), 0))),\n                name=scope\n            )\n\n    return lo\n\n\ndef maxnorm_regularizer(scale=1.0):\n    """"""Max-norm regularization returns a function that can be used to apply max-norm regularization to weights.\n\n    More about max-norm, see `wiki-max norm <https://en.wikipedia.org/wiki/Matrix_norm#Max_norm>`_.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n\n    Returns\n    ---------\n    A function with signature `mn(weights, name=None)` that apply Lo regularization.\n\n    Raises\n    --------\n    ValueError : If scale is outside of the range [0.0, 1.0] or if scale is not a float.\n\n    """"""\n    if isinstance(scale, numbers.Integral):\n        raise ValueError(\'scale cannot be an integer: %s\' % scale)\n\n    if isinstance(scale, numbers.Real):\n        if scale < 0.:\n            raise ValueError(\'Setting a scale less than 0 on a regularizer: %g\' % scale)\n        # if scale >= 1.:\n        #   raise ValueError(\'Setting a scale greater than 1 on a regularizer: %g\' %\n        #                    scale)\n        if scale == 0.:\n            logging.info(\'Scale of 0 disables regularizer.\')\n            return lambda _, name=None: None\n\n    def mn(weights, name=\'max_regularizer\'):\n        """"""Applies max-norm regularization to weights.""""""\n        with tf.name_scope(name) as scope:\n            my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name=\'scale\')\n            #   if tf.__version__ <= \'0.12\':\n            #       standard_ops_fn = standard_ops.mul\n            #   else:\n            standard_ops_fn = standard_ops.multiply\n            return standard_ops_fn(my_scale, standard_ops.reduce_max(standard_ops.abs(weights)), name=scope)\n\n    return mn\n\n\ndef maxnorm_o_regularizer(scale):\n    """"""Max-norm output regularization removes the neurons of current layer.\n    Returns a function that can be used to apply max-norm regularization to each column of weight matrix.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n\n    Returns\n    ---------\n    A function with signature `mn_o(weights, name=None)` that apply Lo regularization.\n\n    Raises\n    ---------\n    ValueError : If scale is outside of the range [0.0, 1.0] or if scale is not a float.\n\n    """"""\n    if isinstance(scale, numbers.Integral):\n        raise ValueError(\'scale cannot be an integer: %s\' % scale)\n\n    if isinstance(scale, numbers.Real):\n        if scale < 0.:\n            raise ValueError(\'Setting a scale less than 0 on a regularizer: %g\' % scale)\n        # if scale >= 1.:\n        #   raise ValueError(\'Setting a scale greater than 1 on a regularizer: %g\' %\n        #                    scale)\n        if scale == 0.:\n            logging.info(\'Scale of 0 disables regularizer.\')\n            return lambda _, name=None: None\n\n    def mn_o(weights, name=\'maxnorm_o_regularizer\'):\n        """"""Applies max-norm regularization to weights.""""""\n        with tf.name_scope(name) as scope:\n            my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name=\'scale\')\n            if tf.__version__ <= \'0.12\':\n                standard_ops_fn = standard_ops.mul\n            else:\n                standard_ops_fn = standard_ops.multiply\n            return standard_ops_fn(\n                my_scale, standard_ops.reduce_sum(standard_ops.reduce_max(standard_ops.abs(weights), 0)), name=scope\n            )\n\n    return mn_o\n\n\ndef maxnorm_i_regularizer(scale):\n    """"""Max-norm input regularization removes the neurons of previous layer.\n    Returns a function that can be used to apply max-norm regularization to each row of weight matrix.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n\n    Returns\n    ---------\n    A function with signature `mn_i(weights, name=None)` that apply Lo regularization.\n\n    Raises\n    ---------\n    ValueError : If scale is outside of the range [0.0, 1.0] or if scale is not a float.\n\n    """"""\n    if isinstance(scale, numbers.Integral):\n        raise ValueError(\'scale cannot be an integer: %s\' % scale)\n\n    if isinstance(scale, numbers.Real):\n        if scale < 0.:\n            raise ValueError(\'Setting a scale less than 0 on a regularizer: %g\' % scale)\n        # if scale >= 1.:\n        #   raise ValueError(\'Setting a scale greater than 1 on a regularizer: %g\' %\n        #                    scale)\n        if scale == 0.:\n            logging.info(\'Scale of 0 disables regularizer.\')\n            return lambda _, name=None: None\n\n    def mn_i(weights, name=\'maxnorm_i_regularizer\'):\n        """"""Applies max-norm regularization to weights.""""""\n        with tf.name_scope(name) as scope:\n            my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name=\'scale\')\n            if tf.__version__ <= \'0.12\':\n                standard_ops_fn = standard_ops.mul\n            else:\n                standard_ops_fn = standard_ops.multiply\n            return standard_ops_fn(\n                my_scale, standard_ops.reduce_sum(standard_ops.reduce_max(standard_ops.abs(weights), 1)), name=scope\n            )\n\n    return mn_i\n\n\ndef huber_loss(\n    output, target, is_mean=True, delta=1.0, dynamichuber=False, reverse=False, axis=-1, epsilon=0.00001, name=None\n):\n    """"""Huber Loss operation, see ``https://en.wikipedia.org/wiki/Huber_loss`` .\n    Reverse Huber Loss operation, see  \'\'https://statweb.stanford.edu/~owen/reports/hhu.pdf\'\'.\n    Dynamic Reverse Huber Loss operation, see  \'\'https://arxiv.org/pdf/1606.00373.pdf\'\'.\n\n    Parameters\n    ----------\n    output : Tensor\n        A distribution with shape: [batch_size, ....], (any dimensions).\n    target : Tensor\n        The target distribution, format the same with `output`.\n    is_mean : boolean\n        Whether compute the mean or sum for each example.\n        - If True, use ``tf.reduce_mean`` to compute the loss between one target and predict data (default).\n        - If False, use ``tf.reduce_sum``.\n    delta: float\n        The point where the huber loss function changes from a quadratic to linear.\n    dynamichuber: boolean\n        Whether compute the coefficient c for each batch.\n        - If True, c is 20% of the maximal per-batch error.\n        - If False, c is delta.\n    reverse: boolean\n        Whether compute the reverse huber loss.\n    axis : int or list of int\n        The dimensions to reduce.\n    epsilon:\n        Eplison.\n    name : string\n        Name of this loss.\n\n    """"""\n    if reverse:\n        if dynamichuber:\n            huber_c = 0.2 * tf.reduce_max(tf.abs(output - target))\n        else:\n            huber_c = delta\n        if is_mean:\n            loss = tf.reduce_mean(\n                tf.where(\n                    tf.less_equal(tf.abs(output - target), huber_c), tf.abs(output - target),\n                    tf.multiply(\n                        tf.pow(output - target, 2.0) + tf.pow(huber_c, 2.0),\n                        tf.math.divide_no_nan(.5, huber_c + epsilon)\n                    )\n                ), name=name\n            )\n        else:\n            loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    tf.where(\n                        tf.less_equal(tf.abs(output - target), huber_c), tf.abs(output - target),\n                        tf.multiply(\n                            tf.pow(output - target, 2.0) + tf.pow(huber_c, 2.0),\n                            tf.math.divide_no_nan(.5, huber_c + epsilon)\n                        )\n                    ), axis\n                ), name=name\n            )\n    elif is_mean:\n        loss = tf.reduce_mean(\n            tf.where(\n                tf.less_equal(tf.abs(output - target), delta), 0.5 * tf.pow(output - target, 2),\n                delta * (tf.abs(output - target) - 0.5 * delta)\n            ), name=name\n        )\n    else:\n        loss = tf.reduce_mean(\n            tf.reduce_sum(\n                tf.where(\n                    tf.less_equal(tf.abs(output - target), delta), 0.5 * tf.pow(output - target, 2),\n                    delta * (tf.abs(output - target) - 0.5 * delta)\n                ), axis\n            ), name=name\n        )\n    return loss\n'"
tensorlayer/db.py,2,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nimport pickle\nimport sys\nimport time\nfrom datetime import datetime\n\nimport numpy as np\nimport tensorflow as tf\n\nimport gridfs\nimport pymongo\nfrom tensorlayer import logging\nfrom tensorlayer.files import (\n    assign_weights, del_folder, exists_or_mkdir, load_hdf5_to_weights, save_weights_to_hdf5, static_graph2net\n)\n\n\nclass TensorHub(object):\n    """"""It is a MongoDB based manager that help you to manage data, network architecture, parameters and logging.\n\n    Parameters\n    -------------\n    ip : str\n        Localhost or IP address.\n    port : int\n        Port number.\n    dbname : str\n        Database name.\n    username : str or None\n        User name, set to None if you do not need authentication.\n    password : str\n        Password.\n    project_name : str or None\n        Experiment key for this entire project, similar with the repository name of Github.\n\n    Attributes\n    ------------\n    ip, port, dbname and other input parameters : see above\n        See above.\n    project_name : str\n        The given project name, if no given, set to the script name.\n    db : mongodb client\n        See ``pymongo.MongoClient``.\n    """"""\n\n    # @deprecated_alias(db_name=\'dbname\', user_name=\'username\', end_support_version=2.1)\n    def __init__(\n        self, ip=\'localhost\', port=27017, dbname=\'dbname\', username=\'None\', password=\'password\', project_name=None\n    ):\n        self.ip = ip\n        self.port = port\n        self.dbname = dbname\n        self.username = username\n\n        print(""[Database] Initializing ..."")\n        # connect mongodb\n        client = pymongo.MongoClient(ip, port)\n        self.db = client[dbname]\n        if username is None:\n            print(username, password)\n            self.db.authenticate(username, password)\n        else:\n            print(""[Database] No username given, it works if authentication is not required"")\n        if project_name is None:\n            self.project_name = sys.argv[0].split(\'.\')[0]\n            print(""[Database] No project_name given, use {}"".format(self.project_name))\n        else:\n            self.project_name = project_name\n\n        # define file system (Buckets)\n        self.dataset_fs = gridfs.GridFS(self.db, collection=""datasetFilesystem"")\n        self.model_fs = gridfs.GridFS(self.db, collection=""modelfs"")\n        # self.params_fs = gridfs.GridFS(self.db, collection=""parametersFilesystem"")\n        # self.architecture_fs = gridfs.GridFS(self.db, collection=""architectureFilesystem"")\n\n        print(""[Database] Connected "")\n        _s = ""[Database] Info:\\n""\n        _s += ""  ip             : {}\\n"".format(self.ip)\n        _s += ""  port           : {}\\n"".format(self.port)\n        _s += ""  dbname         : {}\\n"".format(self.dbname)\n        _s += ""  username       : {}\\n"".format(self.username)\n        _s += ""  password       : {}\\n"".format(""*******"")\n        _s += ""  project_name : {}\\n"".format(self.project_name)\n        self._s = _s\n        print(self._s)\n\n    def __str__(self):\n        """"""Print information of databset.""""""\n        return self._s\n\n    def _fill_project_info(self, args):\n        """"""Fill in project_name for all studies, architectures and parameters.""""""\n        return args.update({\'project_name\': self.project_name})\n\n    @staticmethod\n    def _serialization(ps):\n        """"""Serialize data.""""""\n        return pickle.dumps(ps, protocol=pickle.HIGHEST_PROTOCOL)  # protocol=2)\n        # with open(\'_temp.pkl\', \'wb\') as file:\n        #     return pickle.dump(ps, file, protocol=pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def _deserialization(ps):\n        """"""Deseralize data.""""""\n        return pickle.loads(ps)\n\n    # =========================== MODELS ================================\n    def save_model(self, network=None, model_name=\'model\', **kwargs):\n        """"""Save model architecture and parameters into database, timestamp will be added automatically.\n\n        Parameters\n        ----------\n        network : TensorLayer Model\n            TensorLayer Model instance.\n        model_name : str\n            The name/key of model.\n        kwargs : other events\n            Other events, such as name, accuracy, loss, step number and etc (optinal).\n\n        Examples\n        ---------\n        Save model architecture and parameters into database.\n        >>> db.save_model(net, accuracy=0.8, loss=2.3, name=\'second_model\')\n\n        Load one model with parameters from database (run this in other script)\n        >>> net = db.find_top_model(accuracy=0.8, loss=2.3)\n\n        Find and load the latest model.\n        >>> net = db.find_top_model(sort=[(""time"", pymongo.DESCENDING)])\n        >>> net = db.find_top_model(sort=[(""time"", -1)])\n\n        Find and load the oldest model.\n        >>> net = db.find_top_model(sort=[(""time"", pymongo.ASCENDING)])\n        >>> net = db.find_top_model(sort=[(""time"", 1)])\n\n        Get model information\n        >>> net._accuracy\n        ... 0.8\n\n        Returns\n        ---------\n        boolean : True for success, False for fail.\n        """"""\n        kwargs.update({\'model_name\': model_name})\n        self._fill_project_info(kwargs)  # put project_name into kwargs\n\n        # params = network.get_all_params()\n        params = network.all_weights\n\n        s = time.time()\n\n        # kwargs.update({\'architecture\': network.all_graphs, \'time\': datetime.utcnow()})\n        kwargs.update({\'architecture\': network.config, \'time\': datetime.utcnow()})\n\n        try:\n            params_id = self.model_fs.put(self._serialization(params))\n            kwargs.update({\'params_id\': params_id, \'time\': datetime.utcnow()})\n            self.db.Model.insert_one(kwargs)\n            print(""[Database] Save model: SUCCESS, took: {}s"".format(round(time.time() - s, 2)))\n            return True\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(""{}  {}  {}  {}  {}"".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            print(""[Database] Save model: FAIL"")\n            return False\n\n    def find_top_model(self, sort=None, model_name=\'model\', **kwargs):\n        """"""Finds and returns a model architecture and its parameters from the database which matches the requirement.\n\n        Parameters\n        ----------\n        sort : List of tuple\n            PyMongo sort comment, search ""PyMongo find one sorting"" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        model_name : str or None\n            The name/key of model.\n        kwargs : other events\n            Other events, such as name, accuracy, loss, step number and etc (optinal).\n\n        Examples\n        ---------\n        - see ``save_model``.\n\n        Returns\n        ---------\n        network : TensorLayer Model\n            Note that, the returned network contains all information of the document (record), e.g. if you saved accuracy in the document, you can get the accuracy by using ``net._accuracy``.\n        """"""\n        # print(kwargs)   # {}\n        kwargs.update({\'model_name\': model_name})\n        self._fill_project_info(kwargs)\n\n        s = time.time()\n\n        d = self.db.Model.find_one(filter=kwargs, sort=sort)\n\n        # _temp_file_name = \'_find_one_model_ztemp_file\'\n        if d is not None:\n            params_id = d[\'params_id\']\n            graphs = d[\'architecture\']\n            _datetime = d[\'time\']\n            # exists_or_mkdir(_temp_file_name, False)\n            # with open(os.path.join(_temp_file_name, \'graph.pkl\'), \'wb\') as file:\n            #     pickle.dump(graphs, file, protocol=pickle.HIGHEST_PROTOCOL)\n        else:\n            print(""[Database] FAIL! Cannot find model: {}"".format(kwargs))\n            return False\n        try:\n            params = self._deserialization(self.model_fs.get(params_id).read())\n            # TODO : restore model and load weights\n            network = static_graph2net(graphs)\n            assign_weights(weights=params, network=network)\n            # np.savez(os.path.join(_temp_file_name, \'params.npz\'), params=params)\n            #\n            # network = load_graph_and_params(name=_temp_file_name, sess=sess)\n            # del_folder(_temp_file_name)\n\n            pc = self.db.Model.find(kwargs)\n            print(\n                ""[Database] Find one model SUCCESS. kwargs:{} sort:{} save time:{} took: {}s"".format(\n                    kwargs, sort, _datetime, round(time.time() - s, 2)\n                )\n            )\n\n            # FIXME : not sure what\'s this for\n            # put all informations of model into the TL layer\n            # for key in d:\n            #     network.__dict__.update({""_%s"" % key: d[key]})\n\n            # check whether more parameters match the requirement\n            params_id_list = pc.distinct(\'params_id\')\n            n_params = len(params_id_list)\n            if n_params != 1:\n                print(""     Note that there are {} models match the kwargs"".format(n_params))\n            return network\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(""{}  {}  {}  {}  {}"".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            return False\n\n    def delete_model(self, **kwargs):\n        """"""Delete model.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n        """"""\n        self._fill_project_info(kwargs)\n        self.db.Model.delete_many(kwargs)\n        logging.info(""[Database] Delete Model SUCCESS"")\n\n    # =========================== DATASET ===============================\n    def save_dataset(self, dataset=None, dataset_name=None, **kwargs):\n        """"""Saves one dataset into database, timestamp will be added automatically.\n\n        Parameters\n        ----------\n        dataset : any type\n            The dataset you want to store.\n        dataset_name : str\n            The name of dataset.\n        kwargs : other events\n            Other events, such as description, author and etc (optinal).\n\n        Examples\n        ----------\n        Save dataset\n        >>> db.save_dataset([X_train, y_train, X_test, y_test], \'mnist\', description=\'this is a tutorial\')\n\n        Get dataset\n        >>> dataset = db.find_top_dataset(\'mnist\')\n\n        Returns\n        ---------\n        boolean : Return True if save success, otherwise, return False.\n        """"""\n        self._fill_project_info(kwargs)\n        if dataset_name is None:\n            raise Exception(""dataset_name is None, please give a dataset name"")\n        kwargs.update({\'dataset_name\': dataset_name})\n\n        s = time.time()\n        try:\n            dataset_id = self.dataset_fs.put(self._serialization(dataset))\n            kwargs.update({\'dataset_id\': dataset_id, \'time\': datetime.utcnow()})\n            self.db.Dataset.insert_one(kwargs)\n            # print(""[Database] Save params: {} SUCCESS, took: {}s"".format(file_name, round(time.time()-s, 2)))\n            print(""[Database] Save dataset: SUCCESS, took: {}s"".format(round(time.time() - s, 2)))\n            return True\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(""{}  {}  {}  {}  {}"".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            print(""[Database] Save dataset: FAIL"")\n            return False\n\n    def find_top_dataset(self, dataset_name=None, sort=None, **kwargs):\n        """"""Finds and returns a dataset from the database which matches the requirement.\n\n        Parameters\n        ----------\n        dataset_name : str\n            The name of dataset.\n        sort : List of tuple\n            PyMongo sort comment, search ""PyMongo find one sorting"" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        kwargs : other events\n            Other events, such as description, author and etc (optinal).\n\n        Examples\n        ---------\n        Save dataset\n        >>> db.save_dataset([X_train, y_train, X_test, y_test], \'mnist\', description=\'this is a tutorial\')\n\n        Get dataset\n        >>> dataset = db.find_top_dataset(\'mnist\')\n        >>> datasets = db.find_datasets(\'mnist\')\n\n        Returns\n        --------\n        dataset : the dataset or False\n            Return False if nothing found.\n\n        """"""\n\n        self._fill_project_info(kwargs)\n        if dataset_name is None:\n            raise Exception(""dataset_name is None, please give a dataset name"")\n        kwargs.update({\'dataset_name\': dataset_name})\n\n        s = time.time()\n\n        d = self.db.Dataset.find_one(filter=kwargs, sort=sort)\n\n        if d is not None:\n            dataset_id = d[\'dataset_id\']\n        else:\n            print(""[Database] FAIL! Cannot find dataset: {}"".format(kwargs))\n            return False\n        try:\n            dataset = self._deserialization(self.dataset_fs.get(dataset_id).read())\n            pc = self.db.Dataset.find(kwargs)\n            print(""[Database] Find one dataset SUCCESS, {} took: {}s"".format(kwargs, round(time.time() - s, 2)))\n\n            # check whether more datasets match the requirement\n            dataset_id_list = pc.distinct(\'dataset_id\')\n            n_dataset = len(dataset_id_list)\n            if n_dataset != 1:\n                print(""     Note that there are {} datasets match the requirement"".format(n_dataset))\n            return dataset\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(""{}  {}  {}  {}  {}"".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            return False\n\n    def find_datasets(self, dataset_name=None, **kwargs):\n        """"""Finds and returns all datasets from the database which matches the requirement.\n        In some case, the data in a dataset can be stored separately for better management.\n\n        Parameters\n        ----------\n        dataset_name : str\n            The name/key of dataset.\n        kwargs : other events\n            Other events, such as description, author and etc (optional).\n\n        Returns\n        --------\n        params : the parameters, return False if nothing found.\n\n        """"""\n\n        self._fill_project_info(kwargs)\n        if dataset_name is None:\n            raise Exception(""dataset_name is None, please give a dataset name"")\n        kwargs.update({\'dataset_name\': dataset_name})\n\n        s = time.time()\n        pc = self.db.Dataset.find(kwargs)\n\n        if pc is not None:\n            dataset_id_list = pc.distinct(\'dataset_id\')\n            dataset_list = []\n            for dataset_id in dataset_id_list:  # you may have multiple Buckets files\n                tmp = self.dataset_fs.get(dataset_id).read()\n                dataset_list.append(self._deserialization(tmp))\n        else:\n            print(""[Database] FAIL! Cannot find any dataset: {}"".format(kwargs))\n            return False\n\n        print(""[Database] Find {} datasets SUCCESS, took: {}s"".format(len(dataset_list), round(time.time() - s, 2)))\n        return dataset_list\n\n    def delete_datasets(self, **kwargs):\n        """"""Delete datasets.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        """"""\n\n        self._fill_project_info(kwargs)\n        self.db.Dataset.delete_many(kwargs)\n        logging.info(""[Database] Delete Dataset SUCCESS"")\n\n    # =========================== LOGGING ===============================\n    def save_training_log(self, **kwargs):\n        """"""Saves the training log, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Events, such as accuracy, loss, step number and etc.\n\n        Examples\n        ---------\n        >>> db.save_training_log(accuracy=0.33, loss=0.98)\n\n        """"""\n\n        self._fill_project_info(kwargs)\n        kwargs.update({\'time\': datetime.utcnow()})\n        _result = self.db.TrainLog.insert_one(kwargs)\n        _log = self._print_dict(kwargs)\n        logging.info(""[Database] train log: "" + _log)\n\n    def save_validation_log(self, **kwargs):\n        """"""Saves the validation log, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Events, such as accuracy, loss, step number and etc.\n\n        Examples\n        ---------\n        >>> db.save_validation_log(accuracy=0.33, loss=0.98)\n\n        """"""\n\n        self._fill_project_info(kwargs)\n        kwargs.update({\'time\': datetime.utcnow()})\n        _result = self.db.ValidLog.insert_one(kwargs)\n        _log = self._print_dict(kwargs)\n        logging.info(""[Database] valid log: "" + _log)\n\n    def save_testing_log(self, **kwargs):\n        """"""Saves the testing log, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Events, such as accuracy, loss, step number and etc.\n\n        Examples\n        ---------\n        >>> db.save_testing_log(accuracy=0.33, loss=0.98)\n\n        """"""\n\n        self._fill_project_info(kwargs)\n        kwargs.update({\'time\': datetime.utcnow()})\n        _result = self.db.TestLog.insert_one(kwargs)\n        _log = self._print_dict(kwargs)\n        logging.info(""[Database] test log: "" + _log)\n\n    def delete_training_log(self, **kwargs):\n        """"""Deletes training log.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        Save training log\n        >>> db.save_training_log(accuracy=0.33)\n        >>> db.save_training_log(accuracy=0.44)\n\n        Delete logs that match the requirement\n        >>> db.delete_training_log(accuracy=0.33)\n\n        Delete all logs\n        >>> db.delete_training_log()\n        """"""\n        self._fill_project_info(kwargs)\n        self.db.TrainLog.delete_many(kwargs)\n        logging.info(""[Database] Delete TrainLog SUCCESS"")\n\n    def delete_validation_log(self, **kwargs):\n        """"""Deletes validation log.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        - see ``save_training_log``.\n        """"""\n        self._fill_project_info(kwargs)\n        self.db.ValidLog.delete_many(kwargs)\n        logging.info(""[Database] Delete ValidLog SUCCESS"")\n\n    def delete_testing_log(self, **kwargs):\n        """"""Deletes testing log.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        - see ``save_training_log``.\n        """"""\n        self._fill_project_info(kwargs)\n        self.db.TestLog.delete_many(kwargs)\n        logging.info(""[Database] Delete TestLog SUCCESS"")\n\n    # def find_training_logs(self, **kwargs):\n    #     pass\n    #\n    # def find_validation_logs(self, **kwargs):\n    #     pass\n    #\n    # def find_testing_logs(self, **kwargs):\n    #     pass\n\n    # =========================== Task ===================================\n    def create_task(self, task_name=None, script=None, hyper_parameters=None, saved_result_keys=None, **kwargs):\n        """"""Uploads a task to the database, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        script : str\n            File name of the python script.\n        hyper_parameters : dictionary\n            The hyper parameters pass into the script.\n        saved_result_keys : list of str\n            The keys of the task results to keep in the database when the task finishes.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        -----------\n        Uploads a task\n        >>> db.create_task(task_name=\'mnist\', script=\'example/tutorial_mnist_simple.py\', description=\'simple tutorial\')\n\n        Finds and runs the latest task\n        >>> db.run_top_task(sort=[(""time"", pymongo.DESCENDING)])\n        >>> db.run_top_task(sort=[(""time"", -1)])\n\n        Finds and runs the oldest task\n        >>> db.run_top_task(sort=[(""time"", pymongo.ASCENDING)])\n        >>> db.run_top_task(sort=[(""time"", 1)])\n\n        """"""\n        if not isinstance(task_name, str):  # is None:\n            raise Exception(""task_name should be string"")\n        if not isinstance(script, str):  # is None:\n            raise Exception(""script should be string"")\n        if hyper_parameters is None:\n            hyper_parameters = {}\n        if saved_result_keys is None:\n            saved_result_keys = []\n\n        self._fill_project_info(kwargs)\n        kwargs.update({\'time\': datetime.utcnow()})\n        kwargs.update({\'hyper_parameters\': hyper_parameters})\n        kwargs.update({\'saved_result_keys\': saved_result_keys})\n\n        _script = open(script, \'rb\').read()\n\n        kwargs.update({\'status\': \'pending\', \'script\': _script, \'result\': {}})\n        self.db.Task.insert_one(kwargs)\n        logging.info(""[Database] Saved Task - task_name: {} script: {}"".format(task_name, script))\n\n    def run_top_task(self, task_name=None, sort=None, **kwargs):\n        """"""Finds and runs a pending task that in the first of the sorting list.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        sort : List of tuple\n            PyMongo sort comment, search ""PyMongo find one sorting"" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        ---------\n        Monitors the database and pull tasks to run\n        >>> while True:\n        >>>     print(""waiting task from distributor"")\n        >>>     db.run_top_task(task_name=\'mnist\', sort=[(""time"", -1)])\n        >>>     time.sleep(1)\n\n        Returns\n        --------\n        boolean : True for success, False for fail.\n        """"""\n        if not isinstance(task_name, str):  # is None:\n            raise Exception(""task_name should be string"")\n        self._fill_project_info(kwargs)\n        kwargs.update({\'status\': \'pending\'})\n\n        # find task and set status to running\n        task = self.db.Task.find_one_and_update(kwargs, {\'$set\': {\'status\': \'running\'}}, sort=sort)\n\n        # try:\n        # get task info e.g. hyper parameters, python script\n        if task is None:\n            logging.info(""[Database] Find Task FAIL: key: {} sort: {}"".format(task_name, sort))\n            return False\n        else:\n            logging.info(""[Database] Find Task SUCCESS: key: {} sort: {}"".format(task_name, sort))\n        _datetime = task[\'time\']\n        _script = task[\'script\']\n        _id = task[\'_id\']\n        _hyper_parameters = task[\'hyper_parameters\']\n        _saved_result_keys = task[\'saved_result_keys\']\n        logging.info(""  hyper parameters:"")\n        for key in _hyper_parameters:\n            globals()[key] = _hyper_parameters[key]\n            logging.info(""    {}: {}"".format(key, _hyper_parameters[key]))\n        # run task\n        s = time.time()\n        logging.info(""[Database] Start Task: key: {} sort: {} push time: {}"".format(task_name, sort, _datetime))\n        _script = _script.decode(\'utf-8\')\n        with tf.Graph().as_default():  #  # as graph: # clear all TF graphs\n            exec(_script, globals())\n\n        # set status to finished\n        _ = self.db.Task.find_one_and_update({\'_id\': _id}, {\'$set\': {\'status\': \'finished\'}})\n\n        # return results\n        __result = {}\n        for _key in _saved_result_keys:\n            logging.info(""  result: {}={} {}"".format(_key, globals()[_key], type(globals()[_key])))\n            __result.update({""%s"" % _key: globals()[_key]})\n        _ = self.db.Task.find_one_and_update(\n            {\'_id\': _id}, {\'$set\': {\n                \'result\': __result\n            }}, return_document=pymongo.ReturnDocument.AFTER\n        )\n        logging.info(\n            ""[Database] Finished Task: task_name - {} sort: {} push time: {} took: {}s"".format(\n                task_name, sort, _datetime,\n                time.time() - s\n            )\n        )\n        return True\n        # except Exception as e:\n        #     exc_type, exc_obj, exc_tb = sys.exc_info()\n        #     fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n        #     logging.info(""{}  {}  {}  {}  {}"".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n        #     logging.info(""[Database] Fail to run task"")\n        #     # if fail, set status back to pending\n        #     _ = self.db.Task.find_one_and_update({\'_id\': _id}, {\'$set\': {\'status\': \'pending\'}})\n        #     return False\n\n    def delete_tasks(self, **kwargs):\n        """"""Delete tasks.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        >>> db.delete_tasks()\n\n        """"""\n\n        self._fill_project_info(kwargs)\n        self.db.Task.delete_many(kwargs)\n        logging.info(""[Database] Delete Task SUCCESS"")\n\n    def check_unfinished_task(self, task_name=None, **kwargs):\n        """"""Finds and runs a pending task.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        ---------\n        Wait until all tasks finish in user\'s local console\n\n        >>> while not db.check_unfinished_task():\n        >>>     time.sleep(1)\n        >>> print(""all tasks finished"")\n        >>> sess = tf.InteractiveSession()\n        >>> net = db.find_top_model(sess=sess, sort=[(""test_accuracy"", -1)])\n        >>> print(""the best accuracy {} is from model {}"".format(net._test_accuracy, net._name))\n\n        Returns\n        --------\n        boolean : True for success, False for fail.\n\n        """"""\n\n        if not isinstance(task_name, str):  # is None:\n            raise Exception(""task_name should be string"")\n        self._fill_project_info(kwargs)\n\n        kwargs.update({\'$or\': [{\'status\': \'pending\'}, {\'status\': \'running\'}]})\n\n        # ## find task\n        # task = self.db.Task.find_one(kwargs)\n        task = self.db.Task.find(kwargs)\n\n        task_id_list = task.distinct(\'_id\')\n        n_task = len(task_id_list)\n\n        if n_task == 0:\n            logging.info(""[Database] No unfinished task - task_name: {}"".format(task_name))\n            return False\n        else:\n\n            logging.info(""[Database] Find {} unfinished task - task_name: {}"".format(n_task, task_name))\n            return True\n\n    @staticmethod\n    def _print_dict(args):\n        string = \'\'\n        for key, value in args.items():\n            if key is not \'_id\':\n                string += str(key) + "": "" + str(value) + "" / ""\n        return string\n'"
tensorlayer/distributed.py,19,"b'# -*- coding: utf-8 -*-\n\nimport json\nimport os\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.python.training import session_run_hook\n\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated\nfrom tensorlayer.lazy_imports import LazyImport\n\nhvd = LazyImport(\'horovod.tensorflow\')\n\n__all__ = [\'TaskSpecDef\', \'TaskSpec\', \'DistributedSession\', \'StopAtTimeHook\', \'LoadCheckpoint\', \'Trainer\']\n\n\nclass Trainer(object):\n    """"""Trainer for neural networks in a distributed environment.\n\n    TensorLayer Trainer is a high-level training interface built on top of TensorFlow MonitoredSession and\n    `Horovod <https://github.com/uber/horovod>`__. It transparently scales the training of a TensorLayer model\n    from a single GPU to multiple GPUs that be placed on different machines in a single cluster.\n\n    To run the trainer, you will need to install Horovod on your machine. Check the installation script at\n    `tensorlayer/scripts/download_and_install_openmpi3_ubuntu.sh`\n\n    The minimal inputs to the Trainer include (1) a training dataset defined using the TensorFlow DataSet API,\n    and (2) a model build function given the inputs of the training dataset, and returns the neural network\n    to train, the loss function to minimize, and the names of the tensor to log during training, and (3)\n    an optimizer and its arguments.\n\n    The default parameter choices of Trainer is inspired by the Facebook paper:\n    `Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour <https://arxiv.org/abs/1706.02677>`__\n\n    Parameters\n    ----------\n    training_dataset : class TensorFlow ``DataSet``\n        The training dataset which zips samples and labels. The trainer automatically\n        shards the training dataset based on the number of GPUs.\n    build_training_func : function\n        A function that builds the training operator. It takes the training dataset as an input,\n        and returns the neural network, the loss function and a dictionary that maps\n        string tags to tensors to log during training.\n    optimizer : class TensorFlow ``Optimizer``\n        The loss function optimizer. The trainer automatically linearly scale the learning rate based on\n        the number of GPUs.\n    optimizer_args : dict\n        The optimizer argument dictionary. It must contain a `learning_rate` field in type of float.\n        Note that the learning rate is linearly scaled according to the number of GPU by default.\n        You can disable it using the option `scaling_learning_rate`\n    batch_size : int\n        The training mini-batch size (i.e., number of samples per batch).\n    prefetch_size: int or None\n        The dataset prefetch buffer size. Set this parameter to overlap the GPU training and data preparation\n        if the data preparation is heavy.\n    checkpoint_dir : None or str\n        The path to the TensorFlow model checkpoint. Note that only one trainer master would checkpoints its model.\n        If None, checkpoint is disabled.\n    log_step_size : int\n        The trainer logs training information every N mini-batches (i.e., step size).\n    validation_dataset: None or class TensorFlow ``DataSet``\n        The optional validation dataset that zips samples and labels. Note that\n        only the trainer master needs to the validation often.\n    build_validation_func: None or function\n        The function that builds the validation operator. It returns the validation neural network (which\n        share the weights of the training network) and a custom number of validation metrics.\n    scaling_learning_rate: Boolean\n        Linearly scale the learning rate by the number of GPUs. Default is True.\n        This `linear scaling rule` is generally effective and is highly recommended by the practioners.\n        Check `Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour <https://arxiv.org/abs/1706.02677>`__\n    max_iteration: int\n        The maximum iteration (i.e., mini-batch) to train.\n        The default is `math.inf`. You can set it to a small number to end the training earlier. This is\n        usually set for testing purpose.\n\n    Attributes\n    ----------\n    training_network : class TensorLayer ``Layer``\n        The training model.\n    session : class TensorFlow ``MonitoredTrainingSession``\n        The training session tha the Trainer wraps.\n    global_step : int\n        The number of training mini-batch by far.\n    validation_metrics : list of tuples\n        The validation metrics that zips the validation metric property and the average value.\n\n    Examples\n    --------\n    See `tutorial_mnist_distributed_trainer.py\n    <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_distributed_trainer.py>`__.\n\n    """"""\n\n    def __init__(\n        self, training_dataset, build_training_func, optimizer, optimizer_args, batch_size=32, prefetch_size=None,\n        checkpoint_dir=None, scaling_learning_rate=True, log_step_size=1, validation_dataset=None,\n        build_validation_func=None, max_iteration=float(\'inf\')\n    ):\n        # Initialize Horovod.\n        hvd.init()\n        self.is_master = hvd.rank() == 0\n        self._last_global_step = 0\n\n        if prefetch_size is None:\n            prefetch_size = batch_size\n\n        # Define the loss for validation dataset\n        if validation_dataset:\n            validation_dataset = validation_dataset.shard(num_shards=hvd.size(), index=hvd.rank()).batch(batch_size)\n            validation_dataset.prefetch(buffer_size=prefetch_size)\n            self._validation_iterator = validation_dataset.make_initializable_iterator()\n            next_example, next_label = self._validation_iterator.get_next()\n            _, self._validation_metrics = build_validation_func(next_example, next_label)\n            if not isinstance(self._validation_metrics, list):\n                self._validation_metrics = list(self._validation_metrics)\n        else:\n            self._validation_iterator = None\n            self._validation_metrics = None\n\n        # Get the shard of the dataset based on my local rank\n        training_dataset = training_dataset.shard(num_shards=hvd.size(), index=hvd.rank()).batch(batch_size)\n\n        training_dataset.prefetch(buffer_size=prefetch_size)\n        training_iterator = training_dataset.make_one_shot_iterator()\n        self._training_network, loss, log_tensors = build_training_func(*training_iterator.get_next())\n\n        # Adjust learning rate based on number of GPUs.\n        lr = optimizer_args[\'learning_rate\']\n        optimizer_args[\'learning_rate\'] = lr * hvd.size() if scaling_learning_rate else lr\n        opt = optimizer(**optimizer_args)\n\n        # Add Horovod Distributed Optimizer.\n        opt = hvd.DistributedOptimizer(opt)\n\n        self._global_step = tf.train.get_or_create_global_step()\n        if isinstance(log_tensors, list):\n            log_tensors.append(self._global_step)\n        else:\n            log_tensors[\'global_step\'] = self._global_step\n        self._train_op = opt.minimize(loss, global_step=self._global_step)\n\n        hooks = [\n            # Horovod: BroadcastGlobalVariablesHook broadcasts initial variable states\n            # from rank 0 to all other processes. This is necessary to ensure consistent\n            # initialization of all workers when training is started with random weights\n            # or restored from a checkpoint.\n            hvd.BroadcastGlobalVariablesHook(0),\n\n            # Horovod: adjust number of steps based on number of GPUs.\n            tf.train.StopAtStepHook(last_step=max_iteration // hvd.size()),\n            tf.train.LoggingTensorHook(tensors=log_tensors, every_n_iter=log_step_size),\n        ]\n\n        # Pin GPU to be used to process local rank (one GPU per process)\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n        # Save checkpoints only on worker 0 to prevent other workers from\n        # corrupting them.\n        checkpoint_dir = checkpoint_dir if self.is_master else None\n\n        # The MonitoredTrainingSession takes care of session initialization,\n        # restoring from a checkpoint, saving to a checkpoint, and closing when done\n        # or an error occurs.\n        self._sess = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, hooks=hooks, config=config)\n\n    @property\n    def global_step(self):\n        if self._sess.should_stop():\n            return self._last_global_step\n        self._last_global_step = self._sess.run(self._global_step)\n        return self._last_global_step\n\n    @property\n    def session(self):\n        return self._sess\n\n    @property\n    def training_network(self):\n        return self._training_network\n\n    @property\n    def validation_metrics(self):\n        """"""A helper function to compute validation related metrics""""""\n\n        if (self._validation_iterator is None) or (self._validation_metrics is None):\n            raise AttributeError(\'Validation is not setup.\')\n\n        n = 0.0\n        metric_sums = [0.0] * len(self._validation_metrics)\n        self._sess.run(self._validation_iterator.initializer)\n        while True:\n            try:\n                metrics = self._sess.run(self._validation_metrics)\n                for i, m in enumerate(metrics):\n                    metric_sums[i] += m\n                n += 1.0\n            except tf.errors.OutOfRangeError:\n                break\n        for i, m in enumerate(metric_sums):\n            metric_sums[i] = metric_sums[i] / n\n        return zip(self._validation_metrics, metric_sums)\n\n    def train_on_batch(self):\n        """"""Train a mini-batch.""""""\n        self._sess.run(self._train_op)\n\n    def train_and_validate_to_end(self, validate_step_size=50):\n        """"""A helper function that shows how to train and validate a model at the same time.\n\n        Parameters\n        ----------\n        validate_step_size : int\n            Validate the training network every N steps.\n\n        """"""\n        while not self._sess.should_stop():\n            self.train_on_batch()  # Run a training step synchronously.\n            if self.global_step % validate_step_size == 0:\n                # logging.info(""Average loss for validation dataset: %s"" % self.get_validation_metrics())\n                log_str = \'step: %d, \' % self.global_step\n                for n, m in self.validation_metrics:\n                    log_str += \'%s: %f, \' % (n.name, m)\n                logging.info(log_str)\n\n\n@deprecated(date=""2018-10-30"", instructions=""Using the TensorLayer distributed trainer."")\nclass TaskSpecDef(object):\n    """"""Specification for a distributed task.\n\n    It contains the job name, index of the task,\n    the parameter servers and the worker servers. If you want to use the last worker\n    for continuous evaluation you can call the method `use_last_worker_as_evaluator`\n    which returns a new :class:`TaskSpecDef` object without the last worker in the\n    cluster specification.\n\n    Parameters\n    ----------\n    task_type : str\n        Task type. One of `master`, `worker` or `ps`.\n    index : int\n        The zero-based index of the task. Distributed training jobs will have a single\n        master task, one or more parameter servers, and one or more workers.\n    trial : int\n        The identifier of the trial being run.\n    ps_hosts : str OR list of str\n        A string with a coma separate list of hosts for the parameter servers\n        or a list of hosts.\n    worker_hosts : str OR list of str\n        A string with a coma separate list of hosts for the worker servers\n        or a list of hosts.\n    master : str\n        A string with the master hosts\n\n    Notes\n    ----------\n    master might not be included in TF_CONFIG and can be None. The shard_index is adjusted\n    in any case to assign 0 to master and >= 1 to workers.\n    This implementation doesn\'t support sparse arrays in the `TF_CONFIG` variable as the\n    official TensorFlow documentation shows, as it is not a supported by the json\n    definition.\n\n    References\n    ----------\n    - `ML-engine trainer considerations <https://cloud.google.com/ml-engine/docs/trainer-considerations#use_tf_config>`__\n\n    """"""\n\n    def __init__(self, task_type=\'master\', index=0, trial=None, ps_hosts=None, worker_hosts=None, master=None):\n        self.type = task_type\n        self._index = int(index)\n        self._cluster_spec = None\n        self.num_workers = 1\n        self.num_ps = 0\n        self.shard_index = int(index)\n        self._master = True\n        self.trial = trial\n        self.ps_hosts = ps_hosts\n        self.worker_hosts = worker_hosts\n        self.master = master\n        self._server = None\n\n        if ps_hosts and worker_hosts:\n            self.ps_hosts = ps_hosts if isinstance(ps_hosts, list) else ps_hosts.split(\',\')\n            self.num_ps = len(self.ps_hosts)\n            self.worker_hosts = worker_hosts if isinstance(worker_hosts, list) else worker_hosts.split(\',\')\n            if master is not None and len(master) > 0:\n                self._cluster_spec = tf.train.ClusterSpec(\n                    {\n                        \'ps\': self.ps_hosts,\n                        \'worker\': self.worker_hosts,\n                        \'master\': master\n                    }\n                )\n                # master is a worker too\n                self.num_workers = len(self.worker_hosts) + 1\n                if self.type == \'worker\':\n                    self.shard_index = self._index + 1\n                self._master = self.type == \'master\'\n            else:\n                self._cluster_spec = tf.train.ClusterSpec({\'ps\': self.ps_hosts, \'worker\': self.worker_hosts})\n                self.num_workers = len(self.worker_hosts)\n                if self.type == \'worker\':\n                    self.shard_index = self._index\n                self._master = self.type == \'worker\' and self._index == 0\n\n    def is_ps(self):\n        """"""Returns true if this server is a parameter server""""""\n        return self.type == \'ps\'\n\n    def is_worker(self):\n        """"""Returns true if this server is a worker server""""""\n        return self.type == \'worker\'\n\n    def is_master(self):\n        """"""Returns true if this server is the master server""""""\n        return self._master\n\n    def is_evaluator(self):\n        """"""Returns true if this server is the evaluator server""""""\n        return self.type == \'worker\' and self.num_workers == self._index\n\n    def device_fn(self):\n        """"""Returns the function with the specification to create the graph in this server""""""\n        current_device = \'/job:{}/task:{}\'.format(self.type, self._index)\n        ps_devices = \'/job:ps\'\n        return tf.train.replica_device_setter(\n            ps_device=ps_devices, worker_device=current_device, cluster=self._cluster_spec\n        )\n\n    def create_server(self):\n        if self._server is None and self.ps_hosts and self.worker_hosts and not self.is_evaluator():\n            # create server and join if it is a parameter server\n            self._server = tf.train.Server(self._cluster_spec, job_name=self.type, task_index=self._index)\n            if self.is_ps():\n                self._server.join()\n\n    def target(self):\n        if self._server is None:\n            self.create_server()\n        if self._server is not None:\n            return self._server.target\n        else:\n            return None\n\n    def use_last_worker_as_evaluator(self):\n        """"""Returns a new :class:`TaskSpecDef` where the last worker has been removed from\n        the list of worker_hosts, so it is not used for training anymore. You can call\n        is_evaluator to know whether this server is the evaluator one or not.\n        In case there is only one server for training this method raises an exception, as\n        you cannot use any server for evaluation.\n\n        """"""\n        if self.num_workers <= 1:\n            raise Exception(\'You need more than one worker instance to use one as evaluator\')\n\n        return TaskSpecDef(\n            task_type=self.type, index=self._index, trial=self.trial, ps_hosts=self.ps_hosts,\n            worker_hosts=self.worker_hosts[:-1], master=self.master\n        )\n\n\n@deprecated(date=""2018-10-30"", instructions=""Using the TensorLayer distributed trainer."")\ndef create_task_spec_def():\n    """"""Returns the a :class:`TaskSpecDef` based on the environment variables for distributed training.\n\n    References\n    ----------\n    - `ML-engine trainer considerations <https://cloud.google.com/ml-engine/docs/trainer-considerations#use_tf_config>`__\n    - `TensorPort Distributed Computing <https://www.tensorport.com/documentation/code-details/>`__\n\n    """"""\n    if \'TF_CONFIG\' in os.environ:\n        # TF_CONFIG is used in ML-engine\n        env = json.loads(os.environ.get(\'TF_CONFIG\', \'{}\'))\n        task_data = env.get(\'task\', None) or {\'type\': \'master\', \'index\': 0}\n        cluster_data = env.get(\'cluster\', None) or {\'ps\': None, \'worker\': None, \'master\': None}\n        return TaskSpecDef(\n            task_type=task_data[\'type\'], index=task_data[\'index\'],\n            trial=task_data[\'trial\'] if \'trial\' in task_data else None, ps_hosts=cluster_data[\'ps\'],\n            worker_hosts=cluster_data[\'worker\'], master=cluster_data[\'master\'] if \'master\' in cluster_data else None\n        )\n    elif \'JOB_NAME\' in os.environ:\n        # JOB_NAME, TASK_INDEX, PS_HOSTS, WORKER_HOSTS and MASTER_HOST are used in TensorPort\n        return TaskSpecDef(\n            task_type=os.environ[\'JOB_NAME\'], index=os.environ[\'TASK_INDEX\'], ps_hosts=os.environ.get(\'PS_HOSTS\', None),\n            worker_hosts=os.environ.get(\'WORKER_HOSTS\', None), master=os.environ.get(\'MASTER_HOST\', None)\n        )\n    else:\n        raise Exception(\'You need to setup TF_CONFIG or JOB_NAME to define the task.\')\n\n\n@deprecated(date=""2018-10-30"", instructions=""Using the TensorLayer distributed trainer."")\ndef create_distributed_session(\n    task_spec=None, checkpoint_dir=None, scaffold=None, hooks=None, chief_only_hooks=None, save_checkpoint_secs=600,\n    save_summaries_steps=object(), save_summaries_secs=object(), config=None, stop_grace_period_secs=120,\n    log_step_count_steps=100\n):\n    """"""Creates a distributed session.\n\n    It calls `MonitoredTrainingSession` to create a :class:`MonitoredSession` for distributed training.\n\n    Parameters\n    ----------\n    task_spec : :class:`TaskSpecDef`.\n        The task spec definition from create_task_spec_def()\n    checkpoint_dir : str.\n        Optional path to a directory where to restore variables.\n    scaffold : ``Scaffold``\n        A `Scaffold` used for gathering or building supportive ops.\n        If not specified, a default one is created. It\'s used to finalize the graph.\n    hooks : list of ``SessionRunHook`` objects.\n        Optional\n    chief_only_hooks : list of ``SessionRunHook`` objects.\n        Activate these hooks if `is_chief==True`, ignore otherwise.\n    save_checkpoint_secs : int\n        The frequency, in seconds, that a checkpoint is saved\n        using a default checkpoint saver. If `save_checkpoint_secs` is set to\n        `None`, then the default checkpoint saver isn\'t used.\n    save_summaries_steps : int\n        The frequency, in number of global steps, that the\n        summaries are written to disk using a default summary saver. If both\n        `save_summaries_steps` and `save_summaries_secs` are set to `None`, then\n        the default summary saver isn\'t used. Default 100.\n    save_summaries_secs : int\n        The frequency, in secs, that the summaries are written\n        to disk using a default summary saver.  If both `save_summaries_steps` and\n        `save_summaries_secs` are set to `None`, then the default summary saver\n        isn\'t used. Default not enabled.\n    config : ``tf.ConfigProto``\n        an instance of `tf.ConfigProto` proto used to configure the session.\n        It\'s the `config` argument of constructor of `tf.Session`.\n    stop_grace_period_secs : int\n        Number of seconds given to threads to stop after\n        `close()` has been called.\n    log_step_count_steps : int\n        The frequency, in number of global steps, that the\n        global step/sec is logged.\n\n    Examples\n    --------\n    A simple example for distributed training where all the workers use the same dataset:\n\n    >>> task_spec = TaskSpec()\n    >>> with tf.device(task_spec.device_fn()):\n    >>>      tensors = create_graph()\n    >>> with tl.DistributedSession(task_spec=task_spec,\n    ...                            checkpoint_dir=\'/tmp/ckpt\') as session:\n    >>>      while not session.should_stop():\n    >>>           session.run(tensors)\n\n    An example where the dataset is shared among the workers\n    (see https://www.tensorflow.org/programmers_guide/datasets):\n\n    >>> task_spec = TaskSpec()\n    >>> # dataset is a :class:`tf.data.Dataset` with the raw data\n    >>> dataset = create_dataset()\n    >>> if task_spec is not None:\n    >>>     dataset = dataset.shard(task_spec.num_workers, task_spec.shard_index)\n    >>> # shuffle or apply a map function to the new sharded dataset, for example:\n    >>> dataset = dataset.shuffle(buffer_size=10000)\n    >>> dataset = dataset.batch(batch_size)\n    >>> dataset = dataset.repeat(num_epochs)\n    >>> # create the iterator for the dataset and the input tensor\n    >>> iterator = dataset.make_one_shot_iterator()\n    >>> next_element = iterator.get_next()\n    >>> with tf.device(task_spec.device_fn()):\n    >>>      # next_element is the input for the graph\n    >>>      tensors = create_graph(next_element)\n    >>> with tl.DistributedSession(task_spec=task_spec,\n    ...                            checkpoint_dir=\'/tmp/ckpt\') as session:\n    >>>      while not session.should_stop():\n    >>>           session.run(tensors)\n\n    References\n    ----------\n    - `MonitoredTrainingSession <https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession>`__\n\n    """"""\n    target = task_spec.target() if task_spec is not None else None\n    is_chief = task_spec.is_master() if task_spec is not None else True\n    return tf.train.MonitoredTrainingSession(\n        master=target, is_chief=is_chief, checkpoint_dir=checkpoint_dir, scaffold=scaffold,\n        save_checkpoint_secs=save_checkpoint_secs, save_summaries_steps=save_summaries_steps,\n        save_summaries_secs=save_summaries_secs, log_step_count_steps=log_step_count_steps,\n        stop_grace_period_secs=stop_grace_period_secs, config=config, hooks=hooks, chief_only_hooks=chief_only_hooks\n    )\n\n\n@deprecated(date=""2018-10-30"", instructions=""Using the TensorLayer distributed trainer."")\nclass StopAtTimeHook(session_run_hook.SessionRunHook):\n    """"""Hook that requests stop after a specified time.\n\n    Parameters\n    ----------\n    time_running: int\n        Maximum time running in seconds\n\n    """"""\n\n    def __init__(self, time_running):\n        self._time_running = time_running\n        self._end_time = 0\n\n    def begin(self):\n        self._end_time = time.time() + self._time_running\n\n    def after_run(self, run_context, run_values):\n        if time.time() > self._end_time:\n            run_context.request_stop()\n\n\n@deprecated(date=""2018-10-30"", instructions=""Using the TensorLayer distributed trainer."")\nclass LoadCheckpoint(session_run_hook.SessionRunHook):\n    """"""Hook that loads a checkpoint after the session is created.\n\n    >>> from tensorflow.python.ops import variables as tf_variables\n    >>> from tensorflow.python.training.monitored_session import SingularMonitoredSession\n    >>>\n    >>> tensors = create_graph()\n    >>> saver = tf.train.Saver(var_list=tf_variables.trainable_variables())\n    >>> checkpoint_hook = LoadCheckpoint(saver, my_checkpoint_file)\n    >>> with tf.SingularMonitoredSession(hooks=[checkpoint_hook]) as session:\n    >>>      while not session.should_stop():\n    >>>           session.run(tensors)\n\n    """"""\n\n    def __init__(self, saver, checkpoint):\n        self._saver = saver\n        self._checkpoint = checkpoint\n        self._loaded = False\n\n    def after_create_session(self, session, coord):\n        if not self._loaded:\n            self._loaded = True\n            self._saver.restore(self._checkpoint)\n\n\n# Alias\nTaskSpec = create_task_spec_def\nDistributedSession = create_distributed_session\n'"
tensorlayer/initializers.py,14,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\n__all__ = [\n    \'Initializer\', \'Zeros\', \'Ones\', \'Constant\', \'RandomUniform\', \'RandomNormal\', \'TruncatedNormal\',\n    \'deconv2d_bilinear_upsampling_initializer\'\n]\n\n\nclass Initializer(object):\n    """"""Initializer base class: all initializers inherit from this class.\n    """"""\n\n    def __call__(self, shape, dtype=None):\n        """"""Returns a tensor object initialized as specified by the initializer.\n\n        Parameters\n        ----------\n        shape : tuple of int.\n            The shape of the tensor.\n        dtype : Optional dtype of the tensor.\n            If not provided will return tensor of `tf.float32`.\n\n        Returns\n        -------\n\n        """"""\n        raise NotImplementedError\n\n    def get_config(self):\n        """"""Returns the configuration of the initializer as a JSON-serializable dict.\n\n        Returns\n        -------\n            A JSON-serializable Python dict.\n        """"""\n        return {}\n\n    @classmethod\n    def from_config(cls, config):\n        """"""Instantiates an initializer from a configuration dictionary.\n\n        Parameters\n        ----------\n        config : A python dictionary.\n            It will typically be the output of `get_config`.\n\n        Returns\n        -------\n            An Initializer instance.\n        """"""\n        if \'dtype\' in config:\n            config.pop(\'dtype\')\n        return cls(**config)\n\n\nclass Zeros(Initializer):\n    """"""Initializer that generates tensors initialized to 0.\n    """"""\n\n    def __call__(self, shape, dtype=tf.float32):\n        return tf.zeros(shape, dtype=dtype)\n\n\nclass Ones(Initializer):\n    """"""Initializer that generates tensors initialized to 1.\n    """"""\n\n    def __call__(self, shape, dtype=tf.float32):\n        return tf.ones(shape, dtype=dtype)\n\n\nclass Constant(Initializer):\n    """"""Initializer that generates tensors initialized to a constant value.\n\n    Parameters\n    ----------\n    value : A python scalar or a numpy array.\n        The assigned value.\n\n    """"""\n\n    def __init__(self, value=0):\n        self.value = value\n\n    def __call__(self, shape, dtype=None):\n        return tf.constant(self.value, shape=shape, dtype=dtype)\n\n    def get_config(self):\n        return {""value"": self.value}\n\n\nclass RandomUniform(Initializer):\n    """"""Initializer that generates tensors with a uniform distribution.\n\n    Parameters\n    ----------\n    minval : A python scalar or a scalar tensor.\n        Lower bound of the range of random values to generate.\n    maxval : A python scalar or a scalar tensor.\n        Upper bound of the range of random values to generate.\n    seed : A Python integer.\n        Used to seed the random generator.\n\n    """"""\n\n    def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n        self.minval = minval\n        self.maxval = maxval\n        self.seed = seed\n\n    def __call__(self, shape, dtype=tf.float32):\n        return tf.random.uniform(shape, self.minval, self.maxval, dtype=dtype, seed=self.seed)\n\n    def get_config(self):\n        return {""minval"": self.minval, ""maxval"": self.maxval, ""seed"": self.seed}\n\n\nclass RandomNormal(Initializer):\n    """"""Initializer that generates tensors with a normal distribution.\n\n    Parameters\n    ----------\n    mean : A python scalar or a scalar tensor.\n        Mean of the random values to generate.\n    stddev : A python scalar or a scalar tensor.\n        Standard deviation of the random values to generate.\n    seed : A Python integer.\n        Used to seed the random generator.\n    """"""\n\n    def __init__(self, mean=0.0, stddev=0.05, seed=None):\n        self.mean = mean\n        self.stddev = stddev\n        self.seed = seed\n\n    def __call__(self, shape, dtype=tf.float32):\n        return tf.random.normal(shape, self.mean, self.stddev, dtype=dtype, seed=self.seed)\n\n    def get_config(self):\n        return {""mean"": self.mean, ""stddev"": self.stddev, ""seed"": self.seed}\n\n\nclass TruncatedNormal(Initializer):\n    """"""Initializer that generates a truncated normal distribution.\n\n    These values are similar to values from a `RandomNormal`\n    except that values more than two standard deviations from the mean\n    are discarded and re-drawn. This is the recommended initializer for\n    neural network weights and filters.\n\n\n    Parameters\n    ----------\n    mean : A python scalar or a scalar tensor.\n        Mean of the random values to generate.\n    stddev : A python scalar or a scalar tensor.\n        Standard deviation of the andom values to generate.\n    seed : A Python integer.\n        Used to seed the random generator.\n    """"""\n\n    def __init__(self, mean=0.0, stddev=0.05, seed=None):\n        self.mean = mean\n        self.stddev = stddev\n        self.seed = seed\n\n    def __call__(self, shape, dtype=tf.float32):\n        return tf.random.truncated_normal(shape, self.mean, self.stddev, dtype=dtype, seed=self.seed)\n\n    def get_config(self):\n        return {""mean"": self.mean, ""stddev"": self.stddev, ""seed"": self.seed}\n\n\ndef deconv2d_bilinear_upsampling_initializer(shape):\n    """"""Returns the initializer that can be passed to DeConv2dLayer for initializing the\n    weights in correspondence to channel-wise bilinear up-sampling.\n    Used in segmentation approaches such as [FCN](https://arxiv.org/abs/1605.06211)\n\n    Parameters\n    ----------\n    shape : tuple of int\n        The shape of the filters, [height, width, output_channels, in_channels].\n        It must match the shape passed to DeConv2dLayer.\n\n    Returns\n    -------\n    ``tf.constant_initializer``\n        A constant initializer with weights set to correspond to per channel bilinear upsampling\n        when passed as W_int in DeConv2dLayer\n\n    """"""\n    if shape[0] != shape[1]:\n        raise Exception(\'deconv2d_bilinear_upsampling_initializer only supports symmetrical filter sizes\')\n\n    if shape[3] < shape[2]:\n        raise Exception(\n            \'deconv2d_bilinear_upsampling_initializer behaviour is not defined for num_in_channels < num_out_channels \'\n        )\n\n    filter_size = shape[0]\n    num_out_channels = shape[2]\n    num_in_channels = shape[3]\n\n    # Create bilinear filter kernel as numpy array\n    bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)\n    scale_factor = (filter_size + 1) // 2\n    if filter_size % 2 == 1:\n        center = scale_factor - 1\n    else:\n        center = scale_factor - 0.5\n    for x in range(filter_size):\n        for y in range(filter_size):\n            bilinear_kernel[x, y] = (1 - abs(x - center) / scale_factor) * (1 - abs(y - center) / scale_factor)\n    weights = np.zeros((filter_size, filter_size, num_out_channels, num_in_channels), dtype=np.float32)\n    for i in range(num_out_channels):\n        weights[:, :, i, i] = bilinear_kernel\n\n    # assign numpy array to constant_initalizer and pass to get_variable\n    return tf.constant_initializer(value=weights)\n\n\n# Alias\nzeros = Zeros\nones = Ones\nconstant = Constant\nrandom_uniform = RandomUniform\nrandom_normal = RandomNormal\ntruncated_normal = TruncatedNormal\n'"
tensorlayer/iterate.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nfrom six.moves import xrange\n\n__all__ = [\n    \'minibatches\',\n    \'seq_minibatches\',\n    \'seq_minibatches2\',\n    \'ptb_iterator\',\n]\n\n\ndef minibatches(inputs=None, targets=None, batch_size=None, allow_dynamic_batch_size=False, shuffle=False):\n    """"""Generate a generator that input a group of example in numpy.array and\n    their labels, return the examples and labels by the given batch size.\n\n    Parameters\n    ----------\n    inputs : numpy.array\n        The input features, every row is a example.\n    targets : numpy.array\n        The labels of inputs, every row is a example.\n    batch_size : int\n        The batch size.\n    allow_dynamic_batch_size: boolean\n        Allow the use of the last data batch in case the number of examples is not a multiple of batch_size, this may result in unexpected behaviour if other functions expect a fixed-sized batch-size.\n    shuffle : boolean\n        Indicating whether to use a shuffling queue, shuffle the dataset before return.\n\n    Examples\n    --------\n    >>> X = np.asarray([[\'a\',\'a\'], [\'b\',\'b\'], [\'c\',\'c\'], [\'d\',\'d\'], [\'e\',\'e\'], [\'f\',\'f\']])\n    >>> y = np.asarray([0,1,2,3,4,5])\n    >>> for batch in tl.iterate.minibatches(inputs=X, targets=y, batch_size=2, shuffle=False):\n    >>>     print(batch)\n    ... (array([[\'a\', \'a\'], [\'b\', \'b\']], dtype=\'<U1\'), array([0, 1]))\n    ... (array([[\'c\', \'c\'], [\'d\', \'d\']], dtype=\'<U1\'), array([2, 3]))\n    ... (array([[\'e\', \'e\'], [\'f\', \'f\']], dtype=\'<U1\'), array([4, 5]))\n\n    Notes\n    -----\n    If you have two inputs and one label and want to shuffle them together, e.g. X1 (1000, 100), X2 (1000, 80) and Y (1000, 1), you can stack them together (`np.hstack((X1, X2))`)\n    into (1000, 180) and feed to ``inputs``. After getting a batch, you can split it back into X1 and X2.\n\n    """"""\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n\n    if shuffle:\n        indices = np.arange(len(inputs))\n        np.random.shuffle(indices)\n\n    # for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n    # chulei: handling the case where the number of samples is not a multiple of batch_size, avoiding wasting samples\n    for start_idx in range(0, len(inputs), batch_size):\n        end_idx = start_idx + batch_size\n        if end_idx > len(inputs):\n            if allow_dynamic_batch_size:\n                end_idx = len(inputs)\n            else:\n                break\n        if shuffle:\n            excerpt = indices[start_idx:end_idx]\n        else:\n            excerpt = slice(start_idx, end_idx)\n        if (isinstance(inputs, list) or isinstance(targets, list)) and (shuffle ==True):\n            # zsdonghao: for list indexing when shuffle==True\n            yield [inputs[i] for i in excerpt], [targets[i] for i in excerpt]\n        else:\n            yield inputs[excerpt], targets[excerpt]\n\n\ndef seq_minibatches(inputs, targets, batch_size, seq_length, stride=1):\n    """"""Generate a generator that return a batch of sequence inputs and targets.\n    If `batch_size=100` and `seq_length=5`, one return will have 500 rows (examples).\n\n    Parameters\n    ----------\n    inputs : numpy.array\n        The input features, every row is a example.\n    targets : numpy.array\n        The labels of inputs, every element is a example.\n    batch_size : int\n        The batch size.\n    seq_length : int\n        The sequence length.\n    stride : int\n        The stride step, default is 1.\n\n    Examples\n    --------\n    Synced sequence input and output.\n\n    >>> X = np.asarray([[\'a\',\'a\'], [\'b\',\'b\'], [\'c\',\'c\'], [\'d\',\'d\'], [\'e\',\'e\'], [\'f\',\'f\']])\n    >>> y = np.asarray([0, 1, 2, 3, 4, 5])\n    >>> for batch in tl.iterate.seq_minibatches(inputs=X, targets=y, batch_size=2, seq_length=2, stride=1):\n    >>>     print(batch)\n    ... (array([[\'a\', \'a\'], [\'b\', \'b\'], [\'b\', \'b\'], [\'c\', \'c\']], dtype=\'<U1\'), array([0, 1, 1, 2]))\n    ... (array([[\'c\', \'c\'], [\'d\', \'d\'], [\'d\', \'d\'], [\'e\', \'e\']], dtype=\'<U1\'), array([2, 3, 3, 4]))\n\n    Many to One\n\n    >>> return_last = True\n    >>> num_steps = 2\n    >>> X = np.asarray([[\'a\',\'a\'], [\'b\',\'b\'], [\'c\',\'c\'], [\'d\',\'d\'], [\'e\',\'e\'], [\'f\',\'f\']])\n    >>> Y = np.asarray([0,1,2,3,4,5])\n    >>> for batch in tl.iterate.seq_minibatches(inputs=X, targets=Y, batch_size=2, seq_length=num_steps, stride=1):\n    >>>     x, y = batch\n    >>>     if return_last:\n    >>>         tmp_y = y.reshape((-1, num_steps) + y.shape[1:])\n    >>>     y = tmp_y[:, -1]\n    >>>     print(x, y)\n    ... [[\'a\' \'a\']\n    ...  [\'b\' \'b\']\n    ...  [\'b\' \'b\']\n    ...  [\'c\' \'c\']] [1 2]\n    ... [[\'c\' \'c\']\n    ...  [\'d\' \'d\']\n    ...  [\'d\' \'d\']\n    ...  [\'e\' \'e\']] [3 4]\n\n    """"""\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n\n    n_loads = (batch_size * stride) + (seq_length - stride)\n\n    for start_idx in range(0, len(inputs) - n_loads + 1, (batch_size * stride)):\n        seq_inputs = np.zeros((batch_size, seq_length) + inputs.shape[1:], dtype=inputs.dtype)\n        seq_targets = np.zeros((batch_size, seq_length) + targets.shape[1:], dtype=targets.dtype)\n        for b_idx in xrange(batch_size):\n            start_seq_idx = start_idx + (b_idx * stride)\n            end_seq_idx = start_seq_idx + seq_length\n            seq_inputs[b_idx] = inputs[start_seq_idx:end_seq_idx]\n            seq_targets[b_idx] = targets[start_seq_idx:end_seq_idx]\n        flatten_inputs = seq_inputs.reshape((-1, ) + inputs.shape[1:])\n        flatten_targets = seq_targets.reshape((-1, ) + targets.shape[1:])\n        yield flatten_inputs, flatten_targets\n\n\ndef seq_minibatches2(inputs, targets, batch_size, num_steps):\n    """"""Generate a generator that iterates on two list of words. Yields (Returns) the source contexts and\n    the target context by the given batch_size and num_steps (sequence_length).\n    In TensorFlow\'s tutorial, this generates the `batch_size` pointers into the raw PTB data, and allows minibatch iteration along these pointers.\n\n    Parameters\n    ----------\n    inputs : list of data\n        The context in list format; note that context usually be represented by splitting by space, and then convert to unique word IDs.\n    targets : list of data\n        The context in list format; note that context usually be represented by splitting by space, and then convert to unique word IDs.\n    batch_size : int\n        The batch size.\n    num_steps : int\n        The number of unrolls. i.e. sequence length\n\n    Yields\n    ------\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n\n    Raises\n    ------\n    ValueError : if batch_size or num_steps are too high.\n\n    Examples\n    --------\n    >>> X = [i for i in range(20)]\n    >>> Y = [i for i in range(20,40)]\n    >>> for batch in tl.iterate.seq_minibatches2(X, Y, batch_size=2, num_steps=3):\n    ...     x, y = batch\n    ...     print(x, y)\n    ...\n    ... [[  0.   1.   2.]\n    ...  [ 10.  11.  12.]]\n    ... [[ 20.  21.  22.]\n    ...  [ 30.  31.  32.]]\n    ...\n    ... [[  3.   4.   5.]\n    ...  [ 13.  14.  15.]]\n    ... [[ 23.  24.  25.]\n    ...  [ 33.  34.  35.]]\n    ...\n    ... [[  6.   7.   8.]\n    ...  [ 16.  17.  18.]]\n    ... [[ 26.  27.  28.]\n    ...  [ 36.  37.  38.]]\n\n    Notes\n    -----\n    - Hint, if the input data are images, you can modify the source code `data = np.zeros([batch_size, batch_len)` to `data = np.zeros([batch_size, batch_len, inputs.shape[1], inputs.shape[2], inputs.shape[3]])`.\n    """"""\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n\n    data_len = len(inputs)\n    batch_len = data_len // batch_size\n    # data = np.zeros([batch_size, batch_len])\n    data = np.zeros((batch_size, batch_len) + inputs.shape[1:], dtype=inputs.dtype)\n    data2 = np.zeros([batch_size, batch_len])\n\n    for i in range(batch_size):\n        data[i] = inputs[batch_len * i:batch_len * (i + 1)]\n        data2[i] = targets[batch_len * i:batch_len * (i + 1)]\n\n    epoch_size = (batch_len - 1) // num_steps\n\n    if epoch_size == 0:\n        raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n    for i in range(epoch_size):\n        x = data[:, i * num_steps:(i + 1) * num_steps]\n        x2 = data2[:, i * num_steps:(i + 1) * num_steps]\n        yield (x, x2)\n\n\ndef ptb_iterator(raw_data, batch_size, num_steps):\n    """"""Generate a generator that iterates on a list of words, see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm.py>`__.\n    Yields the source contexts and the target context by the given batch_size and num_steps (sequence_length).\n\n    In TensorFlow\'s tutorial, this generates `batch_size` pointers into the raw\n    PTB data, and allows minibatch iteration along these pointers.\n\n    Parameters\n    ----------\n    raw_data : a list\n            the context in list format; note that context usually be\n            represented by splitting by space, and then convert to unique\n            word IDs.\n    batch_size : int\n            the batch size.\n    num_steps : int\n            the number of unrolls. i.e. sequence_length\n\n    Yields\n    ------\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n    The second element of the tuple is the same data time-shifted to the\n    right by one.\n\n    Raises\n    ------\n    ValueError : if batch_size or num_steps are too high.\n\n    Examples\n    --------\n    >>> train_data = [i for i in range(20)]\n    >>> for batch in tl.iterate.ptb_iterator(train_data, batch_size=2, num_steps=3):\n    >>>     x, y = batch\n    >>>     print(x, y)\n    ... [[ 0  1  2] <---x                       1st subset/ iteration\n    ... [10 11 12]]\n    ... [[ 1  2  3] <---y\n    ... [11 12 13]]\n    ...\n    ... [[ 3  4  5]  <--- 1st batch input       2nd subset/ iteration\n    ... [13 14 15]] <--- 2nd batch input\n    ... [[ 4  5  6]  <--- 1st batch target\n    ... [14 15 16]] <--- 2nd batch target\n    ...\n    ... [[ 6  7  8]                             3rd subset/ iteration\n    ... [16 17 18]]\n    ... [[ 7  8  9]\n    ... [17 18 19]]\n    """"""\n    raw_data = np.array(raw_data, dtype=np.int32)\n\n    data_len = len(raw_data)\n    batch_len = data_len // batch_size\n    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n    for i in range(batch_size):\n        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n\n    epoch_size = (batch_len - 1) // num_steps\n\n    if epoch_size == 0:\n        raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n    for i in range(epoch_size):\n        x = data[:, i * num_steps:(i + 1) * num_steps]\n        y = data[:, i * num_steps + 1:(i + 1) * num_steps + 1]\n        yield (x, y)\n'"
tensorlayer/lazy_imports.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""This module provides lazy import functionality to improve the import\nperformance of nitime. For example, some parts of nitime leverage and import\nmatplotlib, which is quite a big package, yet most of the nitime code does not\ndepend on matplotlib. By lazily-loading a module, we defer the overhead of\nimporting it until the first time it is actually used, thereby speeding up\nnitime imports.\n\nA generic :class:`LazyImport` class is implemented which takes the module name\nas a parameter, and acts as a proxy for that module, importing it only when\nthe module is used, but effectively acting as the module in every other way\n(including inside IPython with respect to introspection and tab completion)\nwith the *exception* of reload() - reloading a :class:`LazyImport` raises an\n:class:`ImportError`.\n\nCommonly used nitime lazy imports are also defined in :mod:`nitime.lazy`, so\nthey can be reused throughout nitime.\n""""""\nimport os\nimport sys\nimport types\n\n\nclass LazyImport(types.ModuleType):\n    """"""\n    This class takes the module name as a parameter, and acts as a proxy for\n    that module, importing it only when the module is used, but effectively\n    acting as the module in every other way (including inside IPython with\n    respect to introspection and tab completion) with the *exception* of\n    reload()- reloading a :class:`LazyImport` raises an :class:`ImportError`.\n\n    >>> mlab = LazyImport(\'matplotlib.mlab\')\n\n    No import happens on the above line, until we do something like call an\n    ``mlab`` method or try to do tab completion or introspection on ``mlab``\n    in IPython.\n\n    >>> mlab\n    <module \'matplotlib.mlab\' will be lazily loaded>\n\n    Now the :class:`LazyImport` will do an actual import, and call the dist\n    function of the imported module.\n\n    >>> mlab.dist(1969,2011)\n    42.0\n    """"""\n\n    def __getattribute__(self, x):\n        # This method will be called only once, since we\'ll change\n        # self.__class__ to LoadedLazyImport, and __getattribute__ will point\n        # to module.__getattribute__\n\n        name = object.__getattribute__(self, \'__name__\')\n        __import__(name)\n\n        # if name above is \'package.foo.bar\', package is returned, the docs\n        # recommend that in order to get back the full thing, that we import\n        # and then lookup the full name is sys.modules, see:\n        # http://docs.python.org/library/functions.html#__import__\n\n        module = sys.modules[name]\n\n        # Now that we\'ve done the import, cutout the middleman and make self\n        # act as the imported module\n\n        class LoadedLazyImport(types.ModuleType):\n            __getattribute__ = module.__getattribute__\n            __repr__ = module.__repr__\n\n        object.__setattr__(self, \'__class__\', LoadedLazyImport)\n\n        # The next line will make ""reload(l)"" a silent no-op\n        return module.__getattribute__(x)\n\n    def __repr__(self):\n        return ""<module \'%s\' will be lazily loaded>"" % object.__getattribute__(self, \'__name__\')\n\n\nif \'READTHEDOCS\' in os.environ:\n    lazy_doc = """"""\n               WARNING: To get Sphinx documentation to build we disable\n               LazyImports, which makes Sphinx incorrectly report this\n               class as having a base class of object. In reality,\n               :class:`LazyImport`\'s base class is\n               :class:`types.ModuleType`.\n               """"""\n\n    lazy_doc += LazyImport.__doc__\n\n    class LazyImport(object):\n        __doc__ = lazy_doc\n\n        def __init__(self, x):\n            __import__(x)\n            self.module = sys.modules[x]\n\n        def __getattr__(self, x):\n            return self.module.__getattribute__(x)\n'"
tensorlayer/nlp.py,4,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport collections\nimport os\nimport random\nimport re\nimport subprocess\nimport tempfile\nimport warnings\nfrom collections import Counter\n\nimport numpy as np\nimport six as _six\nimport tensorflow as tf\nfrom six.moves import urllib, xrange\nfrom tensorflow.python.platform import gfile\n\nimport tensorlayer as tl\nfrom tensorlayer.lazy_imports import LazyImport\n\nnltk = LazyImport(""nltk"")\n\n__all__ = [\n    \'generate_skip_gram_batch\',\n    \'sample\',\n    \'sample_top\',\n    \'SimpleVocabulary\',\n    \'Vocabulary\',\n    \'process_sentence\',\n    \'create_vocab\',\n    \'simple_read_words\',\n    \'read_words\',\n    \'read_analogies_file\',\n    \'build_vocab\',\n    \'build_reverse_dictionary\',\n    \'build_words_dataset\',\n    \'words_to_word_ids\',\n    \'word_ids_to_words\',\n    \'save_vocab\',\n    \'basic_tokenizer\',\n    \'create_vocabulary\',\n    \'initialize_vocabulary\',\n    \'sentence_to_token_ids\',\n    \'data_to_token_ids\',\n    \'moses_multi_bleu\',\n]\n\n\ndef as_bytes(bytes_or_text, encoding=\'utf-8\'):\n    """"""Converts either bytes or unicode to `bytes`, using utf-8 encoding for text.\n  Args:\n    bytes_or_text: A `bytes`, `str`, or `unicode` object.\n    encoding: A string indicating the charset for encoding unicode.\n  Returns:\n    A `bytes` object.\n  Raises:\n    TypeError: If `bytes_or_text` is not a binary or unicode string.\n  """"""\n    if isinstance(bytes_or_text, _six.text_type):\n        return bytes_or_text.encode(encoding)\n    elif isinstance(bytes_or_text, bytes):\n        return bytes_or_text\n    else:\n        raise TypeError(\'Expected binary or unicode string, got %r\' % (bytes_or_text, ))\n\n\ndef as_text(bytes_or_text, encoding=\'utf-8\'):\n    """"""Returns the given argument as a unicode string.\n  Args:\n    bytes_or_text: A `bytes`, `str`, or `unicode` object.\n    encoding: A string indicating the charset for decoding unicode.\n  Returns:\n    A `unicode` (Python 2) or `str` (Python 3) object.\n  Raises:\n    TypeError: If `bytes_or_text` is not a binary or unicode string.\n  """"""\n    if isinstance(bytes_or_text, _six.text_type):\n        return bytes_or_text\n    elif isinstance(bytes_or_text, bytes):\n        return bytes_or_text.decode(encoding)\n    else:\n        raise TypeError(\'Expected binary or unicode string, got %r\' % bytes_or_text)\n\n\ndef generate_skip_gram_batch(data, batch_size, num_skips, skip_window, data_index=0):\n    """"""Generate a training batch for the Skip-Gram model.\n\n    See `Word2Vec example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_word2vec_basic.py>`__.\n\n    Parameters\n    ----------\n    data : list of data\n        To present context, usually a list of integers.\n    batch_size : int\n        Batch size to return.\n    num_skips : int\n        How many times to reuse an input to generate a label.\n    skip_window : int\n        How many words to consider left and right.\n    data_index : int\n        Index of the context location. This code use `data_index` to instead of yield like ``tl.iterate``.\n\n    Returns\n    -------\n    batch : list of data\n        Inputs.\n    labels : list of data\n        Labels\n    data_index : int\n        Index of the context location.\n\n    Examples\n    --------\n    Setting num_skips=2, skip_window=1, use the right and left words.\n    In the same way, num_skips=4, skip_window=2 means use the nearby 4 words.\n\n    >>> data = [1,2,3,4,5,6,7,8,9,10,11]\n    >>> batch, labels, data_index = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    >>> print(batch)\n    [2 2 3 3 4 4 5 5]\n    >>> print(labels)\n    [[3]\n    [1]\n    [4]\n    [2]\n    [5]\n    [3]\n    [4]\n    [6]]\n\n    """"""\n    # global data_index   # you can put data_index outside the function, then\n    #       modify the global data_index in the function without return it.\n    # note: without using yield, this code use data_index to instead.\n\n    if batch_size % num_skips != 0:\n        raise Exception(""batch_size should be able to be divided by num_skips."")\n    if num_skips > 2 * skip_window:\n        raise Exception(""num_skips <= 2 * skip_window"")\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    for _ in range(span):\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    for i in range(batch_size // num_skips):\n        target = skip_window  # target label at the center of the buffer\n        targets_to_avoid = [skip_window]\n        for j in range(num_skips):\n            while target in targets_to_avoid:\n                target = random.randint(0, span - 1)\n            targets_to_avoid.append(target)\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[target]\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    return batch, labels, data_index\n\n\ndef sample(a=None, temperature=1.0):\n    """"""Sample an index from a probability array.\n\n    Parameters\n    ----------\n    a : list of float\n        List of probabilities.\n    temperature : float or None\n        The higher the more uniform. When a = [0.1, 0.2, 0.7],\n            - temperature = 0.7, the distribution will be sharpen [0.05048273,  0.13588945,  0.81362782]\n            - temperature = 1.0, the distribution will be the same [0.1,    0.2,    0.7]\n            - temperature = 1.5, the distribution will be filtered [0.16008435,  0.25411807,  0.58579758]\n            - If None, it will be ``np.argmax(a)``\n\n    Notes\n    ------\n    - No matter what is the temperature and input list, the sum of all probabilities will be one. Even if input list = [1, 100, 200], the sum of all probabilities will still be one.\n    - For large vocabulary size, choice a higher temperature or ``tl.nlp.sample_top`` to avoid error.\n\n    """"""\n    if a is None:\n        raise Exception(""a : list of float"")\n    b = np.copy(a)\n    try:\n        if temperature == 1:\n            return np.argmax(np.random.multinomial(1, a, 1))\n        if temperature is None:\n            return np.argmax(a)\n        else:\n            a = np.log(a) / temperature\n            a = np.exp(a) / np.sum(np.exp(a))\n            return np.argmax(np.random.multinomial(1, a, 1))\n    except Exception:\n        # np.set_printoptions(threshold=np.nan)\n        # tl.logging.info(a)\n        # tl.logging.info(np.sum(a))\n        # tl.logging.info(np.max(a))\n        # tl.logging.info(np.min(a))\n        # exit()\n        message = ""For large vocabulary_size, choice a higher temperature\\\n         to avoid log error. Hint : use ``sample_top``. ""\n\n        warnings.warn(message, Warning)\n        # tl.logging.info(a)\n        # tl.logging.info(b)\n        return np.argmax(np.random.multinomial(1, b, 1))\n\n\ndef sample_top(a=None, top_k=10):\n    """"""Sample from ``top_k`` probabilities.\n\n    Parameters\n    ----------\n    a : list of float\n        List of probabilities.\n    top_k : int\n        Number of candidates to be considered.\n\n    """"""\n    if a is None:\n        a = []\n\n    idx = np.argpartition(a, -top_k)[-top_k:]\n    probs = a[idx]\n    # tl.logging.info(""new %f"" % probs)\n    probs = probs / np.sum(probs)\n    choice = np.random.choice(idx, p=probs)\n    return choice\n    # old implementation\n    # a = np.array(a)\n    # idx = np.argsort(a)[::-1]\n    # idx = idx[:top_k]\n    # # a = a[idx]\n    # probs = a[idx]\n    # tl.logging.info(""prev %f"" % probs)\n    # # probs = probs / np.sum(probs)\n    # # choice = np.random.choice(idx, p=probs)\n    # # return choice\n\n\n# Vector representations of words (Advanced)  UNDOCUMENT\nclass SimpleVocabulary(object):\n    """"""Simple vocabulary wrapper, see create_vocab().\n\n    Parameters\n    ------------\n    vocab : dictionary\n        A dictionary that maps word to ID.\n    unk_id : int\n        The ID for \'unknown\' word.\n\n    """"""\n\n    def __init__(self, vocab, unk_id):\n        """"""Initialize the vocabulary.""""""\n        self._vocab = vocab\n        self._unk_id = unk_id\n\n    def word_to_id(self, word):\n        """"""Returns the integer id of a word string.""""""\n        if word in self._vocab:\n            return self._vocab[word]\n        else:\n            return self._unk_id\n\n\nclass Vocabulary(object):\n    """"""Create Vocabulary class from a given vocabulary and its id-word, word-id convert.\n    See create_vocab() and ``tutorial_tfrecord3.py``.\n\n    Parameters\n    -----------\n    vocab_file : str\n        The file contains the vocabulary (can be created via ``tl.nlp.create_vocab``), where the words are the first whitespace-separated token on each line (other tokens are ignored) and the word ids are the corresponding line numbers.\n    start_word : str\n        Special word denoting sentence start.\n    end_word : str\n        Special word denoting sentence end.\n    unk_word : str\n        Special word denoting unknown words.\n\n    Attributes\n    ------------\n    vocab : dictionary\n        A dictionary that maps word to ID.\n    reverse_vocab : list of int\n        A list that maps ID to word.\n    start_id : int\n        For start ID.\n    end_id : int\n        For end ID.\n    unk_id : int\n        For unknown ID.\n    pad_id : int\n        For Padding ID.\n\n    Examples\n    -------------\n    The vocab file looks like follow, includes `start_word` , `end_word` ...\n\n    >>> a 969108\n    >>> <S> 586368\n    >>> </S> 586368\n    >>> . 440479\n    >>> on 213612\n    >>> of 202290\n    >>> the 196219\n    >>> in 182598\n    >>> with 152984\n    >>> and 139109\n    >>> is 97322\n\n    """"""\n\n    def __init__(self, vocab_file, start_word=""<S>"", end_word=""</S>"", unk_word=""<UNK>"", pad_word=""<PAD>""):\n        if not tf.io.gfile.exists(vocab_file):\n            tl.logging.fatal(""Vocab file %s not found."" % vocab_file)\n        tl.logging.info(""Initializing vocabulary from file: %s"" % vocab_file)\n\n        with tf.io.gfile.GFile(vocab_file, mode=""r"") as f:\n            reverse_vocab = list(f.readlines())\n        reverse_vocab = [line.split()[0] for line in reverse_vocab]\n        # assert start_word in reverse_vocab\n        # assert end_word in reverse_vocab\n        if start_word not in reverse_vocab:  # haodong\n            reverse_vocab.append(start_word)\n        if end_word not in reverse_vocab:\n            reverse_vocab.append(end_word)\n        if unk_word not in reverse_vocab:\n            reverse_vocab.append(unk_word)\n        if pad_word not in reverse_vocab:\n            reverse_vocab.append(pad_word)\n\n        vocab = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])\n\n        tl.logging.info(""Vocabulary from %s : %s %s %s"" % (vocab_file, start_word, end_word, unk_word))\n        tl.logging.info(""    vocabulary with %d words (includes start_word, end_word, unk_word)"" % len(vocab))\n        # tl.logging.info(""     vocabulary with %d words"" % len(vocab))\n\n        self.vocab = vocab  # vocab[word] = id\n        self.reverse_vocab = reverse_vocab  # reverse_vocab[id] = word\n\n        # Save special word ids.\n        self.start_id = vocab[start_word]\n        self.end_id = vocab[end_word]\n        self.unk_id = vocab[unk_word]\n        self.pad_id = vocab[pad_word]\n        tl.logging.info(""      start_id: %d"" % self.start_id)\n        tl.logging.info(""      end_id  : %d"" % self.end_id)\n        tl.logging.info(""      unk_id  : %d"" % self.unk_id)\n        tl.logging.info(""      pad_id  : %d"" % self.pad_id)\n\n    def word_to_id(self, word):\n        """"""Returns the integer word id of a word string.""""""\n        if word in self.vocab:\n            return self.vocab[word]\n        else:\n            return self.unk_id\n\n    def id_to_word(self, word_id):\n        """"""Returns the word string of an integer word id.""""""\n        if word_id >= len(self.reverse_vocab):\n            return self.reverse_vocab[self.unk_id]\n        else:\n            return self.reverse_vocab[word_id]\n\n\ndef process_sentence(sentence, start_word=""<S>"", end_word=""</S>""):\n    """"""Seperate a sentence string into a list of string words, add start_word and end_word,\n    see ``create_vocab()`` and ``tutorial_tfrecord3.py``.\n\n    Parameters\n    ----------\n    sentence : str\n        A sentence.\n    start_word : str or None\n        The start word. If None, no start word will be appended.\n    end_word : str or None\n        The end word. If None, no end word will be appended.\n\n    Returns\n    ---------\n    list of str\n        A list of strings that separated into words.\n\n    Examples\n    -----------\n    >>> c = ""how are you?""\n    >>> c = tl.nlp.process_sentence(c)\n    >>> print(c)\n    [\'<S>\', \'how\', \'are\', \'you\', \'?\', \'</S>\']\n\n    Notes\n    -------\n    - You have to install the following package.\n    - `Installing NLTK <http://www.nltk.org/install.html>`__\n    - `Installing NLTK data <http://www.nltk.org/data.html>`__\n\n    """"""\n    if start_word is not None:\n        process_sentence = [start_word]\n    else:\n        process_sentence = []\n    process_sentence.extend(nltk.tokenize.word_tokenize(sentence.lower()))\n\n    if end_word is not None:\n        process_sentence.append(end_word)\n    return process_sentence\n\n\ndef create_vocab(sentences, word_counts_output_file, min_word_count=1):\n    """"""Creates the vocabulary of word to word_id.\n\n    See ``tutorial_tfrecord3.py``.\n\n    The vocabulary is saved to disk in a text file of word counts. The id of each\n    word in the file is its corresponding 0-based line number.\n\n    Parameters\n    ------------\n    sentences : list of list of str\n        All sentences for creating the vocabulary.\n    word_counts_output_file : str\n        The file name.\n    min_word_count : int\n        Minimum number of occurrences for a word.\n\n    Returns\n    --------\n    :class:`SimpleVocabulary`\n        The simple vocabulary object, see :class:`Vocabulary` for more.\n\n    Examples\n    --------\n    Pre-process sentences\n\n    >>> captions = [""one two , three"", ""four five five""]\n    >>> processed_capts = []\n    >>> for c in captions:\n    >>>     c = tl.nlp.process_sentence(c, start_word=""<S>"", end_word=""</S>"")\n    >>>     processed_capts.append(c)\n    >>> print(processed_capts)\n    ...[[\'<S>\', \'one\', \'two\', \',\', \'three\', \'</S>\'], [\'<S>\', \'four\', \'five\', \'five\', \'</S>\']]\n\n    Create vocabulary\n\n    >>> tl.nlp.create_vocab(processed_capts, word_counts_output_file=\'vocab.txt\', min_word_count=1)\n    Creating vocabulary.\n      Total words: 8\n      Words in vocabulary: 8\n      Wrote vocabulary file: vocab.txt\n\n    Get vocabulary object\n\n    >>> vocab = tl.nlp.Vocabulary(\'vocab.txt\', start_word=""<S>"", end_word=""</S>"", unk_word=""<UNK>"")\n    INFO:tensorflow:Initializing vocabulary from file: vocab.txt\n    [TL] Vocabulary from vocab.txt : <S> </S> <UNK>\n    vocabulary with 10 words (includes start_word, end_word, unk_word)\n        start_id: 2\n        end_id: 3\n        unk_id: 9\n        pad_id: 0\n\n    """"""\n    tl.logging.info(""Creating vocabulary."")\n\n    counter = Counter()\n\n    for c in sentences:\n        counter.update(c)\n        # tl.logging.info(\'c\',c)\n    tl.logging.info(""    Total words: %d"" % len(counter))\n\n    # Filter uncommon words and sort by descending count.\n    word_counts = [x for x in counter.items() if x[1] >= min_word_count]\n    word_counts.sort(key=lambda x: x[1], reverse=True)\n    word_counts = [(""<PAD>"", 0)] + word_counts  # 1st id should be reserved for padding\n    # tl.logging.info(word_counts)\n    tl.logging.info(""    Words in vocabulary: %d"" % len(word_counts))\n\n    # Write out the word counts file.\n    with tf.io.gfile.GFile(word_counts_output_file, ""w"") as f:\n        f.write(""\\n"".join([""%s %d"" % (w, c) for w, c in word_counts]))\n    tl.logging.info(""    Wrote vocabulary file: %s"" % word_counts_output_file)\n\n    # Create the vocabulary dictionary.\n    reverse_vocab = [x[0] for x in word_counts]\n    unk_id = len(reverse_vocab)\n    vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])\n    vocab = SimpleVocabulary(vocab_dict, unk_id)\n\n    return vocab\n\n\n# Vector representations of words\ndef simple_read_words(filename=""nietzsche.txt""):\n    """"""Read context from file without any preprocessing.\n\n    Parameters\n    ----------\n    filename : str\n        A file path (like .txt file)\n\n    Returns\n    --------\n    str\n        The context in a string.\n\n    """"""\n    with open(filename, ""r"") as f:\n        words = f.read()\n        return words\n\n\ndef read_words(filename=""nietzsche.txt"", replace=None):\n    """"""Read list format context from a file.\n\n    For customized read_words method, see ``tutorial_generate_text.py``.\n\n    Parameters\n    ----------\n    filename : str\n        a file path.\n    replace : list of str\n        replace original string by target string.\n\n    Returns\n    -------\n    list of str\n        The context in a list (split using space).\n    """"""\n    if replace is None:\n        replace = [\'\\n\', \'<eos>\']\n\n    with tf.io.gfile.GFile(filename, ""r"") as f:\n        try:  # python 3.4 or older\n            context_list = f.read().replace(*replace).split()\n        except Exception:  # python 3.5\n            f.seek(0)\n            replace = [x.encode(\'utf-8\') for x in replace]\n            context_list = f.read().replace(*replace).split()\n        return context_list\n\n\ndef read_analogies_file(eval_file=\'questions-words.txt\', word2id=None):\n    """"""Reads through an analogy question file, return its id format.\n\n    Parameters\n    ----------\n    eval_file : str\n        The file name.\n    word2id : dictionary\n        a dictionary that maps word to ID.\n\n    Returns\n    --------\n    numpy.array\n        A ``[n_examples, 4]`` numpy array containing the analogy question\'s word IDs.\n\n    Examples\n    ---------\n    The file should be in this format\n\n    >>> : capital-common-countries\n    >>> Athens Greece Baghdad Iraq\n    >>> Athens Greece Bangkok Thailand\n    >>> Athens Greece Beijing China\n    >>> Athens Greece Berlin Germany\n    >>> Athens Greece Bern Switzerland\n    >>> Athens Greece Cairo Egypt\n    >>> Athens Greece Canberra Australia\n    >>> Athens Greece Hanoi Vietnam\n    >>> Athens Greece Havana Cuba\n\n    Get the tokenized analogy question data\n\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> analogy_questions = tl.nlp.read_analogies_file(eval_file=\'questions-words.txt\', word2id=dictionary)\n    >>> print(analogy_questions)\n    [[ 3068  1248  7161  1581]\n    [ 3068  1248 28683  5642]\n    [ 3068  1248  3878   486]\n    ...,\n    [ 1216  4309 19982 25506]\n    [ 1216  4309  3194  8650]\n    [ 1216  4309   140   312]]\n\n    """"""\n    if word2id is None:\n        word2id = {}\n\n    questions = []\n    questions_skipped = 0\n\n    with open(eval_file, ""rb"") as analogy_f:\n        for line in analogy_f:\n            if line.startswith(b"":""):  # Skip comments.\n                continue\n            words = line.strip().lower().split(b"" "")  # lowercase\n            ids = [word2id.get(w.strip().decode()) for w in words]\n            if None in ids or len(ids) != 4:\n                questions_skipped += 1\n            else:\n                questions.append(np.array(ids))\n    tl.logging.info(""Eval analogy file: %s"" % eval_file)\n    tl.logging.info(""Questions: %d"", len(questions))\n    tl.logging.info(""Skipped: %d"", questions_skipped)\n    analogy_questions = np.array(questions, dtype=np.int32)\n    return analogy_questions\n\n\ndef build_vocab(data):\n    """"""Build vocabulary.\n\n    Given the context in list format.\n    Return the vocabulary, which is a dictionary for word to id.\n    e.g. {\'campbell\': 2587, \'atlantic\': 2247, \'aoun\': 6746 .... }\n\n    Parameters\n    ----------\n    data : list of str\n        The context in list format\n\n    Returns\n    --------\n    dictionary\n        that maps word to unique ID. e.g. {\'campbell\': 2587, \'atlantic\': 2247, \'aoun\': 6746 .... }\n\n    References\n    ---------------\n    - `tensorflow.models.rnn.ptb.reader <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb>`_\n\n    Examples\n    --------\n    >>> data_path = os.getcwd() + \'/simple-examples/data\'\n    >>> train_path = os.path.join(data_path, ""ptb.train.txt"")\n    >>> word_to_id = build_vocab(read_txt_words(train_path))\n\n    """"""\n    # data = _read_words(filename)\n    counter = collections.Counter(data)\n    # tl.logging.info(\'counter %s\' % counter)   # dictionary for the occurrence number of each word, e.g. \'banknote\': 1, \'photography\': 1, \'kia\': 1\n    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n    # tl.logging.info(\'count_pairs %s\' % count_pairs)  # convert dictionary to list of tuple, e.g. (\'ssangyong\', 1), (\'swapo\', 1), (\'wachter\', 1)\n    words, _ = list(zip(*count_pairs))\n    word_to_id = dict(zip(words, range(len(words))))\n    # tl.logging.info(words)    # list of words\n    # tl.logging.info(word_to_id) # dictionary for word to id, e.g. \'campbell\': 2587, \'atlantic\': 2247, \'aoun\': 6746\n    return word_to_id\n\n\ndef build_reverse_dictionary(word_to_id):\n    """"""Given a dictionary that maps word to integer id.\n    Returns a reverse dictionary that maps a id to word.\n\n    Parameters\n    ----------\n    word_to_id : dictionary\n        that maps word to ID.\n\n    Returns\n    --------\n    dictionary\n        A dictionary that maps IDs to words.\n\n    """"""\n    reverse_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n    return reverse_dictionary\n\n\ndef build_words_dataset(words=None, vocabulary_size=50000, printable=True, unk_key=\'UNK\'):\n    """"""Build the words dictionary and replace rare words with \'UNK\' token.\n    The most common word has the smallest integer id.\n\n    Parameters\n    ----------\n    words : list of str or byte\n        The context in list format. You may need to do preprocessing on the words, such as lower case, remove marks etc.\n    vocabulary_size : int\n        The maximum vocabulary size, limiting the vocabulary size. Then the script replaces rare words with \'UNK\' token.\n    printable : boolean\n        Whether to print the read vocabulary size of the given words.\n    unk_key : str\n        Represent the unknown words.\n\n    Returns\n    --------\n    data : list of int\n        The context in a list of ID.\n    count : list of tuple and list\n        Pair words and IDs.\n            - count[0] is a list : the number of rare words\n            - count[1:] are tuples : the number of occurrence of each word\n            - e.g. [[\'UNK\', 418391], (b\'the\', 1061396), (b\'of\', 593677), (b\'and\', 416629), (b\'one\', 411764)]\n    dictionary : dictionary\n        It is `word_to_id` that maps word to ID.\n    reverse_dictionary : a dictionary\n        It is `id_to_word` that maps ID to word.\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size)\n\n    References\n    -----------------\n    - `tensorflow/examples/tutorials/word2vec/word2vec_basic.py <https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/word2vec/word2vec_basic.py>`__\n\n    """"""\n    if words is None:\n        raise Exception(""words : list of str or byte"")\n\n    count = [[unk_key, -1]]\n    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary[\'UNK\']\n            unk_count += 1\n        data.append(index)\n    count[0][1] = unk_count\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    if printable:\n        tl.logging.info(\'Real vocabulary size    %d\' % len(collections.Counter(words).keys()))\n        tl.logging.info(\'Limited vocabulary size {}\'.format(vocabulary_size))\n    if len(collections.Counter(words).keys()) < vocabulary_size:\n        raise Exception(\n            ""len(collections.Counter(words).keys()) >= vocabulary_size , the limited vocabulary_size must be less than or equal to the read vocabulary_size""\n        )\n    return data, count, dictionary, reverse_dictionary\n\n\ndef words_to_word_ids(data=None, word_to_id=None, unk_key=\'UNK\'):\n    """"""Convert a list of string (words) to IDs.\n\n    Parameters\n    ----------\n    data : list of string or byte\n        The context in list format\n    word_to_id : a dictionary\n        that maps word to ID.\n    unk_key : str\n        Represent the unknown words.\n\n    Returns\n    --------\n    list of int\n        A list of IDs to represent the context.\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> context = [b\'hello\', b\'how\', b\'are\', b\'you\']\n    >>> ids = tl.nlp.words_to_word_ids(words, dictionary)\n    >>> context = tl.nlp.word_ids_to_words(ids, reverse_dictionary)\n    >>> print(ids)\n    [6434, 311, 26, 207]\n    >>> print(context)\n    [b\'hello\', b\'how\', b\'are\', b\'you\']\n\n    References\n    ---------------\n    - `tensorflow.models.rnn.ptb.reader <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb>`__\n\n    """"""\n    if data is None:\n        raise Exception(""data : list of string or byte"")\n    if word_to_id is None:\n        raise Exception(""word_to_id : a dictionary"")\n    # if isinstance(data[0], six.string_types):\n    #     tl.logging.info(type(data[0]))\n    #     # exit()\n    #     tl.logging.info(data[0])\n    #     tl.logging.info(word_to_id)\n    #     return [word_to_id[str(word)] for word in data]\n    # else:\n\n    word_ids = []\n    for word in data:\n        if word_to_id.get(word) is not None:\n            word_ids.append(word_to_id[word])\n        else:\n            word_ids.append(word_to_id[unk_key])\n    return word_ids\n    # return [word_to_id[word] for word in data]    # this one\n\n    # if isinstance(data[0], str):\n    #     # tl.logging.info(\'is a string object\')\n    #     return [word_to_id[word] for word in data]\n    # else:#if isinstance(s, bytes):\n    #     # tl.logging.info(\'is a unicode object\')\n    #     # tl.logging.info(data[0])\n    #     return [word_to_id[str(word)] f\n\n\ndef word_ids_to_words(data, id_to_word):\n    """"""Convert a list of integer to strings (words).\n\n    Parameters\n    ----------\n    data : list of int\n        The context in list format.\n    id_to_word : dictionary\n        a dictionary that maps ID to word.\n\n    Returns\n    --------\n    list of str\n        A list of string or byte to represent the context.\n\n    Examples\n    ---------\n    see ``tl.nlp.words_to_word_ids``\n\n    """"""\n    return [id_to_word[i] for i in data]\n\n\ndef save_vocab(count=None, name=\'vocab.txt\'):\n    """"""Save the vocabulary to a file so the model can be reloaded.\n\n    Parameters\n    ----------\n    count : a list of tuple and list\n        count[0] is a list : the number of rare words,\n        count[1:] are tuples : the number of occurrence of each word,\n        e.g. [[\'UNK\', 418391], (b\'the\', 1061396), (b\'of\', 593677), (b\'and\', 416629), (b\'one\', 411764)]\n\n    Examples\n    ---------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> tl.nlp.save_vocab(count, name=\'vocab_text8.txt\')\n    >>> vocab_text8.txt\n    UNK 418391\n    the 1061396\n    of 593677\n    and 416629\n    one 411764\n    in 372201\n    a 325873\n    to 316376\n\n    """"""\n    if count is None:\n        count = []\n\n    pwd = os.getcwd()\n    vocabulary_size = len(count)\n    with open(os.path.join(pwd, name), ""w"") as f:\n        for i in xrange(vocabulary_size):\n            f.write(""%s %d\\n"" % (as_text(count[i][0]), count[i][1]))\n    tl.logging.info(""%d vocab saved to %s in %s"" % (vocabulary_size, name, pwd))\n\n\n# Functions for translation\n\n\ndef basic_tokenizer(sentence, _WORD_SPLIT=re.compile(b""([.,!?\\""\':;)(])"")):\n    """"""Very basic tokenizer: split the sentence into a list of tokens.\n\n    Parameters\n    -----------\n    sentence : tensorflow.python.platform.gfile.GFile Object\n    _WORD_SPLIT : regular expression for word spliting.\n\n\n    Examples\n    --------\n    >>> see create_vocabulary\n    >>> from tensorflow.python.platform import gfile\n    >>> train_path = ""wmt/giga-fren.release2""\n    >>> with gfile.GFile(train_path + "".en"", mode=""rb"") as f:\n    >>>    for line in f:\n    >>>       tokens = tl.nlp.basic_tokenizer(line)\n    >>>       tl.logging.info(tokens)\n    >>>       exit()\n    [b\'Changing\', b\'Lives\', b\'|\', b\'Changing\', b\'Society\', b\'|\', b\'How\',\n      b\'It\', b\'Works\', b\'|\', b\'Technology\', b\'Drives\', b\'Change\', b\'Home\',\n      b\'|\', b\'Concepts\', b\'|\', b\'Teachers\', b\'|\', b\'Search\', b\'|\', b\'Overview\',\n      b\'|\', b\'Credits\', b\'|\', b\'HHCC\', b\'Web\', b\'|\', b\'Reference\', b\'|\',\n      b\'Feedback\', b\'Virtual\', b\'Museum\', b\'of\', b\'Canada\', b\'Home\', b\'Page\']\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``\n\n    """"""\n    words = []\n    sentence = as_bytes(sentence)\n    for space_separated_fragment in sentence.strip().split():\n        words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n    return [w for w in words if w]\n\n\ndef create_vocabulary(\n    vocabulary_path, data_path, max_vocabulary_size, tokenizer=None, normalize_digits=True,\n    _DIGIT_RE=re.compile(br""\\d""), _START_VOCAB=None\n):\n    r""""""Create vocabulary file (if it does not exist yet) from data file.\n\n    Data file is assumed to contain one sentence per line. Each sentence is\n    tokenized and digits are normalized (if normalize_digits is set).\n    Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n    We write it to vocabulary_path in a one-token-per-line format, so that later\n    token in the first line gets id=0, second line gets id=1, and so on.\n\n    Parameters\n    -----------\n    vocabulary_path : str\n        Path where the vocabulary will be created.\n    data_path : str\n        Data file that will be used to create vocabulary.\n    max_vocabulary_size : int\n        Limit on the size of the created vocabulary.\n    tokenizer : function\n        A function to use to tokenize each data sentence. If None, basic_tokenizer will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by `0`.\n    _DIGIT_RE : regular expression function\n        Default is ``re.compile(br""\\d"")``.\n    _START_VOCAB : list of str\n        The pad, go, eos and unk token, default is ``[b""_PAD"", b""_GO"", b""_EOS"", b""_UNK""]``.\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``\n\n    """"""\n    if _START_VOCAB is None:\n        _START_VOCAB = [b""_PAD"", b""_GO"", b""_EOS"", b""_UNK""]\n    if not gfile.Exists(vocabulary_path):\n        tl.logging.info(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n        vocab = {}\n        with gfile.GFile(data_path, mode=""rb"") as f:\n            counter = 0\n            for line in f:\n                counter += 1\n                if counter % 100000 == 0:\n                    tl.logging.info(""  processing line %d"" % counter)\n                tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n                for w in tokens:\n                    word = re.sub(_DIGIT_RE, b""0"", w) if normalize_digits else w\n                    if word in vocab:\n                        vocab[word] += 1\n                    else:\n                        vocab[word] = 1\n            vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n            if len(vocab_list) > max_vocabulary_size:\n                vocab_list = vocab_list[:max_vocabulary_size]\n            with gfile.GFile(vocabulary_path, mode=""wb"") as vocab_file:\n                for w in vocab_list:\n                    vocab_file.write(w + b""\\n"")\n    else:\n        tl.logging.info(""Vocabulary %s from data %s exists"" % (vocabulary_path, data_path))\n\n\ndef initialize_vocabulary(vocabulary_path):\n    """"""Initialize vocabulary from file, return the `word_to_id` (dictionary)\n    and `id_to_word` (list).\n\n    We assume the vocabulary is stored one-item-per-line, so a file will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will also return the reversed-vocabulary [""dog"", ""cat""].\n\n    Parameters\n    -----------\n    vocabulary_path : str\n        Path to the file containing the vocabulary.\n\n    Returns\n    --------\n    vocab : dictionary\n        a dictionary that maps word to ID.\n    rev_vocab : list of int\n        a list that maps ID to word.\n\n    Examples\n    ---------\n    >>> Assume \'test\' contains\n    dog\n    cat\n    bird\n    >>> vocab, rev_vocab = tl.nlp.initialize_vocabulary(""test"")\n    >>> print(vocab)\n    >>> {b\'cat\': 1, b\'dog\': 0, b\'bird\': 2}\n    >>> print(rev_vocab)\n    >>> [b\'dog\', b\'cat\', b\'bird\']\n\n    Raises\n    -------\n    ValueError : if the provided vocabulary_path does not exist.\n\n    """"""\n    if gfile.Exists(vocabulary_path):\n        rev_vocab = []\n        with gfile.GFile(vocabulary_path, mode=""rb"") as f:\n            rev_vocab.extend(f.readlines())\n        rev_vocab = [as_bytes(line.strip()) for line in rev_vocab]\n        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n        return vocab, rev_vocab\n    else:\n        raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(\n    sentence, vocabulary, tokenizer=None, normalize_digits=True, UNK_ID=3, _DIGIT_RE=re.compile(br""\\d"")\n):\n    """"""Convert a string to list of integers representing token-ids.\n\n    For example, a sentence ""I have a dog"" may become tokenized into\n    [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n    ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n    Parameters\n    -----------\n    sentence : tensorflow.python.platform.gfile.GFile Object\n        The sentence in bytes format to convert to token-ids, see ``basic_tokenizer()`` and ``data_to_token_ids()``.\n    vocabulary : dictionary\n        Mmapping tokens to integers.\n    tokenizer : function\n        A function to use to tokenize each sentence. If None, ``basic_tokenizer`` will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by 0.\n\n    Returns\n    --------\n    list of int\n        The token-ids for the sentence.\n\n    """"""\n    if tokenizer:\n        words = tokenizer(sentence)\n    else:\n        words = basic_tokenizer(sentence)\n    if not normalize_digits:\n        return [vocabulary.get(w, UNK_ID) for w in words]\n    # Normalize digits by 0 before looking words up in the vocabulary.\n    return [vocabulary.get(re.sub(_DIGIT_RE, b""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(\n    data_path, target_path, vocabulary_path, tokenizer=None, normalize_digits=True, UNK_ID=3,\n    _DIGIT_RE=re.compile(br""\\d"")\n):\n    """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n    This function loads data line-by-line from data_path, calls the above\n    sentence_to_token_ids, and saves the result to target_path. See comment\n    for sentence_to_token_ids on the details of token-ids format.\n\n    Parameters\n    -----------\n    data_path : str\n        Path to the data file in one-sentence-per-line format.\n    target_path : str\n        Path where the file with token-ids will be created.\n    vocabulary_path : str\n        Path to the vocabulary file.\n    tokenizer : function\n        A function to use to tokenize each sentence. If None, ``basic_tokenizer`` will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by 0.\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``\n\n    """"""\n    if not gfile.Exists(target_path):\n        tl.logging.info(""Tokenizing data in %s"" % data_path)\n        vocab, _ = initialize_vocabulary(vocabulary_path)\n        with gfile.GFile(data_path, mode=""rb"") as data_file:\n            with gfile.GFile(target_path, mode=""w"") as tokens_file:\n                counter = 0\n                for line in data_file:\n                    counter += 1\n                    if counter % 100000 == 0:\n                        tl.logging.info(""  tokenizing line %d"" % counter)\n                    token_ids = sentence_to_token_ids(\n                        line, vocab, tokenizer, normalize_digits, UNK_ID=UNK_ID, _DIGIT_RE=_DIGIT_RE\n                    )\n                    tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n    else:\n        tl.logging.info(""Target path %s exists"" % target_path)\n\n\ndef moses_multi_bleu(hypotheses, references, lowercase=False):\n    """"""Calculate the bleu score for hypotheses and references\n    using the MOSES ulti-bleu.perl script.\n\n    Parameters\n    ------------\n    hypotheses : numpy.array.string\n        A numpy array of strings where each string is a single example.\n    references : numpy.array.string\n        A numpy array of strings where each string is a single example.\n    lowercase : boolean\n        If True, pass the ""-lc"" flag to the multi-bleu script\n\n    Examples\n    ---------\n    >>> hypotheses = [""a bird is flying on the sky""]\n    >>> references = [""two birds are flying on the sky"", ""a bird is on the top of the tree"", ""an airplane is on the sky"",]\n    >>> score = tl.nlp.moses_multi_bleu(hypotheses, references)\n\n    Returns\n    --------\n    float\n        The BLEU score\n\n    References\n    ----------\n    - `Google/seq2seq/metric/bleu <https://github.com/google/seq2seq>`__\n\n    """"""\n    if np.size(hypotheses) == 0:\n        return np.float32(0.0)\n\n    # Get MOSES multi-bleu script\n    try:\n        multi_bleu_path, _ = urllib.request.urlretrieve(\n            ""https://raw.githubusercontent.com/moses-smt/mosesdecoder/""\n            ""master/scripts/generic/multi-bleu.perl""\n        )\n        os.chmod(multi_bleu_path, 0o755)\n    except Exception:  # pylint: disable=W0702\n        tl.logging.info(""Unable to fetch multi-bleu.perl script, using local."")\n        metrics_dir = os.path.dirname(os.path.realpath(__file__))\n        bin_dir = os.path.abspath(os.path.join(metrics_dir, "".."", "".."", ""bin""))\n        multi_bleu_path = os.path.join(bin_dir, ""tools/multi-bleu.perl"")\n\n    # Dump hypotheses and references to tempfiles\n    hypothesis_file = tempfile.NamedTemporaryFile()\n    hypothesis_file.write(""\\n"".join(hypotheses).encode(""utf-8""))\n    hypothesis_file.write(b""\\n"")\n    hypothesis_file.flush()\n    reference_file = tempfile.NamedTemporaryFile()\n    reference_file.write(""\\n"".join(references).encode(""utf-8""))\n    reference_file.write(b""\\n"")\n    reference_file.flush()\n\n    # Calculate BLEU using multi-bleu script\n    with open(hypothesis_file.name, ""r"") as read_pred:\n        bleu_cmd = [multi_bleu_path]\n        if lowercase:\n            bleu_cmd += [""-lc""]\n        bleu_cmd += [reference_file.name]\n        try:\n            bleu_out = subprocess.check_output(bleu_cmd, stdin=read_pred, stderr=subprocess.STDOUT)\n            bleu_out = bleu_out.decode(""utf-8"")\n            bleu_score = re.search(r""BLEU = (.+?),"", bleu_out).group(1)\n            bleu_score = float(bleu_score)\n        except subprocess.CalledProcessError as error:\n            if error.output is not None:\n                tl.logging.warning(""multi-bleu.perl script returned non-zero exit code"")\n                tl.logging.warning(error.output)\n            bleu_score = np.float32(0.0)\n\n    # Close temp files\n    hypothesis_file.close()\n    reference_file.close()\n\n    return np.float32(bleu_score)\n'"
tensorlayer/package_info.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""Deep learning and Reinforcement learning library for Researchers and Engineers.""""""\n\nMAJOR = 2\nMINOR = 2\nPATCH = 2\nPRE_RELEASE = \'\'\n# Use the following formatting: (major, minor, patch, prerelease)\nVERSION = (MAJOR, MINOR, PATCH, PRE_RELEASE)\n\n__shortversion__ = \'.\'.join(map(str, VERSION[:3]))\n__version__ = \'.\'.join(map(str, VERSION[:3])) + \'\'.join(VERSION[3:])\n\n__package_name__ = \'tensorlayer\'\n__contact_names__ = \'TensorLayer Contributors\'\n__contact_emails__ = \'tensorlayer@gmail.com\'\n__homepage__ = \'http://tensorlayer.readthedocs.io/en/latest/\'\n__repository_url__ = \'https://github.com/tensorlayer/tensorlayer\'\n__download_url__ = \'https://github.com/tensorlayer/tensorlayer\'\n__description__ = \'High Level Tensorflow Deep Learning Library for Researcher and Engineer.\'\n__license__ = \'apache\'\n__keywords__ = \'deep learning, machine learning, computer vision, nlp, \'\n__keywords__ += \'supervised learning, unsupervised learning, reinforcement learning, tensorflow\'\n'"
tensorlayer/prepro.py,7,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport copy\nimport math\nimport random\nimport threading\nimport time\n\nimport numpy as np\nimport PIL\nimport scipy\nimport scipy.ndimage as ndi\nimport skimage\nfrom scipy import linalg\nfrom scipy.ndimage.filters import gaussian_filter\nfrom scipy.ndimage.interpolation import map_coordinates\nfrom six.moves import range\nfrom skimage import exposure, transform\nfrom skimage.morphology import binary_dilation as _binary_dilation\nfrom skimage.morphology import binary_erosion as _binary_erosion\nfrom skimage.morphology import disk\nfrom skimage.morphology import erosion as _erosion\n\nimport tensorlayer as tl\nfrom tensorlayer.lazy_imports import LazyImport\n\ncv2 = LazyImport(""cv2"")\n\n# linalg https://docs.scipy.org/doc/scipy/reference/linalg.html\n# ndimage https://docs.scipy.org/doc/scipy/reference/ndimage.html\n\n__all__ = [\n    \'threading_data\',\n    \'affine_rotation_matrix\',\n    \'affine_horizontal_flip_matrix\',\n    \'affine_shift_matrix\',\n    \'affine_shear_matrix\',\n    \'affine_zoom_matrix\',\n    \'affine_respective_zoom_matrix\',\n    \'transform_matrix_offset_center\',\n    \'affine_transform\',\n    \'affine_transform_cv2\',\n    \'affine_transform_keypoints\',\n    \'projective_transform_by_points\',\n    \'rotation\',\n    \'rotation_multi\',\n    \'crop\',\n    \'crop_multi\',\n    \'flip_axis\',\n    \'flip_axis_multi\',\n    \'shift\',\n    \'shift_multi\',\n    \'shear\',\n    \'shear_multi\',\n    \'shear2\',\n    \'shear_multi2\',\n    \'swirl\',\n    \'swirl_multi\',\n    \'elastic_transform\',\n    \'elastic_transform_multi\',\n    \'zoom\',\n    \'respective_zoom\',\n    \'zoom_multi\',\n    \'brightness\',\n    \'brightness_multi\',\n    \'illumination\',\n    \'rgb_to_hsv\',\n    \'hsv_to_rgb\',\n    \'adjust_hue\',\n    \'imresize\',\n    \'pixel_value_scale\',\n    \'samplewise_norm\',\n    \'featurewise_norm\',\n    \'get_zca_whitening_principal_components_img\',\n    \'zca_whitening\',\n    \'channel_shift\',\n    \'channel_shift_multi\',\n    \'drop\',\n    \'array_to_img\',\n    \'find_contours\',\n    \'pt2map\',\n    \'binary_dilation\',\n    \'dilation\',\n    \'binary_erosion\',\n    \'erosion\',\n    \'obj_box_coords_rescale\',\n    \'obj_box_coord_rescale\',\n    \'obj_box_coord_scale_to_pixelunit\',\n    \'obj_box_coord_centroid_to_upleft_butright\',\n    \'obj_box_coord_upleft_butright_to_centroid\',\n    \'obj_box_coord_centroid_to_upleft\',\n    \'obj_box_coord_upleft_to_centroid\',\n    \'parse_darknet_ann_str_to_list\',\n    \'parse_darknet_ann_list_to_cls_box\',\n    \'obj_box_left_right_flip\',\n    \'obj_box_imresize\',\n    \'obj_box_crop\',\n    \'obj_box_shift\',\n    \'obj_box_zoom\',\n    \'pad_sequences\',\n    \'remove_pad_sequences\',\n    \'process_sequences\',\n    \'sequences_add_start_id\',\n    \'sequences_add_end_id\',\n    \'sequences_add_end_id_after_pad\',\n    \'sequences_get_mask\',\n    \'keypoint_random_crop\',\n    \'keypoint_resize_random_crop\',\n    \'keypoint_random_rotate\',\n    \'keypoint_random_flip\',\n    \'keypoint_random_resize\',\n    \'keypoint_random_resize_shortestedge\',\n]\n\n\ndef threading_data(data=None, fn=None, thread_count=None, **kwargs):\n    """"""Process a batch of data by given function by threading.\n\n    Usually be used for data augmentation.\n\n    Parameters\n    -----------\n    data : numpy.array or others\n        The data to be processed.\n    thread_count : int\n        The number of threads to use.\n    fn : function\n        The function for data processing.\n    more args : the args for `fn`\n        Ssee Examples below.\n\n    Examples\n    --------\n    Process images.\n\n    >>> images, _, _, _ = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3))\n    >>> images = tl.prepro.threading_data(images[0:32], tl.prepro.zoom, zoom_range=[0.5, 1])\n\n    Customized image preprocessing function.\n\n    >>> def distort_img(x):\n    >>>     x = tl.prepro.flip_axis(x, axis=0, is_random=True)\n    >>>     x = tl.prepro.flip_axis(x, axis=1, is_random=True)\n    >>>     x = tl.prepro.crop(x, 100, 100, is_random=True)\n    >>>     return x\n    >>> images = tl.prepro.threading_data(images, distort_img)\n\n    Process images and masks together (Usually be used for image segmentation).\n\n    >>> X, Y --> [batch_size, row, col, 1]\n    >>> data = tl.prepro.threading_data([_ for _ in zip(X, Y)], tl.prepro.zoom_multi, zoom_range=[0.5, 1], is_random=True)\n    data --> [batch_size, 2, row, col, 1]\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n    X_, Y_ --> [batch_size, row, col, 1]\n    >>> tl.vis.save_image(X_, \'images.png\')\n    >>> tl.vis.save_image(Y_, \'masks.png\')\n\n    Process images and masks together by using ``thread_count``.\n\n    >>> X, Y --> [batch_size, row, col, 1]\n    >>> data = tl.prepro.threading_data(X, tl.prepro.zoom_multi, 8, zoom_range=[0.5, 1], is_random=True)\n    data --> [batch_size, 2, row, col, 1]\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n    X_, Y_ --> [batch_size, row, col, 1]\n    >>> tl.vis.save_image(X_, \'after.png\')\n    >>> tl.vis.save_image(Y_, \'before.png\')\n\n    Customized function for processing images and masks together.\n\n    >>> def distort_img(data):\n    >>>    x, y = data\n    >>>    x, y = tl.prepro.flip_axis_multi([x, y], axis=0, is_random=True)\n    >>>    x, y = tl.prepro.flip_axis_multi([x, y], axis=1, is_random=True)\n    >>>    x, y = tl.prepro.crop_multi([x, y], 100, 100, is_random=True)\n    >>>    return x, y\n\n    >>> X, Y --> [batch_size, row, col, channel]\n    >>> data = tl.prepro.threading_data([_ for _ in zip(X, Y)], distort_img)\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n\n    Returns\n    -------\n    list or numpyarray\n        The processed results.\n\n    References\n    ----------\n    - `python queue <https://pymotw.com/2/Queue/index.html#module-Queue>`__\n    - `run with limited queue <http://effbot.org/librarybook/queue.htm>`__\n\n    """"""\n\n    def apply_fn(results, i, data, kwargs):\n        results[i] = fn(data, **kwargs)\n\n    if thread_count is None:\n        results = [None] * len(data)\n        threads = []\n        # for i in range(len(data)):\n        #     t = threading.Thread(name=\'threading_and_return\', target=apply_fn, args=(results, i, data[i], kwargs))\n        for i, d in enumerate(data):\n            t = threading.Thread(name=\'threading_and_return\', target=apply_fn, args=(results, i, d, kwargs))\n            t.start()\n            threads.append(t)\n    else:\n        divs = np.linspace(0, len(data), thread_count + 1)\n        divs = np.round(divs).astype(int)\n        results = [None] * thread_count\n        threads = []\n        for i in range(thread_count):\n            t = threading.Thread(\n                name=\'threading_and_return\', target=apply_fn, args=(results, i, data[divs[i]:divs[i + 1]], kwargs)\n            )\n            t.start()\n            threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    if thread_count is None:\n        try:\n            return np.asarray(results)\n        except Exception:\n            return results\n    else:\n        return np.concatenate(results)\n\n\ndef affine_rotation_matrix(angle=(-20, 20)):\n    """"""Create an affine transform matrix for image rotation.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    angle : int/float or tuple of two int/float\n        Degree to rotate, usually -180 ~ 180.\n            - int/float, a fixed angle.\n            - tuple of 2 floats/ints, randomly sample a value as the angle between these 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    """"""\n    if isinstance(angle, tuple):\n        theta = np.pi / 180 * np.random.uniform(angle[0], angle[1])\n    else:\n        theta = np.pi / 180 * angle\n    rotation_matrix = np.array([[np.cos(theta), np.sin(theta), 0], \\\n                                [-np.sin(theta), np.cos(theta), 0], \\\n                                [0, 0, 1]])\n    return rotation_matrix\n\n\ndef affine_horizontal_flip_matrix(prob=0.5):\n    """"""Create an affine transformation matrix for image horizontal flipping.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    ----------\n    prob : float\n        Probability to flip the image. 1.0 means always flip.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    """"""\n    factor = np.random.uniform(0, 1)\n    if prob >= factor:\n        filp_matrix = np.array([[ -1. , 0., 0. ], \\\n              [ 0., 1., 0. ], \\\n              [ 0., 0., 1. ]])\n        return filp_matrix\n    else:\n        filp_matrix = np.array([[ 1. , 0., 0. ], \\\n              [ 0., 1., 0. ], \\\n              [ 0., 0., 1. ]])\n        return filp_matrix\n\n\ndef affine_vertical_flip_matrix(prob=0.5):\n    """"""Create an affine transformation for image vertical flipping.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    ----------\n    prob : float\n        Probability to flip the image. 1.0 means always flip.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    """"""\n    factor = np.random.uniform(0, 1)\n    if prob >= factor:\n        filp_matrix = np.array([[ 1. , 0., 0. ], \\\n              [ 0., -1., 0. ], \\\n              [ 0., 0., 1. ]])\n        return filp_matrix\n    else:\n        filp_matrix = np.array([[ 1. , 0., 0. ], \\\n              [ 0., 1., 0. ], \\\n              [ 0., 0., 1. ]])\n        return filp_matrix\n\n\ndef affine_shift_matrix(wrg=(-0.1, 0.1), hrg=(-0.1, 0.1), w=200, h=200):\n    """"""Create an affine transform matrix for image shifting.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    wrg : float or tuple of floats\n        Range to shift on width axis, -1 ~ 1.\n            - float, a fixed distance.\n            - tuple of 2 floats, randomly sample a value as the distance between these 2 values.\n    hrg : float or tuple of floats\n        Range to shift on height axis, -1 ~ 1.\n            - float, a fixed distance.\n            - tuple of 2 floats, randomly sample a value as the distance between these 2 values.\n    w, h : int\n        The width and height of the image.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    """"""\n    if isinstance(wrg, tuple):\n        tx = np.random.uniform(wrg[0], wrg[1]) * w\n    else:\n        tx = wrg * w\n    if isinstance(hrg, tuple):\n        ty = np.random.uniform(hrg[0], hrg[1]) * h\n    else:\n        ty = hrg * h\n    shift_matrix = np.array([[1, 0, tx], \\\n                        [0, 1, ty], \\\n                        [0, 0, 1]])\n    return shift_matrix\n\n\ndef affine_shear_matrix(x_shear=(-0.1, 0.1), y_shear=(-0.1, 0.1)):\n    """"""Create affine transform matrix for image shearing.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    shear : tuple of two floats\n        Percentage of shears for width and height directions.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    """"""\n    # if len(shear) != 2:\n    #     raise AssertionError(\n    #         ""shear should be tuple of 2 floats, or you want to use tl.prepro.shear rather than tl.prepro.shear2 ?""\n    #     )\n    # if isinstance(shear, tuple):\n    #     shear = list(shear)\n    # if is_random:\n    #     shear[0] = np.random.uniform(-shear[0], shear[0])\n    #     shear[1] = np.random.uniform(-shear[1], shear[1])\n    if isinstance(x_shear, tuple):\n        x_shear = np.random.uniform(x_shear[0], x_shear[1])\n    if isinstance(y_shear, tuple):\n        y_shear = np.random.uniform(y_shear[0], y_shear[1])\n\n    shear_matrix = np.array([[1, x_shear, 0], \\\n                            [y_shear, 1, 0], \\\n                            [0, 0, 1]])\n    return shear_matrix\n\n\ndef affine_zoom_matrix(zoom_range=(0.8, 1.1)):\n    """"""Create an affine transform matrix for zooming/scaling an image\'s height and width.\n    OpenCV format, x is width.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    zoom_range : float or tuple of 2 floats\n        The zooming/scaling ratio, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between these 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    """"""\n\n    if isinstance(zoom_range, (float, int)):\n        scale = zoom_range\n    elif isinstance(zoom_range, tuple):\n        scale = np.random.uniform(zoom_range[0], zoom_range[1])\n    else:\n        raise Exception(""zoom_range: float or tuple of 2 floats"")\n\n    zoom_matrix = np.array([[scale, 0, 0], \\\n                            [0, scale, 0], \\\n                            [0, 0, 1]])\n    return zoom_matrix\n\n\ndef affine_respective_zoom_matrix(w_range=0.8, h_range=1.1):\n    """"""Get affine transform matrix for zooming/scaling that height and width are changed independently.\n    OpenCV format, x is width.\n\n    Parameters\n    -----------\n    w_range : float or tuple of 2 floats\n        The zooming/scaling ratio of width, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    h_range : float or tuple of 2 floats\n        The zooming/scaling ratio of height, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    """"""\n\n    if isinstance(h_range, (float, int)):\n        zy = h_range\n    elif isinstance(h_range, tuple):\n        zy = np.random.uniform(h_range[0], h_range[1])\n    else:\n        raise Exception(""h_range: float or tuple of 2 floats"")\n\n    if isinstance(w_range, (float, int)):\n        zx = w_range\n    elif isinstance(w_range, tuple):\n        zx = np.random.uniform(w_range[0], w_range[1])\n    else:\n        raise Exception(""w_range: float or tuple of 2 floats"")\n\n    zoom_matrix = np.array([[zx, 0, 0], \\\n                            [0, zy, 0], \\\n                            [0, 0, 1]])\n    return zoom_matrix\n\n\n# affine transform\ndef transform_matrix_offset_center(matrix, y, x):\n    """"""Convert the matrix from Cartesian coordinates (the origin in the middle of image) to Image coordinates (the origin on the top-left of image).\n\n    Parameters\n    ----------\n    matrix : numpy.array\n        Transform matrix.\n    x and y : 2 int\n        Size of image.\n\n    Returns\n    -------\n    numpy.array\n        The transform matrix.\n\n    Examples\n    --------\n    - See ``tl.prepro.rotation``, ``tl.prepro.shear``, ``tl.prepro.zoom``.\n    """"""\n    o_x = (x - 1) / 2.0\n    o_y = (y - 1) / 2.0\n    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n    return transform_matrix\n\n\ndef affine_transform(x, transform_matrix, channel_index=2, fill_mode=\'nearest\', cval=0., order=1):\n    """"""Return transformed images by given an affine matrix in Scipy format (x is height).\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    transform_matrix : numpy.array\n        Transform matrix (offset center), can be generated by ``transform_matrix_offset_center``\n    channel_index : int\n        Index of channel, default 2.\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode=\'constant\'. Default is 0.0\n    order : int\n        The order of interpolation. The order has to be in the range 0-5:\n            - 0 Nearest-neighbor\n            - 1 Bi-linear (default)\n            - 2 Bi-quadratic\n            - 3 Bi-cubic\n            - 4 Bi-quartic\n            - 5 Bi-quintic\n            - `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    >>> M_shear = tl.prepro.affine_shear_matrix(intensity=0.2, is_random=False)\n    >>> M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=0.8)\n    >>> M_combined = M_shear.dot(M_zoom)\n    >>> transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, h, w)\n    >>> result = tl.prepro.affine_transform(image, transform_matrix)\n\n    """"""\n    # transform_matrix = transform_matrix_offset_center()\n    # asdihasid\n    # asd\n\n    x = np.rollaxis(x, channel_index, 0)\n    final_affine_matrix = transform_matrix[:2, :2]\n    final_offset = transform_matrix[:2, 2]\n    channel_images = [\n        ndi.interpolation.affine_transform(\n            x_channel, final_affine_matrix, final_offset, order=order, mode=fill_mode, cval=cval\n        ) for x_channel in x\n    ]\n    x = np.stack(channel_images, axis=0)\n    x = np.rollaxis(x, 0, channel_index + 1)\n    return x\n\n\napply_transform = affine_transform\n\n\ndef affine_transform_cv2(x, transform_matrix, flags=None, border_mode=\'constant\'):\n    """"""Return transformed images by given an affine matrix in OpenCV format (x is width). (Powered by OpenCV2, faster than ``tl.prepro.affine_transform``)\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    transform_matrix : numpy.array\n        A transform matrix, OpenCV format.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Examples\n    --------\n    >>> M_shear = tl.prepro.affine_shear_matrix(intensity=0.2, is_random=False)\n    >>> M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=0.8)\n    >>> M_combined = M_shear.dot(M_zoom)\n    >>> result = tl.prepro.affine_transform_cv2(image, M_combined)\n    """"""\n    rows, cols = x.shape[0], x.shape[1]\n    if flags is None:\n        flags = cv2.INTER_AREA\n    if border_mode is \'constant\':\n        border_mode = cv2.BORDER_CONSTANT\n    elif border_mode is \'replicate\':\n        border_mode = cv2.BORDER_REPLICATE\n    else:\n        raise Exception(""unsupport border_mode, check cv.BORDER_ for more details."")\n    return cv2.warpAffine(x, transform_matrix[0:2,:], \\\n            (cols,rows), flags=flags, borderMode=border_mode)\n\n\ndef affine_transform_keypoints(coords_list, transform_matrix):\n    """"""Transform keypoint coordinates according to a given affine transform matrix.\n    OpenCV format, x is width.\n\n    Note that, for pose estimation task, flipping requires maintaining the left and right body information.\n    We should not flip the left and right body, so please use ``tl.prepro.keypoint_random_flip``.\n\n    Parameters\n    -----------\n    coords_list : list of list of tuple/list\n        The coordinates\n        e.g., the keypoint coordinates of every person in an image.\n    transform_matrix : numpy.array\n        Transform matrix, OpenCV format.\n\n    Examples\n    ---------\n    >>> # 1. get all affine transform matrices\n    >>> M_rotate = tl.prepro.affine_rotation_matrix(angle=20)\n    >>> M_flip = tl.prepro.affine_horizontal_flip_matrix(prob=1)\n    >>> # 2. combine all affine transform matrices to one matrix\n    >>> M_combined = dot(M_flip).dot(M_rotate)\n    >>> # 3. transfrom the matrix from Cartesian coordinate (the origin in the middle of image)\n    >>> # to Image coordinate (the origin on the top-left of image)\n    >>> transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, x=w, y=h)\n    >>> # 4. then we can transfrom the image once for all transformations\n    >>> result = tl.prepro.affine_transform_cv2(image, transform_matrix)  # 76 times faster\n    >>> # 5. transform keypoint coordinates\n    >>> coords = [[(50, 100), (100, 100), (100, 50), (200, 200)], [(250, 50), (200, 50), (200, 100)]]\n    >>> coords_result = tl.prepro.affine_transform_keypoints(coords, transform_matrix)\n    """"""\n    coords_result_list = []\n    for coords in coords_list:\n        coords = np.asarray(coords)\n        coords = coords.transpose([1, 0])\n        coords = np.insert(coords, 2, 1, axis=0)\n        # print(coords)\n        # print(transform_matrix)\n        coords_result = np.matmul(transform_matrix, coords)\n        coords_result = coords_result[0:2, :].transpose([1, 0])\n        coords_result_list.append(coords_result)\n    return coords_result_list\n\n\ndef projective_transform_by_points(\n    x, src, dst, map_args=None, output_shape=None, order=1, mode=\'constant\', cval=0.0, clip=True, preserve_range=False\n):\n    """"""Projective transform by given coordinates, usually 4 coordinates.\n\n    see `scikit-image <http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    src : list or numpy\n        The original coordinates, usually 4 coordinates of (width, height).\n    dst : list or numpy\n        The coordinates after transformation, the number of coordinates is the same with src.\n    map_args : dictionary or None\n        Keyword arguments passed to inverse map.\n    output_shape : tuple of 2 int\n        Shape of the output image generated. By default the shape of the input image is preserved. Note that, even for multi-band images, only rows and columns need to be specified.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5:\n            - 0 Nearest-neighbor\n            - 1 Bi-linear (default)\n            - 2 Bi-quadratic\n            - 3 Bi-cubic\n            - 4 Bi-quartic\n            - 5 Bi-quintic\n    mode : str\n        One of `constant` (default), `edge`, `symmetric`, `reflect` or `wrap`.\n        Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.\n    cval : float\n        Used in conjunction with mode `constant`, the value outside the image boundaries.\n    clip : boolean\n        Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.\n    preserve_range : boolean\n        Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    Assume X is an image from CIFAR-10, i.e. shape == (32, 32, 3)\n\n    >>> src = [[0,0],[0,32],[32,0],[32,32]]     # [w, h]\n    >>> dst = [[10,10],[0,32],[32,0],[32,32]]\n    >>> x = tl.prepro.projective_transform_by_points(X, src, dst)\n\n    References\n    -----------\n    - `scikit-image : geometric transformations <http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html>`__\n    - `scikit-image : examples <http://scikit-image.org/docs/dev/auto_examples/index.html>`__\n\n    """"""\n    if map_args is None:\n        map_args = {}\n    # if type(src) is list:\n    if isinstance(src, list):  # convert to numpy\n        src = np.array(src)\n    # if type(dst) is list:\n    if isinstance(dst, list):\n        dst = np.array(dst)\n    if np.max(x) > 1:  # convert to [0, 1]\n        x = x / 255\n\n    m = transform.ProjectiveTransform()\n    m.estimate(dst, src)\n    warped = transform.warp(\n        x, m, map_args=map_args, output_shape=output_shape, order=order, mode=mode, cval=cval, clip=clip,\n        preserve_range=preserve_range\n    )\n    return warped\n\n\n# rotate\ndef rotation(\n    x, rg=20, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\', cval=0., order=1\n):\n    """"""Rotate an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    rg : int or float\n        Degree to rotate, usually 0 ~ 180.\n    is_random : boolean\n        If True, randomly rotate. Default is False\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode=`constant`. Default is 0.0\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x --> [row, col, 1]\n    >>> x = tl.prepro.rotation(x, rg=40, is_random=False)\n    >>> tl.vis.save_image(x, \'im.png\')\n\n    """"""\n    if is_random:\n        theta = np.pi / 180 * np.random.uniform(-rg, rg)\n    else:\n        theta = np.pi / 180 * rg\n    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0], [np.sin(theta), np.cos(theta), 0], [0, 0, 1]])\n\n    h, w = x.shape[row_index], x.shape[col_index]\n    transform_matrix = transform_matrix_offset_center(rotation_matrix, h, w)\n    x = affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x\n\n\ndef rotation_multi(\n    x, rg=20, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\', cval=0., order=1\n):\n    """"""Rotate multiple images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.rotation``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    Examples\n    --------\n    >>> x, y --> [row, col, 1]  greyscale\n    >>> x, y = tl.prepro.rotation_multi([x, y], rg=90, is_random=False)\n\n    """"""\n    if is_random:\n        theta = np.pi / 180 * np.random.uniform(-rg, rg)\n    else:\n        theta = np.pi / 180 * rg\n    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0], [np.sin(theta), np.cos(theta), 0], [0, 0, 1]])\n\n    h, w = x[0].shape[row_index], x[0].shape[col_index]\n    transform_matrix = transform_matrix_offset_center(rotation_matrix, h, w)\n    results = []\n    for data in x:\n        results.append(affine_transform(data, transform_matrix, channel_index, fill_mode, cval, order))\n    return np.asarray(results)\n\n\n# crop\ndef crop(x, wrg, hrg, is_random=False, row_index=0, col_index=1):\n    """"""Randomly or centrally crop an image.\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    wrg : int\n        Size of width.\n    hrg : int\n        Size of height.\n    is_random : boolean,\n        If True, randomly crop, else central crop. Default is False.\n    row_index: int\n        index of row.\n    col_index: int\n        index of column.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    h, w = x.shape[row_index], x.shape[col_index]\n\n    if (h < hrg) or (w < wrg):\n        raise AssertionError(""The size of cropping should smaller than or equal to the original image"")\n\n    if is_random:\n        h_offset = int(np.random.uniform(0, h - hrg))\n        w_offset = int(np.random.uniform(0, w - wrg))\n        # tl.logging.info(h_offset, w_offset, x[h_offset: hrg+h_offset ,w_offset: wrg+w_offset].shape)\n        return x[h_offset:hrg + h_offset, w_offset:wrg + w_offset]\n    else:  # central crop\n        h_offset = int(np.floor((h - hrg) / 2.))\n        w_offset = int(np.floor((w - wrg) / 2.))\n        h_end = h_offset + hrg\n        w_end = w_offset + wrg\n        return x[h_offset:h_end, w_offset:w_end]\n        # old implementation\n        # h_offset = (h - hrg)/2\n        # w_offset = (w - wrg)/2\n        # tl.logging.info(x[h_offset: h-h_offset ,w_offset: w-w_offset].shape)\n        # return x[h_offset: h-h_offset ,w_offset: w-w_offset]\n        # central crop\n\n\ndef crop_multi(x, wrg, hrg, is_random=False, row_index=0, col_index=1):\n    """"""Randomly or centrally crop multiple images.\n\n    Parameters\n    ----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.crop``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    h, w = x[0].shape[row_index], x[0].shape[col_index]\n\n    if (h < hrg) or (w < wrg):\n        raise AssertionError(""The size of cropping should smaller than or equal to the original image"")\n\n    if is_random:\n        h_offset = int(np.random.uniform(0, h - hrg))\n        w_offset = int(np.random.uniform(0, w - wrg))\n        results = []\n        for data in x:\n            results.append(data[h_offset:hrg + h_offset, w_offset:wrg + w_offset])\n        return np.asarray(results)\n    else:\n        # central crop\n        h_offset = int(np.floor((h - hrg) / 2.))\n        w_offset = int(np.floor((w - wrg) / 2.))\n        results = []\n        for data in x:\n            results.append(data[h_offset:h - h_offset, w_offset:w - w_offset])\n        return np.asarray(results)\n\n\n# flip\ndef flip_axis(x, axis=1, is_random=False):\n    """"""Flip the axis of an image, such as flip left and right, up and down, randomly or non-randomly,\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    axis : int\n        Which axis to flip.\n            - 0, flip up and down\n            - 1, flip left and right\n            - 2, flip channel\n    is_random : boolean\n        If True, randomly flip. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    if is_random:\n        factor = np.random.uniform(-1, 1)\n        if factor > 0:\n            x = np.asarray(x).swapaxes(axis, 0)\n            x = x[::-1, ...]\n            x = x.swapaxes(0, axis)\n            return x\n        else:\n            return x\n    else:\n        x = np.asarray(x).swapaxes(axis, 0)\n        x = x[::-1, ...]\n        x = x.swapaxes(0, axis)\n        return x\n\n\ndef flip_axis_multi(x, axis, is_random=False):\n    """"""Flip the axises of multiple images together, such as flip left and right, up and down, randomly or non-randomly,\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.flip_axis``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    if is_random:\n        factor = np.random.uniform(-1, 1)\n        if factor > 0:\n            # x = np.asarray(x).swapaxes(axis, 0)\n            # x = x[::-1, ...]\n            # x = x.swapaxes(0, axis)\n            # return x\n            results = []\n            for data in x:\n                data = np.asarray(data).swapaxes(axis, 0)\n                data = data[::-1, ...]\n                data = data.swapaxes(0, axis)\n                results.append(data)\n            return np.asarray(results)\n        else:\n            return np.asarray(x)\n    else:\n        # x = np.asarray(x).swapaxes(axis, 0)\n        # x = x[::-1, ...]\n        # x = x.swapaxes(0, axis)\n        # return x\n        results = []\n        for data in x:\n            data = np.asarray(data).swapaxes(axis, 0)\n            data = data[::-1, ...]\n            data = data.swapaxes(0, axis)\n            results.append(data)\n        return np.asarray(results)\n\n\n# shift\ndef shift(\n    x, wrg=0.1, hrg=0.1, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\', cval=0.,\n    order=1\n):\n    """"""Shift an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    wrg : float\n        Percentage of shift in axis x, usually -0.25 ~ 0.25.\n    hrg : float\n        Percentage of shift in axis y, usually -0.25 ~ 0.25.\n    is_random : boolean\n        If True, randomly shift. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode=\'constant\'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    h, w = x.shape[row_index], x.shape[col_index]\n    if is_random:\n        tx = np.random.uniform(-hrg, hrg) * h\n        ty = np.random.uniform(-wrg, wrg) * w\n    else:\n        tx, ty = hrg * h, wrg * w\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n\n    transform_matrix = translation_matrix  # no need to do offset\n    x = affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x\n\n\ndef shift_multi(\n    x, wrg=0.1, hrg=0.1, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\', cval=0.,\n    order=1\n):\n    """"""Shift images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.shift``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    h, w = x[0].shape[row_index], x[0].shape[col_index]\n    if is_random:\n        tx = np.random.uniform(-hrg, hrg) * h\n        ty = np.random.uniform(-wrg, wrg) * w\n    else:\n        tx, ty = hrg * h, wrg * w\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n\n    transform_matrix = translation_matrix  # no need to do offset\n    results = []\n    for data in x:\n        results.append(affine_transform(data, transform_matrix, channel_index, fill_mode, cval, order))\n    return np.asarray(results)\n\n\n# shear\ndef shear(\n    x, intensity=0.1, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\', cval=0., order=1\n):\n    """"""Shear an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    intensity : float\n        Percentage of shear, usually -0.5 ~ 0.5 (is_random==True), 0 ~ 0.5 (is_random==False),\n        you can have a quick try by shear(X, 1).\n    is_random : boolean\n        If True, randomly shear. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode=\'constant\'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `Affine transformation <https://uk.mathworks.com/discovery/affine-transformation.html>`__\n\n    """"""\n    if is_random:\n        shear = np.random.uniform(-intensity, intensity)\n    else:\n        shear = intensity\n    shear_matrix = np.array([[1, -np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n\n    h, w = x.shape[row_index], x.shape[col_index]\n    transform_matrix = transform_matrix_offset_center(shear_matrix, h, w)\n    x = affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x\n\n\ndef shear_multi(\n    x, intensity=0.1, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\', cval=0., order=1\n):\n    """"""Shear images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.shear``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    if is_random:\n        shear = np.random.uniform(-intensity, intensity)\n    else:\n        shear = intensity\n    shear_matrix = np.array([[1, -np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n\n    h, w = x[0].shape[row_index], x[0].shape[col_index]\n    transform_matrix = transform_matrix_offset_center(shear_matrix, h, w)\n    results = []\n    for data in x:\n        results.append(affine_transform(data, transform_matrix, channel_index, fill_mode, cval, order))\n    return np.asarray(results)\n\n\ndef shear2(\n    x, shear=(0.1, 0.1), is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\', cval=0.,\n    order=1\n):\n    """"""Shear an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    shear : tuple of two floats\n        Percentage of shear for height and width direction (0, 1).\n    is_random : boolean\n        If True, randomly shear. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode=\'constant\'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `Affine transformation <https://uk.mathworks.com/discovery/affine-transformation.html>`__\n\n    """"""\n    if len(shear) != 2:\n        raise AssertionError(\n            ""shear should be tuple of 2 floats, or you want to use tl.prepro.shear rather than tl.prepro.shear2 ?""\n        )\n    if isinstance(shear, tuple):\n        shear = list(shear)\n    if is_random:\n        shear[0] = np.random.uniform(-shear[0], shear[0])\n        shear[1] = np.random.uniform(-shear[1], shear[1])\n\n    shear_matrix = np.array([[1, shear[0], 0], \\\n                            [shear[1], 1, 0], \\\n                            [0, 0, 1]])\n\n    h, w = x.shape[row_index], x.shape[col_index]\n    transform_matrix = transform_matrix_offset_center(shear_matrix, h, w)\n    x = affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x\n\n\ndef shear_multi2(\n    x, shear=(0.1, 0.1), is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\', cval=0.,\n    order=1\n):\n    """"""Shear images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.shear2``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    if len(shear) != 2:\n        raise AssertionError(\n            ""shear should be tuple of 2 floats, or you want to use tl.prepro.shear_multi rather than tl.prepro.shear_multi2 ?""\n        )\n    if isinstance(shear, tuple):\n        shear = list(shear)\n    if is_random:\n        shear[0] = np.random.uniform(-shear[0], shear[0])\n        shear[1] = np.random.uniform(-shear[1], shear[1])\n\n    shear_matrix = np.array([[1, shear[0], 0], [shear[1], 1, 0], [0, 0, 1]])\n\n    h, w = x[0].shape[row_index], x[0].shape[col_index]\n    transform_matrix = transform_matrix_offset_center(shear_matrix, h, w)\n    results = []\n    for data in x:\n        results.append(affine_transform(data, transform_matrix, channel_index, fill_mode, cval, order))\n    return np.asarray(results)\n\n\n# swirl\ndef swirl(\n    x, center=None, strength=1, radius=100, rotation=0, output_shape=None, order=1, mode=\'constant\', cval=0, clip=True,\n    preserve_range=False, is_random=False\n):\n    """"""Swirl an image randomly or non-randomly, see `scikit-image swirl API <http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.swirl>`__\n    and `example <http://scikit-image.org/docs/dev/auto_examples/plot_swirl.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    center : tuple or 2 int or None\n        Center coordinate of transformation (optional).\n    strength : float\n        The amount of swirling applied.\n    radius : float\n        The extent of the swirl in pixels. The effect dies out rapidly beyond radius.\n    rotation : float\n        Additional rotation applied to the image, usually [0, 360], relates to center.\n    output_shape : tuple of 2 int or None\n        Shape of the output image generated (height, width). By default the shape of the input image is preserved.\n    order : int, optional\n        The order of the spline interpolation, default is 1. The order has to be in the range 0-5. See skimage.transform.warp for detail.\n    mode : str\n        One of `constant` (default), `edge`, `symmetric` `reflect` and `wrap`.\n        Points outside the boundaries of the input are filled according to the given mode, with `constant` used as the default. Modes match the behaviour of numpy.pad.\n    cval : float\n        Used in conjunction with mode `constant`, the value outside the image boundaries.\n    clip : boolean\n        Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.\n    preserve_range : boolean\n        Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float.\n    is_random : boolean,\n        If True, random swirl. Default is False.\n            - random center = [(0 ~ x.shape[0]), (0 ~ x.shape[1])]\n            - random strength = [0, strength]\n            - random radius = [1e-10, radius]\n            - random rotation = [-rotation, rotation]\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x --> [row, col, 1] greyscale\n    >>> x = tl.prepro.swirl(x, strength=4, radius=100)\n\n    """"""\n    if radius == 0:\n        raise AssertionError(""Invalid radius value"")\n\n    rotation = np.pi / 180 * rotation\n    if is_random:\n        center_h = int(np.random.uniform(0, x.shape[0]))\n        center_w = int(np.random.uniform(0, x.shape[1]))\n        center = (center_h, center_w)\n        strength = np.random.uniform(0, strength)\n        radius = np.random.uniform(1e-10, radius)\n        rotation = np.random.uniform(-rotation, rotation)\n\n    max_v = np.max(x)\n    if max_v > 1:  # Note: the input of this fn should be [-1, 1], rescale is required.\n        x = x / max_v\n    swirled = skimage.transform.swirl(\n        x, center=center, strength=strength, radius=radius, rotation=rotation, output_shape=output_shape, order=order,\n        mode=mode, cval=cval, clip=clip, preserve_range=preserve_range\n    )\n    if max_v > 1:\n        swirled = swirled * max_v\n    return swirled\n\n\ndef swirl_multi(\n    x, center=None, strength=1, radius=100, rotation=0, output_shape=None, order=1, mode=\'constant\', cval=0, clip=True,\n    preserve_range=False, is_random=False\n):\n    """"""Swirl multiple images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.swirl``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    if radius == 0:\n        raise AssertionError(""Invalid radius value"")\n\n    rotation = np.pi / 180 * rotation\n    if is_random:\n        center_h = int(np.random.uniform(0, x[0].shape[0]))\n        center_w = int(np.random.uniform(0, x[0].shape[1]))\n        center = (center_h, center_w)\n        strength = np.random.uniform(0, strength)\n        radius = np.random.uniform(1e-10, radius)\n        rotation = np.random.uniform(-rotation, rotation)\n\n    results = []\n    for data in x:\n        max_v = np.max(data)\n        if max_v > 1:  # Note: the input of this fn should be [-1, 1], rescale is required.\n            data = data / max_v\n        swirled = skimage.transform.swirl(\n            data, center=center, strength=strength, radius=radius, rotation=rotation, output_shape=output_shape,\n            order=order, mode=mode, cval=cval, clip=clip, preserve_range=preserve_range\n        )\n        if max_v > 1:\n            swirled = swirled * max_v\n        results.append(swirled)\n    return np.asarray(results)\n\n\n# elastic_transform\ndef elastic_transform(x, alpha, sigma, mode=""constant"", cval=0, is_random=False):\n    """"""Elastic transformation for image as described in `[Simard2003] <http://deeplearning.cs.cmu.edu/pdfs/Simard.pdf>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        A greyscale image.\n    alpha : float\n        Alpha value for elastic transformation.\n    sigma : float or sequence of float\n        The smaller the sigma, the more transformation. Standard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.\n    mode : str\n        See `scipy.ndimage.filters.gaussian_filter <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.filters.gaussian_filter.html>`__. Default is `constant`.\n    cval : float,\n        Used in conjunction with `mode` of `constant`, the value outside the image boundaries.\n    is_random : boolean\n        Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x = tl.prepro.elastic_transform(x, alpha=x.shape[1]*3, sigma=x.shape[1]*0.07)\n\n    References\n    ------------\n    - `Github <https://gist.github.com/chsasank/4d8f68caf01f041a6453e67fb30f8f5a>`__.\n    - `Kaggle <https://www.kaggle.com/pscion/ultrasound-nerve-segmentation/elastic-transform-for-data-augmentation-0878921a>`__\n\n    """"""\n    if is_random is False:\n        random_state = np.random.RandomState(None)\n    else:\n        random_state = np.random.RandomState(int(time.time()))\n    #\n    is_3d = False\n    if len(x.shape) == 3 and x.shape[-1] == 1:\n        x = x[:, :, 0]\n        is_3d = True\n    elif len(x.shape) == 3 and x.shape[-1] != 1:\n        raise Exception(""Only support greyscale image"")\n\n    if len(x.shape) != 2:\n        raise AssertionError(""input should be grey-scale image"")\n\n    shape = x.shape\n\n    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n\n    x_, y_ = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing=\'ij\')\n    indices = np.reshape(x_ + dx, (-1, 1)), np.reshape(y_ + dy, (-1, 1))\n    if is_3d:\n        return map_coordinates(x, indices, order=1).reshape((shape[0], shape[1], 1))\n    else:\n        return map_coordinates(x, indices, order=1).reshape(shape)\n\n\ndef elastic_transform_multi(x, alpha, sigma, mode=""constant"", cval=0, is_random=False):\n    """"""Elastic transformation for images as described in `[Simard2003] <http://deeplearning.cs.cmu.edu/pdfs/Simard.pdf>`__.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of greyscale images.\n    others : args\n        See ``tl.prepro.elastic_transform``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    if is_random is False:\n        random_state = np.random.RandomState(None)\n    else:\n        random_state = np.random.RandomState(int(time.time()))\n\n    shape = x[0].shape\n    if len(shape) == 3:\n        shape = (shape[0], shape[1])\n    new_shape = random_state.rand(*shape)\n\n    results = []\n    for data in x:\n        is_3d = False\n        if len(data.shape) == 3 and data.shape[-1] == 1:\n            data = data[:, :, 0]\n            is_3d = True\n        elif len(data.shape) == 3 and data.shape[-1] != 1:\n            raise Exception(""Only support greyscale image"")\n\n        if len(data.shape) != 2:\n            raise AssertionError(""input should be grey-scale image"")\n\n        dx = gaussian_filter((new_shape * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n        dy = gaussian_filter((new_shape * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n\n        x_, y_ = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing=\'ij\')\n        indices = np.reshape(x_ + dx, (-1, 1)), np.reshape(y_ + dy, (-1, 1))\n        # tl.logging.info(data.shape)\n        if is_3d:\n            results.append(map_coordinates(data, indices, order=1).reshape((shape[0], shape[1], 1)))\n        else:\n            results.append(map_coordinates(data, indices, order=1).reshape(shape))\n    return np.asarray(results)\n\n\n# zoom\ndef zoom(x, zoom_range=(0.9, 1.1), flags=None, border_mode=\'constant\'):\n    """"""Zooming/Scaling a single image that height and width are changed together.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    zoom_range : float or tuple of 2 floats\n        The zooming/scaling ratio, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    zoom_matrix = affine_zoom_matrix(zoom_range=zoom_range)\n    h, w = x.shape[0], x.shape[1]\n    transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n    x = affine_transform_cv2(x, transform_matrix, flags=flags, border_mode=border_mode)\n    return x\n\n\ndef respective_zoom(x, h_range=(0.9, 1.1), w_range=(0.9, 1.1), flags=None, border_mode=\'constant\'):\n    """"""Zooming/Scaling a single image that height and width are changed independently.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    h_range : float or tuple of 2 floats\n        The zooming/scaling ratio of height, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    w_range : float or tuple of 2 floats\n        The zooming/scaling ratio of width, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    zoom_matrix = affine_respective_zoom_matrix(h_range=h_range, w_range=w_range)\n    h, w = x.shape[0], x.shape[1]\n    transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n    x = affine_transform_cv2(\n        x, transform_matrix, flags=flags, border_mode=border_mode\n    )  #affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x\n\n\ndef zoom_multi(x, zoom_range=(0.9, 1.1), flags=None, border_mode=\'constant\'):\n    """"""Zoom in and out of images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.zoom``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n\n    zoom_matrix = affine_zoom_matrix(zoom_range=zoom_range)\n    results = []\n    for img in x:\n        h, w = x.shape[0], x.shape[1]\n        transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n        results.append(affine_transform_cv2(x, transform_matrix, flags=flags, border_mode=border_mode))\n    return results\n\n\n# image = tf.image.random_brightness(image, max_delta=32. / 255.)\n# image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n# image = tf.image.random_hue(image, max_delta=0.032)\n# image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n\n\ndef brightness(x, gamma=1, gain=1, is_random=False):\n    """"""Change the brightness of a single image, randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    gamma : float\n        Non negative real number. Default value is 1.\n            - Small than 1 means brighter.\n            - If `is_random` is True, gamma in a range of (1-gamma, 1+gamma).\n    gain : float\n        The constant multiplier. Default value is 1.\n    is_random : boolean\n        If True, randomly change brightness. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `skimage.exposure.adjust_gamma <http://scikit-image.org/docs/dev/api/skimage.exposure.html>`__\n    - `chinese blog <http://www.cnblogs.com/denny402/p/5124402.html>`__\n\n    """"""\n    if is_random:\n        gamma = np.random.uniform(1 - gamma, 1 + gamma)\n    x = exposure.adjust_gamma(x, gamma, gain)\n    return x\n\n\ndef brightness_multi(x, gamma=1, gain=1, is_random=False):\n    """"""Change the brightness of multiply images, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpyarray\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.brightness``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    if is_random:\n        gamma = np.random.uniform(1 - gamma, 1 + gamma)\n\n    results = []\n    for data in x:\n        results.append(exposure.adjust_gamma(data, gamma, gain))\n    return np.asarray(results)\n\n\ndef illumination(x, gamma=1., contrast=1., saturation=1., is_random=False):\n    """"""Perform illumination augmentation for a single image, randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    gamma : float\n        Change brightness (the same with ``tl.prepro.brightness``)\n            - if is_random=False, one float number, small than one means brighter, greater than one means darker.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    contrast : float\n        Change contrast.\n            - if is_random=False, one float number, small than one means blur.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    saturation : float\n        Change saturation.\n            - if is_random=False, one float number, small than one means unsaturation.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    is_random : boolean\n        If True, randomly change illumination. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    Random\n\n    >>> x = tl.prepro.illumination(x, gamma=(0.5, 5.0), contrast=(0.3, 1.0), saturation=(0.7, 1.0), is_random=True)\n\n    Non-random\n\n    >>> x = tl.prepro.illumination(x, 0.5, 0.6, 0.8, is_random=False)\n\n    """"""\n    if is_random:\n        if not (len(gamma) == len(contrast) == len(saturation) == 2):\n            raise AssertionError(""if is_random = True, the arguments are (min, max)"")\n\n        ## random change brightness  # small --> brighter\n        illum_settings = np.random.randint(0, 3)  # 0-brighter, 1-darker, 2 keep normal\n\n        if illum_settings == 0:  # brighter\n            gamma = np.random.uniform(gamma[0], 1.0)  # (.5, 1.0)\n        elif illum_settings == 1:  # darker\n            gamma = np.random.uniform(1.0, gamma[1])  # (1.0, 5.0)\n        else:\n            gamma = 1\n        im_ = brightness(x, gamma=gamma, gain=1, is_random=False)\n\n        # tl.logging.info(""using contrast and saturation"")\n        image = PIL.Image.fromarray(im_)  # array -> PIL\n        contrast_adjust = PIL.ImageEnhance.Contrast(image)\n        image = contrast_adjust.enhance(np.random.uniform(contrast[0], contrast[1]))  #0.3,0.9))\n\n        saturation_adjust = PIL.ImageEnhance.Color(image)\n        image = saturation_adjust.enhance(np.random.uniform(saturation[0], saturation[1]))  # (0.7,1.0))\n        im_ = np.array(image)  # PIL -> array\n    else:\n        im_ = brightness(x, gamma=gamma, gain=1, is_random=False)\n        image = PIL.Image.fromarray(im_)  # array -> PIL\n        contrast_adjust = PIL.ImageEnhance.Contrast(image)\n        image = contrast_adjust.enhance(contrast)\n\n        saturation_adjust = PIL.ImageEnhance.Color(image)\n        image = saturation_adjust.enhance(saturation)\n        im_ = np.array(image)  # PIL -> array\n    return np.asarray(im_)\n\n\ndef rgb_to_hsv(rgb):\n    """"""Input RGB image [0~255] return HSV image [0~1].\n\n    Parameters\n    ------------\n    rgb : numpy.array\n        An image with values between 0 and 255.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    # Translated from source of colorsys.rgb_to_hsv\n    # r,g,b should be a numpy arrays with values between 0 and 255\n    # rgb_to_hsv returns an array of floats between 0.0 and 1.0.\n    rgb = rgb.astype(\'float\')\n    hsv = np.zeros_like(rgb)\n    # in case an RGBA array was passed, just copy the A channel\n    hsv[..., 3:] = rgb[..., 3:]\n    r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]\n    maxc = np.max(rgb[..., :3], axis=-1)\n    minc = np.min(rgb[..., :3], axis=-1)\n    hsv[..., 2] = maxc\n    mask = maxc != minc\n    hsv[mask, 1] = (maxc - minc)[mask] / maxc[mask]\n    rc = np.zeros_like(r)\n    gc = np.zeros_like(g)\n    bc = np.zeros_like(b)\n    rc[mask] = (maxc - r)[mask] / (maxc - minc)[mask]\n    gc[mask] = (maxc - g)[mask] / (maxc - minc)[mask]\n    bc[mask] = (maxc - b)[mask] / (maxc - minc)[mask]\n    hsv[..., 0] = np.select([r == maxc, g == maxc], [bc - gc, 2.0 + rc - bc], default=4.0 + gc - rc)\n    hsv[..., 0] = (hsv[..., 0] / 6.0) % 1.0\n    return hsv\n\n\ndef hsv_to_rgb(hsv):\n    """"""Input HSV image [0~1] return RGB image [0~255].\n\n    Parameters\n    -------------\n    hsv : numpy.array\n        An image with values between 0.0 and 1.0\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n    """"""\n    # Translated from source of colorsys.hsv_to_rgb\n    # h,s should be a numpy arrays with values between 0.0 and 1.0\n    # v should be a numpy array with values between 0.0 and 255.0\n    # hsv_to_rgb returns an array of uints between 0 and 255.\n    rgb = np.empty_like(hsv)\n    rgb[..., 3:] = hsv[..., 3:]\n    h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n    i = (h * 6.0).astype(\'uint8\')\n    f = (h * 6.0) - i\n    p = v * (1.0 - s)\n    q = v * (1.0 - s * f)\n    t = v * (1.0 - s * (1.0 - f))\n    i = i % 6\n    conditions = [s == 0.0, i == 1, i == 2, i == 3, i == 4, i == 5]\n    rgb[..., 0] = np.select(conditions, [v, q, p, p, t, v], default=v)\n    rgb[..., 1] = np.select(conditions, [v, v, v, q, p, p], default=t)\n    rgb[..., 2] = np.select(conditions, [v, p, t, v, v, q], default=p)\n    return rgb.astype(\'uint8\')\n\n\ndef adjust_hue(im, hout=0.66, is_offset=True, is_clip=True, is_random=False):\n    """"""Adjust hue of an RGB image.\n\n    This is a convenience method that converts an RGB image to float representation, converts it to HSV, add an offset to the hue channel, converts back to RGB and then back to the original data type.\n    For TF, see `tf.image.adjust_hue <https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue>`__.and `tf.image.random_hue <https://www.tensorflow.org/api_docs/python/tf/image/random_hue>`__.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with values between 0 and 255.\n    hout : float\n        The scale value for adjusting hue.\n            - If is_offset is False, set all hue values to this value. 0 is red; 0.33 is green; 0.66 is blue.\n            - If is_offset is True, add this value as the offset to the hue channel.\n    is_offset : boolean\n        Whether `hout` is added on HSV as offset or not. Default is True.\n    is_clip : boolean\n        If HSV value smaller than 0, set to 0. Default is True.\n    is_random : boolean\n        If True, randomly change hue. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    Random, add a random value between -0.2 and 0.2 as the offset to every hue values.\n\n    >>> im_hue = tl.prepro.adjust_hue(image, hout=0.2, is_offset=True, is_random=False)\n\n    Non-random, make all hue to green.\n\n    >>> im_green = tl.prepro.adjust_hue(image, hout=0.66, is_offset=False, is_random=False)\n\n    References\n    -----------\n    - `tf.image.random_hue <https://www.tensorflow.org/api_docs/python/tf/image/random_hue>`__.\n    - `tf.image.adjust_hue <https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue>`__.\n    - `StackOverflow: Changing image hue with python PIL <https://stackoverflow.com/questions/7274221/changing-image-hue-with-python-pil>`__.\n\n    """"""\n    hsv = rgb_to_hsv(im)\n    if is_random:\n        hout = np.random.uniform(-hout, hout)\n\n    if is_offset:\n        hsv[..., 0] += hout\n    else:\n        hsv[..., 0] = hout\n\n    if is_clip:\n        hsv[..., 0] = np.clip(hsv[..., 0], 0, np.inf)  # Hao : can remove green dots\n\n    rgb = hsv_to_rgb(hsv)\n    return rgb\n\n\n# # contrast\n# def constant(x, cutoff=0.5, gain=10, inv=False, is_random=False):\n#     # TODO\n#     x = exposure.adjust_sigmoid(x, cutoff=cutoff, gain=gain, inv=inv)\n#     return x\n#\n# def constant_multi():\n#     #TODO\n#     pass\n\n\ndef imresize(x, size=None, interp=\'bicubic\', mode=None):\n    """"""Resize an image by given output size and method.\n\n    Warning, this function will rescale the value to [0, 255].\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    size : list of 2 int or None\n        For height and width.\n    interp : str\n        Interpolation method for re-sizing (`nearest`, `lanczos`, `bilinear`, `bicubic` (default) or `cubic`).\n    mode : str\n        The PIL image mode (`P`, `L`, etc.) to convert image before resizing.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    ------------\n    - `scipy.misc.imresize <https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imresize.html>`__\n\n    """"""\n    if size is None:\n        size = [100, 100]\n\n    if x.shape[-1] == 1:\n        # greyscale\n        x = scipy.misc.imresize(x[:, :, 0], size, interp=interp, mode=mode)\n        return x[:, :, np.newaxis]\n    else:\n        # rgb, bgr, rgba\n        return scipy.misc.imresize(x, size, interp=interp, mode=mode)\n\n\n# value scale\ndef pixel_value_scale(im, val=0.9, clip=None, is_random=False):\n    """"""Scales each value in the pixels of the image.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image.\n    val : float\n        The scale value for changing pixel value.\n            - If is_random=False, multiply this value with all pixels.\n            - If is_random=True, multiply a value between [1-val, 1+val] with all pixels.\n    clip : tuple of 2 numbers\n        The minimum and maximum value.\n    is_random : boolean\n        If True, see ``val``.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ----------\n    Random\n\n    >>> im = pixel_value_scale(im, 0.1, [0, 255], is_random=True)\n\n    Non-random\n\n    >>> im = pixel_value_scale(im, 0.9, [0, 255], is_random=False)\n\n    """"""\n\n    clip = clip if clip is not None else (-np.inf, np.inf)\n\n    if is_random:\n        scale = 1 + np.random.uniform(-val, val)\n        im = im * scale\n    else:\n        im = im * val\n\n    if len(clip) == 2:\n        im = np.clip(im, clip[0], clip[1])\n    else:\n        raise Exception(""clip : tuple of 2 numbers"")\n\n    return im\n\n\n# normailization\ndef samplewise_norm(\n    x, rescale=None, samplewise_center=False, samplewise_std_normalization=False, channel_index=2, epsilon=1e-7\n):\n    """"""Normalize an image by rescale, samplewise centering and samplewise centering in order.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    rescale : float\n        Rescaling factor. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (before applying any other transformation)\n    samplewise_center : boolean\n        If True, set each sample mean to 0.\n    samplewise_std_normalization : boolean\n        If True, divide each input by its std.\n    epsilon : float\n        A small position value for dividing standard deviation.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    >>> x = samplewise_norm(x, samplewise_center=True, samplewise_std_normalization=True)\n    >>> print(x.shape, np.mean(x), np.std(x))\n    (160, 176, 1), 0.0, 1.0\n\n    Notes\n    ------\n    When samplewise_center and samplewise_std_normalization are True.\n    - For greyscale image, every pixels are subtracted and divided by the mean and std of whole image.\n    - For RGB image, every pixels are subtracted and divided by the mean and std of this pixel i.e. the mean and std of a pixel is 0 and 1.\n\n    """"""\n    if rescale:\n        x *= rescale\n\n    if x.shape[channel_index] == 1:\n        # greyscale\n        if samplewise_center:\n            x = x - np.mean(x)\n        if samplewise_std_normalization:\n            x = x / np.std(x)\n        return x\n    elif x.shape[channel_index] == 3:\n        # rgb\n        if samplewise_center:\n            x = x - np.mean(x, axis=channel_index, keepdims=True)\n        if samplewise_std_normalization:\n            x = x / (np.std(x, axis=channel_index, keepdims=True) + epsilon)\n        return x\n    else:\n        raise Exception(""Unsupported channels %d"" % x.shape[channel_index])\n\n\ndef featurewise_norm(x, mean=None, std=None, epsilon=1e-7):\n    """"""Normalize every pixels by the same given mean and std, which are usually\n    compute from all examples.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    mean : float\n        Value for subtraction.\n    std : float\n        Value for division.\n    epsilon : float\n        A small position value for dividing standard deviation.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    if mean:\n        x = x - mean\n    if std:\n        x = x / (std + epsilon)\n    return x\n\n\n# whitening\ndef get_zca_whitening_principal_components_img(X):\n    """"""Return the ZCA whitening principal components matrix.\n\n    Parameters\n    -----------\n    x : numpy.array\n        Batch of images with dimension of [n_example, row, col, channel] (default).\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    flatX = np.reshape(X, (X.shape[0], X.shape[1] * X.shape[2] * X.shape[3]))\n    tl.logging.info(""zca : computing sigma .."")\n    sigma = np.dot(flatX.T, flatX) / flatX.shape[0]\n    tl.logging.info(""zca : computing U, S and V .."")\n    U, S, _ = linalg.svd(sigma)  # USV\n    tl.logging.info(""zca : computing principal components .."")\n    principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + 10e-7))), U.T)\n    return principal_components\n\n\ndef zca_whitening(x, principal_components):\n    """"""Apply ZCA whitening on an image by given principal components matrix.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    principal_components : matrix\n        Matrix from ``get_zca_whitening_principal_components_img``.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    flatx = np.reshape(x, (x.size))\n    # tl.logging.info(principal_components.shape, x.shape)  # ((28160, 28160), (160, 176, 1))\n    # flatx = np.reshape(x, (x.shape))\n    # flatx = np.reshape(x, (x.shape[0], ))\n    # tl.logging.info(flatx.shape)  # (160, 176, 1)\n    whitex = np.dot(flatx, principal_components)\n    x = np.reshape(whitex, (x.shape[0], x.shape[1], x.shape[2]))\n    return x\n\n\n# developing\n# def barrel_transform(x, intensity):\n#     # https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py\n#     # TODO\n#     pass\n#\n# def barrel_transform_multi(x, intensity):\n#     # https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py\n#     # TODO\n#     pass\n\n\n# channel shift\ndef channel_shift(x, intensity, is_random=False, channel_index=2):\n    """"""Shift the channels of an image, randomly or non-randomly, see `numpy.rollaxis <https://docs.scipy.org/doc/numpy/reference/generated/numpy.rollaxis.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    intensity : float\n        Intensity of shifting.\n    is_random : boolean\n        If True, randomly shift. Default is False.\n    channel_index : int\n        Index of channel. Default is 2.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    if is_random:\n        factor = np.random.uniform(-intensity, intensity)\n    else:\n        factor = intensity\n    x = np.rollaxis(x, channel_index, 0)\n    min_x, max_x = np.min(x), np.max(x)\n    channel_images = [np.clip(x_channel + factor, min_x, max_x) for x_channel in x]\n    x = np.stack(channel_images, axis=0)\n    x = np.rollaxis(x, 0, channel_index + 1)\n    return x\n    # x = np.rollaxis(x, channel_index, 0)\n    # min_x, max_x = np.min(x), np.max(x)\n    # channel_images = [np.clip(x_channel + np.random.uniform(-intensity, intensity), min_x, max_x)\n    #                   for x_channel in x]\n    # x = np.stack(channel_images, axis=0)\n    # x = np.rollaxis(x, 0, channel_index+1)\n    # return x\n\n\ndef channel_shift_multi(x, intensity, is_random=False, channel_index=2):\n    """"""Shift the channels of images with the same arguments, randomly or non-randomly, see `numpy.rollaxis <https://docs.scipy.org/doc/numpy/reference/generated/numpy.rollaxis.html>`__.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.channel_shift``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    """"""\n    if is_random:\n        factor = np.random.uniform(-intensity, intensity)\n    else:\n        factor = intensity\n\n    results = []\n    for data in x:\n        data = np.rollaxis(data, channel_index, 0)\n        min_x, max_x = np.min(data), np.max(data)\n        channel_images = [np.clip(x_channel + factor, min_x, max_x) for x_channel in x]\n        data = np.stack(channel_images, axis=0)\n        data = np.rollaxis(x, 0, channel_index + 1)\n        results.append(data)\n    return np.asarray(results)\n\n\n# noise\ndef drop(x, keep=0.5):\n    """"""Randomly set some pixels to zero by a given keeping probability.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] or [row, col].\n    keep : float\n        The keeping probability (0, 1), the lower more values will be set to zero.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    """"""\n    if len(x.shape) == 3:\n        if x.shape[-1] == 3:  # color\n            img_size = x.shape\n            mask = np.random.binomial(n=1, p=keep, size=x.shape[:-1])\n            for i in range(3):\n                x[:, :, i] = np.multiply(x[:, :, i], mask)\n        elif x.shape[-1] == 1:  # greyscale image\n            img_size = x.shape\n            x = np.multiply(x, np.random.binomial(n=1, p=keep, size=img_size))\n        else:\n            raise Exception(""Unsupported shape {}"".format(x.shape))\n    elif len(x.shape) == 2 or 1:  # greyscale matrix (image) or vector\n        img_size = x.shape\n        x = np.multiply(x, np.random.binomial(n=1, p=keep, size=img_size))\n    else:\n        raise Exception(""Unsupported shape {}"".format(x.shape))\n    return x\n\n\n# x = np.asarray([[1,2,3,4,5,6,7,8,9,10],[1,2,3,4,5,6,7,8,9,10]])\n# x = np.asarray([x,x,x,x,x,x])\n# x.shape = 10, 4, 3\n# tl.logging.info(x)\n# # exit()\n# tl.logging.info(x.shape)\n# # exit()\n# tl.logging.info(drop(x, keep=1.))\n# exit()\n\n\n# Numpy and PIL\ndef array_to_img(x, dim_ordering=(0, 1, 2), scale=True):\n    """"""Converts a numpy array to PIL image object (uint8 format).\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of 3 and channels of 1 or 3.\n    dim_ordering : tuple of 3 int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    scale : boolean\n        If True, converts image to [0, 255] from any range of value like [-1, 2]. Default is True.\n\n    Returns\n    -------\n    PIL.image\n        An image.\n\n    References\n    -----------\n    `PIL Image.fromarray <http://pillow.readthedocs.io/en/3.1.x/reference/Image.html?highlight=fromarray>`__\n\n    """"""\n    # if dim_ordering == \'default\':\n    #     dim_ordering = K.image_dim_ordering()\n    # if dim_ordering == \'th\':  # theano\n    #     x = x.transpose(1, 2, 0)\n\n    x = x.transpose(dim_ordering)\n\n    if scale:\n        x += max(-np.min(x), 0)\n        x_max = np.max(x)\n        if x_max != 0:\n            # tl.logging.info(x_max)\n            # x /= x_max\n            x = x / x_max\n        x *= 255\n\n    if x.shape[2] == 3:\n        # RGB\n        return PIL.Image.fromarray(x.astype(\'uint8\'), \'RGB\')\n\n    elif x.shape[2] == 1:\n        # grayscale\n        return PIL.Image.fromarray(x[:, :, 0].astype(\'uint8\'), \'L\')\n\n    else:\n        raise Exception(\'Unsupported channel number: \', x.shape[2])\n\n\ndef find_contours(x, level=0.8, fully_connected=\'low\', positive_orientation=\'low\'):\n    """"""Find iso-valued contours in a 2D array for a given level value, returns list of (n, 2)-ndarrays\n    see `skimage.measure.find_contours <http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.find_contours>`__.\n\n    Parameters\n    ------------\n    x : 2D ndarray of double.\n        Input data in which to find contours.\n    level : float\n        Value along which to find contours in the array.\n    fully_connected : str\n        Either `low` or `high`. Indicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)\n    positive_orientation : str\n        Either `low` or `high`. Indicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If `low` then contours will wind counter-clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour.\n\n    Returns\n    --------\n    list of (n,2)-ndarrays\n        Each contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour.\n\n    """"""\n    return skimage.measure.find_contours(\n        x, level, fully_connected=fully_connected, positive_orientation=positive_orientation\n    )\n\n\ndef pt2map(list_points=None, size=(100, 100), val=1):\n    """"""Inputs a list of points, return a 2D image.\n\n    Parameters\n    --------------\n    list_points : list of 2 int\n        [[x, y], [x, y]..] for point coordinates.\n    size : tuple of 2 int\n        (w, h) for output size.\n    val : float or int\n        For the contour value.\n\n    Returns\n    -------\n    numpy.array\n        An image.\n\n    """"""\n    if list_points is None:\n        raise Exception(""list_points : list of 2 int"")\n    i_m = np.zeros(size)\n    if len(list_points) == 0:\n        return i_m\n    for xx in list_points:\n        for x in xx:\n            # tl.logging.info(x)\n            i_m[int(np.round(x[0]))][int(np.round(x[1]))] = val\n    return i_m\n\n\ndef binary_dilation(x, radius=3):\n    """"""Return fast binary morphological dilation of an image.\n    see `skimage.morphology.binary_dilation <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.binary_dilation>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A binary image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed binary image.\n\n    """"""\n    mask = disk(radius)\n    x = _binary_dilation(x, selem=mask)\n\n    return x\n\n\ndef dilation(x, radius=3):\n    """"""Return greyscale morphological dilation of an image,\n    see `skimage.morphology.dilation <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.dilation>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        An greyscale image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed greyscale image.\n\n    """"""\n    mask = disk(radius)\n    x = dilation(x, selem=mask)\n\n    return x\n\n\ndef binary_erosion(x, radius=3):\n    """"""Return binary morphological erosion of an image,\n    see `skimage.morphology.binary_erosion <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.binary_erosion>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A binary image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed binary image.\n\n    """"""\n    mask = disk(radius)\n    x = _binary_erosion(x, selem=mask)\n    return x\n\n\ndef erosion(x, radius=3):\n    """"""Return greyscale morphological erosion of an image,\n    see `skimage.morphology.erosion <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.erosion>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A greyscale image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed greyscale image.\n\n    """"""\n    mask = disk(radius)\n    x = _erosion(x, selem=mask)\n    return x\n\n\ndef obj_box_coords_rescale(coords=None, shape=None):\n    """"""Scale down a list of coordinates from pixel unit to the ratio of image size i.e. in the range of [0, 1].\n\n    Parameters\n    ------------\n    coords : list of list of 4 ints or None\n        For coordinates of more than one images .e.g.[[x, y, w, h], [x, y, w, h], ...].\n    shape : list of 2 int or None\n        \xe3\x80\x90height, width].\n\n    Returns\n    -------\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n\n    Examples\n    ---------\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50], [10, 10, 20, 20]], shape=[100, 100])\n    >>> print(coords)\n      [[0.3, 0.4, 0.5, 0.5], [0.1, 0.1, 0.2, 0.2]]\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[50, 100])\n    >>> print(coords)\n      [[0.3, 0.8, 0.5, 1.0]]\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[100, 200])\n    >>> print(coords)\n      [[0.15, 0.4, 0.25, 0.5]]\n\n    Returns\n    -------\n    list of 4 numbers\n        New coordinates.\n\n    """"""\n    if coords is None:\n        coords = []\n    if shape is None:\n        shape = [100, 200]\n\n    imh, imw = shape[0], shape[1]\n    imh = imh * 1.0  # * 1.0 for python2 : force division to be float point\n    imw = imw * 1.0\n    coords_new = list()\n    for coord in coords:\n\n        if len(coord) != 4:\n            raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n        x = coord[0] / imw\n        y = coord[1] / imh\n        w = coord[2] / imw\n        h = coord[3] / imh\n        coords_new.append([x, y, w, h])\n    return coords_new\n\n\ndef obj_box_coord_rescale(coord=None, shape=None):\n    """"""Scale down one coordinates from pixel unit to the ratio of image size i.e. in the range of [0, 1].\n    It is the reverse process of ``obj_box_coord_scale_to_pixelunit``.\n\n    Parameters\n    ------------\n    coords : list of 4 int or None\n        One coordinates of one image e.g. [x, y, w, h].\n    shape : list of 2 int or None\n        For [height, width].\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> coord = tl.prepro.obj_box_coord_rescale(coord=[30, 40, 50, 50], shape=[100, 100])\n      [0.3, 0.4, 0.5, 0.5]\n\n    """"""\n    if coord is None:\n        coord = []\n    if shape is None:\n        shape = [100, 200]\n\n    return obj_box_coords_rescale(coords=[coord], shape=shape)[0]\n\n\ndef obj_box_coord_scale_to_pixelunit(coord, shape=None):\n    """"""Convert one coordinate [x, y, w (or x2), h (or y2)] in ratio format to image coordinate format.\n    It is the reverse process of ``obj_box_coord_rescale``.\n\n    Parameters\n    -----------\n    coord : list of 4 float\n        One coordinate of one image [x, y, w (or x2), h (or y2)] in ratio format, i.e value range [0~1].\n    shape : tuple of 2 or None\n        For [height, width].\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> x, y, x2, y2 = tl.prepro.obj_box_coord_scale_to_pixelunit([0.2, 0.3, 0.5, 0.7], shape=(100, 200, 3))\n      [40, 30, 100, 70]\n\n    """"""\n    if shape is None:\n        shape = [100, 100]\n\n    imh, imw = shape[0:2]\n    x = int(coord[0] * imw)\n    x2 = int(coord[2] * imw)\n    y = int(coord[1] * imh)\n    y2 = int(coord[3] * imh)\n    return [x, y, x2, y2]\n\n\n# coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50], [10, 10, 20, 20]], shape=[100, 100])\n# tl.logging.info(coords)\n#     #   [[0.3, 0.4, 0.5, 0.5], [0.1, 0.1, 0.2, 0.2]]\n# coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[50, 100])\n# tl.logging.info(coords)\n#     #   [[0.3, 0.8, 0.5, 1.0]]\n# coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[100, 200])\n# tl.logging.info(coords)\n#     #   [[0.15, 0.4, 0.25, 0.5]]\n# exit()\n\n\ndef obj_box_coord_centroid_to_upleft_butright(coord, to_int=False):\n    """"""Convert one coordinate [x_center, y_center, w, h] to [x1, y1, x2, y2] in up-left and botton-right format.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n    to_int : boolean\n        Whether to convert output as integer.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> coord = obj_box_coord_centroid_to_upleft_butright([30, 40, 20, 20])\n      [20, 30, 40, 50]\n\n    """"""\n    if len(coord) != 4:\n        raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n    x_center, y_center, w, h = coord\n    x = x_center - w / 2.\n    y = y_center - h / 2.\n    x2 = x + w\n    y2 = y + h\n    if to_int:\n        return [int(x), int(y), int(x2), int(y2)]\n    else:\n        return [x, y, x2, y2]\n\n\n# coord = obj_box_coord_centroid_to_upleft_butright([30, 40, 20, 20])\n# tl.logging.info(coord)    [20, 30, 40, 50]\n# exit()\n\n\ndef obj_box_coord_upleft_butright_to_centroid(coord):\n    """"""Convert one coordinate [x1, y1, x2, y2] to [x_center, y_center, w, h].\n    It is the reverse process of ``obj_box_coord_centroid_to_upleft_butright``.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    """"""\n    if len(coord) != 4:\n        raise AssertionError(""coordinate should be 4 values : [x1, y1, x2, y2]"")\n    x1, y1, x2, y2 = coord\n    w = x2 - x1\n    h = y2 - y1\n    x_c = x1 + w / 2.\n    y_c = y1 + h / 2.\n    return [x_c, y_c, w, h]\n\n\ndef obj_box_coord_centroid_to_upleft(coord):\n    """"""Convert one coordinate [x_center, y_center, w, h] to [x, y, w, h].\n    It is the reverse process of ``obj_box_coord_upleft_to_centroid``.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    """"""\n    if len(coord) != 4:\n        raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n    x_center, y_center, w, h = coord\n    x = x_center - w / 2.\n    y = y_center - h / 2.\n    return [x, y, w, h]\n\n\ndef obj_box_coord_upleft_to_centroid(coord):\n    """"""Convert one coordinate [x, y, w, h] to [x_center, y_center, w, h].\n    It is the reverse process of ``obj_box_coord_centroid_to_upleft``.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    """"""\n    if len(coord) != 4:\n        raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n    x, y, w, h = coord\n    x_center = x + w / 2.\n    y_center = y + h / 2.\n    return [x_center, y_center, w, h]\n\n\ndef parse_darknet_ann_str_to_list(annotations):\n    r""""""Input string format of class, x, y, w, h, return list of list format.\n\n    Parameters\n    -----------\n    annotations : str\n        The annotations in darkent format ""class, x, y, w, h ...."" seperated by ""\\\\n"".\n\n    Returns\n    -------\n    list of list of 4 numbers\n        List of bounding box.\n\n    """"""\n    annotations = annotations.split(""\\n"")\n    ann = []\n    for a in annotations:\n        a = a.split()\n        if len(a) == 5:\n            for i, _v in enumerate(a):\n                if i == 0:\n                    a[i] = int(a[i])\n                else:\n                    a[i] = float(a[i])\n            ann.append(a)\n    return ann\n\n\ndef parse_darknet_ann_list_to_cls_box(annotations):\n    """"""Parse darknet annotation format into two lists for class and bounding box.\n\n    Input list of [[class, x, y, w, h], ...], return two list of [class ...] and [[x, y, w, h], ...].\n\n    Parameters\n    ------------\n    annotations : list of list\n        A list of class and bounding boxes of images e.g. [[class, x, y, w, h], ...]\n\n    Returns\n    -------\n    list of int\n        List of class labels.\n\n    list of list of 4 numbers\n        List of bounding box.\n\n    """"""\n    class_list = []\n    bbox_list = []\n    for ann in annotations:\n        class_list.append(ann[0])\n        bbox_list.append(ann[1:])\n    return class_list, bbox_list\n\n\ndef obj_box_horizontal_flip(im, coords=None, is_rescale=False, is_center=False, is_random=False):\n    """"""Left-right flip the image and coordinates for object detection.\n\n    Parameters\n    ----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...].\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    is_random : boolean\n        If True, randomly flip. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    Examples\n    --------\n    >>> im = np.zeros([80, 100])    # as an image with shape width=100, height=80\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[0.2, 0.4, 0.3, 0.3], [0.1, 0.5, 0.2, 0.3]], is_rescale=True, is_center=True, is_random=False)\n    >>> print(coords)\n      [[0.8, 0.4, 0.3, 0.3], [0.9, 0.5, 0.2, 0.3]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[0.2, 0.4, 0.3, 0.3]], is_rescale=True, is_center=False, is_random=False)\n    >>> print(coords)\n      [[0.5, 0.4, 0.3, 0.3]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[20, 40, 30, 30]], is_rescale=False, is_center=True, is_random=False)\n    >>> print(coords)\n      [[80, 40, 30, 30]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[20, 40, 30, 30]], is_rescale=False, is_center=False, is_random=False)\n    >>> print(coords)\n      [[50, 40, 30, 30]]\n\n    """"""\n    if coords is None:\n        coords = []\n\n    def _flip(im, coords):\n        im = flip_axis(im, axis=1, is_random=False)\n        coords_new = list()\n\n        for coord in coords:\n\n            if len(coord) != 4:\n                raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n            if is_rescale:\n                if is_center:\n                    # x_center\' = 1 - x\n                    x = 1. - coord[0]\n                else:\n                    # x_center\' = 1 - x - w\n                    x = 1. - coord[0] - coord[2]\n            else:\n                if is_center:\n                    # x\' = im.width - x\n                    x = im.shape[1] - coord[0]\n                else:\n                    # x\' = im.width - x - w\n                    x = im.shape[1] - coord[0] - coord[2]\n            coords_new.append([x, coord[1], coord[2], coord[3]])\n        return im, coords_new\n\n    if is_random:\n        factor = np.random.uniform(-1, 1)\n        if factor > 0:\n            return _flip(im, coords)\n        else:\n            return im, coords\n    else:\n        return _flip(im, coords)\n\n\nobj_box_left_right_flip = obj_box_horizontal_flip\n\n# im = np.zeros([80, 100])    # as an image with shape width=100, height=80\n# im, coords = obj_box_left_right_flip(im, coords=[[0.2, 0.4, 0.3, 0.3], [0.1, 0.5, 0.2, 0.3]], is_rescale=True, is_center=True, is_random=False)\n# tl.logging.info(coords)\n# #   [[0.8, 0.4, 0.3, 0.3], [0.9, 0.5, 0.2, 0.3]]\n# im, coords = obj_box_left_right_flip(im, coords=[[0.2, 0.4, 0.3, 0.3]], is_rescale=True, is_center=False, is_random=False)\n# tl.logging.info(coords)\n# # [[0.5, 0.4, 0.3, 0.3]]\n# im, coords = obj_box_left_right_flip(im, coords=[[20, 40, 30, 30]], is_rescale=False, is_center=True, is_random=False)\n# tl.logging.info(coords)\n# #   [[80, 40, 30, 30]]\n# im, coords = obj_box_left_right_flip(im, coords=[[20, 40, 30, 30]], is_rescale=False, is_center=False, is_random=False)\n# tl.logging.info(coords)\n# # [[50, 40, 30, 30]]\n# exit()\n\n\ndef obj_box_imresize(im, coords=None, size=None, interp=\'bicubic\', mode=None, is_rescale=False):\n    """"""Resize an image, and compute the new bounding box coordinates.\n\n    Parameters\n    -------------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    size interp and mode : args\n        See ``tl.prepro.imresize``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1], then return the original coordinates. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    Examples\n    --------\n    >>> im = np.zeros([80, 100, 3])    # as an image with shape width=100, height=80\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30], [10, 20, 20, 20]], size=[160, 200], is_rescale=False)\n    >>> print(coords)\n      [[40, 80, 60, 60], [20, 40, 40, 40]]\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30]], size=[40, 100], is_rescale=False)\n    >>> print(coords)\n      [[20, 20, 30, 15]]\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30]], size=[60, 150], is_rescale=False)\n    >>> print(coords)\n      [[30, 30, 45, 22]]\n    >>> im2, coords = obj_box_imresize(im, coords=[[0.2, 0.4, 0.3, 0.3]], size=[160, 200], is_rescale=True)\n    >>> print(coords, im2.shape)\n      [[0.2, 0.4, 0.3, 0.3]] (160, 200, 3)\n\n    """"""\n    if coords is None:\n        coords = []\n    if size is None:\n        size = [100, 100]\n\n    imh, imw = im.shape[0:2]\n    imh = imh * 1.0  # * 1.0 for python2 : force division to be float point\n    imw = imw * 1.0\n    im = imresize(im, size=size, interp=interp, mode=mode)\n\n    if is_rescale is False:\n        coords_new = list()\n\n        for coord in coords:\n\n            if len(coord) != 4:\n                raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n            # x\' = x * (imw\'/imw)\n            x = int(coord[0] * (size[1] / imw))\n            # y\' = y * (imh\'/imh)\n            # tl.logging.info(\'>>\', coord[1], size[0], imh)\n            y = int(coord[1] * (size[0] / imh))\n            # w\' = w * (imw\'/imw)\n            w = int(coord[2] * (size[1] / imw))\n            # h\' = h * (imh\'/imh)\n            h = int(coord[3] * (size[0] / imh))\n            coords_new.append([x, y, w, h])\n        return im, coords_new\n    else:\n        return im, coords\n\n\n# im = np.zeros([80, 100, 3])    # as an image with shape width=100, height=80\n# _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30], [10, 20, 20, 20]], size=[160, 200], is_rescale=False)\n# tl.logging.info(coords)\n# #   [[40, 80, 60, 60], [20, 40, 40, 40]]\n# _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30]], size=[40, 100], is_rescale=False)\n# tl.logging.info(coords)\n# #   [20, 20, 30, 15]\n# _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30]], size=[60, 150], is_rescale=False)\n# tl.logging.info(coords)\n# #   [30, 30, 45, 22]\n# im2, coords = obj_box_imresize(im, coords=[[0.2, 0.4, 0.3, 0.3]], size=[160, 200], is_rescale=True)\n# tl.logging.info(coords, im2.shape)\n# # [0.2, 0.4, 0.3, 0.3] (160, 200, 3)\n# exit()\n\n\ndef obj_box_crop(\n    im, classes=None, coords=None, wrg=100, hrg=100, is_rescale=False, is_center=False, is_random=False, thresh_wh=0.02,\n    thresh_wh2=12.\n):\n    """"""Randomly or centrally crop an image, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    wrg hrg and is_random : args\n        See ``tl.prepro.crop``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean, default False\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    """"""\n    if classes is None:\n        classes = []\n    if coords is None:\n        coords = []\n\n    h, w = im.shape[0], im.shape[1]\n\n    if (h <= hrg) or (w <= wrg):\n        raise AssertionError(""The size of cropping should smaller than the original image"")\n\n    if is_random:\n        h_offset = int(np.random.uniform(0, h - hrg) - 1)\n        w_offset = int(np.random.uniform(0, w - wrg) - 1)\n        h_end = hrg + h_offset\n        w_end = wrg + w_offset\n        im_new = im[h_offset:h_end, w_offset:w_end]\n    else:  # central crop\n        h_offset = int(np.floor((h - hrg) / 2.))\n        w_offset = int(np.floor((w - wrg) / 2.))\n        h_end = h_offset + hrg\n        w_end = w_offset + wrg\n        im_new = im[h_offset:h_end, w_offset:w_end]\n\n    #              w\n    #   _____________________________\n    #   |  h/w offset               |\n    #   |       -------             |\n    # h |       |     |             |\n    #   |       |     |             |\n    #   |       -------             |\n    #   |            h/w end        |\n    #   |___________________________|\n\n    def _get_coord(coord):\n        """"""Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,\n        before getting the new coordinates.\n        Boxes outsides the cropped image will be removed.\n\n        """"""\n        if is_center:\n            coord = obj_box_coord_centroid_to_upleft(coord)\n\n        ##======= pixel unit format and upleft, w, h ==========##\n\n        # x = np.clip( coord[0] - w_offset, 0, w_end - w_offset)\n        # y = np.clip( coord[1] - h_offset, 0, h_end - h_offset)\n        # w = np.clip( coord[2]           , 0, w_end - w_offset)\n        # h = np.clip( coord[3]           , 0, h_end - h_offset)\n\n        x = coord[0] - w_offset\n        y = coord[1] - h_offset\n        w = coord[2]\n        h = coord[3]\n\n        if x < 0:\n            if x + w <= 0:\n                return None\n            w = w + x\n            x = 0\n        elif x > im_new.shape[1]:  # object outside the cropped image\n            return None\n\n        if y < 0:\n            if y + h <= 0:\n                return None\n            h = h + y\n            y = 0\n        elif y > im_new.shape[0]:  # object outside the cropped image\n            return None\n\n        if (x is not None) and (x + w > im_new.shape[1]):  # box outside the cropped image\n            w = im_new.shape[1] - x\n\n        if (y is not None) and (y + h > im_new.shape[0]):  # box outside the cropped image\n            h = im_new.shape[0] - y\n\n        if (w / (h + 1.) > thresh_wh2) or (h / (w + 1.) > thresh_wh2):  # object shape strange: too narrow\n            # tl.logging.info(\'xx\', w, h)\n            return None\n\n        if (w / (im_new.shape[1] * 1.) < thresh_wh) or (h / (im_new.shape[0] * 1.) <\n                                                        thresh_wh):  # object shape strange: too narrow\n            # tl.logging.info(\'yy\', w, im_new.shape[1], h, im_new.shape[0])\n            return None\n\n        coord = [x, y, w, h]\n\n        ## convert back if input format is center.\n        if is_center:\n            coord = obj_box_coord_upleft_to_centroid(coord)\n\n        return coord\n\n    coords_new = list()\n    classes_new = list()\n    for i, _ in enumerate(coords):\n        coord = coords[i]\n\n        if len(coord) != 4:\n            raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n        if is_rescale:\n            # for scaled coord, upscaled before process and scale back in the end.\n            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)\n            coord = _get_coord(coord)\n            if coord is not None:\n                coord = obj_box_coord_rescale(coord, im_new.shape)\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n        else:\n            coord = _get_coord(coord)\n            if coord is not None:\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n    return im_new, classes_new, coords_new\n\n\ndef obj_box_shift(\n    im, classes=None, coords=None, wrg=0.1, hrg=0.1, row_index=0, col_index=1, channel_index=2, fill_mode=\'nearest\',\n    cval=0., order=1, is_rescale=False, is_center=False, is_random=False, thresh_wh=0.02, thresh_wh2=12.\n):\n    """"""Shift an image randomly or non-randomly, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    wrg, hrg row_index col_index channel_index is_random fill_mode cval and order : see ``tl.prepro.shift``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    """"""\n    if classes is None:\n        classes = []\n    if coords is None:\n        coords = []\n\n    imh, imw = im.shape[row_index], im.shape[col_index]\n\n    if (hrg >= 1.0) and (hrg <= 0.) and (wrg >= 1.0) and (wrg <= 0.):\n        raise AssertionError(""shift range should be (0, 1)"")\n\n    if is_random:\n        tx = np.random.uniform(-hrg, hrg) * imh\n        ty = np.random.uniform(-wrg, wrg) * imw\n    else:\n        tx, ty = hrg * imh, wrg * imw\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n\n    transform_matrix = translation_matrix  # no need to do offset\n    im_new = affine_transform(im, transform_matrix, channel_index, fill_mode, cval, order)\n\n    # modified from obj_box_crop\n    def _get_coord(coord):\n        """"""Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,\n        before getting the new coordinates.\n        Boxes outsides the cropped image will be removed.\n\n        """"""\n        if is_center:\n            coord = obj_box_coord_centroid_to_upleft(coord)\n\n        ##======= pixel unit format and upleft, w, h ==========##\n        x = coord[0] - ty  # only change this\n        y = coord[1] - tx  # only change this\n        w = coord[2]\n        h = coord[3]\n\n        if x < 0:\n            if x + w <= 0:\n                return None\n            w = w + x\n            x = 0\n        elif x > im_new.shape[1]:  # object outside the cropped image\n            return None\n\n        if y < 0:\n            if y + h <= 0:\n                return None\n            h = h + y\n            y = 0\n        elif y > im_new.shape[0]:  # object outside the cropped image\n            return None\n\n        if (x is not None) and (x + w > im_new.shape[1]):  # box outside the cropped image\n            w = im_new.shape[1] - x\n\n        if (y is not None) and (y + h > im_new.shape[0]):  # box outside the cropped image\n            h = im_new.shape[0] - y\n\n        if (w / (h + 1.) > thresh_wh2) or (h / (w + 1.) > thresh_wh2):  # object shape strange: too narrow\n            # tl.logging.info(\'xx\', w, h)\n            return None\n\n        if (w / (im_new.shape[1] * 1.) < thresh_wh) or (h / (im_new.shape[0] * 1.) <\n                                                        thresh_wh):  # object shape strange: too narrow\n            # tl.logging.info(\'yy\', w, im_new.shape[1], h, im_new.shape[0])\n            return None\n\n        coord = [x, y, w, h]\n\n        ## convert back if input format is center.\n        if is_center:\n            coord = obj_box_coord_upleft_to_centroid(coord)\n\n        return coord\n\n    coords_new = list()\n    classes_new = list()\n    for i, _ in enumerate(coords):\n        coord = coords[i]\n\n        if len(coord) != 4:\n            raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n        if is_rescale:\n            # for scaled coord, upscaled before process and scale back in the end.\n            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)\n            coord = _get_coord(coord)\n            if coord is not None:\n                coord = obj_box_coord_rescale(coord, im_new.shape)\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n        else:\n            coord = _get_coord(coord)\n            if coord is not None:\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n    return im_new, classes_new, coords_new\n\n\ndef obj_box_zoom(\n    im, classes=None, coords=None, zoom_range=(0.9, 1.1), row_index=0, col_index=1, channel_index=2,\n    fill_mode=\'nearest\', cval=0., order=1, is_rescale=False, is_center=False, is_random=False, thresh_wh=0.02,\n    thresh_wh2=12.\n):\n    """"""Zoom in and out of a single image, randomly or non-randomly, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...].\n    zoom_range row_index col_index channel_index is_random fill_mode cval and order : see ``tl.prepro.zoom``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid. (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    """"""\n    if classes is None:\n        classes = []\n    if coords is None:\n        coords = []\n\n    if len(zoom_range) != 2:\n        raise Exception(\'zoom_range should be a tuple or list of two floats. \' \'Received arg: \', zoom_range)\n    if is_random:\n        if zoom_range[0] == 1 and zoom_range[1] == 1:\n            zx, zy = 1, 1\n            tl.logging.info("" random_zoom : not zoom in/out"")\n        else:\n            zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)\n    else:\n        zx, zy = zoom_range\n    # tl.logging.info(zx, zy)\n    zoom_matrix = np.array([[zx, 0, 0], [0, zy, 0], [0, 0, 1]])\n\n    h, w = im.shape[row_index], im.shape[col_index]\n    transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n    im_new = affine_transform(im, transform_matrix, channel_index, fill_mode, cval, order)\n\n    # modified from obj_box_crop\n    def _get_coord(coord):\n        """"""Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,\n        before getting the new coordinates.\n        Boxes outsides the cropped image will be removed.\n\n        """"""\n        if is_center:\n            coord = obj_box_coord_centroid_to_upleft(coord)\n\n        # ======= pixel unit format and upleft, w, h ==========\n        x = (coord[0] - im.shape[1] / 2) / zy + im.shape[1] / 2  # only change this\n        y = (coord[1] - im.shape[0] / 2) / zx + im.shape[0] / 2  # only change this\n        w = coord[2] / zy  # only change this\n        h = coord[3] / zx  # only change thisS\n\n        if x < 0:\n            if x + w <= 0:\n                return None\n            w = w + x\n            x = 0\n        elif x > im_new.shape[1]:  # object outside the cropped image\n            return None\n\n        if y < 0:\n            if y + h <= 0:\n                return None\n            h = h + y\n            y = 0\n        elif y > im_new.shape[0]:  # object outside the cropped image\n            return None\n\n        if (x is not None) and (x + w > im_new.shape[1]):  # box outside the cropped image\n            w = im_new.shape[1] - x\n\n        if (y is not None) and (y + h > im_new.shape[0]):  # box outside the cropped image\n            h = im_new.shape[0] - y\n\n        if (w / (h + 1.) > thresh_wh2) or (h / (w + 1.) > thresh_wh2):  # object shape strange: too narrow\n            # tl.logging.info(\'xx\', w, h)\n            return None\n\n        if (w / (im_new.shape[1] * 1.) < thresh_wh) or (h / (im_new.shape[0] * 1.) <\n                                                        thresh_wh):  # object shape strange: too narrow\n            # tl.logging.info(\'yy\', w, im_new.shape[1], h, im_new.shape[0])\n            return None\n\n        coord = [x, y, w, h]\n\n        # convert back if input format is center.\n        if is_center:\n            coord = obj_box_coord_upleft_to_centroid(coord)\n\n        return coord\n\n    coords_new = list()\n    classes_new = list()\n    for i, _ in enumerate(coords):\n        coord = coords[i]\n\n        if len(coord) != 4:\n            raise AssertionError(""coordinate should be 4 values : [x, y, w, h]"")\n\n        if is_rescale:\n            # for scaled coord, upscaled before process and scale back in the end.\n            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)\n            coord = _get_coord(coord)\n            if coord is not None:\n                coord = obj_box_coord_rescale(coord, im_new.shape)\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n        else:\n            coord = _get_coord(coord)\n            if coord is not None:\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n    return im_new, classes_new, coords_new\n\n\ndef pad_sequences(sequences, maxlen=None, dtype=\'int32\', padding=\'post\', truncating=\'pre\', value=0.):\n    """"""Pads each sequence to the same length:\n    the length of the longest sequence.\n    If maxlen is provided, any sequence longer\n    than maxlen is truncated to maxlen.\n    Truncation happens off either the beginning (default) or\n    the end of the sequence.\n    Supports post-padding and pre-padding (default).\n\n    Parameters\n    ----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    maxlen : int\n        Maximum length.\n    dtype : numpy.dtype or str\n        Data type to cast the resulting sequence.\n    padding : str\n        Either \'pre\' or \'post\', pad either before or after each sequence.\n    truncating : str\n        Either \'pre\' or \'post\', remove values from sequences larger than maxlen either in the beginning or in the end of the sequence\n    value : float\n        Value to pad the sequences to the desired value.\n\n    Returns\n    ----------\n    x : numpy.array\n        With dimensions (number_of_sequences, maxlen)\n\n    Examples\n    ----------\n    >>> sequences = [[1,1,1,1,1],[2,2,2],[3,3]]\n    >>> sequences = pad_sequences(sequences, maxlen=None, dtype=\'int32\',\n    ...                  padding=\'post\', truncating=\'pre\', value=0.)\n    [[1 1 1 1 1]\n     [2 2 2 0 0]\n     [3 3 0 0 0]]\n\n    """"""\n    lengths = [len(s) for s in sequences]\n\n    nb_samples = len(sequences)\n    if maxlen is None:\n        maxlen = np.max(lengths)\n\n    # take the sample shape from the first non empty sequence\n    # checking for consistency in the main loop below.\n    sample_shape = tuple()\n    for s in sequences:\n        if len(s) > 0:\n            sample_shape = np.asarray(s).shape[1:]\n            break\n\n    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n    for idx, s in enumerate(sequences):\n        if len(s) == 0:\n            continue  # empty list was found\n        if truncating == \'pre\':\n            trunc = s[-maxlen:]\n        elif truncating == \'post\':\n            trunc = s[:maxlen]\n        else:\n            raise ValueError(\'Truncating type ""%s"" not understood\' % truncating)\n\n        # check `trunc` has expected shape\n        trunc = np.asarray(trunc, dtype=dtype)\n        if trunc.shape[1:] != sample_shape:\n            raise ValueError(\n                \'Shape of sample %s of sequence at position %s is different from expected shape %s\' %\n                (trunc.shape[1:], idx, sample_shape)\n            )\n\n        if padding == \'post\':\n            x[idx, :len(trunc)] = trunc\n        elif padding == \'pre\':\n            x[idx, -len(trunc):] = trunc\n        else:\n            raise ValueError(\'Padding type ""%s"" not understood\' % padding)\n    return x.tolist()\n\n\ndef remove_pad_sequences(sequences, pad_id=0):\n    """"""Remove padding.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    pad_id : int\n        The pad ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ----------\n    >>> sequences = [[2,3,4,0,0], [5,1,2,3,4,0,0,0], [4,5,0,2,4,0,0,0]]\n    >>> print(remove_pad_sequences(sequences, pad_id=0))\n    [[2, 3, 4], [5, 1, 2, 3, 4], [4, 5, 0, 2, 4]]\n\n    """"""\n    sequences_out = copy.deepcopy(sequences)\n\n    for i, _ in enumerate(sequences):\n        # for j in range(len(sequences[i])):\n        #     if sequences[i][j] == pad_id:\n        #         sequences_out[i] = sequences_out[i][:j]\n        #         break\n        for j in range(1, len(sequences[i])):\n            if sequences[i][-j] != pad_id:\n                sequences_out[i] = sequences_out[i][0:-j + 1]\n                break\n\n    return sequences_out\n\n\ndef process_sequences(sequences, end_id=0, pad_val=0, is_shorten=True, remain_end_id=False):\n    """"""Set all tokens(ids) after END token to the padding value, and then shorten (option) it to the maximum sequence length in this batch.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The special token for END.\n    pad_val : int\n        Replace the `end_id` and the IDs after `end_id` to this value.\n    is_shorten : boolean\n        Shorten the sequences. Default is True.\n    remain_end_id : boolean\n        Keep an `end_id` in the end. Default is False.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4, 3, 5, 3, 2, 2, 2, 2],  <-- end_id is 2\n    ...                  [5, 3, 9, 4, 9, 2, 2, 3]]  <-- end_id is 2\n    >>> sentences_ids = precess_sequences(sentences_ids, end_id=vocab.end_id, pad_val=0, is_shorten=True)\n    [[4, 3, 5, 3, 0], [5, 3, 9, 4, 9]]\n\n    """"""\n    max_length = 0\n    for _, seq in enumerate(sequences):\n        is_end = False\n        for i_w, n in enumerate(seq):\n            if n == end_id and is_end == False:  # 1st time to see end_id\n                is_end = True\n                if max_length < i_w:\n                    max_length = i_w\n                if remain_end_id is False:\n                    seq[i_w] = pad_val  # set end_id to pad_val\n            elif is_end ==True:\n                seq[i_w] = pad_val\n\n    if remain_end_id is True:\n        max_length += 1\n    if is_shorten:\n        for i, seq in enumerate(sequences):\n            sequences[i] = seq[:max_length]\n    return sequences\n\n\ndef sequences_add_start_id(sequences, start_id=0, remove_last=False):\n    """"""Add special start token(id) in the beginning of each sequence.\n\n    Parameters\n    ------------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    start_id : int\n        The start ID.\n    remove_last : boolean\n        Remove the last value of each sequences. Usually be used for removing the end ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4,3,5,3,2,2,2,2], [5,3,9,4,9,2,2,3]]\n    >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2)\n    [[2, 4, 3, 5, 3, 2, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2, 3]]\n    >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2, remove_last=True)\n    [[2, 4, 3, 5, 3, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2]]\n\n    For Seq2seq\n\n    >>> input = [a, b, c]\n    >>> target = [x, y, z]\n    >>> decode_seq = [start_id, a, b] <-- sequences_add_start_id(input, start_id, True)\n\n    """"""\n    sequences_out = [[] for _ in range(len(sequences))]  #[[]] * len(sequences)\n    for i, _ in enumerate(sequences):\n        if remove_last:\n            sequences_out[i] = [start_id] + sequences[i][:-1]\n        else:\n            sequences_out[i] = [start_id] + sequences[i]\n    return sequences_out\n\n\ndef sequences_add_end_id(sequences, end_id=888):\n    """"""Add special end token(id) in the end of each sequence.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The end ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sequences = [[1,2,3],[4,5,6,7]]\n    >>> print(sequences_add_end_id(sequences, end_id=999))\n    [[1, 2, 3, 999], [4, 5, 6, 999]]\n\n    """"""\n    sequences_out = [[] for _ in range(len(sequences))]  #[[]] * len(sequences)\n    for i, _ in enumerate(sequences):\n        sequences_out[i] = sequences[i] + [end_id]\n    return sequences_out\n\n\ndef sequences_add_end_id_after_pad(sequences, end_id=888, pad_id=0):\n    """"""Add special end token(id) in the end of each sequence.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The end ID.\n    pad_id : int\n        The pad ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sequences = [[1,2,0,0], [1,2,3,0], [1,2,3,4]]\n    >>> print(sequences_add_end_id_after_pad(sequences, end_id=99, pad_id=0))\n    [[1, 2, 99, 0], [1, 2, 3, 99], [1, 2, 3, 4]]\n\n    """"""\n    # sequences_out = [[] for _ in range(len(sequences))]#[[]] * len(sequences)\n\n    sequences_out = copy.deepcopy(sequences)\n    # # add a pad to all\n    # for i in range(len(sequences)):\n    #     for j in range(len(sequences[i])):\n    #         sequences_out[i].append(pad_id)\n    # # pad -- > end\n    # max_len = 0\n\n    for i, v in enumerate(sequences):\n        for j, _v2 in enumerate(v):\n            if sequences[i][j] == pad_id:\n                sequences_out[i][j] = end_id\n                # if j > max_len:\n                #     max_len = j\n                break\n\n    # # remove pad if too long\n    # for i in range(len(sequences)):\n    #     for j in range(len(sequences[i])):\n    #         sequences_out[i] = sequences_out[i][:max_len+1]\n    return sequences_out\n\n\ndef sequences_get_mask(sequences, pad_val=0):\n    """"""Return mask for sequences.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    pad_val : int\n        The pad value.\n\n    Returns\n    ----------\n    list of list of int\n        The mask.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4, 0, 5, 3, 0, 0],\n    ...                  [5, 3, 9, 4, 9, 0]]\n    >>> mask = sequences_get_mask(sentences_ids, pad_val=0)\n    [[1 1 1 1 0 0]\n     [1 1 1 1 1 0]]\n\n    """"""\n    mask = np.ones_like(sequences)\n    for i, seq in enumerate(sequences):\n        for i_w in reversed(range(len(seq))):\n            if seq[i_w] == pad_val:\n                mask[i, i_w] = 0\n            else:\n                break  # <-- exit the for loop, prepcess next sequence\n    return mask\n\n\ndef keypoint_random_crop(image, annos, mask=None, size=(368, 368)):\n    """"""Randomly crop an image and corresponding keypoints without influence scales, given by ``keypoint_random_resize_shortestedge``.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    size : tuple of int\n        The size of returned image.\n\n    Returns\n    ----------\n    preprocessed image, annotation, mask\n\n    """"""\n\n    _target_height = size[0]\n    _target_width = size[1]\n    target_size = (_target_width, _target_height)\n\n    if len(np.shape(image)) == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    height, width, _ = np.shape(image)\n\n    for _ in range(50):\n        x = random.randrange(0, width - target_size[0]) if width > target_size[0] else 0\n        y = random.randrange(0, height - target_size[1]) if height > target_size[1] else 0\n\n        # check whether any face is inside the box to generate a reasonably-balanced datasets\n        for joint in annos:\n            if x <= joint[0][0] < x + target_size[0] and y <= joint[0][1] < y + target_size[1]:\n                break\n\n    def pose_crop(image, annos, mask, x, y, w, h):  # TODO : speed up with affine transform\n        # adjust image\n        target_size = (w, h)\n\n        img = image\n        resized = img[y:y + target_size[1], x:x + target_size[0], :]\n        resized_mask = mask[y:y + target_size[1], x:x + target_size[0]]\n        # adjust meta data\n        adjust_joint_list = []\n        for joint in annos:\n            adjust_joint = []\n            for point in joint:\n                if point[0] < -10 or point[1] < -10:\n                    adjust_joint.append((-1000, -1000))\n                    continue\n                new_x, new_y = point[0] - x, point[1] - y\n                # should not crop outside the image\n                if new_x > w - 1 or new_y > h - 1:\n                    adjust_joint.append((-1000, -1000))\n                    continue\n                adjust_joint.append((new_x, new_y))\n            adjust_joint_list.append(adjust_joint)\n\n        return resized, adjust_joint_list, resized_mask\n\n    return pose_crop(image, annos, mask, x, y, target_size[0], target_size[1])\n\n\ndef keypoint_resize_random_crop(image, annos, mask=None, size=(368, 368)):\n    """"""Reszie the image to make either its width or height equals to the given sizes.\n    Then randomly crop image without influence scales.\n    Resize the image match with the minimum size before cropping, this API will change the zoom scale of object.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    size : tuple of int\n        The size (height, width) of returned image.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    """"""\n\n    if len(np.shape(image)) == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n\n    def resize_image(image, annos, mask, target_width, target_height):\n        """"""Reszie image\n\n        Parameters\n        -----------\n        image : 3 channel image\n            The given image.\n        annos : list of list of floats\n            Keypoints of people\n        mask : single channel image or None\n            The mask if available.\n        target_width : int\n            Expected width of returned image.\n        target_height : int\n            Expected height of returned image.\n\n        Returns\n        ----------\n        preprocessed input image, annos, mask\n\n        """"""\n        y, x, _ = np.shape(image)\n\n        ratio_y = target_height / y\n        ratio_x = target_width / x\n\n        new_joints = []\n        # update meta\n        for people in annos:\n            new_keypoints = []\n            for keypoints in people:\n                if keypoints[0] < 0 or keypoints[1] < 0:\n                    new_keypoints.append((-1000, -1000))\n                    continue\n                pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))\n                if pts[0] > target_width - 1 or pts[1] > target_height - 1:\n                    new_keypoints.append((-1000, -1000))\n                    continue\n\n                new_keypoints.append(pts)\n            new_joints.append(new_keypoints)\n        annos = new_joints\n\n        new_image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_AREA)\n        if mask is not None:\n            new_mask = cv2.resize(mask, (target_width, target_height), interpolation=cv2.INTER_AREA)\n            return new_image, annos, new_mask\n        else:\n            return new_image, annos, None\n\n    _target_height = size[0]\n    _target_width = size[1]\n\n    if len(np.shape(image)) == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    input_height, input_width, _ = np.shape(image)\n\n    vertical_ratio = _target_height / input_height\n    horizontal_ratio = _target_width / input_width\n\n    rescale_ratio = max(vertical_ratio, horizontal_ratio)\n\n    image, annos, mask = resize_image(\n        image, annos, mask, round(input_width * rescale_ratio), round(input_height * rescale_ratio)\n    )\n\n    # At this point we should have input image which matches at least target\n    # height or target width, while the other dimensions larger than target.\n    new_height, new_width, _ = np.shape(image)\n\n    if new_height > _target_height:\n        crop_range_y = np.random.randint(0, new_height - _target_height)\n        image = image[crop_range_y:crop_range_y + _target_height, :, :]\n        if mask is not None:\n            mask = mask[crop_range_y:crop_range_y + _target_height, :]\n        new_joints = []\n\n        for people in annos:  # TODO : speed up with affine transform\n            new_keypoints = []\n            for keypoints in people:\n\n                # case orginal points are not usable\n                if keypoints[1] >= crop_range_y and keypoints[1] <= crop_range_y + _target_height - 1:\n                    pts = (int(keypoints[0]), int(keypoints[1] - crop_range_y))\n                else:\n                    pts = (-1000, -1000)\n                new_keypoints.append(pts)\n\n            new_joints.append(new_keypoints)\n        annos = new_joints\n\n    elif new_width > _target_width:\n        crop_range_x = np.random.randint(0, new_width - _target_width)\n        image = image[:, crop_range_x:crop_range_x + _target_width, :]\n        if mask is not None:\n            mask = mask[:, crop_range_x:crop_range_x + _target_width]\n        new_joints = []\n\n        for people in annos:\n            new_keypoints = []\n            for keypoints in people:\n\n                # case orginal points are not usable\n                if keypoints[0] >= crop_range_x and keypoints[0] <= crop_range_x + _target_width - 1:\n                    pts = (int(keypoints[0] - crop_range_x), int(keypoints[1]))\n                else:\n                    pts = (-1000, -1000)\n                new_keypoints.append(pts)\n\n            new_joints.append(new_keypoints)\n        annos = new_joints\n\n    if mask is not None:\n        return image, annos, mask\n    else:\n        return image, annos, None\n\n\ndef keypoint_random_rotate(image, annos, mask=None, rg=15.):\n    """"""Rotate an image and corresponding keypoints.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    rg : int or float\n        Degree to rotate, usually 0 ~ 180.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    """"""\n\n    def _rotate_coord(shape, newxy, point, angle):\n        angle = -1 * angle / 180.0 * math.pi\n        ox, oy = shape\n        px, py = point\n        ox /= 2\n        oy /= 2\n        qx = math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n        qy = math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n        new_x, new_y = newxy\n        qx += ox - new_x\n        qy += oy - new_y\n        return int(qx + 0.5), int(qy + 0.5)\n\n    def _largest_rotated_rect(w, h, angle):\n        """"""\n        Get largest rectangle after rotation.\n        http://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n        """"""\n        angle = angle / 180.0 * math.pi\n        if w <= 0 or h <= 0:\n            return 0, 0\n\n        width_is_longer = w >= h\n        side_long, side_short = (w, h) if width_is_longer else (h, w)\n\n        # since the solutions for angle, -angle and 180-angle are all the same,\n        # if suffices to look at the first quadrant and the absolute values of sin,cos:\n        sin_a, cos_a = abs(math.sin(angle)), abs(math.cos(angle))\n        if side_short <= 2. * sin_a * cos_a * side_long:\n            # half constrained case: two crop corners touch the longer side,\n            #   the other two corners are on the mid-line parallel to the longer line\n            x = 0.5 * side_short\n            wr, hr = (x / sin_a, x / cos_a) if width_is_longer else (x / cos_a, x / sin_a)\n        else:\n            # fully constrained case: crop touches all 4 sides\n            cos_2a = cos_a * cos_a - sin_a * sin_a\n            wr, hr = (w * cos_a - h * sin_a) / cos_2a, (h * cos_a - w * sin_a) / cos_2a\n        return int(np.round(wr)), int(np.round(hr))\n\n    img_shape = np.shape(image)\n    height = img_shape[0]\n    width = img_shape[1]\n    deg = np.random.uniform(-rg, rg)\n\n    img = image\n    center = (img.shape[1] * 0.5, img.shape[0] * 0.5)  # x, y\n    rot_m = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), deg, 1)\n    ret = cv2.warpAffine(img, rot_m, img.shape[1::-1], flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT)\n    if img.ndim == 3 and ret.ndim == 2:\n        ret = ret[:, :, np.newaxis]\n    neww, newh = _largest_rotated_rect(ret.shape[1], ret.shape[0], deg)\n    neww = min(neww, ret.shape[1])\n    newh = min(newh, ret.shape[0])\n    newx = int(center[0] - neww * 0.5)\n    newy = int(center[1] - newh * 0.5)\n    # print(ret.shape, deg, newx, newy, neww, newh)\n    img = ret[newy:newy + newh, newx:newx + neww]\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in annos:  # TODO : speed up with affine transform\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n\n            x, y = _rotate_coord((width, height), (newx, newy), point, deg)\n\n            if x > neww - 1 or y > newh - 1:\n                adjust_joint.append((-1000, -1000))\n                continue\n            if x < 0 or y < 0:\n                adjust_joint.append((-1000, -1000))\n                continue\n\n            adjust_joint.append((x, y))\n        adjust_joint_list.append(adjust_joint)\n    joint_list = adjust_joint_list\n\n    if mask is not None:\n        msk = mask\n        center = (msk.shape[1] * 0.5, msk.shape[0] * 0.5)  # x, y\n        rot_m = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), deg, 1)\n        ret = cv2.warpAffine(msk, rot_m, msk.shape[1::-1], flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT)\n        if msk.ndim == 3 and msk.ndim == 2:\n            ret = ret[:, :, np.newaxis]\n        neww, newh = _largest_rotated_rect(ret.shape[1], ret.shape[0], deg)\n        neww = min(neww, ret.shape[1])\n        newh = min(newh, ret.shape[0])\n        newx = int(center[0] - neww * 0.5)\n        newy = int(center[1] - newh * 0.5)\n        # print(ret.shape, deg, newx, newy, neww, newh)\n        msk = ret[newy:newy + newh, newx:newx + neww]\n        return img, joint_list, msk\n    else:\n        return img, joint_list, None\n\n\ndef keypoint_random_flip(\n    image, annos, mask=None, prob=0.5, flip_list=(0, 1, 5, 6, 7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16, 18)\n):\n    """"""Flip an image and corresponding keypoints.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    prob : float, 0 to 1\n        The probability to flip the image, if 1, always flip the image.\n    flip_list : tuple of int\n        Denotes how the keypoints number be changed after flipping which is required for pose estimation task.\n        The left and right body should be maintained rather than switch.\n        (Default COCO format).\n        Set to an empty tuple if you don\'t need to maintain left and right information.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    """"""\n\n    _prob = np.random.uniform(0, 1.0)\n    if _prob < prob:\n        return image, annos, mask\n\n    _, width, _ = np.shape(image)\n    image = cv2.flip(image, 1)\n    mask = cv2.flip(mask, 1)\n    new_joints = []\n    for people in annos:  # TODO : speed up with affine transform\n        new_keypoints = []\n        for k in flip_list:\n            point = people[k]\n            if point[0] < 0 or point[1] < 0:\n                new_keypoints.append((-1000, -1000))\n                continue\n            if point[0] > image.shape[1] - 1 or point[1] > image.shape[0] - 1:\n                new_keypoints.append((-1000, -1000))\n                continue\n            if (width - point[0]) > image.shape[1] - 1:\n                new_keypoints.append((-1000, -1000))\n                continue\n            new_keypoints.append((width - point[0], point[1]))\n        new_joints.append(new_keypoints)\n    annos = new_joints\n\n    return image, annos, mask\n\n\ndef keypoint_random_resize(image, annos, mask=None, zoom_range=(0.8, 1.2)):\n    """"""Randomly resize an image and corresponding keypoints.\n    The height and width of image will be changed independently, so the scale will be changed.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    zoom_range : tuple of two floats\n        The minimum and maximum factor to zoom in or out, e.g (0.5, 1) means zoom out 1~2 times.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    """"""\n    height = image.shape[0]\n    width = image.shape[1]\n    _min, _max = zoom_range\n    scalew = np.random.uniform(_min, _max)\n    scaleh = np.random.uniform(_min, _max)\n\n    neww = int(width * scalew)\n    newh = int(height * scaleh)\n\n    dst = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)\n    if mask is not None:\n        mask = cv2.resize(mask, (neww, newh), interpolation=cv2.INTER_AREA)\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in annos:  # TODO : speed up with affine transform\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n    if mask is not None:\n        return dst, adjust_joint_list, mask\n    else:\n        return dst, adjust_joint_list, None\n\n\ndef keypoint_random_resize_shortestedge(\n    image, annos, mask=None, min_size=(368, 368), zoom_range=(0.8, 1.2), pad_val=(0, 0, np.random.uniform(0.0, 1.0))\n):\n    """"""Randomly resize an image and corresponding keypoints based on shorter edgeself.\n    If the resized image is smaller than `min_size`, uses padding to make shape matchs `min_size`.\n    The height and width of image will be changed together, the scale would not be changed.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    min_size : tuple of two int\n        The minimum size of height and width.\n    zoom_range : tuple of two floats\n        The minimum and maximum factor to zoom in or out, e.g (0.5, 1) means zoom out 1~2 times.\n    pad_val : int/float, or tuple of int or random function\n        The three padding values for RGB channels respectively.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    """"""\n\n    _target_height = min_size[0]\n    _target_width = min_size[1]\n\n    if len(np.shape(image)) == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    height, width, _ = np.shape(image)\n\n    ratio_w = _target_width / width\n    ratio_h = _target_height / height\n    ratio = min(ratio_w, ratio_h)\n    target_size = int(min(width * ratio + 0.5, height * ratio + 0.5))\n    random_target = np.random.uniform(zoom_range[0], zoom_range[1])\n    target_size = int(target_size * random_target)\n\n    # target_size = int(min(_network_w, _network_h) * random.uniform(0.7, 1.5))\n\n    def pose_resize_shortestedge(image, annos, mask, target_size):\n        """""" """"""\n        # _target_height = 368\n        # _target_width = 368\n        # img = image\n        height, width, _ = np.shape(image)\n\n        # adjust image\n        scale = target_size / min(height, width)\n        if height < width:\n            newh, neww = target_size, int(scale * width + 0.5)\n        else:\n            newh, neww = int(scale * height + 0.5), target_size\n\n        dst = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)\n        mask = cv2.resize(mask, (neww, newh), interpolation=cv2.INTER_AREA)\n        pw = ph = 0\n        if neww < _target_width or newh < _target_height:\n            pw = max(0, (_target_width - neww) // 2)\n            ph = max(0, (_target_height - newh) // 2)\n            mw = (_target_width - neww) % 2\n            mh = (_target_height - newh) % 2\n            # color = np.random.uniform(0.0, 1.0)\n            dst = cv2.copyMakeBorder(dst, ph, ph + mh, pw, pw + mw, cv2.BORDER_CONSTANT, value=pad_val)  #(0, 0, color))\n            if mask is not None:\n                mask = cv2.copyMakeBorder(mask, ph, ph + mh, pw, pw + mw, cv2.BORDER_CONSTANT, value=1)\n        # adjust meta data\n        adjust_joint_list = []\n        for joint in annos:  # TODO : speed up with affine transform\n            adjust_joint = []\n            for point in joint:\n                if point[0] < -100 or point[1] < -100:\n                    adjust_joint.append((-1000, -1000))\n                    continue\n                # if point[0] <= 0 or point[1] <= 0 or int(point[0]*scale+0.5) > neww or int(point[1]*scale+0.5) > newh:\n                #     adjust_joint.append((-1, -1))\n                #     continue\n                adjust_joint.append((int(point[0] * scale + 0.5) + pw, int(point[1] * scale + 0.5) + ph))\n            adjust_joint_list.append(adjust_joint)\n        if mask is not None:\n            return dst, adjust_joint_list, mask\n        else:\n            return dst, adjust_joint_list, None\n\n    return pose_resize_shortestedge(image, annos, mask, target_size)\n'"
tensorlayer/rein.py,10,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import xrange\n\n__all__ = [\n    \'discount_episode_rewards\',\n    \'cross_entropy_reward_loss\',\n    \'log_weight\',\n    \'choice_action_by_probs\',\n]\n\n\ndef discount_episode_rewards(rewards=None, gamma=0.99, mode=0):\n    """"""Take 1D float array of rewards and compute discounted rewards for an\n    episode. When encount a non-zero value, consider as the end a of an episode.\n\n    Parameters\n    ----------\n    rewards : list\n        List of rewards\n    gamma : float\n        Discounted factor\n    mode : int\n        Mode for computing the discount rewards.\n            - If mode == 0, reset the discount process when encount a non-zero reward (Ping-pong game).\n            - If mode == 1, would not reset the discount process.\n\n    Returns\n    --------\n    list of float\n        The discounted rewards.\n\n    Examples\n    ----------\n    >>> rewards = np.asarray([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1])\n    >>> gamma = 0.9\n    >>> discount_rewards = tl.rein.discount_episode_rewards(rewards, gamma)\n    >>> print(discount_rewards)\n    [ 0.72899997  0.81        0.89999998  1.          0.72899997  0.81\n    0.89999998  1.          0.72899997  0.81        0.89999998  1.        ]\n    >>> discount_rewards = tl.rein.discount_episode_rewards(rewards, gamma, mode=1)\n    >>> print(discount_rewards)\n    [ 1.52110755  1.69011939  1.87791049  2.08656716  1.20729685  1.34144104\n    1.49048996  1.65610003  0.72899997  0.81        0.89999998  1.        ]\n\n    """"""\n    if rewards is None:\n        raise Exception(""rewards should be a list"")\n    discounted_r = np.zeros_like(rewards, dtype=np.float32)\n    running_add = 0\n    for t in reversed(xrange(0, rewards.size)):\n        if mode == 0:\n            if rewards[t] != 0: running_add = 0\n\n        running_add = running_add * gamma + rewards[t]\n        discounted_r[t] = running_add\n    return discounted_r\n\n\ndef cross_entropy_reward_loss(logits, actions, rewards, name=None):\n    """"""Calculate the loss for Policy Gradient Network.\n\n    Parameters\n    ----------\n    logits : tensor\n        The network outputs without softmax. This function implements softmax inside.\n    actions : tensor or placeholder\n        The agent actions.\n    rewards : tensor or placeholder\n        The rewards.\n\n    Returns\n    --------\n    Tensor\n        The TensorFlow loss function.\n\n    Examples\n    ----------\n    >>> states_batch_pl = tf.placeholder(tf.float32, shape=[None, D])\n    >>> network = InputLayer(states_batch_pl, name=\'input\')\n    >>> network = DenseLayer(network, n_units=H, act=tf.nn.relu, name=\'relu1\')\n    >>> network = DenseLayer(network, n_units=3, name=\'out\')\n    >>> probs = network.outputs\n    >>> sampling_prob = tf.nn.softmax(probs)\n    >>> actions_batch_pl = tf.placeholder(tf.int32, shape=[None])\n    >>> discount_rewards_batch_pl = tf.placeholder(tf.float32, shape=[None])\n    >>> loss = tl.rein.cross_entropy_reward_loss(probs, actions_batch_pl, discount_rewards_batch_pl)\n    >>> train_op = tf.train.RMSPropOptimizer(learning_rate, decay_rate).minimize(loss)\n\n    """"""\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=actions, logits=logits, name=name)\n\n    return tf.reduce_sum(tf.multiply(cross_entropy, rewards))\n\n\ndef log_weight(probs, weights, name=\'log_weight\'):\n    """"""Log weight.\n\n    Parameters\n    -----------\n    probs : tensor\n        If it is a network output, usually we should scale it to [0, 1] via softmax.\n    weights : tensor\n        The weights.\n\n    Returns\n    --------\n    Tensor\n        The Tensor after appling the log weighted expression.\n\n    """"""\n    with tf.variable_scope(name):\n        exp_v = tf.reduce_mean(tf.log(probs) * weights)\n        return exp_v\n\n\ndef choice_action_by_probs(probs=(0.5, 0.5), action_list=None):\n    """"""Choice and return an an action by given the action probability distribution.\n\n    Parameters\n    ------------\n    probs : list of float.\n        The probability distribution of all actions.\n    action_list : None or a list of int or others\n        A list of action in integer, string or others. If None, returns an integer range between 0 and len(probs)-1.\n\n    Returns\n    --------\n    float int or str\n        The chosen action.\n\n    Examples\n    ----------\n    >>> for _ in range(5):\n    >>>     a = choice_action_by_probs([0.2, 0.4, 0.4])\n    >>>     print(a)\n    0\n    1\n    1\n    2\n    1\n    >>> for _ in range(3):\n    >>>     a = choice_action_by_probs([0.5, 0.5], [\'a\', \'b\'])\n    >>>     print(a)\n    a\n    b\n    b\n\n    """"""\n    if action_list is None:\n        n_action = len(probs)\n        action_list = np.arange(n_action)\n    else:\n        if len(action_list) != len(probs):\n            raise Exception(""number of actions should equal to number of probabilities."")\n    return np.random.choice(action_list, p=probs)\n'"
tensorlayer/utils.py,23,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nimport random\nimport subprocess\nimport sys\nimport time\nfrom collections import Counter\nfrom sys import exit as _exit\nfrom sys import platform as _platform\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n\nimport tensorlayer as tl\n\n__all__ = [\n    \'fit\', \'test\', \'predict\', \'evaluation\', \'dict_to_one\', \'flatten_list\', \'class_balancing_oversample\',\n    \'get_random_int\', \'list_string_to_dict\', \'exit_tensorflow\', \'open_tensorboard\', \'clear_all_placeholder_variables\',\n    \'set_gpu_fraction\', \'train_epoch\', \'run_epoch\'\n]\n\n\ndef fit(\n    network, train_op, cost, X_train, y_train, acc=None, batch_size=100, n_epoch=100, print_freq=5, X_val=None,\n    y_val=None, eval_train=True, tensorboard_dir=None, tensorboard_epoch_freq=5, tensorboard_weight_histograms=True,\n    tensorboard_graph_vis=True\n):\n    """"""Training a given non time-series network by the given cost function, training data, batch_size, n_epoch etc.\n\n    - MNIST example click `here <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_.\n    - In order to control the training details, the authors HIGHLY recommend ``tl.iterate`` see two MNIST examples `1 <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mlp_dropout1.py>`_, `2 <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mlp_dropout1.py>`_.\n\n    Parameters\n    ----------\n    network : TensorLayer Model\n        the network to be trained.\n    train_op : TensorFlow optimizer\n        The optimizer for training e.g. tf.optimizers.Adam().\n    cost : TensorLayer or TensorFlow loss function\n        Metric for loss function, e.g tl.cost.cross_entropy.\n    X_train : numpy.array\n        The input of training data\n    y_train : numpy.array\n        The target of training data\n    acc : TensorFlow/numpy expression or None\n        Metric for accuracy or others. If None, would not print the information.\n    batch_size : int\n        The batch size for training and evaluating.\n    n_epoch : int\n        The number of training epochs.\n    print_freq : int\n        Print the training information every ``print_freq`` epochs.\n    X_val : numpy.array or None\n        The input of validation data. If None, would not perform validation.\n    y_val : numpy.array or None\n        The target of validation data. If None, would not perform validation.\n    eval_train : boolean\n        Whether to evaluate the model during training.\n        If X_val and y_val are not None, it reflects whether to evaluate the model on training data.\n    tensorboard_dir : string\n        path to log dir, if set, summary data will be stored to the tensorboard_dir/ directory for visualization with tensorboard. (default None)\n    tensorboard_epoch_freq : int\n        How many epochs between storing tensorboard checkpoint for visualization to log/ directory (default 5).\n    tensorboard_weight_histograms : boolean\n        If True updates tensorboard data in the logs/ directory for visualization\n        of the weight histograms every tensorboard_epoch_freq epoch (default True).\n    tensorboard_graph_vis : boolean\n        If True stores the graph in the tensorboard summaries saved to log/ (default True).\n\n    Examples\n    --------\n    See `tutorial_mnist_simple.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_\n\n    >>> tl.utils.fit(network, train_op=tf.optimizers.Adam(learning_rate=0.0001),\n    ...              cost=tl.cost.cross_entropy, X_train=X_train, y_train=y_train, acc=acc,\n    ...              batch_size=64, n_epoch=20, _val=X_val, y_val=y_val, eval_train=True)\n    >>> tl.utils.fit(network, train_op, cost, X_train, y_train,\n    ...            acc=acc, batch_size=500, n_epoch=200, print_freq=5,\n    ...            X_val=X_val, y_val=y_val, eval_train=False, tensorboard=True)\n\n    Notes\n    --------\n    \'tensorboard_weight_histograms\' and \'tensorboard_weight_histograms\' are not supported now.\n\n    """"""\n    if X_train.shape[0] < batch_size:\n        raise AssertionError(""Number of training examples should be bigger than the batch size"")\n\n    if tensorboard_dir is not None:\n        tl.logging.info(""Setting up tensorboard ..."")\n        #Set up tensorboard summaries and saver\n        tl.files.exists_or_mkdir(tensorboard_dir)\n\n        #Only write summaries for more recent TensorFlow versions\n        if hasattr(tf, \'summary\') and hasattr(tf.summary, \'create_file_writer\'):\n            train_writer = tf.summary.create_file_writer(tensorboard_dir + \'/train\')\n            val_writer = tf.summary.create_file_writer(tensorboard_dir + \'/validation\')\n            if tensorboard_graph_vis:\n                # FIXME : not sure how to add tl network graph\n                pass\n    else:\n        train_writer = None\n        val_writer = None\n\n        tl.logging.info(""Finished! use `tensorboard --logdir=%s/` to start tensorboard"" % tensorboard_dir)\n\n    tl.logging.info(""Start training the network ..."")\n    start_time_begin = time.time()\n    for epoch in range(n_epoch):\n        start_time = time.time()\n        loss_ep, _, __ = train_epoch(network, X_train, y_train, cost=cost, train_op=train_op, batch_size=batch_size)\n\n        train_loss, train_acc = None, None\n        val_loss, val_acc = None, None\n        if tensorboard_dir is not None and hasattr(tf, \'summary\'):\n            if epoch + 1 == 1 or (epoch + 1) % tensorboard_epoch_freq == 0:\n                if eval_train is True:\n                    train_loss, train_acc, _ = run_epoch(\n                        network, X_train, y_train, cost=cost, acc=acc, batch_size=batch_size\n                    )\n                    with train_writer.as_default():\n                        tf.compat.v2.summary.scalar(\'loss\', train_loss, step=epoch)\n                        if acc is not None:\n                            tf.summary.scalar(\'acc\', train_acc, step=epoch)\n                    # FIXME : there seems to be an internal error in Tensorboard (misuse of tf.name_scope)\n                    # if tensorboard_weight_histograms is not None:\n                    #     for param in network.all_weights:\n                    #         tf.summary.histogram(param.name, param, step=epoch)\n\n                if (X_val is not None) and (y_val is not None):\n                    val_loss, val_acc, _ = run_epoch(network, X_val, y_val, cost=cost, acc=acc, batch_size=batch_size)\n                    with val_writer.as_default():\n                        tf.summary.scalar(\'loss\', val_loss, step=epoch)\n                        if acc is not None:\n                            tf.summary.scalar(\'acc\', val_acc, step=epoch)\n                        # FIXME : there seems to be an internal error in Tensorboard (misuse of tf.name_scope)\n                        # if tensorboard_weight_histograms is not None:\n                        #     for param in network.all_weights:\n                        #         tf.summary.histogram(param.name, param, step=epoch)\n\n        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n            if (X_val is not None) and (y_val is not None):\n                tl.logging.info(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n                if eval_train is True:\n                    if train_loss is None:\n                        train_loss, train_acc, _ = run_epoch(\n                            network, X_train, y_train, cost=cost, acc=acc, batch_size=batch_size\n                        )\n                    tl.logging.info(""   train loss: %f"" % train_loss)\n                    if acc is not None:\n                        tl.logging.info(""   train acc: %f"" % train_acc)\n                if val_loss is None:\n                    val_loss, val_acc, _ = run_epoch(network, X_val, y_val, cost=cost, acc=acc, batch_size=batch_size)\n\n                # tl.logging.info(""   val loss: %f"" % val_loss)\n\n                if acc is not None:\n                    pass\n                    # tl.logging.info(""   val acc: %f"" % val_acc)\n            else:\n                tl.logging.info(\n                    ""Epoch %d of %d took %fs, loss %f"" % (epoch + 1, n_epoch, time.time() - start_time, loss_ep)\n                )\n    tl.logging.info(""Total training time: %fs"" % (time.time() - start_time_begin))\n\n\ndef test(network, acc, X_test, y_test, batch_size, cost=None):\n    """"""\n    Test a given non time-series network by the given test data and metric.\n\n    Parameters\n    ----------\n    network : TensorLayer Model\n        The network.\n    acc : TensorFlow/numpy expression or None\n        Metric for accuracy or others.\n            - If None, would not print the information.\n    X_test : numpy.array\n        The input of testing data.\n    y_test : numpy array\n        The target of testing data\n    batch_size : int or None\n        The batch size for testing, when dataset is large, we should use minibatche for testing;\n        if dataset is small, we can set it to None.\n    cost : TensorLayer or TensorFlow loss function\n        Metric for loss function, e.g tl.cost.cross_entropy. If None, would not print the information.\n\n    Examples\n    --------\n    See `tutorial_mnist_simple.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_\n\n    >>> def acc(_logits, y_batch):\n    ...     return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n    >>> tl.utils.test(network, acc, X_test, y_test, batch_size=None, cost=tl.cost.cross_entropy)\n\n    """"""\n    tl.logging.info(\'Start testing the network ...\')\n    network.eval()\n    if batch_size is None:\n        y_pred = network(X_test)\n        if cost is not None:\n            test_loss = cost(y_pred, y_test)\n            # tl.logging.info(""   test loss: %f"" % test_loss)\n        test_acc = acc(y_pred, y_test)\n        # tl.logging.info(""   test acc: %f"" % (test_acc / test_acc))\n        return test_acc\n    else:\n        test_loss, test_acc, n_batch = run_epoch(\n            network, X_test, y_test, cost=cost, acc=acc, batch_size=batch_size, shuffle=False\n        )\n        if cost is not None:\n            tl.logging.info(""   test loss: %f"" % test_loss)\n        tl.logging.info(""   test acc: %f"" % test_acc)\n        return test_acc\n\n\ndef predict(network, X, batch_size=None):\n    """"""\n    Return the predict results of given non time-series network.\n\n    Parameters\n    ----------\n    network : TensorLayer Model\n        The network.\n    X : numpy.array\n        The inputs.\n    batch_size : int or None\n        The batch size for prediction, when dataset is large, we should use minibatche for prediction;\n        if dataset is small, we can set it to None.\n\n    Examples\n    --------\n    See `tutorial_mnist_simple.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_\n\n    >>> _logits = tl.utils.predict(network, X_test)\n    >>> y_pred = np.argmax(_logits, 1)\n\n    """"""\n    network.eval()\n    if batch_size is None:\n        y_pred = network(X)\n        return y_pred\n    else:\n        result = None\n        for X_a, _ in tl.iterate.minibatches(X, X, batch_size, shuffle=False):\n            result_a = network(X_a)\n            if result is None:\n                result = result_a\n            else:\n                result = np.concatenate((result, result_a))\n        if result is None:\n            if len(X) % batch_size == 0:\n                result_a = network(X[-(len(X) % batch_size):, :])\n                result = result_a\n        else:\n            if len(X) != len(result) and len(X) % batch_size != 0:\n                result_a = network(X[-(len(X) % batch_size):, :])\n                result = np.concatenate((result, result_a))\n        return result\n\n\n## Evaluation\ndef evaluation(y_test=None, y_predict=None, n_classes=None):\n    """"""\n    Input the predicted results, targets results and\n    the number of class, return the confusion matrix, F1-score of each class,\n    accuracy and macro F1-score.\n\n    Parameters\n    ----------\n    y_test : list\n        The target results\n    y_predict : list\n        The predicted results\n    n_classes : int\n        The number of classes\n\n    Examples\n    --------\n    >>> c_mat, f1, acc, f1_macro = tl.utils.evaluation(y_test, y_predict, n_classes)\n\n    """"""\n    c_mat = confusion_matrix(y_test, y_predict, labels=[x for x in range(n_classes)])\n    f1 = f1_score(y_test, y_predict, average=None, labels=[x for x in range(n_classes)])\n    f1_macro = f1_score(y_test, y_predict, average=\'macro\')\n    acc = accuracy_score(y_test, y_predict)\n    tl.logging.info(\'confusion matrix: \\n%s\' % c_mat)\n    tl.logging.info(\'f1-score        : %s\' % f1)\n    tl.logging.info(\'f1-score(macro) : %f\' % f1_macro)  # same output with > f1_score(y_true, y_pred, average=\'macro\')\n    tl.logging.info(\'accuracy-score  : %f\' % acc)\n    return c_mat, f1, acc, f1_macro\n\n\ndef dict_to_one(dp_dict):\n    """"""Input a dictionary, return a dictionary that all items are set to one.\n\n    Used for disable dropout, dropconnect layer and so on.\n\n    Parameters\n    ----------\n    dp_dict : dictionary\n        The dictionary contains key and number, e.g. keeping probabilities.\n\n    """"""\n    return {x: 1 for x in dp_dict}\n\n\ndef flatten_list(list_of_list):\n    """"""Input a list of list, return a list that all items are in a list.\n\n    Parameters\n    ----------\n    list_of_list : a list of list\n\n    Examples\n    --------\n    >>> tl.utils.flatten_list([[1, 2, 3],[4, 5],[6]])\n    [1, 2, 3, 4, 5, 6]\n\n    """"""\n    return sum(list_of_list, [])\n\n\ndef class_balancing_oversample(X_train=None, y_train=None, printable=True):\n    """"""Input the features and labels, return the features and labels after oversampling.\n\n    Parameters\n    ----------\n    X_train : numpy.array\n        The inputs.\n    y_train : numpy.array\n        The targets.\n\n    Examples\n    --------\n    One X\n\n    >>> X_train, y_train = class_balancing_oversample(X_train, y_train, printable=True)\n\n    Two X\n\n    >>> X, y = tl.utils.class_balancing_oversample(X_train=np.hstack((X1, X2)), y_train=y, printable=False)\n    >>> X1 = X[:, 0:5]\n    >>> X2 = X[:, 5:]\n\n    """"""\n    # ======== Classes balancing\n    if printable:\n        tl.logging.info(""Classes balancing for training examples..."")\n\n    c = Counter(y_train)\n\n    if printable:\n        tl.logging.info(\'the occurrence number of each stage: %s\' % c.most_common())\n        tl.logging.info(\'the least stage is Label %s have %s instances\' % c.most_common()[-1])\n        tl.logging.info(\'the most stage is  Label %s have %s instances\' % c.most_common(1)[0])\n\n    most_num = c.most_common(1)[0][1]\n\n    if printable:\n        tl.logging.info(\'most num is %d, all classes tend to be this num\' % most_num)\n\n    locations = {}\n    number = {}\n\n    for lab, num in c.most_common():  # find the index from y_train\n        number[lab] = num\n        locations[lab] = np.where(np.array(y_train) == lab)[0]\n    if printable:\n        tl.logging.info(\'convert list(np.array) to dict format\')\n    X = {}  # convert list to dict\n    for lab, num in number.items():\n        X[lab] = X_train[locations[lab]]\n\n    # oversampling\n    if printable:\n        tl.logging.info(\'start oversampling\')\n    for key in X:\n        temp = X[key]\n        while True:\n            if len(X[key]) >= most_num:\n                break\n            X[key] = np.vstack((X[key], temp))\n    if printable:\n        tl.logging.info(\'first features of label 0 > %d\' % len(X[0][0]))\n        tl.logging.info(\'the occurrence num of each stage after oversampling\')\n    for key in X:\n        tl.logging.info(""%s %d"" % (key, len(X[key])))\n    if printable:\n        tl.logging.info(\'make each stage have same num of instances\')\n    for key in X:\n        X[key] = X[key][0:most_num, :]\n        tl.logging.info(""%s %d"" % (key, len(X[key])))\n\n    # convert dict to list\n    if printable:\n        tl.logging.info(\'convert from dict to list format\')\n    y_train = []\n    X_train = np.empty(shape=(0, len(X[0][0])))\n    for key in X:\n        X_train = np.vstack((X_train, X[key]))\n        y_train.extend([key for i in range(len(X[key]))])\n    # tl.logging.info(len(X_train), len(y_train))\n    c = Counter(y_train)\n    if printable:\n        tl.logging.info(\'the occurrence number of each stage after oversampling: %s\' % c.most_common())\n    # ================ End of Classes balancing\n    return X_train, y_train\n\n\n## Random\ndef get_random_int(min_v=0, max_v=10, number=5, seed=None):\n    """"""Return a list of random integer by the given range and quantity.\n\n    Parameters\n    -----------\n    min_v : number\n        The minimum value.\n    max_v : number\n        The maximum value.\n    number : int\n        Number of value.\n    seed : int or None\n        The seed for random.\n\n    Examples\n    ---------\n    >>> r = get_random_int(min_v=0, max_v=10, number=5)\n    [10, 2, 3, 3, 7]\n\n    """"""\n    rnd = random.Random()\n    if seed:\n        rnd = random.Random(seed)\n    # return [random.randint(min,max) for p in range(0, number)]\n    return [rnd.randint(min_v, max_v) for p in range(0, number)]\n\n\ndef list_string_to_dict(string):\n    """"""Inputs ``[\'a\', \'b\', \'c\']``, returns ``{\'a\': 0, \'b\': 1, \'c\': 2}``.""""""\n    dictionary = {}\n    for idx, c in enumerate(string):\n        dictionary.update({c: idx})\n    return dictionary\n\n\ndef exit_tensorflow(port=6006):\n    """"""Close TensorBoard and Nvidia-process if available.\n\n    Parameters\n    ----------\n    port : int\n        TensorBoard port you want to close, `6006` as default.\n\n    """"""\n    text = ""[TL] Close tensorboard and nvidia-process if available""\n    text2 = ""[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on ""\n\n    if _platform == ""linux"" or _platform == ""linux2"":\n        tl.logging.info(\'linux: %s\' % text)\n        os.system(\'nvidia-smi\')\n        os.system(\'fuser \' + str(port) + \'/tcp -k\')  # kill tensorboard 6006\n        os.system(""nvidia-smi | grep python |awk \'{print $3}\'|xargs kill"")  # kill all nvidia-smi python process\n        _exit()\n\n    elif _platform == ""darwin"":\n        tl.logging.info(\'OS X: %s\' % text)\n        subprocess.Popen(\n            ""lsof -i tcp:"" + str(port) + ""  | grep -v PID | awk \'{print $2}\' | xargs kill"", shell=True\n        )  # kill tensorboard\n    elif _platform == ""win32"":\n        raise NotImplementedError(""this function is not supported on the Windows platform"")\n\n    else:\n        tl.logging.info(text2 + _platform)\n\n\ndef open_tensorboard(log_dir=\'/tmp/tensorflow\', port=6006):\n    """"""Open Tensorboard.\n\n    Parameters\n    ----------\n    log_dir : str\n        Directory where your tensorboard logs are saved\n    port : int\n        TensorBoard port you want to open, 6006 is tensorboard default\n\n    """"""\n    text = ""[TL] Open tensorboard, go to localhost:"" + str(port) + "" to access""\n    text2 = "" not yet supported by this function (tl.ops.open_tb)""\n\n    if not tl.files.exists_or_mkdir(log_dir, verbose=False):\n        tl.logging.info(""[TL] Log reportory was created at %s"" % log_dir)\n\n    if _platform == ""linux"" or _platform == ""linux2"":\n        tl.logging.info(\'linux: %s\' % text)\n        subprocess.Popen(\n            sys.prefix + "" | python -m tensorflow.tensorboard --logdir="" + log_dir + "" --port="" + str(port), shell=True\n        )  # open tensorboard in localhost:6006/ or whatever port you chose\n    elif _platform == ""darwin"":\n        tl.logging.info(\'OS X: %s\' % text)\n        subprocess.Popen(\n            sys.prefix + "" | python -m tensorflow.tensorboard --logdir="" + log_dir + "" --port="" + str(port), shell=True\n        )  # open tensorboard in localhost:6006/ or whatever port you chose\n    elif _platform == ""win32"":\n        raise NotImplementedError(""this function is not supported on the Windows platform"")\n    else:\n        tl.logging.info(_platform + text2)\n\n\ndef clear_all_placeholder_variables(printable=True):\n    """"""Clears all the placeholder variables of keep prob,\n    including keeping probabilities of all dropout, denoising, dropconnect etc.\n\n    Parameters\n    ----------\n    printable : boolean\n        If True, print all deleted variables.\n\n    """"""\n    tl.logging.info(\'clear all .....................................\')\n    gl = globals().copy()\n    for var in gl:\n        if var[0] == \'_\': continue\n        if \'func\' in str(globals()[var]): continue\n        if \'module\' in str(globals()[var]): continue\n        if \'class\' in str(globals()[var]): continue\n\n        if printable:\n            tl.logging.info("" clear_all ------- %s"" % str(globals()[var]))\n\n        del globals()[var]\n\n\ndef set_gpu_fraction(gpu_fraction=0.3):\n    """"""Set the GPU memory fraction for the application.\n\n    Parameters\n    ----------\n    gpu_fraction : None or float\n        Fraction of GPU memory, (0 ~ 1]. If None, allow gpu memory growth.\n\n    References\n    ----------\n    - `TensorFlow using GPU <https://www.tensorflow.org/alpha/guide/using_gpu#allowing_gpu_memory_growth>`__\n\n    """"""\n    if gpu_fraction is None:\n        tl.logging.info(""[TL]: ALLOW GPU MEM GROWTH"")\n        tf.config.gpu.set_per_process_memory_growth(True)\n    else:\n        tl.logging.info(""[TL]: GPU MEM Fraction %f"" % gpu_fraction)\n        tf.config.gpu.set_per_process_memory_fraction(0.4)\n    # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n    # sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n    # return sess\n\n\ndef train_epoch(\n    network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True\n):\n    """"""Training a given non time-series network by the given cost function, training data, batch_size etc.\n    for one epoch.\n\n    Parameters\n    ----------\n    network : TensorLayer Model\n        the network to be trained.\n    X : numpy.array\n        The input of training data\n    y : numpy.array\n        The target of training data\n    cost : TensorLayer or TensorFlow loss function\n        Metric for loss function, e.g tl.cost.cross_entropy.\n    train_op : TensorFlow optimizer\n        The optimizer for training e.g. tf.optimizers.Adam().\n    acc : TensorFlow/numpy expression or None\n        Metric for accuracy or others. If None, would not print the information.\n    batch_size : int\n        The batch size for training and evaluating.\n    shuffle : boolean\n        Indicating whether to shuffle the dataset in training.\n\n    Returns\n    -------\n    loss_ep : Tensor. Average loss of this epoch.\n    acc_ep : Tensor or None. Average accuracy(metric) of this epoch. None if acc is not given.\n    n_step : int. Number of iterations taken in this epoch.\n\n    """"""\n    network.train()\n    loss_ep = 0\n    acc_ep = 0\n    n_step = 0\n    for X_batch, y_batch in tl.iterate.minibatches(X, y, batch_size, shuffle=shuffle):\n        _loss, _acc = _train_step(network, X_batch, y_batch, cost=cost, train_op=train_op, acc=acc)\n\n        loss_ep += _loss\n        if acc is not None:\n            acc_ep += _acc\n        n_step += 1\n\n    loss_ep = loss_ep / n_step\n    acc_ep = acc_ep / n_step if acc is not None else None\n\n    return loss_ep, acc_ep, n_step\n\n\ndef run_epoch(network, X, y, cost=None, acc=None, batch_size=100, shuffle=False):\n    """"""Run a given non time-series network by the given cost function, test data, batch_size etc.\n    for one epoch.\n\n    Parameters\n    ----------\n    network : TensorLayer Model\n        the network to be trained.\n    X : numpy.array\n        The input of training data\n    y : numpy.array\n        The target of training data\n    cost : TensorLayer or TensorFlow loss function\n        Metric for loss function, e.g tl.cost.cross_entropy.\n    acc : TensorFlow/numpy expression or None\n        Metric for accuracy or others. If None, would not print the information.\n    batch_size : int\n        The batch size for training and evaluating.\n    shuffle : boolean\n        Indicating whether to shuffle the dataset in training.\n\n    Returns\n    -------\n    loss_ep : Tensor. Average loss of this epoch. None if \'cost\' is not given.\n    acc_ep : Tensor. Average accuracy(metric) of this epoch. None if \'acc\' is not given.\n    n_step : int. Number of iterations taken in this epoch.\n    """"""\n    network.eval()\n    loss_ep = 0\n    acc_ep = 0\n    n_step = 0\n    for X_batch, y_batch in tl.iterate.minibatches(X, y, batch_size, shuffle=shuffle):\n        _loss, _acc = _run_step(network, X_batch, y_batch, cost=cost, acc=acc)\n        if cost is not None:\n            loss_ep += _loss\n        if acc is not None:\n            acc_ep += _acc\n        n_step += 1\n\n    loss_ep = loss_ep / n_step if cost is not None else None\n    acc_ep = acc_ep / n_step if acc is not None else None\n\n    return loss_ep, acc_ep, n_step\n\n\n@tf.function\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    """"""Train for one step""""""\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\n# @tf.function # FIXME : enable tf.function will cause some bugs in numpy, need fixing\ndef _run_step(network, X_batch, y_batch, cost=None, acc=None):\n    """"""Run for one step""""""\n    y_pred = network(X_batch)\n    _loss, _acc = None, None\n    if cost is not None:\n        _loss = cost(y_pred, y_batch)\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n    return _loss, _acc\n'"
tensorlayer/visualize.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nimport imageio\nimport numpy as np\n\nimport tensorlayer as tl\nfrom tensorlayer.lazy_imports import LazyImport\n\ncv2 = LazyImport(""cv2"")\n\n# Uncomment the following line if you got: _tkinter.TclError: no display name and no $DISPLAY environment variable\n# import matplotlib\n# matplotlib.use(\'Agg\')\n\n__all__ = [\n    \'read_image\',\n    \'read_images\',\n    \'save_image\',\n    \'save_images\',\n    \'draw_boxes_and_labels_to_image\',\n    \'draw_mpii_people_to_image\',\n    \'frame\',\n    \'CNN2d\',\n    \'images2d\',\n    \'tsne_embedding\',\n    \'draw_weights\',\n    \'W\',\n]\n\n\ndef read_image(image, path=\'\'):\n    """"""Read one image.\n\n    Parameters\n    -----------\n    image : str\n        The image file name.\n    path : str\n        The image folder path.\n\n    Returns\n    -------\n    numpy.array\n        The image.\n\n    """"""\n    return imageio.imread(os.path.join(path, image))\n\n\ndef read_images(img_list, path=\'\', n_threads=10, printable=True):\n    """"""Returns all images in list by given path and name of each image file.\n\n    Parameters\n    -------------\n    img_list : list of str\n        The image file names.\n    path : str\n        The image folder path.\n    n_threads : int\n        The number of threads to read image.\n    printable : boolean\n        Whether to print information when reading images.\n\n    Returns\n    -------\n    list of numpy.array\n        The images.\n\n    """"""\n    imgs = []\n    for idx in range(0, len(img_list), n_threads):\n        b_imgs_list = img_list[idx:idx + n_threads]\n        b_imgs = tl.prepro.threading_data(b_imgs_list, fn=read_image, path=path)\n        # tl.logging.info(b_imgs.shape)\n        imgs.extend(b_imgs)\n        if printable:\n            tl.logging.info(\'read %d from %s\' % (len(imgs), path))\n    return imgs\n\n\ndef save_image(image, image_path=\'_temp.png\'):\n    """"""Save a image.\n\n    Parameters\n    -----------\n    image : numpy array\n        [w, h, c]\n    image_path : str\n        path\n\n    """"""\n    try:  # RGB\n        imageio.imwrite(image_path, image)\n    except Exception:  # Greyscale\n        imageio.imwrite(image_path, image[:, :, 0])\n\n\ndef save_images(images, size, image_path=\'_temp.png\'):\n    """"""Save multiple images into one single image.\n\n    Parameters\n    -----------\n    images : numpy array\n        (batch, w, h, c)\n    size : list of 2 ints\n        row and column number.\n        number of images should be equal or less than size[0] * size[1]\n    image_path : str\n        save path\n\n    Examples\n    ---------\n    >>> import numpy as np\n    >>> import tensorlayer as tl\n    >>> images = np.random.rand(64, 100, 100, 3)\n    >>> tl.visualize.save_images(images, [8, 8], \'temp.png\')\n\n    """"""\n    if len(images.shape) == 3:  # Greyscale [batch, h, w] --> [batch, h, w, 1]\n        images = images[:, :, :, np.newaxis]\n\n    def merge(images, size):\n        h, w = images.shape[1], images.shape[2]\n        img = np.zeros((h * size[0], w * size[1], 3), dtype=images.dtype)\n        for idx, image in enumerate(images):\n            i = idx % size[1]\n            j = idx // size[1]\n            img[j * h:j * h + h, i * w:i * w + w, :] = image\n        return img\n\n    def imsave(images, size, path):\n        if np.max(images) <= 1 and (-1 <= np.min(images) < 0):\n            images = ((images + 1) * 127.5).astype(np.uint8)\n        elif np.max(images) <= 1 and np.min(images) >= 0:\n            images = (images * 255).astype(np.uint8)\n\n        return imageio.imwrite(path, merge(images, size))\n\n    if len(images) > size[0] * size[1]:\n        raise AssertionError(""number of images should be equal or less than size[0] * size[1] {}"".format(len(images)))\n\n    return imsave(images, size, image_path)\n\n\ndef draw_boxes_and_labels_to_image(\n    image, classes, coords, scores, classes_list, is_center=True, is_rescale=True, save_name=None\n):\n    """"""Draw bboxes and class labels on image. Return or save the image with bboxes, example in the docs of ``tl.prepro``.\n\n    Parameters\n    -----------\n    image : numpy.array\n        The RGB image [height, width, channel].\n    classes : list of int\n        A list of class ID (int).\n    coords : list of int\n        A list of list for coordinates.\n            - Should be [x, y, x2, y2] (up-left and botton-right format)\n            - If [x_center, y_center, w, h] (set is_center to True).\n    scores : list of float\n        A list of score (float). (Optional)\n    classes_list : list of str\n        for converting ID to string on image.\n    is_center : boolean\n        Whether the coordinates is [x_center, y_center, w, h]\n            - If coordinates are [x_center, y_center, w, h], set it to True for converting it to [x, y, x2, y2] (up-left and botton-right) internally.\n            - If coordinates are [x1, x2, y1, y2], set it to False.\n    is_rescale : boolean\n        Whether to rescale the coordinates from pixel-unit format to ratio format.\n            - If True, the input coordinates are the portion of width and high, this API will scale the coordinates to pixel unit internally.\n            - If False, feed the coordinates with pixel unit format.\n    save_name : None or str\n        The name of image file (i.e. image.png), if None, not to save image.\n\n    Returns\n    -------\n    numpy.array\n        The saved image.\n\n    References\n    -----------\n    - OpenCV rectangle and putText.\n    - `scikit-image <http://scikit-image.org/docs/dev/api/skimage.draw.html#skimage.draw.rectangle>`__.\n\n    """"""\n    if len(coords) != len(classes):\n        raise AssertionError(""number of coordinates and classes are equal"")\n\n    if len(scores) > 0 and len(scores) != len(classes):\n        raise AssertionError(""number of scores and classes are equal"")\n\n    # don\'t change the original image, and avoid error https://stackoverflow.com/questions/30249053/python-opencv-drawing-errors-after-manipulating-array-with-numpy\n    image = image.copy()\n\n    imh, imw = image.shape[0:2]\n    thick = int((imh + imw) // 430)\n\n    for i, _v in enumerate(coords):\n        if is_center:\n            x, y, x2, y2 = tl.prepro.obj_box_coord_centroid_to_upleft_butright(coords[i])\n        else:\n            x, y, x2, y2 = coords[i]\n\n        if is_rescale:  # scale back to pixel unit if the coords are the portion of width and high\n            x, y, x2, y2 = tl.prepro.obj_box_coord_scale_to_pixelunit([x, y, x2, y2], (imh, imw))\n\n        cv2.rectangle(\n            image,\n            (int(x), int(y)),\n            (int(x2), int(y2)),  # up-left and botton-right\n            [0, 255, 0],\n            thick\n        )\n\n        cv2.putText(\n            image,\n            classes_list[classes[i]] + (("" %.2f"" % (scores[i])) if (len(scores) != 0) else "" ""),\n            (int(x), int(y)),  # button left\n            0,\n            1.5e-3 * imh,  # bigger = larger font\n            [0, 0, 256],  # self.meta[\'colors\'][max_indx],\n            int(thick / 2) + 1\n        )  # bold\n\n    if save_name is not None:\n        # cv2.imwrite(\'_my.png\', image)\n        save_image(image, save_name)\n    # if len(coords) == 0:\n    #     tl.logging.info(""draw_boxes_and_labels_to_image: no bboxes exist, cannot draw !"")\n    return image\n\n\ndef draw_mpii_pose_to_image(image, poses, save_name=\'image.png\'):\n    """"""Draw people(s) into image using MPII dataset format as input, return or save the result image.\n\n    This is an experimental API, can be changed in the future.\n\n    Parameters\n    -----------\n    image : numpy.array\n        The RGB image [height, width, channel].\n    poses : list of dict\n        The people(s) annotation in MPII format, see ``tl.files.load_mpii_pose_dataset``.\n    save_name : None or str\n        The name of image file (i.e. image.png), if None, not to save image.\n\n    Returns\n    --------\n    numpy.array\n        The saved image.\n\n    Examples\n    --------\n    >>> import pprint\n    >>> import tensorlayer as tl\n    >>> img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()\n    >>> image = tl.vis.read_image(img_train_list[0])\n    >>> tl.vis.draw_mpii_pose_to_image(image, ann_train_list[0], \'image.png\')\n    >>> pprint.pprint(ann_train_list[0])\n\n    References\n    -----------\n    - `MPII Keyponts and ID <http://human-pose.mpi-inf.mpg.de/#download>`__\n    """"""\n    # import skimage\n    # don\'t change the original image, and avoid error https://stackoverflow.com/questions/30249053/python-opencv-drawing-errors-after-manipulating-array-with-numpy\n    image = image.copy()\n\n    imh, imw = image.shape[0:2]\n    thick = int((imh + imw) // 430)\n    # radius = int(image.shape[1] / 500) + 1\n    radius = int(thick * 1.5)\n\n    if image.max() < 1:\n        image = image * 255\n\n    for people in poses:\n        # Pose Keyponts\n        joint_pos = people[\'joint_pos\']\n        # draw sketch\n        # joint id (0 - r ankle, 1 - r knee, 2 - r hip, 3 - l hip, 4 - l knee,\n        #           5 - l ankle, 6 - pelvis, 7 - thorax, 8 - upper neck,\n        #           9 - head top, 10 - r wrist, 11 - r elbow, 12 - r shoulder,\n        #           13 - l shoulder, 14 - l elbow, 15 - l wrist)\n        #\n        #               9\n        #               8\n        #         12 ** 7 ** 13\n        #        *      *      *\n        #       11      *       14\n        #      *        *         *\n        #     10    2 * 6 * 3     15\n        #           *       *\n        #           1       4\n        #           *       *\n        #           0       5\n\n        lines = [\n            [(0, 1), [100, 255, 100]],\n            [(1, 2), [50, 255, 50]],\n            [(2, 6), [0, 255, 0]],  # right leg\n            [(3, 4), [100, 100, 255]],\n            [(4, 5), [50, 50, 255]],\n            [(6, 3), [0, 0, 255]],  # left leg\n            [(6, 7), [255, 255, 100]],\n            [(7, 8), [255, 150, 50]],  # body\n            [(8, 9), [255, 200, 100]],  # head\n            [(10, 11), [255, 100, 255]],\n            [(11, 12), [255, 50, 255]],\n            [(12, 8), [255, 0, 255]],  # right hand\n            [(8, 13), [0, 255, 255]],\n            [(13, 14), [100, 255, 255]],\n            [(14, 15), [200, 255, 255]]  # left hand\n        ]\n        for line in lines:\n            start, end = line[0]\n            if (start in joint_pos) and (end in joint_pos):\n                cv2.line(\n                    image,\n                    (int(joint_pos[start][0]), int(joint_pos[start][1])),\n                    (int(joint_pos[end][0]), int(joint_pos[end][1])),  # up-left and botton-right\n                    line[1],\n                    thick\n                )\n                # rr, cc, val = skimage.draw.line_aa(int(joint_pos[start][1]), int(joint_pos[start][0]), int(joint_pos[end][1]), int(joint_pos[end][0]))\n                # image[rr, cc] = line[1]\n        # draw circles\n        for pos in joint_pos.items():\n            _, pos_loc = pos  # pos_id, pos_loc\n            pos_loc = (int(pos_loc[0]), int(pos_loc[1]))\n            cv2.circle(image, center=pos_loc, radius=radius, color=(200, 200, 200), thickness=-1)\n            # rr, cc = skimage.draw.circle(int(pos_loc[1]), int(pos_loc[0]), radius)\n            # image[rr, cc] = [0, 255, 0]\n\n        # Head\n        head_rect = people[\'head_rect\']\n        if head_rect:  # if head exists\n            cv2.rectangle(\n                image,\n                (int(head_rect[0]), int(head_rect[1])),\n                (int(head_rect[2]), int(head_rect[3])),  # up-left and botton-right\n                [0, 180, 0],\n                thick\n            )\n\n    if save_name is not None:\n        # cv2.imwrite(save_name, image)\n        save_image(image, save_name)\n    return image\n\n\ndraw_mpii_people_to_image = draw_mpii_pose_to_image\n\n\ndef frame(I=None, second=5, saveable=True, name=\'frame\', cmap=None, fig_idx=12836):\n    """"""Display a frame. Make sure OpenAI Gym render() is disable before using it.\n\n    Parameters\n    ----------\n    I : numpy.array\n        The image.\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    cmap : None or str\n        \'gray\' for greyscale, None for default, etc.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> env = gym.make(""Pong-v0"")\n    >>> observation = env.reset()\n    >>> tl.visualize.frame(observation)\n\n    """"""\n    import matplotlib.pyplot as plt\n    if saveable is False:\n        plt.ion()\n    plt.figure(fig_idx)  # show all feature images\n\n    if len(I.shape) and I.shape[-1] == 1:  # (10,10,1) --> (10,10)\n        I = I[:, :, 0]\n\n    plt.imshow(I, cmap)\n    plt.title(name)\n    # plt.gca().xaxis.set_major_locator(plt.NullLocator())    # distable tick\n    # plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\n    if saveable:\n        plt.savefig(name + \'.pdf\', format=\'pdf\')\n    else:\n        plt.draw()\n        plt.pause(second)\n\n\ndef CNN2d(CNN=None, second=10, saveable=True, name=\'cnn\', fig_idx=3119362):\n    """"""Display a group of RGB or Greyscale CNN masks.\n\n    Parameters\n    ----------\n    CNN : numpy.array\n        The image. e.g: 64 5x5 RGB images can be (5, 5, 3, 64).\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        The matplotlib figure index.\n\n    Examples\n    --------\n    >>> tl.visualize.CNN2d(network.all_params[0].eval(), second=10, saveable=True, name=\'cnn1_mnist\', fig_idx=2012)\n\n    """"""\n    import matplotlib.pyplot as plt\n    # tl.logging.info(CNN.shape)    # (5, 5, 3, 64)\n    # exit()\n    n_mask = CNN.shape[3]\n    n_row = CNN.shape[0]\n    n_col = CNN.shape[1]\n    n_color = CNN.shape[2]\n    row = int(np.sqrt(n_mask))\n    col = int(np.ceil(n_mask / row))\n    plt.ion()  # active mode\n    fig = plt.figure(fig_idx)\n    count = 1\n    for _ir in range(1, row + 1):\n        for _ic in range(1, col + 1):\n            if count > n_mask:\n                break\n            fig.add_subplot(col, row, count)\n            # tl.logging.info(CNN[:,:,:,count-1].shape, n_row, n_col)   # (5, 1, 32) 5 5\n            # exit()\n            # plt.imshow(\n            #         np.reshape(CNN[count-1,:,:,:], (n_row, n_col)),\n            #         cmap=\'gray\', interpolation=""nearest"")     # theano\n            if n_color == 1:\n                plt.imshow(np.reshape(CNN[:, :, :, count - 1], (n_row, n_col)), cmap=\'gray\', interpolation=""nearest"")\n            elif n_color == 3:\n                plt.imshow(\n                    np.reshape(CNN[:, :, :, count - 1], (n_row, n_col, n_color)), cmap=\'gray\', interpolation=""nearest""\n                )\n            else:\n                raise Exception(""Unknown n_color"")\n            plt.gca().xaxis.set_major_locator(plt.NullLocator())  # distable tick\n            plt.gca().yaxis.set_major_locator(plt.NullLocator())\n            count = count + 1\n    if saveable:\n        plt.savefig(name + \'.pdf\', format=\'pdf\')\n    else:\n        plt.draw()\n        plt.pause(second)\n\n\ndef images2d(images=None, second=10, saveable=True, name=\'images\', dtype=None, fig_idx=3119362):\n    """"""Display a group of RGB or Greyscale images.\n\n    Parameters\n    ----------\n    images : numpy.array\n        The images.\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    dtype : None or numpy data type\n        The data type for displaying the images.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n    >>> tl.visualize.images2d(X_train[0:100,:,:,:], second=10, saveable=False, name=\'cifar10\', dtype=np.uint8, fig_idx=20212)\n\n    """"""\n    import matplotlib.pyplot as plt\n    # tl.logging.info(images.shape)    # (50000, 32, 32, 3)\n    # exit()\n    if dtype:\n        images = np.asarray(images, dtype=dtype)\n    n_mask = images.shape[0]\n    n_row = images.shape[1]\n    n_col = images.shape[2]\n    n_color = images.shape[3]\n    row = int(np.sqrt(n_mask))\n    col = int(np.ceil(n_mask / row))\n    plt.ion()  # active mode\n    fig = plt.figure(fig_idx)\n    count = 1\n    for _ir in range(1, row + 1):\n        for _ic in range(1, col + 1):\n            if count > n_mask:\n                break\n            fig.add_subplot(col, row, count)\n            # tl.logging.info(images[:,:,:,count-1].shape, n_row, n_col)   # (5, 1, 32) 5 5\n            # plt.imshow(\n            #         np.reshape(images[count-1,:,:,:], (n_row, n_col)),\n            #         cmap=\'gray\', interpolation=""nearest"")     # theano\n            if n_color == 1:\n                plt.imshow(np.reshape(images[count - 1, :, :], (n_row, n_col)), cmap=\'gray\', interpolation=""nearest"")\n                # plt.title(name)\n            elif n_color == 3:\n                plt.imshow(images[count - 1, :, :], cmap=\'gray\', interpolation=""nearest"")\n                # plt.title(name)\n            else:\n                raise Exception(""Unknown n_color"")\n            plt.gca().xaxis.set_major_locator(plt.NullLocator())  # distable tick\n            plt.gca().yaxis.set_major_locator(plt.NullLocator())\n            count = count + 1\n    if saveable:\n        plt.savefig(name + \'.pdf\', format=\'pdf\')\n    else:\n        plt.draw()\n        plt.pause(second)\n\n\ndef tsne_embedding(embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name=\'tsne\', fig_idx=9862):\n    """"""Visualize the embeddings by using t-SNE.\n\n    Parameters\n    ----------\n    embeddings : numpy.array\n        The embedding matrix.\n    reverse_dictionary : dictionary\n        id_to_word, mapping id to unique word.\n    plot_only : int\n        The number of examples to plot, choice the most common words.\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> see \'tutorial_word2vec_basic.py\'\n    >>> final_embeddings = normalized_embeddings.eval()\n    >>> tl.visualize.tsne_embedding(final_embeddings, labels, reverse_dictionary,\n    ...                   plot_only=500, second=5, saveable=False, name=\'tsne\')\n\n    """"""\n    import matplotlib.pyplot as plt\n\n    def plot_with_labels(low_dim_embs, labels, figsize=(18, 18), second=5, saveable=True, name=\'tsne\', fig_idx=9862):\n\n        if low_dim_embs.shape[0] < len(labels):\n            raise AssertionError(""More labels than embeddings"")\n\n        if saveable is False:\n            plt.ion()\n            plt.figure(fig_idx)\n\n        plt.figure(figsize=figsize)  # in inches\n\n        for i, label in enumerate(labels):\n            x, y = low_dim_embs[i, :]\n            plt.scatter(x, y)\n            plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=\'offset points\', ha=\'right\', va=\'bottom\')\n\n        if saveable:\n            plt.savefig(name + \'.pdf\', format=\'pdf\')\n        else:\n            plt.draw()\n            plt.pause(second)\n\n    try:\n        from sklearn.manifold import TSNE\n        from six.moves import xrange\n\n        tsne = TSNE(perplexity=30, n_components=2, init=\'pca\', n_iter=5000)\n        # plot_only = 500\n        low_dim_embs = tsne.fit_transform(embeddings[:plot_only, :])\n        labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n        plot_with_labels(low_dim_embs, labels, second=second, saveable=saveable, name=name, fig_idx=fig_idx)\n\n    except ImportError:\n        _err = ""Please install sklearn and matplotlib to visualize embeddings.""\n        tl.logging.error(_err)\n        raise ImportError(_err)\n\n\ndef draw_weights(W=None, second=10, saveable=True, shape=None, name=\'mnist\', fig_idx=2396512):\n    """"""Visualize every columns of the weight matrix to a group of Greyscale img.\n\n    Parameters\n    ----------\n    W : numpy.array\n        The weight matrix\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    shape : a list with 2 int or None\n        The shape of feature image, MNIST is [28, 80].\n    name : a string\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> tl.visualize.draw_weights(network.all_params[0].eval(), second=10, saveable=True, name=\'weight_of_1st_layer\', fig_idx=2012)\n\n    """"""\n    if shape is None:\n        shape = [28, 28]\n\n    import matplotlib.pyplot as plt\n    if saveable is False:\n        plt.ion()\n    fig = plt.figure(fig_idx)  # show all feature images\n    n_units = W.shape[1]\n\n    num_r = int(np.sqrt(n_units))  # \xe6\xaf\x8f\xe8\xa1\x8c\xe6\x98\xbe\xe7\xa4\xba\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0   \xe8\x8b\xa525\xe4\xb8\xaahidden unit -> \xe6\xaf\x8f\xe8\xa1\x8c\xe6\x98\xbe\xe7\xa4\xba5\xe4\xb8\xaa\n    num_c = int(np.ceil(n_units / num_r))\n    count = int(1)\n    for _row in range(1, num_r + 1):\n        for _col in range(1, num_c + 1):\n            if count > n_units:\n                break\n            fig.add_subplot(num_r, num_c, count)\n            # ------------------------------------------------------------\n            # plt.imshow(np.reshape(W[:,count-1],(28,28)), cmap=\'gray\')\n            # ------------------------------------------------------------\n            feature = W[:, count - 1] / np.sqrt((W[:, count - 1]**2).sum())\n            # feature[feature<0.0001] = 0   # value threshold\n            # if count == 1 or count == 2:\n            #     print(np.mean(feature))\n            # if np.std(feature) < 0.03:      # condition threshold\n            #     feature = np.zeros_like(feature)\n            # if np.mean(feature) < -0.015:      # condition threshold\n            #     feature = np.zeros_like(feature)\n            plt.imshow(\n                np.reshape(feature, (shape[0], shape[1])), cmap=\'gray\', interpolation=""nearest""\n            )  # , vmin=np.min(feature), vmax=np.max(feature))\n            # plt.title(name)\n            # ------------------------------------------------------------\n            # plt.imshow(np.reshape(W[:,count-1] ,(np.sqrt(size),np.sqrt(size))), cmap=\'gray\', interpolation=""nearest"")\n            plt.gca().xaxis.set_major_locator(plt.NullLocator())  # distable tick\n            plt.gca().yaxis.set_major_locator(plt.NullLocator())\n            count = count + 1\n    if saveable:\n        plt.savefig(name + \'.pdf\', format=\'pdf\')\n    else:\n        plt.draw()\n        plt.pause(second)\n\n\nW = draw_weights\n'"
tests/__init__.py,0,b''
tests/test_activations.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\nimport numpy as np\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Test_Leaky_ReLUs(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.alpha = 0.2\n\n        cls.vmin = 0\n        cls.vmax = 10\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_lrelu(self):\n        for i in range(-5, 15):\n\n            if i > 0:\n                good_output = i\n            else:\n                good_output = self.alpha * i\n\n            computed_output = tl.act.leaky_relu(float(i), alpha=self.alpha)\n\n            self.assertAlmostEqual(computed_output.numpy(), good_output, places=5)\n\n        net = tl.layers.Input([10, 2])\n        net = tl.layers.Dense(n_units=100, act=lambda x: tl.act.lrelu(x, 0.2), name='dense')(net)\n        print(net)\n\n    def test_lrelu6(self):\n        for i in range(-5, 15):\n\n            if i < 0:\n                good_output = self.alpha * i\n            else:\n                good_output = min(6, i)\n\n            computed_output = tl.act.leaky_relu6(float(i), alpha=self.alpha)\n\n            self.assertAlmostEqual(computed_output.numpy(), good_output, places=5)\n\n        net = tl.layers.Input([10, 2])\n        net = tl.layers.Dense(n_units=100, act=lambda x: tl.act.leaky_relu6(x, 0.2), name='dense')(net)\n        print(net)\n\n    def test_ltrelu6(self):\n        for i in range(-5, 15):\n\n            if i < 0:\n                good_output = self.alpha * i\n            elif i < 6:\n                good_output = i\n            else:\n                good_output = 6 + (self.alpha * (i - 6))\n\n            computed_output = tl.act.leaky_twice_relu6(float(i), alpha_low=self.alpha, alpha_high=self.alpha)\n\n            self.assertAlmostEqual(computed_output.numpy(), good_output, places=5)\n\n        net = tl.layers.Input([10, 200])\n        net = tl.layers.Dense(n_units=100, act=lambda x: tl.act.leaky_twice_relu6(x, 0.2, 0.2), name='dense')(net)\n        print(net)\n\n    def test_ramp(self):\n\n        for i in range(-5, 15):\n\n            if i < self.vmin:\n                good_output = self.vmin\n            elif i > self.vmax:\n                good_output = self.vmax\n            else:\n                good_output = i\n\n            computed_output = tl.act.ramp(float(i), v_min=self.vmin, v_max=self.vmax)\n\n            self.assertAlmostEqual(computed_output.numpy(), good_output, places=5)\n\n    def test_sign(self):\n\n        for i in range(-5, 15):\n\n            if i < 0:\n                good_output = -1\n            elif i == 0:\n                good_output = 0\n            else:\n                good_output = 1\n\n            computed_output = tl.act.sign(float(i))\n\n            self.assertAlmostEqual(computed_output.numpy(), good_output, places=5)\n\n    def test_swish(self):\n        import numpy as np\n\n        for i in range(-5, 15):\n\n            good_output = i / (1 + np.math.exp(-i))\n\n            computed_output = tl.act.swish(float(i))\n\n            self.assertAlmostEqual(computed_output.numpy(), good_output, places=5)\n\n    def test_mish(self):\n        for i in range(-5, 15):\n            good_output = i * np.tanh(np.math.log(1 + np.math.exp(i)))\n\n            computed_output = tl.act.mish(float(i))\n\n            self.assertAlmostEqual(computed_output.numpy(), good_output, places=5)\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n"""
tests/test_initializers.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Test_Leaky_ReLUs(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.ni = tl.layers.Input(shape=[16, 10])\n        cls.w_shape = (10, 5)\n        cls.eps = 0.0\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def init_dense(self, w_init):\n        return tl.layers.Dense(n_units=self.w_shape[1], in_channels=self.w_shape[0], W_init=w_init)\n\n    def test_zeros(self):\n        dense = self.init_dense(tl.initializers.zeros())\n        self.assertEqual(np.sum(dense.all_weights[0].numpy() - np.zeros(shape=self.w_shape)), self.eps)\n        nn = dense(self.ni)\n\n    def test_ones(self):\n        dense = self.init_dense(tl.initializers.ones())\n        self.assertEqual(np.sum(dense.all_weights[0].numpy() - np.ones(shape=self.w_shape)), self.eps)\n        nn = dense(self.ni)\n\n    def test_constant(self):\n        dense = self.init_dense(tl.initializers.constant(value=5.0))\n        self.assertEqual(np.sum(dense.all_weights[0].numpy() - np.ones(shape=self.w_shape) * 5.0), self.eps)\n        nn = dense(self.ni)\n\n        # test with numpy arr\n        arr = np.random.uniform(size=self.w_shape).astype(np.float32)\n        dense = self.init_dense(tl.initializers.constant(value=arr))\n        self.assertEqual(np.sum(dense.all_weights[0].numpy() - arr), self.eps)\n        nn = dense(self.ni)\n\n    def test_RandomUniform(self):\n        dense = self.init_dense(tl.initializers.random_uniform(minval=-0.1, maxval=0.1, seed=1234))\n        print(dense.all_weights[0].numpy())\n        nn = dense(self.ni)\n\n    def test_RandomNormal(self):\n        dense = self.init_dense(tl.initializers.random_normal(mean=0.0, stddev=0.1))\n        print(dense.all_weights[0].numpy())\n        nn = dense(self.ni)\n\n    def test_TruncatedNormal(self):\n        dense = self.init_dense(tl.initializers.truncated_normal(mean=0.0, stddev=0.1))\n        print(dense.all_weights[0].numpy())\n        nn = dense(self.ni)\n\n    def test_deconv2d_bilinear_upsampling_initializer(self):\n        rescale_factor = 2\n        imsize = 128\n        num_channels = 3\n        num_in_channels = 3\n        num_out_channels = 3\n        filter_shape = (5, 5, num_out_channels, num_in_channels)\n        ni = tl.layers.Input(shape=(1, imsize, imsize, num_channels))\n        bilinear_init = tl.initializers.deconv2d_bilinear_upsampling_initializer(shape=filter_shape)\n        deconv_layer = tl.layers.DeConv2dLayer(\n            shape=filter_shape, outputs_shape=(1, imsize * rescale_factor, imsize * rescale_factor, num_out_channels),\n            strides=(1, rescale_factor, rescale_factor, 1), W_init=bilinear_init, padding='SAME', act=None,\n            name='g/h1/decon2d'\n        )\n        nn = deconv_layer(ni)\n\n    def test_config(self):\n        init = tl.initializers.constant(value=5.0)\n        new_init = tl.initializers.Constant.from_config(init.get_config())\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n"""
tests/test_nlp.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport nltk\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\nnltk.download(\'punkt\')\n\n\nclass Test_Leaky_ReLUs(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_as_bytes(self):\n        origin_str = ""str""\n        origin_bytes = b\'bytes\'\n        converted_str = tl.nlp.as_bytes(origin_str)\n        converted_bytes = tl.nlp.as_bytes(origin_bytes)\n        print(\'str after using as_bytes:\', converted_str)\n        print(\'bytes after using as_bytes:\', converted_bytes)\n\n    def test_as_text(self):\n        origin_str = ""str""\n        origin_bytes = b\'bytes\'\n        converted_str = tl.nlp.as_text(origin_str)\n        converted_bytes = tl.nlp.as_text(origin_bytes)\n        print(\'str after using as_text:\', converted_str)\n        print(\'bytes after using as_text:\', converted_bytes)\n\n    def test_save_vocab(self):\n        words = tl.files.load_matt_mahoney_text8_dataset()\n        vocabulary_size = 50000\n        data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n        tl.nlp.save_vocab(count, name=\'vocab_text8.txt\')\n\n    def test_basic_tokenizer(self):\n        c = ""how are you?""\n        tokens = tl.nlp.basic_tokenizer(c)\n        print(tokens)\n\n    def test_generate_skip_gram_batch(self):\n        data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n        batch, labels, data_index = tl.nlp.generate_skip_gram_batch(\n            data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0\n        )\n        print(batch)\n        print(labels)\n\n    def test_process_sentence(self):\n        c = ""how are you?""\n        c = tl.nlp.process_sentence(c)\n        print(c)\n\n    def test_words_to_word_id(self):\n        words = tl.files.load_matt_mahoney_text8_dataset()\n        vocabulary_size = 50000\n        data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n        ids = tl.nlp.words_to_word_ids(words, dictionary)\n        context = tl.nlp.word_ids_to_words(ids, reverse_dictionary)\n        # print(ids)\n        # print(context)\n\n\nif __name__ == \'__main__\':\n\n    unittest.main()\n'"
examples/basic_tutorials/tutorial_cifar10_cnn_static.py,24,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport multiprocessing\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (BatchNorm, Conv2d, Dense, Flatten, Input, LocalResponseNorm, MaxPool2d)\nfrom tensorlayer.models import Model\n\n# enable debug logging\ntl.logging.set_verbosity(tl.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# prepare cifar10 data\nX_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n\n\n# define the network\ndef get_model(inputs_shape):\n    # self defined initialization\n    W_init = tl.initializers.truncated_normal(stddev=5e-2)\n    W_init2 = tl.initializers.truncated_normal(stddev=0.04)\n    b_init2 = tl.initializers.constant(value=0.1)\n\n    # build network\n    ni = Input(inputs_shape)\n    nn = Conv2d(64, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, W_init=W_init, b_init=None, name=\'conv1\')(ni)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(nn)\n    nn = LocalResponseNorm(depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=""norm1"")(nn)\n\n    nn = Conv2d(64, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, W_init=W_init, b_init=None, name=\'conv2\')(nn)\n    nn = LocalResponseNorm(depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=""norm2"")(nn)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(nn)\n\n    nn = Flatten(name=\'flatten\')(nn)\n    nn = Dense(384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name=\'dense1relu\')(nn)\n    nn = Dense(192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name=\'dense2relu\')(nn)\n    nn = Dense(10, act=None, W_init=W_init2, name=\'output\')(nn)\n\n    M = Model(inputs=ni, outputs=nn, name=\'cnn\')\n    return M\n\n\ndef get_model_batchnorm(inputs_shape):\n    # self defined initialization\n    W_init = tl.initializers.truncated_normal(stddev=5e-2)\n    W_init2 = tl.initializers.truncated_normal(stddev=0.04)\n    b_init2 = tl.initializers.constant(value=0.1)\n\n    # build network\n    ni = Input(inputs_shape)\n    nn = Conv2d(64, (5, 5), (1, 1), padding=\'SAME\', W_init=W_init, b_init=None, name=\'conv1\')(ni)\n    nn = BatchNorm(decay=0.99, act=tf.nn.relu, name=\'batch1\')(nn)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(nn)\n\n    nn = Conv2d(64, (5, 5), (1, 1), padding=\'SAME\', W_init=W_init, b_init=None, name=\'conv2\')(nn)\n    nn = BatchNorm(decay=0.99, act=tf.nn.relu, name=\'batch2\')(nn)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(nn)\n\n    nn = Flatten(name=\'flatten\')(nn)\n    nn = Dense(384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name=\'dense1relu\')(nn)\n    nn = Dense(192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name=\'dense2relu\')(nn)\n    nn = Dense(10, act=None, W_init=W_init2, name=\'output\')(nn)\n\n    M = Model(inputs=ni, outputs=nn, name=\'cnn\')\n    return M\n\n\n# get the network\nnet = get_model([None, 24, 24, 3])\n\n# training settings\nbatch_size = 128\nn_epoch = 50000\nlearning_rate = 0.0001\nprint_freq = 5\nn_step_epoch = int(len(y_train) / batch_size)\nn_step = n_epoch * n_step_epoch\nshuffle_buffer_size = 128\n\ntrain_weights = net.trainable_weights\noptimizer = tf.optimizers.Adam(learning_rate)\n# looking for decay learning rate? see https://github.com/tensorlayer/srgan/blob/master/train.py\n\n\ndef generator_train():\n    inputs = X_train\n    targets = y_train\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef generator_test():\n    inputs = X_test\n    targets = y_test\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef _map_fn_train(img, target):\n    # 1. Randomly crop a [height, width] section of the image.\n    img = tf.image.random_crop(img, [24, 24, 3])\n    # 2. Randomly flip the image horizontally.\n    img = tf.image.random_flip_left_right(img)\n    # 3. Randomly change brightness.\n    img = tf.image.random_brightness(img, max_delta=63)\n    # 4. Randomly change contrast.\n    img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n    # 5. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _map_fn_test(img, target):\n    # 1. Crop the central [height, width] of the image.\n    img = tf.image.resize_with_pad(img, 24, 24)\n    # 2. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    img = tf.reshape(img, (24, 24, 3))\n    target = tf.reshape(target, ())\n    return img, target\n\n\n# dataset API and augmentation\ntrain_ds = tf.data.Dataset.from_generator(\n    generator_train, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\ntrain_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n# train_ds = train_ds.repeat(n_epoch)\ntrain_ds = train_ds.shuffle(shuffle_buffer_size)\ntrain_ds = train_ds.prefetch(buffer_size=4096)\ntrain_ds = train_ds.batch(batch_size)\n# value = train_ds.make_one_shot_iterator().get_next()\n\ntest_ds = tf.data.Dataset.from_generator(\n    generator_test, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\n# test_ds = test_ds.shuffle(shuffle_buffer_size)\ntest_ds = test_ds.map(_map_fn_test, num_parallel_calls=multiprocessing.cpu_count())\n# test_ds = test_ds.repeat(n_epoch)\ntest_ds = test_ds.prefetch(buffer_size=4096)\ntest_ds = test_ds.batch(batch_size)\n# value_test = test_ds.make_one_shot_iterator().get_next()\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n\n    train_loss, train_acc, n_iter = 0, 0, 0\n    for X_batch, y_batch in train_ds:\n        net.train()\n\n        with tf.GradientTape() as tape:\n            # compute outputs\n            _logits = net(X_batch)\n            # compute loss and update model\n            _loss_ce = tl.cost.cross_entropy(_logits, y_batch, name=\'train_loss\')\n            _loss_L2 = 0\n            # for p in tl.layers.get_variables_with_name(\'relu/W\', True, True):\n            #      _loss_L2 += tl.cost.lo_regularizer(1.0)(p)\n            _loss = _loss_ce + _loss_L2\n\n        grad = tape.gradient(_loss, train_weights)\n        optimizer.apply_gradients(zip(grad, train_weights))\n\n        train_loss += _loss\n        train_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n        n_iter += 1\n\n    # use training and evaluation sets to evaluate the model every print_freq epoch\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n        net.eval()\n        val_loss, val_acc, n_iter = 0, 0, 0\n        for X_batch, y_batch in test_ds:\n            _logits = net(X_batch)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n            n_iter += 1\n        print(""   val loss: {}"".format(val_loss / n_iter))\n        print(""   val acc:  {}"".format(val_acc / n_iter))\n\n# use testing data to evaluate the model\nnet.eval()\ntest_loss, test_acc, n_iter = 0, 0, 0\nfor X_batch, y_batch in test_ds:\n    _logits = net(X_batch)\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n    n_iter += 1\nprint(""   test loss: {}"".format(test_loss / n_iter))\nprint(""   test acc:  {}"".format(test_acc / n_iter))\n'"
examples/basic_tutorials/tutorial_mnist_mlp_dynamic.py,6,"b'import time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\nfrom tensorlayer.layers import Dense, Dropout, Input\r\nfrom tensorlayer.models import Model\r\n\r\n## enable debug logging\r\ntl.logging.set_verbosity(tl.logging.DEBUG)\r\n\r\n## prepare MNIST data\r\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\r\n\r\n\r\n## define the network\r\nclass CustomModel(Model):\r\n\r\n    def __init__(self):\r\n        super(CustomModel, self).__init__()\r\n        self.dropout1 = Dropout(keep=0.8)  #(self.innet)\r\n        self.dense1 = Dense(n_units=800, act=tf.nn.relu, in_channels=784)  #(self.dropout1)\r\n        self.dropout2 = Dropout(keep=0.8)  #(self.dense1)\r\n        self.dense2 = Dense(n_units=800, act=tf.nn.relu, in_channels=800)  #(self.dropout2)\r\n        self.dropout3 = Dropout(keep=0.8)  #(self.dense2)\r\n        self.dense3 = Dense(n_units=10, act=tf.nn.relu, in_channels=800)  #(self.dropout3)\r\n\r\n    def forward(self, x, foo=None):\r\n        z = self.dropout1(x)\r\n        z = self.dense1(z)\r\n        z = self.dropout2(z)\r\n        z = self.dense2(z)\r\n        z = self.dropout3(z)\r\n        out = self.dense3(z)\r\n        if foo is not None:\r\n            out = tf.nn.relu(out)\r\n        return out\r\n\r\n\r\nMLP = CustomModel()\r\n\r\n## start training\r\nn_epoch = 500\r\nbatch_size = 500\r\nprint_freq = 5\r\ntrain_weights = MLP.trainable_weights\r\noptimizer = tf.optimizers.Adam(learning_rate=0.0001)\r\n\r\n## the following code can help you understand SGD deeply\r\nfor epoch in range(n_epoch):  ## iterate the dataset n_epoch times\r\n    start_time = time.time()\r\n    ## iterate over the entire training set once (shuffle the data via training)\r\n    for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\r\n        MLP.train()  # enable dropout\r\n        with tf.GradientTape() as tape:\r\n            ## compute outputs\r\n            _logits = MLP(X_batch, foo=1)\r\n            ## compute loss and update model\r\n            _loss = tl.cost.cross_entropy(_logits, y_batch, name=\'train_loss\')\r\n        grad = tape.gradient(_loss, train_weights)\r\n        optimizer.apply_gradients(zip(grad, train_weights))\r\n\r\n    ## use training and evaluation sets to evaluate the model every print_freq epoch\r\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\r\n        MLP.eval()  # disable dropout\r\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\r\n        train_loss, train_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=False):\r\n            _logits = MLP(X_batch, foo=1)\r\n            train_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            train_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   train foo=1 loss: {}"".format(train_loss / n_iter))\r\n        print(""   train foo=1 acc:  {}"".format(train_acc / n_iter))\r\n        val_loss, val_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\r\n            _logits = MLP(X_batch, foo=1)  # is_train=False, disable dropout\r\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   val foo=1 loss: {}"".format(val_loss / n_iter))\r\n        print(""   val foo=1 acc:  {}"".format(val_acc / n_iter))\r\n        val_loss, val_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\r\n            _logits = MLP(X_batch)  # is_train=False, disable dropout\r\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   val foo=0 loss: {}"".format(val_loss / n_iter))\r\n        print(""   val foo=0 acc:  {}"".format(val_acc / n_iter))\r\n\r\n## use testing data to evaluate the model\r\nMLP.eval()\r\ntest_loss, test_acc, n_iter = 0, 0, 0\r\nfor X_batch, y_batch in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=False):\r\n    _logits = MLP(X_batch, foo=1)\r\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\r\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n    n_iter += 1\r\nprint(""   test foo=1 loss: {}"".format(val_loss / n_iter))\r\nprint(""   test foo=1 acc:  {}"".format(val_acc / n_iter))\r\n'"
examples/basic_tutorials/tutorial_mnist_mlp_dynamic_2.py,6,"b'import time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\nfrom tensorlayer.layers import Dense, Dropout, Input, LayerList\r\nfrom tensorlayer.models import Model\r\n\r\n## enable debug logging\r\ntl.logging.set_verbosity(tl.logging.DEBUG)\r\n\r\n## prepare MNIST data\r\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\r\n\r\n\r\n## define the network\r\nclass CustomModelHidden(Model):\r\n\r\n    def __init__(self):\r\n        super(CustomModelHidden, self).__init__()\r\n        self.dropout1 = Dropout(keep=0.8)  #(self.innet)\r\n        self.seq = LayerList(\r\n            [\r\n                Dense(n_units=800, act=tf.nn.relu, in_channels=784),\r\n                Dropout(keep=0.8),\r\n                Dense(n_units=800, act=tf.nn.relu, in_channels=800),\r\n            ]\r\n        )\r\n        self.dropout3 = Dropout(keep=0.8)  #(self.seq)\r\n\r\n    def forward(self, x):\r\n        z = self.dropout1(x)\r\n        z = self.seq(z)\r\n        z = self.dropout3(z)\r\n        return z\r\n\r\n\r\nclass CustomModelOut(Model):\r\n\r\n    def __init__(self):\r\n        super(CustomModelOut, self).__init__()\r\n        self.dense3 = Dense(n_units=10, act=tf.nn.relu, in_channels=800)\r\n\r\n    def forward(self, x, foo=None):\r\n        out = self.dense3(x)\r\n        if foo is not None:\r\n            out = tf.nn.relu(out)\r\n        return out\r\n\r\n\r\n# NOTE: using previous defined model is different in dynamic network\r\n# a dynamic network cannot be converted into Layer because the inputs and outputs are unknown until forwarding\r\n# therefore, you may reuse a previous defined model in the following way\r\n\r\nMLP1 = CustomModelHidden()\r\nMLP2 = CustomModelOut()\r\n# MLP.print_layers()\r\n# MLP.print_weights()\r\n# print(MLP)\r\n\r\n## start training\r\nn_epoch = 500\r\nbatch_size = 500\r\nprint_freq = 5\r\ntrain_weights = MLP1.trainable_weights + MLP2.trainable_weights\r\noptimizer = tf.optimizers.Adam(learning_rate=0.0001)\r\n\r\n## the following code can help you understand SGD deeply\r\nfor epoch in range(n_epoch):  ## iterate the dataset n_epoch times\r\n    start_time = time.time()\r\n    ## iterate over the entire training set once (shuffle the data via training)\r\n    for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\r\n        MLP1.train()  # enable dropout\r\n        MLP2.train()\r\n        with tf.GradientTape() as tape:\r\n            ## compute outputs\r\n            _hidden = MLP1(X_batch)\r\n            _logits = MLP2(_hidden, foo=1)\r\n            ## compute loss and update model\r\n            _loss = tl.cost.cross_entropy(_logits, y_batch, name=\'train_loss\')\r\n        grad = tape.gradient(_loss, train_weights)\r\n        optimizer.apply_gradients(zip(grad, train_weights))\r\n\r\n    ## use training and evaluation sets to evaluate the model every print_freq epoch\r\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\r\n        MLP1.eval()  # disable dropout\r\n        MLP2.eval()\r\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\r\n        train_loss, train_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=False):\r\n            _hidden = MLP1(X_batch)\r\n            _logits = MLP2(_hidden, foo=1)\r\n            train_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            train_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   train foo=1 loss: {}"".format(train_loss / n_iter))\r\n        print(""   train foo=1 acc:  {}"".format(train_acc / n_iter))\r\n\r\n        val_loss, val_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\r\n            _hidden = MLP1(X_batch)\r\n            _logits = MLP2(_hidden, foo=1)\r\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   val foo=1 loss: {}"".format(val_loss / n_iter))\r\n        print(""   val foo=1 acc:  {}"".format(val_acc / n_iter))\r\n\r\n        val_loss, val_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\r\n            _hidden = MLP1(X_batch)\r\n            _logits = MLP2(_hidden, foo=0)\r\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   val foo=0 loss: {}"".format(val_loss / n_iter))\r\n        print(""   val foo=0 acc:  {}"".format(val_acc / n_iter))\r\n\r\n## use testing data to evaluate the model\r\nMLP1.eval()\r\nMLP2.eval()\r\ntest_loss, test_acc, n_iter = 0, 0, 0\r\nfor X_batch, y_batch in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=False):\r\n    _hidden = MLP1(X_batch)\r\n    _logits = MLP2(_hidden, foo=0)\r\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\r\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n    n_iter += 1\r\nprint(""   test foo=1 loss: {}"".format(val_loss / n_iter))\r\nprint(""   test foo=1 acc:  {}"".format(val_acc / n_iter))\r\n'"
examples/basic_tutorials/tutorial_mnist_mlp_static.py,6,"b'import pprint\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\nfrom tensorlayer.layers import Dense, Dropout, Input\r\nfrom tensorlayer.models import Model\r\n\r\n## enable debug logging\r\ntl.logging.set_verbosity(tl.logging.DEBUG)\r\n\r\n## prepare MNIST data\r\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\r\n\r\n\r\n## define the network\r\n# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\r\n# speed up computation, so we use identity here.\r\n# see tf.nn.sparse_softmax_cross_entropy_with_logits()\r\ndef get_model(inputs_shape):\r\n    ni = Input(inputs_shape)\r\n    nn = Dropout(keep=0.8)(ni)\r\n    nn = Dense(n_units=800, act=tf.nn.relu)(nn)\r\n    nn = Dropout(keep=0.8)(nn)\r\n    nn = Dense(n_units=800, act=tf.nn.relu)(nn)\r\n    nn = Dropout(keep=0.8)(nn)\r\n    nn = Dense(n_units=10, act=tf.nn.relu)(nn)\r\n    M = Model(inputs=ni, outputs=nn, name=""mlp"")\r\n    return M\r\n\r\n\r\nMLP = get_model([None, 784])\r\npprint.pprint(MLP.config)\r\n\r\n## start training\r\nn_epoch = 500\r\nbatch_size = 500\r\nprint_freq = 5\r\ntrain_weights = MLP.trainable_weights\r\noptimizer = tf.optimizers.Adam(lr=0.0001)\r\n\r\n## the following code can help you understand SGD deeply\r\nfor epoch in range(n_epoch):  ## iterate the dataset n_epoch times\r\n    start_time = time.time()\r\n    ## iterate over the entire training set once (shuffle the data via training)\r\n    for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\r\n        MLP.train()  # enable dropout\r\n        with tf.GradientTape() as tape:\r\n            ## compute outputs\r\n            _logits = MLP(X_batch)\r\n            ## compute loss and update model\r\n            _loss = tl.cost.cross_entropy(_logits, y_batch, name=\'train_loss\')\r\n        grad = tape.gradient(_loss, train_weights)\r\n        optimizer.apply_gradients(zip(grad, train_weights))\r\n\r\n    ## use training and evaluation sets to evaluate the model every print_freq epoch\r\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\r\n        MLP.eval()  # disable dropout\r\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\r\n        train_loss, train_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=False):\r\n            _logits = MLP(X_batch)\r\n            train_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            train_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   train loss: {}"".format(train_loss / n_iter))\r\n        print(""   train acc:  {}"".format(train_acc / n_iter))\r\n\r\n        val_loss, val_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\r\n            _logits = MLP(X_batch)  # is_train=False, disable dropout\r\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   val loss: {}"".format(val_loss / n_iter))\r\n        print(""   val acc:  {}"".format(val_acc / n_iter))\r\n\r\n## use testing data to evaluate the model\r\nMLP.eval()\r\ntest_loss, test_acc, n_iter = 0, 0, 0\r\nfor X_batch, y_batch in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=False):\r\n    _logits = MLP(X_batch)\r\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\r\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n    n_iter += 1\r\nprint(""   test loss: {}"".format(test_loss / n_iter))\r\nprint(""   test acc:  {}"".format(test_acc / n_iter))\r\n'"
examples/basic_tutorials/tutorial_mnist_mlp_static_2.py,6,"b'import time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\nfrom tensorlayer.layers import Dense, Dropout, Input\r\nfrom tensorlayer.models import Model\r\n\r\n## enable debug logging\r\ntl.logging.set_verbosity(tl.logging.DEBUG)\r\n\r\n## prepare MNIST data\r\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\r\n\r\n## define the network\r\n# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\r\n# speed up computation, so we use identity here.\r\n# see tf.nn.sparse_softmax_cross_entropy_with_logits()\r\n\r\n\r\ndef hidden_model(inputs_shape):\r\n    ni = Input(inputs_shape)\r\n    nn = Dropout(keep=0.8)(ni)\r\n    nn = Dense(n_units=800, act=tf.nn.relu)(nn)\r\n    nn = Dropout(keep=0.8)(nn)\r\n    nn = Dense(n_units=800, act=tf.nn.relu)(nn)\r\n\r\n    return Model(inputs=ni, outputs=nn, name=""mlp_hidden"")\r\n\r\n\r\ndef get_model(inputs_shape, hmodel):\r\n    hidden = hmodel.as_layer()\r\n    ni = Input(inputs_shape)\r\n    nn = hidden(ni)\r\n    nn = Dropout(keep=0.8)(nn)\r\n    nn = Dense(n_units=10, act=tf.nn.relu)(nn)\r\n\r\n    return Model(inputs=ni, outputs=nn, name=""mlp"")\r\n\r\n\r\nMLP_hidden = hidden_model([None, 784])\r\nMLP = get_model([None, 784], MLP_hidden)\r\n# MLP.print_layers()\r\n# MLP.print_weights()\r\n\r\n## start training\r\nn_epoch = 500\r\nbatch_size = 500\r\nprint_freq = 5\r\ntrain_weights = MLP.trainable_weights\r\noptimizer = tf.optimizers.Adam(lr=0.0001)\r\n\r\n## the following code can help you understand SGD deeply\r\nfor epoch in range(n_epoch):  ## iterate the dataset n_epoch times\r\n    start_time = time.time()\r\n    ## iterate over the entire training set once (shuffle the data via training)\r\n    for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\r\n        MLP.train()  # enable dropout\r\n        with tf.GradientTape() as tape:\r\n            ## compute outputs\r\n            _logits = MLP(X_batch)  # alternatively, you can use MLP(x, is_train=True) and remove MLP.train()\r\n            ## compute loss and update model\r\n            _loss = tl.cost.cross_entropy(_logits, y_batch, name=\'train_loss\')\r\n        grad = tape.gradient(_loss, train_weights)\r\n        optimizer.apply_gradients(zip(grad, train_weights))\r\n\r\n    ## use training and evaluation sets to evaluate the model every print_freq epoch\r\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\r\n        MLP.eval()  # disable dropout\r\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\r\n        train_loss, train_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=False):\r\n\r\n            _logits = MLP(X_batch)  # alternatively, you can use MLP(x, is_train=False) and remove MLP.eval()\r\n            train_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            train_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   train loss: {}"".format(train_loss / n_iter))\r\n        print(""   train acc:  {}"".format(train_acc / n_iter))\r\n        val_loss, val_acc, n_iter = 0, 0, 0\r\n        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\r\n            _logits = MLP(X_batch)  # is_train=False, disable dropout\r\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\r\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n            n_iter += 1\r\n        print(""   val loss: {}"".format(val_loss / n_iter))\r\n        print(""   val acc:  {}"".format(val_acc / n_iter))\r\n\r\n## use testing data to evaluate the model\r\nMLP.eval()\r\ntest_loss, test_acc, n_iter = 0, 0, 0\r\nfor X_batch, y_batch in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=False):\r\n    _logits = MLP(X_batch)\r\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\r\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\r\n    n_iter += 1\r\nprint(""   test loss: {}"".format(test_loss / n_iter))\r\nprint(""   test acc:  {}"".format(test_acc / n_iter))\r\n'"
examples/basic_tutorials/tutorial_mnist_siamese.py,11,"b'\'\'\'Trains a Siamese MLP on pairs of digits from the MNIST dataset.\nGet 96.7% accuracy on test data after 20 epochs training.\n\nFor more details, see the reference paper.\n\n# References\n- Dimensionality Reduction by Learning an Invariant Mapping\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n\n\n\'\'\'\n\nimport random\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import Dense, Dropout, Flatten, Input\nfrom tensorlayer.models import Model\n\nnum_classes = 10\nepochs = 20\nbatch_size = 128\ninput_shape = (None, 784)\n\n\ndef contrastive_loss(label, feature1, feature2):\n    margin = 1.0\n    eucd = tf.sqrt(tf.reduce_sum(tf.square(feature1 - feature2), axis=1))\n    return tf.reduce_mean(label * tf.square(eucd) + (1 - label) * tf.square(tf.maximum(margin - eucd, 0)))\n\n\ndef compute_accuracy(label, feature1, feature2):\n    eucd = tf.sqrt(tf.reduce_sum((feature1 - feature2)**2, axis=1))\n    pred = tf.cast(eucd < 0.5, label.dtype)\n    return tf.reduce_mean(tf.cast(tf.equal(pred, label), tf.float32))\n\n\ndef create_base_network(input_shape):\n    \'\'\'Base network to be shared (eq. to feature extraction).\n    \'\'\'\n    input = Input(shape=input_shape)\n    x = Flatten()(input)\n    x = Dense(128, act=tf.nn.relu)(x)\n    x = Dropout(0.9)(x)\n    x = Dense(128, act=tf.nn.relu)(x)\n    x = Dropout(0.9)(x)\n    x = Dense(128, act=tf.nn.relu)(x)\n    return Model(input, x)\n\n\ndef get_siamese_network(input_shape):\n    """"""Create siamese network with shared base network as layer\n    """"""\n    base_layer = create_base_network(input_shape).as_layer()\n\n    ni_1 = Input(input_shape)\n    ni_2 = Input(input_shape)\n    nn_1 = base_layer(ni_1)\n    nn_2 = base_layer(ni_2)\n    return Model(inputs=[ni_1, ni_2], outputs=[nn_1, nn_2])\n\n\ndef create_pairs(x, digit_indices):\n    pairs = []\n    labels = []\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n    for d in range(num_classes):\n        for i in range(n):\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            pairs += [[x[z1], x[z2]]]\n            inc = random.randrange(1, num_classes)\n            dn = (d + inc) % num_classes\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            pairs += [[x[z1], x[z2]]]\n            labels += [1, 0]\n    return np.array(pairs), np.array(labels).astype(np.float32)\n\n\n# get network\nmodel = get_siamese_network(input_shape)\n\n# create training+val+test positive and negative pairs\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\ntr_pairs, tr_y = create_pairs(X_train, digit_indices)\n\ndigit_indices = [np.where(y_val == i)[0] for i in range(num_classes)]\nval_pairs, val_y = create_pairs(X_val, digit_indices)\n\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\nte_pairs, te_y = create_pairs(X_test, digit_indices)\n\n# training settings\nprint_freq = 5\ntrain_weights = model.trainable_weights\noptimizer = tf.optimizers.RMSprop()\n\n\n@tf.function\ndef train_step(X_batch, y_batch):\n    with tf.GradientTape() as tape:\n        _out1, _out2 = model([X_batch[:, 0, :], X_batch[:, 1, :]])\n        _loss = contrastive_loss(y_batch, _out1, _out2)\n\n    grad = tape.gradient(_loss, train_weights)\n    optimizer.apply_gradients(zip(grad, train_weights))\n\n    _acc = compute_accuracy(y_batch, _out1, _out2)\n    return _loss, _acc\n\n\n# begin training\nfor epoch in range(epochs):\n    start_time = time.time()\n\n    train_loss, train_acc, n_iter = 0, 0, 0\n    model.train()  # enable dropout\n    for X_batch, y_batch in tl.iterate.minibatches(tr_pairs, tr_y, batch_size, shuffle=True):\n        _loss, _acc = train_step(X_batch, y_batch)\n        train_loss += _loss\n        train_acc += _acc\n        n_iter += 1\n\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        print(""Epoch {} of {} took {}"".format(epoch + 1, epochs, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n\n# evaluate on test data\nmodel.eval()\ntest_loss, test_acc, n_iter = 0, 0, 0\nfor X_batch, y_batch in tl.iterate.minibatches(te_pairs, te_y, batch_size, shuffle=False):\n    _out1, _out2 = model([X_batch[:, 0, :], X_batch[:, 1, :]])\n    test_loss += contrastive_loss(y_batch, _out1, _out2)\n    test_acc += compute_accuracy(y_batch, _out1, _out2)\n    n_iter += 1\nprint(""   test loss: {}"".format(test_loss / n_iter))\nprint(""   test acc:  {}"".format(test_acc / n_iter))\n'"
examples/basic_tutorials/tutorial_mnist_simple.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# set gpu mem fraction or allow growth\n# tl.utils.set_gpu_fraction()\n\n# prepare data\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n\n# define the network\nni = tl.layers.Input([None, 784])\nnn = tl.layers.Dropout(keep=0.8)(ni)\nnn = tl.layers.Dense(n_units=800, act=tf.nn.relu)(nn)\nnn = tl.layers.Dropout(keep=0.5)(nn)\nnn = tl.layers.Dense(n_units=800, act=tf.nn.relu)(nn)\nnn = tl.layers.Dropout(keep=0.5)(nn)\nnn = tl.layers.Dense(n_units=10, act=None)(nn)\nnetwork = tl.models.Model(inputs=ni, outputs=nn, name=""mlp"")\n\n\n# define metric.\ndef acc(_logits, y_batch):\n    # return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n    return tf.reduce_mean(\n        tf.cast(tf.equal(tf.argmax(_logits, 1), tf.convert_to_tensor(y_batch, tf.int64)), tf.float32), name=\'accuracy\'\n    )\n\n\n# print network information\nprint(network)\n\n# open tensorboard\n# tl.utils.open_tensorboard(\'./tb_log\', port=6006)\n\n# train the network\ntl.utils.fit(\n    network, train_op=tf.optimizers.Adam(learning_rate=0.0001), cost=tl.cost.cross_entropy, X_train=X_train,\n    y_train=y_train, acc=acc, batch_size=256, n_epoch=20, X_val=X_val, y_val=y_val, eval_train=True,\n    tensorboard_dir=\'./tb_log\'\n)\n\n# test\ntl.utils.test(network, acc, X_test, y_test, batch_size=None, cost=tl.cost.cross_entropy)\n\n# evaluation\n_logits = tl.utils.predict(network, X_test)\ny_pred = np.argmax(_logits, 1)\ntl.utils.evaluation(y_test, y_pred, n_classes=10)\n\n# save network weights\nnetwork.save_weights(\'model.h5\')\n\n# close tensorboard\n# tl.utils.exit_tensorflow(port=6006)\n'"
examples/data_process/tutorial_fast_affine_transform.py,7,"b'""""""\nTutorial of fast affine transformation.\nTo run this tutorial, install opencv-python using pip.\n\nComprehensive explanation of this tutorial can be found https://tensorlayer.readthedocs.io/en/stable/modules/prepro.html\n""""""\n\nimport multiprocessing\nimport time\n\nimport numpy as np\n\nimport cv2\nimport tensorflow as tf\nimport tensorlayer as tl\n\n# tl.logging.set_verbosity(tl.logging.DEBUG)\nimage = tl.vis.read_image(\'data/tiger.jpeg\')\nh, w, _ = image.shape\n\n\ndef create_transformation_matrix():\n    # 1. Create required affine transformation matrices\n    ## fixed\n    # M_rotate = tl.prepro.affine_rotation_matrix(angle=20)\n    # M_flip = tl.prepro.affine_horizontal_flip_matrix(prob=1)\n    # M_shift = tl.prepro.affine_shift_matrix(wrg=0.1, hrg=0, h=h, w=w)\n    # M_shear = tl.prepro.affine_shear_matrix(x_shear=0.2, y_shear=0)\n    # M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=0.8)\n    ## random\n    M_rotate = tl.prepro.affine_rotation_matrix(angle=(-20, 20))\n    M_flip = tl.prepro.affine_horizontal_flip_matrix(prob=0.5)\n    M_shift = tl.prepro.affine_shift_matrix(wrg=(-0.1,0.1), hrg=(-0.1,0.1), h=h, w=w)\n    M_shear = tl.prepro.affine_shear_matrix(x_shear=(-0.2,0.2), y_shear=(-0.2,0.2))\n    M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=(0.8,1.2))\n\n    # 2. Combine matrices\n    # NOTE: operations are applied in a reversed order (i.e., rotation is performed first)\n    M_combined = M_shift.dot(M_zoom).dot(M_shear).dot(M_flip).dot(M_rotate)\n\n    # 3. Convert the matrix from Cartesian coordinates (the origin in the middle of image)\n    # to image coordinates (the origin on the top-left of image)\n    transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, x=w, y=h)\n    return transform_matrix\n\n\ndef example1():\n    """""" Example 1: Applying transformation one-by-one is very SLOW ! """"""\n    st = time.time()\n    for _ in range(100):  # Try 100 times and compute the averaged speed\n        xx = tl.prepro.rotation(image, rg=-20, is_random=False)\n        xx = tl.prepro.flip_axis(xx, axis=1, is_random=False)\n        xx = tl.prepro.shear2(xx, shear=(0., -0.2), is_random=False)\n        xx = tl.prepro.zoom(xx, zoom_range=1 / 0.8)\n        xx = tl.prepro.shift(xx, wrg=-0.1, hrg=0, is_random=False)\n    print(""apply transforms one-by-one took %fs for each image"" % ((time.time() - st) / 100))\n    tl.vis.save_image(xx, \'_result_slow.png\')\n\n\ndef example2():\n    """""" Example 2: Applying all transforms in one is very FAST ! """"""\n    st = time.time()\n    for _ in range(100):  # Repeat 100 times and compute the averaged speed\n        transform_matrix = create_transformation_matrix()\n        result = tl.prepro.affine_transform_cv2(image, transform_matrix, border_mode=\'replicate\')  # Transform the image using a single operation\n        tl.vis.save_image(result, \'_result_fast_{}.png\'.format(_))\n    print(""apply all transforms once took %fs for each image"" % ((time.time() - st) / 100))  # usually 50x faster\n    tl.vis.save_image(result, \'_result_fast.png\')\n\n\ndef example3():\n    """""" Example 3: Using TF dataset API to load and process image for training """"""\n    n_data = 100\n    imgs_file_list = [\'data/tiger.jpeg\'] * n_data\n    train_targets = [np.ones(1)] * n_data\n\n    def generator():\n        if len(imgs_file_list) != len(train_targets):\n            raise RuntimeError(\'len(imgs_file_list) != len(train_targets)\')\n        for _input, _target in zip(imgs_file_list, train_targets):\n            yield _input, _target\n\n    def _data_aug_fn(image):\n        transform_matrix = create_transformation_matrix()\n        result = tl.prepro.affine_transform_cv2(image, transform_matrix)  # Transform the image using a single operation\n        return result\n\n    def _map_fn(image_path, target):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)  # Get RGB with 0~1\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        image = tf.numpy_function(_data_aug_fn, [image], [tf.float32])[0]\n        target = tf.reshape(target, ())\n        return image, target\n\n    n_epoch = 10\n    batch_size = 5\n    dataset = tf.data.Dataset.from_generator(generator, output_types=(tf.string, tf.int64))\n    dataset = dataset.shuffle(buffer_size=4096)  # shuffle before loading images\n    dataset = dataset.repeat(n_epoch)\n    dataset = dataset.map(_map_fn, num_parallel_calls=multiprocessing.cpu_count())\n    dataset = dataset.batch(batch_size)  # TODO: consider using tf.contrib.map_and_batch\n    dataset = dataset.prefetch(1)  # prefetch 1 batch\n\n    n_step = 0\n    st = time.time()\n    for img, target in dataset:\n        n_step += 1\n        pass\n    assert n_step == n_epoch * n_data / batch_size\n    print(""dataset APIs took %fs for each image"" % ((time.time() - st) / batch_size / n_step))  # CPU ~ 100%\n\n\ndef example4():\n    """""" Example 4: Transforming coordinates using affine matrix. """"""\n    transform_matrix = create_transformation_matrix()\n    result = tl.prepro.affine_transform_cv2(image, transform_matrix)  # 76 times faster\n    # Transform keypoint coordinates\n    coords = [[(50, 100), (100, 100), (100, 50), (200, 200)], [(250, 50), (200, 50), (200, 100)]]\n    coords_result = tl.prepro.affine_transform_keypoints(coords, transform_matrix)\n\n    def imwrite(image, coords_list, name):\n        coords_list_ = []\n        for coords in coords_list:\n            coords = np.array(coords, np.int32)\n            coords = coords.reshape((-1, 1, 2))\n            coords_list_.append(coords)\n        image = cv2.polylines(image, coords_list_, True, (0, 255, 255), 3)\n        cv2.imwrite(name, image[..., ::-1])\n\n    imwrite(image, coords, \'_with_keypoints_origin.png\')\n    imwrite(result, coords_result, \'_with_keypoints_result.png\')\n\n\nif __name__ == \'__main__\':\n    example1()\n    example2()\n    example3()\n    example4()\n'"
examples/data_process/tutorial_tf_dataset_voc.py,11,"b'#! /usr/bin/python\n# -*- coding: utf8 -*-\n\n# tf import data dataset.map https://www.tensorflow.org/programmers_guide/datasets#applying_arbitrary_python_logic_with_tfpy_func\n# tf.py_func https://www.tensorflow.org/api_docs/python/tf/py_func\n# tl ref: https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_imagenet_inceptionV3_distributed.py\n# cn ref: https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/79342369\n# cn ref: https://zhuanlan.zhihu.com/p/31466173\n\nimport json\nimport multiprocessing\nimport random\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\n# tf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\nimgs_file_list, _, _, _, classes, _, _, _, objs_info_list, _ = tl.files.load_voc_dataset(dataset=""2007"")\n\nann_list = []\nfor info in objs_info_list:\n    ann = tl.prepro.parse_darknet_ann_str_to_list(info)\n    c, b = tl.prepro.parse_darknet_ann_list_to_cls_box(ann)\n    ann_list.append([c, b])\n\nn_epoch = 10\nbatch_size = 64\nim_size = [416, 416]\njitter = 0.2\nshuffle_buffer_size = 100\n\n\ndef generator():\n    inputs = imgs_file_list\n    targets = objs_info_list\n\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n\n    for _input, _target in zip(inputs, targets):\n        yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n\n\ndef _data_aug_fn(im, ann):\n    ## parse annotation\n    ann = ann.decode()\n    ann = tl.prepro.parse_darknet_ann_str_to_list(ann)\n    clas, coords = tl.prepro.parse_darknet_ann_list_to_cls_box(ann)\n    ## random brightness, contrast and saturation (tf.image API is faster)\n    # im = tl.prepro.brightness(im, gamma=0.5, gain=1, is_random=True)\n    # im = tl.prepro.illumination(im, gamma=(0.5, 1.5),\n    #          contrast=(0.5, 1.5), saturation=(0.5, 1.5), is_random=True)    # TypeError: Cannot handle this data type\n    ## random horizontal flip\n    im, coords = tl.prepro.obj_box_left_right_flip(im, coords, is_rescale=True, is_center=True, is_random=True)\n    ## random resize and crop\n    tmp0 = random.randint(1, int(im_size[0] * jitter))\n    tmp1 = random.randint(1, int(im_size[1] * jitter))\n    im, coords = tl.prepro.obj_box_imresize(im, coords, [im_size[0] + tmp0, im_size[1] + tmp1], \\\n        is_rescale=True, interp=\'bicubic\')\n    im, clas, coords = tl.prepro.obj_box_crop(im, clas, coords, wrg=im_size[1], hrg=im_size[0], \\\n        is_rescale=True, is_center=True, is_random=True)\n    ## value [0, 255] to [-1, 1] (optional)\n    # im = im / 127.5 - 1\n    ## value [0, 255] to [0, 1] (optional)\n    im = im / 255\n    im = np.array(im, dtype=np.float32)  # important\n    return im, str([clas, coords]).encode(\'utf-8\')\n\n\ndef _map_fn(filename, annotation):\n    ## read image\n    image = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    ## data augmentation for image only  0.02s\n    image = tf.image.random_brightness(image, max_delta=63)\n    image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n    # subtract off the mean and divide by the variance of the pixels. (optional)\n    # img = tf.image.per_image_standardization(img)\n    ## data augmentation for image and bounding box\n    image, annotation = tf.numpy_function(_data_aug_fn, [image, annotation], [tf.float32, tf.string])\n    return image, annotation\n\n\nds = tf.data.Dataset.from_generator(generator, output_types=(tf.string, tf.string))\nds = ds.shuffle(shuffle_buffer_size)\nds = ds.map(_map_fn, num_parallel_calls=multiprocessing.cpu_count())\nds = ds.repeat(n_epoch)\nds = ds.prefetch(buffer_size=2048)\nds = ds.batch(batch_size)\n\nst = time.time()\nim, annbyte = next(iter(ds))\nprint(\'took {}s\'.format(time.time() - st))\n\nim = im.numpy()\n\nann = []\nfor a in annbyte:\n    a = a.numpy().decode()\n    ann.append(json.loads(a))\n\n## save all images\nfor i in range(len(im)):\n    print(ann[i][1])\n    tl.vis.draw_boxes_and_labels_to_image(\n        im[i] * 255, ann[i][0], ann[i][1], [], classes, True, save_name=\'_bbox_vis_%d.png\' % i\n    )\n'"
examples/data_process/tutorial_tfrecord.py,15,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""You will learn.\n\n1. How to save data into TFRecord format file.\n2. How to read data from TFRecord format file.\n\nReference:\n-----------\nEnglish : https://www.tensorflow.org/alpha/tutorials/load_data/images#build_a_tfdatadataset\n          https://www.tensorflow.org/alpha/tutorials/load_data/tf_records#tfrecord_files_using_tfdata\nChinese : http://blog.csdn.net/u012759136/article/details/52232266\n          https://github.com/ycszen/tf_lab/blob/master/reading_data/TensorFlow\xe9\xab\x98\xe6\x95\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95.md\n\nMore\n------\n1. tutorial_tfrecord2.py\n2. tutorial_cifar10_tfrecord.py\n\n""""""\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\n\nimport tensorlayer as tl\n\n## Save data ==================================================================\n# see https://www.tensorflow.org/alpha/tutorials/load_data/tf_records#writing_a_tfrecord_file\nclasses = [\'/data/cat\', \'/data/dog\']  # cat is 0, dog is 1\ncwd = os.getcwd()\nwriter = tf.io.TFRecordWriter(""train.tfrecords"")\nfor index, name in enumerate(classes):\n    class_path = cwd + name + ""/""\n    for img_name in os.listdir(class_path):\n        img_path = class_path + img_name\n        img = Image.open(img_path)\n        img = img.resize((224, 224))\n        ## Visualize the image as follow:\n        # tl.visualize.frame(I=img, second=5, saveable=False, name=\'frame\', fig_idx=12836)\n        ## Converts a image to bytes\n        img_raw = img.tobytes()\n        ## Convert the bytes back to image as follow:\n        # image = Image.frombytes(\'RGB\', (224,224), img_raw)\n        # tl.visualize.frame(I=image, second=1, saveable=False, name=\'frame\', fig_idx=1236)\n        ## Write the data into TF format\n        # image     : Feature + BytesList\n        # label     : Feature + Int64List or FloatList\n        # sentence  : FeatureList + Int64List , see Google\'s im2txt example\n        example = tf.train.Example(features=tf.train.Features(feature={ # SequenceExample for seuqnce example\n            ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n            \'img_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n        }))\n        writer.write(example.SerializeToString())  # Serialize To String\nwriter.close()\n\n## Load Data Method 1: Simple read ============================================\n# see https://www.tensorflow.org/alpha/tutorials/load_data/tf_records#reading_a_tfrecord_file_2\n# read data one by one in order\nraw_dataset = tf.data.TFRecordDataset(""train.tfrecords"")\nfor serialized_example in raw_dataset:\n    example = tf.train.Example()  # SequenceExample for seuqnce example\n    example.ParseFromString(serialized_example.numpy())\n    img_raw = example.features.feature[\'img_raw\'].bytes_list.value\n    label = example.features.feature[\'label\'].int64_list.value\n    ## converts a image from bytes\n    image = Image.frombytes(\'RGB\', (224, 224), img_raw[0])\n    # tl.visualize.frame(np.asarray(image), second=0.5, saveable=False, name=\'frame\', fig_idx=1283)\n    print(label)\n\n\n## Read Data Method 2: using tf.data =======================================\n# see https://www.tensorflow.org/alpha/tutorials/load_data/tf_records#reading_a_tfrecord_file\n# use shuffle and batch\ndef read_and_decode(filename):\n    # generate a queue with a given file name\n    raw_dataset = tf.data.TFRecordDataset([filename]).shuffle(1000).batch(4)\n    for serialized_example in raw_dataset:\n        features = tf.io.parse_example(\n            serialized_example, features={\n                \'label\': tf.io.FixedLenFeature([], tf.int64),\n                \'img_raw\': tf.io.FixedLenFeature([], tf.string),\n            }\n        )\n        # You can do more image distortion here for training data\n        img_batch = tf.io.decode_raw(features[\'img_raw\'], tf.uint8)\n        img_batch = tf.reshape(img_batch, [4, 224, 224, 3])\n        # img = tf.cast(img, tf.float32) * (1. / 255) - 0.5\n        label_batch = tf.cast(features[\'label\'], tf.int32)\n        yield img_batch, label_batch\n\n\nimg_batch, label_batch = next(read_and_decode(""train.tfrecords""))\nprint(""img_batch   : %s"" % img_batch.shape)\nprint(""label_batch : %s"" % label_batch.shape)\ntl.visualize.images2d(img_batch, second=1, saveable=False, name=\'batch\', dtype=None, fig_idx=2020121)\n'"
examples/data_process/tutorial_tfrecord2.py,13,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""You will learn.\n\n1. How to convert CIFAR-10 dataset into TFRecord format file.\n2. How to read CIFAR-10 from TFRecord format file.\n\nMore:\n1. tutorial_tfrecord.py\n2. tutoral_cifar10_tfrecord.py\n\n""""""\n\nimport os\n\nimport numpy as np\n# import matplotlib\n# matplotlib.use(\'GTK\')\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\n# Download data, and convert to TFRecord format, see ```tutorial_tfrecord.py```\nX_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n\nX_train = np.asarray(X_train, dtype=np.uint8)\ny_train = np.asarray(y_train, dtype=np.int64)\nX_test = np.asarray(X_test, dtype=np.float32)\ny_test = np.asarray(y_test, dtype=np.int64)\n\nprint(\'X_train.shape\', X_train.shape)  # (50000, 32, 32, 3)\nprint(\'y_train.shape\', y_train.shape)  # (50000,)\nprint(\'X_test.shape\', X_test.shape)  # (10000, 32, 32, 3)\nprint(\'y_test.shape\', y_test.shape)  # (10000,)\nprint(\'X %s   y %s\' % (X_test.dtype, y_test.dtype))\n\ncwd = os.getcwd()\nwriter = tf.io.TFRecordWriter(""train.cifar10"")\nfor index, img in enumerate(X_train):\n    img_raw = img.tobytes()\n    ## Visualize a image\n    # tl.visualize.frame(np.asarray(img, dtype=np.uint8), second=1, saveable=False, name=\'frame\', fig_idx=1236)\n    label = int(y_train[index])\n    # print(label)\n    ## Convert the bytes back to image as follow:\n    # image = Image.frombytes(\'RGB\', (32, 32), img_raw)\n    # image = np.fromstring(img_raw, np.float32)\n    # image = image.reshape([32, 32, 3])\n    # tl.visualize.frame(np.asarray(image, dtype=np.uint8), second=1, saveable=False, name=\'frame\', fig_idx=1236)\n    example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                \'img_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n            }\n        )\n    )\n    writer.write(example.SerializeToString())  # Serialize To String\nwriter.close()\n\n\n## Read Data by Queue and Thread =======================================\ndef read_and_decode(filename):\n    batchsize = 4\n    raw_dataset = tf.data.TFRecordDataset([filename]).shuffle(1000).batch(batchsize)\n    for serialized_example in raw_dataset:\n        features = tf.io.parse_example(\n            serialized_example, features={\n                \'label\': tf.io.FixedLenFeature([], tf.int64),\n                \'img_raw\': tf.io.FixedLenFeature([], tf.string),\n            }\n        )\n        # You can do more image distortion here for training data\n        img_batch = tf.io.decode_raw(features[\'img_raw\'], tf.uint8)\n        img_batch = tf.reshape(img_batch, [-1, 32, 32, 3])\n        # img = tf.cast(img, tf.float32) #* (1. / 255) - 0.5    # don\'t need to cast here, as it is float32 already\n        label_batch = tf.cast(features[\'label\'], tf.int32)\n        yield img_batch, label_batch\n\n\nimg_batch, label_batch = next(read_and_decode(""train.tfrecords""))\nprint(""img_batch   : %s"" % img_batch.shape)\nprint(""label_batch : %s"" % label_batch.shape)\n\ni = 0\nfor img_batch, label_batch in read_and_decode(""train.cifar10""):\n    tl.visualize.images2d(img_batch, second=1, saveable=False, name=\'batch\' + str(i), dtype=np.uint8, fig_idx=2020121)\n    i += 1\n    if i >= 3:\n        break\n'"
examples/data_process/tutorial_tfrecord3.py,92,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nYou will learn.\n\n1. How to save time-series data (e.g. sentence) into TFRecord format file.\n2. How to read time-series data from TFRecord format file.\n3. How to create inputs, targets and mask.\n\nReference\n----------\n1. Google\'s im2txt - MSCOCO Image Captioning example\n2. TFRecord in http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n3. Batching and Padding data in http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n\n""""""\n\nimport json\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\n\nimport tensorlayer as tl\n\n\ndef _int64_feature(value):\n    """"""Wrapper for inserting an int64 Feature into a SequenceExample proto,\n    e.g, An integer label.\n    """"""\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    """"""Wrapper for inserting a bytes Feature into a SequenceExample proto,\n    e.g, an image in byte\n    """"""\n    # return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _int64_feature_list(values):\n    """"""Wrapper for inserting an int64 FeatureList into a SequenceExample proto,\n    e.g, sentence in list of ints\n    """"""\n    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n\n\ndef _bytes_feature_list(values):\n    """"""Wrapper for inserting a bytes FeatureList into a SequenceExample proto,\n    e.g, sentence in list of bytes\n    """"""\n    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n\n\n# 1. Save data into TFRecord =====================================================\ncwd = os.getcwd()\nIMG_DIR = cwd + \'/data/cat/\'\nSEQ_FIR = cwd + \'/data/cat_caption.json\'\nVOC_FIR = cwd + \'/vocab.txt\'\n# read image captions from JSON\nwith tf.gfile.FastGFile(SEQ_FIR, ""r"") as f:\n    caption_data = json.loads(str(f.read()))  # , encoding = ""utf-8""))\n\nprocessed_capts, img_capts = [], []\nfor idx in range(len(caption_data[\'images\'])):\n    img_capt = caption_data[\'images\'][idx][\'caption\']\n    img_capts.append(img_capt)\n    processed_capts.append(tl.nlp.process_sentence(img_capt, start_word=""<S>"", end_word=""</S>""))\nprint(""Original Captions: %s"" % img_capts)\nprint(""Processed Captions: %s\\n"" % processed_capts)\n# build vocab\n_ = tl.nlp.create_vocab(processed_capts, word_counts_output_file=VOC_FIR, min_word_count=1)\nvocab = tl.nlp.Vocabulary(VOC_FIR, start_word=""<S>"", end_word=""</S>"", unk_word=""<UNK>"")\n\n# save\nwriter = tf.python_io.TFRecordWriter(""train.cat_caption"")\nfor idx in range(len(caption_data[\'images\'])):\n    # get data\n    img_name = caption_data[\'images\'][idx][\'file_name\']\n    img_capt = \'<S> \' + caption_data[\'images\'][idx][\'caption\'] + \' </S>\'\n    img_capt_ids = [vocab.word_to_id(word) for word in img_capt.split(\' \')]\n    print(""%s : %s : %s"" % (img_name, img_capt, img_capt_ids))\n    img = Image.open(IMG_DIR + img_name)\n    img = img.resize((299, 299))\n    # tl.visualize.frame(I=img, second=0.2, saveable=False, name=img_name, fig_idx=12234)\n    img_raw = img.tobytes()\n    img_capt_b = [v.encode() for v in img_capt.split(\' \')]\n    context = tf.train.Features(feature={  # Non-serial data uses Feature\n        ""image/img_raw"": _bytes_feature(img_raw),\n    })\n    feature_lists = tf.train.FeatureLists(\n        feature_list={  # Serial data uses FeatureLists\n            ""image/caption"": _bytes_feature_list(img_capt_b),\n            ""image/caption_ids"": _int64_feature_list(img_capt_ids)\n        })\n    sequence_example = tf.train.SequenceExample(context=context, feature_lists=feature_lists)\n    writer.write(sequence_example.SerializeToString())  # Serialize To String\nwriter.close()\n\n# 2. Simple read one image =======================================================\nfilename_queue = tf.train.string_input_producer([""train.cat_caption""])\nreader = tf.TFRecordReader()\n_, serialized_example = reader.read(filename_queue)  # return the file and the name of file\n# features, sequence_features = tf.parse_single_example(serialized_example,  # see parse_single_sequence_example for sequence example\nfeatures, sequence_features = tf.parse_single_sequence_example(\n    serialized_example, context_features={\n        \'image/img_raw\': tf.FixedLenFeature([], tf.string),\n    }, sequence_features={\n        ""image/caption"": tf.FixedLenSequenceFeature([], dtype=tf.string),\n        ""image/caption_ids"": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n    }\n)\nc = tf.contrib.learn.run_n(features, n=1, feed_dict=None)\nim = Image.frombytes(\'RGB\', (299, 299), c[0][\'image/img_raw\'])\ntl.visualize.frame(np.asarray(im), second=1, saveable=False, name=\'frame\', fig_idx=1236)\nc = tf.contrib.learn.run_n(sequence_features, n=1, feed_dict=None)\nprint(c[0])\n\n\n# 3. Prefetch serialized SequenceExample protos ==================================\ndef distort_image(image, thread_id):\n    """"""Perform random distortions on an image.\n    Args:\n        image: A float32 Tensor of shape [height, width, 3] with values in [0, 1).\n        thread_id: Preprocessing thread id used to select the ordering of color\n        distortions. There should be a multiple of 2 preprocessing threads.\n    Returns:````\n        distorted_image: A float32 Tensor of shape [height, width, 3] with values in\n        [0, 1].\n    """"""\n    # Randomly flip horizontally.\n    with tf.name_scope(""flip_horizontal""):  # , values=[image]): # DH MOdify\n        # with tf.name_scope(""flip_horizontal"", values=[image]):\n        image = tf.image.random_flip_left_right(image)\n    # Randomly distort the colors based on thread id.\n    color_ordering = thread_id % 2\n    with tf.name_scope(""distort_color""):  # , values=[image]): # DH MOdify\n        # with tf.name_scope(""distort_color"", values=[image]): # DH MOdify\n        if color_ordering == 0:\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.032)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        elif color_ordering == 1:\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.032)\n        # The random_* ops do not necessarily clamp.\n        image = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image\n\n\n# def process_image(encoded_image,\n#                   is_training,\n#                   height,\n#                   width,\n#                   resize_height=346,\n#                   resize_width=346,\n#                   thread_id=0,\n#                   image_format=""jpeg""):\n#   """"""Decode an image, resize and apply random distortions.\n#   In training, images are distorted slightly differently depending on thread_id.\n#   Args:\n#     encoded_image: String Tensor containing the image.\n#     is_training: Boolean; whether preprocessing for training or eval.\n#     height: Height of the output image.\n#     width: Width of the output image.\n#     resize_height: If > 0, resize height before crop to final dimensions.\n#     resize_width: If > 0, resize width before crop to final dimensions.\n#     thread_id: Preprocessing thread id used to select the ordering of color\n#       distortions. There should be a multiple of 2 preprocessing threads.\n#     image_format: ""jpeg"" or ""png"".\n#   Returns:\n#     A float32 Tensor of shape [height, width, 3] with values in [-1, 1].\n#   Raises:\n#     ValueError: If image_format is invalid.\n#   """"""\n#   # Helper function to log an image summary to the visualizer. Summaries are\n#   # only logged in thread 0.\n#   def image_summary(name, image):\n#     if not thread_id:\n#       tf.image_summary(name, tf.expand_dims(image, 0))\n#\n#   # Decode image into a float32 Tensor of shape [?, ?, 3] with values in [0, 1).\n#   with tf.name_scope(""decode""):#, values=[encoded_image]):   # DH modify\n#   # with tf.name_scope(""decode"", values=[encoded_image]):   # DH modify\n#     if image_format == ""jpeg"":\n#       image = tf.image.decode_jpeg(encoded_image, channels=3)\n#     elif image_format == ""png"":\n#       image = tf.image.decode_png(encoded_image, channels=3)\n#     else:\n#       raise ValueError(""Invalid image format: %s"" % image_format)\n#   image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n#   image_summary(""original_image"", image)\n#\n#   # Resize image.\n#   assert (resize_height > 0) == (resize_width > 0)\n#   if resize_height:\n#     # image = tf.image.resize_images(image,\n#     #                                size=[resize_height, resize_width],\n#     #                                method=tf.image.ResizeMethod.BILINEAR)\n#\n#     image = tf.image.resize_images(image,       # DH Modify\n#                                    new_height=resize_height,\n#                                    new_width=resize_width,\n#                                    method=tf.image.ResizeMethod.BILINEAR)\n#\n#   # Crop to final dimensions.\n#   if is_training:\n#     image = tf.random_crop(image, [height, width, 3])\n#   else:\n#     # Central crop, assuming resize_height > height, resize_width > width.\n#     image = tf.image.resize_image_with_crop_or_pad(image, height, width)\n#\n#   image_summary(""resized_image"", image)\n#\n#   # Randomly distort the image.\n#   if is_training:\n#     image = distort_image(image, thread_id)\n#\n#   image_summary(""final_image"", image)\n#\n#   # Rescale to [-1,1] instead of [0, 1]\n#   image = tf.subtract(image, 0.5)\n#   image = tf.multiply(image, 2.0)\n#   return image\n\n\ndef prefetch_input_data(\n        reader, file_pattern, is_training, batch_size, values_per_shard, input_queue_capacity_factor=16,\n        num_reader_threads=1, shard_queue_name=""filename_queue"", value_queue_name=""input_queue""\n):\n    """"""Prefetches string values from disk into an input queue.\n\n    In training the capacity of the queue is important because a larger queue\n    means better mixing of training examples between shards. The minimum number of\n    values kept in the queue is values_per_shard * input_queue_capacity_factor,\n    where input_queue_memory factor should be chosen to trade-off better mixing\n    with memory usage.\n\n    Args:\n        reader: Instance of tf.ReaderBase.\n        file_pattern: Comma-separated list of file patterns (e.g.\n            /tmp/train_data-?????-of-00100).\n        is_training: Boolean; whether prefetching for training or eval.\n        batch_size: Model batch size used to determine queue capacity.\n        values_per_shard: Approximate number of values per shard.\n        input_queue_capacity_factor: Minimum number of values to keep in the queue\n        in multiples of values_per_shard. See comments above.\n        num_reader_threads: Number of reader threads to fill the queue.\n        shard_queue_name: Name for the shards filename queue.\n        value_queue_name: Name for the values input queue.\n\n    Returns:\n        A Queue containing prefetched string values.\n    """"""\n    data_files = []\n    for pattern in file_pattern.split("",""):\n        data_files.extend(tf.gfile.Glob(pattern))\n    if not data_files:\n        tl.logging.fatal(""Found no input files matching %s"", file_pattern)\n    else:\n        tl.logging.info(""Prefetching values from %d files matching %s"", len(data_files), file_pattern)\n\n    if is_training:\n        print(""   is_training == True : RandomShuffleQueue"")\n        filename_queue = tf.train.string_input_producer(data_files, shuffle=True, capacity=16, name=shard_queue_name)\n        min_queue_examples = values_per_shard * input_queue_capacity_factor\n        capacity = min_queue_examples + 100 * batch_size\n        values_queue = tf.RandomShuffleQueue(\n            capacity=capacity, min_after_dequeue=min_queue_examples, dtypes=[tf.string],\n            name=""random_"" + value_queue_name\n        )\n    else:\n        print(""   is_training == False : FIFOQueue"")\n        filename_queue = tf.train.string_input_producer(data_files, shuffle=False, capacity=1, name=shard_queue_name)\n        capacity = values_per_shard + 3 * batch_size\n        values_queue = tf.FIFOQueue(capacity=capacity, dtypes=[tf.string], name=""fifo_"" + value_queue_name)\n\n    enqueue_ops = []\n    for _ in range(num_reader_threads):\n        _, value = reader.read(filename_queue)\n        enqueue_ops.append(values_queue.enqueue([value]))\n    tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(values_queue, enqueue_ops))\n\n    tf.summary.scalar(\n        ""queue/%s/fraction_of_%d_full"" % (values_queue.name, capacity),\n        tf.cast(values_queue.size(), tf.float32) * (1. / capacity)\n    )\n\n    return values_queue\n\n\nis_training = True\nresize_height = resize_width = 346\nheight = width = 299\n# start to read\nreader = tf.TFRecordReader()\ninput_queue = prefetch_input_data(\n    reader,\n    file_pattern=""train.cat_caption"",  # sets train.???_caption to read many files\n    is_training=is_training,  # if training, shuffle and random choice\n    batch_size=4,\n    values_per_shard=2300,  # mixing between shards in training.\n    input_queue_capacity_factor=2,  # minimum number of shards to keep in the input queue.\n    num_reader_threads=1  # number of threads for prefetching SequenceExample protos.\n)\nserialized_sequence_example = input_queue.dequeue()\n# serialized_sequence_example = tf.train.string_input_producer([""train.cat_caption""])   # don\'t work\ncontext, sequence = tf.parse_single_sequence_example(\n    serialized=serialized_sequence_example, context_features={""image/img_raw"": tf.FixedLenFeature([], dtype=tf.string)},\n    sequence_features={\n        ""image/caption"": tf.FixedLenSequenceFeature([], dtype=tf.string),\n        ""image/caption_ids"": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n    }\n)\n\nimg = tf.decode_raw(context[""image/img_raw""], tf.uint8)\nimg = tf.reshape(img, [height, width, 3])\nimg = tf.image.convert_image_dtype(img, dtype=tf.float32)\n\ntry:\n    # for TensorFlow 0.11\n    img = tf.image.resize_images(img, size=(resize_height, resize_width), method=tf.image.ResizeMethod.BILINEAR)\nexcept Exception:\n    # for TensorFlow 0.10\n    img = tf.image.resize_images(\n        img, new_height=resize_height, new_width=resize_width, method=tf.image.ResizeMethod.BILINEAR\n    )\n# Crop to final dimensions.\nif is_training:\n    img = tf.random_crop(img, [height, width, 3])\nelse:\n    # Central crop, assuming resize_height > height, resize_width > width.\n    img = tf.image.resize_image_with_crop_or_pad(img, height, width)\n# Randomly distort the image.\nif is_training:\n    img = distort_image(img, thread_id=0)\n# Rescale to [-1, 1] instead of [0, 1]\nimg = tf.subtract(img, 0.5)\nimg = tf.multiply(img, 2.0)\nimg_cap = sequence[""image/caption""]\nimg_cap_ids = sequence[""image/caption_ids""]\nimg_batch, img_cap_batch, img_cap_ids_batch = tf.train.batch(\n    [img, img_cap, img_cap_ids],  # Note: shuffle_batch doesn\'t support dynamic_pad\n    batch_size=4,\n    capacity=50000,\n    dynamic_pad=True,  # string list pad with \'\', int list pad with 0\n    num_threads=4\n)\nsess = tf.Session()\n# sess.run(tf.global_variables_initializer())\ntl.layers.initialize_global_variables(sess)\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\nfor _ in range(3):\n    print(""Step %s"" % _)\n    # print(sess.run([img, img_cap, img_cap_ids]))                                 # one example only\n    imgs, caps, caps_id = sess.run([img_batch, img_cap_batch, img_cap_ids_batch])  # batch of examples with dynamic_pad\n    print(caps)\n    print(caps_id)\n    tl.visualize.images2d((imgs + 1) / 2, second=1, saveable=False, name=\'batch\', dtype=None, fig_idx=202025)\ncoord.request_stop()\ncoord.join(threads)\nsess.close()\n\n\n# 4. Prefetch serialized SequenceExample protos. Create MASK and TARGET =======\ndef batch_with_dynamic_pad(images_and_captions, batch_size, queue_capacity, add_summaries=True):\n    """"""Batches input images and captions.\n\n    This function splits the caption into an input sequence and a target sequence,\n    where the target sequence is the input sequence right-shifted by 1. Input and\n    target sequences are batched and padded up to the maximum length of sequences\n    in the batch. A mask is created to distinguish real words from padding words.\n\n    Example:\n        Actual captions in the batch (\'-\' denotes padded character):\n        [\n            [ 1 2 5 4 5 ],\n            [ 1 2 3 4 - ],\n            [ 1 2 3 - - ],\n        ]\n\n        input_seqs:\n        [\n            [ 1 2 3 4 ],\n            [ 1 2 3 - ],\n            [ 1 2 - - ],\n        ]\n\n        target_seqs:\n        [\n            [ 2 3 4 5 ],\n            [ 2 3 4 - ],\n            [ 2 3 - - ],\n        ]\n\n        mask:\n        [\n            [ 1 1 1 1 ],\n            [ 1 1 1 0 ],\n            [ 1 1 0 0 ],\n        ]\n\n    Args:\n        images_and_captions: A list of pairs [image, caption], where image is a\n        Tensor of shape [height, width, channels] and caption is a 1-D Tensor of\n        any length. Each pair will be processed and added to the queue in a\n        separate thread.\n        batch_size: Batch size.\n        queue_capacity: Queue capacity.\n        add_summaries: If true, add caption length summaries.\n\n    Returns:\n        images: A Tensor of shape [batch_size, height, width, channels].\n        input_seqs: An int32 Tensor of shape [batch_size, padded_length].\n        target_seqs: An int32 Tensor of shape [batch_size, padded_length].\n        mask: An int32 0/1 Tensor of shape [batch_size, padded_length].\n    """"""\n    enqueue_list = []\n    for image, caption in images_and_captions:\n        caption_length = tf.shape(caption)[0]\n        input_length = tf.expand_dims(tf.subtract(caption_length, 1), 0)\n\n        input_seq = tf.slice(caption, [0], input_length)\n        target_seq = tf.slice(caption, [1], input_length)\n        indicator = tf.ones(input_length, dtype=tf.int32)\n        enqueue_list.append([image, input_seq, target_seq, indicator])\n\n    images, input_seqs, target_seqs, mask = tf.train.batch_join(\n        enqueue_list, batch_size=batch_size, capacity=queue_capacity, dynamic_pad=True, name=""batch_and_pad""\n    )\n\n    if add_summaries:\n        lengths = tf.add(tf.reduce_sum(mask, 1), 1)\n        tf.summary.scalar(""caption_length/batch_min"", tf.reduce_min(lengths))\n        tf.summary.scalar(""caption_length/batch_max"", tf.reduce_max(lengths))\n        tf.summary.scalar(""caption_length/batch_mean"", tf.reduce_mean(lengths))\n\n    return images, input_seqs, target_seqs, mask\n\n\nimages, input_seqs, target_seqs, input_mask = (\n    batch_with_dynamic_pad(images_and_captions=[[img, img_cap]], batch_size=4, queue_capacity=50000)\n)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\nfor _ in range(3):\n    print(""Step %s"" % _)\n    imgs, inputs, targets, masks = sess.run([images, input_seqs, target_seqs, input_mask])\n    print(inputs)\n    print(targets)\n    print(masks)\n    tl.visualize.images2d((imgs + 1) / 2, second=1, saveable=False, name=\'batch\', dtype=None, fig_idx=202025)\ncoord.request_stop()\ncoord.join(threads)\nsess.close()\n'"
examples/database/dispatch_tasks.py,1,"b'""""""\nA sample script that shows how to distribute multiple tasks to multiple machine\nusing the database module.\n\n""""""\nimport time\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n# tf.logging.set_verbosity(tf.logging.DEBUG)\n\n# connect to database\ndb = tl.db.TensorHub(ip=\'localhost\', port=27017, dbname=\'temp\', project_name=\'tutorial\')\n\n# delete existing tasks, models and datasets in this project\ndb.delete_tasks()\ndb.delete_model()\ndb.delete_datasets()\n\n# save dataset into database, then allow  other servers to use it\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\ndb.save_dataset((X_train, y_train, X_val, y_val, X_test, y_test), \'mnist\', description=\'handwriting digit\')\n\n# push tasks into database, then allow other servers pull tasks to run\ndb.create_task(\n    task_name=\'mnist\', script=\'task_script.py\', hyper_parameters=dict(n_units1=800, n_units2=800),\n    saved_result_keys=[\'test_accuracy\'], description=\'800-800\'\n)\n\ndb.create_task(\n    task_name=\'mnist\', script=\'task_script.py\', hyper_parameters=dict(n_units1=600, n_units2=600),\n    saved_result_keys=[\'test_accuracy\'], description=\'600-600\'\n)\n\ndb.create_task(\n    task_name=\'mnist\', script=\'task_script.py\', hyper_parameters=dict(n_units1=400, n_units2=400),\n    saved_result_keys=[\'test_accuracy\'], description=\'400-400\'\n)\n\n# wait for tasks to finish\nwhile db.check_unfinished_task(task_name=\'mnist\'):\n    print(""waiting runners to finish the tasks"")\n    time.sleep(1)\n\n# get the best model\nprint(""all tasks finished"")\nnet = db.find_top_model(model_name=\'mlp\', sort=[(""test_accuracy"", -1)])\nprint(""the best accuracy {} is from model {}"".format(net._test_accuracy, net._name))\n'"
examples/database/run_tasks.py,0,"b'""""""\nRun this script on servers, it will monitor the database and run tasks when\ntask distributor push a task to the database.\n\n""""""\nimport time\n\nimport tensorlayer as tl\n\n# tl.logging.set_verbosity(tl.logging.DEBUG)\n\n# connect to database\ndb = tl.db.TensorHub(ip=\'localhost\', port=27017, dbname=\'temp\', project_name=\'tutorial\')\n\n# monitors the database and pull tasks to run\nwhile True:\n    print(""waiting task from distributor"")\n    db.run_top_task(task_name=\'mnist\', sort=[(""time"", -1)])\n    time.sleep(1)\n'"
examples/database/task_script.py,7,"b'""""""Sample task script.""""""\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\n# tf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# connect to database\ndb = tl.db.TensorHub(ip=\'localhost\', port=27017, dbname=\'temp\', project_name=\'tutorial\')\n\n# load dataset from database\nX_train, y_train, X_val, y_val, X_test, y_test = db.find_top_dataset(\'mnist\')\n\n\n# define the network\ndef mlp():\n    ni = tl.layers.Input([None, 784], name=\'input\')\n    net = tl.layers.Dropout(keep=0.8, name=\'drop1\')(ni)\n    net = tl.layers.Dense(n_units=n_units1, act=tf.nn.relu, name=\'relu1\')(net)\n    net = tl.layers.Dropout(keep=0.5, name=\'drop2\')(net)\n    net = tl.layers.Dense(n_units=n_units2, act=tf.nn.relu, name=\'relu2\')(net)\n    net = tl.layers.Dropout(keep=0.5, name=\'drop3\')(net)\n    net = tl.layers.Dense(n_units=10, act=None, name=\'output\')(net)\n    M = tl.models.Model(inputs=ni, outputs=net)\n    return M\n\n\nnetwork = mlp()\n\n# cost and accuracy\ncost = tl.cost.cross_entropy\n\n\ndef acc(y, y_):\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.convert_to_tensor(y_, tf.int64))\n    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\n# define the optimizer\ntrain_op = tf.optimizers.Adam(learning_rate=0.0001)\n\n# train the network\n# tl.utils.fit(\n#     network, train_op, cost, X_train, y_train, acc=acc, batch_size=500, n_epoch=20, print_freq=5,\n#     X_val=X_val, y_val=y_val, eval_train=False\n# )\n\ntl.utils.fit(\n    network,\n    train_op=tf.optimizers.Adam(learning_rate=0.0001),\n    cost=tl.cost.cross_entropy,\n    X_train=X_train,\n    y_train=y_train,\n    acc=acc,\n    batch_size=256,\n    n_epoch=20,\n    X_val=X_val,\n    y_val=y_val,\n    eval_train=False,\n)\n\n# evaluation and save result that match the result_key\ntest_accuracy = tl.utils.test(network, acc, X_test, y_test, batch_size=None, cost=cost)\ntest_accuracy = float(test_accuracy)\n\n# save model into database\ndb.save_model(network, model_name=\'mlp\', name=str(n_units1) + \'-\' + str(n_units2), test_accuracy=test_accuracy)\n# in other script, you can load the model as follow\n# net = db.find_model(sess=sess, model_name=str(n_units1)+\'-\'+str(n_units2)\n'"
examples/deprecated_tutorials/tutorial_image_preprocess.py,0,"b'""""""Data Augmentation by numpy, scipy, threading and queue.\n\nNote that, TensorFlow\'s TFRecord and Dataset API are faster.\n\n""""""\n\nimport time\n\nimport tensorlayer as tl\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\nX_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n\n\ndef distort_img(x):\n    x = tl.prepro.flip_axis(x, axis=1, is_random=True)\n    x = tl.prepro.crop(x, wrg=28, hrg=28, is_random=True)\n    return x\n\n\ns = time.time()\nresults = tl.prepro.threading_data(X_train[0:100], distort_img)\nprint(""took %.3fs"" % (time.time() - s))\nprint(results.shape)\n\ntl.vis.save_images(X_train[0:10], [1, 10], \'_original.png\')\ntl.vis.save_images(results[0:10], [1, 10], \'_distorted.png\')\n'"
examples/deprecated_tutorials/tutorial_imagenet_inceptionV3_distributed.py,79,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""Example of training an Inception V3 model with ImageNet.\n\nThe parameters are set as in the best results of the paper: https://arxiv.org/abs/1512.00567\n\nThe dataset can be downloaded from http://www.image-net.org/ or from the Kaggle competition:\nhttps://www.kaggle.com/c/imagenet-object-localization-challenge/data\n\n""""""\n\nimport argparse\nimport logging\nimport multiprocessing\nimport os\nimport random\nimport sys\nimport time\nfrom xml.etree import ElementTree\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import (inception_v3, inception_v3_arg_scope)\nfrom tensorflow.python.framework.errors_impl import OutOfRangeError\nfrom tensorflow.python.training import session_run_hook\nfrom tensorflow.python.training.basic_session_run_hooks import StopAtStepHook\nfrom tensorflow.python.training.monitored_session import \\\n    SingularMonitoredSession\n\nimport tensorlayer as tl\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n########## VARIABLES ##########\n\n# get the dataset: https://www.kaggle.com/c/imagenet-object-localization-challenge/data\n# get the synset dictionary: http://www.image-net.org/archive/words.txt\n\nBASE_DIR = \'./\'\nILSVRC_DIR = os.path.join(BASE_DIR, \'ILSVRC\')\nSYNSET_DICT = os.path.join(BASE_DIR, \'words.txt\')\nTRAIN_FILE = os.path.join(BASE_DIR, \'train.csv\')\nVAL_FILE = os.path.join(BASE_DIR, \'val.csv\')\nCLASSES_FILE = os.path.join(BASE_DIR, \'classes.csv\')\nCLASSES_VAL_FILE = os.path.join(BASE_DIR, \'classes_val.csv\')\nCHECKPOINTS_PATH = \'./checkpoints\'\n\n########## DATASETS ##########\n\n\ndef get_data_sample(annotation_file, annotations_dir, data_dir):\n    labels = []\n    image_file = annotation_file.replace(annotations_dir, data_dir).replace(\'.xml\', \'.JPEG\')\n    if tf.gfile.Exists(annotation_file) and tf.gfile.Exists(image_file):\n        xmltree = ElementTree.parse(annotation_file)\n        objects = xmltree.findall(""object"")\n        for object_iter in objects:\n            labels.append(object_iter.find(""name"").text)\n    else:\n        image_file = None\n    return image_file, labels\n\n\ndef might_create_dataset(prefix, file, shuffle=False, suffix=\'**/*.xml\'):\n    # load data\n    data = []\n    labels = set()\n    annotations_dir = os.path.join(ILSVRC_DIR, \'Annotations\', \'CLS-LOC\', prefix)\n    data_dir = os.path.join(ILSVRC_DIR, \'Data\', \'CLS-LOC\', prefix)\n    for filename in tf.gfile.Glob(os.path.join(annotations_dir, suffix)):\n        image_path, image_labels = get_data_sample(filename, annotations_dir, data_dir)\n        if image_path is not None and len(image_labels) > 0:\n            data.append([image_path] + image_labels)\n            for label in image_labels:\n                labels.add(label)\n    if shuffle:\n        random.shuffle(data)\n    # write data\n    with tf.gfile.Open(file, \'w\') as f:\n        for d in data:\n            f.write(\'{}\\n\'.format(\',\'.join(d)))\n    return sorted(labels)\n\n\ndef might_create_training_set():\n    if not tf.gfile.Exists(TRAIN_FILE):\n        labels = might_create_dataset(\'train\', TRAIN_FILE, shuffle=True)\n        with tf.gfile.Open(CLASSES_FILE, \'w\') as f:\n            for l in labels:\n                f.write(\'{}\\n\'.format(l))\n\n\ndef might_create_validation_set():\n    if not tf.gfile.Exists(VAL_FILE):\n        labels = might_create_dataset(\'val\', VAL_FILE, suffix=\'*.xml\')\n        with tf.gfile.Open(CLASSES_VAL_FILE, \'w\') as f:\n            for l in labels:\n                f.write(\'{}\\n\'.format(l))\n\n\ndef load_data(file, task_spec=None, batch_size=16, epochs=1, shuffle_size=0):\n    # load classes dict:\n    with tf.gfile.Open(CLASSES_FILE) as f:\n        labels = dict()\n        for i, line in enumerate(f.readlines()):\n            label = line.strip()\n            labels[label] = i\n    num_classes = len(labels)\n    # count file examples\n    with tf.gfile.Open(file) as f:\n        size = len(f.readlines())\n\n    image_size = inception_v3.default_image_size\n    dataset = tf.data.TextLineDataset([file])\n    dataset = dataset.repeat(epochs)\n    # split the dataset in shards\n    if task_spec is not None and task_spec.num_workers > 1 and not task_spec.is_evaluator():\n        dataset = dataset.shard(num_shards=task_spec.num_workers, index=task_spec.shard_index)\n    if shuffle_size > 0:\n        dataset = dataset.shuffle(buffer_size=shuffle_size)\n\n    def _parse_example_fn(line):\n        line_split = line.decode().split(\',\')\n        filename = line_split[0]\n        labels_names = line_split[1:]\n        # labels\n        one_hot_labels = np.zeros(num_classes, dtype=np.float32)\n        for l in labels_names:\n            one_hot_labels[labels[l]] = 1.0\n        # image\n        image_bytes = tf.gfile.FastGFile(filename, \'rb\').read()\n        return image_bytes, one_hot_labels\n\n    def _map_fn(example_serialized):\n        image_bytes, one_hot_labels = tf.py_func(\n            _parse_example_fn, [example_serialized], [tf.string, tf.float32], stateful=False\n        )\n\n        image = tf.image.decode_jpeg(image_bytes, channels=3)\n        image = tf.image.resize_images(image, size=[image_size, image_size])\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        one_hot_labels = tf.reshape(one_hot_labels, [num_classes])\n        return image, one_hot_labels\n\n    max_cpus = multiprocessing.cpu_count()\n    dataset = dataset.map(_map_fn, num_parallel_calls=max_cpus)\n    dataset = dataset.prefetch(batch_size * max_cpus + 100)\n    dataset = dataset.batch(batch_size)\n    images, one_hot_classes = dataset.make_one_shot_iterator().get_next()\n\n    images = tf.reshape(images, [batch_size, image_size, image_size, 3])\n    one_hot_classes = tf.reshape(one_hot_classes, [batch_size, num_classes])\n\n    return images, one_hot_classes, num_classes, size\n\n\n########## NETWORK ##########\n\n\ndef build_network(image_input, num_classes=1001, is_training=False):\n    net_in = tl.layers.InputLayer(image_input, name=\'input_layer\')\n    with slim.arg_scope(inception_v3_arg_scope()):\n        network = tl.layers.SlimNetsLayer(\n            prev_layer=net_in, slim_layer=inception_v3, slim_args={\n                \'num_classes\': num_classes,\n                \'is_training\': is_training\n            }, name=\'InceptionV3\'\n        )\n\n    predictions = tf.nn.sigmoid(network.outputs, name=\'Predictions\')\n    return network, predictions\n\n\n########## EVALUATOR ##########\n\n\nclass EvaluatorStops(Exception):\n\n    def __init__(self, message):\n        super(EvaluatorStops, self).__init__(message)\n\n\nclass EvaluatorHook(session_run_hook.SessionRunHook):\n\n    def __init__(self, checkpoints_path, saver):\n        self.checkpoints_path = checkpoints_path\n        self.summary_writer = tf.summary.FileWriter(os.path.join(checkpoints_path, \'validation\'))\n        self.lastest_checkpoint = None\n        self.saver = saver\n        self.summary = None\n\n    def after_create_session(self, session, coord):\n        checkpoint = tf.train.latest_checkpoint(self.checkpoints_path)\n        # wait until a new check point is available\n        total_waited_secs = 0\n        while self.lastest_checkpoint == checkpoint:\n            time.sleep(30)  # sleep 30 seconds waiting for a new checkpoint\n            checkpoint = tf.train.latest_checkpoint(self.checkpoints_path)\n            total_waited_secs += 30\n            if total_waited_secs > 30 * 60 * 60:\n                raise EvaluatorStops(\'Waited more than half an hour to load a new checkpoint\')\n\n        # restore the checkpoint\n        self.saver.restore(session, checkpoint)\n        self.lastest_checkpoint = checkpoint\n        self.eval_step = int(self.lastest_checkpoint.split(\'-\')[-1])\n\n    def end(self, session):\n        super(EvaluatorHook, self).end(session)\n        # save summaries\n        self.summary_writer.add_summary(self.summary, self.eval_step)\n\n\n########## METRICS ##########\n\n\ndef calculate_metrics(predicted_batch, real_batch, threshold=0.5, is_training=False, ema_decay=0.9):\n    with tf.variable_scope(\'metric\'):\n        threshold_graph = tf.constant(threshold, name=\'threshold\')\n        zero_point_five = tf.constant(0.5)\n        predicted_bool = tf.greater_equal(predicted_batch, threshold_graph)\n        real_bool = tf.greater_equal(real_batch, zero_point_five)\n        predicted_bool_neg = tf.logical_not(predicted_bool)\n        real_bool_neg = tf.logical_not(real_bool)\n        differences_bool = tf.logical_xor(predicted_bool, real_bool)\n        tp = tf.logical_and(predicted_bool, real_bool)\n        tn = tf.logical_and(predicted_bool_neg, real_bool_neg)\n        fn = tf.logical_and(differences_bool, real_bool)\n        fp = tf.logical_and(differences_bool, predicted_bool)\n        tp = tf.reduce_sum(tf.cast(tp, tf.float32))\n        tn = tf.reduce_sum(tf.cast(tn, tf.float32))\n        fn = tf.reduce_sum(tf.cast(fn, tf.float32))\n        fp = tf.reduce_sum(tf.cast(fp, tf.float32))\n\n        average_ops = None\n        init_op = None\n        if is_training:\n            ema = tf.train.ExponentialMovingAverage(decay=ema_decay)\n            average_ops = ema.apply([tp, tn, fp, fn])\n            tp = ema.average(tp)\n            tn = ema.average(tn)\n            fp = ema.average(fp)\n            fn = ema.average(fn)\n        else:\n            tp_v = tf.Variable(0, dtype=tf.float32, name=\'true_positive\', trainable=False)\n            tn_v = tf.Variable(0, dtype=tf.float32, name=\'true_negative\', trainable=False)\n            fp_v = tf.Variable(0, dtype=tf.float32, name=\'false_positive\', trainable=False)\n            fn_v = tf.Variable(0, dtype=tf.float32, name=\'false_negative\', trainable=False)\n            init_op = [tf.assign(tp_v, 0), tf.assign(tn_v, 0), tf.assign(fp_v, 0), tf.assign(fn_v, 0)]\n            tp = tf.assign_add(tp_v, tp)\n            tn = tf.assign_add(tn_v, tn)\n            fp = tf.assign_add(fp_v, fp)\n            fn = tf.assign_add(fn_v, fn)\n\n        # calculate metrics\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        accuracy = (tp + tn) / (tp + tn + fp + fn)\n        fall_out = fp / (tn + fp)\n        f1_score = tp * 2 / (tp * 2 + fp + fn)\n\n        # remove NaNs and set them to 0\n        zero = tf.constant(0, dtype=tf.float32)\n        precision = tf.cond(tf.equal(tp, 0.0), lambda: zero, lambda: precision)\n        recall = tf.cond(tf.equal(tp, 0.0), lambda: zero, lambda: recall)\n        accuracy = tf.cond(tf.equal(tp + tn, 0.0), lambda: zero, lambda: accuracy)\n        fall_out = tf.cond(tf.equal(fp, 0.0), lambda: zero, lambda: fall_out)\n        f1_score = tf.cond(tf.equal(tp, 0.0), lambda: zero, lambda: f1_score)\n\n        # add to tensorboard\n        # tf.summary.scalar(\'accuracy\', accuracy)\n        tf.summary.scalar(\'precision\', precision)\n        tf.summary.scalar(\'recall\', recall)\n        tf.summary.scalar(\'fall-out\', fall_out)\n        tf.summary.scalar(\'f1-score\', f1_score)\n        tf.summary.scalar(\'true_positive\', tp)\n        tf.summary.scalar(\'true_negative\', tn)\n        tf.summary.scalar(\'false_positive\', fp)\n        tf.summary.scalar(\'false_negative\', fn)\n\n    metrics_ops = {\n        # \'accuracy\' : accuracy,\n        \'precision\': precision,\n        \'recall\': recall,\n        \'fall-out\': fall_out,\n        \'f1-score\': f1_score,\n        \'true positive\': tp,\n        \'true negative\': tn,\n        \'false positive\': fp,\n        \'false negative\': fn,\n    }\n    return init_op, average_ops, metrics_ops\n\n\ndef run_evaluator(task_spec, checkpoints_path, batch_size=32):\n    with tf.Graph().as_default():\n        # load dataset\n        images_input, one_hot_classes, num_classes, _dataset_size = load_data(\n            file=VAL_FILE, task_spec=task_spec, batch_size=batch_size, epochs=1\n        )\n        _network, predictions = build_network(images_input, num_classes=num_classes, is_training=False)\n        saver = tf.train.Saver()\n        # metrics\n        metrics_init_ops, _, metrics_ops = calculate_metrics(\n            predicted_batch=predictions, real_batch=one_hot_classes, is_training=False\n        )\n        # tensorboard summary\n        summary_op = tf.summary.merge_all()\n        # session hook\n        evaluator_hook = EvaluatorHook(checkpoints_path=checkpoints_path, saver=saver)\n\n        try:\n            # infinite loop\n            while True:\n                with SingularMonitoredSession(hooks=[evaluator_hook]) as sess:\n                    sess.run(metrics_init_ops)\n                    try:\n                        while not sess.should_stop():\n                            metrics, summary = sess.run([metrics_ops, summary_op])\n                            evaluator_hook.summary = summary\n                    except OutOfRangeError:\n                        pass\n                    logging.info(\'step: {}  {}\'.format(evaluator_hook.eval_step, metrics))\n        except EvaluatorStops:\n            # the evaluator has waited too long for a new checkpoint\n            pass\n\n\n########## TRAINING ##########\n\n\ndef run_worker(task_spec, checkpoints_path, batch_size=32, epochs=10):\n    device_fn = task_spec.device_fn() if task_spec is not None else None\n    # create graph\n    with tf.Graph().as_default():\n        global_step = tf.train.get_or_create_global_step()\n        with tf.device(device_fn):\n            # load dataset\n            images_input, one_hot_classes, num_classes, dataset_size = load_data(\n                file=TRAIN_FILE, task_spec=task_spec, batch_size=batch_size, epochs=epochs, shuffle_size=10000\n            )\n            # network\n            network, predictions = build_network(images_input, num_classes=num_classes, is_training=True)\n            # training operations\n            loss = tl.cost.sigmoid_cross_entropy(output=network.outputs, target=one_hot_classes, name=\'loss\')\n            steps_per_epoch = dataset_size / batch_size\n            learning_rate = tf.train.exponential_decay(\n                learning_rate=0.045,\n                global_step=global_step,\n                decay_steps=steps_per_epoch * 2,  # 2 epochs\n                decay_rate=0.94,\n                staircase=True,\n                name=\'learning_rate\'\n            )\n            optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=0.9, epsilon=1.0)\n            # clip and apply gradients\n            gvs = optimizer.compute_gradients(loss=loss, var_list=network.all_params)\n            capped_gvs = []\n            for grad, var in gvs:\n                if grad is not None:\n                    grad = tf.clip_by_value(grad, -2., 2.)\n                capped_gvs.append((grad, var))\n            train_op = optimizer.apply_gradients(grads_and_vars=capped_gvs, global_step=global_step)\n            # metrics\n            tf.summary.scalar(\'learning_rate/value\', learning_rate)\n            tf.summary.scalar(\'loss/logits\', loss)\n            _, metrics_average_ops, metrics_ops = calculate_metrics(\n                predicted_batch=predictions, real_batch=one_hot_classes, is_training=True\n            )\n            with tf.control_dependencies([train_op]):\n                train_op = tf.group(metrics_average_ops)\n\n        # start training\n        hooks = [StopAtStepHook(last_step=steps_per_epoch * epochs)]\n        with tl.distributed.DistributedSession(task_spec=task_spec, hooks=hooks, checkpoint_dir=checkpoints_path,\n                                               save_summaries_secs=None, save_summaries_steps=300,\n                                               save_checkpoint_secs=60 * 60) as sess:\n            # print network information\n            if task_spec is None or task_spec.is_master():\n                network.print_params(False, session=sess)\n                network.print_layers()\n                sys.stdout.flush()\n            # run training\n            try:\n                last_log_time = time.time()\n                next_log_time = last_log_time + 60\n                while not sess.should_stop():\n                    step, loss_val, learning_rate_val, _, metrics = sess.run(\n                        [global_step, loss, learning_rate, train_op, metrics_ops]\n                    )\n                    if task_spec is None or task_spec.is_master():\n                        now = time.time()\n                        if now > next_log_time:\n                            last_log_time = now\n                            next_log_time = last_log_time + 60\n                            current_epoch = \'{:.3f}\'.format(float(step) / steps_per_epoch)\n                            max_steps = epochs * steps_per_epoch\n                            m = \'Epoch: {}/{} Steps: {}/{} Loss: {} Learning rate: {} Metrics: {}\'\n                            logging.info(\n                                m.format(current_epoch, epochs, step, max_steps, loss_val, learning_rate_val, metrics)\n                            )\n            except OutOfRangeError:\n                pass\n\n\n########## MAIN ##########\n\nif __name__ == \'__main__\':\n    # print output logging\n    logging.basicConfig(level=logging.INFO, format=\'%(asctime)-15s %(message)s\')\n\n    if not tf.gfile.Exists(ILSVRC_DIR):\n        raise FileNotFoundError(\n            \'We cannot find the directory ""{}""\\n\'\n            \'You need to modify the variable BASE_DIR with the path where the dataset is.\\n\'\n            \'The dataset can be downloaded from http://www.image-net.org/ or from the Kaggle competition:\\n\'\n            \'https://www.kaggle.com/c/imagenet-object-localization-challenge/data\'.format(ILSVRC_DIR)\n        )\n\n    # args\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--with_evaluator\', dest=\'with_evaluator\', action=\'store_true\')\n    parser.add_argument(\'--batch_size\', dest=\'batch_size\', type=int, default=32)\n    parser.add_argument(\'--epochs\', dest=\'epochs\', type=int, default=100)\n    parser.set_defaults(with_evaluator=False)\n    args = parser.parse_args()\n    logging.info(\'Batch size: {}\'.format(args.batch_size))\n    logging.info(\'Epochs: {}\'.format(args.epochs))\n\n    # check the dataset and create them if necessary\n    might_create_training_set()\n    might_create_validation_set()\n\n    # load environment for distributed training using last worker as evaluator\n    task_spec = tl.distributed.TaskSpec()\n\n    if task_spec is None:\n        logging.info(\'Run in single node\')\n        run_worker(task_spec, CHECKPOINTS_PATH, batch_size=args.batch_size, epochs=args.epochs)\n    else:\n        if args.with_evaluator:\n            # run with evaluator\n            logging.info(\'Last worker is the evaluator\')\n            task_spec = task_spec.use_last_worker_as_evaluator()\n\n        if task_spec.is_evaluator():\n            run_evaluator(task_spec, CHECKPOINTS_PATH, batch_size=args.batch_size)\n        else:\n            task_spec.create_server()\n            run_worker(task_spec, CHECKPOINTS_PATH, batch_size=args.batch_size, epochs=args.epochs)\n'"
examples/deprecated_tutorials/tutorial_mnist_distributed.py,11,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""Alpha Version for Distributed Training\n\nyou can test this example in your local machine using 2 workers and 1 ps like below,\nwhere CUDA_VISIBLE_DEVICES can be used to set the GPUs the process can use.\n\nCUDA_VISIBLE_DEVICES= TF_CONFIG=\'{""cluster"": {""ps"": [""127.0.0.1:3001""], ""worker"": [""127.0.0.1:3002"", ""127.0.0.1:3003""]}, ""task"": {""type"": ""worker"", ""index"": 0}}\' python example/tutorial_mnist_distributed.py > output-master 2>&1 &\nCUDA_VISIBLE_DEVICES= TF_CONFIG=\'{""cluster"": {""ps"": [""127.0.0.1:3001""], ""worker"": [""127.0.0.1:3002"", ""127.0.0.1:3003""]}, ""task"": {""type"": ""worker"", ""index"": 1}}\' python example/tutorial_mnist_distributed.py > output-worker 2>&1 &\nCUDA_VISIBLE_DEVICES= TF_CONFIG=\'{""cluster"": {""ps"": [""127.0.0.1:3001""], ""worker"": [""127.0.0.1:3002"", ""127.0.0.1:3003""]}, ""task"": {""type"": ""ps"", ""index"": 0}}\' python example/tutorial_mnist_distributed.py > output-ps 2>&1 &\nNote: for GPU, please set CUDA_VISIBLE_DEVICES=GPU_ID\n\n""""""\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# load environment for distributed training\ntask_spec = tl.distributed.TaskSpec()\ntask_spec.create_server()\ndevice_fn = task_spec.device_fn() if task_spec is not None else None\n\n# prepare data\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n\n# create graph\nwith tf.device(device_fn):\n    # define placeholder\n    x = tf.placeholder(tf.float32, shape=[None, 784], name=\'x\')\n    y_ = tf.placeholder(tf.int64, shape=[None], name=\'y_\')\n\n    # define the network\n    network = tl.layers.InputLayer(x, name=\'input\')\n    network = tl.layers.DropoutLayer(network, keep=0.8, name=\'drop1\')\n    network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name=\'relu1\')\n    network = tl.layers.DropoutLayer(network, keep=0.5, name=\'drop2\')\n    network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name=\'relu2\')\n    network = tl.layers.DropoutLayer(network, keep=0.5, name=\'drop3\')\n    # the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\n    # speed up computation, so we use identity here.\n    # see tf.nn.sparse_softmax_cross_entropy_with_logits()\n    network = tl.layers.DenseLayer(network, n_units=10, act=None, name=\'output\')\n\n    # define cost function and metric.\n    y = network.outputs\n    cost = tl.cost.cross_entropy(y, y_, name=\'cost\')\n    correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    y_op = tf.argmax(tf.nn.softmax(y), 1)\n\n    # define the optimizer\n    train_params = network.all_params\n    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)\n\n    with tl.distributed.DistributedSession(task_spec=task_spec) as sess:\n        # print network information\n        if task_spec.is_master():\n            network.print_params(session=sess)\n            network.print_layers()\n            print_freq = 5\n            eval_train = False\n        else:\n            print_freq = 1000\n            eval_train = False\n\n        # We do not need to initialize the variables as the session does it\n        #tl.layers.initialize_global_variables(sess)\n\n        # train the network\n        tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_, \\\n            acc=acc, batch_size=500, n_epoch=500, print_freq=print_freq, \\\n            X_val=X_val, y_val=y_val, eval_train=eval_train)\n\n        if task_spec.is_master():\n            # evaluation\n            tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)\n\n            # save the network to .npz file\n            tl.files.save_npz(network.all_params, name=\'model.npz\')\n'"
examples/distributed_training/tutorial_cifar10_distributed_trainer.py,23,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\nr""""""\n1. Before you start, run this script: https://github.com/tensorlayer/tensorlayer/blob/distributed/scripts/download_and_install_openmpi3_linux.sh\n2. Update the PATH with OpenMPI bin by running: PATH=$PATH:$HOME/local/openmpi/bin\n   Update the PATH in ~/.bashrc if you want OpenMPI to be ready once the machine start\n3. Then  XXXXX   Milo please add this part\n    mpirun -np 2 \\\n        -bind-to none -map-by slot \\\n        -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH \\\n        -mca pml ob1 -mca btl ^openib \\\n        python3 xxxxx.py\n""""""\n\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (BatchNormLayer, Conv2d, DenseLayer, FlattenLayer, InputLayer, MaxPool2d)\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n\ndef make_dataset(images, labels, num_epochs=1, shuffle_data_seed=0):\n    img = tf.data.Dataset.from_tensor_slices(images)\n    lab = tf.data.Dataset.from_tensor_slices(np.array(labels, dtype=np.int64))\n    dataset = tf.data.Dataset.zip((img, lab))\n    dataset = dataset.repeat(num_epochs).shuffle(buffer_size=10000, seed=shuffle_data_seed)\n    return dataset\n\n\ndef data_aug_train(img, ann):\n    # 1. Randomly crop a [height, width] section of the image.\n    img = tf.random_crop(img, [24, 24, 3])\n    # 2. Randomly flip the image horizontally.\n    img = tf.image.random_flip_left_right(img)\n    # 3. Randomly change brightness.\n    img = tf.image.random_brightness(img, max_delta=63)\n    # 4. Randomly change contrast.\n    img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n    # 5. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    return img, ann\n\n\ndef data_aug_valid(img, ann):\n    # 1. Crop the central [height, width] of the image.\n    img = tf.image.resize_image_with_crop_or_pad(img, 24, 24)\n    # 2. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    return img, ann\n\n\ndef model(x, is_train):\n    with tf.variable_scope(""model"", reuse=tf.AUTO_REUSE):\n        net = InputLayer(x, name=\'input\')\n        net = Conv2d(net, 64, (5, 5), (1, 1), padding=\'SAME\', b_init=None, name=\'cnn1\')\n        net = BatchNormLayer(net, decay=0.99, is_train=is_train, act=tf.nn.relu, name=\'batch1\')\n        net = MaxPool2d(net, (3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')\n\n        net = Conv2d(net, 64, (5, 5), (1, 1), padding=\'SAME\', b_init=None, name=\'cnn2\')\n        net = BatchNormLayer(net, decay=0.99, is_train=is_train, act=tf.nn.relu, name=\'batch2\')\n        net = MaxPool2d(net, (3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')\n\n        net = FlattenLayer(net, name=\'flatten\')\n        net = DenseLayer(net, 384, act=tf.nn.relu, name=\'d1relu\')\n        net = DenseLayer(net, 192, act=tf.nn.relu, name=\'d2relu\')\n        net = DenseLayer(net, 10, act=None, name=\'output\')\n    return net\n\n\ndef build_train(x, y_):\n    net = model(x, is_train=True)\n    cost = tl.cost.cross_entropy(net.outputs, y_, name=\'cost_train\')\n    L2 = 0\n    for p in tl.layers.get_variables_with_name(\'relu/W\', True, True):\n        L2 += tf.contrib.layers.l2_regularizer(0.004)(p)\n    cost = cost + L2\n    accurate_prediction = tf.equal(tf.argmax(net.outputs, 1), y_)\n    accuracy = tf.reduce_mean(tf.cast(accurate_prediction, tf.float32), name=\'accuracy_train\')\n    log_tensors = {\'cost\': cost, \'accuracy\': accuracy}\n    return net, cost, log_tensors\n\n\ndef build_validation(x, y_):\n    net = model(x, is_train=False)\n    cost = tl.cost.cross_entropy(net.outputs, y_, name=\'cost_test\')\n    accurate_prediction = tf.equal(tf.argmax(net.outputs, 1), y_)\n    accuracy = tf.reduce_mean(tf.cast(accurate_prediction, tf.float32), name=\'accuracy_test\')\n    return net, [cost, accuracy]\n\n\nif __name__ == \'__main__\':\n    # Load CIFAR10 data\n    X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n\n    # Setup the trainer\n    training_dataset = make_dataset(X_train, y_train)\n    training_dataset = training_dataset.map(data_aug_train, num_parallel_calls=multiprocessing.cpu_count())\n    # validation_dataset = make_dataset(X_test, y_test)\n    # validation_dataset = training_dataset.map(data_aug_valid, num_parallel_calls=multiprocessing.cpu_count())\n    trainer = tl.distributed.Trainer(\n        build_training_func=build_train, training_dataset=training_dataset, optimizer=tf.train.AdamOptimizer,\n        optimizer_args={\'learning_rate\': 0.0001}, batch_size=128, prefetch_size=128\n        # validation_dataset=validation_dataset, build_validation_func=build_validation\n    )\n\n    # There are multiple ways to use the trainer:\n    # 1. Easiest way to train all data: trainer.train_to_end()\n    # 2. Train with validation in the middle: trainer.train_and_validate_to_end(validate_step_size=100)\n    # 3. Train with full control like follows:\n    while not trainer.session.should_stop():\n        try:\n            # Run a training step synchronously.\n            trainer.train_on_batch()\n            # TODO: do whatever you like to the training session.\n        except tf.errors.OutOfRangeError:\n            # The dataset would throw the OutOfRangeError when it reaches the end\n            break\n\n    # TODO: Test the trained model\n'"
examples/distributed_training/tutorial_mnist_distributed_trainer.py,14,"b""#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n\ndef make_dataset(images, labels, num_epochs=1, shuffle_data_seed=0):\n    ds1 = tf.data.Dataset.from_tensor_slices(images)\n    ds2 = tf.data.Dataset.from_tensor_slices(np.array(labels, dtype=np.int64))\n    dataset = tf.data.Dataset.zip((ds1, ds2))\n    dataset = dataset.repeat(num_epochs).shuffle(buffer_size=10000, seed=shuffle_data_seed)\n    return dataset\n\n\ndef model(x, is_train):\n    with tf.variable_scope('mlp', reuse=tf.AUTO_REUSE):\n        network = tl.layers.InputLayer(x, name='input')\n        network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1', is_fix=True, is_train=is_train)\n        network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu1')\n        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2', is_fix=True, is_train=is_train)\n        network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu2')\n        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3', is_fix=True, is_train=is_train)\n        network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')\n    return network\n\n\ndef build_train(x, y_):\n    net = model(x, is_train=True)\n    cost = tl.cost.cross_entropy(net.outputs, y_, name='cost_train')\n    accurate_prediction = tf.equal(tf.argmax(net.outputs, 1), y_)\n    accuracy = tf.reduce_mean(tf.cast(accurate_prediction, tf.float32), name='accuracy_train')\n    log_tensors = {'cost': cost, 'accuracy': accuracy}\n    return net, cost, log_tensors\n\n\ndef build_validation(x, y_):\n    net = model(x, is_train=False)\n    cost = tl.cost.cross_entropy(net.outputs, y_, name='cost_test')\n    accurate_prediction = tf.equal(tf.argmax(net.outputs, 1), y_)\n    accuracy = tf.reduce_mean(tf.cast(accurate_prediction, tf.float32), name='accuracy_test')\n    return net, [cost, accuracy]\n\n\nif __name__ == '__main__':\n    # Load MNIST data\n    X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n\n    # Setup the trainer\n    training_dataset = make_dataset(X_train, y_train)\n    # validation_dataset = make_dataset(X_val, y_val)\n    trainer = tl.distributed.Trainer(\n        build_training_func=build_train, training_dataset=training_dataset, optimizer=tf.train.AdamOptimizer,\n        optimizer_args={'learning_rate': 0.001}, batch_size=500, prefetch_size=500\n        # validation_dataset=validation_dataset, build_validation_func=build_validation\n    )\n\n    # There are multiple ways to use the trainer:\n    # 1. Easiest way to train all data: trainer.train_to_end()\n    # 2. Train with validation in the middle: trainer.train_and_validate_to_end(validate_step_size=100)\n    # 3. Train with full control like follows:\n    while not trainer.session.should_stop():\n        try:\n            # Run a training step synchronously.\n            trainer.train_on_batch()\n            # TODO: do whatever you like to the training session.\n        except tf.errors.OutOfRangeError:\n            # The dataset would throw the OutOfRangeError when it reaches the end\n            break\n\n    # TODO: Test the trained model\n"""
examples/keras_tfslim/tutorial_keras.py,12,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import Input, Lambda\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n\nbatch_size = 128\n\n# keras layers\nlayers = [\n    tf.keras.layers.Dropout(0.8),\n    tf.keras.layers.Dense(800, activation=\'relu\'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(800, activation=\'relu\'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(10, activation=\'linear\')\n]\nkeras_block = tf.keras.Sequential(layers)\n# in order to compile keras model and get trainable_variables of the keras model\n_ = keras_block(np.random.random([batch_size, 784]).astype(np.float32))\n\n# build tl model using keras layers\nni = Input([None, 784], dtype=tf.float32)\nnn = Lambda(fn=keras_block, fn_weights=keras_block.trainable_variables)(ni)\nnetwork = tl.models.Model(inputs=ni, outputs=nn)\nprint(network)\n\nn_epoch = 200\nlearning_rate = 0.0001\n\ntrain_params = network.trainable_weights\noptimizer = tf.optimizers.Adam(learning_rate)\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n    ## Training\n    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n        with tf.GradientTape() as tape:\n            _logits = network(X_train_a, is_train=True)\n            err = tl.cost.cross_entropy(_logits, y_train_a, name=\'train_loss\')\n\n        grad = tape.gradient(err, train_params)\n        optimizer.apply_gradients(zip(grad, train_params))\n        # _, _ = sess.run([cost, train_op], feed_dict={x: X_train_a, y_: y_train_a, K.learning_phase(): 1})\n\n    print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n\n    ## Evaluation\n    train_loss, train_acc, n_batch = 0, 0, 0\n    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=False):\n        _logits = network(X_train_a, is_train=False)\n        err = tl.cost.cross_entropy(_logits, y_train_a, name=\'train_loss\')\n        ac = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(_logits, 1), y_train_a), tf.float32))\n        train_loss += err\n        train_acc += ac\n        n_batch += 1\n    print(""   train loss: %f"" % (train_loss / n_batch))\n    print(""   train acc: %f"" % (train_acc / n_batch))\n    val_loss, val_acc, n_batch = 0, 0, 0\n    for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\n        _logits = network(X_val_a, is_train=False)\n        err = tl.cost.cross_entropy(_logits, y_val_a, name=\'train_loss\')\n        ac = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(_logits, 1), y_val_a), tf.float32))\n        val_loss += err\n        val_acc += ac\n        n_batch += 1\n    print(""   val loss: %f"" % (val_loss / n_batch))\n    print(""   val acc: %f"" % (val_acc / n_batch))\n'"
examples/pretrained_cnn/tutorial_load_ckpt_weights_to_tensorlayer.py,8,"b""#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (Input, Conv2d, Flatten, Dense, MaxPool2d)\nfrom tensorlayer.models import Model\nfrom tensorlayer.files import maybe_download_and_extract\nimport numpy as np\nimport tensorflow as tf\n\nfilename = 'ckpt_parameters.zip'\nurl_score = 'https://media.githubusercontent.com/media/tensorlayer/pretrained-models/master/models/'\n\n# download weights\ndown_file = tl.files.maybe_download_and_extract(\n    filename=filename, working_directory='model/', url_source=url_score, extract=True\n)\n\nmodel_file = 'model/ckpt_parameters'\n\n# ckpt to npz, rename_key used to match TL naming rule\ntl.files.ckpt_to_npz_dict(model_file, rename_key=True)\nweights = np.load('model.npz', allow_pickle=True)\n\n# View the parameters and weights shape\nfor key in weights.keys():\n    print(key, weights[key].shape)\n\n\n# build model\ndef create_model(inputs_shape):\n    W_init = tl.initializers.truncated_normal(stddev=5e-2)\n    W_init2 = tl.initializers.truncated_normal(stddev=0.04)\n    ni = Input(inputs_shape)\n    nn = Conv2d(64, (3, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, name='conv1_1')(ni)\n    nn = MaxPool2d((2, 2), (2, 2), padding='SAME', name='pool1_1')(nn)\n    nn = Conv2d(64, (3, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv1_2')(nn)\n    nn = MaxPool2d((2, 2), (2, 2), padding='SAME', name='pool1_2')(nn)\n\n    nn = Conv2d(128, (3, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv2_1')(nn)\n    nn = MaxPool2d((2, 2), (2, 2), padding='SAME', name='pool2_1')(nn)\n    nn = Conv2d(128, (3, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv2_2')(nn)\n    nn = MaxPool2d((2, 2), (2, 2), padding='SAME', name='pool2_2')(nn)\n\n    nn = Conv2d(256, (3, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv3_1')(nn)\n    nn = MaxPool2d((2, 2), (2, 2), padding='SAME', name='pool3_1')(nn)\n    nn = Conv2d(256, (3, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv3_2')(nn)\n    nn = MaxPool2d((2, 2), (2, 2), padding='SAME', name='pool3_2')(nn)\n\n    nn = Conv2d(512, (3, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv4_1')(nn)\n    nn = MaxPool2d((2, 2), (2, 2), padding='SAME', name='pool4_1')(nn)\n    nn = Conv2d(512, (3, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv4_2')(nn)\n    nn = MaxPool2d((2, 2), (2, 2), padding='SAME', name='pool4_2')(nn)\n\n    nn = Flatten(name='flatten')(nn)\n    nn = Dense(1000, act=None, W_init=W_init2, name='output')(nn)\n\n    M = Model(inputs=ni, outputs=nn, name='cnn')\n    return M\n\n\nnet = create_model([None, 224, 224, 3])\n# loaded weights whose name is not found in network's weights will be skipped.\n# If ckpt has the same naming rule as TL, We can restore the model with tl.files.load_and_assign_ckpt(model_dir=, network=, skip=True)\ntl.files.load_and_assign_npz_dict(network=net, skip=True)\n\n# you can use the following code to view the restore the model parameters.\nnet_weights_name = [w.name for w in net.all_weights]\nfor i in range(len(net_weights_name)):\n    print(net_weights_name[i], net.all_weights[net_weights_name.index(net_weights_name[i])])\n"""
examples/pretrained_cnn/tutorial_models_mobilenetv1.py,3,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nMobileNetV1 for ImageNet using TL models\n\n- mobilenetv2 : https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet\n- tf.slim     : https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models\n""""""\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.models.imagenet_classes import class_names\n\n# tf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nmobilenetv1 = tl.models.MobileNetV1(pretrained=True)\n\nimg1 = tl.vis.read_image(\'data/tiger.jpeg\')\nimg1 = tl.prepro.imresize(img1, (224, 224)) / 255\nimg1 = img1.astype(np.float32)[np.newaxis, ...]\n\nstart_time = time.time()\noutput = mobilenetv1(img1, is_train=False)\nprob = tf.nn.softmax(output)[0].numpy()\nprint(""  End time : %.5ss"" % (time.time() - start_time))\npreds = (np.argsort(prob)[::-1])[0:5]\nfor p in preds:\n    print(class_names[p], prob[p])\n'"
examples/pretrained_cnn/tutorial_models_resnet50.py,2,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nResNet50 for ImageNet using TL models\n\n""""""\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.models.imagenet_classes import class_names\n\n# tf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nresnet = tl.models.ResNet50(pretrained=True)\n\nimg1 = tl.vis.read_image(\'data/tiger.jpeg\')\nimg1 = tl.prepro.imresize(img1, (224, 224))[:, :, ::-1]\nimg1 = img1 - np.array([103.939, 116.779, 123.68]).reshape((1, 1, 3))\n\nimg1 = img1.astype(np.float32)[np.newaxis, ...]\n\nstart_time = time.time()\noutput = resnet(img1, is_train=False)\nprob = tf.nn.softmax(output)[0].numpy()\nprint(""  End time : %.5ss"" % (time.time() - start_time))\npreds = (np.argsort(prob)[::-1])[0:5]\nfor p in preds:\n    print(class_names[p], prob[p])\n'"
examples/pretrained_cnn/tutorial_models_squeezenetv1.py,2,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""SqueezeNet for ImageNet using TL models.""""""\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.models.imagenet_classes import class_names\n\n# tf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nsqueezenet = tl.models.SqueezeNetV1(pretrained=True)\nprint(squeezenet)\n\nimg1 = tl.vis.read_image(\'data/tiger.jpeg\')\nimg1 = tl.prepro.imresize(img1, (224, 224)) / 255\nimg1 = img1.astype(np.float32)[np.newaxis, ...]\n\nstart_time = time.time()\noutput = squeezenet(img1, is_train=False)\nprob = tf.nn.softmax(output)[0].numpy()\nprint(""  End time : %.5ss"" % (time.time() - start_time))\npreds = (np.argsort(prob)[::-1])[0:5]\nfor p in preds:\n    print(class_names[p], prob[p])\n'"
examples/pretrained_cnn/tutorial_models_vgg16.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""VGG-16 for ImageNet using TL models.""""""\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.models.imagenet_classes import class_names\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nvgg = tl.models.vgg16(pretrained=True)\n\nimg = tl.vis.read_image(\'data/tiger.jpeg\')\nimg = tl.prepro.imresize(img, (224, 224)).astype(np.float32) / 255\n\nstart_time = time.time()\noutput = vgg(img, is_train=False)\nprobs = tf.nn.softmax(output)[0].numpy()\nprint(""  End time : %.5ss"" % (time.time() - start_time))\npreds = (np.argsort(probs)[::-1])[0:5]\nfor p in preds:\n    print(class_names[p], probs[p])\n'"
examples/pretrained_cnn/tutorial_models_vgg19.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""VGG-19 for ImageNet using TL models.""""""\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.models.imagenet_classes import class_names\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nvgg = tl.models.vgg19(pretrained=True)\n\nimg = tl.vis.read_image(\'data/tiger.jpeg\')\nimg = tl.prepro.imresize(img, (224, 224)).astype(np.float32) / 255\n\nstart_time = time.time()\noutput = vgg(img, is_train=False)\nprobs = tf.nn.softmax(output)[0].numpy()\nprint(""  End time : %.5ss"" % (time.time() - start_time))\npreds = (np.argsort(probs)[::-1])[0:5]\nfor p in preds:\n    print(class_names[p], probs[p])\n'"
examples/pretrained_cnn/tutorial_models_vgg_static.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""VGG for ImageNet using TL models.""""""\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.models.imagenet_classes import class_names\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nvgg = tl.models.vgg16(pretrained=True, mode=\'static\')\n\nimg = tl.vis.read_image(\'data/tiger.jpeg\')\nimg = tl.prepro.imresize(img, (224, 224)).astype(np.float32) / 255\n\nstart_time = time.time()\noutput = vgg(img, is_train=False)\nprobs = tf.nn.softmax(output)[0].numpy()\nprint(""  End time : %.5ss"" % (time.time() - start_time))\npreds = (np.argsort(probs)[::-1])[0:5]\nfor p in preds:\n    print(class_names[p], probs[p])\n'"
examples/quantized_net/tutorial_binarynet_cifar10_tfrecord.py,17,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\n\n- 1. This model has 1,068,298 paramters and Dorefa compression strategy(weight:1 bit, active: 1 bit),\nafter 500 epoches\' training with GPU,accurcy of 41.1% was found.\n\n- 2. For simplified CNN layers see ""Convolutional layer (Simplified)""\nin read the docs website.\n\n- 3. Data augmentation without TFRecord see `tutorial_image_preprocess.py` !!\n\nLinks\n-------\n.. https://www.tensorflow.org/versions/r0.9/tutorials/deep_cnn/index.html\n.. https://github.com/tensorflow/tensorflow/tree/r0.9/tensorflow/models/image/cifar10\n\nNote\n------\nThe optimizers between official code and this code are different.\n\nDescription\n-----------\nThe images are processed as follows:\n.. They are cropped to 24 x 24 pixels, centrally for evaluation or randomly for training.\n.. They are approximately whitened to make the model insensitive to dynamic range.\n\nFor training, we additionally apply a series of random distortions to\nartificially increase the data set size:\n.. Randomly flip the image from left to right.\n.. Randomly distort the image brightness.\n.. Randomly distort the image contrast.\n\nSpeed Up\n--------\nReading images from disk and distorting them can use a non-trivial amount\nof processing time. To prevent these operations from slowing down training,\nwe run them inside 16 separate threads which continuously fill a TensorFlow queue.\n\n""""""\n\nimport multiprocessing\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (\n    BinaryConv2d, BinaryDense, Conv2d, Dense, Flatten, Input, LocalResponseNorm, MaxPool2d, Sign\n)\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# Download data, and convert to TFRecord format, see ```tutorial_tfrecord.py```\n# prepare cifar10 data\nX_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n\n\ndef binary_model(input_shape, n_classes):\n    in_net = Input(shape=input_shape, name=\'input\')\n\n    net = Conv2d(64, (5, 5), (1, 1), act=\'relu\', padding=\'SAME\', name=\'conv1\')(in_net)\n    net = Sign(name=\'sign1\')(net)\n\n    net = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(net)\n    net = LocalResponseNorm(4, 1.0, 0.001 / 9.0, 0.75, name=\'norm1\')(net)\n    net = BinaryConv2d(64, (5, 5), (1, 1), act=\'relu\', padding=\'SAME\', name=\'bconv1\')(net)\n\n    net = LocalResponseNorm(4, 1.0, 0.001 / 9.0, 0.75, name=\'norm2\')(net)\n    net = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(net)\n    net = Flatten(name=\'flatten\')(net)\n    net = Sign(name=\'sign2\')(net)\n    net = BinaryDense(384, act=\'relu\', name=\'d1relu\')(net)\n    net = Sign(name=\'sign3\')(net)\n    net = BinaryDense(192, act=\'relu\', name=\'d2relu\')(net)\n    net = Dense(n_classes, act=None, name=\'output\')(net)\n    net = Model(inputs=in_net, outputs=net, name=\'binarynet\')\n    return net\n\n\n# training settings\nnet = binary_model([None, 24, 24, 3], n_classes=10)\nbatch_size = 128\nn_epoch = 50000\nlearning_rate = 0.0001\nprint_freq = 5\nn_step_epoch = int(len(y_train) / batch_size)\nn_step = n_epoch * n_step_epoch\nshuffle_buffer_size = 128\n\ntrain_weights = net.trainable_weights\noptimizer = tf.optimizers.Adam(learning_rate)\ncost = tl.cost.cross_entropy\n\n\ndef generator_train():\n    inputs = X_train\n    targets = y_train\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef generator_test():\n    inputs = X_test\n    targets = y_test\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef _map_fn_train(img, target):\n    # 1. Randomly crop a [height, width] section of the image.\n    img = tf.image.random_crop(img, [24, 24, 3])\n    # 2. Randomly flip the image horizontally.\n    img = tf.image.random_flip_left_right(img)\n    # 3. Randomly change brightness.\n    img = tf.image.random_brightness(img, max_delta=63)\n    # 4. Randomly change contrast.\n    img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n    # 5. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _map_fn_test(img, target):\n    # 1. Crop the central [height, width] of the image.\n    img = tf.image.resize_with_pad(img, 24, 24)\n    # 2. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    img = tf.reshape(img, (24, 24, 3))\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\ndef accuracy(_logits, y_batch):\n    return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n\n\n# dataset API and augmentation\ntrain_ds = tf.data.Dataset.from_generator(\n    generator_train, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\ntrain_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n# train_ds = train_ds.repeat(n_epoch)\ntrain_ds = train_ds.shuffle(shuffle_buffer_size)\ntrain_ds = train_ds.prefetch(buffer_size=4096)\ntrain_ds = train_ds.batch(batch_size)\n# value = train_ds.make_one_shot_iterator().get_next()\n\ntest_ds = tf.data.Dataset.from_generator(\n    generator_test, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\n# test_ds = test_ds.shuffle(shuffle_buffer_size)\ntest_ds = test_ds.map(_map_fn_test, num_parallel_calls=multiprocessing.cpu_count())\n# test_ds = test_ds.repeat(n_epoch)\ntest_ds = test_ds.prefetch(buffer_size=4096)\ntest_ds = test_ds.batch(batch_size)\n# value_test = test_ds.make_one_shot_iterator().get_next()\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n\n    train_loss, train_acc, n_iter = 0, 0, 0\n    for X_batch, y_batch in train_ds:\n        net.train()\n        _loss, acc = _train_step(net, X_batch, y_batch, cost=cost, train_op=optimizer, acc=accuracy)\n\n        train_loss += _loss\n        train_acc += acc\n        n_iter += 1\n\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n\n    # use training and evaluation sets to evaluate the model every print_freq epoch\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        net.eval()\n        val_loss, val_acc, n_val_iter = 0, 0, 0\n        for X_batch, y_batch in test_ds:\n            _logits = net(X_batch)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n            n_val_iter += 1\n        print(""   val loss: {}"".format(val_loss / n_val_iter))\n        print(""   val acc:  {}"".format(val_acc / n_val_iter))\n\n# use testing data to evaluate the model\nnet.eval()\ntest_loss, test_acc, n_iter = 0, 0, 0\nfor X_batch, y_batch in test_ds:\n    _logits = net(X_batch)\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n    n_iter += 1\nprint(""   test loss: {}"".format(test_loss / n_iter))\nprint(""   test acc:  {}"".format(test_acc / n_iter))\n'"
examples/quantized_net/tutorial_binarynet_mnist_cnn.py,3,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (BatchNorm, BinaryConv2d, BinaryDense, Flatten, Input, MaxPool2d, Sign)\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n\nbatch_size = 128\n\n\ndef model(inputs_shape, n_class=10):\n    # In BNN, all the layers inputs are binary, with the exception of the first layer.\n    # ref: https://github.com/itayhubara/BinaryNet.tf/blob/master/models/BNN_cifar10.py\n    net_in = Input(inputs_shape, name=\'input\')\n    net = BinaryConv2d(32, (5, 5), (1, 1), padding=\'SAME\', b_init=None, name=\'bcnn1\')(net_in)\n    net = MaxPool2d((2, 2), (2, 2), padding=\'SAME\', name=\'pool1\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn1\')(net)\n\n    net = Sign(""sign1"")(net)\n    net = BinaryConv2d(64, (5, 5), (1, 1), padding=\'SAME\', b_init=None, name=\'bcnn2\')(net)\n    net = MaxPool2d((2, 2), (2, 2), padding=\'SAME\', name=\'pool2\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn2\')(net)\n\n    net = Flatten(\'ft\')(net)\n    net = Sign(""sign2"")(net)\n    net = BinaryDense(256, b_init=None, name=\'dense\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn3\')(net)\n\n    net = Sign(""sign3"")(net)\n    net = BinaryDense(10, b_init=None, name=\'bout\')(net)\n    net = BatchNorm(name=\'bno\')(net)\n    net = Model(inputs=net_in, outputs=net, name=\'binarynet\')\n    return net\n\n\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\ndef accuracy(_logits, y_batch):\n    return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n\n\nn_epoch = 200\nprint_freq = 5\n\nnet = model([None, 28, 28, 1])\ntrain_op = tf.optimizers.Adam(learning_rate=0.0001)\ncost = tl.cost.cross_entropy\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n    train_loss, train_acc, n_batch = 0, 0, 0\n    net.train()\n\n    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n        _loss, acc = _train_step(net, X_train_a, y_train_a, cost=cost, train_op=train_op, acc=accuracy)\n        train_loss += _loss\n        train_acc += acc\n        n_batch += 1\n\n        # print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n        # print(""   train loss: %f"" % (train_loss / n_batch))\n        # print(""   train acc: %f"" % (train_acc / n_batch))\n\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: %f"" % (train_loss / n_batch))\n        print(""   train acc: %f"" % (train_acc / n_batch))\n        val_loss, val_acc, val_batch = 0, 0, 0\n        net.eval()\n        for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n            _logits = net(X_val_a)\n            val_loss += tl.cost.cross_entropy(_logits, y_val_a, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_val_a))\n            val_batch += 1\n        print(""   val loss: {}"".format(val_loss / val_batch))\n        print(""   val acc:  {}"".format(val_acc / val_batch))\n\nnet.test()\ntest_loss, test_acc, n_test_batch = 0, 0, 0\nfor X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n    _logits = net(X_test_a)\n    test_loss += tl.cost.cross_entropy(_logits, y_test_a, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_test_a))\n    n_test_batch += 1\nprint(""   test loss: %f"" % (test_loss / n_test_batch))\nprint(""   test acc: %f"" % (test_acc / n_test_batch))\n'"
examples/quantized_net/tutorial_dorefanet_cifar10_tfrecord.py,18,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\n\n- 1. This model has 1,068,298 paramters and Dorefa compression strategy(weight:1 bit, active: 3 bits),\nafter 500 epoches\' training with GPU,accurcy of 81.1% was found.\n\n- 2. For simplified CNN layers see ""Convolutional layer (Simplified)""\nin read the docs website.\n\n- 3. Data augmentation without TFRecord see `tutorial_image_preprocess.py` !!\n\nLinks\n-------\n.. paper:https://arxiv.org/abs/1606.06160\n.. code:https://github.com/XJTUWYD/DoReFa_Cifar10\n\nNote\n------\nThe optimizers between official code and this code are different.\n\nDescription\n-----------\nThe images are processed as follows:\n.. They are cropped to 24 x 24 pixels, centrally for evaluation or randomly for training.\n.. They are approximately whitened to make the model insensitive to dynamic range.\n\nFor training, we additionally apply a series of random distortions to\nartificially increase the data set size:\n.. Randomly flip the image from left to right.\n.. Randomly distort the image brightness.\n.. Randomly distort the image contrast.\n\nSpeed Up\n--------\nReading images from disk and distorting them can use a non-trivial amount\nof processing time. To prevent these operations from slowing down training,\nwe run them inside 16 separate threads which continuously fill a TensorFlow queue.\n\n""""""\n\nimport multiprocessing\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (Conv2d, Dense, DorefaConv2d, DorefaDense, Flatten, Input, LocalResponseNorm, MaxPool2d)\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# Download data, and convert to TFRecord format, see ```tutorial_tfrecord.py```\n# prepare cifar10 data\nX_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n\n\ndef dorefanet_model(input_shape, n_classes):\n    in_net = Input(shape=input_shape, name=\'input\')\n    net = Conv2d(32, (5, 5), (1, 1), act=\'relu\', padding=\'SAME\', name=\'conv1\')(in_net)\n    net = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(net)\n    net = LocalResponseNorm(4, 1.0, 0.001 / 9.0, 0.75, name=\'norm1\')(net)\n    net = tl.layers.Sign(""sign"")(net)\n    net = DorefaConv2d(8, 32, 64, (5, 5), (1, 1), act=\'relu\', padding=\'SAME\', name=\'DorefaConv1\')(net)\n    net = LocalResponseNorm(4, 1.0, 0.001 / 9.0, 0.75, name=\'norm2\')(net)\n    net = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(net)\n    net = Flatten(name=\'flatten\')(net)\n    net = DorefaDense(8, 16, 384, act=\'relu\', name=\'DorefaDense1\')(net)\n    net = DorefaDense(8, 16, 192, act=\'relu\', name=\'DorefaDense2\')(net)\n    net = Dense(n_classes, act=None, name=\'output\')(net)\n    net = Model(inputs=in_net, outputs=net, name=\'dorefanet\')\n    return net\n\n\n# training settings\nnet = dorefanet_model([None, 24, 24, 3], n_classes=10)\nbatch_size = 128\nn_epoch = 50000\nlearning_rate = 0.0001\nprint_freq = 5\nn_step_epoch = int(len(y_train) / batch_size)\nn_step = n_epoch * n_step_epoch\nshuffle_buffer_size = 128\n\noptimizer = tf.optimizers.Adam(learning_rate)\n# optimizer = tf.optimizers.SGD(learning_rate)\ncost = tl.cost.cross_entropy\n\n\ndef generator_train():\n    inputs = X_train\n    targets = y_train\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef generator_test():\n    inputs = X_test\n    targets = y_test\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef _map_fn_train(img, target):\n    # 1. Randomly crop a [height, width] section of the image.\n    img = tf.image.random_crop(img, [24, 24, 3])\n    # 2. Randomly flip the image horizontally.\n    img = tf.image.random_flip_left_right(img)\n    # 3. Randomly change brightness.\n    img = tf.image.random_brightness(img, max_delta=63)\n    # 4. Randomly change contrast.\n    img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n    # 5. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _map_fn_test(img, target):\n    # 1. Crop the central [height, width] of the image.\n    img = tf.image.resize_with_pad(img, 24, 24)\n    # 2. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    img = tf.reshape(img, (24, 24, 3))\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\ndef accuracy(_logits, y_batch):\n    return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n\n\n# dataset API and augmentation\ntrain_ds = tf.data.Dataset.from_generator(\n    generator_train, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\ntrain_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n# train_ds = train_ds.repeat(n_epoch)\ntrain_ds = train_ds.shuffle(shuffle_buffer_size)\ntrain_ds = train_ds.prefetch(buffer_size=4096)\ntrain_ds = train_ds.batch(batch_size)\n# value = train_ds.make_one_shot_iterator().get_next()\n\ntest_ds = tf.data.Dataset.from_generator(\n    generator_test, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\n# test_ds = test_ds.shuffle(shuffle_buffer_size)\ntest_ds = test_ds.map(_map_fn_test, num_parallel_calls=multiprocessing.cpu_count())\n# test_ds = test_ds.repeat(n_epoch)\ntest_ds = test_ds.prefetch(buffer_size=4096)\ntest_ds = test_ds.batch(batch_size)\n# value_test = test_ds.make_one_shot_iterator().get_next()\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n\n    train_loss, train_acc, n_iter = 0, 0, 0\n    net.train()\n    for X_batch, y_batch in train_ds:\n        _loss, acc = _train_step(net, X_batch, y_batch, cost=cost, train_op=optimizer, acc=accuracy)\n\n        train_loss += _loss\n        train_acc += acc\n        n_iter += 1\n\n    # use training and evaluation sets to evaluate the model every print_freq epoch\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n\n        net.eval()\n        val_loss, val_acc, n_val_iter = 0, 0, 0\n        for X_batch, y_batch in test_ds:\n            _logits = net(X_batch)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n            n_val_iter += 1\n        print(""   val loss: {}"".format(val_loss / n_val_iter))\n        print(""   val acc:  {}"".format(val_acc / n_val_iter))\n\n# use testing data to evaluate the model\nnet.eval()\ntest_loss, test_acc, n_iter = 0, 0, 0\nfor X_batch, y_batch in test_ds:\n    _logits = net(X_batch)\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n    n_iter += 1\nprint(""   test loss: {}"".format(test_loss / n_iter))\nprint(""   test acc:  {}"".format(test_acc / n_iter))\n'"
examples/quantized_net/tutorial_dorefanet_mnist_cnn.py,3,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (BatchNorm, Dense, DorefaConv2d, DorefaDense, Flatten, Input, MaxPool2d)\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n\nbatch_size = 128\n\n\ndef model(inputs_shape, n_class=10):\n    in_net = Input(inputs_shape, name=\'input\')\n    net = DorefaConv2d(1, 3, 32, (5, 5), (1, 1), padding=\'SAME\', b_init=None, name=\'bcnn1\')(in_net)\n    net = MaxPool2d((2, 2), (2, 2), padding=\'SAME\', name=\'pool1\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn1\')(net)\n\n    net = DorefaConv2d(1, 3, 64, (5, 5), (1, 1), padding=\'SAME\', b_init=None, name=\'bcnn2\')(net)\n    net = MaxPool2d((2, 2), (2, 2), padding=\'SAME\', name=\'pool2\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn2\')(net)\n\n    net = Flatten(\'flatten\')(net)\n    net = DorefaDense(1, 3, 256, b_init=None, name=\'dense\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn3\')(net)\n\n    net = Dense(n_class, b_init=None, name=\'bout\')(net)\n    net = BatchNorm(name=\'bno\')(net)\n    net = Model(inputs=in_net, outputs=net, name=\'dorefanet\')\n    return net\n\n\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\ndef accuracy(_logits, y_batch):\n    return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n\n\nn_epoch = 200\nprint_freq = 5\n\nnet = model([None, 28, 28, 1])\ntrain_op = tf.optimizers.Adam(learning_rate=0.0001)\ncost = tl.cost.cross_entropy\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n    train_loss, train_acc, n_batch = 0, 0, 0\n    net.train()\n\n    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n        _loss, acc = _train_step(net, X_train_a, y_train_a, cost=cost, train_op=train_op, acc=accuracy)\n        train_loss += _loss\n        train_acc += acc\n        n_batch += 1\n\n        # print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n        # print(""   train loss: %f"" % (train_loss / n_batch))\n        # print(""   train acc: %f"" % (train_acc / n_batch))\n\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: %f"" % (train_loss / n_batch))\n        print(""   train acc: %f"" % (train_acc / n_batch))\n        val_loss, val_acc, val_batch = 0, 0, 0\n        net.eval()\n        for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n            _logits = net(X_val_a)\n            val_loss += tl.cost.cross_entropy(_logits, y_val_a, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_val_a))\n            val_batch += 1\n        print(""   val loss: {}"".format(val_loss / val_batch))\n        print(""   val acc:  {}"".format(val_acc / val_batch))\n\nnet.test()\ntest_loss, test_acc, n_test_batch = 0, 0, 0\nfor X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n    _logits = net(X_test_a)\n    test_loss += tl.cost.cross_entropy(_logits, y_test_a, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_test_a))\n    n_test_batch += 1\nprint(""   test loss: %f"" % (test_loss / n_test_batch))\nprint(""   test acc: %f"" % (test_acc / n_test_batch))\n'"
examples/quantized_net/tutorial_quanconv_cifar10.py,19,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\n\n- 1. This model has 1,068,298 paramters and quantization compression strategy(weight:8 bits, active: 8 bits here, you can change the setting),\nafter 705 epoches\' training with GPU, test accurcy of 84.0% was found.\n\n- 2. For simplified CNN layers see ""Convolutional layer (Simplified)""\nin read the docs website.\n\n- 3. Data augmentation without TFRecord see `tutorial_image_preprocess.py` !!\n\nLinks\n-------\n.. paper:https://arxiv.org/abs/1712.05877\n\n\nNote\n------\nThe optimizers between official code and this code are different.\n\nDescription\n-----------\nThe images are processed as follows:\n.. They are cropped to 24 x 24 pixels, centrally for evaluation or randomly for training.\n.. They are approximately whitened to make the model insensitive to dynamic range.\n\nFor training, we additionally apply a series of random distortions to\nartificially increase the data set size:\n.. Randomly flip the image from left to right.\n.. Randomly distort the image brightness.\n.. Randomly distort the image contrast.\n\nSpeed Up\n--------\nReading images from disk and distorting them can use a non-trivial amount\nof processing time. To prevent these operations from slowing down training,\nwe run them inside 16 separate threads which continuously fill a TensorFlow queue.\n\n""""""\nimport multiprocessing\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (Dense, Flatten, Input, MaxPool2d, QuanConv2dWithBN, QuanDense)\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# Download data, and convert to TFRecord format, see ```tutorial_tfrecord.py```\n# prepare cifar10 data\nX_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n\n\ndef model(input_shape, n_classes, bitW, bitA):\n    in_net = Input(shape=input_shape, name=\'input\')\n    net = QuanConv2dWithBN(64, (5, 5), (1, 1), act=\'relu\', padding=\'SAME\', bitW=bitW, bitA=bitA, name=\'qcnnbn1\')(in_net)\n    net = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(net)\n    net = QuanConv2dWithBN(64, (5, 5), (1, 1), padding=\'SAME\', act=\'relu\', bitW=bitW, bitA=bitA, name=\'qcnnbn2\')(net)\n    net = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(net)\n    net = Flatten(name=\'flatten\')(net)\n    net = QuanDense(384, act=tf.nn.relu, bitW=bitW, bitA=bitA, name=\'qd1relu\')(net)\n    net = QuanDense(192, act=tf.nn.relu, bitW=bitW, bitA=bitA, name=\'qd2relu\')(net)\n    net = Dense(n_classes, act=None, name=\'output\')(net)\n    net = Model(inputs=in_net, outputs=net, name=\'dorefanet\')\n    return net\n\n\n# training settings\nbitW = 8\nbitA = 8\nnet = model([None, 24, 24, 3], n_classes=10, bitW=bitW, bitA=bitA)\nbatch_size = 128\nn_epoch = 50000\nlearning_rate = 0.0001\nprint_freq = 5\nn_step_epoch = int(len(y_train) / batch_size)\nn_step = n_epoch * n_step_epoch\nshuffle_buffer_size = 128\n\noptimizer = tf.optimizers.Adam(learning_rate)\ncost = tl.cost.cross_entropy\n\n\ndef generator_train():\n    inputs = X_train\n    targets = y_train\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef generator_test():\n    inputs = X_test\n    targets = y_test\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef _map_fn_train(img, target):\n    # 1. Randomly crop a [height, width] section of the image.\n    img = tf.image.random_crop(img, [24, 24, 3])\n    # 2. Randomly flip the image horizontally.\n    img = tf.image.random_flip_left_right(img)\n    # 3. Randomly change brightness.\n    img = tf.image.random_brightness(img, max_delta=63)\n    # 4. Randomly change contrast.\n    img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n    # 5. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _map_fn_test(img, target):\n    # 1. Crop the central [height, width] of the image.\n    img = tf.image.resize_with_pad(img, 24, 24)\n    # 2. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    img = tf.reshape(img, (24, 24, 3))\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\ndef accuracy(_logits, y_batch):\n    return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n\n\n# dataset API and augmentation\ntrain_ds = tf.data.Dataset.from_generator(\n    generator_train, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\ntrain_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n# train_ds = train_ds.repeat(n_epoch)\ntrain_ds = train_ds.shuffle(shuffle_buffer_size)\ntrain_ds = train_ds.prefetch(buffer_size=4096)\ntrain_ds = train_ds.batch(batch_size)\n# value = train_ds.make_one_shot_iterator().get_next()\n\ntest_ds = tf.data.Dataset.from_generator(\n    generator_test, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\n# test_ds = test_ds.shuffle(shuffle_buffer_size)\ntest_ds = test_ds.map(_map_fn_test, num_parallel_calls=multiprocessing.cpu_count())\n# test_ds = test_ds.repeat(n_epoch)\ntest_ds = test_ds.prefetch(buffer_size=4096)\ntest_ds = test_ds.batch(batch_size)\n# value_test = test_ds.make_one_shot_iterator().get_next()\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n\n    train_loss, train_acc, n_iter = 0, 0, 0\n    net.train()\n    for X_batch, y_batch in train_ds:\n        _loss, acc = _train_step(net, X_batch, y_batch, cost=cost, train_op=optimizer, acc=accuracy)\n\n        train_loss += _loss\n        train_acc += acc\n        n_iter += 1\n\n    # use training and evaluation sets to evaluate the model every print_freq epoch\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n\n        net.eval()\n        val_loss, val_acc, n_val_iter = 0, 0, 0\n        for X_batch, y_batch in test_ds:\n            _logits = net(X_batch)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n            n_val_iter += 1\n        print(""   val loss: {}"".format(val_loss / n_val_iter))\n        print(""   val acc:  {}"".format(val_acc / n_val_iter))\n\n# use testing data to evaluate the model\nnet.eval()\ntest_loss, test_acc, n_iter = 0, 0, 0\nfor X_batch, y_batch in test_ds:\n    _logits = net(X_batch)\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n    n_iter += 1\nprint(""   test loss: {}"".format(test_loss / n_iter))\nprint(""   test acc:  {}"".format(test_acc / n_iter))\n'"
examples/quantized_net/tutorial_quanconv_mnist.py,3,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (\n    Dense, Dropout, Flatten, Input, MaxPool2d, QuanConv2d, QuanConv2dWithBN, QuanDense, QuanDenseLayerWithBN\n)\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n# X_train, y_train, X_test, y_test = tl.files.load_cropped_svhn(include_extra=False)\n\nbatch_size = 128\n\n\ndef model(inputs_shape, n_class=10):\n    net_in = Input(inputs_shape, name=""input"")\n\n    net = QuanConv2dWithBN(\n        n_filter=32, filter_size=(5, 5), strides=(1, 1), padding=\'SAME\', act=tl.nn.relu, name=\'qconvbn1\'\n    )(net_in)\n    net = MaxPool2d(filter_size=(2, 2), strides=(2, 2), padding=\'SAME\', name=\'pool1\')(net)\n\n    net = QuanConv2dWithBN(\n        n_filter=64, filter_size=(5, 5), strides=(1, 1), padding=\'SAME\', act=tl.nn.relu, name=\'qconvbn2\'\n    )(net)\n    net = MaxPool2d(filter_size=(2, 2), strides=(2, 2), padding=\'SAME\', name=\'pool2\')(net)\n\n    net = Flatten(name=\'ft\')(net)\n\n    # net = QuanDense(256, act=""relu"", name=\'qdbn\')(net)\n    # net = QuanDense(n_class, name=\'qdbn_out\')(net)\n\n    net = QuanDenseLayerWithBN(256, act=""relu"", name=\'qdbn\')(net)\n    net = QuanDenseLayerWithBN(n_class, name=\'qdbn_out\')(net)\n\n    # net = Dense(256, act=\'relu\', name=\'Dense1\')(net)\n    # net = Dense(n_class, name=\'Dense2\')(net)\n\n    net = Model(inputs=net_in, outputs=net, name=\'quan\')\n    return net\n\n\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\ndef accuracy(_logits, y_batch):\n    return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n\n\nn_epoch = 200\nprint_freq = 1\n\n# print(sess.run(net_test.all_params)) # print real values of parameters\nnet = model([None, 28, 28, 1])\ntrain_op = tf.optimizers.Adam(learning_rate=0.0001)\ncost = tl.cost.cross_entropy\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n    train_loss, train_acc, n_iter = 0, 0, 0\n\n    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n        net.train()\n        _loss, acc = _train_step(net, X_train_a, y_train_a, cost=cost, train_op=train_op, acc=accuracy)\n\n        train_loss += _loss\n        train_acc += acc\n        n_iter += 1\n\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n\n        # net.eval()\n        val_loss, val_acc, n_eval = 0, 0, 0\n        for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n            _logits = net(X_val_a)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_val_a, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_val_a))\n            n_eval += 1\n        print(""   val loss: {}"".format(val_loss / n_eval))\n        print(""   val acc:  {}"".format(val_acc / n_eval))\n\n# net.eval()\ntest_loss, test_acc, n_test_batch = 0, 0, 0\nfor X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n    _logits = net(X_test_a)\n    test_loss += tl.cost.cross_entropy(_logits, y_test_a, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_test_a))\n    n_test_batch += 1\nprint(""   test loss: %f"" % (test_loss / n_test_batch))\nprint(""   test acc: %f"" % (test_acc / n_test_batch))\n'"
examples/quantized_net/tutorial_ternaryweight_cifar10_tfrecord.py,21,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\n\n- 1. This model has 1,068,298 paramters and TWN compression strategy(weight:1,0,-1, output: float32),\nafter 500 epoches\' training with GPU,accurcy of 80.6% was found.\n\n- 2. For simplified CNN layers see ""Convolutional layer (Simplified)""\nin read the docs website.\n\n- 3. Data augmentation without TFRecord see `tutorial_image_preprocess.py` !!\n\nLinks\n-------\n.. https://arxiv.org/abs/1605.04711\n.. https://github.com/XJTUWYD/TWN\n\nNote\n------\nThe optimizers between official code and this code are different.\n\nDescription\n-----------\nThe images are processed as follows:\n.. They are cropped to 24 x 24 pixels, centrally for evaluation or randomly for training.\n.. They are approximately whitened to make the model insensitive to dynamic range.\n\nFor training, we additionally apply a series of random distortions to\nartificially increase the data set size:\n.. Randomly flip the image from left to right.\n.. Randomly distort the image brightness.\n.. Randomly distort the image contrast.\n\nSpeed Up\n--------\nReading images from disk and distorting them can use a non-trivial amount\nof processing time. To prevent these operations from slowing down training,\nwe run them inside 16 separate threads which continuously fill a TensorFlow queue.\n\n""""""\nimport multiprocessing\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (\n    Conv2d, Dense, Flatten, Input, LocalResponseNorm, MaxPool2d, TernaryConv2d, TernaryDense\n)\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# Download data, and convert to TFRecord format, see ```tutorial_tfrecord.py```\n# prepare cifar10 data\nX_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3), plotable=False)\n\n\ndef model(input_shape, n_classes):\n    in_net = Input(shape=input_shape, name=\'input\')\n\n    net = Conv2d(64, (5, 5), (1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'cnn1\')(in_net)\n    net = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(net)\n    net = LocalResponseNorm(4, 1.0, 0.001 / 9.0, 0.75, name=\'norm1\')(net)\n\n    net = TernaryConv2d(64, (5, 5), (1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'cnn2\')(net)\n    net = LocalResponseNorm(4, 1.0, 0.001 / 9.0, 0.75, name=\'norm2\')(net)\n    net = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(net)\n\n    net = Flatten(name=\'flatten\')(net)\n\n    net = TernaryDense(384, act=tf.nn.relu, name=\'d1relu\')(net)\n    net = TernaryDense(192, act=tf.nn.relu, name=\'d2relu\')(net)\n    net = Dense(n_classes, act=None, name=\'output\')(net)\n\n    net = Model(inputs=in_net, outputs=net, name=\'dorefanet\')\n    return net\n\n\n# training settings\nbitW = 8\nbitA = 8\nnet = model([None, 24, 24, 3], n_classes=10)\nbatch_size = 128\nn_epoch = 50000\nlearning_rate = 0.0001\nprint_freq = 5\nn_step_epoch = int(len(y_train) / batch_size)\nn_step = n_epoch * n_step_epoch\nshuffle_buffer_size = 128\n\noptimizer = tf.optimizers.Adam(learning_rate)\ncost = tl.cost.cross_entropy\n\n\ndef generator_train():\n    inputs = X_train\n    targets = y_train\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef generator_test():\n    inputs = X_test\n    targets = y_test\n    if len(inputs) != len(targets):\n        raise AssertionError(""The length of inputs and targets should be equal"")\n    for _input, _target in zip(inputs, targets):\n        # yield _input.encode(\'utf-8\'), _target.encode(\'utf-8\')\n        yield _input, _target\n\n\ndef _map_fn_train(img, target):\n    # 1. Randomly crop a [height, width] section of the image.\n    img = tf.image.random_crop(img, [24, 24, 3])\n    # 2. Randomly flip the image horizontally.\n    img = tf.image.random_flip_left_right(img)\n    # 3. Randomly change brightness.\n    img = tf.image.random_brightness(img, max_delta=63)\n    # 4. Randomly change contrast.\n    img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n    # 5. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _map_fn_test(img, target):\n    # 1. Crop the central [height, width] of the image.\n    img = tf.image.resize_with_pad(img, 24, 24)\n    # 2. Subtract off the mean and divide by the variance of the pixels.\n    img = tf.image.per_image_standardization(img)\n    img = tf.reshape(img, (24, 24, 3))\n    target = tf.reshape(target, ())\n    return img, target\n\n\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\ndef accuracy(_logits, y_batch):\n    return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n\n\n# dataset API and augmentation\ntrain_ds = tf.data.Dataset.from_generator(\n    generator_train, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\ntrain_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n# train_ds = train_ds.repeat(n_epoch)\ntrain_ds = train_ds.shuffle(shuffle_buffer_size)\ntrain_ds = train_ds.prefetch(buffer_size=4096)\ntrain_ds = train_ds.batch(batch_size)\n# value = train_ds.make_one_shot_iterator().get_next()\n\ntest_ds = tf.data.Dataset.from_generator(\n    generator_test, output_types=(tf.float32, tf.int32)\n)  # , output_shapes=((24, 24, 3), (1)))\n# test_ds = test_ds.shuffle(shuffle_buffer_size)\ntest_ds = test_ds.map(_map_fn_test, num_parallel_calls=multiprocessing.cpu_count())\n# test_ds = test_ds.repeat(n_epoch)\ntest_ds = test_ds.prefetch(buffer_size=4096)\ntest_ds = test_ds.batch(batch_size)\n# value_test = test_ds.make_one_shot_iterator().get_next()\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n\n    train_loss, train_acc, n_iter = 0, 0, 0\n    net.train()\n    for X_batch, y_batch in train_ds:\n        _loss, acc = _train_step(net, X_batch, y_batch, cost=cost, train_op=optimizer, acc=accuracy)\n\n        train_loss += _loss\n        train_acc += acc\n        n_iter += 1\n\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n\n    # use training and evaluation sets to evaluate the model every print_freq epoch\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: {}"".format(train_loss / n_iter))\n        print(""   train acc:  {}"".format(train_acc / n_iter))\n\n        net.eval()\n        val_loss, val_acc, n_val_iter = 0, 0, 0\n        for X_batch, y_batch in test_ds:\n            _logits = net(X_batch)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n            n_val_iter += 1\n        print(""   val loss: {}"".format(val_loss / n_val_iter))\n        print(""   val acc:  {}"".format(val_acc / n_val_iter))\n\n# use testing data to evaluate the model\nnet.eval()\ntest_loss, test_acc, n_iter = 0, 0, 0\nfor X_batch, y_batch in test_ds:\n    _logits = net(X_batch)\n    test_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n    n_iter += 1\nprint(""   test loss: {}"".format(test_loss / n_iter))\nprint(""   test acc:  {}"".format(test_acc / n_iter))\n'"
examples/quantized_net/tutorial_ternaryweight_mnist_cnn.py,3,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import (BatchNorm, Dense, Flatten, Input, MaxPool2d, TernaryConv2d, TernaryDense)\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\nX_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n\nbatch_size = 128\n\n\ndef model(inputs_shape, n_class=10):\n    in_net = Input(inputs_shape, name=\'input\')\n    net = TernaryConv2d(32, (5, 5), (1, 1), padding=\'SAME\', b_init=None, name=\'bcnn1\')(in_net)\n    net = MaxPool2d((2, 2), (2, 2), padding=\'SAME\', name=\'pool1\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn1\')(net)\n\n    net = TernaryConv2d(64, (5, 5), (1, 1), padding=\'SAME\', b_init=None, name=\'bcnn2\')(net)\n    net = MaxPool2d((2, 2), (2, 2), padding=\'SAME\', name=\'pool2\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn2\')(net)\n\n    net = Flatten(\'flatten\')(net)\n    net = Dense(256, b_init=None, name=\'dense\')(net)\n    net = BatchNorm(act=tl.act.htanh, name=\'bn3\')(net)\n\n    net = TernaryDense(n_class, b_init=None, name=\'bout\')(net)\n    net = BatchNorm(name=\'bno\')(net)\n\n    net = Model(inputs=in_net, outputs=net, name=\'dorefanet\')\n    return net\n\n\ndef _train_step(network, X_batch, y_batch, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None):\n    with tf.GradientTape() as tape:\n        y_pred = network(X_batch)\n        _loss = cost(y_pred, y_batch)\n    grad = tape.gradient(_loss, network.trainable_weights)\n    train_op.apply_gradients(zip(grad, network.trainable_weights))\n    if acc is not None:\n        _acc = acc(y_pred, y_batch)\n        return _loss, _acc\n    else:\n        return _loss, None\n\n\ndef accuracy(_logits, y_batch):\n    return np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n\n\nn_epoch = 200\nprint_freq = 5\n\nnet = model([None, 28, 28, 1])\ntrain_op = tf.optimizers.Adam(learning_rate=0.0001)\ncost = tl.cost.cross_entropy\n\nfor epoch in range(n_epoch):\n    start_time = time.time()\n    train_loss, train_acc, n_batch = 0, 0, 0\n    net.train()\n\n    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n        _loss, acc = _train_step(net, X_train_a, y_train_a, cost=cost, train_op=train_op, acc=accuracy)\n        train_loss += _loss\n        train_acc += acc\n        n_batch += 1\n\n        print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: %f"" % (train_loss / n_batch))\n        print(""   train acc: %f"" % (train_acc / n_batch))\n\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n        print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n        print(""   train loss: %f"" % (train_loss / n_batch))\n        print(""   train acc: %f"" % (train_acc / n_batch))\n        val_loss, val_acc, val_batch = 0, 0, 0\n        net.eval()\n        for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n            _logits = net(X_val_a)\n            val_loss += tl.cost.cross_entropy(_logits, y_val_a, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_val_a))\n            val_batch += 1\n        print(""   val loss: {}"".format(val_loss / val_batch))\n        print(""   val acc:  {}"".format(val_acc / val_batch))\n\nnet.test()\ntest_loss, test_acc, n_test_batch = 0, 0, 0\nfor X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n    _logits = net(X_test_a)\n    test_loss += tl.cost.cross_entropy(_logits, y_test_a, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_test_a))\n    n_test_batch += 1\nprint(""   test loss: %f"" % (test_loss / n_test_batch))\nprint(""   test acc: %f"" % (test_acc / n_test_batch))\n'"
examples/reinforcement_learning/tutorial_A3C.py,28,"b'""""""\nAsynchronous Advantage Actor Critic (A3C) with Continuous Action Space.\n\nActor Critic History\n----------------------\nA3C > DDPG (for continuous action space) > AC\n\nAdvantage\n----------\nTrain faster and more stable than AC.\n\nDisadvantage\n-------------\nHave bias.\n\nReference\n----------\nOriginal Paper: https://arxiv.org/pdf/1602.01783.pdf\nMorvanZhou\'s tutorial: https://morvanzhou.github.io/tutorials/\nMorvanZhou\'s code: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/experiments/Solve_BipedalWalker/A3C.py\n\nEnvironment\n-----------\nBipedalWalker-v2 : https://gym.openai.com/envs/BipedalWalker-v2\n\nReward is given for moving forward, total 300+ points up to the far end.\nIf the robot falls, it gets -100. Applying motor torque costs a small amount of\npoints, more optimal agent will get better score. State consists of hull angle\nspeed, angular velocity, horizontal speed, vertical speed, position of joints\nand joints angular speed, legs contact with ground, and 10 lidar rangefinder\nmeasurements. There\'s no coordinates in the state vector.\n\nPrerequisites\n--------------\ntensorflow 2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer 2.0.0\n&&\npip install box2d box2d-kengz --user\n\nTo run\n------\npython tutorial_A3C.py --train/test\n\n""""""\n\nimport argparse\nimport multiprocessing\nimport os\nimport threading\nimport time\n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorflow_probability as tfp\nimport tensorlayer as tl\n\ntfd = tfp.distributions\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# add arguments in command  --train/test\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\nargs = parser.parse_args()\n\n#####################  hyper parameters  ####################\n\nENV_ID = \'BipedalWalker-v2\'  # BipedalWalkerHardcore-v2   BipedalWalker-v2  LunarLanderContinuous-v2\nRANDOM_SEED = 2  # random seed, can be either an int number or None\nRENDER = False  # render while training\n\nALG_NAME = \'A3C\'\nN_WORKERS = multiprocessing.cpu_count()  # number of workers according to number of cores in cpu\n# N_WORKERS = 2     # manually set number of workers\nMAX_GLOBAL_EP = 15000  # number of training episodes\nTEST_EPISODES = 10  # number of training episodes\nGLOBAL_NET_SCOPE = \'Global_Net\'\nUPDATE_GLOBAL_ITER = 10  # update global policy after several episodes\nGAMMA = 0.99  # reward discount factor\nENTROPY_BETA = 0.005  # factor for entropy boosted exploration\nLR_A = 0.00005  # learning rate for actor\nLR_C = 0.0001  # learning rate for critic\nGLOBAL_RUNNING_R = []\nGLOBAL_EP = 0  # will increase during training, stop training when it >= MAX_GLOBAL_EP\n\n###################  Asynchronous Advantage Actor Critic (A3C)  ####################################\n\n\nclass ACNet(object):\n\n    def __init__(self, scope):\n        self.scope = scope\n\n        w_init = tf.keras.initializers.glorot_normal(seed=None)  # initializer, glorot=xavier\n\n        def get_actor(input_shape):  # policy network\n            with tf.name_scope(self.scope):\n                ni = tl.layers.Input(input_shape, name=\'in\')\n                nn = tl.layers.Dense(n_units=500, act=tf.nn.relu6, W_init=w_init, name=\'la\')(ni)\n                nn = tl.layers.Dense(n_units=300, act=tf.nn.relu6, W_init=w_init, name=\'la2\')(nn)\n                mu = tl.layers.Dense(n_units=N_A, act=tf.nn.tanh, W_init=w_init, name=\'mu\')(nn)\n                sigma = tl.layers.Dense(n_units=N_A, act=tf.nn.softplus, W_init=w_init, name=\'sigma\')(nn)\n            return tl.models.Model(inputs=ni, outputs=[mu, sigma], name=scope + \'/Actor\')\n\n        self.actor = get_actor([None, N_S])\n        self.actor.train()  # train mode for Dropout, BatchNorm\n\n        def get_critic(input_shape):  # we use Value-function here, but not Q-function.\n            with tf.name_scope(self.scope):\n                ni = tl.layers.Input(input_shape, name=\'in\')\n                nn = tl.layers.Dense(n_units=500, act=tf.nn.relu6, W_init=w_init, name=\'lc\')(ni)\n                nn = tl.layers.Dense(n_units=300, act=tf.nn.relu6, W_init=w_init, name=\'lc2\')(nn)\n                v = tl.layers.Dense(n_units=1, W_init=w_init, name=\'v\')(nn)\n            return tl.models.Model(inputs=ni, outputs=v, name=scope + \'/Critic\')\n\n        self.critic = get_critic([None, N_S])\n        self.critic.train()  # train mode for Dropout, BatchNorm\n\n    @tf.function  # convert numpy functions to tf.Operations in the TFgraph, return tensor\n    def update_global(\n            self, buffer_s, buffer_a, buffer_v_target, globalAC\n    ):  # refer to the global Actor-Crtic network for updating it with samples\n        \'\'\' update the global critic \'\'\'\n        with tf.GradientTape() as tape:\n            self.v = self.critic(buffer_s)\n            self.v_target = buffer_v_target\n            td = tf.subtract(self.v_target, self.v, name=\'TD_error\')\n            self.c_loss = tf.reduce_mean(tf.square(td))\n        self.c_grads = tape.gradient(self.c_loss, self.critic.trainable_weights)\n        OPT_C.apply_gradients(zip(self.c_grads, globalAC.critic.trainable_weights))  # local grads applies to global net\n        # del tape # Drop the reference to the tape\n        \'\'\' update the global actor \'\'\'\n        with tf.GradientTape() as tape:\n            self.mu, self.sigma = self.actor(buffer_s)\n            self.test = self.sigma[0]\n            self.mu, self.sigma = self.mu * A_BOUND[1], self.sigma + 1e-5\n\n            normal_dist = tfd.Normal(self.mu, self.sigma)  # no tf.contrib for tf2.0\n            self.a_his = buffer_a  # float32\n            log_prob = normal_dist.log_prob(self.a_his)\n            exp_v = log_prob * td  # td is from the critic part, no gradients for it\n            entropy = normal_dist.entropy()  # encourage exploration\n            self.exp_v = ENTROPY_BETA * entropy + exp_v\n            self.a_loss = tf.reduce_mean(-self.exp_v)\n        self.a_grads = tape.gradient(self.a_loss, self.actor.trainable_weights)\n        OPT_A.apply_gradients(zip(self.a_grads, globalAC.actor.trainable_weights))  # local grads applies to global net\n        return self.test  # for test purpose\n\n    @tf.function\n    def pull_global(self, globalAC):  # run by a local, pull weights from the global nets\n        for l_p, g_p in zip(self.actor.trainable_weights, globalAC.actor.trainable_weights):\n            l_p.assign(g_p)\n        for l_p, g_p in zip(self.critic.trainable_weights, globalAC.critic.trainable_weights):\n            l_p.assign(g_p)\n\n    def get_action(self, s, greedy=False):  # run by a local\n        s = s[np.newaxis, :]\n        self.mu, self.sigma = self.actor(s)\n\n        with tf.name_scope(\'wrap_a_out\'):\n            self.mu, self.sigma = self.mu * A_BOUND[1], self.sigma + 1e-5\n        if greedy:\n            return self.mu.numpy()[0]\n        normal_dist = tfd.Normal(self.mu, self.sigma)  # for continuous action space\n        self.A = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0), *A_BOUND)\n        return self.A.numpy()[0]\n\n    def save(self):  # save trained weights\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        if not os.path.exists(path):\n            os.makedirs(path)\n        tl.files.save_npz(self.actor.trainable_weights, name=os.path.join(path, \'model_actor.npz\'))\n        tl.files.save_npz(self.critic.trainable_weights, name=os.path.join(path, \'model_critic.npz\'))\n\n    def load(self):  # load trained weights\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        tl.files.load_and_assign_npz(name=os.path.join(path, \'model_actor.npz\'), network=self.actor)\n        tl.files.load_and_assign_npz(name=os.path.join(path, \'model_critic.npz\'), network=self.critic)\n\n\nclass Worker(object):\n\n    def __init__(self, name):\n        self.env = gym.make(ENV_ID)\n        self.name = name\n        self.AC = ACNet(name)\n\n    # def work(self):\n    def work(self, globalAC):\n        global GLOBAL_RUNNING_R, GLOBAL_EP\n        total_step = 1\n        buffer_s, buffer_a, buffer_r = [], [], []\n        while not COORD.should_stop() and GLOBAL_EP < MAX_GLOBAL_EP:\n            s = self.env.reset()\n            ep_r = 0\n            while True:\n                # visualize Worker_0 during training\n                if RENDER and self.name == \'Worker_0\' and total_step % 30 == 0:\n                    self.env.render()\n                s = s.astype(\'float32\')  # double to float\n                a = self.AC.get_action(s)\n                s_, r, done, _info = self.env.step(a)\n\n                s_ = s_.astype(\'float32\')  # double to float\n                # set robot falls reward to -2 instead of -100\n                if r == -100: r = -2\n\n                ep_r += r\n                buffer_s.append(s)\n                buffer_a.append(a)\n                buffer_r.append(r)\n\n                if total_step % UPDATE_GLOBAL_ITER == 0 or done:  # update global and assign to local net\n\n                    if done:\n                        v_s_ = 0  # terminal\n                    else:\n                        v_s_ = self.AC.critic(s_[np.newaxis, :])[0, 0]  # reduce dim from 2 to 0\n\n                    buffer_v_target = []\n\n                    for r in buffer_r[::-1]:  # reverse buffer r\n                        v_s_ = r + GAMMA * v_s_\n                        buffer_v_target.append(v_s_)\n\n                    buffer_v_target.reverse()\n\n                    buffer_s = tf.convert_to_tensor(np.vstack(buffer_s))\n                    buffer_a = tf.convert_to_tensor(np.vstack(buffer_a))\n                    buffer_v_target = tf.convert_to_tensor(np.vstack(buffer_v_target).astype(\'float32\'))\n\n                    # update gradients on global network\n                    self.AC.update_global(buffer_s, buffer_a, buffer_v_target, globalAC)\n                    buffer_s, buffer_a, buffer_r = [], [], []\n\n                    # update local network from global network\n                    self.AC.pull_global(globalAC)\n\n                s = s_\n                total_step += 1\n                if done:\n                    if len(GLOBAL_RUNNING_R) == 0:  # record running episode reward\n                        GLOBAL_RUNNING_R.append(ep_r)\n                    else:  # moving average\n                        GLOBAL_RUNNING_R.append(0.95 * GLOBAL_RUNNING_R[-1] + 0.05 * ep_r)\n                    print(\'Training  | {}, Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\' \\\n                          .format(self.name, GLOBAL_EP, MAX_GLOBAL_EP, ep_r, time.time() - T0))\n                    GLOBAL_EP += 1\n                    break\n\n\nif __name__ == ""__main__"":\n\n    env = gym.make(ENV_ID)\n    # reproducible\n    np.random.seed(RANDOM_SEED)\n    tf.random.set_seed(RANDOM_SEED)\n\n    N_S = env.observation_space.shape[0]\n    N_A = env.action_space.shape[0]\n\n    A_BOUND = [env.action_space.low, env.action_space.high]\n    A_BOUND[0] = A_BOUND[0].reshape(1, N_A)\n    A_BOUND[1] = A_BOUND[1].reshape(1, N_A)\n\n    with tf.device(""/cpu:0""):\n        GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)  # we only need its params\n\n    T0 = time.time()\n    if args.train:\n        # ============================= TRAINING ===============================\n        with tf.device(""/cpu:0""):\n            OPT_A = tf.optimizers.RMSprop(LR_A, name=\'RMSPropA\')\n            OPT_C = tf.optimizers.RMSprop(LR_C, name=\'RMSPropC\')\n            workers = []\n            # Create worker\n            for i in range(N_WORKERS):\n                i_name = \'Worker_%i\' % i  # worker name\n                workers.append(Worker(i_name))\n\n        COORD = tf.train.Coordinator()\n\n        # start TF threading\n        worker_threads = []\n        for worker in workers:\n            job = lambda: worker.work(GLOBAL_AC)\n            t = threading.Thread(target=job)\n            t.start()\n            worker_threads.append(t)\n        COORD.join(worker_threads)\n\n        GLOBAL_AC.save()\n\n        plt.plot(GLOBAL_RUNNING_R)\n        if not os.path.exists(\'image\'):\n            os.makedirs(\'image\')\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\n\n    if args.test:\n        # ============================= EVALUATION =============================\n        GLOBAL_AC.load()\n        for episode in range(TEST_EPISODES):\n            s = env.reset()\n            episode_reward = 0\n            while True:\n                env.render()\n                s = s.astype(\'float32\')  # double to float\n                a = GLOBAL_AC.get_action(s, greedy=True)\n                s, r, d, _ = env.step(a)\n                episode_reward += r\n                if d:\n                    break\n            print(\n                \'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    episode + 1, TEST_EPISODES, episode_reward,\n                    time.time() - T0\n                )\n            )\n'"
examples/reinforcement_learning/tutorial_AC.py,9,"b'""""""\nActor-Critic \n-------------\nIt uses TD-error as the Advantage.\n\nActor Critic History\n----------------------\nA3C > DDPG > AC\n\nAdvantage\n----------\nAC converge faster than Policy Gradient.\n\nDisadvantage (IMPORTANT)\n------------------------\nThe Policy is oscillated (difficult to converge), DDPG can solve\nthis problem using advantage of DQN.\n\nReference\n----------\npaper: https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf\nView more on MorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials/\n\nEnvironment\n------------\nCartPole-v0: https://gym.openai.com/envs/CartPole-v0\n\nA pole is attached by an un-actuated joint to a cart, which moves along a\nfrictionless track. The system is controlled by applying a force of +1 or -1\nto the cart. The pendulum starts upright, and the goal is to prevent it from\nfalling over.\n\nA reward of +1 is provided for every timestep that the pole remains upright.\nThe episode ends when the pole is more than 15 degrees from vertical, or the\ncart moves more than 2.4 units from the center.\n\n\nPrerequisites\n--------------\ntensorflow >=2.0.0a0\ntensorlayer >=2.0.0\n\nTo run\n------\npython tutorial_AC.py --train/test\n\n""""""\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\nimport os\n\nimport gym\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# add arguments in command  --train/test\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\nargs = parser.parse_args()\n\n#####################  hyper parameters  ####################\n\nENV_ID = \'CartPole-v1\'  # environment id\nRANDOM_SEED = 2  # random seed, can be either an int number or None\nRENDER = False  # render while training\n\nALG_NAME = \'AC\'\nTRAIN_EPISODES = 200  # number of overall episodes for training\nTEST_EPISODES = 10  # number of overall episodes for testing\nMAX_STEPS = 500  # maximum time step in one episode\nLAM = 0.9  # reward discount in TD error\nLR_A = 0.001  # learning rate for actor\nLR_C = 0.01  # learning rate for critic\n\n\n\n###############################  Actor-Critic  ####################################\n\n\nclass Actor(object):\n\n    def __init__(self, state_dim, action_num, lr=0.001):\n\n        input_layer = tl.layers.Input([None, state_dim], name=\'state\')\n        layer = tl.layers.Dense(\n            n_units=30, act=tf.nn.relu6, W_init=tf.random_uniform_initializer(0, 0.01), name=\'hidden\'\n        )(input_layer)\n        layer = tl.layers.Dense(n_units=action_num, name=\'actions\')(layer)\n        self.model = tl.models.Model(inputs=input_layer, outputs=layer, name=""Actor"")\n\n        self.model.train()\n        self.optimizer = tf.optimizers.Adam(lr)\n\n    def learn(self, state, action, td_error):\n        with tf.GradientTape() as tape:\n            _logits = self.model(np.array([state]))\n            ## cross-entropy loss weighted by td-error (advantage),\n            # the cross-entropy mearsures the difference of two probability distributions: the predicted logits and sampled action distribution,\n            # then weighted by the td-error: small difference of real and predict actions for large td-error (advantage); and vice versa.\n            _exp_v = tl.rein.cross_entropy_reward_loss(logits=_logits, actions=[action], rewards=td_error[0])\n        grad = tape.gradient(_exp_v, self.model.trainable_weights)\n        self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n        return _exp_v\n\n    def get_action(self, state, greedy=False):\n        _logits = self.model(np.array([state]))\n        _probs = tf.nn.softmax(_logits).numpy()\n        if greedy:\n            return np.argmax(_probs.ravel())\n        return tl.rein.choice_action_by_probs(_probs.ravel())  # sample according to probability distribution\n\n    def save(self):  # save trained weights\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        if not os.path.exists(path):\n            os.makedirs(path)\n        tl.files.save_npz(self.model.trainable_weights, name=os.path.join(path, \'model_actor.npz\'))\n\n    def load(self):  # load trained weights\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        tl.files.load_and_assign_npz(name=os.path.join(path, \'model_actor.npz\'), network=self.model)\n\n\nclass Critic(object):\n\n    def __init__(self, state_dim, lr=0.01):\n        input_layer = tl.layers.Input([1, state_dim], name=\'state\')\n        layer = tl.layers.Dense(\n            n_units=30, act=tf.nn.relu6, W_init=tf.random_uniform_initializer(0, 0.01), name=\'hidden\'\n        )(input_layer)\n        layer = tl.layers.Dense(n_units=1, act=None, name=\'value\')(layer)\n        self.model = tl.models.Model(inputs=input_layer, outputs=layer, name=""Critic"")\n        self.model.train()\n\n        self.optimizer = tf.optimizers.Adam(lr)\n\n    def learn(self, state, reward, state_, done):\n        d = 0 if done else 1\n        v_ = self.model(np.array([state_]))\n        with tf.GradientTape() as tape:\n            v = self.model(np.array([state]))\n            ## TD_error = r + d * lambda * V(newS) - V(S)\n            td_error = reward + d * LAM * v_ - v\n            loss = tf.square(td_error)\n        grad = tape.gradient(loss, self.model.trainable_weights)\n        self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n        return td_error\n\n    def save(self):  # save trained weights\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        if not os.path.exists(path):\n            os.makedirs(path)\n        tl.files.save_npz(self.model.trainable_weights, name=os.path.join(path, \'model_critic.npz\'))\n\n    def load(self):  # load trained weights\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        tl.files.load_and_assign_npz(name=os.path.join(path, \'model_critic.npz\'), network=self.model)\n\n\nif __name__ == \'__main__\':\n    \'\'\' \n    choose environment\n    1. Openai gym:\n    env = gym.make()\n    2. DeepMind Control Suite:\n    env = dm_control2gym.make()\n    \'\'\'\n    env = gym.make(ENV_ID).unwrapped\n    # dm_control2gym.create_render_mode(\'example mode\', show=True, return_pixel=False, height=240, width=320, camera_id=-1, overlays=(),\n    #              depth=False, scene_option=None)\n    # env = dm_control2gym.make(domain_name=""cartpole"", task_name=""balance"")\n\n    env.seed(RANDOM_SEED)  # reproducible\n    np.random.seed(RANDOM_SEED)\n    tf.random.set_seed(RANDOM_SEED)  # reproducible\n\n    N_F = env.observation_space.shape[0]\n    N_A = env.action_space.n\n\n    print(""observation dimension: %d"" % N_F)  # 4\n    print(""observation high: %s"" % env.observation_space.high)  # [ 2.4 , inf , 0.41887902 , inf]\n    print(""observation low : %s"" % env.observation_space.low)  # [-2.4 , -inf , -0.41887902 , -inf]\n    print(""num of actions: %d"" % N_A)  # 2 : left or right\n\n    actor = Actor(state_dim=N_F, action_num=N_A, lr=LR_A)\n    # we need a good teacher, so the teacher should learn faster than the actor\n    critic = Critic(state_dim=N_F, lr=LR_C)\n\n    t0 = time.time()\n    if args.train:\n        all_episode_reward = []\n        for episode in range(TRAIN_EPISODES):\n            state = env.reset().astype(np.float32)\n            step = 0  # number of step in this episode\n            episode_reward = 0  # rewards of all steps\n            while True:\n                if RENDER: env.render()\n\n                action = actor.get_action(state)\n\n                state_new, reward, done, info = env.step(action)\n                state_new = state_new.astype(np.float32)\n\n                if done: reward = -20   # reward shaping trick\n                # these may helpful in some tasks\n                # if abs(s_new[0]) >= env.observation_space.high[0]:\n                # #  cart moves more than 2.4 units from the center\n                #     r = -20\n                # reward for the distance between cart to the center\n                # r -= abs(s_new[0])  * .1\n\n                episode_reward += reward\n\n                try:\n                    td_error = critic.learn(\n                        state, reward, state_new, done\n                    )  # learn Value-function : gradient = grad[r + lambda * V(s_new) - V(s)]\n                    actor.learn(state, action, td_error)  # learn Policy : true_gradient = grad[logPi(s, a) * td_error]\n                except KeyboardInterrupt:  # if Ctrl+C at running actor.learn(), then save model, or exit if not at actor.learn()\n                    actor.save()\n                    critic.save()\n\n                state = state_new\n                step += 1\n\n                if done or step >= MAX_STEPS:\n                    break\n\n            if episode == 0:\n                all_episode_reward.append(episode_reward)\n            else:\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\n\n            print(\'Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}\' \\\n                  .format(episode + 1, TRAIN_EPISODES, episode_reward, time.time() - t0))\n\n            # Early Stopping for quick check\n            if step >= MAX_STEPS:\n                print(""Early Stopping"")     # Hao Dong: it is important for this task\n                break\n        actor.save()\n        critic.save()\n\n        plt.plot(all_episode_reward)\n        if not os.path.exists(\'image\'):\n            os.makedirs(\'image\')\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\n\n    if args.test:\n        actor.load()\n        critic.load()\n\n        for episode in range(TEST_EPISODES):\n            episode_time = time.time()\n            state = env.reset().astype(np.float32)\n            t = 0  # number of step in this episode\n            episode_reward = 0\n            while True:\n                env.render()\n                action = actor.get_action(state, greedy=True)\n                state_new, reward, done, info = env.step(action)\n                state_new = state_new.astype(np.float32)\n                if done: reward = -20\n\n                episode_reward += reward\n                state = state_new\n                t += 1\n\n                if done or t >= MAX_STEPS:\n                    print(\'Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}\' \\\n                          .format(episode + 1, TEST_EPISODES, episode_reward, time.time() - t0))\n                    break\n'"
examples/reinforcement_learning/tutorial_C51.py,24,"b'""""""\r\nC51 Algorithm\r\n------------------------\r\nCategorical 51 distributional RL algorithm, 51 means the number of atoms. In\r\nthis algorithm, instead of estimating actual expected value, value distribution\r\nover a series of continuous sub-intervals (atoms) is considered.\r\nReference:\r\n------------------------\r\nBellemare M G, Dabney W, Munos R. A distributional perspective on reinforcement\r\nlearning[C]//Proceedings of the 34th International Conference on Machine\r\nLearning-Volume 70. JMLR. org, 2017: 449-458.\r\nEnvironment:\r\n------------------------\r\nCartpole and Pong in OpenAI Gym\r\nRequirements:\r\n------------------------\r\ntensorflow>=2.0.0a0\r\ntensorlayer>=2.0.0\r\nTo run:\r\n------------------------\r\npython tutorial_C51.py --mode=train\r\npython tutorial_C51.py --mode=test --save_path=c51/8000.npz\r\n""""""\r\nimport argparse\r\nimport os\r\nimport random\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=True)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\nparser.add_argument(\r\n    \'--save_path\', default=None, help=\'folder to save if mode == train else model path,\'\r\n    \'qnet will be saved once target net update\'\r\n)\r\nparser.add_argument(\'--seed\', help=\'random seed\', type=int, default=0)\r\nparser.add_argument(\'--env_id\', default=\'CartPole-v0\', help=\'CartPole-v0 or PongNoFrameskip-v4\')\r\nargs = parser.parse_args()\r\n\r\nrandom.seed(args.seed)\r\nnp.random.seed(args.seed)\r\ntf.random.set_seed(args.seed)  # reproducible\r\nenv_id = args.env_id\r\nenv = gym.make(env_id)\r\nenv.seed(args.seed)\r\nalg_name = \'C51\'\r\n\r\n# ####################  hyper parameters  ####################\r\nif env_id == \'CartPole-v0\':\r\n    qnet_type = \'MLP\'\r\n    number_timesteps = 10000  # total number of time steps to train on\r\n    explore_timesteps = 100\r\n    # epsilon-greedy schedule, final exploit prob is 0.99\r\n    epsilon = lambda i_iter: 1 - 0.99 * min(1, i_iter / explore_timesteps)\r\n    lr = 5e-3  # learning rate\r\n    buffer_size = 1000  # replay buffer size\r\n    target_q_update_freq = 50  # how frequency target q net update\r\n    ob_scale = 1.0  # scale observations\r\n    clipnorm = None\r\nelse:\r\n    # reward will increase obviously after 1e5 time steps\r\n    qnet_type = \'CNN\'\r\n    number_timesteps = int(1e6)  # total number of time steps to train on\r\n    explore_timesteps = 1e5\r\n    # epsilon-greedy schedule, final exploit prob is 0.99\r\n    epsilon = lambda i_iter: 1 - 0.99 * min(1, i_iter / explore_timesteps)\r\n    lr = 1e-4  # learning rate\r\n    buffer_size = 10000  # replay buffer size\r\n    target_q_update_freq = 200  # how frequency target q net update\r\n    ob_scale = 1.0 / 255  # scale observations\r\n    clipnorm = 10\r\n\r\nin_dim = env.observation_space.shape\r\nout_dim = env.action_space.n\r\nreward_gamma = 0.99  # reward discount\r\nbatch_size = 32  # batch size for sampling from replay buffer\r\nwarm_start = buffer_size / 10  # sample times befor learning\r\natom_num = 51\r\nmin_value = -10\r\nmax_value = 10\r\nvrange = np.linspace(min_value, max_value, atom_num)\r\ndeltaz = float(max_value - min_value) / (atom_num - 1)\r\n\r\n\r\n# ##############################  Network  ####################################\r\nclass MLP(tl.models.Model):\r\n\r\n    def __init__(self, name):\r\n        super(MLP, self).__init__(name=name)\r\n        self.h1 = tl.layers.Dense(64, tf.nn.tanh, in_channels=in_dim[0], W_init=tf.initializers.GlorotUniform())\r\n        self.qvalue = tl.layers.Dense(\r\n            out_dim * atom_num, in_channels=64, name=\'q\', W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.reshape = tl.layers.Reshape((-1, out_dim, atom_num))\r\n\r\n    def forward(self, ni):\r\n        qvalues = self.qvalue(self.h1(ni))\r\n        return tf.nn.log_softmax(self.reshape(qvalues), 2)\r\n\r\n\r\nclass CNN(tl.models.Model):\r\n\r\n    def __init__(self, name):\r\n        super(CNN, self).__init__(name=name)\r\n        h, w, in_channels = in_dim\r\n        dense_in_channels = 64 * ((h - 28) // 8) * ((w - 28) // 8)\r\n        self.conv1 = tl.layers.Conv2d(\r\n            32, (8, 8), (4, 4), tf.nn.relu, \'VALID\', in_channels=in_channels, name=\'conv2d_1\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.conv2 = tl.layers.Conv2d(\r\n            64, (4, 4), (2, 2), tf.nn.relu, \'VALID\', in_channels=32, name=\'conv2d_2\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.conv3 = tl.layers.Conv2d(\r\n            64, (3, 3), (1, 1), tf.nn.relu, \'VALID\', in_channels=64, name=\'conv2d_3\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.flatten = tl.layers.Flatten(name=\'flatten\')\r\n        self.preq = tl.layers.Dense(\r\n            256, tf.nn.relu, in_channels=dense_in_channels, name=\'pre_q\', W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.qvalue = tl.layers.Dense(\r\n            out_dim * atom_num, in_channels=256, name=\'q\', W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.reshape = tl.layers.Reshape((-1, out_dim, atom_num))\r\n\r\n    def forward(self, ni):\r\n        feature = self.flatten(self.conv3(self.conv2(self.conv1(ni))))\r\n        qvalues = self.qvalue(self.preq(feature))\r\n        return tf.nn.log_softmax(self.reshape(qvalues), 2)\r\n\r\n\r\n# ##############################  Replay  ####################################\r\nclass ReplayBuffer(object):\r\n\r\n    def __init__(self, size):\r\n        self._storage = []\r\n        self._maxsize = size\r\n        self._next_idx = 0\r\n\r\n    def __len__(self):\r\n        return len(self._storage)\r\n\r\n    def add(self, *args):\r\n        if self._next_idx >= len(self._storage):\r\n            self._storage.append(args)\r\n        else:\r\n            self._storage[self._next_idx] = args\r\n        self._next_idx = (self._next_idx + 1) % self._maxsize\r\n\r\n    def _encode_sample(self, idxes):\r\n        b_o, b_a, b_r, b_o_, b_d = [], [], [], [], []\r\n        for i in idxes:\r\n            o, a, r, o_, d = self._storage[i]\r\n            b_o.append(o)\r\n            b_a.append(a)\r\n            b_r.append(r)\r\n            b_o_.append(o_)\r\n            b_d.append(d)\r\n        return (\r\n            np.stack(b_o).astype(\'float32\') * ob_scale,\r\n            np.stack(b_a).astype(\'int32\'),\r\n            np.stack(b_r).astype(\'float32\'),\r\n            np.stack(b_o_).astype(\'float32\') * ob_scale,\r\n            np.stack(b_d).astype(\'float32\'),\r\n        )\r\n\r\n    def sample(self, batch_size):\r\n        indexes = range(len(self._storage))\r\n        idxes = [random.choice(indexes) for _ in range(batch_size)]\r\n        return self._encode_sample(idxes)\r\n\r\n\r\n# #############################  Functions  ###################################\r\ndef huber_loss(x):\r\n    """"""Loss function for value""""""\r\n    return tf.where(tf.abs(x) < 1, tf.square(x) * 0.5, tf.abs(x) - 0.5)\r\n\r\n\r\ndef sync(net, net_tar):\r\n    """"""Copy q network to target q network""""""\r\n    for var, var_tar in zip(net.trainable_weights, net_tar.trainable_weights):\r\n        var_tar.assign(var)\r\n\r\n\r\n# ###############################  DQN  #####################################\r\nclass DQN(object):\r\n\r\n    def __init__(self):\r\n        model = MLP if qnet_type == \'MLP\' else CNN\r\n        self.qnet = model(\'q\')\r\n        if args.train:\r\n            self.qnet.train()\r\n            self.targetqnet = model(\'targetq\')\r\n            self.targetqnet.infer()\r\n            sync(self.qnet, self.targetqnet)\r\n        else:\r\n            self.qnet.infer()\r\n            self.load(args.save_path)\r\n        self.niter = 0\r\n        if clipnorm is not None:\r\n            self.optimizer = tf.optimizers.Adam(learning_rate=lr, clipnorm=clipnorm)\r\n        else:\r\n            self.optimizer = tf.optimizers.Adam(learning_rate=lr)\r\n\r\n    def get_action(self, obv):\r\n        eps = epsilon(self.niter)\r\n        if args.train and random.random() < eps:\r\n            return int(random.random() * out_dim)\r\n        else:\r\n            obv = np.expand_dims(obv, 0).astype(\'float32\') * ob_scale\r\n            qdist = np.exp(self._qvalues_func(obv).numpy())\r\n            qvalues = (qdist * vrange).sum(-1)\r\n            return qvalues.argmax(1)[0]\r\n\r\n    @tf.function\r\n    def _qvalues_func(self, obv):\r\n        return self.qnet(obv)\r\n\r\n    def train(self, b_o, b_a, b_r, b_o_, b_d):\r\n        # TODO: move q_estimation in tf.function\r\n        b_dist_ = np.exp(self.targetqnet(b_o_).numpy())\r\n        b_a_ = (b_dist_ * vrange).sum(-1).argmax(1)\r\n        b_tzj = np.clip(reward_gamma * (1 - b_d[:, None]) * vrange[None, :] + b_r[:, None], min_value, max_value)\r\n        b_i = (b_tzj - min_value) / deltaz\r\n        b_l = np.floor(b_i).astype(\'int64\')\r\n        b_u = np.ceil(b_i).astype(\'int64\')\r\n        templ = b_dist_[range(batch_size), b_a_, :] * (b_u - b_i)\r\n        tempu = b_dist_[range(batch_size), b_a_, :] * (b_i - b_l)\r\n        b_m = np.zeros((batch_size, atom_num))\r\n        # TODO: aggregate value by index and batch update (scatter_add)\r\n        for j in range(batch_size):\r\n            for k in range(atom_num):\r\n                b_m[j][b_l[j][k]] += templ[j][k]\r\n                b_m[j][b_u[j][k]] += tempu[j][k]\r\n        b_m = tf.convert_to_tensor(b_m, dtype=\'float32\')\r\n        b_index = np.stack([range(batch_size), b_a], 1)\r\n        b_index = tf.convert_to_tensor(b_index, \'int64\')\r\n\r\n        self._train_func(b_o, b_index, b_m)\r\n\r\n        self.niter += 1\r\n        if self.niter % target_q_update_freq == 0:\r\n            sync(self.qnet, self.targetqnet)\r\n            self.save(args.save_path)\r\n\r\n    def save(self, path):\r\n        if path is None:\r\n            path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'q_net.hdf5\'), self.qnet)\r\n\r\n    def load(self, path):\r\n        if path is None:\r\n            path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'q_net.hdf5\'), self.qnet)\r\n\r\n    @tf.function\r\n    def _train_func(self, b_o, b_index, b_m):\r\n        with tf.GradientTape() as tape:\r\n            b_dist_a = tf.gather_nd(self.qnet(b_o), b_index)\r\n            loss = tf.reduce_mean(tf.negative(tf.reduce_sum(b_dist_a * b_m, 1)))\r\n\r\n        grad = tape.gradient(loss, self.qnet.trainable_weights)\r\n        self.optimizer.apply_gradients(zip(grad, self.qnet.trainable_weights))\r\n\r\n\r\n# #############################  Trainer  ###################################\r\nif __name__ == \'__main__\':\r\n    dqn = DQN()\r\n    t0 = time.time()\r\n    if args.train:\r\n        buffer = ReplayBuffer(buffer_size)\r\n        nepisode = 0\r\n        all_episode_reward = []\r\n        for i in range(1, number_timesteps + 1):\r\n            o = env.reset()\r\n            episode_reward = 0\r\n            while True:\r\n                a = dqn.get_action(o)\r\n                # execute action and feed to replay buffer\r\n                # note that `_` tail in var name means next\r\n                o_, r, done, info = env.step(a)\r\n                buffer.add(o, a, r, o_, done)\r\n                episode_reward += r\r\n\r\n                if i >= warm_start:\r\n                    transitions = buffer.sample(batch_size)\r\n                    dqn.train(*transitions)\r\n\r\n                if done:\r\n                    break\r\n                else:\r\n                    o = o_\r\n\r\n            if nepisode == 0:\r\n                all_episode_reward.append(episode_reward)\r\n            else:\r\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\r\n            nepisode += 1\r\n            print(\r\n                \'Training  | Episode: {}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    nepisode, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )  # episode num starts from 1 in print\r\n\r\n        dqn.save(args.save_path)\r\n        plt.plot(all_episode_reward)\r\n        if not os.path.exists(\'image\'):\r\n            os.makedirs(\'image\')\r\n        plt.savefig(os.path.join(\'image\', \'_\'.join([alg_name, env_id])))\r\n\r\n    if args.test:\r\n        nepisode = 0\r\n        for i in range(1, number_timesteps + 1):\r\n            o = env.reset()\r\n            episode_reward = 0\r\n            while True:\r\n                env.render()\r\n                a = dqn.get_action(o)\r\n                o_, r, done, info = env.step(a)\r\n                episode_reward += r\r\n                if done:\r\n                    break\r\n                else:\r\n                    o = o_\r\n            nepisode += 1\r\n            print(\r\n                \'Testing  | Episode: {}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    nepisode, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n'"
examples/reinforcement_learning/tutorial_DDPG.py,15,"b'""""""\r\nDeep Deterministic Policy Gradient (DDPG)\r\n-----------------------------------------\r\nAn algorithm concurrently learns a Q-function and a policy.\r\nIt uses off-policy data and the Bellman equation to learn the Q-function,\r\nand uses the Q-function to learn the policy.\r\n\r\nReference\r\n---------\r\nDeterministic Policy Gradient Algorithms, Silver et al. 2014\r\nContinuous Control With Deep Reinforcement Learning, Lillicrap et al. 2016\r\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials/\r\n\r\nEnvironment\r\n-----------\r\nOpenai Gym Pendulum-v0, continual action space\r\n\r\nPrerequisites\r\n-------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-proactionsbility 0.6.0\r\ntensorlayer >=2.0.0\r\n\r\nTo run\r\n------\r\npython tutorial_DDPG.py --train/test\r\n\r\n""""""\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\n\r\n# add arguments in command  --train/test\r\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\nargs = parser.parse_args()\r\n\r\n#####################  hyper parameters  ####################\r\n\r\nENV_ID = \'Pendulum-v0\'  # environment id\r\nRANDOM_SEED = 2  # random seed, can be either an int number or None\r\nRENDER = False  # render while training\r\n\r\nALG_NAME = \'DDPG\'\r\nTRAIN_EPISODES = 100  # total number of episodes for training\r\nTEST_EPISODES = 10  # total number of episodes for training\r\nMAX_STEPS = 200  # total number of steps for each episode\r\n\r\nLR_A = 0.001  # learning rate for actor\r\nLR_C = 0.002  # learning rate for critic\r\nGAMMA = 0.9  # reward discount\r\nTAU = 0.01  # soft replacement\r\nMEMORY_CAPACITY = 10000  # size of replay buffer\r\nBATCH_SIZE = 32  # update action batch size\r\nVAR = 2  # control exploration\r\n\r\n###############################  DDPG  ####################################\r\n\r\n\r\nclass DDPG(object):\r\n    """"""\r\n    DDPG class\r\n    """"""\r\n\r\n    def __init__(self, action_dim, state_dim, action_range):\r\n        self.memory = np.zeros((MEMORY_CAPACITY, state_dim * 2 + action_dim + 1), dtype=np.float32)\r\n        self.pointer = 0\r\n        self.action_dim, self.state_dim, self.action_range = action_dim, state_dim, action_range\r\n        self.var = VAR\r\n\r\n        W_init = tf.random_normal_initializer(mean=0, stddev=0.3)\r\n        b_init = tf.constant_initializer(0.1)\r\n\r\n        def get_actor(input_state_shape, name=\'\'):\r\n            """"""\r\n            Build actor network\r\n            :param input_state_shape: state\r\n            :param name: name\r\n            :return: act\r\n            """"""\r\n            input_layer = tl.layers.Input(input_state_shape, name=\'A_input\')\r\n            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name=\'A_l1\')(input_layer)\r\n            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name=\'A_l2\')(layer)\r\n            layer = tl.layers.Dense(n_units=action_dim, act=tf.nn.tanh, W_init=W_init, b_init=b_init, name=\'A_a\')(layer)\r\n            layer = tl.layers.Lambda(lambda x: action_range * x)(layer)\r\n            return tl.models.Model(inputs=input_layer, outputs=layer, name=\'Actor\' + name)\r\n\r\n        def get_critic(input_state_shape, input_action_shape, name=\'\'):\r\n            """"""\r\n            Build critic network\r\n            :param input_state_shape: state\r\n            :param input_action_shape: act\r\n            :param name: name\r\n            :return: Q value Q(s,a)\r\n            """"""\r\n            state_input = tl.layers.Input(input_state_shape, name=\'C_s_input\')\r\n            action_input = tl.layers.Input(input_action_shape, name=\'C_a_input\')\r\n            layer = tl.layers.Concat(1)([state_input, action_input])\r\n            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name=\'C_l1\')(layer)\r\n            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name=\'C_l2\')(layer)\r\n            layer = tl.layers.Dense(n_units=1, W_init=W_init, b_init=b_init, name=\'C_out\')(layer)\r\n            return tl.models.Model(inputs=[state_input, action_input], outputs=layer, name=\'Critic\' + name)\r\n\r\n        self.actor = get_actor([None, state_dim])\r\n        self.critic = get_critic([None, state_dim], [None, action_dim])\r\n        self.actor.train()\r\n        self.critic.train()\r\n\r\n        def copy_para(from_model, to_model):\r\n            """"""\r\n            Copy parameters for soft updating\r\n            :param from_model: latest model\r\n            :param to_model: target model\r\n            :return: None\r\n            """"""\r\n            for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):\r\n                j.assign(i)\r\n\r\n        self.actor_target = get_actor([None, state_dim], name=\'_target\')\r\n        copy_para(self.actor, self.actor_target)\r\n        self.actor_target.eval()\r\n\r\n        self.critic_target = get_critic([None, state_dim], [None, action_dim], name=\'_target\')\r\n        copy_para(self.critic, self.critic_target)\r\n        self.critic_target.eval()\r\n\r\n        self.ema = tf.train.ExponentialMovingAverage(decay=1 - TAU)  # soft replacement\r\n\r\n        self.actor_opt = tf.optimizers.Adam(LR_A)\r\n        self.critic_opt = tf.optimizers.Adam(LR_C)\r\n\r\n    def ema_update(self):\r\n        """"""\r\n        Soft updating by exponential smoothing\r\n        :return: None\r\n        """"""\r\n        paras = self.actor.trainable_weights + self.critic.trainable_weights\r\n        self.ema.apply(paras)\r\n        for i, j in zip(self.actor_target.trainable_weights + self.critic_target.trainable_weights, paras):\r\n            i.assign(self.ema.average(j))\r\n\r\n    def get_action(self, s, greedy=False):\r\n        """"""\r\n        Choose action\r\n        :param s: state\r\n        :param greedy: get action greedy or not\r\n        :return: act\r\n        """"""\r\n        a = self.actor(np.array([s], dtype=np.float32))[0]\r\n        if greedy:\r\n            return a\r\n        return np.clip(\r\n            np.random.normal(a, self.var), -self.action_range, self.action_range\r\n        )  # add randomness to action selection for exploration\r\n\r\n    def learn(self):\r\n        """"""\r\n        Update parameters\r\n        :return: None\r\n        """"""\r\n        self.var *= .9995\r\n        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\r\n        datas = self.memory[indices, :]\r\n        states = datas[:, :self.state_dim]\r\n        actions = datas[:, self.state_dim:self.state_dim + self.action_dim]\r\n        rewards = datas[:, -self.state_dim - 1:-self.state_dim]\r\n        states_ = datas[:, -self.state_dim:]\r\n\r\n        with tf.GradientTape() as tape:\r\n            actions_ = self.actor_target(states_)\r\n            q_ = self.critic_target([states_, actions_])\r\n            y = rewards + GAMMA * q_\r\n            q = self.critic([states, actions])\r\n            td_error = tf.losses.mean_squared_error(y, q)\r\n        critic_grads = tape.gradient(td_error, self.critic.trainable_weights)\r\n        self.critic_opt.apply_gradients(zip(critic_grads, self.critic.trainable_weights))\r\n\r\n        with tf.GradientTape() as tape:\r\n            a = self.actor(states)\r\n            q = self.critic([states, a])\r\n            actor_loss = -tf.reduce_mean(q)  # maximize the q\r\n        actor_grads = tape.gradient(actor_loss, self.actor.trainable_weights)\r\n        self.actor_opt.apply_gradients(zip(actor_grads, self.actor.trainable_weights))\r\n        self.ema_update()\r\n\r\n    def store_transition(self, s, a, r, s_):\r\n        """"""\r\n        Store data in data buffer\r\n        :param s: state\r\n        :param a: act\r\n        :param r: reward\r\n        :param s_: next state\r\n        :return: None\r\n        """"""\r\n        s = s.astype(np.float32)\r\n        s_ = s_.astype(np.float32)\r\n        transition = np.hstack((s, a, [r], s_))\r\n        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\r\n        self.memory[index, :] = transition\r\n        self.pointer += 1\r\n\r\n    def save(self):\r\n        """"""\r\n        save trained weights\r\n        :return: None\r\n        """"""\r\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'actor.hdf5\'), self.actor)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'actor_target.hdf5\'), self.actor_target)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'critic.hdf5\'), self.critic)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'critic_target.hdf5\'), self.critic_target)\r\n\r\n    def load(self):\r\n        """"""\r\n        load trained weights\r\n        :return: None\r\n        """"""\r\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'actor.hdf5\'), self.actor)\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'actor_target.hdf5\'), self.actor_target)\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'critic.hdf5\'), self.critic)\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'critic_target.hdf5\'), self.critic_target)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    env = gym.make(ENV_ID).unwrapped\r\n\r\n    # reproducible\r\n    env.seed(RANDOM_SEED)\r\n    np.random.seed(RANDOM_SEED)\r\n    tf.random.set_seed(RANDOM_SEED)\r\n\r\n    state_dim = env.observation_space.shape[0]\r\n    action_dim = env.action_space.shape[0]\r\n    action_range = env.action_space.high  # scale action, [-action_range, action_range]\r\n\r\n    agent = DDPG(action_dim, state_dim, action_range)\r\n\r\n    t0 = time.time()\r\n    if args.train:  # train\r\n        all_episode_reward = []\r\n        for episode in range(TRAIN_EPISODES):\r\n            state = env.reset()\r\n            episode_reward = 0\r\n            for step in range(MAX_STEPS):\r\n                if RENDER:\r\n                    env.render()\r\n                # Add exploration noise\r\n                action = agent.get_action(state)\r\n                state_, reward, done, info = env.step(action)\r\n                agent.store_transition(state, action, reward, state_)\r\n\r\n                if agent.pointer > MEMORY_CAPACITY:\r\n                    agent.learn()\r\n\r\n                state = state_\r\n                episode_reward += reward\r\n                if done:\r\n                    break\r\n\r\n            if episode == 0:\r\n                all_episode_reward.append(episode_reward)\r\n            else:\r\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\r\n            print(\r\n                \'Training  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    episode + 1, TRAIN_EPISODES, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n        agent.save()\r\n        plt.plot(all_episode_reward)\r\n        if not os.path.exists(\'image\'):\r\n            os.makedirs(\'image\')\r\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\r\n\r\n    if args.test:\r\n        # test\r\n        agent.load()\r\n        for episode in range(TEST_EPISODES):\r\n            state = env.reset()\r\n            episode_reward = 0\r\n            for step in range(MAX_STEPS):\r\n                env.render()\r\n                state, reward, done, info = env.step(agent.get_action(state, greedy=True))\r\n                episode_reward += reward\r\n                if done:\r\n                    break\r\n            print(\r\n                \'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    episode + 1, TEST_EPISODES, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n'"
examples/reinforcement_learning/tutorial_DPPO.py,27,"b'""""""\nDistributed Proximal Policy Optimization (DPPO)\n----------------------------\nA distributed version of OpenAI\'s Proximal Policy Optimization (PPO).\nWorkers in parallel to collect data, then stop worker\'s roll-out and train PPO on collected data.\nRestart workers once PPO is updated.\n\nReference\n---------\nEmergence of Locomotion Behaviours in Rich Environments, Heess et al. 2017\nProximal Policy Optimization Algorithms, Schulman et al. 2017\nHigh Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials\n\nEnvironment\n-----------\nOpenai Gym Pendulum-v0, continual action space\n\nPrerequisites\n--------------\ntensorflow >=2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer >=2.0.0\n\nTo run\n------\npython tutorial_DPPO.py --train/test\n""""""\n\nimport argparse\nimport os\nimport queue\nimport threading\nimport time\n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nimport tensorlayer as tl\n\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\nargs = parser.parse_args()\n\n#####################  hyper parameters  ####################\n\nENV_ID = \'Pendulum-v0\'  # environment name\nRANDOMSEED = 2  # random seed\nRENDER = False  # render while training\n\nALG_NAME = \'DPPO\'\nTRAIN_EPISODES = 1000  # total number of episodes for training\nTEST_EPISODES = 10  # number of overall episodes for testing\nMAX_STEPS = 200  # total number of steps for each episode\nGAMMA = 0.9  # reward discount\nLR_A = 0.0001  # learning rate for actor\nLR_C = 0.0002  # learning rate for critic\nACTOR_UPDATE_STEPS = 10  # actor update steps\nCRITIC_UPDATE_STEPS = 10  # critic update steps\nMIN_BATCH_SIZE = 64  # minimum batch size for updating PPO\n\nN_WORKER = 4  # parallel workers\nUPDATE_STEP = 10  # loop update operation n-steps\n\n# ppo-penalty parameters\nKL_TARGET = 0.01\nLAM = 0.5\n\n# ppo-clip parameters\nEPSILON = 0.2\n\n\n###############################  DPPO  ####################################\n\n\nclass PPO(object):\n    """"""\n    PPO class\n    """"""\n\n    def __init__(self, state_dim, action_dim, action_bound, method=\'clip\'):\n\n        # critic\n        with tf.name_scope(\'critic\'):\n            inputs = tl.layers.Input([None, state_dim], tf.float32, \'state\')\n            layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n            layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n            v = tl.layers.Dense(1)(layer)\n        self.critic = tl.models.Model(inputs, v)\n        self.critic.train()\n        self.method = method\n\n        # actor\n        with tf.name_scope(\'actor\'):\n            inputs = tl.layers.Input([None, state_dim], tf.float32, \'state\')\n            layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n            layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n            a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\n            mean = tl.layers.Lambda(lambda x: x * action_bound, name=\'lambda\')(a)\n            logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))\n        self.actor = tl.models.Model(inputs, mean)\n        self.actor.trainable_weights.append(logstd)\n        self.actor.logstd = logstd\n        self.actor.train()\n\n        self.actor_opt = tf.optimizers.Adam(LR_A)\n        self.critic_opt = tf.optimizers.Adam(LR_C)\n\n        self.method = method\n        if method == \'penalty\':\n            self.kl_target = KL_TARGET\n            self.lam = LAM\n        elif method == \'clip\':\n            self.epsilon = EPSILON\n\n        self.state_buffer, self.action_buffer = [], []\n        self.reward_buffer, self.cumulative_reward_buffer = [], []\n        self.action_bound = action_bound\n\n    def train_actor(self, state, action, adv, old_pi):\n        """"""\n        Update policy network\n        :param state: state batch\n        :param action: action batch\n        :param adv: advantage batch\n        :param old_pi: old pi distribution\n        :return: kl_mean or None\n        """"""\n        with tf.GradientTape() as tape:\n            mean, std = self.actor(state), tf.exp(self.actor.logstd)\n            pi = tfp.distributions.Normal(mean, std)\n\n            ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))\n            surr = ratio * adv\n            if self.method == \'penalty\':  # ppo penalty\n                kl = tfp.distributions.kl_divergence(old_pi, pi)\n                kl_mean = tf.reduce_mean(kl)\n                loss = -(tf.reduce_mean(surr - self.lam * kl))\n            else:  # ppo clip\n                loss = -tf.reduce_mean(\n                    tf.minimum(surr,\n                               tf.clip_by_value(ratio, 1. - self.epsilon, 1. + self.epsilon) * adv)\n                )\n        a_gard = tape.gradient(loss, self.actor.trainable_weights)\n        self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\n\n        if self.method == \'kl_pen\':\n            return kl_mean\n\n    def train_critic(self, reward, state):\n        """"""\n        Update actor network\n        :param reward: cumulative reward batch\n        :param state: state batch\n        :return: None\n        """"""\n        reward = np.array(reward, dtype=np.float32)\n        with tf.GradientTape() as tape:\n            advantage = reward - self.critic(state)\n            loss = tf.reduce_mean(tf.square(advantage))\n        grad = tape.gradient(loss, self.critic.trainable_weights)\n        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))\n\n    def update(self):\n        """"""\n        Update parameter with the constraint of KL divergent\n        :return: None\n        """"""\n        global GLOBAL_UPDATE_COUNTER\n        while not COORD.should_stop():\n            if GLOBAL_EP < TRAIN_EPISODES:\n                UPDATE_EVENT.wait()  # wait until get batch of data\n\n                data = [QUEUE.get() for _ in range(QUEUE.qsize())]  # collect data from all workers\n                s, a, r = zip(*data)\n                s = np.vstack(s).astype(np.float32)\n                a = np.vstack(a).astype(np.float32)\n                r = np.vstack(r).astype(np.float32)\n                mean, std = self.actor(s), tf.exp(self.actor.logstd)\n                pi = tfp.distributions.Normal(mean, std)\n                adv = r - self.critic(s)\n                # adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful\n\n                # update actor\n                if self.method == \'kl_pen\':\n                    for _ in range(ACTOR_UPDATE_STEPS):\n                        kl = self.train_actor(s, a, adv, pi)\n                    if kl < self.kl_target / 1.5:\n                        self.lam /= 2\n                    elif kl > self.kl_target * 1.5:\n                        self.lam *= 2\n                else:\n                    for _ in range(ACTOR_UPDATE_STEPS):\n                        self.train_actor(s, a, adv, pi)\n\n                # update critic\n                for _ in range(CRITIC_UPDATE_STEPS):\n                    self.train_critic(r, s)\n\n                UPDATE_EVENT.clear()  # updating finished\n                GLOBAL_UPDATE_COUNTER = 0  # reset counter\n                ROLLING_EVENT.set()  # set roll-out available\n\n    def get_action(self, state, greedy=False):\n        """"""\n        Choose action\n        :param state: state\n        :param greedy: choose action greedy or not\n        :return: clipped action\n        """"""\n        state = state[np.newaxis, :].astype(np.float32)\n        mean, std = self.actor(state), tf.exp(self.actor.logstd)\n        if greedy:\n            action = mean[0]\n        else:\n            pi = tfp.distributions.Normal(mean, std)\n            action = tf.squeeze(pi.sample(1), axis=0)[0]  # choosing action\n        return np.clip(action, -self.action_bound, self.action_bound)\n\n    def save(self):\n        """"""\n        save trained weights\n        :return: None\n        """"""\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        if not os.path.exists(path):\n            os.makedirs(path)\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'actor.hdf5\'), self.actor)\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'critic.hdf5\'), self.critic)\n\n    def load(self):\n        """"""\n        load trained weights\n        :return: None\n        """"""\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'actor.hdf5\'), self.actor)\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'critic.hdf5\'), self.critic)\n\n\n""""""--------------------------------------------------------------""""""\n\n\nclass Worker(object):\n    """"""\n    Worker class for distributional running\n    """"""\n\n    def __init__(self, wid):\n        self.wid = wid\n        self.env = gym.make(ENV_ID).unwrapped\n        self.env.seed(wid * 100 + RANDOMSEED)\n        self.ppo = GLOBAL_PPO\n\n    def work(self):\n        """"""\n        Define a worker\n        :return: None\n        """"""\n        global GLOBAL_EP, GLOBAL_RUNNING_R, GLOBAL_UPDATE_COUNTER\n        while not COORD.should_stop():\n            s = self.env.reset()\n            ep_r = 0\n            buffer_s, buffer_a, buffer_r = [], [], []\n            for t in range(MAX_STEPS):\n                if not ROLLING_EVENT.is_set():  # while global PPO is updating\n                    ROLLING_EVENT.wait()  # wait until PPO is updated\n                    buffer_s, buffer_a, buffer_r = [], [], []  # clear history buffer, use new policy to collect data\n                a = self.ppo.get_action(s)\n                s_, r, done, _ = self.env.step(a)\n                if RENDER and self.wid == 0:\n                    self.env.render()\n                buffer_s.append(s)\n                buffer_a.append(a)\n                buffer_r.append(r)\n                s = s_\n                ep_r += r\n\n                GLOBAL_UPDATE_COUNTER += 1  # count to minimum batch size, no need to wait other workers\n                if t == MAX_STEPS - 1 or GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE:\n                    # finish patyh\n                    if done:\n                        v_s_ = 0\n                    else:\n                        v_s_ = self.ppo.critic(np.array([s_], np.float32))[0][0]\n                    discounted_r = []  # compute discounted reward\n                    for r in buffer_r[::-1]:\n                        v_s_ = r + GAMMA * v_s_\n                        discounted_r.append(v_s_)\n                    discounted_r.reverse()\n                    buffer_r = np.array(discounted_r)[:, np.newaxis]\n                    QUEUE.put([buffer_s, buffer_a, buffer_r])  # put data in the queue\n                    buffer_s, buffer_a, buffer_r = [], [], []\n\n                    # update\n                    if GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE:\n                        ROLLING_EVENT.clear()  # stop collecting data\n                        UPDATE_EVENT.set()  # globalPPO update\n\n                    # stop training\n                    if GLOBAL_EP >= TRAIN_EPISODES:\n                        COORD.request_stop()\n                        break\n\n            print(\n                \'Training  | Episode: {}/{}  | Worker: {} | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    GLOBAL_EP + 1, TRAIN_EPISODES, self.wid, ep_r, time.time() - T0\n                )\n            )\n            # record reward changes, plot later\n            if len(GLOBAL_RUNNING_R) == 0:\n                GLOBAL_RUNNING_R.append(ep_r)\n            else:\n                GLOBAL_RUNNING_R.append(GLOBAL_RUNNING_R[-1] * 0.9 + ep_r * 0.1)\n            GLOBAL_EP += 1\n\n\nif __name__ == \'__main__\':\n\n    # reproducible\n    np.random.seed(RANDOMSEED)\n    tf.random.set_seed(RANDOMSEED)\n\n    env = gym.make(ENV_ID)\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n    action_bound = env.action_space.high\n    env.close()\n\n    GLOBAL_PPO = PPO(state_dim, action_dim, action_bound)\n    T0 = time.time()\n    if args.train:  # train\n        UPDATE_EVENT, ROLLING_EVENT = threading.Event(), threading.Event()\n        UPDATE_EVENT.clear()  # not update now\n        ROLLING_EVENT.set()  # start to roll out\n        workers = [Worker(wid=i) for i in range(N_WORKER)]\n\n        GLOBAL_UPDATE_COUNTER, GLOBAL_EP = 0, 0\n        GLOBAL_RUNNING_R = []\n        COORD = tf.train.Coordinator()\n        QUEUE = queue.Queue()  # workers putting data in this queue\n        threads = []\n        for worker in workers:  # worker threads\n            t = threading.Thread(target=worker.work)\n            t.start()  # training\n            threads.append(t)\n        # add a PPO updating thread\n        threads.append(threading.Thread(target=GLOBAL_PPO.update))\n        threads[-1].start()\n        COORD.join(threads)\n\n        GLOBAL_PPO.save()\n\n        plt.plot(GLOBAL_RUNNING_R)\n        if not os.path.exists(\'image\'):\n            os.makedirs(\'image\')\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\n\n    # test\n    if args.test:\n        GLOBAL_PPO.load()\n        for episode in range(TEST_EPISODES):\n            state = env.reset()\n            episode_reward = 0\n            for step in range(MAX_STEPS):\n                env.render()\n                state, reward, done, info = env.step(GLOBAL_PPO.get_action(state, greedy=True))\n                episode_reward += reward\n                if done:\n                    break\n            print(\n                \'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    episode + 1, TEST_EPISODES, episode_reward,\n                    time.time() - T0))\n'"
examples/reinforcement_learning/tutorial_DQN.py,3,"b'""""""\r\nDeep Q-Network Q(a, s)\r\n-----------------------\r\nTD Learning, Off-Policy, e-Greedy Exploration (GLIE).\r\nQ(S, A) <- Q(S, A) + alpha * (R + lambda * Q(newS, newA) - Q(S, A))\r\ndelta_w = R + lambda * Q(newS, newA)\r\nSee David Silver RL Tutorial Lecture 5 - Q-Learning for more details.\r\nReference\r\n----------\r\noriginal paper: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\r\nEN: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.5m3361vlw\r\nCN: https://zhuanlan.zhihu.com/p/25710327\r\nNote: Policy Network has been proved to be better than Q-Learning, see tutorial_atari_pong.py\r\nEnvironment\r\n-----------\r\n# The FrozenLake v0 environment\r\nhttps://gym.openai.com/envs/FrozenLake-v0\r\nThe agent controls the movement of a character in a grid world. Some tiles of\r\nthe grid are walkable, and others lead to the agent falling into the water.\r\nAdditionally, the movement direction of the agent is uncertain and only partially\r\ndepends on the chosen direction. The agent is rewarded for finding a walkable\r\npath to a goal tile.\r\nSFFF       (S: starting point, safe)\r\nFHFH       (F: frozen surface, safe)\r\nFFFH       (H: hole, fall to your doom)\r\nHFFG       (G: goal, where the frisbee is located)\r\nThe episode ends when you reach the goal or fall in a hole. You receive a reward\r\nof 1 if you reach the goal, and zero otherwise.\r\nPrerequisites\r\n--------------\r\ntensorflow>=2.0.0a0\r\ntensorlayer>=2.0.0\r\nTo run\r\n-------\r\npython tutorial_DQN.py --train/test\r\n""""""\r\nimport argparse\r\nimport os\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\n\r\n# add arguments in command  --train/test\r\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=True)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\nargs = parser.parse_args()\r\n\r\ntl.logging.set_verbosity(tl.logging.DEBUG)\r\n\r\n#####################  hyper parameters  ####################\r\nenv_id = \'FrozenLake-v0\'\r\nalg_name = \'DQN\'\r\nlambd = .99  # decay factor\r\ne = 0.1  # e-Greedy Exploration, the larger the more random\r\nnum_episodes = 10000\r\nrender = False  # display the game environment\r\n\r\n##################### DQN ##########################\r\n\r\n\r\ndef to_one_hot(i, n_classes=None):\r\n    a = np.zeros(n_classes, \'uint8\')\r\n    a[i] = 1\r\n    return a\r\n\r\n\r\n## Define Q-network q(a,s) that ouput the rewards of 4 actions by given state, i.e. Action-Value Function.\r\n# encoding for state: 4x4 grid can be represented by one-hot vector with 16 integers.\r\ndef get_model(inputs_shape):\r\n    ni = tl.layers.Input(inputs_shape, name=\'observation\')\r\n    nn = tl.layers.Dense(4, act=None, W_init=tf.random_uniform_initializer(0, 0.01), b_init=None, name=\'q_a_s\')(ni)\r\n    return tl.models.Model(inputs=ni, outputs=nn, name=""Q-Network"")\r\n\r\n\r\ndef save_ckpt(model):  # save trained weights\r\n    path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n    if not os.path.exists(path):\r\n        os.makedirs(path)\r\n    tl.files.save_weights_to_hdf5(os.path.join(path, \'dqn_model.hdf5\'), model)\r\n\r\n\r\ndef load_ckpt(model):  # load trained weights\r\n    path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n    tl.files.save_weights_to_hdf5(os.path.join(path, \'dqn_model.hdf5\'), model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    qnetwork = get_model([None, 16])\r\n    qnetwork.train()\r\n    train_weights = qnetwork.trainable_weights\r\n\r\n    optimizer = tf.optimizers.SGD(learning_rate=0.1)\r\n    env = gym.make(env_id)\r\n\r\n    t0 = time.time()\r\n    if args.train:\r\n        all_episode_reward = []\r\n        for i in range(num_episodes):\r\n            ## Reset environment and get first new observation\r\n            s = env.reset()  # observation is state, integer 0 ~ 15\r\n            rAll = 0\r\n            if render: env.render()\r\n            for j in range(99):  # step index, maximum step is 99\r\n                ## Choose an action by greedily (with e chance of random action) from the Q-network\r\n                allQ = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32)).numpy()\r\n                a = np.argmax(allQ, 1)\r\n\r\n                ## e-Greedy Exploration !!! sample random action\r\n                if np.random.rand(1) < e:\r\n                    a[0] = env.action_space.sample()\r\n                ## Get new state and reward from environment\r\n                s1, r, d, _ = env.step(a[0])\r\n                if render: env.render()\r\n                ## Obtain the Q\' values by feeding the new state through our network\r\n                Q1 = qnetwork(np.asarray([to_one_hot(s1, 16)], dtype=np.float32)).numpy()\r\n\r\n                ## Obtain maxQ\' and set our target value for chosen action.\r\n                maxQ1 = np.max(Q1)  # in Q-Learning, policy is greedy, so we use ""max"" to select the next action.\r\n                targetQ = allQ\r\n                targetQ[0, a[0]] = r + lambd * maxQ1\r\n                ## Train network using target and predicted Q values\r\n                # it is not real target Q value, it is just an estimation,\r\n                # but check the Q-Learning update formula:\r\n                #    Q\'(s,a) <- Q(s,a) + alpha(r + lambd * maxQ(s\',a\') - Q(s, a))\r\n                # minimizing |r + lambd * maxQ(s\',a\') - Q(s, a)|^2 equals to force Q\'(s,a) \xe2\x89\x88 Q(s,a)\r\n                with tf.GradientTape() as tape:\r\n                    _qvalues = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32))\r\n                    _loss = tl.cost.mean_squared_error(targetQ, _qvalues, is_mean=False)\r\n                grad = tape.gradient(_loss, train_weights)\r\n                optimizer.apply_gradients(zip(grad, train_weights))\r\n\r\n                rAll += r\r\n                s = s1\r\n                ## Reduce chance of random action if an episode is done.\r\n                if d ==True:\r\n                    e = 1. / ((i / 50) + 10)  # reduce e, GLIE: Greey in the limit with infinite Exploration\r\n                    break\r\n\r\n            ## Note that, the rewards here with random action\r\n            print(\'Training  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}\' \\\r\n                  .format(i, num_episodes, rAll, time.time() - t0))\r\n\r\n            if i == 0:\r\n                all_episode_reward.append(rAll)\r\n            else:\r\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + rAll * 0.1)\r\n\r\n        save_ckpt(qnetwork)  # save model\r\n        plt.plot(all_episode_reward)\r\n        if not os.path.exists(\'image\'):\r\n            os.makedirs(\'image\')\r\n        plt.savefig(os.path.join(\'image\', \'_\'.join([alg_name, env_id])))\r\n\r\n    if args.test:\r\n        load_ckpt(qnetwork)  # load model\r\n        for i in range(num_episodes):\r\n            ## Reset environment and get first new observation\r\n            s = env.reset()  # observation is state, integer 0 ~ 15\r\n            rAll = 0\r\n            if render: env.render()\r\n            for j in range(99):  # step index, maximum step is 99\r\n                ## Choose an action by greedily (with e chance of random action) from the Q-network\r\n                allQ = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32)).numpy()\r\n                a = np.argmax(allQ, 1)  # no epsilon, only greedy for testing\r\n\r\n                ## Get new state and reward from environment\r\n                s1, r, d, _ = env.step(a[0])\r\n                rAll += r\r\n                s = s1\r\n                if render: env.render()\r\n                ## Reduce chance of random action if an episode is done.\r\n                if d: break\r\n\r\n            print(\'Testing  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}\' \\\r\n                  .format(i, num_episodes, rAll, time.time() - t0))\r\n'"
examples/reinforcement_learning/tutorial_DQN_variants.py,30,"b'""""""\r\nDQN and its variants\r\n------------------------\r\nWe implement Double DQN, Dueling DQN and Noisy DQN here.\r\nThe max operator in standard DQN uses the same values both to select and to\r\nevaluate an action by\r\nQ(s_t, a_t) = R_{t+1} + \\gamma * max_{a}Q_{tar}(s_{t+1}, a).\r\nDouble DQN propose to use following evaluation to address overestimation problem\r\nof max operator:\r\nQ(s_t, a_t) = R_{t+1} + \\gamma * Q_{tar}(s_{t+1}, max_{a}Q(s_{t+1}, a)).\r\nDueling DQN uses dueling architecture where the value of state and the advantage\r\nof each action is estimated separately.\r\nNoisy DQN propose to explore by adding parameter noises.\r\nReference:\r\n------------------------\r\n1. Double DQN\r\n    Van Hasselt H, Guez A, Silver D. Deep reinforcement learning with double\r\n    q-learning[C]//Thirtieth AAAI Conference on Artificial Intelligence. 2016.\r\n2. Dueling DQN\r\n    Wang Z, Schaul T, Hessel M, et al. Dueling network architectures for deep\r\n    reinforcement learning[J]. arXiv preprint arXiv:1511.06581, 2015.\r\n3. Noisy DQN\r\n    Plappert M, Houthooft R, Dhariwal P, et al. Parameter space noise for\r\n    exploration[J]. arXiv preprint arXiv:1706.01905, 2017.\r\nEnvironment:\r\n------------------------\r\nCartpole and Pong in OpenAI Gym\r\nRequirements:\r\n------------------------\r\ntensorflow>=2.0.0a0\r\ntensorlayer>=2.0.0\r\nTo run:\r\n------------------------\r\npython tutorial_DQN_variantes.py --mode=train\r\npython tutorial_DQN_variantes.py --mode=test --save_path=dqn_variants/8000.npz\r\n""""""\r\nimport argparse\r\nimport os\r\nimport random\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=True)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\nparser.add_argument(\r\n    \'--save_path\', default=None, help=\'folder to save if mode == train else model path,\'\r\n    \'qnet will be saved once target net update\'\r\n)\r\nparser.add_argument(\'--seed\', help=\'random seed\', type=int, default=0)\r\nparser.add_argument(\'--env_id\', default=\'CartPole-v0\', help=\'CartPole-v0 or PongNoFrameskip-v4\')\r\nparser.add_argument(\'--noisy_scale\', type=float, default=1e-2)\r\nparser.add_argument(\'--disable_double\', action=\'store_true\', default=False)\r\nparser.add_argument(\'--disable_dueling\', action=\'store_true\', default=False)\r\nargs = parser.parse_args()\r\n\r\nrandom.seed(args.seed)\r\nnp.random.seed(args.seed)\r\ntf.random.set_seed(args.seed)  # reproducible\r\n\r\nenv_id = args.env_id\r\nenv = gym.make(env_id)\r\nenv.seed(args.seed)\r\nnoise_scale = args.noisy_scale\r\ndouble = not args.disable_double\r\ndueling = not args.disable_dueling\r\n\r\nalg_name = \'DQN\'\r\nif dueling: alg_name = \'Dueling_\' + alg_name\r\nif double: alg_name = \'Double_\' + alg_name\r\nif noise_scale != 0: alg_name = \'Noisy_\' + alg_name\r\nprint(alg_name)\r\n# ####################  hyper parameters  ####################\r\nif env_id == \'CartPole-v0\':\r\n    qnet_type = \'MLP\'\r\n    number_timesteps = 10000  # total number of time steps to train on\r\n    explore_timesteps = 100\r\n    # epsilon-greedy schedule, final exploit prob is 0.99\r\n    epsilon = lambda i_iter: 1 - 0.99 * min(1, i_iter / explore_timesteps)\r\n    lr = 5e-3  # learning rate\r\n    buffer_size = 1000  # replay buffer size\r\n    target_q_update_freq = 50  # how frequency target q net update\r\n    ob_scale = 1.0  # scale observations\r\n    clipnorm = None\r\nelse:\r\n    # reward will increase obviously after 1e5 time steps\r\n    qnet_type = \'CNN\'\r\n    number_timesteps = int(1e6)  # total number of time steps to train on\r\n    explore_timesteps = 1e5\r\n    # epsilon-greedy schedule, final exploit prob is 0.99\r\n    epsilon = lambda i_iter: 1 - 0.99 * min(1, i_iter / explore_timesteps)\r\n    lr = 1e-4  # learning rate\r\n    buffer_size = 10000  # replay buffer size\r\n    target_q_update_freq = 200  # how frequency target q net update\r\n    ob_scale = 1.0 / 255  # scale observations\r\n    clipnorm = 10\r\n\r\nin_dim = env.observation_space.shape\r\nout_dim = env.action_space.n\r\nreward_gamma = 0.99  # reward discount\r\nbatch_size = 32  # batch size for sampling from replay buffer\r\nwarm_start = buffer_size / 10  # sample times befor learning\r\nnoise_update_freq = 50  # how frequency param noise net update\r\n\r\n\r\n# ##############################  Network  ####################################\r\nclass MLP(tl.models.Model):\r\n\r\n    def __init__(self, name):\r\n        super(MLP, self).__init__(name=name)\r\n        self.h1 = tl.layers.Dense(64, tf.nn.tanh, in_channels=in_dim[0])\r\n        self.qvalue = tl.layers.Dense(out_dim, in_channels=64, name=\'q\', W_init=tf.initializers.GlorotUniform())\r\n        self.svalue = tl.layers.Dense(1, in_channels=64, name=\'s\', W_init=tf.initializers.GlorotUniform())\r\n        self.noise_scale = 0\r\n\r\n    def forward(self, ni):\r\n        feature = self.h1(ni)\r\n\r\n        # apply noise to all linear layer\r\n        if self.noise_scale != 0:\r\n            noises = []\r\n            for layer in [self.qvalue, self.svalue]:\r\n                for var in layer.trainable_weights:\r\n                    noise = tf.random.normal(tf.shape(var), 0, self.noise_scale)\r\n                    noises.append(noise)\r\n                    var.assign_add(noise)\r\n\r\n        qvalue = self.qvalue(feature)\r\n        svalue = self.svalue(feature)\r\n\r\n        if self.noise_scale != 0:\r\n            idx = 0\r\n            for layer in [self.qvalue, self.svalue]:\r\n                for var in layer.trainable_weights:\r\n                    var.assign_sub(noises[idx])\r\n                    idx += 1\r\n\r\n        if dueling:\r\n            # dueling network\r\n            return svalue + qvalue - tf.reduce_mean(qvalue, 1, keepdims=True)\r\n        else:\r\n            return qvalue\r\n\r\n\r\nclass CNN(tl.models.Model):\r\n\r\n    def __init__(self, name):\r\n        super(CNN, self).__init__(name=name)\r\n        h, w, in_channels = in_dim\r\n        dense_in_channels = 64 * ((h - 28) // 8) * ((w - 28) // 8)\r\n        self.conv1 = tl.layers.Conv2d(\r\n            32, (8, 8), (4, 4), tf.nn.relu, \'VALID\', in_channels=in_channels, name=\'conv2d_1\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.conv2 = tl.layers.Conv2d(\r\n            64, (4, 4), (2, 2), tf.nn.relu, \'VALID\', in_channels=32, name=\'conv2d_2\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.conv3 = tl.layers.Conv2d(\r\n            64, (3, 3), (1, 1), tf.nn.relu, \'VALID\', in_channels=64, name=\'conv2d_3\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.flatten = tl.layers.Flatten(name=\'flatten\')\r\n        self.preq = tl.layers.Dense(\r\n            256, tf.nn.relu, in_channels=dense_in_channels, name=\'pre_q\', W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.qvalue = tl.layers.Dense(out_dim, in_channels=256, name=\'q\', W_init=tf.initializers.GlorotUniform())\r\n        self.pres = tl.layers.Dense(\r\n            256, tf.nn.relu, in_channels=dense_in_channels, name=\'pre_s\', W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.svalue = tl.layers.Dense(1, in_channels=256, name=\'state\', W_init=tf.initializers.GlorotUniform())\r\n        self.noise_scale = 0\r\n\r\n    def forward(self, ni):\r\n        feature = self.flatten(self.conv3(self.conv2(self.conv1(ni))))\r\n\r\n        # apply noise to all linear layer\r\n        if self.noise_scale != 0:\r\n            noises = []\r\n            for layer in [self.preq, self.qvalue, self.pres, self.svalue]:\r\n                for var in layer.trainable_weights:\r\n                    noise = tf.random.normal(tf.shape(var), 0, self.noise_scale)\r\n                    noises.append(noise)\r\n                    var.assign_add(noise)\r\n\r\n        qvalue = self.qvalue(self.preq(feature))\r\n        svalue = self.svalue(self.pres(feature))\r\n\r\n        if self.noise_scale != 0:\r\n            idx = 0\r\n            for layer in [self.preq, self.qvalue, self.pres, self.svalue]:\r\n                for var in layer.trainable_weights:\r\n                    var.assign_sub(noises[idx])\r\n                    idx += 1\r\n\r\n        if dueling:\r\n            # dueling network\r\n            return svalue + qvalue - tf.reduce_mean(qvalue, 1, keepdims=True)\r\n        else:\r\n            return qvalue\r\n\r\n\r\n# ##############################  Replay  ####################################\r\nclass ReplayBuffer(object):\r\n\r\n    def __init__(self, size):\r\n        self._storage = []\r\n        self._maxsize = size\r\n        self._next_idx = 0\r\n\r\n    def __len__(self):\r\n        return len(self._storage)\r\n\r\n    def add(self, *args):\r\n        if self._next_idx >= len(self._storage):\r\n            self._storage.append(args)\r\n        else:\r\n            self._storage[self._next_idx] = args\r\n        self._next_idx = (self._next_idx + 1) % self._maxsize\r\n\r\n    def _encode_sample(self, idxes):\r\n        b_o, b_a, b_r, b_o_, b_d = [], [], [], [], []\r\n        for i in idxes:\r\n            o, a, r, o_, d = self._storage[i]\r\n            b_o.append(o)\r\n            b_a.append(a)\r\n            b_r.append(r)\r\n            b_o_.append(o_)\r\n            b_d.append(d)\r\n        return (\r\n            np.stack(b_o).astype(\'float32\') * ob_scale,\r\n            np.stack(b_a).astype(\'int32\'),\r\n            np.stack(b_r).astype(\'float32\'),\r\n            np.stack(b_o_).astype(\'float32\') * ob_scale,\r\n            np.stack(b_d).astype(\'float32\'),\r\n        )\r\n\r\n    def sample(self, batch_size):\r\n        indexes = range(len(self._storage))\r\n        idxes = [random.choice(indexes) for _ in range(batch_size)]\r\n        return self._encode_sample(idxes)\r\n\r\n\r\n# #############################  Functions  ###################################\r\ndef huber_loss(x):\r\n    """"""Loss function for value""""""\r\n    return tf.where(tf.abs(x) < 1, tf.square(x) * 0.5, tf.abs(x) - 0.5)\r\n\r\n\r\ndef sync(net, net_tar):\r\n    """"""Copy q network to target q network""""""\r\n    for var, var_tar in zip(net.trainable_weights, net_tar.trainable_weights):\r\n        var_tar.assign(var)\r\n\r\n\r\ndef log_softmax(x, dim):\r\n    temp = x - np.max(x, dim, keepdims=True)\r\n    return temp - np.log(np.exp(temp).sum(dim, keepdims=True))\r\n\r\n\r\ndef softmax(x, dim):\r\n    temp = np.exp(x - np.max(x, dim, keepdims=True))\r\n    return temp / temp.sum(dim, keepdims=True)\r\n\r\n\r\n# ###############################  DQN  #####################################\r\nclass DQN(object):\r\n\r\n    def __init__(self):\r\n        model = MLP if qnet_type == \'MLP\' else CNN\r\n        self.qnet = model(\'q\')\r\n        if args.train:\r\n            self.qnet.train()\r\n            self.targetqnet = model(\'targetq\')\r\n            self.targetqnet.infer()\r\n            sync(self.qnet, self.targetqnet)\r\n        else:\r\n            self.qnet.infer()\r\n            self.load(args.save_path)\r\n        self.niter = 0\r\n        if clipnorm is not None:\r\n            self.optimizer = tf.optimizers.Adam(learning_rate=lr, clipnorm=clipnorm)\r\n        else:\r\n            self.optimizer = tf.optimizers.Adam(learning_rate=lr)\r\n        self.noise_scale = noise_scale\r\n\r\n    def get_action(self, obv):\r\n        eps = epsilon(self.niter)\r\n        if args.train:\r\n            if random.random() < eps:\r\n                return int(random.random() * out_dim)\r\n            obv = np.expand_dims(obv, 0).astype(\'float32\') * ob_scale\r\n            if self.niter < explore_timesteps:\r\n                self.qnet.noise_scale = self.noise_scale\r\n                q_ptb = self._qvalues_func(obv).numpy()\r\n                self.qnet.noise_scale = 0\r\n                if i % noise_update_freq == 0:\r\n                    q = self._qvalues_func(obv).numpy()\r\n                    kl_ptb = (log_softmax(q, 1) - log_softmax(q_ptb, 1))\r\n                    kl_ptb = np.sum(kl_ptb * softmax(q, 1), 1).mean()\r\n                    kl_explore = -np.log(1 - eps + eps / out_dim)\r\n                    if kl_ptb < kl_explore:\r\n                        self.noise_scale *= 1.01\r\n                    else:\r\n                        self.noise_scale /= 1.01\r\n                return q_ptb.argmax(1)[0]\r\n            else:\r\n                return self._qvalues_func(obv).numpy().argmax(1)[0]\r\n        else:\r\n            obv = np.expand_dims(obv, 0).astype(\'float32\') * ob_scale\r\n            return self._qvalues_func(obv).numpy().argmax(1)[0]\r\n\r\n    @tf.function\r\n    def _qvalues_func(self, obv):\r\n        return self.qnet(obv)\r\n\r\n    def train(self, b_o, b_a, b_r, b_o_, b_d):\r\n        self._train_func(b_o, b_a, b_r, b_o_, b_d)\r\n\r\n        self.niter += 1\r\n        if self.niter % target_q_update_freq == 0:\r\n            sync(self.qnet, self.targetqnet)\r\n            self.save(args.save_path)\r\n\r\n    @tf.function\r\n    def _train_func(self, b_o, b_a, b_r, b_o_, b_d):\r\n        with tf.GradientTape() as tape:\r\n            td_errors = self._tderror_func(b_o, b_a, b_r, b_o_, b_d)\r\n            loss = tf.reduce_mean(huber_loss(td_errors))\r\n\r\n        grad = tape.gradient(loss, self.qnet.trainable_weights)\r\n        self.optimizer.apply_gradients(zip(grad, self.qnet.trainable_weights))\r\n\r\n        return td_errors\r\n\r\n    @tf.function\r\n    def _tderror_func(self, b_o, b_a, b_r, b_o_, b_d):\r\n        if double:\r\n            b_a_ = tf.one_hot(tf.argmax(self.qnet(b_o_), 1), out_dim)\r\n            b_q_ = (1 - b_d) * tf.reduce_sum(self.targetqnet(b_o_) * b_a_, 1)\r\n        else:\r\n            b_q_ = (1 - b_d) * tf.reduce_max(self.targetqnet(b_o_), 1)\r\n\r\n        b_q = tf.reduce_sum(self.qnet(b_o) * tf.one_hot(b_a, out_dim), 1)\r\n        return b_q - (b_r + reward_gamma * b_q_)\r\n\r\n    def save(self, path):\r\n        if path is None:\r\n            path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'q_net.hdf5\'), self.qnet)\r\n\r\n    def load(self, path):\r\n        if path is None:\r\n            path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'q_net.hdf5\'), self.qnet)\r\n\r\n\r\n# #############################  Trainer  ###################################\r\nif __name__ == \'__main__\':\r\n    dqn = DQN()\r\n    t0 = time.time()\r\n    if args.train:\r\n        buffer = ReplayBuffer(buffer_size)\r\n        nepisode = 0\r\n        all_episode_reward = []\r\n        for i in range(1, number_timesteps + 1):\r\n            o = env.reset()\r\n            episode_reward = 0\r\n            while True:\r\n                a = dqn.get_action(o)\r\n\r\n                # execute action and feed to replay buffer\r\n                # note that `_` tail in var name means next\r\n                o_, r, done, info = env.step(a)\r\n                buffer.add(o, a, r, o_, done)\r\n                episode_reward += r\r\n\r\n                if i >= warm_start:\r\n                    transitions = buffer.sample(batch_size)\r\n                    dqn.train(*transitions)\r\n\r\n                if done:\r\n                    break\r\n                else:\r\n                    o = o_\r\n\r\n            if nepisode == 0:\r\n                all_episode_reward.append(episode_reward)\r\n            else:\r\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\r\n            nepisode += 1\r\n            print(\r\n                \'Training  | Episode: {}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    nepisode, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )  # episode num starts from 1 in print\r\n\r\n        dqn.save(args.save_path)\r\n        plt.plot(all_episode_reward)\r\n        if not os.path.exists(\'image\'):\r\n            os.makedirs(\'image\')\r\n        plt.savefig(os.path.join(\'image\', \'_\'.join([alg_name, env_id])))\r\n\r\n    if args.test:\r\n        nepisode = 0\r\n        for i in range(1, number_timesteps + 1):\r\n            o = env.reset()\r\n            episode_reward = 0\r\n            while True:\r\n                env.render()\r\n                a = dqn.get_action(o)\r\n                o_, r, done, info = env.step(a)\r\n                episode_reward += r\r\n                if done:\r\n                    break\r\n                else:\r\n                    o = o_\r\n            nepisode += 1\r\n            print(\r\n                \'Testing  | Episode: {}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    nepisode, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n'"
examples/reinforcement_learning/tutorial_PG.py,11,"b'""""""\r\nVanilla Policy Gradient(VPG or REINFORCE)\r\n-----------------------------------------\r\nThe policy gradient algorithm works by updating policy parameters via stochastic gradient ascent on policy performance.\r\nIt\'s an on-policy algorithm can be used for environments with either discrete or continuous action spaces.\r\nHere is an example on discrete action space game CartPole-v0.\r\nTo apply it on continuous action space, you need to change the last softmax layer and the get_action function.\r\n\r\nReference\r\n---------\r\nCookbook: Barto A G, Sutton R S. Reinforcement Learning: An Introduction[J]. 1998.\r\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials/\r\n\r\nEnvironment\r\n-----------\r\nOpenai Gym CartPole-v0, discrete action space\r\n\r\nPrerequisites\r\n--------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-probability 0.6.0\r\ntensorlayer >=2.0.0\r\n\r\nTo run\r\n------\r\npython tutorial_PG.py --train/test\r\n\r\n""""""\r\nimport argparse\r\nimport os\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\n\r\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\nargs = parser.parse_args()\r\n\r\n#####################  hyper parameters  ####################\r\n\r\nENV_ID = \'CartPole-v1\'  # environment id\r\nRANDOM_SEED = 1  # random seed, can be either an int number or None\r\nRENDER = False  # render while training\r\n\r\nALG_NAME = \'PG\'\r\nTRAIN_EPISODES = 200\r\nTEST_EPISODES = 10\r\nMAX_STEPS = 500\r\n\r\n###############################  PG  ####################################\r\n\r\n\r\nclass PolicyGradient:\r\n    """"""\r\n    PG class\r\n    """"""\r\n\r\n    def __init__(self, state_dim, action_num, learning_rate=0.02, gamma=0.99):\r\n        self.gamma = gamma\r\n\r\n        self.state_buffer, self.action_buffer, self.reward_buffer = [], [], []\r\n\r\n        input_layer = tl.layers.Input([None, state_dim], tf.float32)\r\n        layer = tl.layers.Dense(\r\n            n_units=30, act=tf.nn.tanh, W_init=tf.random_normal_initializer(mean=0, stddev=0.3),\r\n            b_init=tf.constant_initializer(0.1)\r\n        )(input_layer)\r\n        all_act = tl.layers.Dense(\r\n            n_units=action_num, act=None, W_init=tf.random_normal_initializer(mean=0, stddev=0.3),\r\n            b_init=tf.constant_initializer(0.1)\r\n        )(layer)\r\n\r\n        self.model = tl.models.Model(inputs=input_layer, outputs=all_act)\r\n        self.model.train()\r\n        self.optimizer = tf.optimizers.Adam(learning_rate)\r\n\r\n    def get_action(self, s, greedy=False):\r\n        """"""\r\n        choose action with probabilities.\r\n        :param s: state\r\n        :param greedy: choose action greedy or not\r\n        :return: act\r\n        """"""\r\n        _logits = self.model(np.array([s], np.float32))\r\n        _probs = tf.nn.softmax(_logits).numpy()\r\n        if greedy:\r\n            return np.argmax(_probs.ravel())\r\n        return tl.rein.choice_action_by_probs(_probs.ravel())\r\n\r\n    def store_transition(self, s, a, r):\r\n        """"""\r\n        store data in memory buffer\r\n        :param s: state\r\n        :param a: act\r\n        :param r: reward\r\n        :return:\r\n        """"""\r\n        self.state_buffer.append(np.array([s], np.float32))\r\n        self.action_buffer.append(a)\r\n        self.reward_buffer.append(r)\r\n\r\n    def learn(self):\r\n        """"""\r\n        update policy parameters via stochastic gradient ascent\r\n        :return: None\r\n        """"""\r\n        discounted_reward_buffer_norm = self._discount_and_norm_rewards()\r\n\r\n        with tf.GradientTape() as tape:\r\n            _logits = self.model(np.vstack(self.state_buffer))\r\n            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n                logits=_logits, labels=np.array(self.action_buffer)\r\n            )\r\n            loss = tf.reduce_mean(neg_log_prob * discounted_reward_buffer_norm)\r\n\r\n        grad = tape.gradient(loss, self.model.trainable_weights)\r\n        self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\r\n\r\n        self.state_buffer, self.action_buffer, self.reward_buffer = [], [], []  # empty episode data\r\n        return discounted_reward_buffer_norm\r\n\r\n    def _discount_and_norm_rewards(self):\r\n        """"""\r\n        compute discount_and_norm_rewards\r\n        :return: discount_and_norm_rewards\r\n        """"""\r\n        # discount episode rewards\r\n        discounted_reward_buffer = np.zeros_like(self.reward_buffer)\r\n        running_add = 0\r\n        for t in reversed(range(0, len(self.reward_buffer))):\r\n            running_add = running_add * self.gamma + self.reward_buffer[t]\r\n            discounted_reward_buffer[t] = running_add\r\n\r\n        # normalize episode rewards\r\n        discounted_reward_buffer -= np.mean(discounted_reward_buffer)\r\n        discounted_reward_buffer /= np.std(discounted_reward_buffer)\r\n        return discounted_reward_buffer\r\n\r\n    def save(self):\r\n        """"""\r\n        save trained weights\r\n        :return: None\r\n        """"""\r\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'pg_policy.hdf5\'), self.model)\r\n\r\n    def load(self):\r\n        """"""\r\n        load trained weights\r\n        :return: None\r\n        """"""\r\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'pg_policy.hdf5\'), self.model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    env = gym.make(ENV_ID).unwrapped\r\n\r\n    # reproducible\r\n    np.random.seed(RANDOM_SEED)\r\n    tf.random.set_seed(RANDOM_SEED)\r\n    env.seed(RANDOM_SEED)\r\n\r\n    agent = PolicyGradient(\r\n        action_num=env.action_space.n,\r\n        state_dim=env.observation_space.shape[0],\r\n    )\r\n\r\n    t0 = time.time()\r\n\r\n    if args.train:\r\n        all_episode_reward = []\r\n        for episode in range(TRAIN_EPISODES):\r\n\r\n            state = env.reset()\r\n            episode_reward = 0\r\n\r\n            for step in range(MAX_STEPS):  # in one episode\r\n                if RENDER:\r\n                    env.render()\r\n\r\n                action = agent.get_action(state)\r\n                next_state, reward, done, info = env.step(action)\r\n                agent.store_transition(state, action, reward)\r\n                state = next_state\r\n                episode_reward += reward\r\n                if done:\r\n                    break\r\n            agent.learn()\r\n            print(\r\n                \'Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}\'.format(\r\n                    episode + 1, TRAIN_EPISODES, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n\r\n            if episode == 0:\r\n                all_episode_reward.append(episode_reward)\r\n            else:\r\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\r\n\r\n        agent.save()\r\n        plt.plot(all_episode_reward)\r\n        if not os.path.exists(\'image\'):\r\n            os.makedirs(\'image\')\r\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\r\n\r\n    if args.test:\r\n        # test\r\n        agent.load()\r\n        for episode in range(TEST_EPISODES):\r\n            state = env.reset()\r\n            episode_reward = 0\r\n            for step in range(MAX_STEPS):\r\n                env.render()\r\n                state, reward, done, info = env.step(agent.get_action(state, True))\r\n                episode_reward += reward\r\n                if done:\r\n                    break\r\n            print(\r\n                \'Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}\'.format(\r\n                    episode + 1, TEST_EPISODES, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n'"
examples/reinforcement_learning/tutorial_PPO.py,26,"b'""""""\r\nProximal Policy Optimization (PPO)\r\n----------------------------\r\nA simple version of Proximal Policy Optimization (PPO) using single thread.\r\nPPO is a family of first-order methods that use a few other tricks to keep new policies close to old.\r\nPPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO.\r\nReference\r\n---------\r\nProximal Policy Optimization Algorithms, Schulman et al. 2017\r\nHigh Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016\r\nEmergence of Locomotion Behaviours in Rich Environments, Heess et al. 2017\r\nMorvanZhou\'s tutorial page: https://morvanzhou.github.io/tutorials\r\nEnvironment\r\n-----------\r\nOpenai Gym Pendulum-v0, continual action space\r\nPrerequisites\r\n--------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-probability 0.6.0\r\ntensorlayer >=2.0.0\r\nTo run\r\n------\r\npython tutorial_PPO.py --train/test\r\n""""""\r\nimport argparse\r\nimport os\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\nimport tensorlayer as tl\r\n\r\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\nargs = parser.parse_args()\r\n\r\n#####################  hyper parameters  ####################\r\n\r\nENV_ID = \'Pendulum-v0\'  # environment id\r\nRANDOM_SEED = 1  # random seed\r\nRENDER = False  # render while training\r\n\r\nALG_NAME = \'PPO\'\r\nTRAIN_EPISODES = 1000  # total number of episodes for training\r\nTEST_EPISODES = 10  # total number of episodes for testing\r\nMAX_STEPS = 200  # total number of steps for each episode\r\nGAMMA = 0.9  # reward discount\r\nLR_A = 0.0001  # learning rate for actor\r\nLR_C = 0.0002  # learning rate for critic\r\nBATCH_SIZE = 32  # update batch size\r\nACTOR_UPDATE_STEPS = 10  # actor update steps\r\nCRITIC_UPDATE_STEPS = 10  # critic update steps\r\n\r\n# ppo-penalty parameters\r\nKL_TARGET = 0.01\r\nLAM = 0.5\r\n\r\n# ppo-clip parameters\r\nEPSILON = 0.2\r\n\r\n\r\n###############################  PPO  ####################################\r\n\r\n\r\nclass PPO(object):\r\n    """"""\r\n    PPO class\r\n    """"""\r\n    def __init__(self, state_dim, action_dim, action_bound, method=\'clip\'):\r\n        # critic\r\n        with tf.name_scope(\'critic\'):\r\n            inputs = tl.layers.Input([None, state_dim], tf.float32, \'state\')\r\n            layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\r\n            layer = tl.layers.Dense(64, tf.nn.relu)(layer)\r\n            v = tl.layers.Dense(1)(layer)\r\n        self.critic = tl.models.Model(inputs, v)\r\n        self.critic.train()\r\n\r\n        # actor\r\n        with tf.name_scope(\'actor\'):\r\n            inputs = tl.layers.Input([None, state_dim], tf.float32, \'state\')\r\n            layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\r\n            layer = tl.layers.Dense(64, tf.nn.relu)(layer)\r\n            a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\r\n            mean = tl.layers.Lambda(lambda x: x * action_bound, name=\'lambda\')(a)\r\n            logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))\r\n        self.actor = tl.models.Model(inputs, mean)\r\n        self.actor.trainable_weights.append(logstd)\r\n        self.actor.logstd = logstd\r\n        self.actor.train()\r\n\r\n        self.actor_opt = tf.optimizers.Adam(LR_A)\r\n        self.critic_opt = tf.optimizers.Adam(LR_C)\r\n\r\n        self.method = method\r\n        if method == \'penalty\':\r\n            self.kl_target = KL_TARGET\r\n            self.lam = LAM\r\n        elif method == \'clip\':\r\n            self.epsilon = EPSILON\r\n\r\n        self.state_buffer, self.action_buffer = [], []\r\n        self.reward_buffer, self.cumulative_reward_buffer = [], []\r\n        self.action_bound = action_bound\r\n\r\n    def train_actor(self, state, action, adv, old_pi):\r\n        """"""\r\n        Update policy network\r\n        :param state: state batch\r\n        :param action: action batch\r\n        :param adv: advantage batch\r\n        :param old_pi: old pi distribution\r\n        :return: kl_mean or None\r\n        """"""\r\n        with tf.GradientTape() as tape:\r\n            mean, std = self.actor(state), tf.exp(self.actor.logstd)\r\n            pi = tfp.distributions.Normal(mean, std)\r\n\r\n            ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))\r\n            surr = ratio * adv\r\n            if self.method == \'penalty\':  # ppo penalty\r\n                kl = tfp.distributions.kl_divergence(old_pi, pi)\r\n                kl_mean = tf.reduce_mean(kl)\r\n                loss = -(tf.reduce_mean(surr - self.lam * kl))\r\n            else:  # ppo clip\r\n                loss = -tf.reduce_mean(\r\n                    tf.minimum(surr,\r\n                               tf.clip_by_value(ratio, 1. - self.epsilon, 1. + self.epsilon) * adv)\r\n                )\r\n        a_gard = tape.gradient(loss, self.actor.trainable_weights)\r\n        self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\r\n\r\n        if self.method == \'kl_pen\':\r\n            return kl_mean\r\n\r\n    def train_critic(self, reward, state):\r\n        """"""\r\n        Update actor network\r\n        :param reward: cumulative reward batch\r\n        :param state: state batch\r\n        :return: None\r\n        """"""\r\n        reward = np.array(reward, dtype=np.float32)\r\n        with tf.GradientTape() as tape:\r\n            advantage = reward - self.critic(state)\r\n            loss = tf.reduce_mean(tf.square(advantage))\r\n        grad = tape.gradient(loss, self.critic.trainable_weights)\r\n        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))\r\n\r\n    def update(self):\r\n        """"""\r\n        Update parameter with the constraint of KL divergent\r\n        :return: None\r\n        """"""\r\n        s = np.array(self.state_buffer, np.float32)\r\n        a = np.array(self.action_buffer, np.float32)\r\n        r = np.array(self.cumulative_reward_buffer, np.float32)\r\n        mean, std = self.actor(s), tf.exp(self.actor.logstd)\r\n        pi = tfp.distributions.Normal(mean, std)\r\n        adv = r - self.critic(s)\r\n\r\n        # update actor\r\n        if self.method == \'kl_pen\':\r\n            for _ in range(ACTOR_UPDATE_STEPS):\r\n                kl = self.train_actor(s, a, adv, pi)\r\n            if kl < self.kl_target / 1.5:\r\n                self.lam /= 2\r\n            elif kl > self.kl_target * 1.5:\r\n                self.lam *= 2\r\n        else:\r\n            for _ in range(ACTOR_UPDATE_STEPS):\r\n                self.train_actor(s, a, adv, pi)\r\n\r\n        # update critic\r\n        for _ in range(CRITIC_UPDATE_STEPS):\r\n            self.train_critic(r, s)\r\n\r\n        self.state_buffer.clear()\r\n        self.action_buffer.clear()\r\n        self.cumulative_reward_buffer.clear()\r\n        self.reward_buffer.clear()\r\n\r\n    def get_action(self, state, greedy=False):\r\n        """"""\r\n        Choose action\r\n        :param state: state\r\n        :param greedy: choose action greedy or not\r\n        :return: clipped action\r\n        """"""\r\n        state = state[np.newaxis, :].astype(np.float32)\r\n        mean, std = self.actor(state), tf.exp(self.actor.logstd)\r\n        if greedy:\r\n            action = mean[0]\r\n        else:\r\n            pi = tfp.distributions.Normal(mean, std)\r\n            action = tf.squeeze(pi.sample(1), axis=0)[0]  # choosing action\r\n        return np.clip(action, -self.action_bound, self.action_bound)\r\n\r\n    def save(self):\r\n        """"""\r\n        save trained weights\r\n        :return: None\r\n        """"""\r\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'actor.hdf5\'), self.actor)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'critic.hdf5\'), self.critic)\r\n\r\n    def load(self):\r\n        """"""\r\n        load trained weights\r\n        :return: None\r\n        """"""\r\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'actor.hdf5\'), self.actor)\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'critic.hdf5\'), self.critic)\r\n\r\n    def store_transition(self, state, action, reward):\r\n        """"""\r\n        Store state, action, reward at each step\r\n        :param state:\r\n        :param action:\r\n        :param reward:\r\n        :return: None\r\n        """"""\r\n        self.state_buffer.append(state)\r\n        self.action_buffer.append(action)\r\n        self.reward_buffer.append(reward)\r\n\r\n    def finish_path(self, next_state, done):\r\n        """"""\r\n        Calculate cumulative reward\r\n        :param next_state:\r\n        :return: None\r\n        """"""\r\n        if done:\r\n            v_s_ = 0\r\n        else:\r\n            v_s_ = self.critic(np.array([next_state], np.float32))[0, 0]\r\n        discounted_r = []\r\n        for r in self.reward_buffer[::-1]:\r\n            v_s_ = r + GAMMA * v_s_\r\n            discounted_r.append(v_s_)\r\n        discounted_r.reverse()\r\n        discounted_r = np.array(discounted_r)[:, np.newaxis]\r\n        self.cumulative_reward_buffer.extend(discounted_r)\r\n        self.reward_buffer.clear()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    env = gym.make(ENV_ID).unwrapped\r\n\r\n    # reproducible\r\n    env.seed(RANDOM_SEED)\r\n    np.random.seed(RANDOM_SEED)\r\n    tf.random.set_seed(RANDOM_SEED)\r\n\r\n    state_dim = env.observation_space.shape[0]\r\n    action_dim = env.action_space.shape[0]\r\n    action_bound = env.action_space.high\r\n\r\n    agent = PPO(state_dim, action_dim, action_bound)\r\n\r\n    t0 = time.time()\r\n    if args.train:\r\n        all_episode_reward = []\r\n        for episode in range(TRAIN_EPISODES):\r\n            state = env.reset()\r\n            episode_reward = 0\r\n            for step in range(MAX_STEPS):  # in one episode\r\n                if RENDER:\r\n                    env.render()\r\n                action = agent.get_action(state)\r\n                state_, reward, done, info = env.step(action)\r\n                agent.store_transition(state, action, reward)\r\n                state = state_\r\n                episode_reward += reward\r\n\r\n                # update ppo\r\n                if len(agent.state_buffer) >= BATCH_SIZE:\r\n                    agent.finish_path(state_, done)\r\n                    agent.update()\r\n                if done:\r\n                    break\r\n            agent.finish_path(state_, done)\r\n            print(\r\n                \'Training  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    episode + 1, TRAIN_EPISODES, episode_reward, time.time() - t0)\r\n            )\r\n            if episode == 0:\r\n                all_episode_reward.append(episode_reward)\r\n            else:\r\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\r\n        agent.save()\r\n\r\n        plt.plot(all_episode_reward)\r\n        if not os.path.exists(\'image\'):\r\n            os.makedirs(\'image\')\r\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\r\n\r\n    if args.test:\r\n        # test\r\n        agent.load()\r\n        for episode in range(TEST_EPISODES):\r\n            state = env.reset()\r\n            episode_reward = 0\r\n            for step in range(MAX_STEPS):\r\n                env.render()\r\n                state, reward, done, info = env.step(agent.get_action(state, greedy=True))\r\n                episode_reward += reward\r\n                if done:\r\n                    break\r\n            print(\r\n                \'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    episode + 1, TEST_EPISODES, episode_reward,\r\n                    time.time() - t0))\r\n'"
examples/reinforcement_learning/tutorial_Qlearning.py,0,"b'""""""Q-Table learning algorithm.\r\nNon deep learning - TD Learning, Off-Policy, e-Greedy Exploration\r\nQ(S, A) <- Q(S, A) + alpha * (R + lambda * Q(newS, newA) - Q(S, A))\r\nSee David Silver RL Tutorial Lecture 5 - Q-Learning for more details.\r\nFor Q-Network, see tutorial_frozenlake_q_network.py\r\nEN: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.5m3361vlw\r\nCN: https://zhuanlan.zhihu.com/p/25710327\r\ntensorflow==2.0.0a0\r\ntensorlayer==2.0.0\r\n""""""\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=True)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\n\r\nparser.add_argument(\r\n    \'--save_path\', default=None, help=\'folder to save if mode == train else model path,\'\r\n    \'qnet will be saved once target net update\'\r\n)\r\nparser.add_argument(\'--seed\', help=\'random seed\', type=int, default=0)\r\nparser.add_argument(\'--env_id\', default=\'FrozenLake-v0\')\r\nargs = parser.parse_args()\r\n\r\n## Load the environment\r\nalg_name = \'Qlearning\'\r\nenv_id = args.env_id\r\nenv = gym.make(env_id)\r\nrender = False  # display the game environment\r\n\r\n##================= Implement Q-Table learning algorithm =====================##\r\n## Initialize table with all zeros\r\nQ = np.zeros([env.observation_space.n, env.action_space.n])\r\n## Set learning parameters\r\nlr = .85  # alpha, if use value function approximation, we can ignore it\r\nlambd = .99  # decay factor\r\nnum_episodes = 10000\r\nt0 = time.time()\r\n\r\nif args.train:\r\n    all_episode_reward = []\r\n    for i in range(num_episodes):\r\n        ## Reset environment and get first new observation\r\n        s = env.reset()\r\n        rAll = 0\r\n        ## The Q-Table learning algorithm\r\n        for j in range(99):\r\n            if render: env.render()\r\n            ## Choose an action by greedily (with noise) picking from Q table\r\n            a = np.argmax(Q[s, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))\r\n            ## Get new state and reward from environment\r\n            s1, r, d, _ = env.step(a)\r\n            ## Update Q-Table with new knowledge\r\n            Q[s, a] = Q[s, a] + lr * (r + lambd * np.max(Q[s1, :]) - Q[s, a])\r\n            rAll += r\r\n            s = s1\r\n            if d is True:\r\n                break\r\n        print(\r\n            \'Training  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                i + 1, num_episodes, rAll,\r\n                time.time() - t0\r\n            )\r\n        )\r\n        if i == 0:\r\n            all_episode_reward.append(rAll)\r\n        else:\r\n            all_episode_reward.append(all_episode_reward[-1] * 0.9 + rAll * 0.1)\r\n\r\n    # save\r\n    path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n    if not os.path.exists(path):\r\n        os.makedirs(path)\r\n    np.save(os.path.join(path, \'Q_table.npy\'), Q)\r\n\r\n    plt.plot(all_episode_reward)\r\n    if not os.path.exists(\'image\'):\r\n        os.makedirs(\'image\')\r\n    plt.savefig(os.path.join(\'image\', \'_\'.join([alg_name, env_id])))\r\n\r\n    # print(""Final Q-Table Values:/n %s"" % Q)\r\n\r\nif args.test:\r\n    path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n    Q = np.load(os.path.join(path, \'Q_table.npy\'))\r\n    for i in range(num_episodes):\r\n        ## Reset environment and get first new observation\r\n        s = env.reset()\r\n        rAll = 0\r\n        ## The Q-Table learning algorithm\r\n        for j in range(99):\r\n            ## Choose an action by greedily (with noise) picking from Q table\r\n            a = np.argmax(Q[s, :])\r\n            ## Get new state and reward from environment\r\n            s1, r, d, _ = env.step(a)\r\n            ## Update Q-Table with new knowledge\r\n            rAll += r\r\n            s = s1\r\n            if d is True:\r\n                break\r\n        print(\r\n            \'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                i + 1, num_episodes, rAll,\r\n                time.time() - t0\r\n            )\r\n        )\r\n'"
examples/reinforcement_learning/tutorial_SAC.py,41,"b'"""""" \r\nSoft Actor-Critic (SAC)\r\n------------------\r\nActor policy in SAC is stochastic, with off-policy training. \r\nAnd \'soft\' in SAC indicates the trade-off between the entropy and expected return. \r\nThe additional consideration of entropy term helps with more explorative policy.\r\nAnd this implementation contains an automatic update for the entropy factor.\r\nThis version of Soft Actor-Critic (SAC) implementation contains 5 networks: \r\n2 Q net, 2 target Q net, 1 policy net.\r\nIt uses alpha loss.\r\nReference\r\n---------\r\npaper: https://arxiv.org/pdf/1812.05905.pdf\r\nEnvironment\r\n---\r\nOpenai Gym Pendulum-v0, continuous action space\r\nhttps://gym.openai.com/envs/Pendulum-v0/\r\nPrerequisites\r\n--------------\r\ntensorflow >=2.0.0a0\r\ntensorflow-probability 0.6.0\r\ntensorlayer >=2.0.0\r\n&&\r\npip install box2d box2d-kengz --user\r\nTo run\r\n------\r\npython tutorial_SAC.py --train/test\r\n""""""\r\n\r\nimport argparse\r\nimport os\r\nimport random\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorflow_probability as tfp\r\nimport tensorlayer as tl\r\nfrom tensorlayer.layers import Dense\r\nfrom tensorlayer.models import Model\r\n\r\nNormal = tfp.distributions.Normal\r\ntl.logging.set_verbosity(tl.logging.DEBUG)\r\n\r\n# add arguments in command  --train/test\r\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\nargs = parser.parse_args()\r\n\r\n#####################  hyper parameters  ####################\r\n\r\nENV_ID = \'Pendulum-v0\'  # environment id\r\nRANDOM_SEED = 2  # random seed\r\nRENDER = False  # render while training\r\n\r\n# RL training\r\nALG_NAME = \'SAC\'\r\nTRAIN_EPISODES = 100  # total number of episodes for training\r\nTEST_EPISODES = 10  # total number of episodes for training\r\nMAX_STEPS = 200  # total number of steps for each episode\r\nEXPLORE_STEPS = 100  # 500 for random action sampling in the beginning of training\r\n\r\nBATCH_SIZE = 256  # update batch size\r\nHIDDEN_DIM = 32  # size of hidden layers for networks\r\nUPDATE_ITR = 3  # repeated updates for single step\r\nSOFT_Q_LR = 3e-4  # q_net learning rate\r\nPOLICY_LR = 3e-4  # policy_net learning rate\r\nALPHA_LR = 3e-4  # alpha learning rate\r\nPOLICY_TARGET_UPDATE_INTERVAL = 3  # delayed update for the policy network and target networks\r\nREWARD_SCALE = 1.  # value range of reward\r\nREPLAY_BUFFER_SIZE = 5e5  # size of the replay buffer\r\n\r\nAUTO_ENTROPY = True  # automatically updating variable alpha for entropy\r\n\r\n###############################  SAC  ####################################\r\n\r\n\r\nclass ReplayBuffer:\r\n    """"""\r\n    a ring buffer for storing transitions and sampling for training\r\n    :state: (state_dim,)\r\n    :action: (action_dim,)\r\n    :reward: (,), scalar\r\n    :next_state: (state_dim,)\r\n    :done: (,), scalar (0 and 1) or bool (True and False)\r\n    """"""\r\n\r\n    def __init__(self, capacity):\r\n        self.capacity = capacity\r\n        self.buffer = []\r\n        self.position = 0\r\n\r\n    def push(self, state, action, reward, next_state, done):\r\n        if len(self.buffer) < self.capacity:\r\n            self.buffer.append(None)\r\n        self.buffer[self.position] = (state, action, reward, next_state, done)\r\n        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\r\n\r\n    def sample(self, BATCH_SIZE):\r\n        batch = random.sample(self.buffer, BATCH_SIZE)\r\n        state, action, reward, next_state, done = map(np.stack, zip(*batch))  # stack for each element\r\n        """""" \r\n        the * serves as unpack: sum(a,b) <=> batch=(a,b), sum(*batch) ;\r\n        zip: a=[1,2], b=[2,3], zip(a,b) => [(1, 2), (2, 3)] ;\r\n        the map serves as mapping the function on each list element: map(square, [2,3]) => [4,9] ;\r\n        np.stack((1,2)) => array([1, 2])\r\n        """"""\r\n        return state, action, reward, next_state, done\r\n\r\n    def __len__(self):\r\n        return len(self.buffer)\r\n\r\n\r\nclass SoftQNetwork(Model):\r\n    """""" the network for evaluate values of state-action pairs: Q(s,a) """"""\r\n\r\n    def __init__(self, num_inputs, num_actions, hidden_dim, init_w=3e-3):\r\n        super(SoftQNetwork, self).__init__()\r\n        input_dim = num_inputs + num_actions\r\n        w_init = tf.keras.initializers.glorot_normal(\r\n            seed=None\r\n        )  # glorot initialization is better than uniform in practice\r\n        # w_init = tf.random_uniform_initializer(-init_w, init_w)\r\n\r\n        self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=input_dim, name=\'q1\')\r\n        self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name=\'q2\')\r\n        self.linear3 = Dense(n_units=1, W_init=w_init, in_channels=hidden_dim, name=\'q3\')\r\n\r\n    def forward(self, input):\r\n        x = self.linear1(input)\r\n        x = self.linear2(x)\r\n        x = self.linear3(x)\r\n        return x\r\n\r\n\r\nclass PolicyNetwork(Model):\r\n    """""" the network for generating non-determinstic (Gaussian distributed) action from the state input """"""\r\n\r\n    def __init__(\r\n            self, num_inputs, num_actions, hidden_dim, action_range=1., init_w=3e-3, log_std_min=-20, log_std_max=2\r\n    ):\r\n        super(PolicyNetwork, self).__init__()\r\n\r\n        self.log_std_min = log_std_min\r\n        self.log_std_max = log_std_max\r\n\r\n        w_init = tf.keras.initializers.glorot_normal(seed=None)\r\n        # w_init = tf.random_uniform_initializer(-init_w, init_w)\r\n\r\n        self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=num_inputs, name=\'policy1\')\r\n        self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name=\'policy2\')\r\n        self.linear3 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name=\'policy3\')\r\n\r\n        self.mean_linear = Dense(\r\n            n_units=num_actions, W_init=w_init, b_init=tf.random_uniform_initializer(-init_w, init_w),\r\n            in_channels=hidden_dim, name=\'policy_mean\'\r\n        )\r\n        self.log_std_linear = Dense(\r\n            n_units=num_actions, W_init=w_init, b_init=tf.random_uniform_initializer(-init_w, init_w),\r\n            in_channels=hidden_dim, name=\'policy_logstd\'\r\n        )\r\n\r\n        self.action_range = action_range\r\n        self.num_actions = num_actions\r\n\r\n    def forward(self, state):\r\n        x = self.linear1(state)\r\n        x = self.linear2(x)\r\n        x = self.linear3(x)\r\n\r\n        mean = self.mean_linear(x)\r\n        log_std = self.log_std_linear(x)\r\n        log_std = tf.clip_by_value(log_std, self.log_std_min, self.log_std_max)\r\n\r\n        return mean, log_std\r\n\r\n    def evaluate(self, state, epsilon=1e-6):\r\n        """""" generate action with state for calculating gradients """"""\r\n        state = state.astype(np.float32)\r\n        mean, log_std = self.forward(state)\r\n        std = tf.math.exp(log_std)  # no clip in evaluation, clip affects gradients flow\r\n\r\n        normal = Normal(0, 1)\r\n        z = normal.sample(mean.shape)\r\n        action_0 = tf.math.tanh(mean + std * z)  # TanhNormal distribution as actions; reparameterization trick\r\n        action = self.action_range * action_0\r\n        # according to original paper, with an extra last term for normalizing different action range\r\n        log_prob = Normal(mean, std).log_prob(mean + std * z) - tf.math.log(1. - action_0**2 +\r\n                                                                            epsilon) - np.log(self.action_range)\r\n        # both dims of normal.log_prob and -log(1-a**2) are (N,dim_of_action);\r\n        # the Normal.log_prob outputs the same dim of input features instead of 1 dim probability,\r\n        # needs sum up across the dim of actions to get 1 dim probability; or else use Multivariate Normal.\r\n        log_prob = tf.reduce_sum(log_prob, axis=1)[:, np.newaxis]  # expand dim as reduce_sum causes 1 dim reduced\r\n\r\n        return action, log_prob, z, mean, log_std\r\n\r\n    def get_action(self, state, greedy=False):\r\n        """""" generate action with state for interaction with envronment """"""\r\n        mean, log_std = self.forward([state])\r\n        std = tf.math.exp(log_std)\r\n\r\n        normal = Normal(0, 1)\r\n        z = normal.sample(mean.shape)\r\n        action = self.action_range * tf.math.tanh(\r\n            mean + std * z\r\n        )  # TanhNormal distribution as actions; reparameterization trick\r\n\r\n        action = self.action_range * tf.math.tanh(mean) if greedy else action\r\n        return action.numpy()[0]\r\n\r\n    def sample_action(self, ):\r\n        """""" generate random actions for exploration """"""\r\n        a = tf.random.uniform([self.num_actions], -1, 1)\r\n        return self.action_range * a.numpy()\r\n\r\n\r\nclass SAC:\r\n\r\n    def __init__(\r\n            self, state_dim, action_dim, action_range, hidden_dim, replay_buffer, SOFT_Q_LR=3e-4, POLICY_LR=3e-4,\r\n            ALPHA_LR=3e-4\r\n    ):\r\n        self.replay_buffer = replay_buffer\r\n\r\n        # initialize all networks\r\n        self.soft_q_net1 = SoftQNetwork(state_dim, action_dim, hidden_dim)\r\n        self.soft_q_net2 = SoftQNetwork(state_dim, action_dim, hidden_dim)\r\n        self.target_soft_q_net1 = SoftQNetwork(state_dim, action_dim, hidden_dim)\r\n        self.target_soft_q_net2 = SoftQNetwork(state_dim, action_dim, hidden_dim)\r\n        self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range)\r\n        self.soft_q_net1.train()\r\n        self.soft_q_net2.train()\r\n        self.target_soft_q_net1.eval()\r\n        self.target_soft_q_net2.eval()\r\n        self.policy_net.train()\r\n\r\n        self.log_alpha = tf.Variable(0, dtype=np.float32, name=\'log_alpha\')\r\n        self.alpha = tf.math.exp(self.log_alpha)\r\n        print(\'Soft Q Network (1,2): \', self.soft_q_net1)\r\n        print(\'Policy Network: \', self.policy_net)\r\n        # set mode\r\n        self.soft_q_net1.train()\r\n        self.soft_q_net2.train()\r\n        self.target_soft_q_net1.eval()\r\n        self.target_soft_q_net2.eval()\r\n        self.policy_net.train()\r\n\r\n        # initialize weights of target networks\r\n        self.target_soft_q_net1 = self.target_ini(self.soft_q_net1, self.target_soft_q_net1)\r\n        self.target_soft_q_net2 = self.target_ini(self.soft_q_net2, self.target_soft_q_net2)\r\n\r\n        self.soft_q_optimizer1 = tf.optimizers.Adam(SOFT_Q_LR)\r\n        self.soft_q_optimizer2 = tf.optimizers.Adam(SOFT_Q_LR)\r\n        self.policy_optimizer = tf.optimizers.Adam(POLICY_LR)\r\n        self.alpha_optimizer = tf.optimizers.Adam(ALPHA_LR)\r\n\r\n    def target_ini(self, net, target_net):\r\n        """""" hard-copy update for initializing target networks """"""\r\n        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\r\n            target_param.assign(param)\r\n        return target_net\r\n\r\n    def target_soft_update(self, net, target_net, soft_tau):\r\n        """""" soft update the target net with Polyak averaging """"""\r\n        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\r\n            target_param.assign(  # copy weight value into target parameters\r\n                target_param * (1.0 - soft_tau) + param * soft_tau\r\n            )\r\n        return target_net\r\n\r\n    def update(self, batch_size, reward_scale=10., auto_entropy=True, target_entropy=-2, gamma=0.99, soft_tau=1e-2):\r\n        """""" update all networks in SAC """"""\r\n        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\r\n\r\n        reward = reward[:, np.newaxis]  # expand dim\r\n        done = done[:, np.newaxis]\r\n\r\n        reward = reward_scale * (reward - np.mean(reward, axis=0)) / (\r\n            np.std(reward, axis=0) + 1e-6\r\n        )  # normalize with batch mean and std; plus a small number to prevent numerical problem\r\n\r\n        # Training Q Function\r\n        new_next_action, next_log_prob, _, _, _ = self.policy_net.evaluate(next_state)\r\n        target_q_input = tf.concat([next_state, new_next_action], 1)  # the dim 0 is number of samples\r\n        target_q_min = tf.minimum(\r\n            self.target_soft_q_net1(target_q_input), self.target_soft_q_net2(target_q_input)\r\n        ) - self.alpha * next_log_prob\r\n        target_q_value = reward + (1 - done) * gamma * target_q_min  # if done==1, only reward\r\n        q_input = tf.concat([state, action], 1)  # the dim 0 is number of samples\r\n\r\n        with tf.GradientTape() as q1_tape:\r\n            predicted_q_value1 = self.soft_q_net1(q_input)\r\n            q_value_loss1 = tf.reduce_mean(tf.losses.mean_squared_error(predicted_q_value1, target_q_value))\r\n        q1_grad = q1_tape.gradient(q_value_loss1, self.soft_q_net1.trainable_weights)\r\n        self.soft_q_optimizer1.apply_gradients(zip(q1_grad, self.soft_q_net1.trainable_weights))\r\n\r\n        with tf.GradientTape() as q2_tape:\r\n            predicted_q_value2 = self.soft_q_net2(q_input)\r\n            q_value_loss2 = tf.reduce_mean(tf.losses.mean_squared_error(predicted_q_value2, target_q_value))\r\n        q2_grad = q2_tape.gradient(q_value_loss2, self.soft_q_net2.trainable_weights)\r\n        self.soft_q_optimizer2.apply_gradients(zip(q2_grad, self.soft_q_net2.trainable_weights))\r\n\r\n        # Training Policy Function\r\n        with tf.GradientTape() as p_tape:\r\n            new_action, log_prob, z, mean, log_std = self.policy_net.evaluate(state)\r\n            new_q_input = tf.concat([state, new_action], 1)  # the dim 0 is number of samples\r\n            """""" implementation 1 """"""\r\n            predicted_new_q_value = tf.minimum(self.soft_q_net1(new_q_input), self.soft_q_net2(new_q_input))\r\n            # """""" implementation 2 """"""\r\n            # predicted_new_q_value = self.soft_q_net1(new_q_input)\r\n            policy_loss = tf.reduce_mean(self.alpha * log_prob - predicted_new_q_value)\r\n        p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)\r\n        self.policy_optimizer.apply_gradients(zip(p_grad, self.policy_net.trainable_weights))\r\n\r\n        # Updating alpha w.r.t entropy\r\n        # alpha: trade-off between exploration (max entropy) and exploitation (max Q)\r\n        if auto_entropy is True:\r\n            with tf.GradientTape() as alpha_tape:\r\n                alpha_loss = -tf.reduce_mean((self.log_alpha * (log_prob + target_entropy)))\r\n            alpha_grad = alpha_tape.gradient(alpha_loss, [self.log_alpha])\r\n            self.alpha_optimizer.apply_gradients(zip(alpha_grad, [self.log_alpha]))\r\n            self.alpha = tf.math.exp(self.log_alpha)\r\n        else:  # fixed alpha\r\n            self.alpha = 1.\r\n            alpha_loss = 0\r\n\r\n        # Soft update the target value nets\r\n        self.target_soft_q_net1 = self.target_soft_update(self.soft_q_net1, self.target_soft_q_net1, soft_tau)\r\n        self.target_soft_q_net2 = self.target_soft_update(self.soft_q_net2, self.target_soft_q_net2, soft_tau)\r\n\r\n    def save(self):  # save trained weights\r\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        extend_path = lambda s: os.path.join(path, s)\r\n        tl.files.save_npz(self.soft_q_net1.trainable_weights, extend_path(\'model_q_net1.npz\'))\r\n        tl.files.save_npz(self.soft_q_net2.trainable_weights, extend_path(\'model_q_net2.npz\'))\r\n        tl.files.save_npz(self.target_soft_q_net1.trainable_weights, extend_path(\'model_target_q_net1.npz\'))\r\n        tl.files.save_npz(self.target_soft_q_net2.trainable_weights, extend_path(\'model_target_q_net2.npz\'))\r\n        tl.files.save_npz(self.policy_net.trainable_weights, extend_path(\'model_policy_net.npz\'))\r\n        np.save(extend_path(\'log_alpha.npy\'), self.log_alpha.numpy())  # save log_alpha variable\r\n\r\n    def load_weights(self):  # load trained weights\r\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\r\n        extend_path = lambda s: os.path.join(path, s)\r\n        tl.files.load_and_assign_npz(extend_path(\'model_q_net1.npz\'), self.soft_q_net1)\r\n        tl.files.load_and_assign_npz(extend_path(\'model_q_net2.npz\'), self.soft_q_net2)\r\n        tl.files.load_and_assign_npz(extend_path(\'model_target_q_net1.npz\'), self.target_soft_q_net1)\r\n        tl.files.load_and_assign_npz(extend_path(\'model_target_q_net2.npz\'), self.target_soft_q_net2)\r\n        tl.files.load_and_assign_npz(extend_path(\'model_policy_net.npz\'), self.policy_net)\r\n        self.log_alpha.assign(np.load(extend_path(\'log_alpha.npy\')))  # load log_alpha variable\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    # initialization of env\r\n    env = gym.make(ENV_ID).unwrapped\r\n    state_dim = env.observation_space.shape[0]\r\n    action_dim = env.action_space.shape[0]\r\n    action_range = env.action_space.high  # scale action, [-action_range, action_range]\r\n\r\n    # reproducible\r\n    env.seed(RANDOM_SEED)\r\n    random.seed(RANDOM_SEED)\r\n    np.random.seed(RANDOM_SEED)\r\n    tf.random.set_seed(RANDOM_SEED)\r\n\r\n    # initialization of buffer\r\n    replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\r\n    # initialization of trainer\r\n    agent = SAC(state_dim, action_dim, action_range, HIDDEN_DIM, replay_buffer, SOFT_Q_LR, POLICY_LR, ALPHA_LR)\r\n\r\n    t0 = time.time()\r\n    # training loop\r\n    if args.train:\r\n        frame_idx = 0\r\n        all_episode_reward = []\r\n\r\n        # need an extra call here to make inside functions be able to use model.forward\r\n        state = env.reset().astype(np.float32)\r\n        agent.policy_net([state])\r\n\r\n        for episode in range(TRAIN_EPISODES):\r\n            state = env.reset().astype(np.float32)\r\n            episode_reward = 0\r\n            for step in range(MAX_STEPS):\r\n                if RENDER:\r\n                    env.render()\r\n                if frame_idx > EXPLORE_STEPS:\r\n                    action = agent.policy_net.get_action(state)\r\n                else:\r\n                    action = agent.policy_net.sample_action()\r\n\r\n                next_state, reward, done, _ = env.step(action)\r\n                next_state = next_state.astype(np.float32)\r\n                done = 1 if done is True else 0\r\n\r\n                replay_buffer.push(state, action, reward, next_state, done)\r\n                state = next_state\r\n                episode_reward += reward\r\n                frame_idx += 1\r\n\r\n                if len(replay_buffer) > BATCH_SIZE:\r\n                    for i in range(UPDATE_ITR):\r\n                        agent.update(\r\n                            BATCH_SIZE, reward_scale=REWARD_SCALE, auto_entropy=AUTO_ENTROPY,\r\n                            target_entropy=-1. * action_dim\r\n                        )\r\n\r\n                if done:\r\n                    break\r\n            if episode == 0:\r\n                all_episode_reward.append(episode_reward)\r\n            else:\r\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\r\n            print(\r\n                \'Training  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    episode + 1, TRAIN_EPISODES, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n        agent.save()\r\n        plt.plot(all_episode_reward)\r\n        if not os.path.exists(\'image\'):\r\n            os.makedirs(\'image\')\r\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\r\n\r\n    if args.test:\r\n        agent.load_weights()\r\n\r\n        # need an extra call here to make inside functions be able to use model.forward\r\n        state = env.reset().astype(np.float32)\r\n        agent.policy_net([state])\r\n\r\n        for episode in range(TEST_EPISODES):\r\n            state = env.reset().astype(np.float32)\r\n            episode_reward = 0\r\n            for step in range(MAX_STEPS):\r\n                env.render()\r\n                state, reward, done, info = env.step(agent.policy_net.get_action(state, greedy=True))\r\n                state = state.astype(np.float32)\r\n                episode_reward += reward\r\n                if done:\r\n                    break\r\n            print(\r\n                \'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    episode + 1, TEST_EPISODES, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n'"
examples/reinforcement_learning/tutorial_TD3.py,27,"b'""""""\nTwin Delayed DDPG (TD3)\n------------------------\nDDPG suffers from problems like overestimate of Q-values and sensitivity to hyper-parameters.\nTwin Delayed DDPG (TD3) is a variant of DDPG with several tricks:\n* Trick One: Clipped Double-Q Learning. TD3 learns two Q-functions instead of one (hence ""twin""),\nand uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.\n\n* Trick Two: ""Delayed"" Policy Updates. TD3 updates the policy (and target networks) less frequently\nthan the Q-function. \n\n* Trick Three: Target Policy Smoothing. TD3 adds noise to the target action, to make it harder for \nthe policy to exploit Q-function errors by smoothing out Q along changes in action.\n\nThe implementation of TD3 includes 6 networks: 2 Q-net, 2 target Q-net, 1 policy net, 1 target policy net\nActor policy in TD3 is deterministic, with Gaussian exploration noise.\n\nReference\n---------\noriginal paper: https://arxiv.org/pdf/1802.09477.pdf\n\n\nEnvironment\n---\nOpenai Gym Pendulum-v0, continuous action space\nhttps://gym.openai.com/envs/Pendulum-v0/\n\nPrerequisites\n---\ntensorflow >=2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer >=2.0.0\n\n&&\npip install box2d box2d-kengz --user\n\nTo run\n-------\npython tutorial_TD3.py --train/test\n\n""""""\n\nimport argparse\nimport os\nimport random\nimport time\n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorflow_probability as tfp\nimport tensorlayer as tl\nfrom tensorlayer.layers import Dense\nfrom tensorlayer.models import Model\n\nNormal = tfp.distributions.Normal\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# add arguments in command  --train/test\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\nargs = parser.parse_args()\n\n#####################  hyper parameters  ####################\n# choose env\nENV_ID = \'Pendulum-v0\'  # environment id\nRANDOM_SEED = 2  # random seed\nRENDER = False  # render while training\n\n# RL training\nALG_NAME = \'TD3\'\nTRAIN_EPISODES = 100  # total number of episodes for training\nTEST_EPISODES = 10  # total number of episodes for training\nMAX_STEPS = 200  # maximum number of steps for one episode\nBATCH_SIZE = 64  # update batch size\nEXPLORE_STEPS = 500  # 500 for random action sampling in the beginning of training\n\nHIDDEN_DIM = 64  # size of hidden layers for networks\nUPDATE_ITR = 3  # repeated updates for single step\nQ_LR = 3e-4  # q_net learning rate\nPOLICY_LR = 3e-4  # policy_net learning rate\nPOLICY_TARGET_UPDATE_INTERVAL = 3  # delayed steps for updating the policy network and target networks\nEXPLORE_NOISE_SCALE = 1.0  # range of action noise for exploration\nEVAL_NOISE_SCALE = 0.5  # range of action noise for evaluation of action value\nREWARD_SCALE = 1.  # value range of reward\nREPLAY_BUFFER_SIZE = 5e5  # size of replay buffer\n\n###############################  TD3  ####################################\n\n\nclass ReplayBuffer:\n    """"""\n    a ring buffer for storing transitions and sampling for training\n    :state: (state_dim,)\n    :action: (action_dim,)\n    :reward: (,), scalar\n    :next_state: (state_dim,)\n    :done: (,), scalar (0 and 1) or bool (True and False)\n    """"""\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.buffer = []\n        self.position = 0\n\n    def push(self, state, action, reward, next_state, done):\n        if len(self.buffer) < self.capacity:\n            self.buffer.append(None)\n        self.buffer[self.position] = (state, action, reward, next_state, done)\n        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n\n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        state, action, reward, next_state, done = map(np.stack, zip(*batch))  # stack for each element\n        """""" \n        the * serves as unpack: sum(a,b) <=> batch=(a,b), sum(*batch) ;\n        zip: a=[1,2], b=[2,3], zip(a,b) => [(1, 2), (2, 3)] ;\n        the map serves as mapping the function on each list element: map(square, [2,3]) => [4,9] ;\n        np.stack((1,2)) => array([1, 2])\n        """"""\n        return state, action, reward, next_state, done\n\n    def __len__(self):\n        return len(self.buffer)\n\n\nclass QNetwork(Model):\n    """""" the network for evaluate values of state-action pairs: Q(s,a) """"""\n\n    def __init__(self, num_inputs, num_actions, hidden_dim, init_w=3e-3):\n        super(QNetwork, self).__init__()\n        input_dim = num_inputs + num_actions\n        # w_init = tf.keras.initializers.glorot_normal(seed=None)\n        w_init = tf.random_uniform_initializer(-init_w, init_w)\n\n        self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=input_dim, name=\'q1\')\n        self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name=\'q2\')\n        self.linear3 = Dense(n_units=1, W_init=w_init, in_channels=hidden_dim, name=\'q3\')\n\n    def forward(self, input):\n        x = self.linear1(input)\n        x = self.linear2(x)\n        x = self.linear3(x)\n        return x\n\n\nclass PolicyNetwork(Model):\n    """""" the network for generating non-determinstic (Gaussian distributed) action from the state input """"""\n\n    def __init__(self, num_inputs, num_actions, hidden_dim, action_range=1., init_w=3e-3):\n        super(PolicyNetwork, self).__init__()\n        w_init = tf.random_uniform_initializer(-init_w, init_w)\n\n        self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=num_inputs, name=\'policy1\')\n        self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name=\'policy2\')\n        self.linear3 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name=\'policy3\')\n        self.output_linear = Dense(\n            n_units=num_actions, W_init=w_init, b_init=tf.random_uniform_initializer(-init_w, init_w),\n            in_channels=hidden_dim, name=\'policy_output\'\n        )\n        self.action_range = action_range\n        self.num_actions = num_actions\n\n    def forward(self, state):\n        x = self.linear1(state)\n        x = self.linear2(x)\n        x = self.linear3(x)\n        output = tf.nn.tanh(self.output_linear(x))  # unit range output [-1, 1]\n        return output\n\n    def evaluate(self, state, eval_noise_scale):\n        """""" \n        generate action with state for calculating gradients;\n        eval_noise_scale: as the trick of target policy smoothing, for generating noisy actions.\n        """"""\n        state = state.astype(np.float32)\n        action = self.forward(state)\n\n        action = self.action_range * action\n\n        # add noise\n        normal = Normal(0, 1)\n        eval_noise_clip = 2 * eval_noise_scale\n        noise = normal.sample(action.shape) * eval_noise_scale\n        noise = tf.clip_by_value(noise, -eval_noise_clip, eval_noise_clip)\n        action = action + noise\n        return action\n\n    def get_action(self, state, explore_noise_scale, greedy=False):\n        """""" generate action with state for interaction with envronment """"""\n        action = self.forward([state])\n        action = self.action_range * action.numpy()[0]\n        if greedy:\n            return action\n        # add noise\n        normal = Normal(0, 1)\n        noise = normal.sample(action.shape) * explore_noise_scale\n        action += noise\n        return action.numpy()\n\n    def sample_action(self):\n        """""" generate random actions for exploration """"""\n        a = tf.random.uniform([self.num_actions], -1, 1)\n        return self.action_range * a.numpy()\n\n\nclass TD3:\n\n    def __init__(\n            self, state_dim, action_dim, action_range, hidden_dim, replay_buffer, policy_target_update_interval=1,\n            q_lr=3e-4, policy_lr=3e-4\n    ):\n        self.replay_buffer = replay_buffer\n\n        # initialize all networks\n        self.q_net1 = QNetwork(state_dim, action_dim, hidden_dim)\n        self.q_net2 = QNetwork(state_dim, action_dim, hidden_dim)\n        self.target_q_net1 = QNetwork(state_dim, action_dim, hidden_dim)\n        self.target_q_net2 = QNetwork(state_dim, action_dim, hidden_dim)\n        self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range)\n        self.target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range)\n        print(\'Q Network (1,2): \', self.q_net1)\n        print(\'Policy Network: \', self.policy_net)\n\n        # initialize weights of target networks\n        self.target_q_net1 = self.target_ini(self.q_net1, self.target_q_net1)\n        self.target_q_net2 = self.target_ini(self.q_net2, self.target_q_net2)\n        self.target_policy_net = self.target_ini(self.policy_net, self.target_policy_net)\n\n        # set train mode\n        self.q_net1.train()\n        self.q_net2.train()\n        self.target_q_net1.eval()\n        self.target_q_net2.eval()\n        self.policy_net.train()\n        self.target_policy_net.eval()\n\n        self.update_cnt = 0\n        self.policy_target_update_interval = policy_target_update_interval\n\n        self.q_optimizer1 = tf.optimizers.Adam(q_lr)\n        self.q_optimizer2 = tf.optimizers.Adam(q_lr)\n        self.policy_optimizer = tf.optimizers.Adam(policy_lr)\n\n    def target_ini(self, net, target_net):\n        """""" hard-copy update for initializing target networks """"""\n        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n            target_param.assign(param)\n        return target_net\n\n    def target_soft_update(self, net, target_net, soft_tau):\n        """""" soft update the target net with Polyak averaging """"""\n        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n            target_param.assign(  # copy weight value into target parameters\n                target_param * (1.0 - soft_tau) + param * soft_tau\n            )\n        return target_net\n\n    def update(self, batch_size, eval_noise_scale, reward_scale=10., gamma=0.9, soft_tau=1e-2):\n        """""" update all networks in TD3 """"""\n        self.update_cnt += 1\n        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n\n        reward = reward[:, np.newaxis]  # expand dim\n        done = done[:, np.newaxis]\n\n        new_next_action = self.target_policy_net.evaluate(\n            next_state, eval_noise_scale=eval_noise_scale\n        )  # clipped normal noise\n        reward = reward_scale * (reward - np.mean(reward, axis=0)) / (\n            np.std(reward, axis=0) + 1e-6\n        )  # normalize with batch mean and std; plus a small number to prevent numerical problem\n\n        # Training Q Function\n        target_q_input = tf.concat([next_state, new_next_action], 1)  # the dim 0 is number of samples\n        target_q_min = tf.minimum(self.target_q_net1(target_q_input), self.target_q_net2(target_q_input))\n\n        target_q_value = reward + (1 - done) * gamma * target_q_min  # if done==1, only reward\n        q_input = tf.concat([state, action], 1)  # input of q_net\n\n        with tf.GradientTape() as q1_tape:\n            predicted_q_value1 = self.q_net1(q_input)\n            q_value_loss1 = tf.reduce_mean(tf.square(predicted_q_value1 - target_q_value))\n        q1_grad = q1_tape.gradient(q_value_loss1, self.q_net1.trainable_weights)\n        self.q_optimizer1.apply_gradients(zip(q1_grad, self.q_net1.trainable_weights))\n\n        with tf.GradientTape() as q2_tape:\n            predicted_q_value2 = self.q_net2(q_input)\n            q_value_loss2 = tf.reduce_mean(tf.square(predicted_q_value2 - target_q_value))\n        q2_grad = q2_tape.gradient(q_value_loss2, self.q_net2.trainable_weights)\n        self.q_optimizer2.apply_gradients(zip(q2_grad, self.q_net2.trainable_weights))\n\n        # Training Policy Function\n        if self.update_cnt % self.policy_target_update_interval == 0:\n            with tf.GradientTape() as p_tape:\n                new_action = self.policy_net.evaluate(\n                    state, eval_noise_scale=0.0\n                )  # no noise, deterministic policy gradients\n                new_q_input = tf.concat([state, new_action], 1)\n                # """""" implementation 1 """"""\n                # predicted_new_q_value = tf.minimum(self.q_net1(new_q_input),self.q_net2(new_q_input))\n                """""" implementation 2 """"""\n                predicted_new_q_value = self.q_net1(new_q_input)\n                policy_loss = -tf.reduce_mean(predicted_new_q_value)\n            p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)\n            self.policy_optimizer.apply_gradients(zip(p_grad, self.policy_net.trainable_weights))\n\n            # Soft update the target nets\n            self.target_q_net1 = self.target_soft_update(self.q_net1, self.target_q_net1, soft_tau)\n            self.target_q_net2 = self.target_soft_update(self.q_net2, self.target_q_net2, soft_tau)\n            self.target_policy_net = self.target_soft_update(self.policy_net, self.target_policy_net, soft_tau)\n\n    def save(self):  # save trained weights\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        if not os.path.exists(path):\n            os.makedirs(path)\n        extend_path = lambda s: os.path.join(path, s)\n        tl.files.save_npz(self.q_net1.trainable_weights, extend_path(\'model_q_net1.npz\'))\n        tl.files.save_npz(self.q_net2.trainable_weights, extend_path(\'model_q_net2.npz\'))\n        tl.files.save_npz(self.target_q_net1.trainable_weights, extend_path(\'model_target_q_net1.npz\'))\n        tl.files.save_npz(self.target_q_net2.trainable_weights, extend_path(\'model_target_q_net2.npz\'))\n        tl.files.save_npz(self.policy_net.trainable_weights, extend_path(\'model_policy_net.npz\'))\n        tl.files.save_npz(self.target_policy_net.trainable_weights, extend_path(\'model_target_policy_net.npz\'))\n\n    def load(self):  # load trained weights\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        extend_path = lambda s: os.path.join(path, s)\n        tl.files.load_and_assign_npz(extend_path(\'model_q_net1.npz\'), self.q_net1)\n        tl.files.load_and_assign_npz(extend_path(\'model_q_net2.npz\'), self.q_net2)\n        tl.files.load_and_assign_npz(extend_path(\'model_target_q_net1.npz\'), self.target_q_net1)\n        tl.files.load_and_assign_npz(extend_path(\'model_target_q_net2.npz\'), self.target_q_net2)\n        tl.files.load_and_assign_npz(extend_path(\'model_policy_net.npz\'), self.policy_net)\n        tl.files.load_and_assign_npz(extend_path(\'model_target_policy_net.npz\'), self.target_policy_net)\n\n\nif __name__ == \'__main__\':\n    # initialization of env\n    env = gym.make(ENV_ID).unwrapped\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n    action_range = env.action_space.high  # scale action, [-action_range, action_range]\n\n    # reproducible\n    env.seed(RANDOM_SEED)\n    random.seed(RANDOM_SEED)\n    np.random.seed(RANDOM_SEED)\n    tf.random.set_seed(RANDOM_SEED)\n\n    # initialization of buffer\n    replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n    # initialization of trainer\n    agent = TD3(\n        state_dim, action_dim, action_range, HIDDEN_DIM, replay_buffer, POLICY_TARGET_UPDATE_INTERVAL, Q_LR, POLICY_LR\n    )\n    t0 = time.time()\n\n    # training loop\n    if args.train:\n        frame_idx = 0\n        all_episode_reward = []\n\n        # need an extra call here to make inside functions be able to use model.forward\n        state = env.reset().astype(np.float32)\n        agent.policy_net([state])\n        agent.target_policy_net([state])\n\n        for episode in range(TRAIN_EPISODES):\n            state = env.reset().astype(np.float32)\n            episode_reward = 0\n\n            for step in range(MAX_STEPS):\n                if RENDER:\n                    env.render()\n                if frame_idx > EXPLORE_STEPS:\n                    action = agent.policy_net.get_action(state, EXPLORE_NOISE_SCALE)\n                else:\n                    action = agent.policy_net.sample_action()\n\n                next_state, reward, done, _ = env.step(action)\n                next_state = next_state.astype(np.float32)\n                done = 1 if done is True else 0\n\n                replay_buffer.push(state, action, reward, next_state, done)\n                state = next_state\n                episode_reward += reward\n                frame_idx += 1\n\n                if len(replay_buffer) > BATCH_SIZE:\n                    for i in range(UPDATE_ITR):\n                        agent.update(BATCH_SIZE, EVAL_NOISE_SCALE, REWARD_SCALE)\n\n                if done:\n                    break\n            if episode == 0:\n                all_episode_reward.append(episode_reward)\n            else:\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\n            print(\n                \'Training  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    episode + 1, TRAIN_EPISODES, episode_reward,\n                    time.time() - t0\n                )\n            )\n        agent.save()\n        plt.plot(all_episode_reward)\n        if not os.path.exists(\'image\'):\n            os.makedirs(\'image\')\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\n\n    if args.test:\n        agent.load()\n\n        # need an extra call here to make inside functions be able to use model.forward\n        state = env.reset().astype(np.float32)\n        agent.policy_net([state])\n\n        for episode in range(TEST_EPISODES):\n            state = env.reset().astype(np.float32)\n            episode_reward = 0\n            for step in range(MAX_STEPS):\n                env.render()\n                action = agent.policy_net.get_action(state, EXPLORE_NOISE_SCALE, greedy=True)\n                state, reward, done, info = env.step(action)\n                state = state.astype(np.float32)\n                episode_reward += reward\n                if done:\n                    break\n            print(\n                \'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    episode + 1, TEST_EPISODES, episode_reward,\n                    time.time() - t0\n                )\n            )\n'"
examples/reinforcement_learning/tutorial_TRPO.py,30,"b'""""""\nTrust Region Policy Optimization (TRPO)\n---------------------------------------\nPG method with a large step can collapse the policy performance,\neven with a small step can lead a large differences in policy.\nTRPO constraint the step in policy space using KL divergence (rather than in parameter space),\nwhich can monotonically improve performance and avoid a collapsed update.\n\nReference\n---------\nTrust Region Policy Optimization, Schulman et al. 2015\nHigh Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016\nApproximately Optimal Approximate Reinforcement Learning, Kakade and Langford 2002\nopenai/spinningup : http://spinningup.openai.com/en/latest/algorithms/trpo.html\n\nEnvironment\n-----------\nOpenai Gym Pendulum-v0, continual action space\n\nPrerequisites\n--------------\ntensorflow >=2.0.0a0\ntensorflow-probability 0.6.0\ntensorlayer >=2.0.0\n\nTo run\n------\npython tutorial_TRPO.py --train/test\n\n""""""\nimport argparse\nimport copy\nimport os\nimport threading\nimport time\n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.signal\nimport tensorflow as tf\n\nimport tensorflow_probability as tfp\nimport tensorlayer as tl\n\nparser = argparse.ArgumentParser(description=\'Train or test neural net motor controller.\')\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=False)\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\nargs = parser.parse_args()\n\n#####################  hyper parameters  ####################\n\nENV_ID = \'Pendulum-v0\'  # environment id\nRANDOM_SEED = 2  # random seed\nRENDER = False\n\nALG_NAME = \'TRPO\'\nTRAIN_EPISODES = 1000  # total number of episodes for training\nTEST_EPISODES = 100  # total number of episodes for testing\nMAX_STEPS = 200  # total number of steps for each episode\n\nHIDDEN_SIZES = [64, 64]  # hidden layer size\nGAMMA = 0.99  # reward discount\nDELTA = 0.01  # KL-divergence limit for TRPO update.\nVF_LR = 1e-3  # Learning rate for value function optimizer\nTRAIN_VF_ITERS = 100  # Number of gradient descent steps to take on value function per epoch\nDAMPING_COEFF = 0.1  # Artifact for numerical stability\nCG_ITERS = 10  # Number of iterations of conjugate gradient to perform\nBACKTRACK_ITERS = 10  # Maximum number of steps allowed in the backtracking line search\nBACKTRACK_COEFF = 0.8  # How far back to step during backtracking line search\nLAM = 0.97  # lambda for GAE-lambda\nSAVE_FREQ = 10  # How often (in terms of gap between epochs) to save the current policy and value function\nEPS = 1e-8  # epsilon\nBATCH_SIZE = 512  # batch size\n\n#####################  functions  ####################\n\n\nclass GAE_Buffer:\n    """"""\n    A buffer for storing trajectories experienced by a TRPO agent interacting\n    with the environment, and using Generalized Advantage Estimation (GAE-lambda)\n    for calculating the advantages of state-action pairs.\n    """"""\n\n    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n        self.adv_buf = np.zeros(size, dtype=np.float32)\n        self.rew_buf = np.zeros(size, dtype=np.float32)\n        self.ret_buf = np.zeros(size, dtype=np.float32)\n        self.val_buf = np.zeros(size, dtype=np.float32)\n        self.logp_buf = np.zeros(size, dtype=np.float32)\n        self.mean_buf = np.zeros(size, dtype=np.float32)\n        self.log_std_buf = np.zeros(size, dtype=np.float32)\n        self.gamma, self.lam = gamma, lam\n        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n\n    def store(self, obs, act, rew, val, logp, mean, log_std):\n        """"""\n        Append one timestep of agent-environment interaction to the buffer.\n        """"""\n        assert self.ptr < self.max_size  # buffer has to have room so you can store\n        self.obs_buf[self.ptr] = obs\n        self.act_buf[self.ptr] = act\n        self.rew_buf[self.ptr] = rew\n        self.val_buf[self.ptr] = val\n        self.logp_buf[self.ptr] = logp\n        self.mean_buf[self.ptr] = mean\n        self.log_std_buf[self.ptr] = log_std\n        self.ptr += 1\n\n    def finish_path(self, last_val=0):\n        """"""\n        Call this at the end of a trajectory, or when one gets cut off\n        by an epoch ending. This looks back in the buffer to where the\n        trajectory started, and uses rewards and value estimates from\n        the whole trajectory to compute advantage estimates with GAE-lambda,\n        as well as compute the rewards-to-go for each state, to use as\n        the targets for the value function.\n\n        The ""last_val"" argument should be 0 if the trajectory ended\n        because the agent reached a terminal state (died), and otherwise\n        should be V(s_T), the value function estimated for the last state.\n        This allows us to bootstrap the reward-to-go calculation to account\n        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n        """"""\n        path_slice = slice(self.path_start_idx, self.ptr)\n        rews = np.append(self.rew_buf[path_slice], last_val)\n        vals = np.append(self.val_buf[path_slice], last_val)\n        # the next two lines implement GAE-lambda advantage calculation\n        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n        self.adv_buf[path_slice] = self._discount_cumsum(deltas, self.gamma * self.lam)\n\n        # the next line computes rewards-to-go, to be targets for the value function\n        self.ret_buf[path_slice] = self._discount_cumsum(rews, self.gamma)[:-1]\n\n        self.path_start_idx = self.ptr\n\n    def _discount_cumsum(self, x, discount):\n        """"""\n        magic from rllab for computing discounted cumulative sums of vectors.\n\n        input:\n            vector x,\n            [x0,\n             x1,\n             x2]\n\n        output:\n            [x0 + discount * x1 + discount^2 * x2,\n             x1 + discount * x2,\n             x2]\n        """"""\n        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n\n    def is_full(self):\n        return self.ptr == self.max_size\n\n    def get(self):\n        """"""\n        Call this at the end of an epoch to get all of the data from\n        the buffer, with advantages appropriately normalized (shifted to have\n        mean zero and std one). Also, resets some pointers in the buffer.\n        """"""\n        assert self.ptr == self.max_size  # buffer has to be full before you can get\n        self.ptr, self.path_start_idx = 0, 0\n\n        # the next two lines implement the advantage normalization trick\n        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n        return [self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf, self.mean_buf, self.log_std_buf]\n\n\n""""""\nTrust Region Policy Optimization \n""""""\n\n\nclass TRPO:\n    """"""\n    trpo class\n    """"""\n\n    def __init__(self, state_dim, action_dim, action_bound):\n        # critic\n        with tf.name_scope(\'critic\'):\n            layer = input_layer = tl.layers.Input([None, state_dim], tf.float32)\n            for d in HIDDEN_SIZES:\n                layer = tl.layers.Dense(d, tf.nn.relu)(layer)\n            v = tl.layers.Dense(1)(layer)\n        self.critic = tl.models.Model(input_layer, v)\n        self.critic.train()\n\n        # actor\n        with tf.name_scope(\'actor\'):\n            layer = input_layer = tl.layers.Input([None, state_dim], tf.float32)\n            for d in HIDDEN_SIZES:\n                layer = tl.layers.Dense(d, tf.nn.relu)(layer)\n            mean = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\n            mean = tl.layers.Lambda(lambda x: x * action_bound)(mean)\n            log_std = tf.Variable(np.zeros(action_dim, dtype=np.float32))\n\n        self.actor = tl.models.Model(input_layer, mean)\n        self.actor.trainable_weights.append(log_std)\n        self.actor.log_std = log_std\n        self.actor.train()\n\n        self.buf = GAE_Buffer(state_dim, action_dim, BATCH_SIZE, GAMMA, LAM)\n        self.critic_optimizer = tf.optimizers.Adam(learning_rate=VF_LR)\n        self.action_bound = action_bound\n\n    def get_action(self, state, greedy=False):\n        """"""\n        get action\n        :param state: state input\n        :param greedy: get action greedy or not\n        :return: pi, v, logp_pi, mean, log_std\n        """"""\n        state = np.array([state], np.float32)\n        mean = self.actor(state)\n        log_std = tf.convert_to_tensor(self.actor.log_std)\n        std = tf.exp(log_std)\n        std = tf.ones_like(mean) * std\n        pi = tfp.distributions.Normal(mean, std)\n\n        if greedy:\n            action = mean\n        else:\n            action = pi.sample()\n        action = np.clip(action, -self.action_bound, self.action_bound)\n        logp_pi = pi.log_prob(action)\n\n        value = self.critic(state)\n        return action[0], value, logp_pi, mean, log_std\n\n    def pi_loss(self, states, actions, adv, old_log_prob):\n        """"""\n        calculate pi loss\n        :param states: state batch\n        :param actions: action batch\n        :param adv: advantage batch\n        :param old_log_prob: old log probability\n        :return: pi loss\n        """"""\n        mean = self.actor(states)\n        pi = tfp.distributions.Normal(mean, tf.exp(self.actor.log_std))\n        log_prob = pi.log_prob(actions)[:, 0]\n        ratio = tf.exp(log_prob - old_log_prob)\n        surr = tf.reduce_mean(ratio * adv)\n        return -surr\n\n    def gradient(self, states, actions, adv, old_log_prob):\n        """"""\n        pi gradients\n        :param states: state batch\n        :param actions: actions batch\n        :param adv: advantage batch\n        :param old_log_prob: old log probability batch\n        :return: gradient\n        """"""\n        pi_params = self.actor.trainable_weights\n        with tf.GradientTape() as tape:\n            loss = self.pi_loss(states, actions, adv, old_log_prob)\n        grad = tape.gradient(loss, pi_params)\n        gradient = self._flat_concat(grad)\n        return gradient, loss\n\n    def train_vf(self, states, rewards_to_go):\n        """"""\n        train v function\n        :param states: state batch\n        :param rewards_to_go: rewards-to-go batch\n        :return: None\n        """"""\n        with tf.GradientTape() as tape:\n            value = self.critic(states)\n            loss = tf.reduce_mean((rewards_to_go - value[:, 0])**2)\n        grad = tape.gradient(loss, self.critic.trainable_weights)\n        self.critic_optimizer.apply_gradients(zip(grad, self.critic.trainable_weights))\n\n    def kl(self, states, old_mean, old_log_std):\n        """"""\n        calculate kl-divergence\n        :param states: state batch\n        :param old_mean: mean batch of the old pi\n        :param old_log_std: log std batch of the old pi\n        :return: kl_mean or None\n        """"""\n        old_mean = old_mean[:, np.newaxis]\n        old_log_std = old_log_std[:, np.newaxis]\n        old_std = tf.exp(old_log_std)\n        old_pi = tfp.distributions.Normal(old_mean, old_std)\n\n        mean = self.actor(states)\n        std = tf.exp(self.actor.log_std) * tf.ones_like(mean)\n        pi = tfp.distributions.Normal(mean, std)\n\n        kl = tfp.distributions.kl_divergence(pi, old_pi)\n        all_kls = tf.reduce_sum(kl, axis=1)\n        return tf.reduce_mean(all_kls)\n\n    def _flat_concat(self, xs):\n        """"""\n        flat concat input\n        :param xs: a list of tensor\n        :return: flat tensor\n        """"""\n        return tf.concat([tf.reshape(x, (-1, )) for x in xs], axis=0)\n\n    def get_pi_params(self):\n        """"""\n        get actor trainable parameters\n        :return: flat actor trainable parameters\n        """"""\n        pi_params = self.actor.trainable_weights\n        return self._flat_concat(pi_params)\n\n    def set_pi_params(self, flat_params):\n        """"""\n        set actor trainable parameters\n        :param flat_params: inputs\n        :return: None\n        """"""\n        pi_params = self.actor.trainable_weights\n        flat_size = lambda p: int(np.prod(p.shape.as_list()))  # the \'int\' is important for scalars\n        splits = tf.split(flat_params, [flat_size(p) for p in pi_params])\n        new_params = [tf.reshape(p_new, p.shape) for p, p_new in zip(pi_params, splits)]\n        return tf.group([p.assign(p_new) for p, p_new in zip(pi_params, new_params)])\n\n    def save(self):\n        """"""\n        save trained weights\n        :return: None\n        """"""\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        if not os.path.exists(path):\n            os.makedirs(path)\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'actor.hdf5\'), self.actor)\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'critic.hdf5\'), self.critic)\n\n    def load(self):\n        """"""\n        load trained weights\n        :return: None\n        """"""\n        path = os.path.join(\'model\', \'_\'.join([ALG_NAME, ENV_ID]))\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'actor.hdf5\'), self.actor)\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'critic.hdf5\'), self.critic)\n\n    def cg(self, Ax, b):\n        """"""\n        Conjugate gradient algorithm\n        (see https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n        """"""\n        x = np.zeros_like(b)\n        r = copy.deepcopy(b)  # Note: should be \'b - Ax(x)\', but for x=0, Ax(x)=0. Change if doing warm start.\n        p = copy.deepcopy(r)\n        r_dot_old = np.dot(r, r)\n        for _ in range(CG_ITERS):\n            z = Ax(p)\n            alpha = r_dot_old / (np.dot(p, z) + EPS)\n            x += alpha * p\n            r -= alpha * z\n            r_dot_new = np.dot(r, r)\n            p = r + (r_dot_new / r_dot_old) * p\n            r_dot_old = r_dot_new\n        return x\n\n    def hvp(self, states, old_mean, old_log_std, x):\n        """"""\n        calculate Hessian-vector product\n        :param states: state batch\n        :param old_mean: mean batch of the old pi\n        :param old_log_std: log std batch of the old pi\n        :return: hvp\n        """"""\n        pi_params = self.actor.trainable_weights\n        with tf.GradientTape() as tape1:\n            with tf.GradientTape() as tape0:\n                d_kl = self.kl(states, old_mean, old_log_std)\n            g = self._flat_concat(tape0.gradient(d_kl, pi_params))\n            l = tf.reduce_sum(g * x)\n        hvp = self._flat_concat(tape1.gradient(l, pi_params))\n\n        if DAMPING_COEFF > 0:\n            hvp += DAMPING_COEFF * x\n        return hvp\n\n    def update(self):\n        """"""\n        update trpo\n        :return: None\n        """"""\n        states, actions, adv, rewards_to_go, logp_old_ph, old_mu, old_log_std = self.buf.get()\n        g, pi_l_old = self.gradient(states, actions, adv, logp_old_ph)\n\n        Hx = lambda x: self.hvp(states, old_mu, old_log_std, x)\n        x = self.cg(Hx, g)\n\n        alpha = np.sqrt(2 * DELTA / (np.dot(x, Hx(x)) + EPS))\n        old_params = self.get_pi_params()\n\n        def set_and_eval(step):\n            params = old_params - alpha * x * step\n            self.set_pi_params(params)\n            d_kl = self.kl(states, old_mu, old_log_std)\n            loss = self.pi_loss(states, actions, adv, logp_old_ph)\n            return [d_kl, loss]\n\n        # trpo with backtracking line search, hard kl\n        for j in range(BACKTRACK_ITERS):\n            kl, pi_l_new = set_and_eval(step=BACKTRACK_COEFF**j)\n            if kl <= DELTA and pi_l_new <= pi_l_old:\n                # Accepting new params at step of line search\n                break\n        else:\n            # Line search failed! Keeping old params.\n            set_and_eval(step=0.)\n\n        # Value function updates\n        for _ in range(TRAIN_VF_ITERS):\n            self.train_vf(states, rewards_to_go)\n\n    def finish_path(self, done, next_state):\n        """"""\n        finish a trajectory\n        :param done: whether the epoch is done\n        :param next_state: next state\n        :return: None\n        """"""\n        if not done:\n            next_state = np.array([next_state], np.float32)\n            last_val = self.critic(next_state)\n        else:\n            last_val = 0\n        self.buf.finish_path(last_val)\n\n\nif __name__ == \'__main__\':\n    env = gym.make(ENV_ID).unwrapped\n\n    # reproducible\n    np.random.seed(RANDOM_SEED)\n    tf.random.set_seed(RANDOM_SEED)\n    env.seed(RANDOM_SEED)\n\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n    action_bound = env.action_space.high\n\n    agent = TRPO(state_dim, action_dim, action_bound)\n\n    t0 = time.time()\n    if args.train:  # train\n        all_episode_reward = []\n        for episode in range(TRAIN_EPISODES):\n            state = env.reset()\n            state = np.array(state, np.float32)\n            episode_reward = 0\n            for step in range(MAX_STEPS):\n                if RENDER:\n                    env.render()\n                action, value, logp, mean, log_std = agent.get_action(state)\n                next_state, reward, done, _ = env.step(action)\n                next_state = np.array(next_state, np.float32)\n                agent.buf.store(state, action, reward, value, logp, mean, log_std)\n                episode_reward += reward\n                state = next_state\n                if agent.buf.is_full():\n                    agent.finish_path(done, next_state)\n                    agent.update()\n                if done:\n                    break\n            agent.finish_path(done, next_state)\n            if episode == 0:\n                all_episode_reward.append(episode_reward)\n            else:\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\n            print(\n                \'Training  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    episode + 1, TRAIN_EPISODES, episode_reward,\n                    time.time() - t0\n                )\n            )\n            if episode % SAVE_FREQ == 0:\n                agent.save()\n        agent.save()\n        plt.plot(all_episode_reward)\n        if not os.path.exists(\'image\'):\n            os.makedirs(\'image\')\n        plt.savefig(os.path.join(\'image\', \'_\'.join([ALG_NAME, ENV_ID])))\n\n    if args.test:\n        # test\n        agent.load()\n        for episode in range(TEST_EPISODES):\n            state = env.reset()\n            episode_reward = 0\n            for step in range(MAX_STEPS):\n                env.render()\n                action, *_ = agent.get_action(state, greedy=True)\n                state, reward, done, info = env.step(action)\n                episode_reward += reward\n                if done:\n                    break\n            print(\n                \'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\n                    episode + 1, TEST_EPISODES, episode_reward,\n                    time.time() - t0\n                )\n            )\n'"
examples/reinforcement_learning/tutorial_atari_pong.py,4,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""Monte-Carlo Policy Network \xcf\x80(a|s)  (REINFORCE).\nTo understand Reinforcement Learning, we let computer to learn how to play\nPong game from the original screen inputs. Before we start, we highly recommend\nyou to go through a famous blog called \xe2\x80\x9cDeep Reinforcement Learning: Pong from\nPixels\xe2\x80\x9d which is a minimalistic implementation of deep reinforcement learning by\nusing python-numpy and OpenAI gym environment.\nThe code here is the reimplementation of Karpathy\'s Blog by using TensorLayer.\nCompare with Karpathy\'s code, we store observation for a batch, but he store\nobservation for only one episode and gradients. (so we will use\nmore memory if the observation is very large.)\n\nTODO\n-----\n- update grads every step rather than storing all observation!\n- tensorlayer@gmail.com\n\nReferences\n------------\n- http://karpathy.github.io/2016/05/31/rl/\n""""""\nimport time\n\nimport gym\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# hyper-parameters\nimage_size = 80\nD = image_size * image_size\nH = 200\nbatch_size = 10\nlearning_rate = 1e-4\ngamma = 0.99\ndecay_rate = 0.99\nrender = False  # display the game environment\n# resume = True         # load existing policy network\nmodel_file_name = ""model_pong""\nnp.set_printoptions(threshold=np.inf)\n\n\ndef prepro(I):\n    """"""Prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector.""""""\n    I = I[35:195]\n    I = I[::2, ::2, 0]\n    I[I == 144] = 0\n    I[I == 109] = 0\n    I[I != 0] = 1\n    return I.astype(np.float32).ravel()\n\n\nenv = gym.make(""Pong-v0"")\nobservation = env.reset()\nprev_x = None\nrunning_reward = None\nreward_sum = 0\nepisode_number = 0\n\nxs, ys, rs = [], [], []\n\n\n# policy network\ndef get_model(inputs_shape):\n    ni = tl.layers.Input(inputs_shape)\n    nn = tl.layers.Dense(n_units=H, act=tf.nn.relu, name=\'hidden\')(ni)\n    nn = tl.layers.Dense(n_units=3, name=\'output\')(nn)\n    M = tl.models.Model(inputs=ni, outputs=nn, name=""mlp"")\n    return M\n\n\nmodel = get_model([None, D])\ntrain_weights = model.trainable_weights\n\noptimizer = tf.optimizers.RMSprop(lr=learning_rate, decay=decay_rate)\n\nmodel.train()  # set model to train mode (in case you add dropout into the model)\n\nstart_time = time.time()\ngame_number = 0\nwhile True:\n    if render:\n        env.render()\n\n    cur_x = prepro(observation)\n    x = cur_x - prev_x if prev_x is not None else np.zeros(D, dtype=np.float32)\n    x = x.reshape(1, D)\n    prev_x = cur_x\n\n    _prob = model(x)\n    prob = tf.nn.softmax(_prob)\n\n    # action. 1: STOP  2: UP  3: DOWN\n    # action = np.random.choice([1,2,3], p=prob.flatten())\n    # action = tl.rein.choice_action_by_probs(prob.flatten(), [1, 2, 3])\n    action = tl.rein.choice_action_by_probs(prob[0].numpy(), [1, 2, 3])\n\n    observation, reward, done, _ = env.step(action)\n    reward_sum += reward\n    xs.append(x)  # all observations in an episode\n    ys.append(action - 1)  # all fake labels in an episode (action begins from 1, so minus 1)\n    rs.append(reward)  # all rewards in an episode\n\n    if done:\n        episode_number += 1\n        game_number = 0\n\n        if episode_number % batch_size == 0:\n            print(\'batch over...... updating parameters......\')\n            epx = np.vstack(xs)\n            epy = np.asarray(ys)\n            epr = np.asarray(rs)\n            disR = tl.rein.discount_episode_rewards(epr, gamma)\n            disR -= np.mean(disR)\n            disR /= np.std(disR)\n\n            xs, ys, rs = [], [], []\n\n            with tf.GradientTape() as tape:\n                _prob = model(epx)\n                _loss = tl.rein.cross_entropy_reward_loss(_prob, epy, disR)\n            grad = tape.gradient(_loss, train_weights)\n            optimizer.apply_gradients(zip(grad, train_weights))\n\n        ## TODO\n        # if episode_number % (batch_size * 100) == 0:\n        #     tl.files.save_npz(network.all_params, name=model_file_name + \'.npz\')\n\n        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n        print(\'resetting env. episode reward total was {}. running mean: {}\'.format(reward_sum, running_reward))\n        reward_sum = 0\n        observation = env.reset()  # reset env\n        prev_x = None\n\n    if reward != 0:\n        print(\n            (\n                \'episode %d: game %d took %.5fs, reward: %f\' %\n                (episode_number, game_number, time.time() - start_time, reward)\n            ), (\'\' if reward == -1 else \' !!!!!!!!\')\n        )\n        start_time = time.time()\n        game_number += 1\n'"
examples/reinforcement_learning/tutorial_format.py,1,"b""# the format of turorial algorithm #\n# please heavily annotate the code #\n'''\nAlgorithm Name\n------------------------\nBriefly describe the algorithms, add some details.\n\nReference\n---------\noriginal paper: e.g. https://arxiv.org/pdf/1802.09477.pdf\nwebsite: ...\n\n\nEnvironment\n-----------\ne.g. Openai Gym Pendulum-v0, continuous action space\n\nPrerequisites\n---------------\ntensorflow >=2.0.0a0\ntensorlayer >=2.0.0\n...\n\nTo run\n-------\npython tutorial_***.py --train/test\n\n'''\n\nimport argparse\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\n# import 'other package name'\n\nnp.random.seed(2)\ntf.random.set_seed(2)  # reproducible\n\n# add arguments in command  --train/test\nparser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\nparser.add_argument('--train', dest='train', action='store_true', default=False)\nparser.add_argument('--test', dest='test', action='store_true', default=True)\nargs = parser.parse_args()\n\n#####################  hyper parameters  ####################\nA = a  # description of hyper parameter\nB = b  # description of hyper parameter\n\n###############################  Algorithm Name  ####################################\n\n\nclass C():  # algorithm-specific classes\n    ''' description of class '''\n\n    def C1():\n        ''' description of function'''\n\n\ndef D():  # some common functions, could be extracted into utils afterwards\n    ''' description of function '''\n\n\nif __name__ == '__main__':\n    '''initialization of env, buffer, networks in algorithms'''\n    env = 'env model'\n    buffer = 'buffer model'\n    network1 = 'network model1'\n    network2 = 'network model2'\n\n    # training loop\n    if args.train:\n        t0 = time.time()\n        while NOT_FINISHED:  # loop of episodes\n            while NOT_DONE:  # loop of steps in episode\n                ''' step '''\n                ''' train '''\n\n            print('Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'\\\n            .format(episode, all_episodes, episode_reward, time.time()-t0 ))\n        ''' plot , following the format of ./baselines/utils/plot()'''\n        plot(rewards, Algorithm_name='SAC', Env_name='Pendulum-v0')\n        ''' save weights, implemented in defined classes above, following the format of ./baselines/utils/save_model()  '''\n        model.save_weights()\n\n    # testing loop\n    if args.test:\n        t0 = time.time()\n        ''' save weights, implemented in defined classes above, following the format of ./baselines/utils/load_model()  '''\n        model.load_weights()\n\n        while NOT_FINISHED:  # loop of episodes\n            while NOT_DONE:  # loop of steps in episode\n                ''' step '''\n\n            print('Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'\\\n            .format(episode, all_episodes, episode_reward, time.time()-t0 ) )\n"""
examples/reinforcement_learning/tutorial_prioritized_replay.py,21,"b'""""""\r\nPrioritized Experience Replay\r\n------------------------\r\nPrioritized experience replay is an efficient replay method that replay\r\nimportant transitions more frequently. Segment tree data structure is used to\r\nspeed up indexing.\r\nReference:\r\n------------------------\r\nSchaul T, Quan J, Antonoglou I, et al. Prioritized experience replay[J]. arXiv\r\npreprint arXiv:1511.05952, 2015.\r\nDhariwal P, Hesse C, Klimov O, et al. Openai baselines (2017)[J]. URL\r\nhttps://github. com/opfenai/baselines.\r\nEnvironment:\r\n------------------------\r\nCartpole and Pong in OpenAI Gym\r\nRequirements:\r\n------------------------\r\ntensorflow>=2.0.0a0\r\ntensorlayer>=2.0.0\r\nTo run:\r\n------------------------\r\npython tutorial_prioritized_replay.py --mode=train\r\npython tutorial_prioritized_replay.py --mode=test --save_path=per/8000.npz\r\n""""""\r\nimport argparse\r\nimport operator\r\nimport os\r\nimport random\r\nimport time\r\n\r\nimport gym\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorlayer as tl\r\n\r\nparser = argparse.ArgumentParser()\r\n# add arguments in command  --train/test\r\nparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', default=True)\r\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\', default=True)\r\nparser.add_argument(\r\n    \'--save_path\', default=None, help=\'folder to save if mode == train else model path,\'\r\n    \'qnet will be saved once target net update\'\r\n)\r\nparser.add_argument(\'--seed\', help=\'random seed\', type=int, default=0)\r\nparser.add_argument(\'--env_id\', default=\'CartPole-v0\', help=\'CartPole-v0 or PongNoFrameskip-v4\')\r\nargs = parser.parse_args()\r\n\r\nrandom.seed(args.seed)\r\nnp.random.seed(args.seed)\r\ntf.random.set_seed(args.seed)  # reproducible\r\nenv_id = args.env_id\r\nenv = gym.make(env_id)\r\nenv.seed(args.seed)\r\nalg_name = \'prioritized_replay\'\r\n\r\n# ####################  hyper parameters  ####################\r\nif env_id == \'CartPole-v0\':\r\n    qnet_type = \'MLP\'\r\n    number_timesteps = 10000  # total number of time steps to train on\r\n    explore_timesteps = 100\r\n    # epsilon-greedy schedule, final exploit prob is 0.99\r\n    epsilon = lambda i_iter: 1 - 0.99 * min(1, i_iter / explore_timesteps)\r\n    lr = 5e-3  # learning rate\r\n    buffer_size = 1000  # replay buffer size\r\n    target_q_update_freq = 50  # how frequency target q net update\r\n    ob_scale = 1.0  # scale observations\r\n    clipnorm = None\r\nelse:\r\n    # reward will increase obviously after 1e5 time steps\r\n    qnet_type = \'CNN\'\r\n    number_timesteps = int(1e6)  # total number of time steps to train on\r\n    explore_timesteps = 1e5\r\n    # epsilon-greedy schedule, final exploit prob is 0.99\r\n    epsilon = lambda i_iter: 1 - 0.99 * min(1, i_iter / explore_timesteps)\r\n    lr = 1e-4  # learning rate\r\n    buffer_size = 10000  # replay buffer size\r\n    target_q_update_freq = 200  # how frequency target q net update\r\n    ob_scale = 1.0 / 255  # scale observations\r\n    clipnorm = 10\r\n\r\nin_dim = env.observation_space.shape\r\nout_dim = env.action_space.n\r\nreward_gamma = 0.99  # reward discount\r\nbatch_size = 32  # batch size for sampling from replay buffer\r\nwarm_start = buffer_size / 10  # sample times befor learning\r\nprioritized_replay_alpha = 0.6  # alpha in PER\r\nprioritized_replay_beta0 = 0.4  # initial beta in PER\r\n\r\n\r\n# ##############################  Network  ####################################\r\nclass MLP(tl.models.Model):\r\n\r\n    def __init__(self, name):\r\n        super(MLP, self).__init__(name=name)\r\n        self.h1 = tl.layers.Dense(64, tf.nn.tanh, in_channels=in_dim[0])\r\n        self.qvalue = tl.layers.Dense(out_dim, in_channels=64, name=\'q\', W_init=tf.initializers.GlorotUniform())\r\n\r\n    def forward(self, ni):\r\n        return self.qvalue(self.h1(ni))\r\n\r\n\r\nclass CNN(tl.models.Model):\r\n\r\n    def __init__(self, name):\r\n        super(CNN, self).__init__(name=name)\r\n        h, w, in_channels = in_dim\r\n        dense_in_channels = 64 * ((h - 28) // 8) * ((w - 28) // 8)\r\n        self.conv1 = tl.layers.Conv2d(\r\n            32, (8, 8), (4, 4), tf.nn.relu, \'VALID\', in_channels=in_channels, name=\'conv2d_1\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.conv2 = tl.layers.Conv2d(\r\n            64, (4, 4), (2, 2), tf.nn.relu, \'VALID\', in_channels=32, name=\'conv2d_2\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.conv3 = tl.layers.Conv2d(\r\n            64, (3, 3), (1, 1), tf.nn.relu, \'VALID\', in_channels=64, name=\'conv2d_3\',\r\n            W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.flatten = tl.layers.Flatten(name=\'flatten\')\r\n        self.preq = tl.layers.Dense(\r\n            256, tf.nn.relu, in_channels=dense_in_channels, name=\'pre_q\', W_init=tf.initializers.GlorotUniform()\r\n        )\r\n        self.qvalue = tl.layers.Dense(out_dim, in_channels=256, name=\'q\', W_init=tf.initializers.GlorotUniform())\r\n\r\n    def forward(self, ni):\r\n        feature = self.flatten(self.conv3(self.conv2(self.conv1(ni))))\r\n        return self.qvalue(self.preq(feature))\r\n\r\n\r\n# ##############################  Replay  ####################################\r\nclass SegmentTree(object):\r\n\r\n    def __init__(self, capacity, operation, neutral_element):\r\n        """"""Build a Segment Tree data structure.\r\n        https://en.wikipedia.org/wiki/Segment_tree\r\n        Can be used as regular array, but with two\r\n        important differences:\r\n            a) setting item\'s value is slightly slower.\r\n               It is O(lg capacity) instead of O(1).\r\n            b) user has access to an efficient ( O(log segment size) )\r\n               `reduce` operation which reduces `operation` over\r\n               a contiguous subsequence of items in the array.\r\n        Paramters\r\n        ---------\r\n        capacity: int\r\n            Total size of the array - must be a power of two.\r\n        operation: lambda obj, obj -> obj\r\n            and operation for combining elements (eg. sum, max)\r\n            must form a mathematical group together with the set of\r\n            possible values for array elements (i.e. be associative)\r\n        neutral_element: obj\r\n            neutral element for the operation above. eg. float(\'-inf\')\r\n            for max and 0 for sum.\r\n        """"""\r\n        assert capacity > 0 and capacity & (capacity - 1) == 0, \\\r\n            ""capacity must be positive and a power of 2.""\r\n        self._capacity = capacity\r\n        self._value = [neutral_element for _ in range(2 * capacity)]\r\n        self._operation = operation\r\n\r\n    def _reduce_helper(self, start, end, node, node_start, node_end):\r\n        if start == node_start and end == node_end:\r\n            return self._value[node]\r\n        mid = (node_start + node_end) // 2\r\n        if end <= mid:\r\n            return self._reduce_helper(start, end, 2 * node, node_start, mid)\r\n        else:\r\n            if mid + 1 <= start:\r\n                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\r\n            else:\r\n                return self._operation(\r\n                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\r\n                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\r\n                )\r\n\r\n    def reduce(self, start=0, end=None):\r\n        """"""Returns result of applying `self.operation`\r\n        to a contiguous subsequence of the array.\r\n        Parameters\r\n        ----------\r\n        start: int\r\n            beginning of the subsequence\r\n        end: int\r\n            end of the subsequences\r\n        Returns\r\n        -------\r\n        reduced: obj\r\n            result of reducing self.operation over the specified range of array.\r\n        """"""\r\n        if end is None:\r\n            end = self._capacity\r\n        if end < 0:\r\n            end += self._capacity\r\n        end -= 1\r\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\r\n\r\n    def __setitem__(self, idx, val):\r\n        # index of the leaf\r\n        idx += self._capacity\r\n        self._value[idx] = val\r\n        idx //= 2\r\n        while idx >= 1:\r\n            self._value[idx] = self._operation(self._value[2 * idx], self._value[2 * idx + 1])\r\n            idx //= 2\r\n\r\n    def __getitem__(self, idx):\r\n        assert 0 <= idx < self._capacity\r\n        return self._value[self._capacity + idx]\r\n\r\n\r\nclass SumSegmentTree(SegmentTree):\r\n\r\n    def __init__(self, capacity):\r\n        super(SumSegmentTree, self).__init__(capacity=capacity, operation=operator.add, neutral_element=0.0)\r\n\r\n    def sum(self, start=0, end=None):\r\n        """"""Returns arr[start] + ... + arr[end]""""""\r\n        return super(SumSegmentTree, self).reduce(start, end)\r\n\r\n    def find_prefixsum_idx(self, prefixsum):\r\n        """"""Find the highest index `i` in the array such that\r\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\r\n        if array values are probabilities, this function\r\n        allows to sample indexes according to the discrete\r\n        probability efficiently.\r\n        Parameters\r\n        ----------\r\n        perfixsum: float\r\n            upperbound on the sum of array prefix\r\n        Returns\r\n        -------\r\n        idx: int\r\n            highest index satisfying the prefixsum constraint\r\n        """"""\r\n        assert 0 <= prefixsum <= self.sum() + 1e-5\r\n        idx = 1\r\n        while idx < self._capacity:  # while non-leaf\r\n            if self._value[2 * idx] > prefixsum:\r\n                idx = 2 * idx\r\n            else:\r\n                prefixsum -= self._value[2 * idx]\r\n                idx = 2 * idx + 1\r\n        return idx - self._capacity\r\n\r\n\r\nclass MinSegmentTree(SegmentTree):\r\n\r\n    def __init__(self, capacity):\r\n        super(MinSegmentTree, self).__init__(capacity=capacity, operation=min, neutral_element=float(\'inf\'))\r\n\r\n    def min(self, start=0, end=None):\r\n        """"""Returns min(arr[start], ...,  arr[end])""""""\r\n\r\n        return super(MinSegmentTree, self).reduce(start, end)\r\n\r\n\r\nclass ReplayBuffer(object):\r\n\r\n    def __init__(self, size):\r\n        self._storage = []\r\n        self._maxsize = size\r\n        self._next_idx = 0\r\n\r\n    def __len__(self):\r\n        return len(self._storage)\r\n\r\n    def add(self, *args):\r\n        if self._next_idx >= len(self._storage):\r\n            self._storage.append(args)\r\n        else:\r\n            self._storage[self._next_idx] = args\r\n        self._next_idx = (self._next_idx + 1) % self._maxsize\r\n\r\n    def _encode_sample(self, idxes):\r\n        b_o, b_a, b_r, b_o_, b_d = [], [], [], [], []\r\n        for i in idxes:\r\n            o, a, r, o_, d = self._storage[i]\r\n            b_o.append(o)\r\n            b_a.append(a)\r\n            b_r.append(r)\r\n            b_o_.append(o_)\r\n            b_d.append(d)\r\n        return (\r\n            np.stack(b_o).astype(\'float32\') * ob_scale,\r\n            np.stack(b_a).astype(\'int32\'),\r\n            np.stack(b_r).astype(\'float32\'),\r\n            np.stack(b_o_).astype(\'float32\') * ob_scale,\r\n            np.stack(b_d).astype(\'float32\'),\r\n        )\r\n\r\n    def sample(self, batch_size):\r\n        indexes = range(len(self._storage))\r\n        idxes = [random.choice(indexes) for _ in range(batch_size)]\r\n        return self._encode_sample(idxes)\r\n\r\n\r\nclass PrioritizedReplayBuffer(ReplayBuffer):\r\n\r\n    def __init__(self, size, alpha, beta):\r\n        """"""Create Prioritized Replay buffer.\r\n        Parameters\r\n        ----------\r\n        size: int\r\n            Max number of transitions to store in the buffer. When the buffer\r\n            overflows the old memories are dropped.\r\n        alpha: float\r\n            how much prioritization is used\r\n            (0 - no prioritization, 1 - full prioritization)\r\n        See Also\r\n        --------\r\n        ReplayBuffer.__init__\r\n        """"""\r\n        super(PrioritizedReplayBuffer, self).__init__(size)\r\n        assert alpha >= 0\r\n        self._alpha = alpha\r\n\r\n        it_capacity = 1\r\n        while it_capacity < size:\r\n            it_capacity *= 2\r\n\r\n        self._it_sum = SumSegmentTree(it_capacity)\r\n        self._it_min = MinSegmentTree(it_capacity)\r\n        self._max_priority = 1.0\r\n        self.beta = beta\r\n\r\n    def add(self, *args):\r\n        """"""See ReplayBuffer.store_effect""""""\r\n        idx = self._next_idx\r\n        super().add(*args)\r\n        self._it_sum[idx] = self._max_priority**self._alpha\r\n        self._it_min[idx] = self._max_priority**self._alpha\r\n\r\n    def _sample_proportional(self, batch_size):\r\n        res = []\r\n        p_total = self._it_sum.sum(0, len(self._storage) - 1)\r\n        every_range_len = p_total / batch_size\r\n        for i in range(batch_size):\r\n            mass = random.random() * every_range_len + i * every_range_len\r\n            idx = self._it_sum.find_prefixsum_idx(mass)\r\n            res.append(idx)\r\n        return res\r\n\r\n    def sample(self, batch_size):\r\n        """"""Sample a batch of experiences""""""\r\n        idxes = self._sample_proportional(batch_size)\r\n\r\n        it_sum = self._it_sum.sum()\r\n        p_min = self._it_min.min() / it_sum\r\n        max_weight = (p_min * len(self._storage))**(-self.beta)\r\n\r\n        p_samples = np.asarray([self._it_sum[idx] for idx in idxes]) / it_sum\r\n        weights = (p_samples * len(self._storage))**(-self.beta) / max_weight\r\n        encoded_sample = self._encode_sample(idxes)\r\n        return encoded_sample + (weights.astype(\'float32\'), idxes)\r\n\r\n    def update_priorities(self, idxes, priorities):\r\n        """"""Update priorities of sampled transitions""""""\r\n        assert len(idxes) == len(priorities)\r\n        for idx, priority in zip(idxes, priorities):\r\n            assert priority > 0\r\n            assert 0 <= idx < len(self._storage)\r\n            self._it_sum[idx] = priority**self._alpha\r\n            self._it_min[idx] = priority**self._alpha\r\n\r\n            self._max_priority = max(self._max_priority, priority)\r\n\r\n\r\n# #############################  Functions  ###################################\r\ndef huber_loss(x):\r\n    """"""Loss function for value""""""\r\n    return tf.where(tf.abs(x) < 1, tf.square(x) * 0.5, tf.abs(x) - 0.5)\r\n\r\n\r\ndef sync(net, net_tar):\r\n    """"""Copy q network to target q network""""""\r\n    for var, var_tar in zip(net.trainable_weights, net_tar.trainable_weights):\r\n        var_tar.assign(var)\r\n\r\n\r\n# ###############################  DQN  #####################################\r\nclass DQN(object):\r\n\r\n    def __init__(self):\r\n        model = MLP if qnet_type == \'MLP\' else CNN\r\n        self.qnet = model(\'q\')\r\n        if args.train:\r\n            self.qnet.train()\r\n            self.targetqnet = model(\'targetq\')\r\n            self.targetqnet.infer()\r\n            sync(self.qnet, self.targetqnet)\r\n        else:\r\n            self.qnet.infer()\r\n            self.load(args.save_path)\r\n        self.niter = 0\r\n        if clipnorm is not None:\r\n            self.optimizer = tf.optimizers.Adam(learning_rate=lr, clipnorm=clipnorm)\r\n        else:\r\n            self.optimizer = tf.optimizers.Adam(learning_rate=lr)\r\n\r\n    def get_action(self, obv):\r\n        eps = epsilon(self.niter)\r\n        if args.train and random.random() < eps:\r\n            return int(random.random() * out_dim)\r\n        else:\r\n            obv = np.expand_dims(obv, 0).astype(\'float32\') * ob_scale\r\n            return self._qvalues_func(obv).numpy().argmax(1)[0]\r\n\r\n    @tf.function\r\n    def _qvalues_func(self, obv):\r\n        return self.qnet(obv)\r\n\r\n    def train(self, b_o, b_a, b_r, b_o_, b_d, weights=None):\r\n        if weights is None:\r\n            weights = np.ones_like(b_r)\r\n        td_errors = self._train_func(b_o, b_a, b_r, b_o_, b_d, weights)\r\n\r\n        self.niter += 1\r\n        if self.niter % target_q_update_freq == 0:\r\n            sync(self.qnet, self.targetqnet)\r\n            self.save(args.save_path)\r\n        return td_errors.numpy()\r\n\r\n    def save(self, path):\r\n        if path is None:\r\n            path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        tl.files.save_weights_to_hdf5(os.path.join(path, \'q_net.hdf5\'), self.qnet)\r\n\r\n    def load(self, path):\r\n        if path is None:\r\n            path = os.path.join(\'model\', \'_\'.join([alg_name, env_id]))\r\n        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, \'q_net.hdf5\'), self.qnet)\r\n\r\n    @tf.function\r\n    def _train_func(self, b_o, b_a, b_r, b_o_, b_d, weights):\r\n        with tf.GradientTape() as tape:\r\n            td_errors = self._tderror_func(b_o, b_a, b_r, b_o_, b_d)\r\n            loss = tf.reduce_mean(huber_loss(td_errors) * weights)\r\n\r\n        grad = tape.gradient(loss, self.qnet.trainable_weights)\r\n        self.optimizer.apply_gradients(zip(grad, self.qnet.trainable_weights))\r\n\r\n        return td_errors\r\n\r\n    @tf.function\r\n    def _tderror_func(self, b_o, b_a, b_r, b_o_, b_d):\r\n        b_q_ = (1 - b_d) * tf.reduce_max(self.targetqnet(b_o_), 1)\r\n        b_q = tf.reduce_sum(self.qnet(b_o) * tf.one_hot(b_a, out_dim), 1)\r\n        return b_q - (b_r + reward_gamma * b_q_)\r\n\r\n\r\n# #############################  Trainer  ###################################\r\nif __name__ == \'__main__\':\r\n    dqn = DQN()\r\n    t0 = time.time()\r\n    if args.train:\r\n        buffer = PrioritizedReplayBuffer(buffer_size, prioritized_replay_alpha, prioritized_replay_beta0)\r\n        nepisode = 0\r\n        all_episode_reward = []\r\n        for i in range(1, number_timesteps + 1):\r\n            o = env.reset()\r\n            episode_reward = 0\r\n            while True:\r\n                buffer.beta += (1 - prioritized_replay_beta0) / number_timesteps\r\n\r\n                a = dqn.get_action(o)\r\n\r\n                # execute action and feed to replay buffer\r\n                # note that `_` tail in var name means next\r\n                o_, r, done, info = env.step(a)\r\n                buffer.add(o, a, r, o_, done)\r\n                episode_reward += r\r\n\r\n                if i >= warm_start:\r\n                    *transitions, idxs = buffer.sample(batch_size)\r\n                    priorities = dqn.train(*transitions)\r\n                    priorities = np.clip(np.abs(priorities), 1e-6, None)\r\n                    buffer.update_priorities(idxs, priorities)\r\n\r\n                if done:\r\n                    break\r\n                else:\r\n                    o = o_\r\n\r\n            if nepisode == 0:\r\n                all_episode_reward.append(episode_reward)\r\n            else:\r\n                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\r\n            nepisode += 1\r\n            print(\r\n                \'Training  | Episode: {}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    nepisode, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )  # episode num starts from 1 in print\r\n\r\n        dqn.save(args.save_path)\r\n        plt.plot(all_episode_reward)\r\n        if not os.path.exists(\'image\'):\r\n            os.makedirs(\'image\')\r\n        plt.savefig(os.path.join(\'image\', \'_\'.join([alg_name, env_id])))\r\n\r\n    if args.test:\r\n        nepisode = 0\r\n        for i in range(1, number_timesteps + 1):\r\n            o = env.reset()\r\n            episode_reward = 0\r\n            while True:\r\n                env.render()\r\n                a = dqn.get_action(o)\r\n                o_, r, done, info = env.step(a)\r\n                episode_reward += r\r\n                if done:\r\n                    break\r\n                else:\r\n                    o = o_\r\n            nepisode += 1\r\n            print(\r\n                \'Testing  | Episode: {}  | Episode Reward: {:.4f}  | Running Time: {:.4f}\'.format(\r\n                    nepisode, episode_reward,\r\n                    time.time() - t0\r\n                )\r\n            )\r\n'"
examples/reinforcement_learning/tutorial_wrappers.py,0,"b'""""""Env wrappers\nNote that this file is adapted from `https://pypi.org/project/gym-vec-env` and\n`https://github.com/openai/baselines/blob/master/baselines/common/*wrappers.py`\n""""""\nfrom collections import deque\nfrom functools import partial\nfrom multiprocessing import Pipe, Process, cpu_count\nfrom sys import platform\n\nimport cv2\nimport gym\nimport numpy as np\nfrom gym import spaces\n\n__all__ = (\n    \'build_env\',  # build env\n    \'TimeLimit\',  # Time limit wrapper\n    \'NoopResetEnv\',  # Run random number of no-ops on reset\n    \'FireResetEnv\',  # Reset wrapper for envs with fire action\n    \'EpisodicLifeEnv\',  # end-of-life == end-of-episode wrapper\n    \'MaxAndSkipEnv\',  # skip frame wrapper\n    \'ClipRewardEnv\',  # clip reward wrapper\n    \'WarpFrame\',  # warp observation wrapper\n    \'FrameStack\',  # stack frame wrapper\n    \'LazyFrames\',  # lazy store wrapper\n    \'RewardScaler\',  # reward scale\n    \'SubprocVecEnv\',  # vectorized env wrapper\n    \'VecFrameStack\',  # stack frames in vectorized env\n    \'Monitor\',  # Episode reward and length monitor\n)\ncv2.ocl.setUseOpenCL(False)\n# env_id -> env_type\nid2type = dict()\nfor _env in gym.envs.registry.all():\n    id2type[_env.id] = _env._entry_point.split(\':\')[0].rsplit(\'.\', 1)[1]\n\n\ndef build_env(env_id, vectorized=False, seed=0, reward_scale=1.0, nenv=0):\n    """"""Build env based on options""""""\n    env_type = id2type[env_id]\n    nenv = nenv or cpu_count() // (1 + (platform == \'darwin\'))\n    stack = env_type == \'atari\'\n    if not vectorized:\n        env = _make_env(env_id, env_type, seed, reward_scale, stack)\n    else:\n        env = _make_vec_env(env_id, env_type, nenv, seed, reward_scale, stack)\n\n    return env\n\n\ndef _make_env(env_id, env_type, seed, reward_scale, frame_stack=True):\n    """"""Make single env""""""\n    if env_type == \'atari\':\n        env = gym.make(env_id)\n        assert \'NoFrameskip\' in env.spec.id\n        env = NoopResetEnv(env, noop_max=30)\n        env = MaxAndSkipEnv(env, skip=4)\n        env = Monitor(env)\n        # deepmind wrap\n        env = EpisodicLifeEnv(env)\n        if \'FIRE\' in env.unwrapped.get_action_meanings():\n            env = FireResetEnv(env)\n        env = WarpFrame(env)\n        env = ClipRewardEnv(env)\n        if frame_stack:\n            env = FrameStack(env, 4)\n    elif env_type == \'classic_control\':\n        env = Monitor(gym.make(env_id))\n    else:\n        raise NotImplementedError\n    if reward_scale != 1:\n        env = RewardScaler(env, reward_scale)\n    env.seed(seed)\n    return env\n\n\ndef _make_vec_env(env_id, env_type, nenv, seed, reward_scale, frame_stack=True):\n    """"""Make vectorized env""""""\n    env = SubprocVecEnv([partial(_make_env, env_id, env_type, seed + i, reward_scale, False) for i in range(nenv)])\n    if frame_stack:\n        env = VecFrameStack(env, 4)\n    return env\n\n\nclass TimeLimit(gym.Wrapper):\n\n    def __init__(self, env, max_episode_steps=None):\n        super(TimeLimit, self).__init__(env)\n        self._max_episode_steps = max_episode_steps\n        self._elapsed_steps = 0\n\n    def step(self, ac):\n        observation, reward, done, info = self.env.step(ac)\n        self._elapsed_steps += 1\n        if self._elapsed_steps >= self._max_episode_steps:\n            done = True\n            info[\'TimeLimit.truncated\'] = True\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        self._elapsed_steps = 0\n        return self.env.reset(**kwargs)\n\n\nclass NoopResetEnv(gym.Wrapper):\n\n    def __init__(self, env, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        super(NoopResetEnv, self).__init__(env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass FireResetEnv(gym.Wrapper):\n\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        super(FireResetEnv, self).__init__(env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        super(EpisodicLifeEnv, self).__init__(env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if 0 < lives < self.lives:\n            # for Qbert sometimes we stay in lives == 0 condition for a few\n            # frames so it\'s important to keep lives > 0, so that we only reset\n            # once the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        super(MaxAndSkipEnv, self).__init__(env)\n        # most recent raw observations (for max pooling across time steps)\n        shape = (2, ) + env.observation_space.shape\n        self._obs_buffer = np.zeros(shape, dtype=np.uint8)\n        self._skip = skip\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = info = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n\n    def __init__(self, env):\n        super(ClipRewardEnv, self).__init__(env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\n\nclass WarpFrame(gym.ObservationWrapper):\n\n    def __init__(self, env, width=84, height=84, grayscale=True):\n        """"""Warp frames to 84x84 as done in the Nature paper and later work.""""""\n        super(WarpFrame, self).__init__(env)\n        self.width = width\n        self.height = height\n        self.grayscale = grayscale\n        shape = (self.height, self.width, 1 if self.grayscale else 3)\n        self.observation_space = spaces.Box(low=0, high=255, shape=shape, dtype=np.uint8)\n\n    def observation(self, frame):\n        if self.grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        size = (self.width, self.height)\n        frame = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n        if self.grayscale:\n            frame = np.expand_dims(frame, -1)\n        return frame\n\n\nclass FrameStack(gym.Wrapper):\n\n    def __init__(self, env, k):\n        """"""Stack k last frames.\n        Returns lazy array, which is much more memory efficient.\n        See Also `LazyFrames`\n        """"""\n        super(FrameStack, self).__init__(env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        shape = shp[:-1] + (shp[-1] * k, )\n        self.observation_space = spaces.Box(low=0, high=255, shape=shape, dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return np.asarray(self._get_ob())\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return np.asarray(self._get_ob()), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\nclass LazyFrames(object):\n\n    def __init__(self, frames):\n        """"""This object ensures that common frames between the observations are\n        only stored once. It exists purely to optimize memory usage which can be\n        huge for DQN\'s 1M frames replay buffers.\n\n        This object should only be converted to numpy array before being passed\n        to the model. You\'d not believe how complex the previous solution was.\n        """"""\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\n\nclass RewardScaler(gym.RewardWrapper):\n    """"""Bring rewards to a reasonable scale for PPO.\n    This is incredibly important and effects performance drastically.\n    """"""\n\n    def __init__(self, env, scale=0.01):\n        super(RewardScaler, self).__init__(env)\n        self.scale = scale\n\n    def reward(self, reward):\n        return reward * self.scale\n\n\nclass VecFrameStack(object):\n\n    def __init__(self, env, k):\n        self.env = env\n        self.k = k\n        self.action_space = env.action_space\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        shape = shp[:-1] + (shp[-1] * k, )\n        self.observation_space = spaces.Box(low=0, high=255, shape=shape, dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return np.asarray(self._get_ob())\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return np.asarray(self._get_ob()), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\ndef _worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.x()\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            remote.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            remote.send(ob)\n        elif cmd == \'reset_task\':\n            ob = env._reset_task()\n            remote.send(ob)\n        elif cmd == \'close\':\n            remote.close()\n            break\n        elif cmd == \'get_spaces\':\n            remote.send((env.observation_space, env.action_space))\n        else:\n            raise NotImplementedError\n\n\nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents\n    """"""\n\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n\n\nclass SubprocVecEnv(object):\n\n    def __init__(self, env_fns):\n        """"""\n        envs: list of gym environments to run in subprocesses\n        """"""\n        self.num_envs = len(env_fns)\n\n        self.waiting = False\n        self.closed = False\n        nenvs = len(env_fns)\n        self.nenvs = nenvs\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n        zipped_args = zip(self.work_remotes, self.remotes, env_fns)\n        self.ps = [\n            Process(target=_worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n            for (work_remote, remote, env_fn) in zipped_args\n        ]\n\n        for p in self.ps:\n            # if the main process crashes, we should not cause things to hang\n            p.daemon = True\n            p.start()\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        observation_space, action_space = self.remotes[0].recv()\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    def _step_async(self, actions):\n        """"""\n            Tell all the environments to start taking a step\n            with the given actions.\n            Call step_wait() to get the results of the step.\n            You should not call this if a step_async run is\n            already pending.\n            """"""\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def _step_wait(self):\n        """"""\n            Wait for the step taken with step_async().\n            Returns (obs, rews, dones, infos):\n             - obs: an array of observations, or a tuple of\n                    arrays of observations.\n             - rews: an array of rewards\n             - dones: an array of ""episode done"" booleans\n             - infos: a sequence of info objects\n            """"""\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        """"""\n            Reset all the environments and return an array of\n            observations, or a tuple of observation arrays.\n            If step_async is still doing work, that work will\n            be cancelled and step_wait() should not be called\n            until step_async() is invoked again.\n            """"""\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def _reset_task(self):\n        for remote in self.remotes:\n            remote.send((\'reset_task\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def close(self):\n        if self.closed:\n            return\n        if self.waiting:\n            for remote in self.remotes:\n                remote.recv()\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n            self.closed = True\n\n    def __len__(self):\n        return self.nenvs\n\n    def step(self, actions):\n        self._step_async(actions)\n        return self._step_wait()\n\n\nclass Monitor(gym.Wrapper):\n\n    def __init__(self, env):\n        super(Monitor, self).__init__(env)\n        self._monitor_rewards = None\n\n    def reset(self, **kwargs):\n        self._monitor_rewards = []\n        return self.env.reset(**kwargs)\n\n    def step(self, action):\n        o_, r, done, info = self.env.step(action)\n        self._monitor_rewards.append(r)\n        if done:\n            info[\'episode\'] = {\'r\': sum(self._monitor_rewards), \'l\': len(self._monitor_rewards)}\n        return o_, r, done, info\n\n\nclass NormalizedActions(gym.ActionWrapper):\n\n    def _action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = low + (action + 1.0) * 0.5 * (high - low)\n        action = np.clip(action, low, high)\n\n        return action\n\n    def _reverse_action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = 2 * (action - low) / (high - low) - 1\n        action = np.clip(action, low, high)\n\n        return action\n\n\ndef unit_test():\n    env_id = \'CartPole-v0\'\n    unwrapped_env = gym.make(env_id)\n    wrapped_env = build_env(env_id, False)\n    o = wrapped_env.reset()\n    print(\'Reset {} observation shape {}\'.format(env_id, o.shape))\n    done = False\n    while not done:\n        a = unwrapped_env.action_space.sample()\n        o_, r, done, info = wrapped_env.step(a)\n        print(\'Take action {} get reward {} info {}\'.format(a, r, info))\n\n    env_id = \'PongNoFrameskip-v4\'\n    nenv = 2\n    unwrapped_env = gym.make(env_id)\n    wrapped_env = build_env(env_id, True, nenv=nenv)\n    o = wrapped_env.reset()\n    print(\'Reset {} observation shape {}\'.format(env_id, o.shape))\n    for _ in range(1000):\n        a = [unwrapped_env.action_space.sample() for _ in range(nenv)]\n        a = np.asarray(a, \'int64\')\n        o_, r, done, info = wrapped_env.step(a)\n        print(\'Take action {} get reward {} info {}\'.format(a, r, info))\n\n\nif __name__ == \'__main__\':\n    unit_test()\n'"
examples/spatial_transformer_network/tutorial_spatial_transformer_network_dynamic.py,12,"b'#! /usr/bin/python\n# -*- coding: utf8 -*-\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import Model\n\n##================== PREPARE DATA ============================================##\nX_train, y_train, X_val, y_val, X_test, y_test = \\\n    tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n\n\ndef pad_distort_im_fn(x):\n    """""" Zero pads an image to 40x40, and distort it.\n\n    Examples\n    ---------\n    x = pad_distort_im_fn(X_train[0])\n    print(x, x.shape, x.max())\n    tl.vis.save_image(x, \'_xd.png\')\n    tl.vis.save_image(X_train[0], \'_x.png\')\n    """"""\n    b = np.zeros((40, 40, 1), dtype=np.float32)\n    o = int((40 - 28) / 2)\n    b[o:o + 28, o:o + 28] = x\n    x = b\n    x = tl.prepro.rotation(x, rg=30, is_random=True, fill_mode=\'constant\')\n    x = tl.prepro.shear(x, 0.05, is_random=True, fill_mode=\'constant\')\n    x = tl.prepro.shift(x, wrg=0.25, hrg=0.25, is_random=True, fill_mode=\'constant\')\n    x = tl.prepro.zoom(x, zoom_range=(0.95, 1.05))\n    return x\n\n\ndef pad_distort_ims_fn(X):\n    """""" Zero pads images to 40x40, and distort them. """"""\n    X_40 = []\n    for X_a, _ in tl.iterate.minibatches(X, X, 50, shuffle=False):\n        X_40.extend(tl.prepro.threading_data(X_a, fn=pad_distort_im_fn))\n    X_40 = np.asarray(X_40)\n    return X_40\n\n\n# create dataset with size of 40x40 with distortion\nX_train_40 = pad_distort_ims_fn(X_train)\nX_val_40 = pad_distort_ims_fn(X_val)\nX_test_40 = pad_distort_ims_fn(X_test)\n\ntl.vis.save_images(X_test[0:32], [4, 8], \'_imgs_original.png\')\ntl.vis.save_images(X_test_40[0:32], [4, 8], \'_imgs_distorted.png\')\n\n\n##================== DEFINE MODEL ============================================##\nclass Net(Model):\n\n    def __init__(self):\n        super(Net, self).__init__()\n\n        ## 1. Localisation network\n        # use MLP as the localisation net\n        self.flatten1 = Flatten()\n        self.dense1 = Dense(n_units=20, in_channels=1600, act=tf.nn.tanh)\n        self.dropout1 = Dropout(keep=0.8)\n        # you can also use CNN instead for MLP as the localisation net\n\n        ## 2. Spatial transformer module (sampler)\n        self.stn = SpatialTransformer2dAffine(out_size=(40, 40), in_channels=20)\n\n        ## 3. Classifier\n        self.conv1 = Conv2d(16, (3, 3), (2, 2), act=tf.nn.relu, padding=\'SAME\', in_channels=1)\n        self.conv2 = Conv2d(16, (3, 3), (2, 2), act=tf.nn.relu, padding=\'SAME\', in_channels=16)\n        self.flatten2 = Flatten()\n        self.dense2 = Dense(n_units=1024, in_channels=1600, act=tf.nn.relu)\n        self.dense3 = Dense(n_units=10, in_channels=1024, act=tf.identity)\n\n    def forward(self, inputs):\n        theta_input = self.dropout1(self.dense1(self.flatten1(inputs)))\n        V = self.stn((theta_input, inputs))\n        _logits = self.dense3(self.dense2(self.flatten2(self.conv2(self.conv1(V)))))\n        return _logits, V\n\n\nnet = Net()\n\n##================== DEFINE TRAIN OPS ========================================##\nn_epoch = 100\nlearning_rate = 0.0001\nprint_freq = 10\nbatch_size = 64\ntrain_weights = net.trainable_weights\noptimizer = tf.optimizers.Adam(lr=learning_rate)\n\n##================== TRAINING ================================================##\nprint(""Training ..."")\nfor epoch in range(n_epoch):\n    start_time = time.time()\n\n    net.train()  # enable dropout\n\n    for X_train_a, y_train_a in tl.iterate.minibatches(X_train_40, y_train, batch_size, shuffle=True):\n        # input_dim must be of length 4\n        X_train_a = tf.expand_dims(X_train_a, 3)\n\n        with tf.GradientTape() as tape:\n            ## compute outputs\n            _logits, _ = net(X_train_a)  # alternatively, you can use MLP(x, is_train=True) and remove MLP.train()\n            ## compute loss and update model\n            _loss = tl.cost.cross_entropy(_logits, y_train_a, name=\'train_loss\')\n\n        grad = tape.gradient(_loss, train_weights)\n        optimizer.apply_gradients(zip(grad, train_weights))\n\n    ## use training and evaluation sets to evaluate the model every print_freq epoch\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n\n        net.eval()  # disable dropout\n\n        print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n\n        train_loss, train_acc, n_iter = 0, 0, 0\n        for X_train_a, y_train_a in tl.iterate.minibatches(X_train_40, y_train, batch_size, shuffle=False):\n            # input_dim must be of length 4\n            X_train_a = tf.expand_dims(X_train_a, 3)\n\n            _logits, _ = net(X_train_a)  # alternatively, you can use MLP(x, is_train=False) and remove MLP.eval()\n            train_loss += tl.cost.cross_entropy(_logits, y_train_a, name=\'eval_train_loss\')\n            train_acc += np.mean(np.equal(np.argmax(_logits, 1), y_train_a))\n            n_iter += 1\n        print(""   train loss: %f"" % (train_loss / n_iter))\n        print(""   train acc: %f"" % (train_acc / n_iter))\n\n        val_loss, val_acc, n_iter = 0, 0, 0\n        for X_val_a, y_val_a in tl.iterate.minibatches(X_val_40, y_val, batch_size, shuffle=False):\n            # input_dim must be of length 4\n            X_val_a = tf.expand_dims(X_val_a, 3)\n\n            _logits, _ = net(X_val_a)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_val_a, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_val_a))\n            n_iter += 1\n        print(""   val loss: %f"" % (val_loss / n_iter))\n        print(""   val acc: %f"" % (val_acc / n_iter))\n\n        print(\'save images\')\n        _, trans_imgs = net(tf.expand_dims(X_test_40[0:64], 3))\n        trans_imgs = trans_imgs.numpy()\n        tl.vis.save_images(trans_imgs[0:32], [4, 8], \'_imgs_distorted_after_stn_%s.png\' % epoch)\n\n##================== EVALUATION ==============================================##\nprint(\'Evaluation\')\n\nnet.eval()\n\ntest_loss, test_acc, n_iter = 0, 0, 0\nfor X_test_a, y_test_a in tl.iterate.minibatches(X_test_40, y_test, batch_size, shuffle=False):\n    # input_dim must be of length 4\n    X_test_a = tf.expand_dims(X_test_a, 3)\n\n    _logits, _ = net(X_test_a)\n    test_loss += tl.cost.cross_entropy(_logits, y_test_a, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_test_a))\n    n_iter += 1\nprint(""   test loss: %f"" % (test_loss / n_iter))\nprint(""   test acc: %f"" % (test_acc / n_iter))\n'"
examples/spatial_transformer_network/tutorial_spatial_transformer_network_static.py,12,"b'#! /usr/bin/python\n# -*- coding: utf8 -*-\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import Model\n\n##================== PREPARE DATA ============================================##\nX_train, y_train, X_val, y_val, X_test, y_test = \\\n    tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n\n\ndef pad_distort_im_fn(x):\n    """""" Zero pads an image to 40x40, and distort it.\n\n    Examples\n    ---------\n    x = pad_distort_im_fn(X_train[0])\n    print(x, x.shape, x.max())\n    tl.vis.save_image(x, \'_xd.png\')\n    tl.vis.save_image(X_train[0], \'_x.png\')\n    """"""\n    b = np.zeros((40, 40, 1), dtype=np.float32)\n    o = int((40 - 28) / 2)\n    b[o:o + 28, o:o + 28] = x\n    x = b\n    x = tl.prepro.rotation(x, rg=30, is_random=True, fill_mode=\'constant\')\n    x = tl.prepro.shear(x, 0.05, is_random=True, fill_mode=\'constant\')\n    x = tl.prepro.shift(x, wrg=0.25, hrg=0.25, is_random=True, fill_mode=\'constant\')\n    x = tl.prepro.zoom(x, zoom_range=(0.95, 1.05))\n    return x\n\n\ndef pad_distort_ims_fn(X):\n    """""" Zero pads images to 40x40, and distort them. """"""\n    X_40 = []\n    for X_a, _ in tl.iterate.minibatches(X, X, 50, shuffle=False):\n        X_40.extend(tl.prepro.threading_data(X_a, fn=pad_distort_im_fn))\n    X_40 = np.asarray(X_40)\n    return X_40\n\n\n# create dataset with size of 40x40 with distortion\nX_train_40 = pad_distort_ims_fn(X_train)\nX_val_40 = pad_distort_ims_fn(X_val)\nX_test_40 = pad_distort_ims_fn(X_test)\n\ntl.vis.save_images(X_test[0:32], [4, 8], \'_imgs_original.png\')\ntl.vis.save_images(X_test_40[0:32], [4, 8], \'_imgs_distorted.png\')\n\n\n##================== DEFINE MODEL ============================================##\ndef get_model(inputs_shape):\n    ni = Input(inputs_shape)\n\n    ## 1. Localisation network\n    # use MLP as the localisation net\n    nn = Flatten()(ni)\n    nn = Dense(n_units=20, act=tf.nn.tanh)(nn)\n    nn = Dropout(keep=0.8)(nn)\n    # you can also use CNN instead for MLP as the localisation net\n\n    ## 2. Spatial transformer module (sampler)\n    stn = SpatialTransformer2dAffine(out_size=(40, 40), in_channels=20)\n    nn = stn((nn, ni))\n    s = nn\n\n    ## 3. Classifier\n    nn = Conv2d(16, (3, 3), (2, 2), act=tf.nn.relu, padding=\'SAME\')(nn)\n    nn = Conv2d(16, (3, 3), (2, 2), act=tf.nn.relu, padding=\'SAME\')(nn)\n    nn = Flatten()(nn)\n    nn = Dense(n_units=1024, act=tf.nn.relu)(nn)\n    nn = Dense(n_units=10, act=tf.identity)(nn)\n\n    M = Model(inputs=ni, outputs=[nn, s])\n    return M\n\n\nnet = get_model([None, 40, 40, 1])\n\n##================== DEFINE TRAIN OPS ========================================##\nn_epoch = 100\nlearning_rate = 0.0001\nprint_freq = 10\nbatch_size = 64\ntrain_weights = net.trainable_weights\noptimizer = tf.optimizers.Adam(lr=learning_rate)\n\n##================== TRAINING ================================================##\nprint(""Training ..."")\nfor epoch in range(n_epoch):\n    start_time = time.time()\n\n    net.train()  # enable dropout\n\n    for X_train_a, y_train_a in tl.iterate.minibatches(X_train_40, y_train, batch_size, shuffle=True):\n        # input_dim must be of length 4\n        X_train_a = tf.expand_dims(X_train_a, 3)\n\n        with tf.GradientTape() as tape:\n            ## compute outputs\n            _logits, _ = net(X_train_a)  # alternatively, you can use MLP(x, is_train=True) and remove MLP.train()\n            ## compute loss and update model\n            _loss = tl.cost.cross_entropy(_logits, y_train_a, name=\'train_loss\')\n\n        grad = tape.gradient(_loss, train_weights)\n        optimizer.apply_gradients(zip(grad, train_weights))\n\n    ## use training and evaluation sets to evaluate the model every print_freq epoch\n    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n\n        net.eval()  # disable dropout\n\n        print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))\n\n        train_loss, train_acc, n_iter = 0, 0, 0\n        for X_train_a, y_train_a in tl.iterate.minibatches(X_train_40, y_train, batch_size, shuffle=False):\n            # input_dim must be of length 4\n            X_train_a = tf.expand_dims(X_train_a, 3)\n\n            _logits, _ = net(X_train_a)  # alternatively, you can use MLP(x, is_train=False) and remove MLP.eval()\n            train_loss += tl.cost.cross_entropy(_logits, y_train_a, name=\'eval_train_loss\')\n            train_acc += np.mean(np.equal(np.argmax(_logits, 1), y_train_a))\n            n_iter += 1\n        print(""   train loss: %f"" % (train_loss / n_iter))\n        print(""   train acc: %f"" % (train_acc / n_iter))\n\n        val_loss, val_acc, n_iter = 0, 0, 0\n        for X_val_a, y_val_a in tl.iterate.minibatches(X_val_40, y_val, batch_size, shuffle=False):\n            # input_dim must be of length 4\n            X_val_a = tf.expand_dims(X_val_a, 3)\n\n            _logits, _ = net(X_val_a)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_val_a, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_val_a))\n            n_iter += 1\n        print(""   val loss: %f"" % (val_loss / n_iter))\n        print(""   val acc: %f"" % (val_acc / n_iter))\n\n        print(\'save images\')\n        _, trans_imgs = net(tf.expand_dims(X_test_40[0:64], 3))\n        trans_imgs = trans_imgs.numpy()\n        tl.vis.save_images(trans_imgs[0:32], [4, 8], \'_imgs_distorted_after_stn_%s.png\' % epoch)\n\n##================== EVALUATION ==============================================##\nprint(\'Evaluation\')\n\nnet.eval()\n\ntest_loss, test_acc, n_iter = 0, 0, 0\nfor X_test_a, y_test_a in tl.iterate.minibatches(X_test_40, y_test, batch_size, shuffle=False):\n    # input_dim must be of length 4\n    X_test_a = tf.expand_dims(X_test_a, 3)\n\n    _logits, _ = net(X_test_a)\n    test_loss += tl.cost.cross_entropy(_logits, y_test_a, name=\'test_loss\')\n    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_test_a))\n    n_iter += 1\nprint(""   test loss: %f"" % (test_loss / n_iter))\nprint(""   test acc: %f"" % (test_acc / n_iter))\n'"
examples/text_classification/tutorial_imdb_fasttext.py,9,"b'#!/usr/bin/env python\n""""""\nThis demo implements FastText[1] for sentence classification. This demo should be run in eager mode and\ncan be slower than the corresponding demo in graph mode.\n\nFastText is a simple model for text classification with performance often close\nto state-of-the-art, and is useful as a solid baseline.\n\nThere are some important differences between this implementation and what\nis described in the paper. Instead of Hogwild! SGD[2], we use Adam optimizer\nwith mini-batches. Hierarchical softmax is also not supported; if you have\na large label space, consider utilizing candidate sampling methods provided\nby TensorFlow[3].\n\nAfter 5 epochs, you should get test accuracy around 90.3%.\n\n[1] Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2016).\n    Bag of Tricks for Efficient Text Classification.\n    http://arxiv.org/abs/1607.01759\n\n[2] Recht, B., Re, C., Wright, S., & Niu, F. (2011).\n    Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.\n    In Advances in Neural Information Processing Systems 24 (pp. 693\xe2\x80\x93701).\n\n[3] https://www.tensorflow.org/api_guides/python/nn#Candidate_Sampling\n\n""""""\nimport array\nimport hashlib\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\n\n# Hashed n-grams with 1 < n <= N_GRAM are included as features\n# in addition to unigrams.\nN_GRAM = 2\n\n# Size of vocabulary; less frequent words will be treated as ""unknown""\nVOCAB_SIZE = 100000\n\n# Number of buckets used for hashing n-grams\nN_BUCKETS = 1000000\n\n# Size of the embedding vectors\nEMBEDDING_SIZE = 50\n\n# Number of epochs for which the model is trained\nN_EPOCH = 5\n\n# Number of steps for printing\nN_STEPS_TO_PRINT = 100\n\n# Size of training mini-batches\nBATCH_SIZE = 32\n\n# Learning rate\nLEARNING_RATE = 0.01\n\n# Path to which to save the trained model\nMODEL_FILE_PATH = \'model_dynamic.hdf5\'\n\n\nclass FastTextModel(Model):\n    """"""  Model structure and forwarding of FastText """"""\n\n    def __init__(self, vocab_size, embedding_size, n_labels, name=\'fasttext\'):\n        super(FastTextModel, self).__init__(name=name)\n\n        self.avg_embed = AverageEmbedding(vocab_size, embedding_size)\n        self.dense1 = Dense(n_units=10, in_channels=embedding_size)\n        self.dense2 = Dense(n_units=n_labels, in_channels=10)\n\n    def forward(self, x):\n        z = self.avg_embed(x)\n        z = self.dense1(z)\n        z = self.dense2(z)\n        return z\n\n\ndef augment_with_ngrams(unigrams, unigram_vocab_size, n_buckets, n=2):\n    """"""Augment unigram features with hashed n-gram features.""""""\n\n    def get_ngrams(n):\n        return list(zip(*[unigrams[i:] for i in range(n)]))\n\n    def hash_ngram(ngram):\n        bytes_ = array.array(\'L\', ngram).tobytes()\n        hash_ = int(hashlib.sha256(bytes_).hexdigest(), 16)\n        return unigram_vocab_size + hash_ % n_buckets\n\n    return unigrams + [hash_ngram(ngram) for i in range(2, n + 1) for ngram in get_ngrams(i)]\n\n\ndef load_and_preprocess_imdb_data(n_gram=None):\n    """"""Load IMDb data and augment with hashed n-gram features.""""""\n    tl.logging.info(""Loading and preprocessing IMDB data."")\n\n    X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(nb_words=VOCAB_SIZE)\n\n    if n_gram is not None:\n        X_train = np.array([augment_with_ngrams(x, VOCAB_SIZE, N_BUCKETS, n=n_gram) for x in X_train])\n        X_test = np.array([augment_with_ngrams(x, VOCAB_SIZE, N_BUCKETS, n=n_gram) for x in X_test])\n\n    return X_train, y_train, X_test, y_test\n\n\ndef train_test_and_save_model():\n    X_train, y_train, X_test, y_test = load_and_preprocess_imdb_data(N_GRAM)\n    model = FastTextModel(\n        vocab_size=VOCAB_SIZE + N_BUCKETS,\n        embedding_size=EMBEDDING_SIZE,\n        n_labels=2,\n    )\n    optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n\n    if os.path.exists(MODEL_FILE_PATH):\n        # loading pre-trained model if applicable\n        model.load_weights(MODEL_FILE_PATH)\n    else:\n        # training\n        model.train()\n\n        for epoch in range(N_EPOCH):\n            start_time = time.time()\n            print(\'Epoch %d/%d\' % (epoch + 1, N_EPOCH))\n            train_accuracy = list()\n            for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True):\n\n                # forward and define the loss function\n                # TODO: use tf.function to speed up\n                with tf.GradientTape() as tape:\n                    y_pred = model(tl.prepro.pad_sequences(X_batch))\n                    cost = tl.cost.cross_entropy(y_pred, y_batch, name=\'cost\')\n\n                # backward, calculate gradients and update the weights\n                grad = tape.gradient(cost, model.trainable_weights)\n                optimizer.apply_gradients(zip(grad, model.trainable_weights))\n\n                # calculate the accuracy\n                predictions = tf.argmax(y_pred, axis=1, output_type=tf.int32)\n                are_predictions_correct = tf.equal(predictions, y_batch)\n                accuracy = tf.reduce_mean(tf.cast(are_predictions_correct, tf.float32))\n\n                train_accuracy.append(accuracy)\n                if len(train_accuracy) % N_STEPS_TO_PRINT == 0:\n                    print(\n                        ""\\t[%d/%d][%d]accuracy "" % (epoch + 1, N_EPOCH, len(train_accuracy)),\n                        np.mean(train_accuracy[-N_STEPS_TO_PRINT:])\n                    )\n\n            print(""\\tSummary: time %.5fs, overall accuracy"" % (time.time() - start_time), np.mean(train_accuracy))\n\n    # evaluation and testing\n    model.eval()\n\n    # forward and calculate the accuracy\n    y_pred = model(tl.prepro.pad_sequences(X_test))\n    predictions = tf.argmax(y_pred, axis=1, output_type=tf.int32)\n    are_predictions_correct = tf.equal(predictions, y_test)\n    test_accuracy = tf.reduce_mean(tf.cast(are_predictions_correct, tf.float32))\n\n    print(\'Test accuracy: %.5f\' % test_accuracy)\n\n    # saving the model\n    model.save_weights(MODEL_FILE_PATH)\n\n\nif __name__ == \'__main__\':\n    train_test_and_save_model()\n'"
examples/text_generation/tutorial_generate_text.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n# Copyright 2019 TensorLayer. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nExample of Synced sequence input and output.\n\nGenerate text using LSTM.\n\nData: https://github.com/tensorlayer/tensorlayer/tree/master/example/data/\n\n""""""\n\nimport os\nimport re\nimport time\n\nimport nltk\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n_UNK = ""_UNK""\n\n\ndef basic_clean_str(string):\n    """"""Tokenization/string cleaning for a datasets.""""""\n    string = re.sub(r""\\n"", "" "", string)  # \'\\n\'      --> \' \'\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)  # it\'s      --> it \'s\n    string = re.sub(r""\\\xe2\x80\x99s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" have"", string)  # they\'ve   --> they have\n    string = re.sub(r""\\\xe2\x80\x99ve"", "" have"", string)\n    string = re.sub(r""\\\'t"", "" not"", string)  # can\'t     --> can not\n    string = re.sub(r""\\\xe2\x80\x99t"", "" not"", string)\n    string = re.sub(r""\\\'re"", "" are"", string)  # they\'re   --> they are\n    string = re.sub(r""\\\xe2\x80\x99re"", "" are"", string)\n    string = re.sub(r""\\\'d"", """", string)  # I\'d (I had, I would) --> I\n    string = re.sub(r""\\\xe2\x80\x99d"", """", string)\n    string = re.sub(r""\\\'ll"", "" will"", string)  # I\'ll      --> I will\n    string = re.sub(r""\\\xe2\x80\x99ll"", "" will"", string)\n    string = re.sub(r""\\\xe2\x80\x9c"", ""  "", string)  # \xe2\x80\x9ca\xe2\x80\x9d       --> \xe2\x80\x9c a \xe2\x80\x9d\n    string = re.sub(r""\\\xe2\x80\x9d"", ""  "", string)\n    string = re.sub(r""\\"""", ""  "", string)  # ""a""       --> "" a ""\n    string = re.sub(r""\\\'"", ""  "", string)  # they\'     --> they \'\n    string = re.sub(r""\\\xe2\x80\x99"", ""  "", string)  # they\xe2\x80\x99     --> they \xe2\x80\x99\n    string = re.sub(r""\\."", "" . "", string)  # they.     --> they .\n    string = re.sub(r""\\,"", "" , "", string)  # they,     --> they ,\n    string = re.sub(r""\\!"", "" ! "", string)\n    string = re.sub(r""\\-"", ""  "", string)  # ""low-cost""--> lost cost\n    string = re.sub(r""\\("", ""  "", string)  # (they)    --> ( they)\n    string = re.sub(r""\\)"", ""  "", string)  # ( they)   --> ( they )\n    string = re.sub(r""\\]"", ""  "", string)  # they]     --> they ]\n    string = re.sub(r""\\["", ""  "", string)  # they[     --> they [\n    string = re.sub(r""\\?"", ""  "", string)  # they?     --> they ?\n    string = re.sub(r""\\>"", ""  "", string)  # they>     --> they >\n    string = re.sub(r""\\<"", ""  "", string)  # they<     --> they <\n    string = re.sub(r""\\="", ""  "", string)  # easier=   --> easier =\n    string = re.sub(r""\\;"", ""  "", string)  # easier;   --> easier ;\n    string = re.sub(r""\\;"", ""  "", string)\n    string = re.sub(r""\\:"", ""  "", string)  # easier:   --> easier :\n    string = re.sub(r""\\"""", ""  "", string)  # easier""   --> easier ""\n    string = re.sub(r""\\$"", ""  "", string)  # $380      --> $ 380\n    string = re.sub(r""\\_"", ""  "", string)  # _100     --> _ 100\n    string = re.sub(r""\\s{2,}"", "" "", string)  # Akara is    handsome --> Akara is handsome\n    return string.strip().lower()  # lowercase\n\n\ndef customized_clean_str(string):\n    """"""Tokenization/string cleaning for a datasets.""""""\n    string = re.sub(r""\\n"", "" "", string)  # \'\\n\'      --> \' \'\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)  # it\'s      --> it \'s\n    string = re.sub(r""\\\xe2\x80\x99s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" have"", string)  # they\'ve   --> they have\n    string = re.sub(r""\\\xe2\x80\x99ve"", "" have"", string)\n    string = re.sub(r""\\\'t"", "" not"", string)  # can\'t     --> can not\n    string = re.sub(r""\\\xe2\x80\x99t"", "" not"", string)\n    string = re.sub(r""\\\'re"", "" are"", string)  # they\'re   --> they are\n    string = re.sub(r""\\\xe2\x80\x99re"", "" are"", string)\n    string = re.sub(r""\\\'d"", """", string)  # I\'d (I had, I would) --> I\n    string = re.sub(r""\\\xe2\x80\x99d"", """", string)\n    string = re.sub(r""\\\'ll"", "" will"", string)  # I\'ll      --> I will\n    string = re.sub(r""\\\xe2\x80\x99ll"", "" will"", string)\n    string = re.sub(r""\\\xe2\x80\x9c"", "" \xe2\x80\x9c "", string)  # \xe2\x80\x9ca\xe2\x80\x9d       --> \xe2\x80\x9c a \xe2\x80\x9d\n    string = re.sub(r""\\\xe2\x80\x9d"", "" \xe2\x80\x9d "", string)\n    string = re.sub(r""\\"""", "" \xe2\x80\x9c "", string)  # ""a""       --> "" a ""\n    string = re.sub(r""\\\'"", "" \' "", string)  # they\'     --> they \'\n    string = re.sub(r""\\\xe2\x80\x99"", "" \' "", string)  # they\xe2\x80\x99     --> they \'\n    string = re.sub(r""\\."", "" . "", string)  # they.     --> they .\n    string = re.sub(r""\\,"", "" , "", string)  # they,     --> they ,\n    string = re.sub(r""\\-"", "" "", string)  # ""low-cost""--> lost cost\n    string = re.sub(r""\\("", "" ( "", string)  # (they)    --> ( they)\n    string = re.sub(r""\\)"", "" ) "", string)  # ( they)   --> ( they )\n    string = re.sub(r""\\!"", "" ! "", string)  # they!     --> they !\n    string = re.sub(r""\\]"", "" ] "", string)  # they]     --> they ]\n    string = re.sub(r""\\["", "" [ "", string)  # they[     --> they [\n    string = re.sub(r""\\?"", "" ? "", string)  # they?     --> they ?\n    string = re.sub(r""\\>"", "" > "", string)  # they>     --> they >\n    string = re.sub(r""\\<"", "" < "", string)  # they<     --> they <\n    string = re.sub(r""\\="", "" = "", string)  # easier=   --> easier =\n    string = re.sub(r""\\;"", "" ; "", string)  # easier;   --> easier ;\n    string = re.sub(r""\\;"", "" ; "", string)\n    string = re.sub(r""\\:"", "" : "", string)  # easier:   --> easier :\n    string = re.sub(r""\\"""", "" \\"" "", string)  # easier""   --> easier ""\n    string = re.sub(r""\\$"", "" $ "", string)  # $380      --> $ 380\n    string = re.sub(r""\\_"", "" _ "", string)  # _100     --> _ 100\n    string = re.sub(r""\\s{2,}"", "" "", string)  # Akara is    handsome --> Akara is handsome\n    return string.strip().lower()  # lowercase\n\n\ndef customized_read_words(input_fpath):  # , dictionary):\n    with open(input_fpath, ""r"", encoding=""utf8"") as f:\n        words = f.read()\n    # Clean the data\n    words = customized_clean_str(words)\n    # Split each word\n    return words.split()\n\n\ndef main_restore_embedding_layer():\n    """"""How to use Embedding layer, and how to convert IDs to vector,\n    IDs to words, etc.\n    """"""\n    # Step 1: Build the embedding matrix and load the existing embedding matrix.\n    vocabulary_size = 50000\n    embedding_size = 128\n    model_file_name = ""model_word2vec_50k_128""\n    batch_size = None\n\n    if not os.path.exists(model_file_name + "".npy""):\n        raise Exception(\n            ""Pretrained embedding matrix not found. ""\n            ""Hint: Please pre-train the default model in ""\n            ""`examples/text_word_embedding/tutorial_word2vec_basic.py`.""\n        )\n\n    print(""Load existing embedding matrix and dictionaries"")\n    all_var = tl.files.load_npy_to_any(name=model_file_name + \'.npy\')\n    data = all_var[\'data\']\n    count = all_var[\'count\']\n    dictionary = all_var[\'dictionary\']\n    reverse_dictionary = all_var[\'reverse_dictionary\']\n\n    tl.nlp.save_vocab(count, name=\'vocab_\' + model_file_name + \'.txt\')\n\n    del all_var, data, count\n\n    class Embedding_Model(Model):\n\n        def __init__(self):\n            super(Embedding_Model, self).__init__()\n            self.embedding = Embedding(vocabulary_size, embedding_size)\n\n        def forward(self, inputs):\n            return self.embedding(inputs)\n\n    model = Embedding_Model()\n    model.eval()\n\n    # TODO: assign certain parameters to model\n    model.load_weights(model_file_name + "".hdf5"", skip=True, in_order=False)\n\n    # Step 2: Input word(s), output the word vector(s).\n    word = \'hello\'\n    word_id = dictionary[word]\n    print(\'word_id:\', word_id)\n\n    words = [\'i\', \'am\', \'tensor\', \'layer\']\n    word_ids = tl.nlp.words_to_word_ids(words, dictionary, _UNK)\n    context = tl.nlp.word_ids_to_words(word_ids, reverse_dictionary)\n    print(\'word_ids:\', word_ids)\n    print(\'context:\', context)\n\n    vector = model(word_id)\n    print(\'vector:\', vector.shape)\n    print(vector)\n\n    vectors = model(word_ids)\n    print(\'vectors:\', vectors.shape)\n    print(vectors)\n\n\nclass Text_Generation_Net(Model):\n\n    def __init__(self, vocab_size, hidden_size, init):\n        super(Text_Generation_Net, self).__init__()\n\n        self.embedding = Embedding(vocab_size, hidden_size, init, name=\'embedding\')\n        self.lstm = tl.layers.RNN(\n            cell=tf.keras.layers.LSTMCell(hidden_size), return_last_output=False, return_last_state=True,\n            return_seq_2d=True, in_channels=hidden_size\n        )\n        self.out_dense = Dense(vocab_size, in_channels=hidden_size, W_init=init, b_init=init, act=None, name=\'output\')\n\n    def forward(self, inputs, initial_state=None):\n        embedding_vector = self.embedding(inputs)\n        lstm_out, final_state = self.lstm(embedding_vector, initial_state=initial_state)\n        logits = self.out_dense(lstm_out)\n        return logits, final_state\n\n\ndef main_lstm_generate_text():\n    """"""Generate text by Synced sequence input and output.""""""\n    # rnn model and update  (describtion: see tutorial_ptb_lstm.py)\n    init_scale = 0.1\n    learning_rate = 1e-3\n    sequence_length = 20\n    hidden_size = 200\n    max_epoch = 100\n    batch_size = 16\n\n    top_k_list = [1, 3, 5, 10]\n    print_length = 30\n\n    model_file_name = ""model_generate_text.hdf5""\n\n    #  ===== Prepare Data\n    words = customized_read_words(input_fpath=""data/trump/trump_text.txt"")\n\n    vocab = tl.nlp.create_vocab([words], word_counts_output_file=\'vocab.txt\', min_word_count=1)\n    vocab = tl.nlp.Vocabulary(\'vocab.txt\', unk_word=""<UNK>"")\n    vocab_size = vocab.unk_id + 1\n    train_data = [vocab.word_to_id(word) for word in words]\n\n    # Set the seed to generate sentence.\n    seed = ""it is a""\n    # seed = basic_clean_str(seed).split()\n    seed = nltk.tokenize.word_tokenize(seed)\n    print(\'seed : %s\' % seed)\n\n    init = tl.initializers.random_uniform(-init_scale, init_scale)\n\n    net = Text_Generation_Net(vocab_size, hidden_size, init)\n\n    train_weights = net.trainable_weights\n    optimizer = tf.optimizers.Adam(lr=learning_rate)\n\n    # ===== Training\n\n    print(""\\nStart learning a model to generate text"")\n    for i in range(max_epoch):\n\n        print(""Epoch: %d/%d"" % (i + 1, max_epoch))\n        epoch_size = ((len(train_data) // batch_size) - 1) // sequence_length\n\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n\n        net.train()\n        # reset all states at the begining of every epoch\n        lstm_state = None\n        for step, (x, y) in enumerate(tl.iterate.ptb_iterator(train_data, batch_size, sequence_length)):\n            with tf.GradientTape() as tape:\n                ## compute outputs\n                logits, lstm_state = net(x, initial_state=lstm_state)\n                ## compute loss and update model\n                cost = tl.cost.cross_entropy(logits, tf.reshape(y, [-1]), name=\'train_loss\')\n\n            grad = tape.gradient(cost, train_weights)\n            optimizer.apply_gradients(zip(grad, train_weights))\n\n            costs += cost\n            iters += 1\n\n            if step % (epoch_size // 10) == 1:\n                print(\n                    ""%.3f perplexity: %.3f speed: %.0f wps"" % (\n                        step * 1.0 / epoch_size, np.exp(costs / iters),\n                        iters * batch_size * sequence_length * batch_size / (time.time() - start_time)\n                    )\n                )\n        train_perplexity = np.exp(costs / iters)\n        # print(""Epoch: %d Train Perplexity: %.3f"" % (i + 1, train_perplexity))\n        print(""Epoch: %d/%d Train Perplexity: %.3f"" % (i + 1, max_epoch, train_perplexity))\n\n        net.eval()\n        # for diversity in diversity_list:\n        # testing: sample from top k words\n        for top_k in top_k_list:\n            # Testing, generate some text from a given seed.\n            lstm_state = None\n            outs_id = [vocab.word_to_id(w) for w in seed]\n            # feed the seed to initialize the state for generation.\n            for ids in outs_id[:-1]:\n                a_id = np.asarray(ids).reshape(1, 1)\n                _, lstm_state = net(a_id, initial_state=lstm_state)\n\n            # feed the last word in seed, and start to generate sentence.\n            a_id = outs_id[-1]\n            for _ in range(print_length):\n                a_id = np.asarray(a_id).reshape(1, 1)\n                logits, lstm_state = net(a_id, initial_state=lstm_state)\n                out = tf.nn.softmax(logits)\n                # Without sampling\n                # a_id = np.argmax(out[0])\n                # Sample from all words, if vocab_size is large,\n                # this may have numeric error.\n                # a_id = tl.nlp.sample(out[0], diversity)\n                # Sample from the top k words.\n                a_id = tl.nlp.sample_top(out[0].numpy(), top_k=top_k)\n                outs_id.append(a_id)\n            sentence = [vocab.id_to_word(w) for w in outs_id]\n            sentence = "" "".join(sentence)\n            # print(diversity, \':\', sentence)\n            print(top_k, \':\', sentence)\n\n    print(""Save model"")\n    net.save_weights(model_file_name)\n\n\nif __name__ == \'__main__\':\n    # Restore a pretrained embedding matrix\n    # main_restore_embedding_layer()\n\n    # How to generate text from a given context\n    main_lstm_generate_text()\n'"
examples/text_ptb/tutorial_ptb_lstm.py,10,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\nr""""""Example of Synced sequence input and output.\n\nThis is a reimpmentation of the TensorFlow official PTB example in :\ntensorflow/models/rnn/ptb\n\nThe batch_size can be seem as how many concurrent computations.\\n\nAs the following example shows, the first batch learn the sequence information by using 0 to 9.\\n\nThe second batch learn the sequence information by using 10 to 19.\\n\nSo it ignores the information from 9 to 10 !\\n\nIf only if we set the batch_size = 1, it will consider all information from 0 to 20.\\n\n\nThe meaning of batch_size here is not the same with the MNIST example. In MNIST example,\nbatch_size reflects how many examples we consider in each iteration, while in\nPTB example, batch_size is how many concurrent processes (segments)\nfor speed up computation.\n\nSome Information will be ignored if batch_size > 1, however, if your dataset\nis ""long"" enough (a text corpus usually has billions words), the ignored\ninformation would not effect the final result.\n\nIn PTB tutorial, we setted batch_size = 20, so we cut the dataset into 20 segments.\nAt the begining of each epoch, we initialize (reset) the 20 RNN states for 20\nsegments, then go through 20 segments separately.\n\nThe training data will be generated as follow:\\n\n\n>>> train_data = [i for i in range(20)]\n>>> for batch in tl.iterate.ptb_iterator(train_data, batch_size=2, num_steps=3):\n>>>     x, y = batch\n>>>     print(x, \'\\n\',y)\n... [[ 0  1  2] <---x                       1st subset/ iteration\n...  [10 11 12]]\n... [[ 1  2  3] <---y\n...  [11 12 13]]\n...\n... [[ 3  4  5]  <--- 1st batch input       2nd subset/ iteration\n...  [13 14 15]] <--- 2nd batch input\n... [[ 4  5  6]  <--- 1st batch target\n...  [14 15 16]] <--- 2nd batch target\n...\n... [[ 6  7  8]                             3rd subset/ iteration\n...  [16 17 18]]\n... [[ 7  8  9]\n...  [17 18 19]]\n\nHao Dong: This example can also be considered as pre-training of the word\nembedding matrix.\n\nAbout RNN\n----------\n$ Karpathy Blog : http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\nMore TensorFlow official RNN examples can be found here\n---------------------------------------------------------\n$ RNN for PTB : https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html#recurrent-neural-networks\n$ Seq2seq : https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html#sequence-to-sequence-models\n$ translation : tensorflow/models/rnn/translate\n\nExample / benchmark for building a PTB LSTM model.\n\nTrains the model described in:\n(Zaremba, et. al.) Recurrent Neural Network Regularization\nhttp://arxiv.org/abs/1409.2329\n\nThere are 3 supported model configurations:\n===========================================\n| config | epochs | train | valid  | test\n===========================================\n| small  | 13     | 37.99 | 121.39 | 115.91\n| medium | 39     | 48.45 |  86.16 |  82.07\n| large  | 55     | 37.87 |  82.62 |  78.29\nThe exact results may vary depending on the random initialization.\n\nThe hyperparameters used in the model:\n- init_scale - the initial scale of the weights\n- learning_rate - the initial value of the learning rate\n- max_grad_norm - the maximum permissible norm of the gradient\n- num_layers - the number of LSTM layers\n- num_steps - the number of unrolled steps of LSTM\n- hidden_size - the number of LSTM units\n- max_epoch - the number of epochs trained with the initial learning rate\n- max_max_epoch - the total number of epochs for training\n- keep_prob - the probability of keeping weights in the dropout layer\n- lr_decay - the decay of the learning rate for each epoch after ""max_epoch""\n- batch_size - the batch size\n\nThe data required for this example is in the data/ dir of the\nPTB dataset from Tomas Mikolov\'s webpage:\n\n$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n$ tar xvf simple-examples.tgz\n\n\nA) use the zero_state function on the cell object\n\nB) for an rnn, all time steps share weights. We use one matrix to keep all\ngate weights. Split by column into 4 parts to get the 4 gate weight matrices.\n\n""""""\nimport argparse\nimport sys\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.models import Model\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n\ndef process_args(args):\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \'--model\', default=\'small\', choices=[\'small\', \'medium\', \'large\'],\n        help=""A type of model. Possible options are: small, medium, large.""\n    )\n    parameters = parser.parse_args(args)\n    return parameters\n\n\nclass PTB_Net(Model):\n\n    def __init__(self, vocab_size, hidden_size, init, keep):\n        super(PTB_Net, self).__init__()\n\n        self.embedding = tl.layers.Embedding(vocab_size, hidden_size, init)\n        self.dropout1 = tl.layers.Dropout(keep=keep)\n        self.lstm1 = tl.layers.RNN(\n            cell=tf.keras.layers.LSTMCell(hidden_size), return_last_output=False, return_last_state=True,\n            return_seq_2d=False, in_channels=hidden_size\n        )\n        self.dropout2 = tl.layers.Dropout(keep=keep)\n        self.lstm2 = tl.layers.RNN(\n            cell=tf.keras.layers.LSTMCell(hidden_size), return_last_output=False, return_last_state=True,\n            return_seq_2d=True, in_channels=hidden_size\n        )\n        self.dropout3 = tl.layers.Dropout(keep=keep)\n        self.out_dense = tl.layers.Dense(vocab_size, in_channels=hidden_size, W_init=init, b_init=init, act=None)\n\n    def forward(self, inputs, lstm1_initial_state=None, lstm2_initial_state=None):\n        inputs = self.embedding(inputs)\n        inputs = self.dropout1(inputs)\n        lstm1_out, lstm1_state = self.lstm1(inputs, initial_state=lstm1_initial_state)\n        inputs = self.dropout2(lstm1_out)\n        lstm2_out, lstm2_state = self.lstm2(inputs, initial_state=lstm2_initial_state)\n        inputs = self.dropout3(lstm2_out)\n        logits = self.out_dense(inputs)\n        return logits, lstm1_state, lstm2_state\n\n\ndef main():\n    """"""\n    The core of the model consists of an LSTM cell that processes one word at\n    a time and computes probabilities of the possible continuations of the\n    sentence. The memory state of the network is initialized with a vector\n    of zeros and gets updated after reading each word. Also, for computational\n    reasons, we will process data in mini-batches of size batch_size.\n\n    """"""\n    param = process_args(sys.argv[1:])\n\n    if param.model == ""small"":\n        init_scale = 0.1\n        learning_rate = 1e-3\n        max_grad_norm = 5\n        num_steps = 20\n        hidden_size = 200\n        max_epoch = 4\n        max_max_epoch = 13\n        keep_prob = 1.0\n        lr_decay = 0.5\n        batch_size = 20\n        vocab_size = 10000\n    elif param.model == ""medium"":\n        init_scale = 0.05\n        learning_rate = 1e-3\n        max_grad_norm = 5\n        # num_layers = 2\n        num_steps = 35\n        hidden_size = 650\n        max_epoch = 6\n        max_max_epoch = 39\n        keep_prob = 0.5\n        lr_decay = 0.8\n        batch_size = 20\n        vocab_size = 10000\n    elif param.model == ""large"":\n        init_scale = 0.04\n        learning_rate = 1e-3\n        max_grad_norm = 10\n        # num_layers = 2\n        num_steps = 35\n        hidden_size = 1500\n        max_epoch = 14\n        max_max_epoch = 55\n        keep_prob = 0.35\n        lr_decay = 1 / 1.15\n        batch_size = 20\n        vocab_size = 10000\n    else:\n        raise ValueError(""Invalid model: %s"", param.model)\n\n    # Load PTB dataset\n    train_data, valid_data, test_data, vocab_size = tl.files.load_ptb_dataset()\n    # train_data = train_data[0:int(100000/5)]    # for fast testing\n    print(\'len(train_data) {}\'.format(len(train_data)))  # 929589 a list of int\n    print(\'len(valid_data) {}\'.format(len(valid_data)))  # 73760  a list of int\n    print(\'len(test_data)  {}\'.format(len(test_data)))  # 82430  a list of int\n    print(\'vocab_size      {}\'.format(vocab_size))  # 10000\n\n    # One int represents one word, the meaning of batch_size here is not the\n    # same with MNIST example, it is the number of concurrent processes for\n    # computational reasons.\n\n    init = tf.random_uniform_initializer(-init_scale, init_scale)\n    net = PTB_Net(hidden_size=hidden_size, vocab_size=vocab_size, init=init, keep=keep_prob)\n\n    # Truncated Backpropagation for training\n    lr = tf.Variable(0.0, trainable=False)\n    train_weights = net.weights\n    optimizer = tf.optimizers.Adam(lr=lr)\n\n    print(net)\n\n    print(""\\nStart learning a language model by using PTB dataset"")\n    for i in range(max_max_epoch):\n        # decreases the initial learning rate after several\n        # epoachs (defined by ``max_epoch``), by multipling a ``lr_decay``.\n        new_lr_decay = lr_decay**max(i - max_epoch, 0.0)\n        lr.assign(learning_rate * new_lr_decay)\n\n        # Training\n        net.train()\n        print(""Epoch: %d/%d Learning rate: %.3f"" % (i + 1, max_max_epoch, lr.value()))\n        epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n        # reset all states at the begining of every epoch\n        lstm1_state = None\n        lstm2_state = None\n\n        for step, (x, y) in enumerate(tl.iterate.ptb_iterator(train_data, batch_size, num_steps)):\n\n            with tf.GradientTape() as tape:\n                ## compute outputs\n                logits, lstm1_state, lstm2_state = net(\n                    x, lstm1_initial_state=lstm1_state, lstm2_initial_state=lstm2_state\n                )\n                ## compute loss and update model\n                cost = tl.cost.cross_entropy(logits, tf.reshape(y, [-1]), name=\'train_loss\')\n\n            grad, _ = tf.clip_by_global_norm(tape.gradient(cost, train_weights), max_grad_norm)\n            optimizer.apply_gradients(zip(grad, train_weights))\n\n            costs += cost\n            iters += 1\n\n            if step % (epoch_size // 10) == 10:\n                print(\n                    ""%.3f perplexity: %.3f speed: %.0f wps"" % (\n                        step * 1.0 / epoch_size, np.exp(costs / iters), iters * batch_size * num_steps /\n                        (time.time() - start_time)\n                    )\n                )\n        train_perplexity = np.exp(costs / iters)\n        print(""Epoch: %d/%d Train Perplexity: %.3f"" % (i + 1, max_max_epoch, train_perplexity))\n\n        # Validing\n        net.eval()\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n        # reset all states at the begining of every epoch\n        lstm1_state = None\n        lstm2_state = None\n        for step, (x, y) in enumerate(tl.iterate.ptb_iterator(valid_data, batch_size, num_steps)):\n            ## compute outputs\n            logits, lstm1_state, lstm2_state = net(x, lstm1_initial_state=lstm1_state, lstm2_initial_state=lstm2_state)\n            ## compute loss and update model\n            cost = tl.cost.cross_entropy(logits, tf.reshape(y, [-1]), name=\'train_loss\')\n            costs += cost\n            iters += 1\n        valid_perplexity = np.exp(costs / iters)\n        print(""Epoch: %d/%d Valid Perplexity: %.3f"" % (i + 1, max_max_epoch, valid_perplexity))\n\n    print(""Evaluation"")\n    # Testing\n    net.eval()\n    # go through the test set step by step, it will take a while.\n    start_time = time.time()\n    costs = 0.0\n    iters = 0\n    # reset all states at the begining\n    lstm1_state = None\n    lstm2_state = None\n    for step, (x, y) in enumerate(tl.iterate.ptb_iterator(test_data, batch_size=1, num_steps=1)):\n        ## compute outputs\n        logits, lstm1_state, lstm2_state = net(x, lstm1_initial_state=lstm1_state, lstm2_initial_state=lstm2_state)\n        ## compute loss and update model\n        cost = tl.cost.cross_entropy(logits, tf.reshape(y, [-1]), name=\'train_loss\')\n        costs += cost\n        iters += 1\n    test_perplexity = np.exp(costs / iters)\n    print(""Test Perplexity: %.3f took %.2fs"" % (test_perplexity, time.time() - start_time))\n\n    print(\n        ""More example: Text generation using Trump\'s speech data: https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_generate_text.py -- def main_lstm_generate_text():""\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n\n# log of SmallConfig\n# Start learning a language model by using PTB dataset\n# Epoch: 1 Learning rate: 1.000\n# 0.004 perplexity: 5512.735 speed: 4555 wps\n# 0.104 perplexity: 841.289 speed: 8823 wps\n# 0.204 perplexity: 626.273 speed: 9292 wps\n# 0.304 perplexity: 505.628 speed: 9472 wps\n# 0.404 perplexity: 435.580 speed: 9551 wps\n# 0.504 perplexity: 390.108 speed: 9555 wps\n# 0.604 perplexity: 351.379 speed: 9546 wps\n# 0.703 perplexity: 324.846 speed: 9579 wps\n# 0.803 perplexity: 303.824 speed: 9574 wps\n# 0.903 perplexity: 284.468 speed: 9551 wps\n# Epoch: 1 Train Perplexity: 269.981\n# Epoch: 1 Valid Perplexity: 178.561\n# Epoch: 2 Learning rate: 1.000\n# 0.004 perplexity: 211.632 speed: 7697 wps\n# 0.104 perplexity: 151.509 speed: 9488 wps\n# 0.204 perplexity: 158.947 speed: 9674 wps\n# 0.304 perplexity: 153.963 speed: 9806 wps\n# 0.404 perplexity: 150.938 speed: 9817 wps\n# 0.504 perplexity: 148.413 speed: 9824 wps\n# 0.604 perplexity: 143.763 speed: 9765 wps\n# 0.703 perplexity: 141.616 speed: 9731 wps\n# 0.803 perplexity: 139.618 speed: 9781 wps\n# 0.903 perplexity: 135.880 speed: 9735 wps\n# Epoch: 2 Train Perplexity: 133.771\n# Epoch: 2 Valid Perplexity: 142.595\n# Epoch: 3 Learning rate: 1.000\n# 0.004 perplexity: 146.902 speed: 8345 wps\n# 0.104 perplexity: 105.647 speed: 9572 wps\n# 0.204 perplexity: 114.261 speed: 9585 wps\n# 0.304 perplexity: 111.237 speed: 9586 wps\n# 0.404 perplexity: 110.181 speed: 9605 wps\n# 0.504 perplexity: 109.383 speed: 9601 wps\n# 0.604 perplexity: 106.722 speed: 9635 wps\n# 0.703 perplexity: 106.075 speed: 9597 wps\n# 0.803 perplexity: 105.481 speed: 9624 wps\n# 0.903 perplexity: 103.262 speed: 9618 wps\n# Epoch: 3 Train Perplexity: 102.272\n# Epoch: 3 Valid Perplexity: 131.884\n# Epoch: 4 Learning rate: 1.000\n# 0.004 perplexity: 118.127 speed: 7867 wps\n# 0.104 perplexity: 85.530 speed: 9330 wps\n# 0.204 perplexity: 93.559 speed: 9399 wps\n# 0.304 perplexity: 91.141 speed: 9386 wps\n# 0.404 perplexity: 90.668 speed: 9462 wps\n# 0.504 perplexity: 90.366 speed: 9516 wps\n# 0.604 perplexity: 88.479 speed: 9477 wps\n# 0.703 perplexity: 88.275 speed: 9533 wps\n# 0.803 perplexity: 88.091 speed: 9560 wps\n# 0.903 perplexity: 86.430 speed: 9516 wps\n# Epoch: 4 Train Perplexity: 85.839\n# Epoch: 4 Valid Perplexity: 128.408\n# Epoch: 5 Learning rate: 1.000\n# 0.004 perplexity: 100.077 speed: 7682 wps\n# 0.104 perplexity: 73.856 speed: 9197 wps\n# 0.204 perplexity: 81.242 speed: 9266 wps\n# 0.304 perplexity: 79.315 speed: 9375 wps\n# 0.404 perplexity: 79.009 speed: 9439 wps\n# 0.504 perplexity: 78.874 speed: 9377 wps\n# 0.604 perplexity: 77.430 speed: 9436 wps\n# 0.703 perplexity: 77.415 speed: 9417 wps\n# 0.803 perplexity: 77.424 speed: 9407 wps\n# 0.903 perplexity: 76.083 speed: 9407 wps\n# Epoch: 5 Train Perplexity: 75.719\n# Epoch: 5 Valid Perplexity: 127.057\n# Epoch: 6 Learning rate: 0.500\n# 0.004 perplexity: 87.561 speed: 7130 wps\n# 0.104 perplexity: 64.202 speed: 9753 wps\n# 0.204 perplexity: 69.518 speed: 9537 wps\n# 0.304 perplexity: 66.868 speed: 9647 wps\n# 0.404 perplexity: 65.766 speed: 9538 wps\n# 0.504 perplexity: 64.967 speed: 9537 wps\n# 0.604 perplexity: 63.090 speed: 9565 wps\n# 0.703 perplexity: 62.415 speed: 9544 wps\n# 0.803 perplexity: 61.751 speed: 9504 wps\n# 0.903 perplexity: 60.027 speed: 9482 wps\n# Epoch: 6 Train Perplexity: 59.127\n# Epoch: 6 Valid Perplexity: 120.339\n# Epoch: 7 Learning rate: 0.250\n# 0.004 perplexity: 72.069 speed: 7683 wps\n# 0.104 perplexity: 53.331 speed: 9526 wps\n# 0.204 perplexity: 57.897 speed: 9572 wps\n# 0.304 perplexity: 55.557 speed: 9491 wps\n# 0.404 perplexity: 54.597 speed: 9483 wps\n# 0.504 perplexity: 53.817 speed: 9471 wps\n# 0.604 perplexity: 52.147 speed: 9511 wps\n# 0.703 perplexity: 51.473 speed: 9497 wps\n# 0.803 perplexity: 50.788 speed: 9521 wps\n# 0.903 perplexity: 49.203 speed: 9515 wps\n# Epoch: 7 Train Perplexity: 48.303\n# Epoch: 7 Valid Perplexity: 120.782\n# Epoch: 8 Learning rate: 0.125\n# 0.004 perplexity: 63.503 speed: 8425 wps\n# 0.104 perplexity: 47.324 speed: 9433 wps\n# 0.204 perplexity: 51.525 speed: 9653 wps\n# 0.304 perplexity: 49.405 speed: 9520 wps\n# 0.404 perplexity: 48.532 speed: 9487 wps\n# 0.504 perplexity: 47.800 speed: 9610 wps\n# 0.604 perplexity: 46.282 speed: 9554 wps\n# 0.703 perplexity: 45.637 speed: 9536 wps\n# 0.803 perplexity: 44.972 speed: 9493 wps\n# 0.903 perplexity: 43.506 speed: 9496 wps\n# Epoch: 8 Train Perplexity: 42.653\n# Epoch: 8 Valid Perplexity: 122.119\n# Epoch: 9 Learning rate: 0.062\n# 0.004 perplexity: 59.375 speed: 7158 wps\n# 0.104 perplexity: 44.223 speed: 9275 wps\n# 0.204 perplexity: 48.269 speed: 9459 wps\n# 0.304 perplexity: 46.273 speed: 9564 wps\n# 0.404 perplexity: 45.450 speed: 9604 wps\n# 0.504 perplexity: 44.749 speed: 9604 wps\n# 0.604 perplexity: 43.308 speed: 9619 wps\n# 0.703 perplexity: 42.685 speed: 9647 wps\n# 0.803 perplexity: 42.022 speed: 9673 wps\n# 0.903 perplexity: 40.616 speed: 9678 wps\n# Epoch: 9 Train Perplexity: 39.792\n# Epoch: 9 Valid Perplexity: 123.170\n# Epoch: 10 Learning rate: 0.031\n# 0.004 perplexity: 57.333 speed: 7183 wps\n# 0.104 perplexity: 42.631 speed: 9592 wps\n# 0.204 perplexity: 46.580 speed: 9518 wps\n# 0.304 perplexity: 44.625 speed: 9569 wps\n# 0.404 perplexity: 43.832 speed: 9576 wps\n# 0.504 perplexity: 43.153 speed: 9571 wps\n# 0.604 perplexity: 41.761 speed: 9557 wps\n# 0.703 perplexity: 41.159 speed: 9524 wps\n# 0.803 perplexity: 40.494 speed: 9527 wps\n# 0.903 perplexity: 39.111 speed: 9558 wps\n# Epoch: 10 Train Perplexity: 38.298\n# Epoch: 10 Valid Perplexity: 123.658\n# Epoch: 11 Learning rate: 0.016\n# 0.004 perplexity: 56.238 speed: 7190 wps\n# 0.104 perplexity: 41.771 speed: 9171 wps\n# 0.204 perplexity: 45.656 speed: 9415 wps\n# 0.304 perplexity: 43.719 speed: 9472 wps\n# 0.404 perplexity: 42.941 speed: 9483 wps\n# 0.504 perplexity: 42.269 speed: 9494 wps\n# 0.604 perplexity: 40.903 speed: 9530 wps\n# 0.703 perplexity: 40.314 speed: 9545 wps\n# 0.803 perplexity: 39.654 speed: 9580 wps\n# 0.903 perplexity: 38.287 speed: 9597 wps\n# Epoch: 11 Train Perplexity: 37.477\n# Epoch: 11 Valid Perplexity: 123.523\n# Epoch: 12 Learning rate: 0.008\n# 0.004 perplexity: 55.552 speed: 7317 wps\n# 0.104 perplexity: 41.267 speed: 9234 wps\n# 0.204 perplexity: 45.119 speed: 9461 wps\n# 0.304 perplexity: 43.204 speed: 9519 wps\n# 0.404 perplexity: 42.441 speed: 9453 wps\n# 0.504 perplexity: 41.773 speed: 9536 wps\n# 0.604 perplexity: 40.423 speed: 9555 wps\n# 0.703 perplexity: 39.836 speed: 9576 wps\n# 0.803 perplexity: 39.181 speed: 9579 wps\n# 0.903 perplexity: 37.827 speed: 9554 wps\n# Epoch: 12 Train Perplexity: 37.020\n# Epoch: 12 Valid Perplexity: 123.192\n# Epoch: 13 Learning rate: 0.004\n# 0.004 perplexity: 55.124 speed: 8234 wps\n# 0.104 perplexity: 40.970 speed: 9391 wps\n# 0.204 perplexity: 44.804 speed: 9525 wps\n# 0.304 perplexity: 42.912 speed: 9512 wps\n# 0.404 perplexity: 42.162 speed: 9536 wps\n# 0.504 perplexity: 41.500 speed: 9630 wps\n# 0.604 perplexity: 40.159 speed: 9591 wps\n# 0.703 perplexity: 39.574 speed: 9575 wps\n# 0.803 perplexity: 38.921 speed: 9613 wps\n# 0.903 perplexity: 37.575 speed: 9629 wps\n# Epoch: 13 Train Perplexity: 36.771\n# Epoch: 13 Valid Perplexity: 122.917\n# Evaluation\n# Test Perplexity: 116.723 took 124.06s\n\n# MediumConfig\n# Epoch: 1 Learning rate: 1.000\n# 0.008 perplexity: 5173.547 speed: 6469 wps\n# 0.107 perplexity: 1219.527 speed: 6453 wps\n# 0.206 perplexity: 866.163 speed: 6441 wps\n# 0.306 perplexity: 695.163 speed: 6428 wps\n# 0.405 perplexity: 598.464 speed: 6420 wps\n# 0.505 perplexity: 531.875 speed: 6422 wps\n# 0.604 perplexity: 477.079 speed: 6425 wps\n# 0.704 perplexity: 438.297 speed: 6428 wps\n# 0.803 perplexity: 407.928 speed: 6425 wps\n# 0.903 perplexity: 381.264 speed: 6429 wps\n# Epoch: 1 Train Perplexity: 360.795\n# Epoch: 1 Valid Perplexity: 208.854\n# ...\n# Epoch: 39 Learning rate: 0.001\n# 0.008 perplexity: 56.618 speed: 6357 wps\n# 0.107 perplexity: 43.375 speed: 6341 wps\n# 0.206 perplexity: 47.873 speed: 6336 wps\n# 0.306 perplexity: 46.408 speed: 6337 wps\n# 0.405 perplexity: 46.327 speed: 6337 wps\n# 0.505 perplexity: 46.115 speed: 6335 wps\n# 0.604 perplexity: 45.323 speed: 6336 wps\n# 0.704 perplexity: 45.286 speed: 6337 wps\n# 0.803 perplexity: 45.174 speed: 6336 wps\n# 0.903 perplexity: 44.334 speed: 6336 wps\n# Epoch: 39 Train Perplexity: 44.021\n# Epoch: 39 Valid Perplexity: 87.516\n# Evaluation\n# Test Perplexity: 83.858 took 167.58s\n'"
examples/text_ptb/tutorial_ptb_lstm_state_is_tuple.py,29,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""Example of Synced sequence input and output.\n\nThis is a reimpmentation of the TensorFlow official PTB example in :\ntensorflow/models/rnn/ptb\n\nThe batch_size can be seem as how many concurrent computations.n\nAs the following example shows, the first batch learn the sequence information by using 0 to 9.n\nThe second batch learn the sequence information by using 10 to 19.n\nSo it ignores the information from 9 to 10 !n\nIf only if we set the batch_size = 1, it will consider all information from 0 to 20.n\n\nThe meaning of batch_size here is not the same with the MNIST example. In MNIST example,\nbatch_size reflects how many examples we consider in each iteration, while in\nPTB example, batch_size is how many concurrent processes (segments)\nfor speed up computation.\n\nSome Information will be ignored if batch_size > 1, however, if your dataset\nis ""long"" enough (a text corpus usually has billions words), the ignored\ninformation would not effect the final result.\n\nIn PTB tutorial, we setted batch_size = 20, so we cut the dataset into 20 segments.\nAt the begining of each epoch, we initialize (reset) the 20 RNN states for 20\nsegments, then go through 20 segments separately.\n\nThe training data will be generated as follow:n\n\n>>> train_data = [i for i in range(20)]\n>>> for batch in tl.iterate.ptb_iterator(train_data, batch_size=2, num_steps=3):\n>>>     x, y = batch\n>>>     print(x, \'n\',y)\n... [[ 0  1  2] <---x                       1st subset/ iteration\n...  [10 11 12]]\n... [[ 1  2  3] <---y\n...  [11 12 13]]\n...\n... [[ 3  4  5]  <--- 1st batch input       2nd subset/ iteration\n...  [13 14 15]] <--- 2nd batch input\n... [[ 4  5  6]  <--- 1st batch target\n...  [14 15 16]] <--- 2nd batch target\n...\n... [[ 6  7  8]                             3rd subset/ iteration\n...  [16 17 18]]\n... [[ 7  8  9]\n...  [17 18 19]]\n\nHao Dong: This example can also be considered as pre-training of the word\nembedding matrix.\n\nAbout RNN\n----------\n$ Karpathy Blog : http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\nMore TensorFlow official RNN examples can be found here\n---------------------------------------------------------\n$ RNN for PTB : https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html#recurrent-neural-networks\n$ Seq2seq : https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html#sequence-to-sequence-models\n$ translation : tensorflow/models/rnn/translate\n\ntensorflow (0.9.0)\n\nExample / benchmark for building a PTB LSTM model.\n\nTrains the model described in:\n(Zaremba, et. al.) Recurrent Neural Network Regularization\nhttp://arxiv.org/abs/1409.2329\n\nThere are 3 supported model configurations:\n===========================================\n| config | epochs | train | valid  | test\n===========================================\n| small  | 13     | 37.99 | 121.39 | 115.91\n| medium | 39     | 48.45 |  86.16 |  82.07\n| large  | 55     | 37.87 |  82.62 |  78.29\nThe exact results may vary depending on the random initialization.\n\nThe hyperparameters used in the model:\n- init_scale - the initial scale of the weights\n- learning_rate - the initial value of the learning rate\n- max_grad_norm - the maximum permissible norm of the gradient\n- num_layers - the number of LSTM layers\n- num_steps - the number of unrolled steps of LSTM\n- hidden_size - the number of LSTM units\n- max_epoch - the number of epochs trained with the initial learning rate\n- max_max_epoch - the total number of epochs for training\n- keep_prob - the probability of keeping weights in the dropout layer\n- lr_decay - the decay of the learning rate for each epoch after ""max_epoch""\n- batch_size - the batch size\n\nThe data required for this example is in the data/ dir of the\nPTB dataset from Tomas Mikolov\'s webpage:\n\n$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n$ tar xvf simple-examples.tgz\n\nA) use the zero_state function on the cell object\n\nB) for an rnn, all time steps share weights. We use one matrix to keep all\ngate weights. Split by column into 4 parts to get the 4 gate weight matrices.\n\n""""""\n\nimport sys\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\nflags = tf.app.flags\n\nflags.DEFINE_string(""model"", ""small"", ""A type of model. Possible options are: small, medium, large."")\n\nif (tf.VERSION >= \'1.5\'):\n    # parse flags\n    flags.FLAGS(sys.argv, known_only=True)\n    flags.ArgumentParser()\n\nFLAGS = flags.FLAGS\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\n\n\ndef main(_):\n    """"""\n    The core of the model consists of an LSTM cell that processes one word at\n    a time and computes probabilities of the possible continuations of the\n    sentence. The memory state of the network is initialized with a vector\n    of zeros and gets updated after reading each word. Also, for computational\n    reasons, we will process data in mini-batches of size batch_size.\n    """"""\n    if FLAGS.model == ""small"":\n        init_scale = 0.1\n        learning_rate = 1.\n        max_grad_norm = 5\n        num_steps = 20\n        hidden_size = 200\n        max_epoch = 4\n        max_max_epoch = 13\n        keep_prob = 1.0\n        lr_decay = 0.5\n        batch_size = 20\n        vocab_size = 10000\n    elif FLAGS.model == ""medium"":\n        init_scale = 0.05\n        learning_rate = 1.0\n        max_grad_norm = 5\n        # num_layers = 2\n        num_steps = 35\n        hidden_size = 650\n        max_epoch = 6\n        max_max_epoch = 39\n        keep_prob = 0.5\n        lr_decay = 0.8\n        batch_size = 20\n        vocab_size = 10000\n    elif FLAGS.model == ""large"":\n        init_scale = 0.04\n        learning_rate = 1.0\n        max_grad_norm = 10\n        # num_layers = 2\n        num_steps = 35\n        hidden_size = 1500\n        max_epoch = 14\n        max_max_epoch = 55\n        keep_prob = 0.35\n        lr_decay = 1 / 1.15\n        batch_size = 20\n        vocab_size = 10000\n    else:\n        raise ValueError(""Invalid model: %s"", FLAGS.model)\n\n    # Load PTB dataset\n    train_data, valid_data, test_data, vocab_size = tl.files.load_ptb_dataset()\n    # train_data = train_data[0:int(100000/5)]    # for fast testing\n    print(\'len(train_data) {}\'.format(len(train_data)))  # 929589 a list of int\n    print(\'len(valid_data) {}\'.format(len(valid_data)))  # 73760  a list of int\n    print(\'len(test_data)  {}\'.format(len(test_data)))  # 82430  a list of int\n    print(\'vocab_size      {}\'.format(vocab_size))  # 10000\n\n    sess = tf.InteractiveSession()\n\n    # One int represents one word, the meaning of batch_size here is not the\n    # same with MNIST example, it is the number of concurrent processes for\n    # computational reasons.\n\n    # Training and Validation\n    input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n    # Testing (Evaluation)\n    input_data_test = tf.placeholder(tf.int32, [1, 1])\n    targets_test = tf.placeholder(tf.int32, [1, 1])\n\n    def inference(x, is_training, num_steps, reuse=None):\n        """"""If reuse is True, the inferences use the existing parameters,\n        then different inferences share the same parameters.\n\n        Note :\n        - For DynamicRNNLayer, you can set dropout and the number of RNN layer internally.\n        """"""\n        print(""\\nnum_steps : %d, is_training : %s, reuse : %s"" % (num_steps, is_training, reuse))\n        init = tf.random_uniform_initializer(-init_scale, init_scale)\n        with tf.variable_scope(""model"", reuse=reuse):\n            net = tl.layers.EmbeddingInputlayer(x, vocab_size, hidden_size, init, name=\'embedding\')\n            net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name=\'drop1\')\n            net = tl.layers.RNNLayer(\n                net,\n                cell_fn=tf.contrib.rnn.BasicLSTMCell,  # tf.nn.rnn_cell.BasicLSTMCell,\n                cell_init_args={\n                    \'forget_bias\': 0.0,\n                    \'state_is_tuple\': True\n                },\n                n_hidden=hidden_size,\n                initializer=init,\n                n_steps=num_steps,\n                return_last=False,\n                name=\'basic_lstm1\'\n            )\n            lstm1 = net\n            net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name=\'drop2\')\n            net = tl.layers.RNNLayer(\n                net,\n                cell_fn=tf.contrib.rnn.BasicLSTMCell,  # tf.nn.rnn_cell.BasicLSTMCell,\n                cell_init_args={\n                    \'forget_bias\': 0.0,\n                    \'state_is_tuple\': True\n                },\n                n_hidden=hidden_size,\n                initializer=init,\n                n_steps=num_steps,\n                return_last=False,\n                return_seq_2d=True,\n                name=\'basic_lstm2\'\n            )\n            lstm2 = net\n            # Alternatively, if return_seq_2d=False, in the above RNN layer,\n            # you can reshape the outputs as follow:\n            # net = tl.layers.ReshapeLayer(net,\n            #       shape=[-1, int(net.outputs._shape[-1])], name=\'reshape\')\n            net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name=\'drop3\')\n            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name=\'output\')\n        return net, lstm1, lstm2\n\n    # Inference for Training\n    net, lstm1, lstm2 = inference(input_data, is_training=True, num_steps=num_steps, reuse=None)\n    # Inference for Validating\n    net_val, lstm1_val, lstm2_val = inference(input_data, is_training=False, num_steps=num_steps, reuse=True)\n    # Inference for Testing (Evaluation)\n    net_test, lstm1_test, lstm2_test = inference(input_data_test, is_training=False, num_steps=1, reuse=True)\n\n    # sess.run(tf.global_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n\n    def loss_fn(outputs, targets, batch_size):\n        # See tl.cost.cross_entropy_seq()\n        # Returns the cost function of Cross-entropy of two sequences, implement\n        # softmax internally.\n        # outputs : 2D tensor [batch_size*num_steps, n_units of output layer]\n        # targets : 2D tensor [batch_size, num_steps], need to be reshaped.\n        # batch_size : RNN batch_size, number of concurrent processes.\n        # n_examples = batch_size * num_steps\n        # so\n        # cost is the averaged cost of each mini-batch (concurrent process).\n        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n            [outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)]\n        )\n        # [tf.ones([batch_size * num_steps])])\n        cost = tf.reduce_sum(loss) / batch_size\n        return cost\n\n    # Cost for Training\n    cost = loss_fn(net.outputs, targets, batch_size)\n    # Cost for Validating\n    cost_val = loss_fn(net_val.outputs, targets, batch_size)\n    # Cost for Testing (Evaluation)\n    cost_test = loss_fn(net_test.outputs, targets_test, 1)\n\n    # Truncated Backpropagation for training\n    with tf.variable_scope(\'learning_rate\'):\n        lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n    optimizer = tf.train.GradientDescentOptimizer(lr)\n    train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n    # sess.run(tf.global_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n\n    net.print_params()\n    net.print_layers()\n    tl.layers.print_all_variables()\n\n    print(""nStart learning a language model by using PTB dataset"")\n    for i in range(max_max_epoch):\n        # decreases the initial learning rate after several\n        # epoachs (defined by ``max_epoch``), by multipling a ``lr_decay``.\n        new_lr_decay = lr_decay**max(i - max_epoch, 0.0)\n        sess.run(tf.assign(lr, learning_rate * new_lr_decay))\n\n        # Training\n        print(""Epoch: %d/%d Learning rate: %.3f"" % (i + 1, max_max_epoch, sess.run(lr)))\n        epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n        # reset all states at the begining of every epoch\n        state1 = tl.layers.initialize_rnn_state(lstm1.initial_state)\n        state2 = tl.layers.initialize_rnn_state(lstm2.initial_state)\n        for step, (x, y) in enumerate(tl.iterate.ptb_iterator(train_data, batch_size, num_steps)):\n            feed_dict = {\n                input_data: x,\n                targets: y,\n                lstm1.initial_state.c: state1[0],\n                lstm1.initial_state.h: state1[1],\n                lstm2.initial_state.c: state2[0],\n                lstm2.initial_state.h: state2[1],\n            }\n            # For training, enable dropout\n            feed_dict.update(net.all_drop)\n            _cost, state1_c, state1_h, state2_c, state2_h, _ = sess.run(\n                [cost, lstm1.final_state.c, lstm1.final_state.h, lstm2.final_state.c, lstm2.final_state.h, train_op],\n                feed_dict=feed_dict\n            )\n            state1 = (state1_c, state1_h)\n            state2 = (state2_c, state2_h)\n\n            costs += _cost\n            iters += num_steps\n\n            if step % (epoch_size // 10) == 10:\n                print(\n                    ""%.3f perplexity: %.3f speed: %.0f wps"" %\n                    (step * 1.0 / epoch_size, np.exp(costs / iters), iters * batch_size / (time.time() - start_time))\n                )\n        train_perplexity = np.exp(costs / iters)\n        print(""Epoch: %d/%d Train Perplexity: %.3f"" % (i + 1, max_max_epoch, train_perplexity))\n\n        # Validation\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n        # reset all states at the begining of every epoch\n        state1 = tl.layers.initialize_rnn_state(lstm1_val.initial_state)\n        state2 = tl.layers.initialize_rnn_state(lstm2_val.initial_state)\n        for step, (x, y) in enumerate(tl.iterate.ptb_iterator(valid_data, batch_size, num_steps)):\n            feed_dict = {\n                input_data: x,\n                targets: y,\n                lstm1_val.initial_state.c: state1[0],\n                lstm1_val.initial_state.h: state1[1],\n                lstm2_val.initial_state.c: state2[0],\n                lstm2_val.initial_state.h: state2[1],\n            }\n            _cost, state1_c, state1_h, state2_c, state2_h, _ = sess.run(\n                [\n                    cost_val, lstm1_val.final_state.c, lstm1_val.final_state.h, lstm2_val.final_state.c,\n                    lstm2_val.final_state.h,\n                    tf.no_op()\n                ], feed_dict=feed_dict\n            )\n            state1 = (state1_c, state1_h)\n            state2 = (state2_c, state2_h)\n            costs += _cost\n            iters += num_steps\n        valid_perplexity = np.exp(costs / iters)\n        print(""Epoch: %d/%d Valid Perplexity: %.3f"" % (i + 1, max_max_epoch, valid_perplexity))\n\n    print(""Evaluation"")\n    # Testing\n    # go through the test set step by step, it will take a while.\n    start_time = time.time()\n    costs = 0.0\n    iters = 0\n    # reset all states at the begining\n    state1 = tl.layers.initialize_rnn_state(lstm1_test.initial_state)\n    state2 = tl.layers.initialize_rnn_state(lstm2_test.initial_state)\n    for step, (x, y) in enumerate(tl.iterate.ptb_iterator(test_data, batch_size=1, num_steps=1)):\n        feed_dict = {\n            input_data_test: x,\n            targets_test: y,\n            lstm1_test.initial_state.c: state1[0],\n            lstm1_test.initial_state.h: state1[1],\n            lstm2_test.initial_state.c: state2[0],\n            lstm2_test.initial_state.h: state2[1],\n        }\n        _cost, state1_c, state1_h, state2_c, state2_h = sess.run(\n            [\n                cost_test,\n                lstm1_test.final_state.c,\n                lstm1_test.final_state.h,\n                lstm2_test.final_state.c,\n                lstm2_test.final_state.h,\n            ], feed_dict=feed_dict\n        )\n        state1 = (state1_c, state1_h)\n        state2 = (state2_c, state2_h)\n        costs += _cost\n        iters += 1\n    test_perplexity = np.exp(costs / iters)\n    print(""Test Perplexity: %.3f took %.2fs"" % (test_perplexity, time.time() - start_time))\n\n    print(\n        ""More example: Text generation using Trump\'s speech data: https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_generate_text.py  -- def main_lstm_generate_text():""\n    )\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n\n# log of SmallConfig\n# Start learning a language model by using PTB dataset\n# Epoch: 1 Learning rate: 1.000\n# 0.004 perplexity: 5512.735 speed: 4555 wps\n# 0.104 perplexity: 841.289 speed: 8823 wps\n# 0.204 perplexity: 626.273 speed: 9292 wps\n# 0.304 perplexity: 505.628 speed: 9472 wps\n# 0.404 perplexity: 435.580 speed: 9551 wps\n# 0.504 perplexity: 390.108 speed: 9555 wps\n# 0.604 perplexity: 351.379 speed: 9546 wps\n# 0.703 perplexity: 324.846 speed: 9579 wps\n# 0.803 perplexity: 303.824 speed: 9574 wps\n# 0.903 perplexity: 284.468 speed: 9551 wps\n# Epoch: 1 Train Perplexity: 269.981\n# Epoch: 1 Valid Perplexity: 178.561\n# Epoch: 2 Learning rate: 1.000\n# 0.004 perplexity: 211.632 speed: 7697 wps\n# 0.104 perplexity: 151.509 speed: 9488 wps\n# 0.204 perplexity: 158.947 speed: 9674 wps\n# 0.304 perplexity: 153.963 speed: 9806 wps\n# 0.404 perplexity: 150.938 speed: 9817 wps\n# 0.504 perplexity: 148.413 speed: 9824 wps\n# 0.604 perplexity: 143.763 speed: 9765 wps\n# 0.703 perplexity: 141.616 speed: 9731 wps\n# 0.803 perplexity: 139.618 speed: 9781 wps\n# 0.903 perplexity: 135.880 speed: 9735 wps\n# Epoch: 2 Train Perplexity: 133.771\n# Epoch: 2 Valid Perplexity: 142.595\n# Epoch: 3 Learning rate: 1.000\n# 0.004 perplexity: 146.902 speed: 8345 wps\n# 0.104 perplexity: 105.647 speed: 9572 wps\n# 0.204 perplexity: 114.261 speed: 9585 wps\n# 0.304 perplexity: 111.237 speed: 9586 wps\n# 0.404 perplexity: 110.181 speed: 9605 wps\n# 0.504 perplexity: 109.383 speed: 9601 wps\n# 0.604 perplexity: 106.722 speed: 9635 wps\n# 0.703 perplexity: 106.075 speed: 9597 wps\n# 0.803 perplexity: 105.481 speed: 9624 wps\n# 0.903 perplexity: 103.262 speed: 9618 wps\n# Epoch: 3 Train Perplexity: 102.272\n# Epoch: 3 Valid Perplexity: 131.884\n# Epoch: 4 Learning rate: 1.000\n# 0.004 perplexity: 118.127 speed: 7867 wps\n# 0.104 perplexity: 85.530 speed: 9330 wps\n# 0.204 perplexity: 93.559 speed: 9399 wps\n# 0.304 perplexity: 91.141 speed: 9386 wps\n# 0.404 perplexity: 90.668 speed: 9462 wps\n# 0.504 perplexity: 90.366 speed: 9516 wps\n# 0.604 perplexity: 88.479 speed: 9477 wps\n# 0.703 perplexity: 88.275 speed: 9533 wps\n# 0.803 perplexity: 88.091 speed: 9560 wps\n# 0.903 perplexity: 86.430 speed: 9516 wps\n# Epoch: 4 Train Perplexity: 85.839\n# Epoch: 4 Valid Perplexity: 128.408\n# Epoch: 5 Learning rate: 1.000\n# 0.004 perplexity: 100.077 speed: 7682 wps\n# 0.104 perplexity: 73.856 speed: 9197 wps\n# 0.204 perplexity: 81.242 speed: 9266 wps\n# 0.304 perplexity: 79.315 speed: 9375 wps\n# 0.404 perplexity: 79.009 speed: 9439 wps\n# 0.504 perplexity: 78.874 speed: 9377 wps\n# 0.604 perplexity: 77.430 speed: 9436 wps\n# 0.703 perplexity: 77.415 speed: 9417 wps\n# 0.803 perplexity: 77.424 speed: 9407 wps\n# 0.903 perplexity: 76.083 speed: 9407 wps\n# Epoch: 5 Train Perplexity: 75.719\n# Epoch: 5 Valid Perplexity: 127.057\n# Epoch: 6 Learning rate: 0.500\n# 0.004 perplexity: 87.561 speed: 7130 wps\n# 0.104 perplexity: 64.202 speed: 9753 wps\n# 0.204 perplexity: 69.518 speed: 9537 wps\n# 0.304 perplexity: 66.868 speed: 9647 wps\n# 0.404 perplexity: 65.766 speed: 9538 wps\n# 0.504 perplexity: 64.967 speed: 9537 wps\n# 0.604 perplexity: 63.090 speed: 9565 wps\n# 0.703 perplexity: 62.415 speed: 9544 wps\n# 0.803 perplexity: 61.751 speed: 9504 wps\n# 0.903 perplexity: 60.027 speed: 9482 wps\n# Epoch: 6 Train Perplexity: 59.127\n# Epoch: 6 Valid Perplexity: 120.339\n# Epoch: 7 Learning rate: 0.250\n# 0.004 perplexity: 72.069 speed: 7683 wps\n# 0.104 perplexity: 53.331 speed: 9526 wps\n# 0.204 perplexity: 57.897 speed: 9572 wps\n# 0.304 perplexity: 55.557 speed: 9491 wps\n# 0.404 perplexity: 54.597 speed: 9483 wps\n# 0.504 perplexity: 53.817 speed: 9471 wps\n# 0.604 perplexity: 52.147 speed: 9511 wps\n# 0.703 perplexity: 51.473 speed: 9497 wps\n# 0.803 perplexity: 50.788 speed: 9521 wps\n# 0.903 perplexity: 49.203 speed: 9515 wps\n# Epoch: 7 Train Perplexity: 48.303\n# Epoch: 7 Valid Perplexity: 120.782\n# Epoch: 8 Learning rate: 0.125\n# 0.004 perplexity: 63.503 speed: 8425 wps\n# 0.104 perplexity: 47.324 speed: 9433 wps\n# 0.204 perplexity: 51.525 speed: 9653 wps\n# 0.304 perplexity: 49.405 speed: 9520 wps\n# 0.404 perplexity: 48.532 speed: 9487 wps\n# 0.504 perplexity: 47.800 speed: 9610 wps\n# 0.604 perplexity: 46.282 speed: 9554 wps\n# 0.703 perplexity: 45.637 speed: 9536 wps\n# 0.803 perplexity: 44.972 speed: 9493 wps\n# 0.903 perplexity: 43.506 speed: 9496 wps\n# Epoch: 8 Train Perplexity: 42.653\n# Epoch: 8 Valid Perplexity: 122.119\n# Epoch: 9 Learning rate: 0.062\n# 0.004 perplexity: 59.375 speed: 7158 wps\n# 0.104 perplexity: 44.223 speed: 9275 wps\n# 0.204 perplexity: 48.269 speed: 9459 wps\n# 0.304 perplexity: 46.273 speed: 9564 wps\n# 0.404 perplexity: 45.450 speed: 9604 wps\n# 0.504 perplexity: 44.749 speed: 9604 wps\n# 0.604 perplexity: 43.308 speed: 9619 wps\n# 0.703 perplexity: 42.685 speed: 9647 wps\n# 0.803 perplexity: 42.022 speed: 9673 wps\n# 0.903 perplexity: 40.616 speed: 9678 wps\n# Epoch: 9 Train Perplexity: 39.792\n# Epoch: 9 Valid Perplexity: 123.170\n# Epoch: 10 Learning rate: 0.031\n# 0.004 perplexity: 57.333 speed: 7183 wps\n# 0.104 perplexity: 42.631 speed: 9592 wps\n# 0.204 perplexity: 46.580 speed: 9518 wps\n# 0.304 perplexity: 44.625 speed: 9569 wps\n# 0.404 perplexity: 43.832 speed: 9576 wps\n# 0.504 perplexity: 43.153 speed: 9571 wps\n# 0.604 perplexity: 41.761 speed: 9557 wps\n# 0.703 perplexity: 41.159 speed: 9524 wps\n# 0.803 perplexity: 40.494 speed: 9527 wps\n# 0.903 perplexity: 39.111 speed: 9558 wps\n# Epoch: 10 Train Perplexity: 38.298\n# Epoch: 10 Valid Perplexity: 123.658\n# Epoch: 11 Learning rate: 0.016\n# 0.004 perplexity: 56.238 speed: 7190 wps\n# 0.104 perplexity: 41.771 speed: 9171 wps\n# 0.204 perplexity: 45.656 speed: 9415 wps\n# 0.304 perplexity: 43.719 speed: 9472 wps\n# 0.404 perplexity: 42.941 speed: 9483 wps\n# 0.504 perplexity: 42.269 speed: 9494 wps\n# 0.604 perplexity: 40.903 speed: 9530 wps\n# 0.703 perplexity: 40.314 speed: 9545 wps\n# 0.803 perplexity: 39.654 speed: 9580 wps\n# 0.903 perplexity: 38.287 speed: 9597 wps\n# Epoch: 11 Train Perplexity: 37.477\n# Epoch: 11 Valid Perplexity: 123.523\n# Epoch: 12 Learning rate: 0.008\n# 0.004 perplexity: 55.552 speed: 7317 wps\n# 0.104 perplexity: 41.267 speed: 9234 wps\n# 0.204 perplexity: 45.119 speed: 9461 wps\n# 0.304 perplexity: 43.204 speed: 9519 wps\n# 0.404 perplexity: 42.441 speed: 9453 wps\n# 0.504 perplexity: 41.773 speed: 9536 wps\n# 0.604 perplexity: 40.423 speed: 9555 wps\n# 0.703 perplexity: 39.836 speed: 9576 wps\n# 0.803 perplexity: 39.181 speed: 9579 wps\n# 0.903 perplexity: 37.827 speed: 9554 wps\n# Epoch: 12 Train Perplexity: 37.020\n# Epoch: 12 Valid Perplexity: 123.192\n# Epoch: 13 Learning rate: 0.004\n# 0.004 perplexity: 55.124 speed: 8234 wps\n# 0.104 perplexity: 40.970 speed: 9391 wps\n# 0.204 perplexity: 44.804 speed: 9525 wps\n# 0.304 perplexity: 42.912 speed: 9512 wps\n# 0.404 perplexity: 42.162 speed: 9536 wps\n# 0.504 perplexity: 41.500 speed: 9630 wps\n# 0.604 perplexity: 40.159 speed: 9591 wps\n# 0.703 perplexity: 39.574 speed: 9575 wps\n# 0.803 perplexity: 38.921 speed: 9613 wps\n# 0.903 perplexity: 37.575 speed: 9629 wps\n# Epoch: 13 Train Perplexity: 36.771\n# Epoch: 13 Valid Perplexity: 122.917\n# Evaluation\n# Test Perplexity: 116.723 took 124.06s\n\n# MediumConfig\n# Epoch: 1 Learning rate: 1.000\n# 0.008 perplexity: 5173.547 speed: 6469 wps\n# 0.107 perplexity: 1219.527 speed: 6453 wps\n# 0.206 perplexity: 866.163 speed: 6441 wps\n# 0.306 perplexity: 695.163 speed: 6428 wps\n# 0.405 perplexity: 598.464 speed: 6420 wps\n# 0.505 perplexity: 531.875 speed: 6422 wps\n# 0.604 perplexity: 477.079 speed: 6425 wps\n# 0.704 perplexity: 438.297 speed: 6428 wps\n# 0.803 perplexity: 407.928 speed: 6425 wps\n# 0.903 perplexity: 381.264 speed: 6429 wps\n# Epoch: 1 Train Perplexity: 360.795\n# Epoch: 1 Valid Perplexity: 208.854\n# ...\n# Epoch: 39 Learning rate: 0.001\n# 0.008 perplexity: 56.618 speed: 6357 wps\n# 0.107 perplexity: 43.375 speed: 6341 wps\n# 0.206 perplexity: 47.873 speed: 6336 wps\n# 0.306 perplexity: 46.408 speed: 6337 wps\n# 0.405 perplexity: 46.327 speed: 6337 wps\n# 0.505 perplexity: 46.115 speed: 6335 wps\n# 0.604 perplexity: 45.323 speed: 6336 wps\n# 0.704 perplexity: 45.286 speed: 6337 wps\n# 0.803 perplexity: 45.174 speed: 6336 wps\n# 0.903 perplexity: 44.334 speed: 6336 wps\n# Epoch: 39 Train Perplexity: 44.021\n# Epoch: 39 Valid Perplexity: 87.516\n# Evaluation\n# Test Perplexity: 83.858 took 167.58s\n'"
examples/text_word_embedding/tutorial_word2vec_basic.py,14,"b'# Copyright 2019 TensorLayer. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Vector Representations of Words.\n\nThis is the minimalistic reimplementation of\ntensorflow/examples/tutorials/word2vec/word2vec_basic.py\nThis basic example contains the code needed to download some data,\ntrain on it a bit and visualize the result by using t-SNE.\n\nOnce you get comfortable with reading and running the basic version,\nyou can graduate to\ntensorflow/models/embedding/word2vec.py\nwhich is a more serious implementation that showcases some more advanced\nTensorFlow principles about how to efficiently use threads to move data\ninto a text model, how to checkpoint during training, etc.\n\nIf your model is no longer I/O bound but you want still more performance, you\ncan take things further by writing your own TensorFlow Ops, as described in\nAdding a New Op. Again we\'ve provided an example of this for the Skip-Gram case\ntensorflow/models/embedding/word2vec_optimized.py.\n\nLink\n------\nhttps://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#vector-representations-of-words\n\n""""""\n\nimport argparse\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nimport tensorlayer as tl\nimport wget\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    ""--model"", default=\'one\', type=str, required=False, help=""The model name. It can be \'one\', \'two\', \'three\', \'four\'.""\n)\n\nFLAGS = parser.parse_args()\n\n\ndef main_word2vec_basic():\n\n    # Step 1: Download the data, read the context into a list of strings.\n    # Set hyperparameters.\n    words = tl.files.load_matt_mahoney_text8_dataset()\n    data_size = len(words)\n    print(data_size)  # 17005207\n    print(words[0:10])  # [\'anarchism\', \'originated\', \'as\', \'a\', \'term\', \'of\', \'abuse\', \'first\', \'used\', \'against\']\n    # exit()\n\n    resume = False  # load existing model, data and dictionaries\n    _UNK = ""_UNK""\n\n    if FLAGS.model == ""one"":\n        # toy setting (tensorflow/examples/tutorials/word2vec/word2vec_basic.py)\n        vocabulary_size = 50000  # maximum number of word in vocabulary\n        batch_size = 128\n        embedding_size = 128  # Dimension of the embedding vector (hidden layer).\n        skip_window = 1  # How many words to consider left and right.\n        num_skips = 2  # How many times to reuse an input to generate a label.\n        #     (should be double of \'skip_window\' so as to\n        #     use both left and right words)\n        num_sampled = 64  # Number of negative examples to sample.\n        #     more negative samples, higher loss\n        learning_rate = 1.0\n        n_epoch = 20\n        model_file_name = ""model_word2vec_50k_128""\n        # Eval 2084/15851 accuracy = 15.7%\n    elif FLAGS.model == ""two"":\n        # (tensorflow/models/embedding/word2vec.py)\n        vocabulary_size = 80000\n        batch_size = 20  # Note: small batch_size need more steps for a Epoch\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 100\n        learning_rate = 0.2\n        n_epoch = 15\n        model_file_name = ""model_word2vec_80k_200""\n        # 7.9%\n    elif FLAGS.model == ""three"":\n        # (tensorflow/models/embedding/word2vec_optimized.py)\n        vocabulary_size = 80000\n        batch_size = 500\n        embedding_size = 200\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.025\n        n_epoch = 20\n        model_file_name = ""model_word2vec_80k_200_opt""\n        # bad 0%\n    elif FLAGS.model == ""four"":\n        # see: Learning word embeddings efficiently with noise-contrastive estimation\n        vocabulary_size = 80000\n        batch_size = 100\n        embedding_size = 600\n        skip_window = 5\n        num_skips = 10\n        num_sampled = 25\n        learning_rate = 0.03\n        n_epoch = 200 * 10\n        model_file_name = ""model_word2vec_80k_600""\n        # bad\n    else:\n        raise Exception(""Invalid model: %s"" % FLAGS.model)\n\n    num_steps = int((data_size / batch_size) * n_epoch)  # total number of iteration\n\n    print(\'%d Steps in a Epoch, total Epochs %d\' % (int(data_size / batch_size), n_epoch))\n    print(\'   learning_rate: %f\' % learning_rate)\n    print(\'   batch_size: %d\' % batch_size)\n\n    # Step 2: Build the dictionary and replace rare words with \'UNK\' token.\n    print()\n    if resume:\n        print(""Load existing data and dictionaries"" + ""!"" * 10)\n        all_var = tl.files.load_npy_to_any(name=model_file_name + \'.npy\')\n        data = all_var[\'data\']\n        count = all_var[\'count\']\n        dictionary = all_var[\'dictionary\']\n        reverse_dictionary = all_var[\'reverse_dictionary\']\n    else:\n        data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True, _UNK)\n\n    print(\n        \'Most 5 common words (+UNK)\', count[:5]\n    )  # [[\'UNK\', 418391], (b\'the\', 1061396), (b\'of\', 593677), (b\'and\', 416629), (b\'one\', 411764)]\n    print(\'Sample data\', data[:10], [reverse_dictionary[i] for i in data[:10]])\n    # [5243, 3081, 12, 6, 195, 2, 3135, 46, 59, 156] [b\'anarchism\', b\'originated\', b\'as\', b\'a\', b\'term\', b\'of\', b\'abuse\', b\'first\', b\'used\', b\'against\']\n\n    del words  # Hint to reduce memory.\n\n    # Step 3: Function to generate a training batch for the Skip-Gram model.\n    print()\n\n    batch, labels, data_index = tl.nlp.generate_skip_gram_batch(\n        data=data, batch_size=8, num_skips=4, skip_window=2, data_index=0\n    )\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], \'->\', labels[i, 0], reverse_dictionary[labels[i, 0]])\n\n    batch, labels, data_index = tl.nlp.generate_skip_gram_batch(\n        data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0\n    )\n    for i in range(8):\n        print(batch[i], reverse_dictionary[batch[i]], \'->\', labels[i, 0], reverse_dictionary[labels[i, 0]])\n\n    # Step 4: Build a Skip-Gram model.\n    print()\n\n    # We pick a random validation set to sample nearest neighbors. Here we limit the\n    # validation samples to the words that have a low numeric ID, which by\n    # construction are also the most frequent.\n    valid_size = 16  # Random set of words to evaluate similarity on.\n    valid_window = 100  # Only pick dev samples in the head of the distribution.\n    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n    # a list of \'valid_size\' integers smaller than \'valid_window\'\n    # print(valid_examples)   # [90 85 20 33 35 62 37 63 88 38 82 58 83 59 48 64]\n    # n_epoch = int(num_steps / batch_size)\n\n    # train_inputs is a row vector, a input is an integer id of single word.\n    # train_labels is a column vector, a label is an integer id of single word.\n    # valid_dataset is a column vector, a valid set is an integer id of single word.\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Look up embeddings for inputs.\n    inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n    labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n\n    emb_net = tl.layers.Word2vecEmbedding(\n        vocabulary_size=vocabulary_size,\n        embedding_size=embedding_size,\n        num_sampled=num_sampled,\n        activate_nce_loss=True,  # nce loss is activated\n        nce_loss_args={},\n        E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0),\n        nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))),\n        nce_b_init=tl.initializers.constant(value=0.0),\n        name=\'word2vec_layer\',\n    )\n    emb, nce = emb_net([inputs, labels])\n\n    model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name=""word2vec_model"")\n\n    # Compute the average NCE loss for the batch.\n    # tf.nce_loss automatically draws a new sample of the negative labels\n    # each time we evaluate the loss.\n\n    # Construct the optimizer. Note: AdamOptimizer is very slow in this case\n    optimizer = tf.optimizers.Adagrad(learning_rate, initial_accumulator_value=0.1)\n\n    # normalized embedding\n    normalized_embeddings = emb_net.normalized_embeddings\n\n    # Step 5: Start training.\n    model.train()\n\n    if resume:\n        print(""Load existing model"" + ""!"" * 10)\n        model.load_weights(filepath=model_file_name + \'.hdf5\')\n\n    # save vocabulary to txt\n    tl.nlp.save_vocab(count, name=\'vocab_text8.txt\')\n\n    average_loss = 0\n    step = 0\n    print_freq = 2000\n    while step < num_steps:\n        start_time = time.time()\n        batch_inputs, batch_labels, data_index = tl.nlp.generate_skip_gram_batch(\n            data=data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window, data_index=data_index\n        )\n\n        # We perform one update step by evaluating the train_op (including it\n        # in the list of returned values for sess.run()\n\n        with tf.GradientTape() as tape:\n            outputs, nce_cost = model([batch_inputs, batch_labels])\n\n        grad = tape.gradient(nce_cost, model.trainable_weights)\n        optimizer.apply_gradients(zip(grad, model.trainable_weights))\n\n        average_loss += nce_cost\n\n        if step % print_freq == 0:\n            if step > 0:\n                average_loss /= print_freq\n            print(""Average loss at step %d/%d. loss: %f took: %fs/per step"" % \\\n                (step, num_steps, average_loss, time.time() - start_time))\n            average_loss = 0\n\n        # Prints out nearby words given a list of words.\n        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n        if step % (print_freq * 5) == 0:\n\n            # Compute the cosine similarity between minibatch examples and all embeddings.\n            # For simple visualization of validation set.\n            valid_embed = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n            sim = tf.matmul(valid_embed, normalized_embeddings, transpose_b=True)\n            sim = sim.numpy()\n            # multiply all valid word vector with all word vector.\n            # transpose_b=True, normalized_embeddings is transposed before multiplication.\n\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # number of nearest neighbors to print\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = ""Nearest to %s:"" % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = ""%s %s,"" % (log_str, close_word)\n                print(log_str)\n\n        if (step % (print_freq * 20) == 0) and (step != 0):\n            print(""Save model, data and dictionaries"" + ""!"" * 10)\n            model.save_weights(filepath=model_file_name + "".hdf5"")\n            tl.files.save_any_to_npy(\n                save_dict={\n                    \'data\': data,\n                    \'count\': count,\n                    \'dictionary\': dictionary,\n                    \'reverse_dictionary\': reverse_dictionary\n                }, name=model_file_name + \'.npy\'\n            )\n\n        # if step == num_steps-1:\n        #     keeptrain = input(""Training %d finished enter 1 to keep training: "" % num_steps)\n        #     if keeptrain == \'1\':\n        #         step = 0\n        #         learning_rate = float(input(""Input new learning rate: ""))\n        #         train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n        step += 1\n\n    # Step 6: Visualize the normalized embedding matrix by t-SNE.\n    print()\n\n    final_embeddings = normalized_embeddings  #.eval()\n    tl.visualize.tsne_embedding(final_embeddings, reverse_dictionary, plot_only=500, \\\n        second=5, saveable=False, name=\'word2vec_basic\')\n\n    # Step 7: Evaluate by analogy questions. see tensorflow/models/embedding/word2vec_optimized.py\n    print()\n    model.eval()\n\n    #  from tensorflow/models/embedding/word2vec.py\n    if not os.path.exists(""questions-words.txt""):\n        print(""Downloading file \'questions-words.txt\'"")\n        wget.download(\'http://download.tensorflow.org/data/questions-words.txt\')\n\n    analogy_questions = tl.nlp.read_analogies_file(eval_file=\'questions-words.txt\', word2id=dictionary)\n    # For each question (row in dist), find the top \'n_answer\' words.\n    n_answer = 4\n\n    def predict(analogy):\n        # The eval feeds three vectors of word ids for a, b, c, each of\n        # which is of size N, where N is the number of analogies we want to\n        # evaluate in one batch.\n        analogy_a = analogy[:, 0]  # [N]\n        analogy_b = analogy[:, 1]  # [N]\n        analogy_c = analogy[:, 2]  # [N]\n        # Each row of a_emb, b_emb, c_emb is a word\'s embedding vector.\n        # They all have the shape [N, emb_dim]\n        a_emb = tf.gather(normalized_embeddings, analogy_a)  # a\'s embs\n        b_emb = tf.gather(normalized_embeddings, analogy_b)  # b\'s embs\n        c_emb = tf.gather(normalized_embeddings, analogy_c)  # c\'s embs\n        # We expect that d\'s embedding vectors on the unit hyper-sphere is\n        # near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].\n        #   Bangkok Thailand Tokyo Japan -> Thailand - Bangkok = Japan - Tokyo\n        #   Japan = Tokyo + (Thailand - Bangkok)\n        #   d = c + (b - a)\n        target = c_emb + (b_emb - a_emb)\n        # Compute cosine distance between each pair of target and vocab.\n        # dist has shape [N, vocab_size].\n        dist = tf.matmul(target, normalized_embeddings, transpose_b=True)\n        """"""Predict the top 4 answers for analogy questions.""""""\n        _, pred_idx = tf.nn.top_k(dist, n_answer)\n\n        return pred_idx\n\n    # Evaluate analogy questions and reports accuracy.\n    #  i.e. How many questions we get right at precision@1.\n    correct = 0\n    total = analogy_questions.shape[0]\n    start = 0\n    while start < total:\n        limit = start + 2500\n        sub = analogy_questions[start:limit, :]  # question\n        idx = predict(sub)  # 4 answers for each question\n        # print(\'question:\', tl.nlp.word_ids_to_words(sub[0], reverse_dictionary))\n        # print(\'answers:\', tl.nlp.word_ids_to_words(idx[0], reverse_dictionary))\n        start = limit\n        for question in xrange(sub.shape[0]):\n            for j in xrange(n_answer):\n                # if one of the top 4 answers in correct, win !\n                if idx[question, j] == sub[question, 3]:\n                    # Bingo! We predicted correctly. E.g., [italy, rome, france, paris].\n                    print(\n                        j + 1, tl.nlp.word_ids_to_words([idx[question, j]], reverse_dictionary), \':\',\n                        tl.nlp.word_ids_to_words(sub[question, :], reverse_dictionary)\n                    )\n                    correct += 1\n                    break\n                elif idx[question, j] in sub[question, :3]:\n                    # We need to skip words already in the question.\n                    continue\n                else:\n                    # The correct label is not the precision@1\n                    break\n    print(""Eval %4d/%d accuracy = %4.1f%%"" % (correct, total, correct * 100.0 / total))\n\n\nif __name__ == \'__main__\':\n    main_word2vec_basic()\n'"
tensorlayer/cli/__init__.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""The tensorlayer.cli module provides a command-line tool for some common tasks.""""""\n'"
tensorlayer/cli/__main__.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport argparse\n\nfrom tensorlayer.cli import train\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(prog=\'tl\')\n    subparsers = parser.add_subparsers(dest=\'cmd\')\n    train_parser = subparsers.add_parser(\'train\', help=\'train a model using multiple local GPUs or CPUs.\')\n    train.build_arg_parser(train_parser)\n    args = parser.parse_args()\n    if args.cmd == \'train\':\n        train.main(args)\n    else:\n        parser.print_help()\n'"
tensorlayer/cli/train.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\ntl train\n========\n\n(Alpha release - usage might change later)\n\nThe tensorlayer.cli.train module provides the ``tl train`` subcommand.\nIt helps the user bootstrap a TensorFlow/TensorLayer program for distributed training\nusing multiple GPU cards or CPUs on a computer.\n\nYou need to first setup the `CUDA_VISIBLE_DEVICES <http://acceleware.com/blog/cudavisibledevices-masking-gpus>`_\nto tell ``tl train`` which GPUs are available. If the CUDA_VISIBLE_DEVICES is not given,\n``tl train`` would try best to discover all available GPUs.\n\nIn distribute training, each TensorFlow program needs a TF_CONFIG environment variable to describe\nthe cluster. It also needs a master daemon to\nmonitor all trainers. ``tl train`` is responsible\nfor automatically managing these two tasks.\n\nUsage\n-----\n\ntl train [-h] [-p NUM_PSS] [-c CPU_TRAINERS] <file> [args [args ...]]\n\n.. code-block:: bash\n\n  # example of using GPU 0 and 1 for training mnist\n  CUDA_VISIBLE_DEVICES=""0,1""\n  tl train example/tutorial_mnist_distributed.py\n\n  # example of using CPU trainers for inception v3\n  tl train -c 16 example/tutorial_imagenet_inceptionV3_distributed.py\n\n  # example of using GPU trainers for inception v3 with customized arguments\n  # as CUDA_VISIBLE_DEVICES is not given, tl would try to discover all available GPUs\n  tl train example/tutorial_imagenet_inceptionV3_distributed.py -- --batch_size 16\n\n\nCommand-line Arguments\n----------------------\n\n- ``file``: python file path.\n\n- ``NUM_PSS`` : The number of parameter servers.\n\n- ``CPU_TRAINERS``: The number of CPU trainers.\n\n  It is recommended that ``NUM_PSS + CPU_TRAINERS <= cpu count``\n\n- ``args``: Any parameter after ``--`` would be passed to the python program.\n\n\nNotes\n-----\nA parallel training program would require multiple parameter servers\nto help parallel trainers to exchange intermediate gradients.\nThe best number of parameter servers is often proportional to the\nsize of your model as well as the number of CPUs available.\nYou can control the number of parameter servers using the ``-p`` parameter.\n\nIf you have a single computer with massive CPUs, you can use the ``-c`` parameter\nto enable CPU-only parallel training.\nThe reason we are not supporting GPU-CPU co-training is because GPU and\nCPU are running at different speeds. Using them together in training would\nincur stragglers.\n\n""""""\n\nimport argparse\nimport json\nimport multiprocessing\nimport os\nimport platform\nimport re\nimport subprocess\nimport sys\n\nPORT_BASE = 10000\n\n\ndef _get_gpu_ids():\n    if \'CUDA_VISIBLE_DEVICES\' in os.environ:\n        return [int(x) for x in os.environ.get(\'CUDA_VISIBLE_DEVICES\', \'\').split(\',\')]\n    if platform.system() in [\'Darwin\', \'Linux\']:\n        return [int(d.replace(\'nvidia\', \'\')) for d in os.listdir(\'/dev\') if re.match(\'^nvidia\\d+$\', d)]\n    else:\n        print(\'Please set CUDA_VISIBLE_DEVICES (see http://acceleware.com/blog/cudavisibledevices-masking-gpus)\')\n        return []\n\n\nGPU_IDS = _get_gpu_ids()\n\n\ndef create_tf_config(cluster_spec, task_type, task_index):\n    return {\n        \'cluster\': cluster_spec,\n        \'task\': {\n            \'type\': task_type,\n            \'index\': task_index\n        },\n    }\n\n\ndef create_tf_jobs(cluster_spec, prog, args):\n    gpu_assignment = dict(((\'worker\', idx), gpu_idx) for (idx, gpu_idx) in enumerate(GPU_IDS))\n    for job_type in cluster_spec:\n        for task_index in range(len(cluster_spec[job_type])):\n            new_env = os.environ.copy()\n            new_env.update(\n                {\n                    \'CUDA_VISIBLE_DEVICES\': str(gpu_assignment.get((job_type, task_index), \'\')),\n                    \'TF_CONFIG\': json.dumps(create_tf_config(cluster_spec, job_type, task_index)),\n                }\n            )\n            yield subprocess.Popen([\'python3\', prog] + args, env=new_env)\n\n\ndef validate_arguments(args):\n    if args.num_pss < 1:\n        print(\'Value error: must have ore than one parameter servers.\')\n        exit(1)\n\n    if not GPU_IDS:\n        num_cpus = multiprocessing.cpu_count()\n        if args.cpu_trainers > num_cpus:\n            print(\'Value error: there are %s available CPUs but you are requiring %s.\' % (num_cpus, args.cpu_trainers))\n            exit(1)\n\n    if not os.path.isfile(args.file):\n        print(\'Value error: model trainning file does not exist\')\n        exit(1)\n\n\ndef main(args):\n    validate_arguments(args)\n    num_workers = len(GPU_IDS) if GPU_IDS else args.cpu_trainers\n    print(\'Using program %s with args %s\' % (args.file, \' \'.join(args.args)))\n    print(\'Using %d workers, %d parameter servers, %d GPUs.\' % (num_workers, args.num_pss, len(GPU_IDS)))\n    cluster_spec = {\n        \'ps\': [\'localhost: %d\' % (PORT_BASE + i) for i in range(args.num_pss)],\n        \'worker\': [\'localhost: %d\' % (PORT_BASE + args.num_pss + i) for i in range(num_workers)]\n    }\n    processes = list(create_tf_jobs(cluster_spec, args.file, args.args))\n    try:\n        print(\'Press ENTER to exit the training ...\')\n        sys.stdin.readline()\n    except KeyboardInterrupt:  # https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt\n        print(\'Keyboard interrupt received\')\n    finally:\n        print(\'stopping all subprocesses ...\')\n        for p in processes:\n            p.kill()\n        for p in processes:\n            p.wait()\n        print(\'END\')\n\n\ndef build_arg_parser(parser):\n    parser.add_argument(\'-p\', \'--pss\', dest=\'num_pss\', type=int, default=1, help=\'number of parameter servers\')\n    parser.add_argument(\'-c\', \'--cpu_trainers\', dest=\'cpu_trainers\', type=int, default=1, help=\'number of CPU trainers\')\n    parser.add_argument(\'file\', help=\'model trainning file path\')\n    parser.add_argument(\'args\', nargs=\'*\', type=str, help=\'arguments to <file>\')\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    build_arg_parser(parser)\n    args = parser.parse_args()\n    main(args)\n'"
tensorlayer/decorators/__init__.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nTensorLayer provides rich layer implementations trailed for\nvarious benchmarks and domain-specific problems. In addition, we also\nsupport transparent access to native TensorFlow parameters.\nFor example, we provide not only layers for local response normalization, but also\nlayers that allow user to apply ``tf.nn.lrn`` on ``network.outputs``.\nMore functions can be found in `TensorFlow API <https://www.tensorflow.org/versions/master/api_docs/index.html>`__.\n""""""\n\nfrom .deprecated import deprecated\nfrom .deprecated_alias import deprecated_alias\nfrom .method_decorator import private_method, protected_method\n\n__all__ = [\'deprecated\', \'deprecated_alias\', \'private_method\', \'protected_method\']\n'"
tensorlayer/decorators/deprecated.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport functools\nimport inspect\nimport sys\n\nimport wrapt\n\nfrom tensorlayer.decorators.utils import (\n    add_deprecation_notice_to_docstring, get_qualified_name, validate_deprecation_args\n)\n\n__all__ = [\'deprecated\']\n\n# Allow deprecation warnings to be silenced temporarily with a context manager.\n_PRINT_DEPRECATION_WARNINGS = True\n\n# Remember which deprecation warnings have been printed already.\n_PRINTED_WARNING = {}\n\n\ndef deprecated(wrapped=None, date=\'\', instructions=\'\', warn_once=True):\n\n    if wrapped is None:\n        return functools.partial(deprecated, date=date, instructions=instructions, warn_once=warn_once)\n\n    @wrapt.decorator\n    def wrapper(wrapped, instance=None, args=None, kwargs=None):\n\n        validate_deprecation_args(date, instructions)\n\n        if _PRINT_DEPRECATION_WARNINGS:\n\n            class_or_func_name = get_qualified_name(wrapped)\n\n            if class_or_func_name not in _PRINTED_WARNING:\n                if warn_once:\n                    _PRINTED_WARNING[class_or_func_name] = True\n\n                from tensorlayer import logging\n\n                logging.warning(\n                    \'%s: `%s.%s` (in file: %s) is deprecated and will be removed %s.\\n\'\n                    \'Instructions for updating: %s\\n\' % (\n                        ""Class"" if inspect.isclass(wrapped) else ""Function"", wrapped.__module__, class_or_func_name,\n                        wrapped.__code__.co_filename, \'in a future version\' if date is None else\n                        (\'after %s\' % date), instructions\n                    )\n                )\n\n        return wrapped(*args, **kwargs)\n\n    decorated = wrapper(wrapped)\n\n    if sys.version_info > (3, 0):  # docstring can only be edited with Python 3\n        wrapt.FunctionWrapper.__setattr__(\n            decorated, ""__doc__"", add_deprecation_notice_to_docstring(wrapped.__doc__, date, instructions)\n        )\n\n    return decorated\n'"
tensorlayer/decorators/deprecated_alias.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport functools\nimport warnings\n\nfrom tensorlayer import logging\n\n\ndef deprecated_alias(end_support_version, **aliases):\n\n    def deco(f):\n\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n\n            try:\n                func_name = ""{}.{}"".format(args[0].__class__.__name__, f.__name__)\n            except (NameError, IndexError):\n                func_name = f.__name__\n\n            rename_kwargs(kwargs, aliases, end_support_version, func_name)\n\n            return f(*args, **kwargs)\n\n        return wrapper\n\n    return deco\n\n\ndef rename_kwargs(kwargs, aliases, end_support_version, func_name):\n\n    for alias, new in aliases.items():\n\n        if alias in kwargs:\n\n            if new in kwargs:\n                raise TypeError(\'{}() received both {} and {}\'.format(func_name, alias, new))\n\n            warnings.warn(\'{}() - {} is deprecated; use {}\'.format(func_name, alias, new), DeprecationWarning)\n            logging.warning(\n                ""DeprecationWarning: {}(): ""\n                ""`{}` argument is deprecated and will be removed in version {}, ""\n                ""please change for `{}.`"".format(func_name, alias, end_support_version, new)\n            )\n            kwargs[new] = kwargs.pop(alias)\n'"
tensorlayer/decorators/method_decorator.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport inspect\n\n\ndef private_method(func):\n    """"""Decorator for making an instance method private.""""""\n\n    def func_wrapper(*args, **kwargs):\n        """"""Decorator wrapper function.""""""\n        outer_frame = inspect.stack()[1][0]\n        if \'self\' not in outer_frame.f_locals or outer_frame.f_locals[\'self\'] is not args[0]:\n            raise RuntimeError(\'%s.%s is a private method\' % (args[0].__class__.__name__, func.__name__))\n\n        return func(*args, **kwargs)\n\n    return func_wrapper\n\n\ndef protected_method(func):\n    """"""Decorator for making an instance method private.""""""\n\n    def func_wrapper(*args, **kwargs):\n        """"""Decorator wrapper function.""""""\n        outer_frame = inspect.stack()[1][0]\n\n        caller = inspect.getmro(outer_frame.f_locals[\'self\'].__class__)[:-1]\n        target = inspect.getmro(args[0].__class__)[:-1]\n\n        share_subsclass = False\n\n        for cls_ in target:\n            if issubclass(caller[0], cls_) or caller[0] is cls_:\n                share_subsclass = True\n                break\n\n        if (\'self\' not in outer_frame.f_locals or\n                outer_frame.f_locals[\'self\'] is not args[0]) and (not share_subsclass):\n            raise RuntimeError(\'%s.%s is a protected method\' % (args[0].__class__.__name__, func.__name__))\n\n        return func(*args, **kwargs)\n\n    return func_wrapper\n'"
tensorlayer/decorators/utils.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nNOTE: DO NOT REMOVE THESE FILES. They are copied from Tensorflow repository and are necessary to build the library without installing TF.\n\nSource: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/util\n\nThey replace the following imports:\n>>> from tensorflow.python.util import decorator_utils\n>>> from tensorflow.python.util.deprecation import _validate_deprecation_args\n""""""\n\nimport re\nimport sys\n\n__all__ = [""add_deprecation_notice_to_docstring"", ""get_qualified_name"", ""validate_deprecation_args""]\n\n\ndef add_deprecation_notice_to_docstring(doc, date, instructions):\n    return _add_deprecated_function_notice_to_docstring(doc, date, instructions)\n\n\ndef get_qualified_name(function):\n    # Python 3\n    if hasattr(function, \'__qualname__\'):\n        return function.__qualname__\n\n    # Python 2\n    if hasattr(function, \'im_class\'):\n        return function.im_class.__name__ + \'.\' + function.__name__\n    return function.__name__\n\n\ndef validate_deprecation_args(date, instructions):\n    if date is not None and not re.match(r\'20\\d\\d-[01]\\d-[0123]\\d\', date):\n        raise ValueError(\'Date must be YYYY-MM-DD.\')\n    if not instructions:\n        raise ValueError(\'Don\\\'t deprecate things without conversion instructions!\')\n\n\ndef _add_deprecated_function_notice_to_docstring(doc, date, instructions):\n    """"""Adds a deprecation notice to a docstring for deprecated functions.""""""\n\n    if instructions:\n        deprecation_message = """"""\n            .. warning::\n                **THIS FUNCTION IS DEPRECATED:** It will be removed after %s.\n                *Instructions for updating:* %s.\n        """""" % ((\'in a future version\' if date is None else (\'after %s\' % date)), instructions)\n\n    else:\n        deprecation_message = """"""\n            .. warning::\n                **THIS FUNCTION IS DEPRECATED:** It will be removed after %s.\n        """""" % ((\'in a future version\' if date is None else (\'after %s\' % date)))\n\n    main_text = [deprecation_message]\n\n    return _add_notice_to_docstring(doc, \'DEPRECATED FUNCTION\', main_text)\n\n\ndef _add_notice_to_docstring(doc, no_doc_str, notice):\n    """"""Adds a deprecation notice to a docstring.""""""\n    if not doc:\n        lines = [no_doc_str]\n\n    else:\n        lines = _normalize_docstring(doc).splitlines()\n\n    notice = [\'\'] + notice\n\n    if len(lines) > 1:\n        # Make sure that we keep our distance from the main body\n        if lines[1].strip():\n            notice.append(\'\')\n\n        lines[1:1] = notice\n    else:\n        lines += notice\n\n    return \'\\n\'.join(lines)\n\n\ndef _normalize_docstring(docstring):\n    """"""Normalizes the docstring.\n\n    Replaces tabs with spaces, removes leading and trailing blanks lines, and\n    removes any indentation.\n\n    Copied from PEP-257:\n    https://www.python.org/dev/peps/pep-0257/#handling-docstring-indentation\n\n    Args:\n        docstring: the docstring to normalize\n\n    Returns:\n        The normalized docstring\n    """"""\n    if not docstring:\n        return \'\'\n    # Convert tabs to spaces (following the normal Python rules)\n    # and split into a list of lines:\n    lines = docstring.expandtabs().splitlines()\n    # Determine minimum indentation (first line doesn\'t count):\n    # (we use sys.maxsize because sys.maxint doesn\'t exist in Python 3)\n    indent = sys.maxsize\n    for line in lines[1:]:\n        stripped = line.lstrip()\n        if stripped:\n            indent = min(indent, len(line) - len(stripped))\n    # Remove indentation (first line is special):\n    trimmed = [lines[0].strip()]\n    if indent < sys.maxsize:\n        for line in lines[1:]:\n            trimmed.append(line[indent:].rstrip())\n    # Strip off trailing and leading blank lines:\n    while trimmed and not trimmed[-1]:\n        trimmed.pop()\n    while trimmed and not trimmed[0]:\n        trimmed.pop(0)\n    # Return a single string:\n    return \'\\n\'.join(trimmed)\n'"
tensorlayer/files/__init__.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nTensorLayer provides rich layer implementations trailed for\nvarious benchmarks and domain-specific problems. In addition, we also\nsupport transparent access to native TensorFlow parameters.\nFor example, we provide not only layers for local response normalization, but also\nlayers that allow user to apply ``tf.nn.lrn`` on ``network.outputs``.\nMore functions can be found in `TensorFlow API <https://www.tensorflow.org/versions/master/api_docs/index.html>`__.\n""""""\n\nfrom tensorlayer.lazy_imports import LazyImport\n\nfrom .dataset_loaders.celebA_dataset import *\nfrom .dataset_loaders.cifar10_dataset import *\nfrom .dataset_loaders.cyclegan_dataset import *\nfrom .dataset_loaders.flickr_1M_dataset import *\nfrom .dataset_loaders.flickr_25k_dataset import *\nfrom .dataset_loaders.imdb_dataset import *\nfrom .dataset_loaders.matt_mahoney_dataset import *\nfrom .dataset_loaders.mnist_dataset import *\nfrom .dataset_loaders.mnist_fashion_dataset import *\nfrom .dataset_loaders.mpii_dataset import *\nfrom .dataset_loaders.nietzsche_dataset import *\nfrom .dataset_loaders.ptb_dataset import *\nfrom .dataset_loaders.voc_dataset import *\nfrom .dataset_loaders.wmt_en_fr_dataset import *\nfrom .utils import *\n\n__all__ = [\n    # Dataset Loaders\n    \'load_celebA_dataset\',\n    \'load_cifar10_dataset\',\n    \'load_cyclegan_dataset\',\n    \'load_fashion_mnist_dataset\',\n    \'load_flickr1M_dataset\',\n    \'load_flickr25k_dataset\',\n    \'load_imdb_dataset\',\n    \'load_matt_mahoney_text8_dataset\',\n    \'load_mnist_dataset\',\n    \'load_mpii_pose_dataset\',\n    \'load_nietzsche_dataset\',\n    \'load_ptb_dataset\',\n    \'load_voc_dataset\',\n    \'load_wmt_en_fr_dataset\',\n\n    # Util Functions\n    \'assign_params\',\n    \'del_file\',\n    \'del_folder\',\n    \'download_file_from_google_drive\',\n    \'exists_or_mkdir\',\n    \'file_exists\',\n    \'folder_exists\',\n    \'load_and_assign_npz\',\n    \'load_and_assign_npz_dict\',\n    \'load_ckpt\',\n    \'load_cropped_svhn\',\n    \'load_file_list\',\n    \'load_folder_list\',\n    \'load_npy_to_any\',\n    \'load_npz\',\n    \'maybe_download_and_extract\',\n    \'natural_keys\',\n    \'npz_to_W_pdf\',\n    \'read_file\',\n    \'save_any_to_npy\',\n    \'save_ckpt\',\n    \'save_npz\',\n    \'save_npz_dict\',\n    #\'save_graph\',\n    #\'load_graph\',\n    #\'save_graph_and_params\',\n    #\'load_graph_and_params\',\n    \'load_and_assign_ckpt\',\n    \'ckpt_to_npz_dict\'\n]\n'"
tensorlayer/files/utils.py,9,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport base64\nimport datetime\nimport gzip\nimport json\nimport math\nimport os\nimport pickle\nimport re\nimport shutil\n# import ast\nimport sys\nimport tarfile\nimport time\nimport zipfile\n\nimport cloudpickle\nimport h5py\nimport numpy as np\nimport scipy.io as sio\nimport tensorflow as tf\nfrom six.moves import cPickle\nfrom tensorflow.python.keras.saving import model_config as model_config_lib\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.util import serialization\nfrom tensorflow.python.util.tf_export import keras_export\nfrom tensorflow.python import pywrap_tensorflow\n\nimport progressbar\nimport tensorlayer as tl\nfrom tensorlayer import logging, nlp, utils, visualize\n\n# from six.moves import zip\n\nif sys.version_info[0] == 2:\n    from urllib import urlretrieve\nelse:\n    from urllib.request import urlretrieve\n\n# import tensorflow.contrib.eager.python.saver as tfes\n# TODO: tf2.0 not stable, cannot import tensorflow.contrib.eager.python.saver\n\n__all__ = [\n    \'assign_weights\',\n    \'del_file\',\n    \'del_folder\',\n    \'download_file_from_google_drive\',\n    \'exists_or_mkdir\',\n    \'file_exists\',\n    \'folder_exists\',\n    \'load_and_assign_npz\',\n    \'load_and_assign_npz_dict\',\n    \'load_ckpt\',\n    \'load_cropped_svhn\',\n    \'load_file_list\',\n    \'load_folder_list\',\n    \'load_npy_to_any\',\n    \'load_npz\',\n    \'maybe_download_and_extract\',\n    \'natural_keys\',\n    \'npz_to_W_pdf\',\n    \'read_file\',\n    \'save_any_to_npy\',\n    \'save_ckpt\',\n    \'save_npz\',\n    \'save_npz_dict\',\n    \'tf_variables_to_numpy\',\n    \'assign_tf_variable\',\n    \'save_weights_to_hdf5\',\n    \'load_hdf5_to_weights_in_order\',\n    \'load_hdf5_to_weights\',\n    \'save_hdf5_graph\',\n    \'load_hdf5_graph\',\n    # \'net2static_graph\',\n    \'static_graph2net\',\n    # \'save_pkl_graph\',\n    # \'load_pkl_graph\',\n    \'load_and_assign_ckpt\',\n    \'ckpt_to_npz_dict\',\n]\n\n\ndef func2str(expr):\n    b = cloudpickle.dumps(expr)\n    s = base64.b64encode(b).decode()\n    return s\n\n\ndef str2func(s):\n    b = base64.b64decode(s)\n    expr = cloudpickle.loads(b)\n    return expr\n\n\n# def net2static_graph(network):\n#     saved_file = dict()\n#     # if network._NameNone is True:\n#     #     saved_file.update({""name"": None})\n#     # else:\n#     #     saved_file.update({""name"": network.name})\n#     # if not isinstance(network.inputs, list):\n#     #     saved_file.update({""inputs"": network.inputs._info[0].name})\n#     # else:\n#     #     saved_inputs = []\n#     #     for saved_input in network.inputs:\n#     #         saved_inputs.append(saved_input._info[0].name)\n#     #     saved_file.update({""inputs"": saved_inputs})\n#     # if not isinstance(network.outputs, list):\n#     #     saved_file.update({""outputs"": network.outputs._info[0].name})\n#     # else:\n#     #     saved_outputs = []\n#     #     for saved_output in network.outputs:\n#     #         saved_outputs.append(saved_output._info[0].name)\n#     #     saved_file.update({""outputs"": saved_outputs})\n#     saved_file.update({""config"": network.config})\n#\n#     return saved_file\n\n\n@keras_export(\'keras.models.save_model\')\ndef save_keras_model(model):\n    # f.attrs[\'keras_model_config\'] = json.dumps(\n    #     {\n    #         \'class_name\': model.__class__.__name__,\n    #         \'config\': model.get_config()\n    #     },\n    #     default=serialization.get_json_type).encode(\'utf8\')\n    #\n    # f.flush()\n\n    return json.dumps(\n        {\n            \'class_name\': model.__class__.__name__,\n            \'config\': model.get_config()\n        }, default=serialization.get_json_type\n    ).encode(\'utf8\')\n\n\n@keras_export(\'keras.models.load_model\')\ndef load_keras_model(model_config):\n\n    custom_objects = {}\n\n    if model_config is None:\n        raise ValueError(\'No model found in config.\')\n    model_config = json.loads(model_config.decode(\'utf-8\'))\n    model = model_config_lib.model_from_config(model_config, custom_objects=custom_objects)\n\n    return model\n\n\ndef save_hdf5_graph(network, filepath=\'model.hdf5\', save_weights=False, customized_data=None):\n    """"""Save the architecture of TL model into a hdf5 file. Support saving model weights.\n\n    Parameters\n    -----------\n    network : TensorLayer Model.\n        The network to save.\n    filepath : str\n        The name of model file.\n    save_weights : bool\n        Whether to save model weights.\n    customized_data : dict\n        The user customized meta data.\n\n    Examples\n    --------\n    >>> # Save the architecture (with parameters)\n    >>> tl.files.save_hdf5_graph(network, filepath=\'model.hdf5\', save_weights=True)\n    >>> # Save the architecture (without parameters)\n    >>> tl.files.save_hdf5_graph(network, filepath=\'model.hdf5\', save_weights=False)\n    >>> # Load the architecture in another script (no parameters restore)\n    >>> net = tl.files.load_hdf5_graph(filepath=\'model.hdf5\', load_weights=False)\n    >>> # Load the architecture in another script (restore parameters)\n    >>> net = tl.files.load_hdf5_graph(filepath=\'model.hdf5\', load_weights=True)\n    """"""\n    if network.outputs is None:\n        raise RuntimeError(""save_hdf5_graph not support dynamic mode yet"")\n\n    logging.info(""[*] Saving TL model into {}, saving weights={}"".format(filepath, save_weights))\n\n    model_config = network.config  # net2static_graph(network)\n    model_config[""version_info""][""save_date""] = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc\n                                                                                  ).isoformat()\n    model_config_str = str(model_config)\n    customized_data_str = str(customized_data)\n    # version_info = {\n    #     ""tensorlayer_version"": tl.__version__,\n    #     ""backend"": ""tensorflow"",\n    #     ""backend_version"": tf.__version__,\n    #     ""training_device"": ""gpu"",\n    #     ""save_date"": datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).isoformat()\n    # }\n    # version_info_str = str(version_info)\n\n    with h5py.File(filepath, \'w\') as f:\n        f.attrs[""model_config""] = model_config_str.encode(\'utf8\')\n        f.attrs[""customized_data""] = customized_data_str.encode(\'utf8\')\n        # f.attrs[""version_info""] = version_info_str.encode(\'utf8\')\n        if save_weights:\n            _save_weights_to_hdf5_group(f, network.all_layers)\n        f.flush()\n\n    logging.info(""[*] Saved TL model into {}, saving weights={}"".format(filepath, save_weights))\n\n\ndef generate_func(args):\n    for key in args:\n        if isinstance(args[key], tuple) and args[key][0] == \'is_Func\':\n            fn = str2func(args[key][1])\n            args[key] = fn\n        # if key in [\'act\']:\n        #     # fn_dict = args[key]\n        #     # module_path = fn_dict[\'module_path\']\n        #     # func_name = fn_dict[\'func_name\']\n        #     # lib = importlib.import_module(module_path)\n        #     # fn = getattr(lib, func_name)\n        #     # args[key] = fn\n        #     fn = str2func(args[key])\n        #     args[key] = fn\n        # elif key in [\'fn\']:\n        #     fn = str2func(args[key])\n        #     args[key] = fn\n\n\ndef eval_layer(layer_kwargs):\n    layer_class = layer_kwargs.pop(\'class\')\n    args = layer_kwargs[\'args\']\n    layer_type = args.pop(\'layer_type\')\n    if layer_type == ""normal"":\n        generate_func(args)\n        return eval(\'tl.layers.\' + layer_class)(**args)\n    elif layer_type == ""layerlist"":\n        ret_layer = []\n        layers = args[""layers""]\n        for layer_graph in layers:\n            ret_layer.append(eval_layer(layer_graph))\n        args[\'layers\'] = ret_layer\n        return eval(\'tl.layers.\' + layer_class)(**args)\n    elif layer_type == ""modellayer"":\n        M = static_graph2net(args[\'model\'])\n        args[\'model\'] = M\n        return eval(\'tl.layers.\' + layer_class)(**args)\n    elif layer_type == ""keraslayer"":\n        M = load_keras_model(args[\'fn\'])\n        input_shape = args.pop(\'keras_input_shape\')\n        _ = M(np.random.random(input_shape).astype(np.float32))\n        args[\'fn\'] = M\n        args[\'fn_weights\'] = M.trainable_variables\n        return eval(\'tl.layers.\' + layer_class)(**args)\n    else:\n        raise RuntimeError(""Unknown layer type."")\n\n\ndef static_graph2net(model_config):\n    layer_dict = {}\n    model_name = model_config[""name""]\n    inputs_tensors = model_config[""inputs""]\n    outputs_tensors = model_config[""outputs""]\n    all_args = model_config[""model_architecture""]\n    for idx, layer_kwargs in enumerate(all_args):\n        layer_class = layer_kwargs[""class""]  # class of current layer\n        prev_layers = layer_kwargs.pop(""prev_layer"")  # name of previous layers\n        net = eval_layer(layer_kwargs)\n        if layer_class in tl.layers.inputs.__all__:\n            net = net._nodes[0].out_tensors[0]\n        if prev_layers is not None:\n            for prev_layer in prev_layers:\n                if not isinstance(prev_layer, list):\n                    output = net(layer_dict[prev_layer])\n                    layer_dict[output._info[0].name] = output\n                else:\n                    list_layers = [layer_dict[layer] for layer in prev_layer]\n                    output = net(list_layers)\n                    layer_dict[output._info[0].name] = output\n        else:\n            layer_dict[net._info[0].name] = net\n\n    if not isinstance(inputs_tensors, list):\n        model_inputs = layer_dict[inputs_tensors]\n    else:\n        model_inputs = []\n        for inputs_tensor in inputs_tensors:\n            model_inputs.append(layer_dict[inputs_tensor])\n    if not isinstance(outputs_tensors, list):\n        model_outputs = layer_dict[outputs_tensors]\n    else:\n        model_outputs = []\n        for outputs_tensor in outputs_tensors:\n            model_outputs.append(layer_dict[outputs_tensor])\n    from tensorlayer.models import Model\n    M = Model(inputs=model_inputs, outputs=model_outputs, name=model_name)\n    logging.info(""[*] Load graph finished"")\n    return M\n\n\ndef load_hdf5_graph(filepath=\'model.hdf5\', load_weights=False):\n    """"""Restore TL model archtecture from a a pickle file. Support loading model weights.\n\n    Parameters\n    -----------\n    filepath : str\n        The name of model file.\n    load_weights : bool\n        Whether to load model weights.\n\n    Returns\n    --------\n    network : TensorLayer Model.\n\n    Examples\n    --------\n    - see ``tl.files.save_hdf5_graph``\n    """"""\n    logging.info(""[*] Loading TL model from {}, loading weights={}"".format(filepath, load_weights))\n\n    f = h5py.File(filepath, \'r\')\n\n    model_config_str = f.attrs[""model_config""].decode(\'utf8\')\n    model_config = eval(model_config_str)\n\n    # version_info_str = f.attrs[""version_info""].decode(\'utf8\')\n    # version_info = eval(version_info_str)\n    version_info = model_config[""version_info""]\n    backend_version = version_info[""backend_version""]\n    tensorlayer_version = version_info[""tensorlayer_version""]\n    if backend_version != tf.__version__:\n        logging.warning(\n            ""Saved model uses tensorflow version {}, but now you are using tensorflow version {}"".format(\n                backend_version, tf.__version__\n            )\n        )\n    if tensorlayer_version != tl.__version__:\n        logging.warning(\n            ""Saved model uses tensorlayer version {}, but now you are using tensorlayer version {}"".format(\n                tensorlayer_version, tl.__version__\n            )\n        )\n\n    M = static_graph2net(model_config)\n    if load_weights:\n        if not (\'layer_names\' in f.attrs.keys()):\n            raise RuntimeError(""Saved model does not contain weights."")\n        M.load_weights(filepath=filepath)\n\n    f.close()\n\n    logging.info(""[*] Loaded TL model from {}, loading weights={}"".format(filepath, load_weights))\n\n    return M\n\n\n# def load_pkl_graph(name=\'model.pkl\'):\n#     """"""Restore TL model archtecture from a a pickle file. No parameters be restored.\n#\n#     Parameters\n#     -----------\n#     name : str\n#         The name of graph file.\n#\n#     Returns\n#     --------\n#     network : TensorLayer Model.\n#\n#     Examples\n#     --------\n#     >>> # It is better to use load_hdf5_graph\n#     """"""\n#     logging.info(""[*] Loading TL graph from {}"".format(name))\n#     with open(name, \'rb\') as file:\n#         saved_file = pickle.load(file)\n#\n#     M = static_graph2net(saved_file)\n#\n#     return M\n#\n#\n# def save_pkl_graph(network, name=\'model.pkl\'):\n#     """"""Save the architecture of TL model into a pickle file. No parameters be saved.\n#\n#     Parameters\n#     -----------\n#     network : TensorLayer layer\n#         The network to save.\n#     name : str\n#         The name of graph file.\n#\n#     Example\n#     --------\n#     >>> # It is better to use save_hdf5_graph\n#     """"""\n#     if network.outputs is None:\n#         raise AssertionError(""save_graph not support dynamic mode yet"")\n#\n#     logging.info(""[*] Saving TL graph into {}"".format(name))\n#\n#     saved_file = net2static_graph(network)\n#\n#     with open(name, \'wb\') as file:\n#         pickle.dump(saved_file, file, protocol=pickle.HIGHEST_PROTOCOL)\n#     logging.info(""[*] Saved graph"")\n\n\n# Load dataset functions\ndef load_mnist_dataset(shape=(-1, 784), path=\'data\'):\n    """"""Load the original mnist.\n\n    Automatically download MNIST dataset and return the training, validation and test set with 50000, 10000 and 10000 digit images respectively.\n\n    Parameters\n    ----------\n    shape : tuple\n        The shape of digit images (the default is (-1, 784), alternatively (-1, 28, 28, 1)).\n    path : str\n        The path that the data is downloaded to.\n\n    Returns\n    -------\n    X_train, y_train, X_val, y_val, X_test, y_test: tuple\n        Return splitted training/validation/test set respectively.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1,784), path=\'datasets\')\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n    """"""\n    return _load_mnist_dataset(shape, path, name=\'mnist\', url=\'http://yann.lecun.com/exdb/mnist/\')\n\n\ndef load_fashion_mnist_dataset(shape=(-1, 784), path=\'data\'):\n    """"""Load the fashion mnist.\n\n    Automatically download fashion-MNIST dataset and return the training, validation and test set with 50000, 10000 and 10000 fashion images respectively, `examples <http://marubon-ds.blogspot.co.uk/2017/09/fashion-mnist-exploring.html>`__.\n\n    Parameters\n    ----------\n    shape : tuple\n        The shape of digit images (the default is (-1, 784), alternatively (-1, 28, 28, 1)).\n    path : str\n        The path that the data is downloaded to.\n\n    Returns\n    -------\n    X_train, y_train, X_val, y_val, X_test, y_test: tuple\n        Return splitted training/validation/test set respectively.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_fashion_mnist_dataset(shape=(-1,784), path=\'datasets\')\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_fashion_mnist_dataset(shape=(-1, 28, 28, 1))\n    """"""\n    return _load_mnist_dataset(\n        shape, path, name=\'fashion_mnist\', url=\'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\'\n    )\n\n\ndef _load_mnist_dataset(shape, path, name=\'mnist\', url=\'http://yann.lecun.com/exdb/mnist/\'):\n    """"""A generic function to load mnist-like dataset.\n\n    Parameters:\n    ----------\n    shape : tuple\n        The shape of digit images.\n    path : str\n        The path that the data is downloaded to.\n    name : str\n        The dataset name you want to use(the default is \'mnist\').\n    url : str\n        The url of dataset(the default is \'http://yann.lecun.com/exdb/mnist/\').\n    """"""\n    path = os.path.join(path, name)\n\n    # Define functions for loading mnist-like data\'s images and labels.\n    # For convenience, they also download the requested files if needed.\n    def load_mnist_images(path, filename):\n        filepath = maybe_download_and_extract(filename, path, url)\n\n        logging.info(filepath)\n        # Read the inputs in Yann LeCun\'s binary format.\n        with gzip.open(filepath, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=16)\n        # The inputs are vectors now, we reshape them to monochrome 2D images,\n        # following the shape convention: (examples, channels, rows, columns)\n        data = data.reshape(shape)\n        # The inputs come as bytes, we convert them to float32 in range [0,1].\n        # (Actually to range [0, 255/256], for compatibility to the version\n        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n        return data / np.float32(256)\n\n    def load_mnist_labels(path, filename):\n        filepath = maybe_download_and_extract(filename, path, url)\n        # Read the labels in Yann LeCun\'s binary format.\n        with gzip.open(filepath, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=8)\n        # The labels are vectors of integers now, that\'s exactly what we want.\n        return data\n\n    # Download and read the training and test set images and labels.\n    logging.info(""Load or Download {0} > {1}"".format(name.upper(), path))\n    X_train = load_mnist_images(path, \'train-images-idx3-ubyte.gz\')\n    y_train = load_mnist_labels(path, \'train-labels-idx1-ubyte.gz\')\n    X_test = load_mnist_images(path, \'t10k-images-idx3-ubyte.gz\')\n    y_test = load_mnist_labels(path, \'t10k-labels-idx1-ubyte.gz\')\n\n    # We reserve the last 10000 training examples for validation.\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    # We just return all the arrays in order, as expected in main().\n    # (It doesn\'t matter how we do this as long as we can read them again.)\n    X_train = np.asarray(X_train, dtype=np.float32)\n    y_train = np.asarray(y_train, dtype=np.int32)\n    X_val = np.asarray(X_val, dtype=np.float32)\n    y_val = np.asarray(y_val, dtype=np.int32)\n    X_test = np.asarray(X_test, dtype=np.float32)\n    y_test = np.asarray(y_test, dtype=np.int32)\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\n\ndef load_cifar10_dataset(shape=(-1, 32, 32, 3), path=\'data\', plotable=False):\n    """"""Load CIFAR-10 dataset.\n\n    It consists of 60000 32x32 colour images in 10 classes, with\n    6000 images per class. There are 50000 training images and 10000 test images.\n\n    The dataset is divided into five training batches and one test batch, each with\n    10000 images. The test batch contains exactly 1000 randomly-selected images from\n    each class. The training batches contain the remaining images in random order,\n    but some training batches may contain more images from one class than another.\n    Between them, the training batches contain exactly 5000 images from each class.\n\n    Parameters\n    ----------\n    shape : tupe\n        The shape of digit images e.g. (-1, 3, 32, 32) and (-1, 32, 32, 3).\n    path : str\n        The path that the data is downloaded to, defaults is ``data/cifar10/``.\n    plotable : boolean\n        Whether to plot some image examples, False as default.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3))\n\n    References\n    ----------\n    - `CIFAR website <https://www.cs.toronto.edu/~kriz/cifar.html>`__\n    - `Data download link <https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz>`__\n    - `<https://teratail.com/questions/28932>`__\n\n    """"""\n    path = os.path.join(path, \'cifar10\')\n    logging.info(""Load or Download cifar10 > {}"".format(path))\n\n    # Helper function to unpickle the data\n    def unpickle(file):\n        fp = open(file, \'rb\')\n        if sys.version_info.major == 2:\n            data = pickle.load(fp)\n        elif sys.version_info.major == 3:\n            data = pickle.load(fp, encoding=\'latin-1\')\n        fp.close()\n        return data\n\n    filename = \'cifar-10-python.tar.gz\'\n    url = \'https://www.cs.toronto.edu/~kriz/\'\n    # Download and uncompress file\n    maybe_download_and_extract(filename, path, url, extract=True)\n\n    # Unpickle file and fill in data\n    X_train = None\n    y_train = []\n    for i in range(1, 6):\n        data_dic = unpickle(os.path.join(path, \'cifar-10-batches-py/\', ""data_batch_{}"".format(i)))\n        if i == 1:\n            X_train = data_dic[\'data\']\n        else:\n            X_train = np.vstack((X_train, data_dic[\'data\']))\n        y_train += data_dic[\'labels\']\n\n    test_data_dic = unpickle(os.path.join(path, \'cifar-10-batches-py/\', ""test_batch""))\n    X_test = test_data_dic[\'data\']\n    y_test = np.array(test_data_dic[\'labels\'])\n\n    if shape == (-1, 3, 32, 32):\n        X_test = X_test.reshape(shape)\n        X_train = X_train.reshape(shape)\n    elif shape == (-1, 32, 32, 3):\n        X_test = X_test.reshape(shape, order=\'F\')\n        X_train = X_train.reshape(shape, order=\'F\')\n        X_test = np.transpose(X_test, (0, 2, 1, 3))\n        X_train = np.transpose(X_train, (0, 2, 1, 3))\n    else:\n        X_test = X_test.reshape(shape)\n        X_train = X_train.reshape(shape)\n\n    y_train = np.array(y_train)\n\n    if plotable:\n\n        if sys.platform.startswith(\'darwin\'):\n            import matplotlib\n            matplotlib.use(\'TkAgg\')\n        import matplotlib.pyplot as plt\n\n        logging.info(\'\\nCIFAR-10\')\n        fig = plt.figure(1)\n\n        logging.info(\'Shape of a training image: X_train[0] %s\' % X_train[0].shape)\n\n        plt.ion()  # interactive mode\n        count = 1\n        for _ in range(10):  # each row\n            for _ in range(10):  # each column\n                _ = fig.add_subplot(10, 10, count)\n                if shape == (-1, 3, 32, 32):\n                    # plt.imshow(X_train[count-1], interpolation=\'nearest\')\n                    plt.imshow(np.transpose(X_train[count - 1], (1, 2, 0)), interpolation=\'nearest\')\n                    # plt.imshow(np.transpose(X_train[count-1], (2, 1, 0)), interpolation=\'nearest\')\n                elif shape == (-1, 32, 32, 3):\n                    plt.imshow(X_train[count - 1], interpolation=\'nearest\')\n                    # plt.imshow(np.transpose(X_train[count-1], (1, 0, 2)), interpolation=\'nearest\')\n                else:\n                    raise Exception(""Do not support the given \'shape\' to plot the image examples"")\n                plt.gca().xaxis.set_major_locator(plt.NullLocator())  # \xe4\xb8\x8d\xe6\x98\xbe\xe7\xa4\xba\xe5\x88\xbb\xe5\xba\xa6(tick)\n                plt.gca().yaxis.set_major_locator(plt.NullLocator())\n                count = count + 1\n        plt.draw()  # interactive mode\n        plt.pause(3)  # interactive mode\n\n        logging.info(""X_train: %s"" % X_train.shape)\n        logging.info(""y_train: %s"" % y_train.shape)\n        logging.info(""X_test:  %s"" % X_test.shape)\n        logging.info(""y_test:  %s"" % y_test.shape)\n\n    X_train = np.asarray(X_train, dtype=np.float32)\n    X_test = np.asarray(X_test, dtype=np.float32)\n    y_train = np.asarray(y_train, dtype=np.int32)\n    y_test = np.asarray(y_test, dtype=np.int32)\n\n    return X_train, y_train, X_test, y_test\n\n\ndef load_cropped_svhn(path=\'data\', include_extra=True):\n    """"""Load Cropped SVHN.\n\n    The Cropped Street View House Numbers (SVHN) Dataset contains 32x32x3 RGB images.\n    Digit \'1\' has label 1, \'9\' has label 9 and \'0\' has label 0 (the original dataset uses 10 to represent \'0\'), see `ufldl website <http://ufldl.stanford.edu/housenumbers/>`__.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to.\n    include_extra : boolean\n        If True (default), add extra images to the training set.\n\n    Returns\n    -------\n    X_train, y_train, X_test, y_test: tuple\n        Return splitted training/test set respectively.\n\n    Examples\n    ---------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_cropped_svhn(include_extra=False)\n    >>> tl.vis.save_images(X_train[0:100], [10, 10], \'svhn.png\')\n\n    """"""\n    start_time = time.time()\n\n    path = os.path.join(path, \'cropped_svhn\')\n    logging.info(""Load or Download Cropped SVHN > {} | include extra images: {}"".format(path, include_extra))\n    url = ""http://ufldl.stanford.edu/housenumbers/""\n\n    np_file = os.path.join(path, ""train_32x32.npz"")\n    if file_exists(np_file) is False:\n        filename = ""train_32x32.mat""\n        filepath = maybe_download_and_extract(filename, path, url)\n        mat = sio.loadmat(filepath)\n        X_train = mat[\'X\'] / 255.0  # to [0, 1]\n        X_train = np.transpose(X_train, (3, 0, 1, 2))\n        y_train = np.squeeze(mat[\'y\'], axis=1)\n        y_train[y_train == 10] = 0  # replace 10 to 0\n        np.savez(np_file, X=X_train, y=y_train)\n        del_file(filepath)\n    else:\n        v = np.load(np_file, allow_pickle=True)\n        X_train = v[\'X\']\n        y_train = v[\'y\']\n    logging.info(""  n_train: {}"".format(len(y_train)))\n\n    np_file = os.path.join(path, ""test_32x32.npz"")\n    if file_exists(np_file) is False:\n        filename = ""test_32x32.mat""\n        filepath = maybe_download_and_extract(filename, path, url)\n        mat = sio.loadmat(filepath)\n        X_test = mat[\'X\'] / 255.0\n        X_test = np.transpose(X_test, (3, 0, 1, 2))\n        y_test = np.squeeze(mat[\'y\'], axis=1)\n        y_test[y_test == 10] = 0\n        np.savez(np_file, X=X_test, y=y_test)\n        del_file(filepath)\n    else:\n        v = np.load(np_file, allow_pickle=True)\n        X_test = v[\'X\']\n        y_test = v[\'y\']\n    logging.info(""  n_test: {}"".format(len(y_test)))\n\n    if include_extra:\n        logging.info(""  getting extra 531131 images, please wait ..."")\n        np_file = os.path.join(path, ""extra_32x32.npz"")\n        if file_exists(np_file) is False:\n            logging.info(""  the first time to load extra images will take long time to convert the file format ..."")\n            filename = ""extra_32x32.mat""\n            filepath = maybe_download_and_extract(filename, path, url)\n            mat = sio.loadmat(filepath)\n            X_extra = mat[\'X\'] / 255.0\n            X_extra = np.transpose(X_extra, (3, 0, 1, 2))\n            y_extra = np.squeeze(mat[\'y\'], axis=1)\n            y_extra[y_extra == 10] = 0\n            np.savez(np_file, X=X_extra, y=y_extra)\n            del_file(filepath)\n        else:\n            v = np.load(np_file, allow_pickle=True)\n            X_extra = v[\'X\']\n            y_extra = v[\'y\']\n        # print(X_train.shape, X_extra.shape)\n        logging.info(""  adding n_extra {} to n_train {}"".format(len(y_extra), len(y_train)))\n        t = time.time()\n        X_train = np.concatenate((X_train, X_extra), 0)\n        y_train = np.concatenate((y_train, y_extra), 0)\n        # X_train = np.append(X_train, X_extra, axis=0)\n        # y_train = np.append(y_train, y_extra, axis=0)\n        logging.info(""  added n_extra {} to n_train {} took {}s"".format(len(y_extra), len(y_train), time.time() - t))\n    else:\n        logging.info(""  no extra images are included"")\n    logging.info(""  image size: %s n_train: %d n_test: %d"" % (str(X_train.shape[1:4]), len(y_train), len(y_test)))\n    logging.info(""  took: {}s"".format(int(time.time() - start_time)))\n    return X_train, y_train, X_test, y_test\n\n\ndef load_ptb_dataset(path=\'data\'):\n    """"""Load Penn TreeBank (PTB) dataset.\n\n    It is used in many LANGUAGE MODELING papers,\n    including ""Empirical Evaluation and Combination of Advanced Language\n    Modeling Techniques"", ""Recurrent Neural Network Regularization"".\n    It consists of 929k training words, 73k validation words, and 82k test\n    words. It has 10k words in its vocabulary.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/ptb/``.\n\n    Returns\n    --------\n    train_data, valid_data, test_data : list of int\n        The training, validating and testing data in integer format.\n    vocab_size : int\n        The vocabulary size.\n\n    Examples\n    --------\n    >>> train_data, valid_data, test_data, vocab_size = tl.files.load_ptb_dataset()\n\n    References\n    ---------------\n    - ``tensorflow.models.rnn.ptb import reader``\n    - `Manual download <http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz>`__\n\n    Notes\n    ------\n    - If you want to get the raw data, see the source code.\n\n    """"""\n    path = os.path.join(path, \'ptb\')\n    logging.info(""Load or Download Penn TreeBank (PTB) dataset > {}"".format(path))\n\n    # Maybe dowload and uncompress tar, or load exsisting files\n    filename = \'simple-examples.tgz\'\n    url = \'http://www.fit.vutbr.cz/~imikolov/rnnlm/\'\n    maybe_download_and_extract(filename, path, url, extract=True)\n\n    data_path = os.path.join(path, \'simple-examples\', \'data\')\n    train_path = os.path.join(data_path, ""ptb.train.txt"")\n    valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n    test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n    word_to_id = nlp.build_vocab(nlp.read_words(train_path))\n\n    train_data = nlp.words_to_word_ids(nlp.read_words(train_path), word_to_id)\n    valid_data = nlp.words_to_word_ids(nlp.read_words(valid_path), word_to_id)\n    test_data = nlp.words_to_word_ids(nlp.read_words(test_path), word_to_id)\n    vocab_size = len(word_to_id)\n\n    # logging.info(nlp.read_words(train_path)) # ... \'according\', \'to\', \'mr.\', \'<unk>\', \'<eos>\']\n    # logging.info(train_data)                 # ...  214,         5,    23,    1,       2]\n    # logging.info(word_to_id)                 # ... \'beyond\': 1295, \'anti-nuclear\': 9599, \'trouble\': 1520, \'<eos>\': 2 ... }\n    # logging.info(vocabulary)                 # 10000\n    # exit()\n    return train_data, valid_data, test_data, vocab_size\n\n\ndef load_matt_mahoney_text8_dataset(path=\'data\'):\n    """"""Load Matt Mahoney\'s dataset.\n\n    Download a text file from Matt Mahoney\'s website\n    if not present, and make sure it\'s the right size.\n    Extract the first file enclosed in a zip file as a list of words.\n    This dataset can be used for Word Embedding.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/mm_test8/``.\n\n    Returns\n    --------\n    list of str\n        The raw text data e.g. [.... \'their\', \'families\', \'who\', \'were\', \'expelled\', \'from\', \'jerusalem\', ...]\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> print(\'Data size\', len(words))\n\n    """"""\n    path = os.path.join(path, \'mm_test8\')\n    logging.info(""Load or Download matt_mahoney_text8 Dataset> {}"".format(path))\n\n    filename = \'text8.zip\'\n    url = \'http://mattmahoney.net/dc/\'\n    maybe_download_and_extract(filename, path, url, expected_bytes=31344016)\n\n    with zipfile.ZipFile(os.path.join(path, filename)) as f:\n        word_list = f.read(f.namelist()[0]).split()\n        for idx, _ in enumerate(word_list):\n            word_list[idx] = word_list[idx].decode()\n    return word_list\n\n\ndef load_imdb_dataset(\n    path=\'data\', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2,\n    index_from=3\n):\n    """"""Load IMDB dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\n    nb_words : int\n        Number of words to get.\n    skip_top : int\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\n    maxlen : int\n        Maximum sequence length. Any longer sequence will be truncated.\n    seed : int\n        Seed for reproducible data shuffling.\n    start_char : int\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\n    oov_char : int\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\n    index_from : int\n        Index actual words with this index and higher.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\n    ...                                 nb_words=20000, test_split=0.2)\n    >>> print(\'X_train.shape\', X_train.shape)\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\n    >>> print(\'y_train.shape\', y_train.shape)\n    (20000,)  [1 0 0 ..., 1 0 1]\n\n    References\n    -----------\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\n\n    """"""\n    path = os.path.join(path, \'imdb\')\n\n    filename = ""imdb.pkl""\n    url = \'https://s3.amazonaws.com/text-datasets/\'\n    maybe_download_and_extract(filename, path, url)\n\n    if filename.endswith("".gz""):\n        f = gzip.open(os.path.join(path, filename), \'rb\')\n    else:\n        f = open(os.path.join(path, filename), \'rb\')\n\n    X, labels = cPickle.load(f)\n    f.close()\n\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for x, y in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception(\n            \'After filtering for sequences shorter than maxlen=\' + str(maxlen) + \', no sequence was kept. \'\n            \'Increase maxlen.\'\n        )\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n\n    # by convention, use 2 as OOV word\n    # reserve \'index_from\' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n    if oov_char is not None:\n        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if (w >= nb_words or w < skip_top):\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n\n    return X_train, y_train, X_test, y_test\n\n\ndef load_nietzsche_dataset(path=\'data\'):\n    """"""Load Nietzsche dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/nietzsche/``.\n\n    Returns\n    --------\n    str\n        The content.\n\n    Examples\n    --------\n    >>> see tutorial_generate_text.py\n    >>> words = tl.files.load_nietzsche_dataset()\n    >>> words = basic_clean_str(words)\n    >>> words = words.split()\n\n    """"""\n    logging.info(""Load or Download nietzsche dataset > {}"".format(path))\n    path = os.path.join(path, \'nietzsche\')\n\n    filename = ""nietzsche.txt""\n    url = \'https://s3.amazonaws.com/text-datasets/\'\n    filepath = maybe_download_and_extract(filename, path, url)\n\n    with open(filepath, ""r"") as f:\n        words = f.read()\n        return words\n\n\ndef load_wmt_en_fr_dataset(path=\'data\'):\n    """"""Load WMT\'15 English-to-French translation dataset.\n\n    It will download the data from the WMT\'15 Website (10^9-French-English corpus), and the 2013 news test from the same site as development set.\n    Returns the directories of training data and test data.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/wmt_en_fr/``.\n\n    References\n    ----------\n    - Code modified from /tensorflow/models/rnn/translation/data_utils.py\n\n    Notes\n    -----\n    Usually, it will take a long time to download this dataset.\n\n    """"""\n    path = os.path.join(path, \'wmt_en_fr\')\n    # URLs for WMT data.\n    _WMT_ENFR_TRAIN_URL = ""http://www.statmt.org/wmt10/""\n    _WMT_ENFR_DEV_URL = ""http://www.statmt.org/wmt15/""\n\n    def gunzip_file(gz_path, new_path):\n        """"""Unzips from gz_path into new_path.""""""\n        logging.info(""Unpacking %s to %s"" % (gz_path, new_path))\n        with gzip.open(gz_path, ""rb"") as gz_file:\n            with open(new_path, ""wb"") as new_file:\n                for line in gz_file:\n                    new_file.write(line)\n\n    def get_wmt_enfr_train_set(path):\n        """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n        filename = ""training-giga-fren.tar""\n        maybe_download_and_extract(filename, path, _WMT_ENFR_TRAIN_URL, extract=True)\n        train_path = os.path.join(path, ""giga-fren.release2.fixed"")\n        gunzip_file(train_path + "".fr.gz"", train_path + "".fr"")\n        gunzip_file(train_path + "".en.gz"", train_path + "".en"")\n        return train_path\n\n    def get_wmt_enfr_dev_set(path):\n        """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n        filename = ""dev-v2.tgz""\n        dev_file = maybe_download_and_extract(filename, path, _WMT_ENFR_DEV_URL, extract=False)\n        dev_name = ""newstest2013""\n        dev_path = os.path.join(path, ""newstest2013"")\n        if not (gfile.Exists(dev_path + "".fr"") and gfile.Exists(dev_path + "".en"")):\n            logging.info(""Extracting tgz file %s"" % dev_file)\n            with tarfile.open(dev_file, ""r:gz"") as dev_tar:\n                fr_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".fr"")\n                en_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".en"")\n                fr_dev_file.name = dev_name + "".fr""  # Extract without ""dev/"" prefix.\n                en_dev_file.name = dev_name + "".en""\n                dev_tar.extract(fr_dev_file, path)\n                dev_tar.extract(en_dev_file, path)\n        return dev_path\n\n    logging.info(""Load or Download WMT English-to-French translation > {}"".format(path))\n\n    train_path = get_wmt_enfr_train_set(path)\n    dev_path = get_wmt_enfr_dev_set(path)\n\n    return train_path, dev_path\n\n\ndef load_flickr25k_dataset(tag=\'sky\', path=""data"", n_threads=50, printable=False):\n    """"""Load Flickr25K dataset.\n\n    Returns a list of images by a given tag from Flick25k dataset,\n    it will download Flickr25k from `the official website <http://press.liacs.nl/mirflickr/mirdownload.html>`__\n    at the first time you use it.\n\n    Parameters\n    ------------\n    tag : str or None\n        What images to return.\n            - If you want to get images with tag, use string like \'dog\', \'red\', see `Flickr Search <https://www.flickr.com/search/>`__.\n            - If you want to get all images, set to ``None``.\n\n    path : str\n        The path that the data is downloaded to, defaults is ``data/flickr25k/``.\n    n_threads : int\n        The number of thread to read image.\n    printable : boolean\n        Whether to print infomation when reading images, default is ``False``.\n\n    Examples\n    -----------\n    Get images with tag of sky\n\n    >>> images = tl.files.load_flickr25k_dataset(tag=\'sky\')\n\n    Get all images\n\n    >>> images = tl.files.load_flickr25k_dataset(tag=None, n_threads=100, printable=True)\n\n    """"""\n    path = os.path.join(path, \'flickr25k\')\n\n    filename = \'mirflickr25k.zip\'\n    url = \'http://press.liacs.nl/mirflickr/mirflickr25k/\'\n\n    # download dataset\n    if folder_exists(os.path.join(path, ""mirflickr"")) is False:\n        logging.info(""[*] Flickr25k is nonexistent in {}"".format(path))\n        maybe_download_and_extract(filename, path, url, extract=True)\n        del_file(os.path.join(path, filename))\n\n    # return images by the given tag.\n    # 1. image path list\n    folder_imgs = os.path.join(path, ""mirflickr"")\n    path_imgs = load_file_list(path=folder_imgs, regx=\'\\\\.jpg\', printable=False)\n    path_imgs.sort(key=natural_keys)\n\n    # 2. tag path list\n    folder_tags = os.path.join(path, ""mirflickr"", ""meta"", ""tags"")\n    path_tags = load_file_list(path=folder_tags, regx=\'\\\\.txt\', printable=False)\n    path_tags.sort(key=natural_keys)\n\n    # 3. select images\n    if tag is None:\n        logging.info(""[Flickr25k] reading all images"")\n    else:\n        logging.info(""[Flickr25k] reading images with tag: {}"".format(tag))\n    images_list = []\n    for idx, _v in enumerate(path_tags):\n        tags = read_file(os.path.join(folder_tags, path_tags[idx])).split(\'\\n\')\n        # logging.info(idx+1, tags)\n        if tag is None or tag in tags:\n            images_list.append(path_imgs[idx])\n\n    images = visualize.read_images(images_list, folder_imgs, n_threads=n_threads, printable=printable)\n    return images\n\n\ndef load_flickr1M_dataset(tag=\'sky\', size=10, path=""data"", n_threads=50, printable=False):\n    """"""Load Flick1M dataset.\n\n    Returns a list of images by a given tag from Flickr1M dataset,\n    it will download Flickr1M from `the official website <http://press.liacs.nl/mirflickr/mirdownload.html>`__\n    at the first time you use it.\n\n    Parameters\n    ------------\n    tag : str or None\n        What images to return.\n            - If you want to get images with tag, use string like \'dog\', \'red\', see `Flickr Search <https://www.flickr.com/search/>`__.\n            - If you want to get all images, set to ``None``.\n\n    size : int\n        integer between 1 to 10. 1 means 100k images ... 5 means 500k images, 10 means all 1 million images. Default is 10.\n    path : str\n        The path that the data is downloaded to, defaults is ``data/flickr25k/``.\n    n_threads : int\n        The number of thread to read image.\n    printable : boolean\n        Whether to print infomation when reading images, default is ``False``.\n\n    Examples\n    ----------\n    Use 200k images\n\n    >>> images = tl.files.load_flickr1M_dataset(tag=\'zebra\', size=2)\n\n    Use 1 Million images\n\n    >>> images = tl.files.load_flickr1M_dataset(tag=\'zebra\')\n\n    """"""\n    path = os.path.join(path, \'flickr1M\')\n    logging.info(""[Flickr1M] using {}% of images = {}"".format(size * 10, size * 100000))\n    images_zip = [\n        \'images0.zip\', \'images1.zip\', \'images2.zip\', \'images3.zip\', \'images4.zip\', \'images5.zip\', \'images6.zip\',\n        \'images7.zip\', \'images8.zip\', \'images9.zip\'\n    ]\n    tag_zip = \'tags.zip\'\n    url = \'http://press.liacs.nl/mirflickr/mirflickr1m/\'\n\n    # download dataset\n    for image_zip in images_zip[0:size]:\n        image_folder = image_zip.split(""."")[0]\n        # logging.info(path+""/""+image_folder)\n        if folder_exists(os.path.join(path, image_folder)) is False:\n            # logging.info(image_zip)\n            logging.info(""[Flickr1M] {} is missing in {}"".format(image_folder, path))\n            maybe_download_and_extract(image_zip, path, url, extract=True)\n            del_file(os.path.join(path, image_zip))\n            # os.system(""mv {} {}"".format(os.path.join(path, \'images\'), os.path.join(path, image_folder)))\n            shutil.move(os.path.join(path, \'images\'), os.path.join(path, image_folder))\n        else:\n            logging.info(""[Flickr1M] {} exists in {}"".format(image_folder, path))\n\n    # download tag\n    if folder_exists(os.path.join(path, ""tags"")) is False:\n        logging.info(""[Flickr1M] tag files is nonexistent in {}"".format(path))\n        maybe_download_and_extract(tag_zip, path, url, extract=True)\n        del_file(os.path.join(path, tag_zip))\n    else:\n        logging.info(""[Flickr1M] tags exists in {}"".format(path))\n\n    # 1. image path list\n    images_list = []\n    images_folder_list = []\n    for i in range(0, size):\n        images_folder_list += load_folder_list(path=os.path.join(path, \'images%d\' % i))\n    images_folder_list.sort(key=lambda s: int(s.split(\'/\')[-1]))  # folder/images/ddd\n\n    for folder in images_folder_list[0:size * 10]:\n        tmp = load_file_list(path=folder, regx=\'\\\\.jpg\', printable=False)\n        tmp.sort(key=lambda s: int(s.split(\'.\')[-2]))  # ddd.jpg\n        images_list.extend([os.path.join(folder, x) for x in tmp])\n\n    # 2. tag path list\n    tag_list = []\n    tag_folder_list = load_folder_list(os.path.join(path, ""tags""))\n\n    # tag_folder_list.sort(key=lambda s: int(s.split(""/"")[-1]))  # folder/images/ddd\n    tag_folder_list.sort(key=lambda s: int(os.path.basename(s)))\n\n    for folder in tag_folder_list[0:size * 10]:\n        tmp = load_file_list(path=folder, regx=\'\\\\.txt\', printable=False)\n        tmp.sort(key=lambda s: int(s.split(\'.\')[-2]))  # ddd.txt\n        tmp = [os.path.join(folder, s) for s in tmp]\n        tag_list += tmp\n\n    # 3. select images\n    logging.info(""[Flickr1M] searching tag: {}"".format(tag))\n    select_images_list = []\n    for idx, _val in enumerate(tag_list):\n        tags = read_file(tag_list[idx]).split(\'\\n\')\n        if tag in tags:\n            select_images_list.append(images_list[idx])\n\n    logging.info(""[Flickr1M] reading images with tag: {}"".format(tag))\n    images = visualize.read_images(select_images_list, \'\', n_threads=n_threads, printable=printable)\n    return images\n\n\ndef load_cyclegan_dataset(filename=\'summer2winter_yosemite\', path=\'data\'):\n    """"""Load images from CycleGAN\'s database, see `this link <https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/>`__.\n\n    Parameters\n    ------------\n    filename : str\n        The dataset you want, see `this link <https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/>`__.\n    path : str\n        The path that the data is downloaded to, defaults is `data/cyclegan`\n\n    Examples\n    ---------\n    >>> im_train_A, im_train_B, im_test_A, im_test_B = load_cyclegan_dataset(filename=\'summer2winter_yosemite\')\n\n    """"""\n    path = os.path.join(path, \'cyclegan\')\n    url = \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\'\n\n    if folder_exists(os.path.join(path, filename)) is False:\n        logging.info(""[*] {} is nonexistent in {}"".format(filename, path))\n        maybe_download_and_extract(filename + \'.zip\', path, url, extract=True)\n        del_file(os.path.join(path, filename + \'.zip\'))\n\n    def load_image_from_folder(path):\n        path_imgs = load_file_list(path=path, regx=\'\\\\.jpg\', printable=False)\n        return visualize.read_images(path_imgs, path=path, n_threads=10, printable=False)\n\n    im_train_A = load_image_from_folder(os.path.join(path, filename, ""trainA""))\n    im_train_B = load_image_from_folder(os.path.join(path, filename, ""trainB""))\n    im_test_A = load_image_from_folder(os.path.join(path, filename, ""testA""))\n    im_test_B = load_image_from_folder(os.path.join(path, filename, ""testB""))\n\n    def if_2d_to_3d(images):  # [h, w] --> [h, w, 3]\n        for i, _v in enumerate(images):\n            if len(images[i].shape) == 2:\n                images[i] = images[i][:, :, np.newaxis]\n                images[i] = np.tile(images[i], (1, 1, 3))\n        return images\n\n    im_train_A = if_2d_to_3d(im_train_A)\n    im_train_B = if_2d_to_3d(im_train_B)\n    im_test_A = if_2d_to_3d(im_test_A)\n    im_test_B = if_2d_to_3d(im_test_B)\n\n    return im_train_A, im_train_B, im_test_A, im_test_B\n\n\ndef download_file_from_google_drive(ID, destination):\n    """"""Download file from Google Drive.\n\n    See ``tl.files.load_celebA_dataset`` for example.\n\n    Parameters\n    --------------\n    ID : str\n        The driver ID.\n    destination : str\n        The destination for save file.\n\n    """"""\n    try:\n        from tqdm import tqdm\n    except ImportError as e:\n        print(e)\n        raise ImportError(""Module tqdm not found. Please install tqdm via pip or other package managers."")\n\n    try:\n        import requests\n    except ImportError as e:\n        print(e)\n        raise ImportError(""Module requests not found. Please install requests via pip or other package managers."")\n\n    def save_response_content(response, destination, chunk_size=32 * 1024):\n\n        total_size = int(response.headers.get(\'content-length\', 0))\n        with open(destination, ""wb"") as f:\n            for chunk in tqdm(response.iter_content(chunk_size), total=total_size, unit=\'B\', unit_scale=True,\n                              desc=destination):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n\n    def get_confirm_token(response):\n        for key, value in response.cookies.items():\n            if key.startswith(\'download_warning\'):\n                return value\n        return None\n\n    URL = ""https://docs.google.com/uc?export=download""\n    session = requests.Session()\n\n    response = session.get(URL, params={\'id\': ID}, stream=True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = {\'id\': ID, \'confirm\': token}\n        response = session.get(URL, params=params, stream=True)\n    save_response_content(response, destination)\n\n\ndef load_celebA_dataset(path=\'data\'):\n    """"""Load CelebA dataset\n\n    Return a list of image path.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/celebA/``.\n\n    """"""\n    data_dir = \'celebA\'\n    filename, drive_id = ""img_align_celeba.zip"", ""0B7EVK8r0v71pZjFTYXZWM3FlRnM""\n    save_path = os.path.join(path, filename)\n    image_path = os.path.join(path, data_dir)\n    if os.path.exists(image_path):\n        logging.info(\'[*] {} already exists\'.format(save_path))\n    else:\n        exists_or_mkdir(path)\n        download_file_from_google_drive(drive_id, save_path)\n        zip_dir = \'\'\n        with zipfile.ZipFile(save_path) as zf:\n            zip_dir = zf.namelist()[0]\n            zf.extractall(path)\n        os.remove(save_path)\n        os.rename(os.path.join(path, zip_dir), image_path)\n\n    data_files = load_file_list(path=image_path, regx=\'\\\\.jpg\', printable=False)\n    for i, _v in enumerate(data_files):\n        data_files[i] = os.path.join(image_path, data_files[i])\n    return data_files\n\n\ndef load_voc_dataset(path=\'data\', dataset=\'2012\', contain_classes_in_person=False):\n    """"""Pascal VOC 2007/2012 Dataset.\n\n    It has 20 objects:\n    aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, sofa, train, tvmonitor\n    and additional 3 classes : head, hand, foot for person.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/VOC``.\n    dataset : str\n        The VOC dataset version, `2012`, `2007`, `2007test` or `2012test`. We usually train model on `2007+2012` and test it on `2007test`.\n    contain_classes_in_person : boolean\n        Whether include head, hand and foot annotation, default is False.\n\n    Returns\n    ---------\n    imgs_file_list : list of str\n        Full paths of all images.\n    imgs_semseg_file_list : list of str\n        Full paths of all maps for semantic segmentation. Note that not all images have this map!\n    imgs_insseg_file_list : list of str\n        Full paths of all maps for instance segmentation. Note that not all images have this map!\n    imgs_ann_file_list : list of str\n        Full paths of all annotations for bounding box and object class, all images have this annotations.\n    classes : list of str\n        Classes in order.\n    classes_in_person : list of str\n        Classes in person.\n    classes_dict : dictionary\n        Class label to integer.\n    n_objs_list : list of int\n        Number of objects in all images in ``imgs_file_list`` in order.\n    objs_info_list : list of str\n        Darknet format for the annotation of all images in ``imgs_file_list`` in order. ``[class_id x_centre y_centre width height]`` in ratio format.\n    objs_info_dicts : dictionary\n        The annotation of all images in ``imgs_file_list``, ``{imgs_file_list : dictionary for annotation}``,\n        format from `TensorFlow/Models/object-detection <https://github.com/tensorflow/models/blob/master/object_detection/create_pascal_tf_record.py>`__.\n\n    Examples\n    ----------\n    >>> imgs_file_list, imgs_semseg_file_list, imgs_insseg_file_list, imgs_ann_file_list,\n    >>>     classes, classes_in_person, classes_dict,\n    >>>     n_objs_list, objs_info_list, objs_info_dicts = tl.files.load_voc_dataset(dataset=""2012"", contain_classes_in_person=False)\n    >>> idx = 26\n    >>> print(classes)\n    [\'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\', \'pottedplant\', \'sheep\', \'sofa\', \'train\', \'tvmonitor\']\n    >>> print(classes_dict)\n    {\'sheep\': 16, \'horse\': 12, \'bicycle\': 1, \'bottle\': 4, \'cow\': 9, \'sofa\': 17, \'car\': 6, \'dog\': 11, \'cat\': 7, \'person\': 14, \'train\': 18, \'diningtable\': 10, \'aeroplane\': 0, \'bus\': 5, \'pottedplant\': 15, \'tvmonitor\': 19, \'chair\': 8, \'bird\': 2, \'boat\': 3, \'motorbike\': 13}\n    >>> print(imgs_file_list[idx])\n    data/VOC/VOC2012/JPEGImages/2007_000423.jpg\n    >>> print(n_objs_list[idx])\n    2\n    >>> print(imgs_ann_file_list[idx])\n    data/VOC/VOC2012/Annotations/2007_000423.xml\n    >>> print(objs_info_list[idx])\n    14 0.173 0.461333333333 0.142 0.496\n    14 0.828 0.542666666667 0.188 0.594666666667\n    >>> ann = tl.prepro.parse_darknet_ann_str_to_list(objs_info_list[idx])\n    >>> print(ann)\n    [[14, 0.173, 0.461333333333, 0.142, 0.496], [14, 0.828, 0.542666666667, 0.188, 0.594666666667]]\n    >>> c, b = tl.prepro.parse_darknet_ann_list_to_cls_box(ann)\n    >>> print(c, b)\n    [14, 14] [[0.173, 0.461333333333, 0.142, 0.496], [0.828, 0.542666666667, 0.188, 0.594666666667]]\n\n    References\n    -------------\n    - `Pascal VOC2012 Website <http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#devkit>`__.\n    - `Pascal VOC2007 Website <http://host.robots.ox.ac.uk/pascal/VOC/voc2007/>`__.\n\n    """"""\n\n    import xml.etree.ElementTree as ET\n\n    try:\n        import lxml.etree as etree\n    except ImportError as e:\n        print(e)\n        raise ImportError(""Module lxml not found. Please install lxml via pip or other package managers."")\n\n    path = os.path.join(path, \'VOC\')\n\n    def _recursive_parse_xml_to_dict(xml):\n        """"""Recursively parses XML contents to python dict.\n\n        We assume that `object` tags are the only ones that can appear\n        multiple times at the same level of a tree.\n\n        Args:\n            xml: xml tree obtained by parsing XML file contents using lxml.etree\n\n        Returns:\n            Python dictionary holding XML contents.\n\n        """"""\n        if not xml:\n            # if xml is not None:\n            return {xml.tag: xml.text}\n        result = {}\n        for child in xml:\n            child_result = _recursive_parse_xml_to_dict(child)\n            if child.tag != \'object\':\n                result[child.tag] = child_result[child.tag]\n            else:\n                if child.tag not in result:\n                    result[child.tag] = []\n                result[child.tag].append(child_result[child.tag])\n        return {xml.tag: result}\n\n    if dataset == ""2012"":\n        url = ""http://host.robots.ox.ac.uk/pascal/VOC/voc2012/""\n        tar_filename = ""VOCtrainval_11-May-2012.tar""\n        extracted_filename = ""VOC2012""  # ""VOCdevkit/VOC2012""\n        logging.info(""    [============= VOC 2012 =============]"")\n    elif dataset == ""2012test"":\n        extracted_filename = ""VOC2012test""  # ""VOCdevkit/VOC2012""\n        logging.info(""    [============= VOC 2012 Test Set =============]"")\n        logging.info(\n            ""    \\nAuthor: 2012test only have person annotation, so 2007test is highly recommended for testing !\\n""\n        )\n        time.sleep(3)\n        if os.path.isdir(os.path.join(path, extracted_filename)) is False:\n            logging.info(""For VOC 2012 Test data - online registration required"")\n            logging.info(\n                "" Please download VOC2012test.tar from:  \\n register: http://host.robots.ox.ac.uk:8080 \\n voc2012 : http://host.robots.ox.ac.uk:8080/eval/challenges/voc2012/ \\ndownload: http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar""\n            )\n            logging.info("" unzip VOC2012test.tar,rename the folder to VOC2012test and put it into %s"" % path)\n            exit()\n        # # http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar\n        # url = ""http://host.robots.ox.ac.uk:8080/eval/downloads/""\n        # tar_filename = ""VOC2012test.tar""\n    elif dataset == ""2007"":\n        url = ""http://host.robots.ox.ac.uk/pascal/VOC/voc2007/""\n        tar_filename = ""VOCtrainval_06-Nov-2007.tar""\n        extracted_filename = ""VOC2007""\n        logging.info(""    [============= VOC 2007 =============]"")\n    elif dataset == ""2007test"":\n        # http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html#testdata\n        # http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n        url = ""http://host.robots.ox.ac.uk/pascal/VOC/voc2007/""\n        tar_filename = ""VOCtest_06-Nov-2007.tar""\n        extracted_filename = ""VOC2007test""\n        logging.info(""    [============= VOC 2007 Test Set =============]"")\n    else:\n        raise Exception(""Please set the dataset aug to 2012, 2012test or 2007."")\n\n    # download dataset\n    if dataset != ""2012test"":\n        _platform = sys.platform\n        if folder_exists(os.path.join(path, extracted_filename)) is False:\n            logging.info(""[VOC] {} is nonexistent in {}"".format(extracted_filename, path))\n            maybe_download_and_extract(tar_filename, path, url, extract=True)\n            del_file(os.path.join(path, tar_filename))\n            if dataset == ""2012"":\n                if _platform == ""win32"":\n                    os.system(""mv {}\\VOCdevkit\\VOC2012 {}\\VOC2012"".format(path, path))\n                else:\n                    os.system(""mv {}/VOCdevkit/VOC2012 {}/VOC2012"".format(path, path))\n            elif dataset == ""2007"":\n                if _platform == ""win32"":\n                    os.system(""mv {}\\VOCdevkit\\VOC2007 {}\\VOC2007"".format(path, path))\n                else:\n                    os.system(""mv {}/VOCdevkit/VOC2007 {}/VOC2007"".format(path, path))\n            elif dataset == ""2007test"":\n                if _platform == ""win32"":\n                    os.system(""mv {}\\VOCdevkit\\VOC2007 {}\\VOC2007test"".format(path, path))\n                else:\n                    os.system(""mv {}/VOCdevkit/VOC2007 {}/VOC2007test"".format(path, path))\n            del_folder(os.path.join(path, \'VOCdevkit\'))\n    # object classes(labels)  NOTE: YOU CAN CUSTOMIZE THIS LIST\n    classes = [\n        ""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"",\n        ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""\n    ]\n    if contain_classes_in_person:\n        classes_in_person = [""head"", ""hand"", ""foot""]\n    else:\n        classes_in_person = []\n\n    classes += classes_in_person  # use extra 3 classes for person\n\n    classes_dict = utils.list_string_to_dict(classes)\n    logging.info(""[VOC] object classes {}"".format(classes_dict))\n\n    # 1. image path list\n    # folder_imgs = path+""/""+extracted_filename+""/JPEGImages/""\n    folder_imgs = os.path.join(path, extracted_filename, ""JPEGImages"")\n    imgs_file_list = load_file_list(path=folder_imgs, regx=\'\\\\.jpg\', printable=False)\n    logging.info(""[VOC] {} images found"".format(len(imgs_file_list)))\n\n    imgs_file_list.sort(\n        key=lambda s: int(s.replace(\'.\', \' \').replace(\'_\', \'\').split(\' \')[-2])\n    )  # 2007_000027.jpg --> 2007000027\n\n    imgs_file_list = [os.path.join(folder_imgs, s) for s in imgs_file_list]\n    # logging.info(\'IM\',imgs_file_list[0::3333], imgs_file_list[-1])\n    if dataset != ""2012test"":\n        # ======== 2. semantic segmentation maps path list\n        # folder_semseg = path+""/""+extracted_filename+""/SegmentationClass/""\n        folder_semseg = os.path.join(path, extracted_filename, ""SegmentationClass"")\n        imgs_semseg_file_list = load_file_list(path=folder_semseg, regx=\'\\\\.png\', printable=False)\n        logging.info(""[VOC] {} maps for semantic segmentation found"".format(len(imgs_semseg_file_list)))\n        imgs_semseg_file_list.sort(\n            key=lambda s: int(s.replace(\'.\', \' \').replace(\'_\', \'\').split(\' \')[-2])\n        )  # 2007_000032.png --> 2007000032\n        imgs_semseg_file_list = [os.path.join(folder_semseg, s) for s in imgs_semseg_file_list]\n        # logging.info(\'Semantic Seg IM\',imgs_semseg_file_list[0::333], imgs_semseg_file_list[-1])\n        # ======== 3. instance segmentation maps path list\n        # folder_insseg = path+""/""+extracted_filename+""/SegmentationObject/""\n        folder_insseg = os.path.join(path, extracted_filename, ""SegmentationObject"")\n        imgs_insseg_file_list = load_file_list(path=folder_insseg, regx=\'\\\\.png\', printable=False)\n        logging.info(""[VOC] {} maps for instance segmentation found"".format(len(imgs_semseg_file_list)))\n        imgs_insseg_file_list.sort(\n            key=lambda s: int(s.replace(\'.\', \' \').replace(\'_\', \'\').split(\' \')[-2])\n        )  # 2007_000032.png --> 2007000032\n        imgs_insseg_file_list = [os.path.join(folder_insseg, s) for s in imgs_insseg_file_list]\n        # logging.info(\'Instance Seg IM\',imgs_insseg_file_list[0::333], imgs_insseg_file_list[-1])\n    else:\n        imgs_semseg_file_list = []\n        imgs_insseg_file_list = []\n    # 4. annotations for bounding box and object class\n    # folder_ann = path+""/""+extracted_filename+""/Annotations/""\n    folder_ann = os.path.join(path, extracted_filename, ""Annotations"")\n    imgs_ann_file_list = load_file_list(path=folder_ann, regx=\'\\\\.xml\', printable=False)\n    logging.info(\n        ""[VOC] {} XML annotation files for bounding box and object class found"".format(len(imgs_ann_file_list))\n    )\n    imgs_ann_file_list.sort(\n        key=lambda s: int(s.replace(\'.\', \' \').replace(\'_\', \'\').split(\' \')[-2])\n    )  # 2007_000027.xml --> 2007000027\n    imgs_ann_file_list = [os.path.join(folder_ann, s) for s in imgs_ann_file_list]\n    # logging.info(\'ANN\',imgs_ann_file_list[0::3333], imgs_ann_file_list[-1])\n\n    if dataset == ""2012test"":  # remove unused images in JPEG folder\n        imgs_file_list_new = []\n        for ann in imgs_ann_file_list:\n            ann = os.path.split(ann)[-1].split(\'.\')[0]\n            for im in imgs_file_list:\n                if ann in im:\n                    imgs_file_list_new.append(im)\n                    break\n        imgs_file_list = imgs_file_list_new\n        logging.info(""[VOC] keep %d images"" % len(imgs_file_list_new))\n\n    # parse XML annotations\n    def convert(size, box):\n        dw = 1. / size[0]\n        dh = 1. / size[1]\n        x = (box[0] + box[1]) / 2.0\n        y = (box[2] + box[3]) / 2.0\n        w = box[1] - box[0]\n        h = box[3] - box[2]\n        x = x * dw\n        w = w * dw\n        y = y * dh\n        h = h * dh\n        return x, y, w, h\n\n    def convert_annotation(file_name):\n        """"""Given VOC2012 XML Annotations, returns number of objects and info.""""""\n        in_file = open(file_name)\n        out_file = """"\n        tree = ET.parse(in_file)\n        root = tree.getroot()\n        size = root.find(\'size\')\n        w = int(size.find(\'width\').text)\n        h = int(size.find(\'height\').text)\n        n_objs = 0\n\n        for obj in root.iter(\'object\'):\n            if dataset != ""2012test"":\n                difficult = obj.find(\'difficult\').text\n                cls = obj.find(\'name\').text\n                if cls not in classes or int(difficult) == 1:\n                    continue\n            else:\n                cls = obj.find(\'name\').text\n                if cls not in classes:\n                    continue\n            cls_id = classes.index(cls)\n            xmlbox = obj.find(\'bndbox\')\n            b = (\n                float(xmlbox.find(\'xmin\').text), float(xmlbox.find(\'xmax\').text), float(xmlbox.find(\'ymin\').text),\n                float(xmlbox.find(\'ymax\').text)\n            )\n            bb = convert((w, h), b)\n\n            out_file += str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\'\n            n_objs += 1\n            if cls in ""person"":\n                for part in obj.iter(\'part\'):\n                    cls = part.find(\'name\').text\n                    if cls not in classes_in_person:\n                        continue\n                    cls_id = classes.index(cls)\n                    xmlbox = part.find(\'bndbox\')\n                    b = (\n                        float(xmlbox.find(\'xmin\').text), float(xmlbox.find(\'xmax\').text),\n                        float(xmlbox.find(\'ymin\').text), float(xmlbox.find(\'ymax\').text)\n                    )\n                    bb = convert((w, h), b)\n                    # out_file.write(str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\')\n                    out_file += str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\'\n                    n_objs += 1\n        in_file.close()\n        return n_objs, out_file\n\n    logging.info(""[VOC] Parsing xml annotations files"")\n    n_objs_list = []\n    objs_info_list = []  # Darknet Format list of string\n    objs_info_dicts = {}\n    for idx, ann_file in enumerate(imgs_ann_file_list):\n        n_objs, objs_info = convert_annotation(ann_file)\n        n_objs_list.append(n_objs)\n        objs_info_list.append(objs_info)\n        with tf.io.gfile.GFile(ann_file, \'r\') as fid:\n            xml_str = fid.read()\n        xml = etree.fromstring(xml_str)\n        data = _recursive_parse_xml_to_dict(xml)[\'annotation\']\n        objs_info_dicts.update({imgs_file_list[idx]: data})\n\n    return imgs_file_list, imgs_semseg_file_list, imgs_insseg_file_list, imgs_ann_file_list, classes, classes_in_person, classes_dict, n_objs_list, objs_info_list, objs_info_dicts\n\n\ndef load_mpii_pose_dataset(path=\'data\', is_16_pos_only=False):\n    """"""Load MPII Human Pose Dataset.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to.\n    is_16_pos_only : boolean\n        If True, only return the peoples contain 16 pose keypoints. (Usually be used for single person pose estimation)\n\n    Returns\n    ----------\n    img_train_list : list of str\n        The image directories of training data.\n    ann_train_list : list of dict\n        The annotations of training data.\n    img_test_list : list of str\n        The image directories of testing data.\n    ann_test_list : list of dict\n        The annotations of testing data.\n\n    Examples\n    --------\n    >>> import pprint\n    >>> import tensorlayer as tl\n    >>> img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()\n    >>> image = tl.vis.read_image(img_train_list[0])\n    >>> tl.vis.draw_mpii_pose_to_image(image, ann_train_list[0], \'image.png\')\n    >>> pprint.pprint(ann_train_list[0])\n\n    References\n    -----------\n    - `MPII Human Pose Dataset. CVPR 14 <http://human-pose.mpi-inf.mpg.de>`__\n    - `MPII Human Pose Models. CVPR 16 <http://pose.mpi-inf.mpg.de>`__\n    - `MPII Human Shape, Poselet Conditioned Pictorial Structures and etc <http://pose.mpi-inf.mpg.de/#related>`__\n    - `MPII Keyponts and ID <http://human-pose.mpi-inf.mpg.de/#download>`__\n    """"""\n    path = os.path.join(path, \'mpii_human_pose\')\n    logging.info(""Load or Download MPII Human Pose > {}"".format(path))\n\n    # annotation\n    url = ""http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/""\n    tar_filename = ""mpii_human_pose_v1_u12_2.zip""\n    extracted_filename = ""mpii_human_pose_v1_u12_2""\n    if folder_exists(os.path.join(path, extracted_filename)) is False:\n        logging.info(""[MPII] (annotation) {} is nonexistent in {}"".format(extracted_filename, path))\n        maybe_download_and_extract(tar_filename, path, url, extract=True)\n        del_file(os.path.join(path, tar_filename))\n\n    # images\n    url = ""http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/""\n    tar_filename = ""mpii_human_pose_v1.tar.gz""\n    extracted_filename2 = ""images""\n    if folder_exists(os.path.join(path, extracted_filename2)) is False:\n        logging.info(""[MPII] (images) {} is nonexistent in {}"".format(extracted_filename, path))\n        maybe_download_and_extract(tar_filename, path, url, extract=True)\n        del_file(os.path.join(path, tar_filename))\n\n    # parse annotation, format see http://human-pose.mpi-inf.mpg.de/#download\n    logging.info(""reading annotations from mat file ..."")\n    # mat = sio.loadmat(os.path.join(path, extracted_filename, ""mpii_human_pose_v1_u12_1.mat""))\n\n    # def fix_wrong_joints(joint):    # https://github.com/mitmul/deeppose/blob/master/datasets/mpii_dataset.py\n    #     if \'12\' in joint and \'13\' in joint and \'2\' in joint and \'3\' in joint:\n    #         if ((joint[\'12\'][0] < joint[\'13\'][0]) and\n    #                 (joint[\'3\'][0] < joint[\'2\'][0])):\n    #             joint[\'2\'], joint[\'3\'] = joint[\'3\'], joint[\'2\']\n    #         if ((joint[\'12\'][0] > joint[\'13\'][0]) and\n    #                 (joint[\'3\'][0] > joint[\'2\'][0])):\n    #             joint[\'2\'], joint[\'3\'] = joint[\'3\'], joint[\'2\']\n    #     return joint\n\n    ann_train_list = []\n    ann_test_list = []\n    img_train_list = []\n    img_test_list = []\n\n    def save_joints():\n        # joint_data_fn = os.path.join(path, \'data.json\')\n        # fp = open(joint_data_fn, \'w\')\n        mat = sio.loadmat(os.path.join(path, extracted_filename, ""mpii_human_pose_v1_u12_1.mat""))\n\n        for _, (anno, train_flag) in enumerate(  # all images\n                zip(mat[\'RELEASE\'][\'annolist\'][0, 0][0], mat[\'RELEASE\'][\'img_train\'][0, 0][0])):\n\n            img_fn = anno[\'image\'][\'name\'][0, 0][0]\n            train_flag = int(train_flag)\n\n            # print(i, img_fn, train_flag) # DEBUG print all images\n\n            if train_flag:\n                img_train_list.append(img_fn)\n                ann_train_list.append([])\n            else:\n                img_test_list.append(img_fn)\n                ann_test_list.append([])\n\n            head_rect = []\n            if \'x1\' in str(anno[\'annorect\'].dtype):\n                head_rect = zip(\n                    [x1[0, 0] for x1 in anno[\'annorect\'][\'x1\'][0]], [y1[0, 0] for y1 in anno[\'annorect\'][\'y1\'][0]],\n                    [x2[0, 0] for x2 in anno[\'annorect\'][\'x2\'][0]], [y2[0, 0] for y2 in anno[\'annorect\'][\'y2\'][0]]\n                )\n            else:\n                head_rect = []  # TODO\n\n            if \'annopoints\' in str(anno[\'annorect\'].dtype):\n                annopoints = anno[\'annorect\'][\'annopoints\'][0]\n                head_x1s = anno[\'annorect\'][\'x1\'][0]\n                head_y1s = anno[\'annorect\'][\'y1\'][0]\n                head_x2s = anno[\'annorect\'][\'x2\'][0]\n                head_y2s = anno[\'annorect\'][\'y2\'][0]\n\n                for annopoint, head_x1, head_y1, head_x2, head_y2 in zip(annopoints, head_x1s, head_y1s, head_x2s,\n                                                                         head_y2s):\n                    # if annopoint != []:\n                    # if len(annopoint) != 0:\n                    if annopoint.size:\n                        head_rect = [\n                            float(head_x1[0, 0]),\n                            float(head_y1[0, 0]),\n                            float(head_x2[0, 0]),\n                            float(head_y2[0, 0])\n                        ]\n\n                        # joint coordinates\n                        annopoint = annopoint[\'point\'][0, 0]\n                        j_id = [str(j_i[0, 0]) for j_i in annopoint[\'id\'][0]]\n                        x = [x[0, 0] for x in annopoint[\'x\'][0]]\n                        y = [y[0, 0] for y in annopoint[\'y\'][0]]\n                        joint_pos = {}\n                        for _j_id, (_x, _y) in zip(j_id, zip(x, y)):\n                            joint_pos[int(_j_id)] = [float(_x), float(_y)]\n                        # joint_pos = fix_wrong_joints(joint_pos)\n\n                        # visibility list\n                        if \'is_visible\' in str(annopoint.dtype):\n                            vis = [v[0] if v.size > 0 else [0] for v in annopoint[\'is_visible\'][0]]\n                            vis = dict([(k, int(v[0])) if len(v) > 0 else v for k, v in zip(j_id, vis)])\n                        else:\n                            vis = None\n\n                        # if len(joint_pos) == 16:\n                        if ((is_16_pos_only ==True) and (len(joint_pos) == 16)) or (is_16_pos_only == False):\n                            # only use image with 16 key points / or use all\n                            data = {\n                                \'filename\': img_fn,\n                                \'train\': train_flag,\n                                \'head_rect\': head_rect,\n                                \'is_visible\': vis,\n                                \'joint_pos\': joint_pos\n                            }\n                            # print(json.dumps(data), file=fp)  # py3\n                            if train_flag:\n                                ann_train_list[-1].append(data)\n                            else:\n                                ann_test_list[-1].append(data)\n\n    # def write_line(datum, fp):\n    #     joints = sorted([[int(k), v] for k, v in datum[\'joint_pos\'].items()])\n    #     joints = np.array([j for i, j in joints]).flatten()\n    #\n    #     out = [datum[\'filename\']]\n    #     out.extend(joints)\n    #     out = [str(o) for o in out]\n    #     out = \',\'.join(out)\n    #\n    #     print(out, file=fp)\n\n    # def split_train_test():\n    #     # fp_test = open(\'data/mpii/test_joints.csv\', \'w\')\n    #     fp_test = open(os.path.join(path, \'test_joints.csv\'), \'w\')\n    #     # fp_train = open(\'data/mpii/train_joints.csv\', \'w\')\n    #     fp_train = open(os.path.join(path, \'train_joints.csv\'), \'w\')\n    #     # all_data = open(\'data/mpii/data.json\').readlines()\n    #     all_data = open(os.path.join(path, \'data.json\')).readlines()\n    #     N = len(all_data)\n    #     N_test = int(N * 0.1)\n    #     N_train = N - N_test\n    #\n    #     print(\'N:{}\'.format(N))\n    #     print(\'N_train:{}\'.format(N_train))\n    #     print(\'N_test:{}\'.format(N_test))\n    #\n    #     np.random.seed(1701)\n    #     perm = np.random.permutation(N)\n    #     test_indices = perm[:N_test]\n    #     train_indices = perm[N_test:]\n    #\n    #     print(\'train_indices:{}\'.format(len(train_indices)))\n    #     print(\'test_indices:{}\'.format(len(test_indices)))\n    #\n    #     for i in train_indices:\n    #         datum = json.loads(all_data[i].strip())\n    #         write_line(datum, fp_train)\n    #\n    #     for i in test_indices:\n    #         datum = json.loads(all_data[i].strip())\n    #         write_line(datum, fp_test)\n\n    save_joints()\n    # split_train_test()  #\n\n    # read images dir\n    logging.info(""reading images list ..."")\n    img_dir = os.path.join(path, extracted_filename2)\n    _img_list = load_file_list(path=os.path.join(path, extracted_filename2), regx=\'\\\\.jpg\', printable=False)\n    # ann_list = json.load(open(os.path.join(path, \'data.json\')))\n    for i, im in enumerate(img_train_list):\n        if im not in _img_list:\n            print(\'missing training image {} in {} (remove from img(ann)_train_list)\'.format(im, img_dir))\n            # img_train_list.remove(im)\n            del img_train_list[i]\n            del ann_train_list[i]\n    for i, im in enumerate(img_test_list):\n        if im not in _img_list:\n            print(\'missing testing image {} in {} (remove from img(ann)_test_list)\'.format(im, img_dir))\n            # img_test_list.remove(im)\n            del img_train_list[i]\n            del ann_train_list[i]\n\n    # check annotation and images\n    n_train_images = len(img_train_list)\n    n_test_images = len(img_test_list)\n    n_images = n_train_images + n_test_images\n    logging.info(""n_images: {} n_train_images: {} n_test_images: {}"".format(n_images, n_train_images, n_test_images))\n    n_train_ann = len(ann_train_list)\n    n_test_ann = len(ann_test_list)\n    n_ann = n_train_ann + n_test_ann\n    logging.info(""n_ann: {} n_train_ann: {} n_test_ann: {}"".format(n_ann, n_train_ann, n_test_ann))\n    n_train_people = len(sum(ann_train_list, []))\n    n_test_people = len(sum(ann_test_list, []))\n    n_people = n_train_people + n_test_people\n    logging.info(""n_people: {} n_train_people: {} n_test_people: {}"".format(n_people, n_train_people, n_test_people))\n    # add path to all image file name\n    for i, value in enumerate(img_train_list):\n        img_train_list[i] = os.path.join(img_dir, value)\n    for i, value in enumerate(img_test_list):\n        img_test_list[i] = os.path.join(img_dir, value)\n    return img_train_list, ann_train_list, img_test_list, ann_test_list\n\n\ndef save_npz(save_list=None, name=\'model.npz\'):\n    """"""Input parameters and the file name, save parameters into .npz file. Use tl.utils.load_npz() to restore.\n\n    Parameters\n    ----------\n    save_list : list of tensor\n        A list of parameters (tensor) to be saved.\n    name : str\n        The name of the `.npz` file.\n\n    Examples\n    --------\n    Save model to npz\n\n    >>> tl.files.save_npz(network.all_weights, name=\'model.npz\')\n\n    Load model from npz (Method 1)\n\n    >>> load_params = tl.files.load_npz(name=\'model.npz\')\n    >>> tl.files.assign_weights(load_params, network)\n\n    Load model from npz (Method 2)\n\n    >>> tl.files.load_and_assign_npz(name=\'model.npz\', network=network)\n\n    References\n    ----------\n    `Saving dictionary using numpy <http://stackoverflow.com/questions/22315595/saving-dictionary-of-header-information-using-numpy-savez>`__\n\n    """"""\n    logging.info(""[*] Saving TL weights into %s"" % name)\n    if save_list is None:\n        save_list = []\n\n    save_list_var = tf_variables_to_numpy(save_list)\n    np.savez(name, params=save_list_var)\n    save_list_var = None\n    del save_list_var\n    logging.info(""[*] Saved"")\n\n\ndef load_npz(path=\'\', name=\'model.npz\'):\n    """"""Load the parameters of a Model saved by tl.files.save_npz().\n\n    Parameters\n    ----------\n    path : str\n        Folder path to `.npz` file.\n    name : str\n        The name of the `.npz` file.\n\n    Returns\n    --------\n    list of array\n        A list of parameters in order.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``\n\n    References\n    ----------\n    - `Saving dictionary using numpy <http://stackoverflow.com/questions/22315595/saving-dictionary-of-header-information-using-numpy-savez>`__\n\n    """"""\n    d = np.load(os.path.join(path, name), allow_pickle=True)\n    return d[\'params\']\n\n\ndef assign_params(**kwargs):\n    raise Exception(""please change assign_params --> assign_weights"")\n\n\ndef assign_weights(weights, network):\n    """"""Assign the given parameters to the TensorLayer network.\n\n    Parameters\n    ----------\n    weights : list of array\n        A list of model weights (array) in order.\n    network : :class:`Layer`\n        The network to be assigned.\n\n    Returns\n    --------\n    1) list of operations if in graph mode\n            A list of tf ops in order that assign weights. Support sess.run(ops) manually.\n    2) list of tf variables if in eager mode\n            A list of tf variables (assigned weights) in order.\n\n    Examples\n    --------\n\n    References\n    ----------\n    - `Assign value to a TensorFlow variable <http://stackoverflow.com/questions/34220532/how-to-assign-value-to-a-tensorflow-variable>`__\n\n    """"""\n    ops = []\n    for idx, param in enumerate(weights):\n        ops.append(network.all_weights[idx].assign(param))\n    return ops\n\n\ndef load_and_assign_npz(name=None, network=None):\n    """"""Load model from npz and assign to a network.\n\n    Parameters\n    -------------\n    name : str\n        The name of the `.npz` file.\n    network : :class:`Model`\n        The network to be assigned.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``\n\n    """"""\n    if network is None:\n        raise ValueError(""network is None."")\n\n    if not os.path.exists(name):\n        logging.error(""file {} doesn\'t exist."".format(name))\n        return False\n    else:\n        weights = load_npz(name=name)\n        assign_weights(weights, network)\n        logging.info(""[*] Load {} SUCCESS!"".format(name))\n\n\ndef save_npz_dict(save_list=None, name=\'model.npz\'):\n    """"""Input parameters and the file name, save parameters as a dictionary into .npz file.\n\n    Use ``tl.files.load_and_assign_npz_dict()`` to restore.\n\n    Parameters\n    ----------\n    save_list : list of parameters\n        A list of parameters (tensor) to be saved.\n    name : str\n        The name of the `.npz` file.\n\n    """"""\n    if save_list is None:\n        save_list = []\n\n    save_list_names = [tensor.name for tensor in save_list]\n    save_list_var = tf_variables_to_numpy(save_list)\n    save_var_dict = {save_list_names[idx]: val for idx, val in enumerate(save_list_var)}\n    np.savez(name, **save_var_dict)\n    save_list_var = None\n    save_var_dict = None\n    del save_list_var\n    del save_var_dict\n    logging.info(""[*] Model saved in npz_dict %s"" % name)\n\n\ndef load_and_assign_npz_dict(name=\'model.npz\', network=None, skip=False):\n    """"""Restore the parameters saved by ``tl.files.save_npz_dict()``.\n\n    Parameters\n    -------------\n    name : str\n        The name of the `.npz` file.\n    network : :class:`Model`\n        The network to be assigned.\n    skip : boolean\n        If \'skip\' == True, loaded weights whose name is not found in network\'s weights will be skipped.\n        If \'skip\' is False, error will be raised when mismatch is found. Default False.\n\n    """"""\n    if not os.path.exists(name):\n        logging.error(""file {} doesn\'t exist."".format(name))\n        return False\n\n    weights = np.load(name, allow_pickle=True)\n    if len(weights.keys()) != len(set(weights.keys())):\n        raise Exception(""Duplication in model npz_dict %s"" % name)\n\n    net_weights_name = [w.name for w in network.all_weights]\n\n    for key in weights.keys():\n        if key not in net_weights_name:\n            if skip:\n                logging.warning(""Weights named \'%s\' not found in network. Skip it."" % key)\n            else:\n                raise RuntimeError(\n                    ""Weights named \'%s\' not found in network. Hint: set argument skip=Ture ""\n                    ""if you want to skip redundant or mismatch weights."" % key\n                )\n        else:\n            assign_tf_variable(network.all_weights[net_weights_name.index(key)], weights[key])\n    logging.info(""[*] Model restored from npz_dict %s"" % name)\n\n\ndef save_ckpt(mode_name=\'model.ckpt\', save_dir=\'checkpoint\', var_list=None, global_step=None, printable=False):\n    """"""Save parameters into `ckpt` file.\n\n    Parameters\n    ------------\n    mode_name : str\n        The name of the model, default is ``model.ckpt``.\n    save_dir : str\n        The path / file directory to the `ckpt`, default is ``checkpoint``.\n    var_list : list of tensor\n        The parameters / variables (tensor) to be saved. If empty, save all global variables (default).\n    global_step : int or None\n        Step number.\n    printable : boolean\n        Whether to print all parameters information.\n\n    See Also\n    --------\n    load_ckpt\n\n    """"""\n\n    if var_list is None:\n        if sess is None:\n            # FIXME: not sure whether global variables can be accessed in eager mode\n            raise ValueError(\n                ""If var_list is None, sess must be specified. ""\n                ""In eager mode, can not access global variables easily. ""\n            )\n        var_list = []\n\n    ckpt_file = os.path.join(save_dir, mode_name)\n    if var_list == []:\n        var_list = tf.global_variables()\n\n    logging.info(""[*] save %s n_weights: %d"" % (ckpt_file, len(var_list)))\n\n    if printable:\n        for idx, v in enumerate(var_list):\n            logging.info(""  param {:3}: {:15}   {}"".format(idx, v.name, str(v.get_shape())))\n\n    if sess:\n        # graph mode\n        saver = tf.train.Saver(var_list)\n        saver.save(sess, ckpt_file, global_step=global_step)\n    else:\n        # eager mode\n        # saver = tfes.Saver(var_list)\n        # saver.save(ckpt_file, global_step=global_step)\n        # TODO: tf2.0 not stable, cannot import tensorflow.contrib.eager.python.saver\n        pass\n\n\ndef load_ckpt(sess=None, mode_name=\'model.ckpt\', save_dir=\'checkpoint\', var_list=None, is_latest=True, printable=False):\n    """"""Load parameters from `ckpt` file.\n\n    Parameters\n    ------------\n    sess : Session\n        TensorFlow Session.\n    mode_name : str\n        The name of the model, default is ``model.ckpt``.\n    save_dir : str\n        The path / file directory to the `ckpt`, default is ``checkpoint``.\n    var_list : list of tensor\n        The parameters / variables (tensor) to be saved. If empty, save all global variables (default).\n    is_latest : boolean\n        Whether to load the latest `ckpt`, if False, load the `ckpt` with the name of ```mode_name``.\n    printable : boolean\n        Whether to print all parameters information.\n\n    Examples\n    ----------\n    - Save all global parameters.\n\n    >>> tl.files.save_ckpt(sess=sess, mode_name=\'model.ckpt\', save_dir=\'model\', printable=True)\n\n    - Save specific parameters.\n\n    >>> tl.files.save_ckpt(sess=sess, mode_name=\'model.ckpt\', var_list=net.all_params, save_dir=\'model\', printable=True)\n\n    - Load latest ckpt.\n\n    >>> tl.files.load_ckpt(sess=sess, var_list=net.all_params, save_dir=\'model\', printable=True)\n\n    - Load specific ckpt.\n\n    >>> tl.files.load_ckpt(sess=sess, mode_name=\'model.ckpt\', var_list=net.all_params, save_dir=\'model\', is_latest=False, printable=True)\n\n    """"""\n    # if sess is None:\n    #     raise ValueError(""session is None."")\n    if var_list is None:\n        if sess is None:\n            # FIXME: not sure whether global variables can be accessed in eager mode\n            raise ValueError(\n                ""If var_list is None, sess must be specified. ""\n                ""In eager mode, can not access global variables easily. ""\n            )\n        var_list = []\n\n    if is_latest:\n        ckpt_file = tf.train.latest_checkpoint(save_dir)\n    else:\n        ckpt_file = os.path.join(save_dir, mode_name)\n\n    if not var_list:\n        var_list = tf.global_variables()\n\n    logging.info(""[*] load %s n_weights: %d"" % (ckpt_file, len(var_list)))\n\n    if printable:\n        for idx, v in enumerate(var_list):\n            logging.info(""  weights {:3}: {:15}   {}"".format(idx, v.name, str(v.get_shape())))\n\n    try:\n        if sess:\n            # graph mode\n            saver = tf.train.Saver(var_list)\n            saver.restore(sess, ckpt_file)\n        else:\n            # eager mode\n            # saver = tfes.Saver(var_list)\n            # saver.restore(ckpt_file)\n            # TODO: tf2.0 not stable, cannot import tensorflow.contrib.eager.python.saver\n            pass\n\n    except Exception as e:\n        logging.info(e)\n        logging.info(""[*] load ckpt fail ..."")\n\n\ndef save_any_to_npy(save_dict=None, name=\'file.npy\'):\n    """"""Save variables to `.npy` file.\n\n    Parameters\n    ------------\n    save_dict : directory\n        The variables to be saved.\n    name : str\n        File name.\n\n    Examples\n    ---------\n    >>> tl.files.save_any_to_npy(save_dict={\'data\': [\'a\',\'b\']}, name=\'test.npy\')\n    >>> data = tl.files.load_npy_to_any(name=\'test.npy\')\n    >>> print(data)\n    {\'data\': [\'a\',\'b\']}\n\n    """"""\n    if save_dict is None:\n        save_dict = {}\n    np.save(name, save_dict)\n\n\ndef load_npy_to_any(path=\'\', name=\'file.npy\'):\n    """"""Load `.npy` file.\n\n    Parameters\n    ------------\n    path : str\n        Path to the file (optional).\n    name : str\n        File name.\n\n    Examples\n    ---------\n    - see tl.files.save_any_to_npy()\n\n    """"""\n    file_path = os.path.join(path, name)\n    try:\n        return np.load(file_path, allow_pickle=True).item()\n    except Exception:\n        return np.load(file_path, allow_pickle=True)\n    raise Exception(""[!] Fail to load %s"" % file_path)\n\n\ndef file_exists(filepath):\n    """"""Check whether a file exists by given file path.""""""\n    return os.path.isfile(filepath)\n\n\ndef folder_exists(folderpath):\n    """"""Check whether a folder exists by given folder path.""""""\n    return os.path.isdir(folderpath)\n\n\ndef del_file(filepath):\n    """"""Delete a file by given file path.""""""\n    os.remove(filepath)\n\n\ndef del_folder(folderpath):\n    """"""Delete a folder by given folder path.""""""\n    shutil.rmtree(folderpath)\n\n\ndef read_file(filepath):\n    """"""Read a file and return a string.\n\n    Examples\n    ---------\n    >>> data = tl.files.read_file(\'data.txt\')\n\n    """"""\n    with open(filepath, \'r\') as afile:\n        return afile.read()\n\n\ndef load_file_list(path=None, regx=\'\\.jpg\', printable=True, keep_prefix=False):\n    r""""""Return a file list in a folder by given a path and regular expression.\n\n    Parameters\n    ----------\n    path : str or None\n        A folder path, if `None`, use the current directory.\n    regx : str\n        The regx of file name.\n    printable : boolean\n        Whether to print the files infomation.\n    keep_prefix : boolean\n        Whether to keep path in the file name.\n\n    Examples\n    ----------\n    >>> file_list = tl.files.load_file_list(path=None, regx=\'w1pre_[0-9]+\\.(npz)\')\n\n    """"""\n    if path is None:\n        path = os.getcwd()\n    file_list = os.listdir(path)\n    return_list = []\n    for _, f in enumerate(file_list):\n        if re.search(regx, f):\n            return_list.append(f)\n    # return_list.sort()\n    if keep_prefix:\n        for i, f in enumerate(return_list):\n            return_list[i] = os.path.join(path, f)\n\n    if printable:\n        logging.info(\'Match file list = %s\' % return_list)\n        logging.info(\'Number of files = %d\' % len(return_list))\n    return return_list\n\n\ndef load_folder_list(path=""""):\n    """"""Return a folder list in a folder by given a folder path.\n\n    Parameters\n    ----------\n    path : str\n        A folder path.\n\n    """"""\n    return [os.path.join(path, o) for o in os.listdir(path) if os.path.isdir(os.path.join(path, o))]\n\n\ndef exists_or_mkdir(path, verbose=True):\n    """"""Check a folder by given name, if not exist, create the folder and return False,\n    if directory exists, return True.\n\n    Parameters\n    ----------\n    path : str\n        A folder path.\n    verbose : boolean\n        If True (default), prints results.\n\n    Returns\n    --------\n    boolean\n        True if folder already exist, otherwise, returns False and create the folder.\n\n    Examples\n    --------\n    >>> tl.files.exists_or_mkdir(""checkpoints/train"")\n\n    """"""\n    if not os.path.exists(path):\n        if verbose:\n            logging.info(""[*] creates %s ..."" % path)\n        os.makedirs(path)\n        return False\n    else:\n        if verbose:\n            logging.info(""[!] %s exists ..."" % path)\n        return True\n\n\ndef maybe_download_and_extract(filename, working_directory, url_source, extract=False, expected_bytes=None):\n    """"""Checks if file exists in working_directory otherwise tries to dowload the file,\n    and optionally also tries to extract the file if format is "".zip"" or "".tar""\n\n    Parameters\n    -----------\n    filename : str\n        The name of the (to be) dowloaded file.\n    working_directory : str\n        A folder path to search for the file in and dowload the file to\n    url : str\n        The URL to download the file from\n    extract : boolean\n        If True, tries to uncompress the dowloaded file is "".tar.gz/.tar.bz2"" or "".zip"" file, default is False.\n    expected_bytes : int or None\n        If set tries to verify that the downloaded file is of the specified size, otherwise raises an Exception, defaults is None which corresponds to no check being performed.\n\n    Returns\n    ----------\n    str\n        File path of the dowloaded (uncompressed) file.\n\n    Examples\n    --------\n    >>> down_file = tl.files.maybe_download_and_extract(filename=\'train-images-idx3-ubyte.gz\',\n    ...                                            working_directory=\'data/\',\n    ...                                            url_source=\'http://yann.lecun.com/exdb/mnist/\')\n    >>> tl.files.maybe_download_and_extract(filename=\'ADEChallengeData2016.zip\',\n    ...                                             working_directory=\'data/\',\n    ...                                             url_source=\'http://sceneparsing.csail.mit.edu/data/\',\n    ...                                             extract=True)\n\n    """"""\n\n    # We first define a download function, supporting both Python 2 and 3.\n    def _download(filename, working_directory, url_source):\n\n        progress_bar = progressbar.ProgressBar()\n\n        def _dlProgress(count, blockSize, totalSize, pbar=progress_bar):\n            if (totalSize != 0):\n\n                if not pbar.max_value:\n                    totalBlocks = math.ceil(float(totalSize) / float(blockSize))\n                    pbar.max_value = int(totalBlocks)\n\n                pbar.update(count, force=True)\n\n        filepath = os.path.join(working_directory, filename)\n\n        logging.info(\'Downloading %s...\\n\' % filename)\n\n        urlretrieve(url_source + filename, filepath, reporthook=_dlProgress)\n\n    exists_or_mkdir(working_directory, verbose=False)\n    filepath = os.path.join(working_directory, filename)\n\n    if not os.path.exists(filepath):\n\n        _download(filename, working_directory, url_source)\n        statinfo = os.stat(filepath)\n        logging.info(\'Succesfully downloaded %s %s bytes.\' % (filename, statinfo.st_size))  # , \'bytes.\')\n        if (not (expected_bytes is None) and (expected_bytes != statinfo.st_size)):\n            raise Exception(\'Failed to verify \' + filename + \'. Can you get to it with a browser?\')\n        if (extract):\n            if tarfile.is_tarfile(filepath):\n                logging.info(\'Trying to extract tar file\')\n                tarfile.open(filepath, \'r\').extractall(working_directory)\n                logging.info(\'... Success!\')\n            elif zipfile.is_zipfile(filepath):\n                logging.info(\'Trying to extract zip file\')\n                with zipfile.ZipFile(filepath) as zf:\n                    zf.extractall(working_directory)\n                logging.info(\'... Success!\')\n            else:\n                logging.info(""Unknown compression_format only .tar.gz/.tar.bz2/.tar and .zip supported"")\n    return filepath\n\n\ndef natural_keys(text):\n    """"""Sort list of string with number in human order.\n\n    Examples\n    ----------\n    >>> l = [\'im1.jpg\', \'im31.jpg\', \'im11.jpg\', \'im21.jpg\', \'im03.jpg\', \'im05.jpg\']\n    >>> l.sort(key=tl.files.natural_keys)\n    [\'im1.jpg\', \'im03.jpg\', \'im05\', \'im11.jpg\', \'im21.jpg\', \'im31.jpg\']\n    >>> l.sort() # that is what we dont want\n    [\'im03.jpg\', \'im05\', \'im1.jpg\', \'im11.jpg\', \'im21.jpg\', \'im31.jpg\']\n\n    References\n    ----------\n    - `link <http://nedbatchelder.com/blog/200712/human_sorting.html>`__\n\n    """"""\n\n    # - alist.sort(key=natural_keys) sorts in human order\n    # http://nedbatchelder.com/blog/200712/human_sorting.html\n    # (See Toothy\'s implementation in the comments)\n    def atoi(text):\n        return int(text) if text.isdigit() else text\n\n    return [atoi(c) for c in re.split(\'(\\d+)\', text)]\n\n\n# Visualizing npz files\ndef npz_to_W_pdf(path=None, regx=\'w1pre_[0-9]+\\.(npz)\'):\n    r""""""Convert the first weight matrix of `.npz` file to `.pdf` by using `tl.visualize.W()`.\n\n    Parameters\n    ----------\n    path : str\n        A folder path to `npz` files.\n    regx : str\n        Regx for the file name.\n\n    Examples\n    ---------\n    Convert the first weight matrix of w1_pre...npz file to w1_pre...pdf.\n\n    >>> tl.files.npz_to_W_pdf(path=\'/Users/.../npz_file/\', regx=\'w1pre_[0-9]+\\.(npz)\')\n\n    """"""\n    file_list = load_file_list(path=path, regx=regx)\n    for f in file_list:\n        W = load_npz(path, f)[0]\n        logging.info(""%s --> %s"" % (f, f.split(\'.\')[0] + \'.pdf\'))\n        visualize.draw_weights(W, second=10, saveable=True, name=f.split(\'.\')[0], fig_idx=2012)\n\n\ndef tf_variables_to_numpy(variables):\n    """"""Convert TF tensor or a list of tensors into a list of numpy array""""""\n    if not isinstance(variables, list):\n        var_list = [variables]\n    else:\n        var_list = variables\n\n    results = [v.numpy() for v in var_list]\n    return results\n\n\ndef assign_tf_variable(variable, value):\n    """"""Assign value to a TF variable""""""\n    variable.assign(value)\n\n\ndef _save_weights_to_hdf5_group(f, layers):\n    """"""\n    Save layer/model weights into hdf5 group recursively.\n\n    Parameters\n    ----------\n    f: hdf5 group\n        A hdf5 group created by h5py.File() or create_group().\n    layers: list\n        A list of layers to save weights.\n\n    """"""\n    f.attrs[\'layer_names\'] = [layer.name.encode(\'utf8\') for layer in layers]\n\n    for layer in layers:\n        g = f.create_group(layer.name)\n        if isinstance(layer, tl.models.Model):\n            _save_weights_to_hdf5_group(g, layer.all_layers)\n        elif isinstance(layer, tl.layers.ModelLayer):\n            _save_weights_to_hdf5_group(g, layer.model.all_layers)\n        elif isinstance(layer, tl.layers.LayerList):\n            _save_weights_to_hdf5_group(g, layer.layers)\n        elif isinstance(layer, tl.layers.Layer):\n            if layer.all_weights is not None:\n                weight_values = tf_variables_to_numpy(layer.all_weights)\n                weight_names = [w.name.encode(\'utf8\') for w in layer.all_weights]\n            else:\n                weight_values = []\n                weight_names = []\n            g.attrs[\'weight_names\'] = weight_names\n            for name, val in zip(weight_names, weight_values):\n                val_dataset = g.create_dataset(name, val.shape, dtype=val.dtype)\n                if not val.shape:\n                    # scalar\n                    val_dataset[()] = val\n                else:\n                    val_dataset[:] = val\n        else:\n            raise Exception(""Only layer or model can be saved into hdf5."")\n\n\ndef _load_weights_from_hdf5_group_in_order(f, layers):\n    """"""\n    Load layer weights from a hdf5 group sequentially.\n\n    Parameters\n    ----------\n    f: hdf5 group\n        A hdf5 group created by h5py.File() or create_group().\n    layers: list\n        A list of layers to load weights.\n\n    """"""\n    layer_names = [n.decode(\'utf8\') for n in f.attrs[""layer_names""]]\n\n    for idx, name in enumerate(layer_names):\n        g = f[name]\n        layer = layers[idx]\n        if isinstance(layer, tl.models.Model):\n            _load_weights_from_hdf5_group_in_order(g, layer.all_layers)\n        elif isinstance(layer, tl.layers.ModelLayer):\n            _load_weights_from_hdf5_group_in_order(g, layer.model.all_layers)\n        elif isinstance(layer, tl.layers.LayerList):\n            _load_weights_from_hdf5_group_in_order(g, layer.layers)\n        elif isinstance(layer, tl.layers.Layer):\n            weight_names = [n.decode(\'utf8\') for n in g.attrs[\'weight_names\']]\n            for iid, w_name in enumerate(weight_names):\n                assign_tf_variable(layer.all_weights[iid], np.asarray(g[w_name]))\n        else:\n            raise Exception(""Only layer or model can be saved into hdf5."")\n        if idx == len(layers) - 1:\n            break\n\n\ndef _load_weights_from_hdf5_group(f, layers, skip=False):\n    """"""\n    Load layer weights from a hdf5 group by layer name.\n\n    Parameters\n    ----------\n    f: hdf5 group\n        A hdf5 group created by h5py.File() or create_group().\n    layers: list\n        A list of layers to load weights.\n    skip : boolean\n        If \'skip\' == True, loaded layer whose name is not found in \'layers\' will be skipped. If \'skip\' is False,\n        error will be raised when mismatch is found. Default False.\n\n    """"""\n    layer_names = [n.decode(\'utf8\') for n in f.attrs[""layer_names""]]\n    layer_index = {layer.name: layer for layer in layers}\n\n    for idx, name in enumerate(layer_names):\n        if name not in layer_index.keys():\n            if skip:\n                logging.warning(""Layer named \'%s\' not found in network. Skip it."" % name)\n            else:\n                raise RuntimeError(\n                    ""Layer named \'%s\' not found in network. Hint: set argument skip=Ture ""\n                    ""if you want to skip redundant or mismatch Layers."" % name\n                )\n        else:\n            g = f[name]\n            layer = layer_index[name]\n            if isinstance(layer, tl.models.Model):\n                _load_weights_from_hdf5_group(g, layer.all_layers, skip)\n            elif isinstance(layer, tl.layers.ModelLayer):\n                _load_weights_from_hdf5_group(g, layer.model.all_layers, skip)\n            elif isinstance(layer, tl.layers.LayerList):\n                _load_weights_from_hdf5_group(g, layer.layers, skip)\n            elif isinstance(layer, tl.layers.Layer):\n                weight_names = [n.decode(\'utf8\') for n in g.attrs[\'weight_names\']]\n                for iid, w_name in enumerate(weight_names):\n                    # FIXME : this is only for compatibility\n                    if isinstance(layer, tl.layers.BatchNorm) and np.asarray(g[w_name]).ndim > 1:\n                        assign_tf_variable(layer.all_weights[iid], np.asarray(g[w_name]).squeeze())\n                        continue\n                    assign_tf_variable(layer.all_weights[iid], np.asarray(g[w_name]))\n            else:\n                raise Exception(""Only layer or model can be saved into hdf5."")\n\n\ndef save_weights_to_hdf5(filepath, network):\n    """"""Input filepath and save weights in hdf5 format.\n\n    Parameters\n    ----------\n    filepath : str\n        Filename to which the weights will be saved.\n    network : Model\n        TL model.\n\n    Returns\n    -------\n\n    """"""\n    logging.info(""[*] Saving TL weights into %s"" % filepath)\n\n    with h5py.File(filepath, \'w\') as f:\n        _save_weights_to_hdf5_group(f, network.all_layers)\n\n    logging.info(""[*] Saved"")\n\n\ndef load_hdf5_to_weights_in_order(filepath, network):\n    """"""Load weights sequentially from a given file of hdf5 format\n\n    Parameters\n    ----------\n    filepath : str\n        Filename to which the weights will be loaded, should be of hdf5 format.\n    network : Model\n        TL model.\n\n    Notes:\n        If the file contains more weights than given \'weights\', then the redundant ones will be ignored\n        if all previous weights match perfectly.\n\n    Returns\n    -------\n\n    """"""\n    f = h5py.File(filepath, \'r\')\n    try:\n        layer_names = [n.decode(\'utf8\') for n in f.attrs[""layer_names""]]\n    except Exception:\n        raise NameError(\n            ""The loaded hdf5 file needs to have \'layer_names\' as attributes. ""\n            ""Please check whether this hdf5 file is saved from TL.""\n        )\n\n    if len(network.all_layers) != len(layer_names):\n        logging.warning(\n            ""Number of weights mismatch.""\n            ""Trying to load a saved file with "" + str(len(layer_names)) + "" layers into a model with "" +\n            str(len(network.all_layers)) + "" layers.""\n        )\n\n    _load_weights_from_hdf5_group_in_order(f, network.all_layers)\n\n    f.close()\n    logging.info(""[*] Load %s SUCCESS!"" % filepath)\n\n\ndef load_hdf5_to_weights(filepath, network, skip=False):\n    """"""Load weights by name from a given file of hdf5 format\n\n    Parameters\n    ----------\n    filepath : str\n        Filename to which the weights will be loaded, should be of hdf5 format.\n    network : Model\n        TL model.\n    skip : bool\n        If \'skip\' == True, loaded weights whose name is not found in \'weights\' will be skipped. If \'skip\' is False,\n        error will be raised when mismatch is found. Default False.\n\n    Returns\n    -------\n\n    """"""\n    f = h5py.File(filepath, \'r\')\n    try:\n        layer_names = [n.decode(\'utf8\') for n in f.attrs[""layer_names""]]\n    except Exception:\n        raise NameError(\n            ""The loaded hdf5 file needs to have \'layer_names\' as attributes. ""\n            ""Please check whether this hdf5 file is saved from TL.""\n        )\n\n    net_index = {layer.name: layer for layer in network.all_layers}\n\n    if len(network.all_layers) != len(layer_names):\n        logging.warning(\n            ""Number of weights mismatch.""\n            ""Trying to load a saved file with "" + str(len(layer_names)) + "" layers into a model with "" +\n            str(len(network.all_layers)) + "" layers.""\n        )\n\n    # check mismatch form network weights to hdf5\n    for name in net_index.keys():\n        if name not in layer_names:\n            logging.warning(""Network layer named \'%s\' not found in loaded hdf5 file. It will be skipped."" % name)\n\n    # load weights from hdf5 to network\n    _load_weights_from_hdf5_group(f, network.all_layers, skip)\n\n    f.close()\n    logging.info(""[*] Load %s SUCCESS!"" % filepath)\n\n\ndef check_ckpt_file(model_dir):\n    model_dir = model_dir\n    model_path = None\n    count_extension = 0\n    for root, dirs, files in os.walk(model_dir):\n        for file in files:\n            filename, extension = os.path.splitext(file)\n            if extension in [\'.data-00000-of-00001\', \'.index\', \'.meta\']:\n                count_extension += 1\n        if count_extension == 3:\n            model_path = model_dir + \'/\' + filename\n        else:\n            raise Exception(""Check the file extension for missing .data-00000-of-00001, .index, .meta"")\n        if model_path is None:\n            raise Exception(\'The ckpt file is not found\')\n    return model_path, filename\n\n\ndef rename_weight_or_biases(variable_name):\n    if variable_name is None:\n        return variable_name\n    split_var = variable_name.split(\'/\')\n\n    str_temp = \'\'\n    for i in range(len(split_var)):\n        if \'w\' in split_var[i]:\n            split_var[i] = \'filters:0\'\n        elif \'b\' in split_var[i]:\n            split_var[i] = \'biases:0\'\n        else:\n            pass\n\n        if i < len(split_var) - 1:\n            str_temp = str_temp + split_var[i] + \'/\'\n        else:\n            str_temp = str_temp + split_var[i]\n\n    return str_temp\n\n\ndef load_and_assign_ckpt(model_dir, network=None, skip=True):\n    """"""Load weights by name from a given file of ckpt format\n\n    Parameters\n    ----------\n    model_dir : str\n        Filename to which the weights will be loaded, should be of ckpt format.\n        Examples: model_dir = /root/cnn_model/\n    network : Model\n        TL model.\n    skip : bool\n        If \'skip\' == True, loaded weights whose name is not found in \'weights\' will be skipped. If \'skip\' is False,\n        error will be raised when mismatch is found. Default False.\n\n    Returns\n    -------\n\n    """"""\n    model_path, filename = check_ckpt_file(model_dir)\n\n    reader = pywrap_tensorflow.NewCheckpointReader(model_path)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n\n    net_weights_name = [w.name for w in network.all_weights]\n\n    for key in var_to_shape_map:\n        if key not in net_weights_name:\n            if skip:\n                logging.warning(""Weights named \'%s\' not found in network. Skip it."" % key)\n            else:\n                raise RuntimeError(\n                    ""Weights named \'%s\' not found in network. Hint: set argument skip=Ture ""\n                    ""if you want to skip redundant or mismatch weights."" % key\n                )\n        else:\n            assign_tf_variable(network.all_weights[net_weights_name.index(key)], reader.get_tensor(key))\n    logging.info(""[*] Model restored from ckpt %s"" % filename)\n\n\ndef ckpt_to_npz_dict(model_dir, save_name=\'model.npz\', rename_key=False):\n    """""" Save ckpt weights to npz file\n\n    Parameters\n    ----------\n    model_dir : str\n        Filename to which the weights will be loaded, should be of ckpt format.\n        Examples: model_dir = /root/cnn_model/\n    save_name : str\n        The save_name of the `.npz` file.\n    rename_key : bool\n        Modify parameter naming,  used to match TL naming rule.\n        Examples: conv1_1/b_b --> conv1_1/biases:0 ; conv1_1/w_w --> conv1_1/filters:0\n\n    Returns\n    -------\n\n    """"""\n    model_path, _ = check_ckpt_file(model_dir)\n\n    reader = pywrap_tensorflow.NewCheckpointReader(model_path)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n\n    parameters_dict = {}\n    if rename_key is False:\n        for key in sorted(var_to_shape_map):\n            parameters_dict[key] = reader.get_tensor(key)\n    elif rename_key is True:\n        for key in sorted(var_to_shape_map):\n            parameters_dict[rename_weight_or_biases(key)] = reader.get_tensor(key)\n\n    np.savez(save_name, **parameters_dict)\n    parameters_dict = None\n    del parameters_dict\n    logging.info(""[*] Ckpt weights saved in npz_dict %s"" % save_name)\n'"
tensorlayer/layers/__init__.py,0,b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom .activation import *\nfrom .convolution import *\nfrom .core import *\nfrom .dense import *\nfrom .deprecated import *\nfrom .dropout import *\nfrom .embedding import *\nfrom .extend import *\nfrom .image_resampling import *\nfrom .inputs import *\nfrom .lambda_layers import *\nfrom .merge import *\nfrom .noise import *\nfrom .normalization import *\nfrom .padding import *\nfrom .pooling import *\nfrom .quantize import *\nfrom .recurrent import *\nfrom .scale import *\nfrom .shape import *\nfrom .spatial_transformer import *\nfrom .stack import *\nfrom .utils import *\n'
tensorlayer/layers/activation.py,18,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.activation import leaky_relu6, leaky_twice_relu6\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.initializers import truncated_normal\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\n    \'PRelu\',\n    \'PRelu6\',\n    \'PTRelu6\',\n]\n\n\nclass PRelu(Layer):\n    """"""\n    The :class:`PRelu` class is Parametric Rectified Linear layer.\n    It follows f(x) = alpha * x for x < 0, f(x) = x for x >= 0,\n    where alpha is a learned array with the same shape as x.\n\n    Parameters\n    ----------\n    channel_shared : boolean\n        If True, single weight is shared by all channels.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    a_init : initializer\n        The initializer for initializing the alpha(s).\n    name : None or str\n        A unique layer name.\n\n    Examples\n    -----------\n    >>> inputs = tl.layers.Input([10, 5])\n    >>> prelulayer = tl.layers.PRelu(channel_shared=True)\n    >>> print(prelulayer)\n    PRelu(channel_shared=True,in_channels=None,name=prelu)\n    >>> prelu = prelulayer(inputs)\n    >>> model = tl.models.Model(inputs=inputs, outputs=prelu)\n    >>> out = model(data, is_train=True)\n\n    References\n    -----------\n    - `Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification <http://arxiv.org/abs/1502.01852>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    """"""\n\n    def __init__(\n        self,\n        channel_shared=False,\n        in_channels=None,\n        a_init=truncated_normal(mean=0.0, stddev=0.05),\n        name=None  # ""prelu""\n    ):\n\n        super(PRelu, self).__init__(name)\n        self.channel_shared = channel_shared\n        self.in_channels = in_channels\n        self.a_init = a_init\n\n        if self.channel_shared:\n            self.build((None, ))\n            self._built = True\n        elif self.in_channels is not None:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(""PRelu %s: channel_shared: %s"" % (self.name, self.channel_shared))\n\n    def __repr__(self):\n        s = (\'{classname}(\')\n        s += \'channel_shared={channel_shared},\'\n        s += \'in_channels={in_channels},\'\n        s += \'name={name}\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.channel_shared:\n            w_shape = (1, )\n        else:\n            w_shape = (inputs_shape[-1], )\n        self.alpha_var = self._get_weights(""alpha"", shape=w_shape, init=self.a_init)\n        self.alpha_var_constrained = tf.nn.sigmoid(self.alpha_var, name=""constraining_alpha_var_in_0_1"")\n\n    # @tf.function\n    def forward(self, inputs):\n\n        pos = tf.nn.relu(inputs)\n        self.alpha_var_constrained = tf.nn.sigmoid(self.alpha_var, name=""constraining_alpha_var_in_0_1"")\n        neg = -self.alpha_var_constrained * tf.nn.relu(-inputs)\n\n        return pos + neg\n\n\nclass PRelu6(Layer):\n    """"""\n    The :class:`PRelu6` class is Parametric Rectified Linear layer integrating ReLU6 behaviour.\n\n    This Layer is a modified version of the :class:`PRelu`.\n\n    This activation layer use a modified version :func:`tl.act.leaky_relu` introduced by the following paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    This activation function also use a modified version of the activation function :func:`tf.nn.relu6` introduced by the following paper:\n    `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    This activation layer push further the logic by adding `leaky` behaviour both below zero and above six.\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x in [0, 6]: ``f(x) = x``.\n      - When x > 6: ``f(x) = 6``.\n\n    Parameters\n    ----------\n    channel_shared : boolean\n        If True, single weight is shared by all channels.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    a_init : initializer\n        The initializer for initializing the alpha(s).\n    name : None or str\n        A unique layer name.\n\n    References\n    -----------\n    - `Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification <http://arxiv.org/abs/1502.01852>`__\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    """"""\n\n    def __init__(\n        self,\n        channel_shared=False,\n        in_channels=None,\n        a_init=truncated_normal(mean=0.0, stddev=0.05),\n        name=None  # ""prelu6""\n    ):\n\n        super(PRelu6, self).__init__(name)\n        self.channel_shared = channel_shared\n        self.in_channels = in_channels\n        self.a_init = a_init\n\n        if self.channel_shared:\n            self.build((None, ))\n            self._built = True\n        elif self.in_channels is not None:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(""PRelu6 %s: channel_shared: %s"" % (self.name, self.channel_shared))\n\n    def __repr__(self):\n        s = (\'{classname}(\')\n        s += \'channel_shared={channel_shared},\'\n        s += \'in_channels={in_channels},\'\n        s += \'name={name}\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.channel_shared:\n            w_shape = (1, )\n        else:\n            w_shape = (inputs_shape[-1], )\n        self.alpha_var = self._get_weights(""alpha"", shape=w_shape, init=self.a_init)\n        self.alpha_var_constrained = tf.nn.sigmoid(self.alpha_var, name=""constraining_alpha_var_in_0_1"")\n\n    # @tf.function\n    def forward(self, inputs):\n        pos = tf.nn.relu(inputs)\n        pos_6 = -tf.nn.relu(inputs - 6)\n        neg = -self.alpha_var_constrained * tf.nn.relu(-inputs)\n\n        return pos + pos_6 + neg\n\n\nclass PTRelu6(Layer):\n    """"""\n    The :class:`PTRelu6` class is Parametric Rectified Linear layer integrating ReLU6 behaviour.\n\n    This Layer is a modified version of the :class:`PRelu`.\n\n    This activation layer use a modified version :func:`tl.act.leaky_relu` introduced by the following paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    This activation function also use a modified version of the activation function :func:`tf.nn.relu6` introduced by the following paper:\n    `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    This activation layer push further the logic by adding `leaky` behaviour both below zero and above six.\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x in [0, 6]: ``f(x) = x``.\n      - When x > 6: ``f(x) = 6 + (alpha_high * (x-6))``.\n\n    This version goes one step beyond :class:`PRelu6` by introducing leaky behaviour on the positive side when x > 6.\n\n    Parameters\n    ----------\n    channel_shared : boolean\n        If True, single weight is shared by all channels.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    a_init : initializer\n        The initializer for initializing the alpha(s).\n    name : None or str\n        A unique layer name.\n\n    References\n    -----------\n    - `Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification <http://arxiv.org/abs/1502.01852>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    """"""\n\n    def __init__(\n        self,\n        channel_shared=False,\n        in_channels=None,\n        a_init=truncated_normal(mean=0.0, stddev=0.05),\n        name=None  # ""ptrelu6""\n    ):\n\n        super(PTRelu6, self).__init__(name)\n        self.channel_shared = channel_shared\n        self.in_channels = in_channels\n        self.a_init = a_init\n\n        if self.channel_shared:\n            self.build((None, ))\n            self._built = True\n        elif self.in_channels:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(""PTRelu6 %s: channel_shared: %s"" % (self.name, self.channel_shared))\n\n    def __repr__(self):\n        s = (\'{classname}(\')\n        s += \'channel_shared={channel_shared},\'\n        s += \'in_channels={in_channels},\'\n        s += \'name={name}\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.channel_shared:\n            w_shape = (1, )\n        else:\n            w_shape = (inputs_shape[-1], )\n\n        # Alpha for outputs lower than zeros\n        self.alpha_low = self._get_weights(""alpha_low"", shape=w_shape, init=self.a_init)\n        self.alpha_low_constrained = tf.nn.sigmoid(self.alpha_low, name=""constraining_alpha_low_in_0_1"")\n\n        # Alpha for outputs higher than 6\n        self.alpha_high = self._get_weights(""alpha_high"", shape=w_shape, init=self.a_init)\n        self.alpha_high_constrained = tf.nn.sigmoid(self.alpha_high, name=""constraining_alpha_high_in_0_1"")\n\n    # @tf.function\n    def forward(self, inputs):\n        pos = tf.nn.relu(inputs)\n        pos_6 = -tf.nn.relu(inputs - 6) + self.alpha_high_constrained * tf.nn.relu(inputs - 6)\n        neg = -self.alpha_low_constrained * tf.nn.relu(-inputs)\n\n        return pos + pos_6 + neg\n'"
tensorlayer/layers/core.py,11,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport inspect\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import (deprecated_alias, private_method, protected_method)\nfrom tensorlayer.files import utils\nfrom tensorlayer.layers.utils import (get_variable_with_initializer, list_remove_repeat)\n\n__all__ = [\'Layer\', \'ModelLayer\', \'LayerList\']\n\n_global_layer_name_dict = {}  # TODO: better implementation?\n\n_act_dict = {\n    ""relu"": tf.nn.relu,\n    ""relu6"": tf.nn.relu6,\n    ""leaky_relu"": tf.nn.leaky_relu,\n    ""lrelu"": tf.nn.leaky_relu,\n    ""softplus"": tf.nn.softplus,\n    ""tanh"": tf.nn.tanh,\n    ""sigmoid"": tf.nn.sigmoid,\n}\n\n\ndef str2act(act):\n    if len(act) > 5 and act[0:5] == ""lrelu"":\n        try:\n            alpha = float(act[5:])\n            return lambda x: tf.nn.leaky_relu(x, alpha=alpha)\n        except Exception as e:\n            raise Exception(""{} can not be parsed as a float"".format(act[5:]))\n\n    if len(act) > 10 and act[0:10] == ""leaky_relu"":\n        try:\n            alpha = float(act[10:])\n            return lambda x: tf.nn.leaky_relu(x, alpha=alpha)\n        except Exception as e:\n            raise Exception(""{} can not be parsed as a float"".format(act[10:]))\n\n    if act not in _act_dict.keys():\n        raise Exception(""Unsupported act: {}"".format(act))\n    return _act_dict[act]\n\n\nclass Layer(object):\n    """"""The basic :class:`Layer` class represents a single layer of a neural network.\n\n    It should be subclassed when implementing new types of layers.\n\n    Parameters\n    ----------\n    name : str or None\n        A unique layer name. If None, a unique name will be automatically assigned.\n\n    Methods\n    ---------\n    __init__()\n        Initializing the Layer.\n    __call__()\n        (1) Building the Layer if necessary. (2) Forwarding the computation.\n    all_weights()\n        Return a list of Tensor which are all weights of this Layer.\n    trainable_weights()\n        Return a list of Tensor which are all trainable weights of this Layer.\n    nontrainable_weights()\n        Return a list of Tensor which are all nontrainable weights of this Layer.\n    build()\n        Abstract method. Build the Layer. All trainable weights should be defined in this function.\n    forward()\n        Abstract method. Forward computation and return computation results.\n\n    """"""\n\n    def __init__(self, name=None, act=None, *args, **kwargs):\n        """"""\n        Initializing the Layer.\n\n        :param name: str or None\n        :param name: str or function or None\n        """"""\n\n        # Layer constants\n        # for key in kwargs.keys():\n        #     setattr(self, key, self._argument_dict_checkup(kwargs[key]))\n\n        # Auto naming if the name is not given\n        global _global_layer_name_dict\n        if name is None:\n            prefix = self.__class__.__name__.lower()\n\n            if _global_layer_name_dict.get(prefix) is not None:\n                _global_layer_name_dict[prefix] += 1\n                name = prefix + \'_\' + str(_global_layer_name_dict[prefix])\n            else:\n                _global_layer_name_dict[prefix] = 0\n                name = prefix\n            while True:\n                if _global_layer_name_dict.get(name) is None:\n                    break\n                _global_layer_name_dict[prefix] += 1\n                name = prefix + \'_\' + str(_global_layer_name_dict[prefix])\n        else:\n            if _global_layer_name_dict.get(name) is not None:\n                pass\n                # raise ValueError(\n                #     \'Layer name \\\'%s\\\' has already been used by another layer. Please change the layer name.\' % name\n                # )\n            else:\n                _global_layer_name_dict[name] = 0\n\n        self.name = name\n        if isinstance(act, str):\n            self.act = str2act(act)\n        else:\n            self.act = act\n\n        # Layer building state\n        self._built = False\n\n        # Layer nodes state\n        self._nodes = []\n        self._nodes_fixed = False\n\n        # Layer weight state\n        self._all_weights = None\n        self._trainable_weights = []\n        self._nontrainable_weights = []\n\n        # nested layers\n        self._layers = None\n\n        # Layer training state\n        self.is_train = True\n\n        # layer config and init_args\n        self._config = None\n        self.layer_args = self._get_init_args(skip=3)\n\n    @staticmethod\n    def _compute_shape(tensors):\n        if isinstance(tensors, list):\n            shape_mem = [t.get_shape().as_list() for t in tensors]\n        else:\n            shape_mem = tensors.get_shape().as_list()\n        return shape_mem\n\n    @property\n    def config(self):\n        # if not self._nodes_fixed:\n        #     raise RuntimeError(""Model can not be saved when nodes are not fixed."")\n        if self._config is not None:\n            return self._config\n        else:\n            _config = {}\n            _config.update({\'class\': self.__class__.__name__.split(\'.\')[-1]})\n            self.layer_args.update(self.get_args())\n            self.layer_args[""name""] = self.name\n            _config.update({""args"": self.layer_args})\n            if self.__class__.__name__ in tl.layers.inputs.__all__:\n                _config.update({\'prev_layer\': None})\n            else:\n                _config.update({\'prev_layer\': []})\n                for node in self._nodes:\n                    in_nodes = node.in_nodes\n                    if not isinstance(in_nodes, list):\n                        prev_name = in_nodes.name\n                    else:\n                        prev_name = [in_node.name for in_node in in_nodes]\n                        if len(prev_name) == 1:\n                            prev_name = prev_name[0]\n                    _config[\'prev_layer\'].append(prev_name)\n            if self._nodes_fixed:\n                self._config = _config\n            return _config\n\n    @property\n    def all_weights(self):\n        if self._all_weights is not None and len(self._all_weights) > 0:\n            pass\n        else:\n            self._all_weights = self.trainable_weights + self.nontrainable_weights\n        return self._all_weights\n\n    @property\n    def trainable_weights(self):\n        nested = self._collect_sublayers_attr(\'trainable_weights\')\n        return self._trainable_weights + nested\n\n    @property\n    def nontrainable_weights(self):\n        nested = self._collect_sublayers_attr(\'nontrainable_weights\')\n        return self._nontrainable_weights + nested\n\n    @property\n    def weights(self):\n        raise Exception(\n            ""no property .weights exists, do you mean .all_weights, .trainable_weights, or .nontrainable_weights ?""\n        )\n\n    def _collect_sublayers_attr(self, attr):\n        if attr not in [\'trainable_weights\', \'nontrainable_weights\']:\n            raise ValueError(\n                ""Only support to collect some certain attributes of nested layers,""\n                ""e.g. \'trainable_weights\', \'nontrainable_weights\', but got {}"".format(attr)\n            )\n        if self._layers is None:\n            return []\n        nested = []\n        for layer in self._layers:\n            value = getattr(layer, attr)\n            if value is not None:\n                nested.extend(value)\n        return nested\n\n    def __call__(self, inputs, *args, **kwargs):\n        """"""\n        (1) Build the Layer if necessary.\n        (2) Forward the computation and return results.\n        (3) Add LayerNode if necessary\n\n        :param prev_layer: np.ndarray, Tensor, Layer, list of Layers\n        :param kwargs:\n        :return: Layer\n        """"""\n        if self.__class__.__name__ in tl.layers.inputs.__all__:\n            input_tensors = tf.convert_to_tensor(inputs)\n        else:\n            input_tensors = inputs\n\n        if not self._built:\n            if isinstance(self, LayerList):\n                self._input_tensors = input_tensors\n            inputs_shape = self._compute_shape(input_tensors)\n            self.build(inputs_shape)\n            self._built = True\n\n        outputs = self.forward(input_tensors, *args, **kwargs)\n\n        if not self._nodes_fixed:\n            self._add_node(input_tensors, outputs)\n\n        return outputs\n\n    def _add_node(self, input_tensors, output_tensors):\n        """"""Add a LayerNode for this layer given input_tensors, output_tensors.\n\n        WARINING: This function should not be called from outside, it should only be called\n        in layer.__call__ when building static model.\n\n        Parameters\n        ----------\n        input_tensors : Tensor or a list of tensors\n            Input tensors to this layer.\n        output_tensors : Tensor or a list of tensors\n            Output tensors to this layer.\n\n        """"""\n        inputs_list = tolist(input_tensors)\n        outputs_list = tolist(output_tensors)\n\n        if self.__class__.__name__ in tl.layers.inputs.__all__:\n            # for InputLayer, there should be no in_nodes\n            in_nodes = []\n            in_tensor_idxes = [0]\n        else:\n            in_nodes = [tensor._info[0] for tensor in inputs_list]\n            in_tensor_idxes = [tensor._info[1] for tensor in inputs_list]\n        node_index = len(self._nodes)\n\n        new_node = LayerNode(self, node_index, in_nodes, inputs_list, outputs_list, in_tensor_idxes)\n        self._nodes.append(new_node)\n        for idx, tensor in enumerate(outputs_list):\n            tensor._info = (new_node, idx)  # FIXME : modify tensor outside layers? how to deal?\n\n    def _release_memory(self):\n        """"""\n        WARINING: This function should be called with great caution.\n\n        self.inputs and self.outputs will be set as None but not deleted in order to release memory.\n        """"""\n        # FIXME : not understand why saving inputs/outputs shape\n        for node in self._nodes:\n            node.in_tensors = None\n            node.out_tensors = None\n\n    def _set_mode_for_layers(self, is_train):\n        """""" Set training/evaluation mode for the Layer""""""\n        self.is_train = is_train\n\n    def _fix_nodes_for_layers(self):\n        """""" fix LayerNodes to stop growing for this layer""""""\n        self._nodes_fixed = True\n\n    def _get_weights(self, var_name, shape, init=tl.initializers.random_normal(), trainable=True):\n        """""" Get trainable variables. """"""\n        weight = get_variable_with_initializer(scope_name=self.name, var_name=var_name, shape=shape, init=init)\n        if trainable is True:\n            if self._trainable_weights is None:\n                self._trainable_weights = list()\n            self._trainable_weights.append(weight)\n        else:\n            if self._nontrainable_weights is None:\n                self._nontrainable_weights = list()\n            self._nontrainable_weights.append(weight)\n        return weight\n\n    @abstractmethod\n    def build(self, inputs_shape):\n        """"""\n        An abstract method which should be overwritten in derived classes\n        to define all necessary trainable weights of the layer.\n\n        self.built should be set as True after self.build() is called.\n\n        :param inputs_shape: tuple\n        """"""\n        raise Exception(""The build(self, inputs_shape) method must be implemented by inherited class"")\n\n    @abstractmethod\n    def forward(self, inputs):\n        """"""\n        An abstract method which should be overwritten in derived classes\n        to define forward feeding operations of the layer.\n\n        :param inputs: Tensor\n        :return: Tensor\n        """"""\n        raise Exception(""The forward method must be implemented by inherited class"")\n\n    @abstractmethod\n    def __repr__(self):\n        reprstr = ""Layer""\n        return reprstr\n\n    def __setitem__(self, key, item):\n        raise TypeError(""The Layer API does not allow to use the method: `__setitem__`"")\n\n    def __delitem__(self, key):\n        raise TypeError(""The Layer API does not allow to use the method: `__delitem__`"")\n\n    def __setattr__(self, key, value):\n        if isinstance(value, Layer):\n            value._fix_nodes_for_layers()\n            if self._layers is None:\n                self._layers = []\n            self._layers.append(value)\n        super().__setattr__(key, value)\n\n    def __delattr__(self, name):\n        value = getattr(self, name, None)\n        if isinstance(value, Layer):\n            self._layers.remove(value)\n        super().__delattr__(name)\n\n    @protected_method\n    def get_args(self):\n        init_args = {""layer_type"": ""normal""}\n        return init_args\n\n    @protected_method\n    def _get_init_args(self, skip=3):\n        """"""Get all arguments of current layer for saving the graph.""""""\n        stack = inspect.stack()\n\n        if len(stack) < skip + 1:\n            raise ValueError(""The length of the inspection stack is shorter than the requested start position."")\n\n        args, _, _, values = inspect.getargvalues(stack[skip][0])\n\n        params = {}\n\n        for arg in args:\n\n            # some args dont need to be saved into the graph. e.g. the input placeholder\n            if values[arg] is not None and arg not in [\'self\', \'prev_layer\', \'inputs\']:\n\n                val = values[arg]\n\n                if arg == ""dtype"" and isinstance(val, tf.DType):\n                    params[arg] = repr(val)\n                    continue\n\n                # change function (e.g. act) into dictionary of module path and function name\n                if inspect.isfunction(val):\n                    if (""__module__"" in dir(val)) and (len(val.__module__) > 10) and (val.__module__[0:10]\n                                                                                      == ""tensorflow""):\n                        params[arg] = val.__name__\n                    else:\n                        params[arg] = (\'is_Func\', utils.func2str(val))\n                # ignore more args e.g. TL initializer\n                elif arg.endswith(\'init\'):\n                    continue\n                # for other data type, save them directly\n                else:\n                    params[arg] = val\n\n        return params\n\n\nclass LayerNode(object):\n    """"""\n    The class :class:`LayerNode` class represents a conceptional node for a layer.\n\n    LayerNode is used for building static model and it is actually a light weighted\n    wrapper over Layer. Specifically, it is used for building static computational graph\n    (see _construct_graph() in tl.models.Model). In static model, each layer relates to\n    one or more LayerNode, and the connection relationship between layers is built upon\n    LayerNode. In addition, LayerNode eases layer reuse and weights sharing.\n\n    Parameters\n    ----------\n    layer : tl.layers.Layer\n        A tl layer that wants to create a node.\n    node_index : int\n        Index of this node in layer._nodes.\n    in_nodes \xef\xbc\x9aa list of LayerNode\n        Father nodes to this node.\n    in_tensors : a list of tensors\n        Input tensors to this node.\n    out_tensors : a list of tensors\n        Output tensors to this node.\n    in_tensor_idxes : a list of int\n        Indexes of each input tensor in its corresponding node\'s out_tensors.\n\n    Methods\n    ---------\n    __init__()\n        Initializing the LayerNode.\n    __call__()\n        (1) Forwarding through the layer. (2) Update its input/output tensors.\n    """"""\n\n    def __init__(self, layer, node_index, in_nodes, in_tensors, out_tensors, in_tensor_idxes):\n        """"""\n\n        Parameters\n        ----------\n        layer\n        node_index\n        in_nodes\n        in_tensors\n        out_tensors\n        in_tensor_idxes\n        """"""\n        self.layer = layer\n        self.node_index = node_index\n        self.in_nodes = in_nodes\n        self.out_nodes = []\n        self.in_tensors = in_tensors\n        self.out_tensors = out_tensors\n        self.name = layer.name + ""_node_{}"".format(node_index)\n\n        self.in_tensors_idxes = in_tensor_idxes\n\n        self.visited = False\n\n    def __call__(self, inputs, **kwargs):\n        """"""(1) Forwarding through the layer. (2) Update its input/output tensors.""""""\n        outputs = self.layer.forward(inputs, **kwargs)\n        self.in_tensors = tolist(inputs)\n        self.out_tensors = tolist(outputs)\n        return self.out_tensors\n\n\nclass ModelLayer(Layer):\n    """"""\n    The class :class:`ModelLayer` converts a :class:`Model` to a :class:`Layer` instance.\n\n    Note that only a :class:`Model` with specified inputs and outputs can be converted to a :class:`ModelLayer`.\n    For example, a customized model in dynamic eager mode normally does NOT have specified inputs and outputs so the\n    customized model in dynamic eager mode can NOT be converted to a :class:`ModelLayer`.\n\n    Parameters\n    ----------\n    model: tl.models.Model\n        A model.\n    name : str or None\n        A unique layer name. If None, a unique name will be automatically assigned.\n\n    Methods\n    ---------\n    __init__()\n        Initializing the ModelLayer.\n    weights()\n        Same as the weights of the given model.\n    build()\n        Do nothing because the given model has already been built.\n    forward()\n        Forward the computation. Simply call the forward() of the given model.\n    """"""\n\n    def __init__(self, model, name=None):\n        """"""\n        Initializing the ModelLayer given a instance of Model.\n\n        :param model:  tl.models.Model\n        """"""\n        super(ModelLayer, self).__init__(name=name)\n\n        self.model = model\n\n        # Layer building state\n        self._built = True\n\n        # Layer weight state\n        self._all_weights = model.all_weights\n        self._trainable_weights = model.trainable_weights\n        self._nontrainable_weights = model.nontrainable_weights\n\n        # Layer training state\n        self.is_train = True\n\n        logging.info(""ModelLayer %s from Model: %s"" % (self.name, self.model.name))\n\n    def __repr__(self):\n        tmpstr = \'ModelLayer\' + \'(\\n\'\n\n        modstr = self.model.__repr__()\n        modstr = _addindent(modstr, 2)\n\n        tmpstr += modstr + \')\'\n        return tmpstr\n\n    def build(self, inputs_shape):\n        pass\n\n    def forward(self, inputs):\n        return self.model.forward(inputs)\n\n    def _set_mode_for_layers(self, is_train):\n        """""" Set training/evaluation mode for the ModelLayer.""""""\n        self.is_train = is_train\n        return self.model._set_mode_for_layers(is_train)\n\n    def _fix_nodes_for_layers(self):\n        """""" fix LayerNodes to stop growing for this ModelLayer.""""""\n        self._nodes_fixed = True\n        self.model._fix_nodes_for_layers()\n\n    def _release_memory(self):\n        """"""\n        WARINING: This function should be called with great caution.\n\n        self.inputs and self.outputs will be set as None but not deleted in order to release memory.\n        """"""\n\n        super(ModelLayer, self)._release_memory()\n        self.model.release_memory()\n\n    def get_args(self):\n        init_args = {}\n        init_args.update({""layer_type"": ""modellayer""})\n        # init_args[""model""] = utils.net2static_graph(self.layer_args[""model""])\n        init_args[""model""] = self.layer_args[""model""].config\n        return init_args\n\n\nclass LayerList(Layer):\n    """"""\n    The class :class:`LayerList` is a linear stack of layers.\n\n    The :class:`LayerList` can be created by passing a list of layer instances.\n    The given layer instances will be automatically connected one by one.\n\n    Parameters\n    ----------\n    layers: list of Layer\n        A list of layers.\n    name : str or None\n        A unique layer name. If None, a unique name will be automatically assigned.\n\n    Methods\n    ---------\n    __init__()\n        Initializing the LayerList.\n    weights()\n        A collection of weights of all the layer instances.\n    build()\n        Build the LayerList. The layer instances will be connected automatically one by one.\n    forward()\n        Forward the computation. The computation will go through all layer instances.\n    """"""\n\n    def __init__(self, layers, name=None):\n        """"""\n        Initializing the LayerList given a list of Layer.\n\n        :param layers: list of Layer\n        :param name: str or None\n        """"""\n\n        super(LayerList, self).__init__(name=name)\n        self.layers = layers\n\n        is_built = True\n        for layer in self.layers:\n            self._trainable_weights.extend(layer.trainable_weights)\n            self._nontrainable_weights.extend(layer.nontrainable_weights)\n            if layer._built is False:\n                is_built = False\n            if layer._built and layer.all_weights is not None:\n                # some layers in the list passed in have already been built\n                # e.g. using input shape to construct layers in dynamic eager\n                if self._all_weights is None:\n                    self._all_weights = list()\n                self._all_weights.extend(layer.all_weights)\n        if is_built:\n            self._built = True\n\n        logging.info(\n            ""LayerList %s including layers [%s]"" % (self.name, \', \'.join([layer.name for layer in self.layers]))\n        )\n\n        # check layer name uniqueness in LayerList\n        local_layer_name_set = set()\n        for layer in self.layers:\n            if layer.name not in local_layer_name_set:\n                local_layer_name_set.add(layer.name)\n            else:\n                raise ValueError(\n                    \'Layer name \\\'%s\\\' has already been used by another layer. Please change the layer name.\' %\n                    layer.name\n                )\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            return LayerList(list(self.layers)[idx])\n        else:\n            return self.layers[idx]\n\n    def __len__(self):\n        return len(self.layers)\n\n    def __repr__(self):\n        tmpstr = \'LayerList\' + \'(\\n\'\n        for idx, layer in enumerate(self.layers):\n            modstr = layer.__repr__()\n            modstr = _addindent(modstr, 2)\n            tmpstr = tmpstr + \'  (\' + str(idx) + \'): \' + modstr + \'\\n\'\n\n        tmpstr = tmpstr + \')\'\n        return tmpstr\n\n    def build(self, inputs_shape):\n        """"""\n        Build the LayerList. The layer instances will be connected automatically one by one.\n        """"""\n        in_tensor = self._input_tensors\n        # in_layer = self._input_layer\n        for layer in self.layers:\n            is_build = layer._built\n            out_tensor = layer(in_tensor)\n            # nlayer = layer(in_layer)\n            if is_build is False and layer.all_weights is not None:\n                if self._all_weights is None:\n                    self._all_weights = list()\n                self._all_weights.extend(layer.all_weights)\n            layer._built = True\n            in_tensor = out_tensor\n            # in_layer = nlayer\n\n    def forward(self, inputs):\n        """"""\n        Forward the computation. The computation will go through all layer instances.\n        """"""\n        z = inputs\n        for layer in self.layers:\n            z = layer.forward(z)\n        return z\n\n    def _set_mode_for_layers(self, is_train):\n        """"""Set training/evaluation mode for all layer instances.""""""\n        self.is_train = is_train\n        for layer in self.layers:\n            if isinstance(layer, ModelLayer):\n                layer._set_mode_for_layers(is_train)\n            elif isinstance(layer, LayerList):\n                layer._set_mode_for_layers(is_train)\n            else:\n                layer.is_train = is_train\n\n    def _fix_nodes_for_layers(self):\n        """""" fix LayerNodes to stop growing for this LayerList.""""""\n        self._nodes_fixed = True\n        for layer in self.layers:\n            layer._fix_nodes_for_layers()\n\n    def _release_memory(self):\n        """"""\n        WARINING: This function should be called with great caution.\n\n        self.inputs and self.outputs will be set as None but not deleted.\n        """"""\n        super(LayerList, self)._release_memory()\n        for layer in self.layers:\n            layer._release_memory()\n\n    def get_args(self):\n        init_args = {}\n        layers = self.layer_args[""layers""]\n        init_args[""layers""] = [layer.config for layer in layers]\n        init_args.update({""layer_type"": ""layerlist""})\n        return init_args\n\n\ndef _addindent(s_, numSpaces):\n    s = s_.split(\'\\n\')\n    # don\'t do anything for single-line stuff\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [(numSpaces * \' \') + line for line in s]\n    s = \'\\n\'.join(s)\n    s = first + \'\\n\' + s\n    return s\n\n\ndef tolist(tensors):\n    if isinstance(tensors, list) or isinstance(tensors, tuple):\n        ntensors = list()\n        for t in tensors:\n            ntensors += tolist(t)\n        return ntensors\n    else:\n        return [tensors]\n'"
tensorlayer/layers/deprecated.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\n__all__ = []\n\n\nclass NonExistingLayerError(Exception):\n    pass\n\n\n# activation.py\n__all__ += [\n    \'PReluLayer\',\n    \'PRelu6Layer\',\n    \'PTRelu6Layer\',\n]\n\n__log__ = \'\\n Hint: 1) downgrade TF and TL from version 2.x to 1.x. 2) check the documentation of TF and TL version 2.x\'\n\n\ndef PReluLayer(*args, **kwargs):\n    raise NonExistingLayerError(""PReluLayer(net, name=\'a\') --> PRelu(name=\'a\')(net))"" + __log__)\n\n\ndef PRelu6Layer(*args, **kwargs):\n    raise NonExistingLayerError(""PRelu6Layer(net, name=\'a\') --> PRelu6(name=\'a\')(net))"" + __log__)\n\n\ndef PTRelu6Layer(*args, **kwargs):\n    raise NonExistingLayerError(""PTRelu6Layer(net, name=\'a\') --> PTRelu(name=\'a\')(net))"" + __log__)\n\n\n# convolution/atrous_conv.py\n__all__ += [\n    \'AtrousConv1dLayer\',\n    \'AtrousConv2dLayer\',\n    \'AtrousDeConv2dLayer\',\n]\n\n\ndef AtrousConv1dLayer(*args, **kwargs):\n    raise NonExistingLayerError(""use `tl.layers.Conv1d` with dilation instead"" + __log__)\n\n\ndef AtrousConv2dLayer(*args, **kwargs):\n    raise NonExistingLayerError(""use `tl.layers.Conv2d` with dilation instead"" + __log__)\n\n\ndef AtrousDeConv2dLayer(*args, **kwargs):\n    # raise NonExistingLayerError(""AtrousDeConv2dLayer(net, name=\'a\') --> AtrousDeConv2d(name=\'a\')(net)"")\n    raise NonExistingLayerError(""use `tl.layers.DeConv2d` with dilation instead"" + __log__)\n\n\n# dense/base_dense.py\n__all__ += [\n    \'DenseLayer\',\n]\n\n\ndef DenseLayer(*args, **kwargs):\n    raise NonExistingLayerError(""DenseLayer(net, name=\'a\') --> Dense(name=\'a\')(net)"" + __log__)\n\n\n# dense/binary_dense.py\n__all__ += [\n    \'BinaryDenseLayer\',\n]\n\n\ndef BinaryDenseLayer(*args, **kwargs):\n    raise NonExistingLayerError(""BinaryDenseLayer(net, name=\'a\') --> BinaryDense(name=\'a\')(net)"" + __log__)\n\n\n# dense/dorefa_dense.py\n__all__ += [\n    \'DorefaDenseLayer\',\n]\n\n\ndef DorefaDenseLayer(*args, **kwargs):\n    raise NonExistingLayerError(""DorefaDenseLayer(net, name=\'a\') --> DorefaDense(name=\'a\')(net)"" + __log__)\n\n\n# dense/dropconnect.py\n__all__ += [\n    \'DropconnectDenseLayer\',\n]\n\n\ndef DropconnectDenseLayer(*args, **kwargs):\n    raise NonExistingLayerError(""DropconnectDenseLayer(net, name=\'a\') --> DropconnectDense(name=\'a\')(net)"" + __log__)\n\n\n# dense/quan_dense_bn.py\n__all__ += [\n    \'QuanDenseLayerWithBN\',\n]\n\n\ndef QuanDenseLayerWithBN(*args, **kwargs):\n    raise NonExistingLayerError(""QuanDenseLayerWithBN(net, name=\'a\') --> QuanDenseWithBN(name=\'a\')(net)"" + __log__)\n\n\n# dense/ternary_dense.py\n__all__ += [\n    \'TernaryDenseLayer\',\n]\n\n\ndef TernaryDenseLayer(*args, **kwargs):\n    raise NonExistingLayerError(""TernaryDenseLayer(net, name=\'a\') --> TernaryDense(name=\'a\')(net)"" + __log__)\n\n\n# dropout.py\n__all__ += [\n    \'DropoutLayer\',\n]\n\n\ndef DropoutLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""DropoutLayer(net, is_train=True, name=\'a\') --> Dropout(name=\'a\')(net, is_train=True)"" + __log__\n    )\n\n\n# extend.py\n__all__ += [\n    \'ExpandDimsLayer\',\n    \'TileLayer\',\n]\n\n\ndef ExpandDimsLayer(*args, **kwargs):\n    raise NonExistingLayerError(""ExpandDimsLayer(net, name=\'a\') --> ExpandDims(name=\'a\')(net)"" + __log__)\n\n\ndef TileLayer(*args, **kwargs):\n    raise NonExistingLayerError(""TileLayer(net, name=\'a\') --> Tile(name=\'a\')(net)"" + __log__)\n\n\n# image_resampling.py\n__all__ += [\n    \'UpSampling2dLayer\',\n    \'DownSampling2dLayer\',\n]\n\n\ndef UpSampling2dLayer(*args, **kwargs):\n    raise NonExistingLayerError(""UpSampling2dLayer(net, name=\'a\') --> UpSampling2d(name=\'a\')(net)"" + __log__)\n\n\ndef DownSampling2dLayer(*args, **kwargs):\n    raise NonExistingLayerError(""DownSampling2dLayer(net, name=\'a\') --> DownSampling2d(name=\'a\')(net)"" + __log__)\n\n\n# importer.py\n__all__ += [\n    \'SlimNetsLayer\',\n    \'KerasLayer\',\n]\n\n\ndef SlimNetsLayer(*args, **kwargs):\n    raise NonExistingLayerError(""SlimNetsLayer(net, name=\'a\') --> SlimNets(name=\'a\')(net)"" + __log__)\n\n\ndef KerasLayer(*args, **kwargs):\n    raise NonExistingLayerError(""KerasLayer(net, name=\'a\') --> Keras(name=\'a\')(net)"" + __log__)\n\n\n# inputs.py\n__all__ += [\n    \'InputLayer\',\n]\n\n\ndef InputLayer(*args, **kwargs):\n    raise NonExistingLayerError(""InputLayer(x, name=\'a\') --> Input(name=\'a\')(x)"" + __log__)\n\n\n# embedding.py\n__all__ += [\n    \'OneHotInputLayer\',\n    \'Word2vecEmbeddingInputlayer\',\n    \'EmbeddingInputlayer\',\n    \'AverageEmbeddingInputlayer\',\n]\n\n\ndef OneHotInputLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""Not longer Input layer: OneHotInputLayer(x, name=\'a\') --> OneHot(name=\'a\')(layer)"" + __log__\n    )\n\n\ndef Word2vecEmbeddingInputlayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""Not longer Input layer: Word2vecEmbeddingInputlayer(x, name=\'a\') --> Word2vecEmbedding(name=\'a\')(layer)"" +\n        __log__\n    )\n\n\ndef EmbeddingInputlayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""Not longer Input layer: EmbeddingInputlayer(x, name=\'a\') --> Embedding(name=\'a\')(layer)"" + __log__\n    )\n\n\ndef AverageEmbeddingInputlayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""Not longer Input layer: AverageEmbeddingInputlayer(x, name=\'a\') --> AverageEmbedding(name=\'a\')(layer)"" +\n        __log__\n    )\n\n\n# lambda.py\n__all__ += [\n    \'LambdaLayer\',\n    \'ElementwiseLambdaLayer\',\n]\n\n\ndef LambdaLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""LambdaLayer(x, lambda x: 2*x, name=\'a\') --> Lambda(lambda x: 2*x, name=\'a\')(x)"" + __log__\n    )\n\n\ndef ElementwiseLambdaLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""ElementwiseLambdaLayer(x, ..., name=\'a\') --> ElementwiseLambda(..., name=\'a\')(x)"" + __log__\n    )\n\n\n# merge.py\n__all__ += [\n    \'ConcatLayer\',\n    \'ElementwiseLayer\',\n]\n\n\ndef ConcatLayer(*args, **kwargs):\n    raise NonExistingLayerError(""ConcatLayer(x, ..., name=\'a\') --> Concat(..., name=\'a\')(x)"" + __log__)\n\n\ndef ElementwiseLayer(*args, **kwargs):\n    raise NonExistingLayerError(""ElementwiseLayer(x, ..., name=\'a\') --> Elementwise(..., name=\'a\')(x)"" + __log__)\n\n\n# noise.py\n__all__ += [\n    \'GaussianNoiseLayer\',\n]\n\n\ndef GaussianNoiseLayer(*args, **kwargs):\n    raise NonExistingLayerError(""GaussianNoiseLayer(x, ..., name=\'a\') --> GaussianNoise(..., name=\'a\')(x)"" + __log__)\n\n\n# normalization.py\n__all__ += [\n    \'BatchNormLayer\',\n    \'InstanceNormLayer\',\n    \'LayerNormLayer\',\n    \'LocalResponseNormLayer\',\n    \'GroupNormLayer\',\n    \'SwitchNormLayer\',\n]\n\n\ndef BatchNormLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""BatchNormLayer(x, is_train=True, name=\'a\') --> BatchNorm(name=\'a\')(x, is_train=True)"" + __log__\n    )\n\n\ndef InstanceNormLayer(*args, **kwargs):\n    raise NonExistingLayerError(""InstanceNormLayer(x, name=\'a\') --> InstanceNorm(name=\'a\')(x)"" + __log__)\n\n\ndef LayerNormLayer(*args, **kwargs):\n    raise NonExistingLayerError(""LayerNormLayer(x, name=\'a\') --> LayerNorm(name=\'a\')(x)"" + __log__)\n\n\ndef LocalResponseNormLayer(*args, **kwargs):\n    raise NonExistingLayerError(""LocalResponseNormLayer(x, name=\'a\') --> LocalResponseNorm(name=\'a\')(x)"" + __log__)\n\n\ndef GroupNormLayer(*args, **kwargs):\n    raise NonExistingLayerError(""GroupNormLayer(x, name=\'a\') --> GroupNorm(name=\'a\')(x)"" + __log__)\n\n\ndef SwitchNormLayer(*args, **kwargs):\n    raise NonExistingLayerError(""SwitchNormLayer(x, name=\'a\') --> SwitchNorm(name=\'a\')(x)"" + __log__)\n\n\n# quantize_layer.py\n__all__ += [\n    \'SignLayer\',\n]\n\n\ndef SignLayer(*args, **kwargs):\n    raise NonExistingLayerError(""SignLayer(x, name=\'a\') --> Sign(name=\'a\')(x)"" + __log__)\n\n\n# recurrent/lstm_layers.py\n__all__ += [\n    \'ConvLSTMLayer\',\n]\n\n\ndef ConvLSTMLayer(*args, **kwargs):\n    raise NonExistingLayerError(""ConvLSTMLayer(x, name=\'a\') --> ConvLSTM(name=\'a\')(x)"" + __log__)\n\n\n# recurrent/rnn_dynamic_layers.py\n__all__ += [\n    \'DynamicRNNLayer\',\n    \'BiDynamicRNNLayer\',\n]\n\n\ndef DynamicRNNLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""DynamicRNNLayer(x, is_train=True, name=\'a\') --> DynamicRNN(name=\'a\')(x, is_train=True)"" + __log__\n    )\n\n\ndef BiDynamicRNNLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""BiDynamicRNNLayer(x, is_train=True, name=\'a\') --> BiDynamicRNN(name=\'a\')(x, is_train=True)"" + __log__\n    )\n\n\n# recurrent/rnn_layers.py\n__all__ += [\n    \'RNNLayer\',\n    \'BiRNNLayer\',\n]\n\n\ndef RNNLayer(*args, **kwargs):\n    raise NonExistingLayerError(""RNNLayer(x, name=\'a\') --> RNN(name=\'a\')(x)"" + __log__)\n\n\ndef BiRNNLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""BiRNNLayer(x, is_train=True, name=\'a\') --> BiRNN(name=\'a\')(x, is_train=True)"" + __log__\n    )\n\n\n# reshape.py\n__all__ += [\n    \'FlattenLayer\',\n    \'ReshapeLayer\',\n    \'TransposeLayer\',\n]\n\n\ndef FlattenLayer(*args, **kwargs):\n    raise NonExistingLayerError(""FlattenLayer(x, name=\'a\') --> Flatten(name=\'a\')(x)"" + __log__)\n\n\ndef ReshapeLayer(*args, **kwargs):\n    raise NonExistingLayerError(""ReshapeLayer(x, name=\'a\') --> Reshape(name=\'a\')(x)"" + __log__)\n\n\ndef TransposeLayer(*args, **kwargs):\n    raise NonExistingLayerError(""TransposeLayer(x, name=\'a\') --> Transpose(name=\'a\')(x)"" + __log__)\n\n\n# scale.py\n__all__ += [\n    \'ScaleLayer\',\n]\n\n\ndef ScaleLayer(*args, **kwargs):\n    raise NonExistingLayerError(""ScaleLayer(x, name=\'a\') --> Scale(name=\'a\')(x)"" + __log__)\n\n\n# spatial_transformer.py\n__all__ += [\'SpatialTransformer2dAffineLayer\']\n\n\ndef SpatialTransformer2dAffineLayer(*args, **kwargs):\n    raise NonExistingLayerError(\n        ""SpatialTransformer2dAffineLayer(x1, x2, name=\'a\') --> SpatialTransformer2dAffine(name=\'a\')(x1, x2)"" + __log__\n    )\n\n\n# stack.py\n__all__ += [\n    \'StackLayer\',\n    \'UnStackLayer\',\n]\n\n\ndef StackLayer(*args, **kwargs):\n    raise NonExistingLayerError(""StackLayer(x1, x2, name=\'a\') --> Stack(name=\'a\')(x1, x2)"" + __log__)\n\n\ndef UnStackLayer(*args, **kwargs):\n    raise NonExistingLayerError(""UnStackLayer(x1, x2, name=\'a\') --> UnStack(name=\'a\')(x1, x2)"" + __log__)\n\n\n# time_distributed.py\n__all__ += [\n    \'TimeDistributedLayer\',\n]\n\n\ndef TimeDistributedLayer(*args, **kwargs):\n    # raise NonExistingLayerError(""TimeDistributedLayer(x1, x2, name=\'a\') --> TimeDistributed(name=\'a\')(x1, x2)"")\n    raise NonExistingLayerError(""TimeDistributedLayer is removed for TF 2.0, please use eager mode instead."" + __log__)\n'"
tensorlayer/layers/dropout.py,2,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\n    \'Dropout\',\n]\n\n\nclass Dropout(Layer):\n    """"""\n    The :class:`Dropout` class is a noise layer which randomly set some\n    activations to zero according to a keeping probability.\n\n    Parameters\n    ----------\n    keep : float\n        The keeping probability.\n        The lower the probability it is, the more activations are set to zero.\n    seed : int or None\n        The seed for random dropout.\n    name : None or str\n        A unique layer name.\n\n    """"""\n\n    def __init__(self, keep, seed=None, name=None):  #""dropout""):\n        super(Dropout, self).__init__(name)\n        self.keep = keep\n        self.seed = seed\n\n        self.build()\n        self._built = True\n\n        logging.info(""Dropout %s: keep: %f "" % (self.name, self.keep))\n\n    def __repr__(self):\n        s = (\'{classname}(keep={keep}\')\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        if self.is_train:\n            outputs = tf.nn.dropout(inputs, rate=1 - (self.keep), seed=self.seed, name=self.name)\n        else:\n            outputs = inputs\n        return outputs\n'"
tensorlayer/layers/embedding.py,29,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\n    \'OneHot\',\n    \'Word2vecEmbedding\',\n    \'Embedding\',\n    \'AverageEmbedding\',\n]\n\n\nclass OneHot(Layer):\n    """"""\n    The :class:`OneHot` class is the starting layer of a neural network, see ``tf.one_hot``.\n    Useful link: `https://www.tensorflow.org/api_docs/python/tf/one_hot`.\n\n    Parameters\n    ----------\n    depth : None or int\n        If the input indices is rank N, the output will have rank N+1. The new axis is created at dimension `axis` (default: the new axis is appended at the end).\n    on_value : None or number\n        The value to represnt `ON`. If None, it will default to the value 1.\n    off_value : None or number\n        The value to represnt `OFF`. If None, it will default to the value 0.\n    axis : None or int\n        The axis.\n    dtype : None or TensorFlow dtype\n        The data type, None means tf.float32.\n    name : str\n        A unique layer name.\n\n    Examples\n    ---------\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.Input([32], dtype=tf.int32)\n    >>> onehot = tl.layers.OneHot(depth=8)\n    >>> print(onehot)\n    OneHot(depth=8, name=\'onehot\')\n    >>> tensor = tl.layers.OneHot(depth=8)(net)\n    >>> print(tensor)\n    tf.Tensor([...], shape=(32, 8), dtype=float32)\n\n    """"""\n\n    def __init__(self, depth=None, on_value=None, off_value=None, axis=None, dtype=None, name=None):  #\'input\'):\n\n        super(OneHot, self).__init__(name)\n        self.depth = depth\n        self.on_value = on_value\n        self.off_value = off_value\n        self.axis = axis\n        self.dtype = dtype\n        logging.info(""OneHotInput  %s"" % (self.name))\n\n        if not self._built:\n            self.build(tuple())\n            self._built = True\n\n        if self.depth is None:\n            raise RuntimeError(self.__class__.__name__ + "": depth == None the number of output units is undefined"")\n\n    def __repr__(self):\n        s = (\'{classname}(depth={depth}\')\n        if self.on_value is not None:\n            s += \', on_value={on_value}\'\n        if self.off_value is not None:\n            s += \', off_value={off_value}\'\n        if self.axis is not None:\n            s += \', axis={axis}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        """"""\n        Parameters\n        ----------\n        inputs : input tensor\n            The inputs are indices. The locations represented by indices in indices take value on_value, while all other locations take value off_value.\n        """"""\n        outputs = tf.one_hot(\n            inputs, self.depth, on_value=self.on_value, off_value=self.off_value, axis=self.axis, dtype=self.dtype\n        )\n        return outputs\n\n\nclass Word2vecEmbedding(Layer):\n    """"""\n    The :class:`Word2vecEmbedding` class is a fully connected layer.\n    For Word Embedding, words are input as integer index.\n    The output is the embedded word vector.\n\n    The layer integrates NCE loss by default (activate_nce_loss=True).\n    If the NCE loss is activated, in a dynamic model,\n    the computation of nce loss can be turned off in customised forward feeding\n    by setting use_nce_loss=False when the layer is called.\n    The NCE loss can be deactivated by setting activate_nce_loss=False.\n\n    Parameters\n    ----------\n    vocabulary_size : int\n        The size of vocabulary, number of words\n    embedding_size : int\n        The number of embedding dimensions\n    num_sampled : int\n        The number of negative examples for NCE loss\n    activate_nce_loss : boolean\n        Whether activate nce loss or not. By default, True\n        If True, the layer will return both outputs of embedding and nce_cost in forward feeding.\n        If False, the layer will only return outputs of embedding.\n        In a dynamic model, the computation of nce loss can be turned off in forward feeding\n        by setting use_nce_loss=False when the layer is called.\n        In a static model, once the model is constructed, the computation of nce loss\n        cannot be changed (always computed or not computed).\n    nce_loss_args : dictionary\n        The arguments for tf.nn.nce_loss()\n    E_init : initializer\n        The initializer for initializing the embedding matrix\n    nce_W_init : initializer\n        The initializer for initializing the nce decoder weight matrix\n    nce_b_init : initializer\n        The initializer for initializing of the nce decoder bias vector\n    name : str\n        A unique layer name\n\n    Attributes\n    ----------\n    outputs : Tensor\n        The embedding layer outputs.\n    normalized_embeddings : Tensor\n        Normalized embedding matrix.\n    nce_weights : Tensor\n        The NCE weights only when activate_nce_loss is True.\n    nce_biases: Tensor\n        The NCE biases only when activate_nce_loss is True.\n\n    Examples\n    --------\n    Word2Vec With TensorLayer (Example in `examples/text_word_embedding/tutorial_word2vec_basic.py`)\n\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> batch_size = 8\n    >>> embedding_size = 50\n    >>> inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n    >>> labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n    >>> emb_net = tl.layers.Word2vecEmbedding(\n    >>>     vocabulary_size=10000,\n    >>>     embedding_size=embedding_size,\n    >>>     num_sampled=100,\n    >>>     activate_nce_loss=True, # the nce loss is activated\n    >>>     nce_loss_args={},\n    >>>     E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0),\n    >>>     nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))),\n    >>>     nce_b_init=tl.initializers.constant(value=0.0),\n    >>>     name=\'word2vec_layer\',\n    >>> )\n    >>> print(emb_net)\n    Word2vecEmbedding(vocabulary_size=10000, embedding_size=50, num_sampled=100, activate_nce_loss=True, nce_loss_args={})\n    >>> embed_tensor = emb_net(inputs, use_nce_loss=False) # the nce loss is turned off and no need to provide labels\n    >>> embed_tensor = emb_net([inputs, labels], use_nce_loss=False) # the nce loss is turned off and the labels will be ignored\n    >>> embed_tensor, embed_nce_loss = emb_net([inputs, labels]) # the nce loss is calculated\n    >>> outputs = tl.layers.Dense(n_units=10, name=""dense"")(embed_tensor)\n    >>> model = tl.models.Model(inputs=[inputs, labels], outputs=[outputs, embed_nce_loss], name=""word2vec_model"") # a static model\n    >>> out = model([data_x, data_y], is_train=True) # where data_x is inputs and data_y is labels\n\n    References\n    ----------\n    `https://www.tensorflow.org/tutorials/representation/word2vec`\n\n    """"""\n\n    def __init__(\n        self,\n        vocabulary_size,\n        embedding_size,\n        num_sampled=64,\n        activate_nce_loss=True,\n        nce_loss_args=None,\n        E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0),\n        nce_W_init=tl.initializers.truncated_normal(stddev=0.03),\n        nce_b_init=tl.initializers.constant(value=0.0),\n        name=None,  #\'word2vec\',\n    ):\n\n        super(Word2vecEmbedding, self).__init__(name)\n        self.vocabulary_size = vocabulary_size\n        self.embedding_size = embedding_size\n        self.num_sampled = num_sampled\n        self.E_init = E_init\n        self.activate_nce_loss = activate_nce_loss\n\n        if self.activate_nce_loss:\n            self.nce_loss_args = nce_loss_args\n            self.nce_W_init = nce_W_init\n            self.nce_b_init = nce_b_init\n\n        if not self._built:\n            self.build(tuple())\n            self._built = True\n\n        logging.info(""Word2vecEmbedding %s: (%d, %d)"" % (self.name, self.vocabulary_size, self.embedding_size))\n\n    def __repr__(self):\n        s = (\'{classname}(\')\n        s += \'vocabulary_size={vocabulary_size}\'\n        s += \', embedding_size={embedding_size}\'\n        s += \', num_sampled={num_sampled}\'\n        s += \', activate_nce_loss={activate_nce_loss}\'\n        if self.activate_nce_loss:\n            s += \', nce_loss_args={nce_loss_args}\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        """"""\n        Parameters\n        ----------\n        inputs_shape : tuple\n            the shape of inputs tensor\n        """"""\n        # Look up embeddings for inputs.\n        # Note: a row of \'embeddings\' is the vector representation of a word.\n        # for the sake of speed, it is better to slice the embedding matrix\n        # instead of transferring a word id to one-hot-format vector and then\n        # multiply by the embedding matrix.\n        # embed is the outputs of the hidden layer (embedding layer), it is a\n        # row vector with \'embedding_size\' values.\n\n        self.embeddings = self._get_weights(\n            ""embeddings"",\n            shape=(self.vocabulary_size, self.embedding_size),\n            init=self.E_init,\n        )\n\n        self.normalized_embeddings = tf.nn.l2_normalize(self.embeddings, 1)\n\n        if self.activate_nce_loss:\n            # Construct the variables for the NCE loss (i.e. negative sampling)\n            self.nce_weights = self._get_weights(\n                ""nce_weights"",\n                shape=(self.vocabulary_size, self.embedding_size),\n                init=self.nce_W_init,\n            )\n\n            self.nce_biases = self._get_weights(\n                ""nce_biases"",\n                shape=(self.vocabulary_size, ),\n                init=self.nce_b_init,\n            )\n\n    # @tf.function\n    def forward(self, inputs, use_nce_loss=None):\n        """"""\n        Parameters\n        ----------\n        inputs : tensor or list\n            If the nce loss is activated and is used, the argument should be a list of two tensors [inputs, labels].\n            Otherwise, the argument should be a single tensor which is inputs.\n        use_nce_loss: boolean\n            Whether use NCE loss in this run.\n            If the nce loss is used, the activate_nce_loss should be True when the layer is initialized.\n            By default, same as activate_nce_loss.\n\n        Outputs:\n        ----------\n        outputs: tensor\n        nce_cost: tensor\n            The nce_cost is returned only if the nce_loss is used.\n        """"""\n\n        if isinstance(inputs, list):\n            outputs = tf.nn.embedding_lookup(params=self.embeddings, ids=inputs[0])\n        else:\n            outputs = tf.nn.embedding_lookup(params=self.embeddings, ids=inputs)\n\n        if use_nce_loss is True and not self.activate_nce_loss:\n            raise AttributeError(\n                ""The nce loss is not activated when the %s is initialized. Please set activate_nce_loss=True."" %\n                self.__class__.__name__\n            )\n\n        if self.activate_nce_loss and (use_nce_loss is True or use_nce_loss is None):\n            if not isinstance(inputs, list):\n                raise ValueError(""If nce loss is used, the labels of inputs must be provided."")\n\n            nce_cost = tf.reduce_mean(\n                input_tensor=tf.nn.nce_loss(\n                    weights=self.nce_weights, biases=self.nce_biases, inputs=outputs, labels=inputs[1],\n                    num_sampled=self.num_sampled, num_classes=self.vocabulary_size, **self.nce_loss_args\n                )\n            )\n\n            return outputs, nce_cost\n\n        return outputs\n\n\nclass Embedding(Layer):\n    """"""\n    The :class:`Embedding` class is a look-up table for word embedding.\n\n    Word content are accessed using integer indexes, then the output is the embedded word vector.\n    To train a word embedding matrix, you can used :class:`Word2vecEmbedding`.\n    If you have a pre-trained matrix, you can assign the parameters into it.\n\n    Parameters\n    ----------\n    vocabulary_size : int\n        The size of vocabulary, number of words.\n    embedding_size : int\n        The number of embedding dimensions.\n    E_init : initializer\n        The initializer for the embedding matrix.\n    E_init_args : dictionary\n        The arguments for embedding matrix initializer.\n    name : str\n        A unique layer name.\n\n    Attributes\n    ----------\n    outputs : tensor\n        The embedding layer output is a 3D tensor in the shape: (batch_size, num_steps(num_words), embedding_size).\n\n    Examples\n    --------\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> input = tl.layers.Input([8, 100], dtype=tf.int32)\n    >>> embed = tl.layers.Embedding(vocabulary_size=1000, embedding_size=50, name=\'embed\')\n    >>> print(embed)\n    Embedding(vocabulary_size=1000, embedding_size=50)\n    >>> tensor = embed(input)\n    >>> print(tensor)\n    tf.Tensor([...], shape=(8, 100, 50), dtype=float32)\n\n    """"""\n\n    def __init__(\n        self,\n        vocabulary_size,\n        embedding_size,\n        E_init=tl.initializers.random_uniform(-0.1, 0.1),\n        name=None,  #\'embedding\',\n    ):\n        super(Embedding, self).__init__(name)\n        self.vocabulary_size = vocabulary_size\n        self.embedding_size = embedding_size\n        self.E_init = E_init\n\n        if not self._built:\n            self.build(tuple())\n            self._built = True\n\n        logging.info(""Embedding %s: (%d, %d)"" % (self.name, self.vocabulary_size, self.embedding_size))\n\n    def __repr__(self):\n        s = (\'{classname}(\')\n        s += \'vocabulary_size={vocabulary_size}\'\n        s += \', embedding_size={embedding_size}\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        """"""\n        Parameters\n        ----------\n        inputs_shape : tuple\n            the shape of inputs tensor\n        """"""\n\n        self.embeddings = self._get_weights(\n            ""embeddings"",\n            shape=(self.vocabulary_size, self.embedding_size),\n            init=self.E_init,\n        )\n\n    # @tf.function\n    def forward(self, inputs):\n        """"""\n        Parameters\n        ----------\n        inputs : Tensor\n            The input of a network.\n        """"""\n        outputs = tf.nn.embedding_lookup(params=self.embeddings, ids=inputs)\n        return outputs\n\n\nclass AverageEmbedding(Layer):\n    """"""The :class:`AverageEmbedding` averages over embeddings of inputs.\n    This is often used as the input layer for models like DAN[1] and FastText[2].\n\n    Parameters\n    ----------\n    vocabulary_size : int\n        The size of vocabulary.\n    embedding_size : int\n        The dimension of the embedding vectors.\n    pad_value : int\n        The scalar padding value used in inputs, 0 as default.\n    E_init : initializer\n        The initializer of the embedding matrix.\n    name : str\n        A unique layer name.\n\n    Attributes\n    ----------\n    outputs : tensor\n        The embedding layer output is a 2D tensor in the shape: (batch_size, embedding_size).\n\n    References\n    ----------\n    - [1] Iyyer, M., Manjunatha, V., Boyd-Graber, J., & Daum\xe2\x80\x99e III, H. (2015). Deep Unordered Composition Rivals Syntactic Methods for Text Classification. In Association for Computational Linguistics.\n    - [2] Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2016). `Bag of Tricks for Efficient Text Classification. <http://arxiv.org/abs/1607.01759>`__\n\n    Examples\n    ---------\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> batch_size = 8\n    >>> length = 5\n    >>> input = tl.layers.Input([batch_size, length], dtype=tf.int32)\n    >>> avgembed = tl.layers.AverageEmbedding(vocabulary_size=1000, embedding_size=50, name=\'avg\')\n    >>> print(avgembed)\n    AverageEmbedding(vocabulary_size=1000, embedding_size=50, pad_value=0)\n    >>> tensor = avgembed(input)\n    >>> print(tensor)\n    tf.Tensor([...], shape=(8, 50), dtype=float32)\n\n    """"""\n\n    def __init__(\n        self,\n        vocabulary_size,\n        embedding_size,\n        pad_value=0,\n        E_init=tl.initializers.random_uniform(-0.1, 0.1),\n        name=None,  # \'average_embedding\',\n    ):\n\n        super(AverageEmbedding, self).__init__(name)\n        self.vocabulary_size = vocabulary_size\n        self.embedding_size = embedding_size\n        self.pad_value = pad_value\n        self.E_init = E_init\n\n        if not self._built:\n            self.build(tuple())\n            self._built = True\n\n        logging.info(""AverageEmbedding %s: (%d, %d)"" % (self.name, self.vocabulary_size, self.embedding_size))\n\n    def __repr__(self):\n        s = (\'{classname}(\')\n        s += \'vocabulary_size={vocabulary_size}\'\n        s += \', embedding_size={embedding_size}\'\n        s += \', pad_value={pad_value}\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        """"""\n        Parameters\n        ----------\n        inputs_shape : tuple\n            the shape of inputs tensor.\n        """"""\n        # if len(inputs_shape) != 2:\n        #     raise ValueError(\'inputs must be of size (batch_size, sentence_length)\')\n\n        self.embeddings = self._get_weights(\n            ""embeddings"",\n            shape=(self.vocabulary_size, self.embedding_size),\n            init=self.E_init,\n        )\n\n    # @tf.function\n    def forward(self, inputs):\n        """"""\n        Parameters\n        ----------\n        inputs : tensor\n            The network input.\n            For word inputs, please use integer index format, 2D tensor: (batch_size, sentence_length).\n        """"""\n        word_embeddings = tf.nn.embedding_lookup(\n            params=self.embeddings,\n            ids=inputs,\n            name=\'word_embeddings\',\n        )\n\n        # Zero out embeddings of pad value\n        masks = tf.not_equal(inputs, self.pad_value, name=\'masks\')\n        word_embeddings *= tf.cast(tf.expand_dims(masks, axis=-1), dtype=tf.float32)\n        sum_word_embeddings = tf.reduce_sum(input_tensor=word_embeddings, axis=1)\n\n        # Count number of non-padding words in each sentence\n        sentence_lengths = tf.math.count_nonzero(\n            masks,\n            axis=1,\n            keepdims=True,\n            dtype=tf.float32,\n            name=\'sentence_lengths\',\n        )\n\n        sentence_embeddings = tf.divide(\n            sum_word_embeddings,\n            sentence_lengths + 1e-8,  # Add epsilon to avoid dividing by 0\n            name=\'sentence_embeddings\'\n        )\n\n        outputs = sentence_embeddings\n\n        return outputs\n'"
tensorlayer/layers/extend.py,6,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'ExpandDims\',\n    \'Tile\',\n]\n\n\nclass ExpandDims(Layer):\n    """"""\n    The :class:`ExpandDims` class inserts a dimension of 1 into a tensor\'s shape,\n    see `tf.expand_dims() <https://www.tensorflow.org/api_docs/python/tf/expand_dims>`__ .\n\n    Parameters\n    ----------\n    axis : int\n        The dimension index at which to expand the shape of input.\n    name : str\n        A unique layer name. If None, a unique name will be automatically assigned.\n\n    Examples\n    --------\n    >>> x = tl.layers.Input([10, 3], name=\'in\')\n    >>> y = tl.layers.ExpandDims(axis=-1)(x)\n    [10, 3, 1]\n    """"""\n\n    def __init__(\n        self,\n        axis,\n        name=None  # \'expand_dims\',\n    ):\n        super(ExpandDims, self).__init__(name)\n        self.axis = axis\n\n        self.build((None, ))\n        self._built = True\n\n        logging.info(""ExpandDims  %s: axis: %d"" % (self.name, self.axis))\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'axis={axis},\'\n        s += \'name={name}\'\n        s += "")""\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        outputs = tf.expand_dims(inputs, axis=self.axis, name=self.name)\n        return outputs\n\n\nclass Tile(Layer):\n    """"""\n    The :class:`Tile` class constructs a tensor by tiling a given tensor,\n    see `tf.tile() <https://www.tensorflow.org/api_docs/python/tf/tile>`__ .\n\n    Parameters\n    ----------\n    multiples: tensor\n        Must be one of the following types: int32, int64.\n        1-D Length must be the same as the number of dimensions in input.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    >>> x = tl.layers.Input([10, 3], name=\'in\')\n    >>> y = tl.layers.Tile(multiples=[2, 3])(x)\n    [20, 9]\n    """"""\n\n    def __init__(self, multiples=None, name=None):  #\'tile\'):\n\n        super(Tile, self).__init__(name)\n        self.multiples = multiples\n\n        self.build((None, ))\n        self._built = True\n\n        logging.info(""Tile  %s: multiples: %s"" % (self.name, self.multiples))\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'multiples={multiples},\'\n        s += \'name={name}\'\n        s += "")""\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        outputs = tf.tile(inputs, multiples=self.multiples, name=self.name)\n        return outputs\n'"
tensorlayer/layers/image_resampling.py,6,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'UpSampling2d\',\n    \'DownSampling2d\',\n]\n\n\nclass UpSampling2d(Layer):\n    """"""The :class:`UpSampling2d` class is a up-sampling 2D layer.\n\n    See `tf.image.resize_images <https://www.tensorflow.org/api_docs/python/tf/image/resize_images>`__.\n\n    Parameters\n    ----------\n    scale : int/float or tuple of int/float\n        (height, width) scale factor.\n    method : str\n        The resize method selected through the given string. Default \'bilinear\'.\n            - \'bilinear\', Bilinear interpolation.\n            - \'nearest\', Nearest neighbor interpolation.\n            - \'bicubic\', Bicubic interpolation.\n            - \'area\', Area interpolation.\n    antialias : boolean\n        Whether to use an anti-aliasing filter when downsampling an image.\n    data_format : str\n        channels_last \'channel_last\' (default) or channels_first.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> ni = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> ni = tl.layers.UpSampling2d(scale=(2, 2))(ni)\n    >>> output shape : [None, 100, 100, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        scale,\n        method=\'bilinear\',\n        antialias=False,\n        data_format=\'channel_last\',\n        name=None,\n    ):\n        super(UpSampling2d, self).__init__(name)\n        self.method = method\n        self.antialias = antialias\n        self.data_format = data_format\n\n        logging.info(\n            ""UpSampling2d %s: scale: %s method: %s antialias: %s"" % (self.name, scale, self.method, self.antialias)\n        )\n\n        self.build(None)\n        self._built = True\n\n        if isinstance(scale, (list, tuple)) and len(scale) != 2:\n            raise ValueError(""scale must be int or tuple/list of length 2"")\n\n        self.scale = (scale, scale) if isinstance(scale, int) else scale\n\n    def __repr__(self):\n        s = \'{classname}(scale={scale}, method={method}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, scale=self.scale, method=self.method, name=self.name)\n\n    def build(self, inputs_shape):\n        if self.data_format != \'channel_last\':\n            raise Exception(""UpSampling2d tf.image.resize_images only support channel_last"")\n\n    def forward(self, inputs):\n        """"""\n\n        Parameters\n        ------------\n        inputs : :class:`Tensor`\n            Inputs tensors with 4-D Tensor of the shape (batch, height, width, channels)\n        """"""\n        output_size = [int(inputs.shape[1] * self.scale[0]), int(inputs.shape[2] * self.scale[1])]\n        outputs = tf.image.resize(inputs, size=output_size, method=self.method, antialias=self.antialias)\n        return outputs\n\n\nclass DownSampling2d(Layer):\n    """"""The :class:`DownSampling2d` class is down-sampling 2D layer.\n\n    See `tf.image.resize_images <https://www.tensorflow.org/versions/master/api_docs/python/image/resizing#resize_images>`__.\n\n    Parameters\n    ----------\n    scale : int/float or tuple of int/float\n        (height, width) scale factor.\n    method : str\n        The resize method selected through the given string. Default \'bilinear\'.\n            - \'bilinear\', Bilinear interpolation.\n            - \'nearest\', Nearest neighbor interpolation.\n            - \'bicubic\', Bicubic interpolation.\n            - \'area\', Area interpolation.\n    antialias : boolean\n        Whether to use an anti-aliasing filter when downsampling an image.\n    data_format : str\n        channels_last \'channel_last\' (default) or channels_first.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> ni = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> ni = tl.layers.DownSampling2d(scale=(2, 2))(ni)\n    >>> output shape : [None, 25, 25, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        scale,\n        method=\'bilinear\',\n        antialias=False,\n        data_format=\'channel_last\',\n        name=None,\n    ):\n        super(DownSampling2d, self).__init__(name)\n        self.method = method\n        self.antialias = antialias\n        self.data_format = data_format\n\n        logging.info(\n            ""DownSampling2d %s: scale: %s method: %s antialias: %s"" % (self.name, scale, self.method, self.antialias)\n        )\n\n        self.build(None)\n        self._built = True\n\n        if isinstance(scale, (list, tuple)) and len(scale) != 2:\n            raise ValueError(""scale must be int or tuple/list of length 2"")\n\n        self.scale = (scale, scale) if isinstance(scale, int) else scale\n\n    def __repr__(self):\n        s = \'{classname}(scale={scale}, method={method}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, scale=self.scale, method=self.method, name=self.name)\n\n    def build(self, inputs_shape):\n        if self.data_format != \'channel_last\':\n            raise Exception(""DownSampling2d tf.image.resize_images only support channel_last"")\n\n    def forward(self, inputs):\n        """"""\n\n        Parameters\n        ------------\n        inputs : :class:`Tensor`\n            Inputs tensors with 4-D Tensor of the shape (batch, height, width, channels)\n        """"""\n        output_size = [int(inputs.shape[1] * 1.0 / self.scale[0]), int(inputs.shape[2] * 1.0 / self.scale[1])]\n        outputs = tf.image.resize(inputs, size=output_size, method=self.method, antialias=self.antialias)\n        return outputs\n'"
tensorlayer/layers/inputs.py,4,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.layers.core import Layer, LayerNode\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\'Input\', \'_InputLayer\']\n\n\nclass _InputLayer(Layer):\n    """"""\n    The :class:`Input` class is the starting layer of a neural network.\n\n    Parameters\n    ----------\n    shape : tuple (int)\n        Including batch size.\n    dtype: dtype\n        The type of input values. By default, tf.float32.\n    name : None or str\n        A unique layer name.\n\n    """"""\n\n    def __init__(self, shape, dtype=tf.float32, name=None):  #\'input\'):\n        # super(InputLayer, self).__init__(prev_layer=inputs, name=name)\n        super(_InputLayer, self).__init__(name)\n\n        if isinstance(dtype, str):\n            try:\n                dtype = eval(dtype)\n            except Exception as e:\n                raise RuntimeError(""%s is not a valid dtype for InputLayer."" % (dtype))\n        if not isinstance(dtype, tf.DType):\n            raise RuntimeError(""%s is not a valid dtype for InputLayer."" % (dtype))\n\n        logging.info(""Input  %s: %s"" % (self.name, str(shape)))\n        self.shape = shape  # shape is needed in __repr__\n\n        shape_without_none = [_ if _ is not None else 1 for _ in shape]\n        # self.outputs = self.forward(tl.initializers.random_normal()(shape_without_none))\n        outputs = self.forward(tl.initializers.ones()(shape_without_none, dtype=dtype))\n\n        self._built = True\n\n        self._add_node(outputs, outputs)\n\n    def __repr__(self):\n        s = \'Input(shape=%s\' % str(self.shape)\n        if self.name is not None:\n            s += (\', name=\\\'%s\\\'\' % self.name)\n        s += \')\'\n        return s\n\n    def __call__(self, inputs, *args, **kwargs):\n        return super(_InputLayer, self).__call__(inputs)\n\n    def build(self, inputs_shape):\n        pass\n\n    def forward(self, inputs):\n        return inputs\n\n\ndef Input(shape, dtype=tf.float32, name=None):\n    """"""\n    The :class:`Input` class is the starting layer of a neural network.\n\n    Parameters\n    ----------\n    shape : tuple (int)\n        Including batch size.\n    name : None or str\n        A unique layer name.\n\n    """"""\n    input_layer = _InputLayer(shape, dtype=dtype, name=name)\n    outputs = input_layer._nodes[0].out_tensors[0]\n    return outputs\n'"
tensorlayer/layers/lambda_layers.py,16,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.files import utils\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import TF_GRAPHKEYS_VARIABLES\n\n__all__ = [\n    \'Lambda\',\n    \'ElementwiseLambda\',\n]\n\n\nclass Lambda(Layer):\n    """"""A layer that takes a user-defined function using Lambda.\n    If the function has trainable weights, the weights should be provided.\n    Remember to make sure the weights provided when the layer is constructed are SAME as\n    the weights used when the layer is forwarded.\n    For multiple inputs see :class:`ElementwiseLambda`.\n\n    Parameters\n    ----------\n    fn : function\n        The function that applies to the inputs (e.g. tensor from the previous layer).\n    fn_weights : list\n        The trainable weights for the function if any. Optional.\n    fn_args : dict\n        The arguments for the function if any. Optional.\n    name : str or None\n        A unique layer name.\n\n    Examples\n    ---------\n    Non-parametric and non-args case:\n    This case is supported in the Model.save() / Model.load() to save / load the whole model architecture and weights(optional).\n\n    >>> x = tl.layers.Input([8, 3], name=\'input\')\n    >>> y = tl.layers.Lambda(lambda x: 2*x, name=\'lambda\')(x)\n\n\n    Non-parametric and with args case:\n    This case is supported in the Model.save() / Model.load() to save / load the whole model architecture and weights(optional).\n\n    >>> def customize_func(x, foo=42): # x is the inputs, foo is an argument\n    >>>     return foo * x\n    >>> x = tl.layers.Input([8, 3], name=\'input\')\n    >>> lambdalayer = tl.layers.Lambda(customize_func, fn_args={\'foo\': 2}, name=\'lambda\')(x)\n\n\n    Any function with outside variables:\n    This case has not been supported in Model.save() / Model.load() yet.\n    Please avoid using Model.save() / Model.load() to save / load models that contain such Lambda layer. Instead, you may use Model.save_weights() / Model.load_weights() to save / load model weights.\n    Note: In this case, fn_weights should be a list, and then the trainable weights in this Lambda layer can be added into the weights of the whole model.\n\n    >>> a = tf.Variable(1.0)\n    >>> def func(x):\n    >>>     return x + a\n    >>> x = tl.layers.Input([8, 3], name=\'input\')\n    >>> y = tl.layers.Lambda(func, fn_weights=[a], name=\'lambda\')(x)\n\n\n    Parametric case, merge other wrappers into TensorLayer:\n    This case is supported in the Model.save() / Model.load() to save / load the whole model architecture and weights(optional).\n\n    >>> layers = [\n    >>>     tf.keras.layers.Dense(10, activation=tf.nn.relu),\n    >>>     tf.keras.layers.Dense(5, activation=tf.nn.sigmoid),\n    >>>     tf.keras.layers.Dense(1, activation=tf.identity)\n    >>> ]\n    >>> perceptron = tf.keras.Sequential(layers)\n    >>> # in order to compile keras model and get trainable_variables of the keras model\n    >>> _ = perceptron(np.random.random([100, 5]).astype(np.float32))\n    >>>\n    >>> class CustomizeModel(tl.models.Model):\n    >>>     def __init__(self):\n    >>>         super(CustomizeModel, self).__init__()\n    >>>         self.dense = tl.layers.Dense(in_channels=1, n_units=5)\n    >>>         self.lambdalayer = tl.layers.Lambda(perceptron, perceptron.trainable_variables)\n    >>>\n    >>>     def forward(self, x):\n    >>>         z = self.dense(x)\n    >>>         z = self.lambdalayer(z)\n    >>>         return z\n    >>>\n    >>> optimizer = tf.optimizers.Adam(learning_rate=0.1)\n    >>> model = CustomizeModel()\n    >>> model.train()\n    >>>\n    >>> for epoch in range(50):\n    >>>     with tf.GradientTape() as tape:\n    >>>         pred_y = model(data_x)\n    >>>         loss = tl.cost.mean_squared_error(pred_y, data_y)\n    >>>\n    >>>     gradients = tape.gradient(loss, model.trainable_weights)\n    >>>     optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n\n    """"""\n\n    def __init__(\n        self,\n        fn,\n        fn_weights=None,\n        fn_args=None,\n        name=None,\n    ):\n\n        super(Lambda, self).__init__(name=name)\n        self.fn = fn\n        self._trainable_weights = fn_weights if fn_weights is not None else []\n        self.fn_args = fn_args if fn_args is not None else {}\n\n        try:\n            fn_name = repr(self.fn)\n        except:\n            fn_name = \'name not available\'\n        logging.info(""Lambda  %s: func: %s, len_weights: %s"" % (self.name, fn_name, len(self._trainable_weights)))\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'fn={fn_name},\'\n        s += \'len_weights={len_weights},\'\n        s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        try:\n            fn_name = repr(self.fn)\n        except:\n            fn_name = \'name not available\'\n        return s.format(\n            classname=self.__class__.__name__, fn_name=fn_name, len_weights=len(self._trainable_weights),\n            **self.__dict__\n        )\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs, **kwargs):\n\n        if len(kwargs) == 0:\n            outputs = self.fn(inputs, **self.fn_args)\n        else:\n            outputs = self.fn(inputs, **kwargs)\n\n        return outputs\n\n    def get_args(self):\n        init_args = {}\n        if isinstance(self.fn, tf.keras.layers.Layer) or isinstance(self.fn, tf.keras.Model):\n            init_args.update({""layer_type"": ""keraslayer""})\n            init_args[""fn""] = utils.save_keras_model(self.fn)\n            init_args[""fn_weights""] = None\n            if len(self._nodes) == 0:\n                init_args[""keras_input_shape""] = []\n            else:\n                init_args[""keras_input_shape""] = self._nodes[0].in_tensors[0].get_shape().as_list()\n        else:\n            init_args = {""layer_type"": ""normal""}\n        return init_args\n\n\nclass ElementwiseLambda(Layer):\n    """"""A layer that use a custom function to combine multiple :class:`Layer` inputs.\n    If the function has trainable weights, the weights should be provided.\n    Remember to make sure the weights provided when the layer is constructed are SAME as\n    the weights used when the layer is forwarded.\n\n    Parameters\n    ----------\n    fn : function\n        The function that applies to the inputs (e.g. tensor from the previous layer).\n    fn_weights : list\n        The trainable weights for the function if any. Optional.\n    fn_args : dict\n        The arguments for the function if any. Optional.\n    name : str or None\n        A unique layer name.\n\n    Examples\n    --------\n\n    Non-parametric and with args case\n    This case is supported in the Model.save() / Model.load() to save / load the whole model architecture and weights(optional).\n\n    >>> # z = mean + noise * tf.exp(std * 0.5) + foo\n    >>> def func(noise, mean, std, foo=42):\n    >>>     return mean + noise * tf.exp(std * 0.5) + foo\n    >>> noise = tl.layers.Input([100, 1])\n    >>> mean = tl.layers.Input([100, 1])\n    >>> std = tl.layers.Input([100, 1])\n    >>> out = tl.layers.ElementwiseLambda(fn=func, fn_args={\'foo\': 84}, name=\'elementwiselambda\')([noise, mean, std])\n\n\n    Non-parametric and non-args case\n    This case is supported in the Model.save() / Model.load() to save / load the whole model architecture and weights(optional).\n\n    >>> # z = mean + noise * tf.exp(std * 0.5)\n    >>> noise = tl.layers.Input([100, 1])\n    >>> mean = tl.layers.Input([100, 1])\n    >>> std = tl.layers.Input([100, 1])\n    >>> out = tl.layers.ElementwiseLambda(fn=lambda x, y, z: x + y * tf.exp(z * 0.5), name=\'elementwiselambda\')([noise, mean, std])\n\n\n    Any function with outside variables\n    This case has not been supported in Model.save() / Model.load() yet.\n    Please avoid using Model.save() / Model.load() to save / load models that contain such ElementwiseLambda layer. Instead, you may use Model.save_weights() / Model.load_weights() to save / load model weights.\n    Note: In this case, fn_weights should be a list, and then the trainable weights in this ElementwiseLambda layer can be added into the weights of the whole model.\n\n    >>> # z = mean + noise * tf.exp(std * 0.5) + vara\n    >>> vara = [tf.Variable(1.0)]\n    >>> def func(noise, mean, std):\n    >>>     return mean + noise * tf.exp(std * 0.5) + vara\n    >>> noise = tl.layers.Input([100, 1])\n    >>> mean = tl.layers.Input([100, 1])\n    >>> std = tl.layers.Input([100, 1])\n    >>> out = tl.layers.ElementwiseLambda(fn=func, fn_weights=vara, name=\'elementwiselambda\')([noise, mean, std])\n\n    """"""\n\n    def __init__(\n        self,\n        fn,\n        fn_weights=None,\n        fn_args=None,\n        name=None,  #\'elementwiselambda\',\n    ):\n\n        super(ElementwiseLambda, self).__init__(name=name)\n        self.fn = fn\n        self._trainable_weights = fn_weights if fn_weights is not None else []\n        self.fn_args = fn_args if fn_args is not None else {}\n\n        try:\n            fn_name = repr(self.fn)\n        except:\n            fn_name = \'name not available\'\n        logging.info(\n            ""ElementwiseLambda  %s: func: %s, len_weights: %s"" % (self.name, fn_name, len(self._trainable_weights))\n        )\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'fn={fn_name},\'\n        s += \'len_weights={len_weights},\'\n        s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        try:\n            fn_name = repr(self.fn)\n        except:\n            fn_name = \'name not available\'\n        return s.format(\n            classname=self.__class__.__name__, fn_name=fn_name, len_weights=len(self._trainable_weights),\n            **self.__dict__\n        )\n\n    def build(self, inputs_shape=None):\n        # do nothing\n        # the weights of the function are provided when the Lambda layer is constructed\n        pass\n\n    # @tf.function\n    def forward(self, inputs, **kwargs):\n\n        if not isinstance(inputs, list):\n            raise TypeError(\n                ""The inputs should be a list of values which corresponds with the customised lambda function.""\n            )\n\n        if len(kwargs) == 0:\n            outputs = self.fn(*inputs, **self.fn_args)\n        else:\n            outputs = self.fn(*inputs, **kwargs)\n\n        return outputs\n'"
tensorlayer/layers/merge.py,10,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'Concat\',\n    \'Elementwise\',\n]\n\n\nclass Concat(Layer):\n    """"""A layer that concats multiple tensors according to given axis.\n\n    Parameters\n    ----------\n    concat_dim : int\n        The dimension to concatenate.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ----------\n    >>> class CustomModel(tl.models.Model):\n    >>>     def __init__(self):\n    >>>         super(CustomModel, self).__init__(name=""custom"")\n    >>>         self.dense1 = tl.layers.Dense(in_channels=20, n_units=10, act=tf.nn.relu, name=\'relu1_1\')\n    >>>         self.dense2 = tl.layers.Dense(in_channels=20, n_units=10, act=tf.nn.relu, name=\'relu2_1\')\n    >>>         self.concat = tl.layers.Concat(concat_dim=1, name=\'concat_layer\')\n\n    >>>     def forward(self, inputs):\n    >>>         d1 = self.dense1(inputs)\n    >>>         d2 = self.dense2(inputs)\n    >>>         outputs = self.concat([d1, d2])\n    >>>         return outputs\n\n    """"""\n\n    def __init__(\n        self,\n        concat_dim=-1,\n        name=None,  #\'concat\',\n    ):\n\n        super(Concat, self).__init__(name)\n        self.concat_dim = concat_dim\n\n        self.build(None)\n        self._built = True\n\n        logging.info(""Concat %s: concat_dim: %d"" % (self.name, concat_dim))\n\n    def __repr__(self):\n        s = (\'{classname}(concat_dim={concat_dim})\')\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        """"""\n\n        prev_layer : list of :class:`Layer`\n            List of layers to concatenate.\n        """"""\n        outputs = tf.concat(inputs, self.concat_dim, name=self.name)\n\n        return outputs\n\n\nclass Elementwise(Layer):\n    """"""A layer that combines multiple :class:`Layer` that have the same output shapes\n    according to an element-wise operation.\n    If the element-wise operation is complicated, please consider to use :class:`ElementwiseLambda`.\n\n    Parameters\n    ----------\n    combine_fn : a TensorFlow element-wise combine function\n        e.g. AND is ``tf.minimum`` ;  OR is ``tf.maximum`` ; ADD is ``tf.add`` ; MUL is ``tf.multiply`` and so on.\n        See `TensorFlow Math API <https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#math>`__ .\n        If the combine function is more complicated, please consider to use :class:`ElementwiseLambda`.\n    act : activation function\n        The activation function of this layer.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    >>> class CustomModel(tl.models.Model):\n    >>>     def __init__(self):\n    >>>         super(CustomModel, self).__init__(name=""custom"")\n    >>>         self.dense1 = tl.layers.Dense(in_channels=20, n_units=10, act=tf.nn.relu, name=\'relu1_1\')\n    >>>         self.dense2 = tl.layers.Dense(in_channels=20, n_units=10, act=tf.nn.relu, name=\'relu2_1\')\n    >>>         self.element = tl.layers.Elementwise(combine_fn=tf.minimum, name=\'minimum\', act=tf.identity)\n\n    >>>     def forward(self, inputs):\n    >>>         d1 = self.dense1(inputs)\n    >>>         d2 = self.dense2(inputs)\n    >>>         outputs = self.element([d1, d2])\n    >>>         return outputs\n    """"""\n\n    def __init__(\n        self,\n        combine_fn=tf.minimum,\n        act=None,\n        name=None,  #\'elementwise\',\n    ):\n\n        super(Elementwise, self).__init__(name, act=act)\n        self.combine_fn = combine_fn\n\n        self.build(None)\n        self._built = True\n\n        logging.info(\n            ""Elementwise %s: fn: %s act: %s"" %\n            (self.name, combine_fn.__name__, (\'No Activation\' if self.act is None else self.act.__name__))\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(combine_fn={combine_fn}, \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        outputs = inputs[0]\n        for input in inputs[1:]:\n            outputs = self.combine_fn(outputs, input, name=self.name)\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/noise.py,3,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'GaussianNoise\',\n]\n\n\nclass GaussianNoise(Layer):\n    """"""\n    The :class:`GaussianNoise` class is noise layer that adding noise with\n    gaussian distribution to the activation.\n\n    Parameters\n    ------------\n    mean : float\n        The mean. Default is 0.0.\n    stddev : float\n        The standard deviation. Default is 1.0.\n    is_always : boolean\n        Is True, add noise for train and eval mode. If False, skip this layer in eval mode.\n    seed : int or None\n        The seed for random noise.\n    name : str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([64, 200], name=\'input\')\n    >>> net = tl.layers.Dense(n_units=100, act=tf.nn.relu, name=\'dense\')(net)\n    >>> gaussianlayer = tl.layers.GaussianNoise(name=\'gaussian\')(net)\n    >>> print(gaussianlayer)\n    >>> output shape : (64, 100)\n\n    """"""\n\n    def __init__(\n        self,\n        mean=0.0,\n        stddev=1.0,\n        is_always=True,\n        seed=None,\n        name=None,  # \'gaussian_noise\',\n    ):\n        super().__init__(name)\n        self.mean = mean\n        self.stddev = stddev\n        self.seed = seed\n        self.is_always = is_always\n\n        self.build()\n        self._built = True\n\n        logging.info(""GaussianNoise %s: mean: %f stddev: %f"" % (self.name, self.mean, self.stddev))\n\n    def __repr__(self):\n        s = \'{classname}(mean={mean}, stddev={stddev}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs=None):\n        pass\n\n    def forward(self, inputs):\n        if (self.is_train or self.is_always) is False:\n            return inputs\n        else:\n            # noise = np.random.normal(0.0 , sigma , tf.to_int64(self.inputs).get_shape())\n            noise = tf.random.normal(shape=inputs.get_shape(), mean=self.mean, stddev=self.stddev, seed=self.seed)\n            outputs = inputs + noise\n        return outputs\n'"
tensorlayer/layers/normalization.py,59,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.training import moving_averages\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'LocalResponseNorm\',\n    \'BatchNorm\',  # FIXME: wthether to keep BatchNorm\n    \'BatchNorm1d\',\n    \'BatchNorm2d\',\n    \'BatchNorm3d\',\n    \'InstanceNorm\',\n    \'InstanceNorm1d\',\n    \'InstanceNorm2d\',\n    \'InstanceNorm3d\',\n    \'LayerNorm\',\n    \'GroupNorm\',\n    \'SwitchNorm\',\n]\n\n\nclass LocalResponseNorm(Layer):\n    """"""The :class:`LocalResponseNorm` layer is for Local Response Normalization.\n    See ``tf.nn.local_response_normalization`` or ``tf.nn.lrn`` for new TF version.\n    The 4-D input tensor is a 3-D array of 1-D vectors (along the last dimension), and each vector is normalized independently.\n    Within a given vector, each component is divided by the weighted square-sum of inputs within depth_radius.\n\n    Parameters\n    -----------\n    depth_radius : int\n        Depth radius. 0-D. Half-width of the 1-D normalization window.\n    bias : float\n        An offset which is usually positive and shall avoid dividing by 0.\n    alpha : float\n        A scale factor which is usually positive.\n    beta : float\n        An exponent.\n    name : None or str\n        A unique layer name.\n\n    """"""\n\n    def __init__(\n        self,\n        depth_radius=None,\n        bias=None,\n        alpha=None,\n        beta=None,\n        name=None,  #\'lrn\',\n    ):\n        # super(LocalResponseNorm, self).__init__(prev_layer=prev_layer, name=name)\n        super().__init__(name)\n        self.depth_radius = depth_radius\n        self.bias = bias\n        self.alpha = alpha\n        self.beta = beta\n\n        logging.info(\n            ""LocalResponseNorm %s: depth_radius: %s, bias: %s, alpha: %s, beta: %s"" %\n            (self.name, str(depth_radius), str(bias), str(alpha), str(beta))\n        )\n\n    def build(self, inputs):\n        pass\n\n    def forward(self, inputs):\n        """"""\n        prev_layer : :class:`Layer`\n            The previous layer with a 4D output shape.\n        """"""\n        outputs = tf.nn.lrn(inputs, depth_radius=self.depth_radius, bias=self.bias, alpha=self.alpha, beta=self.beta)\n        return outputs\n\n\ndef _to_channel_first_bias(b):\n    """"""Reshape [c] to [c, 1, 1].""""""\n    channel_size = int(b.shape[0])\n    new_shape = (channel_size, 1, 1)\n    # new_shape = [-1, 1, 1]  # doesn\'t work with tensorRT\n    return tf.reshape(b, new_shape)\n\n\ndef _bias_scale(x, b, data_format):\n    """"""The multiplication counter part of tf.nn.bias_add.""""""\n    if data_format == \'NHWC\':\n        return x * b\n    elif data_format == \'NCHW\':\n        return x * _to_channel_first_bias(b)\n    else:\n        raise ValueError(\'invalid data_format: %s\' % data_format)\n\n\ndef _bias_add(x, b, data_format):\n    """"""Alternative implementation of tf.nn.bias_add which is compatiable with tensorRT.""""""\n    if data_format == \'NHWC\':\n        return tf.add(x, b)\n    elif data_format == \'NCHW\':\n        return tf.add(x, _to_channel_first_bias(b))\n    else:\n        raise ValueError(\'invalid data_format: %s\' % data_format)\n\n\ndef _compute_shape(tensors):\n    if isinstance(tensors, list):\n        shape_mem = [t.get_shape().as_list() for t in tensors]\n    else:\n        shape_mem = tensors.get_shape().as_list()\n    return shape_mem\n\n\ndef batch_normalization(x, mean, variance, offset, scale, variance_epsilon, data_format, name=None):\n    """"""Data Format aware version of tf.nn.batch_normalization.""""""\n    if data_format == \'channels_last\':\n        mean = tf.reshape(mean, [1] * (len(x.shape) - 1) + [-1])\n        variance = tf.reshape(variance, [1] * (len(x.shape) - 1) + [-1])\n        offset = tf.reshape(offset, [1] * (len(x.shape) - 1) + [-1])\n        scale = tf.reshape(scale, [1] * (len(x.shape) - 1) + [-1])\n    elif data_format == \'channels_first\':\n        mean = tf.reshape(mean, [1] + [-1] + [1] * (len(x.shape) - 2))\n        variance = tf.reshape(variance, [1] + [-1] + [1] * (len(x.shape) - 2))\n        offset = tf.reshape(offset, [1] + [-1] + [1] * (len(x.shape) - 2))\n        scale = tf.reshape(scale, [1] + [-1] + [1] * (len(x.shape) - 2))\n    else:\n        raise ValueError(\'invalid data_format: %s\' % data_format)\n\n    with ops.name_scope(name, \'batchnorm\', [x, mean, variance, scale, offset]):\n        inv = math_ops.rsqrt(variance + variance_epsilon)\n        if scale is not None:\n            inv *= scale\n\n        a = math_ops.cast(inv, x.dtype)\n        b = math_ops.cast(offset - mean * inv if offset is not None else -mean * inv, x.dtype)\n\n        # Return a * x + b with customized data_format.\n        # Currently TF doesn\'t have bias_scale, and tensorRT has bug in converting tf.nn.bias_add\n        # So we reimplemted them to allow make the model work with tensorRT.\n        # See https://github.com/tensorlayer/openpose-plus/issues/75 for more details.\n        df = {\'channels_first\': \'NCHW\', \'channels_last\': \'NHWC\'}\n        return _bias_add(_bias_scale(x, a, df[data_format]), b, df[data_format])\n\n\nclass BatchNorm(Layer):\n    """"""\n    The :class:`BatchNorm` is a batch normalization layer for both fully-connected and convolution outputs.\n    See ``tf.nn.batch_normalization`` and ``tf.nn.moments``.\n\n    Parameters\n    ----------\n    decay : float\n        A decay factor for `ExponentialMovingAverage`.\n        Suggest to use a large value for large dataset.\n    epsilon : float\n        Eplison.\n    act : activation function\n        The activation function of this layer.\n    is_train : boolean\n        Is being used for training or inference.\n    beta_init : initializer or None\n        The initializer for initializing beta, if None, skip beta.\n        Usually you should not skip beta unless you know what happened.\n    gamma_init : initializer or None\n        The initializer for initializing gamma, if None, skip gamma.\n        When the batch normalization layer is use instead of \'biases\', or the next layer is linear, this can be\n        disabled since the scaling can be done by the next layer. see `Inception-ResNet-v2 <https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py>`__\n    moving_mean_init : initializer or None\n        The initializer for initializing moving mean, if None, skip moving mean.\n    moving_var_init : initializer or None\n        The initializer for initializing moving var, if None, skip moving var.\n    num_features: int\n        Number of features for input tensor. Useful to build layer if using BatchNorm1d, BatchNorm2d or BatchNorm3d,\n        but should be left as None if using BatchNorm. Default None.\n    data_format : str\n        channels_last \'channel_last\' (default) or channels_first.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.BatchNorm()(net)\n\n    Notes\n    -----\n    The :class:`BatchNorm` is universally suitable for 3D/4D/5D input in static model, but should not be used\n    in dynamic model where layer is built upon class initialization. So the argument \'num_features\' should only be used\n    for subclasses :class:`BatchNorm1d`, :class:`BatchNorm2d` and :class:`BatchNorm3d`. All the three subclasses are\n    suitable under all kinds of conditions.\n\n    References\n    ----------\n    - `Source <https://github.com/ry/tensorflow-resnet/blob/master/resnet.py>`__\n    - `stackoverflow <http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow>`__\n\n    """"""\n\n    def __init__(\n        self,\n        decay=0.9,\n        epsilon=0.00001,\n        act=None,\n        is_train=False,\n        beta_init=tl.initializers.zeros(),\n        gamma_init=tl.initializers.random_normal(mean=1.0, stddev=0.002),\n        moving_mean_init=tl.initializers.zeros(),\n        moving_var_init=tl.initializers.zeros(),\n        num_features=None,\n        data_format=\'channels_last\',\n        name=None,\n    ):\n        super(BatchNorm, self).__init__(name=name, act=act)\n        self.decay = decay\n        self.epsilon = epsilon\n        self.data_format = data_format\n        self.beta_init = beta_init\n        self.gamma_init = gamma_init\n        self.moving_mean_init = moving_mean_init\n        self.moving_var_init = moving_var_init\n        self.num_features = num_features\n\n        self.channel_axis = -1 if data_format == \'channels_last\' else 1\n        self.axes = None\n\n        if num_features is not None:\n            self.build(None)\n            self._built = True\n\n        if self.decay < 0.0 or 1.0 < self.decay:\n            raise ValueError(""decay should be between 0 to 1"")\n\n        logging.info(\n            ""BatchNorm %s: decay: %f epsilon: %f act: %s is_train: %s"" %\n            (self.name, decay, epsilon, self.act.__name__ if self.act is not None else \'No Activation\', is_train)\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(num_features={num_features}, decay={decay}\' \', epsilon={epsilon}\')\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=""{name}""\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def _get_param_shape(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            axis = -1\n        elif self.data_format == \'channels_first\':\n            axis = 1\n        else:\n            raise ValueError(\'data_format should be either %s or %s\' % (\'channels_last\', \'channels_first\'))\n\n        channels = inputs_shape[axis]\n        params_shape = [channels]\n\n        return params_shape\n\n    def _check_input_shape(self, inputs):\n        inputs_shape = _compute_shape(inputs)\n        if len(inputs_shape) <= 1:\n            raise ValueError(\'expected input at least 2D, but got {}D input\'.format(inputs.ndim))\n\n    def build(self, inputs_shape):\n        params_shape = [self.num_features] if self.num_features is not None else self._get_param_shape(inputs_shape)\n\n        self.beta, self.gamma = None, None\n        if self.beta_init:\n            self.beta = self._get_weights(""beta"", shape=params_shape, init=self.beta_init)\n\n        if self.gamma_init:\n            self.gamma = self._get_weights(""gamma"", shape=params_shape, init=self.gamma_init)\n\n        self.moving_mean = self._get_weights(\n            ""moving_mean"", shape=params_shape, init=self.moving_mean_init, trainable=False\n        )\n        self.moving_var = self._get_weights(\n            ""moving_var"", shape=params_shape, init=self.moving_var_init, trainable=False\n        )\n\n    def forward(self, inputs):\n        self._check_input_shape(inputs)\n\n        if self.axes is None:\n            self.axes = [i for i in range(len(inputs.shape)) if i != self.channel_axis]\n\n        if self.is_train:\n            # update moving_mean and moving_var\n            mean, var = tf.nn.moments(inputs, self.axes, keepdims=False)\n            self.moving_mean = moving_averages.assign_moving_average(\n                self.moving_mean, mean, self.decay, zero_debias=False\n            )\n            self.moving_var = moving_averages.assign_moving_average(self.moving_var, var, self.decay, zero_debias=False)\n            outputs = batch_normalization(inputs, mean, var, self.beta, self.gamma, self.epsilon, self.data_format)\n        else:\n            outputs = batch_normalization(\n                inputs, self.moving_mean, self.moving_var, self.beta, self.gamma, self.epsilon, self.data_format\n            )\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass BatchNorm1d(BatchNorm):\n    """"""The :class:`BatchNorm1d` applies Batch Normalization over 2D/3D input (a mini-batch of 1D\n    inputs (optional) with additional channel dimension), of shape (N, C) or (N, L, C) or (N, C, L).\n    See more details in :class:`BatchNorm`.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> # in static model, no need to specify num_features\n    >>> net = tl.layers.Input([None, 50, 32], name=\'input\')\n    >>> net = tl.layers.BatchNorm1d()(net)\n    >>> # in dynamic model, build by specifying num_features\n    >>> conv = tl.layers.Conv1d(32, 5, 1, in_channels=3)\n    >>> bn = tl.layers.BatchNorm1d(num_features=32)\n\n    """"""\n\n    def _check_input_shape(self, inputs):\n        inputs_shape = _compute_shape(inputs)\n        if len(inputs_shape) != 2 and len(inputs_shape) != 3:\n            raise ValueError(\'expected input to be 2D or 3D, but got {}D input\'.format(inputs.ndim))\n\n\nclass BatchNorm2d(BatchNorm):\n    """"""The :class:`BatchNorm2d` applies Batch Normalization over 4D input (a mini-batch of 2D\n    inputs with additional channel dimension) of shape (N, H, W, C) or (N, C, H, W).\n    See more details in :class:`BatchNorm`.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> # in static model, no need to specify num_features\n    >>> net = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.BatchNorm2d()(net)\n    >>> # in dynamic model, build by specifying num_features\n    >>> conv = tl.layers.Conv2d(32, (5, 5), (1, 1), in_channels=3)\n    >>> bn = tl.layers.BatchNorm2d(num_features=32)\n\n    """"""\n\n    def _check_input_shape(self, inputs):\n        inputs_shape = _compute_shape(inputs)\n        if len(inputs_shape) != 4:\n            raise ValueError(\'expected input to be 4D, but got {}D input\'.format(inputs.ndim))\n\n\nclass BatchNorm3d(BatchNorm):\n    """"""The :class:`BatchNorm3d` applies Batch Normalization over 5D input (a mini-batch of 3D\n    inputs with additional channel dimension) with shape (N, D, H, W, C) or (N, C, D, H, W).\n    See more details in :class:`BatchNorm`.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> # in static model, no need to specify num_features\n    >>> net = tl.layers.Input([None, 50, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.BatchNorm3d()(net)\n    >>> # in dynamic model, build by specifying num_features\n    >>> conv = tl.layers.Conv3d(32, (5, 5, 5), (1, 1), in_channels=3)\n    >>> bn = tl.layers.BatchNorm3d(num_features=32)\n\n    """"""\n\n    def _check_input_shape(self, inputs):\n        inputs_shape = _compute_shape(inputs)\n        if len(inputs_shape) != 5:\n            raise ValueError(\'expected input to be 5D, but got {}D input\'.format(inputs.ndim))\n\n\nclass InstanceNorm(Layer):\n    """"""\n    The :class:`InstanceNorm` is an instance normalization layer for both fully-connected and convolution outputs.\n    See ``tf.nn.batch_normalization`` and  ``tf.nn.moments``.\n\n    Parameters\n    -----------\n    act : activation function.\n        The activation function of this layer.\n    epsilon : float\n        Eplison.\n    beta_init : initializer or None\n        The initializer for initializing beta, if None, skip beta.\n        Usually you should not skip beta unless you know what happened.\n    gamma_init : initializer or None\n        The initializer for initializing gamma, if None, skip gamma.\n        When the instance normalization layer is use instead of \'biases\', or the next layer is linear, this can be\n        disabled since the scaling can be done by the next layer. see `Inception-ResNet-v2 <https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py>`__\n    num_features: int\n        Number of features for input tensor. Useful to build layer if using InstanceNorm1d, InstanceNorm2d or InstanceNorm3d,\n        but should be left as None if using InstanceNorm. Default None.\n    data_format : str\n        channels_last \'channel_last\' (default) or channels_first.\n    name : None or str\n        A unique layer name.\n\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.InstanceNorm()(net)\n\n    Notes\n    -----\n    The :class:`InstanceNorm` is universally suitable for 3D/4D/5D input in static model, but should not be used\n    in dynamic model where layer is built upon class initialization. So the argument \'num_features\' should only be used\n    for subclasses :class:`InstanceNorm1d`, :class:`InstanceNorm2d` and :class:`InstanceNorm3d`. All the three subclasses are\n    suitable under all kinds of conditions.\n    """"""\n\n    def __init__(\n        self, act=None, epsilon=0.00001, beta_init=tl.initializers.zeros(),\n        gamma_init=tl.initializers.random_normal(mean=1.0, stddev=0.002), num_features=None,\n        data_format=\'channels_last\', name=None\n    ):\n        super(InstanceNorm, self).__init__(name=name, act=act)\n        self.epsilon = epsilon\n        self.beta_init = beta_init\n        self.gamma_init = gamma_init\n        self.num_features = num_features\n        self.data_format = data_format\n\n        if num_features is not None:\n            if not isinstance(self, InstanceNorm1d) and not isinstance(self, InstanceNorm2d) and not isinstance(\n                    self, InstanceNorm3d):\n                raise ValueError(\n                    ""Please use InstanceNorm1d or InstanceNorm2d or InstanceNorm3d instead of InstanceNorm ""\n                    ""if you want to specify \'num_features\'.""\n                )\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""InstanceNorm %s: epsilon: %f act: %s "" %\n            (self.name, epsilon, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = \'{classname}(num_features=num_features, epsilon={epsilon}\' + actstr\n        if self.name is not None:\n            s += \', name=""{name}""\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def _get_param_shape(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            axis = len(inputs_shape) - 1\n        elif self.data_format == \'channels_first\':\n            axis = 1\n        else:\n            raise ValueError(\'data_format should be either %s or %s\' % (\'channels_last\', \'channels_first\'))\n\n        channels = inputs_shape[axis]\n        params_shape = [1] * len(inputs_shape)\n        params_shape[axis] = channels\n\n        axes = [i for i in range(len(inputs_shape)) if i != 0 and i != axis]\n        return params_shape, axes\n\n    def build(self, inputs_shape):\n        params_shape, self.axes = self._get_param_shape(inputs_shape)\n\n        self.beta, self.gamma = None, None\n        if self.beta_init:\n            self.beta = self._get_weights(""beta"", shape=params_shape, init=self.beta_init)\n\n        if self.gamma_init:\n            self.gamma = self._get_weights(""gamma"", shape=params_shape, init=self.gamma_init)\n\n    def forward(self, inputs):\n        mean, var = tf.nn.moments(inputs, self.axes, keepdims=True)\n        outputs = batch_normalization(inputs, mean, var, self.beta, self.gamma, self.epsilon, self.data_format)\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass InstanceNorm1d(InstanceNorm):\n    """"""The :class:`InstanceNorm1d` applies Instance Normalization over 3D input (a mini-instance of 1D\n    inputs with additional channel dimension), of shape (N, L, C) or (N, C, L).\n    See more details in :class:`InstanceNorm`.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> # in static model, no need to specify num_features\n    >>> net = tl.layers.Input([None, 50, 32], name=\'input\')\n    >>> net = tl.layers.InstanceNorm1d()(net)\n    >>> # in dynamic model, build by specifying num_features\n    >>> conv = tl.layers.Conv1d(32, 5, 1, in_channels=3)\n    >>> bn = tl.layers.InstanceNorm1d(num_features=32)\n\n    """"""\n\n    def _get_param_shape(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            axis = 2\n        elif self.data_format == \'channels_first\':\n            axis = 1\n        else:\n            raise ValueError(\'data_format should be either %s or %s\' % (\'channels_last\', \'channels_first\'))\n\n        if self.num_features is None:\n            channels = inputs_shape[axis]\n        else:\n            channels = self.num_features\n        params_shape = [1] * 3\n        params_shape[axis] = channels\n\n        axes = [i for i in range(3) if i != 0 and i != axis]\n        return params_shape, axes\n\n\nclass InstanceNorm2d(InstanceNorm):\n    """"""The :class:`InstanceNorm2d` applies Instance Normalization over 4D input (a mini-instance of 2D\n    inputs with additional channel dimension) of shape (N, H, W, C) or (N, C, H, W).\n    See more details in :class:`InstanceNorm`.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> # in static model, no need to specify num_features\n    >>> net = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.InstanceNorm2d()(net)\n    >>> # in dynamic model, build by specifying num_features\n    >>> conv = tl.layers.Conv2d(32, (5, 5), (1, 1), in_channels=3)\n    >>> bn = tl.layers.InstanceNorm2d(num_features=32)\n\n    """"""\n\n    def _get_param_shape(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            axis = 3\n        elif self.data_format == \'channels_first\':\n            axis = 1\n        else:\n            raise ValueError(\'data_format should be either %s or %s\' % (\'channels_last\', \'channels_first\'))\n\n        if self.num_features is None:\n            channels = inputs_shape[axis]\n        else:\n            channels = self.num_features\n        params_shape = [1] * 4\n        params_shape[axis] = channels\n\n        axes = [i for i in range(4) if i != 0 and i != axis]\n        return params_shape, axes\n\n\nclass InstanceNorm3d(InstanceNorm):\n    """"""The :class:`InstanceNorm3d` applies Instance Normalization over 5D input (a mini-instance of 3D\n    inputs with additional channel dimension) with shape (N, D, H, W, C) or (N, C, D, H, W).\n    See more details in :class:`InstanceNorm`.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> # in static model, no need to specify num_features\n    >>> net = tl.layers.Input([None, 50, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.InstanceNorm3d()(net)\n    >>> # in dynamic model, build by specifying num_features\n    >>> conv = tl.layers.Conv3d(32, (5, 5, 5), (1, 1), in_channels=3)\n    >>> bn = tl.layers.InstanceNorm3d(num_features=32)\n\n    """"""\n\n    def _get_param_shape(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            axis = 4\n        elif self.data_format == \'channels_first\':\n            axis = 1\n        else:\n            raise ValueError(\'data_format should be either %s or %s\' % (\'channels_last\', \'channels_first\'))\n\n        if self.num_features is None:\n            channels = inputs_shape[axis]\n        else:\n            channels = self.num_features\n        params_shape = [1] * 5\n        params_shape[axis] = channels\n\n        axes = [i for i in range(5) if i != 0 and i != axis]\n        return params_shape, axes\n\n\n# FIXME : not sure about the correctness, need testing\nclass LayerNorm(Layer):\n    """"""\n    The :class:`LayerNorm` class is for layer normalization, see `tf.contrib.layers.layer_norm <https://www.tensorflow.org/api_docs/python/tf/contrib/layers/layer_norm>`__.\n\n    Parameters\n    ----------\n    prev_layer : :class:`Layer`\n        The previous layer.\n    act : activation function\n        The activation function of this layer.\n    others : _\n        `tf.contrib.layers.layer_norm <https://www.tensorflow.org/api_docs/python/tf/contrib/layers/layer_norm>`__.\n\n    """"""\n\n    def __init__(\n        self,  #prev_layer,\n        center=True,\n        scale=True,\n        act=None,\n        # reuse=None,\n        # variables_collections=None,\n        # outputs_collections=None,\n        # trainable=True,\n        epsilon=1e-12,\n        begin_norm_axis=1,\n        begin_params_axis=-1,\n        beta_init=tl.initializers.zeros(),\n        gamma_init=tl.initializers.ones(),\n        data_format=\'channels_last\',\n        name=None,\n    ):\n\n        # super(LayerNorm, self).__init__(prev_layer=prev_layer, act=act, name=name)\n        super(LayerNorm, self).__init__(name, act=act)\n        self.center = center\n        self.scale = scale\n        self.epsilon = epsilon\n        self.begin_norm_axis = begin_norm_axis\n        self.begin_params_axis = begin_params_axis\n        self.beta_init = beta_init\n        self.gamma_init = gamma_init\n        self.data_format = data_format\n\n        logging.info(\n            ""LayerNorm %s: act: %s"" % (self.name, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def build(self, inputs_shape):\n        params_shape = inputs_shape[self.begin_params_axis:]\n        self.beta, self.gamma = None, None\n        if self.center:\n            self.beta = self._get_weights(""beta"", shape=params_shape, init=self.beta_init)\n        if self.scale:\n            self.gamma = self._get_weights(""gamma"", shape=params_shape, init=self.gamma_init)\n\n        self.norm_axes = range(self.begin_norm_axis, len(inputs_shape))\n\n    def forward(self, inputs):\n        mean, var = tf.nn.moments(inputs, self.norm_axes, keepdims=True)\n        # compute layer normalization using batch_normalization function\n        outputs = batch_normalization(\n            inputs, mean, var, self.beta, self.gamma, self.epsilon, data_format=self.data_format\n        )\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n    #     with tf.compat.v1.variable_scope(name) as vs:\n    #         self.outputs = tf.contrib.layers.layer_norm(\n    #             self.inputs,\n    #             center=center,\n    #             scale=scale,\n    #             activation_fn=self.act,\n    #             reuse=reuse,\n    #             variables_collections=variables_collections,\n    #             outputs_collections=outputs_collections,\n    #             trainable=trainable,\n    #             begin_norm_axis=begin_norm_axis,\n    #             begin_params_axis=begin_params_axis,\n    #             scope=\'var\',\n    #         )\n    #\n    #         variables = tf.compat.v1.get_collection(""TF_GRAPHKEYS_VARIABLES"", scope=vs.name)\n    #\n    #     self._add_layers(self.outputs)\n    #     self._add_params(variables)\n\n\nclass GroupNorm(Layer):\n    """"""The :class:`GroupNorm` layer is for Group Normalization.\n    See `tf.contrib.layers.group_norm <https://www.tensorflow.org/api_docs/python/tf/contrib/layers/group_norm>`__.\n\n    Parameters\n    -----------\n    # prev_layer : :class:`Layer`\n    #     The previous layer.\n    groups : int\n        The number of groups\n    act : activation function\n        The activation function of this layer.\n    epsilon : float\n        Eplison.\n    data_format : str\n        channels_last \'channel_last\' (default) or channels_first.\n    name : None or str\n        A unique layer name\n\n    """"""\n\n    def __init__(self, groups=32, epsilon=1e-06, act=None, data_format=\'channels_last\', name=None):  #\'groupnorm\'):\n        # super(GroupNorm, self).__init__(prev_layer=prev_layer, act=act, name=name)\n        super().__init__(name, act=act)\n        self.groups = groups\n        self.epsilon = epsilon\n        self.data_format = data_format\n\n        logging.info(\n            ""GroupNorm %s: act: %s"" % (self.name, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def build(self, inputs_shape):\n        # shape = inputs.get_shape().as_list()\n        if len(inputs_shape) != 4:\n            raise Exception(""This GroupNorm only supports 2D images."")\n\n        if self.data_format == \'channels_last\':\n            channels = inputs_shape[-1]\n            self.int_shape = tf.concat(\n                [#tf.shape(input=self.inputs)[0:3],\n                inputs_shape[0:3],\n                tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0\n            )\n        elif self.data_format == \'channels_first\':\n            channels = inputs_shape[1]\n            self.int_shape = tf.concat(\n                [\n                    # tf.shape(input=self.inputs)[0:1],\n                    inputs_shape[0:1],\n                    tf.convert_to_tensor(value=[self.groups, channels // self.groups]),\n                    # tf.shape(input=self.inputs)[2:4]\n                    inputs_shape[2:4],\n                ],\n                axis=0\n            )\n        else:\n            raise ValueError(""data_format must be \'channels_last\' or \'channels_first\'."")\n\n        if self.groups > channels:\n            raise ValueError(\'Invalid groups %d for %d channels.\' % (self.groups, channels))\n        if channels % self.groups != 0:\n            raise ValueError(\'%d channels is not commensurate with %d groups.\' % (channels, self.groups))\n\n        if self.data_format == \'channels_last\':\n            # mean, var = tf.nn.moments(x, [1, 2, 4], keep_dims=True)\n            self.gamma = self._get_weights(""gamma"", shape=channels, init=tl.initializers.ones())\n            # self.gamma = tf.compat.v1.get_variable(\'gamma\', channels, initializer=tf.compat.v1.initializers.ones())\n            self.beta = self._get_weights(""beta"", shape=channels, init=tl.initializers.zeros())\n            # self.beta = tf.compat.v1.get_variable(\'beta\', channels, initializer=tf.compat.v1.initializers.zeros())\n        elif self.data_format == \'channels_first\':\n            # mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\n            self.gamma = self._get_weights(""gamma"", shape=[1, channels, 1, 1], init=tl.initializers.ones())\n            # self.gamma = tf.compat.v1.get_variable(\'gamma\', [1, channels, 1, 1], initializer=tf.compat.v1.initializers.ones())\n            self.beta = self._get_weights(""beta"", shape=[1, channels, 1, 1], init=tl.initializers.zeros())\n            # self.beta = tf.compat.v1.get_variable(\'beta\', [1, channels, 1, 1], initializer=tf.compat.v1.initializers.zeros())\n        # self.add_weights([self.gamma, self.bata])\n\n    def forward(self, inputs):\n        x = tf.reshape(inputs, self.int_shape)\n        if self.data_format == \'channels_last\':\n            mean, var = tf.nn.moments(x=x, axes=[1, 2, 4], keepdims=True)\n        elif self.data_format == \'channels_first\':\n            mean, var = tf.nn.moments(x=x, axes=[2, 3, 4], keepdims=True)\n        else:\n            raise Exception(""unknown data_format"")\n        x = (x - mean) / tf.sqrt(var + self.epsilon)\n\n        outputs = tf.reshape(x, tf.shape(input=inputs)) * self.gamma + self.beta\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass SwitchNorm(Layer):\n    """"""\n    The :class:`SwitchNorm` is a switchable normalization.\n\n    Parameters\n    ----------\n    act : activation function\n        The activation function of this layer.\n    epsilon : float\n        Eplison.\n    beta_init : initializer or None\n        The initializer for initializing beta, if None, skip beta.\n        Usually you should not skip beta unless you know what happened.\n    gamma_init : initializer or None\n        The initializer for initializing gamma, if None, skip gamma.\n        When the batch normalization layer is use instead of \'biases\', or the next layer is linear, this can be\n        disabled since the scaling can be done by the next layer. see `Inception-ResNet-v2 <https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py>`__\n    moving_mean_init : initializer or None\n        The initializer for initializing moving mean, if None, skip moving mean.\n    data_format : str\n        channels_last \'channel_last\' (default) or channels_first.\n    name : None or str\n        A unique layer name.\n\n    References\n    ----------\n    - `Differentiable Learning-to-Normalize via Switchable Normalization <https://arxiv.org/abs/1806.10779>`__\n    - `Zhihu (CN) <https://zhuanlan.zhihu.com/p/39296570?utm_source=wechat_session&utm_medium=social&utm_oi=984862267107651584>`__\n\n    """"""\n\n    def __init__(\n        self,\n        act=None,\n        epsilon=1e-5,\n        beta_init=tl.initializers.constant(0.0),\n        gamma_init=tl.initializers.constant(1.0),\n        moving_mean_init=tl.initializers.zeros(),\n        # beta_init=tf.compat.v1.initializers.constant(0.0),\n        # gamma_init=tf.compat.v1.initializers.constant(1.0),\n        # moving_mean_init=tf.compat.v1.initializers.zeros(),\n        data_format=\'channels_last\',\n        name=None,  #\'switchnorm\',\n    ):\n        # super(SwitchNorm, self).__init__(prev_layer=prev_layer, act=act, name=name)\n        super().__init__(name, act=act)\n        self.epsilon = epsilon\n        self.beta_init = beta_init\n        self.gamma_init = gamma_init\n        self.moving_mean_init = moving_mean_init\n        self.data_format = data_format\n\n        logging.info(\n            ""SwitchNorm %s: epsilon: %f act: %s"" %\n            (self.name, epsilon, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def build(self, inputs_shape):\n        if len(inputs_shape) != 4:\n            raise Exception(""This SwitchNorm only supports 2D images."")\n        if self.data_format != \'channels_last\':\n            raise Exception(""This SwitchNorm only supports channels_last."")\n        ch = inputs_shape[-1]\n        self.gamma = self._get_weights(""gamma"", shape=[ch], init=self.gamma_init)\n        # self.gamma = tf.compat.v1.get_variable(""gamma"", [ch], initializer=gamma_init)\n        self.beta = self._get_weights(""beta"", shape=[ch], init=self.beta_init)\n        # self.beta = tf.compat.v1.get_variable(""beta"", [ch], initializer=beta_init)\n\n        self.mean_weight_var = self._get_weights(""mean_weight"", shape=[3], init=tl.initializers.constant(1.0))\n        # self.mean_weight_var = tf.compat.v1.get_variable(""mean_weight"", [3], initializer=tf.compat.v1.initializers.constant(1.0))\n        self.var_weight_var = self._get_weights(""var_weight"", shape=[3], init=tl.initializers.constant(1.0))\n        # self.var_weight_var = tf.compat.v1.get_variable(""var_weight"", [3], initializer=tf.compat.v1.initializers.constant(1.0))\n\n        # self.add_weights([self.gamma, self.beta, self.mean_weight_var, self.var_weight_var])\n\n    def forward(self, inputs):\n\n        batch_mean, batch_var = tf.nn.moments(x=inputs, axes=[0, 1, 2], keepdims=True)\n        ins_mean, ins_var = tf.nn.moments(x=inputs, axes=[1, 2], keepdims=True)\n        layer_mean, layer_var = tf.nn.moments(x=inputs, axes=[1, 2, 3], keepdims=True)\n\n        mean_weight = tf.nn.softmax(self.mean_weight_var)\n        var_weight = tf.nn.softmax(self.var_weight_var)\n\n        mean = mean_weight[0] * batch_mean + mean_weight[1] * ins_mean + mean_weight[2] * layer_mean\n        var = var_weight[0] * batch_var + var_weight[1] * ins_var + var_weight[2] * layer_var\n\n        inputs = (inputs - mean) / (tf.sqrt(var + self.epsilon))\n        outputs = inputs * self.gamma + self.beta\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/padding.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'PadLayer\',\n    \'ZeroPad1d\',\n    \'ZeroPad2d\',\n    \'ZeroPad3d\',\n]\n\n\nclass PadLayer(Layer):\n    """"""The :class:`PadLayer` class is a padding layer for any mode and dimension.\n    Please see `tf.pad <https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/pad>`__ for usage.\n\n    Parameters\n    ----------\n    padding : list of lists of 2 ints, or a Tensor of type int32.\n        The int32 values to pad.\n    mode : str\n        ""CONSTANT"", ""REFLECT"", or ""SYMMETRIC"" (case-insensitive).\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 224, 224, 3], name=\'input\')\n    >>> padlayer = tl.layers.PadLayer([[0, 0], [3, 3], [3, 3], [0, 0]], ""REFLECT"", name=\'inpad\')(net)\n    >>> print(padlayer)\n    >>> output shape : (None, 106, 106, 3)\n\n    """"""\n\n    def __init__(\n        self,\n        padding=None,\n        mode=\'CONSTANT\',\n        name=None,  # \'pad_layer\',\n    ):\n        super().__init__(name)\n        self.padding = padding\n        self.mode = mode\n\n        logging.info(""PadLayer   %s: padding: %s mode: %s"" % (self.name, list(self.padding), self.mode))\n\n        if self.padding is None:\n            raise Exception(\n                ""padding should be a Tensor of type int32. see https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/pad""\n            )\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(padding={padding}, mode={mode}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        outputs = tf.pad(tensor=inputs, paddings=self.padding, mode=self.mode, name=self.name)\n        return outputs\n\n\nclass ZeroPad1d(Layer):\n    """"""\n    The :class:`ZeroPad1d` class is a 1D padding layer for signal [batch, length, channel].\n\n    Parameters\n    ----------\n    padding : int, or tuple of 2 ints\n            - If int, zeros to add at the beginning and end of the padding dimension (axis 1).\n            - If tuple of 2 ints, zeros to add at the beginning and at the end of the padding dimension.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 1], name=\'input\')\n    >>> pad1d = tl.layers.ZeroPad1d(padding=(2, 3))(net)\n    >>> print(pad1d)\n    >>> output shape : (None, 106, 1)\n\n    """"""\n\n    def __init__(\n        self,\n        padding,\n        name=None,  # \'zeropad1d\',\n    ):\n        super().__init__(name)\n        self.padding = padding\n        logging.info(""ZeroPad1d   %s: padding: %s"" % (self.name, str(padding)))\n\n        if not isinstance(self.padding, (int, tuple, dict)):\n            raise AssertionError()\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(padding={padding}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        self.layer = tf.keras.layers.ZeroPadding1D(padding=self.padding, name=self.name)\n\n    def forward(self, inputs):\n        outputs = self.layer(inputs)\n        return outputs\n\n\nclass ZeroPad2d(Layer):\n    """"""\n    The :class:`ZeroPad2d` class is a 2D padding layer for image [batch, height, width, channel].\n\n    Parameters\n    ----------\n    padding : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n            - If int, the same symmetric padding is applied to width and height.\n            - If tuple of 2 ints, interpreted as two different symmetric padding values for height and width as ``(symmetric_height_pad, symmetric_width_pad)``.\n            - If tuple of 2 tuples of 2 ints, interpreted as ``((top_pad, bottom_pad), (left_pad, right_pad))``.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 100, 3], name=\'input\')\n    >>> pad2d = tl.layers.ZeroPad2d(padding=((3, 3), (4, 4)))(net)\n    >>> print(pad2d)\n    >>> output shape : (None, 106, 108, 3)\n\n    """"""\n\n    def __init__(\n        self,\n        padding,\n        name=None,  # \'zeropad2d\',\n    ):\n        super().__init__(name)\n\n        self.padding = padding\n        logging.info(""ZeroPad2d   %s: padding: %s"" % (self.name, str(self.padding)))\n\n        if not isinstance(self.padding, (int, tuple)):\n            raise AssertionError(""Padding should be of type `int` or `tuple`"")\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(padding={padding}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        self.layer = tf.keras.layers.ZeroPadding2D(padding=self.padding, name=self.name)\n\n    def forward(self, inputs):\n        outputs = self.layer(inputs)\n        return outputs\n\n\nclass ZeroPad3d(Layer):\n    """"""\n    The :class:`ZeroPad3d` class is a 3D padding layer for volume [batch, depth, height, width, channel].\n\n    Parameters\n    ----------\n    padding : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n            - If int, the same symmetric padding is applied to width and height.\n            - If tuple of 2 ints, interpreted as two different symmetric padding values for height and width as ``(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad)``.\n            - If tuple of 2 tuples of 2 ints, interpreted as ``((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad))``.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 100, 100, 3], name=\'input\')\n    >>> pad3d = tl.layers.ZeroPad3d(padding=((3, 3), (4, 4), (5, 5)))(net)\n    >>> print(pad3d)\n    >>> output shape : (None, 106, 108, 110, 3)\n\n    """"""\n\n    def __init__(\n        self,\n        padding,\n        name=None,  # \'zeropad3d\',\n    ):\n        super().__init__(name)\n        self.padding = padding\n\n        logging.info(""ZeroPad3d   %s: padding: %s"" % (self.name, str(self.padding)))\n\n        if not isinstance(self.padding, (int, tuple)):\n            raise AssertionError()\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(padding={padding}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        self.layer = tf.keras.layers.ZeroPadding3D(padding=self.padding, name=self.name)\n\n    def forward(self, inputs):\n        outputs = self.layer(inputs)\n        return outputs\n'"
tensorlayer/layers/pooling.py,40,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'PoolLayer\',\n    \'MaxPool1d\',\n    \'MeanPool1d\',\n    \'MaxPool2d\',\n    \'MeanPool2d\',\n    \'MaxPool3d\',\n    \'MeanPool3d\',\n    \'GlobalMaxPool1d\',\n    \'GlobalMeanPool1d\',\n    \'GlobalMaxPool2d\',\n    \'GlobalMeanPool2d\',\n    \'GlobalMaxPool3d\',\n    \'GlobalMeanPool3d\',\n    \'CornerPool2d\',\n]\n\n\nclass PoolLayer(Layer):\n    """"""\n    The :class:`PoolLayer` class is a Pooling layer.\n    You can choose ``tf.nn.max_pool`` and ``tf.nn.avg_pool`` for 2D input or\n    ``tf.nn.max_pool3d`` and ``tf.nn.avg_pool3d`` for 3D input.\n\n    Parameters\n    ----------\n    filter_size : tuple of int\n        The size of the window for each dimension of the input tensor.\n        Note that: len(filter_size) >= 4.\n    strides : tuple of int\n        The stride of the sliding window for each dimension of the input tensor.\n        Note that: len(strides) >= 4.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    pool : pooling function\n        One of ``tf.nn.max_pool``, ``tf.nn.avg_pool``, ``tf.nn.max_pool3d`` and ``f.nn.avg_pool3d``.\n        See `TensorFlow pooling APIs <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/>`__\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.PoolLayer()(net)\n    >>> output shape : [None, 25, 25, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        filter_size=(1, 2, 2, 1),\n        strides=(1, 2, 2, 1),\n        padding=\'SAME\',\n        pool=tf.nn.max_pool,\n        name=None  # \'pool_pro\',\n    ):\n        super().__init__(name)\n        self.filter_size = filter_size\n        self.strides = strides\n        self.padding = padding\n        self.pool = pool\n\n        self.build()\n        self._built = True\n\n        logging.info(\n            ""PoolLayer %s: filter_size: %s strides: %s padding: %s pool: %s"" %\n            (self.name, str(self.filter_size), str(self.strides), self.padding, pool.__name__)\n        )\n\n    def __repr__(self):\n        s = \'{classname}(pool={poolname}, filter_size={strides}, padding={padding}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, poolname=self.pool.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        outputs = self.pool(inputs, ksize=self.filter_size, strides=self.strides, padding=self.padding, name=self.name)\n        return outputs\n\n\nclass MaxPool1d(Layer):\n    """"""Max pooling for 1D signal.\n\n    Parameters\n    ----------\n    filter_size : int\n        Pooling window size.\n    strides : int\n        Stride of the pooling operation.\n    padding : str\n        The padding method: \'VALID\' or \'SAME\'.\n    data_format : str\n        One of channels_last (default, [batch, length, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 32], name=\'input\')\n    >>> net = tl.layers.MaxPool1d(filter_size=3, strides=2, padding=\'SAME\', name=\'maxpool1d\')(net)\n    >>> output shape : [None, 25, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        filter_size=3,\n        strides=2,\n        padding=\'SAME\',\n        data_format=\'channels_last\',\n        dilation_rate=1,\n        name=None  # \'maxpool1d\'\n    ):\n        super().__init__(name)\n        self.filter_size = self._filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = self._dilation_rate = dilation_rate\n\n        self.build()\n        self._built = True\n\n        logging.info(\n            ""MaxPool1d %s: filter_size: %s strides: %s padding: %s"" %\n            (self.name, str(filter_size), str(strides), str(padding))\n        )\n\n    def __repr__(self):\n        s = (\'{classname}(filter_size={filter_size}\' \', strides={strides}, padding={padding}\')\n        if self.dilation_rate != 1:\n            s += \', dilation={dilation_rate}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        # https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/pool\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NWC\'\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCW\'\n        else:\n            raise Exception(""unsupported data format"")\n        self._filter_size = [self.filter_size]\n        self._strides = [self.strides]\n        self._dilation_rate = [self.dilation_rate]\n\n    def forward(self, inputs):\n        outputs = tf.nn.pool(\n            input=inputs,\n            window_shape=self._filter_size,\n            pooling_type=""MAX"",\n            strides=self._strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilations=self._dilation_rate,\n            name=self.name,\n        )\n        return outputs\n\n\nclass MeanPool1d(Layer):\n    """"""Mean pooling for 1D signal.\n\n    Parameters\n    ------------\n    filter_size : int\n        Pooling window size.\n    strides : int\n        Strides of the pooling operation.\n    padding : str\n        The padding method: \'VALID\' or \'SAME\'.\n    data_format : str\n        One of channels_last (default, [batch, length, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 32], name=\'input\')\n    >>> net = tl.layers.MeanPool1d(filter_size=3, strides=2, padding=\'SAME\')(net)\n    >>> output shape : [None, 25, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        filter_size=3,\n        strides=2,\n        padding=\'SAME\',\n        data_format=\'channels_last\',\n        dilation_rate=1,\n        name=None  # \'meanpool1d\'\n    ):\n        super().__init__(name)\n        self.filter_size = self._filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = self._dilation_rate = dilation_rate\n\n        self.build()\n        self._built = True\n\n        logging.info(\n            ""MeanPool1d %s: filter_size: %s strides: %s padding: %s"" %\n            (self.name, str(filter_size), str(strides), str(padding))\n        )\n\n    def __repr__(self):\n        s = (\'{classname}(filter_size={filter_size}\' \', strides={strides}, padding={padding}\')\n        if self.dilation_rate != 1:\n            s += \', dilation={dilation_rate}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        # pass\n        # https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/pool\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NWC\'\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCW\'\n        else:\n            raise Exception(""unsupported data format"")\n        self._filter_size = [self.filter_size]\n        self._strides = [self.strides]\n        self._dilation_rate = [self.dilation_rate]\n\n    def forward(self, inputs):\n        outputs = tf.nn.pool(\n            input=inputs,\n            window_shape=self._filter_size,\n            pooling_type=""AVG"",\n            padding=self.padding,\n            dilations=None,  # TODO: support dilations\n            strides=self._strides,\n            name=self.name,\n            data_format=self.data_format\n        )\n        return outputs\n\n\nclass MaxPool2d(Layer):\n    """"""Max pooling for 2D image.\n\n    Parameters\n    -----------\n    filter_size : tuple of int\n        (height, width) for filter size.\n    strides : tuple of int\n        (height, width) for strides.\n    padding : str\n        The padding method: \'VALID\' or \'SAME\'.\n    data_format : str\n        One of channels_last (default, [batch, height, width, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.MaxPool2d(filter_size=(3, 3), strides=(2, 2), padding=\'SAME\')(net)\n    >>> output shape : [None, 25, 25, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        filter_size=(3, 3),\n        strides=(2, 2),\n        padding=\'SAME\',\n        data_format=\'channels_last\',\n        name=None  # \'maxpool2d\'\n    ):\n        super().__init__(name)\n        self.filter_size = filter_size\n        if strides is None:\n            strides = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(\n            ""MaxPool2d %s: filter_size: %s strides: %s padding: %s"" %\n            (self.name, str(filter_size), str(strides), str(padding))\n        )\n\n    def __repr__(self):\n        s = (\'{classname}(filter_size={filter_size}\' \', strides={strides}, padding={padding}\')\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        self._strides = [1, self.strides[0], self.strides[1], 1]\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n        else:\n            raise Exception(""unsupported data format"")\n\n    def forward(self, inputs):\n        outputs = tf.nn.max_pool(\n            input=inputs, ksize=self.filter_size, strides=self._strides, padding=self.padding, name=self.name\n        )\n        return outputs\n\n\nclass MeanPool2d(Layer):\n    """"""Mean pooling for 2D image [batch, height, width, channel].\n\n    Parameters\n    -----------\n    filter_size : tuple of int\n        (height, width) for filter size.\n    strides : tuple of int\n        (height, width) for strides.\n    padding : str\n        The padding method: \'VALID\' or \'SAME\'.\n    data_format : str\n        One of channels_last (default, [batch, height, width, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.MeanPool2d(filter_size=(3, 3), strides=(2, 2), padding=\'SAME\')(net)\n    >>> output shape : [None, 25, 25, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        filter_size=(3, 3),\n        strides=(2, 2),\n        padding=\'SAME\',\n        data_format=\'channels_last\',\n        name=None  # \'meanpool2d\'\n    ):\n        super().__init__(name)\n        self.filter_size = filter_size\n        if strides is None:\n            strides = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(\n            ""MeanPool2d %s: filter_size: %s strides: %s padding: %s"" %\n            (self.name, str(filter_size), str(strides), str(padding))\n        )\n\n    def __repr__(self):\n        s = (\'{classname}(filter_size={filter_size}\' \', strides={strides}, padding={padding}\')\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        self._strides = [1, self.strides[0], self.strides[1], 1]\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n        else:\n            raise Exception(""unsupported data format"")\n\n    def forward(self, inputs):\n        outputs = tf.nn.avg_pool(\n            input=inputs, ksize=self.filter_size, strides=self._strides, padding=self.padding, name=self.name\n        )\n        return outputs\n\n\nclass MaxPool3d(Layer):\n    """"""Max pooling for 3D volume.\n\n    Parameters\n    ------------\n    filter_size : tuple of int\n        Pooling window size.\n    strides : tuple of int\n        Strides of the pooling operation.\n    padding : str\n        The padding method: \'VALID\' or \'SAME\'.\n    data_format : str\n        One of channels_last (default, [batch, depth, height, width, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Returns\n    -------\n    :class:`tf.Tensor`\n        A max pooling 3-D layer with a output rank as 5.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.MaxPool3d(filter_size=(3, 3, 3), strides=(2, 2, 2), padding=\'SAME\')(net)\n    >>> output shape : [None, 25, 25, 25, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        filter_size=(3, 3, 3),\n        strides=(2, 2, 2),\n        padding=\'VALID\',\n        data_format=\'channels_last\',\n        name=None  # \'maxpool3d\'\n    ):\n        super().__init__(name)\n        self.filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(\n            ""MaxPool3d %s: filter_size: %s strides: %s padding: %s"" %\n            (self.name, str(filter_size), str(strides), str(padding))\n        )\n\n    def __repr__(self):\n        s = (\'{classname}(filter_size={filter_size}\' \', strides={strides}, padding={padding}\')\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        self._strides = [1, self.strides[0], self.strides[1], self.strides[2], 1]\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NDHWC\'\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCDHW\'\n        else:\n            raise Exception(""unsupported data format"")\n\n    def forward(self, inputs):\n        outputs = tf.nn.max_pool3d(\n            input=inputs,\n            ksize=self.filter_size,\n            strides=self._strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            name=self.name,\n        )\n        return outputs\n\n\nclass MeanPool3d(Layer):\n    """"""Mean pooling for 3D volume.\n\n    Parameters\n    ------------\n    filter_size : tuple of int\n        Pooling window size.\n    strides : tuple of int\n        Strides of the pooling operation.\n    padding : str\n        The padding method: \'VALID\' or \'SAME\'.\n    data_format : str\n        One of channels_last (default, [batch, depth, height, width, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Returns\n    -------\n    :class:`tf.Tensor`\n        A mean pooling 3-D layer with a output rank as 5.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 50, 50, 50, 32], name=\'input\')\n    >>> net = tl.layers.MeanPool3d(filter_size=(3, 3, 3), strides=(2, 2, 2), padding=\'SAME\')(net)\n    >>> output shape : [None, 25, 25, 25, 32]\n\n    """"""\n\n    def __init__(\n        self,\n        filter_size=(3, 3, 3),\n        strides=(2, 2, 2),\n        padding=\'VALID\',\n        data_format=\'channels_last\',\n        name=None  # \'meanpool3d\'\n    ):\n        super().__init__(name)\n        self.filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(\n            ""MeanPool3d %s: filter_size: %s strides: %s padding: %s"" %\n            (self.name, str(filter_size), str(strides), str(padding))\n        )\n\n    def __repr__(self):\n        s = (\'{classname}(filter_size={filter_size}\' \', strides={strides}, padding={padding}\')\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        self._strides = [1, self.strides[0], self.strides[1], self.strides[2], 1]\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NDHWC\'\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCDHW\'\n        else:\n            raise Exception(""unsupported data format"")\n\n    def forward(self, inputs):\n        outputs = tf.nn.avg_pool3d(\n            input=inputs,\n            ksize=self.filter_size,\n            strides=self._strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            name=self.name,\n        )\n        return outputs\n\n\nclass GlobalMaxPool1d(Layer):\n    """"""The :class:`GlobalMaxPool1d` class is a 1D Global Max Pooling layer.\n\n    Parameters\n    ------------\n    data_format : str\n        One of channels_last (default, [batch, length, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 30], name=\'input\')\n    >>> net = tl.layers.GlobalMaxPool1d()(net)\n    >>> output shape : [None, 30]\n\n    """"""\n\n    def __init__(\n        self,\n        data_format=""channels_last"",\n        name=None  # \'globalmaxpool1d\'\n    ):\n        super().__init__(name)\n\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(""GlobalMaxPool1d %s"" % self.name)\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        if self.name is not None:\n            s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        if self.data_format == \'channels_last\':\n            outputs = tf.reduce_max(input_tensor=inputs, axis=1, name=self.name)\n        elif self.data_format == \'channels_first\':\n            outputs = tf.reduce_max(input_tensor=inputs, axis=2, name=self.name)\n        else:\n            raise ValueError(\n                ""`data_format` should have one of the following values: [`channels_last`, `channels_first`]""\n            )\n        return outputs\n\n\nclass GlobalMeanPool1d(Layer):\n    """"""The :class:`GlobalMeanPool1d` class is a 1D Global Mean Pooling layer.\n\n    Parameters\n    ------------\n    data_format : str\n        One of channels_last (default, [batch, length, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 30], name=\'input\')\n    >>> net = tl.layers.GlobalMeanPool1d()(net)\n    >>> output shape : [None, 30]\n\n    """"""\n\n    def __init__(\n        self,\n        data_format=\'channels_last\',\n        name=None  # \'globalmeanpool1d\'\n    ):\n        super().__init__(name)\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(""GlobalMeanPool1d %s"" % self.name)\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        if self.name is not None:\n            s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        if self.data_format == \'channels_last\':\n            outputs = tf.reduce_mean(input_tensor=inputs, axis=1, name=self.name)\n        elif self.data_format == \'channels_first\':\n            outputs = tf.reduce_mean(input_tensor=inputs, axis=2, name=self.name)\n        else:\n            raise ValueError(\n                ""`data_format` should have one of the following values: [`channels_last`, `channels_first`]""\n            )\n        return outputs\n\n\nclass GlobalMaxPool2d(Layer):\n    """"""The :class:`GlobalMaxPool2d` class is a 2D Global Max Pooling layer.\n\n    Parameters\n    ------------\n    data_format : str\n        One of channels_last (default, [batch, height, width, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 100, 30], name=\'input\')\n    >>> net = tl.layers.GlobalMaxPool2d()(net)\n    >>> output shape : [None, 30]\n\n    """"""\n\n    def __init__(\n        self,\n        data_format=\'channels_last\',\n        name=None  # \'globalmaxpool2d\'\n    ):\n        super().__init__(name)\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(""GlobalMaxPool2d %s"" % self.name)\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        if self.name is not None:\n            s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        if self.data_format == \'channels_last\':\n            outputs = tf.reduce_max(input_tensor=inputs, axis=[1, 2], name=self.name)\n        elif self.data_format == \'channels_first\':\n            outputs = tf.reduce_max(input_tensor=inputs, axis=[2, 3], name=self.name)\n        else:\n            raise ValueError(\n                ""`data_format` should have one of the following values: [`channels_last`, `channels_first`]""\n            )\n        return outputs\n\n\nclass GlobalMeanPool2d(Layer):\n    """"""The :class:`GlobalMeanPool2d` class is a 2D Global Mean Pooling layer.\n\n    Parameters\n    ------------\n    data_format : str\n        One of channels_last (default, [batch, height, width, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 100, 30], name=\'input\')\n    >>> net = tl.layers.GlobalMeanPool2d()(net)\n    >>> output shape : [None, 30]\n\n    """"""\n\n    def __init__(\n        self,\n        data_format=\'channels_last\',\n        name=None  # \'globalmeanpool2d\'\n    ):\n        super().__init__(name)\n\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(""GlobalMeanPool2d %s"" % self.name)\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        if self.name is not None:\n            s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        if self.data_format == \'channels_last\':\n            outputs = tf.reduce_mean(input_tensor=inputs, axis=[1, 2], name=self.name)\n        elif self.data_format == \'channels_first\':\n            outputs = tf.reduce_mean(input_tensor=inputs, axis=[2, 3], name=self.name)\n        else:\n            raise ValueError(\n                ""`data_format` should have one of the following values: [`channels_last`, `channels_first`]""\n            )\n        return outputs\n\n\nclass GlobalMaxPool3d(Layer):\n    """"""The :class:`GlobalMaxPool3d` class is a 3D Global Max Pooling layer.\n\n    Parameters\n    ------------\n    data_format : str\n        One of channels_last (default, [batch, depth, height, width, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 100, 100, 30], name=\'input\')\n    >>> net = tl.layers.GlobalMaxPool3d()(net)\n    >>> output shape : [None, 30]\n\n    """"""\n\n    def __init__(\n        self,\n        data_format=\'channels_last\',\n        name=None  # \'globalmaxpool3d\'\n    ):\n        super().__init__(name)\n\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(""GlobalMaxPool3d %s"" % self.name)\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        if self.name is not None:\n            s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        if self.data_format == \'channels_last\':\n            outputs = tf.reduce_max(input_tensor=inputs, axis=[1, 2, 3], name=self.name)\n        elif self.data_format == \'channels_first\':\n            outputs = tf.reduce_max(input_tensor=inputs, axis=[2, 3, 4], name=self.name)\n        else:\n            raise ValueError(\n                ""`data_format` should have one of the following values: [`channels_last`, `channels_first`]""\n            )\n        return outputs\n\n\nclass GlobalMeanPool3d(Layer):\n    """"""The :class:`GlobalMeanPool3d` class is a 3D Global Mean Pooling layer.\n\n    Parameters\n    ------------\n    data_format : str\n        One of channels_last (default, [batch, depth, height, width, channel]) or channels_first. The ordering of the dimensions in the inputs.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 100, 100, 100, 30], name=\'input\')\n    >>> net = tl.layers.GlobalMeanPool3d()(net)\n    >>> output shape : [None, 30]\n\n    """"""\n\n    def __init__(\n        self,\n        data_format=\'channels_last\',\n        name=None  # \'globalmeanpool3d\'\n    ):\n        super().__init__(name)\n        self.data_format = data_format\n\n        self.build()\n        self._built = True\n\n        logging.info(""GlobalMeanPool3d %s"" % self.name)\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        if self.name is not None:\n            s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        if self.data_format == \'channels_last\':\n            outputs = tf.reduce_mean(input_tensor=inputs, axis=[1, 2, 3], name=self.name)\n        elif self.data_format == \'channels_first\':\n            outputs = tf.reduce_mean(input_tensor=inputs, axis=[2, 3, 4], name=self.name)\n        else:\n            raise ValueError(\n                ""`data_format` should have one of the following values: [`channels_last`, `channels_first`]""\n            )\n        return outputs\n\n\nclass CornerPool2d(Layer):\n    """"""Corner pooling for 2D image [batch, height, width, channel], see `here <https://arxiv.org/abs/1808.01244>`__.\n\n    Parameters\n    ----------\n    mode : str\n        TopLeft for the top left corner,\n        Bottomright for the bottom right corner.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([None, 32, 32, 8], name=\'input\')\n    >>> net = tl.layers.CornerPool2d(mode=\'TopLeft\',name=\'cornerpool2d\')(net)\n    >>> output shape : [None, 32, 32, 8]\n\n    """"""\n\n    def __init__(\n        self,\n        mode=\'TopLeft\',\n        name=None  # \'cornerpool2d\'\n    ):\n        super().__init__(name)\n        self.mode = mode\n        self.build()\n        self._built = True\n\n        logging.info(""CornerPool2d %s : mode: %s"" % (self.name, str(mode)))\n\n    def __repr__(self):\n        s = (\'{classname}(mode={mode}\')\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def forward(self, inputs):\n        input_width = inputs.shape[2]\n        input_height = inputs.shape[1]\n        batch_min = tf.reduce_min(inputs)\n        if self.mode == \'TopLeft\':\n            temp_bottom = tf.pad(\n                inputs, tf.constant([[0, 0], [0, input_height - 1], [0, 0], [0, 0]]), constant_values=batch_min\n            )\n            temp_right = tf.pad(\n                inputs, tf.constant([[0, 0], [0, 0], [0, input_width - 1], [0, 0]]), constant_values=batch_min\n            )\n            temp_bottom = tf.nn.max_pool(temp_bottom, ksize=(input_height, 1), strides=(1, 1), padding=\'VALID\')\n            temp_right = tf.nn.max_pool(temp_right, ksize=(1, input_width), strides=(1, 1), padding=\'VALID\')\n            outputs = tf.add(temp_bottom, temp_right, name=self.name)\n        elif self.mode == \'BottomRight\':\n            temp_top = tf.pad(\n                inputs, tf.constant([[0, 0], [input_height - 1, 0], [0, 0], [0, 0]]), constant_values=batch_min\n            )\n            temp_left = tf.pad(\n                inputs, tf.constant([[0, 0], [0, 0], [input_width - 1, 0], [0, 0]]), constant_values=batch_min\n            )\n            temp_top = tf.nn.max_pool(temp_top, ksize=(input_height, 1), strides=(1, 1), padding=\'VALID\')\n            temp_left = tf.nn.max_pool(temp_left, ksize=(1, input_width), strides=(1, 1), padding=\'VALID\')\n            outputs = tf.add(temp_top, temp_left, name=self.name)\n        else:\n            outputs = tf.identity(inputs, name=self.name)\n        return outputs\n'"
tensorlayer/layers/quantize.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import quantize\n\n__all__ = [\n    \'Sign\',\n]\n\n\nclass Sign(Layer):\n    """"""The :class:`SignLayer` class is for quantizing the layer outputs to -1 or 1 while inferencing.\n\n    Parameters\n    ----------\n    name : a str\n        A unique layer name.\n\n    """"""\n\n    # @deprecated_alias(layer=\'prev_layer\', end_support_version=1.9)  # TODO remove this line for the 1.9 release\n    def __init__(\n        self,\n        name=None  # \'sign\',\n    ):\n        super().__init__(name)\n        logging.info(""Sign  %s"" % self.name)\n\n        self.build()\n        self._built = True\n\n    def build(self, inputs_shape=None):\n        pass\n\n    def __repr__(self):\n        s = (\'{classname}(\')\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def forward(self, inputs):\n        # with tf.variable_scope(name):\n        ## self.outputs = tl.act.sign(self.inputs)\n        # self.outputs = quantize(self.inputs)\n        outputs = quantize(inputs)\n        return outputs\n'"
tensorlayer/layers/recurrent.py,96,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n# TODO: uncomment\n__all__ = [\n    \'RNN\',\n    \'SimpleRNN\',\n    \'GRURNN\',\n    \'LSTMRNN\',\n    \'BiRNN\',\n    # \'ConvRNNCell\',\n    # \'BasicConvLSTMCell\',\n    # \'ConvLSTM\',\n    \'retrieve_seq_length_op\',\n    \'retrieve_seq_length_op2\',\n    \'retrieve_seq_length_op3\',\n    \'target_mask_op\',\n]\n\n\nclass RNN(Layer):\n    """"""\n    The :class:`RNN` class is a fixed length recurrent layer for implementing simple RNN,\n    LSTM, GRU and etc.\n\n    Parameters\n    ----------\n    cell : TensorFlow cell function\n        A RNN cell implemented by tf.keras\n            - E.g. tf.keras.layers.SimpleRNNCell, tf.keras.layers.LSTMCell, tf.keras.layers.GRUCell\n            - Note TF2.0+, TF1.0+ and TF1.0- are different\n\n    return_last_output : boolean\n        Whether return last output or all outputs in a sequence.\n\n            - If True, return the last output, ""Sequence input and single output""\n            - If False, return all outputs, ""Synced sequence input and output""\n            - In other word, if you want to stack more RNNs on this layer, set to False\n\n        In a dynamic model, `return_last_output` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_seq_2d : boolean\n        Only consider this argument when `return_last_output` is `False`\n\n            - If True, return 2D Tensor [batch_size * n_steps, n_hidden], for stacking Dense layer after it.\n            - If False, return 3D Tensor [batch_size, n_steps, n_hidden], for stacking multiple RNN after it.\n\n        In a dynamic model, `return_seq_2d` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_last_state: boolean\n        Whether to return the last state of the RNN cell. The state is a list of Tensor.\n        For simple RNN and GRU, last_state = [last_output]; For LSTM, last_state = [last_output, last_cell_state]\n\n            - If True, the layer will return outputs and the final state of the cell.\n            - If False, the layer will return outputs only.\n\n        In a dynamic model, `return_last_state` can be updated when it is called in customised forward().\n        By default, `False`.\n    in_channels: int\n        Optional, the number of channels of the previous layer which is normally the size of embedding.\n        If given, the layer will be built when init.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : str\n        A unique layer name.\n\n    Examples\n    --------\n    For synced sequence input and output, see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm.py>`__\n\n    A simple regression model below.\n    \n    >>> inputs = tl.layers.Input([batch_size, num_steps, embedding_size])\n    >>> rnn_out, lstm_state = tl.layers.RNN(\n    >>>     cell=tf.keras.layers.LSTMCell(units=hidden_size, dropout=0.1),\n    >>>     in_channels=embedding_size,\n    >>>     return_last_output=True, return_last_state=True, name=\'lstmrnn\'\n    >>> )(inputs)\n    >>> outputs = tl.layers.Dense(n_units=1)(rnn_out)\n    >>> rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0], rnn_state[1]], name=\'rnn_model\')\n    >>> # If LSTMCell is applied, the rnn_state is [h, c] where h the hidden state and c the cell state of LSTM.\n\n    A stacked RNN model.\n    \n    >>> inputs = tl.layers.Input([batch_size, num_steps, embedding_size])\n    >>> rnn_out1 = tl.layers.RNN(\n    >>>     cell=tf.keras.layers.SimpleRNNCell(units=hidden_size, dropout=0.1),\n    >>>     return_last_output=False, return_seq_2d=False, return_last_state=False\n    >>> )(inputs)\n    >>> rnn_out2 = tl.layers.RNN(\n    >>>     cell=tf.keras.layers.SimpleRNNCell(units=hidden_size, dropout=0.1),\n    >>>     return_last_output=True, return_last_state=False\n    >>> )(rnn_out1)\n    >>> outputs = tl.layers.Dense(n_units=1)(rnn_out2)\n    >>> rnn_model = tl.models.Model(inputs=inputs, outputs=outputs)\n\n    An example if the sequences have different length and contain padding.\n    Similar to the DynamicRNN in TL 1.x.\n\n    If the `sequence_length` is provided in RNN\'s forwarding and both `return_last_output` and `return_last_state`\n    are set as `True`, the forward function will automatically ignore the paddings. Note that if `return_last_output`\n    is set as `False`, the synced sequence outputs will still include outputs which correspond with paddings,\n    but users are free to select which slice of outputs to be used in following procedure.\n\n    The `sequence_length` should be a list of integers which indicates the length of each sequence.\n    It is recommended to\n    `tl.layers.retrieve_seq_length_op3 <https://tensorlayer.readthedocs.io/en/latest/modules/layers.html#compute-sequence-length-3>`__\n    to calculate the `sequence_length`.\n\n    >>> data = [[[1], [2], [0], [0], [0]], [[1], [2], [3], [0], [0]], [[1], [2], [6], [1], [1]]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> class DynamicRNNExample(tl.models.Model):\n    >>>     def __init__(self):\n    >>>         super(DynamicRNNExample, self).__init__()\n    >>>         self.rnnlayer = tl.layers.RNN(\n    >>>             cell=tf.keras.layers.SimpleRNNCell(units=6, dropout=0.1), in_channels=1, return_last_output=True,\n    >>>             return_last_state=True\n    >>>         )\n    >>>     def forward(self, x):\n    >>>         z, s = self.rnnlayer(x, sequence_length=tl.layers.retrieve_seq_length_op3(x))\n    >>>         return z, s\n    >>> model = DynamicRNNExample()\n    >>> model.eval()\n    >>> output, state = model(data)\n\n\n    Notes\n    -----\n    Input dimension should be rank 3 : [batch_size, n_steps, n_features], if no, please see layer :class:`Reshape`.\n\n    """"""\n\n    def __init__(\n        self,\n        cell,\n        return_last_output=False,\n        return_seq_2d=False,\n        return_last_state=True,\n        in_channels=None,\n        name=None,  # \'rnn\'\n    ):\n\n        super(RNN, self).__init__(name=name)\n\n        self.cell = cell\n        self.return_last_output = return_last_output\n        self.return_seq_2d = return_seq_2d\n        self.return_last_state = return_last_state\n\n        if in_channels is not None:\n            self.build((None, None, in_channels))\n            self._built = True\n\n        logging.info(""RNN %s: cell: %s, n_units: %s"" % (self.name, self.cell.__class__.__name__, self.cell.units))\n\n    def __repr__(self):\n        s = (\'{classname}(cell={cellname}, n_units={n_units}\')\n        s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(\n            classname=self.__class__.__name__, cellname=self.cell.__class__.__name__, n_units=self.cell.units,\n            **self.__dict__\n        )\n\n    def build(self, inputs_shape):\n        """"""\n        Parameters\n        ----------\n        inputs_shape : tuple\n            the shape of inputs tensor\n        """"""\n        # Input dimension should be rank 3 [batch_size, n_steps(max), n_features]\n        if len(inputs_shape) != 3:\n            raise Exception(""RNN : Input dimension should be rank 3 : [batch_size, n_steps, n_features]"")\n\n        with tf.name_scope(self.name) as scope:\n            self.cell.build(tuple(inputs_shape))\n\n        if self._trainable_weights is None:\n            self._trainable_weights = list()\n        for var in self.cell.trainable_variables:\n            self._trainable_weights.append(var)\n\n    # @tf.function\n    def forward(self, inputs, sequence_length=None, initial_state=None, **kwargs):\n        """"""\n        Parameters\n        ----------\n        inputs : input tensor\n            The input of a network\n        sequence_length: None or list of integers\n            The actual length of each sequence in batch without padding.\n            If provided, when `return_last_output` and `return_last_state` are `True`,\n            the RNN will perform in the manner of a dynamic RNN, i.e.\n            the RNN will return the actual last output / state without padding.\n        initial_state : None or list of Tensor (RNN State)\n            If None, `initial_state` is zero state.\n\n        **kwargs: dict\n            Some attributes can be updated during forwarding\n            such as `return_last_output`, `return_seq_2d`, `return_last_state`.\n        """"""\n        if kwargs:\n            for attr in kwargs:\n                if attr in self.__dict__:\n                    setattr(self, attr, kwargs[attr])\n\n        batch_size = inputs.get_shape().as_list()[0]\n        total_steps = inputs.get_shape().as_list()[1]\n\n        # checking the type and values of sequence_length\n        if sequence_length is not None:\n            if isinstance(sequence_length, list):\n                pass\n            elif isinstance(sequence_length, tf.Tensor):\n                pass\n            elif isinstance(sequence_length, np.ndarray):\n                sequence_length = sequence_length.tolist()\n            else:\n                raise TypeError(\n                    ""The argument sequence_length should be either None or a list of integers. ""\n                    ""Type got %s"" % type(sequence_length)\n                )\n            if (len(sequence_length) != batch_size):\n                raise ValueError(\n                    ""The argument sequence_length should contain %d "" % batch_size +\n                    ""elements indicating the initial length of each sequence, but got only %d. "" % len(sequence_length)\n                )\n            for i in sequence_length:\n                if not (type(i) is int or (isinstance(i, tf.Tensor) and i.dtype.is_integer)):\n                    raise TypeError(\n                        ""The argument sequence_length should be either None or a list of integers. ""\n                        ""One element of sequence_length has the type %s"" % type(i)\n                    )\n                if i > total_steps:\n                    raise ValueError(\n                        ""The actual length of a sequence should not be longer than ""\n                        ""that of the longest sequence (total steps) in this mini-batch. ""\n                        ""Total steps of this mini-batch %d, "" % total_steps +\n                        ""but got an actual length of a sequence %d"" % i\n                    )\n\n            sequence_length = [i - 1 if i >= 1 else 0 for i in sequence_length]\n\n        # set warning\n        # if (not self.return_last_output) and sequence_length is not None:\n        #     warnings.warn(\n        #         \'return_last_output is set as %s \' % self.return_last_output +\n        #         \'When sequence_length is provided, it is recommended to set as True. \' +\n        #         \'Otherwise, padding will be considered while RNN is forwarding.\'\n        #     )\n\n        # return the last output, iterating each seq including padding ones. No need to store output during each\n        # time step.\n        if self.return_last_output and sequence_length is None:\n            outputs = [-1]\n        else:\n            outputs = list()\n\n        # initialize the states if provided\n        states = initial_state if initial_state is not None else self.cell.get_initial_state(inputs)\n        if not isinstance(states, list):\n            states = [states]\n\n        stored_states = list()\n\n        # initialize the cell\n        self.cell.reset_dropout_mask()\n        self.cell.reset_recurrent_dropout_mask()\n\n        # recurrent computation\n        # FIXME: if sequence_length is provided (dynamic rnn), only iterate max(sequence_length) times.\n        for time_step in range(total_steps):\n\n            cell_output, states = self.cell.call(inputs[:, time_step, :], states, training=self.is_train)\n            stored_states.append(states)\n\n            if self.return_last_output and sequence_length is None:\n                outputs[-1] = cell_output\n            else:\n                outputs.append(cell_output)\n\n        # prepare to return results\n        if self.return_last_output and sequence_length is None:\n            outputs = outputs[-1]\n\n        elif self.return_last_output and sequence_length is not None:\n            outputs = tf.convert_to_tensor(outputs)\n            outputs = tf.gather(outputs, sequence_length, axis=0)\n\n            outputs_without_padding = []\n            for i in range(batch_size):\n                outputs_without_padding.append(outputs[i][i][:])\n            outputs = tf.convert_to_tensor(outputs_without_padding)\n        else:\n            if self.return_seq_2d:\n                # PTB tutorial: stack dense layer after that, or compute the cost from the output\n                # 2D Tensor [batch_size * n_steps, n_hidden]\n                outputs = tf.reshape(tf.concat(outputs, 1), [-1, self.cell.units])\n            else:\n                # <akara>: stack more RNN layer after that\n                # 3D Tensor [batch_size, n_steps, n_hidden]\n                outputs = tf.reshape(tf.concat(outputs, 1), [-1, total_steps, self.cell.units])\n\n        if self.return_last_state and sequence_length is None:\n            return outputs, states\n        elif self.return_last_state and sequence_length is not None:\n\n            stored_states = tf.convert_to_tensor(stored_states)\n            stored_states = tf.gather(stored_states, sequence_length, axis=0)\n\n            states = []\n            for i in range(stored_states.shape[1]):\n                states.append(tf.convert_to_tensor([stored_states[b, i, b, :] for b in range(batch_size)]))\n\n            return outputs, states\n        else:\n            return outputs\n\n\nclass SimpleRNN(RNN):\n    """"""\n    The :class:`SimpleRNN` class is a fixed length recurrent layer for implementing simple RNN.\n\n    Parameters\n    ----------\n    units: int\n        Positive integer, the dimension of hidden space.\n    return_last_output : boolean\n        Whether return last output or all outputs in a sequence.\n            - If True, return the last output, ""Sequence input and single output""\n            - If False, return all outputs, ""Synced sequence input and output""\n            - In other word, if you want to stack more RNNs on this layer, set to False\n\n        In a dynamic model, `return_last_output` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_seq_2d : boolean\n        Only consider this argument when `return_last_output` is `False`\n            - If True, return 2D Tensor [batch_size * n_steps, n_hidden], for stacking Dense layer after it.\n            - If False, return 3D Tensor [batch_size, n_steps, n_hidden], for stacking multiple RNN after it.\n\n        In a dynamic model, `return_seq_2d` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_last_state: boolean\n        Whether to return the last state of the RNN cell. The state is a list of Tensor.\n        For simple RNN, last_state = [last_output]\n\n            - If True, the layer will return outputs and the final state of the cell.\n            - If False, the layer will return outputs only.\n\n        In a dynamic model, `return_last_state` can be updated when it is called in customised forward().\n        By default, `False`.\n    in_channels: int\n        Optional, the number of channels of the previous layer which is normally the size of embedding.\n        If given, the layer will be built when init.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : str\n        A unique layer name.\n    `**kwargs`:\n        Advanced arguments to configure the simple RNN cell.\n        Please check tf.keras.layers.SimpleRNNCell.\n\n    Examples\n    --------\n\n    A simple regression model below.\n\n    >>> inputs = tl.layers.Input([batch_size, num_steps, embedding_size])\n    >>> rnn_out, lstm_state = tl.layers.SimpleRNN(\n    >>>     units=hidden_size, dropout=0.1, # both units and dropout are used to configure the simple rnn cell.\n    >>>     in_channels=embedding_size,\n    >>>     return_last_output=True, return_last_state=True, name=\'simplernn\'\n    >>> )(inputs)\n    >>> outputs = tl.layers.Dense(n_units=1)(rnn_out)\n    >>> rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0]], name=\'rnn_model\')\n\n    Notes\n    -----\n    Input dimension should be rank 3 : [batch_size, n_steps, n_features], if no, please see layer :class:`Reshape`.\n\n    """"""\n\n    def __init__(\n        self,\n        units,\n        return_last_output=False,\n        return_seq_2d=False,\n        return_last_state=True,\n        in_channels=None,\n        name=None,  # \'simplernn\'\n        **kwargs\n    ):\n        super(SimpleRNN, self).__init__(\n            cell=tf.keras.layers.SimpleRNNCell(units=units, **kwargs), return_last_output=return_last_output,\n            return_seq_2d=return_seq_2d, return_last_state=return_last_state, in_channels=in_channels, name=name\n        )\n\n\nclass GRURNN(RNN):\n    """"""\n    The :class:`GRURNN` class is a fixed length recurrent layer for implementing RNN with GRU cell.\n\n    Parameters\n    ----------\n    units: int\n        Positive integer, the dimension of hidden space.\n    return_last_output : boolean\n        Whether return last output or all outputs in a sequence.\n            - If True, return the last output, ""Sequence input and single output""\n            - If False, return all outputs, ""Synced sequence input and output""\n            - In other word, if you want to stack more RNNs on this layer, set to False\n\n        In a dynamic model, `return_last_output` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_seq_2d : boolean\n        Only consider this argument when `return_last_output` is `False`\n            - If True, return 2D Tensor [batch_size * n_steps, n_hidden], for stacking Dense layer after it.\n            - If False, return 3D Tensor [batch_size, n_steps, n_hidden], for stacking multiple RNN after it.\n\n        In a dynamic model, `return_seq_2d` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_last_state: boolean\n        Whether to return the last state of the RNN cell. The state is a list of Tensor.\n        For GRU, last_state = [last_output]\n\n            - If True, the layer will return outputs and the final state of the cell.\n            - If False, the layer will return outputs only.\n\n        In a dynamic model, `return_last_state` can be updated when it is called in customised forward().\n        By default, `False`.\n    in_channels: int\n        Optional, the number of channels of the previous layer which is normally the size of embedding.\n        If given, the layer will be built when init.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : str\n        A unique layer name.\n    `**kwargs`:\n        Advanced arguments to configure the GRU cell.\n        Please check tf.keras.layers.GRUCell.\n\n    Examples\n    --------\n\n    A simple regression model below.\n\n    >>> inputs = tl.layers.Input([batch_size, num_steps, embedding_size])\n    >>> rnn_out, lstm_state = tl.layers.GRURNN(\n    >>>     units=hidden_size, dropout=0.1, # both units and dropout are used to configure the GRU cell.\n    >>>     in_channels=embedding_size,\n    >>>     return_last_output=True, return_last_state=True, name=\'grurnn\'\n    >>> )(inputs)\n    >>> outputs = tl.layers.Dense(n_units=1)(rnn_out)\n    >>> rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0]], name=\'rnn_model\')\n\n    Notes\n    -----\n    Input dimension should be rank 3 : [batch_size, n_steps, n_features], if no, please see layer :class:`Reshape`.\n\n    """"""\n\n    def __init__(\n        self,\n        units,\n        return_last_output=False,\n        return_seq_2d=False,\n        return_last_state=True,\n        in_channels=None,\n        name=None,  # \'grurnn\'\n        **kwargs\n    ):\n        super(GRURNN, self).__init__(\n            cell=tf.keras.layers.GRUCell(units=units, **kwargs), return_last_output=return_last_output,\n            return_seq_2d=return_seq_2d, return_last_state=return_last_state, in_channels=in_channels, name=name\n        )\n\n\nclass LSTMRNN(RNN):\n    """"""\n    The :class:`LSTMRNN` class is a fixed length recurrent layer for implementing RNN with LSTM cell.\n\n    Parameters\n    ----------\n    units: int\n        Positive integer, the dimension of hidden space.\n    return_last_output : boolean\n        Whether return last output or all outputs in a sequence.\n            - If True, return the last output, ""Sequence input and single output""\n            - If False, return all outputs, ""Synced sequence input and output""\n            - In other word, if you want to stack more RNNs on this layer, set to False\n\n        In a dynamic model, `return_last_output` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_seq_2d : boolean\n        Only consider this argument when `return_last_output` is `False`\n            - If True, return 2D Tensor [batch_size * n_steps, n_hidden], for stacking Dense layer after it.\n            - If False, return 3D Tensor [batch_size, n_steps, n_hidden], for stacking multiple RNN after it.\n\n        In a dynamic model, `return_seq_2d` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_last_state: boolean\n        Whether to return the last state of the RNN cell. The state is a list of Tensor.\n        For LSTM, last_state = [last_output, last_cell_state]\n\n            - If True, the layer will return outputs and the final state of the cell.\n            - If False, the layer will return outputs only.\n\n        In a dynamic model, `return_last_state` can be updated when it is called in customised forward().\n        By default, `False`.\n    in_channels: int\n        Optional, the number of channels of the previous layer which is normally the size of embedding.\n        If given, the layer will be built when init.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : str\n        A unique layer name.\n    `**kwargs`:\n        Advanced arguments to configure the LSTM cell.\n        Please check tf.keras.layers.LSTMCell.\n\n    Examples\n    --------\n\n    A simple regression model below.\n\n    >>> inputs = tl.layers.Input([batch_size, num_steps, embedding_size])\n    >>> rnn_out, lstm_state = tl.layers.LSTMRNN(\n    >>>     units=hidden_size, dropout=0.1, # both units and dropout are used to configure the LSTM cell.\n    >>>     in_channels=embedding_size,\n    >>>     return_last_output=True, return_last_state=True, name=\'grurnn\'\n    >>> )(inputs)\n    >>> outputs = tl.layers.Dense(n_units=1)(rnn_out)\n    >>> rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0]], name=\'rnn_model\')\n\n    Notes\n    -----\n    Input dimension should be rank 3 : [batch_size, n_steps, n_features], if no, please see layer :class:`Reshape`.\n\n    """"""\n\n    def __init__(\n        self,\n        units,\n        return_last_output=False,\n        return_seq_2d=False,\n        return_last_state=True,\n        in_channels=None,\n        name=None,  # \'lstmrnn\'\n        **kwargs\n    ):\n        super(LSTMRNN, self).__init__(\n            cell=tf.keras.layers.LSTMCell(units=units, **kwargs), return_last_output=return_last_output,\n            return_seq_2d=return_seq_2d, return_last_state=return_last_state, in_channels=in_channels, name=name\n        )\n\n\nclass BiRNN(Layer):\n    """"""\n    The :class:`BiRNN` class is a fixed length Bidirectional recurrent layer.\n\n    Parameters\n    ----------\n    fw_cell : TensorFlow cell function for forward direction\n        A RNN cell implemented by tf.keras, e.g. tf.keras.layers.SimpleRNNCell, tf.keras.layers.LSTMCell, tf.keras.layers.GRUCell.\n        Note TF2.0+, TF1.0+ and TF1.0- are different\n    bw_cell: TensorFlow cell function for backward direction similar with `fw_cell`\n    return_seq_2d : boolean.\n        If True, return 2D Tensor [batch_size * n_steps, n_hidden], for stacking Dense layer after it.\n        If False, return 3D Tensor [batch_size, n_steps, n_hidden], for stacking multiple RNN after it.\n        In a dynamic model, `return_seq_2d` can be updated when it is called in customised forward().\n        By default, `False`.\n    return_last_state: boolean\n        Whether to return the last state of the two cells. The state is a list of Tensor.\n            - If True, the layer will return outputs, the final state of `fw_cell` and the final state of `bw_cell`.\n            - If False, the layer will return outputs only.\n\n        In a dynamic model, `return_last_state` can be updated when it is called in customised forward().\n        By default, `False`.\n    in_channels: int\n        Optional, the number of channels of the previous layer which is normally the size of embedding.\n        If given, the layer will be built when init.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : str\n        A unique layer name.\n\n    Examples\n    --------\n    A simple regression model below.\n    \n    >>> inputs = tl.layers.Input([batch_size, num_steps, embedding_size])\n    >>> # the fw_cell and bw_cell can be different\n    >>> rnnlayer = tl.layers.BiRNN(\n    >>>     fw_cell=tf.keras.layers.SimpleRNNCell(units=hidden_size, dropout=0.1),\n    >>>     bw_cell=tf.keras.layers.SimpleRNNCell(units=hidden_size + 1, dropout=0.1),\n    >>>     return_seq_2d=True, return_last_state=True\n    >>> )\n    >>> # if return_last_state=True, the final state of the two cells will be returned together with the outputs\n    >>> # if return_last_state=False, only the outputs will be returned\n    >>> rnn_out, rnn_fw_state, rnn_bw_state = rnnlayer(inputs)\n    >>> # if the BiRNN is followed by a Dense, return_seq_2d should be True.\n    >>> # if the BiRNN is followed by other RNN, return_seq_2d can be False.\n    >>> dense = tl.layers.Dense(n_units=1)(rnn_out)\n    >>> outputs = tl.layers.Reshape([-1, num_steps])(dense)\n    >>> rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_out, rnn_fw_state[0], rnn_bw_state[0]])\n\n    A stacked BiRNN model.\n    \n    >>> inputs = tl.layers.Input([batch_size, num_steps, embedding_size])\n    >>> rnn_out1 = tl.layers.BiRNN(\n    >>>     fw_cell=tf.keras.layers.SimpleRNNCell(units=hidden_size, dropout=0.1),\n    >>>     bw_cell=tf.keras.layers.SimpleRNNCell(units=hidden_size + 1, dropout=0.1),\n    >>>     return_seq_2d=False, return_last_state=False\n    >>> )(inputs)\n    >>> rnn_out2 = tl.layers.BiRNN(\n    >>>     fw_cell=tf.keras.layers.SimpleRNNCell(units=hidden_size, dropout=0.1),\n    >>>     bw_cell=tf.keras.layers.SimpleRNNCell(units=hidden_size + 1, dropout=0.1),\n    >>>     return_seq_2d=True, return_last_state=False\n    >>> )(rnn_out1)\n    >>> dense = tl.layers.Dense(n_units=1)(rnn_out2)\n    >>> outputs = tl.layers.Reshape([-1, num_steps])(dense)\n    >>> rnn_model = tl.models.Model(inputs=inputs, outputs=outputs)\n\n    Notes\n    -----\n    Input dimension should be rank 3 : [batch_size, n_steps, n_features]. If not, please see layer :class:`Reshape`.\n\n    """"""\n\n    def __init__(\n        self,\n        fw_cell,\n        bw_cell,\n        return_seq_2d=False,\n        return_last_state=False,\n        in_channels=None,\n        name=None,  # \'birnn\'\n    ):\n        super(BiRNN, self).__init__(name)\n\n        self.fw_cell = fw_cell\n        self.bw_cell = bw_cell\n        self.return_seq_2d = return_seq_2d\n        self.return_last_state = return_last_state\n\n        if in_channels is not None:\n            self.build((None, None, in_channels))\n            self._built = True\n\n        logging.info(\n            ""BiRNN %s: fw_cell: %s, fw_n_units: %s, bw_cell: %s, bw_n_units\xef\xbc\x9a %s"" % (\n                self.name, self.fw_cell.__class__.__name__, self.fw_cell.units, self.bw_cell.__class__.__name__,\n                self.bw_cell.units\n            )\n        )\n\n    def __repr__(self):\n        s = (\n            \'{classname}(fw_cell={fw_cellname}, fw_n_units={fw_n_units}\'\n            \', bw_cell={bw_cellname}, bw_n_units={bw_n_units}\'\n        )\n        s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(\n            classname=self.__class__.__name__, fw_cellname=self.fw_cell.__class__.__name__,\n            fw_n_units=self.fw_cell.units, bw_cellname=self.bw_cell.__class__.__name__, bw_n_units=self.bw_cell.units,\n            **self.__dict__\n        )\n\n    def build(self, inputs_shape):\n        """"""\n        Parameters\n        ----------\n        inputs_shape : tuple\n            the shape of inputs tensor\n        """"""\n        # Input dimension should be rank 3 [batch_size, n_steps(max), n_features]\n        if len(inputs_shape) != 3:\n            raise Exception(""RNN : Input dimension should be rank 3 : [batch_size, n_steps, n_features]"")\n\n        with tf.name_scope(self.name) as scope:\n            self.fw_cell.build(tuple(inputs_shape))\n            self.bw_cell.build(tuple(inputs_shape))\n\n        if self._trainable_weights is None:\n            self._trainable_weights = list()\n        for var in self.fw_cell.trainable_variables:\n            self._trainable_weights.append(var)\n        for var in self.bw_cell.trainable_variables:\n            self._trainable_weights.append(var)\n\n    # @tf.function\n    def forward(self, inputs, fw_initial_state=None, bw_initial_state=None, **kwargs):\n        """"""\n        Parameters\n        ----------\n        inputs : input tensor\n            The input of a network\n        fw_initial_state : None or list of Tensor (RNN State)\n            If None, `fw_initial_state` is zero state.\n        bw_initial_state : None or list of Tensor (RNN State)\n            If None, `bw_initial_state` is zero state.\n        **kwargs: dict\n            Some attributes can be updated during forwarding\n            such as `return_last_output`, `return_seq_2d`, `return_last_state`.\n        """"""\n\n        if kwargs:\n            for attr in kwargs:\n                if attr in self.__dict__:\n                    setattr(self, attr, kwargs[attr])\n\n        fw_outputs = list()\n        bw_outputs = list()\n\n        fw_states = fw_initial_state if fw_initial_state is not None else self.fw_cell.get_initial_state(inputs)\n        bw_states = bw_initial_state if bw_initial_state is not None else self.bw_cell.get_initial_state(inputs)\n\n        if not isinstance(fw_states, list):\n            fw_states = [fw_states]\n        if not isinstance(bw_states, list):\n            bw_states = [bw_states]\n\n        total_steps = inputs.get_shape().as_list()[1]\n\n        self.fw_cell.reset_dropout_mask()\n        self.fw_cell.reset_recurrent_dropout_mask()\n        self.bw_cell.reset_dropout_mask()\n        self.bw_cell.reset_recurrent_dropout_mask()\n\n        for time_step in range(total_steps):\n\n            fw_cell_output, fw_states = self.fw_cell.call(inputs[:, time_step, :], fw_states, training=self.is_train)\n            bw_cell_output, bw_states = self.bw_cell.call(\n                inputs[:, -time_step - 1, :], bw_states, training=self.is_train\n            )\n\n            fw_outputs.append(fw_cell_output)\n            bw_outputs.append(bw_cell_output)\n\n        if self.return_seq_2d:\n            # PTB tutorial: stack dense layer after that, or compute the cost from the output\n            # 2D Tensor [batch_size * n_steps, n_hidden]\n            fw_outputs = tf.reshape(tf.concat(fw_outputs, 1), [-1, self.fw_cell.units])\n            bw_outputs = tf.reshape(tf.concat(bw_outputs, 1), [-1, self.bw_cell.units])\n        else:\n            # <akara>: stack more RNN layer after that\n            # 3D Tensor [batch_size, n_steps, n_hidden]\n            fw_outputs = tf.reshape(tf.concat(fw_outputs, 1), [-1, total_steps, self.fw_cell.units])\n            bw_outputs = tf.reshape(tf.concat(bw_outputs, 1), [-1, total_steps, self.bw_cell.units])\n\n        outputs = tf.concat([fw_outputs, bw_outputs], -1)\n\n        if self.return_last_state:\n            return outputs, fw_states, bw_states\n        else:\n            return outputs\n\n\n\'\'\'\nclass ConvRNNCell(object):\n    """"""Abstract object representing an Convolutional RNN Cell.""""""\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run this RNN cell on inputs, starting from the given state.""""""\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def state_size(self):\n        """"""size(s) of state(s) used by this cell.""""""\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def output_size(self):\n        """"""Integer or TensorShape: size of outputs produced by this cell.""""""\n        raise NotImplementedError(""Abstract method"")\n\n    def zero_state(self, batch_size):  #, dtype=LayersConfig.tf_dtype):\n        """"""Return zero-filled state tensor(s).\n        Args:\n          batch_size: int, float, or unit Tensor representing the batch size.\n        Returns:\n          tensor of shape \'[batch_size x shape[0] x shape[1] x num_features]\n          filled with zeros\n\n        """"""\n        dtype = LayersConfig.tf_dtype\n        shape = self.shape\n        num_features = self.num_features\n        # TODO : TypeError: \'NoneType\' object is not subscriptable\n        zeros = tf.zeros([batch_size, shape[0], shape[1], num_features * 2], dtype=dtype)\n        return zeros\n\n\nclass BasicConvLSTMCell(ConvRNNCell):\n    """"""Basic Conv LSTM recurrent network cell.\n\n    Parameters\n    -----------\n    shape : tuple of int\n        The height and width of the cell.\n    filter_size : tuple of int\n        The height and width of the filter\n    num_features : int\n        The hidden size of the cell\n    forget_bias : float\n        The bias added to forget gates (see above).\n    input_size : int\n        Deprecated and unused.\n    state_is_tuple : boolen\n        If True, accepted and returned states are 2-tuples of the `c_state` and `m_state`.\n        If False, they are concatenated along the column axis. The latter behavior will soon be deprecated.\n    act : activation function\n        The activation function of this layer, tanh as default.\n\n    """"""\n\n    def __init__(\n            self, shape, filter_size, num_features, forget_bias=1.0, input_size=None, state_is_tuple=False,\n            act=tf.nn.tanh\n    ):\n        """"""Initialize the basic Conv LSTM cell.""""""\n        # if not state_is_tuple:\n        # logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n        #             ""deprecated.  Use state_is_tuple=True."", self)\n        if input_size is not None:\n            logging.warn(""%s: The input_size parameter is deprecated."", self)\n        self.shape = shape\n        self.filter_size = filter_size\n        self.num_features = num_features\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._activation = act\n\n    @property\n    def state_size(self):\n        """"""State size of the LSTMStateTuple.""""""\n        return (LSTMStateTuple(self._num_units, self._num_units) if self._state_is_tuple else 2 * self._num_units)\n\n    @property\n    def output_size(self):\n        """"""Number of units in outputs.""""""\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Long short-term memory cell (LSTM).""""""\n        with tf.compat.v1.variable_scope(scope or type(self).__name__):  # ""BasicLSTMCell""\n            # Parameters of gates are concatenated into one multiply for efficiency.\n            if self._state_is_tuple:\n                c, h = state\n            else:\n                # print state\n                # c, h = tf.split(3, 2, state)\n                c, h = tf.split(state, 2, 3)\n            concat = _conv_linear([inputs, h], self.filter_size, self.num_features * 4, True)\n\n            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n            # i, j, f, o = tf.split(3, 4, concat)\n            i, j, f, o = tf.split(concat, 4, 3)\n\n            new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + tf.nn.sigmoid(i) * self._activation(j))\n            new_h = self._activation(new_c) * tf.nn.sigmoid(o)\n\n            if self._state_is_tuple:\n                new_state = LSTMStateTuple(new_c, new_h)\n            else:\n                new_state = tf.concat([new_c, new_h], 3)\n            return new_h, new_state\n\n\ndef _conv_linear(args, filter_size, num_features, bias, bias_start=0.0, scope=None):\n    """"""convolution:\n\n    Parameters\n    ----------\n    args : tensor\n        4D Tensor or a list of 4D, batch x n, Tensors.\n    filter_size : tuple of int\n        Filter height and width.\n    num_features : int\n        Nnumber of features.\n    bias_start : float\n        Starting value to initialize the bias; 0 by default.\n    scope : VariableScope\n        For the created subgraph; defaults to ""Linear"".\n\n    Returns\n    --------\n    - A 4D Tensor with shape [batch h w num_features]\n\n    Raises\n    -------\n    - ValueError : if some of the arguments has unspecified or wrong shape.\n\n    """"""\n    # Calculate the total size of arguments on dimension 1.\n    total_arg_size_depth = 0\n    shapes = [a.get_shape().as_list() for a in args]\n    for shape in shapes:\n        if len(shape) != 4:\n            raise ValueError(""Linear is expecting 4D arguments: %s"" % str(shapes))\n        if not shape[3]:\n            raise ValueError(""Linear expects shape[4] of arguments: %s"" % str(shapes))\n        else:\n            total_arg_size_depth += shape[3]\n\n    dtype = [a.dtype for a in args][0]\n\n    # Now the computation.\n    with tf.compat.v1.variable_scope(scope or ""Conv""):\n        matrix = tf.compat.v1.get_variable(\n            ""Matrix"", [filter_size[0], filter_size[1], total_arg_size_depth, num_features], dtype=dtype\n        )\n        if len(args) == 1:\n            res = tf.nn.conv2d(args[0], matrix, strides=[1, 1, 1, 1], padding=\'SAME\')\n        else:\n            res = tf.nn.conv2d(tf.concat(args, 3), matrix, strides=[1, 1, 1, 1], padding=\'SAME\')\n        if not bias:\n            return res\n        bias_term = tf.compat.v1.get_variable(\n            ""Bias"", [num_features], dtype=dtype,\n            initializer=tf.compat.v1.initializers.constant(bias_start, dtype=dtype)\n        )\n    return res + bias_term\n\n\nclass ConvLSTM(Layer):\n    """"""A fixed length Convolutional LSTM layer.\n\n    See this `paper <https://arxiv.org/abs/1506.04214>`__ .\n\n    Parameters\n    ----------\n    prev_layer : :class:`Layer`\n        Previous layer\n    cell_shape : tuple of int\n        The shape of each cell width * height\n    filter_size : tuple of int\n        The size of filter width * height\n    cell_fn : a convolutional RNN cell\n        Cell function like :class:`BasicConvLSTMCell`\n    feature_map : int\n        The number of feature map in the layer.\n    initializer : initializer\n        The initializer for initializing the parameters.\n    n_steps : int\n        The sequence length.\n    initial_state : None or ConvLSTM State\n        If None, `initial_state` is zero state.\n    return_last : boolean\n        Whether return last output or all outputs in each step.\n            - If True, return the last output, ""Sequence input and single output"".\n            - If False, return all outputs, ""Synced sequence input and output"".\n            - In other word, if you want to stack more RNNs on this layer, set to False.\n\n    return_seq_2d : boolean\n        Only consider this argument when `return_last_output` is `False`\n            - If True, return 2D Tensor [n_example, n_hidden], for stacking DenseLayer after it.\n            - If False, return 3D Tensor [n_example/n_steps, n_steps, n_hidden], for stacking multiple RNN after it.\n\n    name : str\n        A unique layer name.\n\n    Attributes\n    ----------\n    outputs : tensor\n        The output of this RNN. return_last_output = False, outputs = all cell_output, which is the hidden state.\n        cell_output.get_shape() = (?, h, w, c])\n\n    final_state : tensor or StateTuple\n        The finial state of this layer.\n            - When state_is_tuple = False, it is the final hidden and cell states,\n            - When state_is_tuple = True, You can get the final state after each iteration during training, then feed it to the initial state of next iteration.\n\n    initial_state : tensor or StateTuple\n        It is the initial state of this ConvLSTM layer, you can use it to initialize\n        your state at the beginning of each epoch or iteration according to your\n        training procedure.\n\n    batch_size : int or tensor\n        Is int, if able to compute the batch_size, otherwise, tensor for ``?``.\n\n    """"""\n\n    @deprecated_alias(layer=\'prev_layer\', end_support_version=1.9)  # TODO remove this line for the 1.9 release\n    def __init__(\n            self,\n            prev_layer,\n            cell_shape=None,\n            feature_map=1,\n            filter_size=(3, 3),\n            cell_fn=BasicConvLSTMCell,\n            initializer=tf.compat.v1.initializers.random_uniform(-0.1, 0.1),\n            n_steps=5,\n            initial_state=None,\n            return_last=False,\n            return_seq_2d=False,\n            name=\'convlstm\',\n    ):\n        super(ConvLSTM, self).__init__(prev_layer=prev_layer, name=name)\n\n        logging.info(\n            ""ConvLSTM %s: feature_map: %d, n_steps: %d, ""\n            ""in_dim: %d %s, cell_fn: %s "" %\n            (self.name, feature_map, n_steps, self.inputs.get_shape().ndims, self.inputs.get_shape(), cell_fn.__name__)\n        )\n        # You can get the dimension by .get_shape() or ._shape, and check the\n        # dimension by .with_rank() as follow.\n        # self.inputs.get_shape().with_rank(2)\n        # self.inputs.get_shape().with_rank(3)\n\n        # Input dimension should be rank 5 [batch_size, n_steps(max), h, w, c]\n        try:\n            self.inputs.get_shape().with_rank(5)\n        except Exception:\n            raise Exception(\n                ""RNN : Input dimension should be rank 5 : [batch_size, n_steps, input_x, ""\n                ""input_y, feature_map]""\n            )\n\n        fixed_batch_size = self.inputs.get_shape().with_rank_at_least(1)[0]\n\n        if fixed_batch_size.value:\n            batch_size = fixed_batch_size.value\n            logging.info(""     RNN batch_size (concurrent processes): %d"" % batch_size)\n\n        else:\n            batch_size = array_ops.shape(self.inputs)[0]\n            logging.info(""     non specified batch_size, uses a tensor instead."")\n        self.batch_size = batch_size\n        outputs = []\n        self.cell = cell = cell_fn(shape=cell_shape, filter_size=filter_size, num_features=feature_map)\n\n        if initial_state is None:\n            self.initial_state = cell.zero_state(batch_size, dtype=LayersConfig.tf_dtype)\n        else:\n            self.initial_state = initial_state\n\n        state = self.initial_state\n\n        # with tf.variable_scope(""model"", reuse=None, initializer=initializer):\n        with tf.compat.v1.variable_scope(name, initializer=initializer) as vs:\n            for time_step in range(n_steps):\n                if time_step > 0: tf.compat.v1.get_variable_scope().reuse_variables()\n                (cell_output, state) = cell(self.inputs[:, time_step, :, :, :], state)\n                outputs.append(cell_output)\n\n            # Retrieve just the RNN variables.\n            # rnn_variables = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n            rnn_variables = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.VARIABLES, scope=vs.name)\n\n            logging.info("" n_params : %d"" % (len(rnn_variables)))\n\n            if return_last:\n                # 2D Tensor [batch_size, n_hidden]\n                self.outputs = outputs[-1]\n            else:\n                if return_seq_2d:\n                    # PTB tutorial: stack dense layer after that, or compute the cost from the output\n                    # 4D Tensor [n_example, h, w, c]\n                    self.outputs = tf.reshape(tf.concat(outputs, 1), [-1, cell_shape[0] * cell_shape[1] * feature_map])\n                else:\n                    # <akara>: stack more RNN layer after that\n                    # 5D Tensor [n_example/n_steps, n_steps, h, w, c]\n                    self.outputs = tf.reshape(\n                        tf.concat(outputs, 1), [-1, n_steps, cell_shape[0], cell_shape[1], feature_map]\n                    )\n\n        self.final_state = state\n\n        self._add_layers(self.outputs)\n        self._add_params(rnn_variables)\n\n\'\'\'\n\n\n# @tf.function\ndef retrieve_seq_length_op(data):\n    """"""An op to compute the length of a sequence from input shape of [batch_size, n_step(max), n_features],\n    it can be used when the features of padding (on right hand side) are all zeros.\n\n    Parameters\n    -----------\n    data : tensor\n        [batch_size, n_step(max), n_features] with zero padding on right hand side.\n\n    Examples\n    -----------\n    Single feature\n\n    >>> data = [[[1],[2],[0],[0],[0]],\n    >>>         [[1],[2],[3],[0],[0]],\n    >>>         [[1],[2],[6],[1],[0]]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> length = tl.layers.retrieve_seq_length_op(data)\n    [2 3 4]\n\n    Multiple features\n\n    >>> data = [[[1,2],[2,2],[1,2],[1,2],[0,0]],\n    >>>          [[2,3],[2,4],[3,2],[0,0],[0,0]],\n    >>>          [[3,3],[2,2],[5,3],[1,2],[0,0]]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> length = tl.layers.retrieve_seq_length_op(data)\n    [4 3 4]\n\n    References\n    ------------\n    Borrow from `TFlearn <https://github.com/tflearn/tflearn/blob/master/tflearn/layers/recurrent.py>`__.\n\n    """"""\n    with tf.name_scope(\'GetLength\'):\n        used = tf.sign(tf.reduce_max(input_tensor=tf.abs(data), axis=2))\n        length = tf.reduce_sum(input_tensor=used, axis=1)\n\n        return tf.cast(length, tf.int32)\n\n\n# @tf.function\ndef retrieve_seq_length_op2(data):\n    """"""An op to compute the length of a sequence, from input shape of [batch_size, n_step(max)],\n    it can be used when the features of padding (on right hand side) are all zeros.\n\n    Parameters\n    -----------\n    data : tensor\n        [batch_size, n_step(max)] with zero padding on right hand side.\n\n    Examples\n    -----------\n    >>> data = [[1,2,0,0,0],\n    >>>         [1,2,3,0,0],\n    >>>         [1,2,6,1,0]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> length = tl.layers.retrieve_seq_length_op2(data)\n    tensor([2 3 4])\n\n    """"""\n    return tf.reduce_sum(input_tensor=tf.cast(tf.greater(data, tf.zeros_like(data)), tf.int32), axis=1)\n\n\n# @tf.function\ndef retrieve_seq_length_op3(data, pad_val=0):\n    """"""An op to compute the length of a sequence, the data shape can be [batch_size, n_step(max)] or\n    [batch_size, n_step(max), n_features].\n\n    If the data has type of tf.string and pad_val is assigned as empty string (\'\'), this op will compute the\n    length of the string sequence.\n\n    Parameters\n    -----------\n    data : tensor\n        [batch_size, n_step(max)] or [batch_size, n_step(max), n_features] with zero padding on the right hand side.\n    pad_val:\n        By default 0. If the data is tf.string, please assign this as empty string (\'\')\n\n    Examples\n    -----------\n    >>> data = [[[1],[2],[0],[0],[0]],\n    >>>         [[1],[2],[3],[0],[0]],\n    >>>         [[1],[2],[6],[1],[0]]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> length = tl.layers.retrieve_seq_length_op3(data)\n    tensor([2, 3, 4])\n    >>> data = [[[1,2],[2,2],[1,2],[1,2],[0,0]],\n    >>>         [[2,3],[2,4],[3,2],[0,0],[0,0]],\n    >>>         [[3,3],[2,2],[5,3],[1,2],[0,0]]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> length = tl.layers.retrieve_seq_length_op3(data)\n    tensor([4, 3, 4])\n    >>> data = [[1,2,0,0,0],\n    >>>         [1,2,3,0,0],\n    >>>         [1,2,6,1,0]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> length = tl.layers.retrieve_seq_length_op3(data)\n    tensor([2, 3, 4])\n    >>> data = [[\'hello\',\'world\',\'\',\'\',\'\'],\n    >>>         [\'hello\',\'world\',\'tensorlayer\',\'\',\'\'],\n    >>>         [\'hello\',\'world\',\'tensorlayer\',\'2.0\',\'\']]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.string)\n    >>> length = tl.layers.retrieve_seq_length_op3(data, pad_val=\'\')\n    tensor([2, 3, 4])\n\n    """"""\n    data_shape_size = data.get_shape().ndims\n    if data_shape_size == 3:\n        return tf.reduce_sum(\n            input_tensor=tf.cast(tf.reduce_any(input_tensor=tf.not_equal(data, pad_val), axis=2), dtype=tf.int32),\n            axis=1\n        )\n    elif data_shape_size == 2:\n        return tf.reduce_sum(input_tensor=tf.cast(tf.not_equal(data, pad_val), dtype=tf.int32), axis=1)\n    elif data_shape_size == 1:\n        raise ValueError(""retrieve_seq_length_op3: data has wrong shape! Shape got "", data.get_shape().as_list())\n    else:\n        raise ValueError(\n            ""retrieve_seq_length_op3: handling data with num of dims %s hasn\'t been implemented!"" % (data_shape_size)\n        )\n\n\ndef target_mask_op(data, pad_val=0):\n    """""" Return the mask of the input sequence data based on the padding values.\n\n    Parameters\n    -----------\n    data : tf.Tensor\n        A tensor with 2 or 3 dimensions.\n    pad_val: int, float, string, etc\n        The value that represent padding. By default, 0. For tf.string, you may use empty string.\n\n    Examples\n    -----------\n    >>> data = [[\'hello\', \'world\', \'\', \'\', \'\'],\n    >>>         [\'hello\', \'world\', \'tensorlayer\', \'\', \'\'],\n    >>>         [\'hello\', \'world\', \'tensorlayer\', \'2.0\', \'\']]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.string)\n    >>> mask = tl.layers.target_mask_op(data, pad_val=\'\')\n    >>> print(mask)\n    tf.Tensor(\n    [[1 1 0 0 0]\n     [1 1 1 0 0]\n     [1 1 1 1 0]], shape=(3, 5), dtype=int32)\n    >>> data = [[[1], [0], [0], [0], [0]],\n    >>>         [[1], [2], [3], [0], [0]],\n    >>>         [[1], [2], [0], [1], [0]]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> mask = tl.layers.target_mask_op(data)\n    >>> print(mask)\n    tf.Tensor(\n    [[1 0 0 0 0]\n     [1 1 1 0 0]\n     [1 1 0 1 0]], shape=(3, 5), dtype=int32)\n    >>> data = [[[0,0],[2,2],[1,2],[1,2],[0,0]],\n    >>>         [[2,3],[2,4],[3,2],[1,0],[0,0]],\n    >>>         [[3,3],[0,1],[5,3],[1,2],[0,0]]]\n    >>> data = tf.convert_to_tensor(data, dtype=tf.float32)\n    >>> mask = tl.layers.target_mask_op(data)\n    >>> print(mask)\n    tf.Tensor(\n    [[0 1 1 1 0]\n     [1 1 1 1 0]\n     [1 1 1 1 0]], shape=(3, 5), dtype=int32)\n    """"""\n\n    if not isinstance(data, tf.Tensor):\n        raise AttributeError(""target_mask_op: the type of input data should be tf.Tensor but got %s."" % type(data))\n    data_shape_size = data.get_shape().ndims\n    if data_shape_size == 3:\n        return tf.cast(tf.reduce_any(input_tensor=tf.not_equal(data, pad_val), axis=2), dtype=tf.int32)\n    elif data_shape_size == 2:\n        return tf.cast(tf.not_equal(data, pad_val), dtype=tf.int32)\n    elif data_shape_size == 1:\n        raise ValueError(\n            ""target_mask_op: data_shape %s is not supported. ""\n            ""The shape of data should have 2 or 3 dims."" % (data.get_shape())\n        )\n    else:\n        raise ValueError(\n            ""target_mask_op: handling data_shape %s hasn\'t been implemented! ""\n            ""The shape of data should have 2 or 3 dims"" % (data.get_shape())\n        )\n'"
tensorlayer/layers/scale.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.initializers import constant\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'Scale\',\n]\n\n\nclass Scale(Layer):\n    """"""The :class:`Scale` class is to multiple a trainable scale value to the layer outputs. Usually be used on the output of binary net.\n\n    Parameters\n    ----------\n    init_scale : float\n        The initial value for the scale factor.\n    name : a str\n        A unique layer name.\n\n    Examples\n    ----------\n    >>> inputs = tl.layers.Input([8, 3])\n    >>> dense = tl.layers.Dense(n_units=10)(inputs)\n    >>> outputs = tl.layers.Scale(init_scale=0.5)(dense)\n    >>> model = tl.models.Model(inputs=inputs, outputs=[dense, outputs])\n    >>> dense_out, scale_out = model(data, is_train=True)\n\n    """"""\n\n    def __init__(\n        self,\n        init_scale=0.05,\n        name=\'scale\',\n    ):\n        super(Scale, self).__init__(name)\n        self.init_scale = init_scale\n\n        self.build((None, ))\n        self._built = True\n\n        logging.info(""Scale  %s: init_scale: %f"" % (self.name, self.init_scale))\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'init_scale={init_scale},\'\n        s += \'name={name}\'\n        s += "")""\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        self.scale = self._get_weights(""scale"", shape=[1], init=constant(value=self.init_scale))\n\n    # @tf.function\n    def forward(self, inputs):\n        outputs = inputs * self.scale\n        return outputs\n'"
tensorlayer/layers/shape.py,11,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import flatten_reshape\n\n__all__ = [\n    \'Flatten\',\n    \'Reshape\',\n    \'Transpose\',\n    \'Shuffle\',\n]\n\n\nclass Flatten(Layer):\n    """"""A layer that reshapes high-dimension input into a vector.\n\n    Then we often apply Dense, RNN, Concat and etc on the top of a flatten layer.\n    [batch_size, mask_row, mask_col, n_mask] ---> [batch_size, mask_row * mask_col * n_mask]\n\n    Parameters\n    ----------\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    >>> x = tl.layers.Input([8, 4, 3], name=\'input\')\n    >>> y = tl.layers.Flatten(name=\'flatten\')(x)\n    [8, 12]\n\n    """"""\n\n    def __init__(self, name=None):  #\'flatten\'):\n        super(Flatten, self).__init__(name)\n\n        self.build()\n        self._built = True\n\n        logging.info(""Flatten %s:"" % (self.name))\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        outputs = flatten_reshape(inputs, name=self.name)\n        return outputs\n\n\nclass Reshape(Layer):\n    """"""A layer that reshapes a given tensor.\n\n    Parameters\n    ----------\n    shape : tuple of int\n        The output shape, see ``tf.reshape``.\n    name : str\n        A unique layer name.\n\n    Examples\n    --------\n    >>> x = tl.layers.Input([8, 4, 3], name=\'input\')\n    >>> y = tl.layers.Reshape(shape=[-1, 12], name=\'reshape\')(x)\n    (8, 12)\n\n    """"""\n\n    def __init__(self, shape, name=None):  #\'reshape\'):\n        super(Reshape, self).__init__(name)\n        self.shape = shape\n\n        logging.info(""Reshape %s"" % (self.name))\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'shape={shape},\'\n        s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        outputs = tf.reshape(inputs, shape=self.shape, name=self.name)\n        return outputs\n\n\nclass Transpose(Layer):\n    """"""A layer that transposes the dimension of a tensor.\n\n    See `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ .\n\n    Parameters\n    ----------\n    perm: list of int\n        The permutation of the dimensions, similar with ``numpy.transpose``.\n        If None, it is set to (n-1...0), where n is the rank of the input tensor.\n    conjugate: bool\n        By default False. If True, returns the complex conjugate of complex numbers (and transposed)\n        For example [[1+1j, 2+2j]] --> [[1-1j], [2-2j]]\n    name : str\n        A unique layer name.\n\n    Examples\n    ----------\n    >>> x = tl.layers.Input([8, 4, 3], name=\'input\')\n    >>> y = tl.layers.Transpose(perm=[0, 2, 1], conjugate=False, name=\'trans\')(x)\n    (8, 3, 4)\n\n    """"""\n\n    def __init__(self, perm=None, conjugate=False, name=None):  #\'transpose\'):\n        super(Transpose, self).__init__(name)\n        self.perm = perm\n        self.conjugate = conjugate\n\n        logging.info(""Transpose  %s: perm: %s, conjugate: %s"" % (self.name, self.perm, self.conjugate))\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'perm={perm},\'\n        s += \'conjugate={conjugate},\'\n        s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        outputs = tf.transpose(a=inputs, perm=self.perm, conjugate=self.conjugate, name=self.name)\n        return outputs\n\n\nclass Shuffle(Layer):\n    """"""A layer that shuffle a 2D image [batch, height, width, channel], see `here <https://arxiv.org/abs/1707.01083>`__.\n\n    Parameters\n    ----------\n    group: int\n        The number of groups.\n    name : str\n        A unique layer name.\n\n    Examples\n    --------\n    >>> x = tl.layers.Input([1, 16, 16, 8], name=\'input\')\n    >>> y = tl.layers.Shuffle(group=2, name=\'shuffle\')(x)\n    (1, 16, 16, 8)\n\n    """"""\n\n    def __init__(self, group, name=None):  #\'reshape\'):\n        super(Shuffle, self).__init__(name)\n        self.group = group\n\n        logging.info(""Shuffle %s"" % (self.name))\n\n        self.build()\n        self._built = True\n\n    def __repr__(self):\n        s = \'{classname}(\'\n        s += \'group={group},\'\n        s += \'name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape=None):\n        pass\n\n    # @tf.function\n    def forward(self, inputs):\n        in_shape = inputs.get_shape().as_list()\n        h, w, in_channel = in_shape[1:]\n        if in_channel % self.group != 0:\n            raise ValueError(\n                ""The in_channel must be a multiple of the number of groups. The in_channel got %d and the number of groups is %d.""\n                % (in_channel, self.group)\n            )\n        temp = tf.reshape(inputs, [-1, h, w, in_channel // self.group, self.group])\n        temp = tf.transpose(temp, [0, 1, 2, 4, 3])\n        outputs = tf.reshape(temp, [-1, h, w, in_channel], name=self.name)\n        return outputs\n'"
tensorlayer/layers/spatial_transformer.py,64,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import xrange\nfrom tensorflow.python.ops import array_ops\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import flatten_reshape\n\n# from tensorlayer.layers.core import LayersConfig\n# from tensorlayer.layers.core import TF_GRAPHKEYS_VARIABLES\n\n__all__ = [\n    \'transformer\',\n    \'batch_transformer\',\n    \'SpatialTransformer2dAffine\',\n]\n\n\ndef transformer(U, theta, out_size, name=\'SpatialTransformer2dAffine\'):\n    """"""Spatial Transformer Layer for `2D Affine Transformation <https://en.wikipedia.org/wiki/Affine_transformation>`__\n    , see :class:`SpatialTransformer2dAffine` class.\n\n    Parameters\n    ----------\n    U : list of float\n        The output of a convolutional net should have the\n        shape [num_batch, height, width, num_channels].\n    theta: float\n        The output of the localisation network should be [num_batch, 6], value range should be [0, 1] (via tanh).\n    out_size: tuple of int\n        The size of the output of the network (height, width)\n    name: str\n        Optional function name\n\n    Returns\n    -------\n    Tensor\n        The transformed tensor.\n\n    References\n    ----------\n    - `Spatial Transformer Networks <https://arxiv.org/abs/1506.02025>`__\n    - `TensorFlow/Models <https://github.com/tensorflow/models/tree/master/transformer>`__\n\n    Notes\n    -----\n    To initialize the network to the identity transform init.\n\n    >>> import tensorflow as tf\n    >>> # ``theta`` to\n    >>> identity = np.array([[1., 0., 0.], [0., 1., 0.]])\n    >>> identity = identity.flatten()\n    >>> theta = tf.Variable(initial_value=identity)\n\n    """"""\n\n    def _repeat(x, n_repeats):\n        rep = tf.transpose(a=tf.expand_dims(tf.ones(shape=tf.stack([\n            n_repeats,\n        ])), 1), perm=[1, 0])\n        rep = tf.cast(rep, \'int32\')\n        x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n        return tf.reshape(x, [-1])\n\n    def _interpolate(im, x, y, out_size):\n        # constants\n        num_batch = tf.shape(input=im)[0]\n        height = tf.shape(input=im)[1]\n        width = tf.shape(input=im)[2]\n        channels = tf.shape(input=im)[3]\n\n        x = tf.cast(x, \'float32\')\n        y = tf.cast(y, \'float32\')\n        height_f = tf.cast(height, \'float32\')\n        width_f = tf.cast(width, \'float32\')\n        out_height = out_size[0]\n        out_width = out_size[1]\n        zero = tf.zeros([], dtype=\'int32\')\n        max_y = tf.cast(tf.shape(input=im)[1] - 1, \'int32\')\n        max_x = tf.cast(tf.shape(input=im)[2] - 1, \'int32\')\n\n        # scale indices from [-1, 1] to [0, width/height]\n        x = (x + 1.0) * (width_f) / 2.0\n        y = (y + 1.0) * (height_f) / 2.0\n\n        # do sampling\n        x0 = tf.cast(tf.floor(x), \'int32\')\n        x1 = x0 + 1\n        y0 = tf.cast(tf.floor(y), \'int32\')\n        y1 = y0 + 1\n\n        x0 = tf.clip_by_value(x0, zero, max_x)\n        x1 = tf.clip_by_value(x1, zero, max_x)\n        y0 = tf.clip_by_value(y0, zero, max_y)\n        y1 = tf.clip_by_value(y1, zero, max_y)\n        dim2 = width\n        dim1 = width * height\n        base = _repeat(tf.range(num_batch) * dim1, out_height * out_width)\n        base_y0 = base + y0 * dim2\n        base_y1 = base + y1 * dim2\n        idx_a = base_y0 + x0\n        idx_b = base_y1 + x0\n        idx_c = base_y0 + x1\n        idx_d = base_y1 + x1\n\n        # use indices to lookup pixels in the flat image and restore\n        # channels dim\n        im_flat = tf.reshape(im, tf.stack([-1, channels]))\n        im_flat = tf.cast(im_flat, \'float32\')\n        Ia = tf.gather(im_flat, idx_a)\n        Ib = tf.gather(im_flat, idx_b)\n        Ic = tf.gather(im_flat, idx_c)\n        Id = tf.gather(im_flat, idx_d)\n\n        # and finally calculate interpolated values\n        x0_f = tf.cast(x0, \'float32\')\n        x1_f = tf.cast(x1, \'float32\')\n        y0_f = tf.cast(y0, \'float32\')\n        y1_f = tf.cast(y1, \'float32\')\n        wa = tf.expand_dims(((x1_f - x) * (y1_f - y)), 1)\n        wb = tf.expand_dims(((x1_f - x) * (y - y0_f)), 1)\n        wc = tf.expand_dims(((x - x0_f) * (y1_f - y)), 1)\n        wd = tf.expand_dims(((x - x0_f) * (y - y0_f)), 1)\n        output = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n        return output\n\n    def _meshgrid(height, width):\n        # This should be equivalent to:\n        #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n        #                         np.linspace(-1, 1, height))\n        #  ones = np.ones(np.prod(x_t.shape))\n        #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n        x_t = tf.matmul(\n            tf.ones(shape=tf.stack([height, 1])),\n            tf.transpose(a=tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), perm=[1, 0])\n        )\n        y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1), tf.ones(shape=tf.stack([1, width])))\n\n        x_t_flat = tf.reshape(x_t, (1, -1))\n        y_t_flat = tf.reshape(y_t, (1, -1))\n\n        ones = tf.ones_like(x_t_flat)\n        grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])\n        return grid\n\n    def _transform(theta, input_dim, out_size):\n        num_batch = tf.shape(input=input_dim)[0]\n        num_channels = tf.shape(input=input_dim)[3]\n        theta = tf.reshape(theta, (-1, 2, 3))\n        theta = tf.cast(theta, \'float32\')\n\n        # grid of (x_t, y_t, 1), eq (1) in ref [1]\n        out_height = out_size[0]\n        out_width = out_size[1]\n        grid = _meshgrid(out_height, out_width)\n        grid = tf.expand_dims(grid, 0)\n        grid = tf.reshape(grid, [-1])\n        grid = tf.tile(grid, tf.stack([num_batch]))\n        grid = tf.reshape(grid, tf.stack([num_batch, 3, -1]))\n\n        # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)\n        T_g = tf.matmul(theta, grid)\n        x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])\n        y_s = tf.slice(T_g, [0, 1, 0], [-1, 1, -1])\n        x_s_flat = tf.reshape(x_s, [-1])\n        y_s_flat = tf.reshape(y_s, [-1])\n\n        input_transformed = _interpolate(input_dim, x_s_flat, y_s_flat, out_size)\n\n        output = tf.reshape(input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))\n        return output\n\n    output = _transform(theta, U, out_size)\n    return output\n\n\ndef batch_transformer(U, thetas, out_size, name=\'BatchSpatialTransformer2dAffine\'):\n    """"""Batch Spatial Transformer function for `2D Affine Transformation <https://en.wikipedia.org/wiki/Affine_transformation>`__.\n\n    Parameters\n    ----------\n    U : list of float\n        tensor of inputs [batch, height, width, num_channels]\n    thetas : list of float\n        a set of transformations for each input [batch, num_transforms, 6]\n    out_size : list of int\n        the size of the output [out_height, out_width]\n    name : str\n        optional function name\n\n    Returns\n    ------\n    float\n        Tensor of size [batch * num_transforms, out_height, out_width, num_channels]\n\n    """"""\n    with tf.compat.v1.variable_scope(name):\n        num_batch, num_transforms = map(int, thetas.get_shape().as_list()[:2])\n        indices = [[i] * num_transforms for i in xrange(num_batch)]\n        input_repeated = tf.gather(U, tf.reshape(indices, [-1]))\n        return transformer(input_repeated, thetas, out_size)\n\n\nclass SpatialTransformer2dAffine(Layer):\n    """"""The :class:`SpatialTransformer2dAffine` class is a 2D `Spatial Transformer Layer <https://arxiv.org/abs/1506.02025>`__ for\n    `2D Affine Transformation <https://en.wikipedia.org/wiki/Affine_transformation>`__.\n\n    Parameters\n    -----------\n    out_size : tuple of int or None\n        - The size of the output of the network (height, width), the feature maps will be resized by this.\n    in_channels : int\n        The number of in channels.\n    data_format : str\n        ""channel_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    name : str\n        - A unique layer name.\n\n    References\n    -----------\n    - `Spatial Transformer Networks <https://arxiv.org/abs/1506.02025>`__\n    - `TensorFlow/Models <https://github.com/tensorflow/models/tree/master/transformer>`__\n\n    """"""\n\n    def __init__(\n        self,\n        out_size=(40, 40),\n        in_channels=None,\n        data_format=\'channel_last\',\n        name=None,\n    ):\n        super(SpatialTransformer2dAffine, self).__init__(name)\n\n        self.in_channels = in_channels\n        self.out_size = out_size\n        self.data_format = data_format\n        if self.in_channels is not None:\n            self.build(self.in_channels)\n            self._built = True\n\n        logging.info(""SpatialTransformer2dAffine %s"" % self.name)\n\n    def __repr__(self):\n        s = \'{classname}(out_size={out_size}, \'\n        if self.in_channels is not None:\n            s += \'in_channels=\\\'{in_channels}\\\'\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.in_channels is None and len(inputs_shape) != 2:\n            raise AssertionError(""The dimension of theta layer input must be rank 2, please reshape or flatten it"")\n        if self.in_channels:\n            shape = [self.in_channels, 6]\n        else:\n            # self.in_channels = inputs_shape[1]    # BUG\n            # shape = [inputs_shape[1], 6]\n            self.in_channels = inputs_shape[0][-1]  # zsdonghao\n            shape = [self.in_channels, 6]\n        self.W = self._get_weights(""weights"", shape=tuple(shape), init=tl.initializers.Zeros())\n        identity = np.reshape(np.array([[1, 0, 0], [0, 1, 0]], dtype=np.float32), newshape=(6, ))\n        self.b = self._get_weights(""biases"", shape=(6, ), init=tl.initializers.Constant(identity))\n\n    def forward(self, inputs):\n        """"""\n        :param inputs: a tuple (theta_input, U).\n                    - theta_input is of size [batch, in_channels]. We will use a :class:`Dense` to\n                    make the theta size to [batch, 6], value range to [0, 1] (via tanh).\n                    - U is the previous layer, which the affine transformation is applied to.\n        :return: tensor of size [batch, out_size[0], out_size[1], n_channels] after affine transformation,\n                    n_channels is identical to that of U.\n        """"""\n        theta_input, U = inputs\n        theta = tf.nn.tanh(tf.matmul(theta_input, self.W) + self.b)\n        outputs = transformer(U, theta, out_size=self.out_size)\n        # automatically set batch_size and channels\n        # e.g. [?, 40, 40, ?] --> [64, 40, 40, 1] or [64, 20, 20, 4]\n        batch_size = theta_input.shape[0]\n        n_channels = U.shape[-1]\n        if self.data_format == \'channel_last\':\n            outputs = tf.reshape(outputs, shape=[batch_size, self.out_size[0], self.out_size[1], n_channels])\n        else:\n            raise Exception(""unimplement data_format {}"".format(self.data_format))\n        return outputs\n'"
tensorlayer/layers/stack.py,4,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'Stack\',\n    \'UnStack\',\n]\n\n\nclass Stack(Layer):\n    """"""\n    The :class:`Stack` class is a layer for stacking a list of rank-R tensors into one rank-(R+1) tensor, see `tf.stack() <https://www.tensorflow.org/api_docs/python/tf/stack>`__.\n\n    Parameters\n    ----------\n    axis : int\n        New dimension along which to stack.\n    name : str\n        A unique layer name.\n\n    Examples\n    ---------\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> ni = tl.layers.Input([None, 784], name=\'input\')\n    >>> net1 = tl.layers.Dense(10, name=\'dense1\')(ni)\n    >>> net2 = tl.layers.Dense(10, name=\'dense2\')(ni)\n    >>> net3 = tl.layers.Dense(10, name=\'dense3\')(ni)\n    >>> net = tl.layers.Stack(axis=1, name=\'stack\')([net1, net2, net3])\n    (?, 3, 10)\n\n    """"""\n\n    def __init__(\n        self,\n        axis=1,\n        name=None,  #\'stack\',\n    ):\n        super().__init__(name)\n        self.axis = axis\n\n        self.build(None)\n        self._built = True\n        logging.info(""Stack %s: axis: %d"" % (self.name, self.axis))\n\n    def __repr__(self):\n        s = \'{classname}(axis={axis}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        pass\n\n    def forward(self, inputs):\n        outputs = tf.stack(inputs, axis=self.axis, name=self.name)\n        return outputs\n\n\nclass UnStack(Layer):\n    """"""\n    The :class:`UnStack` class is a layer for unstacking the given dimension of a rank-R tensor into rank-(R-1) tensors., see `tf.unstack() <https://www.tensorflow.org/api_docs/python/tf/unstack>`__.\n\n    Parameters\n    ----------\n    num : int or None\n        The length of the dimension axis. Automatically inferred if None (the default).\n    axis : int\n        Dimension along which axis to concatenate.\n    name : str\n        A unique layer name.\n\n    Returns\n    -------\n    list of :class:`Layer`\n        The list of layer objects unstacked from the input.\n\n    Examples\n    --------\n    >>> ni = Input([4, 10], name=\'input\')\n    >>> nn = Dense(n_units=5)(ni)\n    >>> nn = UnStack(axis=1)(nn)  # unstack in channel axis\n    >>> len(nn)  # 5\n    >>> nn[0].shape  # (4,)\n\n    """"""\n\n    def __init__(self, num=None, axis=0, name=None):  #\'unstack\'):\n        super().__init__(name)\n        self.num = num\n        self.axis = axis\n\n        self.build(None)\n        self._built = True\n        logging.info(""UnStack %s: num: %s axis: %d"" % (self.name, self.num, self.axis))\n\n    def __repr__(self):\n        s = \'{classname}(num={num}, axis={axis}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        pass\n\n    def forward(self, inputs):\n        outputs = tf.unstack(inputs, num=self.num, axis=self.axis, name=self.name)\n        return outputs\n'"
tensorlayer/layers/utils.py,50,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn_cell import LSTMStateTuple\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated, deprecated_alias\n\n__all__ = [\n    \'cabs\',\n    \'compute_alpha\',\n    \'flatten_reshape\',\n    \'get_collection_trainable\',\n    \'get_layers_with_name\',\n    \'get_variables_with_name\',\n    \'initialize_global_variables\',\n    \'initialize_rnn_state\',\n    \'list_remove_repeat\',\n    \'merge_networks\',\n    \'print_all_variables\',\n    \'quantize\',\n    \'quantize_active\',\n    \'quantize_weight\',\n    \'quantize_active_overflow\',\n    \'quantize_weight_overflow\',\n    \'set_name_reuse\',\n    \'ternary_operation\',\n]\n\n########## Module Public Functions ##########\n\n\ndef cabs(x):\n    return tf.minimum(1.0, tf.abs(x), name=\'cabs\')\n\n\ndef compute_alpha(x):\n    """"""Computing the scale parameter.""""""\n    threshold = _compute_threshold(x)\n    alpha1_temp1 = tf.where(tf.greater(x, threshold), x, tf.zeros_like(x, tf.float32))\n    alpha1_temp2 = tf.where(tf.less(x, -threshold), x, tf.zeros_like(x, tf.float32))\n    alpha_array = tf.add(alpha1_temp1, alpha1_temp2, name=None)\n    alpha_array_abs = tf.abs(alpha_array)\n    alpha_array_abs1 = tf.where(\n        tf.greater(alpha_array_abs, 0), tf.ones_like(alpha_array_abs, tf.float32),\n        tf.zeros_like(alpha_array_abs, tf.float32)\n    )\n    alpha_sum = tf.reduce_sum(input_tensor=alpha_array_abs)\n    n = tf.reduce_sum(input_tensor=alpha_array_abs1)\n    # alpha = tf.compat.v1.div(alpha_sum, n)\n    alpha = tf.math.divide(alpha_sum, n)\n    return alpha\n\n\ndef flatten_reshape(variable, name=\'flatten\'):\n    """"""Reshapes a high-dimension vector input.\n\n    [batch_size, mask_row, mask_col, n_mask] ---> [batch_size, mask_row x mask_col x n_mask]\n\n    Parameters\n    ----------\n    variable : TensorFlow variable or tensor\n        The variable or tensor to be flatten.\n    name : str\n        A unique layer name.\n\n    Returns\n    -------\n    Tensor\n        Flatten Tensor\n\n    """"""\n    dim = 1\n    for d in variable.get_shape()[1:].as_list():\n        dim *= d\n    return tf.reshape(variable, shape=[-1, dim], name=name)\n\n\ndef get_collection_trainable(name=\'\'):\n    variables = []\n    for p in tf.compat.v1.trainable_variables():\n        # print(p.name.rpartition(\'/\')[0], self.name)\n        if p.name.rpartition(\'/\')[0] == name:\n            variables.append(p)\n    return variables\n\n\n@deprecated_alias(printable=\'verbose\', end_support_version=1.9)  # TODO remove this line for the 1.9 release\ndef get_layers_with_name(net, name="""", verbose=False):\n    """"""Get a list of layers\' output in a network by a given name scope.\n\n    Parameters\n    -----------\n    net : :class:`Layer`\n        The last layer of the network.\n    name : str\n        Get the layers\' output that contain this name.\n    verbose : boolean\n        If True, print information of all the layers\' output\n\n    Returns\n    --------\n    list of Tensor\n        A list of layers\' output (TensorFlow tensor)\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> layers = tl.layers.get_layers_with_name(net, ""CNN"", True)\n\n    """"""\n    logging.info(""  [*] geting layers with %s"" % name)\n\n    layers = []\n    i = 0\n\n    for layer in net.all_layers:\n        # logging.info(type(layer.name))\n        if name in layer.name:\n            layers.append(layer)\n\n            if verbose:\n                logging.info(""  got {:3}: {:15}   {}"".format(i, layer.name, str(layer.get_shape())))\n                i = i + 1\n\n    return layers\n\n\ndef get_variable_with_initializer(scope_name, var_name, shape, init=tl.initializers.random_normal()):\n    # FIXME: documentation needed\n    # if tf.executing_eagerly():\n    var_name = scope_name + ""/"" + var_name\n    # if init_args is not None and len(init_args) != 0:\n    #     initial_value = init(**init_args)(shape=shape)\n    # else:\n    #     initial_value = init()(shape=shape)\n    # var = tf.Variable(initial_value=initial_value, name=var_name)\n    # FIXME: not sure whether this is correct?\n    initial_value = init(shape=shape)\n    var = tf.Variable(initial_value=initial_value, name=var_name)  #, **init_args)\n\n    # else:\n    #     with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE):\n    #         var = tf.get_variable(name=var_name, initializer=tf.zeros(shape), trainable=train)\n    return var\n\n\n@deprecated_alias(printable=\'verbose\', end_support_version=1.9)  # TODO remove this line for the 1.9 release\ndef get_variables_with_name(name=None, train_only=True, verbose=False):\n    """"""Get a list of TensorFlow variables by a given name scope.\n\n    Parameters\n    ----------\n    name : str\n        Get the variables that contain this name.\n    train_only : boolean\n        If Ture, only get the trainable variables.\n    verbose : boolean\n        If True, print the information of all variables.\n\n    Returns\n    -------\n    list of Tensor\n        A list of TensorFlow variables\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> dense_vars = tl.layers.get_variables_with_name(\'dense\', True, True)\n\n    """"""\n    if name is None:\n        raise Exception(""please input a name"")\n\n    logging.info(""  [*] geting variables with %s"" % name)\n\n    # tvar = tf.trainable_variables() if train_only else tf.all_variables()\n    if train_only:\n        t_vars = tf.compat.v1.trainable_variables()\n\n    else:\n        t_vars = tf.compat.v1.global_variables()\n\n    d_vars = [var for var in t_vars if name in var.name]\n\n    if verbose:\n        for idx, v in enumerate(d_vars):\n            logging.info(""  got {:3}: {:15}   {}"".format(idx, v.name, str(v.get_shape())))\n\n    return d_vars\n\n\n@deprecated(\n    date=""2018-09-30"", instructions=""This API is deprecated in favor of `sess.run(tf.global_variables_initializer())`""\n)\ndef initialize_global_variables(sess):\n    """"""Initialize the global variables of TensorFlow.\n\n    Run ``sess.run(tf.global_variables_initializer())`` for TF 0.12+ or\n    ``sess.run(tf.initialize_all_variables())`` for TF 0.11.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow session.\n\n    """"""\n    if sess is None:\n        raise AssertionError(\'The session must be defined\')\n\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n\ndef initialize_rnn_state(state, feed_dict=None):\n    """"""Returns the initialized RNN state.\n    The inputs are `LSTMStateTuple` or `State` of `RNNCells`, and an optional `feed_dict`.\n\n    Parameters\n    ----------\n    state : RNN state.\n        The TensorFlow\'s RNN state.\n    feed_dict : dictionary\n        Initial RNN state; if None, returns zero state.\n\n    Returns\n    -------\n    RNN state\n        The TensorFlow\'s RNN state.\n\n    """"""\n    if isinstance(state, LSTMStateTuple):\n        c = state.c.eval(feed_dict=feed_dict)\n        h = state.h.eval(feed_dict=feed_dict)\n        return c, h\n    else:\n        new_state = state.eval(feed_dict=feed_dict)\n        return new_state\n\n\ndef list_remove_repeat(x):\n    """"""Remove the repeated items in a list, and return the processed list.\n    You may need it to create merged layer like Concat, Elementwise and etc.\n\n    Parameters\n    ----------\n    x : list\n        Input\n\n    Returns\n    -------\n    list\n        A list that after removing it\'s repeated items\n\n    Examples\n    -------\n    >>> l = [2, 3, 4, 2, 3]\n    >>> l = list_remove_repeat(l)\n    [2, 3, 4]\n\n    """"""\n    y = []\n    for i in x:\n        if i not in y:\n            y.append(i)\n\n    return y\n\n\ndef merge_networks(layers=None):\n    """"""Merge all parameters, layers and dropout probabilities to a :class:`Layer`.\n    The output of return network is the first network in the list.\n\n    Parameters\n    ----------\n    layers : list of :class:`Layer`\n        Merge all parameters, layers and dropout probabilities to the first layer in the list.\n\n    Returns\n    --------\n    :class:`Layer`\n        The network after merging all parameters, layers and dropout probabilities to the first network in the list.\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> n1 = ...\n    >>> n2 = ...\n    >>> n1 = tl.layers.merge_networks([n1, n2])\n\n    """"""\n    if layers is None:\n        raise Exception(""layers should be a list of TensorLayer\'s Layers."")\n    layer = layers[0]\n\n    all_params = []\n    all_layers = []\n    all_drop = {}\n\n    for l in layers:\n        all_params.extend(l.all_params)\n        all_layers.extend(l.all_layers)\n        all_drop.update(l.all_drop)\n\n    layer.all_params = list(all_params)\n    layer.all_layers = list(all_layers)\n    layer.all_drop = dict(all_drop)\n\n    layer.all_layers = list_remove_repeat(layer.all_layers)\n    layer.all_params = list_remove_repeat(layer.all_params)\n\n    return layer\n\n\ndef print_all_variables(train_only=False):\n    """"""Print information of trainable or all variables,\n    without ``tl.layers.initialize_global_variables(sess)``.\n\n    Parameters\n    ----------\n    train_only : boolean\n        Whether print trainable variables only.\n            - If True, print the trainable variables.\n            - If False, print all variables.\n\n    """"""\n    # tvar = tf.trainable_variables() if train_only else tf.all_variables()\n    if train_only:\n        t_vars = tf.compat.v1.trainable_variables()\n        logging.info(""  [*] printing trainable variables"")\n\n    else:\n        t_vars = tf.compat.v1.global_variables()\n        logging.info(""  [*] printing global variables"")\n\n    for idx, v in enumerate(t_vars):\n        logging.info(""  var {:3}: {:15}   {}"".format(idx, str(v.get_shape()), v.name))\n\n\ndef quantize(x):\n    # ref: https://github.com/AngusG/tensorflow-xnor-bnn/blob/master/models/binary_net.py#L70\n    #  https://github.com/itayhubara/BinaryNet.tf/blob/master/nnUtils.py\n    with tf.compat.v1.get_default_graph().gradient_override_map({""Sign"": ""TL_Sign_QuantizeGrad""}):\n        return tf.sign(x)\n\n\ndef quantize_active(x, bitA):\n    if bitA == 32:\n        return x\n    return _quantize_dorefa(x, bitA)\n\n\ndef quantize_weight(x, bitW, force_quantization=False):\n    G = tf.compat.v1.get_default_graph()\n    if bitW == 32 and not force_quantization:\n        return x\n    if bitW == 1:  # BWN\n        with G.gradient_override_map({""Sign"": ""Identity""}):\n            E = tf.stop_gradient(tf.reduce_mean(input_tensor=tf.abs(x)))\n            return tf.sign(x / E) * E\n    x = tf.clip_by_value(x * 0.5 + 0.5, 0.0, 1.0)  # it seems as though most weights are within -1 to 1 region anyways\n    return 2 * _quantize_dorefa(x, bitW) - 1\n\n\ndef quantize_active_overflow(x, bitA):\n    if bitA == 32:\n        return x\n    return _quantize_overflow(x, bitA)\n\n\ndef quantize_weight_overflow(x, bitW):\n    if bitW == 32:\n        return x\n    return _quantize_overflow(x, bitW)\n\n\n@deprecated(date=""2018-06-30"", instructions=""TensorLayer relies on TensorFlow to check name reusing"")\ndef set_name_reuse(enable=True):\n    logging.warning(\'this method is DEPRECATED and has no effect, please remove it from your code.\')\n\n\ndef ternary_operation(x):\n    """"""Ternary operation use threshold computed with weights.""""""\n    g = tf.compat.v1.get_default_graph()\n    with g.gradient_override_map({""Sign"": ""Identity""}):\n        threshold = _compute_threshold(x)\n        x = tf.sign(tf.add(tf.sign(tf.add(x, threshold)), tf.sign(tf.add(x, -threshold))))\n        return x\n\n\n########## Module Private Functions ##########\n\n\n@tf.RegisterGradient(""TL_Sign_QuantizeGrad"")\ndef _quantize_grad(op, grad):\n    """"""Clip and binarize tensor using the straight through estimator (STE) for the gradient.""""""\n    return tf.clip_by_value(grad, -1, 1)\n\n\ndef _quantize_dorefa(x, k):\n    G = tf.compat.v1.get_default_graph()\n    n = float(2**k - 1)\n    with G.gradient_override_map({""Round"": ""Identity""}):\n        return tf.round(x * n) / n\n\n\ndef _quantize_overflow(x, k):\n    G = tf.compat.v1.get_default_graph()\n    n = float(2**k - 1)\n    max_value = tf.reduce_max(input_tensor=x)\n    min_value = tf.reduce_min(input_tensor=x)\n    with G.gradient_override_map({""Round"": ""Identity""}):\n        step = tf.stop_gradient((max_value - min_value) / n)\n        return tf.round((tf.maximum(tf.minimum(x, max_value), min_value) - min_value) / step) * step + min_value\n\n\ndef _compute_threshold(x):\n    """"""\n    ref: https://github.com/XJTUWYD/TWN\n    Computing the threshold.\n    """"""\n    x_sum = tf.reduce_sum(input_tensor=tf.abs(x), axis=None, keepdims=False, name=None)\n    # threshold = tf.compat.v1.div(x_sum, tf.cast(tf.size(input=x), tf.float32), name=None)\n    threshold = tf.math.divide(x_sum, tf.cast(tf.size(input=x), tf.float32), name=None)\n    threshold = tf.multiply(0.7, threshold, name=None)\n    return threshold\n'"
tensorlayer/logging/__init__.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nTensorLayer provides rich layer implementations trailed for\nvarious benchmarks and domain-specific problems. In addition, we also\nsupport transparent access to native TensorFlow parameters.\nFor example, we provide not only layers for local response normalization, but also\nlayers that allow user to apply ``tf.nn.lrn`` on ``network.outputs``.\nMore functions can be found in `TensorFlow API <https://www.tensorflow.org/versions/master/api_docs/index.html>`__.\n""""""\n\nfrom tensorlayer.lazy_imports import LazyImport\n\nfrom .tl_logging import *\n\n# Lazy Imports\ncontrib = LazyImport(""tensorlayer.logging.contrib"")\n\n__all__ = [\n    # tl_logging\n    \'DEBUG\',\n    \'debug\',\n    \'ERROR\',\n    \'error\',\n    \'FATAL\',\n    \'fatal\',\n    \'INFO\',\n    \'info\',\n    \'WARN\',\n    \'warn\',\n    \'warning\',\n    \'set_verbosity\',\n    \'get_verbosity\'\n]\n'"
tensorlayer/logging/tl_logging.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport logging as _logging\nimport os as _os\nimport sys as _sys\nimport threading\nimport time as _time\nfrom logging import DEBUG, ERROR, FATAL, INFO, WARN\n\nimport six\n\nfrom tensorlayer.decorators import deprecated\n\n__all__ = [\n    \'DEBUG\',\n    \'debug\',\n    \'ERROR\',\n    \'error\',\n    \'FATAL\',\n    \'fatal\',\n    \'INFO\',\n    \'info\',\n    \'WARN\',\n    \'warning\',\n    \'warn\',  # Deprecated\n    \'set_verbosity\',\n    \'get_verbosity\'\n]\n\n# Don\'t use this directly. Use _get_logger() instead.\n_logger = None\n_logger_lock = threading.Lock()\n\n_level_names = {\n    FATAL: \'FATAL\',\n    ERROR: \'ERROR\',\n    WARN: \'WARN\',\n    INFO: \'INFO\',\n    DEBUG: \'DEBUG\',\n}\n\n\ndef _get_logger():\n    global _logger\n\n    # Use double-checked locking to avoid taking lock unnecessarily.\n    if _logger is not None:\n        return _logger\n\n    _logger_lock.acquire()\n\n    try:\n        if _logger:\n            return _logger\n\n        # Scope the TensorFlow logger to not conflict with users\' loggers.\n        logger = _logging.getLogger(\'tensorlayer\')\n\n        # Don\'t further configure the TensorFlow logger if the root logger is\n        # already configured. This prevents double logging in those cases.\n        if not _logging.getLogger().handlers:\n            # Determine whether we are in an interactive environment\n            # This is only defined in interactive shells.\n            if hasattr(_sys, ""ps1""):\n                _interactive = True\n            else:\n                _interactive = _sys.flags.interactive\n\n            # If we are in an interactive environment (like Jupyter), set loglevel\n            # to INFO and pipe the output to stdout.\n            if _interactive:\n                logger.setLevel(INFO)\n                _logging_target = _sys.stdout\n            else:\n                _logging_target = _sys.stderr\n\n            # Add the output handler.\n            _handler = _logging.StreamHandler(_logging_target)\n            _handler.setFormatter(_logging.Formatter(\'[TL] %(message)s\'))\n            logger.addHandler(_handler)\n\n        _logger = logger\n        return _logger\n\n    finally:\n        _logger_lock.release()\n\n\ndef log(level, msg, *args, **kwargs):\n    _get_logger().log(level, msg, *args, **kwargs)\n\n\ndef debug(msg, *args, **kwargs):\n    _get_logger().debug(msg, *args, **kwargs)\n\n\ndef info(msg, *args, **kwargs):\n    _get_logger().info(msg, *args, **kwargs)\n\n\ndef error(msg, *args, **kwargs):\n    _get_logger().error(""ERROR: %s"" % msg, *args, **kwargs)\n\n\ndef fatal(msg, *args, **kwargs):\n    _get_logger().fatal(""FATAL: %s"" % msg, *args, **kwargs)\n\n\n@deprecated(date=""2018-09-30"", instructions=""This API is deprecated. Please use as `tl.logging.warning`"")\ndef warn(msg, *args, **kwargs):\n    warning(msg, *args, **kwargs)\n\n\ndef warning(msg, *args, **kwargs):\n    _get_logger().warning(""WARNING: %s"" % msg, *args, **kwargs)\n\n\n# Mask to convert integer thread ids to unsigned quantities for logging\n# purposes\n_THREAD_ID_MASK = 2 * _sys.maxsize + 1\n\n_log_prefix = None  # later set to google2_log_prefix\n\n# Counter to keep track of number of log entries per token.\n_log_counter_per_token = {}\n\n\ndef TaskLevelStatusMessage(msg):\n    error(msg)\n\n\ndef flush():\n    raise NotImplementedError()\n\n\ndef vlog(level, msg, *args, **kwargs):\n    _get_logger().log(level, msg, *args, **kwargs)\n\n\ndef _GetNextLogCountPerToken(token):\n    """"""Wrapper for _log_counter_per_token.\n\n    Args:\n    token: The token for which to look up the count.\n\n    Returns:\n    The number of times this function has been called with\n    *token* as an argument (starting at 0)\n    """"""\n    global _log_counter_per_token  # pylint: disable=global-variable-not-assigned\n    _log_counter_per_token[token] = 1 + _log_counter_per_token.get(token, -1)\n    return _log_counter_per_token[token]\n\n\ndef log_every_n(level, msg, n, *args):\n    """"""Log \'msg % args\' at level \'level\' once per \'n\' times.\n\n    Logs the 1st call, (N+1)st call, (2N+1)st call,  etc.\n    Not threadsafe.\n\n    Args:\n    level: The level at which to log.\n    msg: The message to be logged.\n    n: The number of times this should be called before it is logged.\n    *args: The args to be substituted into the msg.\n    """"""\n    count = _GetNextLogCountPerToken(_GetFileAndLine())\n    log_if(level, msg, not (count % n), *args)\n\n\ndef log_first_n(level, msg, n, *args):  # pylint: disable=g-bad-name\n    """"""Log \'msg % args\' at level \'level\' only first \'n\' times.\n\n    Not threadsafe.\n\n    Args:\n    level: The level at which to log.\n    msg: The message to be logged.\n    n: The number of times this should be called before it is logged.\n    *args: The args to be substituted into the msg.\n    """"""\n    count = _GetNextLogCountPerToken(_GetFileAndLine())\n    log_if(level, msg, count < n, *args)\n\n\ndef log_if(level, msg, condition, *args):\n    """"""Log \'msg % args\' at level \'level\' only if condition is fulfilled.""""""\n    if condition:\n        vlog(level, msg, *args)\n\n\ndef _GetFileAndLine():\n    """"""Returns (filename, linenumber) for the stack frame.""""""\n    # Use sys._getframe().  This avoids creating a traceback object.\n    # pylint: disable=protected-access\n    f = _sys._getframe()\n    # pylint: enable=protected-access\n    our_file = f.f_code.co_filename\n    f = f.f_back\n    while f:\n        code = f.f_code\n        if code.co_filename != our_file:\n            return (code.co_filename, f.f_lineno)\n        f = f.f_back\n    return (\'<unknown>\', 0)\n\n\ndef google2_log_prefix(level, timestamp=None, file_and_line=None):\n    """"""Assemble a logline prefix using the google2 format.""""""\n    # pylint: disable=global-variable-not-assigned\n    global _level_names\n    # pylint: enable=global-variable-not-assigned\n\n    # Record current time\n    now = timestamp or _time.time()\n    now_tuple = _time.localtime(now)\n    now_microsecond = int(1e6 * (now % 1.0))\n\n    (filename, line) = file_and_line or _GetFileAndLine()\n    basename = _os.path.basename(filename)\n\n    # Severity string\n    severity = \'I\'\n    if level in _level_names:\n        severity = _level_names[level][0]\n\n    s = \'%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] \' % (\n        severity,\n        now_tuple[1],  # month\n        now_tuple[2],  # day\n        now_tuple[3],  # hour\n        now_tuple[4],  # min\n        now_tuple[5],  # sec\n        now_microsecond,\n        _get_thread_id(),\n        basename,\n        line\n    )\n\n    return s\n\n\ndef get_verbosity():\n    """"""Return how much logging output will be produced.""""""\n    return _get_logger().getEffectiveLevel()\n\n\ndef set_verbosity(v):\n    """"""Sets the threshold for what messages will be logged.""""""\n    _get_logger().setLevel(v)\n\n\ndef _get_thread_id():\n    """"""Get id of current thread, suitable for logging as an unsigned quantity.""""""\n    # pylint: disable=protected-access\n    thread_id = six.moves._thread.get_ident()\n    # pylint:enable=protected-access\n    return thread_id & _THREAD_ID_MASK\n\n\n_log_prefix = google2_log_prefix\n'"
tensorlayer/models/__init__.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\n# """"""A collections of pre-defined well known models.""""""\n\nfrom .core import *\nfrom .mobilenetv1 import MobileNetV1\nfrom .resnet import ResNet50\nfrom .seq2seq import Seq2seq\nfrom .seq2seq_with_attention import Seq2seqLuongAttention\nfrom .squeezenetv1 import SqueezeNetV1\nfrom .vgg import *\n'"
tensorlayer/models/core.py,12,"b'import os\nfrom abc import abstractmethod\nfrom queue import Queue\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops as tf_ops\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.files import utils\nfrom tensorlayer.layers import Layer, ModelLayer\n\n__all__ = [\n    \'Model\',\n]\n\n_global_model_name_dict = {}  # TODO: better implementation?\n_global_model_name_set = set()\n\n\nclass Model(object):\n    """"""The :class:`Model` class represents a neural network.\n\n    It should be subclassed when implementing a dynamic model,\n    where \'forward\' method must be overwritten.\n    Otherwise, please specify \'inputs\' tensor(s) and \'outputs\' tensor(s)\n    to create a static model. In that case, \'inputs\' tensors should come\n    from tl.layers.Input().\n\n    Parameters\n    -----------\n    inputs : a Layer or list of Layer\n        The input(s) to the model.\n    outputs : a Layer or list of Layer\n        The output(s) to the model.\n    name : None or str\n        The name of the model.\n\n    Methods\n    ---------\n    __init__(self, inputs=None, outputs=None, name=None)\n        Initializing the Model.\n    inputs()\n        Get input tensors to this network (only avaiable for static model).\n    outputs()\n        Get output tensors to this network (only avaiable for static model).\n    __call__(inputs, is_train=None, **kwargs)\n        Forward input tensors through this network.\n    all_layers()\n        Get all layer objects of this network in a list of layers.\n    weights()\n        Get the weights of this network in a list of tensors.\n    train()\n        Set this network in training mode. (affect layers e.g. Dropout, BatchNorm).\n    eval()\n        Set this network in evaluation mode.\n    as_layer()\n        Set this network as a ModelLayer so that it can be integrated into another Model.\n    release_memory()\n        Release the memory that was taken up by tensors which are maintained by this network.\n    save_weights(self, filepath, format=\'hdf5\')\n        Save the weights of this network in a given format.\n    load_weights(self, filepath, format=None, in_order=True, skip=False)\n        Load weights into this network from a specified file.\n    save(self, filepath, save_weights=True)\n        Save the network with/without weights.\n    load(filepath, save_weights=True)\n        Load the network with/without weights.\n\n    Examples\n    ---------\n    >>> import tensorflow as tf\n    >>> import numpy as np\n    >>> from tensorlayer.layers import Input, Dense, Dropout\n    >>> from tensorlayer.models import Model\n\n    Define static model\n\n    >>> class CustomModel(Model):\n    >>>     def __init__(self):\n    >>>         super(CustomModel, self).__init__()\n    >>>         self.dense1 = Dense(n_units=800, act=tf.nn.relu, in_channels=784)\n    >>>         self.dropout1 = Dropout(keep=0.8)\n    >>>         self.dense2 = Dense(n_units=10, in_channels=800)\n    >>>     def forward(self, x):\n    >>>         z = self.dense1(x)\n    >>>         z = self.dropout1(z)\n    >>>         z = self.dense2(z)\n    >>>         return z\n    >>> M_dynamic = CustomModel()\n\n    Define static model\n\n    >>> ni = Input([None, 784])\n    >>> nn = Dense(n_units=800, act=tf.nn.relu)(ni)\n    >>> nn = Dropout(keep=0.8)(nn)\n    >>> nn = Dense(n_units=10, act=tf.nn.relu)(nn)\n    >>> M_static = Model(inputs=ni, outputs=nn, name=""mlp"")\n\n    Get network information\n\n    >>> print(M_static)\n    ... Model(\n    ...  (_inputlayer): Input(shape=[None, 784], name=\'_inputlayer\')\n    ...  (dense): Dense(n_units=800, relu, in_channels=\'784\', name=\'dense\')\n    ...  (dropout): Dropout(keep=0.8, name=\'dropout\')\n    ...  (dense_1): Dense(n_units=10, relu, in_channels=\'800\', name=\'dense_1\')\n    ... )\n\n    Forwarding through this network\n\n    >>> data = np.random.normal(size=[16, 784]).astype(np.float32)\n    >>> outputs_d = M_dynamic(data)\n    >>> outputs_s = M_static(data)\n\n    Save and load weights\n\n    >>> M_static.save_weights(\'./model_weights.h5\')\n    >>> M_static.load_weights(\'./model_weights.h5\')\n\n    Save and load the model\n\n    >>> M_static.save(\'./model.h5\')\n    >>> M = Model.load(\'./model.h5\')\n\n    Convert model to layer\n\n    >>> M_layer = M_static.as_layer()\n\n    """"""\n\n    @property\n    def inputs(self):\n        return self._inputs\n\n    @property\n    def outputs(self):\n        return self._outputs\n\n    def __init__(self, inputs=None, outputs=None, name=None):\n        """"""\n        Initializing the Model.\n\n        Parameters\n        ----------\n        inputs : Tensor or list of tensors\n            Input tensor(s), which must come from tl.layers.Input()\n        outputs : Tensor or list of tensors\n            Output tensor(s), which must be the output(s) of some TL layers\n        name : str or None\n            Name for this network\n        """"""\n        # Auto naming if the name is not given\n        self._NameNone = False\n        global _global_model_name_dict\n        global _global_model_name_set\n        if name is None:\n            self._NameNone = True\n            prefix = self.__class__.__name__.lower()\n            if _global_model_name_dict.get(prefix) is not None:\n                _global_model_name_dict[prefix] += 1\n                name = prefix + \'_\' + str(_global_model_name_dict[prefix])\n            else:\n                _global_model_name_dict[prefix] = 0\n                name = prefix\n            while name in _global_model_name_set:\n                _global_model_name_dict[prefix] += 1\n                name = prefix + \'_\' + str(_global_model_name_dict[prefix])\n            _global_model_name_set.add(name)\n        else:\n            if name in _global_model_name_set:\n                raise ValueError(\n                    \'Model name \\\'%s\\\' has already been used by another model. Please change the model name.\' % name\n                )\n            _global_model_name_set.add(name)\n            _global_model_name_dict[name] = 0\n\n        # Model properties\n        self.name = name\n\n        # Model state: train or test\n        self.is_train = None\n\n        # Model weights\n        self._all_weights = None\n        self._trainable_weights = None\n        self._nontrainable_weights = None\n\n        # Model args of all layers, ordered by all_layers\n        self._config = None\n\n        # Model inputs and outputs\n        # TODO: note that in dynamic network, inputs and outputs are both None, may cause problem, test needed\n        self._inputs = inputs\n        self._outputs = outputs\n\n        # Model converted into a Layer\n        self._model_layer = None\n\n        # Layer Node status\n        self._nodes_fixed = False\n\n        # Model layers\n        self._all_layers = None\n\n        if inputs is None and outputs is None:\n            pass\n\n        else:\n            # check type of inputs and outputs\n            check_order = [\'inputs\', \'outputs\']\n            for co, check_argu in enumerate([inputs, outputs]):\n                if isinstance(check_argu, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(check_argu):\n                    pass\n                elif isinstance(check_argu, list):\n                    if len(check_argu) == 0:\n                        raise ValueError(\n                            ""The argument `%s` is detected as an empty list. "" % check_order[co] +\n                            ""It should be either Tensor or a list of Tensor.""\n                        )\n                    for idx in range(len(check_argu)):\n                        if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like(\n                                check_argu[idx]):\n                            raise TypeError(\n                                ""The argument `%s` should be either Tensor or a list of Tensor "" % (check_order[co]) +\n                                ""but the %s[%d] is detected as %s"" % (check_order[co], idx, type(check_argu[idx]))\n                            )\n                else:\n                    raise TypeError(\n                        ""The argument `%s` should be either Tensor or a list of Tensor but received %s"" %\n                        (check_order[co], type(check_argu))\n                    )\n\n            if not _check_tl_layer_tensors(inputs):\n                raise TypeError(\n                    ""The argument `inputs` should be either Tensor or a list of Tensor ""\n                    ""that come from TensorLayer\'s Input layer: tl.layers.Input(shape). ""\n                )\n            if not _check_tl_layer_tensors(outputs):\n                raise TypeError(\n                    ""The argument `outputs` should be either Tensor or a list of Tensor ""\n                    ""that is/are outputs from some TensorLayer\'s layers, e.g. tl.layers.Dense, tl.layers.Conv2d.""\n                )\n\n            # build network graph\n            self._node_by_depth, self._all_layers = self._construct_graph()\n\n            self._fix_nodes_for_layers()\n\n    def __call__(self, inputs, is_train=None, **kwargs):\n        """"""Forward input tensors through this network by calling.\n\n        Parameters\n        ----------\n        inputs : Tensor or list of Tensors, numpy.ndarray of list of numpy.ndarray\n            Inputs for network forwarding\n        is_train : boolean\n            Network\'s mode for this time forwarding. If \'is_train\' == True, this network is set as training mode.\n            If \'is_train\' == False, this network is set as evaluation mode\n        kwargs :\n            For other keyword-only arguments.\n\n        """"""\n\n        self._check_mode(is_train)\n\n        # FIXME: this may cause inefficiency, this is used to check if every layer is built\n        self.all_layers\n\n        # fix LayerNodes when first calling\n        if self._nodes_fixed is False:\n            self._fix_nodes_for_layers()\n\n        # set training / inference mode if necessary\n        if is_train is not None:\n            self._set_mode_for_layers(is_train)\n\n        # if self._input is a list, then it must be a static network\n        if isinstance(self._inputs, list):\n            if not isinstance(inputs, list):\n                raise ValueError(""The argument `inputs` should be a list of values but detected as %s."" % type(inputs))\n            elif len(inputs) != len(self._inputs):\n                raise ValueError(\n                    ""The argument `inputs` should be a list with len=%d but detected as len=%d."" %\n                    (len(self._inputs), len(inputs))\n                )\n\n        # convert inputs to tensor if it is originally not\n        # FIXME: not sure convert_to_tensor here or ask user to do it\n        if isinstance(inputs, list):\n            for idx in range(len(inputs)):\n                inputs[idx] = tf.convert_to_tensor(inputs[idx])\n        else:\n            inputs = tf.convert_to_tensor(inputs)\n\n        return self.forward(inputs, **kwargs)\n\n    @abstractmethod\n    def forward(self, *inputs, **kwargs):\n        """"""Network forwarding given input tensors\n\n        Parameters\n        ----------\n        inputs : Tensor or list of Tensors\n            input tensor(s)\n        kwargs :\n            For other keyword-only arguments.\n\n        Returns\n        -------\n            output tensor(s) : Tensor or list of Tensor(s)\n\n        """"""\n        # FIXME: currently using self._outputs to judge static network or dynamic network\n        if self._outputs is None:\n            raise ValueError(\n                ""Outputs not defined. Please define inputs and outputs when the model is created. Or overwrite forward() function.""\n            )\n\n        memory = dict()\n\n        # get each layer\'s output by going through the graph in depth order\n        for depth, nodes in enumerate(self._node_by_depth):\n            if depth == 0:\n                if isinstance(self.inputs, list):\n                    assert len(inputs[0]) == len(nodes)\n                    for idx, node in enumerate(nodes):\n                        memory[node.name] = node(inputs[0][idx])\n                else:\n                    memory[nodes[0].name] = nodes[0](inputs[0])\n            else:\n                for node in nodes:\n                    in_nodes = node.in_nodes\n                    in_tensors_idxes = node.in_tensors_idxes\n                    if len(in_nodes) == 1:\n                        node_input = memory[in_nodes[0].name][in_tensors_idxes[0]]\n                    else:\n                        node_input = [memory[inode.name][idx] for inode, idx in zip(in_nodes, in_tensors_idxes)]\n                    memory[node.name] = node(node_input)\n\n        if not isinstance(self._outputs, list):\n            return memory[self._outputs._info[0].name][self._outputs._info[1]]\n        else:\n            return [memory[tensor._info[0].name][tensor._info[1]] for tensor in self._outputs]\n\n    @property\n    def all_layers(self):\n        """"""Return all layers of this network in a list.""""""\n        if self._all_layers is not None:\n            return self._all_layers\n\n        if self._inputs is not None and self._outputs is not None:\n            # static model\n            return self._all_layers\n        else:\n            # dynamic model\n            self._all_layers = list()\n            attr_list = [attr for attr in dir(self) if attr[:2] != ""__""]\n            attr_list.remove(""all_weights"")\n            attr_list.remove(""trainable_weights"")\n            attr_list.remove(""nontrainable_weights"")\n            attr_list.remove(""_all_weights"")\n            attr_list.remove(""_trainable_weights"")\n            attr_list.remove(""_nontrainable_weights"")\n            attr_list.remove(""all_layers"")\n            attr_list.remove(""_all_layers"")\n            attr_list.remove(""n_weights"")\n            for idx, attr in enumerate(attr_list):\n                try:\n                    if isinstance(getattr(self, attr), Layer):\n                        nowlayer = getattr(self, attr)\n                        if not nowlayer._built:\n                            raise AttributeError(""Layer %s not built yet."" % repr(nowlayer))\n                        self._all_layers.append(nowlayer)\n                    elif isinstance(getattr(self, attr), Model):\n                        nowmodel = getattr(self, attr)\n                        self._all_layers.append(nowmodel)\n                    elif isinstance(getattr(self, attr), list):\n                        self._all_layers.extend(_add_list_to_all_layers(getattr(self, attr)))\n                # TODO: define customised exception for TL\n                except AttributeError as e:\n                    raise e\n                except Exception:\n                    pass\n\n            # check layer name uniqueness\n            local_layer_name_dict = set()\n            for layer in self._all_layers:\n                if layer.name in local_layer_name_dict:\n                    raise ValueError(\n                        \'Layer name \\\'%s\\\' has already been used by another layer. Please change the layer name.\' %\n                        layer.name\n                    )\n                else:\n                    local_layer_name_dict.add(layer.name)\n            return self._all_layers\n\n    @property\n    def trainable_weights(self):\n        """"""Return trainable weights of this network in a list.""""""\n        if self._trainable_weights is not None and len(self._trainable_weights) > 0:\n            # self._trainable_weights already extracted, so do nothing\n            pass\n        else:\n            self._trainable_weights = []\n            for layer in self.all_layers:\n                if layer.trainable_weights is not None:\n                    self._trainable_weights.extend(layer.trainable_weights)\n\n        return self._trainable_weights.copy()\n\n    @property\n    def nontrainable_weights(self):\n        """"""Return nontrainable weights of this network in a list.""""""\n        if self._nontrainable_weights is not None and len(self._nontrainable_weights) > 0:\n            # self._nontrainable_weights already extracted, so do nothing\n            pass\n        else:\n            self._nontrainable_weights = []\n            for layer in self.all_layers:\n                if layer.nontrainable_weights is not None:\n                    self._nontrainable_weights.extend(layer.nontrainable_weights)\n\n        return self._nontrainable_weights.copy()\n\n    @property\n    def all_weights(self):\n        """"""Return all weights of this network in a list.""""""\n        if self._all_weights is not None and len(self._all_weights) > 0:\n            # self._all_weights already extracted, so do nothing\n            pass\n        else:\n            self._all_weights = []\n            for layer in self.all_layers:\n                if layer.all_weights is not None:\n                    self._all_weights.extend(layer.all_weights)\n\n        return self._all_weights.copy()\n\n    @property\n    def n_weights(self):\n        """"""Return the number of weights (parameters) in this network.""""""\n        n_weights = 0\n        for i, w in enumerate(self.all_weights):\n            n = 1\n            # for s in p.eval().shape:\n            for s in w.get_shape():\n                try:\n                    s = int(s)\n                except:\n                    s = 1\n                if s:\n                    n = n * s\n            n_weights = n_weights + n\n        # print(""num of weights (parameters) %d"" % n_weights)\n        return n_weights\n\n    @property\n    def config(self):\n        if self._config is not None and len(self._config) > 0:\n            return self._config\n        else:\n            # _config = []\n            _config = {}\n            if self._NameNone is True:\n                _config.update({""name"": None})\n            else:\n                _config.update({""name"": self.name})\n            version_info = {\n                ""tensorlayer_version"": tl.__version__,\n                ""backend"": ""tensorflow"",\n                ""backend_version"": tf.__version__,\n                ""training_device"": ""gpu"",\n                ""save_date"": None,\n            }\n            _config[""version_info""] = version_info\n            # if self.outputs is None:\n            #     raise RuntimeError(\n            #         ""Dynamic mode does not support config yet.""\n            #     )\n            model_architecture = []\n            for layer in self.all_layers:\n                model_architecture.append(layer.config)\n            _config[""model_architecture""] = model_architecture\n            if self.inputs is not None:\n                if not isinstance(self.inputs, list):\n                    _config.update({""inputs"": self.inputs._info[0].name})\n                else:\n                    config_inputs = []\n                    for config_input in self.inputs:\n                        config_inputs.append(config_input._info[0].name)\n                    _config.update({""inputs"": config_inputs})\n            if self.outputs is not None:\n                if not isinstance(self.outputs, list):\n                    _config.update({""outputs"": self.outputs._info[0].name})\n                else:\n                    config_outputs = []\n                    for config_output in self.outputs:\n                        config_outputs.append(config_output._info[0].name)\n                    _config.update({""outputs"": config_outputs})\n            if self._nodes_fixed or self.outputs is None:\n                self._config = _config\n\n            return _config\n\n    def train(self):\n        """"""Set this network in training mode. After calling this method,\n        all layers in network are in training mode, in particular, BatchNorm, Dropout, etc.\n\n        Examples\n        --------\n        >>> import tensorlayer as tl\n        >>> net = tl.models.vgg16()\n        >>> net.train()\n\n        """"""\n        if self.is_train !=True:\n            self.is_train = True\n            self._set_mode_for_layers(True)\n\n    def eval(self):\n        """"""Set this network in evaluation mode. After calling this method,\n        all layers in network are in evaluation mode, in particular, BatchNorm, Dropout, etc.\n\n        Examples\n        --------\n        >>> import tensorlayer as tl\n        >>> net = tl.models.vgg16()\n        >>> net.eval()\n        # do evaluation\n\n        """"""\n        if self.is_train != False:\n            self.is_train = False\n            self._set_mode_for_layers(False)\n\n    def test(self):\n        """"""Set this network in evaluation mode.""""""\n        self.eval()\n\n    def infer(self):\n        """"""Set this network in evaluation mode.""""""\n        self.eval()\n\n    def as_layer(self):\n        """"""Return this network as a ModelLayer so that it can be integrated into another Model.\n\n        Examples\n        --------\n        >>> from tensorlayer.layers import Input, Dense, Dropout\n        >>> from tensorlayer.models import Model\n        >>> ni = Input([None, 784])\n        >>> nn = Dense(n_units=800, act=tf.nn.relu)(ni)\n        >>> nn = Dropout(keep=0.8)(nn)\n        >>> nn = Dense(n_units=10, act=tf.nn.relu)(nn)\n        >>> M_hidden = Model(inputs=ni, outputs=nn, name=""mlp"").as_layer()\n        >>> nn = M_hidden(ni)   # use previously constructed model as layer\n        >>> nn = Dropout(keep=0.8)(nn)\n        >>> nn = Dense(n_units=10, act=tf.nn.relu)(nn)\n        >>> M_full = Model(inputs=ni, outputs=nn, name=""mlp"")\n\n        """"""\n        if self._outputs is None:\n            raise AttributeError(""Dynamic network cannot be converted to Layer."")\n\n        if self._model_layer is None:\n            self._model_layer = ModelLayer(self)\n\n        return self._model_layer\n\n    def _check_mode(self, is_train):\n        """"""Check whether this network is in a given mode.\n\n        Parameters\n        ----------\n        is_train : boolean\n            Network\'s mode. True means training mode while False means evaluation mode.\n\n        """"""\n        # contradiction test\n        if is_train is None and self.is_train is None:\n            raise ValueError(\n                ""Training / inference mode not defined. Argument `is_train` should be set as True / False. Otherwise please use `Model.train()` / `Model.eval()` to switch the mode.""\n            )\n        elif is_train is not None and self.is_train is not None:\n            if is_train == self.is_train:\n                logging.warning(\n                    ""Training / inference mode redefined redundantly. Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.""\n                )\n            else:\n                raise AttributeError(\n                    ""Training / inference mode mismatch. The argument `is_train` is set as %s, "" % is_train +\n                    ""but the mode is currently set as %s. "" %\n                    (\'Training by Model.train()\' if self.is_train else \'Inference by Model.eval()\') +\n                    ""Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.""\n                )\n\n    def _set_mode_for_layers(self, is_train):\n        """"""Set all layers of this network to a given mode.\n\n        Parameters\n        ----------\n        is_train : boolean\n            Network\'s mode. True means training mode while False means evaluation mode.\n\n        """"""\n        for layer in self.all_layers:\n            if isinstance(layer, Model):\n                layer.is_train = is_train\n            layer._set_mode_for_layers(is_train)\n\n    def _fix_nodes_for_layers(self):\n        """"""Fix each Layer\'s LayerNode to stop growing, see LayerNode for more.""""""\n        for layer in self.all_layers:\n            layer._fix_nodes_for_layers()\n        self._nodes_fixed = True\n\n    def __setattr__(self, key, value):\n        if isinstance(value, Layer):\n            if value._built is False:\n                raise AttributeError(\n                    ""The registered layer `{}` should be built in advance. ""\n                    ""Do you forget to pass the keyword argument \'in_channels\'? "".format(value.name)\n                )\n        super().__setattr__(key, value)\n\n    def __repr__(self):\n        # tmpstr = self.__class__.__name__ + \'(\\n\'\n        tmpstr = self.name + \'(\\n\'\n        for idx, layer in enumerate(self.all_layers):\n            modstr = layer.__repr__()\n            modstr = _addindent(modstr, 2)\n            tmpstr = tmpstr + \'  (\' + layer.name + \'): \' + modstr + \'\\n\'\n        tmpstr = tmpstr + \')\'\n        return tmpstr\n\n    ## raise Exceptions for old version codes\n    def print_all_layers(self):\n        raise Exception(""please change net.print_all_layers --> print(net)"")\n\n    def count_params(self, **kwargs):\n        raise Exception(""please change count_params --> count_weights"")\n\n    def print_params(self, **kwargs):\n        raise Exception(""please change print_params --> print_weights"")\n\n    @property\n    def all_params(self):\n        raise Exception(""please change all_params --> weights"")\n\n    @property\n    def all_drop(self):\n        raise Exception(""all_drop is deprecated"")\n\n    def get_layer(self, name=None, index=None):\n        """"""Network forwarding given input tensors\n\n        Parameters\n        ----------\n        name : str or None\n            Name of the requested layer. Default None.\n        index : int or None\n            Index of the requested layer. Default None.\n\n        Returns\n        -------\n            layer : The requested layer\n\n        Notes\n        -----\n        Either a layer name or a layer index should be given.\n\n        """"""\n        if index is not None:\n            if len(self.all_layers) <= index:\n                raise ValueError(\n                    \'model only has \' + str(len(self.all_layers)) + \' layers, but \' + str(index) +\n                    \'-th layer is requested.\'\n                )\n            else:\n                return self.all_layers[index]\n        elif name is not None:\n            for layer in self.all_layers:\n                if layer.name == name:\n                    return layer\n            raise ValueError(\'Model has no layer named \' + name + \'.\')\n        else:\n            raise ValueError(\'Either a layer name or a layer index should be given.\')\n\n    def _construct_graph(self):\n        """"""construct computation graph for static model using LayerNode object""""""\n        all_layers = []\n        node_by_depth = []  # [[node0, node1], [node2, node3], ...]\n\n        input_tensors_list = self.inputs if isinstance(self.inputs, list) else [self.inputs]\n\n        queue_node = Queue()\n\n        # BFS to visit all nodes that should be involved in the computation graph\n        output_tensors_list = self.outputs if isinstance(self.outputs, list) else [self.outputs]\n        output_nodes = [tensor._info[0] for tensor in output_tensors_list]\n\n        visited_node_names = set()\n        for out_node in output_nodes:\n            if out_node.visited:\n                continue\n            queue_node.put(out_node)\n\n            while not queue_node.empty():\n                cur_node = queue_node.get()\n                in_nodes = cur_node.in_nodes\n\n                for node in in_nodes:\n                    node.out_nodes.append(cur_node)\n                    if not node.visited:\n                        queue_node.put(node)\n                        node.visited = True\n                        if node.name not in visited_node_names:\n                            visited_node_names.add(node.name)\n                        # else have multiple layers with the same name\n                        else:\n                            raise ValueError(\n                                \'Layer name \\\'%s\\\' has already been used by another layer. Please change the layer name.\'\n                                % node.layer.name\n                            )\n\n        # construct the computation graph in top-sort order\n        cur_depth = [tensor._info[0] for tensor in input_tensors_list]\n        next_depth = []\n        indegrees = {}\n\n        visited_layer_names = []\n        while not len(cur_depth) == 0:\n            node_by_depth.append(cur_depth)\n            for node in cur_depth:\n                if node.layer.name not in visited_layer_names:\n                    all_layers.append(node.layer)\n                    visited_layer_names.append(node.layer.name)\n                for out_node in node.out_nodes:\n                    if out_node.name not in indegrees.keys():\n                        indegrees[out_node.name] = len(out_node.in_nodes)\n                    indegrees[out_node.name] -= 1\n                    if indegrees[out_node.name] == 0:\n                        next_depth.append(out_node)\n\n            cur_depth = next_depth\n            next_depth = []\n\n        return node_by_depth, all_layers\n\n    def release_memory(self):\n        \'\'\'\n        WARNING: This function should be called with great caution.\n\n        Release objects that MAY NOT be necessary such as layer.outputs (if in a tf.GradientTape() scope).\n        For each layer in the model, layer.inputs and layer.outputs will be set as None but not deleted.\n\n        A void function.\n\n        Examples\n        --------\n        >>> import tensorlayer as tl\n        >>> vgg = tl.models.vgg16()\n        ... # training preparation\n        ... # ...\n        ... # back propagation\n        >>> with tf.GradientTape() as tape:\n        >>>     _logits = vgg(x_batch)\n        >>>     ## compute loss and update model\n        >>>     _loss = tl.cost.cross_entropy(_logits, y_batch, name=\'train_loss\')\n        >>>     ## release unnecessary objects (layer.inputs, layer.outputs)\n        >>>     ## this function should be called with great caution\n        >>>     ## within the scope of tf.GradientTape(), using this function should be fine\n        >>>     vgg.release_memory()\n\n        \'\'\'\n        for layer in self.all_layers:\n            layer._release_memory()\n\n    def save(self, filepath, save_weights=True, customized_data=None):\n        """"""\n        Save model into a given file.\n        This function save can save both the architecture of neural networks and weights (optional).\n        WARNING: If the model contains Lambda / ElementwiseLambda layer, please check the documentation of Lambda / ElementwiseLambda layer and find out the cases that have / have not been supported by Model.save().\n\n        Parameters\n        ----------\n        filepath : str\n            Filename into which the model will be saved.\n        save_weights : bool\n            Whether to save model weights.\n        customized_data : dict\n            The user customized meta data.\n\n        Examples\n        --------\n        >>> net = tl.models.vgg16()\n        >>> net.save(\'./model.h5\', save_weights=True)\n        >>> new_net = Model.load(\'./model.h5\', load_weights=True)\n\n        """"""\n        # TODO: support saving LambdaLayer that includes parametric self defined function with outside variables\n        if self.outputs is None:\n            raise RuntimeError(\n                ""Model save() not support dynamic mode yet.\\nHint: you can use Model save_weights() to save the weights in dynamic mode.""\n            )\n        utils.save_hdf5_graph(\n            network=self, filepath=filepath, save_weights=save_weights, customized_data=customized_data\n        )\n\n    @staticmethod\n    def load(filepath, load_weights=True):\n        """"""\n        Load model from a given file, which should be previously saved by Model.save().\n        This function load can load both the architecture of neural networks and weights (optional, and needs to be saved in Model.save()).\n        When a model is loaded by this function load, there is no need to reimplement or declare the architecture of the model explicitly in code.\n        WARNING: If the model contains Lambda / ElementwiseLambda layer, please check the documentation of Lambda / ElementwiseLambda layer and find out the cases that have / have not been supported by Model.load().\n\n        Parameters\n        ----------\n        filepath : str\n            Filename from which the model will be loaded.\n        load_weights : bool\n            Whether to load model weights.\n\n        Examples\n        --------\n        >>> net = tl.models.vgg16()\n        >>> net.save(\'./model.h5\', save_weights=True)\n        >>> new_net = Model.load(\'./model.h5\', load_weights=True)\n        """"""\n        # TODO: support loading LambdaLayer that includes parametric self defined function with outside variables\n        M = utils.load_hdf5_graph(filepath=filepath, load_weights=load_weights)\n        return M\n\n    def save_weights(self, filepath, format=None):\n        """"""Input filepath, save model weights into a file of given format.\n            Use self.load_weights() to restore.\n\n        Parameters\n        ----------\n        filepath : str\n            Filename to which the model weights will be saved.\n        format : str or None\n            Saved file format.\n            Value should be None, \'hdf5\', \'npz\', \'npz_dict\' or \'ckpt\'. Other format is not supported now.\n            1) If this is set to None, then the postfix of filepath will be used to decide saved format.\n            If the postfix is not in [\'h5\', \'hdf5\', \'npz\', \'ckpt\'], then file will be saved in hdf5 format by default.\n            2) \'hdf5\' will save model weights name in a list and each layer has its weights stored in a group of\n            the hdf5 file.\n            3) \'npz\' will save model weights sequentially into a npz file.\n            4) \'npz_dict\' will save model weights along with its name as a dict into a npz file.\n            5) \'ckpt\' will save model weights into a tensorflow ckpt file.\n\n            Default None.\n\n        Examples\n        --------\n        1) Save model weights in hdf5 format by default.\n        >>> net = tl.models.vgg16()\n        >>> net.save_weights(\'./model.h5\')\n        ...\n        >>> net.load_weights(\'./model.h5\')\n\n        2) Save model weights in npz/npz_dict format\n        >>> net = tl.models.vgg16()\n        >>> net.save_weights(\'./model.npz\')\n        >>> net.save_weights(\'./model.npz\', format=\'npz_dict\')\n\n        """"""\n        if self.all_weights is None or len(self.all_weights) == 0:\n            logging.warning(""Model contains no weights or layers haven\'t been built, nothing will be saved"")\n            return\n\n        if format is None:\n            postfix = filepath.split(\'.\')[-1]\n            if postfix in [\'h5\', \'hdf5\', \'npz\', \'ckpt\']:\n                format = postfix\n            else:\n                format = \'hdf5\'\n\n        if format == \'hdf5\' or format == \'h5\':\n            utils.save_weights_to_hdf5(filepath, self)\n        elif format == \'npz\':\n            utils.save_npz(self.all_weights, filepath)\n        elif format == \'npz_dict\':\n            utils.save_npz_dict(self.all_weights, filepath)\n        elif format == \'ckpt\':\n            # TODO: enable this when tf save ckpt is enabled\n            raise NotImplementedError(""ckpt load/save is not supported now."")\n        else:\n            raise ValueError(\n                ""Save format must be \'hdf5\', \'npz\', \'npz_dict\' or \'ckpt\'.""\n                ""Other format is not supported now.""\n            )\n\n    def load_weights(self, filepath, format=None, in_order=True, skip=False):\n        """"""Load model weights from a given file, which should be previously saved by self.save_weights().\n\n        Parameters\n        ----------\n        filepath : str\n            Filename from which the model weights will be loaded.\n        format : str or None\n            If not specified (None), the postfix of the filepath will be used to decide its format. If specified,\n            value should be \'hdf5\', \'npz\', \'npz_dict\' or \'ckpt\'. Other format is not supported now.\n            In addition, it should be the same format when you saved the file using self.save_weights().\n            Default is None.\n        in_order : bool\n            Allow loading weights into model in a sequential way or by name. Only useful when \'format\' is \'hdf5\'.\n            If \'in_order\' is True, weights from the file will be loaded into model in a sequential way.\n            If \'in_order\' is False, weights from the file will be loaded into model by matching the name\n            with the weights of the model, particularly useful when trying to restore model in eager(graph) mode from\n            a weights file which is saved in graph(eager) mode.\n            Default is True.\n        skip : bool\n            Allow skipping weights whose name is mismatched between the file and model. Only useful when \'format\' is\n            \'hdf5\' or \'npz_dict\'. If \'skip\' is True, \'in_order\' argument will be ignored and those loaded weights\n            whose name is not found in model weights (self.all_weights) will be skipped. If \'skip\' is False, error will\n            occur when mismatch is found.\n            Default is False.\n\n        Examples\n        --------\n        1) load model from a hdf5 file.\n        >>> net = tl.models.vgg16()\n        >>> net.load_weights(\'./model_graph.h5\', in_order=False, skip=True) # load weights by name, skipping mismatch\n        >>> net.load_weights(\'./model_eager.h5\') # load sequentially\n\n        2) load model from a npz file\n        >>> net.load_weights(\'./model.npz\')\n\n        2) load model from a npz file, which is saved as npz_dict previously\n        >>> net.load_weights(\'./model.npz\', format=\'npz_dict\')\n\n        Notes\n        -------\n        1) \'in_order\' is only useful when \'format\' is \'hdf5\'. If you are trying to load a weights file which is\n           saved in a different mode, it is recommended to set \'in_order\' be True.\n        2) \'skip\' is useful when \'format\' is \'hdf5\' or \'npz_dict\'. If \'skip\' is True,\n           \'in_order\' argument will be ignored.\n\n        """"""\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(""file {} doesn\'t exist."".format(filepath))\n\n        if format is None:\n            format = filepath.split(\'.\')[-1]\n\n        if format == \'hdf5\' or format == \'h5\':\n            if skip ==True or in_order == False:\n                # load by weights name\n                utils.load_hdf5_to_weights(filepath, self, skip)\n            else:\n                # load in order\n                utils.load_hdf5_to_weights_in_order(filepath, self)\n        elif format == \'npz\':\n            utils.load_and_assign_npz(filepath, self)\n        elif format == \'npz_dict\':\n            utils.load_and_assign_npz_dict(filepath, self, skip)\n        elif format == \'ckpt\':\n            # TODO: enable this when tf save ckpt is enabled\n            raise NotImplementedError(""ckpt load/save is not supported now."")\n        else:\n            raise ValueError(\n                ""File format must be \'hdf5\', \'npz\', \'npz_dict\' or \'ckpt\'. ""\n                ""Other format is not supported now.""\n            )\n\n    # TODO: not supported now\n    # def save_ckpt(self, sess=None, mode_name=\'model.ckpt\', save_dir=\'checkpoint\', global_step=None, printable=False):\n    #     # TODO: Documentation pending\n    #     """"""""""""\n    #     if not os.path.exists(save_dir):\n    #         raise FileNotFoundError(""Save directory {} doesn\'t exist."".format(save_dir))\n    #     utils.save_ckpt(sess, mode_name, save_dir, self.weights, global_step, printable)\n    #\n    # def load_ckpt(self, sess=None, mode_name=\'model.ckpt\', save_dir=\'checkpoint\', is_latest=True, printable=False):\n    #     # TODO: Documentation pending\n    #     """"""""""""\n    #     utils.load_ckpt(sess, mode_name, save_dir, self.weights, is_latest, printable)\n\n\ndef _addindent(s_, numSpaces):\n    s = s_.split(\'\\n\')\n    # don\'t do anything for single-line stuff\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [(numSpaces * \' \') + line for line in s]\n    s = \'\\n\'.join(s)\n    s = first + \'\\n\' + s\n    return s\n\n\ndef _check_tl_layer_tensors(tensors):\n    if not isinstance(tensors, list):\n        return hasattr(tensors, \'_info\')\n    else:\n        for t in tensors:\n            if not hasattr(t, \'_info\'):\n                return False\n        return True\n\n\ndef _add_list_to_all_layers(list_member):\n    temp_all_layers = list()\n    for component in list_member:\n        if isinstance(component, Layer):\n            temp_all_layers.append(component)\n            if not component._built:\n                raise AttributeError(""Layer %s not built yet."" % repr(component))\n        elif isinstance(component, Model):\n            temp_all_layers.append(component)\n        elif isinstance(component, list):\n            temp_all_layers.extend(_add_list_to_all_layers(component))\n    return temp_all_layers\n'"
tensorlayer/models/imagenet_classes.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nclass_names = \'\'\'tench, Tinca tinca\ngoldfish, Carassius auratus\ngreat white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\ntiger shark, Galeocerdo cuvieri\nhammerhead, hammerhead shark\nelectric ray, crampfish, numbfish, torpedo\nstingray\ncock\nhen\nostrich, Struthio camelus\nbrambling, Fringilla montifringilla\ngoldfinch, Carduelis carduelis\nhouse finch, linnet, Carpodacus mexicanus\njunco, snowbird\nindigo bunting, indigo finch, indigo bird, Passerina cyanea\nrobin, American robin, Turdus migratorius\nbulbul\njay\nmagpie\nchickadee\nwater ouzel, dipper\nkite\nbald eagle, American eagle, Haliaeetus leucocephalus\nvulture\ngreat grey owl, great gray owl, Strix nebulosa\nEuropean fire salamander, Salamandra salamandra\ncommon newt, Triturus vulgaris\neft\nspotted salamander, Ambystoma maculatum\naxolotl, mud puppy, Ambystoma mexicanum\nbullfrog, Rana catesbeiana\ntree frog, tree-frog\ntailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\nloggerhead, loggerhead turtle, Caretta caretta\nleatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\nmud turtle\nterrapin\nbox turtle, box tortoise\nbanded gecko\ncommon iguana, iguana, Iguana iguana\nAmerican chameleon, anole, Anolis carolinensis\nwhiptail, whiptail lizard\nagama\nfrilled lizard, Chlamydosaurus kingi\nalligator lizard\nGila monster, Heloderma suspectum\ngreen lizard, Lacerta viridis\nAfrican chameleon, Chamaeleo chamaeleon\nKomodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\nAfrican crocodile, Nile crocodile, Crocodylus niloticus\nAmerican alligator, Alligator mississipiensis\ntriceratops\nthunder snake, worm snake, Carphophis amoenus\nringneck snake, ring-necked snake, ring snake\nhognose snake, puff adder, sand viper\ngreen snake, grass snake\nking snake, kingsnake\ngarter snake, grass snake\nwater snake\nvine snake\nnight snake, Hypsiglena torquata\nboa constrictor, Constrictor constrictor\nrock python, rock snake, Python sebae\nIndian cobra, Naja naja\ngreen mamba\nsea snake\nhorned viper, cerastes, sand viper, horned asp, Cerastes cornutus\ndiamondback, diamondback rattlesnake, Crotalus adamanteus\nsidewinder, horned rattlesnake, Crotalus cerastes\ntrilobite\nharvestman, daddy longlegs, Phalangium opilio\nscorpion\nblack and gold garden spider, Argiope aurantia\nbarn spider, Araneus cavaticus\ngarden spider, Aranea diademata\nblack widow, Latrodectus mactans\ntarantula\nwolf spider, hunting spider\ntick\ncentipede\nblack grouse\nptarmigan\nruffed grouse, partridge, Bonasa umbellus\nprairie chicken, prairie grouse, prairie fowl\npeacock\nquail\npartridge\nAfrican grey, African gray, Psittacus erithacus\nmacaw\nsulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\nlorikeet\ncoucal\nbee eater\nhornbill\nhummingbird\njacamar\ntoucan\ndrake\nred-breasted merganser, Mergus serrator\ngoose\nblack swan, Cygnus atratus\ntusker\nechidna, spiny anteater, anteater\nplatypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\nwallaby, brush kangaroo\nkoala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\nwombat\njellyfish\nsea anemone, anemone\nbrain coral\nflatworm, platyhelminth\nnematode, nematode worm, roundworm\nconch\nsnail\nslug\nsea slug, nudibranch\nchiton, coat-of-mail shell, sea cradle, polyplacophore\nchambered nautilus, pearly nautilus, nautilus\nDungeness crab, Cancer magister\nrock crab, Cancer irroratus\nfiddler crab\nking crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\nAmerican lobster, Northern lobster, Maine lobster, Homarus americanus\nspiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\ncrayfish, crawfish, crawdad, crawdaddy\nhermit crab\nisopod\nwhite stork, Ciconia ciconia\nblack stork, Ciconia nigra\nspoonbill\nflamingo\nlittle blue heron, Egretta caerulea\nAmerican egret, great white heron, Egretta albus\nbittern\ncrane\nlimpkin, Aramus pictus\nEuropean gallinule, Porphyrio porphyrio\nAmerican coot, marsh hen, mud hen, water hen, Fulica americana\nbustard\nruddy turnstone, Arenaria interpres\nred-backed sandpiper, dunlin, Erolia alpina\nredshank, Tringa totanus\ndowitcher\noystercatcher, oyster catcher\npelican\nking penguin, Aptenodytes patagonica\nalbatross, mollymawk\ngrey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\nkiller whale, killer, orca, grampus, sea wolf, Orcinus orca\ndugong, Dugong dugon\nsea lion\nChihuahua\nJapanese spaniel\nMaltese dog, Maltese terrier, Maltese\nPekinese, Pekingese, Peke\nShih-Tzu\nBlenheim spaniel\npapillon\ntoy terrier\nRhodesian ridgeback\nAfghan hound, Afghan\nbasset, basset hound\nbeagle\nbloodhound, sleuthhound\nbluetick\nblack-and-tan coonhound\nWalker hound, Walker foxhound\nEnglish foxhound\nredbone\nborzoi, Russian wolfhound\nIrish wolfhound\nItalian greyhound\nwhippet\nIbizan hound, Ibizan Podenco\nNorwegian elkhound, elkhound\notterhound, otter hound\nSaluki, gazelle hound\nScottish deerhound, deerhound\nWeimaraner\nStaffordshire bullterrier, Staffordshire bull terrier\nAmerican Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\nBedlington terrier\nBorder terrier\nKerry blue terrier\nIrish terrier\nNorfolk terrier\nNorwich terrier\nYorkshire terrier\nwire-haired fox terrier\nLakeland terrier\nSealyham terrier, Sealyham\nAiredale, Airedale terrier\ncairn, cairn terrier\nAustralian terrier\nDandie Dinmont, Dandie Dinmont terrier\nBoston bull, Boston terrier\nminiature schnauzer\ngiant schnauzer\nstandard schnauzer\nScotch terrier, Scottish terrier, Scottie\nTibetan terrier, chrysanthemum dog\nsilky terrier, Sydney silky\nsoft-coated wheaten terrier\nWest Highland white terrier\nLhasa, Lhasa apso\nflat-coated retriever\ncurly-coated retriever\ngolden retriever\nLabrador retriever\nChesapeake Bay retriever\nGerman short-haired pointer\nvizsla, Hungarian pointer\nEnglish setter\nIrish setter, red setter\nGordon setter\nBrittany spaniel\nclumber, clumber spaniel\nEnglish springer, English springer spaniel\nWelsh springer spaniel\ncocker spaniel, English cocker spaniel, cocker\nSussex spaniel\nIrish water spaniel\nkuvasz\nschipperke\ngroenendael\nmalinois\nbriard\nkelpie\nkomondor\nOld English sheepdog, bobtail\nShetland sheepdog, Shetland sheep dog, Shetland\ncollie\nBorder collie\nBouvier des Flandres, Bouviers des Flandres\nRottweiler\nGerman shepherd, German shepherd dog, German police dog, alsatian\nDoberman, Doberman pinscher\nminiature pinscher\nGreater Swiss Mountain dog\nBernese mountain dog\nAppenzeller\nEntleBucher\nboxer\nbull mastiff\nTibetan mastiff\nFrench bulldog\nGreat Dane\nSaint Bernard, St Bernard\nEskimo dog, husky\nmalamute, malemute, Alaskan malamute\nSiberian husky\ndalmatian, coach dog, carriage dog\naffenpinscher, monkey pinscher, monkey dog\nbasenji\npug, pug-dog\nLeonberg\nNewfoundland, Newfoundland dog\nGreat Pyrenees\nSamoyed, Samoyede\nPomeranian\nchow, chow chow\nkeeshond\nBrabancon griffon\nPembroke, Pembroke Welsh corgi\nCardigan, Cardigan Welsh corgi\ntoy poodle\nminiature poodle\nstandard poodle\nMexican hairless\ntimber wolf, grey wolf, gray wolf, Canis lupus\nwhite wolf, Arctic wolf, Canis lupus tundrarum\nred wolf, maned wolf, Canis rufus, Canis niger\ncoyote, prairie wolf, brush wolf, Canis latrans\ndingo, warrigal, warragal, Canis dingo\ndhole, Cuon alpinus\nAfrican hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\nhyena, hyaena\nred fox, Vulpes vulpes\nkit fox, Vulpes macrotis\nArctic fox, white fox, Alopex lagopus\ngrey fox, gray fox, Urocyon cinereoargenteus\ntabby, tabby cat\ntiger cat\nPersian cat\nSiamese cat, Siamese\nEgyptian cat\ncougar, puma, catamount, mountain lion, painter, panther, Felis concolor\nlynx, catamount\nleopard, Panthera pardus\nsnow leopard, ounce, Panthera uncia\njaguar, panther, Panthera onca, Felis onca\nlion, king of beasts, Panthera leo\ntiger, Panthera tigris\ncheetah, chetah, Acinonyx jubatus\nbrown bear, bruin, Ursus arctos\nAmerican black bear, black bear, Ursus americanus, Euarctos americanus\nice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\nsloth bear, Melursus ursinus, Ursus ursinus\nmongoose\nmeerkat, mierkat\ntiger beetle\nladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\nground beetle, carabid beetle\nlong-horned beetle, longicorn, longicorn beetle\nleaf beetle, chrysomelid\ndung beetle\nrhinoceros beetle\nweevil\nfly\nbee\nant, emmet, pismire\ngrasshopper, hopper\ncricket\nwalking stick, walkingstick, stick insect\ncockroach, roach\nmantis, mantid\ncicada, cicala\nleafhopper\nlacewing, lacewing fly\ndragonfly, darning needle, devil\'s darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\ndamselfly\nadmiral\nringlet, ringlet butterfly\nmonarch, monarch butterfly, milkweed butterfly, Danaus plexippus\ncabbage butterfly\nsulphur butterfly, sulfur butterfly\nlycaenid, lycaenid butterfly\nstarfish, sea star\nsea urchin\nsea cucumber, holothurian\nwood rabbit, cottontail, cottontail rabbit\nhare\nAngora, Angora rabbit\nhamster\nporcupine, hedgehog\nfox squirrel, eastern fox squirrel, Sciurus niger\nmarmot\nbeaver\nguinea pig, Cavia cobaya\nsorrel\nzebra\nhog, pig, grunter, squealer, Sus scrofa\nwild boar, boar, Sus scrofa\nwarthog\nhippopotamus, hippo, river horse, Hippopotamus amphibius\nox\nwater buffalo, water ox, Asiatic buffalo, Bubalus bubalis\nbison\nram, tup\nbighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\nibex, Capra ibex\nhartebeest\nimpala, Aepyceros melampus\ngazelle\nArabian camel, dromedary, Camelus dromedarius\nllama\nweasel\nmink\npolecat, fitch, foulmart, foumart, Mustela putorius\nblack-footed ferret, ferret, Mustela nigripes\notter\nskunk, polecat, wood pussy\nbadger\narmadillo\nthree-toed sloth, ai, Bradypus tridactylus\norangutan, orang, orangutang, Pongo pygmaeus\ngorilla, Gorilla gorilla\nchimpanzee, chimp, Pan troglodytes\ngibbon, Hylobates lar\nsiamang, Hylobates syndactylus, Symphalangus syndactylus\nguenon, guenon monkey\npatas, hussar monkey, Erythrocebus patas\nbaboon\nmacaque\nlangur\ncolobus, colobus monkey\nproboscis monkey, Nasalis larvatus\nmarmoset\ncapuchin, ringtail, Cebus capucinus\nhowler monkey, howler\ntiti, titi monkey\nspider monkey, Ateles geoffroyi\nsquirrel monkey, Saimiri sciureus\nMadagascar cat, ring-tailed lemur, Lemur catta\nindri, indris, Indri indri, Indri brevicaudatus\nIndian elephant, Elephas maximus\nAfrican elephant, Loxodonta africana\nlesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\ngiant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\nbarracouta, snoek\neel\ncoho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\nrock beauty, Holocanthus tricolor\nanemone fish\nsturgeon\ngar, garfish, garpike, billfish, Lepisosteus osseus\nlionfish\npuffer, pufferfish, blowfish, globefish\nabacus\nabaya\nacademic gown, academic robe, judge\'s robe\naccordion, piano accordion, squeeze box\nacoustic guitar\naircraft carrier, carrier, flattop, attack aircraft carrier\nairliner\nairship, dirigible\naltar\nambulance\namphibian, amphibious vehicle\nanalog clock\napiary, bee house\napron\nashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\nassault rifle, assault gun\nbackpack, back pack, knapsack, packsack, rucksack, haversack\nbakery, bakeshop, bakehouse\nbalance beam, beam\nballoon\nballpoint, ballpoint pen, ballpen, Biro\nBand Aid\nbanjo\nbannister, banister, balustrade, balusters, handrail\nbarbell\nbarber chair\nbarbershop\nbarn\nbarometer\nbarrel, cask\nbarrow, garden cart, lawn cart, wheelbarrow\nbaseball\nbasketball\nbassinet\nbassoon\nbathing cap, swimming cap\nbath towel\nbathtub, bathing tub, bath, tub\nbeach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\nbeacon, lighthouse, beacon light, pharos\nbeaker\nbearskin, busby, shako\nbeer bottle\nbeer glass\nbell cote, bell cot\nbib\nbicycle-built-for-two, tandem bicycle, tandem\nbikini, two-piece\nbinder, ring-binder\nbinoculars, field glasses, opera glasses\nbirdhouse\nboathouse\nbobsled, bobsleigh, bob\nbolo tie, bolo, bola tie, bola\nbonnet, poke bonnet\nbookcase\nbookshop, bookstore, bookstall\nbottlecap\nbow\nbow tie, bow-tie, bowtie\nbrass, memorial tablet, plaque\nbrassiere, bra, bandeau\nbreakwater, groin, groyne, mole, bulwark, seawall, jetty\nbreastplate, aegis, egis\nbroom\nbucket, pail\nbuckle\nbulletproof vest\nbullet train, bullet\nbutcher shop, meat market\ncab, hack, taxi, taxicab\ncaldron, cauldron\ncandle, taper, wax light\ncannon\ncanoe\ncan opener, tin opener\ncardigan\ncar mirror\ncarousel, carrousel, merry-go-round, roundabout, whirligig\ncarpenter\'s kit, tool kit\ncarton\ncar wheel\ncash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\ncassette\ncassette player\ncastle\ncatamaran\nCD player\ncello, violoncello\ncellular telephone, cellular phone, cellphone, cell, mobile phone\nchain\nchainlink fence\nchain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\nchain saw, chainsaw\nchest\nchiffonier, commode\nchime, bell, gong\nchina cabinet, china closet\nChristmas stocking\nchurch, church building\ncinema, movie theater, movie theatre, movie house, picture palace\ncleaver, meat cleaver, chopper\ncliff dwelling\ncloak\nclog, geta, patten, sabot\ncocktail shaker\ncoffee mug\ncoffeepot\ncoil, spiral, volute, whorl, helix\ncombination lock\ncomputer keyboard, keypad\nconfectionery, confectionary, candy store\ncontainer ship, containership, container vessel\nconvertible\ncorkscrew, bottle screw\ncornet, horn, trumpet, trump\ncowboy boot\ncowboy hat, ten-gallon hat\ncradle\ncrane\ncrash helmet\ncrate\ncrib, cot\nCrock Pot\ncroquet ball\ncrutch\ncuirass\ndam, dike, dyke\ndesk\ndesktop computer\ndial telephone, dial phone\ndiaper, nappy, napkin\ndigital clock\ndigital watch\ndining table, board\ndishrag, dishcloth\ndishwasher, dish washer, dishwashing machine\ndisk brake, disc brake\ndock, dockage, docking facility\ndogsled, dog sled, dog sleigh\ndome\ndoormat, welcome mat\ndrilling platform, offshore rig\ndrum, membranophone, tympan\ndrumstick\ndumbbell\nDutch oven\nelectric fan, blower\nelectric guitar\nelectric locomotive\nentertainment center\nenvelope\nespresso maker\nface powder\nfeather boa, boa\nfile, file cabinet, filing cabinet\nfireboat\nfire engine, fire truck\nfire screen, fireguard\nflagpole, flagstaff\nflute, transverse flute\nfolding chair\nfootball helmet\nforklift\nfountain\nfountain pen\nfour-poster\nfreight car\nFrench horn, horn\nfrying pan, frypan, skillet\nfur coat\ngarbage truck, dustcart\ngasmask, respirator, gas helmet\ngas pump, gasoline pump, petrol pump, island dispenser\ngoblet\ngo-kart\ngolf ball\ngolfcart, golf cart\ngondola\ngong, tam-tam\ngown\ngrand piano, grand\ngreenhouse, nursery, glasshouse\ngrille, radiator grille\ngrocery store, grocery, food market, market\nguillotine\nhair slide\nhair spray\nhalf track\nhammer\nhamper\nhand blower, blow dryer, blow drier, hair dryer, hair drier\nhand-held computer, hand-held microcomputer\nhandkerchief, hankie, hanky, hankey\nhard disc, hard disk, fixed disk\nharmonica, mouth organ, harp, mouth harp\nharp\nharvester, reaper\nhatchet\nholster\nhome theater, home theatre\nhoneycomb\nhook, claw\nhoopskirt, crinoline\nhorizontal bar, high bar\nhorse cart, horse-cart\nhourglass\niPod\niron, smoothing iron\njack-o\'-lantern\njean, blue jean, denim\njeep, landrover\njersey, T-shirt, tee shirt\njigsaw puzzle\njinrikisha, ricksha, rickshaw\njoystick\nkimono\nknee pad\nknot\nlab coat, laboratory coat\nladle\nlampshade, lamp shade\nlaptop, laptop computer\nlawn mower, mower\nlens cap, lens cover\nletter opener, paper knife, paperknife\nlibrary\nlifeboat\nlighter, light, igniter, ignitor\nlimousine, limo\nliner, ocean liner\nlipstick, lip rouge\nLoafer\nlotion\nloudspeaker, speaker, speaker unit, loudspeaker system, speaker system\nloupe, jeweler\'s loupe\nlumbermill, sawmill\nmagnetic compass\nmailbag, postbag\nmailbox, letter box\nmaillot\nmaillot, tank suit\nmanhole cover\nmaraca\nmarimba, xylophone\nmask\nmatchstick\nmaypole\nmaze, labyrinth\nmeasuring cup\nmedicine chest, medicine cabinet\nmegalith, megalithic structure\nmicrophone, mike\nmicrowave, microwave oven\nmilitary uniform\nmilk can\nminibus\nminiskirt, mini\nminivan\nmissile\nmitten\nmixing bowl\nmobile home, manufactured home\nModel T\nmodem\nmonastery\nmonitor\nmoped\nmortar\nmortarboard\nmosque\nmosquito net\nmotor scooter, scooter\nmountain bike, all-terrain bike, off-roader\nmountain tent\nmouse, computer mouse\nmousetrap\nmoving van\nmuzzle\nnail\nneck brace\nnecklace\nnipple\nnotebook, notebook computer\nobelisk\noboe, hautboy, hautbois\nocarina, sweet potato\nodometer, hodometer, mileometer, milometer\noil filter\norgan, pipe organ\noscilloscope, scope, cathode-ray oscilloscope, CRO\noverskirt\noxcart\noxygen mask\npacket\npaddle, boat paddle\npaddlewheel, paddle wheel\npadlock\npaintbrush\npajama, pyjama, pj\'s, jammies\npalace\npanpipe, pandean pipe, syrinx\npaper towel\nparachute, chute\nparallel bars, bars\npark bench\nparking meter\npassenger car, coach, carriage\npatio, terrace\npay-phone, pay-station\npedestal, plinth, footstall\npencil box, pencil case\npencil sharpener\nperfume, essence\nPetri dish\nphotocopier\npick, plectrum, plectron\npickelhaube\npicket fence, paling\npickup, pickup truck\npier\npiggy bank, penny bank\npill bottle\npillow\nping-pong ball\npinwheel\npirate, pirate ship\npitcher, ewer\nplane, carpenter\'s plane, woodworking plane\nplanetarium\nplastic bag\nplate rack\nplow, plough\nplunger, plumber\'s helper\nPolaroid camera, Polaroid Land camera\npole\npolice van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\nponcho\npool table, billiard table, snooker table\npop bottle, soda bottle\npot, flowerpot\npotter\'s wheel\npower drill\nprayer rug, prayer mat\nprinter\nprison, prison house\nprojectile, missile\nprojector\npuck, hockey puck\npunching bag, punch bag, punching ball, punchball\npurse\nquill, quill pen\nquilt, comforter, comfort, puff\nracer, race car, racing car\nracket, racquet\nradiator\nradio, wireless\nradio telescope, radio reflector\nrain barrel\nrecreational vehicle, RV, R.V.\nreel\nreflex camera\nrefrigerator, icebox\nremote control, remote\nrestaurant, eating house, eating place, eatery\nrevolver, six-gun, six-shooter\nrifle\nrocking chair, rocker\nrotisserie\nrubber eraser, rubber, pencil eraser\nrugby ball\nrule, ruler\nrunning shoe\nsafe\nsafety pin\nsaltshaker, salt shaker\nsandal\nsarong\nsax, saxophone\nscabbard\nscale, weighing machine\nschool bus\nschooner\nscoreboard\nscreen, CRT screen\nscrew\nscrewdriver\nseat belt, seatbelt\nsewing machine\nshield, buckler\nshoe shop, shoe-shop, shoe store\nshoji\nshopping basket\nshopping cart\nshovel\nshower cap\nshower curtain\nski\nski mask\nsleeping bag\nslide rule, slipstick\nsliding door\nslot, one-armed bandit\nsnorkel\nsnowmobile\nsnowplow, snowplough\nsoap dispenser\nsoccer ball\nsock\nsolar dish, solar collector, solar furnace\nsombrero\nsoup bowl\nspace bar\nspace heater\nspace shuttle\nspatula\nspeedboat\nspider web, spider\'s web\nspindle\nsports car, sport car\nspotlight, spot\nstage\nsteam locomotive\nsteel arch bridge\nsteel drum\nstethoscope\nstole\nstone wall\nstopwatch, stop watch\nstove\nstrainer\nstreetcar, tram, tramcar, trolley, trolley car\nstretcher\nstudio couch, day bed\nstupa, tope\nsubmarine, pigboat, sub, U-boat\nsuit, suit of clothes\nsundial\nsunglass\nsunglasses, dark glasses, shades\nsunscreen, sunblock, sun blocker\nsuspension bridge\nswab, swob, mop\nsweatshirt\nswimming trunks, bathing trunks\nswing\nswitch, electric switch, electrical switch\nsyringe\ntable lamp\ntank, army tank, armored combat vehicle, armoured combat vehicle\ntape player\nteapot\nteddy, teddy bear\ntelevision, television system\ntennis ball\nthatch, thatched roof\ntheater curtain, theatre curtain\nthimble\nthresher, thrasher, threshing machine\nthrone\ntile roof\ntoaster\ntobacco shop, tobacconist shop, tobacconist\ntoilet seat\ntorch\ntotem pole\ntow truck, tow car, wrecker\ntoyshop\ntractor\ntrailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\ntray\ntrench coat\ntricycle, trike, velocipede\ntrimaran\ntripod\ntriumphal arch\ntrolleybus, trolley coach, trackless trolley\ntrombone\ntub, vat\nturnstile\ntypewriter keyboard\numbrella\nunicycle, monocycle\nupright, upright piano\nvacuum, vacuum cleaner\nvase\nvault\nvelvet\nvending machine\nvestment\nviaduct\nviolin, fiddle\nvolleyball\nwaffle iron\nwall clock\nwallet, billfold, notecase, pocketbook\nwardrobe, closet, press\nwarplane, military plane\nwashbasin, handbasin, washbowl, lavabo, wash-hand basin\nwasher, automatic washer, washing machine\nwater bottle\nwater jug\nwater tower\nwhiskey jug\nwhistle\nwig\nwindow screen\nwindow shade\nWindsor tie\nwine bottle\nwing\nwok\nwooden spoon\nwool, woolen, woollen\nworm fence, snake fence, snake-rail fence, Virginia fence\nwreck\nyawl\nyurt\nweb site, website, internet site, site\ncomic book\ncrossword puzzle, crossword\nstreet sign\ntraffic light, traffic signal, stoplight\nbook jacket, dust cover, dust jacket, dust wrapper\nmenu\nplate\nguacamole\nconsomme\nhot pot, hotpot\ntrifle\nice cream, icecream\nice lolly, lolly, lollipop, popsicle\nFrench loaf\nbagel, beigel\npretzel\ncheeseburger\nhotdog, hot dog, red hot\nmashed potato\nhead cabbage\nbroccoli\ncauliflower\nzucchini, courgette\nspaghetti squash\nacorn squash\nbutternut squash\ncucumber, cuke\nartichoke, globe artichoke\nbell pepper\ncardoon\nmushroom\nGranny Smith\nstrawberry\norange\nlemon\nfig\npineapple, ananas\nbanana\njackfruit, jak, jack\ncustard apple\npomegranate\nhay\ncarbonara\nchocolate sauce, chocolate syrup\ndough\nmeat loaf, meatloaf\npizza, pizza pie\npotpie\nburrito\nred wine\nespresso\ncup\neggnog\nalp\nbubble\ncliff, drop, drop-off\ncoral reef\ngeyser\nlakeside, lakeshore\npromontory, headland, head, foreland\nsandbar, sand bar\nseashore, coast, seacoast, sea-coast\nvalley, vale\nvolcano\nballplayer, baseball player\ngroom, bridegroom\nscuba diver\nrapeseed\ndaisy\nyellow lady\'s slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\ncorn\nacorn\nhip, rose hip, rosehip\nbuckeye, horse chestnut, conker\ncoral fungus\nagaric\ngyromitra\nstinkhorn, carrion fungus\nearthstar\nhen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\nbolete\near, spike, capitulum\ntoilet tissue, toilet paper, bathroom tissue\'\'\'.split(""\\n"")\n'"
tensorlayer/models/mobilenetv1.py,4,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""MobileNet for ImageNet.""""""\n\nimport os\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.files import (assign_weights, load_npz, maybe_download_and_extract)\nfrom tensorlayer.layers import (BatchNorm, Conv2d, DepthwiseConv2d, Flatten, GlobalMeanPool2d, Input, Reshape)\nfrom tensorlayer.models import Model\n\n__all__ = [\n    \'MobileNetV1\',\n]\n\nlayer_names = [\n    \'conv\', \'depth1\', \'depth2\', \'depth3\', \'depth4\', \'depth5\', \'depth6\', \'depth7\', \'depth8\', \'depth9\', \'depth10\',\n    \'depth11\', \'depth12\', \'depth13\', \'globalmeanpool\', \'reshape\', \'out\'\n]\nn_filters = [32, 64, 128, 128, 256, 256, 512, 512, 512, 512, 512, 512, 1024, 1024]\n\n\ndef conv_block(n, n_filter, filter_size=(3, 3), strides=(1, 1), name=\'conv_block\'):\n    # ref: https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet.py\n    n = Conv2d(n_filter, filter_size, strides, b_init=None, name=name + \'.conv\')(n)\n    n = BatchNorm(decay=0.99, act=tf.nn.relu6, name=name + \'.batchnorm\')(n)\n    return n\n\n\ndef depthwise_conv_block(n, n_filter, strides=(1, 1), name=""depth_block""):\n    n = DepthwiseConv2d((3, 3), strides, b_init=None, name=name + \'.depthwise\')(n)\n    n = BatchNorm(decay=0.99, act=tf.nn.relu6, name=name + \'.batchnorm1\')(n)\n    n = Conv2d(n_filter, (1, 1), (1, 1), b_init=None, name=name + \'.conv\')(n)\n    n = BatchNorm(decay=0.99, act=tf.nn.relu6, name=name + \'.batchnorm2\')(n)\n    return n\n\n\ndef restore_params(network, path=\'models\'):\n    logging.info(""Restore pre-trained parameters"")\n    maybe_download_and_extract(\n        \'mobilenet.npz\', path, \'https://github.com/tensorlayer/pretrained-models/raw/master/models/\',\n        expected_bytes=25600116\n    )  # ls -al\n    params = load_npz(name=os.path.join(path, \'mobilenet.npz\'))\n    # for idx, net_weight in enumerate(network.all_weights):\n    #     if \'batchnorm\' in net_weight.name:\n    #         params[idx] = params[idx].reshape(1, 1, 1, -1)\n    assign_weights(params[:len(network.all_weights)], network)\n    del params\n\n\ndef MobileNetV1(pretrained=False, end_with=\'out\', name=None):\n    """"""Pre-trained MobileNetV1 model (static mode). Input shape [?, 224, 224, 3], value range [0, 1].\n\n    Parameters\n    ----------\n    pretrained : boolean\n        Whether to load pretrained weights. Default False.\n    end_with : str\n        The end point of the model [conv, depth1, depth2 ... depth13, globalmeanpool, out]. Default ``out`` i.e. the whole model.\n    name : None or str\n        Name for this model.\n\n    Examples\n    ---------\n    Classify ImageNet classes, see `tutorial_models_mobilenetv1.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_models_mobilenetv1.py>`__\n\n    >>> # get the whole model with pretrained weights\n    >>> mobilenetv1 = tl.models.MobileNetV1(pretrained=True)\n    >>> # use for inferencing\n    >>> output = mobilenetv1(img1, is_train=False)\n    >>> prob = tf.nn.softmax(output)[0].numpy()\n\n    Extract features and Train a classifier with 100 classes\n\n    >>> # get model without the last layer\n    >>> cnn = tl.models.MobileNetV1(pretrained=True, end_with=\'reshape\').as_layer()\n    >>> # add one more layer and build new model\n    >>> ni = Input([None, 224, 224, 3], name=""inputs"")\n    >>> nn = cnn(ni)\n    >>> nn = Conv2d(100, (1, 1), (1, 1), name=\'out\')(nn)\n    >>> nn = Flatten(name=\'flatten\')(nn)\n    >>> model = tl.models.Model(inputs=ni, outputs=nn)\n    >>> # train your own classifier (only update the last layer)\n    >>> train_params = model.get_layer(\'out\').trainable_weights\n\n    Returns\n    -------\n        static MobileNetV1.\n    """"""\n    ni = Input([None, 224, 224, 3], name=""input"")\n\n    for i in range(len(layer_names)):\n        if i == 0:\n            n = conv_block(ni, n_filters[i], strides=(2, 2), name=layer_names[i])\n        elif layer_names[i] in [\'depth2\', \'depth4\', \'depth6\', \'depth12\']:\n            n = depthwise_conv_block(n, n_filters[i], strides=(2, 2), name=layer_names[i])\n        elif layer_names[i] == \'globalmeanpool\':\n            n = GlobalMeanPool2d(name=\'globalmeanpool\')(n)\n        elif layer_names[i] == \'reshape\':\n            n = Reshape([-1, 1, 1, 1024], name=\'reshape\')(n)\n        elif layer_names[i] == \'out\':\n            n = Conv2d(1000, (1, 1), (1, 1), name=\'out\')(n)\n            n = Flatten(name=\'flatten\')(n)\n        else:\n            n = depthwise_conv_block(n, n_filters[i], name=layer_names[i])\n\n        if layer_names[i] == end_with:\n            break\n\n    network = Model(inputs=ni, outputs=n, name=name)\n\n    if pretrained:\n        restore_params(network)\n\n    return network\n'"
tensorlayer/models/resnet.py,11,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""ResNet for ImageNet.\n\n# Reference:\n- [Deep Residual Learning for Image Recognition](\n    https://arxiv.org/abs/1512.03385) (CVPR 2016 Best Paper Award)\n\n""""""\n\nimport os\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.files import (assign_weights, load_npz, maybe_download_and_extract)\nfrom tensorlayer.layers import (BatchNorm, Conv2d, Dense, Elementwise, GlobalMeanPool2d, Input, MaxPool2d)\nfrom tensorlayer.models import Model\n\n__all__ = [\n    \'ResNet50\',\n]\n\n\ndef identity_block(input, kernel_size, n_filters, stage, block):\n    """"""The identity block where there is no conv layer at shortcut.\n\n    Parameters\n    ----------\n    input : tf tensor\n        Input tensor from above layer.\n    kernel_size : int\n        The kernel size of middle conv layer at main path.\n    n_filters : list of integers\n        The numbers of filters for 3 conv layer at main path.\n    stage : int\n        Current stage label.\n    block : str\n        Current block label.\n\n    Returns\n    -------\n        Output tensor of this block.\n\n    """"""\n    filters1, filters2, filters3 = n_filters\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n    x = Conv2d(filters1, (1, 1), W_init=tf.initializers.he_normal(), name=conv_name_base + \'2a\')(input)\n    x = BatchNorm(name=bn_name_base + \'2a\', act=\'relu\')(x)\n\n    ks = (kernel_size, kernel_size)\n    x = Conv2d(filters2, ks, padding=\'SAME\', W_init=tf.initializers.he_normal(), name=conv_name_base + \'2b\')(x)\n    x = BatchNorm(name=bn_name_base + \'2b\', act=\'relu\')(x)\n\n    x = Conv2d(filters3, (1, 1), W_init=tf.initializers.he_normal(), name=conv_name_base + \'2c\')(x)\n    x = BatchNorm(name=bn_name_base + \'2c\')(x)\n\n    x = Elementwise(tf.add, act=\'relu\')([x, input])\n    return x\n\n\ndef conv_block(input, kernel_size, n_filters, stage, block, strides=(2, 2)):\n    """"""The conv block where there is a conv layer at shortcut.\n\n    Parameters\n    ----------\n    input : tf tensor\n        Input tensor from above layer.\n    kernel_size : int\n        The kernel size of middle conv layer at main path.\n    n_filters : list of integers\n        The numbers of filters for 3 conv layer at main path.\n    stage : int\n        Current stage label.\n    block : str\n        Current block label.\n    strides : tuple\n        Strides for the first conv layer in the block.\n\n    Returns\n    -------\n        Output tensor of this block.\n\n    """"""\n    filters1, filters2, filters3 = n_filters\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n    x = Conv2d(filters1, (1, 1), strides=strides, W_init=tf.initializers.he_normal(), name=conv_name_base + \'2a\')(input)\n    x = BatchNorm(name=bn_name_base + \'2a\', act=\'relu\')(x)\n\n    ks = (kernel_size, kernel_size)\n    x = Conv2d(filters2, ks, padding=\'SAME\', W_init=tf.initializers.he_normal(), name=conv_name_base + \'2b\')(x)\n    x = BatchNorm(name=bn_name_base + \'2b\', act=\'relu\')(x)\n\n    x = Conv2d(filters3, (1, 1), W_init=tf.initializers.he_normal(), name=conv_name_base + \'2c\')(x)\n    x = BatchNorm(name=bn_name_base + \'2c\')(x)\n\n    shortcut = Conv2d(filters3, (1, 1), strides=strides, W_init=tf.initializers.he_normal(),\n                      name=conv_name_base + \'1\')(input)\n    shortcut = BatchNorm(name=bn_name_base + \'1\')(shortcut)\n\n    x = Elementwise(tf.add, act=\'relu\')([x, shortcut])\n    return x\n\n\nblock_names = [\'2a\', \'2b\', \'2c\', \'3a\', \'3b\', \'3c\', \'3d\', \'4a\', \'4b\', \'4c\', \'4d\', \'4e\', \'4f\', \'5a\', \'5b\', \'5c\'\n              ] + [\'avg_pool\', \'fc1000\']\nblock_filters = [[64, 64, 256], [128, 128, 512], [256, 256, 1024], [512, 512, 2048]]\n\n\ndef ResNet50(pretrained=False, end_with=\'fc1000\', n_classes=1000, name=None):\n    """"""Pre-trained MobileNetV1 model (static mode). Input shape [?, 224, 224, 3].\n    To use pretrained model, input should be in BGR format and subtracted from ImageNet mean [103.939, 116.779, 123.68].\n\n    Parameters\n    ----------\n    pretrained : boolean\n        Whether to load pretrained weights. Default False.\n    end_with : str\n        The end point of the model [conv, depth1, depth2 ... depth13, globalmeanpool, out].\n        Default ``out`` i.e. the whole model.\n    n_classes : int\n        Number of classes in final prediction.\n    name : None or str\n        Name for this model.\n\n    Examples\n    ---------\n    Classify ImageNet classes, see `tutorial_models_resnet50.py`\n\n    >>> # get the whole model with pretrained weights\n    >>> resnet = tl.models.ResNet50(pretrained=True)\n    >>> # use for inferencing\n    >>> output = resnet(img1, is_train=False)\n    >>> prob = tf.nn.softmax(output)[0].numpy()\n\n    Extract the features before fc layer\n    >>> resnet = tl.models.ResNet50(pretrained=True, end_with=\'5c\')\n    >>> output = resnet(img1, is_train=False)\n\n    Returns\n    -------\n        ResNet50 model.\n\n    """"""\n    ni = Input([None, 224, 224, 3], name=""input"")\n    n = Conv2d(64, (7, 7), strides=(2, 2), padding=\'SAME\', W_init=tf.initializers.he_normal(), name=\'conv1\')(ni)\n    n = BatchNorm(name=\'bn_conv1\', act=\'relu\')(n)\n    n = MaxPool2d((3, 3), strides=(2, 2), name=\'max_pool1\')(n)\n\n    for i, block_name in enumerate(block_names):\n        if len(block_name) == 2:\n            stage = int(block_name[0])\n            block = block_name[1]\n            if block == \'a\':\n                strides = (1, 1) if stage == 2 else (2, 2)\n                n = conv_block(n, 3, block_filters[stage - 2], stage=stage, block=block, strides=strides)\n            else:\n                n = identity_block(n, 3, block_filters[stage - 2], stage=stage, block=block)\n        elif block_name == \'avg_pool\':\n            n = GlobalMeanPool2d(name=\'avg_pool\')(n)\n        elif block_name == \'fc1000\':\n            n = Dense(n_classes, name=\'fc1000\')(n)\n\n        if block_name == end_with:\n            break\n\n    network = Model(inputs=ni, outputs=n, name=name)\n\n    if pretrained:\n        restore_params(network)\n\n    return network\n\n\ndef restore_params(network, path=\'models\'):\n    logging.info(""Restore pre-trained parameters"")\n    maybe_download_and_extract(\n        \'resnet50_weights_tf_dim_ordering_tf_kernels.h5\',\n        path,\n        \'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/\',\n    )  # ls -al\n    try:\n        import h5py\n    except Exception:\n        raise ImportError(\'h5py not imported\')\n\n    f = h5py.File(os.path.join(path, \'resnet50_weights_tf_dim_ordering_tf_kernels.h5\'), \'r\')\n\n    for layer in network.all_layers:\n        if len(layer.all_weights) == 0:\n            continue\n        w_names = list(f[layer.name])\n        params = [f[layer.name][n][:] for n in w_names]\n        # if \'bn\' in layer.name:\n        #     params = [x.reshape(1, 1, 1, -1) for x in params]\n        assign_weights(params, layer)\n        del params\n\n    f.close()\n'"
tensorlayer/models/seq2seq.py,7,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import Dense, Dropout, Input\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.models import Model\n\n__all__ = [\'Seq2seq\']\n\n\nclass Seq2seq(Model):\n    """"""vanilla stacked layer Seq2Seq model.\n\n    Parameters\n    ----------\n    decoder_seq_length: int\n        The length of your target sequence\n    cell_enc : TensorFlow cell function\n        The RNN function cell for your encoder stack, e.g tf.keras.layers.GRUCell\n    cell_dec : TensorFlow cell function\n        The RNN function cell for your decoder stack, e.g. tf.keras.layers.GRUCell\n    n_layer : int\n        The number of your RNN layers for both encoder and decoder block\n    embedding_layer : tl.Layer\n        A embedding layer, e.g. tl.layers.Embedding(vocabulary_size=voc_size, embedding_size=emb_dim)\n    name : str\n        The model name\n    \n    Examples\n    ---------\n    Classify stacked-layer Seq2Seq model, see `chatbot <https://github.com/tensorlayer/seq2seq-chatbot>`__\n\n    Returns\n    -------\n        static stacked-layer Seq2Seq model.\n    """"""\n\n    def __init__(self, decoder_seq_length, cell_enc, cell_dec, n_units=256, n_layer=3, embedding_layer=None, name=None):\n        super(Seq2seq, self).__init__(name=name)\n        self.embedding_layer = embedding_layer\n        self.vocabulary_size = embedding_layer.vocabulary_size\n        self.embedding_size = embedding_layer.embedding_size\n        self.n_layer = n_layer\n        self.enc_layers = []\n        self.dec_layers = []\n        for i in range(n_layer):\n            if (i == 0):\n                self.enc_layers.append(\n                    tl.layers.RNN(\n                        cell=cell_enc(units=n_units), in_channels=self.embedding_size, return_last_state=True\n                    )\n                )\n            else:\n                self.enc_layers.append(\n                    tl.layers.RNN(cell=cell_enc(units=n_units), in_channels=n_units, return_last_state=True)\n                )\n\n        for i in range(n_layer):\n            if (i == 0):\n                self.dec_layers.append(\n                    tl.layers.RNN(\n                        cell=cell_dec(units=n_units), in_channels=self.embedding_size, return_last_state=True\n                    )\n                )\n            else:\n                self.dec_layers.append(\n                    tl.layers.RNN(cell=cell_dec(units=n_units), in_channels=n_units, return_last_state=True)\n                )\n\n        self.reshape_layer = tl.layers.Reshape([-1, n_units])\n        self.dense_layer = tl.layers.Dense(n_units=self.vocabulary_size, in_channels=n_units)\n        self.reshape_layer_after = tl.layers.Reshape([-1, decoder_seq_length, self.vocabulary_size])\n        self.reshape_layer_individual_sequence = tl.layers.Reshape([-1, 1, self.vocabulary_size])\n\n    def inference(self, encoding, seq_length, start_token, top_n):\n        """"""Inference mode""""""\n        """"""\n        Parameters\n        ----------\n        encoding : input tensor\n            The source sequences \n        seq_length : int\n            The expected length of your predicted sequence.\n        start_token : int\n            <SOS> : The token of ""start of sequence""\n        top_n : int\n            Random search algorithm based on the top top_n words sorted by the probablity. \n        """"""\n        feed_output = self.embedding_layer(encoding[0])\n        state = [None for i in range(self.n_layer)]\n\n        for i in range(self.n_layer):\n            feed_output, state[i] = self.enc_layers[i](feed_output, return_state=True)\n        batch_size = len(encoding[0].numpy())\n        decoding = [[start_token] for i in range(batch_size)]\n        feed_output = self.embedding_layer(decoding)\n        for i in range(self.n_layer):\n            feed_output, state[i] = self.dec_layers[i](feed_output, initial_state=state[i], return_state=True)\n\n        feed_output = self.reshape_layer(feed_output)\n        feed_output = self.dense_layer(feed_output)\n        feed_output = self.reshape_layer_individual_sequence(feed_output)\n        feed_output = tf.argmax(feed_output, -1)\n        # [B, 1]\n        final_output = feed_output\n\n        for i in range(seq_length - 1):\n            feed_output = self.embedding_layer(feed_output)\n            for i in range(self.n_layer):\n                feed_output, state[i] = self.dec_layers[i](feed_output, initial_state=state[i], return_state=True)\n            feed_output = self.reshape_layer(feed_output)\n            feed_output = self.dense_layer(feed_output)\n            feed_output = self.reshape_layer_individual_sequence(feed_output)\n            ori_feed_output = feed_output\n            if (top_n is not None):\n                for k in range(batch_size):\n                    idx = np.argpartition(ori_feed_output[k][0], -top_n)[-top_n:]\n                    probs = [ori_feed_output[k][0][i] for i in idx]\n                    probs = probs / np.sum(probs)\n                    feed_output = np.random.choice(idx, p=probs)\n                    feed_output = tf.convert_to_tensor([[feed_output]], dtype=tf.int64)\n                    if (k == 0):\n                        final_output_temp = feed_output\n                    else:\n                        final_output_temp = tf.concat([final_output_temp, feed_output], 0)\n                feed_output = final_output_temp\n            else:\n                feed_output = tf.argmax(feed_output, -1)\n            final_output = tf.concat([final_output, feed_output], 1)\n\n        return final_output, state\n\n    def forward(self, inputs, seq_length=20, start_token=None, return_state=False, top_n=None):\n\n        state = [None for i in range(self.n_layer)]\n        if (self.is_train):\n            encoding = inputs[0]\n            enc_output = self.embedding_layer(encoding)\n\n            for i in range(self.n_layer):\n                enc_output, state[i] = self.enc_layers[i](enc_output, return_state=True)\n\n            decoding = inputs[1]\n            dec_output = self.embedding_layer(decoding)\n\n            for i in range(self.n_layer):\n                dec_output, state[i] = self.dec_layers[i](dec_output, initial_state=state[i], return_state=True)\n\n            dec_output = self.reshape_layer(dec_output)\n            denser_output = self.dense_layer(dec_output)\n            output = self.reshape_layer_after(denser_output)\n        else:\n            encoding = inputs\n            output, state = self.inference(encoding, seq_length, start_token, top_n)\n\n        if (return_state):\n            return output, state\n        else:\n            return output\n'"
tensorlayer/models/seq2seq_with_attention.py,32,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import Dense, Dropout, Input\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.models import Model\n\n__all__ = [\'Seq2seqLuongAttention\']\n\n\nclass Encoder(Layer):\n\n    def __init__(self, hidden_size, cell, embedding_layer, name=None):\n        super(Encoder, self).__init__(name)\n        self.cell = cell(hidden_size)\n        self.hidden_size = hidden_size\n        self.embedding_layer = embedding_layer\n        self.build((None, None, self.embedding_layer.embedding_size))\n        self._built = True\n\n    def build(self, inputs_shape):\n        self.cell.build(input_shape=tuple(inputs_shape))\n        self._built = True\n        if self._trainable_weights is None:\n            self._trainable_weights = list()\n\n        for var in self.cell.trainable_variables:\n            self._trainable_weights.append(var)\n\n    def forward(self, src_seq, initial_state=None):\n\n        states = initial_state if initial_state is not None else self.cell.get_initial_state(src_seq)\n        encoding_hidden_states = list()\n        total_steps = src_seq.get_shape().as_list()[1]\n        for time_step in range(total_steps):\n            if not isinstance(states, list):\n                states = [states]\n            output, states = self.cell.call(src_seq[:, time_step, :], states, training=self.is_train)\n            encoding_hidden_states.append(states[0])\n        return output, encoding_hidden_states, states[0]\n\n\nclass Decoder_Attention(Layer):\n\n    def __init__(self, hidden_size, cell, embedding_layer, method, name=None):\n        super(Decoder_Attention, self).__init__(name)\n        self.cell = cell(hidden_size)\n        self.hidden_size = hidden_size\n        self.embedding_layer = embedding_layer\n        self.method = method\n        self.build((None, hidden_size + self.embedding_layer.embedding_size))\n        self._built = True\n\n    def build(self, inputs_shape):\n        self.cell.build(input_shape=tuple(inputs_shape))\n        self._built = True\n        if self.method is ""concat"":\n            self.W = self._get_weights(""W"", shape=(2 * self.hidden_size, self.hidden_size))\n            self.V = self._get_weights(""V"", shape=(self.hidden_size, 1))\n        elif self.method is ""general"":\n            self.W = self._get_weights(""W"", shape=(self.hidden_size, self.hidden_size))\n        if self._trainable_weights is None:\n            self._trainable_weights = list()\n\n        for var in self.cell.trainable_variables:\n            self._trainable_weights.append(var)\n\n    def score(self, encoding_hidden, hidden, method):\n        # encoding = [B, T, H]\n        # hidden = [B, H]\n        # combined = [B,T,2H]\n        if method is ""concat"":\n            # hidden = [B,H]->[B,1,H]->[B,T,H]\n            hidden = tf.expand_dims(hidden, 1)\n            hidden = tf.tile(hidden, [1, encoding_hidden.shape[1], 1])\n            # combined = [B,T,2H]\n            combined = tf.concat([hidden, encoding_hidden], 2)\n            combined = tf.cast(combined, tf.float32)\n            score = tf.tensordot(combined, self.W, axes=[[2], [0]])  # score = [B,T,H]\n            score = tf.nn.tanh(score)  # score = [B,T,H]\n            score = tf.tensordot(self.V, score, axes=[[0], [2]])  # score = [1,B,T]\n            score = tf.squeeze(score, axis=0)  # score = [B,T]\n\n        elif method is ""dot"":\n            # hidden = [B,H]->[B,H,1]\n            hidden = tf.expand_dims(hidden, 2)\n            score = tf.matmul(encoding_hidden, hidden)\n            score = tf.squeeze(score, axis=2)\n        elif method is ""general"":\n            # hidden = [B,H]->[B,H,1]\n            score = tf.matmul(hidden, self.W)\n            score = tf.expand_dims(score, 2)\n            score = tf.matmul(encoding_hidden, score)\n            score = tf.squeeze(score, axis=2)\n\n        score = tf.nn.softmax(score, axis=-1)  # score = [B,T]\n        return score\n\n    def forward(self, dec_seq, enc_hiddens, last_hidden, method, return_last_state=False):\n        # dec_seq = [B, T_, V], enc_hiddens = [B, T, H], last_hidden = [B, H]\n        total_steps = dec_seq.get_shape().as_list()[1]\n        states = last_hidden\n        cell_outputs = list()\n        for time_step in range(total_steps):\n            attention_weights = self.score(enc_hiddens, last_hidden, method)\n            attention_weights = tf.expand_dims(attention_weights, 1)  #[B, 1, T]\n            context = tf.matmul(attention_weights, enc_hiddens)  #[B, 1, H]\n            context = tf.squeeze(context, 1)  #[B, H]\n            inputs = tf.concat([dec_seq[:, time_step, :], context], 1)\n            if not isinstance(states, list):\n                states = [states]\n            cell_output, states = self.cell.call(inputs, states, training=self.is_train)\n            cell_outputs.append(cell_output)\n            last_hidden = states[0]\n\n        cell_outputs = tf.convert_to_tensor(cell_outputs)\n        cell_outputs = tf.transpose(cell_outputs, perm=[1, 0, 2])\n        if (return_last_state):\n            return cell_outputs, last_hidden\n        return cell_outputs\n\n\nclass Seq2seqLuongAttention(Model):\n    """"""Luong Attention-based Seq2Seq model. Implementation based on https://arxiv.org/pdf/1508.04025.pdf.\n\n    Parameters\n    ----------\n    hidden_size: int\n        The hidden size of both encoder and decoder RNN cells\n    cell : TensorFlow cell function\n        The RNN function cell for your encoder and decoder stack, e.g. tf.keras.layers.GRUCell\n    embedding_layer : tl.Layer\n        A embedding layer, e.g. tl.layers.Embedding(vocabulary_size=voc_size, embedding_size=emb_dim)\n    method : str\n        The three alternatives to calculate the attention scores, e.g. ""dot"", ""general"" and ""concat""\n    name : str\n        The model name\n    \n\n    Returns\n    -------\n        static single layer attention-based Seq2Seq model.\n    """"""\n\n    def __init__(self, hidden_size, embedding_layer, cell, method, name=None):\n        super(Seq2seqLuongAttention, self).__init__(name)\n        self.enc_layer = Encoder(hidden_size, cell, embedding_layer)\n        self.dec_layer = Decoder_Attention(hidden_size, cell, embedding_layer, method=method)\n        self.embedding_layer = embedding_layer\n        self.dense_layer = tl.layers.Dense(n_units=self.embedding_layer.vocabulary_size, in_channels=hidden_size)\n        self.method = method\n\n    def inference(self, src_seq, encoding_hidden_states, last_hidden_states, seq_length, sos):\n        """"""Inference mode""""""\n        """"""\n        Parameters\n        ----------\n        src_seq : input tensor\n            The source sequences \n        encoding_hidden_states : a list of tensor\n            The list of encoder\'s hidden states at each time step\n        last_hidden_states: tensor\n            The last hidden_state from encoder\n        seq_length : int\n            The expected length of your predicted sequence.\n        sos : int\n            <SOS> : The token of ""start of sequence""\n        """"""\n\n        batch_size = src_seq.shape[0]\n        decoding = [[sos] for i in range(batch_size)]\n        dec_output = self.embedding_layer(decoding)\n        outputs = [[0] for i in range(batch_size)]\n        for step in range(seq_length):\n            dec_output, last_hidden_states = self.dec_layer(\n                dec_output, encoding_hidden_states, last_hidden_states, method=self.method, return_last_state=True\n            )\n            dec_output = tf.reshape(dec_output, [-1, dec_output.shape[-1]])\n            dec_output = self.dense_layer(dec_output)\n            dec_output = tf.reshape(dec_output, [batch_size, -1, dec_output.shape[-1]])\n            dec_output = tf.argmax(dec_output, -1)\n            outputs = tf.concat([outputs, dec_output], 1)\n            dec_output = self.embedding_layer(dec_output)\n\n        return outputs[:, 1:]\n\n    def forward(self, inputs, seq_length=20, sos=None):\n        src_seq = inputs[0]\n        src_seq = self.embedding_layer(src_seq)\n        enc_output, encoding_hidden_states, last_hidden_states = self.enc_layer(src_seq)\n        encoding_hidden_states = tf.convert_to_tensor(encoding_hidden_states)\n        encoding_hidden_states = tf.transpose(encoding_hidden_states, perm=[1, 0, 2])\n        last_hidden_states = tf.convert_to_tensor(last_hidden_states)\n\n        if (self.is_train):\n            dec_seq = inputs[1]\n            dec_seq = self.embedding_layer(dec_seq)\n            dec_output = self.dec_layer(dec_seq, encoding_hidden_states, last_hidden_states, method=self.method)\n            batch_size = dec_output.shape[0]\n            dec_output = tf.reshape(dec_output, [-1, dec_output.shape[-1]])\n            dec_output = self.dense_layer(dec_output)\n            dec_output = tf.reshape(dec_output, [batch_size, -1, dec_output.shape[-1]])\n        else:\n            dec_output = self.inference(src_seq, encoding_hidden_states, last_hidden_states, seq_length, sos)\n\n        return dec_output\n'"
tensorlayer/models/squeezenetv1.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""SqueezeNet for ImageNet.""""""\n\nimport os\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging\nfrom tensorlayer.files import (assign_weights, load_npz, maybe_download_and_extract)\nfrom tensorlayer.layers import (Concat, Conv2d, Dropout, GlobalMeanPool2d, Input, Lambda, MaxPool2d)\nfrom tensorlayer.models import Model\n\n__all__ = [\n    \'SqueezeNetV1\',\n]\n\nlayer_names = [\n    \'conv1\', \'maxpool1\', \'fire2\', \'fire3\', \'fire4\', \'fire5\', \'fire6\', \'fire7\', \'fire8\', \'fire9\', \'drop1\', \'out\'\n]\nn_filters = [16, 16, 32, 32, 48, 48, 64, 64]\n\n\ndef fire_block(n, n_filter, max_pool=False, name=\'fire_block\'):\n    n = Conv2d(n_filter, (1, 1), (1, 1), tf.nn.relu, \'SAME\', name=name + \'.squeeze1x1\')(n)\n    n1 = Conv2d(n_filter * 4, (1, 1), (1, 1), tf.nn.relu, \'SAME\', name=name + \'.expand1x1\')(n)\n    n2 = Conv2d(n_filter * 4, (3, 3), (1, 1), tf.nn.relu, \'SAME\', name=name + \'.expand3x3\')(n)\n    n = Concat(-1, name=name + \'.concat\')([n1, n2])\n    if max_pool:\n        n = MaxPool2d((3, 3), (2, 2), \'VALID\', name=name + \'.max\')(n)\n    return n\n\n\ndef restore_params(network, path=\'models\'):\n    logging.info(""Restore pre-trained parameters"")\n    maybe_download_and_extract(\n        \'squeezenet.npz\', path, \'https://github.com/tensorlayer/pretrained-models/raw/master/models/\',\n        expected_bytes=7405613\n    )  # ls -al\n    params = load_npz(name=os.path.join(path, \'squeezenet.npz\'))\n    assign_weights(params[:len(network.all_weights)], network)\n    del params\n\n\ndef SqueezeNetV1(pretrained=False, end_with=\'out\', name=None):\n    """"""Pre-trained SqueezeNetV1 model (static mode). Input shape [?, 224, 224, 3], value range [0, 1].\n\n    Parameters\n    ------------\n    pretrained : boolean\n        Whether to load pretrained weights. Default False.\n    end_with : str\n        The end point of the model [conv1, maxpool1, fire2, fire3, fire4, ..., out]. Default ``out`` i.e. the whole model.\n    name : None or str\n        Name for this model.\n\n    Examples\n    ---------\n    Classify ImageNet classes, see `tutorial_models_squeezenetv1.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_models_squeezenetv1.py>`__\n\n    >>> # get the whole model\n    >>> squeezenet = tl.models.SqueezeNetV1(pretrained=True)\n    >>> # use for inferencing\n    >>> output = squeezenet(img1, is_train=False)\n    >>> prob = tf.nn.softmax(output)[0].numpy()\n\n    Extract features and Train a classifier with 100 classes\n\n    >>> # get model without the last layer\n    >>> cnn = tl.models.SqueezeNetV1(pretrained=True, end_with=\'drop1\').as_layer()\n    >>> # add one more layer and build new model\n    >>> ni = Input([None, 224, 224, 3], name=""inputs"")\n    >>> nn = cnn(ni)\n    >>> nn = Conv2d(100, (1, 1), (1, 1), padding=\'VALID\', name=\'conv10\')(nn)\n    >>> nn = GlobalMeanPool2d(name=\'globalmeanpool\')(nn)\n    >>> model = tl.models.Model(inputs=ni, outputs=nn)\n    >>> # train your own classifier (only update the last layer)\n    >>> train_params = model.get_layer(\'conv10\').trainable_weights\n\n    Returns\n    -------\n        static SqueezeNetV1.\n\n    """"""\n    ni = Input([None, 224, 224, 3], name=""input"")\n    n = Lambda(lambda x: x * 255, name=\'scale\')(ni)\n\n    for i in range(len(layer_names)):\n        if layer_names[i] == \'conv1\':\n            n = Conv2d(64, (3, 3), (2, 2), tf.nn.relu, \'SAME\', name=\'conv1\')(n)\n        elif layer_names[i] == \'maxpool1\':\n            n = MaxPool2d((3, 3), (2, 2), \'VALID\', name=\'maxpool1\')(n)\n        elif layer_names[i] == \'drop1\':\n            n = Dropout(keep=0.5, name=\'drop1\')(n)\n        elif layer_names[i] == \'out\':\n            n = Conv2d(1000, (1, 1), (1, 1), padding=\'VALID\', name=\'conv10\')(n)  # 13, 13, 1000\n            n = GlobalMeanPool2d(name=\'globalmeanpool\')(n)\n        elif layer_names[i] in [\'fire3\', \'fire5\']:\n            n = fire_block(n, n_filters[i - 2], max_pool=True, name=layer_names[i])\n        else:\n            n = fire_block(n, n_filters[i - 2], max_pool=False, name=layer_names[i])\n\n        if layer_names[i] == end_with:\n            break\n\n    network = Model(inputs=ni, outputs=n, name=name)\n\n    if pretrained:\n        restore_params(network)\n\n    return network\n'"
tensorlayer/models/vgg.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nVGG for ImageNet.\n\nIntroduction\n----------------\nVGG is a convolutional neural network model proposed by K. Simonyan and A. Zisserman\nfrom the University of Oxford in the paper ""Very Deep Convolutional Networks for\nLarge-Scale Image Recognition""  . The model achieves 92.7% top-5 test accuracy in ImageNet,\nwhich is a dataset of over 14 million images belonging to 1000 classes.\n\nDownload Pre-trained Model\n----------------------------\n- Model weights in this example - vgg16_weights.npz : http://www.cs.toronto.edu/~frossard/post/vgg16/\n- Model weights in this example - vgg19.npy : https://media.githubusercontent.com/media/tensorlayer/pretrained-models/master/models/\n- Caffe VGG 16 model : https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n- Tool to convert the Caffe models to TensorFlow\'s : https://github.com/ethereon/caffe-tensorflow\n\nNote\n------\n- For simplified CNN layer see ""Convolutional layer (Simplified)""\nin read the docs website.\n- When feeding other images to the model be sure to properly resize or crop them\nbeforehand. Distorted images might end up being misclassified. One way of safely\nfeeding images of multiple sizes is by doing center cropping.\n\n""""""\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.files import assign_weights, maybe_download_and_extract\nfrom tensorlayer.layers import (BatchNorm, Conv2d, Dense, Flatten, Input, Lambda, LayerList, MaxPool2d)\nfrom tensorlayer.models import Model\n\n__all__ = [\n    \'VGG\',\n    \'vgg16\',\n    \'vgg19\',\n    \'VGG16\',\n    \'VGG19\',\n    #    \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    #    \'vgg19_bn\', \'vgg19\',\n]\n\nlayer_names = [\n    [\'conv1_1\', \'conv1_2\'], \'pool1\', [\'conv2_1\', \'conv2_2\'], \'pool2\',\n    [\'conv3_1\', \'conv3_2\', \'conv3_3\', \'conv3_4\'], \'pool3\', [\'conv4_1\', \'conv4_2\', \'conv4_3\', \'conv4_4\'], \'pool4\',\n    [\'conv5_1\', \'conv5_2\', \'conv5_3\', \'conv5_4\'], \'pool5\', \'flatten\', \'fc1_relu\', \'fc2_relu\', \'outputs\'\n]\n\ncfg = {\n    \'A\': [[64], \'M\', [128], \'M\', [256, 256], \'M\', [512, 512], \'M\', [512, 512], \'M\', \'F\', \'fc1\', \'fc2\', \'O\'],\n    \'B\': [[64, 64], \'M\', [128, 128], \'M\', [256, 256], \'M\', [512, 512], \'M\', [512, 512], \'M\', \'F\', \'fc1\', \'fc2\', \'O\'],\n    \'D\':\n        [\n            [64, 64], \'M\', [128, 128], \'M\', [256, 256, 256], \'M\', [512, 512, 512], \'M\', [512, 512, 512], \'M\', \'F\',\n            \'fc1\', \'fc2\', \'O\'\n        ],\n    \'E\':\n        [\n            [64, 64], \'M\', [128, 128], \'M\', [256, 256, 256, 256], \'M\', [512, 512, 512, 512], \'M\', [512, 512, 512, 512],\n            \'M\', \'F\', \'fc1\', \'fc2\', \'O\'\n        ],\n}\n\nmapped_cfg = {\n    \'vgg11\': \'A\',\n    \'vgg11_bn\': \'A\',\n    \'vgg13\': \'B\',\n    \'vgg13_bn\': \'B\',\n    \'vgg16\': \'D\',\n    \'vgg16_bn\': \'D\',\n    \'vgg19\': \'E\',\n    \'vgg19_bn\': \'E\'\n}\n\nmodel_urls = {\n    \'vgg16\': \'http://www.cs.toronto.edu/~frossard/vgg16/\',\n    \'vgg19\': \'https://media.githubusercontent.com/media/tensorlayer/pretrained-models/master/models/\'\n}\n\nmodel_saved_name = {\'vgg16\': \'vgg16_weights.npz\', \'vgg19\': \'vgg19.npy\'}\n\n\nclass VGG(Model):\n\n    def __init__(self, layer_type, batch_norm=False, end_with=\'outputs\', name=None):\n        super(VGG, self).__init__(name=name)\n        self.end_with = end_with\n\n        config = cfg[mapped_cfg[layer_type]]\n        self.layers = make_layers(config, batch_norm, end_with)\n\n    def forward(self, inputs):\n        """"""\n        inputs : tensor\n            Shape [None, 224, 224, 3], value range [0, 1].\n        """"""\n\n        inputs = inputs * 255 - np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([1, 1, 1, 3])\n\n        out = self.layers(inputs)\n        return out\n\n\ndef make_layers(config, batch_norm=False, end_with=\'outputs\'):\n    layer_list = []\n    is_end = False\n    for layer_group_idx, layer_group in enumerate(config):\n        if isinstance(layer_group, list):\n            for idx, layer in enumerate(layer_group):\n                layer_name = layer_names[layer_group_idx][idx]\n                n_filter = layer\n                if idx == 0:\n                    if layer_group_idx > 0:\n                        in_channels = config[layer_group_idx - 2][-1]\n                    else:\n                        in_channels = 3\n                else:\n                    in_channels = layer_group[idx - 1]\n                layer_list.append(\n                    Conv2d(\n                        n_filter=n_filter, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\',\n                        in_channels=in_channels, name=layer_name\n                    )\n                )\n                if batch_norm:\n                    layer_list.append(BatchNorm())\n                if layer_name == end_with:\n                    is_end = True\n                    break\n        else:\n            layer_name = layer_names[layer_group_idx]\n            if layer_group == \'M\':\n                layer_list.append(MaxPool2d(filter_size=(2, 2), strides=(2, 2), padding=\'SAME\', name=layer_name))\n            elif layer_group == \'O\':\n                layer_list.append(Dense(n_units=1000, in_channels=4096, name=layer_name))\n            elif layer_group == \'F\':\n                layer_list.append(Flatten(name=\'flatten\'))\n            elif layer_group == \'fc1\':\n                layer_list.append(Dense(n_units=4096, act=tf.nn.relu, in_channels=512 * 7 * 7, name=layer_name))\n            elif layer_group == \'fc2\':\n                layer_list.append(Dense(n_units=4096, act=tf.nn.relu, in_channels=4096, name=layer_name))\n            if layer_name == end_with:\n                is_end = True\n        if is_end:\n            break\n    return LayerList(layer_list)\n\n\ndef restore_model(model, layer_type):\n    logging.info(""Restore pre-trained weights"")\n    # download weights\n    maybe_download_and_extract(model_saved_name[layer_type], \'models\', model_urls[layer_type])\n    weights = []\n    if layer_type == \'vgg16\':\n        npz = np.load(os.path.join(\'models\', model_saved_name[layer_type]), allow_pickle=True)\n        # get weight list\n        for val in sorted(npz.items()):\n            logging.info(""  Loading weights %s in %s"" % (str(val[1].shape), val[0]))\n            weights.append(val[1])\n            if len(model.all_weights) == len(weights):\n                break\n    elif layer_type == \'vgg19\':\n        npz = np.load(os.path.join(\'models\', model_saved_name[layer_type]), allow_pickle=True, encoding=\'latin1\').item()\n        # get weight list\n        for val in sorted(npz.items()):\n            logging.info(""  Loading %s in %s"" % (str(val[1][0].shape), val[0]))\n            logging.info(""  Loading %s in %s"" % (str(val[1][1].shape), val[0]))\n            weights.extend(val[1])\n            if len(model.all_weights) == len(weights):\n                break\n    # assign weight values\n    assign_weights(weights, model)\n    del weights\n\n\ndef VGG_static(layer_type, batch_norm=False, end_with=\'outputs\', name=None):\n    ni = Input([None, 224, 224, 3])\n    n = Lambda(\n        lambda x: x * 255 - np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([1, 1, 1, 3]), name=\'scale\'\n    )(ni)\n\n    config = cfg[mapped_cfg[layer_type]]\n    layers = make_layers(config, batch_norm, end_with)\n\n    nn = layers(n)\n\n    M = Model(inputs=ni, outputs=nn, name=name)\n    return M\n\n\ndef vgg16(pretrained=False, end_with=\'outputs\', mode=\'dynamic\', name=None):\n    """"""Pre-trained VGG16 model.\n\n    Parameters\n    ------------\n    pretrained : boolean\n        Whether to load pretrained weights. Default False.\n    end_with : str\n        The end point of the model. Default ``fc3_relu`` i.e. the whole model.\n    mode : str.\n        Model building mode, \'dynamic\' or \'static\'. Default \'dynamic\'.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    Classify ImageNet classes with VGG16, see `tutorial_models_vgg.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_models_vgg.py>`__\n    With TensorLayer\n\n    >>> # get the whole model, without pre-trained VGG parameters\n    >>> vgg = tl.models.vgg16()\n    >>> # get the whole model, restore pre-trained VGG parameters\n    >>> vgg = tl.models.vgg16(pretrained=True)\n    >>> # use for inferencing\n    >>> output = vgg(img, is_train=False)\n    >>> probs = tf.nn.softmax(output)[0].numpy()\n\n    Extract features with VGG16 and Train a classifier with 100 classes\n\n    >>> # get VGG without the last layer\n    >>> cnn = tl.models.vgg16(end_with=\'fc2_relu\', mode=\'static\').as_layer()\n    >>> # add one more layer and build a new model\n    >>> ni = Input([None, 224, 224, 3], name=""inputs"")\n    >>> nn = cnn(ni)\n    >>> nn = tl.layers.Dense(n_units=100, name=\'out\')(nn)\n    >>> model = tl.models.Model(inputs=ni, outputs=nn)\n    >>> # train your own classifier (only update the last layer)\n    >>> train_params = model.get_layer(\'out\').trainable_weights\n\n    Reuse model\n\n    >>> # in dynamic model, we can directly use the same model\n    >>> # in static model\n    >>> vgg_layer = tl.models.vgg16().as_layer()\n    >>> ni_1 = tl.layers.Input([None, 224, 244, 3])\n    >>> ni_2 = tl.layers.Input([None, 224, 244, 3])\n    >>> a_1 = vgg_layer(ni_1)\n    >>> a_2 = vgg_layer(ni_2)\n    >>> M = Model(inputs=[ni_1, ni_2], outputs=[a_1, a_2])\n\n    """"""\n    if mode == \'dynamic\':\n        model = VGG(layer_type=\'vgg16\', batch_norm=False, end_with=end_with, name=name)\n    elif mode == \'static\':\n        model = VGG_static(layer_type=\'vgg16\', batch_norm=False, end_with=end_with, name=name)\n    else:\n        raise Exception(""No such mode %s"" % mode)\n    if pretrained:\n        restore_model(model, layer_type=\'vgg16\')\n    return model\n\n\ndef vgg19(pretrained=False, end_with=\'outputs\', mode=\'dynamic\', name=None):\n    """"""Pre-trained VGG19 model.\n\n    Parameters\n    ------------\n    pretrained : boolean\n        Whether to load pretrained weights. Default False.\n    end_with : str\n        The end point of the model. Default ``fc3_relu`` i.e. the whole model.\n    mode : str.\n        Model building mode, \'dynamic\' or \'static\'. Default \'dynamic\'.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    Classify ImageNet classes with VGG19, see `tutorial_models_vgg.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_models_vgg.py>`__\n    With TensorLayer\n\n    >>> # get the whole model, without pre-trained VGG parameters\n    >>> vgg = tl.models.vgg19()\n    >>> # get the whole model, restore pre-trained VGG parameters\n    >>> vgg = tl.models.vgg19(pretrained=True)\n    >>> # use for inferencing\n    >>> output = vgg(img, is_train=False)\n    >>> probs = tf.nn.softmax(output)[0].numpy()\n\n    Extract features with VGG19 and Train a classifier with 100 classes\n\n    >>> # get VGG without the last layer\n    >>> cnn = tl.models.vgg19(end_with=\'fc2_relu\', mode=\'static\').as_layer()\n    >>> # add one more layer and build a new model\n    >>> ni = Input([None, 224, 224, 3], name=""inputs"")\n    >>> nn = cnn(ni)\n    >>> nn = tl.layers.Dense(n_units=100, name=\'out\')(nn)\n    >>> model = tl.models.Model(inputs=ni, outputs=nn)\n    >>> # train your own classifier (only update the last layer)\n    >>> train_params = model.get_layer(\'out\').trainable_weights\n\n    Reuse model\n\n    >>> # in dynamic model, we can directly use the same model\n    >>> # in static model\n    >>> vgg_layer = tl.models.vgg19().as_layer()\n    >>> ni_1 = tl.layers.Input([None, 224, 244, 3])\n    >>> ni_2 = tl.layers.Input([None, 224, 244, 3])\n    >>> a_1 = vgg_layer(ni_1)\n    >>> a_2 = vgg_layer(ni_2)\n    >>> M = Model(inputs=[ni_1, ni_2], outputs=[a_1, a_2])\n\n    """"""\n    if mode == \'dynamic\':\n        model = VGG(layer_type=\'vgg19\', batch_norm=False, end_with=end_with, name=name)\n    elif mode == \'static\':\n        model = VGG_static(layer_type=\'vgg19\', batch_norm=False, end_with=end_with, name=name)\n    else:\n        raise Exception(""No such mode %s"" % mode)\n    if pretrained:\n        restore_model(model, layer_type=\'vgg19\')\n    return model\n\n\nVGG16 = vgg16\nVGG19 = vgg19\n\n# models without pretrained parameters\n# def vgg11(pretrained=False, end_with=\'outputs\'):\n#     model = VGG(layer_type=\'vgg11\', batch_norm=False, end_with=end_with)\n#     if pretrained:\n#         model.restore_weights()\n#     return model\n#\n#\n# def vgg11_bn(pretrained=False, end_with=\'outputs\'):\n#     model = VGG(layer_type=\'vgg11_bn\', batch_norm=True, end_with=end_with)\n#     if pretrained:\n#         model.restore_weights()\n#     return model\n#\n#\n# def vgg13(pretrained=False, end_with=\'outputs\'):\n#     model = VGG(layer_type=\'vgg13\', batch_norm=False, end_with=end_with)\n#     if pretrained:\n#         model.restore_weights()\n#     return model\n#\n#\n# def vgg13_bn(pretrained=False, end_with=\'outputs\'):\n#     model = VGG(layer_type=\'vgg13_bn\', batch_norm=True, end_with=end_with)\n#     if pretrained:\n#         model.restore_weights()\n#     return model\n#\n#\n# def vgg16_bn(pretrained=False, end_with=\'outputs\'):\n#     model = VGG(layer_type=\'vgg16_bn\', batch_norm=True, end_with=end_with)\n#     if pretrained:\n#         model.restore_weights()\n#     return model\n#\n#\n# def vgg19_bn(pretrained=False, end_with=\'outputs\'):\n#     model = VGG(layer_type=\'vgg19_bn\', batch_norm=True, end_with=end_with)\n#     if pretrained:\n#         model.restore_weights()\n#     return model\n'"
tensorlayer/optimizers/__init__.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nTensorLayer provides rich layer implementations trailed for\nvarious benchmarks and domain-specific problems. In addition, we also\nsupport transparent access to native TensorFlow parameters.\nFor example, we provide not only layers for local response normalization, but also\nlayers that allow user to apply ``tf.nn.lrn`` on ``network.outputs``.\nMore functions can be found in `TensorFlow API <https://www.tensorflow.org/versions/master/api_docs/index.html>`__.\n""""""\n\nfrom .amsgrad import AMSGrad\n'"
tensorlayer/optimizers/amsgrad.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""AMSGrad Implementation based on the paper: ""On the Convergence of Adam and Beyond"" (ICLR 2018)\nArticle Link: https://openreview.net/pdf?id=ryQu7f-RZ\nOriginal Implementation by: https://github.com/taki0112/AMSGrad-Tensorflow\n""""""\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import (control_flow_ops, math_ops, resource_variable_ops, state_ops, variable_scope)\nfrom tensorflow.python.training import optimizer\n\n\nclass AMSGrad(optimizer.Optimizer):\n    """"""Implementation of the AMSGrad optimization algorithm.\n\n    See: `On the Convergence of Adam and Beyond - [Reddi et al., 2018] <https://openreview.net/pdf?id=ryQu7f-RZ>`__.\n\n    Parameters\n    ----------\n    learning_rate: float\n        A Tensor or a floating point value.  The learning rate.\n    beta1: float\n        A float value or a constant float tensor.\n        The exponential decay rate for the 1st moment estimates.\n    beta2: float\n        A float value or a constant float tensor.\n        The exponential decay rate for the 2nd moment estimates.\n    epsilon: float\n        A small constant for numerical stability.\n        This epsilon is ""epsilon hat"" in the Kingma and Ba paper\n        (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper.\n    use_locking: bool\n        If True use locks for update operations.\n    name: str\n        Optional name for the operations created when applying gradients.\n        Defaults to ""AMSGrad"".\n    """"""\n\n    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.99, epsilon=1e-8, use_locking=False, name=""AMSGrad""):\n        """"""Construct a new Adam optimizer.""""""\n        super(AMSGrad, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n\n        self._lr_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n\n        self._beta1_power = None\n        self._beta2_power = None\n\n    def _create_slots(self, var_list):\n        first_var = min(var_list, key=lambda x: x.name)\n\n        create_new = self._beta1_power is None\n        if not create_new and context.in_graph_mode():\n            create_new = (self._beta1_power.graph is not first_var.graph)\n\n        if create_new:\n            with ops.colocate_with(first_var):\n                self._beta1_power = variable_scope.variable(self._beta1, name=""beta1_power"", trainable=False)\n                self._beta2_power = variable_scope.variable(self._beta2, name=""beta2_power"", trainable=False)\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n            self._zeros_slot(v, ""vhat"", self._name)\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr)\n        self._beta1_t = ops.convert_to_tensor(self._beta1)\n        self._beta2_t = ops.convert_to_tensor(self._beta2)\n        self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n\n    def _apply_dense(self, grad, var):\n        beta1_power = math_ops.cast(self._beta1_power, var.dtype.base_dtype)\n        beta2_power = math_ops.cast(self._beta2_power, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * (1 - beta1_t)\n        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n\n        # amsgrad\n        vhat = self.get_slot(var, ""vhat"")\n        vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n        v_sqrt = math_ops.sqrt(vhat_t)\n\n        var_update = state_ops.assign_sub(var, lr * m_t / (v_sqrt + epsilon_t), use_locking=self._use_locking)\n        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n\n    def _resource_apply_dense(self, grad, var):\n        var = var.handle\n        beta1_power = math_ops.cast(self._beta1_power, grad.dtype.base_dtype)\n        beta2_power = math_ops.cast(self._beta2_power, grad.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, grad.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, grad.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, grad.dtype.base_dtype)\n\n        lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"").handle\n        m_scaled_g_values = grad * (1 - beta1_t)\n        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"").handle\n        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n\n        # amsgrad\n        vhat = self.get_slot(var, ""vhat"").handle\n        vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n        v_sqrt = math_ops.sqrt(vhat_t)\n\n        var_update = state_ops.assign_sub(var, lr * m_t / (v_sqrt + epsilon_t), use_locking=self._use_locking)\n        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n\n    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n        beta1_power = math_ops.cast(self._beta1_power, var.dtype.base_dtype)\n        beta2_power = math_ops.cast(self._beta2_power, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * (1 - beta1_t)\n        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = scatter_add(m, indices, m_scaled_g_values)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = scatter_add(v, indices, v_scaled_g_values)\n\n        # amsgrad\n        vhat = self.get_slot(var, ""vhat"")\n        vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n        v_sqrt = math_ops.sqrt(vhat_t)\n        var_update = state_ops.assign_sub(var, lr * m_t / (v_sqrt + epsilon_t), use_locking=self._use_locking)\n        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n\n    def _apply_sparse(self, grad, var):\n        return self._apply_sparse_shared(\n            grad.values,\n            var,\n            grad.indices,\n            lambda x, i, v: state_ops.\n            scatter_add(  # pylint: disable=g-long-lambda\n                x, i, v, use_locking=self._use_locking\n            )\n        )\n\n    def _resource_scatter_add(self, x, i, v):\n        with ops.control_dependencies([resource_variable_ops.resource_scatter_add(x.handle, i, v)]):\n            return x.value()\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        return self._apply_sparse_shared(grad, var, indices, self._resource_scatter_add)\n\n    def _finish(self, update_ops, name_scope):\n        # Update the power accumulators.\n        with ops.control_dependencies(update_ops):\n            with ops.colocate_with(self._beta1_power):\n                update_beta1 = self._beta1_power.assign(\n                    self._beta1_power * self._beta1_t, use_locking=self._use_locking\n                )\n                update_beta2 = self._beta2_power.assign(\n                    self._beta2_power * self._beta2_t, use_locking=self._use_locking\n                )\n        return control_flow_ops.group(*update_ops + [update_beta1, update_beta2], name=name_scope)\n'"
tests/files/test_utils_saveload.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef basic_static_model():\n    ni = Input((None, 24, 24, 3))\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=""conv1"")(ni)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(nn)\n\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=""conv2"")(nn)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(nn)\n\n    nn = Flatten(name=\'flatten\')(nn)\n    nn = Dense(100, act=None, name=""dense1"")(nn)\n    nn = Dense(10, act=None, name=""dense2"")(nn)\n    M = Model(inputs=ni, outputs=nn, name=\'basic_static\')\n    return M\n\n\nclass basic_dynamic_model(Model):\n\n    def __init__(self):\n        super(basic_dynamic_model, self).__init__(name=""basic_dynamic"")\n        self.conv1 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=3, name=""conv1"")\n        self.pool1 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')\n\n        self.conv2 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=16, name=""conv2"")\n        self.pool2 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')\n\n        self.flatten = Flatten(name=\'flatten\')\n        self.dense1 = Dense(100, act=None, in_channels=576, name=""dense1"")\n        self.dense2 = Dense(10, act=None, in_channels=100, name=""dense2"")\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.flatten(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        return x\n\n\nclass Model_Core_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.static_model = basic_static_model()\n        cls.dynamic_model = basic_dynamic_model()\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_hdf5(self):\n        modify_val = np.zeros_like(self.static_model.all_weights[-2].numpy())\n        ori_val = self.static_model.all_weights[-2].numpy()\n        tl.files.save_weights_to_hdf5(""./model_basic.h5"", self.static_model)\n\n        self.static_model.all_weights[-2].assign(modify_val)\n        tl.files.load_hdf5_to_weights_in_order(""./model_basic.h5"", self.static_model)\n        self.assertLess(np.max(np.abs(ori_val - self.static_model.all_weights[-2].numpy())), 1e-7)\n\n        self.static_model.all_weights[-2].assign(modify_val)\n        tl.files.load_hdf5_to_weights(""./model_basic.h5"", self.static_model)\n        self.assertLess(np.max(np.abs(ori_val - self.static_model.all_weights[-2].numpy())), 1e-7)\n\n        ori_weights = self.static_model._all_weights\n        self.static_model._all_weights = self.static_model._all_weights[1:]\n        self.static_model.all_weights[-2].assign(modify_val)\n        tl.files.load_hdf5_to_weights(""./model_basic.h5"", self.static_model, skip=True)\n        self.assertLess(np.max(np.abs(ori_val - self.static_model.all_weights[-2].numpy())), 1e-7)\n        self.static_model._all_weights = ori_weights\n\n    def test_npz(self):\n        modify_val = np.zeros_like(self.dynamic_model.all_weights[-2].numpy())\n        ori_val = self.dynamic_model.all_weights[-2].numpy()\n        tl.files.save_npz(self.dynamic_model.all_weights, ""./model_basic.npz"")\n\n        self.dynamic_model.all_weights[-2].assign(modify_val)\n        tl.files.load_and_assign_npz(""./model_basic.npz"", self.dynamic_model)\n        self.assertLess(np.max(np.abs(ori_val - self.dynamic_model.all_weights[-2].numpy())), 1e-7)\n\n    def test_npz_dict(self):\n        modify_val = np.zeros_like(self.dynamic_model.all_weights[-2].numpy())\n        ori_val = self.dynamic_model.all_weights[-2].numpy()\n        tl.files.save_npz_dict(self.dynamic_model.all_weights, ""./model_basic.npz"")\n\n        self.dynamic_model.all_weights[-2].assign(modify_val)\n        tl.files.load_and_assign_npz_dict(""./model_basic.npz"", self.dynamic_model)\n        self.assertLess(np.max(np.abs(ori_val - self.dynamic_model.all_weights[-2].numpy())), 1e-7)\n\n        ori_weights = self.dynamic_model._all_weights\n        self.dynamic_model._all_weights = self.static_model._all_weights[1:]\n        self.dynamic_model.all_weights[-2].assign(modify_val)\n        tl.files.load_and_assign_npz_dict(""./model_basic.npz"", self.dynamic_model, skip=True)\n        self.assertLess(np.max(np.abs(ori_val - self.dynamic_model.all_weights[-2].numpy())), 1e-7)\n        self.dynamic_model._all_weights = ori_weights\n\n\nif __name__ == \'__main__\':\n\n    unittest.main()\n'"
tests/layers/__init__.py,0,b''
tests/layers/test_layernode.py,9,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn_cell import LSTMCell\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import Model\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass LayerNode_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_net1(self):\n        print('-' * 20, 'test_net1', '-' * 20)\n\n        def get_model(input_shape):\n            ni = Input(input_shape)\n\n            nii = Conv2d(32, filter_size=(3, 3), strides=(1, 1), name='conv1')(ni)\n            nn = Dropout(keep=0.9, name='drop1')(nii)\n\n            conv = Conv2d(32, filter_size=(3, 3), strides=(1, 1), name='conv2')\n            tt = conv(nn)  # conv2_node_0\n            nn = conv(nn)  # conv2_node_1\n\n            # a branch\n            na = Conv2d(64, filter_size=(3, 3), strides=(1, 1), name='conv3')(nn)\n            na = MaxPool2d(name='pool1')(na)\n\n            # b branch\n            nb = MaxPool2d(name='pool2')(nn)\n            nb = conv(nb)  # conv2_node_2\n\n            out = Concat(name='concat')([na, nb])\n            M = Model(inputs=ni, outputs=[out, nn, nb])\n\n            gg = conv(nii)  # this node will not be added since model fixed\n\n            return M\n\n        net = get_model([None, 24, 24, 3])\n\n        for k, v in enumerate(net._node_by_depth):\n            print(k, [x.name for x in v], [x.in_tensors_idxes for x in v])\n\n        all_node_names = []\n        for k, v in enumerate(net._node_by_depth):\n            all_node_names.extend([x.name for x in v])\n\n        self.assertNotIn('conv2_node_0', all_node_names)\n        self.assertNotIn('conv2_node_3', all_node_names)\n\n        self.assertEqual(len(net.all_layers), 8)\n        print(net.all_layers)\n\n        data = np.random.normal(size=[2, 24, 24, 3]).astype(np.float32)\n        out, nn, nb = net(data, is_train=True)\n\n        self.assertEqual(nn.shape, [2, 24, 24, 32])\n        self.assertEqual(nb.shape, [2, 12, 12, 32])\n\n    def test_net2(self):\n        print('-' * 20, 'test_net2', '-' * 20)\n\n        def get_unstack_model(input_shape):\n            ni = Input(input_shape)\n\n            nn = Dropout(keep=0.9)(ni)\n\n            a, b, c = UnStack(axis=-1)(nn)\n\n            b = Flatten()(b)\n            b = Dense(10)(b)\n\n            c = Flatten()(c)\n\n            M = Model(inputs=ni, outputs=[a, b, c])\n            return M\n\n        net = get_unstack_model([None, 24, 24, 3])\n\n        for k, v in enumerate(net._node_by_depth):\n            print(k, [x.name for x in v], [x.in_tensors_idxes for x in v])\n\n        data = np.random.normal(size=[2, 24, 24, 3]).astype(np.float32)\n        out = net(data, is_train=True)\n\n        self.assertEqual(len(out), 3)\n\n    def test_word2vec(self):\n        print('-' * 20, 'test_word2vec', '-' * 20)\n\n        def get_word2vec():\n            vocabulary_size = 800\n            batch_size = 10\n            embedding_size = 60\n            num_sampled = 25\n            inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n            labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n\n            emb_net = tl.layers.Word2vecEmbedding(\n                vocabulary_size=vocabulary_size,\n                embedding_size=embedding_size,\n                num_sampled=num_sampled,\n                activate_nce_loss=True,  # nce loss is activated\n                nce_loss_args={},\n                E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0),\n                nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))),\n                nce_b_init=tl.initializers.constant(value=0.0),\n                name='word2vec_layer',\n            )\n            emb, nce = emb_net([inputs, labels])\n\n            model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce])\n            return model\n\n        net = get_word2vec()\n\n        for k, v in enumerate(net._node_by_depth):\n            print(k, [x.name for x in v], [x.in_tensors_idxes for x in v])\n\n        x = tf.ones(shape=(10, ), dtype=tf.int32)\n        y = tf.ones(shape=(10, 1), dtype=tf.int32)\n        out = net([x, y], is_train=True)\n\n        self.assertEqual(len(out), 2)\n\n    def test_layerlist(self):\n        print('-' * 20, 'layerlist', '-' * 20)\n\n        class MyModel(Model):\n\n            def __init__(self):\n                super(MyModel, self).__init__()\n                self.layers = LayerList([Dense(50, in_channels=100), Dropout(0.9), Dense(10, in_channels=50)])\n\n            def forward(self, x):\n                return self.layers(x)\n\n        net = MyModel()\n        self.assertEqual(net._nodes_fixed, False)\n\n        data = np.random.normal(size=[4, 100]).astype(np.float32)\n        out = net(data, is_train=False)\n\n        self.assertEqual(net._nodes_fixed, True)\n        self.assertEqual(net.layers._nodes_fixed, True)\n        self.assertEqual(net.layers[0]._nodes_fixed, True)\n        self.assertEqual(net.layers[1]._nodes_fixed, True)\n        self.assertEqual(net.layers[2]._nodes_fixed, True)\n\n    def test_ModelLayer(self):\n        print('-' * 20, 'ModelLayer', '-' * 20)\n\n        def MyModel():\n            nii = Input(shape=[None, 100])\n            nn = Dense(50, in_channels=100)(nii)\n            nn = Dropout(0.9)(nn)\n            nn = Dense(10)(nn)\n            M = Model(inputs=nii, outputs=nn)\n            return M\n\n        mlayer = MyModel().as_layer()\n\n        ni = Input(shape=[None, 100])\n        nn = mlayer(ni)\n        nn = Dense(5)(nn)\n        net = Model(inputs=ni, outputs=nn)\n\n        self.assertEqual(net._nodes_fixed, True)\n\n        data = np.random.normal(size=[4, 100]).astype(np.float32)\n        out = net(data, is_train=False)\n\n        self.assertEqual(net._nodes_fixed, True)\n        self.assertEqual(net.all_layers[1]._nodes_fixed, True)\n        self.assertEqual(net.all_layers[1].model._nodes_fixed, True)\n        self.assertEqual(net.all_layers[1].model.all_layers[0]._nodes_fixed, True)\n\n    def test_STN(self):\n        print('-' * 20, 'test STN', '-' * 20)\n\n        def get_model(inputs_shape):\n            ni = Input(inputs_shape)\n\n            ## 1. Localisation network\n            # use MLP as the localisation net\n            nn = Flatten()(ni)\n            nn = Dense(n_units=20, act=tf.nn.tanh)(nn)\n            nn = Dropout(keep=0.8)(nn)\n            # you can also use CNN instead for MLP as the localisation net\n\n            ## 2. Spatial transformer module (sampler)\n            stn = SpatialTransformer2dAffine(out_size=(40, 40), in_channels=20)\n            # s = stn((nn, ni))\n            nn = stn((nn, ni))\n            s = nn\n\n            ## 3. Classifier\n            nn = Conv2d(16, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME')(nn)\n            nn = Conv2d(16, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME')(nn)\n            nn = Flatten()(nn)\n            nn = Dense(n_units=1024, act=tf.nn.relu)(nn)\n            nn = Dense(n_units=10, act=tf.identity)(nn)\n\n            M = Model(inputs=ni, outputs=[nn, s])\n            return M\n\n        net = get_model([None, 40, 40, 1])\n\n        inputs = np.random.randn(2, 40, 40, 1).astype(np.float32)\n        o1, o2 = net(inputs, is_train=True)\n        self.assertEqual(o1.shape, (2, 10))\n        self.assertEqual(o2.shape, (2, 40, 40, 1))\n\n        self.assertEqual(len(net._node_by_depth), 10)\n\n\nif __name__ == '__main__':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/layers/test_layers_activation.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Activation_Layer_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.data = (10 + 10) * np.random.random(size=[10, 5]).astype(np.float32) - 10\n        cls.data2 = (10 + 10) * np.random.random(size=[10, 10, 5]).astype(np.float32) - 10\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_prelu_1(self):\n        inputs = tl.layers.Input([10, 5])\n        prelulayer = tl.layers.PRelu(channel_shared=True)\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data.shape)\n        for i in range(len(gt)):\n            for j in range(len(gt[i])):\n                if self.data[i][j] >= 0:\n                    gt[i][j] = self.data[i][j]\n                else:\n                    gt[i][j] = prelulayer.alpha_var_constrained.numpy() * self.data[i][j]\n\n        self.assertTrue(np.array_equal(out.numpy(), gt))\n\n    def test_prelu_2(self):\n        inputs = tl.layers.Input([10, 5])\n        prelulayer = tl.layers.PRelu(in_channels=5)\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data.shape)\n        for i in range(len(gt)):\n            for j in range(len(gt[i])):\n                if self.data[i][j] >= 0:\n                    gt[i][j] = self.data[i][j]\n                else:\n                    gt[i][j] = prelulayer.alpha_var_constrained.numpy()[j] * self.data[i][j]\n\n        self.assertTrue(np.array_equal(out.numpy(), gt))\n\n    def test_prelu_3(self):\n        inputs = tl.layers.Input([10, 10, 5])\n        prelulayer = tl.layers.PRelu(in_channels=5)\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data2, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data2.shape)\n        for i in range(len(gt)):\n            for k in range(len(gt[i])):\n                for j in range(len(gt[i][k])):\n                    if self.data2[i][k][j] >= 0:\n                        gt[i][k][j] = self.data2[i][k][j]\n                    else:\n                        gt[i][k][j] = prelulayer.alpha_var_constrained.numpy()[j] * self.data2[i][k][j]\n\n        self.assertTrue(np.array_equal(out.numpy(), gt))\n\n    def test_prelu6_1(self):\n        inputs = tl.layers.Input([10, 5])\n        prelulayer = tl.layers.PRelu6(channel_shared=True)\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data.shape)\n        for i in range(len(gt)):\n            for j in range(len(gt[i])):\n                if self.data[i][j] >= 0 and self.data[i][j] <= 6:\n                    gt[i][j] = self.data[i][j]\n                elif self.data[i][j] > 6:\n                    gt[i][j] = 6\n                else:\n                    gt[i][j] = prelulayer.alpha_var_constrained.numpy() * self.data[i][j]\n\n        self.assertTrue(np.array_equal(out.numpy(), gt))\n\n    def test_prelu6_2(self):\n        inputs = tl.layers.Input([10, 5])\n        prelulayer = tl.layers.PRelu6(in_channels=5)\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data.shape)\n        for i in range(len(gt)):\n            for j in range(len(gt[i])):\n                if self.data[i][j] >= 0 and self.data[i][j] <= 6:\n                    gt[i][j] = self.data[i][j]\n                elif self.data[i][j] > 6:\n                    gt[i][j] = 6\n                else:\n                    gt[i][j] = prelulayer.alpha_var_constrained.numpy()[j] * self.data[i][j]\n\n        self.assertTrue(np.array_equal(out.numpy(), gt))\n\n    def test_prelu6_3(self):\n        inputs = tl.layers.Input([10, 10, 5])\n        prelulayer = tl.layers.PRelu6(in_channels=5)\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data2, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data2.shape)\n        for i in range(len(gt)):\n            for k in range(len(gt[i])):\n                for j in range(len(gt[i][k])):\n                    if self.data2[i][k][j] >= 0 and self.data2[i][k][j] <= 6:\n                        gt[i][k][j] = self.data2[i][k][j]\n                    elif self.data2[i][k][j] > 6:\n                        gt[i][k][j] = 6\n                    else:\n                        gt[i][k][j] = prelulayer.alpha_var_constrained.numpy()[j] * self.data2[i][k][j]\n\n        self.assertTrue(np.array_equal(out.numpy(), gt))\n\n    def test_ptrelu6_1(self):\n        inputs = tl.layers.Input([10, 5])\n        prelulayer = tl.layers.PTRelu6(channel_shared=True)\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data.shape)\n        for i in range(len(gt)):\n            for j in range(len(gt[i])):\n                if self.data[i][j] >= 0 and self.data[i][j] <= 6:\n                    gt[i][j] = self.data[i][j]\n                elif self.data[i][j] > 6:\n                    gt[i][j] = 6 + prelulayer.alpha_high_constrained.numpy() * (self.data[i][j] - 6)\n                else:\n                    gt[i][j] = prelulayer.alpha_low_constrained.numpy() * self.data[i][j]\n\n        # FIXME: Figure out why this assert randomly fail in CI.\n        # self.assertTrue(np.array_equal(out.numpy(), gt))\n\n    def test_ptrelu6_2(self):\n        inputs = tl.layers.Input([10, 5])\n        prelulayer = tl.layers.PTRelu6(in_channels=5)\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data.shape)\n        for i in range(len(gt)):\n            for j in range(len(gt[i])):\n                if self.data[i][j] >= 0 and self.data[i][j] <= 6:\n                    gt[i][j] = self.data[i][j]\n                elif self.data[i][j] > 6:\n                    gt[i][j] = 6 + prelulayer.alpha_high_constrained.numpy()[j] * (self.data[i][j] - 6)\n                else:\n                    gt[i][j] = prelulayer.alpha_low_constrained.numpy()[j] * self.data[i][j]\n\n        self.assertTrue(np.allclose(out.numpy(), gt))\n\n    def test_ptrelu6_3(self):\n        inputs = tl.layers.Input([3, 2, 5])\n        prelulayer = tl.layers.PTRelu6()\n        prelu = prelulayer(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=prelu)\n        out = model(self.data2, is_train=True)\n\n        print(prelulayer)\n\n        gt = np.zeros(shape=self.data2.shape)\n        for i in range(len(gt)):\n            for k in range(len(gt[i])):\n                for j in range(len(gt[i][k])):\n                    if self.data2[i][k][j] >= 0 and self.data2[i][k][j] <= 6:\n                        gt[i][k][j] = self.data2[i][k][j]\n                    elif self.data2[i][k][j] > 6:\n                        gt[i][k][j] = 6 + prelulayer.alpha_high_constrained.numpy()[j] * (self.data2[i][k][j] - 6)\n                    else:\n                        gt[i][k][j] = prelulayer.alpha_low_constrained.numpy()[j] * self.data2[i][k][j]\n\n        self.assertTrue(np.allclose(out.numpy(), gt))\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n"""
tests/layers/test_layers_convolution.py,25,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Convolution_1D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""\\n#################################"")\n\n        cls.batch_size = 8\n        cls.inputs_shape = [cls.batch_size, 100, 1]\n        cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n\n        cls.n1 = tl.layers.Conv1dLayer(shape=(5, 1, 32), stride=2)(cls.input_layer)\n\n        cls.n2 = tl.layers.Conv1d(n_filter=32, filter_size=5, stride=2)(cls.n1)\n\n        cls.n3 = tl.layers.DeConv1dLayer(\n            shape=(5, 64, 32), outputs_shape=(cls.batch_size, 50, 64), strides=(1, 2, 1), name=\'deconv1dlayer\'\n        )(cls.n2)\n\n        cls.n4 = tl.layers.SeparableConv1d(\n            n_filter=32, filter_size=3, strides=2, padding=\'SAME\', act=\'relu\', name=\'separable_1d\'\n        )(cls.n3)\n\n        cls.n5 = tl.layers.SubpixelConv1d(scale=2, act=tf.nn.relu, in_channels=32, name=\'subpixel_1d\')(cls.n4)\n\n        cls.model = Model(inputs=cls.input_layer, outputs=cls.n5)\n        print(""Testing Conv1d model: \\n"", cls.model)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n        # tf.reset_default_graph()\n\n    def test_layer_n1(self):\n\n        # self.assertEqual(len(self.n1.all_layers), 2)\n        # self.assertEqual(len(self.n1.all_params), 2)\n        # self.assertEqual(self.n1.count_params(), 192)\n        self.assertEqual(len(self.n1._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n1.get_shape().as_list()[1:], [50, 32])\n\n    def test_layer_n2(self):\n\n        # self.assertEqual(len(self.n2.all_layers), 3)\n        # self.assertEqual(len(self.n2.all_params), 4)\n        # self.assertEqual(self.n2.count_params(), 5344)\n        self.assertEqual(len(self.n2._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n2.get_shape().as_list()[1:], [25, 32])\n\n    def test_layer_n3(self):\n\n        # self.assertEqual(len(self.n2.all_layers), 3)\n        # self.assertEqual(len(self.n2.all_params), 4)\n        # self.assertEqual(self.n2.count_params(), 5344)\n        self.assertEqual(len(self.n3._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n3.get_shape().as_list()[1:], [50, 64])\n\n    def test_layer_n4(self):\n\n        # self.assertEqual(len(self.n2.all_layers), 3)\n        # self.assertEqual(len(self.n2.all_params), 4)\n        # self.assertEqual(self.n2.count_params(), 5344)\n        self.assertEqual(len(self.n4._info[0].layer.all_weights), 3)\n        self.assertEqual(self.n4.get_shape().as_list()[1:], [25, 32])\n\n    def test_layer_n5(self):\n\n        # self.assertEqual(len(self.n2.all_layers), 3)\n        # self.assertEqual(len(self.n2.all_params), 4)\n        # self.assertEqual(self.n2.count_params(), 5344)\n        self.assertEqual(self.n5.get_shape().as_list()[1:], [50, 16])\n\n    # def test_layer_n3(self):\n    #\n    #     self.assertEqual(len(self.n3.all_layers), 4)\n    #     self.assertEqual(len(self.n3.all_params), 7)\n    #     self.assertEqual(self.n3.count_params(), 6496)\n    #     self.assertEqual(self.n3.outputs.get_shape().as_list()[1:], [23, 32])\n\n\n# FIXME: TF2.0 only supports NHWC now\n# class Layer_Convolution_1D_NCW_Test(CustomTestCase):\n#\n#     @classmethod\n#     def setUpClass(cls):\n#         print(""\\n#################################"")\n#\n#         cls.batch_size = 8\n#         cls.inputs_shape = [cls.batch_size, 1, 100]\n#         cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n#\n#         cls.n1 = tl.layers.Conv1dLayer(\n#             shape=(5, 1, 32), stride=2, data_format=""NCW""\n#         )(cls.input_layer)\n#         cls.n2 = tl.layers.Conv1d(\n#             n_filter=32, filter_size=5, stride=2, data_format=\'channels_first\'\n#         )(cls.n1)\n#         cls.model = Model(inputs=cls.input_layer, outputs=cls.n2)\n#         print(""Testing Conv1d model: \\n"", cls.model)\n#\n#         # cls.n3 = tl.layers.SeparableConv1d(\n#         #     cls.n2, n_filter=32, filter_size=3, strides=1, padding=\'VALID\', act=tf.nn.relu, name=\'separable_1d\'\n#         # )\n#\n#     @classmethod\n#     def tearDownClass(cls):\n#         pass\n#         # tf.reset_default_graph()\n#\n#     def test_layer_n1(self):\n#\n#         # self.assertEqual(len(self.n1.all_layers), 2)\n#         # self.assertEqual(len(self.n1.all_params), 2)\n#         # self.assertEqual(self.n1.count_params(), 192)\n#         self.assertEqual(len(self.n1._info[0].layer.all_weights), 2)\n#         self.assertEqual(self.n1.get_shape().as_list()[1:], [50, 32])\n#\n#     def test_layer_n2(self):\n#\n#         # self.assertEqual(len(self.n2.all_layers), 3)\n#         # self.assertEqual(len(self.n2.all_params), 4)\n#         # self.assertEqual(self.n2.count_params(), 5344)\n#         self.assertEqual(len(self.n2._info[0].layer.all_weights), 2)\n#         self.assertEqual(self.n2.get_shape().as_list()[1:], [25, 32])\n#\n#     # def test_layer_n3(self):\n#     #\n#     #     self.assertEqual(len(self.n3.all_layers), 4)\n#     #     self.assertEqual(len(self.n3.all_params), 7)\n#     #     self.assertEqual(self.n3.count_params(), 6496)\n#     #     self.assertEqual(self.n3.outputs.get_shape().as_list()[1:], [23, 32])\n\n\nclass Layer_Convolution_2D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""\\n#################################"")\n\n        cls.batch_size = 5\n        cls.inputs_shape = [cls.batch_size, 400, 400, 3]\n        cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n\n        cls.n1 = tl.layers.Conv2dLayer(\n            act=tf.nn.relu, shape=(5, 5, 3, 32), strides=(1, 2, 2, 1), padding=\'SAME\',\n            b_init=tf.constant_initializer(value=0.0), name=\'conv2dlayer\'\n        )(cls.input_layer)\n\n        cls.n2 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=None, name=\'conv2d\')(cls.n1)\n\n        cls.n3 = tl.layers.Conv2d(\n            n_filter=32, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, b_init=None, name=\'conv2d_no_bias\'\n        )(cls.n2)\n\n        cls.n4 = tl.layers.DeConv2dLayer(\n            shape=(5, 5, 32, 32), outputs_shape=(cls.batch_size, 100, 100, 32), strides=(1, 2, 2, 1),\n            name=\'deconv2dlayer\'\n        )(cls.n3)\n\n        cls.n5 = tl.layers.DeConv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), name=\'DeConv2d\')(cls.n4)\n\n        cls.n6 = tl.layers.DepthwiseConv2d(\n            filter_size=(3, 3), strides=(1, 1), dilation_rate=(2, 2), act=tf.nn.relu, depth_multiplier=2,\n            name=\'depthwise\'\n        )(cls.n5)\n\n        cls.n7 = tl.layers.Conv2d(\n            n_filter=32, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, in_channels=64, name=\'conv2d2\'\n        )(cls.n6)\n\n        cls.n8 = tl.layers.BinaryConv2d(\n            n_filter=64, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, in_channels=32, name=\'binaryconv2d\'\n        )(cls.n7)\n\n        cls.n9 = tl.layers.SeparableConv2d(\n            n_filter=32, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, name=\'separableconv2d\'\n        )(cls.n8)\n\n        cls.n10 = tl.layers.GroupConv2d(n_filter=64, filter_size=(3, 3), strides=(2, 2), n_group=2,\n                                        name=\'group\')(cls.n9)\n\n        cls.n11 = tl.layers.DorefaConv2d(\n            n_filter=32, filter_size=(5, 5), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'dorefaconv2d\'\n        )(cls.n10)\n\n        cls.n12 = tl.layers.TernaryConv2d(\n            n_filter=64, filter_size=(5, 5), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'ternaryconv2d\'\n        )(cls.n11)\n\n        cls.n13 = tl.layers.QuanConv2d(\n            n_filter=32, filter_size=(5, 5), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'quancnn2d\'\n        )(cls.n12)\n\n        cls.n14 = tl.layers.SubpixelConv2d(scale=2, act=tf.nn.relu, name=\'subpixelconv2d\')(cls.n13)\n\n        cls.n15 = tl.layers.QuanConv2dWithBN(\n            n_filter=64, filter_size=(5, 5), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'quancnnbn2d\'\n        )(cls.n14)\n\n        cls.model = Model(cls.input_layer, cls.n15)\n        print(""Testing Conv2d model: \\n"", cls.model)\n\n        # cls.n12 = tl.layers.QuanConv2d(cls.n11, 64, (5, 5), (1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'quancnn\')\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n        # tf.reset_default_graph()\n\n    def test_layer_n1(self):\n\n        # self.assertEqual(len(self.n1.all_layers), 2)\n        # self.assertEqual(len(self.n1.all_params), 2)\n        # self.assertEqual(self.n1.count_params(), 2432)\n        self.assertEqual(len(self.n1._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n1.get_shape().as_list()[1:], [200, 200, 32])\n\n    def test_layer_n2(self):\n\n        # self.assertEqual(len(self.n2.all_layers), 3)\n        # self.assertEqual(len(self.n2.all_params), 4)\n        # self.assertEqual(self.n2.count_params(), 11680)\n        self.assertEqual(len(self.n2._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n2.get_shape().as_list()[1:], [100, 100, 32])\n\n    def test_layer_n3(self):\n\n        # self.assertEqual(len(self.n3.all_layers), 4)\n        # self.assertEqual(len(self.n3.all_params), 5)\n        # self.assertEqual(self.n3.count_params(), 20896)\n        self.assertEqual(len(self.n3._info[0].layer.all_weights), 1)  # b_init is None\n        self.assertEqual(self.n3.get_shape().as_list()[1:], [50, 50, 32])\n\n    def test_layer_n4(self):\n\n        # self.assertEqual(len(self.n4.all_layers), 5)\n        # self.assertEqual(len(self.n4.all_params), 7)\n        # self.assertEqual(self.n4.count_params(), 46528)\n        self.assertEqual(len(self.n4._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n4.get_shape().as_list()[1:], [100, 100, 32])\n\n    def test_layer_n5(self):\n\n        # self.assertEqual(len(self.n5.all_layers), 6)\n        # self.assertEqual(len(self.n5.all_params), 9)\n        # self.assertEqual(self.n5.count_params(), 55776)\n        self.assertEqual(len(self.n5._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n5.get_shape().as_list()[1:], [200, 200, 32])\n\n    def test_layer_n6(self):\n\n        # self.assertEqual(len(self.n6.all_layers), 7)\n        # self.assertEqual(len(self.n6.all_params), 11)\n        # self.assertEqual(self.n6.count_params(), 56416)\n        self.assertEqual(len(self.n6._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n6.get_shape().as_list()[1:], [200, 200, 64])\n\n    def test_layer_n7(self):\n\n        # self.assertEqual(len(self.n7.all_layers), 8)\n        # self.assertEqual(len(self.n7.all_params), 13)\n        # self.assertEqual(self.n7.count_params(), 74880)\n        self.assertEqual(len(self.n7._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n7.get_shape().as_list()[1:], [100, 100, 32])\n\n    def test_layer_n8(self):\n\n        # self.assertEqual(len(self.n7.all_layers), 8)\n        # self.assertEqual(len(self.n7.all_params), 13)\n        # self.assertEqual(self.n7.count_params(), 74880)\n        self.assertEqual(len(self.n8._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n8.get_shape().as_list()[1:], [50, 50, 64])\n\n    def test_layer_n9(self):\n\n        # self.assertEqual(len(self.n7.all_layers), 8)\n        # self.assertEqual(len(self.n7.all_params), 13)\n        # self.assertEqual(self.n7.count_params(), 74880)\n        self.assertEqual(len(self.n9._info[0].layer.all_weights), 3)\n        self.assertEqual(self.n9.get_shape().as_list()[1:], [24, 24, 32])\n\n    def test_layer_n10(self):\n        # self.assertEqual(len(self.n7.all_layers), 8)\n        # self.assertEqual(len(self.n7.all_params), 13)\n        # self.assertEqual(self.n7.count_params(), 74880)\n        self.assertEqual(len(self.n10._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n10.get_shape().as_list()[1:], [12, 12, 64])\n\n    def test_layer_n11(self):\n        # self.assertEqual(len(self.n7.all_layers), 8)\n        # self.assertEqual(len(self.n7.all_params), 13)\n        # self.assertEqual(self.n7.count_params(), 74880)\n        self.assertEqual(len(self.n11._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n11.get_shape().as_list()[1:], [12, 12, 32])\n\n    def test_layer_n12(self):\n        # self.assertEqual(len(self.n7.all_layers), 8)\n        # self.assertEqual(len(self.n7.all_params), 13)\n        # self.assertEqual(self.n7.count_params(), 74880)\n        self.assertEqual(len(self.n12._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n12.get_shape().as_list()[1:], [12, 12, 64])\n\n    def test_layer_n13(self):\n        # self.assertEqual(len(self.n7.all_layers), 8)\n        # self.assertEqual(len(self.n7.all_params), 13)\n        # self.assertEqual(self.n7.count_params(), 74880)\n        self.assertEqual(len(self.n13._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n13.get_shape().as_list()[1:], [12, 12, 32])\n\n    def test_layer_n14(self):\n        self.assertEqual(self.n14.get_shape().as_list()[1:], [24, 24, 8])\n\n    def test_layer_n15(self):\n        self.assertEqual(len(self.n15._info[0].layer.all_weights), 5)\n        self.assertEqual(self.n15.get_shape().as_list()[1:], [24, 24, 64])\n\n    # def test_layer_n8(self):\n    #\n    #     self.assertEqual(len(self.n8.all_layers), 9)\n    #     self.assertEqual(len(self.n8.all_params), 15)\n    #     self.assertEqual(self.n8.count_params(), 79520)\n    #     self.assertEqual(self.n8.outputs.get_shape().as_list()[1:], [50, 50, 32])\n    #\n    # def test_layer_n9(self):\n    #\n    #     self.assertEqual(len(self.n9.all_layers), 10)\n    #     self.assertEqual(len(self.n9.all_params), 18)\n    #     self.assertEqual(self.n9.count_params(), 80864)\n    #     self.assertEqual(self.n9.outputs.get_shape().as_list()[1:], [48, 48, 32])\n    #\n    # def test_layer_n10(self):\n    #\n    #     self.assertEqual(len(self.n10.all_layers), 11)\n    #     self.assertEqual(len(self.n10.all_params), 20)\n    #     self.assertEqual(self.n10.count_params(), 132128)\n    #     self.assertEqual(self.n10.outputs.get_shape().as_list()[1:], [48, 48, 64])\n    #\n    # def test_layer_n11(self):\n    #\n    #     self.assertEqual(len(self.n11.all_layers), 12)\n    #     self.assertEqual(len(self.n11.all_params), 22)\n    #     self.assertEqual(self.n11.count_params(), 150592)\n    #     self.assertEqual(self.n11.outputs.get_shape().as_list()[1:], [96, 96, 32])\n    #\n    # def test_layer_n12(self):\n    #\n    #     self.assertEqual(len(self.n12.all_layers), 13)\n    #     self.assertEqual(len(self.n12.all_params), 24)\n    #     self.assertEqual(self.n12.count_params(), 201856)\n    #     self.assertEqual(self.n12.outputs.get_shape().as_list()[1:], [96, 96, 64])\n\n\nclass Layer_Convolution_3D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""\\n#################################"")\n\n        cls.batch_size = 5\n        cls.inputs_shape = [cls.batch_size, 20, 20, 20, 3]\n        cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n\n        cls.n1 = tl.layers.Conv3dLayer(shape=(2, 2, 2, 3, 32), strides=(1, 2, 2, 2, 1))(cls.input_layer)\n\n        cls.n2 = tl.layers.DeConv3dLayer(\n            shape=(2, 2, 2, 128, 32), outputs_shape=(cls.batch_size, 20, 20, 20, 128), strides=(1, 2, 2, 2, 1)\n        )(cls.n1)\n\n        cls.n3 = tl.layers.Conv3d(\n            n_filter=64, filter_size=(3, 3, 3), strides=(3, 3, 3), act=tf.nn.relu, b_init=None, in_channels=128,\n            name=\'conv3d_no_bias\'\n        )(cls.n2)\n\n        cls.n4 = tl.layers.DeConv3d(n_filter=32, filter_size=(3, 3, 3), strides=(2, 2, 2))(cls.n3)\n\n        cls.model = Model(inputs=cls.input_layer, outputs=cls.n4)\n        print(""Testing Conv3d model: \\n"", cls.model)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n        # tf.reset_default_graph()\n\n    def test_layer_n1(self):\n\n        # self.assertEqual(len(self.n1.all_layers), 2)\n        # self.assertEqual(len(self.n1.all_params), 2)\n        # self.assertEqual(self.n1.count_params(), 800)\n        self.assertEqual(len(self.n1._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n1.get_shape().as_list()[1:], [10, 10, 10, 32])\n\n    def test_layer_n2(self):\n\n        # self.assertEqual(len(self.n2.all_layers), 3)\n        # self.assertEqual(len(self.n2.all_params), 4)\n        # self.assertEqual(self.n2.count_params(), 33696)\n        self.assertEqual(len(self.n2._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n2.get_shape().as_list()[1:], [20, 20, 20, 128])\n\n    def test_layer_n3(self):\n\n        # self.assertEqual(len(self.n3.all_layers), 4)\n        # self.assertEqual(len(self.n3.all_params), 6)\n        # self.assertEqual(self.n3.count_params(), 144320)\n        self.assertEqual(len(self.n3._info[0].layer.all_weights), 1)  # b_init is None\n        self.assertEqual(self.n3.get_shape().as_list()[1:], [7, 7, 7, 64])\n\n    def test_layer_n4(self):\n\n        # self.assertEqual(len(self.n3.all_layers), 4)\n        # self.assertEqual(len(self.n3.all_params), 6)\n        # self.assertEqual(self.n3.count_params(), 144320)\n        self.assertEqual(len(self.n4._info[0].layer.all_weights), 2)\n        self.assertEqual(self.n4.get_shape().as_list()[1:], [14, 14, 14, 32])\n\n\n# class Layer_DeformableConvolution_Test(CustomTestCase):\n#\n#     @classmethod\n#     def setUpClass(cls):\n#\n#         cls.batch_size = 5\n#         cls.inputs_shape = [cls.batch_size, 299, 299, 3]\n#         cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n#\n#         offset1 = tl.layers.Conv2d(\n#             18, (3, 3), (1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'offset1\'\n#         )(cls.input_layer)\n#         cls.net1 = tl.layers.DeformableConv2d(\n#             offset1, 32, (3, 3), act=tf.nn.relu, name=\'deformable1\'\n#         )(cls.input_layer)\n#\n#         offset2 = tl.layers.Conv2d(\n#             18, (3, 3), (1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'offset2\'\n#         )(cls.net1)\n#         cls.net2 = tl.layers.DeformableConv2d(\n#             offset2, 64, (3, 3), act=tf.nn.relu, name=\'deformable2\'\n#         )(cls.net1)\n#\n#     @classmethod\n#     def tearDownClass(cls):\n#         pass\n#\n#     def test_layer_n1(self):\n#\n#         self.assertEqual(len(self.net1.all_layers), 2)\n#         self.assertEqual(len(self.net1.all_params), 2)\n#         self.assertEqual(self.net1.count_params(), 896)\n#         self.assertEqual(self.net1.outputs.get_shape().as_list()[1:], [299, 299, 32])\n#\n#     def test_layer_n2(self):\n#\n#         self.assertEqual(len(self.net2.all_layers), 3)\n#         self.assertEqual(len(self.net2.all_params), 4)\n#         self.assertEqual(self.net2.count_params(), 19392)\n#         self.assertEqual(self.net2.outputs.get_shape().as_list()[1:], [299, 299, 64])\n\n\nclass Exception_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing exception in activation #####"")\n\n    def test_exception(cls):\n\n        cls.batch_size = 5\n        cls.inputs_shape = [cls.batch_size, 400, 400, 3]\n        cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n\n        try:\n            cls.n1 = tl.layers.Conv2dLayer(\n                act=\'activation\', shape=(5, 5, 3, 32), strides=(1, 2, 2, 1), padding=\'SAME\',\n                b_init=tf.constant_initializer(value=0.0), name=\'conv2dlayer\'\n            )(cls.input_layer)\n        except Exception as e:\n            cls.assertIsInstance(e, Exception)\n            print(e)\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_core_act.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Convolution_2D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing activation #####"")\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n        # tf.reset_default_graph()\n\n    def test_layer_core_act(cls):\n\n        cls.batch_size = 5\n        cls.inputs_shape = [cls.batch_size, 400, 400, 3]\n        cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n\n        cls.n1 = tl.layers.Conv2dLayer(\n            act=tf.nn.relu, shape=(5, 5, 3, 32), strides=(1, 2, 2, 1), padding=\'SAME\',\n            b_init=tf.constant_initializer(value=0.0), name=\'conv2dlayer\'\n        )(cls.input_layer)\n\n        cls.n2 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""relu"", name=\'conv2d\')(cls.n1)\n\n        cls.n3 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""leaky_relu"",\n                                  b_init=None)(cls.n2)\n\n        cls.n4 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""lrelu"", b_init=None)(cls.n2)\n\n        cls.n5 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""sigmoid"",\n                                  in_channels=32)(cls.n4)\n\n        cls.n6 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""tanh"", in_channels=32)(cls.n5)\n\n        cls.n7 = tl.layers.Conv2d(\n            n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""leaky_relu0.22"", in_channels=32\n        )(cls.n6)\n\n        cls.n8 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""lrelu0.22"",\n                                  in_channels=32)(cls.n7)\n\n        cls.n9 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""softplus"",\n                                  in_channels=32)(cls.n8)\n\n        cls.n10 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=""relu6"", in_channels=32)(cls.n9)\n\n        cls.model = Model(cls.input_layer, cls.n8)\n\n\nclass Exception_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing exception in activation #####"")\n\n    def test_exception(cls):\n\n        cls.batch_size = 5\n        cls.inputs_shape = [cls.batch_size, 400, 400, 3]\n        cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n\n        try:\n            cls.n1 = tl.layers.Conv2dLayer(\n                act=\'activation\', shape=(5, 5, 3, 32), strides=(1, 2, 2, 1), padding=\'SAME\',\n                b_init=tf.constant_initializer(value=0.0), name=\'conv2dlayer\'\n            )(cls.input_layer)\n        except Exception as e:\n            cls.assertIsInstance(e, Exception)\n            print(e)\n\n        try:\n            cls.n2 = tl.layers.Conv2dLayer(\n                act=\'leaky_relu0.2x\', shape=(5, 5, 3, 32), strides=(1, 2, 2, 1), padding=\'SAME\',\n                b_init=tf.constant_initializer(value=0.0), name=\'conv2dlayer\'\n            )(cls.input_layer)\n        except Exception as e:\n            cls.assertIsInstance(e, Exception)\n            print(e)\n\n        try:\n            cls.n3 = tl.layers.Conv2dLayer(\n                act=\'lrelu0.2x\', shape=(5, 5, 3, 32), strides=(1, 2, 2, 1), padding=\'SAME\',\n                b_init=tf.constant_initializer(value=0.0), name=\'conv2dlayer\'\n            )(cls.input_layer)\n        except Exception as e:\n            cls.assertIsInstance(e, Exception)\n            print(e)\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_core_basedense_dropout.py,4,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Core_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.batch_size = 8\n\n        # ============== Layer ==============\n\n        cls.base_layer = Layer(what=None)\n\n        # ============== DenseLayer ==============\n\n        cls.inputs_shape = [None, 784]\n        cls.innet = Input(cls.inputs_shape)\n        cls.dense1 = Dense(n_units=800, act=tf.nn.relu, in_channels=784, name='test_dense')(cls.innet)\n        cls.dropout1 = Dropout(keep=0.8)(cls.dense1)\n        cls.dense2 = Dense(n_units=10, act=tf.nn.relu, b_init=None)(cls.dropout1)\n        cls.dense3 = Dense(n_units=10, act=tf.nn.relu, b_init=None)\n        cls.concat = Concat(concat_dim=-1)([cls.dense2, cls.dropout1])\n\n        cls.model = Model(inputs=cls.innet, outputs=cls.dense2)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_net1(self):\n\n        # test exceptional cases\n        try:\n            self.base_layer.build(None)\n        except Exception as e:\n            print(e)\n\n        try:\n            self.base_layer.forward(None)\n        except Exception as e:\n            print(e)\n\n        try:\n            self.base_layer[4] = 1\n        except Exception as e:\n            print(e)\n\n        try:\n            del self.base_layer[4]\n        except Exception as e:\n            print(e)\n\n        try:\n            Layer(what=1)\n        except Exception as e:\n            print(e)\n\n    def test_net2(self):\n\n        # test weights\n        self.assertEqual(self.innet._info[0].layer.all_weights, [])\n        self.assertEqual(self.dropout1._info[0].layer.all_weights, [])\n        self.assertEqual(self.dense1._info[0].layer.all_weights[0].get_shape().as_list(), [784, 800])\n        self.assertEqual(self.dense1._info[0].layer.all_weights[1].get_shape().as_list(), [\n            800,\n        ])\n        self.assertEqual(self.dense2._info[0].layer.all_weights[0].get_shape().as_list(), [800, 10])\n        self.assertEqual(len(self.dense1._info[0].layer.all_weights), 2)\n        self.assertEqual(len(self.dense2._info[0].layer.all_weights), 1)\n\n        self.assertEqual(len(self.model.all_weights), 3)\n\n        # a special case\n        self.model.release_memory()\n\n        # test printing\n        # print(self.innet)\n        # print(self.dense1)\n        # print(self.dropout1)\n        # print(self.dense2)\n        # print(self.dense3)\n\n    def test_special_cases(self):\n        try:\n            innet = Input([121])\n            dense1 = Dense(n_units=800, act=tf.nn.relu)(innet)\n        except Exception as e:\n            print(e)\n\n    def test_modellayer(self):\n\n        data = np.random.normal(size=[self.batch_size, self.inputs_shape[1]]).astype(np.float32)\n\n        origin_results_train = self.model(data, is_train=True)\n        origin_results_test = self.model(data, is_train=False)\n\n        new_innet = Input(self.inputs_shape)\n        new_mlayer = ModelLayer(self.model)(new_innet)\n\n        newmodel = Model(inputs=new_innet, outputs=new_mlayer)\n\n        new_results_train = newmodel(data, is_train=True)\n        new_results_test = newmodel(data, is_train=False)\n\n        self.assertEqual(origin_results_train.shape, new_results_train.shape)\n        self.assertTrue(np.array_equal(origin_results_test.shape, new_results_test.shape))\n\n        newmodel.release_memory()\n\n    def test_layerlist(self):\n        innet = Input(self.inputs_shape)\n        hlayer = LayerList(\n            [\n                ModelLayer(self.model),\n                LayerList([Dense(n_units=100), Dense(n_units=10)]),\n                Dense(n_units=5),\n                Dense(n_units=4)\n            ]\n        )(innet)\n        model = Model(inputs=innet, outputs=hlayer)\n\n        # for w in model.all_weights:\n        #     print(w.name)\n\n        data = np.random.normal(size=[self.batch_size, self.inputs_shape[1]]).astype(np.float32)\n        pred = model(data, is_train=False)\n        self.assertEqual(pred.get_shape().as_list(), [self.batch_size, 4])\n\n        print(model)\n\n        model.release_memory()\n\n    def test_duplicate_names(self):\n        dense1 = tl.layers.Dense(n_units=10, name='test_densehh')\n        print(dense1)\n        try:\n            dense2 = tl.layers.Dense(n_units=10, name='test_densehh')\n            print(dense2)\n        except Exception as e:\n            print(e)\n        dense1 = tl.layers.Dense(n_units=10, name='test_densehh1')\n        dense2 = tl.layers.Dense(n_units=10, name='test_densehh2')\n        print(dense1)\n        print(dense2)\n\n    def test_dropout(self):\n        data_x = np.random.random([10, 784]).astype(np.float32)\n        pred_y_1 = self.model(data_x, is_train=True)\n        pred_y_2 = self.model(data_x, is_train=True)\n        self.assertFalse(np.allclose(pred_y_1, pred_y_2))\n        pred_y_1 = self.model(data_x, is_train=False)\n        pred_y_2 = self.model(data_x, is_train=False)\n        self.assertTrue(np.allclose(pred_y_1, pred_y_2))\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n"""
tests/layers/test_layers_core_nested.py,13,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\\\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_nested(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing nested layer #####"")\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n        # tf.reset_default_graph()\n\n    def test_nested_layer_with_inchannels(cls):\n\n        class MyLayer(tl.layers.Layer):\n\n            def __init__(self, name=None):\n                super(MyLayer, self).__init__(name=name)\n                self.input_layer = tl.layers.Dense(in_channels=50, n_units=20)\n                self.build(None)\n                self._built = True\n\n            def build(self, inputs_shape=None):\n                self.W = self._get_weights(\'weights\', shape=(20, 10))\n\n            def forward(self, inputs):\n                inputs = self.input_layer(inputs)\n                output = tf.matmul(inputs, self.W)\n                return output\n\n        class model(tl.models.Model):\n\n            def __init__(self, name=None):\n                super(model, self).__init__(name=name)\n                self.layer = MyLayer()\n\n            def forward(self, inputs):\n                return self.layer(inputs)\n\n        input = tf.random.normal(shape=(100, 50))\n        model_dynamic = model()\n        model_dynamic.train()\n        cls.assertEqual(model_dynamic(input).shape, (100, 10))\n        cls.assertEqual(len(model_dynamic.all_weights), 3)\n        cls.assertEqual(len(model_dynamic.trainable_weights), 3)\n        model_dynamic.layer.input_layer.b.assign_add(tf.ones((20, )))\n        cls.assertEqual(np.sum(model_dynamic.all_weights[-1].numpy() - tf.ones(20, ).numpy()), 0)\n\n        ni = tl.layers.Input(shape=(100, 50))\n        nn = MyLayer(name=\'mylayer1\')(ni)\n        model_static = tl.models.Model(inputs=ni, outputs=nn)\n        model_static.eval()\n        cls.assertEqual(model_static(input).shape, (100, 10))\n        cls.assertEqual(len(model_static.all_weights), 3)\n        cls.assertEqual(len(model_static.trainable_weights), 3)\n        model_static.get_layer(\'mylayer1\').input_layer.b.assign_add(tf.ones((20, )))\n        cls.assertEqual(np.sum(model_static.all_weights[-1].numpy() - tf.ones(20, ).numpy()), 0)\n\n    def test_nested_layer_without_inchannels(cls):\n\n        class MyLayer(tl.layers.Layer):\n\n            def __init__(self, name=None):\n                super(MyLayer, self).__init__(name=name)\n                self.input_layer = tl.layers.Dense(n_units=20)  # no need for in_channels here\n                self.build(None)\n                self._built = True\n\n            def build(self, inputs_shape=None):\n                self.W = self._get_weights(\'weights\', shape=(20, 10))\n\n            def forward(self, inputs):\n                inputs = self.input_layer(inputs)\n                output = tf.matmul(inputs, self.W)\n                return output\n\n        class model(tl.models.Model):\n\n            def __init__(self, name=None):\n                super(model, self).__init__(name=name)\n                self.layer = MyLayer()\n\n            def forward(self, inputs):\n                return self.layer(inputs)\n\n        input = tf.random.normal(shape=(100, 50))\n        model_dynamic = model()\n        model_dynamic.train()\n        cls.assertEqual(model_dynamic(input).shape, (100, 10))\n        cls.assertEqual(len(model_dynamic.all_weights), 3)\n        cls.assertEqual(len(model_dynamic.trainable_weights), 3)\n        model_dynamic.layer.input_layer.b.assign_add(tf.ones((20, )))\n        cls.assertEqual(np.sum(model_dynamic.all_weights[-1].numpy() - tf.ones(20, ).numpy()), 0)\n\n        ni = tl.layers.Input(shape=(100, 50))\n        nn = MyLayer(name=\'mylayer2\')(ni)\n        model_static = tl.models.Model(inputs=ni, outputs=nn)\n        model_static.eval()\n        cls.assertEqual(model_static(input).shape, (100, 10))\n        cls.assertEqual(len(model_static.all_weights), 3)\n        cls.assertEqual(len(model_static.trainable_weights), 3)\n        model_static.get_layer(\'mylayer2\').input_layer.b.assign_add(tf.ones((20, )))\n        cls.assertEqual(np.sum(model_static.all_weights[-1].numpy() - tf.ones(20, ).numpy()), 0)\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_deformable_convolution.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Convolution_2D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""\\n#################################"")\n\n        cls.batch_size = 5\n        cls.inputs_shape = [cls.batch_size, 10, 10, 16]\n        cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n\n        cls.offset1 = tl.layers.Conv2d(n_filter=18, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\',\n                                       name=\'offset1\')(cls.input_layer)\n        cls.deformconv1 = tl.layers.DeformableConv2d(\n            offset_layer=cls.offset1, n_filter=32, filter_size=(3, 3), act=tf.nn.relu, name=\'deformable1\'\n        )(cls.input_layer)\n        cls.offset2 = tl.layers.Conv2d(n_filter=18, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\',\n                                       name=\'offset2\')(cls.deformconv1)\n        cls.deformconv2 = tl.layers.DeformableConv2d(\n            offset_layer=cls.offset2, n_filter=64, filter_size=(3, 3), act=tf.nn.relu, name=\'deformable2\'\n        )(cls.deformconv1)\n\n        cls.model = Model(cls.input_layer, cls.deformconv2)\n        print(""Testing Deformable Conv2d model: \\n"", cls.model)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n\n        self.assertEqual(len(self.deformconv1._info[0].layer.all_weights), 2)\n        self.assertEqual(self.deformconv1.get_shape().as_list()[1:], [10, 10, 32])\n\n    def test_layer_n2(self):\n\n        self.assertEqual(len(self.deformconv2._info[0].layer.all_weights), 2)\n        self.assertEqual(self.deformconv2.get_shape().as_list()[1:], [10, 10, 64])\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_dense.py,13,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_BinaryDense_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""-"" * 20, ""Layer_BinaryDense_Test"", ""-"" * 20)\n        cls.batch_size = 4\n        cls.inputs_shape = [cls.batch_size, 10]\n\n        cls.ni = Input(cls.inputs_shape, name=\'input_layer\')\n        cls.layer1 = BinaryDense(n_units=5)\n        nn = cls.layer1(cls.ni)\n        cls.layer1._nodes_fixed = True\n        cls.M = Model(inputs=cls.ni, outputs=nn)\n\n        cls.layer2 = BinaryDense(n_units=5, in_channels=10)\n        cls.layer2._nodes_fixed = True\n\n        cls.inputs = tf.ones((cls.inputs_shape))\n        cls.n1 = cls.layer1(cls.inputs)\n        cls.n2 = cls.layer2(cls.inputs)\n        cls.n3 = cls.M(cls.inputs, is_train=True)\n\n        print(cls.layer1)\n        print(cls.layer2)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        print(self.n1[0])\n        self.assertEqual(tf.reduce_sum(self.n1).numpy() % 1, 0.0)  # should be integer\n\n    def test_layer_n2(self):\n        print(self.n2[0])\n        self.assertEqual(tf.reduce_sum(self.n2).numpy() % 1, 0.0)  # should be integer\n\n    def test_model_n3(self):\n        print(self.n3[0])\n        self.assertEqual(tf.reduce_sum(self.n3).numpy() % 1, 0.0)  # should be integer\n\n    def test_exception(self):\n        try:\n            layer = BinaryDense(n_units=5)\n            inputs = Input([4, 10, 5], name=\'ill_inputs\')\n            out = layer(inputs)\n            self.fail(\'ill inputs\')\n        except Exception as e:\n            print(e)\n\n        try:\n            layer = BinaryDense(n_units=5, use_gemm=True)\n            out = layer(self.ni)\n            self.fail(\'use gemm\')\n        except Exception as e:\n            print(e)\n\n\nclass Layer_DorefaDense_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""-"" * 20, ""Layer_DorefaDense_Test"", ""-"" * 20)\n        cls.batch_size = 4\n        cls.inputs_shape = [cls.batch_size, 10]\n\n        cls.ni = Input(cls.inputs_shape, name=\'input_layer\')\n        cls.layer1 = DorefaDense(n_units=5)\n        nn = cls.layer1(cls.ni)\n        cls.layer1._nodes_fixed = True\n        cls.M = Model(inputs=cls.ni, outputs=nn)\n\n        cls.layer2 = DorefaDense(n_units=5, in_channels=10)\n        cls.layer2._nodes_fixed = True\n\n        cls.inputs = tf.ones((cls.inputs_shape))\n        cls.n1 = cls.layer1(cls.inputs)\n        cls.n2 = cls.layer2(cls.inputs)\n        cls.n3 = cls.M(cls.inputs, is_train=True)\n\n        print(cls.layer1)\n        print(cls.layer2)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        print(self.n1[0])\n        # self.assertEqual(tf.reduce_sum(self.n1).numpy() % 1, 0.0)  # should be integer\n\n    def test_layer_n2(self):\n        print(self.n2[0])\n        # self.assertEqual(tf.reduce_sum(self.n2).numpy() % 1, 0.0)  # should be integer\n\n    def test_model_n3(self):\n        print(self.n3[0])\n        # self.assertEqual(tf.reduce_sum(self.n3).numpy() % 1, 0.0)  # should be integer\n\n    def test_exception(self):\n        try:\n            layer = DorefaDense(n_units=5)\n            inputs = Input([4, 10, 5], name=\'ill_inputs\')\n            out = layer(inputs)\n            self.fail(\'ill inputs\')\n        except Exception as e:\n            print(e)\n\n        try:\n            layer = DorefaDense(n_units=5, use_gemm=True)\n            out = layer(self.ni)\n            self.fail(\'use gemm\')\n        except Exception as e:\n            print(e)\n\n\nclass Layer_DropconnectDense_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""-"" * 20, ""Layer_DropconnectDense_Test"", ""-"" * 20)\n        cls.batch_size = 4\n        cls.inputs_shape = [cls.batch_size, 10]\n\n        cls.ni = Input(cls.inputs_shape, name=\'input_layer\')\n        cls.layer1 = DropconnectDense(n_units=5, keep=1.0)\n        nn = cls.layer1(cls.ni)\n        cls.layer1._nodes_fixed = True\n        cls.M = Model(inputs=cls.ni, outputs=nn)\n\n        cls.layer2 = DropconnectDense(n_units=5, in_channels=10, keep=0.01)\n        cls.layer2._nodes_fixed = True\n\n        cls.inputs = tf.ones((cls.inputs_shape))\n        cls.n1 = cls.layer1(cls.inputs)\n        cls.n2 = cls.layer2(cls.inputs)\n        cls.n3 = cls.M(cls.inputs, is_train=True)\n\n        print(cls.layer1)\n        print(cls.layer2)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        print(self.n1[0])\n\n    def test_layer_n2(self):\n        zero_rate = tf.reduce_mean(tf.cast(tf.equal(self.n2, 0.0), tf.float32))\n        print(zero_rate)\n        self.assertGreater(zero_rate, 0.0)\n        print(self.n2[0])\n\n    def test_model_n3(self):\n        print(self.n3[0])\n\n    def test_exception(self):\n        try:\n            layer = DropconnectDense(n_units=5)\n            inputs = Input([4, 10, 5], name=\'ill_inputs\')\n            out = layer(inputs)\n            self.fail(\'ill inputs\')\n        except Exception as e:\n            print(e)\n\n        try:\n            layer = DropconnectDense(n_units=5, keep=0.0)\n            self.fail(\'keep no elements\')\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n\nclass Layer_QuanDense_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""-"" * 20, ""Layer_QuanDense_Test"", ""-"" * 20)\n        cls.batch_size = 4\n        cls.inputs_shape = [cls.batch_size, 10]\n\n        cls.ni = Input(cls.inputs_shape, name=\'input_layer\')\n        cls.layer1 = QuanDense(n_units=5)\n        nn = cls.layer1(cls.ni)\n        cls.layer1._nodes_fixed = True\n        cls.M = Model(inputs=cls.ni, outputs=nn)\n\n        cls.layer2 = QuanDense(n_units=5, in_channels=10)\n        cls.layer2._nodes_fixed = True\n\n        cls.inputs = tf.random.uniform((cls.inputs_shape))\n        cls.n1 = cls.layer1(cls.inputs)\n        cls.n2 = cls.layer2(cls.inputs)\n        cls.n3 = cls.M(cls.inputs, is_train=True)\n\n        print(cls.layer1)\n        print(cls.layer2)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        print(self.n1[0])\n\n    def test_layer_n2(self):\n        print(self.n2[0])\n\n    def test_model_n3(self):\n        print(self.n3[0])\n\n    def test_exception(self):\n        try:\n            layer = QuanDense(n_units=5)\n            inputs = Input([4, 10, 5], name=\'ill_inputs\')\n            out = layer(inputs)\n            self.fail(\'ill inputs\')\n        except Exception as e:\n            print(e)\n\n        try:\n            layer = QuanDense(n_units=5, use_gemm=True)\n            out = layer(self.ni)\n            self.fail(\'use gemm\')\n        except Exception as e:\n            print(e)\n\n\nclass Layer_QuanDenseWithBN_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""-"" * 20, ""Layer_QuanDenseWithBN_Test"", ""-"" * 20)\n        cls.batch_size = 4\n        cls.inputs_shape = [cls.batch_size, 10]\n\n        cls.ni = Input(cls.inputs_shape, name=\'input_layer\')\n        cls.layer1 = QuanDenseWithBN(n_units=5)\n        nn = cls.layer1(cls.ni)\n        cls.layer1._nodes_fixed = True\n        cls.M = Model(inputs=cls.ni, outputs=nn)\n\n        cls.layer2 = QuanDenseWithBN(n_units=5, in_channels=10)\n        cls.layer2._nodes_fixed = True\n\n        cls.inputs = tf.random.uniform((cls.inputs_shape))\n        cls.n1 = cls.layer1(cls.inputs)\n        cls.n2 = cls.layer2(cls.inputs)\n        cls.n3 = cls.M(cls.inputs, is_train=True)\n\n        print(cls.layer1)\n        print(cls.layer2)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        print(self.n1[0])\n\n    def test_layer_n2(self):\n        print(self.n2[0])\n\n    def test_model_n3(self):\n        print(self.n3[0])\n\n    def test_exception(self):\n        try:\n            layer = QuanDenseWithBN(n_units=5)\n            inputs = Input([4, 10, 5], name=\'ill_inputs\')\n            out = layer(inputs)\n            self.fail(\'ill inputs\')\n        except Exception as e:\n            print(e)\n\n        try:\n            layer = QuanDenseWithBN(n_units=5, use_gemm=True)\n            out = layer(self.ni)\n            self.fail(\'use gemm\')\n        except Exception as e:\n            print(e)\n\n\nclass Layer_TernaryDense_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""-"" * 20, ""Layer_BinaryDense_Test"", ""-"" * 20)\n        cls.batch_size = 4\n        cls.inputs_shape = [cls.batch_size, 10]\n\n        cls.ni = Input(cls.inputs_shape, name=\'input_layer\')\n        cls.layer1 = TernaryDense(n_units=5)\n        nn = cls.layer1(cls.ni)\n        cls.layer1._nodes_fixed = True\n        cls.M = Model(inputs=cls.ni, outputs=nn)\n\n        cls.layer2 = TernaryDense(n_units=5, in_channels=10)\n        cls.layer2._nodes_fixed = True\n\n        cls.inputs = tf.ones((cls.inputs_shape))\n        cls.n1 = cls.layer1(cls.inputs)\n        cls.n2 = cls.layer2(cls.inputs)\n        cls.n3 = cls.M(cls.inputs, is_train=True)\n\n        print(cls.layer1)\n        print(cls.layer2)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        print(np.unique(self.n1.numpy().reshape(-1)))\n        print(self.n1[0])\n\n    def test_layer_n2(self):\n        print(np.unique(self.n2.numpy().reshape(-1)))\n        print(self.n2[0])\n\n    def test_model_n3(self):\n        print(np.unique(self.n3.numpy().reshape(-1)))\n        print(self.n3[0])\n\n    def test_exception(self):\n        try:\n            layer = TernaryDense(n_units=5)\n            inputs = Input([4, 10, 5], name=\'ill_inputs\')\n            out = layer(inputs)\n            self.fail(\'ill inputs\')\n        except Exception as e:\n            print(e)\n\n        try:\n            layer = TernaryDense(n_units=5, use_gemm=True)\n            out = layer(self.ni)\n            self.fail(\'use gemm\')\n        except Exception as e:\n            print(e)\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_embedding.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Embed_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_onehot(self):\n        input = tl.layers.Input([32], dtype=tf.int32)\n        onehot = tl.layers.OneHot(depth=8, on_value=1, off_value=0, axis=-1)\n        print(onehot)\n        tensor = tl.layers.OneHot(depth=8)(input)\n        self.assertEqual(tensor.get_shape().as_list(), [32, 8])\n        model = tl.models.Model(inputs=input, outputs=tensor)\n\n    def test_embed(self):\n        input = tl.layers.Input([8, 100], dtype=tf.int32)\n        embed = tl.layers.Embedding(vocabulary_size=1000, embedding_size=50, name=\'embed\')\n        print(embed)\n        tensor = embed(input)\n        self.assertEqual(tensor.get_shape().as_list(), [8, 100, 50])\n        model = tl.models.Model(inputs=input, outputs=tensor)\n\n    def test_avg_embed(self):\n        batch_size = 8\n        length = 5\n        input = tl.layers.Input([batch_size, length], dtype=tf.int32)\n        avgembed = tl.layers.AverageEmbedding(vocabulary_size=1000, embedding_size=50, name=\'avg\')\n        print(avgembed)\n        tensor = avgembed(input)\n        # print(tensor)\n        self.assertEqual(tensor.get_shape().as_list(), [batch_size, 50])\n        model = tl.models.Model(inputs=input, outputs=tensor)\n\n    def test_word2vec_nce(self):\n        batch_size = 8\n        embedding_size = 50\n        inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n        labels = tl.layers.Input([batch_size, 1], dtype=tf.int32)\n        emb_net = tl.layers.Word2vecEmbedding(\n            vocabulary_size=10000,\n            embedding_size=embedding_size,\n            num_sampled=100,\n            activate_nce_loss=True,  # the nce loss is activated\n            nce_loss_args={},\n            E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0),\n            nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))),\n            nce_b_init=tl.initializers.constant(value=0.0),\n        )\n        print(emb_net)\n        try:\n            embed_tensor, embed_nce_loss = emb_net(inputs)\n        except ValueError as e:\n            print(e)\n        try:\n            embed_tensor = emb_net(inputs, use_nce_loss=False)\n            print(""Not use NCE without labels"")\n        except Exception as e:\n            print(e)\n        embed_tensor = emb_net([inputs, labels], use_nce_loss=False)\n        embed_tensor, embed_nce_loss = emb_net([inputs, labels], use_nce_loss=True)\n        embed_tensor, embed_nce_loss = emb_net([inputs, labels])\n        self.assertEqual(embed_tensor.get_shape().as_list(), [batch_size, embedding_size])\n\n        outputs = tl.layers.Dense(n_units=10)(embed_tensor)\n        model = tl.models.Model(inputs=[inputs, labels], outputs=[outputs, embed_nce_loss])\n        out, nce = model(\n            [np.random.randint(0, 1, size=[batch_size]),\n             np.random.randint(0, 1, size=[batch_size, 1])], is_train=True\n        )\n        self.assertEqual(out.get_shape().as_list(), [batch_size, 10])\n        print(nce)\n\n    def test_word2vec_no_nce(self):\n        batch_size = 8\n        embedding_size = 50\n        inputs = tl.layers.Input([batch_size], dtype=tf.int32)\n        emb_net = tl.layers.Word2vecEmbedding(\n            vocabulary_size=10000,\n            embedding_size=embedding_size,\n            num_sampled=100,\n            activate_nce_loss=False,  # the nce loss is activated\n            nce_loss_args={},\n            E_init=tl.initializers.random_uniform(minval=-1.0, maxval=1.0),\n            nce_W_init=tl.initializers.truncated_normal(stddev=float(1.0 / np.sqrt(embedding_size))),\n            nce_b_init=tl.initializers.constant(value=0.0),\n        )\n        print(emb_net)\n        embed_tensor = emb_net(inputs)\n        embed_tensor = emb_net(inputs, use_nce_loss=False)\n        try:\n            embed_tensor = emb_net(inputs, use_nce_loss=True)\n        except AttributeError as e:\n            print(e)\n        self.assertEqual(embed_tensor.get_shape().as_list(), [batch_size, embedding_size])\n        model = tl.models.Model(inputs=inputs, outputs=embed_tensor)\n\n\nif __name__ == \'__main__\':\n\n    unittest.main()\n'"
tests/layers/test_layers_extend.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Extend_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_expand_dims(self):\n        x = tl.layers.Input([8, 3])\n        expandlayer = tl.layers.ExpandDims(axis=-1)\n        y = expandlayer(x)\n        print(expandlayer)\n        self.assertEqual(y.get_shape().as_list(), [8, 3, 1])\n\n    def test_tile(self):\n        x = tl.layers.Input([8, 3])\n        tilelayer = tl.layers.Tile(multiples=[2, 3])\n        y = tilelayer(x)\n        print(tilelayer)\n        self.assertEqual(y.get_shape().as_list(), [16, 9])\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n"""
tests/layers/test_layers_lambda.py,15,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Lambda_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.data_x = np.random.random([100, 1]).astype(np.float32)\n        cls.data_y = cls.data_x**3 + np.random.random() * cls.data_x**2 + np.random.random() * cls.data_x\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_lambda_keras(self):\n        layers = [\n            tf.keras.layers.Dense(10, activation=tf.nn.relu),\n            tf.keras.layers.Dense(5, activation=tf.nn.sigmoid),\n            tf.keras.layers.Dense(1, activation=tf.identity)\n        ]\n        perceptron = tf.keras.Sequential(layers)\n        # in order to get trainable_variables of keras\n        _ = perceptron(np.random.random([100, 5]).astype(np.float32))\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.dense = tl.layers.Dense(in_channels=1, n_units=5)\n                self.lambdalayer = tl.layers.Lambda(perceptron, perceptron.trainable_variables)\n\n            def forward(self, x):\n                z = self.dense(x)\n                z = self.lambdalayer(z)\n                return z\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.1)\n\n        model = CustomizeModel()\n        print(model.lambdalayer)\n\n        model.train()\n\n        for epoch in range(10):\n            with tf.GradientTape() as tape:\n                pred_y = model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n\n            print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_lambda_func_with_args(self):\n\n        def customize_func(x, foo=42):\n            if foo == 0:\n                return tf.nn.relu(x)\n            elif foo == 1:\n                return tf.nn.sigmoid(x)\n            else:\n                return tf.identity(x)\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.dense = tl.layers.Dense(in_channels=1, n_units=5)\n                self.lambdalayer = tl.layers.Lambda(customize_func, fn_weights=[], fn_args={\'foo\': 0})\n\n            def forward(self, x, bar):\n                z = self.dense(x)\n                if bar == -1:\n                    zf = self.lambdalayer(z)\n                else:\n                    zf = self.lambdalayer(z, foo=bar)\n                return z, zf\n\n        model = CustomizeModel()\n        print(model.lambdalayer)\n        model.train()\n\n        out, out2 = model(self.data_x, bar=-1)\n        self.assertTrue(np.array_equal(out2.numpy(), tf.nn.relu(out).numpy()))\n        out, out2 = model(self.data_x, bar=0)\n        self.assertTrue(np.array_equal(out2.numpy(), tf.nn.relu(out).numpy()))\n        out, out2 = model(self.data_x, bar=1)\n        self.assertTrue(np.array_equal(out2.numpy(), tf.nn.sigmoid(out).numpy()))\n        out, out2 = model(self.data_x, bar=2)\n        self.assertTrue(np.array_equal(out2.numpy(), out.numpy()))\n\n    def test_lambda_func_with_weight(self):\n\n        a = tf.Variable(1.0)\n\n        def customize_fn(x):\n            return x + a\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.dense = tl.layers.Dense(in_channels=1, n_units=5)\n                self.lambdalayer = tl.layers.Lambda(customize_fn, fn_weights=[a])\n\n            def forward(self, x):\n                z = self.dense(x)\n                z = self.lambdalayer(z)\n                return z\n\n        model = CustomizeModel()\n        print(model.lambdalayer)\n        model.train()\n\n        out = model(self.data_x)\n        print(out.shape)\n\n    def test_lambda_func_without_args(self):\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.dense = tl.layers.Dense(in_channels=1, n_units=5)\n                self.lambdalayer = tl.layers.Lambda(lambda x: 2 * x)\n\n            def forward(self, x):\n                z = self.dense(x)\n                zf = self.lambdalayer(z)\n                return z, zf\n\n        model = CustomizeModel()\n        print(model.lambdalayer)\n        model.train()\n\n        out, out2 = model(self.data_x)\n        self.assertTrue(np.array_equal(out2.numpy(), out.numpy() * 2))\n\n    def test_elementwiselambda_func_with_args(self):\n\n        def customize_func(noise, mean, std, foo=42):\n            return mean + noise * tf.exp(std * 0.5) + foo\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.dense1 = tl.layers.Dense(in_channels=1, n_units=5)\n                self.dense2 = tl.layers.Dense(in_channels=1, n_units=5)\n                self.dense3 = tl.layers.Dense(in_channels=1, n_units=5)\n                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_args={\'foo\': 1024})\n\n            def forward(self, x, bar=None):\n                noise = self.dense1(x)\n                mean = self.dense2(x)\n                std = self.dense3(x)\n                if bar is None:\n                    out = self.lambdalayer([noise, mean, std])\n                else:\n                    out = self.lambdalayer([noise, mean, std], foo=bar)\n                return noise, mean, std, out\n\n        model = CustomizeModel()\n        print(model.lambdalayer)\n        model.train()\n\n        noise, mean, std, out = model(self.data_x)\n        self.assertTrue(np.allclose(out.numpy(), customize_func(noise, mean, std, foo=1024).numpy()))\n        noise, mean, std, out = model(self.data_x, bar=2048)\n        self.assertTrue(np.allclose(out.numpy(), customize_func(noise, mean, std, foo=2048).numpy()))\n\n    def test_elementwiselambda_func_without_args(self):\n\n        def customize_func(noise, mean, std):\n            return mean + noise * tf.exp(std * 0.5)\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.dense1 = tl.layers.Dense(in_channels=1, n_units=5)\n                self.dense2 = tl.layers.Dense(in_channels=1, n_units=5)\n                self.dense3 = tl.layers.Dense(in_channels=1, n_units=5)\n                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_weights=[])\n\n            def forward(self, x):\n                noise = self.dense1(x)\n                mean = self.dense2(x)\n                std = self.dense3(x)\n                out = self.lambdalayer([noise, mean, std])\n                return noise, mean, std, out\n\n        model = CustomizeModel()\n        print(model.lambdalayer)\n        model.train()\n\n        noise, mean, std, out = model(self.data_x)\n        self.assertTrue(np.array_equal(out.numpy(), customize_func(noise, mean, std).numpy()))\n\n\nif __name__ == \'__main__\':\n\n    unittest.main()\n'"
tests/layers/test_layers_merge.py,8,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Merge_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_concat(self):\n\n        class CustomModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomModel, self).__init__()\n                self.dense1 = tl.layers.Dense(in_channels=20, n_units=10, act=tf.nn.relu, name='relu1_1')\n                self.dense2 = tl.layers.Dense(in_channels=20, n_units=10, act=tf.nn.relu, name='relu2_1')\n                self.concat = tl.layers.Concat(concat_dim=1, name='concat_layer')\n\n            def forward(self, inputs):\n                d1 = self.dense1(inputs)\n                d2 = self.dense2(inputs)\n                outputs = self.concat([d1, d2])\n                return outputs\n\n        model = CustomModel()\n        model.train()\n        inputs = tf.convert_to_tensor(np.random.random([4, 20]).astype(np.float32))\n        outputs = model(inputs)\n        print(model)\n\n        self.assertEqual(outputs.get_shape().as_list(), [4, 20])\n\n    def test_elementwise(self):\n\n        class CustomModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomModel, self).__init__()\n                self.dense1 = tl.layers.Dense(in_channels=20, n_units=10, act=tf.nn.relu, name='relu1_1')\n                self.dense2 = tl.layers.Dense(in_channels=20, n_units=10, act=tf.nn.relu, name='relu2_1')\n                self.element = tl.layers.Elementwise(combine_fn=tf.minimum, name='minimum', act=tf.identity)\n\n            def forward(self, inputs):\n                d1 = self.dense1(inputs)\n                d2 = self.dense2(inputs)\n                outputs = self.element([d1, d2])\n                return outputs, d1, d2\n\n        model = CustomModel()\n        model.train()\n        inputs = tf.convert_to_tensor(np.random.random([4, 20]).astype(np.float32))\n        outputs, d1, d2 = model(inputs)\n        print(model)\n\n        min = tf.minimum(d1, d2)\n        self.assertEqual(outputs.get_shape().as_list(), [4, 10])\n        self.assertTrue(np.array_equal(min.numpy(), outputs.numpy()))\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n"""
tests/layers/test_layers_noise.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Convolution_1D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""\\n#################################"")\n\n        cls.batch_size = 8\n        cls.inputs_shape = [cls.batch_size, 200]\n        cls.input_layer = Input(cls.inputs_shape, name=\'input_layer\')\n\n        cls.dense = tl.layers.Dense(n_units=100, act=tf.nn.relu, in_channels=200)(cls.input_layer)\n\n        cls.noiselayer = tl.layers.GaussianNoise(name=\'gaussian\')(cls.dense)\n\n        print(""Testing GaussianNoise: \\n"", cls.noiselayer._info[0].layer)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        self.assertEqual(self.noiselayer.get_shape().as_list()[1:], [100])\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_normalization.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import Model\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Laye_BatchNorm_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        x_0_input_shape = [None, 10]\n        x_1_input_shape = [None, 100, 1]\n        x_2_input_shape = [None, 100, 100, 3]\n        x_3_input_shape = [None, 100, 100, 100, 3]\n        batchsize = 2\n\n        cls.x0 = tf.random.normal([batchsize] + x_0_input_shape[1:])\n        cls.x1 = tf.random.normal([batchsize] + x_1_input_shape[1:])\n        cls.x2 = tf.random.normal([batchsize] + x_2_input_shape[1:])\n        cls.x3 = tf.random.normal([batchsize] + x_3_input_shape[1:])\n\n        ## Base\n        ni_1 = Input(x_1_input_shape, name=\'test_ni1\')\n        nn_1 = Conv1d(n_filter=32, filter_size=5, stride=2, name=\'test_conv1d\')(ni_1)\n        n1_b = BatchNorm(name=\'test_conv\')(nn_1)\n        cls.n1_b = n1_b\n        cls.base_1d = Model(inputs=ni_1, outputs=n1_b, name=\'test_base_1d\')\n\n        ni_2 = Input(x_2_input_shape, name=\'test_ni2\')\n        nn_2 = Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), name=\'test_conv2d\')(ni_2)\n        n2_b = BatchNorm(name=\'test_bn2d\')(nn_2)\n        cls.n2_b = n2_b\n        cls.base_2d = Model(inputs=ni_2, outputs=n2_b, name=\'test_base_2d\')\n\n        ni_3 = Input(x_3_input_shape, name=\'test_ni2\')\n        nn_3 = Conv3d(n_filter=32, filter_size=(3, 3, 3), strides=(2, 2, 2), name=\'test_conv3d\')(ni_3)\n        n3_b = BatchNorm(name=\'test_bn3d\')(nn_3)\n        cls.n3_b = n3_b\n        cls.base_3d = Model(inputs=ni_3, outputs=n3_b, name=\'test_base_3d\')\n\n        class bn_0d_model(Model):\n\n            def __init__(self):\n                super(bn_0d_model, self).__init__()\n                self.fc = Dense(32, in_channels=10)\n                self.bn = BatchNorm(num_features=32, name=\'test_bn1d\')\n\n            def forward(self, x):\n                x = self.bn(self.fc(x))\n                return x\n\n        dynamic_base = bn_0d_model()\n        cls.n0_b = dynamic_base(cls.x0, is_train=True)\n\n        ## 0D ========================================================================\n\n        nin_0 = Input(x_0_input_shape, name=\'test_in1\')\n\n        n0 = Dense(32)(nin_0)\n        n0 = BatchNorm1d(name=\'test_bn0d\')(n0)\n\n        cls.n0 = n0\n\n        cls.static_0d = Model(inputs=nin_0, outputs=n0)\n\n        class bn_0d_model(Model):\n\n            def __init__(self):\n                super(bn_0d_model, self).__init__(name=\'test_bn_0d_model\')\n                self.fc = Dense(32, in_channels=10)\n                self.bn = BatchNorm1d(num_features=32, name=\'test_bn1d\')\n\n            def forward(self, x):\n                x = self.bn(self.fc(x))\n                return x\n\n        cls.dynamic_0d = bn_0d_model()\n\n        print(""Printing BatchNorm0d"")\n        print(cls.static_0d)\n        print(cls.dynamic_0d)\n\n        ## 1D ========================================================================\n\n        nin_1 = Input(x_1_input_shape, name=\'test_in1\')\n\n        n1 = Conv1d(n_filter=32, filter_size=5, stride=2, name=\'test_conv1d\')(nin_1)\n        n1 = BatchNorm1d(name=\'test_bn1d\')(n1)\n\n        cls.n1 = n1\n\n        cls.static_1d = Model(inputs=nin_1, outputs=n1)\n\n        class bn_1d_model(Model):\n\n            def __init__(self):\n                super(bn_1d_model, self).__init__(name=\'test_bn_1d_model\')\n                self.conv = Conv1d(n_filter=32, filter_size=5, stride=2, name=\'test_conv1d\', in_channels=1)\n                self.bn = BatchNorm1d(num_features=32, name=\'test_bn1d\')\n\n            def forward(self, x):\n                x = self.bn(self.conv(x))\n                return x\n\n        cls.dynamic_1d = bn_1d_model()\n\n        print(""Printing BatchNorm1d"")\n        print(cls.static_1d)\n        print(cls.dynamic_1d)\n\n        ## 2D ========================================================================\n\n        nin_2 = Input(x_2_input_shape, name=\'test_in2\')\n\n        n2 = Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), name=\'test_conv2d\')(nin_2)\n        n2 = BatchNorm2d(name=\'test_bn2d\')(n2)\n\n        cls.n2 = n2\n\n        cls.static_2d = Model(inputs=nin_2, outputs=n2)\n\n        class bn_2d_model(Model):\n\n            def __init__(self):\n                super(bn_2d_model, self).__init__(name=\'test_bn_2d_model\')\n                self.conv = Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), name=\'test_conv2d\', in_channels=3)\n                self.bn = BatchNorm2d(num_features=32, name=\'test_bn2d\')\n\n            def forward(self, x):\n                x = self.bn(self.conv(x))\n                return x\n\n        cls.dynamic_2d = bn_2d_model()\n\n        print(""Printing BatchNorm1d"")\n        print(cls.static_2d)\n        print(cls.dynamic_2d)\n\n        ## 3D ========================================================================\n\n        nin_3 = Input(x_3_input_shape, name=\'test_in3\')\n\n        n3 = Conv3d(n_filter=32, filter_size=(3, 3, 3), strides=(2, 2, 2), name=\'test_conv3d\')(nin_3)\n        n3 = BatchNorm3d(name=\'test_bn3d\', act=tf.nn.relu)(n3)\n\n        cls.n3 = n3\n\n        cls.static_3d = Model(inputs=nin_3, outputs=n3)\n\n        class bn_3d_model(Model):\n\n            def __init__(self):\n                super(bn_3d_model, self).__init__(name=\'test_bn_3d_model\')\n                self.conv = Conv3d(\n                    n_filter=32, filter_size=(3, 3, 3), strides=(2, 2, 2), name=\'test_conv3d\', in_channels=3\n                )\n                self.bn = BatchNorm3d(num_features=32, name=\'test_bn3d\')\n\n            def forward(self, x):\n                x = self.bn(self.conv(x))\n                return x\n\n        cls.dynamic_3d = bn_3d_model()\n\n        print(""Printing BatchNorm1d"")\n        print(cls.static_3d)\n        print(cls.dynamic_3d)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n        # tf.reset_default_graph()\n\n    def test_BatchNorm(self):\n        self.assertEqual(self.n1_b.shape[1:], (50, 32))\n        out = self.base_1d(self.x1, is_train=True)\n\n        self.assertEqual(self.n2_b.shape[1:], (50, 50, 32))\n        out = self.base_2d(self.x2, is_train=True)\n\n        self.assertEqual(self.n3_b.shape[1:], (50, 50, 50, 32))\n        out = self.base_3d(self.x3, is_train=True)\n\n        self.assertEqual(self.n0_b.shape[1:], (32))\n        print(""test_BatchNorm OK"")\n\n    def test_BatchNorm0d(self):\n        self.assertEqual(self.n0.shape[1:], (32))\n        out = self.static_0d(self.x0, is_train=True)\n        out = self.dynamic_0d(self.x0, is_train=True)\n\n    def test_BatchNorm1d(self):\n        self.assertEqual(self.n1.shape[1:], (50, 32))\n        out = self.static_1d(self.x1, is_train=True)\n        out = self.dynamic_1d(self.x1, is_train=True)\n\n    def test_BatchNorm2d(self):\n        self.assertEqual(self.n2.shape[1:], (50, 50, 32))\n        out = self.static_2d(self.x2, is_train=True)\n        out = self.dynamic_2d(self.x2, is_train=True)\n        out = self.dynamic_2d(self.x2, is_train=False)\n\n    def test_BatchNorm3d(self):\n        self.assertEqual(self.n3.shape[1:], (50, 50, 50, 32))\n        out = self.static_3d(self.x3, is_train=True)\n        out = self.dynamic_3d(self.x3, is_train=True)\n\n    def test_dataformat(self):\n        bn1d = BatchNorm1d(data_format=\'channels_first\', num_features=32)\n        bn2d = BatchNorm2d(data_format=\'channels_first\', num_features=32)\n        bn3d = BatchNorm3d(data_format=\'channels_first\', num_features=32)\n        bn = BatchNorm(data_format=\'channels_first\')\n\n        try:\n            bn_fail = BatchNorm1d(data_format=\'xyz\', num_features=32)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n    def test_exception(self):\n        try:\n            bn = BatchNorm(num_features=32)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n        try:\n            ni = Input([None, 100, 1], name=\'test_ni1\')\n            bn = BatchNorm(decay=1.5)(ni)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n    def test_input_shape(self):\n        try:\n            bn = BatchNorm1d(num_features=32)\n            out = bn(self.x2)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n        try:\n            bn = BatchNorm2d(num_features=32)\n            out = bn(self.x3)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n        try:\n            bn = BatchNorm3d(num_features=32)\n            out = bn(self.x1)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_padding.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Padding_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        ## 1D\n        cls.input_layer1 = tl.layers.Input([None, 100, 1], name=\'input_layer1\')\n\n        n1 = tl.layers.ZeroPad1d(padding=1)(cls.input_layer1)\n        n2 = tl.layers.ZeroPad1d(padding=(2, 3))(cls.input_layer1)\n\n        print(n1._info[0].layer)\n        print(n2._info[0].layer)\n\n        cls.n1_shape = n1.get_shape().as_list()\n        cls.n2_shape = n2.get_shape().as_list()\n\n        ## 2D\n        cls.input_layer2 = tl.layers.Input([None, 100, 100, 3], name=\'input_layer2\')\n\n        n0 = tl.layers.PadLayer([[0, 0], [3, 3], [3, 3], [0, 0]], ""REFLECT"", name=\'inpad\')(cls.input_layer2)\n        n3 = tl.layers.ZeroPad2d(padding=2)(cls.input_layer2)\n        n4 = tl.layers.ZeroPad2d(padding=(2, 3))(cls.input_layer2)\n        n5 = tl.layers.ZeroPad2d(padding=((3, 3), (4, 4)))(cls.input_layer2)\n\n        print(n0._info[0].layer)\n        print(n3._info[0].layer)\n        print(n4._info[0].layer)\n        print(n5._info[0].layer)\n\n        cls.n0_shape = n0.get_shape().as_list()\n        print(cls.n0_shape)\n        cls.n3_shape = n3.get_shape().as_list()\n        cls.n4_shape = n4.get_shape().as_list()\n        cls.n5_shape = n5.get_shape().as_list()\n\n        ## 3D\n        cls.input_layer3 = tl.layers.Input([None, 100, 100, 100, 3], name=\'input_layer3\')\n\n        n6 = tl.layers.ZeroPad3d(padding=2)(cls.input_layer3)\n        n7 = tl.layers.ZeroPad3d(padding=(2, 3, 4))(cls.input_layer3)\n        n8 = tl.layers.ZeroPad3d(padding=((3, 3), (4, 4), (5, 5)))(cls.input_layer3)\n\n        print(n6._info[0].layer)\n        print(n7._info[0].layer)\n        print(n8._info[0].layer)\n\n        cls.n6_shape = n6.get_shape().as_list()\n        cls.n7_shape = n7.get_shape().as_list()\n        cls.n8_shape = n8.get_shape().as_list()\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_n0_shape(self):\n        self.assertEqual(self.n0_shape[1:], [106, 106, 3])\n\n    def test_n1_shape(self):\n        self.assertEqual(self.n1_shape[1:], [102, 1])\n\n    def test_n2_shape(self):\n        self.assertEqual(self.n2_shape[1:], [105, 1])\n\n    def test_n3_shape(self):\n        self.assertEqual(self.n3_shape[1:], [104, 104, 3])\n\n    def test_n4_shape(self):\n        self.assertEqual(self.n4_shape[1:], [104, 106, 3])\n\n    def test_n5_shape(self):\n        self.assertEqual(self.n5_shape[1:], [106, 108, 3])\n\n    def test_n6_shape(self):\n        self.assertEqual(self.n6_shape[1:], [104, 104, 104, 3])\n\n    def test_n7_shape(self):\n        self.assertEqual(self.n7_shape[1:], [104, 106, 108, 3])\n\n    def test_n8_shape(self):\n        self.assertEqual(self.n8_shape[1:], [106, 108, 110, 3])\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_pooling.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Pooling_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        ## 1D ========================================================================\n\n        x_1_input_shape = [None, 100, 1]\n        nin_1 = Input(x_1_input_shape, name=\'test_in1\')\n\n        n1 = tl.layers.Conv1d(n_filter=32, filter_size=5, stride=2, name=\'test_conv1d\')(nin_1)\n        n2 = tl.layers.MaxPool1d(filter_size=3, strides=2, padding=\'SAME\', name=\'test_maxpool1d\')(n1)\n        n3 = tl.layers.MeanPool1d(filter_size=3, strides=2, padding=\'SAME\', name=\'test_meanpool1d\')(n1)\n        n4 = tl.layers.GlobalMaxPool1d(name=\'test_maxpool1d\')(n1)\n        n5 = tl.layers.GlobalMeanPool1d(name=\'test_meanpool1d\')(n1)\n        n16 = tl.layers.MaxPool1d(filter_size=3, strides=1, padding=\'VALID\', dilation_rate=2, name=\'test_maxpool1d\')(n1)\n        n17 = tl.layers.MeanPool1d(filter_size=3, strides=1, padding=\'VALID\', dilation_rate=2,\n                                   name=\'test_meanpool1d\')(n1)\n\n        cls.n1_shape = n1.get_shape().as_list()\n        cls.n2_shape = n2.get_shape().as_list()\n        cls.n3_shape = n3.get_shape().as_list()\n        cls.n4_shape = n4.get_shape().as_list()\n        cls.n5_shape = n5.get_shape().as_list()\n        cls.n16_shape = n16.get_shape().as_list()\n        cls.n17_shape = n17.get_shape().as_list()\n\n        print(""Printing Pool1d"")\n        print(nin_1._info[0].layer)\n        print(n1._info[0].layer)\n        print(n2._info[0].layer)\n        print(n3._info[0].layer)\n        print(n4._info[0].layer)\n        print(n5._info[0].layer)\n        print(n16._info[0].layer)\n        print(n17._info[0].layer)\n\n        ## 2D ========================================================================\n\n        x_2_input_shape = [None, 100, 100, 3]\n        nin_2 = Input(x_2_input_shape, name=\'test_in2\')\n\n        n6 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), name=\'test_conv2d\')(nin_2)\n        n7 = tl.layers.MaxPool2d(filter_size=(3, 3), strides=(2, 2), padding=\'SAME\', name=\'test_maxpool2d\')(n6)\n        n8 = tl.layers.MeanPool2d(filter_size=(3, 3), strides=(2, 2), padding=\'SAME\', name=\'test_meanpool2d\')(n6)\n        n9 = tl.layers.GlobalMaxPool2d(name=\'test_maxpool2d\')(n6)\n        n10 = tl.layers.GlobalMeanPool2d(name=\'test_meanpool2d\')(n6)\n        n15 = tl.layers.PoolLayer(name=\'test_pool2d\')(n6)\n        n18 = tl.layers.CornerPool2d(\'TopLeft\', name=\'test_cornerpool2d\')(n6)\n\n        cls.n6_shape = n6.get_shape().as_list()\n        cls.n7_shape = n7.get_shape().as_list()\n        cls.n8_shape = n8.get_shape().as_list()\n        cls.n9_shape = n9.get_shape().as_list()\n        cls.n10_shape = n10.get_shape().as_list()\n        cls.n15_shape = n15.get_shape().as_list()\n        cls.n18_shape = n18.get_shape().as_list()\n\n        print(""Printing Pool2d"")\n        print(nin_2._info[0].layer)\n        print(n6._info[0].layer)\n        print(n7._info[0].layer)\n        print(n8._info[0].layer)\n        print(n9._info[0].layer)\n        print(n10._info[0].layer)\n        print(n15._info[0].layer)\n        print(n18._info[0].layer)\n\n        ## 3D ========================================================================\n\n        x_3_input_shape = [None, 100, 100, 100, 3]\n        nin_3 = Input(x_3_input_shape, name=\'test_in3\')\n\n        n11 = tl.layers.MeanPool3d(filter_size=(3, 3, 3), strides=(2, 2, 2), padding=\'SAME\',\n                                   name=\'test_meanpool3d\')(nin_3)\n        n12 = tl.layers.GlobalMaxPool3d(name=\'test_maxpool3d\')(nin_3)\n        n13 = tl.layers.GlobalMeanPool3d(name=\'test_meanpool3d\')(nin_3)\n        n14 = tl.layers.MaxPool3d(filter_size=(3, 3, 3), strides=(2, 2, 2), padding=\'SAME\',\n                                  name=\'test_maxpool3d\')(nin_3)\n\n        cls.n11_shape = n11.get_shape().as_list()\n        cls.n12_shape = n12.get_shape().as_list()\n        cls.n13_shape = n13.get_shape().as_list()\n        cls.n14_shape = n14.get_shape().as_list()\n\n        print(""Printing Pool3d"")\n        print(nin_3._info[0].layer)\n        print(n11._info[0].layer)\n        print(n12._info[0].layer)\n        print(n13._info[0].layer)\n        print(n14._info[0].layer)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n        # tf.reset_default_graph()\n\n    def test_n1_shape(self):\n        self.assertEqual(self.n1_shape[1:3], [50, 32])\n\n    def test_n2_shape(self):\n        self.assertEqual(self.n2_shape[1:3], [25, 32])\n\n    def test_n3_shape(self):\n        self.assertEqual(self.n3_shape[1:3], [25, 32])\n\n    def test_n4_shape(self):\n        self.assertEqual(self.n4_shape[-1], 32)\n\n    def test_n5_shape(self):\n        self.assertEqual(self.n5_shape[-1], 32)\n\n    def test_n6_shape(self):\n        self.assertEqual(self.n6_shape[1:4], [50, 50, 32])\n\n    def test_n7_shape(self):\n        self.assertEqual(self.n7_shape[1:4], [25, 25, 32])\n\n    def test_n8_shape(self):\n        self.assertEqual(self.n8_shape[1:4], [25, 25, 32])\n\n    def test_n9_shape(self):\n        self.assertEqual(self.n9_shape[-1], 32)\n\n    def test_n10_shape(self):\n        self.assertEqual(self.n10_shape[-1], 32)\n\n    def test_n11_shape(self):\n        self.assertEqual(self.n11_shape[1:5], [50, 50, 50, 3])\n\n    def test_n12_shape(self):\n        self.assertEqual(self.n12_shape[-1], 3)\n\n    def test_n13_shape(self):\n        self.assertEqual(self.n13_shape[-1], 3)\n\n    def test_n14_shape(self):\n        self.assertEqual(self.n14_shape[1:5], [50, 50, 50, 3])\n\n    def test_n15_shape(self):\n        self.assertEqual(self.n15_shape[1:4], [25, 25, 32])\n\n    def test_n16_shape(self):\n        self.assertEqual(self.n16_shape[1:4], [46, 32])\n\n    def test_n17_shape(self):\n        self.assertEqual(self.n17_shape[1:4], [48, 32])\n\n    def test_n18_shape(self):\n        self.assertEqual(self.n18_shape[1:], [50, 50, 32])\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_recurrent.py,76,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_RNN_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.batch_size = 2\n\n        cls.vocab_size = 20\n        cls.embedding_size = 4\n\n        cls.hidden_size = 8\n        cls.num_steps = 6\n\n        cls.data_n_steps = np.random.randint(low=cls.num_steps // 2, high=cls.num_steps + 1, size=cls.batch_size)\n        cls.data_x = np.random.random([cls.batch_size, cls.num_steps, cls.embedding_size]).astype(np.float32)\n\n        for i in range(cls.batch_size):\n            for j in range(cls.data_n_steps[i], cls.num_steps):\n                cls.data_x[i][j][:] = 0\n\n        cls.data_y = np.zeros([cls.batch_size, 1]).astype(np.float32)\n        cls.data_y2 = np.zeros([cls.batch_size, cls.num_steps]).astype(np.float32)\n\n        map1 = np.random.random([1, cls.num_steps])\n        map2 = np.random.random([cls.embedding_size, 1])\n        for i in range(cls.batch_size):\n            cls.data_y[i] = np.matmul(map1, np.matmul(cls.data_x[i], map2))\n            cls.data_y2[i] = np.matmul(cls.data_x[i], map2)[:, 0]\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_basic_simplernn(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.RNN(\n            cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.1), return_last_output=True,\n            return_seq_2d=False, return_last_state=True\n        )\n        rnn, rnn_state = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0]])\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y, final_state = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_simplernn_class(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.SimpleRNN(\n            units=self.hidden_size, dropout=0.1, return_last_output=True, return_seq_2d=False, return_last_state=True\n        )\n        rnn, rnn_state = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0]])\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y, final_state = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_simplernn2(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.RNN(\n            cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.1), return_last_output=False,\n            return_seq_2d=True, return_last_state=False\n        )\n        rnn = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn])\n        print(rnn_model)\n\n        rnn_model.eval()\n        assert not rnnlayer.is_train\n\n        pred_y, rnn_y = rnn_model(self.data_x)\n        self.assertEqual(pred_y.get_shape().as_list(), [self.batch_size * self.num_steps, 1])\n        self.assertEqual(rnn_y.get_shape().as_list(), [self.batch_size * self.num_steps, self.hidden_size])\n\n    def test_basic_simplernn3(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.RNN(\n            cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.1), return_last_output=False,\n            return_seq_2d=False, return_last_state=False\n        )\n        rnn = rnnlayer(inputs)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=rnn)\n        print(rnn_model)\n\n        rnn_model.eval()\n        assert not rnnlayer.is_train\n\n        rnn_y = rnn_model(self.data_x)\n        self.assertEqual(rnn_y.get_shape().as_list(), [self.batch_size, self.num_steps, self.hidden_size])\n\n    def test_basic_simplernn_dynamic(self):\n\n        class CustomisedModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomisedModel, self).__init__()\n                self.rnnlayer = tl.layers.RNN(\n                    cell=tf.keras.layers.SimpleRNNCell(units=8, dropout=0.1), in_channels=4, return_last_output=False,\n                    return_seq_2d=False, return_last_state=False\n                )\n                self.dense = tl.layers.Dense(in_channels=8, n_units=1)\n\n            def forward(self, x):\n                z = self.rnnlayer(x)\n                z = self.dense(z[:, -1, :])\n                return z\n\n        rnn_model = CustomisedModel()\n        print(rnn_model)\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n        rnn_model.train()\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_simplernn_dynamic_class(self):\n\n        class CustomisedModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomisedModel, self).__init__()\n                self.rnnlayer = tl.layers.SimpleRNN(\n                    units=8, dropout=0.1, in_channels=4, return_last_output=False, return_seq_2d=False,\n                    return_last_state=False\n                )\n                self.dense = tl.layers.Dense(in_channels=8, n_units=1)\n\n            def forward(self, x):\n                z = self.rnnlayer(x)\n                z = self.dense(z[:, -1, :])\n                return z\n\n        rnn_model = CustomisedModel()\n        print(rnn_model)\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n        rnn_model.train()\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_simplernn_dynamic_2(self):\n\n        class CustomisedModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomisedModel, self).__init__()\n                self.rnnlayer = tl.layers.RNN(\n                    cell=tf.keras.layers.SimpleRNNCell(units=8, dropout=0.1), in_channels=4, return_last_output=False,\n                    return_seq_2d=False, return_last_state=False\n                )\n                self.dense = tl.layers.Dense(in_channels=8, n_units=1)\n\n            def forward(self, x):\n                z = self.rnnlayer(x, return_seq_2d=True)\n                z = self.dense(z[-2:, :])\n                return z\n\n        rnn_model = CustomisedModel()\n        print(rnn_model)\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n        rnn_model.train()\n        assert rnn_model.rnnlayer.is_train\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_simplernn_dynamic_3(self):\n\n        class CustomisedModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomisedModel, self).__init__()\n                self.rnnlayer1 = tl.layers.RNN(\n                    cell=tf.keras.layers.SimpleRNNCell(units=8, dropout=0.1), in_channels=4, return_last_output=True,\n                    return_last_state=True\n                )\n                self.rnnlayer2 = tl.layers.RNN(\n                    cell=tf.keras.layers.SimpleRNNCell(units=8, dropout=0.1), in_channels=4, return_last_output=True,\n                    return_last_state=False\n                )\n                self.dense = tl.layers.Dense(in_channels=8, n_units=1)\n\n            def forward(self, x):\n                _, state = self.rnnlayer1(x[:, :2, :])\n                z = self.rnnlayer2(x[:, 2:, :], initial_state=state)\n                z = self.dense(z)\n                return z\n\n        rnn_model = CustomisedModel()\n        print(rnn_model)\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n        rnn_model.train()\n        assert rnn_model.rnnlayer1.is_train\n        assert rnn_model.rnnlayer2.is_train\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_lstmrnn(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.RNN(\n            cell=tf.keras.layers.LSTMCell(units=self.hidden_size, dropout=0.1), return_last_output=True,\n            return_seq_2d=False, return_last_state=True\n        )\n        rnn, rnn_state = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0], rnn_state[1]])\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y, final_h, final_c = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_lstmrnn_class(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.LSTMRNN(\n            units=self.hidden_size, dropout=0.1, return_last_output=True, return_seq_2d=False, return_last_state=True\n        )\n        rnn, rnn_state = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0], rnn_state[1]])\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y, final_h, final_c = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_grurnn(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.RNN(\n            cell=tf.keras.layers.GRUCell(units=self.hidden_size, dropout=0.1), return_last_output=True,\n            return_seq_2d=False, return_last_state=True\n        )\n        rnn, rnn_state = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0]])\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y, final_h = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_grurnn_class(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.GRURNN(\n            units=self.hidden_size, dropout=0.1, return_last_output=True, return_seq_2d=False, return_last_state=True\n        )\n        rnn, rnn_state = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn_state[0]])\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y, final_h = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_birnn_simplernncell(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.BiRNN(\n            fw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.1),\n            bw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size + 1,\n                                                  dropout=0.1), return_seq_2d=True, return_last_state=True\n        )\n        rnn, rnn_fw_state, rnn_bw_state = rnnlayer(inputs)\n        dense = tl.layers.Dense(n_units=1)(rnn)\n        outputs = tl.layers.Reshape([-1, self.num_steps])(dense)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn, rnn_fw_state[0], rnn_bw_state[0]])\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y, r, rfw, rbw = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y2)\n\n            self.assertEqual(\n                r.get_shape().as_list(), [self.batch_size * self.num_steps, self.hidden_size + self.hidden_size + 1]\n            )\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_birnn_lstmcell(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.BiRNN(\n            fw_cell=tf.keras.layers.LSTMCell(units=self.hidden_size, dropout=0.1),\n            bw_cell=tf.keras.layers.LSTMCell(units=self.hidden_size + 1,\n                                             dropout=0.1), return_seq_2d=False, return_last_state=True\n        )\n        rnn, rnn_fw_state, rnn_bw_state = rnnlayer(inputs)\n        din = tl.layers.Reshape([-1, self.hidden_size + self.hidden_size + 1])(rnn)\n        dense = tl.layers.Dense(n_units=1)(din)\n        outputs = tl.layers.Reshape([-1, self.num_steps])(dense)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn, rnn_fw_state[0], rnn_bw_state[0]])\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y, r, rfw, rbw = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y2)\n\n            self.assertEqual(\n                r.get_shape().as_list(), [self.batch_size, self.num_steps, self.hidden_size + self.hidden_size + 1]\n            )\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_birnn_grucell(self):\n\n        class CustomisedModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomisedModel, self).__init__()\n                self.rnnlayer = tl.layers.BiRNN(\n                    fw_cell=tf.keras.layers.GRUCell(units=8,\n                                                    dropout=0.1), bw_cell=tf.keras.layers.GRUCell(units=8, dropout=0.1),\n                    in_channels=4, return_seq_2d=False, return_last_state=False\n                )\n                self.dense = tl.layers.Dense(in_channels=16, n_units=1)\n                self.reshape = tl.layers.Reshape([-1, 6])\n\n            def forward(self, x):\n                z = self.rnnlayer(x, return_seq_2d=True)\n                z = self.dense(z)\n                z = self.reshape(z)\n                return z\n\n        rnn_model = CustomisedModel()\n        print(rnn_model)\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n        rnn_model.train()\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_stack_simplernn(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer1 = tl.layers.RNN(\n            cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.1), return_last_output=False,\n            return_seq_2d=False, return_last_state=False\n        )\n        rnn1 = rnnlayer1(inputs)\n        rnnlayer2 = tl.layers.RNN(\n            cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.1), return_last_output=True,\n            return_seq_2d=False, return_last_state=False\n        )\n        rnn2 = rnnlayer2(rnn1)\n        outputs = tl.layers.Dense(n_units=1)(rnn2)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=outputs)\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n        assert rnnlayer1.is_train\n        assert rnnlayer2.is_train\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_stack_birnn_simplernncell(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.BiRNN(\n            fw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.1),\n            bw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size + 1,\n                                                  dropout=0.1), return_seq_2d=False, return_last_state=False\n        )\n        rnn = rnnlayer(inputs)\n        rnnlayer2 = tl.layers.BiRNN(\n            fw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.1),\n            bw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size + 1,\n                                                  dropout=0.1), return_seq_2d=True, return_last_state=False\n        )\n        rnn2 = rnnlayer2(rnn)\n        dense = tl.layers.Dense(n_units=1)(rnn2)\n        outputs = tl.layers.Reshape([-1, self.num_steps])(dense)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=outputs)\n        print(rnn_model)\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n        assert rnnlayer2.is_train\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y2)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n    def test_basic_simplernn_dropout_1(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.RNN(\n            cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.5), return_last_output=True,\n            return_seq_2d=False, return_last_state=False\n        )\n        rnn = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn])\n        print(rnn_model)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n\n        pred_y, rnn_1 = rnn_model(self.data_x)\n        pred_y, rnn_2 = rnn_model(self.data_x)\n        self.assertFalse(np.allclose(rnn_1, rnn_2))\n\n        rnn_model.eval()\n        assert not rnnlayer.is_train\n\n        pred_y_1, rnn_1 = rnn_model(self.data_x)\n        pred_y_2, rnn_2 = rnn_model(self.data_x)\n        self.assertTrue(np.allclose(rnn_1, rnn_2))\n\n    def test_basic_simplernn_dropout_2(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.RNN(\n            cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, recurrent_dropout=0.5), return_last_output=True,\n            return_seq_2d=False, return_last_state=False\n        )\n        rnn = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn])\n        print(rnn_model)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n\n        pred_y, rnn_1 = rnn_model(self.data_x)\n        pred_y, rnn_2 = rnn_model(self.data_x)\n        self.assertFalse(np.allclose(rnn_1, rnn_2))\n\n        rnn_model.eval()\n        assert not rnnlayer.is_train\n\n        pred_y_1, rnn_1 = rnn_model(self.data_x)\n        pred_y_2, rnn_2 = rnn_model(self.data_x)\n        self.assertTrue(np.allclose(rnn_1, rnn_2))\n\n    def test_basic_birnn_simplernn_dropout_1(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.BiRNN(\n            fw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, dropout=0.5),\n            bw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size,\n                                                  dropout=0.5), return_seq_2d=True, return_last_state=False\n        )\n        rnn = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn])\n        print(rnn_model)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n\n        pred_y, rnn_1 = rnn_model(self.data_x)\n        pred_y, rnn_2 = rnn_model(self.data_x)\n        self.assertFalse(np.allclose(rnn_1, rnn_2))\n\n        rnn_model.eval()\n        assert not rnnlayer.is_train\n\n        pred_y_1, rnn_1 = rnn_model(self.data_x)\n        pred_y_2, rnn_2 = rnn_model(self.data_x)\n        self.assertTrue(np.allclose(rnn_1, rnn_2))\n\n    def test_basic_birnn_simplernn_dropout_2(self):\n\n        inputs = tl.layers.Input([self.batch_size, self.num_steps, self.embedding_size])\n        rnnlayer = tl.layers.BiRNN(\n            fw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size, recurrent_dropout=0.5),\n            bw_cell=tf.keras.layers.SimpleRNNCell(units=self.hidden_size,\n                                                  recurrent_dropout=0.5), return_seq_2d=True, return_last_state=False\n        )\n        rnn = rnnlayer(inputs)\n        outputs = tl.layers.Dense(n_units=1)(rnn)\n        rnn_model = tl.models.Model(inputs=inputs, outputs=[outputs, rnn])\n        print(rnn_model)\n\n        rnn_model.train()\n        assert rnnlayer.is_train\n\n        pred_y, rnn_1 = rnn_model(self.data_x)\n        pred_y, rnn_2 = rnn_model(self.data_x)\n        self.assertFalse(np.allclose(rnn_1, rnn_2))\n\n        rnn_model.eval()\n        assert not rnnlayer.is_train\n\n        pred_y_1, rnn_1 = rnn_model(self.data_x)\n        pred_y_2, rnn_2 = rnn_model(self.data_x)\n        self.assertTrue(np.allclose(rnn_1, rnn_2))\n\n    def test_sequence_length(self):\n        data = [[[1], [2], [0], [0], [0]], [[1], [2], [3], [0], [0]], [[1], [2], [6], [1], [0]]]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n        length = tl.layers.retrieve_seq_length_op(data)\n        print(length)\n        data = [\n            [[1, 2], [2, 2], [1, 2], [1, 2], [0, 0]], [[2, 3], [2, 4], [3, 2], [0, 0], [0, 0]],\n            [[3, 3], [2, 2], [5, 3], [1, 2], [0, 0]]\n        ]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n        length = tl.layers.retrieve_seq_length_op(data)\n        print(length)\n\n    def test_sequence_length2(self):\n        data = [[1, 2, 0, 0, 0], [1, 2, 3, 0, 0], [1, 2, 6, 1, 0]]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n        length = tl.layers.retrieve_seq_length_op2(data)\n        print(length)\n\n    def test_sequence_length3(self):\n        data = [[[1], [2], [0], [0], [0]], [[1], [2], [3], [0], [0]], [[1], [2], [6], [1], [0]]]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n        length = tl.layers.retrieve_seq_length_op3(data)\n        print(length)\n        data = [\n            [[1, 2], [2, 2], [1, 2], [1, 2], [0, 0]], [[2, 3], [2, 4], [3, 2], [0, 0], [0, 0]],\n            [[3, 3], [2, 2], [5, 3], [1, 2], [0, 0]]\n        ]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n        length = tl.layers.retrieve_seq_length_op3(data)\n        print(length)\n        data = [[1, 2, 0, 0, 0], [1, 2, 3, 0, 0], [1, 2, 6, 1, 0]]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n        length = tl.layers.retrieve_seq_length_op3(data)\n        print(length)\n        data = [\n            [\'hello\', \'world\', \'\', \'\', \'\'], [\'hello\', \'world\', \'tensorlayer\', \'\', \'\'],\n            [\'hello\', \'world\', \'tensorlayer\', \'2.0\', \'\']\n        ]\n        data = tf.convert_to_tensor(data, dtype=tf.string)\n        length = tl.layers.retrieve_seq_length_op3(data, pad_val=\'\')\n        print(length)\n\n        try:\n            data = [1, 2, 0, 0, 0]\n            data = tf.convert_to_tensor(data, dtype=tf.float32)\n            length = tl.layers.retrieve_seq_length_op3(data)\n            print(length)\n        except Exception as e:\n            print(e)\n\n        try:\n            data = np.random.random([4, 2, 6, 2])\n            data = tf.convert_to_tensor(data, dtype=tf.float32)\n            length = tl.layers.retrieve_seq_length_op3(data)\n            print(length)\n        except Exception as e:\n            print(e)\n\n    def test_target_mask_op(self):\n        fail_flag = False\n        data = [\n            [\'hello\', \'world\', \'\', \'\', \'\'], [\'hello\', \'world\', \'tensorlayer\', \'\', \'\'],\n            [\'hello\', \'world\', \'tensorlayer\', \'2.0\', \'\']\n        ]\n        try:\n            tl.layers.target_mask_op(data, pad_val=\'\')\n            fail_flag = True\n        except AttributeError as e:\n            print(e)\n        if fail_flag:\n            self.fail(""Type error not raised"")\n\n        data = tf.convert_to_tensor(data, dtype=tf.string)\n        mask = tl.layers.target_mask_op(data, pad_val=\'\')\n        print(mask)\n\n        data = [[[1], [0], [0], [0], [0]], [[1], [2], [3], [0], [0]], [[1], [2], [0], [1], [0]]]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n        mask = tl.layers.target_mask_op(data)\n        print(mask)\n\n        data = [\n            [[0, 0], [2, 2], [1, 2], [1, 2], [0, 0]], [[2, 3], [2, 4], [3, 2], [1, 0], [0, 0]],\n            [[3, 3], [0, 1], [5, 3], [1, 2], [0, 0]]\n        ]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n        mask = tl.layers.target_mask_op(data)\n        print(mask)\n\n        fail_flag = False\n        try:\n            data = [1, 2, 0, 0, 0]\n            data = tf.convert_to_tensor(data, dtype=tf.float32)\n            tl.layers.target_mask_op(data)\n            fail_flag = True\n        except ValueError as e:\n            print(e)\n        if fail_flag:\n            self.fail(""Wrong data shape not detected."")\n\n        fail_flag = False\n        try:\n            data = np.random.random([4, 2, 6, 2])\n            data = tf.convert_to_tensor(data, dtype=tf.float32)\n            tl.layers.target_mask_op(data)\n            fail_flag = True\n        except ValueError as e:\n            print(e)\n        if fail_flag:\n            self.fail(""Wrong data shape not detected."")\n\n    def test_dynamic_rnn(self):\n        batch_size = 3\n        num_steps = 5\n        embedding_size = 6\n\n        hidden_size = 4\n        inputs = tl.layers.Input([batch_size, num_steps, embedding_size])\n\n        rnn_layer = tl.layers.RNN(\n            cell=tf.keras.layers.LSTMCell(units=hidden_size, dropout=0.1), in_channels=embedding_size,\n            return_last_output=True, return_last_state=True\n        )\n\n        rnn_layer.is_train = False\n\n        print(tl.layers.retrieve_seq_length_op3(inputs))\n        _ = rnn_layer(inputs, sequence_length=tl.layers.retrieve_seq_length_op3(inputs))\n        _ = rnn_layer(inputs, sequence_length=np.array([5, 5, 5]))\n\n        # test exceptions\n        except_flag = False\n        try:\n            _ = rnn_layer(inputs, sequence_length=1)\n            except_flag = True\n        except TypeError as e:\n            print(e)\n\n        try:\n            _ = rnn_layer(inputs, sequence_length=[""str"", 1, 2])\n            except_flag = True\n        except TypeError as e:\n            print(e)\n\n        try:\n            _ = rnn_layer(inputs, sequence_length=[10, 2, 2])\n            except_flag = True\n        except ValueError as e:\n            print(e)\n\n        try:\n            _ = rnn_layer(inputs, sequence_length=[1])\n            except_flag = True\n        except ValueError as e:\n            print(e)\n\n        if except_flag:\n            self.fail(""Exception not detected."")\n\n        # test warning\n        for _ in range(5):\n            _ = rnn_layer(inputs, sequence_length=[5, 5, 5], return_last_output=False, return_last_state=True)\n            _ = rnn_layer(inputs, sequence_length=[5, 5, 5], return_last_output=True, return_last_state=False)\n\n        x = rnn_layer(inputs, sequence_length=None, return_last_output=True, return_last_state=True)\n        y = rnn_layer(inputs, sequence_length=[5, 5, 5], return_last_output=True, return_last_state=True)\n\n        assert len(x) == 2\n        assert len(y) == 2\n\n        for i, j in zip(x, y):\n            self.assertTrue(np.allclose(i, j))\n\n    def test_dynamic_rnn_with_seq_len_op2(self):\n        data = [[[1], [2], [0], [0], [0]], [[1], [2], [3], [0], [0]], [[1], [2], [6], [1], [1]]]\n        data = tf.convert_to_tensor(data, dtype=tf.float32)\n\n        class DynamicRNNExample(tl.models.Model):\n\n            def __init__(self):\n                super(DynamicRNNExample, self).__init__()\n\n                self.rnnlayer = tl.layers.RNN(\n                    cell=tf.keras.layers.SimpleRNNCell(units=6, dropout=0.1), in_channels=1, return_last_output=True,\n                    return_last_state=True\n                )\n\n            def forward(self, x):\n                z0, s0 = self.rnnlayer(x, sequence_length=None)\n                z1, s1 = self.rnnlayer(x, sequence_length=tl.layers.retrieve_seq_length_op3(x))\n                z2, s2 = self.rnnlayer(x, sequence_length=tl.layers.retrieve_seq_length_op3(x), initial_state=s1)\n                print(z0)\n                print(z1)\n                print(z2)\n                print(""==="")\n                print(s0)\n                print(s1)\n                print(s2)\n                return z2, s2\n\n        model = DynamicRNNExample()\n        model.eval()\n\n        output, state = model(data)\n        print(output.shape)\n        print(state)\n\n    def test_dynamic_rnn_with_fake_data(self):\n\n        class CustomisedModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomisedModel, self).__init__()\n                self.rnnlayer = tl.layers.LSTMRNN(\n                    units=8, dropout=0.1, in_channels=4, return_last_output=True, return_last_state=False\n                )\n                self.dense = tl.layers.Dense(in_channels=8, n_units=1)\n\n            def forward(self, x):\n                z = self.rnnlayer(x, sequence_length=tl.layers.retrieve_seq_length_op3(x))\n                z = self.dense(z[:, :])\n                return z\n\n        rnn_model = CustomisedModel()\n        print(rnn_model)\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n        rnn_model.train()\n\n        for epoch in range(50):\n            with tf.GradientTape() as tape:\n                pred_y = rnn_model(self.data_x)\n                loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n\n            gradients = tape.gradient(loss, rnn_model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, rnn_model.trainable_weights))\n\n            if (epoch + 1) % 10 == 0:\n                print(""epoch %d, loss %f"" % (epoch, loss))\n\n        filename = ""dynamic_rnn.h5""\n        rnn_model.save_weights(filename)\n\n        # Testing saving and restoring of RNN weights\n        rnn_model2 = CustomisedModel()\n        rnn_model2.eval()\n        pred_y = rnn_model2(self.data_x)\n        loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n        print(""MODEL INIT loss %f"" % (loss))\n\n        rnn_model2.load_weights(filename)\n        pred_y = rnn_model2(self.data_x)\n        loss = tl.cost.mean_squared_error(pred_y, self.data_y)\n        print(""MODEL RESTORE W loss %f"" % (loss))\n\n        import os\n        os.remove(filename)\n\n\nif __name__ == \'__main__\':\n\n    unittest.main()\n'"
tests/layers/test_layers_resampling.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport sys\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tests.utils import CustomTestCase\n\nsys.path.append(""/home/wurundi/workspace/tensorlayer2"")\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Pooling_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        ## 1D ========================================================================\n\n        ## 2D ========================================================================\n\n        x_2_input_shape = [None, 100, 100, 3]\n        nin_2 = Input(x_2_input_shape)\n\n        n6 = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), name=\'test_conv2d\')(nin_2)\n\n        n7 = tl.layers.UpSampling2d(scale=(2, 2), name=\'test_UpSampling2d_1\')(n6)\n\n        n8 = tl.layers.UpSampling2d(scale=3, name=\'test_UpSampling2d_2\')(n6)\n\n        n9 = tl.layers.DownSampling2d(scale=(2, 2), name=\'test_DownSampling2d_1\')(n6)\n\n        n10 = tl.layers.DownSampling2d(scale=5, name=\'test_DownSampling2d_2\')(n6)\n\n        cls.n6_shape = n6.get_shape().as_list()\n        cls.n7_shape = n7.get_shape().as_list()\n        cls.n8_shape = n8.get_shape().as_list()\n        cls.n9_shape = n9.get_shape().as_list()\n        cls.n10_shape = n10.get_shape().as_list()\n\n        print(""Printing UpSampling2d"")\n        print(nin_2._info[0].layer)\n        print(n6._info[0].layer)\n        print(n7._info[0].layer)\n        print(n8._info[0].layer)\n        print(n9._info[0].layer)\n        print(n10._info[0].layer)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n        # tf.reset_default_graph()\n\n    def test_UpSampling2d(self):\n        self.assertEqual(self.n7_shape[1:3], [100, 100])\n        self.assertEqual(self.n8_shape[1:3], [150, 150])\n\n        try:\n            layer = tl.layers.UpSampling2d(scale=(2, 2, 2))\n        except Exception as e:\n            print(e)\n\n    def test_DownSampling2d(self):\n        self.assertEqual(self.n9_shape[1:3], [25, 25])\n        self.assertEqual(self.n10_shape[1:3], [10, 10])\n\n        try:\n            layer = tl.layers.DownSampling2d(scale=(2, 2, 2))\n        except Exception as e:\n            print(e)\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/layers/test_layers_scale.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Scale_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_scale(self):\n        inputs = tl.layers.Input([8, 3])\n        dense = tl.layers.Dense(n_units=10)(inputs)\n        scalelayer = tl.layers.Scale(init_scale=0.5)\n        outputs = scalelayer(dense)\n        model = tl.models.Model(inputs=inputs, outputs=[dense, outputs])\n\n        print(scalelayer)\n\n        data = np.random.random(size=[8, 3]).astype(np.float32)\n        dout, fout = model(data, is_train=True)\n\n        for i in range(len(dout)):\n            for j in range(len(dout[i])):\n                self.assertEqual(dout[i][j].numpy() * 0.5, fout[i][j].numpy())\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n"""
tests/layers/test_layers_shape.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Shape_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.data = np.random.random(size=[8, 4, 3]).astype(np.float32)\n        cls.imgdata = np.random.random(size=[2, 16, 16, 8]).astype(np.float32)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_flatten(self):\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.flatten = tl.layers.Flatten()\n\n            def forward(self, x):\n                return self.flatten(x)\n\n        model = CustomizeModel()\n        print(model.flatten)\n        model.train()\n        out = model(self.data)\n        self.assertEqual(out.get_shape().as_list(), [8, 12])\n\n    def test_reshape(self):\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.reshape1 = tl.layers.Reshape(shape=(8, 12))\n                self.reshape2 = tl.layers.Reshape(shape=(-1, 12))\n                self.reshape3 = tl.layers.Reshape(shape=())\n\n            def forward(self, x):\n                return self.reshape1(x), self.reshape2(x), self.reshape3(x[0][0][0])\n\n        model = CustomizeModel()\n        print(model.reshape1)\n        print(model.reshape2)\n        print(model.reshape3)\n        model.train()\n        out1, out2, out3 = model(self.data)\n        self.assertEqual(out1.get_shape().as_list(), [8, 12])\n        self.assertEqual(out2.get_shape().as_list(), [8, 12])\n        self.assertEqual(out3.get_shape().as_list(), [])\n\n    def test_transpose(self):\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self):\n                super(CustomizeModel, self).__init__()\n                self.transpose1 = tl.layers.Transpose()\n                self.transpose2 = tl.layers.Transpose([2, 1, 0])\n                self.transpose3 = tl.layers.Transpose([0, 2, 1])\n                self.transpose4 = tl.layers.Transpose(conjugate=True)\n\n            def forward(self, x):\n                return self.transpose1(x), self.transpose2(x), self.transpose3(x), self.transpose4(x)\n\n        real = np.random.random([8, 4, 3]).astype(np.float32)\n        comp = np.random.random([8, 4, 3]).astype(np.float32)\n        complex_data = real + 1j * comp\n        model = CustomizeModel()\n        print(model.transpose1)\n        print(model.transpose2)\n        print(model.transpose3)\n        print(model.transpose4)\n        model.train()\n        out1, out2, out3, out4 = model(self.data)\n        self.assertEqual(out1.get_shape().as_list(), [3, 4, 8])\n        self.assertEqual(out2.get_shape().as_list(), [3, 4, 8])\n        self.assertEqual(out3.get_shape().as_list(), [8, 3, 4])\n        self.assertEqual(out4.get_shape().as_list(), [3, 4, 8])\n        self.assertTrue(np.array_equal(out1.numpy(), out4.numpy()))\n\n        out1, out2, out3, out4 = model(complex_data)\n        self.assertEqual(out1.get_shape().as_list(), [3, 4, 8])\n        self.assertEqual(out2.get_shape().as_list(), [3, 4, 8])\n        self.assertEqual(out3.get_shape().as_list(), [8, 3, 4])\n        self.assertEqual(out4.get_shape().as_list(), [3, 4, 8])\n        self.assertTrue(np.array_equal(np.conj(out1.numpy()), out4.numpy()))\n\n    def test_shuffle(self):\n\n        class CustomizeModel(tl.models.Model):\n\n            def __init__(self, x):\n                super(CustomizeModel, self).__init__()\n                self.shuffle = tl.layers.Shuffle(x)\n\n            def forward(self, x):\n                return self.shuffle(x)\n\n        model = CustomizeModel(2)\n        print(model.shuffle)\n        model.train()\n        out = model(self.imgdata)\n        self.assertEqual(out.get_shape().as_list(), [2, 16, 16, 8])\n        try:\n            model_fail = CustomizeModel(3)\n            print(model_fail.shuffle)\n            model_fail.train()\n            out = model_fail(self.imgdata)\n            self.assertEqual(out.get_shape().as_list(), [2, 16, 16, 8])\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n\nif __name__ == '__main__':\n\n    unittest.main()\n"""
tests/layers/test_layers_stack.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Stack_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""-"" * 20, ""Layer_Stack_Test"", ""-"" * 20)\n        cls.batch_size = 4\n        cls.inputs_shape = [cls.batch_size, 10]\n\n        cls.ni = Input(cls.inputs_shape, name=\'input_layer\')\n        a = Dense(n_units=5)(cls.ni)\n        b = Dense(n_units=5)(cls.ni)\n        cls.layer1 = Stack(axis=1)\n        cls.n1 = cls.layer1([a, b])\n        cls.M = Model(inputs=cls.ni, outputs=cls.n1)\n\n        cls.inputs = tf.random.uniform(cls.inputs_shape)\n        cls.n2 = cls.M(cls.inputs, is_train=True)\n\n        print(cls.layer1)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        self.assertEqual(self.n1.shape, (4, 2, 5))\n\n    def test_layer_n2(self):\n        self.assertEqual(self.n2.shape, (4, 2, 5))\n\n\nclass Layer_UnStack_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""-"" * 20, ""Layer_UnStack_Test"", ""-"" * 20)\n        cls.batch_size = 4\n        cls.inputs_shape = [cls.batch_size, 10]\n\n        cls.ni = Input(cls.inputs_shape, name=\'input_layer\')\n        a = Dense(n_units=5)(cls.ni)\n        cls.layer1 = UnStack(axis=1)  # unstack in channel axis\n        cls.n1 = cls.layer1(a)\n        cls.M = Model(inputs=cls.ni, outputs=cls.n1)\n\n        cls.inputs = tf.random.uniform(cls.inputs_shape)\n        cls.n2 = cls.M(cls.inputs, is_train=True)\n\n        print(cls.layer1)\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_layer_n1(self):\n        self.assertEqual(len(self.n1), 5)\n        self.assertEqual(self.n1[0].shape, (self.batch_size, ))\n\n    def test_layer_n2(self):\n        self.assertEqual(len(self.n2), 5)\n        self.assertEqual(self.n1[0].shape, (self.batch_size, ))\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/models/test_auto_naming.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef basic_static_model(name=None, conv1_name=""conv1"", conv2_name=""conv2""):\n    ni = Input((None, 24, 24, 3))\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=conv1_name)(ni)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(nn)\n\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=conv2_name)(nn)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(nn)\n\n    M = Model(inputs=ni, outputs=nn, name=name)\n    return M\n\n\ndef nested_static_model(name=None, inner_model_name=None):\n    ni = Input((None, 24, 24, 3))\n    nn = ModelLayer(basic_static_model(inner_model_name))(ni)\n    M = Model(inputs=ni, outputs=nn, name=name)\n    return M\n\n\nclass basic_dynamic_model(Model):\n\n    def __init__(self, name=None, conv1_name=""conv1"", conv2_name=""conv2""):\n        super(basic_dynamic_model, self).__init__(name=name)\n        self.conv1 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=3, name=conv1_name)\n        self.pool1 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')\n\n        self.conv2 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=16, name=conv2_name)\n        self.pool2 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        return x\n\n\nclass nested_dynamic_model(Model):\n\n    def __init__(self, name=None, inner_model_name_1=None, inner_model_name_2=None):\n        super(nested_dynamic_model, self).__init__(name=name)\n\n        self.inner_model_1 = basic_dynamic_model(name=inner_model_name_1)\n        self.inner_model_2 = basic_dynamic_model(name=inner_model_name_2)\n\n    def forward(self, x):\n        x = self.inner_model_1(x)\n        x = self.inner_model_2(x)\n        return x\n\n\nclass Auto_Naming_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_dynamic_model_auto_naming(self):\n        print(\'-\' * 20, \'test_dynamic_model_auto_naming\', \'-\' * 20)\n        test_flag = True\n\n        model_basic = basic_dynamic_model()\n        model_basic_1 = basic_dynamic_model()\n        model_basic_2 = basic_dynamic_model(""basic_dynamic_model_2"")\n        model_basic_3 = basic_dynamic_model()\n        model_basic_given_name = basic_dynamic_model(""a_dynamic_model"")\n\n        self.assertEqual(model_basic.name, ""basic_dynamic_model"")\n        self.assertEqual(model_basic.conv1.name, ""conv1"")\n        self.assertEqual(model_basic_1.name, ""basic_dynamic_model_1"")\n        self.assertEqual(model_basic_1.conv1.name, ""conv1"")\n        self.assertEqual(model_basic_2.name, ""basic_dynamic_model_2"")\n        self.assertEqual(model_basic_3.name, ""basic_dynamic_model_3"")\n        self.assertEqual(model_basic_given_name.name, ""a_dynamic_model"")\n\n        try:\n            model_basic_given_repeat_name = basic_dynamic_model(""basic_dynamic_model_1"")\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Failed to detect repeat user given names"")\n\n        model_nested = nested_dynamic_model()\n        model_nested_1 = nested_dynamic_model(inner_model_name_1=""a_inner_dynamic_model"")\n\n        self.assertEqual(model_nested.name, ""nested_dynamic_model"")\n        self.assertEqual(model_nested.inner_model_1.name, ""basic_dynamic_model_4"")\n        self.assertEqual(model_nested.inner_model_2.name, ""basic_dynamic_model_5"")\n        self.assertEqual(model_nested_1.name, ""nested_dynamic_model_1"")\n        self.assertEqual(model_nested_1.inner_model_1.name, ""a_inner_dynamic_model"")\n        self.assertEqual(model_nested_1.inner_model_2.name, ""basic_dynamic_model_6"")\n\n        try:\n            model_nested_given_repeat_name = nested_dynamic_model(inner_model_name_2=""basic_dynamic_model_1"")\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Failed to detect nested repeat user given names"")\n\n        try:\n            model_nested_given_repeat_name_1 = nested_dynamic_model(name=""basic_dynamic_model_5"")\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Failed to detect nested repeat user given names"")\n\n    def test_static_model_auto_naming(self):\n        print(\'-\' * 20, \'test_static_model_auto_naming\', \'-\' * 20)\n        test_flag = True\n\n        model_basic = basic_static_model()\n        model_basic_1 = basic_static_model()\n        assname = ""model_%d"" % (int(model_basic_1.name.split(""_"")[-1]) + 1)\n        model_basic_2 = basic_static_model(name=assname)\n        model_basic_3 = basic_static_model()\n        model_basic_given_name = basic_static_model(""a_static_model"")\n\n        # self.assertEqual(model_basic.name, ""model"")\n        basename = model_basic.name\n        bnum = basename.split(""_"")[-1]\n        try:\n            bnum = int(bnum)\n        except:\n            bnum = 0\n        self.assertEqual(model_basic_1.name, ""model_%d"" % (bnum + 1))\n        self.assertEqual(model_basic_2.name, assname)\n        self.assertEqual(model_basic_3.name, ""model_%d"" % (int(assname.split(""_"")[-1]) + 1))\n        self.assertEqual(model_basic_given_name.name, ""a_static_model"")\n\n        try:\n            model_basic_given_repeat_name = basic_static_model(""model_1"")\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Failed to detect repeat user given names"")\n\n        model_nested = nested_static_model()\n        model_nested_1 = nested_static_model(inner_model_name=""a_inner_static_model"")\n\n        # self.assertEqual(model_nested.name, ""model_5"")\n        self.assertEqual(model_nested_1.name, ""model_%d"" % (int(model_nested.name.split(""_"")[-1]) + 1))\n\n        try:\n            model_nested_given_repeat_name = nested_static_model(inner_model_name=""a_inner_static_model"")\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Failed to detect repeat user given names"")\n\n    def test_layer_name_uniqueness(self):\n        print(\'-\' * 20, \'test_layer_name_uniqueness\', \'-\' * 20)\n        test_flag = True\n\n        # dynamic\n        try:\n            model_dynamic = basic_dynamic_model(conv1_name=""conv"", conv2_name=""conv"", name=""test_layer_name_dynamic"")\n            # dynamic mode check uniqueness when self.all_layers is called\n            all_layers = model_dynamic.all_layers\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Failed to detect that layers inside a model have the same name in dynamic mode"")\n\n        # static\n        try:\n            model_static = basic_static_model(conv1_name=""conv"", conv2_name=""conv"", name=""test_layer_name_static"")\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Failed to detect that layers inside a model have the same name in static mode"")\n\n    def test_vgg_auto_naming(self):\n        print(\'-\' * 20, \'test_vgg_auto_naming\', \'-\' * 20)\n        test_flag = True\n\n        vgg = vgg16()\n        vgg_1 = vgg16()\n        vgg_2 = vgg16(name=""vgg16_2"")\n        vgg_3 = vgg16()\n        vgg_given_name = vgg16(name=""a_vgg_model"")\n\n        # self.assertEqual(vgg.name, ""vgg16"")\n        # self.assertEqual(vgg_1.name, ""vgg16_1"")\n        # self.assertEqual(vgg_2.name, ""vgg16_2"")\n        # self.assertEqual(vgg_3.name, ""vgg16_3"")\n        # self.assertEqual(vgg_given_name.name, ""a_vgg_model"")\n\n        # try:\n        #     vgg_given_repeat_name = vgg16(name=""vgg16_1"")\n        #     test_flag = False\n        # except Exception as e:\n        #     print(e)\n        # if not test_flag:\n        #     self.fail(""Failed to detect repeat user given names"")\n\n    def test_layerlist(self):\n        print(\'-\' * 20, \'test_layerlist\', \'-\' * 20)\n        test_flag = True\n\n        try:\n            inputs = tl.layers.Input([10, 5])\n            layer1 = tl.layers.LayerList(\n                [tl.layers.Dense(n_units=4, name=\'dense1\'),\n                 tl.layers.Dense(n_units=3, name=\'dense1\')]\n            )(inputs)\n            model = tl.models.Model(inputs=inputs, outputs=layer1, name=\'layerlistmodel\')\n            print([w.name for w in model.all_weights])\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Fail to detect duplicate name in LayerList"")\n\n    def test_modellayer(self):\n        print(\'-\' * 20, \'test_modellayer\', \'-\' * 20)\n        test_flag = True\n\n        try:\n\n            class inner_model(Model):\n\n                def __init__(self):\n                    super(inner_model, self).__init__()\n                    self.layer1 = tl.layers.Dense(n_units=4, in_channels=5, name=\'dense1\')\n                    self.layer2 = tl.layers.Dense(n_units=4, in_channels=4, name=\'dense1\')\n\n                def forward(self, x):\n                    return self.layer2(self.layer1(x))\n\n            inputs = tl.layers.Input([10, 5])\n            model_layer = tl.layers.ModelLayer(inner_model())(inputs)\n            model = tl.models.Model(inputs=inputs, outputs=model_layer, name=\'modellayermodel\')\n            print(model)\n            print([w.name for w in model.all_weights])\n            test_flag = False\n        except Exception as e:\n            print(e)\n        if not test_flag:\n            self.fail(""Fail to detect duplicate name in ModelLayer"")\n\n    def test_layerlist(self):\n        try:\n            inputs = tl.layers.Input([10, 5])\n            layer1 = tl.layers.LayerList(\n                [tl.layers.Dense(n_units=4, name=\'dense1\'),\n                 tl.layers.Dense(n_units=3, name=\'dense1\')]\n            )(inputs)\n            model = tl.models.Model(inputs=inputs, outputs=layer1, name=\'layerlistmodel\')\n            print([w.name for w in model.all_weights])\n            self.fail(""Fail to detect duplicate name in layerlist"")\n        except Exception as e:\n            print(e)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/models/test_keras_save.py,0,"b""import tensorflow as tf\nfrom tensorflow.python.keras import Model\nfrom tensorflow.python.keras.applications import VGG16\nfrom tensorflow.python.keras.layers import Conv2D, Dense\nfrom tensorflow.python.training import saver\n\n# get the whole model\n# vgg = VGG16(weights=None)\n# print([x.name for x in vgg.weights])\n\n\nclass Nested_VGG(Model):\n\n    def __init__(self):\n        super(Nested_VGG, self).__init__()\n        self.vgg1 = VGG16(weights=None)\n        # print([x.name for x in self.vgg1.weights])\n        self.vgg2 = VGG16(weights=None)\n        self.dense = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')\n\n    def call(self, inputs, training=None, mask=None):\n        pass\n\n\nclass MyModel(Model):\n\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.inner = Nested_VGG()\n\n    def call(self, inputs, training=None, mask=None):\n        pass\n\n\nmodel = MyModel()\nprint([x.name for x in model.layers])\n# print([x.name for x in model.inner.weights])\nprint('vgg1:')\nprint([x.name for x in model.inner.vgg1.weights])\nprint([x.name for x in model.inner.vgg1.layers])\n\nprint('vgg2')\nprint(model.inner.vgg2.get_layer('block1_conv1').kernel.name)\nprint([x.name for x in model.inner.vgg2.weights])\nprint([x.name for x in model.inner.vgg2.layers])\nmodel.save_weights('./keras_model.h5')\n"""
tests/models/test_model_core.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef basic_static_model():\n    ni = Input((None, 24, 24, 3))\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=""conv1"")(ni)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(nn)\n\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=""conv2"")(nn)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(nn)\n\n    nn = Flatten(name=\'flatten\')(nn)\n    nn = Dense(100, act=None, name=""dense1"")(nn)\n    nn = Dense(10, act=None, name=""dense2"")(nn)\n    M = Model(inputs=ni, outputs=nn)\n    return M\n\n\nclass basic_dynamic_model(Model):\n\n    def __init__(self):\n        super(basic_dynamic_model, self).__init__()\n        self.conv1 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=3, name=""conv1"")\n        self.pool1 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')\n\n        self.conv2 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=16, name=""conv2"")\n        self.pool2 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')\n\n        self.flatten = Flatten(name=\'flatten\')\n        self.dense1 = Dense(100, act=None, in_channels=576, name=""dense1"")\n        self.dense2 = Dense(10, act=None, in_channels=100, name=""dense2"")\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.flatten(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        return x\n\n\nclass Model_Core_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_dynamic_basic(self):\n        print(\'-\' * 20, \'test_dynamic_basic\', \'-\' * 20)\n        model_basic = basic_dynamic_model()\n\n        # test empty model before calling\n        self.assertEqual(model_basic.is_train, None)\n        self.assertEqual(model_basic._all_weights, None)\n        self.assertEqual(model_basic._inputs, None)\n        self.assertEqual(model_basic._outputs, None)\n        self.assertEqual(model_basic._model_layer, None)\n        self.assertEqual(model_basic._all_layers, None)\n        self.assertEqual(model_basic._nodes_fixed, False)\n\n        # test layer and weights access\n        all_layers = model_basic.all_layers\n        self.assertEqual(len(model_basic.all_layers), 7)\n        self.assertEqual(model_basic._all_weights, None)\n\n        self.assertIsNotNone(model_basic.all_weights)\n        print([w.name for w in model_basic.all_weights])\n\n        # test model mode\n        model_basic.train()\n        self.assertEqual(model_basic.is_train, True)\n        model_basic.eval()\n        self.assertEqual(model_basic.is_train, False)\n        model_basic.test()\n        self.assertEqual(model_basic.is_train, False)\n        model_basic.infer()\n        self.assertEqual(model_basic.is_train, False)\n\n        # test as_layer\n        try:\n            model_basic.as_layer()\n        except Exception as e:\n            print(e)\n        self.assertIsNone(model_basic._model_layer)\n\n        # test print\n        try:\n            print(model_basic)\n        except Exception as e:\n            print(e)\n\n        # test forwarding\n        inputs = np.random.normal(size=[2, 24, 24, 3]).astype(np.float32)\n        outputs1 = model_basic(inputs)\n        self.assertEqual(model_basic._nodes_fixed, True)\n        self.assertEqual(model_basic.is_train, False)\n\n        try:\n            outputs2 = model_basic(inputs, is_train=True)\n        except Exception as e:\n            print(e)\n        outputs2 = model_basic(inputs, is_train=False)\n        self.assertEqual(model_basic.is_train, False)\n\n        self.assertLess(np.max(np.abs(outputs1.numpy() - outputs2.numpy())), 1e-7)\n\n        # test layer node\n        self.assertEqual(len(model_basic.all_layers[-1]._nodes), 0)\n        self.assertEqual(model_basic.all_layers[-2]._nodes_fixed, True)\n\n        # test release_memory\n        try:\n            model_basic.release_memory()\n        except Exception as e:\n            print(e)\n\n    def test_static_basic(self):\n        print(\'-\' * 20, \'test_static_basic\', \'-\' * 20)\n        model_basic = basic_static_model()\n\n        # test empty model before calling\n        self.assertEqual(model_basic.is_train, None)\n        self.assertEqual(model_basic._all_weights, None)\n        self.assertIsNotNone(model_basic._inputs)\n        self.assertIsNotNone(model_basic._outputs)\n        self.assertEqual(model_basic._model_layer, None)\n        self.assertIsNotNone(model_basic._all_layers)\n        self.assertIsNotNone(model_basic._nodes_fixed)\n\n        # test layer and weights access\n        all_layers = model_basic.all_layers\n        self.assertEqual(len(model_basic.all_layers), 8)\n        self.assertEqual(model_basic._all_weights, None)\n\n        self.assertIsNotNone(model_basic.all_weights)\n        print([w.name for w in model_basic.all_weights])\n\n        # test model mode\n        model_basic.train()\n        self.assertEqual(model_basic.is_train, True)\n        model_basic.eval()\n        self.assertEqual(model_basic.is_train, False)\n        model_basic.test()\n        self.assertEqual(model_basic.is_train, False)\n        model_basic.infer()\n        self.assertEqual(model_basic.is_train, False)\n\n        # test as_layer\n        self.assertIsInstance(model_basic.as_layer(), tl.layers.Layer)\n        self.assertIsNotNone(model_basic._model_layer)\n\n        # test print\n        try:\n            print(model_basic)\n        except Exception as e:\n            print(e)\n\n        # test forwarding\n        inputs = np.random.normal(size=[2, 24, 24, 3]).astype(np.float32)\n        outputs1 = model_basic(inputs)\n        self.assertEqual(model_basic._nodes_fixed, True)\n        self.assertEqual(model_basic.is_train, False)\n\n        try:\n            outputs2 = model_basic(inputs, is_train=True)\n        except Exception as e:\n            print(e)\n        outputs2 = model_basic(inputs, is_train=False)\n        self.assertEqual(model_basic.is_train, False)\n\n        self.assertLess(np.max(np.abs(outputs1.numpy() - outputs2.numpy())), 1e-7)\n\n        # test layer node\n        self.assertEqual(len(model_basic.all_layers[-1]._nodes), 1)\n        self.assertEqual(model_basic.all_layers[-2]._nodes_fixed, True)\n\n        # test release_memory\n        try:\n            model_basic.release_memory()\n        except Exception as e:\n            print(e)\n\n    def test_deprecated_function(self):\n        print(\'-\' * 20, \'test_deprecated_function\', \'-\' * 20)\n        model = basic_dynamic_model()\n\n        try:\n            model.print_all_layers()\n        except Exception as e:\n            print(e)\n\n        try:\n            model.count_params()\n        except Exception as e:\n            print(e)\n\n        try:\n            model.print_params()\n        except Exception as e:\n            print(e)\n\n        try:\n            model.all_params()\n        except Exception as e:\n            print(e)\n\n        try:\n            model.all_drop()\n        except Exception as e:\n            print(e)\n\n    def test_exceptions(self):\n        print(\'-\' * 20, \'test exceptions\', \'-\' * 20)\n        np_arr = np.random.normal(size=[4, 784]).astype(np.float32)\n        tf_tensor = tf.random.normal(shape=[4, 784])\n        ni = Input(shape=[4, 784])\n\n        try:\n            model = Model(inputs=[], outputs=[])\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n        try:\n            model = Model(inputs=np_arr, outputs=np_arr + 1)\n        except Exception as e:\n            self.assertIsInstance(e, TypeError)\n            print(e)\n\n        try:\n            model = Model(inputs=[np_arr], outputs=[np_arr + 1])\n        except Exception as e:\n            self.assertIsInstance(e, TypeError)\n            print(e)\n\n        try:\n            model = Model(inputs=[tf_tensor], outputs=[tf_tensor + 1])\n        except Exception as e:\n            self.assertIsInstance(e, TypeError)\n            print(e)\n\n        try:\n            model = Model(inputs=tf_tensor, outputs=[tf_tensor + 1])\n        except Exception as e:\n            self.assertIsInstance(e, TypeError)\n            print(e)\n\n        try:\n            model = Model(inputs=ni, outputs=[tf_tensor + 1])\n        except Exception as e:\n            self.assertIsInstance(e, TypeError)\n            print(e)\n\n        try:\n\n            class ill_model(Model):\n\n                def __init__(self):\n                    super(ill_model, self).__init__()\n                    self.dense2 = Dense(10, act=None)\n\n                def forward(self, x):\n                    x = self.dense2(x)\n                    return x\n\n            model = ill_model()\n            weights = model.all_weights\n        except Exception as e:\n            self.assertIsInstance(e, AttributeError)\n            print(e)\n\n        try:\n            ni = Input([4, 784])\n            nn = Dense(10)(ni)\n            model = Model(inputs=ni, outputs=nn)\n            outputs = model(np_arr)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n        try:\n            ni = Input([4, 784])\n            model = Model(inputs=ni, outputs=ni)\n            model.save_weights(\'./empty_model.h5\')\n        except Exception as e:\n            print(e)\n\n        try:\n            ni = Input([4, 784])\n            nn = Dense(10)(ni)\n            model = Model(inputs=ni, outputs=nn)\n            model._outputs = None\n            outputs = model(np_arr, is_train=True)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n    def test_list_inputs_outputs(self):\n        print(\'-\' * 20, \'test_list_inputs_outputs\', \'-\' * 20)\n        ni_1 = Input(shape=[4, 16])\n        ni_2 = Input(shape=[4, 32])\n        a_1 = Dense(80)(ni_1)\n        b_1 = Dense(160)(ni_2)\n        concat = Concat()([a_1, b_1])\n        a_2 = Dense(10)(concat)\n        b_2 = Dense(20)(concat)\n\n        model = Model(inputs=[ni_1, ni_2], outputs=[a_2, b_2])\n\n        model.train()\n        np_arr1 = np.random.normal(size=[4, 16]).astype(np.float32)\n        np_arr2 = np.random.normal(size=[4, 32]).astype(np.float32)\n\n        try:\n            outputs = model(np_arr1)\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n        try:\n            outputs = model([np_arr1])\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n            print(e)\n\n        out_a, out_b = model([np_arr1, np_arr2])\n        self.assertEqual(out_a.shape, [4, 10])\n        self.assertEqual(out_b.shape, [4, 20])\n\n    def test_special_case(self):\n        print(\'-\' * 20, \'test_special_case\', \'-\' * 20)\n\n        class my_model(Model):\n\n            def __init__(self):\n                super(my_model, self).__init__()\n                self.dense = Dense(64, in_channels=3)\n                self.vgg = tl.models.vgg16()\n\n            def forward(self, x):\n                return x\n\n        model = my_model()\n        weights = model.all_weights\n        self.assertGreater(len(weights), 2)\n        print(len(weights))\n\n    def test_get_layer(self):\n        print(\'-\' * 20, \'test_get_layer\', \'-\' * 20)\n        model_basic = basic_dynamic_model()\n        self.assertIsInstance(model_basic.get_layer(\'conv2\'), tl.layers.Conv2d)\n        try:\n            model_basic.get_layer(\'abc\')\n        except Exception as e:\n            print(e)\n\n        try:\n            model_basic.get_layer(index=99)\n        except Exception as e:\n            print(e)\n\n        model_basic = basic_static_model()\n        self.assertIsInstance(model_basic.get_layer(\'conv2\'), tl.layers.Conv2d)\n        self.assertIsInstance(model_basic.get_layer(index=2), tl.layers.MaxPool2d)\n        print([w.name for w in model_basic.get_layer(index=-1).all_weights])\n        try:\n            model_basic.get_layer(\'abc\')\n        except Exception as e:\n            print(e)\n\n        try:\n            model_basic.get_layer(index=99)\n        except Exception as e:\n            print(e)\n\n    def test_model_weights_copy(self):\n        print(\'-\' * 20, \'test_model_weights_copy\', \'-\' * 20)\n        model_basic = basic_static_model()\n        model_weights = model_basic.trainable_weights\n        ori_len = len(model_weights)\n        model_weights.append(np.arange(5))\n        new_len = len(model_weights)\n        self.assertEqual(new_len - 1, ori_len)\n\n    def test_inchannels_exception(self):\n        print(\'-\' * 20, \'test_inchannels_exception\', \'-\' * 20)\n\n        class my_model(Model):\n\n            def __init__(self):\n                super(my_model, self).__init__()\n                self.dense = Dense(64)\n                self.vgg = tl.models.vgg16()\n\n            def forward(self, x):\n                return x\n\n        try:\n            M = my_model()\n        except Exception as e:\n            self.assertIsInstance(e, AttributeError)\n            print(e)\n\n\nif __name__ == \'__main__\':\n\n    unittest.main()\n'"
tests/models/test_model_save.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef basic_static_model(include_top=True):\n    ni = Input((None, 24, 24, 3))\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=""conv1"")(ni)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(nn)\n\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=""conv2"")(nn)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(nn)\n\n    nn = Flatten(name=\'flatten\')(nn)\n    nn = Dense(100, act=None, name=""dense1"")(nn)\n    if include_top is True:\n        nn = Dense(10, act=None, name=""dense2"")(nn)\n    M = Model(inputs=ni, outputs=nn)\n    return M\n\n\nclass basic_dynamic_model(Model):\n\n    def __init__(self, include_top=True):\n        super(basic_dynamic_model, self).__init__()\n        self.include_top = include_top\n        self.conv1 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=3, name=""conv1"")\n        self.pool1 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')\n\n        self.conv2 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=16, name=""conv2"")\n        self.pool2 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')\n\n        self.flatten = Flatten(name=\'flatten\')\n        self.dense1 = Dense(100, act=None, in_channels=576, name=""dense1"")\n        if include_top is True:\n            self.dense2 = Dense(10, act=None, in_channels=100, name=""dense2"")\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.flatten(x)\n        x = self.dense1(x)\n        if self.include_top:\n            x = self.dense2(x)\n        return x\n\n\nclass Nested_VGG(Model):\n\n    def __init__(self):\n        super(Nested_VGG, self).__init__()\n        self.vgg1 = tl.models.vgg16()\n        self.vgg2 = tl.models.vgg16()\n\n    def forward(self, x):\n        pass\n\n\nclass Model_Save_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.static_basic = basic_static_model()\n        cls.dynamic_basic = basic_dynamic_model()\n        cls.static_basic_skip = basic_static_model(include_top=False)\n        cls.dynamic_basic_skip = basic_dynamic_model(include_top=False)\n\n        print([l.name for l in cls.dynamic_basic.all_layers])\n        print([l.name for l in cls.dynamic_basic_skip.all_layers])\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def normal_save(self, model_basic):\n        # Default save\n        model_basic.save_weights(\'./model_basic.none\')\n\n        # hdf5\n        print(\'testing hdf5 saving...\')\n        modify_val = np.zeros_like(model_basic.all_weights[-2].numpy())\n        ori_val = model_basic.all_weights[-2].numpy()\n        model_basic.save_weights(""./model_basic.h5"")\n        model_basic.all_weights[-2].assign(modify_val)\n        model_basic.load_weights(""./model_basic.h5"")\n        self.assertLess(np.max(np.abs(ori_val - model_basic.all_weights[-2].numpy())), 1e-7)\n\n        model_basic.all_weights[-2].assign(modify_val)\n        model_basic.load_weights(""./model_basic.h5"", format=""hdf5"")\n        self.assertLess(np.max(np.abs(ori_val - model_basic.all_weights[-2].numpy())), 1e-7)\n\n        model_basic.all_weights[-2].assign(modify_val)\n        model_basic.load_weights(""./model_basic.h5"", format=""hdf5"", in_order=False)\n        self.assertLess(np.max(np.abs(ori_val - model_basic.all_weights[-2].numpy())), 1e-7)\n\n        # npz\n        print(\'testing npz saving...\')\n        model_basic.save_weights(""./model_basic.npz"", format=\'npz\')\n        model_basic.all_weights[-2].assign(modify_val)\n        model_basic.load_weights(""./model_basic.npz"")\n\n        model_basic.all_weights[-2].assign(modify_val)\n        model_basic.load_weights(""./model_basic.npz"", format=\'npz\')\n        model_basic.save_weights(""./model_basic.npz"")\n        self.assertLess(np.max(np.abs(ori_val - model_basic.all_weights[-2].numpy())), 1e-7)\n\n        # npz_dict\n        print(\'testing npz_dict saving...\')\n        model_basic.save_weights(""./model_basic.npz"", format=\'npz_dict\')\n        model_basic.all_weights[-2].assign(modify_val)\n        model_basic.load_weights(""./model_basic.npz"", format=\'npz_dict\')\n        self.assertLess(np.max(np.abs(ori_val - model_basic.all_weights[-2].numpy())), 1e-7)\n\n        # ckpt\n        try:\n            model_basic.save_weights(\'./model_basic.ckpt\', format=\'ckpt\')\n        except Exception as e:\n            self.assertIsInstance(e, NotImplementedError)\n\n        # other cases\n        try:\n            model_basic.save_weights(\'./model_basic.xyz\', format=\'xyz\')\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n        try:\n            model_basic.load_weights(\'./model_basic.xyz\', format=\'xyz\')\n        except Exception as e:\n            self.assertIsInstance(e, FileNotFoundError)\n        try:\n            model_basic.load_weights(\'./model_basic.h5\', format=\'xyz\')\n        except Exception as e:\n            self.assertIsInstance(e, ValueError)\n\n    def test_normal_save(self):\n        print(\'-\' * 20, \'test save weights\', \'-\' * 20)\n\n        self.normal_save(self.static_basic)\n        self.normal_save(self.dynamic_basic)\n\n        print(\'testing save dynamic and load static...\')\n        try:\n            self.dynamic_basic.save_weights(""./model_basic.h5"")\n            self.static_basic.load_weights(""./model_basic.h5"", in_order=False)\n        except Exception as e:\n            print(e)\n\n    def test_skip(self):\n        print(\'-\' * 20, \'test skip save/load\', \'-\' * 20)\n\n        print(""testing dynamic skip load..."")\n        self.dynamic_basic.save_weights(""./model_basic.h5"")\n        ori_weights = self.dynamic_basic_skip.all_weights\n        ori_val = ori_weights[1].numpy()\n        modify_val = np.zeros_like(ori_val)\n        self.dynamic_basic_skip.all_weights[1].assign(modify_val)\n        self.dynamic_basic_skip.load_weights(""./model_basic.h5"", skip=True)\n        self.assertLess(np.max(np.abs(ori_val - self.dynamic_basic_skip.all_weights[1].numpy())), 1e-7)\n\n        try:\n            self.dynamic_basic_skip.load_weights(""./model_basic.h5"", in_order=False, skip=False)\n        except Exception as e:\n            print(e)\n\n        print(""testing static skip load..."")\n        self.static_basic.save_weights(""./model_basic.h5"")\n        ori_weights = self.static_basic_skip.all_weights\n        ori_val = ori_weights[1].numpy()\n        modify_val = np.zeros_like(ori_val)\n        self.static_basic_skip.all_weights[1].assign(modify_val)\n        self.static_basic_skip.load_weights(""./model_basic.h5"", skip=True)\n        self.assertLess(np.max(np.abs(ori_val - self.static_basic_skip.all_weights[1].numpy())), 1e-7)\n\n        try:\n            self.static_basic_skip.load_weights(""./model_basic.h5"", in_order=False, skip=False)\n        except Exception as e:\n            print(e)\n\n    def test_nested_vgg(self):\n        print(\'-\' * 20, \'test nested vgg\', \'-\' * 20)\n        nested_vgg = Nested_VGG()\n        print([l.name for l in nested_vgg.all_layers])\n        nested_vgg.save_weights(""nested_vgg.h5"")\n\n        # modify vgg1 weight val\n        tar_weight1 = nested_vgg.vgg1.layers[0].all_weights[0]\n        print(tar_weight1.name)\n        ori_val1 = tar_weight1.numpy()\n        modify_val1 = np.zeros_like(ori_val1)\n        tar_weight1.assign(modify_val1)\n        # modify vgg2 weight val\n        tar_weight2 = nested_vgg.vgg2.layers[1].all_weights[0]\n        print(tar_weight2.name)\n        ori_val2 = tar_weight2.numpy()\n        modify_val2 = np.zeros_like(ori_val2)\n        tar_weight2.assign(modify_val2)\n\n        nested_vgg.load_weights(""nested_vgg.h5"")\n\n        self.assertLess(np.max(np.abs(ori_val1 - tar_weight1.numpy())), 1e-7)\n        self.assertLess(np.max(np.abs(ori_val2 - tar_weight2.numpy())), 1e-7)\n\n    def test_double_nested_vgg(self):\n        print(\'-\' * 20, \'test_double_nested_vgg\', \'-\' * 20)\n\n        class mymodel(Model):\n\n            def __init__(self):\n                super(mymodel, self).__init__()\n                self.inner = Nested_VGG()\n                self.list = LayerList(\n                    [\n                        tl.layers.Dense(n_units=4, in_channels=10, name=\'dense1\'),\n                        tl.layers.Dense(n_units=3, in_channels=4, name=\'dense2\')\n                    ]\n                )\n\n            def forward(self, *inputs, **kwargs):\n                pass\n\n        net = mymodel()\n        net.save_weights(""double_nested.h5"")\n        print([x.name for x in net.all_layers])\n\n        # modify vgg1 weight val\n        tar_weight1 = net.inner.vgg1.layers[0].all_weights[0]\n        ori_val1 = tar_weight1.numpy()\n        modify_val1 = np.zeros_like(ori_val1)\n        tar_weight1.assign(modify_val1)\n        # modify vgg2 weight val\n        tar_weight2 = net.inner.vgg2.layers[1].all_weights[0]\n        ori_val2 = tar_weight2.numpy()\n        modify_val2 = np.zeros_like(ori_val2)\n        tar_weight2.assign(modify_val2)\n\n        net.load_weights(""double_nested.h5"")\n        self.assertLess(np.max(np.abs(ori_val1 - tar_weight1.numpy())), 1e-7)\n        self.assertLess(np.max(np.abs(ori_val2 - tar_weight2.numpy())), 1e-7)\n\n    def test_layerlist(self):\n        print(\'-\' * 20, \'test_layerlist\', \'-\' * 20)\n\n        # simple modellayer\n        ni = tl.layers.Input([10, 4])\n        nn = tl.layers.Dense(n_units=3, name=\'dense1\')(ni)\n        modellayer = tl.models.Model(inputs=ni, outputs=nn, name=\'modellayer\').as_layer()\n\n        # nested layerlist with modellayer\n        inputs = tl.layers.Input([10, 5])\n        layer1 = tl.layers.LayerList([tl.layers.Dense(n_units=4, name=\'dense1\'), modellayer])(inputs)\n        model = tl.models.Model(inputs=inputs, outputs=layer1, name=\'layerlistmodel\')\n\n        model.save_weights(""layerlist.h5"")\n        tar_weight = model.get_layer(index=-1)[0].all_weights[0]\n        print(tar_weight.name)\n        ori_val = tar_weight.numpy()\n        modify_val = np.zeros_like(ori_val)\n        tar_weight.assign(modify_val)\n\n        model.load_weights(""layerlist.h5"")\n        self.assertLess(np.max(np.abs(ori_val - tar_weight.numpy())), 1e-7)\n\n    def test_exceptions(self):\n        print(\'-\' * 20, \'test_exceptions\', \'-\' * 20)\n        try:\n            ni = Input([4, 784])\n            model = Model(inputs=ni, outputs=ni)\n            model.save_weights(\'./empty_model.h5\')\n        except Exception as e:\n            print(e)\n\n\nif __name__ == \'__main__\':\n\n    unittest.main()\n'"
tests/models/test_model_save_graph.py,29,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom tensorlayer.models import *\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef RemoveDateInConfig(config):\n    config[""version_info""][""save_date""] = None\n    return config\n\n\ndef basic_static_model():\n    ni = Input((None, 24, 24, 3))\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=""conv1"")(ni)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')(nn)\n\n    nn = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, name=""conv2"")(nn)\n    nn = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')(nn)\n\n    nn = Flatten(name=\'flatten\')(nn)\n    nn = Dense(100, act=None, name=""dense1"")(nn)\n    M = Model(inputs=ni, outputs=nn)\n    return M\n\n\nclass Model_Save_and_Load_without_weights(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing save_graph, load_graph, without weights #####"")\n\n    def test_save(self):\n        M1 = basic_static_model()\n        print(""Model config = \\n"", M1.config)\n        print(""Model = \\n"", M1)\n        M1.save(filepath=""basic_model_without_weights.hdf5"", save_weights=False)\n        M2 = Model.load(filepath=""basic_model_without_weights.hdf5"", load_weights=False)\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual(M1_config, M2_config)\n\n\ndef get_model(inputs_shape):\n    ni = Input(inputs_shape)\n    nn = Dropout(keep=0.8)(ni)\n    nn = Dense(n_units=800, act=tf.nn.relu,\n               in_channels=784)(nn)  # in_channels is optional in this case as it can be inferred by the previous layer\n    nn = Dropout(keep=0.8)(nn)\n    nn = Dense(n_units=800, act=tf.nn.relu,\n               in_channels=800)(nn)  # in_channels is optional in this case as it can be inferred by the previous layer\n    nn = Dropout(keep=0.8)(nn)\n    nn = Dense(n_units=10, act=tf.nn.relu,\n               in_channels=800)(nn)  # in_channels is optional in this case as it can be inferred by the previous layer\n    M = Model(inputs=ni, outputs=nn)\n    return M\n\n\nclass Model_Save_with_weights(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing save_graph, after training, with weights #####"")\n\n    def test_save(self):\n        tl.logging.set_verbosity(tl.logging.DEBUG)\n        X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n        MLP = get_model([None, 784])\n        print(MLP)\n        n_epoch = 3\n        batch_size = 500\n        train_weights = MLP.trainable_weights\n        optimizer = tf.optimizers.Adam(lr=0.0001)\n\n        for epoch in range(n_epoch):  ## iterate the dataset n_epoch times\n            print(""epoch = "", epoch)\n\n            for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n                MLP.train()  # enable dropout\n\n                with tf.GradientTape() as tape:\n                    ## compute outputs\n                    _logits = MLP(X_batch)  # alternatively, you can use MLP(x, is_train=True) and remove MLP.train()\n                    ## compute loss and update model\n                    _loss = tl.cost.cross_entropy(_logits, y_batch, name=\'train_loss\')\n\n                grad = tape.gradient(_loss, train_weights)\n                optimizer.apply_gradients(zip(grad, train_weights))\n\n        MLP.eval()\n\n        val_loss, val_acc, n_iter = 0, 0, 0\n        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\n            _logits = MLP(X_batch)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n            n_iter += 1\n        print(""   val loss: {}"".format(val_loss / n_iter))\n        print(""   val acc:  {}"".format(val_acc / n_iter))\n\n        MLP.save(""MLP.hdf5"")\n\n\nclass Model_Load_with_weights_and_train(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing load_graph, after training, with weights, and train again #####"")\n\n    def test_save(self):\n        MLP = Model.load(""MLP.hdf5"", )\n\n        MLP.eval()\n\n        n_epoch = 3\n        batch_size = 500\n        train_weights = MLP.trainable_weights\n        optimizer = tf.optimizers.Adam(lr=0.0001)\n        X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n        val_loss, val_acc, n_iter = 0, 0, 0\n        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):\n            _logits = MLP(X_batch)  # is_train=False, disable dropout\n            val_loss += tl.cost.cross_entropy(_logits, y_batch, name=\'eval_loss\')\n            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))\n            n_iter += 1\n        print(""   val loss: {}"".format(val_loss / n_iter))\n        print(""   val acc:  {}"".format(val_acc / n_iter))\n        assert val_acc > 0.7\n\n        for epoch in range(n_epoch):  ## iterate the dataset n_epoch times\n            print(""epoch = "", epoch)\n\n            for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n                MLP.train()  # enable dropout\n\n                with tf.GradientTape() as tape:\n                    ## compute outputs\n                    _logits = MLP(X_batch)  # alternatively, you can use MLP(x, is_train=True) and remove MLP.train()\n                    ## compute loss and update model\n                    _loss = tl.cost.cross_entropy(_logits, y_batch, name=\'train_loss\')\n\n                grad = tape.gradient(_loss, train_weights)\n                optimizer.apply_gradients(zip(grad, train_weights))\n\n        MLP.save(""MLP.hdf5"")\n\n\ndef create_base_network(input_shape):\n    \'\'\'Base network to be shared (eq. to feature extraction).\n    \'\'\'\n    input = Input(shape=input_shape)\n    x = Flatten()(input)\n    x = Dense(128, act=tf.nn.relu)(x)\n    x = Dropout(0.9)(x)\n    x = Dense(128, act=tf.nn.relu)(x)\n    x = Dropout(0.9)(x)\n    x = Dense(128, act=tf.nn.relu)(x)\n    return Model(input, x)\n\n\ndef get_siamese_network(input_shape):\n    """"""Create siamese network with shared base network as layer\n    """"""\n    base_layer = create_base_network(input_shape).as_layer()\n\n    ni_1 = Input(input_shape)\n    ni_2 = Input(input_shape)\n    nn_1 = base_layer(ni_1)\n    nn_2 = base_layer(ni_2)\n    return Model(inputs=[ni_1, ni_2], outputs=[nn_1, nn_2])\n\n\nclass Reuse_ModelLayer_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing save_graph, load_graph, including ModelLayer and reuse #####"")\n\n    def test_save(self):\n        input_shape = (None, 784)\n        M1 = get_siamese_network(input_shape)\n        print(""Model config = \\n"", M1.config)\n        print(""Model = \\n"", M1)\n        M1.save(filepath=""siamese.hdf5"", save_weights=False)\n        M2 = Model.load(filepath=""siamese.hdf5"", load_weights=False)\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual(M1_config, M2_config)\n\n\nclass Vgg_LayerList_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing save_graph, load_graph, including LayerList #####"")\n\n    def test_save(self):\n        M1 = tl.models.vgg16(mode=\'static\')\n        print(""Model config = \\n"", M1.config)\n        print(""Model = \\n"", M1)\n        M1.save(filepath=""vgg.hdf5"", save_weights=False)\n        M2 = Model.load(filepath=""vgg.hdf5"", load_weights=False)\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual(M1_config, M2_config)\n\n\nclass List_inputs_outputs_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing model with list inputs and outputs #####"")\n\n    def test_list_inputs_outputs(self):\n        ni_1 = Input(shape=[4, 16])\n        ni_2 = Input(shape=[4, 32])\n        a_1 = Dense(80)(ni_1)\n        b_1 = Dense(160)(ni_2)\n        concat = Concat()([a_1, b_1])\n        a_2 = Dense(10)(concat)\n        b_2 = Dense(20)(concat)\n\n        M1 = Model(inputs=[ni_1, ni_2], outputs=[a_2, b_2])\n        print(""Model config = \\n"", M1.config)\n        print(""Model = \\n"", M1)\n        M1.save(filepath=""list.hdf5"", save_weights=False)\n        M2 = Model.load(filepath=""list.hdf5"", load_weights=False)\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual(M1_config, M2_config)\n\n\nclass Lambda_layer_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing lambda layer #####"")\n\n    def test_lambda_layer_no_para_no_args(self):\n        x = tl.layers.Input([8, 3], name=\'input\')\n        y = tl.layers.Lambda(lambda x: 2 * x, name=\'lambda\')(x)\n        M1 = tl.models.Model(x, y)\n        M1.save(""lambda_no_para_no_args.hdf5"")\n        M2 = tl.models.Model.load(""lambda_no_para_no_args.hdf5"")\n        print(M1)\n        print(M2)\n        M1.eval()\n        M2.eval()\n        npInput = np.zeros((8, 3)) + 3\n        output1 = M1(npInput).numpy()\n        output2 = M1(npInput).numpy()\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual((output1 == output2).all(), True)\n        self.assertEqual(M1_config, M2_config)\n\n    def test_lambda_layer_no_para_with_args(self):\n\n        def customize_func(x, foo=42):  # x is the inputs, foo is an argument\n            return foo * x\n\n        x = tl.layers.Input([8, 3], name=\'input\')\n        y = tl.layers.Lambda(customize_func, fn_args={\'foo\': 3}, name=\'lambda\')(x)\n        M1 = tl.models.Model(x, y)\n        M1.save(""lambda_no_para_with_args.hdf5"")\n        M2 = tl.models.Model.load(""lambda_no_para_with_args.hdf5"")\n        print(M1)\n        print(M2)\n        M1.eval()\n        M2.eval()\n        npInput = np.zeros((8, 3)) + 3\n        output1 = M1(npInput).numpy()\n        output2 = M2(npInput).numpy()\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual((output1 == output2).all(), True)\n        self.assertEqual((output1 == (np.zeros((8, 3)) + 9)).all(), True)\n        self.assertEqual(M1_config, M2_config)\n\n    def test_lambda_layer_keras_model(self):\n        input_shape = [100, 5]\n        in_2 = tl.layers.Input(input_shape, name=\'input\')\n        layers = [\n            tf.keras.layers.Dense(10, activation=tf.nn.relu),\n            tf.keras.layers.Dense(5, activation=tf.nn.sigmoid),\n            tf.keras.layers.Dense(1, activation=tf.nn.relu)\n        ]\n        perceptron = tf.keras.Sequential(layers)\n        # in order to compile keras model and get trainable_variables of the keras model\n        _ = perceptron(np.random.random(input_shape).astype(np.float32))\n        plambdalayer = tl.layers.Lambda(perceptron, perceptron.trainable_variables)(in_2)\n        M2 = tl.models.Model(inputs=in_2, outputs=plambdalayer)\n\n        M2.save(\'M2_keras.hdf5\')\n        M4 = Model.load(\'M2_keras.hdf5\')\n\n        M2.eval()\n        M4.eval()\n        npInput = np.zeros(input_shape) + 3\n        output2 = M2(npInput).numpy()\n        output4 = M4(npInput).numpy()\n\n        M2_config = RemoveDateInConfig(M2.config)\n        M4_config = RemoveDateInConfig(M4.config)\n\n        self.assertEqual((output2 == output4).all(), True)\n        self.assertEqual(M2_config, M4_config)\n\n        ori_weights = M4.all_weights\n        ori_val = ori_weights[1].numpy()\n        modify_val = np.zeros_like(ori_val) + 10\n        M4.all_weights[1].assign(modify_val)\n        M4 = Model.load(\'M2_keras.hdf5\')\n\n        self.assertLess(np.max(np.abs(ori_val - M4.all_weights[1].numpy())), 1e-7)\n\n    def test_lambda_layer_keras_layer(self):\n        input_shape = [100, 5]\n        in_1 = tl.layers.Input(input_shape, name=\'input\')\n        denselayer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n        # in order to compile keras model and get trainable_variables of the keras model\n        _ = denselayer(np.random.random(input_shape).astype(np.float32))\n        dlambdalayer = tl.layers.Lambda(denselayer, denselayer.trainable_variables)(in_1)\n        M1 = tl.models.Model(inputs=in_1, outputs=dlambdalayer)\n\n        M1.save(\'M1_keras.hdf5\')\n        M3 = Model.load(\'M1_keras.hdf5\')\n\n        M1.eval()\n        M3.eval()\n        npInput = np.zeros(input_shape) + 3\n        output1 = M1(npInput).numpy()\n        output3 = M3(npInput).numpy()\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M3_config = RemoveDateInConfig(M3.config)\n\n        self.assertEqual((output1 == output3).all(), True)\n        self.assertEqual(M1_config, M3_config)\n\n        ori_weights = M3.all_weights\n        ori_val = ori_weights[1].numpy()\n        modify_val = np.zeros_like(ori_val) + 10\n        M3.all_weights[1].assign(modify_val)\n        M3 = Model.load(\'M1_keras.hdf5\')\n\n        self.assertLess(np.max(np.abs(ori_val - M3.all_weights[1].numpy())), 1e-7)\n\n\nclass ElementWise_lambda_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing elementwise lambda layer #####"")\n\n    def test_elementwise_no_para_with_args(self):\n        # z = mean + noise * tf.exp(std * 0.5) + foo\n        def func(noise, mean, std, foo=42):\n            return mean + noise * tf.exp(std * 0.5) + foo\n\n        noise = tl.layers.Input([100, 1])\n        mean = tl.layers.Input([100, 1])\n        std = tl.layers.Input([100, 1])\n        out = tl.layers.ElementwiseLambda(fn=func, fn_args={\'foo\': 84}, name=\'elementwiselambda\')([noise, mean, std])\n        M1 = Model(inputs=[noise, mean, std], outputs=out)\n        M1.save(""elementwise_npwa.hdf5"")\n        M2 = Model.load(""elementwise_npwa.hdf5"")\n\n        M1.eval()\n        M2.eval()\n        ipt = [np.zeros((100, 1)) + 11, np.zeros((100, 1)) + 21, np.zeros((100, 1)) + 31]\n        output1 = M1(ipt).numpy()\n        output2 = M2(ipt).numpy()\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual((output1 == output2).all(), True)\n        self.assertEqual(M1_config, M2_config)\n\n    def test_elementwise_no_para_no_args(self):\n        # z = mean + noise * tf.exp(std * 0.5) + foo\n        def func(noise, mean, std, foo=42):\n            return mean + noise * tf.exp(std * 0.5) + foo\n\n        noise = tl.layers.Input([100, 1])\n        mean = tl.layers.Input([100, 1])\n        std = tl.layers.Input([100, 1])\n        out = tl.layers.ElementwiseLambda(fn=func, name=\'elementwiselambda\')([noise, mean, std])\n        M1 = Model(inputs=[noise, mean, std], outputs=out)\n        M1.save(""elementwise_npna.hdf5"")\n        M2 = Model.load(""elementwise_npna.hdf5"")\n\n        M1.eval()\n        M2.eval()\n        ipt = [np.zeros((100, 1)) + 11, np.zeros((100, 1)) + 21, np.zeros((100, 1)) + 31]\n        output1 = M1(ipt).numpy()\n        output2 = M2(ipt).numpy()\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual((output1 == output2).all(), True)\n        self.assertEqual(M1_config, M2_config)\n\n    def test_elementwise_lambda_func(self):\n        # z = mean + noise * tf.exp(std * 0.5)\n        noise = tl.layers.Input([100, 1])\n        mean = tl.layers.Input([100, 1])\n        std = tl.layers.Input([100, 1])\n        out = tl.layers.ElementwiseLambda(fn=lambda x, y, z: x + y * tf.exp(z * 0.5),\n                                          name=\'elementwiselambda\')([noise, mean, std])\n        M1 = Model(inputs=[noise, mean, std], outputs=out)\n        M1.save(""elementwise_lambda.hdf5"")\n        M2 = Model.load(""elementwise_lambda.hdf5"")\n\n        M1.eval()\n        M2.eval()\n        ipt = [\n            (np.zeros((100, 1)) + 11).astype(np.float32), (np.zeros((100, 1)) + 21).astype(np.float32),\n            (np.zeros((100, 1)) + 31).astype(np.float32)\n        ]\n        output1 = M1(ipt).numpy()\n        output2 = M2(ipt).numpy()\n\n        M1_config = RemoveDateInConfig(M1.config)\n        M2_config = RemoveDateInConfig(M2.config)\n\n        self.assertEqual((output1 == output2).all(), True)\n        self.assertEqual(M1_config, M2_config)\n\n    # # ElementwiseLambda does not support keras layer/model func yet\n    # def test_elementwise_keras_model(self):\n    #     kerasinput1 = tf.keras.layers.Input(shape=(100, ))\n    #     kerasinput2 = tf.keras.layers.Input(shape=(100, ))\n    #     kerasconcate = tf.keras.layers.concatenate(inputs=[kerasinput1, kerasinput2])\n    #     kerasmodel = tf.keras.models.Model(inputs=[kerasinput1, kerasinput2], outputs=kerasconcate)\n    #     _ = kerasmodel([np.random.random([100,]).astype(np.float32), np.random.random([100,]).astype(np.float32)])\n    #\n    #     input1 = tl.layers.Input([100, 1])\n    #     input2 = tl.layers.Input([100, 1])\n    #     out = tl.layers.ElementwiseLambda(fn=kerasmodel, name=\'elementwiselambda\')([input1, input2])\n    #     M1 = Model(inputs=[input1, input2], outputs=out)\n    #     M1.save(""elementwise_keras_model.hdf5"")\n    #     M2 = Model.load(""elementwise_keras_model.hdf5"")\n    #\n    #     M1.eval()\n    #     M2.eval()\n    #     ipt = [np.zeros((100, 1)) + 11, np.zeros((100, 1)) + 21, np.zeros((100, 1)) + 31]\n    #     output1 = M1(ipt).numpy()\n    #     output2 = M2(ipt).numpy()\n    #\n    #     M1_config = RemoveDateInConfig(M1.config)\n    #     M2_config = RemoveDateInConfig(M2.config)\n    #\n    #     self.assertEqual((output1 == output2).all(), True)\n    #     self.assertEqual(M1_config, M2_config)\n\n\nclass basic_dynamic_model(Model):\n\n    def __init__(self):\n        super(basic_dynamic_model, self).__init__()\n        self.conv1 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=3, name=""conv1"")\n        self.pool1 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool1\')\n\n        self.conv2 = Conv2d(16, (5, 5), (1, 1), padding=\'SAME\', act=tf.nn.relu, in_channels=16, name=""conv2"")\n        self.pool2 = MaxPool2d((3, 3), (2, 2), padding=\'SAME\', name=\'pool2\')\n\n        self.flatten = Flatten(name=\'flatten\')\n        self.dense1 = Dense(100, act=None, in_channels=576, name=""dense1"")\n        self.dense2 = Dense(10, act=None, in_channels=100, name=""dense2"")\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.flatten(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        return x\n\n\nclass Dynamic_config_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing exception in dynamic mode #####"")\n\n    def test_dynamic_config(self):\n        M1 = basic_dynamic_model()\n        print(M1.config)\n        for layer in M1.all_layers:\n            print(layer.config)\n\n\nclass Exception_test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        print(""##### begin testing exception in dynamic mode #####"")\n\n    def test_exception(self):\n        M1 = basic_dynamic_model()\n        try:\n            M1.save(""dynamic.hdf5"", save_weights=False)\n        except Exception as e:\n            self.assertIsInstance(e, RuntimeError)\n            print(e)\n\n        M2 = basic_static_model()\n        M2.save(""basic_static_mode.hdf5"", save_weights=False)\n        try:\n            M3 = Model.load(""basic_static_mode.hdf5"")\n        except Exception as e:\n            self.assertIsInstance(e, RuntimeError)\n            print(e)\n\n\nif __name__ == \'__main__\':\n\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/models/test_seq2seq_model.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm\n\nimport tensorlayer as tl\nfrom tensorlayer.cost import cross_entropy_seq\nfrom tensorlayer.models.seq2seq import Seq2seq\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Model_SEQ2SEQ_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.batch_size = 16\n\n        cls.vocab_size = 20\n        cls.embedding_size = 32\n        cls.dec_seq_length = 5\n        cls.trainX = np.random.randint(20, size=(50, 6))\n        cls.trainY = np.random.randint(20, size=(50, cls.dec_seq_length + 1))\n        cls.trainY[:, 0] = 0  # start_token == 0\n\n        # Parameters\n        cls.src_len = len(cls.trainX)\n        cls.tgt_len = len(cls.trainY)\n\n        assert cls.src_len == cls.tgt_len\n\n        cls.num_epochs = 100\n        cls.n_step = cls.src_len // cls.batch_size\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_basic_simpleSeq2Seq(self):\n        model_ = Seq2seq(\n            decoder_seq_length=5,\n            cell_enc=tf.keras.layers.GRUCell,\n            cell_dec=tf.keras.layers.GRUCell,\n            n_layer=3,\n            n_units=128,\n            embedding_layer=tl.layers.Embedding(vocabulary_size=self.vocab_size, embedding_size=self.embedding_size),\n        )\n\n        optimizer = tf.optimizers.Adam(learning_rate=0.001)\n\n        for epoch in range(self.num_epochs):\n            model_.train()\n            trainX, trainY = shuffle(self.trainX, self.trainY)\n            total_loss, n_iter = 0, 0\n            for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=self.batch_size,\n                                                    shuffle=False), total=self.n_step,\n                             desc=\'Epoch[{}/{}]\'.format(epoch + 1, self.num_epochs), leave=False):\n\n                dec_seq = Y[:, :-1]\n                target_seq = Y[:, 1:]\n\n                with tf.GradientTape() as tape:\n                    ## compute outputs\n                    output = model_(inputs=[X, dec_seq])\n\n                    output = tf.reshape(output, [-1, self.vocab_size])\n\n                    loss = cross_entropy_seq(logits=output, target_seqs=target_seq)\n\n                    grad = tape.gradient(loss, model_.all_weights)\n                    optimizer.apply_gradients(zip(grad, model_.all_weights))\n\n                total_loss += loss\n                n_iter += 1\n\n            model_.eval()\n            test_sample = trainX[0:2, :].tolist()\n\n            top_n = 1\n            for i in range(top_n):\n                prediction = model_([test_sample], seq_length=self.dec_seq_length, start_token=0, top_n=1)\n                print(""Prediction: >>>>>  "", prediction, ""\\n Target: >>>>>  "", trainY[0:2, 1:], ""\\n\\n"")\n\n            # printing average loss after every epoch\n            print(\'Epoch [{}/{}]: loss {:.4f}\'.format(epoch + 1, self.num_epochs, total_loss / n_iter))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/models/test_seq2seq_with_attention.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm\n\nimport tensorlayer as tl\nfrom tensorlayer.cost import cross_entropy_seq\nfrom tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Model_SEQ2SEQ_WITH_ATTENTION_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.batch_size = 16\n\n        cls.vocab_size = 200\n        cls.embedding_size = 32\n        cls.dec_seq_length = 5\n        cls.pure_time = np.linspace(-1, 1, 21)\n        cls.pure_signal = 100 * np.sin(cls.pure_time)\n        cls.dataset = np.zeros((100, 21))\n        for i in range(100):\n            noise = 100 + 1 * np.random.normal(0, 1, cls.pure_signal.shape)\n            cls.dataset[i] = cls.pure_signal + noise\n        cls.dataset = cls.dataset.astype(int)\n        np.random.shuffle(cls.dataset)\n        cls.trainX = cls.dataset[:80, :15]\n        cls.trainY = cls.dataset[:80, 15:]\n        cls.testX = cls.dataset[80:, :15]\n        cls.testY = cls.dataset[80:, 15:]\n\n        cls.trainY[:, 0] = 0  # start_token == 0\n        cls.testY[:, 0] = 0  # start_token == 0\n\n        # Parameters\n        cls.src_len = len(cls.trainX)\n        cls.tgt_len = len(cls.trainY)\n\n        assert cls.src_len == cls.tgt_len\n\n        cls.num_epochs = 500\n        cls.n_step = cls.src_len // cls.batch_size\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def test_basic_simpleSeq2Seq(self):\n\n        model_ = Seq2seqLuongAttention(\n            hidden_size=128, cell=tf.keras.layers.SimpleRNNCell,\n            embedding_layer=tl.layers.Embedding(vocabulary_size=self.vocab_size,\n                                                embedding_size=self.embedding_size), method=\'dot\'\n        )\n        optimizer = tf.optimizers.Adam(learning_rate=0.001)\n\n        for epoch in range(self.num_epochs):\n            model_.train()\n            trainX, trainY = shuffle(self.trainX, self.trainY)\n            total_loss, n_iter = 0, 0\n            for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=self.batch_size,\n                                                    shuffle=False), total=self.n_step,\n                             desc=\'Epoch[{}/{}]\'.format(epoch + 1, self.num_epochs), leave=False):\n                dec_seq = Y[:, :-1]\n                target_seq = Y[:, 1:]\n\n                with tf.GradientTape() as tape:\n                    ## compute outputs\n                    output = model_(inputs=[X, dec_seq])\n                    # print(output)\n                    output = tf.reshape(output, [-1, self.vocab_size])\n\n                    loss = cross_entropy_seq(logits=output, target_seqs=target_seq)\n                    grad = tape.gradient(loss, model_.trainable_weights)\n                    optimizer.apply_gradients(zip(grad, model_.trainable_weights))\n\n                total_loss += loss\n                n_iter += 1\n\n            model_.eval()\n            test_sample = self.testX[:5, :].tolist()  # Can\'t capture the sequence.\n            top_n = 1\n            for i in range(top_n):\n                prediction = model_([test_sample], seq_length=self.dec_seq_length, sos=0)\n                print(""Prediction: >>>>>  "", prediction, ""\\n Target: >>>>>  "", self.testY[:5, 1:], ""\\n\\n"")\n\n            # printing average loss after every epoch\n            print(\'Epoch [{}/{}]: loss {:.4f}\'.format(epoch + 1, self.num_epochs, total_loss / n_iter))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/pending/__init__.py,0,b''
tests/pending/test_array_ops.py,6,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Array_Op_Alphas_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        b1 = tl.alphas([4, 3, 2, 1], 0.5431)\n        b2 = tl.alphas([4, 3, 2], 5)\n        b3 = tl.alphas([1, 2, 3, 4], -5)\n        b4 = tl.alphas([2, 3, 4], True)\n\n        with tf.Session() as sess:\n            cls._b1, cls._b2, cls._b3, cls._b4 = sess.run([b1, b2, b3, b4])\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_b1(self):\n        self.assertEqual(self._b1.shape, (4, 3, 2, 1))\n\n        b1 = np.array(\n            [\n                [\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                ], [\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                ], [\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                ], [\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                    [\n                        [0.5431],\n                        [0.5431],\n                    ],\n                ]\n            ]\n        )\n\n        np.array_equal(self._b1, b1)\n\n    def test_b2(self):\n        self.assertEqual(self._b2.shape, (4, 3, 2))\n\n        b2 = np.array(\n            [\n                [\n                    [\n                        5,\n                        5,\n                    ],\n                    [\n                        5,\n                        5,\n                    ],\n                    [\n                        5,\n                        5,\n                    ],\n                ], [\n                    [\n                        5,\n                        5,\n                    ],\n                    [\n                        5,\n                        5,\n                    ],\n                    [\n                        5,\n                        5,\n                    ],\n                ], [\n                    [\n                        5,\n                        5,\n                    ],\n                    [\n                        5,\n                        5,\n                    ],\n                    [\n                        5,\n                        5,\n                    ],\n                ], [\n                    [\n                        5,\n                        5,\n                    ],\n                    [\n                        5,\n                        5,\n                    ],\n                    [\n                        5,\n                        5,\n                    ],\n                ]\n            ]\n        )\n\n        np.array_equal(self._b2, b2)\n\n    def test_b3(self):\n        self.assertEqual(self._b3.shape, (1, 2, 3, 4))\n\n        b3 = np.array(\n            [\n                [\n                    [[-5, -5, -5, -5], [-5, -5, -5, -5], [-5, -5, -5, -5]],\n                    [[-5, -5, -5, -5], [-5, -5, -5, -5], [-5, -5, -5, -5]],\n                ]\n            ]\n        )\n\n        np.array_equal(self._b3, b3)\n\n    def test_b4(self):\n        self.assertEqual(self._b4.shape, (2, 3, 4))\n\n        b4 = np.array(\n            [\n                [[True, True, True, True], [True, True, True, True], [True, True, True, True]],\n                [[True, True, True, True], [True, True, True, True], [True, True, True, True]],\n            ]\n        )\n\n        np.array_equal(self._b4, b4)\n\n\nclass Array_Op_Alphas_Like_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        a = tf.constant([[[4, 5, 6], [1, 2, 3]], [[4, 5, 6], [1, 2, 3]]])\n\n        b1 = tl.alphas_like(a, 0.5431)\n        b2 = tl.alphas_like(a, 5)\n        b3 = tl.alphas_like(a, -5)\n        b4 = tl.alphas_like(a, True)\n\n        with tf.Session() as sess:\n            cls._b1, cls._b2, cls._b3, cls._b4 = sess.run([b1, b2, b3, b4])\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_b1(self):\n        self.assertEqual(self._b1.shape, (2, 2, 3))\n\n        b1 = np.array(\n            [\n                [[0.5431, 0.5431, 0.5431], [0.5431, 0.5431, 0.5431]],\n                [[0.5431, 0.5431, 0.5431], [0.5431, 0.5431, 0.5431]]\n            ]\n        )\n\n        np.array_equal(self._b1, b1)\n\n    def test_b2(self):\n        self.assertEqual(self._b2.shape, (2, 2, 3))\n\n        b2 = np.array([[[5, 5, 5], [5, 5, 5]], [[5, 5, 5], [5, 5, 5]]])\n\n        np.array_equal(self._b2, b2)\n\n    def test_b3(self):\n        self.assertEqual(self._b3.shape, (2, 2, 3))\n\n        b3 = np.array([[[-5, -5, -5], [-5, -5, -5]], [[-5, -5, -5], [-5, -5, -5]]])\n\n        np.array_equal(self._b3, b3)\n\n    def test_b4(self):\n        self.assertEqual(self._b4.shape, (2, 2, 3))\n\n        b4 = np.array([[[True, True, True], [True, True, True]], [[True, True, True], [True, True, True]]])\n\n        np.array_equal(self._b4, b4)\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_decorators.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.decorators import private_method\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Pooling_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        class MyClass(object):\n\n            @private_method\n            def _private_func(self):\n                tl.logging.debug(""I am private"")\n\n            def public_func(self):\n                tl.logging.debug(""I am public and calling now the private func"")\n                self._private_func()\n\n        cls.my_object = MyClass()\n\n    def test_call_from_public_method(self):\n        with self.assertNotRaises(RuntimeError):\n            self.my_object.public_func()\n\n    def test_call_private_method(self):\n        with self.assertRaises(RuntimeError):\n            self.my_object._private_func()\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_documentation.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nfrom sphinx.application import Sphinx\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass DocTest(unittest.TestCase):\n    source_dir = u\'docs/\'\n    config_dir = u\'docs/\'\n    output_dir = u\'docs/test_build\'\n    doctree_dir = u\'docs/test_build/doctrees\'\n\n    all_files = True\n\n    @classmethod\n    def setUpClass(cls):\n        os.environ[""SPHINXBUILD""] = ""1""\n\n    def test_html_documentation(self):\n        app = Sphinx(\n            self.source_dir,\n            self.config_dir,\n            self.output_dir,\n            self.doctree_dir,\n            buildername=\'html\',\n            warningiserror=True,\n        )\n        app.build(force_all=self.all_files)\n        # TODO: additional checks here if needed\n\n    def test_text_documentation(self):\n        # The same, but with different buildername\n        app = Sphinx(\n            self.source_dir,\n            self.config_dir,\n            self.output_dir,\n            self.doctree_dir,\n            buildername=\'text\',\n            warningiserror=False,\n        )\n        app.build(force_all=self.all_files)\n        # TODO:  additional checks if needed\n\n    def tearDown(self):\n        # TODO: clean up the output directory\n        pass\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/pending/test_layers_basic.py,3,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Basic_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        x = tf.placeholder(tf.float32, [None, 100])\n\n        n = tl.layers.InputLayer(x, name='in')\n        n = tl.layers.DenseLayer(n, n_units=80, name='d1')\n        n = tl.layers.DenseLayer(n, n_units=80, name='d2')\n\n        n.print_layers()\n        n.print_params(False)\n\n        n2 = n[:, :30]\n        n2.print_layers()\n\n        cls.n_params = n.count_params()\n        cls.all_layers = n.all_layers\n        cls.all_params = n.all_params\n        cls.shape_n = n.outputs.get_shape().as_list()\n\n        cls.shape_n2 = n2.outputs.get_shape().as_list()\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_n_params(self):\n        self.assertEqual(self.n_params, 14560)\n\n    def test_shape_n(self):\n        self.assertEqual(self.shape_n[-1], 80)\n\n    def test_all_layers(self):\n        self.assertEqual(len(self.all_layers), 3)\n\n    def test_all_params(self):\n        self.assertEqual(len(self.all_params), 4)\n\n    def test_shape_n2(self):\n        self.assertEqual(self.shape_n2[-1], 30)\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_layers_flow_control.py,8,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Flow_Control_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        x = tf.placeholder(tf.float32, shape=(None, 784), name='x')\n\n        # define the network\n        net_in = tl.layers.InputLayer(x, name='in')\n        net_in = tl.layers.DropoutLayer(net_in, keep=0.8, name='in/drop')\n        # net 0\n        net_0 = tl.layers.DenseLayer(net_in, n_units=800, act=tf.nn.relu, name='net0/relu1')\n        net_0 = tl.layers.DropoutLayer(net_0, keep=0.5, name='net0/drop1')\n        net_0 = tl.layers.DenseLayer(net_0, n_units=800, act=tf.nn.relu, name='net0/relu2')\n        # net 1\n        net_1 = tl.layers.DenseLayer(net_in, n_units=800, act=tf.nn.relu, name='net1/relu1')\n        net_1 = tl.layers.DropoutLayer(net_1, keep=0.8, name='net1/drop1')\n        net_1 = tl.layers.DenseLayer(net_1, n_units=800, act=tf.nn.relu, name='net1/relu2')\n        net_1 = tl.layers.DropoutLayer(net_1, keep=0.8, name='net1/drop2')\n        net_1 = tl.layers.DenseLayer(net_1, n_units=800, act=tf.nn.relu, name='net1/relu3')\n        # multiplexer\n        net_mux = tl.layers.MultiplexerLayer(layers=[net_0, net_1], name='mux')\n        network = tl.layers.ReshapeLayer(net_mux, shape=(-1, 800), name='reshape')\n        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n        # output layer\n        network = tl.layers.DenseLayer(network, n_units=10, name='output')\n\n        network.print_layers()\n        network.print_params(False)\n\n        cls.net_shape = network.outputs.get_shape().as_list()\n        cls.net_layers = network.all_layers\n        cls.net_params = network.all_params\n        cls.net_all_drop = network.all_drop\n        cls.net_n_params = network.count_params()\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_net_shape(self):\n        self.assertEqual(self.net_shape[-1], 10)\n\n    def test_net_layers(self):\n        self.assertEqual(len(self.net_layers), 14)\n\n    def test_net_params(self):\n        self.assertEqual(len(self.net_params), 12)\n\n    def test_net_all_drop(self):\n        self.assertEqual(len(self.net_all_drop), 5)\n\n    def test_net_n_params(self):\n        self.assertEqual(self.net_n_params, 3186410)\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_layers_importer.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import (inception_v3, inception_v3_arg_scope)\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\nslim = tf.contrib.slim\nkeras = tf.keras\n\n\nclass Layer_Importer_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.net_in = dict()\n\n        # ============================= #\n        #          LambdaLayer\n        # ============================= #\n        x = tf.placeholder(tf.float32, shape=[None, 784])\n        cls.net_in[""lambda""] = tl.layers.InputLayer(x, name=\'input\')\n\n        # ============================= #\n        #          SlimNetsLayer\n        # ============================= #\n        x = tf.placeholder(tf.float32, shape=[None, 299, 299, 3])\n        cls.net_in[""slim""] = tl.layers.InputLayer(x, name=\'input_layer\')\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_lambda_layer(self):\n\n        def keras_block(x):\n            x = keras.layers.Dropout(0.8)(x)\n            x = keras.layers.Dense(100, activation=\'relu\')(x)\n            # x = keras.layers.Dropout(0.8)(x)\n            # x = keras.layers.Dense(100, activation=\'relu\')(x)\n            x = keras.layers.Dropout(0.5)(x)\n            logits = keras.layers.Dense(10, activation=\'linear\')(x)\n\n            return logits\n\n        with self.assertNotRaises(Exception):\n            tl.layers.LambdaLayer(self.net_in[""lambda""], fn=keras_block, name=\'keras\')\n\n    def test_slim_layer(self):\n\n        with self.assertNotRaises(Exception):\n            with slim.arg_scope(inception_v3_arg_scope()):\n                # Alternatively, you should implement inception_v3 without TensorLayer as follow.\n                # logits, end_points = inception_v3(X, num_classes=1001,\n                #                                   is_training=False)\n                tl.layers.SlimNetsLayer(\n                    self.net_in[""slim""],\n                    slim_layer=inception_v3,\n                    slim_args={\n                        \'num_classes\': 1001,\n                        \'is_training\': False,\n                        #  \'dropout_keep_prob\' : 0.8,       # for training\n                        #  \'min_depth\' : 16,\n                        #  \'depth_multiplier\' : 1.0,\n                        #  \'prediction_fn\' : slim.softmax,\n                        #  \'spatial_squeeze\' : True,\n                        #  \'reuse\' : None,\n                        #  \'scope\' : \'InceptionV3\'\n                    },\n                    name=\'InceptionV3\'  # <-- the name should be the same with the ckpt model\n                )\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_layers_normalization.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef model(x, is_train=True, reuse=False):\n    with tf.variable_scope(""model"", reuse=reuse):\n        n = tl.layers.InputLayer(x, name=\'in\')\n        n = tl.layers.Conv2d(n, n_filter=80, name=\'conv2d_1\')\n        n = tl.layers.BatchNormLayer(n, is_train=is_train, name=\'norm_batch\')\n        n = tl.layers.Conv2d(n, n_filter=80, name=\'conv2d_2\')\n        n = tl.layers.LocalResponseNormLayer(n, name=\'norm_local\')\n        n = tl.layers.LayerNormLayer(n, reuse=reuse, name=\'norm_layer\')\n        n = tl.layers.InstanceNormLayer(n, name=\'norm_instance\')\n        # n = tl.layers.GroupNormLayer(n, groups=40, name=\'groupnorm\')\n        n.outputs = tf.reshape(n.outputs, [-1, 80, 100, 100])\n        n = tl.layers.GroupNormLayer(n, groups=40, data_format=\'channels_first\', name=\'groupnorm\')\n        n.outputs = tf.reshape(n.outputs, [-1, 100, 100, 80])\n        n = tl.layers.SwitchNormLayer(n, name=\'switchnorm\')\n        n = tl.layers.QuanConv2dWithBN(n, n_filter=3, is_train=is_train, name=\'quan_cnn_with_bn\')\n        n = tl.layers.FlattenLayer(n, name=\'flatten\')\n        n = tl.layers.QuanDenseLayerWithBN(n, n_units=10, name=\'quan_dense_with_bn\')\n    return n\n\n\nclass Layer_Normalization_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        x = tf.placeholder(tf.float32, [None, 100, 100, 3])\n\n        net_train = model(x, is_train=True, reuse=False)\n        net_eval = model(x, is_train=False, reuse=True)\n\n        net_train.print_layers()\n        net_train.print_params(False)\n\n        cls.data = dict()\n        cls.data[""train_network""] = dict()\n        cls.data[""eval_network""] = dict()\n\n        cls.data[""train_network""][""layers""] = net_train.all_layers\n        cls.data[""eval_network""][""layers""] = net_eval.all_layers\n\n        cls.data[""train_network""][""params""] = net_train.all_params\n\n        cls.data[""train_network""][""n_params""] = net_train.count_params()\n\n        print(net_train.count_params())\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_all_layers(self):\n        self.assertEqual(len(self.data[""train_network""][""layers""]), 12)\n        self.assertEqual(len(self.data[""eval_network""][""layers""]), 12)\n\n    def test_all_params(self):\n        self.assertEqual(len(self.data[""train_network""][""params""]), 28)\n\n    def test_n_params(self):\n        self.assertEqual(self.data[""train_network""][""n_params""], 363098)\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_layers_padding.py,5,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Padding_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        ## 1D\n        x = tf.placeholder(tf.float32, (None, 100, 1))\n        n = tl.layers.InputLayer(x)\n\n        n1 = tl.layers.ZeroPad1d(n, padding=1)\n        n2 = tl.layers.ZeroPad1d(n, padding=(2, 3))\n\n        n1.print_layers()\n        n2.print_layers()\n\n        cls.n1_shape = n1.outputs.get_shape().as_list()\n        cls.n2_shape = n2.outputs.get_shape().as_list()\n\n        ## 2D\n        x = tf.placeholder(tf.float32, (None, 100, 100, 3))\n        n = tl.layers.InputLayer(x)\n\n        n3 = tl.layers.ZeroPad2d(n, padding=2)\n        n4 = tl.layers.ZeroPad2d(n, padding=(2, 3))\n        n5 = tl.layers.ZeroPad2d(n, padding=((3, 3), (4, 4)))\n\n        n3.print_layers()\n        n4.print_layers()\n        n5.print_layers()\n\n        cls.n3_shape = n3.outputs.get_shape().as_list()\n        cls.n4_shape = n4.outputs.get_shape().as_list()\n        cls.n5_shape = n5.outputs.get_shape().as_list()\n\n        ## 3D\n        x = tf.placeholder(tf.float32, (None, 100, 100, 100, 3))\n        n = tl.layers.InputLayer(x)\n\n        n6 = tl.layers.ZeroPad3d(n, padding=2)\n        n7 = tl.layers.ZeroPad3d(n, padding=(2, 3, 4))\n        n8 = tl.layers.ZeroPad3d(n, padding=((3, 3), (4, 4), (5, 5)))\n\n        n6.print_layers()\n        n7.print_layers()\n        n8.print_layers()\n\n        cls.n6_shape = n6.outputs.get_shape().as_list()\n        cls.n7_shape = n7.outputs.get_shape().as_list()\n        cls.n8_shape = n8.outputs.get_shape().as_list()\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_n1_shape(self):\n        self.assertEqual(self.n1_shape[1:], [102, 1])\n\n    def test_n2_shape(self):\n        self.assertEqual(self.n2_shape[1:], [105, 1])\n\n    def test_n3_shape(self):\n        self.assertEqual(self.n3_shape[1:], [104, 104, 3])\n\n    def test_n4_shape(self):\n        self.assertEqual(self.n4_shape[1:], [104, 106, 3])\n\n    def test_n5_shape(self):\n        self.assertEqual(self.n5_shape[1:], [106, 108, 3])\n\n    def test_n6_shape(self):\n        self.assertEqual(self.n6_shape[1:], [104, 104, 104, 3])\n\n    def test_n7_shape(self):\n        self.assertEqual(self.n7_shape[1:], [104, 106, 108, 3])\n\n    def test_n8_shape(self):\n        self.assertEqual(self.n8_shape[1:], [106, 108, 110, 3])\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_layers_spatial_transformer.py,10,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef model(x, is_train, reuse):\n    with tf.variable_scope(""STN"", reuse=reuse):\n        nin = tl.layers.InputLayer(x, name=\'in\')\n        ## 1. Localisation network\n        # use MLP as the localisation net\n        nt = tl.layers.FlattenLayer(nin, name=\'flatten\')\n        nt = tl.layers.DenseLayer(nt, n_units=20, act=tf.nn.tanh, name=\'dense1\')\n        nt = tl.layers.DropoutLayer(nt, keep=0.8, is_fix=True, is_train=is_train, name=\'drop1\')\n        # you can also use CNN instead for MLP as the localisation net\n        # nt = Conv2d(nin, 16, (3, 3), (2, 2), act=tf.nn.relu, padding=\'SAME\', name=\'tc1\')\n        # nt = Conv2d(nt, 8, (3, 3), (2, 2), act=tf.nn.relu, padding=\'SAME\', name=\'tc2\')\n        ## 2. Spatial transformer module (sampler)\n        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name=\'spatial\')\n        s = n\n        ## 3. Classifier\n        n = tl.layers.Conv2d(\n            n, n_filter=16, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, padding=\'SAME\', name=\'conv1\'\n        )\n\n        n = tl.layers.Conv2d(\n            n, n_filter=16, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, padding=\'SAME\', name=\'conv2\'\n        )\n        n = tl.layers.FlattenLayer(n, name=\'flatten2\')\n        n = tl.layers.DenseLayer(n, n_units=1024, act=tf.nn.relu, name=\'out1\')\n        n = tl.layers.DenseLayer(n, n_units=10, name=\'out2\')\n    return n, s\n\n\nclass Layer_Spatial_Transformer_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n\n        net, s = model(cls.x, is_train=True, reuse=False)\n\n        net.print_layers()\n        net.print_params(False)\n\n        cls.s_shape = s.outputs.get_shape().as_list()\n        cls.net_layers = net.all_layers\n        cls.net_params = net.all_params\n        cls.net_n_params = net.count_params()\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_reuse(self):\n\n        with self.assertNotRaises(Exception):\n            _, _ = model(self.x, is_train=True, reuse=True)\n\n    def test_net_shape(self):\n        self.assertEqual(self.s_shape[1:], [40, 40, 1])\n\n    def test_net_layers(self):\n        self.assertEqual(len(self.net_layers), 10)\n\n    def test_net_params(self):\n        self.assertEqual(len(self.net_params), 12)\n\n    def test_net_n_params(self):\n        self.assertEqual(self.net_n_params, 1667980)\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_layers_stack.py,3,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Stack_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        x = tf.placeholder(tf.float32, shape=[None, 30])\n        net_in = tl.layers.InputLayer(x, name='input')\n\n        net_d1 = tl.layers.DenseLayer(net_in, n_units=10, name='dense1')\n        net_d2 = tl.layers.DenseLayer(net_in, n_units=10, name='dense2')\n        net_d3 = tl.layers.DenseLayer(net_in, n_units=10, name='dense3')\n\n        cls.net_stack = tl.layers.StackLayer([net_d1, net_d2, net_d3], axis=1, name='stack')\n\n        cls.net_stack.print_layers()\n        cls.net_stack.print_params(False)\n\n        cls.net_unstack = tl.layers.UnStackLayer(cls.net_stack, axis=1, name='unstack')\n\n        cls.net_unstack.print_layers()\n        cls.net_unstack.print_params(False)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_StackLayer(self):\n        self.assertEqual(self.net_stack.outputs.get_shape().as_list()[-1], 10)\n        self.assertEqual(len(self.net_stack.all_layers), 5)\n        self.assertEqual(len(self.net_stack.all_params), 6)\n        self.assertEqual(self.net_stack.count_params(), 930)\n\n    def test_UnStackLayer(self):\n\n        for n in self.net_unstack.outputs:\n            shape = n.outputs.get_shape().as_list()\n\n            self.assertEqual(shape[-1], 10)\n            self.assertEqual(len(n.all_layers), 5)\n            self.assertEqual(len(n.all_params), 6)\n            self.assertEqual(n.count_params(), 930)\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_layers_super_resolution.py,4,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Super_Resolution_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        t_signal = tf.placeholder('float32', [10, 100, 4], name='x')\n        n = tl.layers.InputLayer(t_signal, name='in')\n        n = tl.layers.Conv1d(n, n_filter=32, filter_size=3, stride=1, padding='SAME', name='conv1d')\n        net1 = tl.layers.SubpixelConv1d(n, scale=2, name='subpixel')\n\n        net1.print_layers()\n        net1.print_params(False)\n\n        cls.net1_shape = net1.outputs.get_shape().as_list()\n        cls.net1_layers = net1.all_layers\n        cls.net1_params = net1.all_params\n        cls.net1_n_params = net1.count_params()\n\n        ## 2D\n        x = tf.placeholder('float32', [10, 100, 100, 3], name='x')\n        n = tl.layers.InputLayer(x, name='in')\n        n = tl.layers.Conv2d(n, n_filter=32, filter_size=(3, 2), strides=(1, 1), padding='SAME', name='conv2d')\n        net2 = tl.layers.SubpixelConv2d(n, scale=2, name='subpixel2d')\n\n        net2.print_layers()\n        net2.print_params(False)\n\n        cls.net2_shape = net2.outputs.get_shape().as_list()\n        cls.net2_layers = net2.all_layers\n        cls.net2_params = net2.all_params\n        cls.net2_n_params = net2.count_params()\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_net1_shape(self):\n        self.assertEqual(self.net1_shape, [10, 200, 16])\n        self.assertEqual(len(self.net1_layers), 3)\n        self.assertEqual(len(self.net1_params), 2)\n        self.assertEqual(self.net1_n_params, 416)\n\n    def test_net2_shape(self):\n        self.assertEqual(self.net2_shape, [10, 200, 200, 8])\n        self.assertEqual(len(self.net2_layers), 3)\n        self.assertEqual(len(self.net2_params), 2)\n        self.assertEqual(self.net2_n_params, 608)\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_layers_time_distributed.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef model(x, is_train=True, reuse=False, name_scope=""env1""):\n    with tf.variable_scope(name_scope, reuse=reuse):\n        net = tl.layers.InputLayer(x, name=\'input\')\n        net = tl.layers.TimeDistributedLayer(\n            net, layer_class=tl.layers.DenseLayer, args={\n                \'n_units\': 50,\n                \'name\': \'dense\'\n            }, name=\'time_dense\'\n        )\n    return net\n\n\nclass Layer_Time_Distributed_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        batch_size = 32\n        timestep = 20\n        input_dim = 100\n\n        cls.x = tf.placeholder(dtype=tf.float32, shape=[batch_size, timestep, input_dim], name=""encode_seqs"")\n        net = model(cls.x, is_train=True, reuse=False)\n\n        cls.net_shape = net.outputs.get_shape().as_list()\n        cls.n_params = net.count_params()\n        net.print_params(False)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_net_shape(self):\n        self.assertEqual(self.net_shape, [32, 20, 50])\n\n    def test_net_n_params(self):\n        self.assertEqual(self.n_params, 5050)\n\n    def test_reuse(self):\n\n        with self.assertNotRaises(Exception):\n            model(self.x, is_train=True, reuse=False, name_scope=""env2"")\n            model(self.x, is_train=False, reuse=True, name_scope=""env2"")\n\n        with self.assertRaises(Exception):\n            model(self.x, is_train=True, reuse=False)  # Already defined model with the same var_scope\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_logging.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass TL_Logger_Test(CustomTestCase):\n\n    def test_debug(self):\n        with self.assertNotRaises(Exception):\n            tl.logging.debug(""This is a debug message"")\n\n    def test_error(self):\n        with self.assertNotRaises(Exception):\n            tl.logging.error(""This is an error message"")\n\n    def test_fatal(self):\n        with self.assertNotRaises(Exception):\n            tl.logging.fatal(""This is a fatal error message"")\n\n    def test_info(self):\n        with self.assertNotRaises(Exception):\n            tl.logging.info(""This is an information message"")\n\n    def test_warn(self):\n        with self.assertNotRaises(Exception):\n            tl.logging.warn(""This is a warning message"")\n\n    def test_set_verbosity(self):\n        with self.assertNotRaises(Exception):\n            tl.logging.set_verbosity(tl.logging.DEBUG)\n            tl.logging.set_verbosity(tl.logging.INFO)\n            tl.logging.set_verbosity(tl.logging.WARN)\n            tl.logging.set_verbosity(tl.logging.ERROR)\n            tl.logging.set_verbosity(tl.logging.FATAL)\n\n    def test_get_verbosity(self):\n        with self.assertNotRaises(Exception):\n            tl.logging.get_verbosity()\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_logging_hyperdash.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport time\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer.logging.contrib import hyperdash as hd\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass TL_Logger_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.apikey = os.getenv(\'HYPERDASH_APIKEY\', ""test_api_key"")\n\n    def test_apikey_unset(self):\n\n        with self.assertRaises(ValueError):\n            hd.HyperDashHandler.reset_apikey()\n            hd.HyperDashHandler.get_apikey()\n\n    def test_apikey_set(self):\n\n        with self.assertNotRaises(ValueError):\n            hd.HyperDashHandler.set_apikey(self.apikey)\n            hd.HyperDashHandler.get_apikey()\n\n    def test_monitor(self):\n\n        with self.assertNotRaises(Exception):\n\n            hd.HyperDashHandler.set_apikey(self.apikey)\n\n            @hd.monitor(""TRAVIS 1 - dogs vs. cats"")\n            def train_dogs_vs_cats(exp=None):\n\n                # Record the value of hyperparameter gamma for this experiment\n                lr = exp.param(""learning rate"", 0.005)\n                tl.logging.debug(""Learning Rate: %f"" % lr)\n\n                for epoch, accuracy in enumerate([10, 30, 50, 70, 80, 90, 95, 100]):\n                    tl.logging.debug(""Epoch %d - Accuracy %d%%"" % (epoch + 1, accuracy))\n\n                    # Record a numerical performance metric\n                    exp.metric(name=""accuracy"", value=accuracy)\n\n                    time.sleep(0.1)\n\n            train_dogs_vs_cats()\n\n    def test_monitor_variant(self):\n\n        with self.assertNotRaises(Exception):\n\n            @hd.monitor(""TRAVIS 2 - dogs vs. cats"", api_key=self.apikey)\n            def train_dogs_vs_cats(exp=None):\n\n                # Record the value of hyperparameter gamma for this experiment\n                lr = exp.param(""learning rate"", 0.005)\n                tl.logging.debug(""Learning Rate: %f"" % lr)\n\n                for epoch, accuracy in enumerate([10, 30, 50, 70, 80, 90, 95, 100]):\n                    tl.logging.debug(""Epoch %d - Accuracy %d%%"" % (epoch + 1, accuracy))\n\n                    # Record a numerical performance metric\n                    exp.metric(name=""accuracy"", value=accuracy)\n\n                    time.sleep(0.1)\n\n            train_dogs_vs_cats()\n\n    def test_Experiment(self):\n\n        hd.HyperDashHandler.set_apikey(self.apikey)\n\n        with self.assertNotRaises(Exception):\n\n            def train_dogs_vs_cats():\n\n                # Create an experiment with a model name, then autostart\n                exp = hd.Experiment(""TRAVIS 3 - dogs vs. cats"")\n\n                # Record the value of hyperparameter gamma for this experiment\n                lr = exp.param(""learning rate"", 0.005)\n                tl.logging.debug(""Learning Rate: %f"" % lr)\n\n                for epoch, accuracy in enumerate([10, 30, 50, 70, 80, 90, 95, 100]):\n                    tl.logging.debug(""Epoch %d - Accuracy %d%%"" % (epoch + 1, accuracy))\n\n                    # Record a numerical performance metric\n                    exp.metric(name=""accuracy"", value=accuracy)\n\n                    time.sleep(0.1)\n\n                # Cleanup and mark that the experiment successfully completed\n                exp.end()\n\n            train_dogs_vs_cats()\n\n    def test_Experiment_variant(self):\n\n        with self.assertNotRaises(Exception):\n\n            def train_dogs_vs_cats():\n\n                # Create an experiment with a model name, then autostart\n                exp = hd.Experiment(""TRAVIS 4 - dogs vs. cats"", api_key=self.apikey)\n\n                # Record the value of hyperparameter gamma for this experiment\n                lr = exp.param(""learning rate"", 0.005)\n                tl.logging.debug(""Learning Rate: %f"" % lr)\n\n                for epoch, accuracy in enumerate([10, 30, 50, 70, 80, 90, 95, 100]):\n                    tl.logging.debug(""Epoch %d - Accuracy %d%%"" % (epoch + 1, accuracy))\n\n                    # Record a numerical performance metric\n                    exp.metric(name=""accuracy"", value=accuracy)\n\n                    time.sleep(0.1)\n\n                # Cleanup and mark that the experiment successfully completed\n                exp.end()\n\n            train_dogs_vs_cats()\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_mnist_simple.py,12,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Simple_MNIST_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        # define placeholders\n        cls.x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n        cls.y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n\n        # define the network\n        network = tl.layers.InputLayer(cls.x, name='input')\n        network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n        network = tl.layers.DenseLayer(network, n_units=100, act=tf.nn.relu, name='relu1')\n        network = tl.layers.DropoutLayer(network, keep=0.8, name='drop2')\n        network = tl.layers.DenseLayer(network, n_units=100, act=tf.nn.relu, name='relu2')\n        network = tl.layers.DropoutLayer(network, keep=0.8, name='drop3')\n\n        # the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\n        # speed up computation, so we use identity here.\n        # see tf.nn.sparse_softmax_cross_entropy_with_logits()\n        cls.network = tl.layers.DenseLayer(network, n_units=10, name='output')\n\n        # define cost function and metric.\n        y = cls.network.outputs\n        cls.cost = tl.cost.cross_entropy(y, cls.y_, name='cost')\n\n        correct_prediction = tf.equal(tf.argmax(y, 1), cls.y_)\n\n        cls.acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        # y_op = tf.argmax(tf.nn.softmax(y), 1)\n\n        # define the optimizer\n        train_params = cls.network.trainable_weights\n        cls.train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cls.cost, var_list=train_params)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_reuse_vgg(self):\n\n        # prepare data\n        X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n\n        # for fashion_MNIST dataset test\n        # X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_fashion_mnist_dataset(shape=(-1, 784))\n\n        with self.assertNotRaises(Exception):\n            with tf.Session() as sess:\n\n                # initialize all variables in the session\n                tl.layers.initialize_global_variables(sess)\n\n                # print network information\n                self.network.print_params()\n                self.network.print_layers()\n\n                # train the network\n                tl.utils.fit(\n                    sess, self.network, self.train_op, self.cost, X_train, y_train, self.x, self.y_, acc=self.acc,\n                    batch_size=500, n_epoch=1, print_freq=1, X_val=X_val, y_val=y_val, eval_train=False\n                )\n\n                # evaluation\n                tl.utils.test(\n                    sess, self.network, self.acc, X_test, y_test, self.x, self.y_, batch_size=None, cost=self.cost\n                )\n\n                # save the network to .npz file\n                tl.files.save_npz(self.network.all_params, name='model.npz')\n                sess.close()\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_models.py,13,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass VGG_Model_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        with tf.Graph().as_default():\n            # - Classify ImageNet classes with VGG16, see `tutorial_models_vgg16.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_models_vgg16.py>__`\n            x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n            # get the whole model\n            vgg1 = tl.models.VGG16(x)\n            # restore pre-trained VGG parameters\n            # sess = tf.InteractiveSession()\n            # vgg.restore_params(sess)\n            # use for inferencing\n            # probs = tf.nn.softmax(vgg1.outputs)\n\n            cls.vgg1_layers = vgg1.all_layers\n            cls.vgg1_params = vgg1.all_params\n\n        with tf.Graph().as_default():\n            # - Extract features with VGG16 and Train a classifier with 100 classes\n            x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n            # get VGG without the last layer\n            vgg2 = tl.models.VGG16(x, end_with=\'fc2_relu\')\n\n            cls.vgg2_layers = vgg2.all_layers\n            cls.vgg2_params = vgg2.all_params\n\n            print(""TYPE:"", type(vgg2))\n\n            # add one more layer\n            _ = tl.layers.DenseLayer(vgg2, n_units=100, name=\'out\')\n            # initialize all parameters\n            # sess = tf.InteractiveSession()\n            # tl.layers.initialize_global_variables(sess)\n            # restore pre-trained VGG parameters\n            # vgg.restore_params(sess)\n            # train your own classifier (only update the last layer)\n\n            cls.vgg2_train_params = tl.layers.get_variables_with_name(\'out\')\n\n        with tf.Graph().as_default() as graph:\n            # - Reuse model\n            x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n            # get VGG without the last layer\n            vgg3 = tl.models.VGG16(x, end_with=\'fc2_relu\')\n            # reuse the parameters of vgg1 with different input\n            # restore pre-trained VGG parameters (as they share parameters, we don\xe2\x80\x99t need to restore vgg2)\n            # sess = tf.InteractiveSession()\n            # vgg1.restore_params(sess)\n\n            cls.vgg3_layers = vgg3.all_layers\n            cls.vgg3_params = vgg3.all_params\n            cls.vgg3_graph = graph\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_vgg1_layers(self):\n        self.assertEqual(len(self.vgg1_layers), 23)\n\n    def test_vgg2_layers(self):\n        self.assertEqual(len(self.vgg2_layers), 22)\n\n    def test_vgg3_layers(self):\n        self.assertEqual(len(self.vgg3_layers), 22)\n\n    def test_vgg1_params(self):\n        self.assertEqual(len(self.vgg1_params), 32)\n\n    def test_vgg2_params(self):\n        self.assertEqual(len(self.vgg2_params), 30)\n\n    def test_vgg3_params(self):\n        self.assertEqual(len(self.vgg3_params), 30)\n\n    def test_vgg2_train_params(self):\n        self.assertEqual(len(self.vgg2_train_params), 2)\n\n    def test_reuse_vgg(self):\n\n        with self.assertNotRaises(Exception):\n            with self.vgg3_graph.as_default():\n                x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n                _ = tl.models.VGG16(x, end_with=\'fc2_relu\', reuse=True)\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_optimizer_amsgrad.py,9,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Layer_Pooling_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n        cls.y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n\n        # define the network\n        cls.network = tl.layers.InputLayer(cls.x, name='input')\n        cls.network = tl.layers.DropoutLayer(cls.network, keep=0.8, name='drop1')\n        cls.network = tl.layers.DenseLayer(cls.network, 800, tf.nn.relu, name='relu1')\n        cls.network = tl.layers.DropoutLayer(cls.network, keep=0.5, name='drop2')\n        cls.network = tl.layers.DenseLayer(cls.network, 800, tf.nn.relu, name='relu2')\n        cls.network = tl.layers.DropoutLayer(cls.network, keep=0.5, name='drop3')\n\n        cls.network = tl.layers.DenseLayer(cls.network, n_units=10, name='output')\n\n        # define cost function and metric.\n        cls.y = cls.network.outputs\n        cls.cost = tl.cost.cross_entropy(cls.y, cls.y_, name='cost')\n\n        correct_prediction = tf.equal(tf.argmax(cls.y, 1), cls.y_)\n\n        cls.acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        # define the optimizer\n        train_params = cls.network.all_params\n        optimizer = tl.optimizers.AMSGrad(learning_rate=1e-4, beta1=0.9, beta2=0.999, epsilon=1e-8)\n        cls.train_op = optimizer.minimize(cls.cost, var_list=train_params)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_training(self):\n\n        with self.assertNotRaises(Exception):\n\n            X_train, y_train, X_val, y_val, _, _ = tl.files.load_mnist_dataset(shape=(-1, 784))\n\n            with tf.Session() as sess:\n                # initialize all variables in the session\n                tl.layers.initialize_global_variables(sess)\n\n                # print network information\n                self.network.print_params()\n                self.network.print_layers()\n\n                # train the network\n                tl.utils.fit(\n                    sess, self.network, self.train_op, self.cost, X_train, y_train, self.x, self.y_, acc=self.acc,\n                    batch_size=500, n_epoch=1, print_freq=1, X_val=X_val, y_val=y_val, eval_train=False\n                )\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_pydocstyle.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nfrom pydocstyle.checker import check, violations\nfrom tests.utils import list_all_py_files\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\nregistry = violations.ErrorRegistry\n\n\ndef lookup_error_params(code):\n    for group in registry.groups:\n        for error_params in group.errors:\n            if error_params.code == code:\n                return error_params\n\n\nclass PyDOC_Style_Test(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.violations = list()\n\n        # TODO: fix all violations to make it empty\n        _disabled_checks = [\n            \'D205\',  # 1 blank line required between summary line and description\n            \'D102\',  # Missing docstring in public method\n            \'D400\',  # First line should end with a period\n            \'D100\',  # Missing docstring in public module\n            \'D107\',  # Missing docstring in __init__\n            \'D103\',  # Missing docstring in public function\n            \'D401\',  # First line should be in imperative mood\n            \'D101\',  # Missing docstring in public class\n            \'D413\',  # Missing blank line after last section\n            \'D105\',  # Missing docstring in magic method\n            \'D104\',  # Missing docstring in public package\n            \'D302\',  # Use u""""""for Unicode docstrings\n\n            # Rules that has conflict with yapf\n            \'D202\',  # No blank lines allowed after function docstring\n        ]\n\n        for filename in list_all_py_files():\n            print(filename)\n            for err in check([filename]):\n                if not err.code in _disabled_checks:\n                    cls.violations.append(err)\n\n    def test_violations(self):\n        if self.violations:\n            counts = dict()\n\n            for err in self.violations:\n                counts[err.code] = counts.get(err.code, 0) + 1\n                print(err)\n\n            for n, code in sorted([(n, code) for code, n in counts.items()], reverse=True):\n                p = lookup_error_params(code)\n                print(\'%s %8d %s\' % (code, n, p.short_desc))\n\n            raise Exception(\'PyDoc Coding Style: %d violations have been found\' % (len(self.violations)))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/pending/test_reuse_mlp.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\n# define the network\ndef mlp(x, is_train=True, reuse=False):\n    with tf.variable_scope(""MLP"", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)  # print warning\n        network = tl.layers.InputLayer(x, name=\'input\')\n        network = tl.layers.DropoutLayer(network, keep=0.8, is_fix=True, is_train=is_train, name=\'drop1\')\n        network = tl.layers.DenseLayer(network, n_units=800, act=tf.nn.relu, name=\'relu1\')\n        network = tl.layers.DropoutLayer(network, keep=0.5, is_fix=True, is_train=is_train, name=\'drop2\')\n        network = tl.layers.DenseLayer(network, n_units=800, act=tf.nn.relu, name=\'relu2\')\n        network = tl.layers.DropoutLayer(network, keep=0.5, is_fix=True, is_train=is_train, name=\'drop3\')\n        network = tl.layers.DenseLayer(network, n_units=10, name=\'output\')\n    return network\n\n\nclass MLP_Reuse_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        # define placeholder\n        cls.x = tf.placeholder(tf.float32, shape=[None, 784], name=\'x\')\n\n        # define inferences\n        mlp(cls.x, is_train=True, reuse=False)\n        mlp(cls.x, is_train=False, reuse=True)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_reuse(self):\n\n        with self.assertRaises(Exception):\n            mlp(self.x, is_train=False, reuse=False)  # Already defined model with the same var_scope\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_tf_layers.py,13,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass Layer_Convolution_1D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        def get_network_1d(inputs, reuse=False):\n\n            with tf.variable_scope(""1D_network"", reuse=reuse):\n                net = tl.layers.InputLayer(inputs)\n\n                net1 = tl.layers.Conv1d(net, name=""Conv1d"")  # 2 params\n                net2 = tl.layers.SeparableConv1d(net1, name=""SeparableConv1d"")  # 3 params\n                net3 = tl.layers.MaxPool1d(net2, (1, ), name=""MaxPool1d"")  # 0 params\n                net4 = tl.layers.MeanPool1d(net3, (1, ), name=""MeanPool1d"")  # 0 params\n\n                # HAO Test\n                net5 = tl.layers.Conv1d(net4, name=""Conv1d1"")  # 2 params\n                net6 = tl.layers.SeparableConv1d(net5, name=""SeparableConv1d1"")  # 3 params\n                net7 = tl.layers.SeparableConv1d(net6, name=""SeparableConv1d2"")  # 3 params\n\n            return [net, net1, net2, net3, net4, net5, net6, net7]\n\n        input_pl_train = tf.placeholder(tf.float32, [None, 32, 3])\n        input_plh_test = tf.placeholder(tf.float32, [None, 32, 3])\n\n        cls.network_1 = get_network_1d(input_pl_train, reuse=False)\n        cls.network_2 = get_network_1d(input_plh_test, reuse=True)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_layer_net0(self):\n        self.assertEqual(len(self.network_1[0].all_params), 0)\n        self.assertEqual(len(self.network_2[0].all_params), 0)\n\n    def test_layer_net1(self):\n        self.assertEqual(len(self.network_1[1].all_params), 2)\n        self.assertEqual(len(self.network_2[1].all_params), 2)\n\n    def test_layer_net2(self):\n        self.assertEqual(len(self.network_1[2].all_params), 5)\n        self.assertEqual(len(self.network_2[2].all_params), 5)\n\n    def test_layer_net3(self):\n        self.assertEqual(len(self.network_1[3].all_params), 5)\n        self.assertEqual(len(self.network_2[3].all_params), 5)\n\n    def test_layer_net4(self):\n        self.assertEqual(len(self.network_1[4].all_params), 5)\n        self.assertEqual(len(self.network_2[4].all_params), 5)\n\n    def test_layer_net5(self):\n        self.assertEqual(len(self.network_1[5].all_params), 7)\n        self.assertEqual(len(self.network_2[5].all_params), 7)\n\n    def test_layer_net6(self):\n        self.assertEqual(len(self.network_1[6].all_params), 10)\n        self.assertEqual(len(self.network_2[6].all_params), 10)\n\n    def test_layer_net7(self):\n        self.assertEqual(len(self.network_1[7].all_params), 13)\n        self.assertEqual(len(self.network_2[7].all_params), 13)\n\n\nclass Layer_Convolution_2D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        def get_network_2d(inputs, reuse=False):\n\n            with tf.variable_scope(""2D_network"", reuse=reuse):\n                net = tl.layers.InputLayer(inputs)\n\n                net1 = tl.layers.Conv2d(net, name=""Conv2d"")  # 2 params\n                net2 = tl.layers.DeConv2d(net1, name=""DeConv2d"")  # 2 params\n                net3 = tl.layers.SeparableConv2d(net2, name=""SeparableConv2d"")  # 3 params\n                net4 = tl.layers.MaxPool2d(net3, (1, 1), name=""MaxPool2d"")  # 0 params\n                net5 = tl.layers.MeanPool2d(net4, (1, 1), name=""MeanPool2d"")  # 0 params\n\n                # HAO Test\n                net6 = tl.layers.Conv2d(net5, name=""Conv2d1"")  # 2 params\n                net7 = tl.layers.DeConv2d(net6, name=""DeConv2d1"")  # 2 params\n                net8 = tl.layers.DeConv2d(net7, name=""DeConv2d2"")  # 2 params\n                net9 = tl.layers.SeparableConv2d(net8, name=""SeparableConv2d1"")  # 3 params\n\n            return [net, net1, net2, net3, net4, net5, net6, net7, net8, net9]\n\n        input_pl_train = tf.placeholder(tf.float32, [None, 32, 32, 3])\n        input_plh_test = tf.placeholder(tf.float32, [None, 32, 32, 3])\n\n        cls.network_1 = get_network_2d(input_pl_train, reuse=False)\n        cls.network_2 = get_network_2d(input_plh_test, reuse=True)\n\n    def test_layer_net0(self):\n        self.assertEqual(len(self.network_1[0].all_params), 0)\n        self.assertEqual(len(self.network_2[0].all_params), 0)\n\n    def test_layer_net1(self):\n        self.assertEqual(len(self.network_1[1].all_params), 2)\n        self.assertEqual(len(self.network_2[1].all_params), 2)\n\n    def test_layer_net2(self):\n        self.assertEqual(len(self.network_1[2].all_params), 4)\n        self.assertEqual(len(self.network_2[2].all_params), 4)\n\n    def test_layer_net3(self):\n        self.assertEqual(len(self.network_1[3].all_params), 7)\n        self.assertEqual(len(self.network_2[3].all_params), 7)\n\n    def test_layer_net4(self):\n        self.assertEqual(len(self.network_1[4].all_params), 7)\n        self.assertEqual(len(self.network_2[4].all_params), 7)\n\n    def test_layer_net5(self):\n        self.assertEqual(len(self.network_1[5].all_params), 7)\n        self.assertEqual(len(self.network_2[5].all_params), 7)\n\n    def test_layer_net6(self):\n        self.assertEqual(len(self.network_1[6].all_params), 9)\n        self.assertEqual(len(self.network_2[6].all_params), 9)\n\n    def test_layer_net7(self):\n        self.assertEqual(len(self.network_1[7].all_params), 11)\n        self.assertEqual(len(self.network_2[7].all_params), 11)\n\n    def test_layer_net8(self):\n        self.assertEqual(len(self.network_1[8].all_params), 13)\n        self.assertEqual(len(self.network_2[8].all_params), 13)\n\n    def test_layer_net9(self):\n        self.assertEqual(len(self.network_1[9].all_params), 16)\n        self.assertEqual(len(self.network_2[9].all_params), 16)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n\nclass Layer_Convolution_3D_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        def get_network_3d(inputs, reuse=False):\n\n            with tf.variable_scope(""3D_network"", reuse=reuse):\n                net = tl.layers.InputLayer(inputs)\n\n                net1 = tl.layers.Conv3dLayer(\n                    net, shape=(2, 2, 2, 3, 32), strides=(1, 2, 2, 2, 1), name=""Conv3dLayer""\n                )  # 2 params\n                net2 = tl.layers.DeConv3d(net1, name=""DeConv3d"")  # 2 params\n                net3 = tl.layers.MaxPool3d(net2, (1, 1, 1), name=""MaxPool3d"")  # 0 params\n                net4 = tl.layers.MeanPool3d(net3, (1, 1, 1), name=""MeanPool3d"")  # 0 params\n\n                # HAO Test\n                net5 = tl.layers.DeConv3d(net4, name=""DeConv3d1"")  # 2 params\n\n                return [net, net1, net2, net3, net4, net5]\n\n        input_pl_train = tf.placeholder(tf.float32, [None, 32, 32, 32, 3])\n        input_plh_test = tf.placeholder(tf.float32, [None, 32, 32, 32, 3])\n\n        cls.network_1 = get_network_3d(input_pl_train, reuse=False)\n        cls.network_2 = get_network_3d(input_plh_test, reuse=True)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_layer_net0(self):\n        self.assertEqual(len(self.network_1[0].all_params), 0)\n        self.assertEqual(len(self.network_2[0].all_params), 0)\n\n    def test_layer_net1(self):\n        self.assertEqual(len(self.network_1[1].all_params), 2)\n        self.assertEqual(len(self.network_2[1].all_params), 2)\n\n    def test_layer_net2(self):\n        self.assertEqual(len(self.network_1[2].all_params), 4)\n        self.assertEqual(len(self.network_2[2].all_params), 4)\n\n    def test_layer_net3(self):\n        self.assertEqual(len(self.network_1[3].all_params), 4)\n        self.assertEqual(len(self.network_2[3].all_params), 4)\n\n    def test_layer_net4(self):\n        self.assertEqual(len(self.network_1[4].all_params), 4)\n        self.assertEqual(len(self.network_2[4].all_params), 4)\n\n    def test_layer_net5(self):\n        self.assertEqual(len(self.network_1[5].all_params), 6)\n        self.assertEqual(len(self.network_2[5].all_params), 6)\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_timeout.py,3,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport time\nimport unittest\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import (CustomTestCase, TimeoutContext, TimeoutError, WindowsError)\nfrom tests.utils.custom_networks import InceptionV4_Network\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\nif os.getenv(""TRAVIS"", None) is not None:\n    NETWORK_CREATION_TIMEOUT = 120  # Seconds before timeout\nelse:\n    NETWORK_CREATION_TIMEOUT = 40  # Seconds before timeout\n\n######################################################################################\n#                                                                                    #\n#                                UNITTEST TIMEOUT                                    #\n#                                                                                    #\n######################################################################################\n\n\nclass Layer_Timeoutt_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        #######################################################################\n        ####  =============    Placeholders Declaration      ============= ####\n        #######################################################################\n\n        cls.input_plh = tf.placeholder(tf.float32, [None, 299, 299, 3], name=\'input_placeholder\')\n\n        #######################################################################\n        ####  =============        Model Declaration         ============= ####\n        #######################################################################\n\n        cls.inception_v4_net = InceptionV4_Network(include_FC_head=True, flatten_output=False)\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_timeout_not_reuse(self):\n\n        with self.assertNotRaises(TimeoutError):\n            try:\n                with TimeoutContext(NETWORK_CREATION_TIMEOUT):\n                    start_time = time.time()\n\n                    _ = self.inception_v4_net(self.input_plh, reuse=False, is_train=False)\n\n                    tl.logging.info(""Seconds Elapsed [Not Reused]: %d"" % int(time.time() - start_time))\n\n            except WindowsError:\n                tl.logging.warning(""This unittest can not run on Windows"")\n\n    def test_timeout_reuse(self):\n\n        with self.assertNotRaises(TimeoutError):\n            try:\n                with TimeoutContext(NETWORK_CREATION_TIMEOUT):\n                    start_time = time.time()\n\n                    _ = self.inception_v4_net(self.input_plh, reuse=True, is_train=False)\n\n                    tl.logging.info(""Seconds Elapsed [Reused Model]: %d"" % int(time.time() - start_time))\n\n            except WindowsError:\n                tl.logging.warning(""This unittest can not run on Windows"")\n\n\nif __name__ == \'__main__\':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n'"
tests/pending/test_utils_predict.py,8,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils import CustomTestCase\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\nclass Util_Predict_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.x1 = tf.placeholder(tf.float32, [None, 5, 5, 3])\n        cls.x2 = tf.placeholder(tf.float32, [8, 5, 5, 3])\n        cls.X1 = np.ones([127, 5, 5, 3])\n        cls.X2 = np.ones([7, 5, 5, 3])\n        cls.batch_size = 8\n\n    @classmethod\n    def tearDownClass(cls):\n        tf.reset_default_graph()\n\n    def test_case1(self):\n        with self.assertNotRaises(Exception):\n            with tf.Session() as sess:\n                n = tl.layers.InputLayer(self.x1)\n                y = n.outputs\n                y_op = tf.nn.softmax(y)\n                tl.utils.predict(sess, n, self.X1, self.x1, y_op, batch_size=self.batch_size)\n                sess.close()\n\n    def test_case2(self):\n        with self.assertRaises(Exception):\n            with tf.Session() as sess:\n                n = tl.layers.InputLayer(self.x2)\n                y = n.outputs\n                y_op = tf.nn.softmax(y)\n                tl.utils.predict(sess, n, self.X2, self.x2, y_op, batch_size=self.batch_size)\n                sess.close()\n\n\nif __name__ == '__main__':\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    tl.logging.set_verbosity(tl.logging.DEBUG)\n\n    unittest.main()\n"""
tests/pending/test_yapf_format.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport unittest\n\nfrom yapf.yapflib.yapf_api import FormatCode\n\nfrom tests.utils import CustomTestCase, list_all_py_files\n\n\ndef _read_utf_8_file(filename):\n    if sys.version_info.major == 2:  ## Python 2 specific\n        with open(filename, \'rb\') as f:\n            return unicode(f.read(), \'utf-8\')\n    else:\n        with open(filename, encoding=\'utf-8\') as f:\n            return f.read()\n\n\nclass YAPF_Style_Test(CustomTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.badly_formatted_files = list()\n        cls.files_2_test = list_all_py_files()\n\n    def test_files_format(self):\n\n        for file in list_all_py_files():\n\n            try:\n\n                print(file)\n                code = _read_utf_8_file(file)\n\n                # https://pypi.python.org/pypi/yapf/0.20.2#example-as-a-module\n                diff, changed = FormatCode(code, filename=file, style_config=\'setup.cfg\', print_diff=True)\n\n                if changed:\n                    print(diff)\n                    self.badly_formatted_files.append(file)\n            except Exception as e:\n                print(""Error while processing file: `%s`\\n"" ""Error: %s"" % (file, str(e)))\n\n        with self.assertNotRaises(Exception):\n\n            str_err = """"\n\n            if self.badly_formatted_files:\n                for filename in self.badly_formatted_files:\n                    str_err += \'yapf -i --style=setup.cfg %s\\n\' % filename\n\n                str_err = ""\\n======================================================================================\\n"" \\\n                          ""Bad Coding Style: %d file(s) need to be formatted, run the following commands to fix: \\n%s"" \\\n                          ""======================================================================================"" % (\n                    len(self.badly_formatted_files), str_err)\n\n                raise Exception(str_err)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/performance_test/__init__.py,0,b''
tests/utils/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom tests.utils.custom_layers import *\nfrom tests.utils.custom_networks import *\nfrom tests.utils.custom_testcase import *\nfrom tests.utils.list_py_files import *\nfrom tests.utils.timeout_utils import *\n'
tests/utils/custom_testcase.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport unittest\nfrom contextlib import contextmanager\n\n__all__ = [\n    'CustomTestCase',\n]\n\n\nclass CustomTestCase(unittest.TestCase):\n\n    @contextmanager\n    def assertNotRaises(self, exc_type):\n        try:\n            yield None\n        except exc_type:\n            raise self.failureException('{} raised'.format(exc_type.__name__))\n"""
tests/utils/list_py_files.py,0,"b""import os\n\n__all__ = [\n    'list_all_py_files',\n]\n\n_excludes = [\n    'tensorlayer/db.py',\n]\n\n\ndef _list_py_files(root):\n    for root, _dirs, files in os.walk(root):\n        if root.find('third_party') != -1:\n            continue\n        for file in files:\n            if file.endswith('.py'):\n                yield os.path.join(root, file)\n\n\ndef list_all_py_files():\n    dirs = ['tensorlayer', 'tests', 'example']\n    for d in dirs:\n        for filename in _list_py_files(d):\n            if filename not in _excludes:\n                yield filename\n"""
tests/utils/timeout_utils.py,0,"b'import platform\n\nif platform.system() != ""Windows"":\n    import signal\nelse:\n    signal = None\n\n__all__ = [\'TimeoutError\', \'WindowsError\', \'TimeoutContext\']\n\n\nclass TimeoutError(Exception):\n    pass\n\n\nclass WindowsError(Exception):\n    pass\n\n\nclass TimeoutContext():\n    """"""Timeout class using ALARM signal.""""""\n\n    def __init__(self, sec):\n        self.sec = sec\n\n    def __enter__(self):\n        if signal is None:\n            raise WindowsError(""Windows is not supported for this test"")\n\n        signal.signal(signal.SIGALRM, self.raise_timeout)\n        signal.alarm(self.sec)\n\n    def __exit__(self, *args):\n        signal.alarm(0)  # disable alarm\n\n    def raise_timeout(self, *args):\n        raise TimeoutError(""A timeout error have been raised."")\n'"
examples/data_process/data/__init__.py,0,b'from __future__ import absolute_import\n\nfrom . import *\n'
examples/pretrained_cnn/data/__init__.py,0,b'from __future__ import absolute_import\n\nfrom . import *\n'
examples/pretrained_cnn/data/imagenet_classes.py,0,"b'class_names = \'\'\'tench, Tinca tinca\ngoldfish, Carassius auratus\ngreat white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\ntiger shark, Galeocerdo cuvieri\nhammerhead, hammerhead shark\nelectric ray, crampfish, numbfish, torpedo\nstingray\ncock\nhen\nostrich, Struthio camelus\nbrambling, Fringilla montifringilla\ngoldfinch, Carduelis carduelis\nhouse finch, linnet, Carpodacus mexicanus\njunco, snowbird\nindigo bunting, indigo finch, indigo bird, Passerina cyanea\nrobin, American robin, Turdus migratorius\nbulbul\njay\nmagpie\nchickadee\nwater ouzel, dipper\nkite\nbald eagle, American eagle, Haliaeetus leucocephalus\nvulture\ngreat grey owl, great gray owl, Strix nebulosa\nEuropean fire salamander, Salamandra salamandra\ncommon newt, Triturus vulgaris\neft\nspotted salamander, Ambystoma maculatum\naxolotl, mud puppy, Ambystoma mexicanum\nbullfrog, Rana catesbeiana\ntree frog, tree-frog\ntailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\nloggerhead, loggerhead turtle, Caretta caretta\nleatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\nmud turtle\nterrapin\nbox turtle, box tortoise\nbanded gecko\ncommon iguana, iguana, Iguana iguana\nAmerican chameleon, anole, Anolis carolinensis\nwhiptail, whiptail lizard\nagama\nfrilled lizard, Chlamydosaurus kingi\nalligator lizard\nGila monster, Heloderma suspectum\ngreen lizard, Lacerta viridis\nAfrican chameleon, Chamaeleo chamaeleon\nKomodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\nAfrican crocodile, Nile crocodile, Crocodylus niloticus\nAmerican alligator, Alligator mississipiensis\ntriceratops\nthunder snake, worm snake, Carphophis amoenus\nringneck snake, ring-necked snake, ring snake\nhognose snake, puff adder, sand viper\ngreen snake, grass snake\nking snake, kingsnake\ngarter snake, grass snake\nwater snake\nvine snake\nnight snake, Hypsiglena torquata\nboa constrictor, Constrictor constrictor\nrock python, rock snake, Python sebae\nIndian cobra, Naja naja\ngreen mamba\nsea snake\nhorned viper, cerastes, sand viper, horned asp, Cerastes cornutus\ndiamondback, diamondback rattlesnake, Crotalus adamanteus\nsidewinder, horned rattlesnake, Crotalus cerastes\ntrilobite\nharvestman, daddy longlegs, Phalangium opilio\nscorpion\nblack and gold garden spider, Argiope aurantia\nbarn spider, Araneus cavaticus\ngarden spider, Aranea diademata\nblack widow, Latrodectus mactans\ntarantula\nwolf spider, hunting spider\ntick\ncentipede\nblack grouse\nptarmigan\nruffed grouse, partridge, Bonasa umbellus\nprairie chicken, prairie grouse, prairie fowl\npeacock\nquail\npartridge\nAfrican grey, African gray, Psittacus erithacus\nmacaw\nsulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\nlorikeet\ncoucal\nbee eater\nhornbill\nhummingbird\njacamar\ntoucan\ndrake\nred-breasted merganser, Mergus serrator\ngoose\nblack swan, Cygnus atratus\ntusker\nechidna, spiny anteater, anteater\nplatypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\nwallaby, brush kangaroo\nkoala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\nwombat\njellyfish\nsea anemone, anemone\nbrain coral\nflatworm, platyhelminth\nnematode, nematode worm, roundworm\nconch\nsnail\nslug\nsea slug, nudibranch\nchiton, coat-of-mail shell, sea cradle, polyplacophore\nchambered nautilus, pearly nautilus, nautilus\nDungeness crab, Cancer magister\nrock crab, Cancer irroratus\nfiddler crab\nking crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\nAmerican lobster, Northern lobster, Maine lobster, Homarus americanus\nspiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\ncrayfish, crawfish, crawdad, crawdaddy\nhermit crab\nisopod\nwhite stork, Ciconia ciconia\nblack stork, Ciconia nigra\nspoonbill\nflamingo\nlittle blue heron, Egretta caerulea\nAmerican egret, great white heron, Egretta albus\nbittern\ncrane\nlimpkin, Aramus pictus\nEuropean gallinule, Porphyrio porphyrio\nAmerican coot, marsh hen, mud hen, water hen, Fulica americana\nbustard\nruddy turnstone, Arenaria interpres\nred-backed sandpiper, dunlin, Erolia alpina\nredshank, Tringa totanus\ndowitcher\noystercatcher, oyster catcher\npelican\nking penguin, Aptenodytes patagonica\nalbatross, mollymawk\ngrey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\nkiller whale, killer, orca, grampus, sea wolf, Orcinus orca\ndugong, Dugong dugon\nsea lion\nChihuahua\nJapanese spaniel\nMaltese dog, Maltese terrier, Maltese\nPekinese, Pekingese, Peke\nShih-Tzu\nBlenheim spaniel\npapillon\ntoy terrier\nRhodesian ridgeback\nAfghan hound, Afghan\nbasset, basset hound\nbeagle\nbloodhound, sleuthhound\nbluetick\nblack-and-tan coonhound\nWalker hound, Walker foxhound\nEnglish foxhound\nredbone\nborzoi, Russian wolfhound\nIrish wolfhound\nItalian greyhound\nwhippet\nIbizan hound, Ibizan Podenco\nNorwegian elkhound, elkhound\notterhound, otter hound\nSaluki, gazelle hound\nScottish deerhound, deerhound\nWeimaraner\nStaffordshire bullterrier, Staffordshire bull terrier\nAmerican Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\nBedlington terrier\nBorder terrier\nKerry blue terrier\nIrish terrier\nNorfolk terrier\nNorwich terrier\nYorkshire terrier\nwire-haired fox terrier\nLakeland terrier\nSealyham terrier, Sealyham\nAiredale, Airedale terrier\ncairn, cairn terrier\nAustralian terrier\nDandie Dinmont, Dandie Dinmont terrier\nBoston bull, Boston terrier\nminiature schnauzer\ngiant schnauzer\nstandard schnauzer\nScotch terrier, Scottish terrier, Scottie\nTibetan terrier, chrysanthemum dog\nsilky terrier, Sydney silky\nsoft-coated wheaten terrier\nWest Highland white terrier\nLhasa, Lhasa apso\nflat-coated retriever\ncurly-coated retriever\ngolden retriever\nLabrador retriever\nChesapeake Bay retriever\nGerman short-haired pointer\nvizsla, Hungarian pointer\nEnglish setter\nIrish setter, red setter\nGordon setter\nBrittany spaniel\nclumber, clumber spaniel\nEnglish springer, English springer spaniel\nWelsh springer spaniel\ncocker spaniel, English cocker spaniel, cocker\nSussex spaniel\nIrish water spaniel\nkuvasz\nschipperke\ngroenendael\nmalinois\nbriard\nkelpie\nkomondor\nOld English sheepdog, bobtail\nShetland sheepdog, Shetland sheep dog, Shetland\ncollie\nBorder collie\nBouvier des Flandres, Bouviers des Flandres\nRottweiler\nGerman shepherd, German shepherd dog, German police dog, alsatian\nDoberman, Doberman pinscher\nminiature pinscher\nGreater Swiss Mountain dog\nBernese mountain dog\nAppenzeller\nEntleBucher\nboxer\nbull mastiff\nTibetan mastiff\nFrench bulldog\nGreat Dane\nSaint Bernard, St Bernard\nEskimo dog, husky\nmalamute, malemute, Alaskan malamute\nSiberian husky\ndalmatian, coach dog, carriage dog\naffenpinscher, monkey pinscher, monkey dog\nbasenji\npug, pug-dog\nLeonberg\nNewfoundland, Newfoundland dog\nGreat Pyrenees\nSamoyed, Samoyede\nPomeranian\nchow, chow chow\nkeeshond\nBrabancon griffon\nPembroke, Pembroke Welsh corgi\nCardigan, Cardigan Welsh corgi\ntoy poodle\nminiature poodle\nstandard poodle\nMexican hairless\ntimber wolf, grey wolf, gray wolf, Canis lupus\nwhite wolf, Arctic wolf, Canis lupus tundrarum\nred wolf, maned wolf, Canis rufus, Canis niger\ncoyote, prairie wolf, brush wolf, Canis latrans\ndingo, warrigal, warragal, Canis dingo\ndhole, Cuon alpinus\nAfrican hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\nhyena, hyaena\nred fox, Vulpes vulpes\nkit fox, Vulpes macrotis\nArctic fox, white fox, Alopex lagopus\ngrey fox, gray fox, Urocyon cinereoargenteus\ntabby, tabby cat\ntiger cat\nPersian cat\nSiamese cat, Siamese\nEgyptian cat\ncougar, puma, catamount, mountain lion, painter, panther, Felis concolor\nlynx, catamount\nleopard, Panthera pardus\nsnow leopard, ounce, Panthera uncia\njaguar, panther, Panthera onca, Felis onca\nlion, king of beasts, Panthera leo\ntiger, Panthera tigris\ncheetah, chetah, Acinonyx jubatus\nbrown bear, bruin, Ursus arctos\nAmerican black bear, black bear, Ursus americanus, Euarctos americanus\nice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\nsloth bear, Melursus ursinus, Ursus ursinus\nmongoose\nmeerkat, mierkat\ntiger beetle\nladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\nground beetle, carabid beetle\nlong-horned beetle, longicorn, longicorn beetle\nleaf beetle, chrysomelid\ndung beetle\nrhinoceros beetle\nweevil\nfly\nbee\nant, emmet, pismire\ngrasshopper, hopper\ncricket\nwalking stick, walkingstick, stick insect\ncockroach, roach\nmantis, mantid\ncicada, cicala\nleafhopper\nlacewing, lacewing fly\ndragonfly, darning needle, devil\'s darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\ndamselfly\nadmiral\nringlet, ringlet butterfly\nmonarch, monarch butterfly, milkweed butterfly, Danaus plexippus\ncabbage butterfly\nsulphur butterfly, sulfur butterfly\nlycaenid, lycaenid butterfly\nstarfish, sea star\nsea urchin\nsea cucumber, holothurian\nwood rabbit, cottontail, cottontail rabbit\nhare\nAngora, Angora rabbit\nhamster\nporcupine, hedgehog\nfox squirrel, eastern fox squirrel, Sciurus niger\nmarmot\nbeaver\nguinea pig, Cavia cobaya\nsorrel\nzebra\nhog, pig, grunter, squealer, Sus scrofa\nwild boar, boar, Sus scrofa\nwarthog\nhippopotamus, hippo, river horse, Hippopotamus amphibius\nox\nwater buffalo, water ox, Asiatic buffalo, Bubalus bubalis\nbison\nram, tup\nbighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\nibex, Capra ibex\nhartebeest\nimpala, Aepyceros melampus\ngazelle\nArabian camel, dromedary, Camelus dromedarius\nllama\nweasel\nmink\npolecat, fitch, foulmart, foumart, Mustela putorius\nblack-footed ferret, ferret, Mustela nigripes\notter\nskunk, polecat, wood pussy\nbadger\narmadillo\nthree-toed sloth, ai, Bradypus tridactylus\norangutan, orang, orangutang, Pongo pygmaeus\ngorilla, Gorilla gorilla\nchimpanzee, chimp, Pan troglodytes\ngibbon, Hylobates lar\nsiamang, Hylobates syndactylus, Symphalangus syndactylus\nguenon, guenon monkey\npatas, hussar monkey, Erythrocebus patas\nbaboon\nmacaque\nlangur\ncolobus, colobus monkey\nproboscis monkey, Nasalis larvatus\nmarmoset\ncapuchin, ringtail, Cebus capucinus\nhowler monkey, howler\ntiti, titi monkey\nspider monkey, Ateles geoffroyi\nsquirrel monkey, Saimiri sciureus\nMadagascar cat, ring-tailed lemur, Lemur catta\nindri, indris, Indri indri, Indri brevicaudatus\nIndian elephant, Elephas maximus\nAfrican elephant, Loxodonta africana\nlesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\ngiant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\nbarracouta, snoek\neel\ncoho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\nrock beauty, Holocanthus tricolor\nanemone fish\nsturgeon\ngar, garfish, garpike, billfish, Lepisosteus osseus\nlionfish\npuffer, pufferfish, blowfish, globefish\nabacus\nabaya\nacademic gown, academic robe, judge\'s robe\naccordion, piano accordion, squeeze box\nacoustic guitar\naircraft carrier, carrier, flattop, attack aircraft carrier\nairliner\nairship, dirigible\naltar\nambulance\namphibian, amphibious vehicle\nanalog clock\napiary, bee house\napron\nashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\nassault rifle, assault gun\nbackpack, back pack, knapsack, packsack, rucksack, haversack\nbakery, bakeshop, bakehouse\nbalance beam, beam\nballoon\nballpoint, ballpoint pen, ballpen, Biro\nBand Aid\nbanjo\nbannister, banister, balustrade, balusters, handrail\nbarbell\nbarber chair\nbarbershop\nbarn\nbarometer\nbarrel, cask\nbarrow, garden cart, lawn cart, wheelbarrow\nbaseball\nbasketball\nbassinet\nbassoon\nbathing cap, swimming cap\nbath towel\nbathtub, bathing tub, bath, tub\nbeach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\nbeacon, lighthouse, beacon light, pharos\nbeaker\nbearskin, busby, shako\nbeer bottle\nbeer glass\nbell cote, bell cot\nbib\nbicycle-built-for-two, tandem bicycle, tandem\nbikini, two-piece\nbinder, ring-binder\nbinoculars, field glasses, opera glasses\nbirdhouse\nboathouse\nbobsled, bobsleigh, bob\nbolo tie, bolo, bola tie, bola\nbonnet, poke bonnet\nbookcase\nbookshop, bookstore, bookstall\nbottlecap\nbow\nbow tie, bow-tie, bowtie\nbrass, memorial tablet, plaque\nbrassiere, bra, bandeau\nbreakwater, groin, groyne, mole, bulwark, seawall, jetty\nbreastplate, aegis, egis\nbroom\nbucket, pail\nbuckle\nbulletproof vest\nbullet train, bullet\nbutcher shop, meat market\ncab, hack, taxi, taxicab\ncaldron, cauldron\ncandle, taper, wax light\ncannon\ncanoe\ncan opener, tin opener\ncardigan\ncar mirror\ncarousel, carrousel, merry-go-round, roundabout, whirligig\ncarpenter\'s kit, tool kit\ncarton\ncar wheel\ncash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\ncassette\ncassette player\ncastle\ncatamaran\nCD player\ncello, violoncello\ncellular telephone, cellular phone, cellphone, cell, mobile phone\nchain\nchainlink fence\nchain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\nchain saw, chainsaw\nchest\nchiffonier, commode\nchime, bell, gong\nchina cabinet, china closet\nChristmas stocking\nchurch, church building\ncinema, movie theater, movie theatre, movie house, picture palace\ncleaver, meat cleaver, chopper\ncliff dwelling\ncloak\nclog, geta, patten, sabot\ncocktail shaker\ncoffee mug\ncoffeepot\ncoil, spiral, volute, whorl, helix\ncombination lock\ncomputer keyboard, keypad\nconfectionery, confectionary, candy store\ncontainer ship, containership, container vessel\nconvertible\ncorkscrew, bottle screw\ncornet, horn, trumpet, trump\ncowboy boot\ncowboy hat, ten-gallon hat\ncradle\ncrane\ncrash helmet\ncrate\ncrib, cot\nCrock Pot\ncroquet ball\ncrutch\ncuirass\ndam, dike, dyke\ndesk\ndesktop computer\ndial telephone, dial phone\ndiaper, nappy, napkin\ndigital clock\ndigital watch\ndining table, board\ndishrag, dishcloth\ndishwasher, dish washer, dishwashing machine\ndisk brake, disc brake\ndock, dockage, docking facility\ndogsled, dog sled, dog sleigh\ndome\ndoormat, welcome mat\ndrilling platform, offshore rig\ndrum, membranophone, tympan\ndrumstick\ndumbbell\nDutch oven\nelectric fan, blower\nelectric guitar\nelectric locomotive\nentertainment center\nenvelope\nespresso maker\nface powder\nfeather boa, boa\nfile, file cabinet, filing cabinet\nfireboat\nfire engine, fire truck\nfire screen, fireguard\nflagpole, flagstaff\nflute, transverse flute\nfolding chair\nfootball helmet\nforklift\nfountain\nfountain pen\nfour-poster\nfreight car\nFrench horn, horn\nfrying pan, frypan, skillet\nfur coat\ngarbage truck, dustcart\ngasmask, respirator, gas helmet\ngas pump, gasoline pump, petrol pump, island dispenser\ngoblet\ngo-kart\ngolf ball\ngolfcart, golf cart\ngondola\ngong, tam-tam\ngown\ngrand piano, grand\ngreenhouse, nursery, glasshouse\ngrille, radiator grille\ngrocery store, grocery, food market, market\nguillotine\nhair slide\nhair spray\nhalf track\nhammer\nhamper\nhand blower, blow dryer, blow drier, hair dryer, hair drier\nhand-held computer, hand-held microcomputer\nhandkerchief, hankie, hanky, hankey\nhard disc, hard disk, fixed disk\nharmonica, mouth organ, harp, mouth harp\nharp\nharvester, reaper\nhatchet\nholster\nhome theater, home theatre\nhoneycomb\nhook, claw\nhoopskirt, crinoline\nhorizontal bar, high bar\nhorse cart, horse-cart\nhourglass\niPod\niron, smoothing iron\njack-o\'-lantern\njean, blue jean, denim\njeep, landrover\njersey, T-shirt, tee shirt\njigsaw puzzle\njinrikisha, ricksha, rickshaw\njoystick\nkimono\nknee pad\nknot\nlab coat, laboratory coat\nladle\nlampshade, lamp shade\nlaptop, laptop computer\nlawn mower, mower\nlens cap, lens cover\nletter opener, paper knife, paperknife\nlibrary\nlifeboat\nlighter, light, igniter, ignitor\nlimousine, limo\nliner, ocean liner\nlipstick, lip rouge\nLoafer\nlotion\nloudspeaker, speaker, speaker unit, loudspeaker system, speaker system\nloupe, jeweler\'s loupe\nlumbermill, sawmill\nmagnetic compass\nmailbag, postbag\nmailbox, letter box\nmaillot\nmaillot, tank suit\nmanhole cover\nmaraca\nmarimba, xylophone\nmask\nmatchstick\nmaypole\nmaze, labyrinth\nmeasuring cup\nmedicine chest, medicine cabinet\nmegalith, megalithic structure\nmicrophone, mike\nmicrowave, microwave oven\nmilitary uniform\nmilk can\nminibus\nminiskirt, mini\nminivan\nmissile\nmitten\nmixing bowl\nmobile home, manufactured home\nModel T\nmodem\nmonastery\nmonitor\nmoped\nmortar\nmortarboard\nmosque\nmosquito net\nmotor scooter, scooter\nmountain bike, all-terrain bike, off-roader\nmountain tent\nmouse, computer mouse\nmousetrap\nmoving van\nmuzzle\nnail\nneck brace\nnecklace\nnipple\nnotebook, notebook computer\nobelisk\noboe, hautboy, hautbois\nocarina, sweet potato\nodometer, hodometer, mileometer, milometer\noil filter\norgan, pipe organ\noscilloscope, scope, cathode-ray oscilloscope, CRO\noverskirt\noxcart\noxygen mask\npacket\npaddle, boat paddle\npaddlewheel, paddle wheel\npadlock\npaintbrush\npajama, pyjama, pj\'s, jammies\npalace\npanpipe, pandean pipe, syrinx\npaper towel\nparachute, chute\nparallel bars, bars\npark bench\nparking meter\npassenger car, coach, carriage\npatio, terrace\npay-phone, pay-station\npedestal, plinth, footstall\npencil box, pencil case\npencil sharpener\nperfume, essence\nPetri dish\nphotocopier\npick, plectrum, plectron\npickelhaube\npicket fence, paling\npickup, pickup truck\npier\npiggy bank, penny bank\npill bottle\npillow\nping-pong ball\npinwheel\npirate, pirate ship\npitcher, ewer\nplane, carpenter\'s plane, woodworking plane\nplanetarium\nplastic bag\nplate rack\nplow, plough\nplunger, plumber\'s helper\nPolaroid camera, Polaroid Land camera\npole\npolice van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\nponcho\npool table, billiard table, snooker table\npop bottle, soda bottle\npot, flowerpot\npotter\'s wheel\npower drill\nprayer rug, prayer mat\nprinter\nprison, prison house\nprojectile, missile\nprojector\npuck, hockey puck\npunching bag, punch bag, punching ball, punchball\npurse\nquill, quill pen\nquilt, comforter, comfort, puff\nracer, race car, racing car\nracket, racquet\nradiator\nradio, wireless\nradio telescope, radio reflector\nrain barrel\nrecreational vehicle, RV, R.V.\nreel\nreflex camera\nrefrigerator, icebox\nremote control, remote\nrestaurant, eating house, eating place, eatery\nrevolver, six-gun, six-shooter\nrifle\nrocking chair, rocker\nrotisserie\nrubber eraser, rubber, pencil eraser\nrugby ball\nrule, ruler\nrunning shoe\nsafe\nsafety pin\nsaltshaker, salt shaker\nsandal\nsarong\nsax, saxophone\nscabbard\nscale, weighing machine\nschool bus\nschooner\nscoreboard\nscreen, CRT screen\nscrew\nscrewdriver\nseat belt, seatbelt\nsewing machine\nshield, buckler\nshoe shop, shoe-shop, shoe store\nshoji\nshopping basket\nshopping cart\nshovel\nshower cap\nshower curtain\nski\nski mask\nsleeping bag\nslide rule, slipstick\nsliding door\nslot, one-armed bandit\nsnorkel\nsnowmobile\nsnowplow, snowplough\nsoap dispenser\nsoccer ball\nsock\nsolar dish, solar collector, solar furnace\nsombrero\nsoup bowl\nspace bar\nspace heater\nspace shuttle\nspatula\nspeedboat\nspider web, spider\'s web\nspindle\nsports car, sport car\nspotlight, spot\nstage\nsteam locomotive\nsteel arch bridge\nsteel drum\nstethoscope\nstole\nstone wall\nstopwatch, stop watch\nstove\nstrainer\nstreetcar, tram, tramcar, trolley, trolley car\nstretcher\nstudio couch, day bed\nstupa, tope\nsubmarine, pigboat, sub, U-boat\nsuit, suit of clothes\nsundial\nsunglass\nsunglasses, dark glasses, shades\nsunscreen, sunblock, sun blocker\nsuspension bridge\nswab, swob, mop\nsweatshirt\nswimming trunks, bathing trunks\nswing\nswitch, electric switch, electrical switch\nsyringe\ntable lamp\ntank, army tank, armored combat vehicle, armoured combat vehicle\ntape player\nteapot\nteddy, teddy bear\ntelevision, television system\ntennis ball\nthatch, thatched roof\ntheater curtain, theatre curtain\nthimble\nthresher, thrasher, threshing machine\nthrone\ntile roof\ntoaster\ntobacco shop, tobacconist shop, tobacconist\ntoilet seat\ntorch\ntotem pole\ntow truck, tow car, wrecker\ntoyshop\ntractor\ntrailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\ntray\ntrench coat\ntricycle, trike, velocipede\ntrimaran\ntripod\ntriumphal arch\ntrolleybus, trolley coach, trackless trolley\ntrombone\ntub, vat\nturnstile\ntypewriter keyboard\numbrella\nunicycle, monocycle\nupright, upright piano\nvacuum, vacuum cleaner\nvase\nvault\nvelvet\nvending machine\nvestment\nviaduct\nviolin, fiddle\nvolleyball\nwaffle iron\nwall clock\nwallet, billfold, notecase, pocketbook\nwardrobe, closet, press\nwarplane, military plane\nwashbasin, handbasin, washbowl, lavabo, wash-hand basin\nwasher, automatic washer, washing machine\nwater bottle\nwater jug\nwater tower\nwhiskey jug\nwhistle\nwig\nwindow screen\nwindow shade\nWindsor tie\nwine bottle\nwing\nwok\nwooden spoon\nwool, woolen, woollen\nworm fence, snake fence, snake-rail fence, Virginia fence\nwreck\nyawl\nyurt\nweb site, website, internet site, site\ncomic book\ncrossword puzzle, crossword\nstreet sign\ntraffic light, traffic signal, stoplight\nbook jacket, dust cover, dust jacket, dust wrapper\nmenu\nplate\nguacamole\nconsomme\nhot pot, hotpot\ntrifle\nice cream, icecream\nice lolly, lolly, lollipop, popsicle\nFrench loaf\nbagel, beigel\npretzel\ncheeseburger\nhotdog, hot dog, red hot\nmashed potato\nhead cabbage\nbroccoli\ncauliflower\nzucchini, courgette\nspaghetti squash\nacorn squash\nbutternut squash\ncucumber, cuke\nartichoke, globe artichoke\nbell pepper\ncardoon\nmushroom\nGranny Smith\nstrawberry\norange\nlemon\nfig\npineapple, ananas\nbanana\njackfruit, jak, jack\ncustard apple\npomegranate\nhay\ncarbonara\nchocolate sauce, chocolate syrup\ndough\nmeat loaf, meatloaf\npizza, pizza pie\npotpie\nburrito\nred wine\nespresso\ncup\neggnog\nalp\nbubble\ncliff, drop, drop-off\ncoral reef\ngeyser\nlakeside, lakeshore\npromontory, headland, head, foreland\nsandbar, sand bar\nseashore, coast, seacoast, sea-coast\nvalley, vale\nvolcano\nballplayer, baseball player\ngroom, bridegroom\nscuba diver\nrapeseed\ndaisy\nyellow lady\'s slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\ncorn\nacorn\nhip, rose hip, rosehip\nbuckeye, horse chestnut, conker\ncoral fungus\nagaric\ngyromitra\nstinkhorn, carrion fungus\nearthstar\nhen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\nbolete\near, spike, capitulum\ntoilet tissue, toilet paper, bathroom tissue\'\'\'.split(""\\n"")\n'"
examples/text_generation/data/__init__.py,0,b'from __future__ import absolute_import\n\nfrom . import imagenet_classes\n\n# from . import\n'
tensorlayer/files/dataset_loaders/__init__.py,0,"b""#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom .celebA_dataset import *\nfrom .cifar10_dataset import *\nfrom .cyclegan_dataset import *\nfrom .flickr_1M_dataset import *\nfrom .flickr_25k_dataset import *\nfrom .imdb_dataset import *\nfrom .matt_mahoney_dataset import *\nfrom .mnist_dataset import *\nfrom .mnist_fashion_dataset import *\nfrom .mpii_dataset import *\nfrom .nietzsche_dataset import *\nfrom .ptb_dataset import *\nfrom .voc_dataset import *\nfrom .wmt_en_fr_dataset import *\n\n__all__ = [\n    'load_celebA_dataset',\n    'load_cifar10_dataset',\n    'load_cyclegan_dataset',\n    'load_fashion_mnist_dataset',\n    'load_flickr1M_dataset',\n    'load_flickr25k_dataset',\n    'load_imdb_dataset',\n    'load_matt_mahoney_text8_dataset',\n    'load_mnist_dataset',\n    'load_mpii_pose_dataset',\n    'load_nietzsche_dataset',\n    'load_ptb_dataset',\n    'load_voc_dataset',\n    'load_wmt_en_fr_dataset',\n]\n"""
tensorlayer/files/dataset_loaders/celebA_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nimport zipfile\n\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import (download_file_from_google_drive, exists_or_mkdir, load_file_list)\n\n__all__ = [\'load_celebA_dataset\']\n\n\ndef load_celebA_dataset(path=\'data\'):\n    """"""Load CelebA dataset\n\n    Return a list of image path.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/celebA/``.\n\n    """"""\n    data_dir = \'celebA\'\n    filename, drive_id = ""img_align_celeba.zip"", ""0B7EVK8r0v71pZjFTYXZWM3FlRnM""\n    save_path = os.path.join(path, filename)\n    image_path = os.path.join(path, data_dir)\n    if os.path.exists(image_path):\n        logging.info(\'[*] {} already exists\'.format(save_path))\n    else:\n        exists_or_mkdir(path)\n        download_file_from_google_drive(drive_id, save_path)\n        zip_dir = \'\'\n        with zipfile.ZipFile(save_path) as zf:\n            zip_dir = zf.namelist()[0]\n            zf.extractall(path)\n        os.remove(save_path)\n        os.rename(os.path.join(path, zip_dir), image_path)\n\n    data_files = load_file_list(path=image_path, regx=\'\\\\.jpg\', printable=False)\n    for i, _v in enumerate(data_files):\n        data_files[i] = os.path.join(image_path, data_files[i])\n    return data_files\n'"
tensorlayer/files/dataset_loaders/cifar10_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nimport pickle\nimport sys\n\nimport numpy as np\n\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import maybe_download_and_extract\n\n__all__ = [\'load_cifar10_dataset\']\n\n\ndef load_cifar10_dataset(shape=(-1, 32, 32, 3), path=\'data\', plotable=False):\n    """"""Load CIFAR-10 dataset.\n\n    It consists of 60000 32x32 colour images in 10 classes, with\n    6000 images per class. There are 50000 training images and 10000 test images.\n\n    The dataset is divided into five training batches and one test batch, each with\n    10000 images. The test batch contains exactly 1000 randomly-selected images from\n    each class. The training batches contain the remaining images in random order,\n    but some training batches may contain more images from one class than another.\n    Between them, the training batches contain exactly 5000 images from each class.\n\n    Parameters\n    ----------\n    shape : tupe\n        The shape of digit images e.g. (-1, 3, 32, 32) and (-1, 32, 32, 3).\n    path : str\n        The path that the data is downloaded to, defaults is ``data/cifar10/``.\n    plotable : boolean\n        Whether to plot some image examples, False as default.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3))\n\n    References\n    ----------\n    - `CIFAR website <https://www.cs.toronto.edu/~kriz/cifar.html>`__\n    - `Data download link <https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz>`__\n    - `<https://teratail.com/questions/28932>`__\n\n    """"""\n    path = os.path.join(path, \'cifar10\')\n    logging.info(""Load or Download cifar10 > {}"".format(path))\n\n    #Helper function to unpickle the data\n    def unpickle(file):\n        fp = open(file, \'rb\')\n        if sys.version_info.major == 2:\n            data = pickle.load(fp)\n        elif sys.version_info.major == 3:\n            data = pickle.load(fp, encoding=\'latin-1\')\n        else:\n            raise RuntimeError(""Sys Version Unsupported"")\n        fp.close()\n        return data\n\n    filename = \'cifar-10-python.tar.gz\'\n    url = \'https://www.cs.toronto.edu/~kriz/\'\n    #Download and uncompress file\n    maybe_download_and_extract(filename, path, url, extract=True)\n\n    #Unpickle file and fill in data\n    X_train = None\n    y_train = []\n    for i in range(1, 6):\n        data_dic = unpickle(os.path.join(path, \'cifar-10-batches-py/\', ""data_batch_{}"".format(i)))\n        if i == 1:\n            X_train = data_dic[\'data\']\n        else:\n            X_train = np.vstack((X_train, data_dic[\'data\']))\n        y_train += data_dic[\'labels\']\n\n    test_data_dic = unpickle(os.path.join(path, \'cifar-10-batches-py/\', ""test_batch""))\n    X_test = test_data_dic[\'data\']\n    y_test = np.array(test_data_dic[\'labels\'])\n\n    if shape == (-1, 3, 32, 32):\n        X_test = X_test.reshape(shape)\n        X_train = X_train.reshape(shape)\n    elif shape == (-1, 32, 32, 3):\n        X_test = X_test.reshape(shape, order=\'F\')\n        X_train = X_train.reshape(shape, order=\'F\')\n        X_test = np.transpose(X_test, (0, 2, 1, 3))\n        X_train = np.transpose(X_train, (0, 2, 1, 3))\n    else:\n        X_test = X_test.reshape(shape)\n        X_train = X_train.reshape(shape)\n\n    y_train = np.array(y_train)\n\n    if plotable:\n        logging.info(\'\\nCIFAR-10\')\n        import matplotlib.pyplot as plt\n        fig = plt.figure(1)\n\n        logging.info(\'Shape of a training image: X_train[0] %s\' % X_train[0].shape)\n\n        plt.ion()  # interactive mode\n        count = 1\n        for _ in range(10):  # each row\n            for _ in range(10):  # each column\n                _ = fig.add_subplot(10, 10, count)\n                if shape == (-1, 3, 32, 32):\n                    # plt.imshow(X_train[count-1], interpolation=\'nearest\')\n                    plt.imshow(np.transpose(X_train[count - 1], (1, 2, 0)), interpolation=\'nearest\')\n                    # plt.imshow(np.transpose(X_train[count-1], (2, 1, 0)), interpolation=\'nearest\')\n                elif shape == (-1, 32, 32, 3):\n                    plt.imshow(X_train[count - 1], interpolation=\'nearest\')\n                    # plt.imshow(np.transpose(X_train[count-1], (1, 0, 2)), interpolation=\'nearest\')\n                else:\n                    raise Exception(""Do not support the given \'shape\' to plot the image examples"")\n                plt.gca().xaxis.set_major_locator(plt.NullLocator())\n                plt.gca().yaxis.set_major_locator(plt.NullLocator())\n                count = count + 1\n        plt.draw()  # interactive mode\n        plt.pause(3)  # interactive mode\n\n        logging.info(""X_train: %s"" % X_train.shape)\n        logging.info(""y_train: %s"" % y_train.shape)\n        logging.info(""X_test:  %s"" % X_test.shape)\n        logging.info(""y_test:  %s"" % y_test.shape)\n\n    X_train = np.asarray(X_train, dtype=np.float32)\n    X_test = np.asarray(X_test, dtype=np.float32)\n    y_train = np.asarray(y_train, dtype=np.int32)\n    y_test = np.asarray(y_test, dtype=np.int32)\n\n    return X_train, y_train, X_test, y_test\n'"
tensorlayer/files/dataset_loaders/cyclegan_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nimport numpy as np\n\nfrom tensorlayer import logging, visualize\nfrom tensorlayer.files.utils import (del_file, folder_exists, load_file_list, maybe_download_and_extract)\n\n__all__ = [\'load_cyclegan_dataset\']\n\n\ndef load_cyclegan_dataset(filename=\'summer2winter_yosemite\', path=\'data\'):\n    """"""Load images from CycleGAN\'s database, see `this link <https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/>`__.\n\n    Parameters\n    ------------\n    filename : str\n        The dataset you want, see `this link <https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/>`__.\n    path : str\n        The path that the data is downloaded to, defaults is `data/cyclegan`\n\n    Examples\n    ---------\n    >>> im_train_A, im_train_B, im_test_A, im_test_B = load_cyclegan_dataset(filename=\'summer2winter_yosemite\')\n\n    """"""\n    path = os.path.join(path, \'cyclegan\')\n    url = \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\'\n\n    if folder_exists(os.path.join(path, filename)) is False:\n        logging.info(""[*] {} is nonexistent in {}"".format(filename, path))\n        maybe_download_and_extract(filename + \'.zip\', path, url, extract=True)\n        del_file(os.path.join(path, filename + \'.zip\'))\n\n    def load_image_from_folder(path):\n        path_imgs = load_file_list(path=path, regx=\'\\\\.jpg\', printable=False)\n        return visualize.read_images(path_imgs, path=path, n_threads=10, printable=False)\n\n    im_train_A = load_image_from_folder(os.path.join(path, filename, ""trainA""))\n    im_train_B = load_image_from_folder(os.path.join(path, filename, ""trainB""))\n    im_test_A = load_image_from_folder(os.path.join(path, filename, ""testA""))\n    im_test_B = load_image_from_folder(os.path.join(path, filename, ""testB""))\n\n    def if_2d_to_3d(images):  # [h, w] --> [h, w, 3]\n        for i, _v in enumerate(images):\n            if len(images[i].shape) == 2:\n                images[i] = images[i][:, :, np.newaxis]\n                images[i] = np.tile(images[i], (1, 1, 3))\n        return images\n\n    im_train_A = if_2d_to_3d(im_train_A)\n    im_train_B = if_2d_to_3d(im_train_B)\n    im_test_A = if_2d_to_3d(im_test_A)\n    im_test_B = if_2d_to_3d(im_test_B)\n\n    return im_train_A, im_train_B, im_test_A, im_test_B\n'"
tensorlayer/files/dataset_loaders/flickr_1M_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nfrom tensorlayer import logging, visualize\nfrom tensorlayer.files.utils import (\n    del_file, folder_exists, load_file_list, load_folder_list, maybe_download_and_extract, read_file\n)\n\n__all__ = [\'load_flickr1M_dataset\']\n\n\ndef load_flickr1M_dataset(tag=\'sky\', size=10, path=""data"", n_threads=50, printable=False):\n    """"""Load Flick1M dataset.\n\n    Returns a list of images by a given tag from Flickr1M dataset,\n    it will download Flickr1M from `the official website <http://press.liacs.nl/mirflickr/mirdownload.html>`__\n    at the first time you use it.\n\n    Parameters\n    ------------\n    tag : str or None\n        What images to return.\n            - If you want to get images with tag, use string like \'dog\', \'red\', see `Flickr Search <https://www.flickr.com/search/>`__.\n            - If you want to get all images, set to ``None``.\n\n    size : int\n        integer between 1 to 10. 1 means 100k images ... 5 means 500k images, 10 means all 1 million images. Default is 10.\n    path : str\n        The path that the data is downloaded to, defaults is ``data/flickr25k/``.\n    n_threads : int\n        The number of thread to read image.\n    printable : boolean\n        Whether to print infomation when reading images, default is ``False``.\n\n    Examples\n    ----------\n    Use 200k images\n\n    >>> images = tl.files.load_flickr1M_dataset(tag=\'zebra\', size=2)\n\n    Use 1 Million images\n\n    >>> images = tl.files.load_flickr1M_dataset(tag=\'zebra\')\n\n    """"""\n    import shutil\n\n    path = os.path.join(path, \'flickr1M\')\n    logging.info(""[Flickr1M] using {}% of images = {}"".format(size * 10, size * 100000))\n    images_zip = [\n        \'images0.zip\', \'images1.zip\', \'images2.zip\', \'images3.zip\', \'images4.zip\', \'images5.zip\', \'images6.zip\',\n        \'images7.zip\', \'images8.zip\', \'images9.zip\'\n    ]\n    tag_zip = \'tags.zip\'\n    url = \'http://press.liacs.nl/mirflickr/mirflickr1m/\'\n\n    # download dataset\n    for image_zip in images_zip[0:size]:\n        image_folder = image_zip.split(""."")[0]\n        # logging.info(path+""/""+image_folder)\n        if folder_exists(os.path.join(path, image_folder)) is False:\n            # logging.info(image_zip)\n            logging.info(""[Flickr1M] {} is missing in {}"".format(image_folder, path))\n            maybe_download_and_extract(image_zip, path, url, extract=True)\n            del_file(os.path.join(path, image_zip))\n            # os.system(""mv {} {}"".format(os.path.join(path, \'images\'), os.path.join(path, image_folder)))\n            shutil.move(os.path.join(path, \'images\'), os.path.join(path, image_folder))\n        else:\n            logging.info(""[Flickr1M] {} exists in {}"".format(image_folder, path))\n\n    # download tag\n    if folder_exists(os.path.join(path, ""tags"")) is False:\n        logging.info(""[Flickr1M] tag files is nonexistent in {}"".format(path))\n        maybe_download_and_extract(tag_zip, path, url, extract=True)\n        del_file(os.path.join(path, tag_zip))\n    else:\n        logging.info(""[Flickr1M] tags exists in {}"".format(path))\n\n    # 1. image path list\n    images_list = []\n    images_folder_list = []\n    for i in range(0, size):\n        images_folder_list += load_folder_list(path=os.path.join(path, \'images%d\' % i))\n    images_folder_list.sort(key=lambda s: int(s.split(\'/\')[-1]))  # folder/images/ddd\n\n    for folder in images_folder_list[0:size * 10]:\n        tmp = load_file_list(path=folder, regx=\'\\\\.jpg\', printable=False)\n        tmp.sort(key=lambda s: int(s.split(\'.\')[-2]))  # ddd.jpg\n        images_list.extend([os.path.join(folder, x) for x in tmp])\n\n    # 2. tag path list\n    tag_list = []\n    tag_folder_list = load_folder_list(os.path.join(path, ""tags""))\n\n    # tag_folder_list.sort(key=lambda s: int(s.split(""/"")[-1]))  # folder/images/ddd\n    tag_folder_list.sort(key=lambda s: int(os.path.basename(s)))\n\n    for folder in tag_folder_list[0:size * 10]:\n        tmp = load_file_list(path=folder, regx=\'\\\\.txt\', printable=False)\n        tmp.sort(key=lambda s: int(s.split(\'.\')[-2]))  # ddd.txt\n        tmp = [os.path.join(folder, s) for s in tmp]\n        tag_list += tmp\n\n    # 3. select images\n    logging.info(""[Flickr1M] searching tag: {}"".format(tag))\n    select_images_list = []\n    for idx, _val in enumerate(tag_list):\n        tags = read_file(tag_list[idx]).split(\'\\n\')\n        if tag in tags:\n            select_images_list.append(images_list[idx])\n\n    logging.info(""[Flickr1M] reading images with tag: {}"".format(tag))\n    images = visualize.read_images(select_images_list, \'\', n_threads=n_threads, printable=printable)\n    return images\n'"
tensorlayer/files/dataset_loaders/flickr_25k_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nfrom tensorlayer import logging, visualize\nfrom tensorlayer.files.utils import (\n    del_file, folder_exists, load_file_list, maybe_download_and_extract, natural_keys, read_file\n)\n\n__all__ = [\'load_flickr25k_dataset\']\n\n\ndef load_flickr25k_dataset(tag=\'sky\', path=""data"", n_threads=50, printable=False):\n    """"""Load Flickr25K dataset.\n\n    Returns a list of images by a given tag from Flick25k dataset,\n    it will download Flickr25k from `the official website <http://press.liacs.nl/mirflickr/mirdownload.html>`__\n    at the first time you use it.\n\n    Parameters\n    ------------\n    tag : str or None\n        What images to return.\n            - If you want to get images with tag, use string like \'dog\', \'red\', see `Flickr Search <https://www.flickr.com/search/>`__.\n            - If you want to get all images, set to ``None``.\n\n    path : str\n        The path that the data is downloaded to, defaults is ``data/flickr25k/``.\n    n_threads : int\n        The number of thread to read image.\n    printable : boolean\n        Whether to print infomation when reading images, default is ``False``.\n\n    Examples\n    -----------\n    Get images with tag of sky\n\n    >>> images = tl.files.load_flickr25k_dataset(tag=\'sky\')\n\n    Get all images\n\n    >>> images = tl.files.load_flickr25k_dataset(tag=None, n_threads=100, printable=True)\n\n    """"""\n    path = os.path.join(path, \'flickr25k\')\n\n    filename = \'mirflickr25k.zip\'\n    url = \'http://press.liacs.nl/mirflickr/mirflickr25k/\'\n\n    # download dataset\n    if folder_exists(os.path.join(path, ""mirflickr"")) is False:\n        logging.info(""[*] Flickr25k is nonexistent in {}"".format(path))\n        maybe_download_and_extract(filename, path, url, extract=True)\n        del_file(os.path.join(path, filename))\n\n    # return images by the given tag.\n    # 1. image path list\n    folder_imgs = os.path.join(path, ""mirflickr"")\n    path_imgs = load_file_list(path=folder_imgs, regx=\'\\\\.jpg\', printable=False)\n    path_imgs.sort(key=natural_keys)\n\n    # 2. tag path list\n    folder_tags = os.path.join(path, ""mirflickr"", ""meta"", ""tags"")\n    path_tags = load_file_list(path=folder_tags, regx=\'\\\\.txt\', printable=False)\n    path_tags.sort(key=natural_keys)\n\n    # 3. select images\n    if tag is None:\n        logging.info(""[Flickr25k] reading all images"")\n    else:\n        logging.info(""[Flickr25k] reading images with tag: {}"".format(tag))\n    images_list = []\n    for idx, _v in enumerate(path_tags):\n        tags = read_file(os.path.join(folder_tags, path_tags[idx])).split(\'\\n\')\n        # logging.info(idx+1, tags)\n        if tag is None or tag in tags:\n            images_list.append(path_imgs[idx])\n\n    images = visualize.read_images(images_list, folder_imgs, n_threads=n_threads, printable=printable)\n    return images\n'"
tensorlayer/files/dataset_loaders/imdb_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport gzip\nimport os\n\nimport numpy as np\nimport six.moves.cPickle as pickle\n\nfrom tensorlayer.files.utils import maybe_download_and_extract\n\n__all__ = [\'load_imdb_dataset\']\n\n\ndef load_imdb_dataset(\n    path=\'data\', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2,\n    index_from=3\n):\n    """"""Load IMDB dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\n    nb_words : int\n        Number of words to get.\n    skip_top : int\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\n    maxlen : int\n        Maximum sequence length. Any longer sequence will be truncated.\n    seed : int\n        Seed for reproducible data shuffling.\n    start_char : int\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\n    oov_char : int\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\n    index_from : int\n        Index actual words with this index and higher.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\n    ...                                 nb_words=20000, test_split=0.2)\n    >>> print(\'X_train.shape\', X_train.shape)\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\n    >>> print(\'y_train.shape\', y_train.shape)\n    (20000,)  [1 0 0 ..., 1 0 1]\n\n    References\n    -----------\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\n\n    """"""\n    path = os.path.join(path, \'imdb\')\n\n    filename = ""imdb.pkl""\n    url = \'https://s3.amazonaws.com/text-datasets/\'\n    maybe_download_and_extract(filename, path, url)\n\n    if filename.endswith("".gz""):\n        f = gzip.open(os.path.join(path, filename), \'rb\')\n    else:\n        f = open(os.path.join(path, filename), \'rb\')\n\n    X, labels = pickle.load(f)\n    f.close()\n\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for x, y in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception(\n            \'After filtering for sequences shorter than maxlen=\' + str(maxlen) + \', no sequence was kept. \'\n            \'Increase maxlen.\'\n        )\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n\n    # by convention, use 2 as OOV word\n    # reserve \'index_from\' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n    if oov_char is not None:\n        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if (w >= nb_words or w < skip_top):\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n\n    return X_train, y_train, X_test, y_test\n'"
tensorlayer/files/dataset_loaders/matt_mahoney_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nimport zipfile\n\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import maybe_download_and_extract\n\n__all__ = [\'load_matt_mahoney_text8_dataset\']\n\n\ndef load_matt_mahoney_text8_dataset(path=\'data\'):\n    """"""Load Matt Mahoney\'s dataset.\n\n    Download a text file from Matt Mahoney\'s website\n    if not present, and make sure it\'s the right size.\n    Extract the first file enclosed in a zip file as a list of words.\n    This dataset can be used for Word Embedding.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/mm_test8/``.\n\n    Returns\n    --------\n    list of str\n        The raw text data e.g. [.... \'their\', \'families\', \'who\', \'were\', \'expelled\', \'from\', \'jerusalem\', ...]\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> print(\'Data size\', len(words))\n\n    """"""\n    path = os.path.join(path, \'mm_test8\')\n    logging.info(""Load or Download matt_mahoney_text8 Dataset> {}"".format(path))\n\n    filename = \'text8.zip\'\n    url = \'http://mattmahoney.net/dc/\'\n    maybe_download_and_extract(filename, path, url, expected_bytes=31344016)\n\n    with zipfile.ZipFile(os.path.join(path, filename)) as f:\n        word_list = f.read(f.namelist()[0]).split()\n        for idx, _ in enumerate(word_list):\n            word_list[idx] = word_list[idx].decode()\n    return word_list\n'"
tensorlayer/files/dataset_loaders/mnist_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom tensorlayer.files.utils import _load_mnist_dataset\n\n__all__ = [\'load_mnist_dataset\']\n\n\ndef load_mnist_dataset(shape=(-1, 784), path=\'data\'):\n    """"""Load the original mnist.\n\n    Automatically download MNIST dataset and return the training, validation and test set with 50000, 10000 and 10000 digit images respectively.\n\n    Parameters\n    ----------\n    shape : tuple\n        The shape of digit images (the default is (-1, 784), alternatively (-1, 28, 28, 1)).\n    path : str\n        The path that the data is downloaded to.\n\n    Returns\n    -------\n    X_train, y_train, X_val, y_val, X_test, y_test: tuple\n        Return splitted training/validation/test set respectively.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1,784), path=\'datasets\')\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n    """"""\n    return _load_mnist_dataset(shape, path, name=\'mnist\', url=\'http://yann.lecun.com/exdb/mnist/\')\n'"
tensorlayer/files/dataset_loaders/mnist_fashion_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom tensorlayer.files.utils import _load_mnist_dataset\n\n__all__ = [\'load_fashion_mnist_dataset\']\n\n\ndef load_fashion_mnist_dataset(shape=(-1, 784), path=\'data\'):\n    """"""Load the fashion mnist.\n\n    Automatically download fashion-MNIST dataset and return the training, validation and test set with 50000, 10000 and 10000 fashion images respectively, `examples <http://marubon-ds.blogspot.co.uk/2017/09/fashion-mnist-exploring.html>`__.\n\n    Parameters\n    ----------\n    shape : tuple\n        The shape of digit images (the default is (-1, 784), alternatively (-1, 28, 28, 1)).\n    path : str\n        The path that the data is downloaded to.\n\n    Returns\n    -------\n    X_train, y_train, X_val, y_val, X_test, y_test: tuple\n        Return splitted training/validation/test set respectively.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_fashion_mnist_dataset(shape=(-1,784), path=\'datasets\')\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_fashion_mnist_dataset(shape=(-1, 28, 28, 1))\n    """"""\n    return _load_mnist_dataset(\n        shape, path, name=\'fashion_mnist\', url=\'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\'\n    )\n'"
tensorlayer/files/dataset_loaders/mnist_utils.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport gzip\nimport os\n\nimport numpy as np\n\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import maybe_download_and_extract\n\n__all__ = [""_load_mnist_dataset""]\n\n\ndef _load_mnist_dataset(shape, path, name=\'mnist\', url=\'http://yann.lecun.com/exdb/mnist/\'):\n    """"""A generic function to load mnist-like dataset.\n\n    Parameters:\n    ----------\n    shape : tuple\n        The shape of digit images.\n    path : str\n        The path that the data is downloaded to.\n    name : str\n        The dataset name you want to use(the default is \'mnist\').\n    url : str\n        The url of dataset(the default is \'http://yann.lecun.com/exdb/mnist/\').\n    """"""\n    path = os.path.join(path, name)\n\n    # Define functions for loading mnist-like data\'s images and labels.\n    # For convenience, they also download the requested files if needed.\n    def load_mnist_images(path, filename):\n        filepath = maybe_download_and_extract(filename, path, url)\n\n        logging.info(filepath)\n        # Read the inputs in Yann LeCun\'s binary format.\n        with gzip.open(filepath, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=16)\n        # The inputs are vectors now, we reshape them to monochrome 2D images,\n        # following the shape convention: (examples, channels, rows, columns)\n        data = data.reshape(shape)\n        # The inputs come as bytes, we convert them to float32 in range [0,1].\n        # (Actually to range [0, 255/256], for compatibility to the version\n        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n        return data / np.float32(256)\n\n    def load_mnist_labels(path, filename):\n        filepath = maybe_download_and_extract(filename, path, url)\n        # Read the labels in Yann LeCun\'s binary format.\n        with gzip.open(filepath, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=8)\n        # The labels are vectors of integers now, that\'s exactly what we want.\n        return data\n\n    # Download and read the training and test set images and labels.\n    logging.info(""Load or Download {0} > {1}"".format(name.upper(), path))\n    X_train = load_mnist_images(path, \'train-images-idx3-ubyte.gz\')\n    y_train = load_mnist_labels(path, \'train-labels-idx1-ubyte.gz\')\n    X_test = load_mnist_images(path, \'t10k-images-idx3-ubyte.gz\')\n    y_test = load_mnist_labels(path, \'t10k-labels-idx1-ubyte.gz\')\n\n    # We reserve the last 10000 training examples for validation.\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    # We just return all the arrays in order, as expected in main().\n    # (It doesn\'t matter how we do this as long as we can read them again.)\n    X_train = np.asarray(X_train, dtype=np.float32)\n    y_train = np.asarray(y_train, dtype=np.int32)\n    X_val = np.asarray(X_val, dtype=np.float32)\n    y_val = np.asarray(y_val, dtype=np.int32)\n    X_test = np.asarray(X_test, dtype=np.float32)\n    y_test = np.asarray(y_test, dtype=np.int32)\n    return X_train, y_train, X_val, y_val, X_test, y_test\n'"
tensorlayer/files/dataset_loaders/mpii_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import (del_file, folder_exists, load_file_list, maybe_download_and_extract)\n\n__all__ = [\'load_mpii_pose_dataset\']\n\n\ndef load_mpii_pose_dataset(path=\'data\', is_16_pos_only=False):\n    """"""Load MPII Human Pose Dataset.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to.\n    is_16_pos_only : boolean\n        If True, only return the peoples contain 16 pose keypoints. (Usually be used for single person pose estimation)\n\n    Returns\n    ----------\n    img_train_list : list of str\n        The image directories of training data.\n    ann_train_list : list of dict\n        The annotations of training data.\n    img_test_list : list of str\n        The image directories of testing data.\n    ann_test_list : list of dict\n        The annotations of testing data.\n\n    Examples\n    --------\n    >>> import pprint\n    >>> import tensorlayer as tl\n    >>> img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()\n    >>> image = tl.vis.read_image(img_train_list[0])\n    >>> tl.vis.draw_mpii_pose_to_image(image, ann_train_list[0], \'image.png\')\n    >>> pprint.pprint(ann_train_list[0])\n\n    References\n    -----------\n    - `MPII Human Pose Dataset. CVPR 14 <http://human-pose.mpi-inf.mpg.de>`__\n    - `MPII Human Pose Models. CVPR 16 <http://pose.mpi-inf.mpg.de>`__\n    - `MPII Human Shape, Poselet Conditioned Pictorial Structures and etc <http://pose.mpi-inf.mpg.de/#related>`__\n    - `MPII Keyponts and ID <http://human-pose.mpi-inf.mpg.de/#download>`__\n    """"""\n    path = os.path.join(path, \'mpii_human_pose\')\n    logging.info(""Load or Download MPII Human Pose > {}"".format(path))\n\n    # annotation\n    url = ""http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/""\n    tar_filename = ""mpii_human_pose_v1_u12_2.zip""\n    extracted_filename = ""mpii_human_pose_v1_u12_2""\n    if folder_exists(os.path.join(path, extracted_filename)) is False:\n        logging.info(""[MPII] (annotation) {} is nonexistent in {}"".format(extracted_filename, path))\n        maybe_download_and_extract(tar_filename, path, url, extract=True)\n        del_file(os.path.join(path, tar_filename))\n\n    # images\n    url = ""http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/""\n    tar_filename = ""mpii_human_pose_v1.tar.gz""\n    extracted_filename2 = ""images""\n    if folder_exists(os.path.join(path, extracted_filename2)) is False:\n        logging.info(""[MPII] (images) {} is nonexistent in {}"".format(extracted_filename, path))\n        maybe_download_and_extract(tar_filename, path, url, extract=True)\n        del_file(os.path.join(path, tar_filename))\n\n    # parse annotation, format see http://human-pose.mpi-inf.mpg.de/#download\n    import scipy.io as sio\n    logging.info(""reading annotations from mat file ..."")\n    # mat = sio.loadmat(os.path.join(path, extracted_filename, ""mpii_human_pose_v1_u12_1.mat""))\n\n    # def fix_wrong_joints(joint):    # https://github.com/mitmul/deeppose/blob/master/datasets/mpii_dataset.py\n    #     if \'12\' in joint and \'13\' in joint and \'2\' in joint and \'3\' in joint:\n    #         if ((joint[\'12\'][0] < joint[\'13\'][0]) and\n    #                 (joint[\'3\'][0] < joint[\'2\'][0])):\n    #             joint[\'2\'], joint[\'3\'] = joint[\'3\'], joint[\'2\']\n    #         if ((joint[\'12\'][0] > joint[\'13\'][0]) and\n    #                 (joint[\'3\'][0] > joint[\'2\'][0])):\n    #             joint[\'2\'], joint[\'3\'] = joint[\'3\'], joint[\'2\']\n    #     return joint\n\n    ann_train_list = []\n    ann_test_list = []\n    img_train_list = []\n    img_test_list = []\n\n    def save_joints():\n        # joint_data_fn = os.path.join(path, \'data.json\')\n        # fp = open(joint_data_fn, \'w\')\n        mat = sio.loadmat(os.path.join(path, extracted_filename, ""mpii_human_pose_v1_u12_1.mat""))\n\n        for _, (anno, train_flag) in enumerate(  # all images\n                zip(mat[\'RELEASE\'][\'annolist\'][0, 0][0], mat[\'RELEASE\'][\'img_train\'][0, 0][0])):\n\n            img_fn = anno[\'image\'][\'name\'][0, 0][0]\n            train_flag = int(train_flag)\n\n            # print(i, img_fn, train_flag) # DEBUG print all images\n\n            if train_flag:\n                img_train_list.append(img_fn)\n                ann_train_list.append([])\n            else:\n                img_test_list.append(img_fn)\n                ann_test_list.append([])\n\n            head_rect = []\n            if \'x1\' in str(anno[\'annorect\'].dtype):\n                head_rect = zip(\n                    [x1[0, 0] for x1 in anno[\'annorect\'][\'x1\'][0]], [y1[0, 0] for y1 in anno[\'annorect\'][\'y1\'][0]],\n                    [x2[0, 0] for x2 in anno[\'annorect\'][\'x2\'][0]], [y2[0, 0] for y2 in anno[\'annorect\'][\'y2\'][0]]\n                )\n            else:\n                head_rect = []  # TODO\n\n            if \'annopoints\' in str(anno[\'annorect\'].dtype):\n                annopoints = anno[\'annorect\'][\'annopoints\'][0]\n                head_x1s = anno[\'annorect\'][\'x1\'][0]\n                head_y1s = anno[\'annorect\'][\'y1\'][0]\n                head_x2s = anno[\'annorect\'][\'x2\'][0]\n                head_y2s = anno[\'annorect\'][\'y2\'][0]\n\n                for annopoint, head_x1, head_y1, head_x2, head_y2 in zip(annopoints, head_x1s, head_y1s, head_x2s,\n                                                                         head_y2s):\n                    # if annopoint != []:\n                    # if len(annopoint) != 0:\n                    if annopoint.size:\n                        head_rect = [\n                            float(head_x1[0, 0]),\n                            float(head_y1[0, 0]),\n                            float(head_x2[0, 0]),\n                            float(head_y2[0, 0])\n                        ]\n\n                        # joint coordinates\n                        annopoint = annopoint[\'point\'][0, 0]\n                        j_id = [str(j_i[0, 0]) for j_i in annopoint[\'id\'][0]]\n                        x = [x[0, 0] for x in annopoint[\'x\'][0]]\n                        y = [y[0, 0] for y in annopoint[\'y\'][0]]\n                        joint_pos = {}\n                        for _j_id, (_x, _y) in zip(j_id, zip(x, y)):\n                            joint_pos[int(_j_id)] = [float(_x), float(_y)]\n                        # joint_pos = fix_wrong_joints(joint_pos)\n\n                        # visibility list\n                        if \'is_visible\' in str(annopoint.dtype):\n                            vis = [v[0] if v.size > 0 else [0] for v in annopoint[\'is_visible\'][0]]\n                            vis = dict([(k, int(v[0])) if len(v) > 0 else v for k, v in zip(j_id, vis)])\n                        else:\n                            vis = None\n\n                        # if len(joint_pos) == 16:\n                        if ((is_16_pos_only ==True) and (len(joint_pos) == 16)) or (is_16_pos_only == False):\n                            # only use image with 16 key points / or use all\n                            data = {\n                                \'filename\': img_fn,\n                                \'train\': train_flag,\n                                \'head_rect\': head_rect,\n                                \'is_visible\': vis,\n                                \'joint_pos\': joint_pos\n                            }\n                            # print(json.dumps(data), file=fp)  # py3\n                            if train_flag:\n                                ann_train_list[-1].append(data)\n                            else:\n                                ann_test_list[-1].append(data)\n\n    # def write_line(datum, fp):\n    #     joints = sorted([[int(k), v] for k, v in datum[\'joint_pos\'].items()])\n    #     joints = np.array([j for i, j in joints]).flatten()\n    #\n    #     out = [datum[\'filename\']]\n    #     out.extend(joints)\n    #     out = [str(o) for o in out]\n    #     out = \',\'.join(out)\n    #\n    #     print(out, file=fp)\n\n    # def split_train_test():\n    #     # fp_test = open(\'data/mpii/test_joints.csv\', \'w\')\n    #     fp_test = open(os.path.join(path, \'test_joints.csv\'), \'w\')\n    #     # fp_train = open(\'data/mpii/train_joints.csv\', \'w\')\n    #     fp_train = open(os.path.join(path, \'train_joints.csv\'), \'w\')\n    #     # all_data = open(\'data/mpii/data.json\').readlines()\n    #     all_data = open(os.path.join(path, \'data.json\')).readlines()\n    #     N = len(all_data)\n    #     N_test = int(N * 0.1)\n    #     N_train = N - N_test\n    #\n    #     print(\'N:{}\'.format(N))\n    #     print(\'N_train:{}\'.format(N_train))\n    #     print(\'N_test:{}\'.format(N_test))\n    #\n    #     np.random.seed(1701)\n    #     perm = np.random.permutation(N)\n    #     test_indices = perm[:N_test]\n    #     train_indices = perm[N_test:]\n    #\n    #     print(\'train_indices:{}\'.format(len(train_indices)))\n    #     print(\'test_indices:{}\'.format(len(test_indices)))\n    #\n    #     for i in train_indices:\n    #         datum = json.loads(all_data[i].strip())\n    #         write_line(datum, fp_train)\n    #\n    #     for i in test_indices:\n    #         datum = json.loads(all_data[i].strip())\n    #         write_line(datum, fp_test)\n\n    save_joints()\n    # split_train_test()  #\n\n    ## read images dir\n    logging.info(""reading images list ..."")\n    img_dir = os.path.join(path, extracted_filename2)\n    _img_list = load_file_list(path=os.path.join(path, extracted_filename2), regx=\'\\\\.jpg\', printable=False)\n    # ann_list = json.load(open(os.path.join(path, \'data.json\')))\n    for i, im in enumerate(img_train_list):\n        if im not in _img_list:\n            print(\'missing training image {} in {} (remove from img(ann)_train_list)\'.format(im, img_dir))\n            # img_train_list.remove(im)\n            del img_train_list[i]\n            del ann_train_list[i]\n    for i, im in enumerate(img_test_list):\n        if im not in _img_list:\n            print(\'missing testing image {} in {} (remove from img(ann)_test_list)\'.format(im, img_dir))\n            # img_test_list.remove(im)\n            del img_train_list[i]\n            del ann_train_list[i]\n\n    ## check annotation and images\n    n_train_images = len(img_train_list)\n    n_test_images = len(img_test_list)\n    n_images = n_train_images + n_test_images\n    logging.info(""n_images: {} n_train_images: {} n_test_images: {}"".format(n_images, n_train_images, n_test_images))\n    n_train_ann = len(ann_train_list)\n    n_test_ann = len(ann_test_list)\n    n_ann = n_train_ann + n_test_ann\n    logging.info(""n_ann: {} n_train_ann: {} n_test_ann: {}"".format(n_ann, n_train_ann, n_test_ann))\n    n_train_people = len(sum(ann_train_list, []))\n    n_test_people = len(sum(ann_test_list, []))\n    n_people = n_train_people + n_test_people\n    logging.info(""n_people: {} n_train_people: {} n_test_people: {}"".format(n_people, n_train_people, n_test_people))\n    # add path to all image file name\n    for i, value in enumerate(img_train_list):\n        img_train_list[i] = os.path.join(img_dir, value)\n    for i, value in enumerate(img_test_list):\n        img_test_list[i] = os.path.join(img_dir, value)\n    return img_train_list, ann_train_list, img_test_list, ann_test_list\n'"
tensorlayer/files/dataset_loaders/nietzsche_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import maybe_download_and_extract\n\n__all__ = [\'load_nietzsche_dataset\']\n\n\ndef load_nietzsche_dataset(path=\'data\'):\n    """"""Load Nietzsche dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/nietzsche/``.\n\n    Returns\n    --------\n    str\n        The content.\n\n    Examples\n    --------\n    >>> see tutorial_generate_text.py\n    >>> words = tl.files.load_nietzsche_dataset()\n    >>> words = basic_clean_str(words)\n    >>> words = words.split()\n\n    """"""\n    logging.info(""Load or Download nietzsche dataset > {}"".format(path))\n    path = os.path.join(path, \'nietzsche\')\n\n    filename = ""nietzsche.txt""\n    url = \'https://s3.amazonaws.com/text-datasets/\'\n    filepath = maybe_download_and_extract(filename, path, url)\n\n    with open(filepath, ""r"") as f:\n        words = f.read()\n        return words\n'"
tensorlayer/files/dataset_loaders/ptb_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nfrom tensorlayer import logging, nlp\nfrom tensorlayer.files.utils import maybe_download_and_extract\n\n__all__ = [\'load_ptb_dataset\']\n\n\ndef load_ptb_dataset(path=\'data\'):\n    """"""Load Penn TreeBank (PTB) dataset.\n\n    It is used in many LANGUAGE MODELING papers,\n    including ""Empirical Evaluation and Combination of Advanced Language\n    Modeling Techniques"", ""Recurrent Neural Network Regularization"".\n    It consists of 929k training words, 73k validation words, and 82k test\n    words. It has 10k words in its vocabulary.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/ptb/``.\n\n    Returns\n    --------\n    train_data, valid_data, test_data : list of int\n        The training, validating and testing data in integer format.\n    vocab_size : int\n        The vocabulary size.\n\n    Examples\n    --------\n    >>> train_data, valid_data, test_data, vocab_size = tl.files.load_ptb_dataset()\n\n    References\n    ---------------\n    - ``tensorflow.models.rnn.ptb import reader``\n    - `Manual download <http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz>`__\n\n    Notes\n    ------\n    - If you want to get the raw data, see the source code.\n\n    """"""\n    path = os.path.join(path, \'ptb\')\n    logging.info(""Load or Download Penn TreeBank (PTB) dataset > {}"".format(path))\n\n    #Maybe dowload and uncompress tar, or load exsisting files\n    filename = \'simple-examples.tgz\'\n    url = \'http://www.fit.vutbr.cz/~imikolov/rnnlm/\'\n    maybe_download_and_extract(filename, path, url, extract=True)\n\n    data_path = os.path.join(path, \'simple-examples\', \'data\')\n    train_path = os.path.join(data_path, ""ptb.train.txt"")\n    valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n    test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n    word_to_id = nlp.build_vocab(nlp.read_words(train_path))\n\n    train_data = nlp.words_to_word_ids(nlp.read_words(train_path), word_to_id)\n    valid_data = nlp.words_to_word_ids(nlp.read_words(valid_path), word_to_id)\n    test_data = nlp.words_to_word_ids(nlp.read_words(test_path), word_to_id)\n    vocab_size = len(word_to_id)\n\n    # logging.info(nlp.read_words(train_path)) # ... \'according\', \'to\', \'mr.\', \'<unk>\', \'<eos>\']\n    # logging.info(train_data)                 # ...  214,         5,    23,    1,       2]\n    # logging.info(word_to_id)                 # ... \'beyond\': 1295, \'anti-nuclear\': 9599, \'trouble\': 1520, \'<eos>\': 2 ... }\n    # logging.info(vocabulary)                 # 10000\n    # exit()\n    return train_data, valid_data, test_data, vocab_size\n'"
tensorlayer/files/dataset_loaders/voc_dataset.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nimport tensorflow as tf\n\nfrom tensorlayer import logging, utils\nfrom tensorlayer.files.utils import (del_file, del_folder, folder_exists, load_file_list, maybe_download_and_extract)\n\n__all__ = [\'load_voc_dataset\']\n\n\ndef load_voc_dataset(path=\'data\', dataset=\'2012\', contain_classes_in_person=False):\n    """"""Pascal VOC 2007/2012 Dataset.\n\n    It has 20 objects:\n    aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, sofa, train, tvmonitor\n    and additional 3 classes : head, hand, foot for person.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/VOC``.\n    dataset : str\n        The VOC dataset version, `2012`, `2007`, `2007test` or `2012test`. We usually train model on `2007+2012` and test it on `2007test`.\n    contain_classes_in_person : boolean\n        Whether include head, hand and foot annotation, default is False.\n\n    Returns\n    ---------\n    imgs_file_list : list of str\n        Full paths of all images.\n    imgs_semseg_file_list : list of str\n        Full paths of all maps for semantic segmentation. Note that not all images have this map!\n    imgs_insseg_file_list : list of str\n        Full paths of all maps for instance segmentation. Note that not all images have this map!\n    imgs_ann_file_list : list of str\n        Full paths of all annotations for bounding box and object class, all images have this annotations.\n    classes : list of str\n        Classes in order.\n    classes_in_person : list of str\n        Classes in person.\n    classes_dict : dictionary\n        Class label to integer.\n    n_objs_list : list of int\n        Number of objects in all images in ``imgs_file_list`` in order.\n    objs_info_list : list of str\n        Darknet format for the annotation of all images in ``imgs_file_list`` in order. ``[class_id x_centre y_centre width height]`` in ratio format.\n    objs_info_dicts : dictionary\n        The annotation of all images in ``imgs_file_list``, ``{imgs_file_list : dictionary for annotation}``,\n        format from `TensorFlow/Models/object-detection <https://github.com/tensorflow/models/blob/master/object_detection/create_pascal_tf_record.py>`__.\n\n    Examples\n    ----------\n    >>> imgs_file_list, imgs_semseg_file_list, imgs_insseg_file_list, imgs_ann_file_list,\n    >>>     classes, classes_in_person, classes_dict,\n    >>>     n_objs_list, objs_info_list, objs_info_dicts = tl.files.load_voc_dataset(dataset=""2012"", contain_classes_in_person=False)\n    >>> idx = 26\n    >>> print(classes)\n    [\'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\', \'pottedplant\', \'sheep\', \'sofa\', \'train\', \'tvmonitor\']\n    >>> print(classes_dict)\n    {\'sheep\': 16, \'horse\': 12, \'bicycle\': 1, \'bottle\': 4, \'cow\': 9, \'sofa\': 17, \'car\': 6, \'dog\': 11, \'cat\': 7, \'person\': 14, \'train\': 18, \'diningtable\': 10, \'aeroplane\': 0, \'bus\': 5, \'pottedplant\': 15, \'tvmonitor\': 19, \'chair\': 8, \'bird\': 2, \'boat\': 3, \'motorbike\': 13}\n    >>> print(imgs_file_list[idx])\n    data/VOC/VOC2012/JPEGImages/2007_000423.jpg\n    >>> print(n_objs_list[idx])\n    2\n    >>> print(imgs_ann_file_list[idx])\n    data/VOC/VOC2012/Annotations/2007_000423.xml\n    >>> print(objs_info_list[idx])\n    14 0.173 0.461333333333 0.142 0.496\n    14 0.828 0.542666666667 0.188 0.594666666667\n    >>> ann = tl.prepro.parse_darknet_ann_str_to_list(objs_info_list[idx])\n    >>> print(ann)\n    [[14, 0.173, 0.461333333333, 0.142, 0.496], [14, 0.828, 0.542666666667, 0.188, 0.594666666667]]\n    >>> c, b = tl.prepro.parse_darknet_ann_list_to_cls_box(ann)\n    >>> print(c, b)\n    [14, 14] [[0.173, 0.461333333333, 0.142, 0.496], [0.828, 0.542666666667, 0.188, 0.594666666667]]\n\n    References\n    -------------\n    - `Pascal VOC2012 Website <https://pjreddie.com/projects/pascal-voc-dataset-mirror/>`__.\n    - `Pascal VOC2007 Website <https://pjreddie.com/projects/pascal-voc-dataset-mirror/>`__.\n\n    """"""\n    try:\n        import lxml.etree as etree\n    except ImportError as e:\n        print(e)\n        raise ImportError(""Module lxml not found. Please install lxml via pip or other package managers."")\n\n    path = os.path.join(path, \'VOC\')\n\n    def _recursive_parse_xml_to_dict(xml):\n        """"""Recursively parses XML contents to python dict.\n\n        We assume that `object` tags are the only ones that can appear\n        multiple times at the same level of a tree.\n\n        Args:\n            xml: xml tree obtained by parsing XML file contents using lxml.etree\n\n        Returns:\n            Python dictionary holding XML contents.\n\n        """"""\n        if xml is not None:\n            return {xml.tag: xml.text}\n        result = {}\n        for child in xml:\n            child_result = _recursive_parse_xml_to_dict(child)\n            if child.tag != \'object\':\n                result[child.tag] = child_result[child.tag]\n            else:\n                if child.tag not in result:\n                    result[child.tag] = []\n                result[child.tag].append(child_result[child.tag])\n        return {xml.tag: result}\n\n    import xml.etree.ElementTree as ET\n\n    if dataset == ""2012"":\n        url = ""http://pjreddie.com/media/files/""\n        tar_filename = ""VOCtrainval_11-May-2012.tar""\n        extracted_filename = ""VOC2012""  #""VOCdevkit/VOC2012""\n        logging.info(""    [============= VOC 2012 =============]"")\n    elif dataset == ""2012test"":\n        extracted_filename = ""VOC2012test""  #""VOCdevkit/VOC2012""\n        logging.info(""    [============= VOC 2012 Test Set =============]"")\n        logging.info(\n            ""    \\nAuthor: 2012test only have person annotation, so 2007test is highly recommended for testing !\\n""\n        )\n        import time\n        time.sleep(3)\n        if os.path.isdir(os.path.join(path, extracted_filename)) is False:\n            logging.info(""For VOC 2012 Test data - online registration required"")\n            logging.info(\n                "" Please download VOC2012test.tar from:  \\n register: http://host.robots.ox.ac.uk:8080 \\n voc2012 : http://host.robots.ox.ac.uk:8080/eval/challenges/voc2012/ \\ndownload: http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar""\n            )\n            logging.info("" unzip VOC2012test.tar,rename the folder to VOC2012test and put it into %s"" % path)\n            exit()\n        # # http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar\n        # url = ""http://host.robots.ox.ac.uk:8080/eval/downloads/""\n        # tar_filename = ""VOC2012test.tar""\n    elif dataset == ""2007"":\n        url = ""http://pjreddie.com/media/files/""\n        tar_filename = ""VOCtrainval_06-Nov-2007.tar""\n        extracted_filename = ""VOC2007""\n        logging.info(""    [============= VOC 2007 =============]"")\n    elif dataset == ""2007test"":\n        # http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html#testdata\n        # http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n        url = ""http://pjreddie.com/media/files/""\n        tar_filename = ""VOCtest_06-Nov-2007.tar""\n        extracted_filename = ""VOC2007test""\n        logging.info(""    [============= VOC 2007 Test Set =============]"")\n    else:\n        raise Exception(""Please set the dataset aug to 2012, 2012test or 2007."")\n\n    # download dataset\n    if dataset != ""2012test"":\n        from sys import platform as _platform\n        if folder_exists(os.path.join(path, extracted_filename)) is False:\n            logging.info(""[VOC] {} is nonexistent in {}"".format(extracted_filename, path))\n            maybe_download_and_extract(tar_filename, path, url, extract=True)\n            del_file(os.path.join(path, tar_filename))\n            if dataset == ""2012"":\n                if _platform == ""win32"":\n                    os.system(""move {}\\VOCdevkit\\VOC2012 {}\\VOC2012"".format(path, path))\n                else:\n                    os.system(""mv {}/VOCdevkit/VOC2012 {}/VOC2012"".format(path, path))\n            elif dataset == ""2007"":\n                if _platform == ""win32"":\n                    os.system(""move {}\\VOCdevkit\\VOC2007 {}\\VOC2007"".format(path, path))\n                else:\n                    os.system(""mv {}/VOCdevkit/VOC2007 {}/VOC2007"".format(path, path))\n            elif dataset == ""2007test"":\n                if _platform == ""win32"":\n                    os.system(""move {}\\VOCdevkit\\VOC2007 {}\\VOC2007test"".format(path, path))\n                else:\n                    os.system(""mv {}/VOCdevkit/VOC2007 {}/VOC2007test"".format(path, path))\n            del_folder(os.path.join(path, \'VOCdevkit\'))\n    # object classes(labels)  NOTE: YOU CAN CUSTOMIZE THIS LIST\n    classes = [\n        ""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"",\n        ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""\n    ]\n    if contain_classes_in_person:\n        classes_in_person = [""head"", ""hand"", ""foot""]\n    else:\n        classes_in_person = []\n\n    classes += classes_in_person  # use extra 3 classes for person\n\n    classes_dict = utils.list_string_to_dict(classes)\n    logging.info(""[VOC] object classes {}"".format(classes_dict))\n\n    # 1. image path list\n    # folder_imgs = path+""/""+extracted_filename+""/JPEGImages/""\n    folder_imgs = os.path.join(path, extracted_filename, ""JPEGImages"")\n    imgs_file_list = load_file_list(path=folder_imgs, regx=\'\\\\.jpg\', printable=False)\n    logging.info(""[VOC] {} images found"".format(len(imgs_file_list)))\n\n    imgs_file_list.sort(\n        key=lambda s: int(s.replace(\'.\', \' \').replace(\'_\', \'\').split(\' \')[-2])\n    )  # 2007_000027.jpg --> 2007000027\n\n    imgs_file_list = [os.path.join(folder_imgs, s) for s in imgs_file_list]\n    # logging.info(\'IM\',imgs_file_list[0::3333], imgs_file_list[-1])\n    if dataset != ""2012test"":\n        ##======== 2. semantic segmentation maps path list\n        # folder_semseg = path+""/""+extracted_filename+""/SegmentationClass/""\n        folder_semseg = os.path.join(path, extracted_filename, ""SegmentationClass"")\n        imgs_semseg_file_list = load_file_list(path=folder_semseg, regx=\'\\\\.png\', printable=False)\n        logging.info(""[VOC] {} maps for semantic segmentation found"".format(len(imgs_semseg_file_list)))\n        imgs_semseg_file_list.sort(\n            key=lambda s: int(s.replace(\'.\', \' \').replace(\'_\', \'\').split(\' \')[-2])\n        )  # 2007_000032.png --> 2007000032\n        imgs_semseg_file_list = [os.path.join(folder_semseg, s) for s in imgs_semseg_file_list]\n        # logging.info(\'Semantic Seg IM\',imgs_semseg_file_list[0::333], imgs_semseg_file_list[-1])\n        ##======== 3. instance segmentation maps path list\n        # folder_insseg = path+""/""+extracted_filename+""/SegmentationObject/""\n        folder_insseg = os.path.join(path, extracted_filename, ""SegmentationObject"")\n        imgs_insseg_file_list = load_file_list(path=folder_insseg, regx=\'\\\\.png\', printable=False)\n        logging.info(""[VOC] {} maps for instance segmentation found"".format(len(imgs_semseg_file_list)))\n        imgs_insseg_file_list.sort(\n            key=lambda s: int(s.replace(\'.\', \' \').replace(\'_\', \'\').split(\' \')[-2])\n        )  # 2007_000032.png --> 2007000032\n        imgs_insseg_file_list = [os.path.join(folder_insseg, s) for s in imgs_insseg_file_list]\n        # logging.info(\'Instance Seg IM\',imgs_insseg_file_list[0::333], imgs_insseg_file_list[-1])\n    else:\n        imgs_semseg_file_list = []\n        imgs_insseg_file_list = []\n    # 4. annotations for bounding box and object class\n    # folder_ann = path+""/""+extracted_filename+""/Annotations/""\n    folder_ann = os.path.join(path, extracted_filename, ""Annotations"")\n    imgs_ann_file_list = load_file_list(path=folder_ann, regx=\'\\\\.xml\', printable=False)\n    logging.info(\n        ""[VOC] {} XML annotation files for bounding box and object class found"".format(len(imgs_ann_file_list))\n    )\n    imgs_ann_file_list.sort(\n        key=lambda s: int(s.replace(\'.\', \' \').replace(\'_\', \'\').split(\' \')[-2])\n    )  # 2007_000027.xml --> 2007000027\n    imgs_ann_file_list = [os.path.join(folder_ann, s) for s in imgs_ann_file_list]\n    # logging.info(\'ANN\',imgs_ann_file_list[0::3333], imgs_ann_file_list[-1])\n\n    if dataset == ""2012test"":  # remove unused images in JPEG folder\n        imgs_file_list_new = []\n        for ann in imgs_ann_file_list:\n            ann = os.path.split(ann)[-1].split(\'.\')[0]\n            for im in imgs_file_list:\n                if ann in im:\n                    imgs_file_list_new.append(im)\n                    break\n        imgs_file_list = imgs_file_list_new\n        logging.info(""[VOC] keep %d images"" % len(imgs_file_list_new))\n\n    # parse XML annotations\n    def convert(size, box):\n        dw = 1. / size[0]\n        dh = 1. / size[1]\n        x = (box[0] + box[1]) / 2.0\n        y = (box[2] + box[3]) / 2.0\n        w = box[1] - box[0]\n        h = box[3] - box[2]\n        x = x * dw\n        w = w * dw\n        y = y * dh\n        h = h * dh\n        return x, y, w, h\n\n    def convert_annotation(file_name):\n        """"""Given VOC2012 XML Annotations, returns number of objects and info.""""""\n        in_file = open(file_name)\n        out_file = """"\n        tree = ET.parse(in_file)\n        root = tree.getroot()\n        size = root.find(\'size\')\n        w = int(size.find(\'width\').text)\n        h = int(size.find(\'height\').text)\n        n_objs = 0\n\n        for obj in root.iter(\'object\'):\n            if dataset != ""2012test"":\n                difficult = obj.find(\'difficult\').text\n                cls = obj.find(\'name\').text\n                if cls not in classes or int(difficult) == 1:\n                    continue\n            else:\n                cls = obj.find(\'name\').text\n                if cls not in classes:\n                    continue\n            cls_id = classes.index(cls)\n            xmlbox = obj.find(\'bndbox\')\n            b = (\n                float(xmlbox.find(\'xmin\').text), float(xmlbox.find(\'xmax\').text), float(xmlbox.find(\'ymin\').text),\n                float(xmlbox.find(\'ymax\').text)\n            )\n            bb = convert((w, h), b)\n\n            out_file += str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\'\n            n_objs += 1\n            if cls in ""person"":\n                for part in obj.iter(\'part\'):\n                    cls = part.find(\'name\').text\n                    if cls not in classes_in_person:\n                        continue\n                    cls_id = classes.index(cls)\n                    xmlbox = part.find(\'bndbox\')\n                    b = (\n                        float(xmlbox.find(\'xmin\').text), float(xmlbox.find(\'xmax\').text),\n                        float(xmlbox.find(\'ymin\').text), float(xmlbox.find(\'ymax\').text)\n                    )\n                    bb = convert((w, h), b)\n                    # out_file.write(str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\')\n                    out_file += str(cls_id) + "" "" + "" "".join([str(a) for a in bb]) + \'\\n\'\n                    n_objs += 1\n        in_file.close()\n        return n_objs, out_file\n\n    logging.info(""[VOC] Parsing xml annotations files"")\n    n_objs_list = []\n    objs_info_list = []  # Darknet Format list of string\n    objs_info_dicts = {}\n    for idx, ann_file in enumerate(imgs_ann_file_list):\n        n_objs, objs_info = convert_annotation(ann_file)\n        n_objs_list.append(n_objs)\n        objs_info_list.append(objs_info)\n        with tf.io.gfile.GFile(ann_file, \'r\') as fid:\n            xml_str = fid.read()\n        xml = etree.fromstring(xml_str)\n        data = _recursive_parse_xml_to_dict(xml)[\'annotation\']\n        objs_info_dicts.update({imgs_file_list[idx]: data})\n\n    return imgs_file_list, imgs_semseg_file_list, imgs_insseg_file_list, imgs_ann_file_list, classes, classes_in_person, classes_dict, n_objs_list, objs_info_list, objs_info_dicts\n'"
tensorlayer/files/dataset_loaders/wmt_en_fr_dataset.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport gzip\nimport os\nimport tarfile\n\nfrom tensorflow.python.platform import gfile\n\nfrom tensorlayer import logging\nfrom tensorlayer.files.utils import maybe_download_and_extract\n\n__all__ = [\'load_wmt_en_fr_dataset\']\n\n\ndef load_wmt_en_fr_dataset(path=\'data\'):\n    """"""Load WMT\'15 English-to-French translation dataset.\n\n    It will download the data from the WMT\'15 Website (10^9-French-English corpus), and the 2013 news test from the same site as development set.\n    Returns the directories of training data and test data.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/wmt_en_fr/``.\n\n    References\n    ----------\n    - Code modified from /tensorflow/models/rnn/translation/data_utils.py\n\n    Notes\n    -----\n    Usually, it will take a long time to download this dataset.\n\n    """"""\n    path = os.path.join(path, \'wmt_en_fr\')\n    # URLs for WMT data.\n    _WMT_ENFR_TRAIN_URL = ""http://www.statmt.org/wmt10/""\n    _WMT_ENFR_DEV_URL = ""http://www.statmt.org/wmt15/""\n\n    def gunzip_file(gz_path, new_path):\n        """"""Unzips from gz_path into new_path.""""""\n        logging.info(""Unpacking %s to %s"" % (gz_path, new_path))\n        with gzip.open(gz_path, ""rb"") as gz_file:\n            with open(new_path, ""wb"") as new_file:\n                for line in gz_file:\n                    new_file.write(line)\n\n    def get_wmt_enfr_train_set(path):\n        """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n        filename = ""training-giga-fren.tar""\n        maybe_download_and_extract(filename, path, _WMT_ENFR_TRAIN_URL, extract=True)\n        train_path = os.path.join(path, ""giga-fren.release2.fixed"")\n        gunzip_file(train_path + "".fr.gz"", train_path + "".fr"")\n        gunzip_file(train_path + "".en.gz"", train_path + "".en"")\n        return train_path\n\n    def get_wmt_enfr_dev_set(path):\n        """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n        filename = ""dev-v2.tgz""\n        dev_file = maybe_download_and_extract(filename, path, _WMT_ENFR_DEV_URL, extract=False)\n        dev_name = ""newstest2013""\n        dev_path = os.path.join(path, ""newstest2013"")\n        if not (gfile.Exists(dev_path + "".fr"") and gfile.Exists(dev_path + "".en"")):\n            logging.info(""Extracting tgz file %s"" % dev_file)\n            with tarfile.open(dev_file, ""r:gz"") as dev_tar:\n                fr_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".fr"")\n                en_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".en"")\n                fr_dev_file.name = dev_name + "".fr""  # Extract without ""dev/"" prefix.\n                en_dev_file.name = dev_name + "".en""\n                dev_tar.extract(fr_dev_file, path)\n                dev_tar.extract(en_dev_file, path)\n        return dev_path\n\n    logging.info(""Load or Download WMT English-to-French translation > {}"".format(path))\n\n    train_path = get_wmt_enfr_train_set(path)\n    dev_path = get_wmt_enfr_dev_set(path)\n\n    return train_path, dev_path\n'"
tensorlayer/layers/convolution/__init__.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nTensorLayer provides rich layer implementations trailed for\nvarious benchmarks and domain-specific problems. In addition, we also\nsupport transparent access to native TensorFlow parameters.\nFor example, we provide not only layers for local response normalization, but also\nlayers that allow user to apply ``tf.nn.lrn`` on ``network.outputs``.\nMore functions can be found in `TensorFlow API <https://www.tensorflow.org/versions/master/api_docs/index.html>`__.\n""""""\n\nfrom .binary_conv import *\nfrom .deformable_conv import *\nfrom .depthwise_conv import *\nfrom .dorefa_conv import *\nfrom .expert_conv import *\nfrom .expert_deconv import *\nfrom .group_conv import *\nfrom .quan_conv import *\nfrom .quan_conv_bn import *\nfrom .separable_conv import *\nfrom .simplified_conv import *\nfrom .simplified_deconv import *\nfrom .super_resolution import *\nfrom .ternary_conv import *\n\n__all__ = [\n\n    # simplified conv\n    \'Conv1d\',\n    \'Conv2d\',\n    \'Conv3d\',\n\n    # simplified deconv\n    \'DeConv2d\',\n    \'DeConv3d\',\n\n    # expert conv\n    \'Conv1dLayer\',\n    \'Conv2dLayer\',\n    \'Conv3dLayer\',\n\n    # expert conv\n    \'DeConv1dLayer\',\n    \'DeConv2dLayer\',\n    \'DeConv3dLayer\',\n\n    # atrous\n    # \'AtrousConv1dLayer\',\n    # \'AtrousConv2dLayer\',\n    # \'AtrousDeConv2d\',\n\n    # binary\n    \'BinaryConv2d\',\n\n    # deformable\n    \'DeformableConv2d\',\n\n    # depthwise\n    \'DepthwiseConv2d\',\n\n    # dorefa\n    \'DorefaConv2d\',\n\n    # group\n    \'GroupConv2d\',\n\n    # separable\n    \'SeparableConv1d\',\n    \'SeparableConv2d\',\n\n    # subpixel\n    \'SubpixelConv1d\',\n    \'SubpixelConv2d\',\n\n    # ternary\n    \'TernaryConv2d\',\n\n    #quan_conv\n    \'QuanConv2d\',\n    \'QuanConv2dWithBN\',\n]\n'"
tensorlayer/layers/convolution/binary_conv.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import quantize\n\n__all__ = [\'BinaryConv2d\']\n\n\nclass BinaryConv2d(Layer):\n    """"""\n    The :class:`BinaryConv2d` class is a 2D binary CNN layer, which weights are either -1 or 1 while inference.\n\n    Note that, the bias vector would not be binarized.\n\n    Parameters\n    ----------\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    strides : tuple of int\n        The sliding window strides of corresponding input dimensions.\n        It must be in the same order as the ``shape`` parameter.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inference.\n        TODO: support gemm\n    data_format : str\n        ""channels_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    dilation_rate : tuple of int\n        Specifying the dilation rate to use for dilated convolution.\n    W_init : initializer\n        The initializer for the the weight matrix.\n    b_init : initializer or None\n        The initializer for the the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 100, 100, 32], name=\'input\')\n    >>> binaryconv2d = tl.layers.QuanConv2d(\n    ...     n_filter=64, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, in_channels=32, name=\'binaryconv2d\'\n    ... )(net)\n    >>> print(binaryconv2d)\n    >>> output shape : (8, 50, 50, 64)\n\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        act=None,\n        padding=\'SAME\',\n        use_gemm=False,\n        data_format=""channels_last"",\n        dilation_rate=(1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'binary_cnn2d\',\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.use_gemm = use_gemm\n        self.data_format = data_format\n        self._dilation_rate = self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""BinaryConv2d %s: n_filter: %d filter_size: %s strides: %s pad: %s act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n        if use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        if len(self.strides) != 2:\n            raise ValueError(""len(strides) should be 2."")\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self._strides[0], self._strides[1], 1]\n            self._dilation_rate = [1, self._dilation_rate[0], self._dilation_rate[1], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self._strides[0], self._strides[1]]\n            self._dilation_rate = [1, 1, self._dilation_rate[0], self._dilation_rate[1]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (self.filter_size[0], self.filter_size[1], self.in_channels, self.n_filter)\n\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter, ), init=self.b_init)\n\n    def forward(self, inputs):\n\n        _W = quantize(self.W)\n\n        outputs = tf.nn.conv2d(\n            input=inputs, filters=_W, strides=self._strides, padding=self.padding, data_format=self.data_format,\n            dilations=self._dilation_rate, name=self.name\n        )\n\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n\n        return outputs\n'"
tensorlayer/layers/convolution/deformable_conv.py,73,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias, private_method\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'DeformableConv2d\',\n]\n\n\nclass DeformableConv2d(Layer):\n    """"""The :class:`DeformableConv2d` class is a 2D\n    `Deformable Convolutional Networks <https://arxiv.org/abs/1703.06211>`__.\n\n    Parameters\n    ----------\n    offset_layer : tf.Tensor\n        To predict the offset of convolution operations.\n        The shape is (batchsize, input height, input width, 2*(number of element in the convolution kernel))\n        e.g. if apply a 3*3 kernel, the number of the last dimension should be 18 (2*3*3)\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.InputLayer([5, 10, 10, 16], name=\'input\')\n    >>> offset1 = tl.layers.Conv2d(\n    ...     n_filter=18, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', name=\'offset1\'\n    ... )(net)\n    >>> deformconv1 = tl.layers.DeformableConv2d(\n    ...     offset_layer=offset1, n_filter=32, filter_size=(3, 3), name=\'deformable1\'\n    ... )(net)\n    >>> offset2 = tl.layers.Conv2d(\n    ...     n_filter=18, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', name=\'offset2\'\n    ... )(deformconv1)\n    >>> deformconv2 = tl.layers.DeformableConv2d(\n    ...     offset_layer=offset2, n_filter=64, filter_size=(3, 3), name=\'deformable2\'\n    ... )(deformconv1)\n\n    References\n    ----------\n    - The deformation operation was adapted from the implementation in `here <https://github.com/kastnerkyle/deform-conv>`__\n\n    Notes\n    -----\n    - The padding is fixed to \'SAME\'.\n    - The current implementation is not optimized for memory usgae. Please use it carefully.\n\n    """"""\n\n    # @deprecated_alias(layer=\'prev_layer\', end_support_version=1.9)  # TODO remove this line for the 1.9 release\n    def __init__(\n        self,\n        offset_layer=None,\n        # shape=(3, 3, 1, 100),\n        n_filter=32,\n        filter_size=(3, 3),\n        act=None,\n        padding=\'SAME\',\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'deformable_conv_2d\',\n    ):\n        super().__init__(name, act=act)\n\n        self.offset_layer = offset_layer\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.padding = padding\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        self.kernel_n = filter_size[0] * filter_size[1]\n        if self.offset_layer.get_shape()[-1] != 2 * self.kernel_n:\n            raise AssertionError(""offset.get_shape()[-1] is not equal to: %d"" % 2 * self.kernel_n)\n\n        logging.info(\n            ""DeformableConv2d %s: n_filter: %d, filter_size: %s act: %s"" % (\n                self.name, self.n_filter, str(self.filter_size\n                                             ), self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n        # try:\n        #     pre_channel = int(prev_layer.outputs.get_shape()[-1])\n        # except Exception:  # if pre_channel is ?, it happens when using Spatial Transformer Net\n        #     pre_channel = 1\n        #     logging.info(""[warnings] unknow input channels, set to 1"")\n        # shape = (filter_size[0], filter_size[1], pre_channel, n_filter)\n\n        # with tf.compat.v1.variable_scope(name):\n        #     offset = self.offset_layer # .outputs\n        #\n        #     # if offset.get_shape()[-1] != 2 * shape[0] * shape[1]:\n        #     #     raise AssertionError(""offset.get_shape()[-1] is not equal to: %d"" % 2 * shape[0] * shape[1])\n        #\n        #     # Grid initialisation\n        #     input_h = int(self.inputs.get_shape()[1])\n        #     input_w = int(self.inputs.get_shape()[2])\n        #     # kernel_n = shape[0] * shape[1]\n        #     initial_offsets = tf.stack(\n        #         tf.meshgrid(tf.range(shape[0]), tf.range(shape[1]), indexing=\'ij\')\n        #     )  # initial_offsets --> (kh, kw, 2)\n        #     initial_offsets = tf.reshape(initial_offsets, (-1, 2))  # initial_offsets --> (n, 2)\n        #     initial_offsets = tf.expand_dims(initial_offsets, 0)  # initial_offsets --> (1, n, 2)\n        #     initial_offsets = tf.expand_dims(initial_offsets, 0)  # initial_offsets --> (1, 1, n, 2)\n        #     initial_offsets = tf.tile(initial_offsets, [input_h, input_w, 1, 1])  # initial_offsets --> (h, w, n, 2)\n        #     initial_offsets = tf.cast(initial_offsets, \'float32\')\n        #     grid = tf.meshgrid(\n        #         tf.range(-int((shape[0] - 1) / 2.0), int(input_h - int((shape[0] - 1) / 2.0)), 1),\n        #         tf.range(-int((shape[1] - 1) / 2.0), int(input_w - int((shape[1] - 1) / 2.0)), 1), indexing=\'ij\'\n        #     )\n        #\n        #     grid = tf.stack(grid, axis=-1)\n        #     grid = tf.cast(grid, \'float32\')  # grid --> (h, w, 2)\n        #     grid = tf.expand_dims(grid, 2)  # grid --> (h, w, 1, 2)\n        #     grid = tf.tile(grid, [1, 1, self.kernel_n, 1])  # grid --> (h, w, n, 2)\n        #     grid_offset = grid + initial_offsets  # grid_offset --> (h, w, n, 2)\n        #\n        #     input_deform = self._tf_batch_map_offsets(self.inputs, offset, grid_offset)\n        #\n        #     # W = tf.compat.v1.get_variable(\n        #     #     name=\'W_deformableconv2d\', shape=[1, 1, shape[0] * shape[1], shape[-2], shape[-1]], initializer=W_init,\n        #     #     dtype=LayersConfig.tf_dtype,\n        #     # )\n        #\n        #     # _tensor = tf.nn.conv3d(input_deform, W, strides=[1, 1, 1, 1, 1], padding=\'VALID\', name=None)\n        #     # _tensor = tf.nn.conv3d(\n        #     #     input=input_deform,\n        #     #     filters=W,\n        #     #     strides=[1, 1, 1, 1, 1],\n        #     #     padding=\'VALID\',\n        #     #     name=None\n        #     # )\n        #\n        #     # if b_init:\n        #     #     b = tf.compat.v1.get_variable(\n        #     #         name=\'b_deformableconv2d\', shape=(shape[-1]), initializer=b_init, # dtype=LayersConfig.tf_dtype,\n        #     #     )\n        #     #\n        #     #     _tensor = tf.nn.bias_add(_tensor, b, name=\'bias_add\')\n        #\n        #     # self.outputs = tf.reshape(\n        #     #     tensor=self._apply_activation(_tensor),\n        #     #     shape=[tf.shape(input=self.inputs)[0], input_h, input_w, shape[-1]]\n        #     # )\n        #\n        # # self._add_layers(self.outputs)\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', padding={padding}\'\n        )\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n\n        self.in_channels = inputs_shape[-1]\n\n        self.input_h = int(inputs_shape[1])\n        self.input_w = int(inputs_shape[2])\n        initial_offsets = tf.stack(\n            tf.meshgrid(tf.range(self.filter_size[0]), tf.range(self.filter_size[1]), indexing=\'ij\')\n        )  # initial_offsets --> (kh, kw, 2)\n        initial_offsets = tf.reshape(initial_offsets, (-1, 2))  # initial_offsets --> (n, 2)\n        initial_offsets = tf.expand_dims(initial_offsets, 0)  # initial_offsets --> (1, n, 2)\n        initial_offsets = tf.expand_dims(initial_offsets, 0)  # initial_offsets --> (1, 1, n, 2)\n        initial_offsets = tf.tile(\n            initial_offsets, [self.input_h, self.input_w, 1, 1]\n        )  # initial_offsets --> (h, w, n, 2)\n        initial_offsets = tf.cast(initial_offsets, \'float32\')\n        grid = tf.meshgrid(\n            tf.range(\n                -int((self.filter_size[0] - 1) / 2.0), int(self.input_h - int((self.filter_size[0] - 1) / 2.0)), 1\n            ),\n            tf.range(\n                -int((self.filter_size[1] - 1) / 2.0), int(self.input_w - int((self.filter_size[1] - 1) / 2.0)), 1\n            ), indexing=\'ij\'\n        )\n\n        grid = tf.stack(grid, axis=-1)\n        grid = tf.cast(grid, \'float32\')  # grid --> (h, w, 2)\n        grid = tf.expand_dims(grid, 2)  # grid --> (h, w, 1, 2)\n        grid = tf.tile(grid, [1, 1, self.kernel_n, 1])  # grid --> (h, w, n, 2)\n        self.grid_offset = grid + initial_offsets  # grid_offset --> (h, w, n, 2)\n\n        self.filter_shape = (1, 1, self.kernel_n, self.in_channels, self.n_filter)\n\n        self.W = self._get_weights(""W_deformableconv2d"", shape=self.filter_shape, init=self.W_init)\n\n        if self.b_init:\n            self.b = self._get_weights(""b_deformableconv2d"", shape=(self.n_filter, ), init=self.b_init)\n\n    def forward(self, inputs):\n        # shape = (filter_size[0], filter_size[1], pre_channel, n_filter)\n        offset = self.offset_layer\n        grid_offset = self.grid_offset\n\n        input_deform = self._tf_batch_map_offsets(inputs, offset, grid_offset)\n        outputs = tf.nn.conv3d(input=input_deform, filters=self.W, strides=[1, 1, 1, 1, 1], padding=\'VALID\', name=None)\n        outputs = tf.reshape(tensor=outputs, shape=[outputs.get_shape()[0], self.input_h, self.input_w, self.n_filter])\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n    def _to_bc_h_w(self, x, x_shape):\n        """"""(b, h, w, c) -> (b*c, h, w)""""""\n        x = tf.transpose(a=x, perm=[0, 3, 1, 2])\n        x = tf.reshape(x, (-1, x_shape[1], x_shape[2]))\n        return x\n\n    def _to_b_h_w_n_c(self, x, x_shape):\n        """"""(b*c, h, w, n) -> (b, h, w, n, c)""""""\n        x = tf.reshape(x, (-1, x_shape[4], x_shape[1], x_shape[2], x_shape[3]))\n        x = tf.transpose(a=x, perm=[0, 2, 3, 4, 1])\n        return x\n\n    def tf_flatten(self, a):\n        """"""Flatten tensor""""""\n        return tf.reshape(a, [-1])\n\n    def _get_vals_by_coords(self, inputs, coords, idx, out_shape):\n        indices = tf.stack(\n            [idx, self.tf_flatten(coords[:, :, :, :, 0]),\n             self.tf_flatten(coords[:, :, :, :, 1])], axis=-1\n        )\n        vals = tf.gather_nd(inputs, indices)\n        vals = tf.reshape(vals, out_shape)\n        return vals\n\n    def _tf_repeat(self, a, repeats):\n        """"""Tensorflow version of np.repeat for 1D""""""\n        # https://github.com/tensorflow/tensorflow/issues/8521\n\n        if len(a.get_shape()) != 1:\n            raise AssertionError(""This is not a 1D Tensor"")\n\n        a = tf.expand_dims(a, -1)\n        a = tf.tile(a, [1, repeats])\n        a = self.tf_flatten(a)\n        return a\n\n    def _tf_batch_map_coordinates(self, inputs, coords):\n        """"""Batch version of tf_map_coordinates\n\n        Only supports 2D feature maps\n\n        Parameters\n        ----------\n        inputs : ``tf.Tensor``\n            shape = (b*c, h, w)\n        coords : ``tf.Tensor``\n            shape = (b*c, h, w, n, 2)\n\n        Returns\n        -------\n        ``tf.Tensor``\n            A Tensor with the shape as (b*c, h, w, n)\n\n        """"""\n        inputs_shape = inputs.get_shape()\n        coords_shape = coords.get_shape()\n        batch_channel = tf.shape(input=inputs)[0]\n        input_h = int(inputs_shape[1])\n        input_w = int(inputs_shape[2])\n        kernel_n = int(coords_shape[3])\n        n_coords = input_h * input_w * kernel_n\n\n        coords_lt = tf.cast(tf.floor(coords), \'int32\')\n        coords_rb = tf.cast(tf.math.ceil(coords), \'int32\')\n        coords_lb = tf.stack([coords_lt[:, :, :, :, 0], coords_rb[:, :, :, :, 1]], axis=-1)\n        coords_rt = tf.stack([coords_rb[:, :, :, :, 0], coords_lt[:, :, :, :, 1]], axis=-1)\n\n        idx = self._tf_repeat(tf.range(batch_channel), n_coords)\n\n        vals_lt = self._get_vals_by_coords(inputs, coords_lt, idx, (batch_channel, input_h, input_w, kernel_n))\n        vals_rb = self._get_vals_by_coords(inputs, coords_rb, idx, (batch_channel, input_h, input_w, kernel_n))\n        vals_lb = self._get_vals_by_coords(inputs, coords_lb, idx, (batch_channel, input_h, input_w, kernel_n))\n        vals_rt = self._get_vals_by_coords(inputs, coords_rt, idx, (batch_channel, input_h, input_w, kernel_n))\n\n        coords_offset_lt = coords - tf.cast(coords_lt, \'float32\')\n\n        vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, :, :, :, 0]\n        vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, :, :, :, 0]\n        mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, :, :, :, 1]\n\n        return mapped_vals\n\n    def _tf_batch_map_offsets(self, inputs, offsets, grid_offset):\n        """"""Batch map offsets into input\n\n        Parameters\n        ------------\n        inputs : ``tf.Tensor``\n            shape = (b, h, w, c)\n        offsets: ``tf.Tensor``\n            shape = (b, h, w, 2*n)\n        grid_offset: `tf.Tensor``\n            Offset grids shape = (h, w, n, 2)\n\n        Returns\n        -------\n        ``tf.Tensor``\n            A Tensor with the shape as (b, h, w, c)\n\n        """"""\n        inputs_shape = inputs.get_shape()\n        batch_size = tf.shape(input=inputs)[0]\n        kernel_n = int(int(offsets.get_shape()[3]) / 2)\n        input_h = inputs_shape[1]\n        input_w = inputs_shape[2]\n        channel = inputs_shape[3]\n\n        # inputs (b, h, w, c) --> (b*c, h, w)\n        inputs = self._to_bc_h_w(inputs, inputs_shape)\n\n        # offsets (b, h, w, 2*n) --> (b, h, w, n, 2)\n        offsets = tf.reshape(offsets, (batch_size, input_h, input_w, kernel_n, 2))\n        # offsets (b, h, w, n, 2) --> (b*c, h, w, n, 2)\n        # offsets = tf.tile(offsets, [channel, 1, 1, 1, 1])\n\n        coords = tf.expand_dims(grid_offset, 0)  # grid_offset --> (1, h, w, n, 2)\n        coords = tf.tile(coords, [batch_size, 1, 1, 1, 1]) + offsets  # grid_offset --> (b, h, w, n, 2)\n\n        # clip out of bound\n        coords = tf.stack(\n            [\n                tf.clip_by_value(coords[:, :, :, :, 0], 0.0, tf.cast(input_h - 1, \'float32\')),\n                tf.clip_by_value(coords[:, :, :, :, 1], 0.0, tf.cast(input_w - 1, \'float32\'))\n            ], axis=-1\n        )\n        coords = tf.tile(coords, [channel, 1, 1, 1, 1])\n\n        mapped_vals = self._tf_batch_map_coordinates(inputs, coords)\n        # (b*c, h, w, n) --> (b, h, w, n, c)\n        mapped_vals = self._to_b_h_w_n_c(mapped_vals, [batch_size, input_h, input_w, kernel_n, channel])\n\n        return mapped_vals\n'"
tensorlayer/layers/convolution/depthwise_conv.py,4,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\n    \'DepthwiseConv2d\',\n]\n\n\nclass DepthwiseConv2d(Layer):\n    """"""Separable/Depthwise Convolutional 2D layer, see `tf.nn.depthwise_conv2d <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/depthwise_conv2d>`__.\n\n    Input:\n        4-D Tensor (batch, height, width, in_channels).\n    Output:\n        4-D Tensor (batch, new height, new width, in_channels * depth_multiplier).\n\n    Parameters\n    ------------\n    filter_size : tuple of 2 int\n        The filter size (height, width).\n    strides : tuple of 2 int\n        The stride step (height, width).\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""channels_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    dilation_rate: tuple of 2 int\n        The dilation rate in which we sample input values across the height and width dimensions in atrous convolution. If it is greater than 1, then all values of strides must be 1.\n    depth_multiplier : int\n        The number of channels to expand to.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip bias.\n    in_channels : int\n        The number of in channels.\n    name : str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 200, 200, 32], name=\'input\')\n    >>> depthwiseconv2d = tl.layers.DepthwiseConv2d(\n    ...     filter_size=(3, 3), strides=(1, 1), dilation_rate=(2, 2), act=tf.nn.relu, depth_multiplier=2, name=\'depthwise\'\n    ... )(net)\n    >>> print(depthwiseconv2d)\n    >>> output shape : (8, 200, 200, 64)\n\n\n    References\n    -----------\n    - tflearn\'s `grouped_conv_2d <https://github.com/tflearn/tflearn/blob/3e0c3298ff508394f3ef191bcd7d732eb8860b2e/tflearn/layers/conv.py>`__\n    - keras\'s `separableconv2d <https://keras.io/layers/convolutional/#separableconv2d>`__\n\n    """"""\n\n    # https://zhuanlan.zhihu.com/p/31551004  https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/MobileNet.py\n    def __init__(\n        self,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        act=None,\n        padding=\'SAME\',\n        data_format=\'channels_last\',\n        dilation_rate=(1, 1),\n        depth_multiplier=1,\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'depthwise_conv2d\'\n    ):\n        super().__init__(name, act=act)\n        self.filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.dilation_rate = self._dilation_rate = dilation_rate\n        self.data_format = data_format\n        self.depth_multiplier = depth_multiplier\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""DepthwiseConv2d %s: filter_size: %s strides: %s pad: %s act: %s"" % (\n                self.name, str(filter_size), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(\n            classname=self.__class__.__name__, n_filter=self.in_channels * self.depth_multiplier, **self.__dict__\n        )\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self._strides[0], self._strides[1], 1]\n            self._dilation_rate = [1, self._dilation_rate[0], self._dilation_rate[1], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self._strides[0], self._strides[1]]\n            self._dilation_rate = [1, 1, self._dilation_rate[0], self._dilation_rate[1]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (self.filter_size[0], self.filter_size[1], self.in_channels, self.depth_multiplier)\n\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.in_channels * self.depth_multiplier), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.depthwise_conv2d(\n            input=inputs,\n            filter=self.W,\n            strides=self._strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilations=self.dilation_rate,\n            name=self.name,\n        )\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/convolution/dorefa_conv.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import cabs, quantize_active, quantize_weight\n\n__all__ = [\'DorefaConv2d\']\n\n\nclass DorefaConv2d(Layer):\n    """"""The :class:`DorefaConv2d` class is a 2D quantized convolutional layer, which weights are \'bitW\' bits and the output of the previous layer\n    are \'bitA\' bits while inferencing.\n\n    Note that, the bias vector would not be binarized.\n\n    Parameters\n    ----------\n    bitW : int\n        The bits of this layer\'s parameter\n    bitA : int\n        The bits of the output of previous layer\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    strides : tuple of int\n        The sliding window strides of corresponding input dimensions.\n        It must be in the same order as the ``shape`` parameter.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inferencing.\n        TODO: support gemm\n    data_format : str\n        ""channels_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    dilation_rate : tuple of int\n        Specifying the dilation rate to use for dilated convolution.\n    W_init : initializer\n        The initializer for the the weight matrix.\n    b_init : initializer or None\n        The initializer for the the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 12, 12, 32], name=\'input\')\n    >>> dorefaconv2d = tl.layers.QuanConv2d(\n    ...     n_filter=32, filter_size=(5, 5), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'dorefaconv2d\'\n    ... )(net)\n    >>> print(dorefaconv2d)\n    >>> output shape : (8, 12, 12, 32)\n\n    """"""\n\n    def __init__(\n        self,\n        bitW=1,\n        bitA=3,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        act=None,\n        padding=\'SAME\',\n        use_gemm=False,\n        data_format=""channels_last"",\n        dilation_rate=(1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'dorefa_cnn2d\',\n    ):\n        super().__init__(name, act=act)\n        self.bitW = bitW\n        self.bitA = bitA\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.use_gemm = use_gemm\n        self.data_format = data_format\n        self.dilation_rate = self._dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""DorefaConv2d %s: n_filter: %d filter_size: %s strides: %s pad: %s act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n        if self.use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        if len(self.strides) != 2:\n            raise ValueError(""len(strides) should be 2."")\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self._strides[0], self._strides[1], 1]\n            self._dilation_rate = [1, self._dilation_rate[0], self._dilation_rate[1], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self._strides[0], self._strides[1]]\n            self._dilation_rate = [1, 1, self._dilation_rate[0], self._dilation_rate[1]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (self.filter_size[0], self.filter_size[1], self.in_channels, self.n_filter)\n\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter, ), init=self.b_init)\n\n    def forward(self, inputs):\n\n        inputs = quantize_active(cabs(inputs), self.bitA)  # Do not remove\n\n        W_ = quantize_weight(self.W, self.bitW)\n\n        outputs = tf.nn.conv2d(\n            input=inputs, filters=W_, strides=self._strides, padding=self.padding, data_format=self.data_format,\n            dilations=self._dilation_rate, name=self.name\n        )\n\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n\n        return outputs\n'"
tensorlayer/layers/convolution/expert_conv.py,12,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\n    \'Conv1dLayer\',\n    \'Conv2dLayer\',\n    \'Conv3dLayer\',\n]\n\n\nclass Conv1dLayer(Layer):\n    """"""\n    The :class:`Conv1dLayer` class is a 1D CNN layer, see `tf.nn.conv1d <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv1d>`__.\n\n    Parameters\n    ----------\n    act : activation function\n        The activation function of this layer.\n    shape : tuple of int\n        The shape of the filters: (filter_length, in_channels, out_channels).\n    stride : int\n        The number of entries by which the filter is moved right at a step.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        \'NWC\' or \'NCW\', Default is \'NWC\' as it is a 1D CNN.\n    dilation_rate : int\n        Filter up-sampling/input down-sampling rate.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    name : None or str\n        A unique layer name\n\n    Notes\n    -----\n    - shape = [w, the number of output channel of previous layer, the number of output channels]\n    - the number of output channel of a layer is its last dimension.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 100, 1], name=\'input\')\n    >>> conv1d = tl.layers.Conv1dLayer(shape=(5, 1, 32), stride=2, b_init=None, name=\'conv1d_1\')\n    >>> print(conv1d)\n    >>> tensor = tl.layers.Conv1dLayer(shape=(5, 1, 32), stride=2, act=tf.nn.relu, name=\'conv1d_2\')(net)\n    >>> print(tensor)\n\n    """"""\n\n    def __init__(\n        self,\n        act=None,\n        shape=(5, 1, 5),\n        stride=1,\n        padding=\'SAME\',\n        data_format=\'NWC\',\n        dilation_rate=1,\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        name=None  # \'cnn1d_layer\',\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = shape[-1]\n        self.filter_size = shape[0]\n        self.shape = shape\n        self.stride = stride\n        self.dilation_rate = dilation_rate\n        self.padding = padding\n        self.data_format = data_format\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = shape[-2]\n\n        self.build(None)\n        self._built = True\n\n        logging.info(\n            ""Conv1dLayer %s: shape: %s stride: %s pad: %s act: %s"" % (\n                self.name, str(shape), str(stride), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', stride={stride}, padding={padding}\'\n        )\n        if self.dilation_rate != 1:\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        self.W = self._get_weights(""filters"", shape=self.shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter), init=self.b_init)\n\n    def forward(self, inputs):\n\n        outputs = tf.nn.conv1d(\n            input=inputs,\n            filters=self.W,\n            stride=self.stride,\n            padding=self.padding,\n            dilations=[\n                self.dilation_rate,\n            ],\n            data_format=self.data_format,\n            name=self.name,\n        )\n\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass Conv2dLayer(Layer):\n    """"""\n    The :class:`Conv2dLayer` class is a 2D CNN layer, see `tf.nn.conv2d <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv2d>`__.\n\n    Parameters\n    ----------\n    act : activation function\n        The activation function of this layer.\n    shape : tuple of int\n        The shape of the filters: (filter_height, filter_width, in_channels, out_channels).\n    strides : tuple of int\n        The sliding window strides of corresponding input dimensions.\n        It must be in the same order as the ``shape`` parameter.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""NHWC"" or ""NCHW"", default is ""NHWC"".\n    dilation_rate : tuple of int\n        Filter up-sampling/input down-sampling rate.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    name : None or str\n        A unique layer name.\n\n    Notes\n    -----\n    - shape = [h, w, the number of output channel of previous layer, the number of output channels]\n    - the number of output channel of a layer is its last dimension.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 28, 28, 1], name=\'input\')\n    >>> conv2d = tl.layers.Conv2dLayer(shape=(5, 5, 1, 32), strides=(1, 1, 1, 1), b_init=None, name=\'conv2d_1\')\n    >>> print(conv2d)\n    >>> tensor = tl.layers.Conv2dLayer(shape=(5, 5, 1, 32), strides=(1, 1, 1, 1), act=tf.nn.relu, name=\'conv2d_2\')(net)\n    >>> print(tensor)\n\n    """"""\n\n    def __init__(\n        self,\n        act=None,\n        shape=(5, 5, 1, 100),\n        strides=(1, 1, 1, 1),\n        padding=\'SAME\',\n        data_format=\'NHWC\',\n        dilation_rate=(1, 1, 1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        name=None  # \'cnn2d_layer\',\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = shape[-1]\n        self.filter_size = (shape[0], shape[1])\n        self.shape = shape\n        self.strides = strides\n        self.dilation_rate = dilation_rate\n        self.padding = padding\n        self.data_format = data_format\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = shape[-2]\n\n        self.build(None)\n        self._built = True\n\n        logging.info(\n            ""Conv2dLayer %s: shape: %s strides: %s pad: %s act: %s"" % (\n                self.name, str(shape), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != [\n                1,\n        ] * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs):\n        self.W = self._get_weights(""filters"", shape=self.shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.conv2d(\n            input=inputs,\n            filters=self.W,\n            strides=self.strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilations=list(self.dilation_rate),\n            name=self.name,\n        )\n\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass Conv3dLayer(Layer):\n    """"""\n    The :class:`Conv3dLayer` class is a 3D CNN layer, see `tf.nn.conv3d <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv3d>`__.\n\n    Parameters\n    ----------\n    act : activation function\n        The activation function of this layer.\n    shape : tuple of int\n        Shape of the filters: (filter_depth, filter_height, filter_width, in_channels, out_channels).\n    strides : tuple of int\n        The sliding window strides for corresponding input dimensions.\n        Must be in the same order as the shape dimension.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""NDHWC"" or ""NCDHW"", default is ""NDHWC"".\n    dilation_rate : tuple of int\n        Filter up-sampling/input down-sampling rate.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    name : None or str\n        A unique layer name.\n\n    Notes\n    -----\n    - shape = [d, h, w, the number of output channel of previous layer, the number of output channels]\n    - the number of output channel of a layer is its last dimension.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 100, 100, 100, 3], name=\'input\')\n    >>> conv3d = tl.layers.Conv3dLayer(shape=(2, 2, 2, 3, 32), strides=(1, 2, 2, 2, 1), b_init=None, name=\'conv3d_1\')\n    >>> print(conv3d)\n    >>> tensor = tl.layers.Conv3dLayer(shape=(2, 2, 2, 3, 32), strides=(1, 2, 2, 2, 1), act=tf.nn.relu, name=\'conv3d_2\')(net)\n    >>> print(tensor)\n\n    """"""\n\n    def __init__(\n        self,\n        act=None,\n        shape=(2, 2, 2, 3, 32),\n        strides=(1, 2, 2, 2, 1),\n        padding=\'SAME\',\n        data_format=\'NDHWC\',\n        dilation_rate=(1, 1, 1, 1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        name=None  # \'cnn3d_layer\'\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = shape[-1]\n        self.filter_size = (shape[0], shape[1], shape[2])\n        self.shape = shape\n        self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = shape[-2]\n\n        self.build(None)\n        self._built = True\n\n        logging.info(\n            ""Conv3dLayer %s: shape: %s strides: %s pad: %s act: %s"" % (\n                self.name, str(shape), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != [\n                1,\n        ] * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs):\n\n        self.W = self._get_weights(""filters"", shape=self.shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.conv3d(\n            input=inputs,\n            filters=self.W,\n            strides=self.strides,\n            padding=self.padding,\n            data_format=self.data_format,  #\'NDHWC\',\n            dilations=list(self.dilation_rate),  #[1, 1, 1, 1, 1],\n            name=self.name,\n        )\n\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/convolution/expert_deconv.py,11,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\n    \'DeConv1dLayer\',\n    \'DeConv2dLayer\',\n    \'DeConv3dLayer\',\n]\n\n\nclass DeConv1dLayer(Layer):\n    """"""A de-convolution 1D layer.\n\n    See `tf.nn.conv1d_transpose <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv1d_transpose>`__.\n\n    Parameters\n    ----------\n    act : activation function or None\n        The activation function of this layer.\n    shape : tuple of int\n        Shape of the filters: (height, width, output_channels, in_channels).\n        The filter\'s ``in_channels`` dimension must match that of value.\n    outputs_shape : tuple of int\n        Output shape of the deconvolution,\n    strides : tuple of int\n        The sliding window strides for corresponding input dimensions.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""NWC"" or ""NCW"", default is ""NWC"".\n    dilation_rate : int\n        Filter up-sampling/input down-sampling rate.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    name : None or str\n        A unique layer name.\n\n    Notes\n    -----\n    - shape = [w, the number of output channels of this layer, the number of output channel of the previous layer].\n    - outputs_shape = [batch_size, any, the number of output channels of this layer].\n    - the number of output channel of a layer is its last dimension.\n\n    Examples\n    --------\n    >>> input_layer = Input([8, 25, 32], name=\'input_layer\')\n    >>> deconv1d = tl.layers.DeConv1dLayer(\n    ...     shape=(5, 64, 32), outputs_shape=(8, 50, 64), strides=(1, 2, 1), name=\'deconv1dlayer\'\n    ... )\n    >>> print(deconv1d)\n    >>> tensor = tl.layers.DeConv1dLayer(\n    ...     shape=(5, 64, 32), outputs_shape=(8, 50, 64), strides=(1, 2, 1), name=\'deconv1dlayer\'\n    ... )(input_layer)\n    >>> print(tensor)\n    >>> output shape : (8, 50, 64)\n\n    """"""\n\n    def __init__(\n        self,\n        act=None,\n        shape=(3, 128, 256),\n        outputs_shape=(1, 256, 128),\n        strides=(1, 2, 1),\n        padding=\'SAME\',\n        data_format=\'NWC\',\n        dilation_rate=(1, 1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        name=None  # \'decnn1d_layer\',\n    ):\n        super().__init__(name, act=act)\n        self.shape = shape\n        self.outputs_shape = outputs_shape\n        self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = self.shape[-1]\n\n        self.build(None)\n        self._built = True\n\n        logging.info(\n            ""DeConv1dLayer %s: shape: %s out_shape: %s strides: %s pad: %s act: %s"" % (\n                self.name, str(shape), str(outputs_shape), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(\n            classname=self.__class__.__name__, n_filter=self.shape[-2], filter_size=self.shape[0], **self.__dict__\n        )\n\n    def build(self, inputs):\n        self.W = self._get_weights(""filters"", shape=self.shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.shape[-2]), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.conv1d_transpose(\n            input=inputs,\n            filters=self.W,\n            output_shape=self.outputs_shape,\n            strides=list(self.strides),\n            padding=self.padding,\n            data_format=self.data_format,\n            dilations=list(self.dilation_rate),\n            name=self.name,\n        )\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass DeConv2dLayer(Layer):\n    """"""A de-convolution 2D layer.\n\n    See `tf.nn.conv2d_transpose <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv2d_transpose>`__.\n\n    Parameters\n    ----------\n    act : activation function or None\n        The activation function of this layer.\n    shape : tuple of int\n        Shape of the filters: (height, width, output_channels, in_channels).\n        The filter\'s ``in_channels`` dimension must match that of value.\n    outputs_shape : tuple of int\n        Output shape of the deconvolution,\n    strides : tuple of int\n        The sliding window strides for corresponding input dimensions.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""NHWC"" or ""NCHW"", default is ""NHWC"".\n    dilation_rate : tuple of int\n        Filter up-sampling/input down-sampling rate.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    name : None or str\n        A unique layer name.\n\n    Notes\n    -----\n    - shape = [h, w, the number of output channels of this layer, the number of output channel of the previous layer].\n    - outputs_shape = [batch_size, any, any, the number of output channels of this layer].\n    - the number of output channel of a layer is its last dimension.\n\n    Examples\n    --------\n    With TensorLayer\n\n    TODO: Add the example code of a part of the generator in DCGAN example\n\n    U-Net\n\n    >>> ....\n    >>> conv10 = tl.layers.Conv2dLayer(\n    ...        act=tf.nn.relu,\n    ...        shape=(3, 3, 1024, 1024), strides=(1, 1, 1, 1), padding=\'SAME\',\n    ...        W_init=w_init, b_init=b_init, name=\'conv10\'\n    ... )(conv9)\n    >>> print(conv10)\n    (batch_size, 32, 32, 1024)\n    >>> deconv1 = tl.layers.DeConv2dLayer(\n    ...         act=tf.nn.relu,\n    ...         shape=(3, 3, 512, 1024), strides=(1, 2, 2, 1), outputs_shape=(batch_size, 64, 64, 512),\n    ...         padding=\'SAME\', W_init=w_init, b_init=b_init, name=\'devcon1_1\'\n    ... )(conv10)\n\n    """"""\n\n    def __init__(\n        self,\n        act=None,\n        shape=(3, 3, 128, 256),\n        outputs_shape=(1, 256, 256, 128),\n        strides=(1, 2, 2, 1),\n        padding=\'SAME\',\n        data_format=\'NHWC\',\n        dilation_rate=(1, 1, 1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        name=None  # \'decnn2d_layer\',\n    ):\n        super().__init__(name, act=act)\n        self.shape = shape\n        self.outputs_shape = outputs_shape\n        self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = self.shape[-1]\n\n        self.build(None)\n        self._built = True\n\n        logging.info(\n            ""DeConv2dLayer %s: shape: %s out_shape: %s strides: %s pad: %s act: %s"" % (\n                self.name, str(shape), str(outputs_shape), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(\n            classname=self.__class__.__name__, n_filter=self.shape[-2], filter_size=(self.shape[0], self.shape[1]),\n            **self.__dict__\n        )\n\n    def build(self, inputs):\n        self.W = self._get_weights(""filters"", shape=self.shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.shape[-2]), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.conv2d_transpose(\n            input=inputs,\n            filters=self.W,\n            output_shape=self.outputs_shape,\n            strides=self.strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilations=list(self.dilation_rate),\n            name=self.name,\n        )\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass DeConv3dLayer(Layer):\n    """"""A de-convolution 3D layer.\n\n    See `tf.nn.conv3d_transpose <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv3d_transpose>`__.\n\n    Parameters\n    ----------\n    act : activation function or None\n        The activation function of this layer.\n    shape : tuple of int\n        The shape of the filters: (depth, height, width, output_channels, in_channels).\n        The filter\'s in_channels dimension must match that of value.\n    outputs_shape : tuple of int\n        The output shape of the deconvolution.\n    strides : tuple of int\n        The sliding window strides for corresponding input dimensions.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""NDHWC"" or ""NCDHW"", default is ""NDHWC"".\n    dilation_rate : tuple of int\n        Filter up-sampling/input down-sampling rate.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    name : None or str\n        A unique layer name.\n\n    Notes\n    -----\n    - shape = [d, h, w, the number of output channels of this layer, the number of output channel of the previous layer].\n    - outputs_shape = [batch_size, any, any, any, the number of output channels of this layer].\n    - the number of output channel of a layer is its last dimension.\n\n    Examples\n    --------\n    >>> input_layer = Input([8, 10, 10, 10 32], name=\'input_layer\')\n    >>> deconv3d = tl.layers.DeConv3dLayer(\n    ...     shape=(2, 2, 2, 128, 32), outputs_shape=(8, 20, 20, 20, 128), strides=(1, 2, 2, 2, 1), name=\'deconv3dlayer\'\n    ... )\n    >>> print(deconv3d)\n    >>> tensor = tl.layers.DeConv1dLayer(\n    ...     shape=(2, 2, 2, 128, 32), outputs_shape=(8, 20, 20, 20, 128), strides=(1, 2, 2, 2, 1), name=\'deconv3dlayer\'\n    ... )(input_layer)\n    >>> print(tensor)\n    >>> output shape : (8, 20, 20, 20, 128)\n\n    """"""\n\n    def __init__(\n        self,\n        act=None,\n        shape=(2, 2, 2, 128, 256),\n        outputs_shape=(1, 12, 32, 32, 128),\n        strides=(1, 2, 2, 2, 1),\n        padding=\'SAME\',\n        data_format=\'NDHWC\',\n        dilation_rate=(1, 1, 1, 1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        name=None  # \'decnn3d_layer\',\n    ):\n        super().__init__(name, act=act)\n        self.shape = shape\n        self.outputs_shape = outputs_shape\n        self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = self.shape[-1]\n\n        self.build(None)\n        self._built = True\n\n        logging.info(\n            ""DeConv3dLayer %s: shape: %s out_shape: %s strides: %s pad: %s act: %s"" % (\n                self.name, str(shape), str(outputs_shape), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(\n            classname=self.__class__.__name__, n_filter=self.shape[-2],\n            filter_size=(self.shape[0], self.shape[1], self.shape[2]), **self.__dict__\n        )\n\n    def build(self, inputs):\n        self.W = self._get_weights(""filters"", shape=self.shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.shape[-2]), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.conv3d_transpose(\n            input=inputs, filters=self.W, output_shape=self.outputs_shape, strides=self.strides, padding=self.padding,\n            data_format=self.data_format, dilations=list(self.dilation_rate), name=self.name\n        )\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/convolution/group_conv.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\n    \'GroupConv2d\',\n]\n\n\nclass GroupConv2d(Layer):\n    """"""The :class:`GroupConv2d` class is 2D grouped convolution, see `here <https://blog.yani.io/filter-group-tutorial/>`__.\n\n    Parameters\n    --------------\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size.\n    strides : tuple of int\n        The stride step.\n    n_group : int\n        The number of groups.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""channels_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    dilation_rate : tuple of int\n        Specifying the dilation rate to use for dilated convolution.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 24, 24, 32], name=\'input\')\n    >>> groupconv2d = tl.layers.QuanConv2d(\n    ...     n_filter=64, filter_size=(3, 3), strides=(2, 2), n_group=2, name=\'group\'\n    ... )(net)\n    >>> print(groupconv2d)\n    >>> output shape : (8, 12, 12, 64)\n\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(2, 2),\n        n_group=2,\n        act=None,\n        padding=\'SAME\',\n        data_format=\'channels_last\',\n        dilation_rate=(1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'groupconv\',\n    ):  # Windaway\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = self._strides = strides\n        self.n_group = n_group\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = self._dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""GroupConv2d %s: n_filter: %d size: %s strides: %s n_group: %d pad: %s act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), n_group, padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self._strides[0], self._strides[1], 1]\n            self._dilation_rate = [1, self._dilation_rate[0], self._dilation_rate[1], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self._strides[0], self._strides[1]]\n            self._dilation_rate = [1, 1, self._dilation_rate[0], self._dilation_rate[1]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.groupConv = lambda i, k: tf.nn.conv2d(\n            i, k, strides=self._strides, padding=self.padding, data_format=self.data_format, dilations=self.\n            _dilation_rate, name=self.name\n        )\n\n        self.filter_shape = (\n            self.filter_size[0], self.filter_size[1], int(self.in_channels / self.n_group), self.n_filter\n        )\n\n        self.We = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=self.n_filter, init=self.b_init)\n\n    def forward(self, inputs):\n        if self.n_group == 1:\n            outputs = self.groupConv(inputs, self.We)\n        else:\n            inputGroups = tf.split(axis=3, num_or_size_splits=self.n_group, value=inputs)\n            weightsGroups = tf.split(axis=3, num_or_size_splits=self.n_group, value=self.We)\n            convGroups = [self.groupConv(i, k) for i, k in zip(inputGroups, weightsGroups)]\n            outputs = tf.concat(axis=3, values=convGroups)\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/convolution/quan_conv.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import (quantize_active_overflow, quantize_weight_overflow)\n\n__all__ = [\'QuanConv2d\']\n\n\nclass QuanConv2d(Layer):\n    """"""The :class:`QuanConv2d` class is a quantized convolutional layer without BN, which weights are \'bitW\' bits and the output of the previous layer\n    are \'bitA\' bits while inferencing.\n    Note that, the bias vector would not be binarized.\n\n    Parameters\n    ----------\n    With TensorLayer\n\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    strides : tuple of int\n        The sliding window strides of corresponding input dimensions.\n        It must be in the same order as the ``shape`` parameter.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    bitW : int\n        The bits of this layer\'s parameter\n    bitA : int\n        The bits of the output of previous layer\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inference.\n        TODO: support gemm\n    data_format : str\n        ""channels_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    dilation_rate : tuple of int\n        Specifying the dilation rate to use for dilated convolution.\n    W_init : initializer\n        The initializer for the the weight matrix.\n    b_init : initializer or None\n        The initializer for the the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 12, 12, 64], name=\'input\')\n    >>> quanconv2d = tl.layers.QuanConv2d(\n    ...     n_filter=32, filter_size=(5, 5), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'quancnn2d\'\n    ... )(net)\n    >>> print(quanconv2d)\n    >>> output shape : (8, 12, 12, 32)\n\n    """"""\n\n    def __init__(\n        self,\n        bitW=8,\n        bitA=8,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        act=None,\n        padding=\'SAME\',\n        use_gemm=False,\n        data_format=""channels_last"",\n        dilation_rate=(1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'quan_cnn2d\',\n    ):\n        super().__init__(name, act=act)\n        self.bitW = bitW\n        self.bitA = bitA\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.use_gemm = use_gemm\n        self.data_format = data_format\n        self.dilation_rate = self._dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""QuanConv2d %s: n_filter: %d filter_size: %s strides: %s pad: %s act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n        if self.use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        if len(self.strides) != 2:\n            raise ValueError(""len(strides) should be 2."")\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self._strides[0], self._strides[1], 1]\n            self._dilation_rate = [1, self._dilation_rate[0], self._dilation_rate[1], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self._strides[0], self._strides[1]]\n            self._dilation_rate = [1, 1, self._dilation_rate[0], self._dilation_rate[1]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (self.filter_size[0], self.filter_size[1], self.in_channels, self.n_filter)\n\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter, ), init=self.b_init)\n\n    def forward(self, inputs):\n\n        inputs = quantize_active_overflow(inputs, self.bitA)  # Do not remove\n\n        W_ = quantize_weight_overflow(self.W, self.bitW)\n\n        outputs = tf.nn.conv2d(\n            input=inputs, filters=W_, strides=self.strides, padding=self.padding, data_format=self.data_format,\n            dilations=self._dilation_rate, name=self.name\n        )\n\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n\n        return outputs\n'"
tensorlayer/layers/convolution/quan_conv_bn.py,10,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.training import moving_averages\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import (quantize_active_overflow, quantize_weight_overflow)\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\'QuanConv2dWithBN\']\n\n\nclass QuanConv2dWithBN(Layer):\n    """"""The :class:`QuanConv2dWithBN` class is a quantized convolutional layer with BN, which weights are \'bitW\' bits and the output of the previous layer\n    are \'bitA\' bits while inferencing.\n\n    Note that, the bias vector would keep the same.\n\n    Parameters\n    ----------\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    strides : tuple of int\n        The sliding window strides of corresponding input dimensions.\n        It must be in the same order as the ``shape`` parameter.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    act : activation function\n        The activation function of this layer.\n    decay : float\n        A decay factor for `ExponentialMovingAverage`.\n        Suggest to use a large value for large dataset.\n    epsilon : float\n        Eplison.\n    is_train : boolean\n        Is being used for training or inference.\n    beta_init : initializer or None\n        The initializer for initializing beta, if None, skip beta.\n        Usually you should not skip beta unless you know what happened.\n    gamma_init : initializer or None\n        The initializer for initializing gamma, if None, skip gamma.\n    bitW : int\n        The bits of this layer\'s parameter\n    bitA : int\n        The bits of the output of previous layer\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inferencing. (TODO).\n    W_init : initializer\n        The initializer for the the weight matrix.\n    W_init_args : dictionary\n        The arguments for the weight matrix initializer.\n    data_format : str\n        ""NHWC"" or ""NCHW"", default is ""NHWC"".\n    dilation_rate : tuple of int\n        Specifying the dilation rate to use for dilated convolution.\n    in_channels : int\n        The number of in channels.\n    name : str\n        A unique layer name.\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.Input([50, 256, 256, 3])\n    >>> layer = tl.layers.QuanConv2dWithBN(n_filter=64, filter_size=(5,5),strides=(1,1),padding=\'SAME\',name=\'qcnnbn1\')\n    >>> print(layer)\n    >>> net = tl.layers.QuanConv2dWithBN(n_filter=64, filter_size=(5,5),strides=(1,1),padding=\'SAME\',name=\'qcnnbn1\')(net)\n    >>> print(net)\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        padding=\'SAME\',\n        act=None,\n        decay=0.9,\n        epsilon=1e-5,\n        is_train=False,\n        gamma_init=tl.initializers.truncated_normal(stddev=0.02),\n        beta_init=tl.initializers.truncated_normal(stddev=0.02),\n        bitW=8,\n        bitA=8,\n        use_gemm=False,\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        W_init_args=None,\n        data_format=""channels_last"",\n        dilation_rate=(1, 1),\n        in_channels=None,\n        name=\'quan_cnn2d_bn\',\n    ):\n        super(QuanConv2dWithBN, self).__init__(act=act, name=name)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = strides\n        self.padding = padding\n        self.decay = decay\n        self.epsilon = epsilon\n        self.is_train = is_train\n        self.gamma_init = gamma_init\n        self.beta_init = beta_init\n        self.bitW = bitW\n        self.bitA = bitA\n        self.use_gemm = use_gemm\n        self.W_init = W_init\n        self.W_init_args = W_init_args\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.in_channels = in_channels\n        logging.info(\n            ""QuanConv2dWithBN %s: n_filter: %d filter_size: %s strides: %s pad: %s act: %s "" % (\n                self.name, n_filter, filter_size, str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        if use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        if len(strides) != 2:\n            raise ValueError(""len(strides) should be 2."")\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\' + actstr\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self.strides[0], self.strides[1], 1]\n            self._dilation_rate = [1, self.dilation_rate[0], self.dilation_rate[1], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self.strides[0], self.strides[1]]\n            self._dilation_rate = [1, 1, self.dilation_rate[0], self.dilation_rate[1]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (self.filter_size[0], self.filter_size[1], self.in_channels, self.n_filter)\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n\n        para_bn_shape = (self.n_filter, )\n        if self.gamma_init:\n            self.scale_para = self._get_weights(\n                ""scale_para"", shape=para_bn_shape, init=self.gamma_init, trainable=self.is_train\n            )\n        else:\n            self.scale_para = None\n\n        if self.beta_init:\n            self.offset_para = self._get_weights(\n                ""offset_para"", shape=para_bn_shape, init=self.beta_init, trainable=self.is_train\n            )\n        else:\n            self.offset_para = None\n\n        self.moving_mean = self._get_weights(\n            ""moving_mean"", shape=para_bn_shape, init=tl.initializers.constant(1.0), trainable=False\n        )\n        self.moving_variance = self._get_weights(\n            ""moving_variance"", shape=para_bn_shape, init=tl.initializers.constant(1.0), trainable=False\n        )\n\n    def forward(self, inputs):\n        x = inputs\n        inputs = quantize_active_overflow(inputs, self.bitA)  # Do not remove\n        outputs = tf.nn.conv2d(\n            input=x, filters=self.W, strides=self._strides, padding=self.padding, data_format=self.data_format,\n            dilations=self._dilation_rate, name=self.name\n        )\n\n        mean, variance = tf.nn.moments(outputs, axes=list(range(len(outputs.get_shape()) - 1)))\n\n        update_moving_mean = moving_averages.assign_moving_average(\n            self.moving_mean, mean, self.decay, zero_debias=False\n        )  # if zero_debias=True, has bias\n        update_moving_variance = moving_averages.assign_moving_average(\n            self.moving_variance, mean, self.decay, zero_debias=False\n        )  # if zero_debias=True, has bias\n\n        if self.is_train:\n            mean, var = self.mean_var_with_update(update_moving_mean, update_moving_variance, mean, variance)\n        else:\n            mean, var = self.moving_mean, self.moving_variance\n\n        w_fold = self._w_fold(self.W, self.scale_para, var, self.epsilon)\n\n        W_ = quantize_weight_overflow(w_fold, self.bitW)\n\n        conv_fold = tf.nn.conv2d(inputs, W_, strides=self.strides, padding=self.padding, data_format=self.data_format)\n\n        if self.beta_init:\n            bias_fold = self._bias_fold(self.offset_para, self.scale_para, mean, var, self.epsilon)\n            conv_fold = tf.nn.bias_add(conv_fold, bias_fold, name=\'bn_bias_add\')\n\n        if self.act:\n            conv_fold = self.act(conv_fold)\n\n        return conv_fold\n\n    def mean_var_with_update(self, update_moving_mean, update_moving_variance, mean, variance):\n        with tf.control_dependencies([update_moving_mean, update_moving_variance]):\n            return tf.identity(mean), tf.identity(variance)\n\n    def _w_fold(self, w, gama, var, epsilon):\n        return tf.compat.v1.div(tf.multiply(gama, w), tf.sqrt(var + epsilon))\n\n    def _bias_fold(self, beta, gama, mean, var, epsilon):\n        return tf.subtract(beta, tf.compat.v1.div(tf.multiply(gama, mean), tf.sqrt(var + epsilon)))\n'"
tensorlayer/layers/convolution/separable_conv.py,10,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import get_collection_trainable\n\n__all__ = [\n    \'SeparableConv1d\',\n    \'SeparableConv2d\',\n]\n\n\nclass SeparableConv1d(Layer):\n    """"""The :class:`SeparableConv1d` class is a 1D depthwise separable convolutional layer.\n\n    This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels.\n\n    Parameters\n    ------------\n    n_filter : int\n        The dimensionality of the output space (i.e. the number of filters in the convolution).\n    filter_size : int\n        Specifying the spatial dimensions of the filters. Can be a single integer to specify the same value for all spatial dimensions.\n    strides : int\n        Specifying the stride of the convolution. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n    padding : str\n        One of ""valid"" or ""same"" (case-insensitive).\n    data_format : str\n        One of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width).\n    dilation_rate : int\n        Specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1.\n    depth_multiplier : int\n        The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to num_filters_in * depth_multiplier.\n    depthwise_init : initializer\n        for the depthwise convolution kernel.\n    pointwise_init : initializer\n        For the pointwise convolution kernel.\n    b_init : initializer\n        For the bias vector. If None, ignore bias in the pointwise part only.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 50, 64], name=\'input\')\n    >>> separableconv1d = tl.layers.Conv1d(n_filter=32, filter_size=3, strides=2, padding=\'SAME\', act=tf.nn.relu, name=\'separable_1d\')(net)\n    >>> print(separableconv1d)\n    >>> output shape : (8, 25, 32)\n\n    """"""\n\n    # @deprecated_alias(layer=\'prev_layer\', end_support_version=1.9)  # TODO remove this line for the 1.9 release\n    def __init__(\n        self,\n        n_filter=100,\n        filter_size=3,\n        strides=1,\n        act=None,\n        padding=\'valid\',\n        data_format=\'channels_last\',\n        dilation_rate=1,\n        depth_multiplier=1,\n        depthwise_init=None,\n        pointwise_init=None,\n        b_init=tl.initializers.constant(value=0.0),\n        # depthwise_regularizer=None,\n        # pointwise_regularizer=None,\n        # bias_regularizer=None,\n        # activity_regularizer=None,\n        # depthwise_constraint=None,\n        # pointwise_constraint=None,\n        # W_init=tf.truncated_normal_initializer(stddev=0.1),\n        # b_init=tf.constant_initializer(value=0.0),\n        in_channels=None,\n        name=None  # \'seperable1d\',\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.depth_multiplier = depth_multiplier\n        self.depthwise_init = depthwise_init\n        self.pointwise_init = pointwise_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        logging.info(\n            ""SeparableConv1d  %s: n_filter: %d filter_size: %s strides: %s depth_multiplier: %d act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), depth_multiplier,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', stride={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != 1:\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        self.layer = tf.keras.layers.SeparableConv1D(\n            filters=self.n_filter,\n            kernel_size=self.filter_size,\n            strides=self.strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate,\n            depth_multiplier=self.depth_multiplier,\n            activation=self.act,\n            use_bias=(True if self.b_init is not None else False),\n            depthwise_initializer=self.depthwise_init,\n            pointwise_initializer=self.pointwise_init,\n            bias_initializer=self.b_init,\n            # depthwise_regularizer=None,\n            # pointwise_regularizer=None,\n            # bias_regularizer=None,\n            # activity_regularizer=None,\n            # depthwise_constraint=None,\n            # pointwise_constraint=None,\n            # bias_constraint=None,\n            trainable=True,\n            name=self.name\n        )\n        if self.data_format == ""channels_first"":\n            self.in_channels = inputs_shape[1]\n        else:\n            self.in_channels = inputs_shape[-1]\n\n        # _out = self.layer(np.random.uniform([1] + list(inputs_shape)))  # initialize weights\n        _out = self.layer(\n            tf.convert_to_tensor(np.random.uniform(size=list(inputs_shape)), dtype=np.float)\n        )  # initialize weights\n        outputs_shape = _out.shape\n        # self._add_weights(self.layer.weights)\n        self._trainable_weights = self.layer.weights\n\n    def forward(self, inputs):\n        outputs = self.layer(inputs)\n        return outputs\n\n\nclass SeparableConv2d(Layer):\n    """"""The :class:`SeparableConv2d` class is a 2D depthwise separable convolutional layer.\n\n    This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels.\n    While :class:`DepthwiseConv2d` performs depthwise convolution only, which allow us to add batch normalization between depthwise and pointwise convolution.\n\n    Parameters\n    ------------\n    n_filter : int\n        The dimensionality of the output space (i.e. the number of filters in the convolution).\n    filter_size : tuple/list of 2 int\n        Specifying the spatial dimensions of the filters. Can be a single integer to specify the same value for all spatial dimensions.\n    strides : tuple/list of 2 int\n        Specifying the strides of the convolution. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n    padding : str\n        One of ""valid"" or ""same"" (case-insensitive).\n    data_format : str\n        One of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width).\n    dilation_rate : integer or tuple/list of 2 int\n        Specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1.\n    depth_multiplier : int\n        The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to num_filters_in * depth_multiplier.\n    depthwise_init : initializer\n        for the depthwise convolution kernel.\n    pointwise_init : initializer\n        For the pointwise convolution kernel.\n    b_init : initializer\n        For the bias vector. If None, ignore bias in the pointwise part only.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 50, 50, 64], name=\'input\')\n    >>> separableconv2d = tl.layers.Conv1d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, padding=\'VALID\', name=\'separableconv2d\')(net)\n    >>> print(separableconv2d)\n    >>> output shape : (8, 24, 24, 32)\n\n    """"""\n\n    # @deprecated_alias(layer=\'prev_layer\', end_support_version=1.9)  # TODO remove this line for the 1.9 release\n    def __init__(\n        self,\n        n_filter=100,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        act=None,\n        padding=\'valid\',\n        data_format=\'channels_last\',\n        dilation_rate=(1, 1),\n        depth_multiplier=1,\n        depthwise_init=None,\n        pointwise_init=None,\n        b_init=tl.initializers.constant(value=0.0),\n        # depthwise_regularizer=None,\n        # pointwise_regularizer=None,\n        # bias_regularizer=None,\n        # activity_regularizer=None,\n        # depthwise_constraint=None,\n        # pointwise_constraint=None,\n        # W_init=tf.truncated_normal_initializer(stddev=0.1),\n        # b_init=tf.constant_initializer(value=0.0),\n        in_channels=None,\n        name=None  # \'seperable2d\',\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.depth_multiplier = depth_multiplier\n        self.depthwise_init = depthwise_init\n        self.pointwise_init = pointwise_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        logging.info(\n            ""SeparableConv2d  %s: n_filter: %d filter_size: %s filter_size: %s depth_multiplier: %d act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), depth_multiplier,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', stride={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != 1:\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        self.layer = tf.keras.layers.SeparableConv2D(\n            filters=self.n_filter,\n            kernel_size=self.filter_size,\n            strides=self.strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate,\n            depth_multiplier=self.depth_multiplier,\n            activation=self.act,\n            use_bias=(True if self.b_init is not None else False),\n            depthwise_initializer=self.depthwise_init,\n            pointwise_initializer=self.pointwise_init,\n            bias_initializer=self.b_init,\n            # depthwise_regularizer=None,\n            # pointwise_regularizer=None,\n            # bias_regularizer=None,\n            # activity_regularizer=None,\n            # depthwise_constraint=None,\n            # pointwise_constraint=None,\n            # bias_constraint=None,\n            trainable=True,\n            name=self.name\n        )\n        if self.data_format == ""channels_first"":\n            self.in_channels = inputs_shape[1]\n        else:\n            self.in_channels = inputs_shape[-1]\n        # _out = self.layer(np.random.uniform([1] + list(inputs_shape)))  # initialize weights\n        _out = self.layer(\n            tf.convert_to_tensor(np.random.uniform(size=list(inputs_shape)), dtype=np.float)\n        )  # initialize weights\n        outputs_shape = _out.shape\n        self._trainable_weights = self.layer.weights\n\n    def forward(self, inputs):\n        outputs = self.layer(inputs)\n        return outputs\n'"
tensorlayer/layers/convolution/simplified_conv.py,9,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import get_collection_trainable\n\n__all__ = [\n    \'Conv1d\',\n    \'Conv2d\',\n    \'Conv3d\',\n]\n\n\nclass Conv1d(Layer):\n    """"""Simplified version of :class:`Conv1dLayer`.\n\n    Parameters\n    ----------\n    n_filter : int\n        The number of filters\n    filter_size : int\n        The filter size\n    stride : int\n        The stride step\n    dilation_rate : int\n        Specifying the dilation rate to use for dilated convolution.\n    act : activation function\n        The function that is applied to the layer activations\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""channel_last"" (NWC, default) or ""channels_first"" (NCW).\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 100, 1], name=\'input\')\n    >>> conv1d = tl.layers.Conv1d(n_filter=32, filter_size=5, stride=2, b_init=None, in_channels=1, name=\'conv1d_1\')\n    >>> print(conv1d)\n    >>> tensor = tl.layers.Conv1d(n_filter=32, filter_size=5, stride=2, act=tf.nn.relu, name=\'conv1d_2\')(net)\n    >>> print(tensor)\n\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=5,\n        stride=1,\n        act=None,\n        padding=\'SAME\',\n        data_format=""channels_last"",\n        dilation_rate=1,\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'conv1d\'\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.stride = stride\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""Conv1d %s: n_filter: %d filter_size: %s stride: %d pad: %s act: %s"" % (\n                self.name, n_filter, filter_size, stride, padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', stride={stride}, padding={padding}\'\n        )\n        if self.dilation_rate != 1:\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (self.filter_size, self.in_channels, self.n_filter)\n\n        # TODO : check\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.conv1d(\n            input=inputs,\n            filters=self.W,\n            stride=self.stride,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilations=self.dilation_rate,\n            name=self.name,\n        )\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass Conv2d(Layer):\n    """"""Simplified version of :class:`Conv2dLayer`.\n\n    Parameters\n    ----------\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    strides : tuple of int\n        The sliding window strides of corresponding input dimensions.\n        It must be in the same order as the ``shape`` parameter.\n    dilation_rate : tuple of int\n        Specifying the dilation rate to use for dilated convolution.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""channels_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    W_init : initializer\n        The initializer for the the weight matrix.\n    b_init : initializer or None\n        The initializer for the the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 400, 400, 3], name=\'input\')\n    >>> conv2d = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), b_init=None, in_channels=3, name=\'conv2d_1\')\n    >>> print(conv2d)\n    >>> tensor = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), act=tf.nn.relu, name=\'conv2d_2\')(net)\n    >>> print(tensor)\n\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        act=None,\n        padding=\'SAME\',\n        data_format=\'channels_last\',\n        dilation_rate=(1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'conv2d\',\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self._strides = self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self._dilation_rate = self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""Conv2d %s: n_filter: %d filter_size: %s strides: %s pad: %s act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self._strides[0], self._strides[1], 1]\n            self._dilation_rate = [1, self._dilation_rate[0], self._dilation_rate[1], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self._strides[0], self._strides[1]]\n            self._dilation_rate = [1, 1, self._dilation_rate[0], self._dilation_rate[1]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (self.filter_size[0], self.filter_size[1], self.in_channels, self.n_filter)\n\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter, ), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.conv2d(\n            input=inputs,\n            filters=self.W,\n            strides=self._strides,\n            padding=self.padding,\n            data_format=self.data_format,  #\'NHWC\',\n            dilations=self._dilation_rate,  #[1, 1, 1, 1],\n            name=self.name,\n        )\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n\n\nclass Conv3d(Layer):\n    """"""Simplified version of :class:`Conv3dLayer`.\n\n    Parameters\n    ----------\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    strides : tuple of int\n        The sliding window strides of corresponding input dimensions.\n        It must be in the same order as the ``shape`` parameter.\n    dilation_rate : tuple of int\n        Specifying the dilation rate to use for dilated convolution.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    data_format : str\n        ""channels_last"" (NDHWC, default) or ""channels_first"" (NCDHW).\n    W_init : initializer\n        The initializer for the the weight matrix.\n    b_init : initializer or None\n        The initializer for the the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 20, 20, 20, 3], name=\'input\')\n    >>> conv3d = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3, 3), strides=(2, 2, 2), b_init=None, in_channels=3, name=\'conv3d_1\')\n    >>> print(conv3d)\n    >>> tensor = tl.layers.Conv2d(n_filter=32, filter_size=(3, 3, 3), strides=(2, 2, 2), act=tf.nn.relu, name=\'conv3d_2\')(net)\n    >>> print(tensor)\n\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=(3, 3, 3),\n        strides=(1, 1, 1),\n        act=None,\n        padding=\'SAME\',\n        data_format=\'channels_last\',\n        dilation_rate=(1, 1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'conv3d\',\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self._strides = self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self._dilation_rate = self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""Conv3d %s: n_filter: %d filter_size: %s strides: %s pad: %s act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NDHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self._strides[0], self._strides[1], self._strides[2], 1]\n            self._dilation_rate = [1, self.dilation_rate[0], self.dilation_rate[1], self.dilation_rate[2], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCDHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self._strides[0], self._strides[1], self._strides[2]]\n            self._dilation_rate = [1, 1, self._dilation_rate[0], self._dilation_rate[1], self._dilation_rate[2]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (\n            self.filter_size[0], self.filter_size[1], self.filter_size[2], self.in_channels, self.n_filter\n        )\n\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter, ), init=self.b_init)\n\n    def forward(self, inputs):\n        outputs = tf.nn.conv3d(\n            input=inputs,\n            filters=self.W,\n            strides=self._strides,\n            padding=self.padding,\n            data_format=self.data_format,  #\'NDHWC\',\n            dilations=self._dilation_rate,  #[1, 1, 1, 1, 1],\n            name=self.name,\n        )\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/convolution/simplified_deconv.py,7,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import get_collection_trainable\n\n__all__ = [\n    # \'DeConv1d\'  # TODO: Shall be implemented\n    \'DeConv2d\',\n    \'DeConv3d\',\n]\n\n\nclass DeConv2d(Layer):\n    """"""Simplified version of :class:`DeConv2dLayer`, see `tf.nn.conv3d_transpose <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv2d_transpose>`__.\n\n    Parameters\n    ----------\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    strides : tuple of int\n        The stride step (height, width).\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    act : activation function\n        The activation function of this layer.\n    data_format : str\n        ""channels_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    dilation_rate : int of tuple of int\n        The dilation rate to use for dilated convolution\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([5, 100, 100, 32], name=\'input\')\n    >>> deconv2d = tl.layers.DeConv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), in_channels=32, name=\'DeConv2d_1\')\n    >>> print(deconv2d)\n    >>> tensor = tl.layers.DeConv2d(n_filter=32, filter_size=(3, 3), strides=(2, 2), name=\'DeConv2d_2\')(net)\n    >>> print(tensor)\n\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(2, 2),\n        act=None,\n        padding=\'SAME\',\n        dilation_rate=(1, 1),\n        data_format=\'channels_last\',\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'decnn2d\'\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        # Attention: To build, we need not only the in_channels! Solved.\n        if self.in_channels is not None:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""DeConv2d {}: n_filters: {} strides: {} padding: {} act: {} dilation: {}"".format(\n                self.name,\n                str(n_filter),\n                str(strides),\n                padding,\n                self.act.__name__ if self.act is not None else \'No Activation\',\n                dilation_rate,\n            )\n        )\n\n        if len(strides) != 2:\n            raise ValueError(""len(strides) should be 2, DeConv2d and DeConv2dLayer are different."")\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        self.layer = tf.keras.layers.Conv2DTranspose(\n            filters=self.n_filter,\n            kernel_size=self.filter_size,\n            strides=self.strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate,\n            activation=self.act,\n            use_bias=(True if self.b_init is not None else False),\n            kernel_initializer=self.W_init,\n            bias_initializer=self.b_init,\n            # dtype=tf.float32,\n            name=self.name,\n        )\n        if inputs_shape is not None:\n            self.in_channels = inputs_shape[1 if self.data_format == ""channels_first"" else -1]\n        elif self.in_channels is not None:\n            inputs_shape = [1, self.in_channels, 1, 1\n                           ] if self.data_format == ""channels_first"" else [1, 1, 1, self.in_channels]\n        else:\n            raise ValueError(""Either inputs_shape or in_channels must be specified for build."")\n        _out = self.layer(\n            tf.convert_to_tensor(np.random.uniform(size=inputs_shape), dtype=np.float32)\n        )  #np.random.uniform([1] + list(inputs_shape)))  # initialize weights\n        outputs_shape = _out.shape\n        self._trainable_weights = self.layer.weights\n\n    def forward(self, inputs):\n        outputs = self.layer(inputs)\n        return outputs\n\n\nclass DeConv3d(Layer):\n    """"""Simplified version of :class:`DeConv3dLayer`, see `tf.nn.conv3d_transpose <https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/nn/conv3d_transpose>`__.\n\n    Parameters\n    ----------\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (depth, height, width).\n    strides : tuple of int\n        The stride step (depth, height, width).\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    act : activation function\n        The activation function of this layer.\n    data_format : str\n        ""channels_last"" (NDHWC, default) or ""channels_first"" (NCDHW).\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip bias.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([5, 100, 100, 100, 32], name=\'input\')\n    >>> deconv3d = tl.layers.DeConv3d(n_filter=32, filter_size=(3, 3, 3), strides=(2, 2, 2), in_channels=32, name=\'DeConv3d_1\')\n    >>> print(deconv3d)\n    >>> tensor = tl.layers.DeConv3d(n_filter=32, filter_size=(3, 3, 3), strides=(2, 2, 2), name=\'DeConv3d_2\')(net)\n    >>> print(tensor)\n\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=(3, 3, 3),\n        strides=(2, 2, 2),\n        padding=\'SAME\',\n        act=None,\n        data_format=\'channels_last\',\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'decnn3d\'\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = strides\n        self.padding = padding\n        self.data_format = data_format\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        # Attention: To build, we need not only the in_channels! Solved.\n        if self.in_channels is not None:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""DeConv3d %s: n_filters: %s strides: %s pad: %s act: %s"" % (\n                self.name, str(n_filter), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n        if len(strides) != 3:\n            raise ValueError(""len(strides) should be 3, DeConv3d and DeConv3dLayer are different."")\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        # if self.dilation_rate != (1,) * len(self.dilation_rate):\n        #     s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        self.layer = tf.keras.layers.Conv3DTranspose(\n            filters=self.n_filter,\n            kernel_size=self.filter_size,\n            strides=self.strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            activation=self.act,\n            use_bias=(True if self.b_init is not None else False),\n            kernel_initializer=self.W_init,\n            bias_initializer=self.b_init,\n            name=self.name,\n        )\n        if inputs_shape is not None:\n            self.in_channels = inputs_shape[1 if self.data_format == ""channels_first"" else -1]\n        elif self.in_channels is not None:\n            inputs_shape = [1, self.in_channels, 1, 1, 1\n                           ] if self.data_format == ""channels_first"" else [1, 1, 1, 1, self.in_channels]\n        else:\n            raise ValueError(""Either inputs_shape or in_channels must be specified for build."")\n        _out = self.layer(\n            tf.convert_to_tensor(np.random.uniform(size=inputs_shape), dtype=np.float32)\n        )  #self.layer(np.random.uniform([1] + list(inputs_shape)))  # initialize weights\n        outputs_shape = _out.shape\n        self._trainable_weights = self.layer.weights\n\n    def forward(self, inputs):\n        outputs = self.layer(inputs)\n        return outputs\n'"
tensorlayer/layers/convolution/super_resolution.py,4,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias, private_method\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'SubpixelConv1d\',\n    \'SubpixelConv2d\',\n]\n\n\nclass SubpixelConv1d(Layer):\n    """"""It is a 1D sub-pixel up-sampling layer.\n\n    Calls a TensorFlow function that directly implements this functionality.\n    We assume input has dim (batch, width, r)\n\n    Parameters\n    ------------\n    scale : int\n        The up-scaling ratio, a wrong setting will lead to Dimension size error.\n    act : activation function\n        The activation function of this layer.\n    in_channels : int\n        The number of in channels.\n    name : str\n        A unique layer name.\n\n    Examples\n    ----------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 25, 32], name=\'input\')\n    >>> subpixelconv1d = tl.layers.SubpixelConv1d(scale=2, name=\'subpixelconv1d\')(net)\n    >>> print(subpixelconv1d)\n    >>> output shape : (8, 50, 16)\n\n    References\n    -----------\n    `Audio Super Resolution Implementation <https://github.com/kuleshov/audio-super-res/blob/master/src/models/layers/subpixel.py>`__.\n\n    """"""\n\n    def __init__(\n        self,\n        scale=2,\n        act=None,\n        in_channels=None,\n        name=None  # \'subpixel_conv1d\'\n    ):\n        super().__init__(name, act=act)\n        self.scale = scale\n        self.in_channels = in_channels\n        self.out_channels = int(self.in_channels / self.scale)\n\n        if self.in_channels is not None:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""SubpixelConv1d  %s: scale: %d act: %s"" %\n            (self.name, scale, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(in_channels={in_channels}, out_channels={out_channels}\')\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if inputs_shape is not None:\n            self.in_channels = inputs_shape[-1]\n        self.out_channels = int(self.in_channels / self.scale)\n\n    def forward(self, inputs):\n        outputs = self._PS(inputs, r=self.scale)\n        if self.act is not None:\n            outputs = self.act(outputs)\n        return outputs\n\n    def _PS(self, I, r):\n        X = tf.transpose(a=I, perm=[2, 1, 0])  # (r, w, b)\n        X = tf.batch_to_space(input=X, block_shape=[r], crops=[[0, 0]])  # (1, r*w, b)\n        X = tf.transpose(a=X, perm=[2, 1, 0])\n        return X\n\n\nclass SubpixelConv2d(Layer):\n    """"""It is a 2D sub-pixel up-sampling layer, usually be used\n    for Super-Resolution applications, see `SRGAN <https://github.com/tensorlayer/srgan/>`__ for example.\n\n    Parameters\n    ------------\n    scale : int\n        The up-scaling ratio, a wrong setting will lead to dimension size error.\n    n_out_channel : int or None\n        The number of output channels.\n        - If None, automatically set n_out_channel == the number of input channels / (scale x scale).\n        - The number of input channels == (scale x scale) x The number of output channels.\n    act : activation function\n        The activation function of this layer.\n    in_channels : int\n        The number of in channels.\n    name : str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> # examples here just want to tell you how to set the n_out_channel.\n    >>> net = tl.layers.Input([2, 16, 16, 4], name=\'input1\')\n    >>> subpixelconv2d = tl.layers.SubpixelConv2d(scale=2, n_out_channel=1, name=\'subpixel_conv2d1\')(net)\n    >>> print(subpixelconv2d)\n    >>> output shape : (2, 32, 32, 1)\n\n    >>> net = tl.layers.Input([2, 16, 16, 4*10], name=\'input2\')\n    >>> subpixelconv2d = tl.layers.SubpixelConv2d(scale=2, n_out_channel=10, name=\'subpixel_conv2d2\')(net)\n    >>> print(subpixelconv2d)\n    >>> output shape : (2, 32, 32, 10)\n\n    >>> net = tl.layers.Input([2, 16, 16, 25*10], name=\'input3\')\n    >>> subpixelconv2d = tl.layers.SubpixelConv2d(scale=5, n_out_channel=10, name=\'subpixel_conv2d3\')(net)\n    >>> print(subpixelconv2d)\n    >>> output shape : (2, 80, 80, 10)\n\n    References\n    ------------\n    - `Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network <https://arxiv.org/pdf/1609.05158.pdf>`__\n\n    """"""\n\n    # github/Tetrachrome/subpixel  https://github.com/Tetrachrome/subpixel/blob/master/subpixel.py\n    def __init__(\n        self,\n        scale=2,\n        n_out_channels=None,\n        act=None,\n        in_channels=None,\n        name=None  # \'subpixel_conv2d\'\n    ):\n        super().__init__(name, act=act)\n        self.scale = scale\n        self.n_out_channels = n_out_channels\n        self.in_channels = in_channels\n\n        if self.in_channels is not None:\n            self.build(None)\n            self._built = True\n        logging.info(\n            ""SubpixelConv2d  %s: scale: %d act: %s"" %\n            (self.name, scale, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(in_channels={in_channels}, out_channels={n_out_channels}\')\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n\n        if inputs_shape is not None:\n            self.in_channels = inputs_shape[-1]\n\n        if self.in_channels / (self.scale**2) % 1 != 0:\n            raise Exception(\n                ""SubpixelConv2d: The number of input channels == (scale x scale) x The number of output channels""\n            )\n        self.n_out_channels = int(self.in_channels / (self.scale**2))\n\n    def forward(self, inputs):\n        outputs = self._PS(X=inputs, r=self.scale, n_out_channels=self.n_out_channels)\n        if self.act is not None:\n            outputs = self.act(outputs)\n        return outputs\n\n    def _PS(self, X, r, n_out_channels):\n\n        _err_log = ""SubpixelConv2d: The number of input channels == (scale x scale) x The number of output channels""\n\n        if n_out_channels >= 1:\n            if int(X.get_shape()[-1]) != (r**2) * n_out_channels:\n                raise Exception(_err_log)\n\n            X = tf.compat.v1.depth_to_space(input=X, block_size=r)\n        else:\n            raise RuntimeError(_err_log)\n\n        return X\n'"
tensorlayer/layers/convolution/ternary_conv.py,6,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import compute_alpha, ternary_operation\n\n__all__ = [\'TernaryConv2d\']\n\n\nclass TernaryConv2d(Layer):\n    """"""\n    The :class:`TernaryConv2d` class is a 2D ternary CNN layer, which weights are either -1 or 1 or 0 while inference.\n\n    Note that, the bias vector would not be tenarized.\n\n    Parameters\n    ----------\n    n_filter : int\n        The number of filters.\n    filter_size : tuple of int\n        The filter size (height, width).\n    strides : tuple of int\n        The sliding window strides of corresponding input dimensions.\n        It must be in the same order as the ``shape`` parameter.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: ""SAME"" or ""VALID"".\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inference.\n        TODO: support gemm\n    data_format : str\n        ""channels_last"" (NHWC, default) or ""channels_first"" (NCHW).\n    dilation_rate : tuple of int\n        Specifying the dilation rate to use for dilated convolution.\n    W_init : initializer\n        The initializer for the the weight matrix.\n    b_init : initializer or None\n        The initializer for the the bias vector. If None, skip biases.\n    in_channels : int\n        The number of in channels.\n    name : None or str\n        A unique layer name.\n\n    Examples\n    ---------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([8, 12, 12, 32], name=\'input\')\n    >>> ternaryconv2d = tl.layers.QuanConv2d(\n    ...     n_filter=64, filter_size=(5, 5), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'ternaryconv2d\'\n    ... )(net)\n    >>> print(ternaryconv2d)\n    >>> output shape : (8, 12, 12, 64)\n\n    """"""\n\n    def __init__(\n        self,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        act=None,\n        padding=\'SAME\',\n        use_gemm=False,\n        data_format=""channels_last"",\n        dilation_rate=(1, 1),\n        W_init=tl.initializers.truncated_normal(stddev=0.02),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None  # \'ternary_cnn2d\',\n    ):\n        super().__init__(name, act=act)\n        self.n_filter = n_filter\n        self.filter_size = filter_size\n        self.strides = self._strides = strides\n        self.padding = padding\n        self.use_gemm = use_gemm\n        self.data_format = data_format\n        self.dilation_rate = self._dilation_rate = dilation_rate\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels:\n            self.build(None)\n            self._built = True\n\n        logging.info(\n            ""TernaryConv2d %s: n_filter: %d filter_size: %s strides: %s pad: %s act: %s"" % (\n                self.name, n_filter, str(filter_size), str(strides), padding,\n                self.act.__name__ if self.act is not None else \'No Activation\'\n            )\n        )\n\n        if use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        if len(self.strides) != 2:\n            raise ValueError(""len(strides) should be 2."")\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\n            \'{classname}(in_channels={in_channels}, out_channels={n_filter}, kernel_size={filter_size}\'\n            \', strides={strides}, padding={padding}\'\n        )\n        if self.dilation_rate != (1, ) * len(self.dilation_rate):\n            s += \', dilation={dilation_rate}\'\n        if self.b_init is None:\n            s += \', bias=False\'\n        s += (\', \' + actstr)\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.data_format == \'channels_last\':\n            self.data_format = \'NHWC\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[-1]\n            self._strides = [1, self._strides[0], self._strides[1], 1]\n            self._dilation_rate = [1, self._dilation_rate[0], self._dilation_rate[1], 1]\n        elif self.data_format == \'channels_first\':\n            self.data_format = \'NCHW\'\n            if self.in_channels is None:\n                self.in_channels = inputs_shape[1]\n            self._strides = [1, 1, self._strides[0], self._strides[1]]\n            self._dilation_rate = [1, 1, self._dilation_rate[0], self._dilation_rate[1]]\n        else:\n            raise Exception(""data_format should be either channels_last or channels_first"")\n\n        self.filter_shape = (self.filter_size[0], self.filter_size[1], self.in_channels, self.n_filter)\n\n        self.W = self._get_weights(""filters"", shape=self.filter_shape, init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_filter, ), init=self.b_init)\n\n    def forward(self, inputs):\n\n        alpha = compute_alpha(self.W)\n\n        W_ = ternary_operation(self.W)\n        W_ = tf.multiply(alpha, W_)\n\n        outputs = tf.nn.conv2d(\n            input=inputs, filters=W_, strides=self._strides, padding=self.padding, data_format=self.data_format,\n            dilations=self._dilation_rate, name=self.name\n        )\n\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n\n        return outputs\n'"
tensorlayer/layers/dense/__init__.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nTensorLayer provides rich layer implementations trailed for\nvarious benchmarks and domain-specific problems. In addition, we also\nsupport transparent access to native TensorFlow parameters.\nFor example, we provide not only layers for local response normalization, but also\nlayers that allow user to apply ``tf.nn.lrn`` on ``network.outputs``.\nMore functions can be found in `TensorFlow API <https://www.tensorflow.org/versions/master/api_docs/index.html>`__.\n""""""\n\nfrom .base_dense import *\nfrom .binary_dense import *\nfrom .dorefa_dense import *\nfrom .dropconnect import *\nfrom .quan_dense import *\nfrom .quan_dense_bn import *\nfrom .ternary_dense import *\n\n__all__ = [\n    \'BinaryDense\',\n    \'Dense\',\n    \'DorefaDense\',\n    \'DropconnectDense\',\n    \'TernaryDense\',\n    \'QuanDense\',\n    \'QuanDenseWithBN\',\n]\n'"
tensorlayer/layers/dense/base_dense.py,6,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n# from tensorlayer.layers.core import LayersConfig\n\n__all__ = [\n    \'Dense\',\n]\n\n\nclass Dense(Layer):\n    """"""The :class:`Dense` class is a fully connected layer.\n\n    Parameters\n    ----------\n    n_units : int\n        The number of units of this layer.\n    act : activation function\n        The activation function of this layer.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : None or str\n        A unique layer name. If None, a unique name will be automatically generated.\n\n    Examples\n    --------\n    With TensorLayer\n\n    >>> net = tl.layers.Input([100, 50], name=\'input\')\n    >>> dense = tl.layers.Dense(n_units=800, act=tf.nn.relu, in_channels=50, name=\'dense_1\')\n    >>> print(dense)\n    Dense(n_units=800, relu, in_channels=\'50\', name=\'dense_1\')\n    >>> tensor = tl.layers.Dense(n_units=800, act=tf.nn.relu, name=\'dense_2\')(net)\n    >>> print(tensor)\n    tf.Tensor([...], shape=(100, 800), dtype=float32)\n\n    Notes\n    -----\n    If the layer input has more than two axes, it needs to be flatten by using :class:`Flatten`.\n\n    """"""\n\n    def __init__(\n        self,\n        n_units,\n        act=None,\n        W_init=tl.initializers.truncated_normal(stddev=0.05),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None,  # \'dense\',\n    ):\n\n        super(Dense, self).__init__(name, act=act)\n\n        self.n_units = n_units\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels is not None:\n            self.build(self.in_channels)\n            self._built = True\n\n        logging.info(\n            ""Dense  %s: %d %s"" %\n            (self.name, self.n_units, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(n_units={n_units}, \' + actstr)\n        if self.in_channels is not None:\n            s += \', in_channels=\\\'{in_channels}\\\'\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.in_channels is None and len(inputs_shape) != 2:\n            raise AssertionError(""The input dimension must be rank 2, please reshape or flatten it"")\n        if self.in_channels:\n            shape = [self.in_channels, self.n_units]\n        else:\n            self.in_channels = inputs_shape[1]\n            shape = [inputs_shape[1], self.n_units]\n        self.W = self._get_weights(""weights"", shape=tuple(shape), init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_units, ), init=self.b_init)\n\n    # @tf.function\n    def forward(self, inputs):\n        z = tf.matmul(inputs, self.W)\n        if self.b_init:\n            z = tf.add(z, self.b)\n        if self.act:\n            z = self.act(z)\n        return z\n'"
tensorlayer/layers/dense/binary_dense.py,6,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import quantize\n\n__all__ = [\n    \'BinaryDense\',\n]\n\n\nclass BinaryDense(Layer):\n    """"""The :class:`BinaryDense` class is a binary fully connected layer, which weights are either -1 or 1 while inferencing.\n\n    Note that, the bias vector would not be binarized.\n\n    Parameters\n    ----------\n    n_units : int\n        The number of units of this layer.\n    act : activation function\n        The activation function of this layer, usually set to ``tf.act.sign`` or apply :class:`Sign` after :class:`BatchNorm`.\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inference. (TODO).\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : None or str\n        A unique layer name.\n\n    """"""\n\n    def __init__(\n        self,\n        n_units=100,\n        act=None,\n        use_gemm=False,\n        W_init=tl.initializers.truncated_normal(stddev=0.05),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None,  #\'binary_dense\',\n    ):\n        super().__init__(name, act=act)\n        self.n_units = n_units\n        self.use_gemm = use_gemm\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels is not None:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(\n            ""BinaryDense  %s: %d %s"" %\n            (self.name, n_units, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(n_units={n_units}, \' + actstr)\n        if self.in_channels is not None:\n            s += \', in_channels=\\\'{in_channels}\\\'\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if len(inputs_shape) != 2:\n            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")\n\n        if self.in_channels is None:\n            self.in_channels = inputs_shape[1]\n\n        if self.use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        n_in = inputs_shape[-1]\n        self.W = self._get_weights(""weights"", shape=(n_in, self.n_units), init=self.W_init)\n        if self.b_init is not None:\n            self.b = self._get_weights(""biases"", shape=(self.n_units), init=self.b_init)\n\n    def forward(self, inputs):\n        # W = tl.act.sign(W)    # dont update ...\n        W_ = quantize(self.W)\n        # W = tf.Variable(W)\n\n        outputs = tf.matmul(inputs, W_)\n        # self.outputs = xnor_gemm(self.inputs, W) # TODO\n\n        if self.b_init is not None:\n            outputs = tf.nn.bias_add(outputs, self.b, name=\'bias_add\')\n\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/dense/dorefa_dense.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import cabs, quantize_active, quantize_weight\n\n__all__ = [\n    \'DorefaDense\',\n]\n\n\nclass DorefaDense(Layer):\n    """"""The :class:`DorefaDense` class is a binary fully connected layer, which weights are \'bitW\' bits and the output of the previous layer\n    are \'bitA\' bits while inferencing.\n\n    Note that, the bias vector would not be binarized.\n\n    Parameters\n    ----------\n    bitW : int\n        The bits of this layer\'s parameter\n    bitA : int\n        The bits of the output of previous layer\n    n_units : int\n        The number of units of this layer.\n    act : activation function\n        The activation function of this layer, usually set to ``tf.act.sign`` or apply :class:`Sign` after :class:`BatchNorm`.\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inferencing. (TODO).\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : a str\n        A unique layer name.\n\n    """"""\n\n    def __init__(\n        self,\n        bitW=1,\n        bitA=3,\n        n_units=100,\n        act=None,\n        use_gemm=False,\n        W_init=tl.initializers.truncated_normal(stddev=0.05),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None,  #\'dorefa_dense\',\n    ):\n        super().__init__(name, act=act)\n        self.bitW = bitW\n        self.bitA = bitA\n        self.n_units = n_units\n        self.use_gemm = use_gemm\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels is not None:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(\n            ""DorefaDense  %s: %d %s"" %\n            (self.name, n_units, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(n_units={n_units}, \' + actstr)\n        s += \', bitW={bitW}, bitA={bitA}\'\n        if self.in_channels is not None:\n            s += \', in_channels=\\\'{in_channels}\\\'\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if len(inputs_shape) != 2:\n            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")\n\n        if self.in_channels is None:\n            self.in_channels = inputs_shape[1]\n\n        if self.use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        n_in = inputs_shape[-1]\n        self.W = self._get_weights(""weights"", shape=(n_in, self.n_units), init=self.W_init)\n        if self.b_init is not None:\n            self.b = self._get_weights(""biases"", shape=(self.n_units), init=self.b_init)\n\n    def forward(self, inputs):\n        inputs = quantize_active(cabs(inputs), self.bitA)\n        W_ = quantize_weight(self.W, self.bitW)\n        outputs = tf.matmul(inputs, W_)\n        # self.outputs = xnor_gemm(self.inputs, W) # TODO\n        if self.b_init is not None:\n            outputs = tf.nn.bias_add(outputs, self.b, name=\'bias_add\')\n            # self.outputs = xnor_gemm(self.inputs, W) + b # TODO\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/dense/dropconnect.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numbers\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\n\n__all__ = [\n    \'DropconnectDense\',\n]\n\n\nclass DropconnectDense(Layer):\n    """"""\n    The :class:`DropconnectDense` class is :class:`Dense` with DropConnect\n    behaviour which randomly removes connections between this layer and the previous\n    layer according to a keeping probability.\n\n    Parameters\n    ----------\n    keep : float\n        The keeping probability.\n        The lower the probability it is, the more activations are set to zero.\n    n_units : int\n        The number of units of this layer.\n    act : activation function\n        The activation function of this layer.\n    W_init : weights initializer\n        The initializer for the weight matrix.\n    b_init : biases initializer\n        The initializer for the bias vector.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : str\n        A unique layer name.\n\n    Examples\n    --------\n    >>> net = tl.layers.Input([None, 784], name=\'input\')\n    >>> net = tl.layers.DropconnectDense(keep=0.8,\n    ...         n_units=800, act=tf.nn.relu, name=\'relu1\')(net)\n    >>> net = tl.layers.DropconnectDense(keep=0.5,\n    ...         n_units=800, act=tf.nn.relu, name=\'relu2\')(net)\n    >>> net = tl.layers.DropconnectDense(keep=0.5,\n    ...         n_units=10, name=\'output\')(net)\n\n    References\n    ----------\n    - `Wan, L. (2013). Regularization of neural networks using dropconnect <http://machinelearning.wustl.edu/mlpapers/papers/icml2013_wan13>`__\n\n    """"""\n\n    def __init__(\n        self,\n        keep=0.5,\n        n_units=100,\n        act=None,\n        W_init=tl.initializers.truncated_normal(stddev=0.05),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None,  # \'dropconnect\',\n    ):\n        super().__init__(name, act=act)\n\n        if isinstance(keep, numbers.Real) and not (keep > 0 and keep <= 1):\n            raise ValueError(""keep must be a scalar tensor or a float in the "" ""range (0, 1], got %g"" % keep)\n\n        self.keep = keep\n        self.n_units = n_units\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels is not None:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(\n            ""DropconnectDense %s: %d %s"" %\n            (self.name, n_units, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(n_units={n_units}, \' + actstr)\n        s += \', keep={keep}\'\n        if self.in_channels is not None:\n            s += \', in_channels=\\\'{in_channels}\\\'\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if len(inputs_shape) != 2:\n            raise Exception(""The input dimension must be rank 2"")\n\n        if self.in_channels is None:\n            self.in_channels = inputs_shape[1]\n\n        n_in = inputs_shape[-1]\n        self.W = self._get_weights(""weights"", shape=(n_in, self.n_units), init=self.W_init)\n        if self.b_init:\n            self.b = self._get_weights(""biases"", shape=(self.n_units), init=self.b_init)\n\n    def forward(self, inputs):\n        W_dropcon = tf.nn.dropout(self.W, 1 - (self.keep))\n        outputs = tf.matmul(inputs, W_dropcon)\n        if self.b_init:\n            outputs = tf.nn.bias_add(outputs, self.b, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/dense/quan_dense.py,5,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import (quantize_active_overflow, quantize_weight_overflow)\n\n__all__ = [\n    \'QuanDense\',\n]\n\n\nclass QuanDense(Layer):\n    """"""The :class:`QuanDense` class is a quantized fully connected layer with BN, which weights are \'bitW\' bits and the output of the previous layer\n    are \'bitA\' bits while inferencing.\n\n    Parameters\n    ----------\n    n_units : int\n        The number of units of this layer.\n    act : activation function\n        The activation function of this layer.\n    bitW : int\n        The bits of this layer\'s parameter\n    bitA : int\n        The bits of the output of previous layer\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inference. (TODO).\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : None or str\n        A unique layer name.\n\n    """"""\n\n    def __init__(\n        self,\n        n_units=100,\n        act=None,\n        bitW=8,\n        bitA=8,\n        use_gemm=False,\n        W_init=tl.initializers.truncated_normal(stddev=0.05),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None,  #\'quan_dense\',\n    ):\n        super().__init__(name, act=act)\n        self.n_units = n_units\n        self.bitW = bitW\n        self.bitA = bitA\n        self.use_gemm = use_gemm\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels is not None:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(\n            ""QuanDense  %s: %d %s"" %\n            (self.name, n_units, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(n_units={n_units}, \' + actstr)\n        s += \', bitW={bitW}, bitA={bitA}\'\n        if self.in_channels is not None:\n            s += \', in_channels=\\\'{in_channels}\\\'\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if len(inputs_shape) != 2:\n            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")\n\n        if self.in_channels is None:\n            self.in_channels = inputs_shape[1]\n\n        if self.use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        n_in = inputs_shape[-1]\n        self.W = self._get_weights(""weights"", shape=(n_in, self.n_units), init=self.W_init)\n        if self.b_init is not None:\n            self.b = self._get_weights(""biases"", shape=int(self.n_units), init=self.b_init)\n\n    def forward(self, inputs):\n\n        inputs = quantize_active_overflow(inputs, self.bitA)\n\n        W_ = quantize_weight_overflow(self.W, self.bitW)\n\n        # outputs = tf.matmul(inputs, self.W)\n        outputs = tf.matmul(inputs, W_)  # hao dong change to this\n\n        if self.b_init is not None:\n            outputs = tf.nn.bias_add(outputs, self.b, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/layers/dense/quan_dense_bn.py,10,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n# from tensorlayer.layers.core import LayersConfig\nfrom tensorflow.python.training import moving_averages\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import (quantize_active_overflow, quantize_weight_overflow)\n\n__all__ = [\n    \'QuanDenseWithBN\',\n]\n\n\nclass QuanDenseWithBN(Layer):\n    """"""The :class:`QuanDenseWithBN` class is a quantized fully connected layer with BN, which weights are \'bitW\' bits and the output of the previous layer\n    are \'bitA\' bits while inferencing.\n\n    Parameters\n    ----------\n    n_units : int\n        The number of units of this layer.\n    act : activation function\n        The activation function of this layer.\n    decay : float\n        A decay factor for `ExponentialMovingAverage`.\n        Suggest to use a large value for large dataset.\n    epsilon : float\n        Eplison.\n    is_train : boolean\n        Is being used for training or inference.\n    beta_init : initializer or None\n        The initializer for initializing beta, if None, skip beta.\n        Usually you should not skip beta unless you know what happened.\n    gamma_init : initializer or None\n        The initializer for initializing gamma, if None, skip gamma.\n    bitW : int\n        The bits of this layer\'s parameter\n    bitA : int\n        The bits of the output of previous layer\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inferencing. (TODO).\n    W_init : initializer\n        The initializer for the the weight matrix.\n    W_init_args : dictionary\n        The arguments for the weight matrix initializer.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : a str\n        A unique layer name.\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.Input([50, 256])\n    >>> layer = tl.layers.QuanDenseWithBN(128, act=\'relu\', name=\'qdbn1\')(net)\n    >>> print(layer)\n    >>> net = tl.layers.QuanDenseWithBN(256, act=\'relu\', name=\'qdbn2\')(net)\n    >>> print(net)\n    """"""\n\n    def __init__(\n        self,\n        n_units=100,\n        act=None,\n        decay=0.9,\n        epsilon=1e-5,\n        is_train=False,\n        bitW=8,\n        bitA=8,\n        gamma_init=tl.initializers.truncated_normal(stddev=0.05),\n        beta_init=tl.initializers.truncated_normal(stddev=0.05),\n        use_gemm=False,\n        W_init=tl.initializers.truncated_normal(stddev=0.05),\n        W_init_args=None,\n        in_channels=None,\n        name=None,  # \'quan_dense_with_bn\',\n    ):\n        super(QuanDenseWithBN, self).__init__(act=act, W_init_args=W_init_args, name=name)\n        self.n_units = n_units\n        self.decay = decay\n        self.epsilon = epsilon\n        self.is_train = is_train\n        self.bitW = bitW\n        self.bitA = bitA\n        self.gamma_init = gamma_init\n        self.beta_init = beta_init\n        self.use_gemm = use_gemm\n        self.W_init = W_init\n        self.in_channels = in_channels\n\n        if self.in_channels is not None:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(\n            ""QuanDenseLayerWithBN  %s: %d %s"" %\n            (self.name, n_units, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(n_units={n_units}, \' + actstr)\n        s += \', bitW={bitW}, bitA={bitA}\'\n        if self.in_channels is not None:\n            s += \', in_channels=\\\'{in_channels}\\\'\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if self.in_channels is None and len(inputs_shape) != 2:\n            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")\n\n        if self.in_channels is None:\n            self.in_channels = inputs_shape[1]\n\n        if self.use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        n_in = inputs_shape[-1]\n        self.W = self._get_weights(""weights"", shape=(n_in, self.n_units), init=self.W_init)\n\n        para_bn_shape = (self.n_units, )\n        if self.gamma_init:\n            self.scale_para = self._get_weights(""gamm_weights"", shape=para_bn_shape, init=self.gamma_init)\n        else:\n            self.scale_para = None\n\n        if self.beta_init:\n            self.offset_para = self._get_weights(""beta_weights"", shape=para_bn_shape, init=self.beta_init)\n        else:\n            self.offset_para = None\n\n        self.moving_mean = self._get_weights(\n            ""moving_mean"", shape=para_bn_shape, init=tl.initializers.constant(1.0), trainable=False\n        )\n        self.moving_variance = self._get_weights(\n            ""moving_variacne"", shape=para_bn_shape, init=tl.initializers.constant(1.0), trainable=False\n        )\n\n    def forward(self, inputs):\n        x = inputs\n        inputs = quantize_active_overflow(inputs, self.bitA)\n        mid_out = tf.matmul(x, self.W)\n\n        mean, variance = tf.nn.moments(x=mid_out, axes=list(range(len(mid_out.get_shape()) - 1)))\n\n        update_moving_mean = moving_averages.assign_moving_average(\n            self.moving_mean, mean, self.decay, zero_debias=False\n        )  # if zero_debias=True, has bias\n\n        update_moving_variance = moving_averages.assign_moving_average(\n            self.moving_variance, variance, self.decay, zero_debias=False\n        )  # if zero_debias=True, has bias\n\n        if self.is_train:\n            mean, var = self.mean_var_with_update(update_moving_mean, update_moving_variance, mean, variance)\n        else:\n            mean, var = self.moving_mean, self.moving_variance\n\n        w_fold = self._w_fold(self.W, self.scale_para, var, self.epsilon)\n\n        W = quantize_weight_overflow(w_fold, self.bitW)\n\n        outputs = tf.matmul(inputs, W)\n\n        if self.beta_init:\n            bias_fold = self._bias_fold(self.offset_para, self.scale_para, mean, var, self.epsilon)\n            outputs = tf.nn.bias_add(outputs, bias_fold, name=\'bias_add\')\n        else:\n            outputs = outputs\n\n        if self.act:\n            outputs = self.act(outputs)\n        else:\n            outputs = outputs\n        return outputs\n\n    def mean_var_with_update(self, update_moving_mean, update_moving_variance, mean, variance):\n        with tf.control_dependencies([update_moving_mean, update_moving_variance]):\n            return tf.identity(mean), tf.identity(variance)\n\n    def _w_fold(self, w, gama, var, epsilon):\n        return tf.compat.v1.div(tf.multiply(gama, w), tf.sqrt(var + epsilon))\n\n    def _bias_fold(self, beta, gama, mean, var, epsilon):\n        return tf.subtract(beta, tf.compat.v1.div(tf.multiply(gama, mean), tf.sqrt(var + epsilon)))\n'"
tensorlayer/layers/dense/ternary_dense.py,7,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tensorlayer import logging\nfrom tensorlayer.decorators import deprecated_alias\nfrom tensorlayer.layers.core import Layer\nfrom tensorlayer.layers.utils import compute_alpha, ternary_operation\n\n__all__ = [\n    \'TernaryDense\',\n]\n\n\nclass TernaryDense(Layer):\n    """"""The :class:`TernaryDense` class is a ternary fully connected layer, which weights are either -1 or 1 or 0 while inference.\n\n    Note that, the bias vector would not be tenaried.\n\n    Parameters\n    ----------\n    n_units : int\n        The number of units of this layer.\n    act : activation function\n        The activation function of this layer, usually set to ``tf.act.sign`` or apply :class:`SignLayer` after :class:`BatchNormLayer`.\n    use_gemm : boolean\n        If True, use gemm instead of ``tf.matmul`` for inference. (TODO).\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    in_channels: int\n        The number of channels of the previous layer.\n        If None, it will be automatically detected when the layer is forwarded for the first time.\n    name : None or str\n        A unique layer name.\n\n    """"""\n\n    def __init__(\n        self,\n        n_units=100,\n        act=None,\n        use_gemm=False,\n        W_init=tl.initializers.truncated_normal(stddev=0.05),\n        b_init=tl.initializers.constant(value=0.0),\n        in_channels=None,\n        name=None,  #\'ternary_dense\',\n    ):\n        super().__init__(name, act=act)\n        self.n_units = n_units\n        self.use_gemm = use_gemm\n        self.W_init = W_init\n        self.b_init = b_init\n        self.in_channels = in_channels\n\n        if self.in_channels is not None:\n            self.build((None, self.in_channels))\n            self._built = True\n\n        logging.info(\n            ""TernaryDense  %s: %d %s"" %\n            (self.name, n_units, self.act.__name__ if self.act is not None else \'No Activation\')\n        )\n\n    def __repr__(self):\n        actstr = self.act.__name__ if self.act is not None else \'No Activation\'\n        s = (\'{classname}(n_units={n_units}, \' + actstr)\n        if self.in_channels is not None:\n            s += \', in_channels=\\\'{in_channels}\\\'\'\n        if self.name is not None:\n            s += \', name=\\\'{name}\\\'\'\n        s += \')\'\n        return s.format(classname=self.__class__.__name__, **self.__dict__)\n\n    def build(self, inputs_shape):\n        if len(inputs_shape) != 2:\n            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")\n\n        if self.in_channels is None:\n            self.in_channels = inputs_shape[1]\n\n        if self.use_gemm:\n            raise Exception(""TODO. The current version use tf.matmul for inferencing."")\n\n        n_in = inputs_shape[-1]\n\n        self.W = self._get_weights(var_name=""weights"", shape=(n_in, self.n_units), init=self.W_init)\n        if self.b_init is not None:\n            self.b = self._get_weights(var_name=""biases"", shape=(self.n_units), init=self.b_init)\n\n    def forward(self, inputs):\n        # W = tl.act.sign(W)    # dont update ...\n        alpha = compute_alpha(self.W)\n        W_ = ternary_operation(self.W)\n        W_ = tf.multiply(alpha, W_)\n        # W = tf.Variable(W)\n\n        outputs = tf.matmul(inputs, W_)\n        # self.outputs = xnor_gemm(self.inputs, W) # TODO\n\n        if self.b_init is not None:\n            outputs = tf.nn.bias_add(outputs, self.b, name=\'bias_add\')\n        if self.act:\n            outputs = self.act(outputs)\n        return outputs\n'"
tensorlayer/logging/contrib/__init__.py,1,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nTensorLayer provides rich layer implementations trailed for\nvarious benchmarks and domain-specific problems. In addition, we also\nsupport transparent access to native TensorFlow parameters.\nFor example, we provide not only layers for local response normalization, but also\nlayers that allow user to apply ``tf.nn.lrn`` on ``network.outputs``.\nMore functions can be found in `TensorFlow API <https://www.tensorflow.org/versions/master/api_docs/index.html>`__.\n""""""\n\nfrom .hyperdash import *\n'"
tensorlayer/logging/contrib/hyperdash.py,0,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\n\nimport hyperdash as hd\nimport tensorlayer as tl\n\n__all__ = [""HyperDashHandler"", ""monitor"", ""Experiment"", ""IPythonMagicsWrapper""]\n\n\nclass HyperDashHandler(object):\n    apikey = None\n\n    @classmethod\n    def reset_apikey(cls):\n        cls.apikey = None\n\n    @classmethod\n    def set_apikey(cls, apikey):\n        cls.apikey = apikey\n\n    @classmethod\n    def get_apikey(cls):\n\n        if cls.apikey is None:\n            raise ValueError(\n                ""Hyperdash API is not set.\\n""\n                ""You can obtain your API Key using: `hyperdash login --email` or `hyperdash login --github`\\n""\n                ""You should first call `HyperDashHandler.set_apikey(\'my_api_key\')` in order to use `hyperdash`""\n            )\n\n        tl.logging.debug(""Hyperdash API Key: %s"" % cls.apikey)\n\n        return cls.apikey\n\n    @classmethod\n    def monitor(cls, model_name, api_key=None, capture_io=True):\n\n        if api_key is not None:\n            cls.set_apikey(api_key)\n\n        return hd.monitor(model_name, api_key_getter=cls.get_apikey, capture_io=capture_io)\n\n\nclass Experiment(hd.Experiment):\n\n    def __init__(\n        self,\n        model_name,\n        api_key=None,\n        capture_io=True,\n    ):\n\n        if api_key is not None:\n            HyperDashHandler.set_apikey(api_key)\n\n        super(Experiment,\n              self).__init__(model_name=model_name, api_key_getter=HyperDashHandler.get_apikey, capture_io=capture_io)\n\n\nmonitor = HyperDashHandler.monitor\nIPythonMagicsWrapper = hd.IPythonMagicsWrapper\n'"
tests/performance_test/vgg/__init__.py,0,b''
tests/performance_test/vgg/exp_config.py,0,"b""import numpy as np\n\n\ndef random_input_generator(num, batchsize=32, format='NHWC'):\n    input_shape = (batchsize, 224, 224, 3) if format == 'NHWC' else (batchsize, 3, 224, 224)\n    rng = np.random.RandomState(1234)\n    for i in range(num):\n        x = rng.uniform(0.0, 1.0, size=input_shape).astype(np.float32)\n        y = rng.randint(0, 1000, size=(batchsize, ))\n        yield (x, y)\n\n\nMONITOR_INTERVAL = 50\nNUM_ITERS = 300\nBATCH_SIZE = 32\nLERANING_RATE = 0.0001\n"""
tests/performance_test/vgg/keras_test.py,2,"b'import os\nimport time\n\nimport psutil\nimport tensorflow as tf\n\nimport keras\nfrom exp_config import (BATCH_SIZE, LERANING_RATE, MONITOR_INTERVAL, NUM_ITERS, random_input_generator)\nfrom keras.applications.vgg16 import VGG16\nfrom keras.backend.tensorflow_backend import set_session\nfrom keras.utils import to_categorical\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nset_session(sess)\n\n# get the whole model\nvgg = VGG16(weights=None)\n\n# system monitor\ninfo = psutil.virtual_memory()\nmonitor_interval = MONITOR_INTERVAL\navg_mem_usage = 0\nmax_mem_usage = 0\ncount = 0\ntotal_time = 0\n\n# training setting\nnum_iter = NUM_ITERS\nbatch_size = BATCH_SIZE\nvgg.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=LERANING_RATE))\n\n# data generator\ngen = random_input_generator(num_iter, batch_size)\n\n# begin training\n\nfor idx, data in enumerate(gen):\n    x_batch = data[0]\n    y_batch = to_categorical(data[1], num_classes=1000)\n\n    start_time = time.time()\n\n    # forward + backward\n    vgg.train_on_batch(x_batch, y_batch)\n\n    end_time = time.time()\n    consume_time = end_time - start_time\n    total_time += consume_time\n\n    if idx % monitor_interval == 0:\n        cur_usage = psutil.Process(os.getpid()).memory_info().rss\n        max_mem_usage = max(cur_usage, max_mem_usage)\n        avg_mem_usage += cur_usage\n        count += 1\n        print(\n            ""[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s"".format(\n                idx, cur_usage / (1024 * 1024), consume_time\n            )\n        )\n\nprint(\'consumed time:\', total_time)\n\navg_mem_usage = avg_mem_usage / count / (1024 * 1024)\nmax_mem_usage = max_mem_usage / (1024 * 1024)\nprint(\'average memory usage: {:.2f}MB\'.format(avg_mem_usage))\nprint(\'maximum memory usage: {:.2f}MB\'.format(max_mem_usage))\n'"
tests/performance_test/vgg/pytorch_test.py,0,"b'import os\nimport time\n\nimport numpy as np\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom exp_config import (BATCH_SIZE, LERANING_RATE, MONITOR_INTERVAL, NUM_ITERS, random_input_generator)\nfrom torchvision.models import vgg16\n\n# set gpu_id 0\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n# system monitor\ninfo = psutil.virtual_memory()\nmonitor_interval = MONITOR_INTERVAL\navg_mem_usage = 0\nmax_mem_usage = 0\ncount = 0\ntotal_time = 0\n\n# get the whole model\nvgg = vgg16()\n\nstart_time = time.time()\nvgg = vgg.to(device)\ntotal_time += time.time() - start_time\n\n# training setting\nnum_iter = NUM_ITERS\nbatch_size = BATCH_SIZE\noptimizer = optim.Adam(vgg.parameters(), lr=LERANING_RATE)\n\n# data generator\ngen = random_input_generator(num_iter, batch_size, format=\'NCHW\')\n\n# begin training\n\nfor idx, data in enumerate(gen):\n\n    start_time = time.time()\n\n    x_batch = torch.Tensor(data[0])\n    y_batch = torch.Tensor(data[1]).long()\n    x_batch = x_batch.to(device)\n    y_batch = y_batch.to(device)\n\n    # forward + backward\n    outputs = vgg(x_batch)\n    loss = F.cross_entropy(outputs, y_batch)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    end_time = time.time()\n    consume_time = end_time - start_time\n    total_time += consume_time\n\n    if idx % monitor_interval == 0:\n        cur_usage = psutil.Process(os.getpid()).memory_info().rss\n        max_mem_usage = max(cur_usage, max_mem_usage)\n        avg_mem_usage += cur_usage\n        count += 1\n        print(\n            ""[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s"".format(\n                idx, cur_usage / (1024 * 1024), consume_time\n            )\n        )\n\nprint(\'consumed time:\', total_time)\n\navg_mem_usage = avg_mem_usage / count / (1024 * 1024)\nmax_mem_usage = max_mem_usage / (1024 * 1024)\nprint(\'average memory usage: {:.2f}MB\'.format(avg_mem_usage))\nprint(\'maximum memory usage: {:.2f}MB\'.format(max_mem_usage))\n'"
tests/performance_test/vgg/tf2-autograph.py,9,"b'import os\nimport time\n\nimport psutil\nimport tensorflow as tf\nfrom tensorflow.python.keras.applications import VGG16\n\nfrom exp_config import (BATCH_SIZE, LERANING_RATE, MONITOR_INTERVAL, NUM_ITERS, random_input_generator)\n\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n\n# get the whole model\nvgg = VGG16(weights=None)\n\n# system monitor\ninfo = psutil.virtual_memory()\nmonitor_interval = MONITOR_INTERVAL\navg_mem_usage = 0\nmax_mem_usage = 0\ncount = 0\ntotal_time = 0\n\n# training setting\nnum_iter = NUM_ITERS\nbatch_size = BATCH_SIZE\ntrain_weights = vgg.trainable_variables\noptimizer = tf.optimizers.Adam(learning_rate=LERANING_RATE)\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# data generator\ngen = random_input_generator(num_iter, batch_size)\n\n\n# training function\n@tf.function\ndef train_step(x_batch, y_batch):\n    # forward + backward\n    with tf.GradientTape() as tape:\n        ## compute outputs\n        _logits = vgg(x_batch, training=True)\n        ## compute loss and update model\n        _loss = loss_object(y_batch, _logits)\n\n    grad = tape.gradient(_loss, train_weights)\n    optimizer.apply_gradients(zip(grad, train_weights))\n\n\n# begin training\nfor idx, data in enumerate(gen):\n    start_time = time.time()\n\n    x_batch = tf.convert_to_tensor(data[0])\n    y_batch = tf.convert_to_tensor(data[1])\n    train_step(x_batch, y_batch)\n\n    end_time = time.time()\n    consume_time = end_time - start_time\n    total_time += consume_time\n\n    if idx % monitor_interval == 0:\n        cur_usage = psutil.Process(os.getpid()).memory_info().rss\n        max_mem_usage = max(cur_usage, max_mem_usage)\n        avg_mem_usage += cur_usage\n        count += 1\n        tf.print(\n            ""[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s"".format(\n                idx, cur_usage / (1024 * 1024), consume_time\n            )\n        )\n\nprint(\'consumed time:\', total_time)\n\navg_mem_usage = avg_mem_usage / count / (1024 * 1024)\nmax_mem_usage = max_mem_usage / (1024 * 1024)\nprint(\'average memory usage: {:.2f}MB\'.format(avg_mem_usage))\nprint(\'maximum memory usage: {:.2f}MB\'.format(max_mem_usage))\n'"
tests/performance_test/vgg/tf2-eager.py,8,"b'import os\nimport time\n\nimport psutil\nimport tensorflow as tf\nfrom tensorflow.python.keras.applications import VGG16\n\nfrom exp_config import (BATCH_SIZE, LERANING_RATE, MONITOR_INTERVAL, NUM_ITERS, random_input_generator)\n\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n\n# get the whole model\nvgg = VGG16(weights=None)\n\n# system monitor\ninfo = psutil.virtual_memory()\nmonitor_interval = MONITOR_INTERVAL\navg_mem_usage = 0\nmax_mem_usage = 0\ncount = 0\ntotal_time = 0\n\n# training setting\nnum_iter = NUM_ITERS\nbatch_size = BATCH_SIZE\ntrain_weights = vgg.trainable_variables\noptimizer = tf.optimizers.Adam(learning_rate=LERANING_RATE)\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# data generator\ngen = random_input_generator(num_iter, batch_size)\n\n\n# training function\ndef train_step(x_batch, y_batch):\n    # forward + backward\n    with tf.GradientTape() as tape:\n        ## compute outputs\n        _logits = vgg(x_batch, training=True)\n        ## compute loss and update model\n        _loss = loss_object(y_batch, _logits)\n\n    grad = tape.gradient(_loss, train_weights)\n    optimizer.apply_gradients(zip(grad, train_weights))\n\n    return _loss\n\n\n# begin training\nfor idx, data in enumerate(gen):\n    start_time = time.time()\n\n    x_batch = tf.convert_to_tensor(data[0])\n    y_batch = tf.convert_to_tensor(data[1])\n    loss = train_step(x_batch, y_batch)\n\n    end_time = time.time()\n    consume_time = end_time - start_time\n    total_time += consume_time\n\n    if idx % monitor_interval == 0:\n        cur_usage = psutil.Process(os.getpid()).memory_info().rss\n        max_mem_usage = max(cur_usage, max_mem_usage)\n        avg_mem_usage += cur_usage\n        count += 1\n        tf.print(\n            ""[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s, loss {:.4f}"".format(\n                idx, cur_usage / (1024 * 1024), consume_time, loss\n            )\n        )\n\nprint(\'consumed time:\', total_time)\n\navg_mem_usage = avg_mem_usage / count / (1024 * 1024)\nmax_mem_usage = max_mem_usage / (1024 * 1024)\nprint(\'average memory usage: {:.2f}MB\'.format(avg_mem_usage))\nprint(\'maximum memory usage: {:.2f}MB\'.format(max_mem_usage))\n'"
tests/performance_test/vgg/tl2-autograph.py,5,"b'import os\nimport time\n\nimport psutil\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom exp_config import (BATCH_SIZE, LERANING_RATE, MONITOR_INTERVAL, NUM_ITERS, random_input_generator)\n\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nvgg = tl.models.vgg16()\n\n# system monitor\ninfo = psutil.virtual_memory()\nmonitor_interval = MONITOR_INTERVAL\navg_mem_usage = 0\nmax_mem_usage = 0\ncount = 0\ntotal_time = 0\n\n# training setting\nnum_iter = NUM_ITERS\nbatch_size = BATCH_SIZE\ntrain_weights = vgg.trainable_weights\noptimizer = tf.optimizers.Adam(learning_rate=LERANING_RATE)\nloss_object = tl.cost.cross_entropy\n\n# data generator\ngen = random_input_generator(num_iter, batch_size)\n\n\n# training function\n@tf.function\ndef train_step(x_batch, y_batch):\n    # forward + backward\n    with tf.GradientTape() as tape:\n        ## compute outputs\n        _logits = vgg(x_batch)\n        ## compute loss and update model\n        _loss = loss_object(_logits, y_batch)\n\n    grad = tape.gradient(_loss, train_weights)\n    optimizer.apply_gradients(zip(grad, train_weights))\n\n\n# begin training\nvgg.train()\n\nfor idx, data in enumerate(gen):\n    start_time = time.time()\n\n    train_step(data[0], data[1])\n\n    end_time = time.time()\n    consume_time = end_time - start_time\n    total_time += consume_time\n\n    if idx % monitor_interval == 0:\n        cur_usage = psutil.Process(os.getpid()).memory_info().rss\n        max_mem_usage = max(cur_usage, max_mem_usage)\n        avg_mem_usage += cur_usage\n        count += 1\n        tl.logging.info(\n            ""[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s"".format(\n                idx, cur_usage / (1024 * 1024), consume_time\n            )\n        )\n\nprint(\'consumed time:\', total_time)\n\navg_mem_usage = avg_mem_usage / count / (1024 * 1024)\nmax_mem_usage = max_mem_usage / (1024 * 1024)\nprint(\'average memory usage: {:.2f}MB\'.format(avg_mem_usage))\nprint(\'maximum memory usage: {:.2f}MB\'.format(max_mem_usage))\n'"
tests/performance_test/vgg/tl2-eager.py,4,"b'import os\nimport time\n\nimport psutil\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom exp_config import (BATCH_SIZE, LERANING_RATE, MONITOR_INTERVAL, NUM_ITERS, random_input_generator)\n\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nvgg = tl.models.vgg16()\n\n# system monitor\ninfo = psutil.virtual_memory()\nmonitor_interval = MONITOR_INTERVAL\navg_mem_usage = 0\nmax_mem_usage = 0\ncount = 0\ntotal_time = 0\n\n# training setting\nnum_iter = NUM_ITERS\nbatch_size = BATCH_SIZE\ntrain_weights = vgg.trainable_weights\noptimizer = tf.optimizers.Adam(learning_rate=LERANING_RATE)\nloss_object = tl.cost.cross_entropy\n\n# data generator\ngen = random_input_generator(num_iter, batch_size)\n\n\n# training function\ndef train_step(x_batch, y_batch):\n    # forward + backward\n    with tf.GradientTape() as tape:\n        ## compute outputs\n        _logits = vgg(x_batch)\n        ## compute loss and update model\n        _loss = loss_object(_logits, y_batch)\n\n    grad = tape.gradient(_loss, train_weights)\n    optimizer.apply_gradients(zip(grad, train_weights))\n    return _loss\n\n\n# begin training\nvgg.train()\n\nfor idx, data in enumerate(gen):\n    start_time = time.time()\n\n    loss = train_step(data[0], data[1])\n\n    end_time = time.time()\n    consume_time = end_time - start_time\n    total_time += consume_time\n\n    if idx % monitor_interval == 0:\n        cur_usage = psutil.Process(os.getpid()).memory_info().rss\n        max_mem_usage = max(cur_usage, max_mem_usage)\n        avg_mem_usage += cur_usage\n        count += 1\n        tl.logging.info(\n            ""[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s, loss {:.4f}"".format(\n                idx, cur_usage / (1024 * 1024), consume_time, loss\n            )\n        )\n\nprint(\'consumed time:\', total_time)\n\navg_mem_usage = avg_mem_usage / count / (1024 * 1024)\nmax_mem_usage = max_mem_usage / (1024 * 1024)\nprint(\'average memory usage: {:.2f}MB\'.format(avg_mem_usage))\nprint(\'maximum memory usage: {:.2f}MB\'.format(max_mem_usage))\n'"
tests/performance_test/vgg/tl2-static-autograph.py,5,"b'import os\nimport time\n\nimport psutil\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom exp_config import (BATCH_SIZE, LERANING_RATE, MONITOR_INTERVAL, NUM_ITERS, random_input_generator)\n\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nvgg = tl.models.vgg16(mode=\'static\')\n\n# system monitor\ninfo = psutil.virtual_memory()\nmonitor_interval = MONITOR_INTERVAL\navg_mem_usage = 0\nmax_mem_usage = 0\ncount = 0\ntotal_time = 0\n\n# training setting\nnum_iter = NUM_ITERS\nbatch_size = BATCH_SIZE\ntrain_weights = vgg.trainable_weights\noptimizer = tf.optimizers.Adam(learning_rate=LERANING_RATE)\nloss_object = tl.cost.cross_entropy\n\n# data generator\ngen = random_input_generator(num_iter, batch_size)\n\n\n# training function\n@tf.function\ndef train_step(x_batch, y_batch):\n    # forward + backward\n    with tf.GradientTape() as tape:\n        ## compute outputs\n        _logits = vgg(x_batch)\n        ## compute loss and update model\n        _loss = loss_object(_logits, y_batch)\n\n    grad = tape.gradient(_loss, train_weights)\n    optimizer.apply_gradients(zip(grad, train_weights))\n\n\n# begin training\nvgg.train()\n\nfor idx, data in enumerate(gen):\n    start_time = time.time()\n\n    train_step(data[0], data[1])\n\n    end_time = time.time()\n    consume_time = end_time - start_time\n    total_time += consume_time\n\n    if idx % monitor_interval == 0:\n        cur_usage = psutil.Process(os.getpid()).memory_info().rss\n        max_mem_usage = max(cur_usage, max_mem_usage)\n        avg_mem_usage += cur_usage\n        count += 1\n        tl.logging.info(\n            ""[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s"".format(\n                idx, cur_usage / (1024 * 1024), consume_time\n            )\n        )\n\nprint(\'consumed time:\', total_time)\n\navg_mem_usage = avg_mem_usage / count / (1024 * 1024)\nmax_mem_usage = max_mem_usage / (1024 * 1024)\nprint(\'average memory usage: {:.2f}MB\'.format(avg_mem_usage))\nprint(\'maximum memory usage: {:.2f}MB\'.format(max_mem_usage))\n'"
tests/performance_test/vgg/tl2-static-eager.py,4,"b'import os\nimport time\n\nimport psutil\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom exp_config import (BATCH_SIZE, LERANING_RATE, MONITOR_INTERVAL, NUM_ITERS, random_input_generator)\n\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n\ntl.logging.set_verbosity(tl.logging.DEBUG)\n\n# get the whole model\nvgg = tl.models.vgg16(mode=\'static\')\n\n# system monitor\ninfo = psutil.virtual_memory()\nmonitor_interval = MONITOR_INTERVAL\navg_mem_usage = 0\nmax_mem_usage = 0\ncount = 0\ntotal_time = 0\n\n# training setting\nnum_iter = NUM_ITERS\nbatch_size = BATCH_SIZE\ntrain_weights = vgg.trainable_weights\noptimizer = tf.optimizers.Adam(learning_rate=LERANING_RATE)\nloss_object = tl.cost.cross_entropy\n\n# data generator\ngen = random_input_generator(num_iter, batch_size)\n\n\n# training function\ndef train_step(x_batch, y_batch):\n    # forward + backward\n    with tf.GradientTape() as tape:\n        ## compute outputs\n        _logits = vgg(x_batch)\n        ## compute loss and update model\n        _loss = loss_object(_logits, y_batch)\n\n    grad = tape.gradient(_loss, train_weights)\n    optimizer.apply_gradients(zip(grad, train_weights))\n    return _loss\n\n\n# begin training\nvgg.train()\n\nfor idx, data in enumerate(gen):\n    start_time = time.time()\n\n    loss = train_step(data[0], data[1])\n\n    end_time = time.time()\n    consume_time = end_time - start_time\n    total_time += consume_time\n\n    if idx % monitor_interval == 0:\n        cur_usage = psutil.Process(os.getpid()).memory_info().rss\n        max_mem_usage = max(cur_usage, max_mem_usage)\n        avg_mem_usage += cur_usage\n        count += 1\n        tl.logging.info(\n            ""[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s, loss {:.4f}"".format(\n                idx, cur_usage / (1024 * 1024), consume_time, loss\n            )\n        )\n\nprint(\'consumed time:\', total_time)\n\navg_mem_usage = avg_mem_usage / count / (1024 * 1024)\nmax_mem_usage = max_mem_usage / (1024 * 1024)\nprint(\'average memory usage: {:.2f}MB\'.format(avg_mem_usage))\nprint(\'maximum memory usage: {:.2f}MB\'.format(max_mem_usage))\n'"
tests/utils/custom_layers/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom tests.utils.custom_layers.basic_layers import *\nfrom tests.utils.custom_layers.inception_blocks import *\n'
tests/utils/custom_layers/basic_layers.py,11,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\n\n__all__ = [\n    \'activation_module\',\n    \'conv_module\',\n    \'dense_module\',\n]\n\n\ndef activation_module(layer, activation_fn, leaky_relu_alpha=0.2, name=None):\n\n    act_name = name + ""/activation"" if name is not None else ""activation""\n\n    if activation_fn not in [""ReLU"", ""ReLU6"", ""Leaky_ReLU"", ""PReLU"", ""PReLU6"", ""PTReLU6"", ""CReLU"", ""ELU"", ""SELU"",\n                             ""tanh"", ""sigmoid"", ""softmax"", None]:\n        raise Exception(""Unknown \'activation_fn\': %s"" % activation_fn)\n\n    elif activation_fn == ""ReLU"":\n        layer = tl.layers.LambdaLayer(prev_layer=layer, fn=tf.nn.relu, name=act_name)\n\n    elif activation_fn == ""ReLU6"":\n        layer = tl.layers.LambdaLayer(prev_layer=layer, fn=tf.nn.relu6, name=act_name)\n\n    elif activation_fn == ""Leaky_ReLU"":\n        layer = tl.layers.LambdaLayer(\n            prev_layer=layer, fn=tf.nn.leaky_relu, fn_args={\'alpha\': leaky_relu_alpha}, name=act_name\n        )\n\n    elif activation_fn == ""PReLU"":\n        layer = tl.layers.PReluLayer(prev_layer=layer, channel_shared=False, name=act_name)\n\n    elif activation_fn == ""PReLU6"":\n        layer = tl.layers.PRelu6Layer(prev_layer=layer, channel_shared=False, name=act_name)\n\n    elif activation_fn == ""PTReLU6"":\n        layer = tl.layers.PTRelu6Layer(prev_layer=layer, channel_shared=False, name=act_name)\n\n    elif activation_fn == ""CReLU"":\n        layer = tl.layers.LambdaLayer(prev_layer=layer, fn=tf.nn.crelu, name=act_name)\n\n    elif activation_fn == ""ELU"":\n        layer = tl.layers.LambdaLayer(prev_layer=layer, fn=tf.nn.elu, name=act_name)\n\n    elif activation_fn == ""SELU"":\n        layer = tl.layers.LambdaLayer(prev_layer=layer, fn=tf.nn.selu, name=act_name)\n\n    elif activation_fn == ""tanh"":\n        layer = tl.layers.LambdaLayer(prev_layer=layer, fn=tf.nn.tanh, name=act_name)\n\n    elif activation_fn == ""sigmoid"":\n        layer = tl.layers.LambdaLayer(prev_layer=layer, fn=tf.nn.sigmoid, name=act_name)\n\n    elif activation_fn == ""softmax"":\n        layer = tl.layers.LambdaLayer(prev_layer=layer, fn=tf.nn.softmax, name=act_name)\n\n    return layer\n\n\ndef conv_module(\n    prev_layer, n_out_channel, filter_size, strides, padding, is_train=True, use_batchnorm=True, activation_fn=None,\n    conv_init=tl.initializers.random_uniform(), batch_norm_init=tl.initializers.truncated_normal(mean=1., stddev=0.02),\n    bias_init=tf.zeros_initializer(), name=None\n):\n\n    if activation_fn not in [""ReLU"", ""ReLU6"", ""Leaky_ReLU"", ""PReLU"", ""PReLU6"", ""PTReLU6"", ""CReLU"", ""ELU"", ""SELU"",\n                             ""tanh"", ""sigmoid"", ""softmax"", None]:\n        raise Exception(""Unknown \'activation_fn\': %s"" % activation_fn)\n\n    conv_name = \'conv2d\' if name is None else name\n    bn_name = \'batch_norm\' if name is None else name + \'/BatchNorm\'\n\n    layer = tl.layers.Conv2d(\n        prev_layer,\n        n_filter=n_out_channel,\n        filter_size=filter_size,\n        strides=strides,\n        padding=padding,\n        act=None,\n        W_init=conv_init,\n        b_init=None if use_batchnorm else bias_init,  # Not useful as the convolutions are batch normalized\n        name=conv_name\n    )\n\n    if use_batchnorm:\n\n        layer = tl.layers.BatchNormLayer(layer, act=None, is_train=is_train, gamma_init=batch_norm_init, name=bn_name)\n\n    logits = layer.outputs\n\n    layer = activation_module(layer, activation_fn, name=conv_name)\n\n    return layer, logits\n\n\ndef dense_module(\n    prev_layer, n_units, is_train, use_batchnorm=True, activation_fn=None, dense_init=tl.initializers.random_uniform(),\n    batch_norm_init=tl.initializers.truncated_normal(mean=1., stddev=0.02), bias_init=tf.zeros_initializer(), name=None\n):\n\n    if activation_fn not in [""ReLU"", ""ReLU6"", ""Leaky_ReLU"", ""PReLU"", ""PReLU6"", ""PTReLU6"", ""CReLU"", ""ELU"", ""SELU"",\n                             ""tanh"", ""sigmoid"", ""softmax"", None]:\n        raise Exception(""Unknown \'activation_fn\': %s"" % activation_fn)\n\n    # Flatten: Conv to FC\n    if prev_layer.outputs.get_shape().__len__() != 2:  # The input dimension must be rank 2\n        layer = tl.layers.FlattenLayer(prev_layer, name=\'flatten\')\n\n    else:\n        layer = prev_layer\n\n    layer = tl.layers.DenseLayer(\n        layer,\n        n_units=n_units,\n        act=None,\n        W_init=dense_init,\n        b_init=None if use_batchnorm else bias_init,  # Not useful as the convolutions are batch normalized\n        name=\'dense\' if name is None else name\n    )\n\n    if use_batchnorm:\n        layer = tl.layers.BatchNormLayer(\n            layer, act=None, is_train=is_train, gamma_init=batch_norm_init, name=\'batch_norm\'\n        )\n\n    logits = layer.outputs\n\n    layer = activation_module(layer, activation_fn)\n\n    return layer, logits\n'"
tests/utils/custom_layers/inception_blocks.py,23,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils.custom_layers.basic_layers import conv_module\n\n__all__ = [\n    \'block_inception_a\',\n    \'block_reduction_a\',\n    \'block_inception_b\',\n    \'block_reduction_b\',\n    \'block_inception_c\',\n    \'block_reduction_b\',\n]\n\n\ndef block_inception_a(inputs, scope=None, is_train=False):\n    """"""Builds Inception-A block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n\n    with tf.variable_scope(name_or_scope=scope, default_name=\'BlockInceptionA\', values=[inputs]):\n        with tf.variable_scope(\'Branch_0\'):\n            branch_0, _ = conv_module(\n                inputs, n_out_channel=96, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n        with tf.variable_scope(\'Branch_1\'):\n            branch_1, _ = conv_module(\n                inputs, n_out_channel=64, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_1, _ = conv_module(\n                branch_1, n_out_channel=96, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_3x3\'\n            )\n\n        with tf.variable_scope(\'Branch_2\'):\n            branch_2, _ = conv_module(\n                inputs, n_out_channel=64, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_2, _ = conv_module(\n                branch_2, n_out_channel=96, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_3x3\'\n            )\n\n            branch_2, _ = conv_module(\n                branch_2, n_out_channel=96, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0c_3x3\'\n            )\n\n        with tf.variable_scope(\'Branch_3\'):\n            branch_3 = tl.layers.MeanPool2d(\n                inputs, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', name=\'AvgPool_0a_3x3\'\n            )\n\n            branch_3, _ = conv_module(\n                branch_3, n_out_channel=96, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_1x1\'\n            )\n\n        return tl.layers.ConcatLayer([branch_0, branch_1, branch_2, branch_3], concat_dim=3, name=\'concat_layer\')\n\n\ndef block_reduction_a(inputs, scope=None, is_train=False):\n    """"""Builds Reduction-A block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs]):\n        with tf.variable_scope(\'Branch_0\'):\n            branch_0, _ = conv_module(\n                inputs, n_out_channel=384, filter_size=(3, 3), strides=(2, 2), padding=\'VALID\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_1a_3x3\'\n            )\n\n        with tf.variable_scope(\'Branch_1\'):\n            branch_1, _ = conv_module(\n                inputs, n_out_channel=192, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_1, _ = conv_module(\n                branch_1, n_out_channel=224, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_3x3\'\n            )\n\n            branch_1, _ = conv_module(\n                branch_1, n_out_channel=256, filter_size=(3, 3), strides=(2, 2), padding=\'VALID\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_1a_3x3\'\n            )\n\n        with tf.variable_scope(\'Branch_2\'):\n            branch_2 = tl.layers.MaxPool2d(inputs, (3, 3), strides=(2, 2), padding=\'VALID\', name=\'MaxPool_1a_3x3\')\n\n        return tl.layers.ConcatLayer([branch_0, branch_1, branch_2], concat_dim=3, name=\'concat_layer\')\n\n\ndef block_inception_b(inputs, scope=None, is_train=False):\n    """"""Builds Inception-B block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs]):\n        with tf.variable_scope(\'Branch_0\'):\n            branch_0, _ = conv_module(\n                inputs, n_out_channel=384, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n        with tf.variable_scope(\'Branch_1\'):\n            branch_1, _ = conv_module(\n                inputs, n_out_channel=192, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_1, _ = conv_module(\n                branch_1, n_out_channel=224, filter_size=(1, 7), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_1x7\'\n            )\n\n            branch_1, _ = conv_module(\n                branch_1, n_out_channel=256, filter_size=(7, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0c_7x1\'\n            )\n\n        with tf.variable_scope(\'Branch_2\'):\n            branch_2, _ = conv_module(\n                inputs, n_out_channel=192, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_2, _ = conv_module(\n                branch_2, n_out_channel=192, filter_size=(7, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_7x1\'\n            )\n\n            branch_2, _ = conv_module(\n                branch_2, n_out_channel=224, filter_size=(1, 7), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0c_1x7\'\n            )\n\n            branch_2, _ = conv_module(\n                branch_2, n_out_channel=224, filter_size=(7, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0d_7x1\'\n            )\n\n            branch_2, _ = conv_module(\n                branch_2, n_out_channel=256, filter_size=(1, 7), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0e_1x7\'\n            )\n\n        with tf.variable_scope(\'Branch_3\'):\n            branch_3 = tl.layers.MeanPool2d(\n                inputs, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', name=\'AvgPool_0a_3x3\'\n            )\n\n            branch_3, _ = conv_module(\n                branch_3, n_out_channel=128, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_1x1\'\n            )\n\n        return tl.layers.ConcatLayer([branch_0, branch_1, branch_2, branch_3], concat_dim=3, name=\'concat_layer\')\n\n\ndef block_reduction_b(inputs, scope=None, is_train=False):\n    """"""Builds Reduction-B block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs]):\n        with tf.variable_scope(\'Branch_0\'):\n            branch_0, _ = conv_module(\n                inputs, n_out_channel=192, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_0, _ = conv_module(\n                branch_0, n_out_channel=192, filter_size=(3, 3), strides=(2, 2), padding=\'VALID\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_1a_3x3\'\n            )\n\n        with tf.variable_scope(\'Branch_1\'):\n            branch_1, _ = conv_module(\n                inputs, n_out_channel=256, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_1, _ = conv_module(\n                branch_1, n_out_channel=256, filter_size=(1, 7), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_1x7\'\n            )\n\n            branch_1, _ = conv_module(\n                branch_1, n_out_channel=320, filter_size=(7, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0c_7x1\'\n            )\n\n            branch_1, _ = conv_module(\n                branch_1, n_out_channel=320, filter_size=(3, 3), strides=(2, 2), padding=\'VALID\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_1a_3x3\'\n            )\n\n        with tf.variable_scope(\'Branch_2\'):\n            branch_2 = tl.layers.MaxPool2d(inputs, (3, 3), strides=(2, 2), padding=\'VALID\', name=\'MaxPool_1a_3x3\')\n\n        return tl.layers.ConcatLayer([branch_0, branch_1, branch_2], concat_dim=3, name=\'concat_layer\')\n\n\ndef block_inception_c(inputs, scope=None, is_train=False):\n    """"""Builds Inception-C block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs]):\n        with tf.variable_scope(\'Branch_0\'):\n            branch_0, _ = conv_module(\n                inputs, n_out_channel=256, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n        with tf.variable_scope(\'Branch_1\'):\n            branch_1, _ = conv_module(\n                inputs, n_out_channel=384, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_1a, _ = conv_module(\n                branch_1, n_out_channel=256, filter_size=(1, 3), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_1x3\'\n            )\n\n            branch_1b, _ = conv_module(\n                branch_1, n_out_channel=256, filter_size=(3, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0c_3x1\'\n            )\n\n            branch_1 = tl.layers.ConcatLayer([branch_1a, branch_1b], concat_dim=3, name=\'concat_layer\')\n\n        with tf.variable_scope(\'Branch_2\'):\n            branch_2, _ = conv_module(\n                inputs, n_out_channel=384, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n            )\n\n            branch_2, _ = conv_module(\n                branch_2, n_out_channel=448, filter_size=(3, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_3x1\'\n            )\n\n            branch_2, _ = conv_module(\n                branch_2, n_out_channel=512, filter_size=(1, 3), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0c_1x3\'\n            )\n\n            branch_2a, _ = conv_module(\n                branch_2, n_out_channel=256, filter_size=(1, 3), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0d_1x3\'\n            )\n\n            branch_2b, _ = conv_module(\n                branch_2, n_out_channel=256, filter_size=(3, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0e_3x1\'\n            )\n\n            branch_2 = tl.layers.ConcatLayer([branch_2a, branch_2b], concat_dim=3, name=\'concat_layer\')\n\n        with tf.variable_scope(\'Branch_3\'):\n            branch_3 = tl.layers.MeanPool2d(\n                inputs, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', name=\'AvgPool_0a_3x3\'\n            )\n\n            branch_3, _ = conv_module(\n                branch_3, n_out_channel=256, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0b_1x1\'\n            )\n\n        return tl.layers.ConcatLayer([branch_0, branch_1, branch_2, branch_3], concat_dim=3, name=\'concat_layer\')\n'"
tests/utils/custom_networks/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom tests.utils.custom_networks.inceptionv4 import *\n'
tests/utils/custom_networks/inceptionv4.py,20,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\n\nimport tensorflow as tf\n\nimport tensorlayer as tl\nfrom tests.utils.custom_layers.basic_layers import conv_module, dense_module\nfrom tests.utils.custom_layers.inception_blocks import (\n    block_inception_a, block_inception_b, block_inception_c, block_reduction_a, block_reduction_b\n)\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n__all__ = [\'InceptionV4_Network\']\n\n\nclass InceptionV4_Network(object):\n    """"""InceptionV4 model.""""""\n\n    def __init__(self, include_FC_head=True, flatten_output=True):\n\n        self.include_FC_head = include_FC_head\n        self.flatten_output = flatten_output\n\n    def __call__(self, inputs, reuse=False, is_train=False):\n\n        with tf.variable_scope(""InceptionV4"", reuse=reuse):\n\n            preprocessed = inputs\n\n            with tf.variable_scope(""preprocessing""):\n\n                max_val = tf.reduce_max(preprocessed)\n                min_val = tf.reduce_min(preprocessed)\n\n                need_int_rescale = tf.logical_and(tf.greater(max_val, 1.0), tf.greater_equal(min_val, 0.0))\n\n                need_float_rescale = tf.logical_and(tf.less_equal(max_val, 1.0), tf.greater_equal(min_val, 0.0))\n\n                preprocessed = tf.cond(\n                    pred=need_int_rescale, true_fn=lambda: tf.subtract(tf.divide(preprocessed, 127.5), 1.0),\n                    false_fn=lambda: preprocessed\n                )\n\n                preprocessed = tf.cond(\n                    pred=need_float_rescale, true_fn=lambda: tf.multiply(tf.subtract(preprocessed, 0.5), 2.0),\n                    false_fn=lambda: preprocessed\n                )\n\n            # Input Layers\n            input_layer = tl.layers.InputLayer(preprocessed, name=\'input\')\n\n            # 299 x 299 x 3\n            net, _ = conv_module(\n                input_layer, n_out_channel=32, filter_size=(3, 3), strides=(2, 2), padding=\'VALID\',\n                batch_norm_init=None, is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_1a_3x3\'\n            )\n\n            # 149 x 149 x 32\n            net, _ = conv_module(\n                net, n_out_channel=32, filter_size=(3, 3), strides=(1, 1), padding=\'VALID\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_2a_3x3\'\n            )\n\n            # 147 x 147 x 32\n            net, _ = conv_module(\n                net, n_out_channel=64, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_2b_3x3\'\n            )\n\n            # 147 x 147 x 64\n            with tf.variable_scope(\'Mixed_3a\'):\n                with tf.variable_scope(\'Branch_0\'):\n                    branch_0 = tl.layers.MaxPool2d(net, (3, 3), strides=(2, 2), padding=\'VALID\', name=\'MaxPool_0a_3x3\')\n\n                with tf.variable_scope(\'Branch_1\'):\n                    branch_1, _ = conv_module(\n                        net, n_out_channel=96, filter_size=(3, 3), strides=(2, 2), padding=\'VALID\',\n                        batch_norm_init=None, is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\',\n                        name=\'Conv2d_0a_3x3\'\n                    )\n\n                net = tl.layers.ConcatLayer([branch_0, branch_1], concat_dim=3)\n\n            # 73 x 73 x 160\n            with tf.variable_scope(\'Mixed_4a\'):\n                with tf.variable_scope(\'Branch_0\'):\n                    branch_0, _ = conv_module(\n                        net, n_out_channel=64, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                        is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n                    )\n\n                    branch_0, _ = conv_module(\n                        branch_0, n_out_channel=96, filter_size=(3, 3), strides=(1, 1), padding=\'VALID\',\n                        batch_norm_init=None, is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\',\n                        name=\'Conv2d_1a_3x3\'\n                    )\n\n                with tf.variable_scope(\'Branch_1\'):\n                    branch_1, _ = conv_module(\n                        net, n_out_channel=64, filter_size=(1, 1), strides=(1, 1), padding=\'SAME\', batch_norm_init=None,\n                        is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\', name=\'Conv2d_0a_1x1\'\n                    )\n\n                    branch_1, _ = conv_module(\n                        branch_1, n_out_channel=64, filter_size=(1, 7), strides=(1, 1), padding=\'SAME\',\n                        batch_norm_init=None, is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\',\n                        name=\'Conv2d_0b_1x7\'\n                    )\n\n                    branch_1, _ = conv_module(\n                        branch_1, n_out_channel=64, filter_size=(7, 1), strides=(1, 1), padding=\'SAME\',\n                        batch_norm_init=None, is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\',\n                        name=\'Conv2d_0c_7x1\'\n                    )\n\n                    branch_1, _ = conv_module(\n                        branch_1, n_out_channel=96, filter_size=(3, 3), strides=(1, 1), padding=\'VALID\',\n                        batch_norm_init=None, is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\',\n                        name=\'Conv2d_1a_3x3\'\n                    )\n\n                net = tl.layers.ConcatLayer([branch_0, branch_1], concat_dim=3)\n\n            # 71 x 71 x 192\n            with tf.variable_scope(\'Mixed_5a\'):\n                with tf.variable_scope(\'Branch_0\'):\n                    # 299 x 299 x 3\n                    branch_0, _ = conv_module(\n                        net, n_out_channel=192, filter_size=(3, 3), strides=(2, 2), padding=\'VALID\',\n                        batch_norm_init=None, is_train=is_train, use_batchnorm=True, activation_fn=\'ReLU\',\n                        name=\'Conv2d_1a_3x3\'\n                    )\n\n                with tf.variable_scope(\'Branch_1\'):\n                    branch_1 = tl.layers.MaxPool2d(net, (3, 3), strides=(2, 2), padding=\'VALID\', name=\'MaxPool_1a_3x3\')\n\n                net = tl.layers.ConcatLayer([branch_0, branch_1], concat_dim=3)\n\n            # 35 x 35 x 384\n            # 4 x Inception-A blocks\n            for idx in range(4):\n                block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n                net = block_inception_a(net, scope=block_scope, is_train=is_train)\n\n            # 35 x 35 x 384\n            # Reduction-A block\n            net = block_reduction_a(net, scope=\'Mixed_6a\', is_train=is_train)\n\n            # 17 x 17 x 1024\n            # 7 x Inception-B blocks\n            for idx in range(7):\n                block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n                net = block_inception_b(net, scope=block_scope, is_train=is_train)\n\n            # 17 x 17 x 1024\n            # Reduction-B block\n            net = block_reduction_b(net, scope=\'Mixed_7a\', is_train=is_train)\n\n            # 8 x 8 x 1536\n            # 3 x Inception-C blocks\n            for idx in range(3):\n                block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n                net = block_inception_c(net, scope=block_scope, is_train=is_train)\n\n            if self.flatten_output and not self.include_FC_head:\n                net = tl.layers.FlattenLayer(net, name=\'flatten\')\n\n            if self.include_FC_head:\n                with tf.variable_scope(""Logits"", reuse=reuse):\n\n                    # 8 x 8 x 1536\n                    net = tl.layers.MeanPool2d(\n                        net, filter_size=net.outputs.get_shape()[1:3], strides=(1, 1), padding=\'VALID\',\n                        name=\'AvgPool_1a\'\n                    )\n\n                    # 1 x 1 x 1536\n                    net = tl.layers.DropoutLayer(net, keep=0.8, is_fix=True, is_train=is_train, name=\'Dropout_1b\')\n                    net = tl.layers.FlattenLayer(net, name=\'PreLogitsFlatten\')\n\n                    # 1536\n                    net, _ = dense_module(\n                        net, n_units=1001, activation_fn=""softmax"", use_batchnorm=False, batch_norm_init=None,\n                        is_train=is_train, name=""Logits""\n                    )\n\n            return net\n'"
