file_path,api_count,code
setup.py,0,"b'from os import path\nimport setuptools\nfrom setuptools import setup, find_packages\n\nversion = int(setuptools.__version__.split(\'.\')[0])\nassert version > 30, ""Tensorpack installation requires setuptools > 30""\n\nthis_directory = path.abspath(path.dirname(__file__))\n\n# setup metainfo\nlibinfo_py = path.join(this_directory, \'tensorpack\', \'libinfo.py\')\nlibinfo_content = open(libinfo_py, ""r"").readlines()\nversion_line = [l.strip() for l in libinfo_content if l.startswith(\'__version__\')][0]\nexec(version_line)  # produce __version__\n\nwith open(path.join(this_directory, \'README.md\'), \'rb\') as f:\n    long_description = f.read().decode(\'utf-8\')\n\n\ndef add_git_version():\n\n    def get_git_version():\n        from subprocess import check_output\n        try:\n            return check_output(""git describe --tags --long --dirty"".split()).decode(\'utf-8\').strip()\n        except Exception:\n            return __version__  # noqa\n\n    newlibinfo_content = [l for l in libinfo_content if not l.startswith(\'__git_version__\')]\n    newlibinfo_content.append(\'__git_version__ = ""{}""\'.format(get_git_version()))\n    with open(libinfo_py, ""w"") as f:\n        f.write("""".join(newlibinfo_content))\n\n\nadd_git_version()\n\n\nsetup(\n    name=\'tensorpack\',\n    author=""TensorPack contributors"",\n    author_email=""ppwwyyxxc@gmail.com"",\n    url=""https://github.com/tensorpack/tensorpack"",\n    keywords=""tensorflow, deep learning, neural network"",\n    license=""Apache"",\n\n    version=__version__,   # noqa\n    description=\'A Neural Network Training Interface on TensorFlow\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n\n    packages=find_packages(exclude=[""examples"", ""tests""]),\n    zip_safe=False,  \t\t    # dataset and __init__ use file\n\n    install_requires=[\n        ""numpy>=1.14"",\n        ""six"",\n        ""termcolor>=1.1"",\n        ""tabulate>=0.7.7"",\n        ""tqdm>4.29.0"",\n        ""msgpack>=0.5.2"",\n        ""msgpack-numpy>=0.4.4.2"",\n        ""pyzmq>=16"",\n        ""psutil>=5"",\n    ],\n    tests_require=[\'flake8\', \'scikit-image\'],\n    extras_require={\n        \'all\': [\'scipy\', \'h5py\', \'lmdb>=0.92\', \'matplotlib\', \'scikit-learn\'],\n        \'all: ""linux"" in sys_platform\': [\'python-prctl\'],\n    },\n\n    # https://packaging.python.org/guides/distributing-packages-using-setuptools/#universal-wheels\n    options={\'bdist_wheel\': {\'universal\': \'1\'}},\n)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n# flake8: noqa\n# tensorpack documentation build configuration file, created by\n# sphinx-quickstart on Sun Mar 27 01:41:24 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys, os, re\nimport mock\nimport inspect\nfrom sphinx.domains import Domain\n\nclass GithubURLDomain(Domain):\n    """"""\n    Resolve certain links in markdown files to github source.\n    """"""\n\n    name = ""githuburl""\n    ROOT = ""https://github.com/tensorpack/tensorpack/blob/master/""\n\n    def resolve_any_xref(self, env, fromdocname, builder, target, node, contnode):\n        github_url = None\n        if "".html"" not in target:\n            if target.startswith(""../../"") and not target.startswith(""../../modules""):\n                url = target.replace(""../"", """")\n                github_url = url\n\n        if github_url is not None:\n            if github_url.endswith(""README""):\n                # bug of recommonmark.\n                # https://github.com/readthedocs/recommonmark/blob/ddd56e7717e9745f11300059e4268e204138a6b1/recommonmark/parser.py#L152-L155\n                github_url += "".md""\n            print(""Ref {} resolved to github:{}"".format(target, github_url))\n            contnode[""refuri""] = self.ROOT + github_url\n            return [(""githuburl:any"", contnode)]\n        else:\n            return []\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'../\'))\nos.environ[\'DOC_BUILDING\'] = \'1\'\nON_RTD = (os.environ.get(\'READTHEDOCS\') == \'True\')\n\n\nMOCK_MODULES = [\'tabulate\', \'h5py\',\n                \'cv2\', \'zmq\', \'lmdb\',\n                \'msgpack\', \'msgpack_numpy\', \'pyarrow\',\n                \'sklearn\', \'sklearn.datasets\',\n                \'scipy\', \'scipy.misc\', \'scipy.io\',\n                \'tornado\', \'tornado.concurrent\',\n                \'horovod\', \'horovod.tensorflow\',\n                \'subprocess32\', \'functools32\', \'psutil\']\n\n# it\'s better to have tensorflow installed (for some docs to show)\n# but it\'s OK to mock it as well\ntry:\n    import tensorflow\nexcept ImportError:\n    mod = sys.modules[\'tensorflow\'] = mock.Mock(name=\'tensorflow\')\n    mod.__version__ = mod.VERSION = \'1.12\'\n    MOCK_MODULES.extend([\'tensorflow.python.training.monitored_session\'])\n    MOCK_MODULES.extend([\'tensorflow.python.training\'])\n    MOCK_MODULES.extend([\'tensorflow.python.client\'])\n    MOCK_MODULES.extend([\'tensorflow.python.framework\'])\n    MOCK_MODULES.extend([\'tensorflow.python.platform\'])\n    MOCK_MODULES.extend([\'tensorflow.python.tools\'])\n    MOCK_MODULES.extend([\'tensorflow.contrib.graph_editor\'])\n\nfor mod_name in MOCK_MODULES:\n    sys.modules[mod_name] = mock.Mock(name=mod_name)\nsys.modules[\'cv2\'].__version__ = \'3.2.1\'    # fake version\nsys.modules[\'msgpack\'].version = (0, 5, 2)\n\nimport tensorpack\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\nneeds_sphinx = \'3.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'recommonmark\',\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.napoleon\',\n    #\'sphinx.ext.autosectionlabel\',\n    #\'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.viewcode\',\n]\n\n# -- Configurations for plugins ------------\nnapoleon_google_docstring = True\nnapoleon_include_init_with_doc = True\nnapoleon_include_special_with_doc = True\nnapoleon_numpy_docstring = False\nnapoleon_use_rtype = False\n\nif ON_RTD:\n    intersphinx_timeout = 10\nelse:\n    # skip this when building locally\n    intersphinx_timeout = 0.1\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3.6\', None),\n    \'numpy\': (\'https://docs.scipy.org/doc/numpy/\', None),\n}\n# -------------------------\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\nsource_suffix = [\'.rst\', \'.md\']\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'tensorpack\'\ncopyright = u\'2015 - 2020, Yuxin Wu, et al.\'\nauthor = u\'Yuxin Wu, et al.\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = tensorpack.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'build\', \'README.md\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\nadd_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\nadd_module_names = True\n# \'tensorpack.\' prefix was removed by js\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\nshow_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\nmodindex_common_prefix = [\'tensorpack.\']\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\nkeep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nimport sphinx_rtd_theme\nhtml_theme = ""sphinx_rtd_theme""\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {}\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\nhtml_favicon = \'_static/favicon.ico\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\nhtml_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\nhtml_domain_indices = True\n\n# If false, no index is generated.\nhtml_use_index = True\n\n# If true, the index is split into individual pages for each letter.\nhtml_split_index = False\n\n# If true, links to the reST sources are added to the pages.\nhtml_show_sourcelink = False\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\nhtml_show_sphinx = False\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\nhtml_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# avoid li fonts being larger\n# TODO but li indices fonts are still larger\nhtml_compact_lists = False\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\'\nhtml_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'tensorpackdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'tensorpack.tex\', u\'tensorpack documentation\',\n     author, \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'tensorpack\', u\'tensorpack documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'tensorpack\', u\'tensorpack documentation\',\n     author, \'tensorpack\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\nsuppress_warnings = [\'image.nonlocal_uri\']\n\n#autodoc_member_order = \'bysource\'\n\ndef process_signature(app, what, name, obj, options, signature,\n            return_annotation):\n    if signature:\n        # replace Mock function names\n        signature = re.sub(\'<Mock name=\\\'([^\\\']+)\\\'.*>\', \'\\g<1>\', signature)\n        signature = re.sub(\'tensorflow\', \'tf\', signature)\n\n        # add scope name to layer signatures:\n        if hasattr(obj, \'use_scope\'):\n            if obj.use_scope:\n                signature = signature[0] + \'variable_scope_name, \' + signature[1:]\n            elif obj.use_scope is None:\n                signature = signature[0] + \'[variable_scope_name,] \' + signature[1:]\n    # signature: arg list\n    return signature, return_annotation\n\n\n_DEPRECATED_NAMES = set([\n    # deprecated stuff:\n    \'QueueInputTrainer\',\n    \'dump_dataflow_to_process_queue\',\n    \'DistributedTrainerReplicated\',\n    \'DistributedTrainerParameterServer\',\n    \'Augmentor\',\n    ""get_model_loader"",\n\n    # renamed items that should not appear in docs\n    \'DumpTensor\',\n    \'DumpParamAsImage\',\n    \'get_nr_gpu\',\n    \'TrainingMonitor\',\n    \'PeakMemoryTracker\',\n    \'TowerFuncWrapper\',\n\n    \'PrefetchData\',\n    \'MultiProcessPrefetchData\',\n    \'PrefetchDataZMQ\',\n    \'MultiThreadPrefetchData\',\n\n    # deprecated or renamed symbolic code\n    \'Deconv2D\',\n\n    # shouldn\'t appear in doc:\n    \'l2_regularizer\', \'l1_regularizer\',\n\n    # internal only\n    \'execute_only_once\',\n    \'humanize_time_delta\',\n    \'SessionUpdate\',\n    \'get_checkpoint_path\',\n    \'IterSpeedCounter\'\n])\n\ndef autodoc_skip_member(app, what, name, obj, skip, options):\n    # we hide something deliberately\n    if getattr(obj, \'__HIDE_SPHINX_DOC__\', False):\n        return True\n    if name == \'__init__\':\n        if obj.__doc__ and skip:\n            # include_init_with_doc doesn\'t work well for decorated init\n            # https://github.com/sphinx-doc/sphinx/issues/4258\n            return False\n    # Hide some names that are deprecated or not intended to be used\n    if name in _DEPRECATED_NAMES:\n        return True\n\n    if name in [\'__iter__\', \'__len__\', \'reset_state\', \'get_data\', \'size\']:\n        # skip these methods with empty docstring\n        if not obj.__doc__ and inspect.isfunction(obj):\n            # https://stackoverflow.com/questions/3589311/get-defining-class-of-unbound-method-object-in-python-3\n            cls = getattr(inspect.getmodule(obj),\n                          obj.__qualname__.split(\'.<locals>\', 1)[0].rsplit(\'.\', 1)[0])\n            if issubclass(cls, tensorpack.DataFlow):\n                return True\n    return None\n\ndef setup(app):\n    from recommonmark.transform import AutoStructify\n    app.add_domain(GithubURLDomain)\n    app.connect(\'autodoc-process-signature\', process_signature)\n    app.connect(\'autodoc-skip-member\', autodoc_skip_member)\n    app.add_config_value(\n        \'recommonmark_config\',\n        {\'auto_toc_tree_section\': \'Contents\',\n         \'enable_math\': True,\n         \'enable_inline_math\': True,\n         \'enable_eval_rst\': True\n        }, True)\n    app.add_transform(AutoStructify)\n'"
examples/boilerplate.py,5,"b'# -*- coding: utf-8 -*-\n# Author: Your Name <your@email.com>\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\n\n""""""\nThis is a boiler-plate template.\nAll code is in this file is the most minimalistic way to solve a deep-learning problem with cross-validation.\n""""""\n\nBATCH_SIZE = 16\nSHAPE = 28\nCHANNELS = 3\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, SHAPE, SHAPE, CHANNELS), tf.float32, \'input1\'),\n                tf.TensorSpec((None,), tf.int32, \'input2\')]\n\n    def build_graph(self, input1, input2):\n\n        cost = tf.identity(input1 - input2, name=\'total_costs\')\n        summary.add_moving_summary(cost)\n        return cost\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=5e-3, trainable=False)\n        return tf.train.AdamOptimizer(lr)\n\n\ndef get_data(subset):\n    # something that yields [[SHAPE, SHAPE, CHANNELS], [1]]\n    ds = FakeData([[SHAPE, SHAPE, CHANNELS], [1]], 1000, random=False,\n                  dtype=[\'float32\', \'uint8\'], domain=[(0, 255), (0, 10)])\n    ds = MultiProcessRunnerZMQ(ds, 2)\n    ds = BatchData(ds, BATCH_SIZE)\n    return ds\n\n\ndef get_config():\n    logger.auto_set_dir()\n\n    ds_train = get_data(\'train\')\n    ds_test = get_data(\'test\')\n\n    return TrainConfig(\n        model=Model(),\n        data=QueueInput(ds_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(ds_test, [ScalarStats(\'total_costs\')]),\n        ],\n        steps_per_epoch=len(ds_train),\n        max_epoch=100,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    config = get_config()\n    config.session_init = SmartInit(args.load)\n\n    launch_train_with_config(config, SimpleTrainer())\n'"
scripts/checkpoint-manipulate.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: checkpoint-manipulate.py\n\n\nimport argparse\nimport numpy as np\n\nfrom tensorpack.tfutils.varmanip import load_chkpt_vars\nfrom tensorpack.utils import logger\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'model\')\n    parser.add_argument(\'--dump\', help=\'dump to an npz file\')\n    parser.add_argument(\'--shell\', action=\'store_true\', help=\'start a shell with the params\')\n    args = parser.parse_args()\n\n    if args.model.endswith(\'.npy\'):\n        params = np.load(args.model, encoding=\'latin1\').item()\n    elif args.model.endswith(\'.npz\'):\n        params = dict(np.load(args.model))\n    else:\n        params = load_chkpt_vars(args.model)\n    logger.info(""Variables in the model:"")\n    logger.info(str(params.keys()))\n\n    if args.dump:\n        assert args.dump.endswith(\'.npz\'), args.dump\n        np.savez(args.dump, **params)\n\n    if args.shell:\n        # params is a dict. play with it\n        import IPython as IP\n        IP.embed(config=IP.terminal.ipapp.load_default_config())\n'"
scripts/checkpoint-prof.py,12,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: checkpoint-prof.py\n\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorpack import get_default_sess_config, get_op_tensor_name\nfrom tensorpack.tfutils.sessinit import SmartInit\nfrom tensorpack.utils import logger\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--model\', help=\'model file\')\n    parser.add_argument(\'--meta\', help=\'metagraph proto file. Will be used to load the graph\', required=True)\n    parser.add_argument(\'-i\', \'--input\', nargs=\'+\', help=\'list of input tensors with their shapes.\')\n    parser.add_argument(\'-o\', \'--output\', nargs=\'+\', help=\'list of output tensors\')\n    parser.add_argument(\'--warmup\', help=\'warmup iterations\', type=int, default=5)\n    parser.add_argument(\'--print-flops\', action=\'store_true\')\n    parser.add_argument(\'--print-params\', action=\'store_true\')\n    parser.add_argument(\'--print-timing\', action=\'store_true\')\n    args = parser.parse_args()\n\n    tf.train.import_meta_graph(args.meta, clear_devices=True)\n    G = tf.get_default_graph()\n    with tf.Session(config=get_default_sess_config()) as sess:\n        init = SmartInit(args.model)\n        init.init(sess)\n\n        feed = {}\n        for inp in args.input:\n            inp = inp.split(\'=\')\n            name = get_op_tensor_name(inp[0].strip())[1]\n            shape = list(map(int, inp[1].strip().split(\',\')))\n            tensor = G.get_tensor_by_name(name)\n            logger.info(""Feeding shape ({}) to tensor {}"".format(\',\'.join(map(str, shape)), name))\n            feed[tensor] = np.random.rand(*shape)\n\n        fetches = []\n        for name in args.output:\n            name = get_op_tensor_name(name)[1]\n            fetches.append(G.get_tensor_by_name(name))\n        logger.info(""Fetching tensors: {}"".format(\', \'.join([k.name for k in fetches])))\n\n        for _ in range(args.warmup):\n            sess.run(fetches, feed_dict=feed)\n\n        opt = tf.RunOptions()\n        opt.trace_level = tf.RunOptions.FULL_TRACE\n        meta = tf.RunMetadata()\n        sess.run(fetches, feed_dict=feed, options=opt, run_metadata=meta)\n\n        if args.print_flops:\n            tf.profiler.profile(\n                G,\n                run_meta=meta,\n                cmd=\'op\',\n                options=tf.profiler.ProfileOptionBuilder.float_operation())\n\n        if args.print_params:\n            tf.profiler.profile(\n                G,\n                run_meta=meta,\n                options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())\n\n        if args.print_timing:\n            tf.profiler.profile(\n                G,\n                run_meta=meta,\n                options=tf.profiler.ProfileOptionBuilder.time_and_memory())\n'"
scripts/dump-model-params.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: dump-model-params.py\n\nimport argparse\nimport numpy as np\nimport os\nimport six\nimport tensorflow as tf\n\nfrom tensorpack import logger\nfrom tensorpack.tfutils import varmanip\nfrom tensorpack.tfutils.common import get_op_tensor_name, get_tf_version_tuple\n\nTF_version = get_tf_version_tuple()\n\n\ndef _import_external_ops(message):\n    if ""horovod"" in message.lower():\n        logger.info(""Importing horovod ..."")\n        import horovod.tensorflow  # noqa\n        return\n    if ""MaxBytesInUse"" in message:\n        logger.info(""Importing memory_stats ..."")\n        from tensorflow.contrib.memory_stats import MaxBytesInUse  # noqa\n        return\n    if \'Nccl\' in message:\n        logger.info(""Importing nccl ..."")\n        if TF_version <= (1, 12):\n            try:\n                from tensorflow.contrib.nccl.python.ops.nccl_ops import _validate_and_load_nccl_so\n            except Exception:\n                pass\n            else:\n                _validate_and_load_nccl_so()\n            from tensorflow.contrib.nccl.ops import gen_nccl_ops  # noqa\n        else:\n            from tensorflow.python.ops import gen_nccl_ops  # noqa\n        return\n    if \'ZMQConnection\' in message:\n        import zmq_ops  # noqa\n        return\n    logger.error(""Unhandled error: "" + message)\n\n\ndef guess_inputs(input_dir):\n    meta_candidates = []\n    model_candidates = []\n    for path in os.listdir(input_dir):\n        if path.startswith(\'graph-\') and path.endswith(\'.meta\'):\n            meta_candidates.append(path)\n        if path.startswith(\'model-\') and path.endswith(\'.index\'):\n            modelid = int(path[len(\'model-\'):-len(\'.index\')])\n            model_candidates.append((path, modelid))\n    assert len(meta_candidates)\n    meta = sorted(meta_candidates)[-1]\n    if len(meta_candidates) > 1:\n        logger.info(""Choosing {} from {} as graph file."".format(meta, meta_candidates))\n    else:\n        logger.info(""Choosing {} as graph file."".format(meta))\n\n    assert len(model_candidates)\n    model = sorted(model_candidates, key=lambda x: x[1])[-1][0]\n    if len(model_candidates) > 1:\n        logger.info(""Choosing {} from {} as model file."".format(model, [x[0] for x in model_candidates]))\n    else:\n        logger.info(""Choosing {} as model file."".format(model))\n    return os.path.join(input_dir, model), os.path.join(input_dir, meta)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'Keep only TRAINABLE and MODEL variables in a checkpoint.\')\n    parser.add_argument(\'--meta\', help=\'metagraph file\')\n    parser.add_argument(dest=\'input\', help=\'input model file, has to be a TF checkpoint\')\n    parser.add_argument(dest=\'output\', help=\'output model file, can be npz or TF checkpoint\')\n    args = parser.parse_args()\n\n    if os.path.isdir(args.input):\n        input, meta = guess_inputs(args.input)\n    else:\n        meta = args.meta\n        input = args.input\n\n    # this script does not need GPU\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n\n    if args.meta is not None:\n        while True:\n            try:\n                tf.reset_default_graph()\n                tf.train.import_meta_graph(meta, clear_devices=True)\n            except KeyError as e:\n                op_name = e.args[0]\n                _import_external_ops(op_name)\n            except tf.errors.NotFoundError as e:\n                _import_external_ops(str(e))\n            else:\n                break\n\n    # loading...\n    if input.endswith(\'.npz\'):\n        dic = np.load(input)\n    else:\n        dic = varmanip.load_chkpt_vars(input)\n    dic = {get_op_tensor_name(k)[1]: v for k, v in six.iteritems(dic)}\n\n    if args.meta is not None:\n        # save variables that are GLOBAL, and either TRAINABLE or MODEL\n        var_to_dump = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n        var_to_dump.extend(tf.get_collection(tf.GraphKeys.MODEL_VARIABLES))\n        if len(set(var_to_dump)) != len(var_to_dump):\n            logger.warn(""TRAINABLE and MODEL variables have duplication!"")\n        var_to_dump = list(set(var_to_dump))\n        globvarname = {k.name for k in tf.global_variables()}\n        var_to_dump = {k.name for k in var_to_dump if k.name in globvarname}\n\n        for name in var_to_dump:\n            assert name in dic, ""Variable {} not found in the model!"".format(name)\n    else:\n        var_to_dump = set(dic.keys())\n\n    dic_to_dump = {k: v for k, v in six.iteritems(dic) if k in var_to_dump}\n    varmanip.save_chkpt_vars(dic_to_dump, args.output)\n'"
scripts/ls-checkpoint.py,1,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: ls-checkpoint.py\n\nimport numpy as np\nimport pprint\nimport sys\nimport six\nimport tensorflow as tf\n\nfrom tensorpack.tfutils.varmanip import get_checkpoint_path\n\nif __name__ == '__main__':\n    fpath = sys.argv[1]\n\n    if fpath.endswith('.npy'):\n        params = np.load(fpath, encoding='latin1').item()\n        dic = {k: v.shape for k, v in six.iteritems(params)}\n    elif fpath.endswith('.npz'):\n        params = dict(np.load(fpath))\n        dic = {k: v.shape for k, v in six.iteritems(params)}\n    else:\n        path = get_checkpoint_path(fpath)\n        reader = tf.train.NewCheckpointReader(path)\n        dic = reader.get_variable_to_shape_map()\n    pprint.pprint(dic)\n"""
sotabench/sotabench.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport tqdm\nfrom contextlib import contextmanager\n\nfrom tensorpack.predict import OfflinePredictor, PredictConfig\nfrom tensorpack.tfutils import SmartInit\nfrom tensorpack.utils.fs import download\n\nfrom sotabencheval.utils import is_server\nfrom sotabencheval.object_detection import COCOEvaluator\n\n# import faster rcnn example\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), "".."", ""examples"", ""FasterRCNN""))\nfrom config import finalize_configs, config as cfg  # noqa\nfrom eval import predict_image  # noqa\nfrom dataset import register_coco  # noqa\nfrom dataset.coco import COCODetection  # noqa\nfrom data import get_eval_dataflow  # noqa\nfrom modeling.generalized_rcnn import ResNetFPNModel, ResNetC4Model  # noqa\n\n\nif is_server():\n    DATA_ROOT = ""./.data/vision/""\nelse:  # local settings\n    DATA_ROOT = os.path.expanduser(""~/data/"")\nCOCO_ROOT = os.path.join(DATA_ROOT, ""coco"")\n\n\nregister_coco(COCO_ROOT)\n\n\n@contextmanager\ndef backup_cfg():\n    orig_config = cfg.to_dict()\n    yield\n    cfg.from_dict(orig_config)\n\n\ndef evaluate_rcnn(model_name, paper_arxiv_id, cfg_list, model_file):\n    evaluator = COCOEvaluator(\n        root=COCO_ROOT, model_name=model_name, paper_arxiv_id=paper_arxiv_id\n    )\n    category_id_to_coco_id = {\n        v: k for k, v in COCODetection.COCO_id_to_category_id.items()\n    }\n\n    cfg.update_args(cfg_list)  # TODO backup/restore config\n    finalize_configs(False)\n    MODEL = ResNetFPNModel() if cfg.MODE_FPN else ResNetC4Model()\n    predcfg = PredictConfig(\n        model=MODEL,\n        session_init=SmartInit(model_file),\n        input_names=MODEL.get_inference_tensor_names()[0],\n        output_names=MODEL.get_inference_tensor_names()[1],\n    )\n    predictor = OfflinePredictor(predcfg)\n\n    def xyxy_to_xywh(box):\n        box[2] -= box[0]\n        box[3] -= box[1]\n        return box\n\n    df = get_eval_dataflow(""coco_val2017"")\n    df.reset_state()\n    for img, img_id in tqdm.tqdm(df, total=len(df)):\n        results = predict_image(img, predictor)\n        res = [\n            {\n                ""image_id"": img_id,\n                ""category_id"": category_id_to_coco_id.get(\n                    int(r.class_id), int(r.class_id)\n                ),\n                ""bbox"": xyxy_to_xywh([round(float(x), 4) for x in r.box]),\n                ""score"": round(float(r.score), 3),\n            }\n            for r in results\n        ]\n        evaluator.add(res)\n        if evaluator.cache_exists:\n            break\n\n    evaluator.save()\n\n\ndownload(\n    ""http://models.tensorpack.com/FasterRCNN/COCO-MaskRCNN-R50FPN2x.npz"",\n    ""./"",\n    expect_size=165362754)\nwith backup_cfg():\n    evaluate_rcnn(\n        ""Mask R-CNN (ResNet-50-FPN, 2x)"", ""1703.06870"", [],\n        ""COCO-MaskRCNN-R50FPN2x.npz"",\n    )\n\n\ndownload(\n    ""http://models.tensorpack.com/FasterRCNN/COCO-MaskRCNN-R50FPN2xGN.npz"",\n    ""./"",\n    expect_size=167363872)\nwith backup_cfg():\n    evaluate_rcnn(\n        ""Mask R-CNN (ResNet-50-FPN, GroupNorm)"", ""1803.08494"",\n        """"""FPN.NORM=GN BACKBONE.NORM=GN\nFPN.FRCNN_HEAD_FUNC=fastrcnn_4conv1fc_gn_head\nFPN.MRCNN_HEAD_FUNC=maskrcnn_up4conv_gn_head"""""".split(),\n        ""COCO-MaskRCNN-R50FPN2xGN.npz"",\n    )\n\n\ndownload(\n    ""http://models.tensorpack.com/FasterRCNN/COCO-MaskRCNN-R101FPN9xGNCasAugScratch.npz"",\n    ""./"",\n    expect_size=355680386)\nwith backup_cfg():\n    evaluate_rcnn(\n        ""Mask R-CNN (ResNet-101-FPN, GN, Cascade)"", ""1811.08883"",\n        """"""\n    FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCKS=[3,4,23,3] FPN.NORM=GN\n    BACKBONE.NORM=GN FPN.FRCNN_HEAD_FUNC=fastrcnn_4conv1fc_gn_head\n    FPN.MRCNN_HEAD_FUNC=maskrcnn_up4conv_gn_head"""""".split(),\n        ""COCO-MaskRCNN-R101FPN9xGNCasAugScratch.npz"",\n    )\n'"
tensorpack/__init__.py,0,"b""# -*- coding: utf-8 -*-\n# File: __init__.py\n\n\nfrom tensorpack.libinfo import __version__, __git_version__, _HAS_TF\n\nfrom tensorpack.utils import *\nfrom tensorpack.dataflow import *\n\n# dataflow can be used alone without installing tensorflow\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()['kcah_acitats'[::-1].upper()] = _HAS_TF\nif STATICA_HACK:\n    from tensorpack.models import *\n\n    from tensorpack.callbacks import *\n    from tensorpack.tfutils import *\n\n    from tensorpack.train import *\n    from tensorpack.input_source import *\n    from tensorpack.predict import *\n"""
tensorpack/libinfo.py,5,"b'\nimport os\n\n# issue#7378 may happen with custom opencv. It doesn\'t hurt to disable opencl\nos.environ[\'OPENCV_OPENCL_RUNTIME\'] = \'disabled\'     # https://github.com/opencv/opencv/pull/10155\ntry:\n    # issue#1924 may happen on old systems\n    import cv2  # noqa\n    # cv2.setNumThreads(0)\n    if int(cv2.__version__.split(\'.\')[0]) >= 3:\n        cv2.ocl.setUseOpenCL(False)\n    # check if cv is built with cuda or openmp\n    info = cv2.getBuildInformation().split(\'\\n\')\n    for line in info:\n        splits = line.split()\n        if not len(splits):\n            continue\n        answer = splits[-1].lower()\n        if answer in [\'yes\', \'no\']:\n            if \'cuda\' in line.lower() and answer == \'yes\':\n                # issue#1197\n                print(""OpenCV is built with CUDA support. ""\n                      ""This may cause slow initialization or sometimes segfault with TensorFlow."")\n        if answer == \'openmp\':\n            print(""OpenCV is built with OpenMP support. This usually results in poor performance. For details, see ""\n                  ""https://github.com/tensorpack/benchmarks/blob/master/ImageNet/benchmark-opencv-resize.py"")\nexcept (ImportError, TypeError):\n    pass\n\nos.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'  # issue#9339\nos.environ[\'TF_AUTOTUNE_THRESHOLD\'] = \'2\'   # use more warm-up\n\n# Since 1.3, this is not needed\nos.environ[\'TF_AVGPOOL_USE_CUDNN\'] = \'1\'   # issue#8566\n\n# TF1.5 features\nos.environ[\'TF_SYNC_ON_FINISH\'] = \'0\'   # will become default\nos.environ[\'TF_GPU_THREAD_MODE\'] = \'gpu_private\'\nos.environ[\'TF_GPU_THREAD_COUNT\'] = \'2\'\n\n# Available in TF1.6+ & cudnn7. Haven\'t seen different performance on R50.\n# NOTE we disable it because:\n# this mode may use scaled atomic integer reduction that may cause a numerical\n# overflow for certain input data range.\nos.environ[\'TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\'] = \'0\'\n\n# Available since 1.12. issue#15874\n# But they\'re sometimes buggy. We leave this decision to users.\n# os.environ[\'TF_ENABLE_WHILE_V2\'] = \'1\'\n# os.environ[\'TF_ENABLE_COND_V2\'] = \'1\'\n\ntry:\n    import tensorflow as tf  # noqa\n    _version = tf.__version__.split(\'.\')\n    assert (int(_version[0]), int(_version[1])) >= (1, 3), ""TF>=1.3 is required!""\n    _HAS_TF = True\nexcept ImportError:\n    print(""Failed to import tensorflow."")\n    _HAS_TF = False\nelse:\n    # Install stacktrace handler\n    try:\n        from tensorflow.python.framework import test_util\n        test_util.InstallStackTraceHandler()\n    except Exception:\n        pass\n\n    # silence the massive deprecation warnings in TF 1.13+\n    if (int(_version[0]), int(_version[1])) >= (1, 13):\n        try:\n            from tensorflow.python.util.deprecation import silence\n        except Exception:\n            pass\n        else:\n            silence().__enter__()\n        try:\n            from tensorflow.python.util import deprecation_wrapper\n            deprecation_wrapper._PER_MODULE_WARNING_LIMIT = 0\n        except Exception:\n            pass\n\n    # Monkey-patch tf.test.is_gpu_available to avoid side effects:\n    # https://github.com/tensorflow/tensorflow/issues/26460\n    try:\n        list_dev = tf.config.experimental.list_physical_devices\n    except AttributeError:\n        pass\n    else:\n        old_is_gpu_available = tf.test.is_gpu_available\n\n        def is_gpu_available(*args, **kwargs):\n            if len(args) == 0 and len(kwargs) == 0:\n                return len(list_dev(\'GPU\')) > 0\n            return old_is_gpu_available(*args, **kwargs)\n\n        tf.test.is_gpu_available = is_gpu_available\n\n\n# These lines will be programatically read/write by setup.py\n# Don\'t touch them.\n__version__ = \'0.10.1\'\n__git_version__ = __version__\n'"
tests/benchmark-serializer.py,0,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport argparse\nimport pyarrow as pa\nfrom tabulate import tabulate\nimport operator\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.serialize import (\n    MsgpackSerializer,\n    PyarrowSerializer,\n    PickleSerializer,\n    ForkingPickler,\n)\nfrom tensorpack.utils.timer import Timer\n\n\ndef benchmark_serializer(dumps, loads, data, num):\n    buf = dumps(data)\n\n    enc_timer = Timer()\n    dec_timer = Timer()\n    enc_timer.pause()\n    dec_timer.pause()\n\n    for k in range(num):\n        enc_timer.resume()\n        buf = dumps(data)\n        enc_timer.pause()\n\n        dec_timer.resume()\n        loads(buf)\n        dec_timer.pause()\n\n    dumps_time = enc_timer.seconds() / num\n    loads_time = dec_timer.seconds() / num\n    return dumps_time, loads_time\n\n\ndef display_results(name, results):\n    logger.info(""Encoding benchmark for {}:"".format(name))\n    data = sorted(((x, y[0]) for x, y in results), key=operator.itemgetter(1))\n    print(tabulate(data, floatfmt=\'.5f\'))\n\n    logger.info(""Decoding benchmark for {}:"".format(name))\n    data = sorted(((x, y[1]) for x, y in results), key=operator.itemgetter(1))\n    print(tabulate(data, floatfmt=\'.5f\'))\n\n\ndef benchmark_all(name, serializers, data, num=30):\n    logger.info(""Benchmarking {} ..."".format(name))\n    results = []\n    for serializer_name, dumps, loads in serializers:\n        results.append((serializer_name, benchmark_serializer(dumps, loads, data, num=num)))\n    display_results(name, results)\n\n\ndef fake_json_data():\n    return {\n        \'words\': """"""\n            Lorem ipsum dolor sit amet, consectetur adipiscing\n            elit. Mauris adipiscing adipiscing placerat.\n            Vestibulum augue augue,\n            pellentesque quis sollicitudin id, adipiscing.\n            """""" * 100,\n        \'list\': list(range(100)) * 500,\n        \'dict\': {str(i): \'a\' for i in range(50000)},\n        \'dict2\': {i: \'a\' for i in range(50000)},\n        \'int\': 3000,\n        \'float\': 100.123456\n    }\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""task"")\n    args = parser.parse_args()\n\n    serializers = [\n        (""msgpack"", MsgpackSerializer.dumps, MsgpackSerializer.loads),\n        (""pyarrow-buf"", PyarrowSerializer.dumps, PyarrowSerializer.loads),\n        (""pyarrow-bytes"", PyarrowSerializer.dumps_bytes, PyarrowSerializer.loads),\n        (""pickle"", PickleSerializer.dumps, PickleSerializer.loads),\n        (""forking-pickle"", ForkingPickler.dumps, ForkingPickler.loads),\n    ]\n\n    if args.task == ""numpy"":\n        numpy_data = [np.random.rand(64, 224, 224, 3).astype(""float32""), np.random.rand(64).astype(\'int32\')]\n        benchmark_all(""numpy data"", serializers, numpy_data)\n    elif args.task == ""json"":\n        benchmark_all(""json data"", serializers, fake_json_data(), num=50)\n    elif args.task == ""torch"":\n        import torch\n        from pyarrow.lib import _default_serialization_context\n\n        pa.register_torch_serialization_handlers(_default_serialization_context)\n        torch_data = [torch.rand(64, 224, 224, 3), torch.rand(64).to(dtype=torch.int32)]\n        benchmark_all(""torch data"", serializers[1:], torch_data)\n'"
tests/case_script.py,0,"b'from abc import abstractproperty\nimport unittest\nimport subprocess\nimport shlex\nimport sys\nimport threading\nimport os\nimport shutil\n\n\nclass PythonScript(threading.Thread):\n    """"""A wrapper to start a python script with timeout.\n\n    To test the actual models even without GPUs we simply start them and\n    test whether they survive a certain amount of time ""timeout"". This allows to\n    test if all imports are correct and the computation graph can be built without\n    run the entire model on the CPU.\n\n    Attributes:\n        cmd (str): command to execute the example with all flags (including python)\n        p: process handle\n        timeout (int): timeout in seconds\n    """"""\n    def __init__(self, cmd, timeout):\n        """"""Prepare a python script\n\n        Args:\n            cmd (str): command to execute the example with all flags (including python)\n            timeout (int): time in seconds the script has to survive\n        """"""\n        threading.Thread.__init__(self)\n        self.cmd = cmd\n        self.timeout = timeout\n\n    def run(self):\n        self.p = subprocess.Popen(shlex.split(self.cmd), stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n        self.out, self.err = self.p.communicate()\n\n    def execute(self):\n        """"""Execute python script in other process.\n\n        Raises:\n            SurviveException: contains the error message of the script if it terminated before timeout\n        """"""\n        self.start()\n        self.join(self.timeout)\n\n        if self.is_alive():\n            self.p.terminate()\n            self.p.kill()  # kill -9\n            self.join()\n        else:\n            # something unexpected happend here, this script was supposed to survive at least the timeout\n            if len(self.err) > 0:\n                output = u""STDOUT: \\n\\n\\n"" + self.out.decode(\'utf-8\')\n                output += u""\\n\\n\\n STDERR: \\n\\n\\n"" + self.err.decode(\'utf-8\')\n                raise AssertionError(output)\n\n\nclass TestPythonScript(unittest.TestCase):\n\n    @abstractproperty\n    def script(self):\n        pass\n\n    @staticmethod\n    def clear_trainlog(script):\n        script = os.path.basename(script)\n        script = script[:-3]\n        if os.path.isdir(os.path.join(""train_log"", script)):\n            shutil.rmtree(os.path.join(""train_log"", script))\n\n    def assertSurvive(self, script, args=None, timeout=20):  # noqa\n        cmd = ""python{} {}"".format(sys.version_info.major, script)\n        if args:\n            cmd += "" "" + "" "".join(args)\n        PythonScript(cmd, timeout=timeout).execute()\n\n    def setUp(self):\n        TestPythonScript.clear_trainlog(self.script)\n\n    def tearDown(self):\n        TestPythonScript.clear_trainlog(self.script)\n'"
tests/test_char_rnn.py,0,"b""import os\n\nfrom case_script import TestPythonScript\n\n\ndef random_content():\n    return ('Lorem ipsum dolor sit amet\\n'\n            'consetetur sadipscing elitr\\n'\n            'sed diam nonumy eirmod tempor invidunt ut labore\\n')\n\n\nclass CharRNNTest(TestPythonScript):\n\n    @property\n    def script(self):\n        return '../examples/Char-RNN/char-rnn.py'\n\n    def setUp(self):\n        super(CharRNNTest, self).setUp()\n        with open('input.txt', 'w') as f:\n            f.write(random_content())\n\n    def test(self):\n        self.assertSurvive(self.script, args=['train'])\n\n    def tearDown(self):\n        super(CharRNNTest, self).tearDown()\n        os.remove('input.txt')\n"""
tests/test_infogan.py,0,"b""from case_script import TestPythonScript\n\nfrom tensorpack.tfutils.common import get_tf_version_tuple\n\n\nclass InfoGANTest(TestPythonScript):\n\n    @property\n    def script(self):\n        return '../examples/GAN/InfoGAN-mnist.py'\n\n    def test(self):\n        return True  # https://github.com/tensorflow/tensorflow/issues/24517\n        if get_tf_version_tuple() < (1, 4):\n            return True     # requires leaky_relu\n        self.assertSurvive(self.script, args=None)\n"""
tests/test_mnist.py,0,"b""from case_script import TestPythonScript\n\n\nclass MnistTest(TestPythonScript):\n\n    @property\n    def script(self):\n        return '../examples/basics/mnist-convnet.py'\n\n    def test(self):\n        self.assertSurvive(self.script, args=None)\n"""
tests/test_mnist_similarity.py,0,"b""from case_script import TestPythonScript\n\n\nclass SimilarityLearningTest(TestPythonScript):\n\n    @property\n    def script(self):\n        return '../examples/SimilarityLearning/mnist-embeddings.py'\n\n    def test(self):\n        self.assertSurvive(self.script, args=['--algorithm triplet'], timeout=10)\n"""
tests/test_resnet.py,0,"b""from case_script import TestPythonScript  # noqa\n\n# this tests occasionally fails (memory issue on travis?)\n\n\n# class ResnetTest(TestPythonScript):\n#     @property\n#     def script(self):\n#         return '../examples/ResNet/imagenet-resnet.py'\n#\n#     def test(self):\n#         self.assertSurvive(\n#             self.script,\n#             args=['--fake', '--data_format NHWC'], timeout=20)\n"""
examples/A3C-Gym/atari_wrapper.py,0,"b'# -*- coding: utf-8 -*-\n# File: atari_wrapper.py\n\nimport numpy as np\nfrom collections import deque\nimport gym\n\n_v0, _v1 = gym.__version__.split(\'.\')[:2]\nassert int(_v0) > 0 or int(_v1) >= 10, gym.__version__\n\n\n""""""\nThe following wrappers are copied or modified from openai/baselines:\nhttps://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n""""""\n\n\nclass MapState(gym.ObservationWrapper):\n    def __init__(self, env, map_func):\n        gym.ObservationWrapper.__init__(self, env)\n        self._func = map_func\n\n    def observation(self, obs):\n        return self._func(obs)\n\n\nclass FrameStack(gym.Wrapper):\n    """"""\n    Buffer consecutive k observations and stack them on a new last axis.\n    The output observation has shape `original_shape + (k, )`.\n    """"""\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n\n    def reset(self):\n        """"""Clear buffer and re-fill by duplicating the first observation.""""""\n        ob = self.env.reset()\n        for _ in range(self.k - 1):\n            self.frames.append(np.zeros_like(ob))\n        self.frames.append(ob)\n        return self.observation()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self.observation(), reward, done, info\n\n    def observation(self):\n        assert len(self.frames) == self.k\n        return np.stack(self.frames, axis=-1)\n\n\nclass _FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self):\n        self.env.reset()\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset()\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset()\n        return obs\n\n    def step(self, action):\n        return self.env.step(action)\n\n\ndef FireResetEnv(env):\n    if isinstance(env, gym.Wrapper):\n        baseenv = env.unwrapped\n    else:\n        baseenv = env\n    if \'FIRE\' in baseenv.get_action_meanings():\n        return _FireResetEnv(env)\n    return env\n\n\nclass LimitLength(gym.Wrapper):\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n\n    def reset(self):\n        # This assumes that reset() will really reset the env.\n        # If the underlying env tries to be smart about reset\n        # (e.g. end-of-life), the assumption doesn\'t hold.\n        ob = self.env.reset()\n        self.cnt = 0\n        return ob\n\n    def step(self, action):\n        ob, r, done, info = self.env.step(action)\n        self.cnt += 1\n        if self.cnt == self.k:\n            done = True\n        return ob, r, done, info\n'"
examples/A3C-Gym/common.py,0,"b'# -*- coding: utf-8 -*-\n# File: common.py\n# Author: Yuxin Wu\n\nimport multiprocessing\nimport numpy as np\nimport random\nimport time\nfrom six.moves import queue\n\nfrom tensorpack.callbacks import Callback\nfrom tensorpack.utils import logger, get_tqdm\nfrom tensorpack.utils.concurrency import ShareSessionThread, StoppableThread\nfrom tensorpack.utils.stats import StatCounter\n\n\ndef play_one_episode(env, func, render=False):\n    def predict(s):\n        """"""\n        Map from observation to action, with 0.01 greedy.\n        """"""\n        s = np.expand_dims(s, 0)  # batch\n        act = func(s)[0][0].argmax()\n        if random.random() < 0.01:\n            spc = env.action_space\n            act = spc.sample()\n        return act\n\n    ob = env.reset()\n    sum_r = 0\n    while True:\n        act = predict(ob)\n        ob, r, isOver, info = env.step(act)\n        if render:\n            env.render()\n        sum_r += r\n        if isOver:\n            return sum_r\n\n\ndef play_n_episodes(player, predfunc, nr, render=False):\n    logger.info(""Start Playing ... "")\n    for k in range(nr):\n        score = play_one_episode(player, predfunc, render=render)\n        print(""{}/{}, score={}"".format(k, nr, score))\n\n\ndef eval_with_funcs(predictors, nr_eval, get_player_fn, verbose=False):\n    """"""\n    Args:\n        predictors ([PredictorBase])\n    """"""\n    class Worker(StoppableThread, ShareSessionThread):\n        def __init__(self, func, queue):\n            super(Worker, self).__init__()\n            self._func = func\n            self.q = queue\n\n        def func(self, *args, **kwargs):\n            if self.stopped():\n                raise RuntimeError(""stopped!"")\n            return self._func(*args, **kwargs)\n\n        def run(self):\n            with self.default_sess():\n                player = get_player_fn(train=False)\n                while not self.stopped():\n                    try:\n                        score = play_one_episode(player, self.func)\n                    except RuntimeError:\n                        return\n                    self.queue_put_stoppable(self.q, score)\n\n    q = queue.Queue()\n    threads = [Worker(f, q) for f in predictors]\n\n    for k in threads:\n        k.start()\n        time.sleep(0.1)  # avoid simulator bugs\n    stat = StatCounter()\n\n    def fetch():\n        r = q.get()\n        stat.feed(r)\n        if verbose:\n            logger.info(""Score: {}"".format(r))\n\n    for _ in get_tqdm(range(nr_eval)):\n        fetch()\n    # waiting is necessary, otherwise the estimated mean score is biased\n    logger.info(""Waiting for all the workers to finish the last run..."")\n    for k in threads:\n        k.stop()\n    for k in threads:\n        k.join()\n    while q.qsize():\n        fetch()\n\n    if stat.count > 0:\n        return (stat.average, stat.max)\n    return (0, 0)\n\n\ndef eval_model_multithread(pred, nr_eval, get_player_fn):\n    """"""\n    Args:\n        pred (OfflinePredictor): state -> [#action]\n    """"""\n    NR_PROC = min(multiprocessing.cpu_count() // 2, 8)\n    with pred.sess.as_default():\n        mean, max = eval_with_funcs(\n            [pred] * NR_PROC, nr_eval,\n            get_player_fn, verbose=True)\n    logger.info(""Average Score: {}; Max Score: {}"".format(mean, max))\n\n\nclass Evaluator(Callback):\n    def __init__(self, nr_eval, input_names, output_names, get_player_fn):\n        self.eval_episode = nr_eval\n        self.input_names = input_names\n        self.output_names = output_names\n        self.get_player_fn = get_player_fn\n\n    def _setup_graph(self):\n        NR_PROC = min(multiprocessing.cpu_count() // 2, 20)\n        self.pred_funcs = [self.trainer.get_predictor(\n            self.input_names, self.output_names)] * NR_PROC\n\n    def _trigger(self):\n        t = time.time()\n        mean, max = eval_with_funcs(\n            self.pred_funcs, self.eval_episode, self.get_player_fn)\n        t = time.time() - t\n        if t > 10 * 60:  # eval takes too long\n            self.eval_episode = int(self.eval_episode * 0.94)\n        self.trainer.monitors.put_scalar(\'mean_score\', mean)\n        self.trainer.monitors.put_scalar(\'max_score\', max)\n'"
examples/A3C-Gym/simulator.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: simulator.py\n# Author: Yuxin Wu\n\nimport multiprocessing as mp\nimport os\nimport threading\nimport time\nfrom abc import ABCMeta, abstractmethod\nfrom collections import defaultdict\nimport six\nimport zmq\nfrom six.moves import queue\n\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.concurrency import LoopThread, enable_death_signal, ensure_proc_terminate\nfrom tensorpack.utils.serialize import dumps, loads\n\n__all__ = [\'SimulatorProcess\', \'SimulatorMaster\',\n           \'TransitionExperience\']\n\n\nclass TransitionExperience(object):\n    """""" A transition of state, or experience""""""\n\n    def __init__(self, state, action, reward, **kwargs):\n        """""" kwargs: whatever other attribute you want to save""""""\n        self.state = state\n        self.action = action\n        self.reward = reward\n        for k, v in six.iteritems(kwargs):\n            setattr(self, k, v)\n\n\n@six.add_metaclass(ABCMeta)\nclass SimulatorProcess(mp.Process):\n    """"""\n    A process that simulates a player and communicates to master to\n    send states and receive the next action\n    """"""\n\n    def __init__(self, idx, pipe_c2s, pipe_s2c):\n        """"""\n        Args:\n            idx: idx of this process\n            pipe_c2s, pipe_s2c (str): name of the pipe\n        """"""\n        super(SimulatorProcess, self).__init__()\n        self.idx = int(idx)\n        self.name = u\'simulator-{}\'.format(self.idx)\n        self.identity = self.name.encode(\'utf-8\')\n\n        self.c2s = pipe_c2s\n        self.s2c = pipe_s2c\n\n    def run(self):\n        enable_death_signal()\n        player = self._build_player()\n        context = zmq.Context()\n        c2s_socket = context.socket(zmq.PUSH)\n        c2s_socket.setsockopt(zmq.IDENTITY, self.identity)\n        c2s_socket.set_hwm(2)\n        c2s_socket.connect(self.c2s)\n\n        s2c_socket = context.socket(zmq.DEALER)\n        s2c_socket.setsockopt(zmq.IDENTITY, self.identity)\n        s2c_socket.connect(self.s2c)\n\n        state = player.reset()\n        reward, isOver = 0, False\n        while True:\n            # after taking the last action, get to this state and get this reward/isOver.\n            # If isOver, get to the next-episode state immediately.\n            # This tuple is not the same as the one put into the memory buffer\n            c2s_socket.send(dumps(\n                (self.identity, state, reward, isOver)),\n                copy=False)\n            action = loads(s2c_socket.recv(copy=False))\n            state, reward, isOver, _ = player.step(action)\n            if isOver:\n                state = player.reset()\n\n    @abstractmethod\n    def _build_player(self):\n        pass\n\n\n@six.add_metaclass(ABCMeta)\nclass SimulatorMaster(threading.Thread):\n    """""" A base thread to communicate with all SimulatorProcess.\n        It should produce action for each simulator, as well as\n        defining callbacks when a transition or an episode is finished.\n    """"""\n    class ClientState(object):\n        def __init__(self):\n            self.memory = []    # list of Experience\n            self.ident = None\n\n    def __init__(self, pipe_c2s, pipe_s2c):\n        """"""\n        Args:\n            pipe_c2s, pipe_s2c (str): names of pipe to be used for communication\n        """"""\n        super(SimulatorMaster, self).__init__()\n        assert os.name != \'nt\', ""Doesn\'t support windows!""\n        self.daemon = True\n        self.name = \'SimulatorMaster\'\n\n        self.context = zmq.Context()\n\n        self.c2s_socket = self.context.socket(zmq.PULL)\n        self.c2s_socket.bind(pipe_c2s)\n        self.c2s_socket.set_hwm(10)\n        self.s2c_socket = self.context.socket(zmq.ROUTER)\n        self.s2c_socket.bind(pipe_s2c)\n        self.s2c_socket.set_hwm(10)\n\n        # queueing messages to client\n        self.send_queue = queue.Queue(maxsize=100)\n\n        def f():\n            msg = self.send_queue.get()\n            self.s2c_socket.send_multipart(msg, copy=False)\n        self.send_thread = LoopThread(f)\n        self.send_thread.daemon = True\n        self.send_thread.start()\n\n        # make sure socket get closed at the end\n        def clean_context(soks, context):\n            for s in soks:\n                s.close()\n            context.term()\n        import atexit\n        atexit.register(clean_context, [self.c2s_socket, self.s2c_socket], self.context)\n\n    def run(self):\n        self.clients = defaultdict(self.ClientState)\n        try:\n            while True:\n                msg = loads(self.c2s_socket.recv(copy=False))\n                ident, state, reward, isOver = msg\n                client = self.clients[ident]\n                if client.ident is None:\n                    client.ident = ident\n                # maybe check history and warn about dead client?\n                self._process_msg(client, state, reward, isOver)\n        except zmq.ContextTerminated:\n            logger.info(""[Simulator] Context was terminated."")\n\n    @abstractmethod\n    def _process_msg(self, client, state, reward, isOver):\n        pass\n\n    def __del__(self):\n        self.context.destroy(linger=0)\n\n\nif __name__ == \'__main__\':\n    import random\n    import gym\n\n    class NaiveSimulator(SimulatorProcess):\n        def _build_player(self):\n            return gym.make(\'Breakout-v0\')\n\n    class NaiveActioner(SimulatorMaster):\n        def _get_action(self, state):\n            time.sleep(1)\n            return random.randint(1, 3)\n\n        def _on_episode_over(self, client):\n            # print(""Over: "", client.memory)\n            client.memory = []\n            client.state = 0\n\n    name = \'ipc://@whatever\'\n    procs = [NaiveSimulator(k, name) for k in range(10)]\n    [k.start() for k in procs]\n\n    th = NaiveActioner(name)\n    ensure_proc_terminate(procs)\n    th.start()\n\n    time.sleep(100)\n'"
examples/A3C-Gym/train-atari.py,30,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: train-atari.py\n# Author: Yuxin Wu\n\nimport argparse\nimport cv2\nimport gym\nimport multiprocessing as mp\nimport numpy as np\nimport os\nimport sys\nimport uuid\nimport tensorflow as tf\nfrom six.moves import queue\nfrom concurrent import futures\nCancelledError = futures.CancelledError\n\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.gradproc import MapGradient, SummaryGradient\nfrom tensorpack.utils.concurrency import ensure_proc_terminate, start_proc_mask_signal\nfrom tensorpack.utils.gpu import get_num_gpu\nfrom tensorpack.utils.serialize import dumps\n\nfrom atari_wrapper import FireResetEnv, FrameStack, LimitLength, MapState\nfrom common import Evaluator, eval_model_multithread, play_n_episodes\nfrom simulator import SimulatorMaster, SimulatorProcess, TransitionExperience\n\nIMAGE_SIZE = (84, 84)\nFRAME_HISTORY = 4\nGAMMA = 0.99\nSTATE_SHAPE = IMAGE_SIZE + (3, )\n\nLOCAL_TIME_MAX = 5\nSTEPS_PER_EPOCH = 6000\nEVAL_EPISODE = 50\nBATCH_SIZE = 128\nPREDICT_BATCH_SIZE = 16     # batch for efficient forward\nSIMULATOR_PROC = mp.cpu_count() * 2\nPREDICTOR_THREAD_PER_GPU = 4\nPREDICTOR_THREAD = None\n\nNUM_ACTIONS = None\nENV_NAME = None\n\n\ndef get_player(train=False, dumpdir=None):\n    use_gym = not ENV_NAME.endswith("".bin"")\n    if use_gym:\n        env = gym.make(ENV_NAME)\n    else:\n        from atari import AtariPlayer\n        env = AtariPlayer(ENV_NAME, frame_skip=4, viz=False,\n                          live_lost_as_eoe=train, max_num_frames=60000,\n                          grayscale=False)\n    if dumpdir:\n        env = gym.wrappers.Monitor(env, dumpdir, video_callable=lambda _: True)\n    env = FireResetEnv(env)\n    env = MapState(env, lambda im: cv2.resize(im, IMAGE_SIZE))\n    env = FrameStack(env, 4)\n    if train and use_gym:\n        env = LimitLength(env, 60000)\n    return env\n\n\nclass MySimulatorWorker(SimulatorProcess):\n    def _build_player(self):\n        return get_player(train=True)\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        assert NUM_ACTIONS is not None\n        return [tf.TensorSpec((None,) + STATE_SHAPE + (FRAME_HISTORY, ), tf.uint8, \'state\'),\n                tf.TensorSpec((None,), tf.int64, \'action\'),\n                tf.TensorSpec((None,), tf.float32, \'futurereward\'),\n                tf.TensorSpec((None,), tf.float32, \'action_prob\'),\n                ]\n\n    def _get_NN_prediction(self, state):\n        assert state.shape.rank == 5  # Batch, H, W, Channel, History\n        state = tf.transpose(state, [0, 1, 2, 4, 3])  # swap channel & history, to be compatible with old models\n        image = tf.reshape(state, [-1] + list(STATE_SHAPE[:2]) + [STATE_SHAPE[2] * FRAME_HISTORY])\n        image = tf.cast(image, tf.float32)\n\n        image = image / 255.0\n        with argscope(Conv2D, activation=tf.nn.relu):\n            l = Conv2D(\'conv0\', image, 32, 5)\n            l = MaxPooling(\'pool0\', l, 2)\n            l = Conv2D(\'conv1\', l, 32, 5)\n            l = MaxPooling(\'pool1\', l, 2)\n            l = Conv2D(\'conv2\', l, 64, 4)\n            l = MaxPooling(\'pool2\', l, 2)\n            l = Conv2D(\'conv3\', l, 64, 3)\n\n        l = FullyConnected(\'fc0\', l, 512)\n        l = PReLU(\'prelu\', l)\n        logits = FullyConnected(\'fc-pi\', l, NUM_ACTIONS)    # unnormalized policy\n        value = FullyConnected(\'fc-v\', l, 1)\n        return logits, value\n\n    def build_graph(self, state, action, futurereward, action_prob):\n        logits, value = self._get_NN_prediction(state)\n        value = tf.squeeze(value, [1], name=\'pred_value\')  # (B,)\n        policy = tf.nn.softmax(logits, name=\'policy\')\n        if not self.training:\n            return\n        log_probs = tf.log(policy + 1e-6)\n\n        log_pi_a_given_s = tf.reduce_sum(\n            log_probs * tf.one_hot(action, NUM_ACTIONS), 1)\n        advantage = tf.subtract(tf.stop_gradient(value), futurereward, name=\'advantage\')\n\n        pi_a_given_s = tf.reduce_sum(policy * tf.one_hot(action, NUM_ACTIONS), 1)  # (B,)\n        importance = tf.stop_gradient(tf.clip_by_value(pi_a_given_s / (action_prob + 1e-8), 0, 10))\n\n        policy_loss = tf.reduce_sum(log_pi_a_given_s * advantage * importance, name=\'policy_loss\')\n        xentropy_loss = tf.reduce_sum(policy * log_probs, name=\'xentropy_loss\')\n        value_loss = tf.nn.l2_loss(value - futurereward, name=\'value_loss\')\n\n        pred_reward = tf.reduce_mean(value, name=\'predict_reward\')\n        advantage = tf.sqrt(tf.reduce_mean(tf.square(advantage)), name=\'rms_advantage\')\n        entropy_beta = tf.get_variable(\'entropy_beta\', shape=[],\n                                       initializer=tf.constant_initializer(0.01), trainable=False)\n        cost = tf.add_n([policy_loss, xentropy_loss * entropy_beta, value_loss])\n        cost = tf.truediv(cost, tf.cast(tf.shape(futurereward)[0], tf.float32), name=\'cost\')\n        summary.add_moving_summary(policy_loss, xentropy_loss,\n                                   value_loss, pred_reward, advantage,\n                                   cost, tf.reduce_mean(importance, name=\'importance\'))\n        return cost\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.001, trainable=False)\n        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n\n        gradprocs = [MapGradient(lambda grad: tf.clip_by_norm(grad, 0.1 * tf.cast(tf.size(grad), tf.float32))),\n                     SummaryGradient()]\n        opt = optimizer.apply_grad_processors(opt, gradprocs)\n        return opt\n\n\nclass MySimulatorMaster(SimulatorMaster, Callback):\n    def __init__(self, pipe_c2s, pipe_s2c, gpus):\n        """"""\n        Args:\n            gpus (list[int]): the gpus used to run inference\n        """"""\n        super(MySimulatorMaster, self).__init__(pipe_c2s, pipe_s2c)\n        self.queue = queue.Queue(maxsize=BATCH_SIZE * 8 * 2)\n        self._gpus = gpus\n\n    def _setup_graph(self):\n        # Create predictors on the available predictor GPUs.\n        num_gpu = len(self._gpus)\n        predictors = [self.trainer.get_predictor(\n            [\'state\'], [\'policy\', \'pred_value\'],\n            self._gpus[k % num_gpu])\n            for k in range(PREDICTOR_THREAD)]\n        self.async_predictor = MultiThreadAsyncPredictor(\n            predictors, batch_size=PREDICT_BATCH_SIZE)\n\n    def _before_train(self):\n        self.async_predictor.start()\n        logger.info(""Starting MySimulatorMaster ..."")\n        start_proc_mask_signal(self)\n\n    def _on_state(self, state, client):\n        """"""\n        Launch forward prediction for the new state given by some client.\n        """"""\n        def cb(outputs):\n            try:\n                distrib, value = outputs.result()\n            except CancelledError:\n                logger.info(""Client {} cancelled."".format(client.ident))\n                return\n            assert np.all(np.isfinite(distrib)), distrib\n            action = np.random.choice(len(distrib), p=distrib)\n            client.memory.append(TransitionExperience(\n                state, action, reward=None, value=value, prob=distrib[action]))\n            self.send_queue.put([client.ident, dumps(action)])\n        self.async_predictor.put_task([state], cb)\n\n    def _process_msg(self, client, state, reward, isOver):\n        """"""\n        Process a message sent from some client.\n        """"""\n        # in the first message, only state is valid,\n        # reward&isOver should be discarded\n        if len(client.memory) > 0:\n            client.memory[-1].reward = reward\n            if isOver:\n                # should clear client\'s memory and put to queue\n                self._parse_memory(0, client, True)\n            else:\n                if len(client.memory) == LOCAL_TIME_MAX + 1:\n                    R = client.memory[-1].value\n                    self._parse_memory(R, client, False)\n        # feed state and return action\n        self._on_state(state, client)\n\n    def _parse_memory(self, init_r, client, isOver):\n        mem = client.memory\n        if not isOver:\n            last = mem[-1]\n            mem = mem[:-1]\n\n        mem.reverse()\n        R = float(init_r)\n        for idx, k in enumerate(mem):\n            R = np.clip(k.reward, -1, 1) + GAMMA * R\n            self.queue.put([k.state, k.action, R, k.prob])\n\n        if not isOver:\n            client.memory = [last]\n        else:\n            client.memory = []\n\n    def get_training_dataflow(self):\n        # the queue contains batched experience\n        return BatchData(DataFromQueue(self.queue), BATCH_SIZE)\n\n\ndef train():\n    # assign GPUs for training & inference\n    num_gpu = get_num_gpu()\n    global PREDICTOR_THREAD\n    if num_gpu > 0:\n        if num_gpu > 1:\n            # use half gpus for inference\n            predict_tower = list(range(num_gpu))[-num_gpu // 2:]\n        else:\n            predict_tower = [0]\n        PREDICTOR_THREAD = len(predict_tower) * PREDICTOR_THREAD_PER_GPU\n        train_tower = list(range(num_gpu))[:-num_gpu // 2] or [0]\n        logger.info(""[Batch-A3C] Train on gpu {} and infer on gpu {}"".format(\n            \',\'.join(map(str, train_tower)), \',\'.join(map(str, predict_tower))))\n    else:\n        logger.warn(""Without GPU this model will never learn! CPU is only useful for debug."")\n        PREDICTOR_THREAD = 1\n        predict_tower, train_tower = [0], [0]\n\n    # setup simulator processes\n    name_base = str(uuid.uuid1())[:6]\n    prefix = \'@\' if sys.platform.startswith(\'linux\') else \'\'\n    namec2s = \'ipc://{}sim-c2s-{}\'.format(prefix, name_base)\n    names2c = \'ipc://{}sim-s2c-{}\'.format(prefix, name_base)\n    procs = [MySimulatorWorker(k, namec2s, names2c) for k in range(SIMULATOR_PROC)]\n    ensure_proc_terminate(procs)\n    start_proc_mask_signal(procs)\n\n    master = MySimulatorMaster(namec2s, names2c, predict_tower)\n    config = TrainConfig(\n        model=Model(),\n        dataflow=master.get_training_dataflow(),\n        callbacks=[\n            ModelSaver(),\n            ScheduledHyperParamSetter(\'learning_rate\', [(20, 0.0003), (120, 0.0001)]),\n            ScheduledHyperParamSetter(\'entropy_beta\', [(80, 0.005)]),\n            master,\n            PeriodicTrigger(Evaluator(\n                EVAL_EPISODE, [\'state\'], [\'policy\'], get_player),\n                every_k_epochs=3),\n        ],\n        session_creator=sesscreate.NewSessionCreator(config=get_default_sess_config(0.5)),\n        steps_per_epoch=STEPS_PER_EPOCH,\n        session_init=SmartInit(args.load),\n        max_epoch=1000,\n    )\n    trainer = SimpleTrainer() if num_gpu == 1 else AsyncMultiGPUTrainer(train_tower)\n    launch_train_with_config(config, trainer)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--env\', help=\'env\', required=True)\n    parser.add_argument(\'--task\', help=\'task to perform\',\n                        choices=[\'play\', \'eval\', \'train\', \'dump_video\'], default=\'train\')\n    parser.add_argument(\'--output\', help=\'output directory for logs and videos\')\n    parser.add_argument(\'--episode\', help=\'number of episode to eval\', default=100, type=int)\n    args = parser.parse_args()\n    if args.output is None:\n        args.output = os.path.join(\'train_log\', \'train-atari-{}\'.format(args.env))\n\n    ENV_NAME = args.env\n    NUM_ACTIONS = get_player().action_space.n\n    logger.info(""Environment: {}, number of actions: {}"".format(ENV_NAME, NUM_ACTIONS))\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    if args.task != \'train\':\n        assert args.load is not None\n        pred = OfflinePredictor(PredictConfig(\n            model=Model(),\n            session_init=SmartInit(args.load),\n            input_names=[\'state\'],\n            output_names=[\'policy\']))\n        if args.task == \'play\':\n            play_n_episodes(get_player(train=False), pred,\n                            args.episode, render=True)\n        elif args.task == \'eval\':\n            eval_model_multithread(pred, args.episode, get_player)\n        elif args.task == \'dump_video\':\n            play_n_episodes(\n                get_player(train=False, dumpdir=args.output),\n                pred, args.episode)\n    else:\n        assert tf.test.is_gpu_available(), ""Training requires GPUs!""\n        logger.set_logger_dir(args.output)\n        train()\n'"
examples/CTC-TIMIT/create-lmdb.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: create-lmdb.py\n# Author: Yuxin Wu\nimport argparse\nimport numpy as np\nimport os\nimport string\nimport bob.ap\nimport scipy.io.wavfile as wavfile\n\nfrom tensorpack.dataflow import DataFlow, LMDBSerializer\nfrom tensorpack.utils import fs, logger, serialize, get_tqdm\nfrom tensorpack.utils.argtools import memoized\nfrom tensorpack.utils.stats import OnlineMoments\n\nCHARSET = set(string.ascii_lowercase + \' \')\nPHONEME_LIST = [\n    \'aa\', \'ae\', \'ah\', \'ao\', \'aw\', \'ax\', \'ax-h\', \'axr\', \'ay\', \'b\', \'bcl\', \'ch\', \'d\', \'dcl\', \'dh\',\n    \'dx\', \'eh\', \'el\', \'em\', \'en\', \'eng\', \'epi\', \'er\', \'ey\', \'f\', \'g\', \'gcl\', \'h#\', \'hh\', \'hv\', \'ih\',\n    \'ix\', \'iy\', \'jh\', \'k\', \'kcl\', \'l\', \'m\', \'n\', \'ng\', \'nx\', \'ow\', \'oy\', \'p\', \'pau\', \'pcl\', \'q\', \'r\',\n    \'s\', \'sh\', \'t\', \'tcl\', \'th\', \'uh\', \'uw\', \'ux\', \'v\', \'w\', \'y\', \'z\', \'zh\']\n\nPHONEME_DIC = {v: k for k, v in enumerate(PHONEME_LIST)}\nWORD_DIC = {v: k for k, v in enumerate(string.ascii_lowercase + \' \')}\n\n\ndef read_timit_txt(f):\n    f = open(f)\n    line = f.readlines()[0].strip().split(\' \')\n    line = line[2:]\n    line = \' \'.join(line)\n    line = line.replace(\'.\', \'\').lower()\n    line = filter(lambda c: c in CHARSET, line)\n    f.close()\n    ret = []\n    for c in line:\n        ret.append(WORD_DIC[c])\n    return np.asarray(ret)\n\n\ndef read_timit_phoneme(f):\n    f = open(f)\n    pho = []\n    for line in f:\n        line = line.strip().split(\' \')[-1]\n        pho.append(PHONEME_DIC[line])\n    f.close()\n    return np.asarray(pho)\n\n\n@memoized\ndef get_bob_extractor(fs, win_length_ms=10, win_shift_ms=5,\n                      n_filters=55, n_ceps=15, f_min=0., f_max=6000,\n                      delta_win=2, pre_emphasis_coef=0.95, dct_norm=True,\n                      mel_scale=True):\n    ret = bob.ap.Ceps(fs, win_length_ms, win_shift_ms, n_filters, n_ceps, f_min,\n                      f_max, delta_win, pre_emphasis_coef, mel_scale, dct_norm)\n    return ret\n\n\ndef diff_feature(feat, nd=1):\n    diff = feat[1:] - feat[:-1]\n    feat = feat[1:]\n    if nd == 1:\n        return np.concatenate((feat, diff), axis=1)\n    elif nd == 2:\n        d2 = diff[1:] - diff[:-1]\n        return np.concatenate((feat[1:], diff[1:], d2), axis=1)\n\n\ndef get_feature(f):\n    fs, signal = wavfile.read(f)\n    signal = signal.astype(\'float64\')\n    feat = get_bob_extractor(fs, n_filters=26, n_ceps=13)(signal)\n    feat = diff_feature(feat, nd=2)\n    return feat\n\n\nclass RawTIMIT(DataFlow):\n    def __init__(self, dirname, label=\'phoneme\'):\n        self.dirname = dirname\n        assert os.path.isdir(dirname), dirname\n        self.filelists = [k for k in fs.recursive_walk(self.dirname)\n                          if k.endswith(\'.wav\')]\n        logger.info(""Found {} wav files ..."".format(len(self.filelists)))\n        assert len(self.filelists), ""Found no \'.wav\' files!""\n        assert label in [\'phoneme\', \'letter\'], label\n        self.label = label\n\n    def __len__(self):\n        return len(self.filelists)\n\n    def __iter__(self):\n        for f in self.filelists:\n            feat = get_feature(f)\n            if self.label == \'phoneme\':\n                label = read_timit_phoneme(f[:-4] + \'.PHN\')\n            elif self.label == \'letter\':\n                label = read_timit_txt(f[:-4] + \'.TXT\')\n            yield [feat, label]\n\n\ndef compute_mean_std(db, fname):\n    ds = LMDBSerializer.load(db, shuffle=False)\n    ds.reset_state()\n    o = OnlineMoments()\n    for dp in get_tqdm(ds):\n        feat = dp[0]  # len x dim\n        for f in feat:\n            o.feed(f)\n    logger.info(""Writing to {} ..."".format(fname))\n    with open(fname, \'wb\') as f:\n        f.write(serialize.dumps([o.mean, o.std]))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(title=\'command\', dest=\'command\')\n    parser_db = subparsers.add_parser(\'build\', help=\'build a LMDB database\')\n    parser_db.add_argument(\'--dataset\',\n                           help=\'path to TIMIT TRAIN or TEST directory\', required=True)\n    parser_db.add_argument(\'--db\', help=\'output lmdb file\', required=True)\n\n    parser_stat = subparsers.add_parser(\'stat\', help=\'compute statistics (mean/std) of dataset\')\n    parser_stat.add_argument(\'--db\', help=\'input lmdb file\', required=True)\n    parser_stat.add_argument(\'-o\', \'--output\',\n                             help=\'output statistics file\', default=\'stats.data\')\n\n    args = parser.parse_args()\n    if args.command == \'build\':\n        ds = RawTIMIT(args.dataset)\n        LMDBSerializer.save(ds, args.db)\n    elif args.command == \'stat\':\n        compute_mean_std(args.db, args.output)\n'"
examples/CTC-TIMIT/timitdata.py,0,"b""# -*- coding: utf-8 -*-\n# File: timitdata.py\n# Author: Yuxin Wu\n\nimport numpy as np\n\nfrom tensorpack import ProxyDataFlow\n\n__all__ = ['TIMITBatch']\n\n\ndef batch_feature(feats):\n    # pad to the longest in the batch\n    maxlen = max([k.shape[0] for k in feats])\n    bsize = len(feats)\n    ret = np.zeros((bsize, maxlen, feats[0].shape[1]))\n    for idx, feat in enumerate(feats):\n        ret[idx, :feat.shape[0], :] = feat\n    return ret\n\n\ndef sparse_label(labels):\n    maxlen = max([k.shape[0] for k in labels])\n    shape = [len(labels), maxlen]   # bxt\n    indices = []\n    values = []\n    for bid, lab in enumerate(labels):\n        for tid, c in enumerate(lab):\n            indices.append([bid, tid])\n            values.append(c)\n    indices = np.asarray(indices)\n    values = np.asarray(values)\n    return (indices, values, shape)\n\n\nclass TIMITBatch(ProxyDataFlow):\n\n    def __init__(self, ds, batch):\n        self.batch = batch\n        self.ds = ds\n\n    def __len__(self):\n        return len(self.ds) // self.batch\n\n    def __iter__(self):\n        itr = self.ds.__iter__()\n        for _ in range(self.__len__()):\n            feats = []\n            labs = []\n            for b in range(self.batch):\n                feat, lab = next(itr)\n                feats.append(feat)\n                labs.append(lab)\n            batchfeat = batch_feature(feats)\n            batchlab = sparse_label(labs)\n            seqlen = np.asarray([k.shape[0] for k in feats])\n            yield [batchfeat, batchlab[0], batchlab[1], batchlab[2], seqlen]\n"""
examples/CTC-TIMIT/train-timit.py,25,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: train-timit.py\n# Author: Yuxin Wu\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.gradproc import GlobalNormClip, SummaryGradient\n\nfrom timitdata import TIMITBatch\n\nrnn = tf.contrib.rnn\n\n\nBATCH = 64\nNLAYER = 2\nHIDDEN = 128\nNR_CLASS = 61 + 1   # 61 phoneme + epsilon\nFEATUREDIM = 39     # MFCC feature dimension\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec([None, None, FEATUREDIM], tf.float32, 'feat'),   # bxmaxseqx39\n                tf.TensorSpec([None, None], tf.int64, 'labelidx'),  # label is b x maxlen, sparse\n                tf.TensorSpec([None], tf.int32, 'labelvalue'),\n                tf.TensorSpec([None], tf.int64, 'labelshape'),\n                tf.TensorSpec([None], tf.int32, 'seqlen'),   # b\n                ]\n\n    def build_graph(self, feat, labelidx, labelvalue, labelshape, seqlen):\n        label = tf.SparseTensor(labelidx, labelvalue, labelshape)\n\n        cell = rnn.MultiRNNCell([rnn.LSTMBlockCell(num_units=HIDDEN) for _ in range(NLAYER)])\n\n        initial = cell.zero_state(tf.shape(feat)[0], tf.float32)\n\n        outputs, last_state = tf.nn.dynamic_rnn(cell, feat,\n                                                seqlen, initial,\n                                                dtype=tf.float32, scope='rnn')\n\n        # o: b x t x HIDDEN\n        output = tf.reshape(outputs, [-1, HIDDEN])  # (Bxt) x rnnsize\n        logits = FullyConnected('fc', output, NR_CLASS, activation=tf.identity,\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n        logits = tf.reshape(logits, (BATCH, -1, NR_CLASS))\n\n        loss = tf.nn.ctc_loss(label, logits, seqlen, time_major=False)\n\n        cost = tf.reduce_mean(loss, name='cost')\n\n        logits = tf.transpose(logits, [1, 0, 2])\n\n        if self.training:\n            # beam search is too slow to run in training\n            predictions = tf.cast(\n                tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0], tf.int32)\n        else:\n            predictions = tf.cast(\n                tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0], tf.int32)\n        err = tf.edit_distance(predictions, label, normalize=True)\n        err.set_shape([None])\n        err = tf.reduce_mean(err, name='error')\n        summary.add_moving_summary(err, cost)\n        return cost\n\n    def optimizer(self):\n        lr = tf.get_variable('learning_rate', initializer=5e-3, trainable=False)\n        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n        return optimizer.apply_grad_processors(\n            opt, [GlobalNormClip(5), SummaryGradient()])\n\n\ndef get_data(path, isTrain, stat_file):\n    ds = LMDBSerializer.load(path, shuffle=isTrain)\n    mean, std = serialize.loads(open(stat_file, 'rb').read())\n    ds = MapDataComponent(ds, lambda x: (x - mean) / std)\n    ds = TIMITBatch(ds, BATCH)\n    if isTrain:\n        ds = MultiProcessRunnerZMQ(ds, 1)\n    return ds\n\n\ndef get_config(ds_train, ds_test):\n    return TrainConfig(\n        data=QueueInput(ds_train),\n        callbacks=[\n            ModelSaver(),\n            StatMonitorParamSetter('learning_rate', 'error',\n                                   lambda x: x * 0.2, 0, 5),\n            HumanHyperParamSetter('learning_rate'),\n            PeriodicTrigger(\n                InferenceRunner(ds_test, [ScalarStats('error')]),\n                every_k_epochs=2),\n        ],\n        model=Model(),\n        max_epoch=70,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')\n    parser.add_argument('--load', help='load model')\n    parser.add_argument('--train', help='path to training lmdb', required=True)\n    parser.add_argument('--test', help='path to testing lmdb', required=True)\n    parser.add_argument('--stat', help='path to the mean/std statistics file',\n                        default='stats.data')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\n    logger.auto_set_dir()\n    ds_train = get_data(args.train, True, args.stat)\n    ds_test = get_data(args.test, False, args.stat)\n\n    config = get_config(ds_train, ds_test)\n    config.session_init = SmartInit(args.load)\n    launch_train_with_config(config, SimpleTrainer())\n"""
examples/CaffeModels/load-alexnet.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: load-alexnet.py\n# Author: Yuxin Wu\n\nfrom __future__ import print_function\nimport argparse\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow.dataset import ILSVRCMeta\nfrom tensorpack.tfutils.summary import *\n\n\ndef tower_func(image):\n    # img: 227x227x3\n    with argscope([Conv2D, FullyConnected], activation=tf.nn.relu):\n        l = Conv2D(\'conv1\', image, filters=96, kernel_size=11, strides=4, padding=\'VALID\')\n        l = tf.nn.lrn(l, 2, bias=1.0, alpha=2e-5, beta=0.75, name=\'norm1\')\n        l = MaxPooling(\'pool1\', l, 3, strides=2, padding=\'VALID\')\n\n        l = Conv2D(\'conv2\', l, filters=256, kernel_size=5, split=2)\n        l = tf.nn.lrn(l, 2, bias=1.0, alpha=2e-5, beta=0.75, name=\'norm2\')\n        l = MaxPooling(\'pool2\', l, 3, strides=2, padding=\'VALID\')\n\n        l = Conv2D(\'conv3\', l, filters=384, kernel_size=3)\n        l = Conv2D(\'conv4\', l, filters=384, kernel_size=3, split=2)\n        l = Conv2D(\'conv5\', l, filters=256, kernel_size=3, split=2)\n        l = MaxPooling(\'pool3\', l, 3, strides=2, padding=\'VALID\')\n\n        # This is just a script to load model, so we ignore the dropout layer\n        l = FullyConnected(\'fc6\', l, 4096)\n        l = FullyConnected(\'fc7\', l, 4096)\n    logits = FullyConnected(\'fc8\', l, 1000)\n    tf.nn.softmax(logits, name=\'prob\')\n\n\ndef run_test(path, input):\n    predictor = OfflinePredictor(PredictConfig(\n        input_signature=[tf.TensorSpec((None, 227, 227, 3), tf.float32, \'input\')],\n        tower_func=tower_func,\n        session_init=SmartInit(path),\n        input_names=[\'input\'],\n        output_names=[\'prob\']\n    ))\n\n    im = cv2.imread(input)\n    assert im is not None, input\n    im = cv2.resize(im, (227, 227))[None, :, :, ::-1].astype(\'float32\') - 110\n    outputs = predictor(im)[0]\n    prob = outputs[0]\n    ret = prob.argsort()[-10:][::-1]\n    print(""Top10 predictions:"", ret)\n\n    meta = ILSVRCMeta().get_synset_words_1000()\n    print(""Top10 class names:"", [meta[k] for k in ret])\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', required=True,\n                        help=\'.npz model file generated by tensorpack.utils.loadcaffe\')\n    parser.add_argument(\'--input\', help=\'an input image\', required=True)\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    # run alexnet with given model (in npz format)\n    run_test(args.load, args.input)\n'"
examples/CaffeModels/load-cpm.py,11,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: load-cpm.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.utils import viz\nfrom tensorpack.utils.argtools import memoized\n\n\n""""""\n15 channels:\n0-1 head, neck\n2-4 right shoulder, right elbow, right wrist\n5-7 left shoulder, left elbow, left wrist\n8-10 right hip, right knee, right ankle\n11-13 left hip, left knee, left ankle\n14: background\n""""""\n\n\ndef colorize(img, heatmap):\n    """""" img: bgr, [0,255]\n        heatmap: [0,1]\n    """"""\n    heatmap = viz.intensity_to_rgb(heatmap, cmap=\'jet\')[:, :, ::-1]\n    return img * 0.5 + heatmap * 0.5\n\n\n@memoized\ndef get_gaussian_map():\n    gaussian_map = np.zeros((368, 368), dtype=\'float32\')\n    for x_p in range(368):\n        for y_p in range(368):\n            dist_sq = (x_p - 368 / 2) * (x_p - 368 / 2) + \\\n                      (y_p - 368 / 2) * (y_p - 368 / 2)\n            exponent = dist_sq / 2.0 / (21**2)\n            gaussian_map[y_p, x_p] = np.exp(-exponent)\n    return gaussian_map.reshape((1, 368, 368, 1))\n\n\ndef CPM(image):\n    image = image / 256.0 - 0.5\n\n    gmap = tf.constant(get_gaussian_map())\n    gmap = tf.pad(gmap, [[0, 0], [0, 1], [0, 1], [0, 0]])\n    pool_center = AvgPooling(\'mappool\', gmap, 9, strides=8, padding=\'VALID\')\n    with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu):\n        shared = (LinearWrap(image)\n                  .Conv2D(\'conv1_1\', 64)\n                  .Conv2D(\'conv1_2\', 64)\n                  .MaxPooling(\'pool1\', 2)\n                  # 184\n                  .Conv2D(\'conv2_1\', 128)\n                  .Conv2D(\'conv2_2\', 128)\n                  .MaxPooling(\'pool2\', 2)\n                  # 92\n                  .Conv2D(\'conv3_1\', 256)\n                  .Conv2D(\'conv3_2\', 256)\n                  .Conv2D(\'conv3_3\', 256)\n                  .Conv2D(\'conv3_4\', 256)\n                  .MaxPooling(\'pool3\', 2)\n                  # 46\n                  .Conv2D(\'conv4_1\', 512)\n                  .Conv2D(\'conv4_2\', 512)\n                  .Conv2D(\'conv4_3_CPM\', 256)\n                  .Conv2D(\'conv4_4_CPM\', 256)\n                  .Conv2D(\'conv4_5_CPM\', 256)\n                  .Conv2D(\'conv4_6_CPM\', 256)\n                  .Conv2D(\'conv4_7_CPM\', 128)())\n\n    def add_stage(stage, l):\n        l = tf.concat([l, shared, pool_center], 3,\n                      name=\'concat_stage{}\'.format(stage))\n        for i in range(1, 6):\n            l = Conv2D(\'Mconv{}_stage{}\'.format(i, stage), l, 128, 7, activation=tf.nn.relu)\n        l = Conv2D(\'Mconv6_stage{}\'.format(stage), l, 128, 1, activation=tf.nn.relu)\n        l = Conv2D(\'Mconv7_stage{}\'.format(stage), l, 15, 1, activation=tf.identity)\n        return l\n\n    out1 = (LinearWrap(shared)\n            .Conv2D(\'conv5_1_CPM\', 512, 1, activation=tf.nn.relu)\n            .Conv2D(\'conv5_2_CPM\', 15, 1, activation=tf.identity)())\n    out2 = add_stage(2, out1)\n    out3 = add_stage(3, out2)\n    out4 = add_stage(4, out3)\n    out5 = add_stage(5, out4)\n    out6 = add_stage(6, out5)\n    tf.image.resize_bilinear(out6, [368, 368], name=\'resized_map\')\n\n\ndef run_test(model_path, img_file):\n    predict_func = OfflinePredictor(PredictConfig(\n        input_signature=[tf.TensorSpec((None, 368, 368, 3), tf.float32, \'input\')],\n        tower_func=CPM,\n        session_init=SmartInit(model_path),\n        input_names=[\'input\'],\n        output_names=[\'resized_map\']\n    ))\n\n    im = cv2.imread(img_file, cv2.IMREAD_COLOR).astype(\'float32\')\n    im = cv2.resize(im, (368, 368))\n    out = predict_func(im[None, :, :, :])[0][0]\n    hm = out[:, :, :14].sum(axis=2)\n    viz = colorize(im, hm)\n    cv2.imwrite(""output.jpg"", viz)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--load\', required=True, help=\'.npz model file\')\n    parser.add_argument(\'--input\', required=True, help=\'input image\')\n    args = parser.parse_args()\n    run_test(args.load, args.input)\n'"
examples/CaffeModels/load-vgg16.py,28,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: load-vgg16.py\n\nfrom __future__ import print_function\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport six\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow.dataset import ILSVRCMeta\n\nenable_argscope_for_module(tf.layers)\n\n\ndef tower_func(image):\n    is_training = get_current_tower_context().is_training\n\n    with argscope([tf.layers.conv2d], kernel_size=3, activation=tf.nn.relu, padding=\'same\'):\n        x = image\n        x = tf.layers.conv2d(x, 64, name=\'conv1_1\')\n        x = tf.layers.conv2d(x, 64, name=\'conv1_2\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool1\')\n\n        x = tf.layers.conv2d(x, 128, name=\'conv2_1\')\n        x = tf.layers.conv2d(x, 128, name=\'conv2_2\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool2\')\n\n        x = tf.layers.conv2d(x, 256, name=\'conv3_1\')\n        x = tf.layers.conv2d(x, 256, name=\'conv3_2\')\n        x = tf.layers.conv2d(x, 256, name=\'conv3_3\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool3\')\n\n        x = tf.layers.conv2d(x, 512, name=\'conv4_1\')\n        x = tf.layers.conv2d(x, 512, name=\'conv4_2\')\n        x = tf.layers.conv2d(x, 512, name=\'conv4_3\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool4\')\n\n        x = tf.layers.conv2d(x, 512, name=\'conv5_1\')\n        x = tf.layers.conv2d(x, 512, name=\'conv5_2\')\n        x = tf.layers.conv2d(x, 512, name=\'conv5_3\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool5\')\n        x = tf.layers.flatten(x, name=\'flatten\')\n\n        x = tf.layers.dense(x, 4096, activation=tf.nn.relu, name=\'fc6\')\n        x = tf.layers.dropout(x, rate=0.5, name=\'drop0\', training=is_training)\n        x = tf.layers.dense(x, 4096, activation=tf.nn.relu, name=\'fc7\')\n        x = tf.layers.dropout(x, rate=0.5, name=\'drop1\', training=is_training)\n        logits = tf.layers.dense(x, 1000, activation=tf.identity, name=\'fc8\')\n\n    tf.nn.softmax(logits, name=\'prob\')\n\n\ndef run_test(path, input):\n    param_dict = dict(np.load(path))\n    param_dict = {k.replace(\'/W\', \'/kernel\').replace(\'/b\', \'/bias\'): v for k, v in six.iteritems(param_dict)}\n\n    predict_func = OfflinePredictor(PredictConfig(\n        input_signature=[tf.TensorSpec((None, 224, 224, 3), tf.float32, \'input\')],\n        tower_func=tower_func,\n        session_init=SmartInit(param_dict),\n        input_names=[\'input\'],\n        output_names=[\'prob\']   # prob:0 is the probability distribution\n    ))\n\n    im = cv2.imread(input)\n    assert im is not None, input\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    im = cv2.resize(im, (224, 224)).reshape((1, 224, 224, 3)).astype(\'float32\')\n\n    # VGG16 requires channelwise mean substraction\n    VGG_MEAN = [103.939, 116.779, 123.68]\n    im -= VGG_MEAN[::-1]\n\n    outputs = predict_func(im)[0]\n    prob = outputs[0]\n    ret = prob.argsort()[-10:][::-1]\n    print(""Top10 predictions:"", ret)\n\n    meta = ILSVRCMeta().get_synset_words_1000()\n    print(""Top10 class names:"", [meta[k] for k in ret])\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', required=True,\n                        help=\'.npz model file generated by tensorpack.utils.loadcaffe\')\n    parser.add_argument(\'--input\', help=\'an input image\', required=True)\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    run_test(args.load, args.input)\n'"
examples/CaffeModels/load-vgg19.py,31,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: load-vgg19.py\n\nfrom __future__ import print_function\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport six\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow.dataset import ILSVRCMeta\n\nenable_argscope_for_module(tf.layers)\n\n\ndef tower_func(image):\n    is_training = get_current_tower_context().is_training\n\n    with argscope([tf.layers.conv2d], kernel_size=3, activation=tf.nn.relu, padding=\'same\'):\n        x = image\n        x = tf.layers.conv2d(x, 64, name=\'conv1_1\')\n        x = tf.layers.conv2d(x, 64, name=\'conv1_2\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool1\')\n\n        x = tf.layers.conv2d(x, 128, name=\'conv2_1\')\n        x = tf.layers.conv2d(x, 128, name=\'conv2_2\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool2\')\n\n        x = tf.layers.conv2d(x, 256, name=\'conv3_1\')\n        x = tf.layers.conv2d(x, 256, name=\'conv3_2\')\n        x = tf.layers.conv2d(x, 256, name=\'conv3_3\')\n        x = tf.layers.conv2d(x, 256, name=\'conv3_4\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool3\')\n\n        x = tf.layers.conv2d(x, 512, name=\'conv4_1\')\n        x = tf.layers.conv2d(x, 512, name=\'conv4_2\')\n        x = tf.layers.conv2d(x, 512, name=\'conv4_3\')\n        x = tf.layers.conv2d(x, 512, name=\'conv4_4\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool4\')\n\n        x = tf.layers.conv2d(x, 512, name=\'conv5_1\')\n        x = tf.layers.conv2d(x, 512, name=\'conv5_2\')\n        x = tf.layers.conv2d(x, 512, name=\'conv5_3\')\n        x = tf.layers.conv2d(x, 512, name=\'conv5_4\')\n        x = tf.layers.max_pooling2d(x, 2, 2, name=\'pool5\')\n        x = tf.layers.flatten(x, name=\'flatten\')\n\n        x = tf.layers.dense(x, 4096, activation=tf.nn.relu, name=\'fc6\')\n        x = tf.layers.dropout(x, rate=0.5, name=\'drop0\', training=is_training)\n        x = tf.layers.dense(x, 4096, activation=tf.nn.relu, name=\'fc7\')\n        x = tf.layers.dropout(x, rate=0.5, name=\'drop1\', training=is_training)\n        logits = tf.layers.dense(x, 1000, activation=tf.identity, name=\'fc8\')\n\n    tf.nn.softmax(logits, name=\'prob\')\n\n\ndef run_test(path, input):\n    param_dict = dict(np.load(path))\n    param_dict = {k.replace(\'/W\', \'/kernel\').replace(\'/b\', \'/bias\'): v for k, v in six.iteritems(param_dict)}\n\n    predict_func = OfflinePredictor(PredictConfig(\n        input_signature=[tf.TensorSpec((None, 224, 224, 3), tf.float32, \'input\')],\n        tower_func=tower_func,\n        session_init=SmartInit(param_dict),\n        input_names=[\'input\'],\n        output_names=[\'prob\']   # prob:0 is the probability distribution\n    ))\n\n    im = cv2.imread(input)\n    assert im is not None, input\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    im = cv2.resize(im, (224, 224)).reshape((1, 224, 224, 3)).astype(\'float32\')\n\n    # VGG19 requires channelwise mean substraction\n    VGG_MEAN = [103.939, 116.779, 123.68]\n    im -= VGG_MEAN[::-1]\n    outputs = predict_func(im)[0]\n    prob = outputs[0]\n    ret = prob.argsort()[-10:][::-1]\n    print(""Top10 predictions:"", ret)\n\n    meta = ILSVRCMeta().get_synset_words_1000()\n    print(""Top10 class names:"", [meta[k] for k in ret])\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', required=True,\n                        help=\'.npz model file generated by tensorpack.utils.loadcaffe\')\n    parser.add_argument(\'--input\', help=\'an input image\', required=True)\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    run_test(args.load, args.input)\n'"
examples/Char-RNN/char-rnn.py,18,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: char-rnn.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport operator\nimport os\nimport sys\nfrom collections import Counter\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils import optimizer, summary\nfrom tensorpack.tfutils.gradproc import GlobalNormClip\n\nrnn = tf.contrib.rnn\n\nclass _NS: pass  # noqa\n\n\nparam = _NS()\n\n# some model hyperparams to set\nparam.batch_size = 128\nparam.rnn_size = 256\nparam.num_rnn_layer = 2\nparam.seq_len = 50\nparam.grad_clip = 5.\nparam.vocab_size = None\nparam.softmax_temprature = 1\nparam.corpus = None\n\n\nclass CharRNNData(RNGDataFlow):\n    def __init__(self, input_file, size):\n        self.seq_length = param.seq_len\n        self._size = size\n\n        logger.info(""Loading corpus..."")\n        # preprocess data\n        with open(input_file, \'rb\') as f:\n            data = f.read()\n        data = [chr(c) for c in data if c < 128]\n        counter = Counter(data)\n        char_cnt = sorted(counter.items(), key=operator.itemgetter(1), reverse=True)\n        self.chars = [x[0] for x in char_cnt]\n        print(sorted(self.chars))\n        self.vocab_size = len(self.chars)\n        param.vocab_size = self.vocab_size\n        self.char2idx = {c: i for i, c in enumerate(self.chars)}\n        self.whole_seq = np.array([self.char2idx[c] for c in data], dtype=\'int32\')\n        logger.info(""Corpus loaded. Vocab size: {}"".format(self.vocab_size))\n\n    def __len__(self):\n        return self._size\n\n    def __iter__(self):\n        random_starts = self.rng.randint(\n            0, self.whole_seq.shape[0] - self.seq_length - 1, (self._size,))\n        for st in random_starts:\n            seq = self.whole_seq[st:st + self.seq_length + 1]\n            yield [seq[:-1], seq[1:]]\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, param.seq_len), tf.int32, \'input\'),\n                tf.TensorSpec((None, param.seq_len), tf.int32, \'nextinput\')]\n\n    def build_graph(self, input, nextinput):\n        cell = rnn.MultiRNNCell([rnn.LSTMBlockCell(num_units=param.rnn_size)\n                                for _ in range(param.num_rnn_layer)])\n\n        def get_v(n):\n            ret = tf.get_variable(n + \'_unused\', [param.batch_size, param.rnn_size],\n                                  trainable=False,\n                                  initializer=tf.constant_initializer())\n            ret = tf.placeholder_with_default(ret, shape=[None, param.rnn_size], name=n)\n            return ret\n        initial = (rnn.LSTMStateTuple(get_v(\'c0\'), get_v(\'h0\')),\n                   rnn.LSTMStateTuple(get_v(\'c1\'), get_v(\'h1\')))\n\n        embeddingW = tf.get_variable(\'embedding\', [param.vocab_size, param.rnn_size])\n        input_feature = tf.nn.embedding_lookup(embeddingW, input)  # B x seqlen x rnnsize\n\n        input_list = tf.unstack(input_feature, axis=1)  # seqlen x (Bxrnnsize)\n\n        outputs, last_state = rnn.static_rnn(cell, input_list, initial, scope=\'rnnlm\')\n        last_state = tf.identity(last_state, \'last_state\')\n\n        # seqlen x (Bxrnnsize)\n        output = tf.reshape(tf.concat(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize\n        logits = FullyConnected(\'fc\', output, param.vocab_size, activation=tf.identity)\n        tf.nn.softmax(logits / param.softmax_temprature, name=\'prob\')\n\n        xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=tf.reshape(nextinput, [-1]))\n        cost = tf.reduce_mean(xent_loss, name=\'cost\')\n        summary.add_param_summary((\'.*/W\', [\'histogram\']))   # monitor histogram of all W\n        summary.add_moving_summary(cost)\n        return cost\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=2e-3, trainable=False)\n        opt = tf.train.AdamOptimizer(lr)\n        return optimizer.apply_grad_processors(opt, [GlobalNormClip(5)])\n\n\ndef get_config():\n    logger.auto_set_dir()\n\n    ds = CharRNNData(param.corpus, 100000)\n    ds = BatchData(ds, param.batch_size)\n\n    return TrainConfig(\n        data=QueueInput(ds),\n        callbacks=[\n            ModelSaver(),\n            ScheduledHyperParamSetter(\'learning_rate\', [(25, 2e-4)])\n        ],\n        model=Model(),\n        max_epoch=50,\n    )\n\n\ndef sample(path, start, length):\n    """"""\n    :param path: path to the model\n    :param start: a `str`. the starting characters\n    :param length: a `int`. the length of text to generate\n    """"""\n    # initialize vocabulary and sequence length\n    param.seq_len = 1\n    ds = CharRNNData(param.corpus, 100000)\n\n    pred = OfflinePredictor(PredictConfig(\n        model=Model(),\n        session_init=SmartInit(path),\n        input_names=[\'input\', \'c0\', \'h0\', \'c1\', \'h1\'],\n        output_names=[\'prob\', \'last_state\']))\n\n    # feed the starting sentence\n    initial = np.zeros((1, param.rnn_size))\n    for c in start[:-1]:\n        x = np.array([[ds.char2idx[c]]], dtype=\'int32\')\n        _, state = pred(x, initial, initial, initial, initial)\n\n    def pick(prob):\n        t = np.cumsum(prob)\n        s = np.sum(prob)\n        return(int(np.searchsorted(t, np.random.rand(1) * s)))\n\n    # generate more\n    ret = start\n    c = start[-1]\n    for k in range(length):\n        x = np.array([[ds.char2idx[c]]], dtype=\'int32\')\n        prob, state = pred(x, state[0, 0], state[0, 1], state[1, 0], state[1, 1])\n        c = ds.chars[pick(prob[0])]\n        ret += c\n    print(ret)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    subparsers = parser.add_subparsers(title=\'command\', dest=\'command\')\n    parser_sample = subparsers.add_parser(\'sample\', help=\'sample a trained model\')\n    parser_sample.add_argument(\'-n\', \'--num\', type=int,\n                               default=300, help=\'length of text to generate\')\n    parser_sample.add_argument(\'-s\', \'--start\',\n                               default=\'The \', help=\'initial text sequence\')\n    parser_sample.add_argument(\'-t\', \'--temperature\', type=float,\n                               default=1, help=\'softmax temperature\')\n    parser_train = subparsers.add_parser(\'train\', help=\'train\')\n    parser_train.add_argument(\'--corpus\', help=\'corpus file\', default=\'input.txt\')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    if args.command == \'sample\':\n        param.softmax_temprature = args.temperature\n        assert args.load is not None, ""Load your model by argument --load""\n        sample(args.load, args.start, args.num)\n        sys.exit()\n    else:\n        param.corpus = args.corpus\n        config = get_config()\n        config.session_init = SmartInit(args.load)\n        launch_train_with_config(config, SimpleTrainer())\n'"
examples/DeepQNetwork/DQN.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: DQN.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport gym\nimport tensorflow as tf\n\nfrom tensorpack import *\n\nfrom atari_wrapper import FireResetEnv, FrameStack, LimitLength, MapState\nfrom common import Evaluator, eval_model_multithread, play_n_episodes\nfrom DQNModel import Model as DQNModel\nfrom expreplay import ExpReplay\n\nBATCH_SIZE = 64\nIMAGE_SIZE = (84, 84)\nFRAME_HISTORY = 4\nUPDATE_FREQ = 4  # the number of new state transitions per parameter update (per training step)\n\nMEMORY_SIZE = 1e6\n# will consume at least 1e6 * 84 * 84 bytes == 6.6G memory.\nINIT_MEMORY_SIZE = MEMORY_SIZE // 20\nSTEPS_PER_EPOCH = 100000 // UPDATE_FREQ  # each epoch is 100k state transitions\nNUM_PARALLEL_PLAYERS = 3\n\nUSE_GYM = False\nENV_NAME = None\n\n\ndef resize_keepdims(im, size):\n    # Opencv\'s resize remove the extra dimension for grayscale images. We add it back.\n    ret = cv2.resize(im, size)\n    if im.ndim == 3 and ret.ndim == 2:\n        ret = ret[:, :, np.newaxis]\n    return ret\n\n\ndef get_player(viz=False, train=False):\n    if USE_GYM:\n        env = gym.make(ENV_NAME)\n    else:\n        from atari import AtariPlayer\n        env = AtariPlayer(ENV_NAME, frame_skip=4, viz=viz,\n                          live_lost_as_eoe=train, max_num_frames=60000)\n    env = FireResetEnv(env)\n    env = MapState(env, lambda im: resize_keepdims(im, IMAGE_SIZE))\n    if not train:\n        # in training, history is taken care of in expreplay buffer\n        env = FrameStack(env, FRAME_HISTORY)\n    if train and USE_GYM:\n        env = LimitLength(env, 60000)\n    return env\n\n\nclass Model(DQNModel):\n    """"""\n    A DQN model for 2D/3D (image) observations.\n    """"""\n    def _get_DQN_prediction(self, image):\n        assert image.shape.rank in [4, 5], image.shape\n        # image: N, H, W, (C), Hist\n        if image.shape.rank == 5:\n            # merge C & Hist\n            image = tf.reshape(\n                image,\n                [-1] + list(self.state_shape[:2]) + [self.state_shape[2] * FRAME_HISTORY])\n\n        image = image / 255.0\n        with argscope(Conv2D, activation=lambda x: PReLU(\'prelu\', x), use_bias=True):\n            l = (LinearWrap(image)\n                 # Nature architecture\n                 .Conv2D(\'conv0\', 32, 8, strides=4)\n                 .Conv2D(\'conv1\', 64, 4, strides=2)\n                 .Conv2D(\'conv2\', 64, 3)\n\n                 # architecture used for the figure in the README, slower but takes fewer iterations to converge\n                 # .Conv2D(\'conv0\', out_channel=32, kernel_shape=5)\n                 # .MaxPooling(\'pool0\', 2)\n                 # .Conv2D(\'conv1\', out_channel=32, kernel_shape=5)\n                 # .MaxPooling(\'pool1\', 2)\n                 # .Conv2D(\'conv2\', out_channel=64, kernel_shape=4)\n                 # .MaxPooling(\'pool2\', 2)\n                 # .Conv2D(\'conv3\', out_channel=64, kernel_shape=3)\n\n                 .FullyConnected(\'fc0\', 512)\n                 .tf.nn.leaky_relu(alpha=0.01)())\n        if self.method != \'Dueling\':\n            Q = FullyConnected(\'fct\', l, self.num_actions)\n        else:\n            # Dueling DQN\n            V = FullyConnected(\'fctV\', l, 1)\n            As = FullyConnected(\'fctA\', l, self.num_actions)\n            Q = tf.add(As, V - tf.reduce_mean(As, 1, keep_dims=True))\n        return tf.identity(Q, name=\'Qvalue\')\n\n\ndef get_config(model):\n    global args\n    expreplay = ExpReplay(\n        predictor_io_names=([\'state\'], [\'Qvalue\']),\n        get_player=lambda: get_player(train=True),\n        num_parallel_players=NUM_PARALLEL_PLAYERS,\n        state_shape=model.state_shape,\n        batch_size=BATCH_SIZE,\n        memory_size=MEMORY_SIZE,\n        init_memory_size=INIT_MEMORY_SIZE,\n        update_frequency=UPDATE_FREQ,\n        history_len=FRAME_HISTORY,\n        state_dtype=model.state_dtype.as_numpy_dtype\n    )\n\n    # Set to other values if you need a different initial exploration\n    # (e.g., # if you\'re resuming a training half-way)\n    # expreplay.exploration = 1.0\n\n    return TrainConfig(\n        data=QueueInput(expreplay),\n        model=model,\n        callbacks=[\n            ModelSaver(),\n            PeriodicTrigger(\n                RunOp(DQNModel.update_target_param, verbose=True),\n                every_k_steps=5000),    # update target network every 5k steps\n            expreplay,\n            ScheduledHyperParamSetter(\'learning_rate\',\n                                      [(0, 1e-3), (60, 5e-4), (400, 1e-4)]),\n            ScheduledHyperParamSetter(\n                ObjAttrParam(expreplay, \'exploration\'),\n                [(0, 1), (10, 0.1), (400, 0.01)],   # 1->0.1 in the first million steps\n                interp=\'linear\'),\n            PeriodicTrigger(Evaluator(\n                args.num_eval, [\'state\'], [\'Qvalue\'], get_player),\n                every_k_epochs=5 if \'pong\' in args.env.lower() else 10),  # eval more frequently for easy games\n        ],\n        steps_per_epoch=STEPS_PER_EPOCH,\n        max_epoch=500,  # a total of 50M state transition\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--task\', help=\'task to perform\',\n                        choices=[\'play\', \'eval\', \'train\'], default=\'train\')\n    parser.add_argument(\'--env\', required=True,\n                        help=\'either an atari rom file (that ends with .bin) or a gym atari environment name\')\n    parser.add_argument(\'--algo\', help=\'algorithm\',\n                        choices=[\'DQN\', \'Double\', \'Dueling\'], default=\'Double\')\n    parser.add_argument(\'--num-eval\', default=50, type=int)\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    ENV_NAME = args.env\n    USE_GYM = not ENV_NAME.endswith(\'.bin\')\n\n    # set num_actions\n    num_actions = get_player().action_space.n\n    logger.info(""ENV: {}, Num Actions: {}"".format(args.env, num_actions))\n\n    state_shape = IMAGE_SIZE + (3, ) if USE_GYM else IMAGE_SIZE\n    model = Model(state_shape, FRAME_HISTORY, args.algo, num_actions)\n\n    if args.task != \'train\':\n        assert args.load is not None\n        pred = OfflinePredictor(PredictConfig(\n            model=model,\n            session_init=SmartInit(args.load),\n            input_names=[\'state\'],\n            output_names=[\'Qvalue\']))\n        if args.task == \'play\':\n            play_n_episodes(get_player(viz=0.01), pred, 100, render=True)\n        elif args.task == \'eval\':\n            eval_model_multithread(pred, args.num_eval, get_player)\n    else:\n        logger.set_logger_dir(\n            os.path.join(\'train_log\', \'DQN-{}\'.format(\n                os.path.basename(args.env).split(\'.\')[0])))\n        config = get_config(model)\n        config.session_init = SmartInit(args.load)\n        launch_train_with_config(config, SimpleTrainer())\n'"
examples/DeepQNetwork/DQNModel.py,27,"b'# -*- coding: utf-8 -*-\n# File: DQNModel.py\n\n\nimport abc\nimport tensorflow as tf\n\nfrom tensorpack import ModelDesc\nfrom tensorpack.tfutils import gradproc, optimizer, summary, varreplace\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.utils import logger\n\n\nclass Model(ModelDesc):\n\n    state_dtype = tf.uint8\n\n    # reward discount factor\n    gamma = 0.99\n\n    def __init__(self, state_shape, history, method, num_actions):\n        """"""\n        Args:\n            state_shape (tuple[int]),\n            history (int):\n        """"""\n        self.state_shape = tuple(state_shape)\n        self._stacked_state_shape = (-1, ) + self.state_shape + (history, )\n        self.history = history\n        self.method = method\n        self.num_actions = num_actions\n\n    def inputs(self):\n        # When we use h history frames, the current state and the next state will have (h-1) overlapping frames.\n        # Therefore we use a combined state for efficiency:\n        # The first h are the current state, and the last h are the next state.\n        return [tf.TensorSpec((None,) + self.state_shape + (self.history + 1, ), self.state_dtype, \'comb_state\'),\n                tf.TensorSpec((None,), tf.int64, \'action\'),\n                tf.TensorSpec((None,), tf.float32, \'reward\'),\n                tf.TensorSpec((None,), tf.bool, \'isOver\')]\n\n    @abc.abstractmethod\n    def _get_DQN_prediction(self, state):\n        """"""\n        state: N + state_shape + history\n        """"""\n        pass\n\n    @auto_reuse_variable_scope\n    def get_DQN_prediction(self, state):\n        return self._get_DQN_prediction(state)\n\n    def build_graph(self, comb_state, action, reward, isOver):\n        comb_state = tf.cast(comb_state, tf.float32)\n        input_rank = comb_state.shape.rank\n\n        state = tf.slice(\n            comb_state,\n            [0] * input_rank,\n            [-1] * (input_rank - 1) + [self.history], name=\'state\')\n\n        self.predict_value = self.get_DQN_prediction(state)\n        if not self.training:\n            return\n\n        reward = tf.clip_by_value(reward, -1, 1)\n        next_state = tf.slice(\n            comb_state,\n            [0] * (input_rank - 1) + [1],\n            [-1] * (input_rank - 1) + [self.history], name=\'next_state\')\n        next_state = tf.reshape(next_state, self._stacked_state_shape)\n        action_onehot = tf.one_hot(action, self.num_actions, 1.0, 0.0)\n\n        pred_action_value = tf.reduce_sum(self.predict_value * action_onehot, 1)  # N,\n        max_pred_reward = tf.reduce_mean(tf.reduce_max(\n            self.predict_value, 1), name=\'predict_reward\')\n        summary.add_moving_summary(max_pred_reward)\n\n        with tf.variable_scope(\'target\'), varreplace.freeze_variables(skip_collection=True):\n            targetQ_predict_value = self.get_DQN_prediction(next_state)    # NxA\n\n        if self.method != \'Double\':\n            # DQN\n            best_v = tf.reduce_max(targetQ_predict_value, 1)    # N,\n        else:\n            # Double-DQN\n            next_predict_value = self.get_DQN_prediction(next_state)\n            self.greedy_choice = tf.argmax(next_predict_value, 1)   # N,\n            predict_onehot = tf.one_hot(self.greedy_choice, self.num_actions, 1.0, 0.0)\n            best_v = tf.reduce_sum(targetQ_predict_value * predict_onehot, 1)\n\n        target = reward + (1.0 - tf.cast(isOver, tf.float32)) * self.gamma * tf.stop_gradient(best_v)\n\n        cost = tf.losses.huber_loss(\n            target, pred_action_value, reduction=tf.losses.Reduction.MEAN)\n        summary.add_param_summary((\'conv.*/W\', [\'histogram\', \'rms\']),\n                                  (\'fc.*/W\', [\'histogram\', \'rms\']))   # monitor all W\n        summary.add_moving_summary(cost)\n        return cost\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=1e-3, trainable=False)\n        tf.summary.scalar(""learning_rate-summary"", lr)\n        opt = tf.train.RMSPropOptimizer(lr, decay=0.95, momentum=0.95, epsilon=1e-2)\n        return optimizer.apply_grad_processors(opt, [gradproc.SummaryGradient()])\n\n    @staticmethod\n    def update_target_param():\n        vars = tf.global_variables()\n        ops = []\n        G = tf.get_default_graph()\n        for v in vars:\n            target_name = v.op.name\n            if target_name.startswith(\'target\'):\n                new_name = target_name.replace(\'target/\', \'\')\n                logger.info(""Target Network Update: {} <- {}"".format(target_name, new_name))\n                ops.append(v.assign(G.get_tensor_by_name(new_name + \':0\')))\n        return tf.group(*ops, name=\'update_target_network\')\n'"
examples/DeepQNetwork/atari.py,0,"b'# -*- coding: utf-8 -*-\n# File: atari.py\n# Author: Yuxin Wu\n\nimport numpy as np\nimport os\nimport threading\nimport cv2\nimport gym\nimport six\nfrom atari_py.ale_python_interface import ALEInterface\nfrom gym import spaces\nfrom gym.envs.atari.atari_env import ACTION_MEANING\n\nfrom tensorpack.utils import logger, execute_only_once, get_rng\nfrom tensorpack.utils.fs import get_dataset_path\n\n__all__ = [\'AtariPlayer\']\n\nROM_URL = ""https://github.com/openai/atari-py/tree/gdb/atari_py/atari_roms""\n_ALE_LOCK = threading.Lock()\n\n\nclass AtariPlayer(gym.Env):\n    """"""\n    A wrapper for ALE emulator, with configurations to mimic DeepMind DQN settings.\n\n    Info:\n        score: the accumulated reward in the current game\n        gameOver: True when the current game is Over\n    """"""\n\n    def __init__(self, rom_file, viz=0,\n                 frame_skip=4, nullop_start=30,\n                 live_lost_as_eoe=True, max_num_frames=0,\n                 grayscale=True):\n        """"""\n        Args:\n            rom_file: path to the rom\n            frame_skip: skip every k frames and repeat the action\n            viz: visualization to be done.\n                Set to 0 to disable.\n                Set to a positive number to be the delay between frames to show.\n                Set to a string to be a directory to store frames.\n            nullop_start: start with random number of null ops.\n            live_losts_as_eoe: consider lost of lives as end of episode. Useful for training.\n            max_num_frames: maximum number of frames per episode.\n            grayscale (bool): if True, return 2D image. Otherwise return HWC image.\n        """"""\n        super(AtariPlayer, self).__init__()\n        if not os.path.isfile(rom_file) and \'/\' not in rom_file:\n            rom_file = get_dataset_path(\'atari_rom\', rom_file)\n        assert os.path.isfile(rom_file), \\\n            ""ROM {} not found. Please download at {}"".format(rom_file, ROM_URL)\n\n        try:\n            ALEInterface.setLoggerMode(ALEInterface.Logger.Error)\n        except AttributeError:\n            if execute_only_once():\n                logger.warn(""You\'re not using latest ALE"")\n\n        # avoid simulator bugs: https://github.com/mgbellemare/Arcade-Learning-Environment/issues/86\n        with _ALE_LOCK:\n            self.ale = ALEInterface()\n            self.rng = get_rng(self)\n            self.ale.setInt(b""random_seed"", self.rng.randint(0, 30000))\n            self.ale.setInt(b""max_num_frames_per_episode"", max_num_frames)\n            self.ale.setBool(b""showinfo"", False)\n\n            self.ale.setInt(b""frame_skip"", 1)\n            self.ale.setBool(b\'color_averaging\', False)\n            # manual.pdf suggests otherwise.\n            self.ale.setFloat(b\'repeat_action_probability\', 0.0)\n\n            # viz setup\n            if isinstance(viz, six.string_types):\n                assert os.path.isdir(viz), viz\n                self.ale.setString(b\'record_screen_dir\', viz)\n                viz = 0\n            if isinstance(viz, int):\n                viz = float(viz)\n            self.viz = viz\n            if self.viz and isinstance(self.viz, float):\n                self.windowname = os.path.basename(rom_file)\n                cv2.namedWindow(self.windowname)\n\n            self.ale.loadROM(rom_file.encode(\'utf-8\'))\n        self.width, self.height = self.ale.getScreenDims()\n        self.actions = self.ale.getMinimalActionSet()\n\n        self.live_lost_as_eoe = live_lost_as_eoe\n        self.frame_skip = frame_skip\n        self.nullop_start = nullop_start\n\n        self.action_space = spaces.Discrete(len(self.actions))\n        self.grayscale = grayscale\n        shape = (self.height, self.width) if grayscale else (self.height, self.width, 3)\n        self.observation_space = spaces.Box(\n            low=0, high=255, shape=shape, dtype=np.uint8)\n        self._restart_episode()\n\n    def get_action_meanings(self):\n        return [ACTION_MEANING[i] for i in self.actions]\n\n    def _grab_raw_image(self):\n        """"""\n        :returns: the current 3-channel image\n        """"""\n        m = self.ale.getScreenRGB()\n        return m.reshape((self.height, self.width, 3))\n\n    def _current_state(self):\n        """"""\n        :returns: a gray-scale (h, w) uint8 image\n        """"""\n        ret = self._grab_raw_image()\n        # max-pooled over the last screen\n        ret = np.maximum(ret, self.last_raw_screen)\n        if self.viz:\n            if isinstance(self.viz, float):\n                cv2.imshow(self.windowname, ret)\n                cv2.waitKey(int(self.viz * 1000))\n        if self.grayscale:\n            # 0.299,0.587.0.114. same as rgb2y in torch/image\n            ret = cv2.cvtColor(ret, cv2.COLOR_RGB2GRAY)\n        return ret.astype(\'uint8\')  # to save some memory\n\n    def _restart_episode(self):\n        with _ALE_LOCK:\n            self.ale.reset_game()\n\n        # random null-ops start\n        n = self.rng.randint(self.nullop_start)\n        self.last_raw_screen = self._grab_raw_image()\n        for k in range(n):\n            if k == n - 1:\n                self.last_raw_screen = self._grab_raw_image()\n            self.ale.act(0)\n\n    def reset(self):\n        if self.ale.game_over():\n            self._restart_episode()\n        return self._current_state()\n\n    def render(self, *args, **kwargs):\n        pass  # visualization for this env is through the viz= argument when creating the player\n\n    def step(self, act):\n        oldlives = self.ale.lives()\n        r = 0\n        for k in range(self.frame_skip):\n            if k == self.frame_skip - 1:\n                self.last_raw_screen = self._grab_raw_image()\n            r += self.ale.act(self.actions[act])\n            newlives = self.ale.lives()\n            if self.ale.game_over() or \\\n                    (self.live_lost_as_eoe and newlives < oldlives):\n                break\n\n        isOver = self.ale.game_over()\n        if self.live_lost_as_eoe:\n            isOver = isOver or newlives < oldlives\n\n        info = {\'ale.lives\': newlives}\n        return self._current_state(), r, isOver, info\n\n\nif __name__ == \'__main__\':\n    import sys\n\n    a = AtariPlayer(sys.argv[1], viz=0.03)\n    num = a.action_space.n\n    rng = get_rng(num)\n    while True:\n        act = rng.choice(range(num))\n        state, reward, isOver, info = a.step(act)\n        if isOver:\n            print(info)\n            a.reset()\n        print(""Reward:"", reward)\n'"
examples/DeepQNetwork/atari_wrapper.py,0,"b'# -*- coding: utf-8 -*-\n# File: atari_wrapper.py\n\nimport numpy as np\nfrom collections import deque\nimport gym\n\n_v0, _v1 = gym.__version__.split(\'.\')[:2]\nassert int(_v0) > 0 or int(_v1) >= 10, gym.__version__\n\n\n""""""\nThe following wrappers are copied or modified from openai/baselines:\nhttps://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n""""""\n\n\nclass MapState(gym.ObservationWrapper):\n    def __init__(self, env, map_func):\n        gym.ObservationWrapper.__init__(self, env)\n        self._func = map_func\n\n    def observation(self, obs):\n        return self._func(obs)\n\n\nclass FrameStack(gym.Wrapper):\n    """"""\n    Buffer consecutive k observations and stack them on a new last axis.\n    The output observation has shape `original_shape + (k, )`.\n    """"""\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n\n    def reset(self):\n        """"""Clear buffer and re-fill by duplicating the first observation.""""""\n        ob = self.env.reset()\n        for _ in range(self.k - 1):\n            self.frames.append(np.zeros_like(ob))\n        self.frames.append(ob)\n        return self.observation()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self.observation(), reward, done, info\n\n    def observation(self):\n        assert len(self.frames) == self.k\n        return np.stack(self.frames, axis=-1)\n\n\nclass _FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self):\n        self.env.reset()\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset()\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset()\n        return obs\n\n    def step(self, action):\n        return self.env.step(action)\n\n\ndef FireResetEnv(env):\n    if isinstance(env, gym.Wrapper):\n        baseenv = env.unwrapped\n    else:\n        baseenv = env\n    if \'FIRE\' in baseenv.get_action_meanings():\n        return _FireResetEnv(env)\n    return env\n\n\nclass LimitLength(gym.Wrapper):\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n\n    def reset(self):\n        # This assumes that reset() will really reset the env.\n        # If the underlying env tries to be smart about reset\n        # (e.g. end-of-life), the assumption doesn\'t hold.\n        ob = self.env.reset()\n        self.cnt = 0\n        return ob\n\n    def step(self, action):\n        ob, r, done, info = self.env.step(action)\n        self.cnt += 1\n        if self.cnt == self.k:\n            done = True\n        return ob, r, done, info\n'"
examples/DeepQNetwork/common.py,0,"b'# -*- coding: utf-8 -*-\n# File: common.py\n# Author: Yuxin Wu\n\nimport multiprocessing\nimport numpy as np\nimport random\nimport time\nfrom six.moves import queue\n\nfrom tensorpack.callbacks import Callback\nfrom tensorpack.utils import logger, get_tqdm\nfrom tensorpack.utils.concurrency import ShareSessionThread, StoppableThread\nfrom tensorpack.utils.stats import StatCounter\n\n\ndef play_one_episode(env, func, render=False):\n    def predict(s):\n        """"""\n        Map from observation to action, with 0.01 greedy.\n        """"""\n        s = np.expand_dims(s, 0)  # batch\n        act = func(s)[0][0].argmax()\n        if random.random() < 0.01:\n            spc = env.action_space\n            act = spc.sample()\n        return act\n\n    ob = env.reset()\n    sum_r = 0\n    while True:\n        act = predict(ob)\n        ob, r, isOver, info = env.step(act)\n        if render:\n            env.render()\n        sum_r += r\n        if isOver:\n            return sum_r\n\n\ndef play_n_episodes(player, predfunc, nr, render=False):\n    logger.info(""Start Playing ... "")\n    for k in range(nr):\n        score = play_one_episode(player, predfunc, render=render)\n        print(""{}/{}, score={}"".format(k, nr, score))\n\n\ndef eval_with_funcs(predictors, nr_eval, get_player_fn, verbose=False):\n    """"""\n    Args:\n        predictors ([PredictorBase])\n    """"""\n    class Worker(StoppableThread, ShareSessionThread):\n        def __init__(self, func, queue):\n            super(Worker, self).__init__()\n            self._func = func\n            self.q = queue\n\n        def func(self, *args, **kwargs):\n            if self.stopped():\n                raise RuntimeError(""stopped!"")\n            return self._func(*args, **kwargs)\n\n        def run(self):\n            with self.default_sess():\n                player = get_player_fn(train=False)\n                while not self.stopped():\n                    try:\n                        score = play_one_episode(player, self.func)\n                    except RuntimeError:\n                        return\n                    self.queue_put_stoppable(self.q, score)\n\n    q = queue.Queue()\n    threads = [Worker(f, q) for f in predictors]\n\n    for k in threads:\n        k.start()\n        time.sleep(0.1)  # avoid simulator bugs\n    stat = StatCounter()\n\n    def fetch():\n        r = q.get()\n        stat.feed(r)\n        if verbose:\n            logger.info(""Score: {}"".format(r))\n\n    for _ in get_tqdm(range(nr_eval)):\n        fetch()\n    # waiting is necessary, otherwise the estimated mean score is biased\n    logger.info(""Waiting for all the workers to finish the last run..."")\n    for k in threads:\n        k.stop()\n    for k in threads:\n        k.join()\n    while q.qsize():\n        fetch()\n\n    if stat.count > 0:\n        return (stat.average, stat.max)\n    return (0, 0)\n\n\ndef eval_model_multithread(pred, nr_eval, get_player_fn):\n    """"""\n    Args:\n        pred (OfflinePredictor): state -> [#action]\n    """"""\n    NR_PROC = min(multiprocessing.cpu_count() // 2, 8)\n    with pred.sess.as_default():\n        mean, max = eval_with_funcs(\n            [pred] * NR_PROC, nr_eval,\n            get_player_fn, verbose=True)\n    logger.info(""Average Score: {}; Max Score: {}"".format(mean, max))\n\n\nclass Evaluator(Callback):\n    def __init__(self, nr_eval, input_names, output_names, get_player_fn):\n        self.eval_episode = nr_eval\n        self.input_names = input_names\n        self.output_names = output_names\n        self.get_player_fn = get_player_fn\n\n    def _setup_graph(self):\n        NR_PROC = min(multiprocessing.cpu_count() // 2, 20)\n        self.pred_funcs = [self.trainer.get_predictor(\n            self.input_names, self.output_names)] * NR_PROC\n\n    def _trigger(self):\n        t = time.time()\n        mean, max = eval_with_funcs(\n            self.pred_funcs, self.eval_episode, self.get_player_fn)\n        t = time.time() - t\n        if t > 10 * 60:  # eval takes too long\n            self.eval_episode = int(self.eval_episode * 0.94)\n        self.trainer.monitors.put_scalar(\'mean_score\', mean)\n        self.trainer.monitors.put_scalar(\'max_score\', max)\n'"
examples/DeepQNetwork/expreplay.py,0,"b'# -*- coding: utf-8 -*-\n# File: expreplay.py\n# Author: Yuxin Wu\n\nimport copy\nimport itertools\nimport numpy as np\nimport threading\nfrom collections import namedtuple\nfrom six.moves import queue, range\n\nfrom tensorpack.utils.concurrency import LoopThread, ShareSessionThread\nfrom tensorpack.callbacks.base import Callback\nfrom tensorpack.dataflow import DataFlow\nfrom tensorpack.utils import logger, get_rng, get_tqdm\nfrom tensorpack.utils.stats import StatCounter\n\n__all__ = [\'ExpReplay\']\n\nExperience = namedtuple(\'Experience\',\n                        [\'state\', \'action\', \'reward\', \'isOver\'])\n\n\nclass ReplayMemory(object):\n    def __init__(self, max_size, state_shape, history_len, dtype=\'uint8\'):\n        """"""\n        Args:\n            state_shape (tuple[int]): shape (without history) of state\n            dtype: numpy dtype for the state\n        """"""\n        self.max_size = int(max_size)\n        self.state_shape = state_shape\n        assert len(state_shape) in [1, 2, 3], state_shape\n        # self._output_shape = self.state_shape + (history_len + 1, )\n        self.history_len = int(history_len)\n        self.dtype = dtype\n\n        all_state_shape = (self.max_size,) + state_shape\n        logger.info(""Creating experience replay buffer of {:.1f} GB ... ""\n                    ""use a smaller buffer if you don\'t have enough CPU memory."".format(\n                        np.prod(all_state_shape) / 1024.0**3))\n        self.state = np.zeros(all_state_shape, dtype=self.dtype)\n        self.action = np.zeros((self.max_size,), dtype=\'int32\')\n        self.reward = np.zeros((self.max_size,), dtype=\'float32\')\n        self.isOver = np.zeros((self.max_size,), dtype=\'bool\')\n\n        self._curr_size = 0\n        self._curr_pos = 0\n\n        self.writer_lock = threading.Lock()  # a lock to guard writing to the memory\n\n    def append(self, exp):\n        """"""\n        Args:\n            exp (Experience):\n        """"""\n        if self._curr_size < self.max_size:\n            self._assign(self._curr_pos, exp)\n            self._curr_pos = (self._curr_pos + 1) % self.max_size\n            self._curr_size += 1\n        else:\n            self._assign(self._curr_pos, exp)\n            self._curr_pos = (self._curr_pos + 1) % self.max_size\n\n    def sample(self, idx):\n        """""" return a tuple of (s,r,a,o),\n            where s is of shape self._output_shape, which is\n            [H, W, (hist_len+1) * channel] if input is (H, W, channel)""""""\n        idx = (self._curr_pos + idx) % self._curr_size\n        k = self.history_len + 1\n        if idx + k <= self._curr_size:\n            state = self.state[idx: idx + k]\n            reward = self.reward[idx: idx + k]\n            action = self.action[idx: idx + k]\n            isOver = self.isOver[idx: idx + k]\n        else:\n            end = idx + k - self._curr_size\n            state = self._slice(self.state, idx, end)\n            reward = self._slice(self.reward, idx, end)\n            action = self._slice(self.action, idx, end)\n            isOver = self._slice(self.isOver, idx, end)\n        ret = self._pad_sample(state, reward, action, isOver)\n        return ret\n\n    # the next_state is a different episode if current_state.isOver==True\n    def _pad_sample(self, state, reward, action, isOver):\n        # state: Hist+1,H,W,C\n        for k in range(self.history_len - 2, -1, -1):\n            if isOver[k]:\n                state = copy.deepcopy(state)\n                state[:k + 1].fill(0)\n                break\n        # move the first dim (history) to the last\n        state = np.moveaxis(state, 0, -1)\n        return (state, reward[-2], action[-2], isOver[-2])\n\n    def _slice(self, arr, start, end):\n        s1 = arr[start:]\n        s2 = arr[:end]\n        return np.concatenate((s1, s2), axis=0)\n\n    def __len__(self):\n        return self._curr_size\n\n    def _assign(self, pos, exp):\n        self.state[pos] = exp.state\n        self.reward[pos] = exp.reward\n        self.action[pos] = exp.action\n        self.isOver[pos] = exp.isOver\n\n\nclass EnvRunner(object):\n    """"""\n    A class which is responsible for\n    stepping the environment with epsilon-greedy,\n    and fill the results to experience replay buffer.\n    """"""\n    def __init__(self, player, predictor, memory, history_len):\n        """"""\n        Args:\n            player (gym.Env)\n            predictor (callable): the model forward function which takes a\n                state and returns the prediction.\n            memory (ReplayMemory): the replay memory to store experience to.\n            history_len (int):\n        """"""\n        self.player = player\n        self.num_actions = player.action_space.n\n        self.predictor = predictor\n        self.memory = memory\n        self.state_shape = memory.state_shape\n        self.dtype = memory.dtype\n        self.history_len = history_len\n\n        self._current_episode = []\n        self._current_ob = player.reset()\n        self._current_game_score = StatCounter()  # store per-step reward\n        self.total_scores = []  # store per-game total score\n\n        self.rng = get_rng(self)\n\n    def step(self, exploration):\n        """"""\n        Run the environment for one step.\n        If the episode ends, store the entire episode to the replay memory.\n        """"""\n        old_s = self._current_ob\n        if self.rng.rand() <= exploration:\n            act = self.rng.choice(range(self.num_actions))\n        else:\n            history = self.recent_state()\n            history.append(old_s)\n            history = np.stack(history, axis=-1)  # state_shape + (Hist,)\n\n            # assume batched network\n            history = np.expand_dims(history, axis=0)\n            q_values = self.predictor(history)[0][0]  # this is the bottleneck\n            act = np.argmax(q_values)\n\n        self._current_ob, reward, isOver, info = self.player.step(act)\n        self._current_game_score.feed(reward)\n        self._current_episode.append(Experience(old_s, act, reward, isOver))\n\n        if isOver:\n            flush_experience = True\n            if \'ale.lives\' in info:  # if running Atari, do something special\n                if info[\'ale.lives\'] != 0:\n                    # only record score and flush experience\n                    # when a whole game is over (not when an episode is over)\n                    flush_experience = False\n            self.player.reset()\n\n            if flush_experience:\n                self.total_scores.append(self._current_game_score.sum)\n                self._current_game_score.reset()\n\n                # Ensure that the whole episode of experience is continuous in the replay buffer\n                with self.memory.writer_lock:\n                    for exp in self._current_episode:\n                        self.memory.append(exp)\n                self._current_episode.clear()\n\n    def recent_state(self):\n        """"""\n        Get the recent state (with stacked history) of the environment.\n\n        Returns:\n            a list of ``hist_len-1`` elements, each of shape ``self.state_shape``\n        """"""\n        expected_len = self.history_len - 1\n        if len(self._current_episode) >= expected_len:\n            return [k.state for k in self._current_episode[-expected_len:]]\n        else:\n            states = [np.zeros(self.state_shape, dtype=self.dtype)] * (expected_len - len(self._current_episode))\n            states.extend([k.state for k in self._current_episode])\n            return states\n\n\nclass EnvRunnerManager(object):\n    """"""\n    A class which manages a list of :class:`EnvRunner`.\n    Its job is to execute them possibly in parallel and aggregate their results.\n    """"""\n    def __init__(self, env_runners, maximum_staleness):\n        """"""\n        Args:\n            env_runners (list[EnvRunner]):\n            maximum_staleness (int): when >1 environments run in parallel,\n                the actual stepping of an environment may happen several steps\n                after calls to `EnvRunnerManager.step()`, in order to achieve better throughput.\n        """"""\n        assert len(env_runners) > 0\n        self._runners = env_runners\n\n        if len(self._runners) > 1:\n            # Only use threads when having >1 runners.\n            self._populate_job_queue = queue.Queue(maxsize=maximum_staleness)\n            self._threads = [self._create_simulator_thread(i) for i in range(len(self._runners))]\n            for t in self._threads:\n                t.start()\n\n    def _create_simulator_thread(self, idx):\n        # spawn a separate thread to run policy\n        def populate_job_func():\n            exp = self._populate_job_queue.get()\n            self._runners[idx].step(exp)\n\n        th = ShareSessionThread(LoopThread(populate_job_func, pausable=False))\n        th.name = ""SimulatorThread-{}"".format(idx)\n        return th\n\n    def step(self, exploration):\n        """"""\n        Execute one step in any of the runners.\n        """"""\n        if len(self._runners) > 1:\n            self._populate_job_queue.put(exploration)\n        else:\n            self._runners[0].step(exploration)\n\n    def reset_stats(self):\n        """"""\n        Returns:\n            mean, max: two stats of the runners, to be added to backend\n        """"""\n        scores = list(itertools.chain.from_iterable([v.total_scores for v in self._runners]))\n        for v in self._runners:\n            v.total_scores.clear()\n\n        try:\n            return np.mean(scores), np.max(scores)\n        except Exception:\n            logger.exception(""Cannot compute total scores in EnvRunner."")\n            return None, None\n\n\nclass ExpReplay(DataFlow, Callback):\n    """"""\n    Implement experience replay in the paper\n    `Human-level control through deep reinforcement learning\n    <http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html>`_.\n\n    This implementation provides the interface as a :class:`DataFlow`.\n    This DataFlow is __not__ fork-safe (thus doesn\'t support multiprocess prefetching).\n\n    It does the following:\n    * Spawn `num_parallel_players` environment thread, each running an instance\n      of the environment with epislon-greedy policy.\n    * All environment instances writes their experiences to a shared replay\n      memory buffer.\n    * Produces batched samples by sampling the replay buffer. After producing\n      each batch, it executes the environment instances by a total of\n      `update_frequency` steps.\n\n    This implementation assumes that state is batch-able, and the network takes batched inputs.\n    """"""\n\n    def __init__(self,\n                 predictor_io_names,\n                 get_player,\n                 num_parallel_players,\n                 state_shape,\n                 batch_size,\n                 memory_size, init_memory_size,\n                 update_frequency, history_len,\n                 state_dtype=\'uint8\'):\n        """"""\n        Args:\n            predictor_io_names (tuple of list of str): input/output names to\n                predict Q value from state.\n            get_player (-> gym.Env): a callable which returns a player.\n            num_parallel_players (int): number of players to run in parallel.\n                Standard DQN uses 1.\n                Parallelism increases speed, but will affect the distribution of\n                experiences in the replay buffer.\n            state_shape (tuple):\n            batch_size (int):\n            memory_size (int):\n            init_memory_size (int):\n            update_frequency (int): number of new transitions to add to memory\n                after sampling a batch of transitions for training.\n            history_len (int): length of history frames to concat. Zero-filled\n                initial frames.\n            state_dtype (str):\n        """"""\n        assert len(state_shape) in [1, 2, 3], state_shape\n        init_memory_size = int(init_memory_size)\n\n        for k, v in locals().items():\n            if k != \'self\':\n                setattr(self, k, v)\n        self.exploration = 1.0  # default initial exploration\n\n        self.rng = get_rng(self)\n        self._init_memory_flag = threading.Event()  # tell if memory has been initialized\n\n        self.mem = ReplayMemory(memory_size, state_shape, self.history_len, dtype=state_dtype)\n\n    def _init_memory(self):\n        logger.info(""Populating replay memory with epsilon={} ..."".format(self.exploration))\n\n        with get_tqdm(total=self.init_memory_size) as pbar:\n            while len(self.mem) < self.init_memory_size:\n                self.runner.step(self.exploration)\n                pbar.update()\n        self._init_memory_flag.set()\n\n    # quickly fill the memory for debug\n    def _fake_init_memory(self):\n        from copy import deepcopy\n        with get_tqdm(total=self.init_memory_size) as pbar:\n            while len(self.mem) < 5:\n                self.runner.step(self.exploration)\n                pbar.update()\n            while len(self.mem) < self.init_memory_size:\n                self.mem.append(deepcopy(self.mem._hist[0]))\n                pbar.update()\n        self._init_memory_flag.set()\n\n    def _debug_sample(self, sample):\n        import cv2\n\n        def view_state(comb_state):\n            # this function assumes comb_state is 3D\n            state = comb_state[:, :, :-1]\n            next_state = comb_state[:, :, 1:]\n            r = np.concatenate([state[:, :, k] for k in range(self.history_len)], axis=1)\n            r2 = np.concatenate([next_state[:, :, k] for k in range(self.history_len)], axis=1)\n            r = np.concatenate([r, r2], axis=0)\n            cv2.imshow(""state"", r)\n            cv2.waitKey()\n        print(""Act: "", sample[2], "" reward:"", sample[1], "" isOver: "", sample[3])\n        if sample[1] or sample[3]:\n            view_state(sample[0])\n\n    def _process_batch(self, batch_exp):\n        state = np.asarray([e[0] for e in batch_exp], dtype=self.state_dtype)\n        reward = np.asarray([e[1] for e in batch_exp], dtype=\'float32\')\n        action = np.asarray([e[2] for e in batch_exp], dtype=\'int8\')\n        isOver = np.asarray([e[3] for e in batch_exp], dtype=\'bool\')\n        return [state, action, reward, isOver]\n\n    # DataFlow method:\n    def __iter__(self):\n        # wait for memory to be initialized\n        self._init_memory_flag.wait()\n\n        while True:\n            idx = self.rng.randint(\n                0, len(self.mem) - self.history_len - 1,\n                size=self.batch_size)\n            batch_exp = [self.mem.sample(i) for i in idx]\n\n            yield self._process_batch(batch_exp)\n\n            # execute update_freq=4 new actions into memory, after each batch update\n            for _ in range(self.update_frequency):\n                self.runner.step(self.exploration)\n\n    # Callback methods:\n    def _setup_graph(self):\n        self.predictor = self.trainer.get_predictor(*self.predictor_io_names)\n\n    def _before_train(self):\n        env_runners = [\n            EnvRunner(self.get_player(), self.predictor, self.mem, self.history_len)\n            for k in range(self.num_parallel_players)\n        ]\n        self.runner = EnvRunnerManager(env_runners, self.update_frequency * 2)\n        self._init_memory()\n\n    def _trigger(self):\n        mean, max = self.runner.reset_stats()\n        if mean is not None:\n            self.trainer.monitors.put_scalar(\'expreplay/mean_score\', mean)\n            self.trainer.monitors.put_scalar(\'expreplay/max_score\', max)\n'"
examples/DisturbLabel/disturb.py,0,"b'# -*- coding: utf-8 -*-\n# File: disturb.py\n# Author: Yuxin Wu\n\nfrom tensorpack.dataflow import ProxyDataFlow, RNGDataFlow\n\n\nclass DisturbLabel(ProxyDataFlow, RNGDataFlow):\n    def __init__(self, ds, prob):\n        super(DisturbLabel, self).__init__(ds)\n        self.prob = prob\n\n    def reset_state(self):\n        RNGDataFlow.reset_state(self)\n        ProxyDataFlow.reset_state(self)\n\n    def __iter__(self):\n        for dp in self.ds:\n            img, l = dp\n            if self.rng.rand() < self.prob:\n                l = self.rng.choice(10)\n            yield [img, l]\n'"
examples/DisturbLabel/mnist-disturb.py,12,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-disturb.py\n\nimport argparse\nimport imp\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.utils import logger\n\nfrom disturb import DisturbLabel\n\nmnist_example = imp.load_source('mnist_example',\n                                os.path.join(os.path.dirname(__file__), '..', 'basics', 'mnist-convnet.py'))\nget_config = mnist_example.get_config\n\n\ndef get_data():\n    dataset_train = BatchData(DisturbLabel(dataset.Mnist('train'), args.prob), 128)\n    dataset_test = BatchData(dataset.Mnist('test'), 256, remainder=True)\n    return dataset_train, dataset_test\n\n\nmnist_example.get_data = get_data\n\n\nclass Model(mnist_example.Model):\n    def build_graph(self, image, label):\n        image = tf.expand_dims(image, 3)\n\n        logits = (LinearWrap(image)  # the starting brace is oactivationy for line-breaking\n                  .Conv2D('conv0', 32, 5, padding='VALID', activation=tf.nn.relu)\n                  .MaxPooling('pool0', 2)\n                  .Conv2D('conv1', 64, 5, padding='VALID', activation=tf.nn.relu)\n                  .MaxPooling('pool1', 2)\n                  .FullyConnected('fc0', 512, activation=tf.nn.relu)\n                  .FullyConnected('fc1', out_dim=10, activation=tf.identity)())\n        tf.nn.softmax(logits, name='prob')\n\n        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='incorrect_vector')\n        add_moving_summary(tf.reduce_mean(wrong, name='train_error'))\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n        wd_cost = tf.multiply(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss),\n                              name='regularize_loss')\n\n        return tf.add_n([wd_cost, cost], name='cost')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')\n    parser.add_argument('--prob', help='disturb prob', type=float, required=True)\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\n    logger.auto_set_dir()\n    config = get_config()\n    launch_train_with_config(config, SimpleTrainer())\n"""
examples/DisturbLabel/svhn-disturb.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: svhn-disturb.py\n\nimport argparse\nimport imp\nimport os\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.utils import logger\n\nfrom disturb import DisturbLabel\n\nsvhn_example = imp.load_source('svhn_example',\n                               os.path.join(os.path.dirname(__file__), '..',\n                                            'basics', 'svhn-digit-convnet.py'))\nModel = svhn_example.Model\n\n\ndef get_data():\n    d1 = dataset.SVHNDigit('train')\n    d2 = dataset.SVHNDigit('extra')\n    data_train = RandomMixData([d1, d2])\n    data_train = DisturbLabel(data_train, args.prob)\n    data_test = dataset.SVHNDigit('test')\n\n    augmentors = [\n        imgaug.Resize((40, 40)),\n        imgaug.Brightness(30),\n        imgaug.Contrast((0.5, 1.5)),\n    ]\n    data_train = AugmentImageComponent(data_train, augmentors)\n    data_train = BatchData(data_train, 128)\n    data_train = MultiProcessRunner(data_train, 5, 5)\n\n    augmentors = [imgaug.Resize((40, 40))]\n    data_test = AugmentImageComponent(data_test, augmentors)\n    data_test = BatchData(data_test, 128, remainder=True)\n    return data_train, data_test\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--prob', help='disturb prob', type=float, required=True)\n    args = parser.parse_args()\n\n    logger.auto_set_dir()\n    data_train, data_test = get_data()\n    config = TrainConfig(\n        model=Model(),\n        data=QueueInput(data_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(data_test,\n                            ScalarStats(['cost', 'accuracy']))\n        ],\n        max_epoch=350,\n    )\n    launch_train_with_config(config, SimpleTrainer())\n"""
examples/DoReFa-Net/alexnet-dorefa.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: alexnet-dorefa.py\n# Author: Yuxin Wu, Yuheng Zou ({wyx,zyh}@megvii.com)\n\nimport argparse\nimport numpy as np\nimport os\nimport sys\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.sessinit import SmartInit\nfrom tensorpack.tfutils.summary import add_param_summary\nfrom tensorpack.tfutils.varreplace import remap_variables\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom dorefa import get_dorefa, ternarize\nfrom imagenet_utils import ImageNetModel, eval_classification, fbresnet_augmentor, get_imagenet_dataflow\n\n""""""\nThis is a tensorpack script for the ImageNet results in paper:\nDoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\nhttp://arxiv.org/abs/1606.06160\n\nThe original experiements are performed on a proprietary framework.\nThis is our attempt to reproduce it on tensorpack & TensorFlow.\n\nTo Train:\n    ./alexnet-dorefa.py --dorefa 1,2,6 --data PATH --gpu 0,1\n\n    PATH should look like:\n    PATH/\n      train/\n        n02134418/\n          n02134418_198.JPEG\n          ...\n        ...\n      val/\n        ILSVRC2012_val_00000001.JPEG\n        ...\n\n    And you\'ll need the following to be able to fetch data efficiently\n        Fast disk random access (Not necessarily SSD. I used a RAID of HDD, but not sure if plain HDD is enough)\n        More than 20 CPU cores (for data processing)\n        More than 10G of free memory\n    On 8 P100s and dorefa==1,2,6, the training should take about 30 minutes per epoch.\n\nTo run pretrained model:\n    ./alexnet-dorefa.py --load alexnet-126.npz --run a.jpg --dorefa 1,2,6\n""""""\n\nBITW = 1\nBITA = 2\nBITG = 6\nTOTAL_BATCH_SIZE = 256\nBATCH_SIZE = None\n\n\nclass Model(ImageNetModel):\n    weight_decay = 5e-6\n    weight_decay_pattern = \'fc.*/W\'\n\n    def get_logits(self, image):\n        if BITW == \'t\':\n            fw, fa, fg = get_dorefa(32, 32, 32)\n            fw = ternarize\n        else:\n            fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n\n        # monkey-patch tf.get_variable to apply fw\n        def new_get_variable(v):\n            name = v.op.name\n            # don\'t binarize first and last layer\n            if not name.endswith(\'W\') or \'conv0\' in name or \'fct\' in name:\n                return v\n            else:\n                logger.info(""Quantizing weight {}"".format(v.op.name))\n                return fw(v)\n\n        def nonlin(x):\n            if BITA == 32:\n                return tf.nn.relu(x)    # still use relu for 32bit cases\n            return tf.clip_by_value(x, 0.0, 1.0)\n\n        def activate(x):\n            return fa(nonlin(x))\n\n        with remap_variables(new_get_variable), \\\n                argscope([Conv2D, BatchNorm, MaxPooling], data_format=\'channels_first\'), \\\n                argscope(BatchNorm, momentum=0.9, epsilon=1e-4), \\\n                argscope(Conv2D, use_bias=False):\n            logits = (LinearWrap(image)\n                      .Conv2D(\'conv0\', 96, 12, strides=4, padding=\'VALID\', use_bias=True)\n                      .apply(activate)\n                      .Conv2D(\'conv1\', 256, 5, padding=\'SAME\', split=2)\n                      .apply(fg)\n                      .BatchNorm(\'bn1\')\n                      .MaxPooling(\'pool1\', 3, 2, padding=\'SAME\')\n                      .apply(activate)\n\n                      .Conv2D(\'conv2\', 384, 3)\n                      .apply(fg)\n                      .BatchNorm(\'bn2\')\n                      .MaxPooling(\'pool2\', 3, 2, padding=\'SAME\')\n                      .apply(activate)\n\n                      .Conv2D(\'conv3\', 384, 3, split=2)\n                      .apply(fg)\n                      .BatchNorm(\'bn3\')\n                      .apply(activate)\n\n                      .Conv2D(\'conv4\', 256, 3, split=2)\n                      .apply(fg)\n                      .BatchNorm(\'bn4\')\n                      .MaxPooling(\'pool4\', 3, 2, padding=\'VALID\')\n                      .apply(activate)\n\n                      .FullyConnected(\'fc0\', 4096)\n                      .apply(fg)\n                      .BatchNorm(\'bnfc0\')\n                      .apply(activate)\n\n                      .FullyConnected(\'fc1\', 4096, use_bias=False)\n                      .apply(fg)\n                      .BatchNorm(\'bnfc1\')\n                      .apply(nonlin)\n                      .FullyConnected(\'fct\', 1000, use_bias=True)())\n        add_param_summary((\'.*/W\', [\'histogram\', \'rms\']))\n        tf.nn.softmax(logits, name=\'output\')  # for prediction\n        return logits\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=2e-4, trainable=False)\n        return tf.train.AdamOptimizer(lr, epsilon=1e-5)\n\n\ndef get_data(dataset_name):\n    isTrain = dataset_name == \'train\'\n    augmentors = fbresnet_augmentor(isTrain)\n    return get_imagenet_dataflow(\n        args.data, dataset_name, BATCH_SIZE, augmentors)\n\n\ndef get_config():\n    data_train = get_data(\'train\')\n    data_test = get_data(\'val\')\n\n    return TrainConfig(\n        dataflow=data_train,\n        callbacks=[\n            ModelSaver(),\n            ScheduledHyperParamSetter(\n                \'learning_rate\', [(60, 4e-5), (75, 8e-6)]),\n            InferenceRunner(data_test,\n                            [ClassificationError(\'wrong-top1\', \'val-error-top1\'),\n                             ClassificationError(\'wrong-top5\', \'val-error-top5\')])\n        ],\n        model=Model(),\n        steps_per_epoch=1280000 // TOTAL_BATCH_SIZE,\n        max_epoch=90,\n    )\n\n\ndef run_image(model, sess_init, inputs):\n    pred_config = PredictConfig(\n        model=model,\n        session_init=sess_init,\n        input_names=[\'input\'],\n        output_names=[\'output\']\n    )\n    predictor = OfflinePredictor(pred_config)\n    meta = dataset.ILSVRCMeta()\n    words = meta.get_synset_words_1000()\n\n    transformers = imgaug.AugmentorList(fbresnet_augmentor(isTrain=False))\n    for f in inputs:\n        assert os.path.isfile(f), f\n        img = cv2.imread(f).astype(\'float32\')\n        assert img is not None\n\n        img = transformers.augment(img)[np.newaxis, :, :, :]\n        outputs = predictor(img)[0]\n        prob = outputs[0]\n        ret = prob.argsort()[-10:][::-1]\n\n        names = [words[i] for i in ret]\n        print(f + "":"")\n        print(list(zip(names, prob[ret])))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'the physical ids of GPUs to use\')\n    parser.add_argument(\'--load\', help=\'load a checkpoint, or a npz (given as the pretrained model)\')\n    parser.add_argument(\'--data\', help=\'ILSVRC dataset dir\')\n    parser.add_argument(\'--dorefa\', required=True,\n                        help=\'number of bits for W,A,G, separated by comma. W=""t"" means TTQ\')\n    parser.add_argument(\'--run\', help=\'run on a list of images with the pretrained model\', nargs=\'*\')\n    parser.add_argument(\'--eval\', action=\'store_true\')\n    args = parser.parse_args()\n\n    dorefa = args.dorefa.split(\',\')\n    if dorefa[0] == \'t\':\n        assert dorefa[1] == \'32\' and dorefa[2] == \'32\'\n        BITW, BITA, BITG = \'t\', 32, 32\n    else:\n        BITW, BITA, BITG = map(int, dorefa)\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    if args.run:\n        assert args.load.endswith(\'.npz\')\n        run_image(Model(), SmartInit(args.load), args.run)\n        sys.exit()\n    if args.eval:\n        BATCH_SIZE = 128\n        ds = get_data(\'val\')\n        eval_classification(Model(), SmartInit(args.load), ds)\n        sys.exit()\n\n    nr_tower = max(get_num_gpu(), 1)\n    BATCH_SIZE = TOTAL_BATCH_SIZE // nr_tower\n    logger.set_logger_dir(os.path.join(\n        \'train_log\', \'alexnet-dorefa-{}\'.format(args.dorefa)))\n    logger.info(""Batch per tower: {}"".format(BATCH_SIZE))\n\n    config = get_config()\n    config.session_init = SmartInit(args.load)\n    launch_train_with_config(config, SyncMultiGPUTrainerReplicated(nr_tower))\n'"
examples/DoReFa-Net/dorefa.py,24,"b'# -*- coding: utf-8 -*-\n# File: dorefa.py\n# Author: Yuxin Wu\n\nimport tensorflow as tf\n\n\ndef get_dorefa(bitW, bitA, bitG):\n    """"""\n    Return the three quantization functions fw, fa, fg, for weights, activations and gradients respectively\n    """"""\n    def quantize(x, k):\n        n = float(2 ** k - 1)\n\n        @tf.custom_gradient\n        def _quantize(x):\n            return tf.round(x * n) / n, lambda dy: dy\n\n        return _quantize(x)\n\n    def fw(x):\n        if bitW == 32:\n            return x\n\n        if bitW == 1:   # BWN\n            E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))\n\n            @tf.custom_gradient\n            def _sign(x):\n                return tf.where(tf.equal(x, 0), tf.ones_like(x), tf.sign(x / E)) * E, lambda dy: dy\n\n            return _sign(x)\n\n        x = tf.tanh(x)\n        x = x / tf.reduce_max(tf.abs(x)) * 0.5 + 0.5\n        return 2 * quantize(x, bitW) - 1\n\n    def fa(x):\n        if bitA == 32:\n            return x\n        return quantize(x, bitA)\n\n    def fg(x):\n        if bitG == 32:\n            return x\n\n        @tf.custom_gradient\n        def _identity(input):\n            def grad_fg(x):\n                rank = x.get_shape().ndims\n                assert rank is not None\n                maxx = tf.reduce_max(tf.abs(x), list(range(1, rank)), keep_dims=True)\n                x = x / maxx\n                n = float(2**bitG - 1)\n                x = x * 0.5 + 0.5 + tf.random_uniform(\n                    tf.shape(x), minval=-0.5 / n, maxval=0.5 / n)\n                x = tf.clip_by_value(x, 0.0, 1.0)\n                x = quantize(x, bitG) - 0.5\n                return x * maxx * 2\n\n            return input, grad_fg\n\n        return _identity(x)\n    return fw, fa, fg\n\n\ndef ternarize(x, thresh=0.05):\n    """"""\n    Implemented Trained Ternary Quantization:\n    https://arxiv.org/abs/1612.01064\n\n    Code modified from the authors\' at:\n    https://github.com/czhu95/ternarynet/blob/master/examples/Ternary-Net/ternary.py\n    """"""\n    shape = x.get_shape()\n\n    thre_x = tf.stop_gradient(tf.reduce_max(tf.abs(x)) * thresh)\n\n    w_p = tf.get_variable(\'Wp\', initializer=1.0, dtype=tf.float32)\n    w_n = tf.get_variable(\'Wn\', initializer=1.0, dtype=tf.float32)\n\n    tf.summary.scalar(w_p.op.name + \'-summary\', w_p)\n    tf.summary.scalar(w_n.op.name + \'-summary\', w_n)\n\n    mask = tf.ones(shape)\n    mask_p = tf.where(x > thre_x, tf.ones(shape) * w_p, mask)\n    mask_np = tf.where(x < -thre_x, tf.ones(shape) * w_n, mask_p)\n    mask_z = tf.where((x < thre_x) & (x > - thre_x), tf.zeros(shape), mask)\n\n    @tf.custom_gradient\n    def _sign_mask(x):\n        return tf.sign(x) * mask_z, lambda dy: dy\n\n    w = _sign_mask(x)\n\n    w = w * mask_np\n\n    tf.summary.histogram(w.name, w)\n    return w\n'"
examples/DoReFa-Net/imagenet_utils.py,66,"b'# -*- coding: utf-8 -*-\n# File: imagenet_utils.py\n\n\nimport multiprocessing\nimport numpy as np\nimport os\nfrom abc import abstractmethod\nimport cv2\nimport tensorflow as tf\nimport tqdm\n\nfrom tensorpack import ModelDesc\nfrom tensorpack.dataflow import (\n    AugmentImageComponent, BatchData, MultiThreadMapData,\n    MultiProcessRunnerZMQ, dataset, imgaug)\nfrom tensorpack.input_source import QueueInput, StagingInput\nfrom tensorpack.models import regularize_cost, l2_regularizer\nfrom tensorpack.predict import FeedfreePredictor, PredictConfig\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.tfutils.optimizer import AccumGradOptimizer\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.stats import RatioCounter\n\n""""""\n====== DataFlow =======\n""""""\n\n\ndef fbresnet_augmentor(isTrain):\n    """"""\n    Augmentor used in fb.resnet.torch, for BGR images in range [0,255].\n    """"""\n    interpolation = cv2.INTER_CUBIC\n    # linear seems to have more stable performance.\n    # but we keep cubic for compatibility with old models\n    if isTrain:\n        augmentors = [\n            imgaug.GoogleNetRandomCropAndResize(interp=interpolation),\n            imgaug.ToFloat32(),  # avoid frequent casting in each color augmentation\n            # It\'s OK to remove the following augs if your CPU is not fast enough.\n            # Removing brightness/contrast/saturation does not have a significant effect on accuracy.\n            # Removing lighting leads to a tiny drop in accuracy.\n            imgaug.RandomOrderAug(\n                [imgaug.BrightnessScale((0.6, 1.4)),\n                 imgaug.Contrast((0.6, 1.4), rgb=False),\n                 imgaug.Saturation(0.4, rgb=False),\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\n                 imgaug.Lighting(0.1,\n                                 eigval=np.asarray(\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                                 eigvec=np.array(\n                                     [[-0.5675, 0.7192, 0.4009],\n                                      [-0.5808, -0.0045, -0.8140],\n                                      [-0.5836, -0.6948, 0.4203]],\n                                     dtype=\'float32\')[::-1, ::-1]\n                                 )]),\n            imgaug.ToUint8(),\n            imgaug.Flip(horiz=True),\n        ]\n    else:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, interp=interpolation),\n            imgaug.CenterCrop((224, 224)),\n        ]\n    return augmentors\n\n\ndef get_imagenet_dataflow(\n        datadir, name, batch_size,\n        augmentors=None, parallel=None):\n    """"""\n    Args:\n        augmentors (list[imgaug.Augmentor]): Defaults to `fbresnet_augmentor(isTrain)`\n\n    Returns: A DataFlow which produces BGR images and labels.\n\n    See explanations in the tutorial:\n    http://tensorpack.readthedocs.io/tutorial/efficient-dataflow.html\n    """"""\n    assert name in [\'train\', \'val\', \'test\']\n    isTrain = name == \'train\'\n    assert datadir is not None\n    if augmentors is None:\n        augmentors = fbresnet_augmentor(isTrain)\n    assert isinstance(augmentors, list)\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n\n    if isTrain:\n        ds = dataset.ILSVRC12(datadir, name, shuffle=True)\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        if parallel < 16:\n            logger.warn(""DataFlow may become the bottleneck when too few processes are used."")\n        ds = MultiProcessRunnerZMQ(ds, parallel)\n        ds = BatchData(ds, batch_size, remainder=False)\n    else:\n        ds = dataset.ILSVRC12Files(datadir, name, shuffle=False)\n        aug = imgaug.AugmentorList(augmentors)\n\n        def mapf(dp):\n            fname, cls = dp\n            im = cv2.imread(fname, cv2.IMREAD_COLOR)\n            im = aug.augment(im)\n            return im, cls\n        ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000, strict=True)\n        ds = BatchData(ds, batch_size, remainder=True)\n        ds = MultiProcessRunnerZMQ(ds, 1)\n    return ds\n\n\n""""""\n====== tf.data =======\n""""""\n\n\ndef get_imagenet_tfdata(datadir, name, batch_size, mapper=None, parallel=None):\n    """"""\n    Args:\n        mapper: a symbolic function that takes a tf.string (the raw bytes read from file) and produces a BGR image.\n            Defaults to `fbresnet_mapper(isTrain)`.\n\n    Returns:\n        A `tf.data.Dataset`. If training, the dataset is infinite.\n        The dataset contains BGR images and labels.\n    """"""\n\n    def get_imglist(dir, name):\n        """"""\n        Returns:\n            [(full filename, label)]\n        """"""\n        dir = os.path.join(dir, name)\n        meta = dataset.ILSVRCMeta()\n        imglist = meta.get_image_list(\n            name,\n            dataset.ILSVRCMeta.guess_dir_structure(dir))\n\n        def _filter(fname):\n            # png\n            return \'n02105855_2933.JPEG\' in fname\n\n        ret = []\n        for fname, label in imglist:\n            if _filter(fname):\n                logger.info(""Image {} was filtered out."".format(fname))\n                continue\n            fname = os.path.join(dir, fname)\n            ret.append((fname, label))\n        return ret\n\n    assert name in [\'train\', \'val\', \'test\']\n    assert datadir is not None\n    isTrain = name == \'train\'\n    if mapper is None:\n        mapper = fbresnet_mapper(isTrain)\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n    imglist = get_imglist(datadir, name)\n\n    N = len(imglist)\n    filenames = tf.constant([k[0] for k in imglist], name=\'filenames\')\n    labels = tf.constant([k[1] for k in imglist], dtype=tf.int32, name=\'labels\')\n\n    ds = tf.data.Dataset.from_tensor_slices((filenames, labels))\n\n    if isTrain:\n        ds = ds.shuffle(N, reshuffle_each_iteration=True).repeat()\n\n    ds = ds.apply(\n        tf.data.experimental.map_and_batch(\n            lambda fname, label: (mapper(tf.read_file(fname)), label),\n            batch_size=batch_size,\n            num_parallel_batches=parallel))\n    ds = ds.prefetch(100)\n    return ds\n\n\ndef fbresnet_mapper(isTrain):\n    """"""\n    Note: compared to fbresnet_augmentor, it\n    lacks some photometric augmentation that may have a small effect (0.1~0.2%) on accuracy.\n    """"""\n    JPEG_OPT = {\'fancy_upscaling\': True, \'dct_method\': \'INTEGER_ACCURATE\'}\n\n    def uint8_resize_bicubic(image, shape):\n        ret = tf.image.resize_bicubic([image], shape)\n        return tf.cast(tf.clip_by_value(ret, 0, 255), tf.uint8)[0]\n\n    def resize_shortest_edge(image, image_shape, size):\n        shape = tf.cast(image_shape, tf.float32)\n        w_greater = tf.greater(image_shape[0], image_shape[1])\n        shape = tf.cond(w_greater,\n                        lambda: tf.cast([shape[0] / shape[1] * size, size], tf.int32),\n                        lambda: tf.cast([size, shape[1] / shape[0] * size], tf.int32))\n\n        return uint8_resize_bicubic(image, shape)\n\n    def center_crop(image, size):\n        image_height = tf.shape(image)[0]\n        image_width = tf.shape(image)[1]\n\n        offset_height = (image_height - size) // 2\n        offset_width = (image_width - size) // 2\n        image = tf.slice(image, [offset_height, offset_width, 0], [size, size, -1])\n        return image\n\n    def lighting(image, std, eigval, eigvec):\n        v = tf.random_normal(shape=[3], stddev=std) * eigval\n        inc = tf.matmul(eigvec, tf.reshape(v, [3, 1]))\n        image = tf.cast(tf.cast(image, tf.float32) + tf.reshape(inc, [3]), image.dtype)\n        return image\n\n    def validation_mapper(byte):\n        image = tf.image.decode_jpeg(\n            tf.reshape(byte, shape=[]), 3, **JPEG_OPT)\n        image = resize_shortest_edge(image, tf.shape(image), 256)\n        image = center_crop(image, 224)\n        image = tf.reverse(image, axis=[2])  # to BGR\n        return image\n\n    def training_mapper(byte):\n        jpeg_shape = tf.image.extract_jpeg_shape(byte)  # hwc\n        bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n            jpeg_shape,\n            bounding_boxes=tf.zeros(shape=[0, 0, 4]),\n            min_object_covered=0,\n            aspect_ratio_range=[0.75, 1.33],\n            area_range=[0.08, 1.0],\n            max_attempts=10,\n            use_image_if_no_bounding_boxes=True)\n\n        is_bad = tf.reduce_sum(tf.cast(tf.equal(bbox_size, jpeg_shape), tf.int32)) >= 2\n\n        def good():\n            offset_y, offset_x, _ = tf.unstack(bbox_begin)\n            target_height, target_width, _ = tf.unstack(bbox_size)\n            crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n            image = tf.image.decode_and_crop_jpeg(\n                byte, crop_window, channels=3, **JPEG_OPT)\n            image = uint8_resize_bicubic(image, [224, 224])\n            return image\n\n        def bad():\n            image = tf.image.decode_jpeg(\n                tf.reshape(byte, shape=[]), 3, **JPEG_OPT)\n            image = resize_shortest_edge(image, jpeg_shape, 224)\n            image = center_crop(image, 224)\n            return image\n\n        image = tf.cond(is_bad, bad, good)\n        # TODO other imgproc\n        image = lighting(image, 0.1,\n                         eigval=np.array([0.2175, 0.0188, 0.0045], dtype=\'float32\') * 255.0,\n                         eigvec=np.array([[-0.5675, 0.7192, 0.4009],\n                                          [-0.5808, -0.0045, -0.8140],\n                                          [-0.5836, -0.6948, 0.4203]], dtype=\'float32\'))\n        image = tf.image.random_flip_left_right(image)\n        image = tf.reverse(image, axis=[2])  # to BGR\n        return image\n\n    return training_mapper if isTrain else validation_mapper\n\n\n""""""\n====== Model & Evaluation =======\n""""""\n\n\ndef eval_classification(model, sessinit, dataflow):\n    """"""\n    Eval a classification model on the dataset. It assumes the model inputs are\n    named ""input"" and ""label"", and contains ""wrong-top1"" and ""wrong-top5"" in the graph.\n    """"""\n    pred_config = PredictConfig(\n        model=model,\n        session_init=sessinit,\n        input_names=[\'input\', \'label\'],\n        output_names=[\'wrong-top1\', \'wrong-top5\']\n    )\n    acc1, acc5 = RatioCounter(), RatioCounter()\n\n    # This does not have a visible improvement over naive predictor,\n    # but will have an improvement if image_dtype is set to float32.\n    pred = FeedfreePredictor(pred_config, StagingInput(QueueInput(dataflow), device=\'/gpu:0\'))\n    for _ in tqdm.trange(dataflow.size()):\n        top1, top5 = pred()\n        batch_size = top1.shape[0]\n        acc1.feed(top1.sum(), batch_size)\n        acc5.feed(top5.sum(), batch_size)\n\n    print(""Top1 Error: {}"".format(acc1.ratio))\n    print(""Top5 Error: {}"".format(acc5.ratio))\n\n\nclass ImageNetModel(ModelDesc):\n    image_shape = 224\n\n    """"""\n    uint8 instead of float32 is used as input type to reduce copy overhead.\n    It might hurt the performance a liiiitle bit.\n    The pretrained models were trained with float32.\n    """"""\n    image_dtype = tf.uint8\n\n    """"""\n    Either \'NCHW\' or \'NHWC\'\n    """"""\n    data_format = \'NCHW\'\n\n    """"""\n    Whether the image is BGR or RGB. If using DataFlow, then it should be BGR.\n    """"""\n    image_bgr = True\n\n    weight_decay = 1e-4\n\n    """"""\n    To apply on normalization parameters, use \'.*/W|.*/gamma|.*/beta\'\n    """"""\n    weight_decay_pattern = \'.*/W\'\n\n    """"""\n    Scale the loss, for whatever reasons (e.g., gradient averaging, fp16 training, etc)\n    """"""\n    loss_scale = 1.\n\n    """"""\n    Label smoothing (See tf.losses.softmax_cross_entropy)\n    """"""\n    label_smoothing = 0.\n\n    """"""\n    Accumulate gradients across several steps (by default 1, which means no accumulation across steps).\n    """"""\n    accum_grad = 1\n\n    def inputs(self):\n        return [tf.TensorSpec([None, self.image_shape, self.image_shape, 3], self.image_dtype, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = self.image_preprocess(image)\n        assert self.data_format in [\'NCHW\', \'NHWC\']\n        if self.data_format == \'NCHW\':\n            image = tf.transpose(image, [0, 3, 1, 2])\n\n        logits = self.get_logits(image)\n        tf.nn.softmax(logits, name=\'prob\')\n        loss = ImageNetModel.compute_loss_and_error(\n            logits, label, label_smoothing=self.label_smoothing)\n\n        if self.weight_decay > 0:\n            wd_loss = regularize_cost(self.weight_decay_pattern,\n                                      l2_regularizer(self.weight_decay),\n                                      name=\'l2_regularize_loss\')\n            add_moving_summary(loss, wd_loss)\n            total_cost = tf.add_n([loss, wd_loss], name=\'cost\')\n        else:\n            total_cost = tf.identity(loss, name=\'cost\')\n            add_moving_summary(total_cost)\n\n        if self.loss_scale != 1.:\n            logger.info(""Scaling the total loss by {} ..."".format(self.loss_scale))\n            return total_cost * self.loss_scale\n        else:\n            return total_cost\n\n    @abstractmethod\n    def get_logits(self, image):\n        """"""\n        Args:\n            image: 4D tensor of ``self.input_shape`` in ``self.data_format``\n\n        Returns:\n            Nx#class logits\n        """"""\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.1, trainable=False)\n        tf.summary.scalar(\'learning_rate-summary\', lr)\n        opt = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n        if self.accum_grad != 1:\n            opt = AccumGradOptimizer(opt, self.accum_grad)\n        return opt\n\n    def image_preprocess(self, image):\n        with tf.name_scope(\'image_preprocess\'):\n            if image.dtype.base_dtype != tf.float32:\n                image = tf.cast(image, tf.float32)\n            mean = [0.485, 0.456, 0.406]    # rgb\n            std = [0.229, 0.224, 0.225]\n            if self.image_bgr:\n                mean = mean[::-1]\n                std = std[::-1]\n            image_mean = tf.constant(mean, dtype=tf.float32) * 255.\n            image_std = tf.constant(std, dtype=tf.float32) * 255.\n            image = (image - image_mean) / image_std\n            return image\n\n    @staticmethod\n    def compute_loss_and_error(logits, label, label_smoothing=0.):\n        if label_smoothing != 0.:\n            nclass = logits.shape[-1]\n            label = tf.one_hot(label, nclass) if label.shape.ndims == 1 else label\n\n        if label.shape.ndims == 1:\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        else:\n            loss = tf.losses.softmax_cross_entropy(\n                label, logits, label_smoothing=label_smoothing,\n                reduction=tf.losses.Reduction.NONE)\n        loss = tf.reduce_mean(loss, name=\'xentropy-loss\')\n\n        def prediction_incorrect(logits, label, topk=1, name=\'incorrect_vector\'):\n            with tf.name_scope(\'prediction_incorrect\'):\n                x = tf.logical_not(tf.nn.in_top_k(logits, label, topk))\n            return tf.cast(x, tf.float32, name=name)\n\n        wrong = prediction_incorrect(logits, label, 1, name=\'wrong-top1\')\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top1\'))\n\n        wrong = prediction_incorrect(logits, label, 5, name=\'wrong-top5\')\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top5\'))\n        return loss\n\n    def create_predict_config(self, session_init):\n        """"""\n        Returns:\n            a :class:`PredictConfig` to be used for inference.\n            The predictor will take inputs and return probabilities.\n\n        Examples:\n\n            pred = OfflinePredictor(model.create_predict_config(SmartInit(args.load)))\n            prob = pred(NCHW_image)[0]  # Nx1000 probabilities\n        """"""\n        return PredictConfig(model=self, input_names=[\'input\'], output_names=[\'prob\'], session_init=session_init)\n\n\nif __name__ == \'__main__\':\n    import argparse\n    from tensorpack.dataflow import TestDataSpeed\n    from tensorpack.tfutils import get_default_sess_config\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data\', required=True)\n    parser.add_argument(\'--batch\', type=int, default=32)\n    parser.add_argument(\'--aug\', choices=[\'train\', \'val\'], default=\'val\')\n    parser.add_argument(\'--symbolic\', action=\'store_true\')\n    args = parser.parse_args()\n\n    if not args.symbolic:\n        augs = fbresnet_augmentor(args.aug == \'train\')\n        df = get_imagenet_dataflow(\n            args.data, \'train\', args.batch, augs)\n        # For val augmentor, Should get >100 it/s (i.e. 3k im/s) here on a decent E5 server.\n        TestDataSpeed(df).start()\n    else:\n        assert args.aug == \'train\'\n        data = get_imagenet_tfdata(args.data, \'train\', args.batch)\n\n        itr = data.make_initializable_iterator()\n        dp = itr.get_next()\n        dpop = tf.group(*dp)\n        with tf.Session(config=get_default_sess_config()) as sess:\n            sess.run(itr.initializer)\n            for _ in tqdm.trange(200):\n                sess.run(dpop)\n            for _ in tqdm.trange(5000, smoothing=0.1):\n                sess.run(dpop)\n'"
examples/DoReFa-Net/resnet-dorefa.py,10,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: resnet-dorefa.py\n\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.varreplace import remap_variables\n\nfrom dorefa import get_dorefa\nfrom imagenet_utils import ImageNetModel, eval_classification, fbresnet_augmentor\n\n""""""\nThis script loads the pre-trained ResNet-18 model with (W,A,G) = (1,4,32)\nIt has 59.2% top-1 and 81.5% top-5 validation error on ILSVRC12 validation set.\n\nTo run on images:\n    ./resnet-dorefa.py --load ResNet-18-14f.npz --run a.jpg b.jpg\n\nTo eval on ILSVRC validation set:\n    ./resnet-dorefa.py --load ResNet-18-14f.npz --eval --data /path/to/ILSVRC\n""""""\n\nBITW = 1\nBITA = 4\nBITG = 32\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec([None, 224, 224, 3], tf.float32, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = image / 256.0\n\n        fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n\n        def new_get_variable(v):\n            name = v.op.name\n            # don\'t binarize first and last layer\n            if not name.endswith(\'W\') or \'conv1\' in name or \'fct\' in name:\n                return v\n            else:\n                logger.info(""Binarizing weight {}"".format(v.op.name))\n                return fw(v)\n\n        def nonlin(x):\n            return tf.clip_by_value(x, 0.0, 1.0)\n\n        def activate(x):\n            return fa(nonlin(x))\n\n        def resblock(x, channel, stride):\n            def get_stem_full(x):\n                return (LinearWrap(x)\n                        .Conv2D(\'c3x3a\', channel, 3)\n                        .BatchNorm(\'stembn\')\n                        .apply(activate)\n                        .Conv2D(\'c3x3b\', channel, 3)())\n            channel_mismatch = channel != x.get_shape().as_list()[3]\n            if stride != 1 or channel_mismatch or \'pool1\' in x.name:\n                # handling pool1 is to work around an architecture bug in our model\n                if stride != 1 or \'pool1\' in x.name:\n                    x = AvgPooling(\'pool\', x, stride, stride)\n                x = BatchNorm(\'bn\', x)\n                x = activate(x)\n                shortcut = Conv2D(\'shortcut\', x, channel, 1)\n                stem = get_stem_full(x)\n            else:\n                shortcut = x\n                x = BatchNorm(\'bn\', x)\n                x = activate(x)\n                stem = get_stem_full(x)\n            return shortcut + stem\n\n        def group(x, name, channel, nr_block, stride):\n            with tf.variable_scope(name + \'blk1\'):\n                x = resblock(x, channel, stride)\n            for i in range(2, nr_block + 1):\n                with tf.variable_scope(name + \'blk{}\'.format(i)):\n                    x = resblock(x, channel, 1)\n            return x\n\n        with remap_variables(new_get_variable), \\\n                argscope(BatchNorm, decay=0.9, epsilon=1e-4), \\\n                argscope(Conv2D, use_bias=False, nl=tf.identity):\n            logits = (LinearWrap(image)\n                      # use explicit padding here, because our private training framework has\n                      # different padding mechanisms from TensorFlow\n                      .tf.pad([[0, 0], [3, 2], [3, 2], [0, 0]])\n                      .Conv2D(\'conv1\', 64, 7, stride=2, padding=\'VALID\', use_bias=True)\n                      .tf.pad([[0, 0], [1, 1], [1, 1], [0, 0]], \'SYMMETRIC\')\n                      .MaxPooling(\'pool1\', 3, 2, padding=\'VALID\')\n                      .apply(group, \'conv2\', 64, 2, 1)\n                      .apply(group, \'conv3\', 128, 2, 2)\n                      .apply(group, \'conv4\', 256, 2, 2)\n                      .apply(group, \'conv5\', 512, 2, 2)\n                      .BatchNorm(\'lastbn\')\n                      .apply(nonlin)\n                      .GlobalAvgPooling(\'gap\')\n                      .tf.multiply(49)  # this is due to a bug in our model design\n                      .FullyConnected(\'fct\', 1000)())\n        tf.nn.softmax(logits, name=\'output\')\n        ImageNetModel.compute_loss_and_error(logits, label)\n\n\ndef get_inference_augmentor():\n    return fbresnet_augmentor(False)\n\n\ndef run_image(model, sess_init, inputs):\n    pred_config = PredictConfig(\n        model=model,\n        session_init=sess_init,\n        input_names=[\'input\'],\n        output_names=[\'output\']\n    )\n    predict_func = OfflinePredictor(pred_config)\n    meta = dataset.ILSVRCMeta()\n    words = meta.get_synset_words_1000()\n\n    transformers = get_inference_augmentor()\n    for f in inputs:\n        assert os.path.isfile(f)\n        img = cv2.imread(f).astype(\'float32\')\n        assert img is not None\n\n        img = transformers.augment(img)[np.newaxis, :, :, :]\n        o = predict_func(img)\n        prob = o[0][0]\n        ret = prob.argsort()[-10:][::-1]\n\n        names = [words[i] for i in ret]\n        print(f + "":"")\n        print(list(zip(names, prob[ret])))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'the physical ids of GPUs to use\')\n    parser.add_argument(\'--load\', help=\'load a npz pretrained model\')\n    parser.add_argument(\'--data\', help=\'ILSVRC dataset dir\')\n    parser.add_argument(\'--dorefa\',\n                        help=\'number of bits for W,A,G, separated by comma. Defaults to \\\'1,4,32\\\'\',\n                        default=\'1,4,32\')\n    parser.add_argument(\n        \'--run\', help=\'run on a list of images with the pretrained model\', nargs=\'*\')\n    parser.add_argument(\'--eval\', action=\'store_true\')\n    args = parser.parse_args()\n\n    BITW, BITA, BITG = map(int, args.dorefa.split(\',\'))\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    if args.eval:\n        ds = dataset.ILSVRC12(args.data, \'val\', shuffle=False)\n        ds = AugmentImageComponent(ds, get_inference_augmentor())\n        ds = BatchData(ds, 192, remainder=True)\n        eval_classification(Model(), SmartInit(args.load), ds)\n    elif args.run:\n        assert args.load.endswith(\'.npz\')\n        run_image(Model(), SmartInit(args.load), args.run)\n'"
examples/DoReFa-Net/svhn-digit-dorefa.py,14,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: svhn-digit-dorefa.py\n# Author: Yuxin Wu\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.summary import add_moving_summary, add_param_summary\nfrom tensorpack.tfutils.varreplace import remap_variables\n\nfrom dorefa import get_dorefa\n\n""""""\nThis is a tensorpack script for the SVHN results in paper:\nDoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\nhttp://arxiv.org/abs/1606.06160\n\nThe original experiements are performed on a proprietary framework.\nThis is our attempt to reproduce it on tensorpack.\n\nAccuracy:\n    With (W,A,G)=(1,1,4), can reach 3.1~3.2% error after 150 epochs.\n    With (W,A,G)=(1,2,4), error is 3.0~3.1%.\n    With (W,A,G)=(32,32,32), error is about 2.3%.\n\nSpeed:\n    With quantization, 60 batch/s on 1 1080Ti. (4721 batch / epoch)\n\nTo Run:\n    ./svhn-digit-dorefa.py --dorefa 1,2,4\n""""""\n\nBITW = 1\nBITA = 2\nBITG = 4\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec([None, 40, 40, 3], tf.float32, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n\n        # monkey-patch tf.get_variable to apply fw\n        def binarize_weight(v):\n            name = v.op.name\n            # don\'t binarize first and last layer\n            if not name.endswith(\'W\') or \'conv0\' in name or \'fc\' in name:\n                return v\n            else:\n                logger.info(""Binarizing weight {}"".format(v.op.name))\n                return fw(v)\n\n        def nonlin(x):\n            if BITA == 32:\n                return tf.nn.relu(x)\n            return tf.clip_by_value(x, 0.0, 1.0)\n\n        def activate(x):\n            return fa(nonlin(x))\n\n        image = image / 256.0\n\n        with remap_variables(binarize_weight), \\\n                argscope(BatchNorm, momentum=0.9, epsilon=1e-4), \\\n                argscope(Conv2D, use_bias=False):\n            logits = (LinearWrap(image)\n                      .Conv2D(\'conv0\', 48, 5, padding=\'VALID\', use_bias=True)\n                      .MaxPooling(\'pool0\', 2, padding=\'SAME\')\n                      .apply(activate)\n                      # 18\n                      .Conv2D(\'conv1\', 64, 3, padding=\'SAME\')\n                      .apply(fg)\n                      .BatchNorm(\'bn1\').apply(activate)\n\n                      .Conv2D(\'conv2\', 64, 3, padding=\'SAME\')\n                      .apply(fg)\n                      .BatchNorm(\'bn2\')\n                      .MaxPooling(\'pool1\', 2, padding=\'SAME\')\n                      .apply(activate)\n                      # 9\n                      .Conv2D(\'conv3\', 128, 3, padding=\'VALID\')\n                      .apply(fg)\n                      .BatchNorm(\'bn3\').apply(activate)\n                      # 7\n\n                      .Conv2D(\'conv4\', 128, 3, padding=\'SAME\')\n                      .apply(fg)\n                      .BatchNorm(\'bn4\').apply(activate)\n\n                      .Conv2D(\'conv5\', 128, 3, padding=\'VALID\')\n                      .apply(fg)\n                      .BatchNorm(\'bn5\').apply(activate)\n                      # 5\n                      .Dropout(rate=0.5 if self.training else 0.0)\n                      .Conv2D(\'conv6\', 512, 5, padding=\'VALID\')\n                      .apply(fg).BatchNorm(\'bn6\')\n                      .apply(nonlin)\n                      .FullyConnected(\'fc1\', 10)())\n        tf.nn.softmax(logits, name=\'output\')\n\n        # compute the number of failed samples\n        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name=\'wrong_tensor\')\n        # monitor training error\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train_error\'))\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')\n        # weight decay on all W of fc layers\n        wd_cost = regularize_cost(\'fc.*/W\', l2_regularizer(1e-7))\n\n        add_param_summary((\'.*/W\', [\'histogram\', \'rms\']))\n        total_cost = tf.add_n([cost, wd_cost], name=\'cost\')\n        add_moving_summary(cost, wd_cost, total_cost)\n        return total_cost\n\n    def optimizer(self):\n        lr = tf.train.exponential_decay(\n            learning_rate=1e-3,\n            global_step=get_global_step_var(),\n            decay_steps=4721 * 100,\n            decay_rate=0.5, staircase=True, name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', lr)\n        return tf.train.AdamOptimizer(lr, epsilon=1e-5)\n\n\ndef get_config():\n    logger.set_logger_dir(os.path.join(\'train_log\', \'svhn-dorefa-{}\'.format(args.dorefa)))\n\n    # prepare dataset\n    d1 = dataset.SVHNDigit(\'train\')\n    d2 = dataset.SVHNDigit(\'extra\')\n    data_train = RandomMixData([d1, d2])\n    data_test = dataset.SVHNDigit(\'test\')\n\n    augmentors = [\n        imgaug.Resize((40, 40)),\n        imgaug.Brightness(30),\n        imgaug.Contrast((0.5, 1.5)),\n    ]\n    data_train = AugmentImageComponent(data_train, augmentors)\n    data_train = BatchData(data_train, 128)\n    data_train = MultiProcessRunnerZMQ(data_train, 5)\n\n    augmentors = [imgaug.Resize((40, 40))]\n    data_test = AugmentImageComponent(data_test, augmentors)\n    data_test = BatchData(data_test, 128, remainder=True)\n\n    return TrainConfig(\n        data=QueueInput(data_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(data_test,\n                            [ScalarStats(\'cost\'), ClassificationError(\'wrong_tensor\')])\n        ],\n        model=Model(),\n        max_epoch=200,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dorefa\',\n                        help=\'number of bits for W,A,G, separated by comma. Defaults to \\\'1,2,4\\\'\',\n                        default=\'1,2,4\')\n    args = parser.parse_args()\n\n    BITW, BITA, BITG = map(int, args.dorefa.split(\',\'))\n    config = get_config()\n    launch_train_with_config(config, SimpleTrainer())\n'"
examples/DynamicFilterNetwork/steering-filter.py,27,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: steering-filter.py\n\nimport argparse\nimport multiprocessing\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom scipy.signal import convolve2d\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.gpu import change_gpu\nfrom tensorpack.utils.argtools import shape2d, shape4d\nfrom tensorpack.utils.viz import *\n\nBATCH = 32\nSHAPE = 64\n\n\n@layer_register()\ndef DynamicConvFilter(inputs, filters, out_channel,\n                      kernel_shape,\n                      stride=1,\n                      padding=\'SAME\'):\n    """""" see ""Dynamic Filter Networks"" (NIPS 2016)\n        by Bert De Brabandere*, Xu Jia*, Tinne Tuytelaars and Luc Van Gool\n\n    Remarks:\n        This is the convolution version of a dynamic filter.\n\n    Args:\n        inputs : unfiltered input [b, h, w, 1] only grayscale images.\n        filters : learned filters of [b, k, k, 1] (dynamically generated by the network).\n        out_channel (int): number of output channel.\n        kernel_shape: (h, w) tuple or a int.\n        stride: (h, w) tuple or a int.\n        padding (str): \'valid\' or \'same\'. Case insensitive.\n\n    Returns\n        tf.Tensor named ``output``.\n    """"""\n\n    # tf.unstack only works with known batch_size :-(\n    batch_size, h, w, in_channel = inputs.get_shape().as_list()\n    stride = shape4d(stride)\n\n    inputs = tf.unstack(inputs)\n    filters = tf.reshape(filters, [batch_size] + shape2d(kernel_shape) + [in_channel, out_channel])\n    filters = tf.unstack(filters)\n\n    # this is ok as TF uses the cuda stream context\n    rsl = [tf.nn.conv2d(tf.reshape(d, [1, h, w, in_channel]),\n                        tf.reshape(k, [kernel_shape, kernel_shape, in_channel, out_channel]),\n                        stride, padding=""SAME"") for d, k in zip(inputs, filters)]\n    rsl = tf.concat(rsl, axis=0, name=\'output\')\n    return rsl\n\n\nclass OnlineTensorboardExport(Callback):\n    """"""Show learned filters for specific thetas in TensorBoard.\n    """"""\n    def __init__(self):\n        # generate 32 filters (8 different, 4 times repeated)\n        self.theta = np.array([i for _ in range(4) for i in range(8)]) / 8. * 2 * np.pi\n        self.filters = np.array([ThetaImages.get_filter(t) for t in self.theta])\n        self.cc = 0\n\n    def _setup_graph(self):\n        self.pred = self.trainer.get_predictor([\'theta\'], [\'pred_filter\'])\n\n    def _trigger_epoch(self):\n        def n(x):\n            x -= x.min()\n            x /= x.max()\n            return x\n\n        o = self.pred(self.theta)\n\n        gt_filters = np.concatenate([self.filters[i, :, :] for i in range(8)], axis=0)\n        pred_filters = np.concatenate([o[0][i, :, :, 0] for i in range(8)], axis=0)\n\n        canvas = np.concatenate([n(gt_filters), n(pred_filters)], axis=1)\n        l = canvas.shape[0] // 2\n        canvas = np.concatenate([canvas[:l], canvas[l:]], axis=1)\n        canvas = cv2.resize(canvas[..., None] * 255, (0, 0), fx=10, fy=10)\n\n        self.trainer.monitors.put_image(\'filter_export\', canvas)\n        # # you might also want to write these images to disk\n        # cv2.imwrite(""export/out%04i.jpg"" % self.cc, canvas)\n        # self.cc += 1\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((BATCH, ), tf.float32, \'theta\'),\n                tf.TensorSpec((BATCH, SHAPE, SHAPE), tf.float32, \'image\'),\n                tf.TensorSpec((BATCH, SHAPE, SHAPE), tf.float32, \'gt_image\'),\n                tf.TensorSpec((BATCH, 9, 9), tf.float32, \'gt_filter\')]\n\n    def _parameter_net(self, theta, kernel_shape=9):\n        """"""Estimate filters for convolution layers\n\n        Args:\n            theta: angle of filter\n            kernel_shape: size of each filter\n\n        Returns:\n            learned filter as [B, k, k, 1]\n        """"""\n        with argscope(FullyConnected, nl=tf.nn.leaky_relu):\n            net = FullyConnected(\'fc1\', theta, 64)\n            net = FullyConnected(\'fc2\', net, 128)\n\n        pred_filter = FullyConnected(\'fc3\', net, kernel_shape ** 2, nl=tf.identity)\n        pred_filter = tf.reshape(pred_filter, [BATCH, kernel_shape, kernel_shape, 1], name=""pred_filter"")\n        logger.info(\'Parameter net output: {}\'.format(pred_filter.get_shape().as_list()))\n        return pred_filter\n\n    def build_graph(self, theta, image, gt_image, gt_filter):\n        kernel_size = 9\n\n        theta = tf.reshape(theta, [BATCH, 1, 1, 1]) - np.pi\n        image = tf.reshape(image, [BATCH, SHAPE, SHAPE, 1])\n        gt_image = tf.reshape(gt_image, [BATCH, SHAPE, SHAPE, 1])\n\n        pred_filter = self._parameter_net(theta)\n        pred_image = DynamicConvFilter(\'dfn\', image, pred_filter, 1, kernel_size)\n\n        with tf.name_scope(\'visualization\'):\n            pred_filter = tf.reshape(pred_filter, [BATCH, kernel_size, kernel_size, 1])\n            gt_filter = tf.reshape(gt_filter, [BATCH, kernel_size, kernel_size, 1])\n\n            filters = tf.concat([pred_filter, gt_filter], axis=2, name=""filters"")\n            images = tf.concat([pred_image, gt_image], axis=2, name=""images"")\n        tf.summary.image(\'pred_gt_filters\', filters, max_outputs=20)\n        tf.summary.image(\'pred_gt_images\', images, max_outputs=20)\n\n        cost = tf.reduce_mean(tf.squared_difference(pred_image, gt_image), name=""cost"")\n        summary.add_moving_summary(cost)\n        return cost\n\n    def optimizer(self):\n        return tf.train.AdamOptimizer(1e-3)\n\n\nclass ThetaImages(ProxyDataFlow, RNGDataFlow):\n    @staticmethod\n    def get_filter(theta, sigma=1., filter_size=9):\n        x = np.arange(-filter_size // 2 + 1, filter_size // 2 + 1)\n        g = np.array([np.exp(-(x**2) / (2 * sigma**2))])\n        gp = np.array([-(x / sigma) * np.exp(-(x**2) / (2 * sigma**2))])\n\n        gt_filter = np.matmul(g.T, gp)\n        gt_filter = np.cos(theta) * gt_filter + np.sin(theta) * gt_filter.T\n\n        return gt_filter\n\n    @staticmethod\n    def filter_with_theta(image, theta, sigma=1., filter_size=9):\n        """"""Implements a steerable Gaussian filter.\n\n        This function can be used to evaluate the first\n        directional derivative of an image, using the\n        method outlined in\n\n            W. T. Freeman and E. H. Adelson, ""The Design\n            and Use of Steerable Filters"", IEEE PAMI, 1991.\n\n        It evaluates the directional derivative of the input\n        image I, oriented at THETA degrees with respect to the\n        image rows. The standard deviation of the Gaussian kernel\n        is given by SIGMA (assumed to be equal to unity by default).\n\n        Args:\n            image: any input image (only one channel)\n            theta: orientation of filter [0, 2 * pi]\n            sigma (float, optional): standard derivation of Gaussian\n            filter_size (int, optional): filter support\n\n        Returns:\n            filtered image and the filter\n        """"""\n        x = np.arange(-filter_size // 2 + 1, filter_size // 2 + 1)\n        # 1D Gaussian\n        g = np.array([np.exp(-(x**2) / (2 * sigma**2))])\n        # first-derivative of 1D Gaussian\n        gp = np.array([-(x / sigma) * np.exp(-(x**2) / (2 * sigma**2))])\n\n        ix = convolve2d(image, -gp, mode=\'same\', boundary=\'fill\', fillvalue=0)\n        ix = convolve2d(ix, g.T, mode=\'same\', boundary=\'fill\', fillvalue=0)\n\n        iy = convolve2d(image, g, mode=\'same\', boundary=\'fill\', fillvalue=0)\n        iy = convolve2d(iy, -gp.T, mode=\'same\', boundary=\'fill\', fillvalue=0)\n\n        output = np.cos(theta) * ix + np.sin(theta) * iy\n\n        # np.cos(theta) * np.matmul(g.T, gp) + np.sin(theta) * np.matmul(gp.T, g)\n        gt_filter = np.matmul(g.T, gp)\n        gt_filter = np.cos(theta) * gt_filter + np.sin(theta) * gt_filter.T\n\n        return output, gt_filter\n\n    def __init__(self, ds):\n        ProxyDataFlow.__init__(self, ds)\n\n    def reset_state(self):\n        ProxyDataFlow.reset_state(self)\n        RNGDataFlow.reset_state(self)\n\n    def __iter__(self):\n        for image, label in self.ds:\n            theta = self.rng.uniform(0, 2 * np.pi)\n            filtered_image, gt_filter = ThetaImages.filter_with_theta(image, theta)\n            yield [theta, image, filtered_image, gt_filter]\n\n\ndef get_data():\n    # probably not the best dataset\n    ds = dataset.BSDS500(\'train\', shuffle=True)\n    ds = AugmentImageComponent(ds, [imgaug.Grayscale(keepdims=False),\n                                    imgaug.Resize((SHAPE, SHAPE))])\n    ds = ThetaImages(ds)\n    ds = RepeatedData(ds, 50)  # just pretend this dataset is bigger\n    # this pre-computation is pretty heavy\n    ds = MultiProcessRunnerZMQ(ds, min(20, multiprocessing.cpu_count()))\n    ds = BatchData(ds, BATCH)\n    return ds\n\n\ndef get_config():\n    logger.auto_set_dir()\n    dataset_train = get_data()\n\n    return TrainConfig(\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(),\n            OnlineTensorboardExport()\n        ],\n        model=Model(),\n        steps_per_epoch=len(dataset_train),\n        max_epoch=50,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\', required=True)\n    parser.add_argument(\'--load\', help=\'load model\')\n    args = parser.parse_args()\n\n    with change_gpu(args.gpu):\n        NGPU = len(args.gpu.split(\',\'))\n        config = get_config()\n        config.session_init = SmartInit(args.load)\n        launch_train_with_config(config, SyncMultiGPUTrainer(NGPU))\n'"
examples/FasterRCNN/common.py,0,"b'# -*- coding: utf-8 -*-\n# File: common.py\n\nimport numpy as np\nimport cv2\n\nfrom tensorpack.dataflow import RNGDataFlow\nfrom tensorpack.dataflow.imgaug import ImageAugmentor, ResizeTransform\n\n\nclass DataFromListOfDict(RNGDataFlow):\n    def __init__(self, lst, keys, shuffle=False):\n        self._lst = lst\n        self._keys = keys\n        self._shuffle = shuffle\n        self._size = len(lst)\n\n    def __len__(self):\n        return self._size\n\n    def __iter__(self):\n        if self._shuffle:\n            self.rng.shuffle(self._lst)\n        for dic in self._lst:\n            dp = [dic[k] for k in self._keys]\n            yield dp\n\n\nclass CustomResize(ImageAugmentor):\n    """"""\n    Try resizing the shortest edge to a certain number\n    while avoiding the longest edge to exceed max_size.\n    """"""\n\n    def __init__(self, short_edge_length, max_size, interp=cv2.INTER_LINEAR):\n        """"""\n        Args:\n            short_edge_length ([int, int]): a [min, max] interval from which to sample the\n                shortest edge length.\n            max_size (int): maximum allowed longest edge length.\n        """"""\n        super(CustomResize, self).__init__()\n        if isinstance(short_edge_length, int):\n            short_edge_length = (short_edge_length, short_edge_length)\n        self._init(locals())\n\n    def get_transform(self, img):\n        h, w = img.shape[:2]\n        size = self.rng.randint(\n            self.short_edge_length[0], self.short_edge_length[1] + 1)\n        scale = size * 1.0 / min(h, w)\n        if h < w:\n            newh, neww = size, scale * w\n        else:\n            newh, neww = scale * h, size\n        if max(newh, neww) > self.max_size:\n            scale = self.max_size * 1.0 / max(newh, neww)\n            newh = newh * scale\n            neww = neww * scale\n        neww = int(neww + 0.5)\n        newh = int(newh + 0.5)\n        return ResizeTransform(h, w, newh, neww, self.interp)\n\n\ndef box_to_point4(boxes):\n    """"""\n    Convert boxes to its corner points.\n\n    Args:\n        boxes: nx4\n\n    Returns:\n        (nx4)x2\n    """"""\n    b = boxes[:, [0, 1, 2, 3, 0, 3, 2, 1]]\n    b = b.reshape((-1, 2))\n    return b\n\n\ndef point4_to_box(points):\n    """"""\n    Args:\n        points: (nx4)x2\n    Returns:\n        nx4 boxes (x1y1x2y2)\n    """"""\n    p = points.reshape((-1, 4, 2))\n    minxy = p.min(axis=1)   # nx2\n    maxxy = p.max(axis=1)   # nx2\n    return np.concatenate((minxy, maxxy), axis=1)\n\n\ndef polygons_to_mask(polys, height, width):\n    """"""\n    Convert polygons to binary masks.\n\n    Args:\n        polys: a list of nx2 float array. Each array contains many (x, y) coordinates.\n\n    Returns:\n        a binary matrix of (height, width)\n    """"""\n    polys = [p.flatten().tolist() for p in polys]\n    assert len(polys) > 0, ""Polygons are empty!""\n\n    import pycocotools.mask as cocomask\n    rles = cocomask.frPyObjects(polys, height, width)\n    rle = cocomask.merge(rles)\n    return cocomask.decode(rle)\n\n\ndef clip_boxes(boxes, shape):\n    """"""\n    Args:\n        boxes: (...)x4, float\n        shape: h, w\n    """"""\n    orig_shape = boxes.shape\n    boxes = boxes.reshape([-1, 4])\n    h, w = shape\n    boxes[:, [0, 1]] = np.maximum(boxes[:, [0, 1]], 0)\n    boxes[:, 2] = np.minimum(boxes[:, 2], w)\n    boxes[:, 3] = np.minimum(boxes[:, 3], h)\n    return boxes.reshape(orig_shape)\n\n\ndef filter_boxes_inside_shape(boxes, shape):\n    """"""\n    Args:\n        boxes: (nx4), float\n        shape: (h, w)\n\n    Returns:\n        indices: (k, )\n        selection: (kx4)\n    """"""\n    assert boxes.ndim == 2, boxes.shape\n    assert len(shape) == 2, shape\n    h, w = shape\n    indices = np.where(\n        (boxes[:, 0] >= 0) &\n        (boxes[:, 1] >= 0) &\n        (boxes[:, 2] <= w) &\n        (boxes[:, 3] <= h))[0]\n    return indices, boxes[indices, :]\n\n\ntry:\n    import pycocotools.mask as cocomask\n\n    # Much faster than utils/np_box_ops\n    def np_iou(A, B):\n        def to_xywh(box):\n            box = box.copy()\n            box[:, 2] -= box[:, 0]\n            box[:, 3] -= box[:, 1]\n            return box\n\n        ret = cocomask.iou(\n            to_xywh(A), to_xywh(B),\n            np.zeros((len(B),), dtype=np.bool))\n        # can accelerate even more, if using float32\n        return ret.astype(\'float32\')\n\nexcept ImportError:\n    from utils.np_box_ops import iou as np_iou  # noqa\n'"
examples/FasterRCNN/config.py,0,"b'# -*- coding: utf-8 -*-\n# File: config.py\n\nimport numpy as np\nimport os\nimport pprint\nimport six\n\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.gpu import get_num_gpu\n\n__all__ = [\'config\', \'finalize_configs\']\n\n\nclass AttrDict():\n\n    _freezed = False\n    """""" Avoid accidental creation of new hierarchies. """"""\n\n    def __getattr__(self, name):\n        if self._freezed:\n            raise AttributeError(name)\n        if name.startswith(\'_\'):\n            # Do not mess with internals. Otherwise copy/pickle will fail\n            raise AttributeError(name)\n        ret = AttrDict()\n        setattr(self, name, ret)\n        return ret\n\n    def __setattr__(self, name, value):\n        if self._freezed and name not in self.__dict__:\n            raise AttributeError(\n                ""Config was freezed! Unknown config: {}"".format(name))\n        super().__setattr__(name, value)\n\n    def __str__(self):\n        return pprint.pformat(self.to_dict(), indent=1, width=100, compact=True)\n\n    __repr__ = __str__\n\n    def to_dict(self):\n        """"""Convert to a nested dict. """"""\n        return {k: v.to_dict() if isinstance(v, AttrDict) else v\n                for k, v in self.__dict__.items() if not k.startswith(\'_\')}\n\n    def from_dict(self, d):\n        self.freeze(False)\n        for k, v in d.items():\n            self_v = getattr(self, k)\n            if isinstance(self_v, AttrDict):\n                self_v.from_dict(v)\n            else:\n                setattr(self, k, v)\n\n    def update_args(self, args):\n        """"""Update from command line args. """"""\n        for cfg in args:\n            keys, v = cfg.split(\'=\', maxsplit=1)\n            keylist = keys.split(\'.\')\n\n            dic = self\n            for i, k in enumerate(keylist[:-1]):\n                assert k in dir(dic), ""Unknown config key: {}"".format(keys)\n                dic = getattr(dic, k)\n            key = keylist[-1]\n\n            oldv = getattr(dic, key)\n            if not isinstance(oldv, str):\n                v = eval(v)\n            setattr(dic, key, v)\n\n    def freeze(self, freezed=True):\n        self._freezed = freezed\n        for v in self.__dict__.values():\n            if isinstance(v, AttrDict):\n                v.freeze(freezed)\n\n    # avoid silent bugs\n    def __eq__(self, _):\n        raise NotImplementedError()\n\n    def __ne__(self, _):\n        raise NotImplementedError()\n\n\nconfig = AttrDict()\n_C = config     # short alias to avoid coding\n\n# mode flags ---------------------\n_C.TRAINER = \'replicated\'  # options: \'horovod\', \'replicated\'\n_C.MODE_MASK = True        # Faster R-CNN or Mask R-CNN\n_C.MODE_FPN = True\n\n# dataset -----------------------\n_C.DATA.BASEDIR = \'/path/to/your/DATA/DIR\'\n# All available dataset names are defined in `dataset/coco.py:register_coco`.\n# All TRAIN dataset will be concatenated for training.\n_C.DATA.TRAIN = (\'coco_train2017\',)   # i.e. trainval35k\n# Each VAL dataset will be evaluated separately (instead of concatenated)\n_C.DATA.VAL = (\'coco_val2017\',)  # AKA minival2014\n\n# These two configs will be populated later inside `finalize_configs`.\n_C.DATA.NUM_CATEGORY = -1  # without the background class (e.g., 80 for COCO)\n_C.DATA.CLASS_NAMES = []  # NUM_CLASS (NUM_CATEGORY+1) strings, the first is ""BG"".\n\n# whether the coordinates in your registered dataset are\n# absolute pixel values in range [0, W or H] or relative values in [0, 1]\n_C.DATA.ABSOLUTE_COORD = True\n# Filter Negative Samples from dataset\n_C.DATA.FILTER_EMPTY_ANNOTATIONS = True\n# Number of data loading workers.\n# In case of horovod training, this is the number of workers per-GPU (so you may want to use a smaller number).\n# Set to 0 to disable parallel data loading\n_C.DATA.NUM_WORKERS = 10\n\n# backbone ----------------------\n_C.BACKBONE.WEIGHTS = \'\'\n# To train from scratch, set it to empty, and set FREEZE_AT to 0\n# To train from ImageNet pre-trained models, use the one that matches your\n#   architecture from http://models.tensorpack.com under the \'FasterRCNN\' section.\n# To train from an existing COCO model, use the path to that file, and change\n#   the other configurations according to that model.\n\n_C.BACKBONE.RESNET_NUM_BLOCKS = [3, 4, 6, 3]     # for resnet50\n# RESNET_NUM_BLOCKS = [3, 4, 23, 3]    # for resnet101\n_C.BACKBONE.FREEZE_AFFINE = False   # do not train affine parameters inside norm layers\n_C.BACKBONE.NORM = \'FreezeBN\'  # options: FreezeBN, SyncBN, GN, None\n_C.BACKBONE.FREEZE_AT = 2  # options: 0, 1, 2. How many stages in backbone to freeze (not training)\n\n# Use a base model with TF-preferred padding mode,\n# which may pad more pixels on right/bottom than top/left.\n# See https://github.com/tensorflow/tensorflow/issues/18213\n# In tensorpack model zoo, ResNet models with TF_PAD_MODE=False are marked with ""-AlignPadding"".\n# All other models under `ResNet/` in the model zoo are using TF_PAD_MODE=True.\n# Using either one should probably give the same performance.\n# We use the ""AlignPadding"" one just to be consistent with caffe2.\n_C.BACKBONE.TF_PAD_MODE = False\n_C.BACKBONE.STRIDE_1X1 = False  # True for MSRA models\n\n# schedule -----------------------\n_C.TRAIN.NUM_GPUS = None         # by default, will be set from code\n_C.TRAIN.WEIGHT_DECAY = 1e-4\n_C.TRAIN.BASE_LR = 1e-2  # defined for total batch size=8. Otherwise it will be adjusted automatically\n_C.TRAIN.WARMUP = 1000   # in terms of iterations. This is not affected by #GPUs\n_C.TRAIN.WARMUP_INIT_LR = 1e-5  # defined for total batch size=8. Otherwise it will be adjusted automatically\n_C.TRAIN.STEPS_PER_EPOCH = 500\n_C.TRAIN.STARTING_EPOCH = 1  # the first epoch to start with, useful to continue a training\n\n# LR_SCHEDULE means equivalent steps when the total batch size is 8.\n# It can be either a string like ""3x"" that refers to standard convention, or a list of int.\n# LR_SCHEDULE=3x is the same as LR_SCHEDULE=[420000, 500000, 540000], which\n# means to decrease LR at steps 420k and 500k and stop training at 540k.\n# When the total bs!=8, the actual iterations to decrease learning rate, and\n# the base learning rate are computed from BASE_LR and LR_SCHEDULE.\n# Therefore, there is *no need* to modify the config if you only change the number of GPUs.\n_C.TRAIN.LR_SCHEDULE = ""1x""      # ""1x"" schedule in detectron\n_C.TRAIN.EVAL_PERIOD = 50  # period (epochs) to run evaluation\n_C.TRAIN.CHECKPOINT_PERIOD = 20  # period (epochs) to save model\n\n# preprocessing --------------------\n# Alternative old (worse & faster) setting: 600\n_C.PREPROC.TRAIN_SHORT_EDGE_SIZE = [800, 800]  # [min, max] to sample from\n_C.PREPROC.TEST_SHORT_EDGE_SIZE = 800\n_C.PREPROC.MAX_SIZE = 1333\n# mean and std in RGB order.\n# Un-scaled version: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n_C.PREPROC.PIXEL_MEAN = [123.675, 116.28, 103.53]\n_C.PREPROC.PIXEL_STD = [58.395, 57.12, 57.375]\n\n# anchors -------------------------\n_C.RPN.ANCHOR_STRIDE = 16\n_C.RPN.ANCHOR_SIZES = (32, 64, 128, 256, 512)   # sqrtarea of the anchor box\n_C.RPN.ANCHOR_RATIOS = (0.5, 1., 2.)\n_C.RPN.POSITIVE_ANCHOR_THRESH = 0.7\n_C.RPN.NEGATIVE_ANCHOR_THRESH = 0.3\n\n# rpn training -------------------------\n_C.RPN.FG_RATIO = 0.5  # fg ratio among selected RPN anchors\n_C.RPN.BATCH_PER_IM = 256  # total (across FPN levels) number of anchors that are marked valid\n_C.RPN.MIN_SIZE = 0\n_C.RPN.PROPOSAL_NMS_THRESH = 0.7\n# Anchors which overlap with a crowd box (IOA larger than threshold) will be ignored.\n# Setting this to a value larger than 1.0 will disable the feature.\n# It is disabled by default because Detectron does not do this.\n_C.RPN.CROWD_OVERLAP_THRESH = 9.99\n_C.RPN.HEAD_DIM = 1024      # used in C4 only\n\n# RPN proposal selection -------------------------------\n# for C4\n_C.RPN.TRAIN_PRE_NMS_TOPK = 12000\n_C.RPN.TRAIN_POST_NMS_TOPK = 2000\n_C.RPN.TEST_PRE_NMS_TOPK = 6000\n_C.RPN.TEST_POST_NMS_TOPK = 1000   # if you encounter OOM in inference, set this to a smaller number\n# for FPN, #proposals per-level and #proposals after merging are (for now) the same\n# if FPN.PROPOSAL_MODE = \'Joint\', these options have no effect\n_C.RPN.TRAIN_PER_LEVEL_NMS_TOPK = 2000\n_C.RPN.TEST_PER_LEVEL_NMS_TOPK = 1000\n\n# fastrcnn training ---------------------\n_C.FRCNN.BATCH_PER_IM = 512\n_C.FRCNN.BBOX_REG_WEIGHTS = [10., 10., 5., 5.]  # Slightly better setting: 20, 20, 10, 10\n_C.FRCNN.FG_THRESH = 0.5\n_C.FRCNN.FG_RATIO = 0.25  # fg ratio in a ROI batch\n\n# FPN -------------------------\n_C.FPN.ANCHOR_STRIDES = (4, 8, 16, 32, 64)  # strides for each FPN level. Must be the same length as ANCHOR_SIZES\n_C.FPN.PROPOSAL_MODE = \'Level\'  # \'Level\', \'Joint\'\n_C.FPN.NUM_CHANNEL = 256\n_C.FPN.NORM = \'None\'  # \'None\', \'GN\'\n# The head option is only used in FPN. For C4 models, the head is C5\n_C.FPN.FRCNN_HEAD_FUNC = \'fastrcnn_2fc_head\'\n# choices: fastrcnn_2fc_head, fastrcnn_4conv1fc_{,gn_}head\n_C.FPN.FRCNN_CONV_HEAD_DIM = 256\n_C.FPN.FRCNN_FC_HEAD_DIM = 1024\n_C.FPN.MRCNN_HEAD_FUNC = \'maskrcnn_up4conv_head\'   # choices: maskrcnn_up4conv_{,gn_}head\n\n# Mask R-CNN\n_C.MRCNN.HEAD_DIM = 256\n_C.MRCNN.ACCURATE_PASTE = True  # slightly more aligned results, but very slow on numpy\n\n# Cascade R-CNN, only available in FPN mode\n_C.FPN.CASCADE = False\n_C.CASCADE.IOUS = [0.5, 0.6, 0.7]\n_C.CASCADE.BBOX_REG_WEIGHTS = [[10., 10., 5., 5.], [20., 20., 10., 10.], [30., 30., 15., 15.]]\n\n# testing -----------------------\n_C.TEST.FRCNN_NMS_THRESH = 0.5\n\n# Smaller threshold value gives significantly better mAP. But we use 0.05 for consistency with Detectron.\n# mAP with 1e-4 threshold can be found at https://github.com/tensorpack/tensorpack/commit/26321ae58120af2568bdbf2269f32aa708d425a8#diff-61085c48abee915b584027e1085e1043  # noqa\n_C.TEST.RESULT_SCORE_THRESH = 0.05\n_C.TEST.RESULT_SCORE_THRESH_VIS = 0.5   # only visualize confident results\n_C.TEST.RESULTS_PER_IM = 100\n\n_C.freeze()  # avoid typo / wrong config keys\n\n\ndef finalize_configs(is_training):\n    """"""\n    Run some sanity checks, and populate some configs from others\n    """"""\n    _C.freeze(False)  # populate new keys now\n    if isinstance(_C.DATA.VAL, six.string_types):  # support single string (the typical case) as well\n        _C.DATA.VAL = (_C.DATA.VAL, )\n    if isinstance(_C.DATA.TRAIN, six.string_types):  # support single string\n        _C.DATA.TRAIN = (_C.DATA.TRAIN, )\n\n    # finalize dataset definitions ...\n    from dataset import DatasetRegistry\n    datasets = list(_C.DATA.TRAIN) + list(_C.DATA.VAL)\n    _C.DATA.CLASS_NAMES = DatasetRegistry.get_metadata(datasets[0], ""class_names"")\n    _C.DATA.NUM_CATEGORY = len(_C.DATA.CLASS_NAMES) - 1\n\n    assert _C.BACKBONE.NORM in [\'FreezeBN\', \'SyncBN\', \'GN\', \'None\'], _C.BACKBONE.NORM\n    if _C.BACKBONE.NORM != \'FreezeBN\':\n        assert not _C.BACKBONE.FREEZE_AFFINE\n    assert _C.BACKBONE.FREEZE_AT in [0, 1, 2]\n\n    _C.RPN.NUM_ANCHOR = len(_C.RPN.ANCHOR_SIZES) * len(_C.RPN.ANCHOR_RATIOS)\n    assert len(_C.FPN.ANCHOR_STRIDES) == len(_C.RPN.ANCHOR_SIZES)\n    # image size into the backbone has to be multiple of this number\n    _C.FPN.RESOLUTION_REQUIREMENT = _C.FPN.ANCHOR_STRIDES[3]  # [3] because we build FPN with features r2,r3,r4,r5\n\n    if _C.MODE_FPN:\n        size_mult = _C.FPN.RESOLUTION_REQUIREMENT * 1.\n        _C.PREPROC.MAX_SIZE = np.ceil(_C.PREPROC.MAX_SIZE / size_mult) * size_mult\n        assert _C.FPN.PROPOSAL_MODE in [\'Level\', \'Joint\']\n        assert _C.FPN.FRCNN_HEAD_FUNC.endswith(\'_head\')\n        assert _C.FPN.MRCNN_HEAD_FUNC.endswith(\'_head\')\n        assert _C.FPN.NORM in [\'None\', \'GN\']\n\n        if _C.FPN.CASCADE:\n            # the first threshold is the proposal sampling threshold\n            assert _C.CASCADE.IOUS[0] == _C.FRCNN.FG_THRESH\n            assert len(_C.CASCADE.BBOX_REG_WEIGHTS) == len(_C.CASCADE.IOUS)\n\n    if is_training:\n        train_scales = _C.PREPROC.TRAIN_SHORT_EDGE_SIZE\n        if isinstance(train_scales, (list, tuple)) and train_scales[1] - train_scales[0] > 100:\n            # don\'t autotune if augmentation is on\n            os.environ[\'TF_CUDNN_USE_AUTOTUNE\'] = \'0\'\n        os.environ[\'TF_AUTOTUNE_THRESHOLD\'] = \'1\'\n        assert _C.TRAINER in [\'horovod\', \'replicated\'], _C.TRAINER\n\n        lr = _C.TRAIN.LR_SCHEDULE\n        if isinstance(lr, six.string_types):\n            if lr.endswith(""x""):\n                LR_SCHEDULE_KITER = {\n                    ""{}x"".format(k):\n                    [180 * k - 120, 180 * k - 40, 180 * k]\n                    for k in range(2, 10)}\n                LR_SCHEDULE_KITER[""1x""] = [120, 160, 180]\n                _C.TRAIN.LR_SCHEDULE = [x * 1000 for x in LR_SCHEDULE_KITER[lr]]\n            else:\n                _C.TRAIN.LR_SCHEDULE = eval(lr)\n\n        # setup NUM_GPUS\n        if _C.TRAINER == \'horovod\':\n            import horovod.tensorflow as hvd\n            ngpu = hvd.size()\n            logger.info(""Horovod Rank={}, Size={}, LocalRank={}"".format(\n                hvd.rank(), hvd.size(), hvd.local_rank()))\n        else:\n            assert \'OMPI_COMM_WORLD_SIZE\' not in os.environ\n            ngpu = get_num_gpu()\n        assert ngpu > 0, ""Has to train with GPU!""\n        assert ngpu % 8 == 0 or 8 % ngpu == 0, ""Can only train with 1,2,4 or >=8 GPUs, but found {} GPUs"".format(ngpu)\n    else:\n        # autotune is too slow for inference\n        os.environ[\'TF_CUDNN_USE_AUTOTUNE\'] = \'0\'\n        ngpu = get_num_gpu()\n\n    if _C.TRAIN.NUM_GPUS is None:\n        _C.TRAIN.NUM_GPUS = ngpu\n    else:\n        if _C.TRAINER == \'horovod\':\n            assert _C.TRAIN.NUM_GPUS == ngpu\n        else:\n            assert _C.TRAIN.NUM_GPUS <= ngpu\n\n    _C.freeze()\n    logger.info(""Config: ------------------------------------------\\n"" + str(_C))\n'"
examples/FasterRCNN/data.py,0,"b'# -*- coding: utf-8 -*-\n# File: data.py\n\nimport copy\nimport itertools\nimport numpy as np\nimport cv2\nfrom tabulate import tabulate\nfrom termcolor import colored\n\nfrom tensorpack.dataflow import (\n    DataFromList, MapData, MapDataComponent,\n    MultiProcessMapData, MultiThreadMapData, TestDataSpeed, imgaug,\n)\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.argtools import log_once\n\nfrom modeling.model_rpn import get_all_anchors\nfrom modeling.model_fpn import get_all_anchors_fpn\nfrom common import (\n    CustomResize, DataFromListOfDict, box_to_point4,\n    filter_boxes_inside_shape, np_iou, point4_to_box, polygons_to_mask,\n)\nfrom config import config as cfg\nfrom dataset import DatasetRegistry, register_coco\nfrom utils.np_box_ops import area as np_area\nfrom utils.np_box_ops import ioa as np_ioa\n\n# import tensorpack.utils.viz as tpviz\n\n\nclass MalformedData(BaseException):\n    pass\n\n\ndef print_class_histogram(roidbs):\n    """"""\n    Args:\n        roidbs (list[dict]): the same format as the output of `training_roidbs`.\n    """"""\n    class_names = DatasetRegistry.get_metadata(cfg.DATA.TRAIN[0], \'class_names\')\n    # labels are in [1, NUM_CATEGORY], hence +2 for bins\n    hist_bins = np.arange(cfg.DATA.NUM_CATEGORY + 2)\n\n    # Histogram of ground-truth objects\n    gt_hist = np.zeros((cfg.DATA.NUM_CATEGORY + 1,), dtype=np.int)\n    for entry in roidbs:\n        # filter crowd?\n        gt_inds = np.where((entry[""class""] > 0) & (entry[""is_crowd""] == 0))[0]\n        gt_classes = entry[""class""][gt_inds]\n        if len(gt_classes):\n            assert gt_classes.max() <= len(class_names) - 1\n        gt_hist += np.histogram(gt_classes, bins=hist_bins)[0]\n    data = list(itertools.chain(*[[class_names[i + 1], v] for i, v in enumerate(gt_hist[1:])]))\n    COL = min(6, len(data))\n    total_instances = sum(data[1::2])\n    data.extend([None] * ((COL - len(data) % COL) % COL))\n    data.extend([""total"", total_instances])\n    data = itertools.zip_longest(*[data[i::COL] for i in range(COL)])\n    # the first line is BG\n    table = tabulate(data, headers=[""class"", ""#box""] * (COL // 2), tablefmt=""pipe"", stralign=""center"", numalign=""left"")\n    logger.info(""Ground-Truth category distribution:\\n"" + colored(table, ""cyan""))\n\n\nclass TrainingDataPreprocessor:\n    """"""\n    The mapper to preprocess the input data for training.\n\n    Since the mapping may run in other processes, we write a new class and\n    explicitly pass cfg to it, in the spirit of ""explicitly pass resources to subprocess"".\n    """"""\n\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.aug = imgaug.AugmentorList([\n            CustomResize(cfg.PREPROC.TRAIN_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE),\n            imgaug.Flip(horiz=True)\n        ])\n\n    def __call__(self, roidb):\n        fname, boxes, klass, is_crowd = roidb[""file_name""], roidb[""boxes""], roidb[""class""], roidb[""is_crowd""]\n        assert boxes.ndim == 2 and boxes.shape[1] == 4, boxes.shape\n        boxes = np.copy(boxes)\n        im = cv2.imread(fname, cv2.IMREAD_COLOR)\n        assert im is not None, fname\n        im = im.astype(""float32"")\n        height, width = im.shape[:2]\n        # assume floatbox as input\n        assert boxes.dtype == np.float32, ""Loader has to return float32 boxes!""\n\n        if not self.cfg.DATA.ABSOLUTE_COORD:\n            boxes[:, 0::2] *= width\n            boxes[:, 1::2] *= height\n\n        # augmentation:\n        tfms = self.aug.get_transform(im)\n        im = tfms.apply_image(im)\n        points = box_to_point4(boxes)\n        points = tfms.apply_coords(points)\n        boxes = point4_to_box(points)\n        if len(boxes):\n            assert klass.max() <= self.cfg.DATA.NUM_CATEGORY, \\\n                ""Invalid category {}!"".format(klass.max())\n            assert np.min(np_area(boxes)) > 0, ""Some boxes have zero area!""\n\n        ret = {""image"": im}\n        # Add rpn data to dataflow:\n        try:\n            if self.cfg.MODE_FPN:\n                multilevel_anchor_inputs = self.get_multilevel_rpn_anchor_input(im, boxes, is_crowd)\n                for i, (anchor_labels, anchor_boxes) in enumerate(multilevel_anchor_inputs):\n                    ret[""anchor_labels_lvl{}"".format(i + 2)] = anchor_labels\n                    ret[""anchor_boxes_lvl{}"".format(i + 2)] = anchor_boxes\n            else:\n                ret[""anchor_labels""], ret[""anchor_boxes""] = self.get_rpn_anchor_input(im, boxes, is_crowd)\n\n            boxes = boxes[is_crowd == 0]  # skip crowd boxes in training target\n            klass = klass[is_crowd == 0]\n            ret[""gt_boxes""] = boxes\n            ret[""gt_labels""] = klass\n        except MalformedData as e:\n            log_once(""Input {} is filtered for training: {}"".format(fname, str(e)), ""warn"")\n            return None\n\n        if self.cfg.MODE_MASK:\n            # augmentation will modify the polys in-place\n            segmentation = copy.deepcopy(roidb[""segmentation""])\n            segmentation = [segmentation[k] for k in range(len(segmentation)) if not is_crowd[k]]\n            assert len(segmentation) == len(boxes)\n\n            # Apply augmentation on polygon coordinates.\n            # And produce one image-sized binary mask per box.\n            masks = []\n            width_height = np.asarray([width, height], dtype=np.float32)\n            gt_mask_width = int(np.ceil(im.shape[1] / 8.0) * 8)   # pad to 8 in order to pack mask into bits\n\n            for polys in segmentation:\n                if not self.cfg.DATA.ABSOLUTE_COORD:\n                    polys = [p * width_height for p in polys]\n                polys = [tfms.apply_coords(p) for p in polys]\n                masks.append(polygons_to_mask(polys, im.shape[0], gt_mask_width))\n\n            if len(masks):\n                masks = np.asarray(masks, dtype=\'uint8\')    # values in {0, 1}\n                masks = np.packbits(masks, axis=-1)\n            else:  # no gt on the image\n                masks = np.zeros((0, im.shape[0], gt_mask_width // 8), dtype=\'uint8\')\n\n            ret[\'gt_masks_packed\'] = masks\n\n            # from viz import draw_annotation, draw_mask\n            # viz = draw_annotation(im, boxes, klass)\n            # for mask in masks:\n            #     viz = draw_mask(viz, mask)\n            # tpviz.interactive_imshow(viz)\n        return ret\n\n    def get_rpn_anchor_input(self, im, boxes, is_crowd):\n        """"""\n        Args:\n            im: an image\n            boxes: nx4, floatbox, gt. shoudn\'t be changed\n            is_crowd: n,\n\n        Returns:\n            The anchor labels and target boxes for each pixel in the featuremap.\n            fm_labels: fHxfWxNA\n            fm_boxes: fHxfWxNAx4\n            NA will be NUM_ANCHOR_SIZES x NUM_ANCHOR_RATIOS\n        """"""\n        boxes = boxes.copy()\n        all_anchors = np.copy(\n            get_all_anchors(\n                stride=self.cfg.RPN.ANCHOR_STRIDE,\n                sizes=self.cfg.RPN.ANCHOR_SIZES,\n                ratios=self.cfg.RPN.ANCHOR_RATIOS,\n                max_size=self.cfg.PREPROC.MAX_SIZE,\n            )\n        )\n        # fHxfWxAx4 -> (-1, 4)\n        featuremap_anchors_flatten = all_anchors.reshape((-1, 4))\n\n        # only use anchors inside the image\n        inside_ind, inside_anchors = filter_boxes_inside_shape(featuremap_anchors_flatten, im.shape[:2])\n        # obtain anchor labels and their corresponding gt boxes\n        anchor_labels, anchor_gt_boxes = self.get_anchor_labels(\n            inside_anchors, boxes[is_crowd == 0], boxes[is_crowd == 1]\n        )\n\n        # Fill them back to original size: fHxfWx1, fHxfWx4\n        num_anchor = self.cfg.RPN.NUM_ANCHOR\n        anchorH, anchorW = all_anchors.shape[:2]\n        featuremap_labels = -np.ones((anchorH * anchorW * num_anchor,), dtype=""int32"")\n        featuremap_labels[inside_ind] = anchor_labels\n        featuremap_labels = featuremap_labels.reshape((anchorH, anchorW, num_anchor))\n        featuremap_boxes = np.zeros((anchorH * anchorW * num_anchor, 4), dtype=""float32"")\n        featuremap_boxes[inside_ind, :] = anchor_gt_boxes\n        featuremap_boxes = featuremap_boxes.reshape((anchorH, anchorW, num_anchor, 4))\n        return featuremap_labels, featuremap_boxes\n\n    # TODO: can probably merge single-level logic with FPN logic to simplify code\n    def get_multilevel_rpn_anchor_input(self, im, boxes, is_crowd):\n        """"""\n        Args:\n            im: an image\n            boxes: nx4, floatbox, gt. shoudn\'t be changed\n            is_crowd: n,\n\n        Returns:\n            [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level.\n            Each tuple contains the anchor labels and target boxes for each pixel in the featuremap.\n\n            fm_labels: fHxfWx NUM_ANCHOR_RATIOS\n            fm_boxes: fHxfWx NUM_ANCHOR_RATIOS x4\n        """"""\n        boxes = boxes.copy()\n        anchors_per_level = get_all_anchors_fpn(\n            strides=self.cfg.FPN.ANCHOR_STRIDES,\n            sizes=self.cfg.RPN.ANCHOR_SIZES,\n            ratios=self.cfg.RPN.ANCHOR_RATIOS,\n            max_size=self.cfg.PREPROC.MAX_SIZE,\n        )\n        flatten_anchors_per_level = [k.reshape((-1, 4)) for k in anchors_per_level]\n        all_anchors_flatten = np.concatenate(flatten_anchors_per_level, axis=0)\n\n        inside_ind, inside_anchors = filter_boxes_inside_shape(all_anchors_flatten, im.shape[:2])\n\n        anchor_labels, anchor_gt_boxes = self.get_anchor_labels(\n            inside_anchors, boxes[is_crowd == 0], boxes[is_crowd == 1]\n        )\n\n        # map back to all_anchors, then split to each level\n        num_all_anchors = all_anchors_flatten.shape[0]\n        all_labels = -np.ones((num_all_anchors,), dtype=""int32"")\n        all_labels[inside_ind] = anchor_labels\n        all_boxes = np.zeros((num_all_anchors, 4), dtype=""float32"")\n        all_boxes[inside_ind] = anchor_gt_boxes\n\n        start = 0\n        multilevel_inputs = []\n        for level_anchor in anchors_per_level:\n            assert level_anchor.shape[2] == len(self.cfg.RPN.ANCHOR_RATIOS)\n            anchor_shape = level_anchor.shape[:3]  # fHxfWxNUM_ANCHOR_RATIOS\n            num_anchor_this_level = np.prod(anchor_shape)\n            end = start + num_anchor_this_level\n            multilevel_inputs.append(\n                (all_labels[start:end].reshape(anchor_shape), all_boxes[start:end, :].reshape(anchor_shape + (4,)))\n            )\n            start = end\n        assert end == num_all_anchors, ""{} != {}"".format(end, num_all_anchors)\n        return multilevel_inputs\n\n    def get_anchor_labels(self, anchors, gt_boxes, crowd_boxes):\n        """"""\n        Label each anchor as fg/bg/ignore.\n        Args:\n            anchors: Ax4 float\n            gt_boxes: Bx4 float, non-crowd\n            crowd_boxes: Cx4 float\n\n        Returns:\n            anchor_labels: (A,) int. Each element is {-1, 0, 1}\n            anchor_boxes: Ax4. Contains the target gt_box for each anchor when the anchor is fg.\n        """"""\n        # This function will modify labels and return the filtered inds\n        def filter_box_label(labels, value, max_num):\n            curr_inds = np.where(labels == value)[0]\n            if len(curr_inds) > max_num:\n                disable_inds = np.random.choice(curr_inds, size=(len(curr_inds) - max_num), replace=False)\n                labels[disable_inds] = -1  # ignore them\n                curr_inds = np.where(labels == value)[0]\n            return curr_inds\n\n        NA, NB = len(anchors), len(gt_boxes)\n        if NB == 0:\n            # No groundtruth. All anchors are either background or ignored.\n            anchor_labels = np.zeros((NA,), dtype=""int32"")\n            filter_box_label(anchor_labels, 0, self.cfg.RPN.BATCH_PER_IM)\n            return anchor_labels, np.zeros((NA, 4), dtype=""float32"")\n\n        box_ious = np_iou(anchors, gt_boxes)  # NA x NB\n        ious_argmax_per_anchor = box_ious.argmax(axis=1)  # NA,\n        ious_max_per_anchor = box_ious.max(axis=1)\n        ious_max_per_gt = np.amax(box_ious, axis=0, keepdims=True)  # 1xNB\n        # for each gt, find all those anchors (including ties) that has the max ious with it\n        anchors_with_max_iou_per_gt = np.where(box_ious == ious_max_per_gt)[0]\n\n        # Setting NA labels: 1--fg 0--bg -1--ignore\n        anchor_labels = -np.ones((NA,), dtype=""int32"")  # NA,\n\n        # the order of setting neg/pos labels matter\n        anchor_labels[anchors_with_max_iou_per_gt] = 1\n        anchor_labels[ious_max_per_anchor >= self.cfg.RPN.POSITIVE_ANCHOR_THRESH] = 1\n        anchor_labels[ious_max_per_anchor < self.cfg.RPN.NEGATIVE_ANCHOR_THRESH] = 0\n\n        # label all non-ignore candidate boxes which overlap crowd as ignore\n        if crowd_boxes.size > 0:\n            cand_inds = np.where(anchor_labels >= 0)[0]\n            cand_anchors = anchors[cand_inds]\n            ioas = np_ioa(crowd_boxes, cand_anchors)\n            overlap_with_crowd = cand_inds[ioas.max(axis=0) > self.cfg.RPN.CROWD_OVERLAP_THRESH]\n            anchor_labels[overlap_with_crowd] = -1\n\n        # Subsample fg labels: ignore some fg if fg is too many\n        target_num_fg = int(self.cfg.RPN.BATCH_PER_IM * self.cfg.RPN.FG_RATIO)\n        fg_inds = filter_box_label(anchor_labels, 1, target_num_fg)\n        # Keep an image even if there is no foreground anchors\n        # if len(fg_inds) == 0:\n        #     raise MalformedData(""No valid foreground for RPN!"")\n\n        # Subsample bg labels. num_bg is not allowed to be too many\n        old_num_bg = np.sum(anchor_labels == 0)\n        if old_num_bg == 0:\n            # No valid bg in this image, skip.\n            raise MalformedData(""No valid background for RPN!"")\n        target_num_bg = self.cfg.RPN.BATCH_PER_IM - len(fg_inds)\n        filter_box_label(anchor_labels, 0, target_num_bg)  # ignore return values\n\n        # Set anchor boxes: the best gt_box for each fg anchor\n        anchor_boxes = np.zeros((NA, 4), dtype=""float32"")\n        fg_boxes = gt_boxes[ious_argmax_per_anchor[fg_inds], :]\n        anchor_boxes[fg_inds, :] = fg_boxes\n        # assert len(fg_inds) + np.sum(anchor_labels == 0) == self.cfg.RPN.BATCH_PER_IM\n        return anchor_labels, anchor_boxes\n\n\ndef get_train_dataflow():\n    """"""\n    Return a training dataflow. Each datapoint consists of the following:\n\n    An image: (h, w, 3),\n\n    1 or more pairs of (anchor_labels, anchor_boxes):\n    anchor_labels: (h\', w\', NA)\n    anchor_boxes: (h\', w\', NA, 4)\n\n    gt_boxes: (N, 4)\n    gt_labels: (N,)\n\n    If MODE_MASK, gt_masks: (N, h, w)\n    """"""\n    roidbs = list(itertools.chain.from_iterable(DatasetRegistry.get(x).training_roidbs() for x in cfg.DATA.TRAIN))\n    print_class_histogram(roidbs)\n\n    # Filter out images that have no gt boxes, but this filter shall not be applied for testing.\n    # The model does support training with empty images, but it is not useful for COCO.\n    num = len(roidbs)\n    if cfg.DATA.FILTER_EMPTY_ANNOTATIONS:\n        roidbs = list(filter(lambda img: len(img[""boxes""][img[""is_crowd""] == 0]) > 0, roidbs))       \n    logger.info(\n        ""Filtered {} images which contain no non-crowd groudtruth boxes. Total #images for training: {}"".format(\n            num - len(roidbs), len(roidbs)\n        )\n    )\n\n    ds = DataFromList(roidbs, shuffle=True)\n\n    preprocess = TrainingDataPreprocessor(cfg)\n\n    if cfg.DATA.NUM_WORKERS > 0:\n        if cfg.TRAINER == ""horovod"":\n            buffer_size = cfg.DATA.NUM_WORKERS * 10  # one dataflow for each process, therefore don\'t need large buffer\n            ds = MultiThreadMapData(ds, cfg.DATA.NUM_WORKERS, preprocess, buffer_size=buffer_size)\n            # MPI does not like fork()\n        else:\n            buffer_size = cfg.DATA.NUM_WORKERS * 20\n            ds = MultiProcessMapData(ds, cfg.DATA.NUM_WORKERS, preprocess, buffer_size=buffer_size)\n    else:\n        ds = MapData(ds, preprocess)\n    return ds\n\n\ndef get_eval_dataflow(name, shard=0, num_shards=1):\n    """"""\n    Args:\n        name (str): name of the dataset to evaluate\n        shard, num_shards: to get subset of evaluation data\n    """"""\n    roidbs = DatasetRegistry.get(name).inference_roidbs()\n    logger.info(""Found {} images for inference."".format(len(roidbs)))\n\n    num_imgs = len(roidbs)\n    img_per_shard = num_imgs // num_shards\n    img_range = (shard * img_per_shard, (shard + 1) * img_per_shard if shard + 1 < num_shards else num_imgs)\n\n    # no filter for training\n    ds = DataFromListOfDict(roidbs[img_range[0]: img_range[1]], [""file_name"", ""image_id""])\n\n    def f(fname):\n        im = cv2.imread(fname, cv2.IMREAD_COLOR)\n        assert im is not None, fname\n        return im\n\n    ds = MapDataComponent(ds, f, 0)\n    # Evaluation itself may be multi-threaded, therefore don\'t add prefetch here.\n    return ds\n\n\nif __name__ == ""__main__"":\n    import os\n    from tensorpack.dataflow import PrintData\n    from config import finalize_configs\n\n    register_coco(os.path.expanduser(""~/data/coco""))\n    finalize_configs()\n    ds = get_train_dataflow()\n    ds = PrintData(ds, 10)\n    TestDataSpeed(ds, 50000).start()\n    for k in ds:\n        pass\n'"
examples/FasterRCNN/eval.py,1,"b'# -*- coding: utf-8 -*-\n# File: eval.py\n\nimport itertools\nimport json\nimport numpy as np\nimport os\nimport sys\nimport tensorflow as tf\nfrom collections import namedtuple\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import ExitStack\nimport cv2\nimport pycocotools.mask as cocomask\nimport tqdm\nfrom scipy import interpolate\n\nfrom tensorpack.callbacks import Callback\nfrom tensorpack.tfutils.common import get_tf_version_tuple\nfrom tensorpack.utils import logger, get_tqdm\n\nfrom common import CustomResize, clip_boxes\nfrom config import config as cfg\nfrom data import get_eval_dataflow\nfrom dataset import DatasetRegistry\n\ntry:\n    import horovod.tensorflow as hvd\nexcept ImportError:\n    pass\n\n\nDetectionResult = namedtuple(\n    \'DetectionResult\',\n    [\'box\', \'score\', \'class_id\', \'mask\'])\n""""""\nbox: 4 float\nscore: float\nclass_id: int, 1~NUM_CLASS\nmask: None, or a binary image of the original image shape\n""""""\n\n\ndef _scale_box(box, scale):\n    w_half = (box[2] - box[0]) * 0.5\n    h_half = (box[3] - box[1]) * 0.5\n    x_c = (box[2] + box[0]) * 0.5\n    y_c = (box[3] + box[1]) * 0.5\n\n    w_half *= scale\n    h_half *= scale\n\n    scaled_box = np.zeros_like(box)\n    scaled_box[0] = x_c - w_half\n    scaled_box[2] = x_c + w_half\n    scaled_box[1] = y_c - h_half\n    scaled_box[3] = y_c + h_half\n    return scaled_box\n\n\ndef _paste_mask(box, mask, shape):\n    """"""\n    Args:\n        box: 4 float\n        mask: MxM floats\n        shape: h,w\n    Returns:\n        A uint8 binary image of hxw.\n    """"""\n    assert mask.shape[0] == mask.shape[1], mask.shape\n\n    if cfg.MRCNN.ACCURATE_PASTE:\n        # This method is accurate but much slower.\n        mask = np.pad(mask, [(1, 1), (1, 1)], mode=\'constant\')\n        box = _scale_box(box, float(mask.shape[0]) / (mask.shape[0] - 2))\n\n        mask_pixels = np.arange(0.0, mask.shape[0]) + 0.5\n        mask_continuous = interpolate.interp2d(mask_pixels, mask_pixels, mask, fill_value=0.0)\n        h, w = shape\n        ys = np.arange(0.0, h) + 0.5\n        xs = np.arange(0.0, w) + 0.5\n        ys = (ys - box[1]) / (box[3] - box[1]) * mask.shape[0]\n        xs = (xs - box[0]) / (box[2] - box[0]) * mask.shape[1]\n        # Waste a lot of compute since most indices are out-of-border\n        res = mask_continuous(xs, ys)\n        return (res >= 0.5).astype(\'uint8\')\n    else:\n        # This method (inspired by Detectron) is less accurate but fast.\n\n        # int() is floor\n        # box fpcoor=0.0 -> intcoor=0.0\n        x0, y0 = list(map(int, box[:2] + 0.5))\n        # box fpcoor=h -> intcoor=h-1, inclusive\n        x1, y1 = list(map(int, box[2:] - 0.5))    # inclusive\n        x1 = max(x0, x1)    # require at least 1x1\n        y1 = max(y0, y1)\n\n        w = x1 + 1 - x0\n        h = y1 + 1 - y0\n\n        # rounding errors could happen here, because masks were not originally computed for this shape.\n        # but it\'s hard to do better, because the network does not know the ""original"" scale\n        mask = (cv2.resize(mask, (w, h)) > 0.5).astype(\'uint8\')\n        ret = np.zeros(shape, dtype=\'uint8\')\n        ret[y0:y1 + 1, x0:x1 + 1] = mask\n        return ret\n\n\ndef predict_image(img, model_func):\n    """"""\n    Run detection on one image, using the TF callable.\n    This function should handle the preprocessing internally.\n\n    Args:\n        img: an image\n        model_func: a callable from the TF model.\n            It takes image and returns (boxes, probs, labels, [masks])\n\n    Returns:\n        [DetectionResult]\n    """"""\n    orig_shape = img.shape[:2]\n    resizer = CustomResize(cfg.PREPROC.TEST_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE)\n    resized_img = resizer.augment(img)\n    scale = np.sqrt(resized_img.shape[0] * 1.0 / img.shape[0] * resized_img.shape[1] / img.shape[1])\n    boxes, probs, labels, *masks = model_func(resized_img)\n\n    # Some slow numpy postprocessing:\n    boxes = boxes / scale\n    # boxes are already clipped inside the graph, but after the floating point scaling, this may not be true any more.\n    boxes = clip_boxes(boxes, orig_shape)\n    if masks:\n        full_masks = [_paste_mask(box, mask, orig_shape)\n                      for box, mask in zip(boxes, masks[0])]\n        masks = full_masks\n    else:\n        # fill with none\n        masks = [None] * len(boxes)\n\n    results = [DetectionResult(*args) for args in zip(boxes, probs, labels.tolist(), masks)]\n    return results\n\n\ndef predict_dataflow(df, model_func, tqdm_bar=None):\n    """"""\n    Args:\n        df: a DataFlow which produces (image, image_id)\n        model_func: a callable from the TF model.\n            It takes image and returns (boxes, probs, labels, [masks])\n        tqdm_bar: a tqdm object to be shared among multiple evaluation instances. If None,\n            will create a new one.\n\n    Returns:\n        list of dict, in the format used by\n        `DatasetSplit.eval_inference_results`\n    """"""\n    df.reset_state()\n    all_results = []\n    with ExitStack() as stack:\n        # tqdm is not quite thread-safe: https://github.com/tqdm/tqdm/issues/323\n        if tqdm_bar is None:\n            tqdm_bar = stack.enter_context(get_tqdm(total=df.size()))\n        for img, img_id in df:\n            results = predict_image(img, model_func)\n            for r in results:\n                # int()/float() to make it json-serializable\n                res = {\n                    \'image_id\': img_id,\n                    \'category_id\': int(r.class_id),\n                    \'bbox\': [round(float(x), 4) for x in r.box],\n                    \'score\': round(float(r.score), 4),\n                }\n\n                # also append segmentation to results\n                if r.mask is not None:\n                    rle = cocomask.encode(\n                        np.array(r.mask[:, :, None], order=\'F\'))[0]\n                    rle[\'counts\'] = rle[\'counts\'].decode(\'ascii\')\n                    res[\'segmentation\'] = rle\n                all_results.append(res)\n            tqdm_bar.update(1)\n    return all_results\n\n\ndef multithread_predict_dataflow(dataflows, model_funcs):\n    """"""\n    Running multiple `predict_dataflow` in multiple threads, and aggregate the results.\n\n    Args:\n        dataflows: a list of DataFlow to be used in :func:`predict_dataflow`\n        model_funcs: a list of callable to be used in :func:`predict_dataflow`\n\n    Returns:\n        list of dict, in the format used by\n        `DatasetSplit.eval_inference_results`\n    """"""\n    num_worker = len(model_funcs)\n    assert len(dataflows) == num_worker\n    if num_worker == 1:\n        return predict_dataflow(dataflows[0], model_funcs[0])\n    kwargs = {\'thread_name_prefix\': \'EvalWorker\'} if sys.version_info.minor >= 6 else {}\n    with ThreadPoolExecutor(max_workers=num_worker, **kwargs) as executor, \\\n            tqdm.tqdm(total=sum([df.size() for df in dataflows])) as pbar:\n        futures = []\n        for dataflow, pred in zip(dataflows, model_funcs):\n            futures.append(executor.submit(predict_dataflow, dataflow, pred, pbar))\n        all_results = list(itertools.chain(*[fut.result() for fut in futures]))\n        return all_results\n\n\nclass EvalCallback(Callback):\n    """"""\n    A callback that runs evaluation once a while.\n    It supports multi-gpu evaluation.\n    """"""\n\n    _chief_only = False\n\n    def __init__(self, eval_dataset, in_names, out_names, output_dir):\n        self._eval_dataset = eval_dataset\n        self._in_names, self._out_names = in_names, out_names\n        self._output_dir = output_dir\n\n    def _setup_graph(self):\n        num_gpu = cfg.TRAIN.NUM_GPUS\n        if cfg.TRAINER == \'replicated\':\n            # TF bug in version 1.11, 1.12: https://github.com/tensorflow/tensorflow/issues/22750\n            buggy_tf = get_tf_version_tuple() in [(1, 11), (1, 12)]\n\n            # Use two predictor threads per GPU to get better throughput\n            self.num_predictor = num_gpu if buggy_tf else num_gpu * 2\n            self.predictors = [self._build_predictor(k % num_gpu) for k in range(self.num_predictor)]\n            self.dataflows = [get_eval_dataflow(self._eval_dataset,\n                                                shard=k, num_shards=self.num_predictor)\n                              for k in range(self.num_predictor)]\n        else:\n            # Only eval on the first machine,\n            # Because evaluation assumes that all horovod workers share the filesystem.\n            # Alternatively, can eval on all ranks and use allgather, but allgather sometimes hangs\n            self._horovod_run_eval = hvd.rank() == hvd.local_rank()\n            if self._horovod_run_eval:\n                self.predictor = self._build_predictor(0)\n                self.dataflow = get_eval_dataflow(self._eval_dataset,\n                                                  shard=hvd.local_rank(), num_shards=hvd.local_size())\n\n            self.barrier = hvd.allreduce(tf.random_normal(shape=[1]))\n\n    def _build_predictor(self, idx):\n        return self.trainer.get_predictor(self._in_names, self._out_names, device=idx)\n\n    def _before_train(self):\n        eval_period = cfg.TRAIN.EVAL_PERIOD\n        self.epochs_to_eval = set()\n        for k in itertools.count(1):\n            if k * eval_period > self.trainer.max_epoch:\n                break\n            self.epochs_to_eval.add(k * eval_period)\n        self.epochs_to_eval.add(self.trainer.max_epoch)\n        logger.info(""[EvalCallback] Will evaluate every {} epochs"".format(eval_period))\n\n    def _eval(self):\n        logdir = self._output_dir\n        if cfg.TRAINER == \'replicated\':\n            all_results = multithread_predict_dataflow(self.dataflows, self.predictors)\n        else:\n            filenames = [os.path.join(\n                logdir, \'outputs{}-part{}.json\'.format(self.global_step, rank)\n            ) for rank in range(hvd.local_size())]\n\n            if self._horovod_run_eval:\n                local_results = predict_dataflow(self.dataflow, self.predictor)\n                fname = filenames[hvd.local_rank()]\n                with open(fname, \'w\') as f:\n                    json.dump(local_results, f)\n            self.barrier.eval()\n            if hvd.rank() > 0:\n                return\n            all_results = []\n            for fname in filenames:\n                with open(fname, \'r\') as f:\n                    obj = json.load(f)\n                all_results.extend(obj)\n                os.unlink(fname)\n\n        scores = DatasetRegistry.get(self._eval_dataset).eval_inference_results(all_results)\n        for k, v in scores.items():\n            self.trainer.monitors.put_scalar(self._eval_dataset + \'-\' + k, v)\n\n    def _trigger_epoch(self):\n        if self.epoch_num in self.epochs_to_eval:\n            logger.info(""Running evaluation ..."")\n            self._eval()\n'"
examples/FasterRCNN/predict.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport argparse\nimport itertools\nimport numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\nimport cv2\nimport tqdm\n\nimport tensorpack.utils.viz as tpviz\nfrom tensorpack.predict import MultiTowerOfflinePredictor, OfflinePredictor, PredictConfig\nfrom tensorpack.tfutils import SmartInit, get_tf_version_tuple\nfrom tensorpack.tfutils.export import ModelExporter\nfrom tensorpack.utils import fs, logger\n\nfrom dataset import DatasetRegistry, register_coco, register_balloon\nfrom config import config as cfg\nfrom config import finalize_configs\nfrom data import get_eval_dataflow, get_train_dataflow\nfrom eval import DetectionResult, multithread_predict_dataflow, predict_image\nfrom modeling.generalized_rcnn import ResNetC4Model, ResNetFPNModel\nfrom viz import (\n    draw_annotation, draw_final_outputs, draw_predictions,\n    draw_proposal_recall, draw_final_outputs_blackwhite)\n\n\ndef do_visualize(model, model_path, nr_visualize=100, output_dir=\'output\'):\n    """"""\n    Visualize some intermediate results (proposals, raw predictions) inside the pipeline.\n    """"""\n    df = get_train_dataflow()\n    df.reset_state()\n\n    pred = OfflinePredictor(PredictConfig(\n        model=model,\n        session_init=SmartInit(model_path),\n        input_names=[\'image\', \'gt_boxes\', \'gt_labels\'],\n        output_names=[\n            \'generate_{}_proposals/boxes\'.format(\'fpn\' if cfg.MODE_FPN else \'rpn\'),\n            \'generate_{}_proposals/scores\'.format(\'fpn\' if cfg.MODE_FPN else \'rpn\'),\n            \'fastrcnn_all_scores\',\n            \'output/boxes\',\n            \'output/scores\',\n            \'output/labels\',\n        ]))\n\n    if os.path.isdir(output_dir):\n        shutil.rmtree(output_dir)\n    fs.mkdir_p(output_dir)\n    with tqdm.tqdm(total=nr_visualize) as pbar:\n        for idx, dp in itertools.islice(enumerate(df), nr_visualize):\n            img, gt_boxes, gt_labels = dp[\'image\'], dp[\'gt_boxes\'], dp[\'gt_labels\']\n\n            rpn_boxes, rpn_scores, all_scores, \\\n                final_boxes, final_scores, final_labels = pred(img, gt_boxes, gt_labels)\n\n            # draw groundtruth boxes\n            gt_viz = draw_annotation(img, gt_boxes, gt_labels)\n            # draw best proposals for each groundtruth, to show recall\n            proposal_viz, good_proposals_ind = draw_proposal_recall(img, rpn_boxes, rpn_scores, gt_boxes)\n            # draw the scores for the above proposals\n            score_viz = draw_predictions(img, rpn_boxes[good_proposals_ind], all_scores[good_proposals_ind])\n\n            results = [DetectionResult(*args) for args in\n                       zip(final_boxes, final_scores, final_labels,\n                           [None] * len(final_labels))]\n            final_viz = draw_final_outputs(img, results)\n\n            viz = tpviz.stack_patches([\n                gt_viz, proposal_viz,\n                score_viz, final_viz], 2, 2)\n\n            if os.environ.get(\'DISPLAY\', None):\n                tpviz.interactive_imshow(viz)\n            cv2.imwrite(""{}/{:03d}.png"".format(output_dir, idx), viz)\n            pbar.update()\n\n\ndef do_evaluate(pred_config, output_file):\n    num_tower = max(cfg.TRAIN.NUM_GPUS, 1)\n    graph_funcs = MultiTowerOfflinePredictor(\n        pred_config, list(range(num_tower))).get_predictors()\n\n    for dataset in cfg.DATA.VAL:\n        logger.info(""Evaluating {} ..."".format(dataset))\n        dataflows = [\n            get_eval_dataflow(dataset, shard=k, num_shards=num_tower)\n            for k in range(num_tower)]\n        all_results = multithread_predict_dataflow(dataflows, graph_funcs)\n        output = output_file + \'-\' + dataset\n        DatasetRegistry.get(dataset).eval_inference_results(all_results, output)\n\n\ndef do_predict(pred_func, input_file):\n    img = cv2.imread(input_file, cv2.IMREAD_COLOR)\n    results = predict_image(img, pred_func)\n    if cfg.MODE_MASK:\n        final = draw_final_outputs_blackwhite(img, results)\n    else:\n        final = draw_final_outputs(img, results)\n    viz = np.concatenate((img, final), axis=1)\n    cv2.imwrite(""output.png"", viz)\n    logger.info(""Inference output for {} written to output.png"".format(input_file))\n    tpviz.interactive_imshow(viz)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--load\', help=\'load a model for evaluation.\', required=True)\n    parser.add_argument(\'--visualize\', action=\'store_true\', help=\'visualize intermediate results\')\n    parser.add_argument(\'--evaluate\', help=""Run evaluation. ""\n                                           ""This argument is the path to the output json evaluation file"")\n    parser.add_argument(\'--predict\', help=""Run prediction on a given image. ""\n                                          ""This argument is the path to the input image file"", nargs=\'+\')\n    parser.add_argument(\'--benchmark\', action=\'store_true\', help=""Benchmark the speed of the model + postprocessing"")\n    parser.add_argument(\'--config\', help=""A list of KEY=VALUE to overwrite those defined in config.py"",\n                        nargs=\'+\')\n    parser.add_argument(\'--output-pb\', help=\'Save a model to .pb\')\n    parser.add_argument(\'--output-serving\', help=\'Save a model to serving file\')\n\n    args = parser.parse_args()\n    if args.config:\n        cfg.update_args(args.config)\n    register_coco(cfg.DATA.BASEDIR)  # add COCO datasets to the registry\n    register_balloon(cfg.DATA.BASEDIR)\n\n    MODEL = ResNetFPNModel() if cfg.MODE_FPN else ResNetC4Model()\n\n    if not tf.test.is_gpu_available():\n        from tensorflow.python.framework import test_util\n        assert get_tf_version_tuple() >= (1, 7) and test_util.IsMklEnabled(), \\\n            ""Inference requires either GPU support or MKL support!""\n    assert args.load\n    finalize_configs(is_training=False)\n\n    if args.predict or args.visualize:\n        cfg.TEST.RESULT_SCORE_THRESH = cfg.TEST.RESULT_SCORE_THRESH_VIS\n\n    if args.visualize:\n        do_visualize(MODEL, args.load)\n    else:\n        predcfg = PredictConfig(\n            model=MODEL,\n            session_init=SmartInit(args.load),\n            input_names=MODEL.get_inference_tensor_names()[0],\n            output_names=MODEL.get_inference_tensor_names()[1])\n\n        if args.output_pb:\n            ModelExporter(predcfg).export_compact(args.output_pb, optimize=False)\n        elif args.output_serving:\n            ModelExporter(predcfg).export_serving(args.output_serving)\n\n        if args.predict:\n            predictor = OfflinePredictor(predcfg)\n            for image_file in args.predict:\n                do_predict(predictor, image_file)\n        elif args.evaluate:\n            assert args.evaluate.endswith(\'.json\'), args.evaluate\n            do_evaluate(predcfg, args.evaluate)\n        elif args.benchmark:\n            df = get_eval_dataflow(cfg.DATA.VAL[0])\n            df.reset_state()\n            predictor = OfflinePredictor(predcfg)\n            for _, img in enumerate(tqdm.tqdm(df, total=len(df), smoothing=0.5)):\n                # This includes post-processing time, which is done on CPU and not optimized\n                # To exclude it, modify `predict_image`.\n                predict_image(img[0], predictor)\n'"
examples/FasterRCNN/train.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: train.py\n\nimport argparse\n\nfrom tensorpack import *\nfrom tensorpack.tfutils import collect_env_info\nfrom tensorpack.tfutils.common import get_tf_version_tuple\n\nfrom dataset import register_coco, register_balloon\nfrom config import config as cfg\nfrom config import finalize_configs\nfrom data import get_train_dataflow\nfrom eval import EvalCallback\nfrom modeling.generalized_rcnn import ResNetC4Model, ResNetFPNModel\n\n\ntry:\n    import horovod.tensorflow as hvd\nexcept ImportError:\n    pass\n\n\nif __name__ == \'__main__\':\n    # ""spawn/forkserver"" is safer than the default ""fork"" method and\n    # produce more deterministic behavior & memory saving\n    # However its limitation is you cannot pass a lambda function to subprocesses.\n    import multiprocessing as mp\n    mp.set_start_method(\'spawn\')\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--load\', help=\'Load a model to start training from. It overwrites BACKBONE.WEIGHTS\')\n    parser.add_argument(\'--logdir\', help=\'Log directory. Will remove the old one if already exists.\',\n                        default=\'train_log/maskrcnn\')\n    parser.add_argument(\'--config\', help=""A list of KEY=VALUE to overwrite those defined in config.py"", nargs=\'+\')\n\n    if get_tf_version_tuple() < (1, 6):\n        # https://github.com/tensorflow/tensorflow/issues/14657\n        logger.warn(""TF<1.6 has a bug which may lead to crash in FasterRCNN if you\'re unlucky."")\n\n    args = parser.parse_args()\n    if args.config:\n        cfg.update_args(args.config)\n    register_coco(cfg.DATA.BASEDIR)  # add COCO datasets to the registry\n    register_balloon(cfg.DATA.BASEDIR)  # add the demo balloon datasets to the registry\n\n    # Setup logging ...\n    is_horovod = cfg.TRAINER == \'horovod\'\n    if is_horovod:\n        hvd.init()\n    if not is_horovod or hvd.rank() == 0:\n        logger.set_logger_dir(args.logdir, \'d\')\n    logger.info(""Environment Information:\\n"" + collect_env_info())\n\n    finalize_configs(is_training=True)\n\n    # Create model\n    MODEL = ResNetFPNModel() if cfg.MODE_FPN else ResNetC4Model()\n\n    # Compute the training schedule from the number of GPUs ...\n    stepnum = cfg.TRAIN.STEPS_PER_EPOCH\n    # warmup is step based, lr is epoch based\n    init_lr = cfg.TRAIN.WARMUP_INIT_LR * min(8. / cfg.TRAIN.NUM_GPUS, 1.)\n    warmup_schedule = [(0, init_lr), (cfg.TRAIN.WARMUP, cfg.TRAIN.BASE_LR)]\n    warmup_end_epoch = cfg.TRAIN.WARMUP * 1. / stepnum\n    lr_schedule = [(int(warmup_end_epoch + 0.5), cfg.TRAIN.BASE_LR)]\n\n    factor = 8. / cfg.TRAIN.NUM_GPUS\n    for idx, steps in enumerate(cfg.TRAIN.LR_SCHEDULE[:-1]):\n        mult = 0.1 ** (idx + 1)\n        lr_schedule.append(\n            (steps * factor // stepnum, cfg.TRAIN.BASE_LR * mult))\n    logger.info(""Warm Up Schedule (steps, value): "" + str(warmup_schedule))\n    logger.info(""LR Schedule (epochs, value): "" + str(lr_schedule))\n    train_dataflow = get_train_dataflow()\n    # This is what\'s commonly referred to as ""epochs""\n    total_passes = cfg.TRAIN.LR_SCHEDULE[-1] * 8 / train_dataflow.size()\n    logger.info(""Total passes of the training set is: {:.5g}"".format(total_passes))\n\n    # Create callbacks ...\n    callbacks = [\n        PeriodicCallback(\n            ModelSaver(max_to_keep=10, keep_checkpoint_every_n_hours=1),\n            every_k_epochs=cfg.TRAIN.CHECKPOINT_PERIOD),\n        # linear warmup\n        ScheduledHyperParamSetter(\n            \'learning_rate\', warmup_schedule, interp=\'linear\', step_based=True),\n        ScheduledHyperParamSetter(\'learning_rate\', lr_schedule),\n        GPUMemoryTracker(),\n        HostMemoryTracker(),\n        ThroughputTracker(samples_per_step=cfg.TRAIN.NUM_GPUS),\n        EstimatedTimeLeft(median=True),\n        SessionRunTimeout(60000),   # 1 minute timeout\n        GPUUtilizationTracker()\n    ]\n    if cfg.TRAIN.EVAL_PERIOD > 0:\n        callbacks.extend([\n            EvalCallback(dataset, *MODEL.get_inference_tensor_names(), args.logdir)\n            for dataset in cfg.DATA.VAL\n        ])\n\n    if is_horovod and hvd.rank() > 0:\n        session_init = None\n    else:\n        if args.load:\n            # ignore mismatched values, so you can `--load` a model for fine-tuning\n            session_init = SmartInit(args.load, ignore_mismatch=True)\n        else:\n            session_init = SmartInit(cfg.BACKBONE.WEIGHTS)\n\n    traincfg = TrainConfig(\n        model=MODEL,\n        data=QueueInput(train_dataflow),\n        callbacks=callbacks,\n        steps_per_epoch=stepnum,\n        max_epoch=cfg.TRAIN.LR_SCHEDULE[-1] * factor // stepnum,\n        session_init=session_init,\n        starting_epoch=cfg.TRAIN.STARTING_EPOCH\n    )\n\n    if is_horovod:\n        trainer = HorovodTrainer(average=False)\n    else:\n        # nccl mode appears faster than cpu mode\n        trainer = SyncMultiGPUTrainerReplicated(cfg.TRAIN.NUM_GPUS, average=False, mode=\'nccl\')\n    launch_train_with_config(traincfg, trainer)\n'"
examples/FasterRCNN/viz.py,0,"b'# -*- coding: utf-8 -*-\n# File: viz.py\n\nimport numpy as np\n\nfrom tensorpack.utils import viz\nfrom tensorpack.utils.palette import PALETTE_RGB\n\nfrom config import config as cfg\nfrom utils.np_box_ops import area as np_area\nfrom utils.np_box_ops import iou as np_iou\nfrom common import polygons_to_mask\n\n\ndef draw_annotation(img, boxes, klass, polygons=None, is_crowd=None):\n    """"""Will not modify img""""""\n    labels = []\n    assert len(boxes) == len(klass)\n    if is_crowd is not None:\n        assert len(boxes) == len(is_crowd)\n        for cls, crd in zip(klass, is_crowd):\n            clsname = cfg.DATA.CLASS_NAMES[cls]\n            if crd == 1:\n                clsname += \';Crowd\'\n            labels.append(clsname)\n    else:\n        for cls in klass:\n            labels.append(cfg.DATA.CLASS_NAMES[cls])\n    img = viz.draw_boxes(img, boxes, labels)\n\n    if polygons is not None:\n        for p in polygons:\n            mask = polygons_to_mask(p, img.shape[0], img.shape[1])\n            img = draw_mask(img, mask)\n    return img\n\n\ndef draw_proposal_recall(img, proposals, proposal_scores, gt_boxes):\n    """"""\n    Draw top3 proposals for each gt.\n    Args:\n        proposals: NPx4\n        proposal_scores: NP\n        gt_boxes: NG\n    """"""\n    box_ious = np_iou(gt_boxes, proposals)    # ng x np\n    box_ious_argsort = np.argsort(-box_ious, axis=1)\n    good_proposals_ind = box_ious_argsort[:, :3]   # for each gt, find 3 best proposals\n    good_proposals_ind = np.unique(good_proposals_ind.ravel())\n\n    proposals = proposals[good_proposals_ind, :]\n    tags = list(map(str, proposal_scores[good_proposals_ind]))\n    img = viz.draw_boxes(img, proposals, tags)\n    return img, good_proposals_ind\n\n\ndef draw_predictions(img, boxes, scores):\n    """"""\n    Args:\n        boxes: kx4\n        scores: kxC\n    """"""\n    if len(boxes) == 0:\n        return img\n    labels = scores.argmax(axis=1)\n    scores = scores.max(axis=1)\n    tags = [""{},{:.2f}"".format(cfg.DATA.CLASS_NAMES[lb], score) for lb, score in zip(labels, scores)]\n    return viz.draw_boxes(img, boxes, tags)\n\n\ndef draw_final_outputs(img, results):\n    """"""\n    Args:\n        results: [DetectionResult]\n    """"""\n    if len(results) == 0:\n        return img\n\n    # Display in largest to smallest order to reduce occlusion\n    boxes = np.asarray([r.box for r in results])\n    areas = np_area(boxes)\n    sorted_inds = np.argsort(-areas)\n\n    ret = img\n    tags = []\n\n    for result_id in sorted_inds:\n        r = results[result_id]\n        if r.mask is not None:\n            ret = draw_mask(ret, r.mask)\n\n    for r in results:\n        tags.append(\n            ""{},{:.2f}"".format(cfg.DATA.CLASS_NAMES[r.class_id], r.score))\n    ret = viz.draw_boxes(ret, boxes, tags)\n    return ret\n\n\ndef draw_final_outputs_blackwhite(img, results):\n    """"""\n    Args:\n        results: [DetectionResult]\n    """"""\n    img_bw = img.mean(axis=2)\n    img_bw = np.stack([img_bw] * 3, axis=2)\n\n    if len(results) == 0:\n        return img_bw\n\n    boxes = np.asarray([r.box for r in results])\n\n    all_masks = [r.mask for r in results]\n    if all_masks[0] is not None:\n        m = all_masks[0] > 0\n        for m2 in all_masks[1:]:\n            m = m | (m2 > 0)\n        img_bw[m] = img[m]\n\n    tags = [""{},{:.2f}"".format(cfg.DATA.CLASS_NAMES[r.class_id], r.score) for r in results]\n    ret = viz.draw_boxes(img_bw, boxes, tags)\n    return ret\n\n\ndef draw_mask(im, mask, alpha=0.5, color=None):\n    """"""\n    Overlay a mask on top of the image.\n\n    Args:\n        im: a 3-channel uint8 image in BGR\n        mask: a binary 1-channel image of the same size\n        color: if None, will choose automatically\n    """"""\n    if color is None:\n        color = PALETTE_RGB[np.random.choice(len(PALETTE_RGB))][::-1]\n    color = np.asarray(color, dtype=np.float32)\n    im = np.where(np.repeat((mask > 0)[:, :, None], 3, axis=2),\n                  im * (1 - alpha) + color * alpha, im)\n    im = im.astype(\'uint8\')\n    return im\n'"
examples/GAN/BEGAN.py,29,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: BEGAN.py\n# Author: Yuxin Wu\n\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.utils.gpu import get_num_gpu\n\nimport DCGAN\nfrom GAN import GANModelDesc, GANTrainer\n\n""""""\nBoundary Equilibrium GAN.\nSee the docstring in DCGAN.py for usage.\n\nA pretrained model on CelebA is at http://models.tensorpack.com/#GAN\n""""""\n\n\nNH = 64\nNF = 64\nGAMMA = 0.5\n\n\nclass Model(GANModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, args.final_size, args.final_size, 3), tf.float32, \'input\')]\n\n    @auto_reuse_variable_scope\n    def decoder(self, z):\n        l = FullyConnected(\'fc\', z, NF * 8 * 8)\n        l = tf.reshape(l, [-1, 8, 8, NF])\n\n        with argscope(Conv2D, activation=tf.nn.elu, kernel_size=3, strides=1):\n            l = (LinearWrap(l)\n                 .Conv2D(\'conv1.1\', NF)\n                 .Conv2D(\'conv1.2\', NF)\n                 .tf.image.resize_nearest_neighbor([16, 16], align_corners=True)\n                 .Conv2D(\'conv2.1\', NF)\n                 .Conv2D(\'conv2.2\', NF)\n                 .tf.image.resize_nearest_neighbor([32, 32], align_corners=True)\n                 .Conv2D(\'conv3.1\', NF)\n                 .Conv2D(\'conv3.2\', NF)\n                 .tf.image.resize_nearest_neighbor([64, 64], align_corners=True)\n                 .Conv2D(\'conv4.1\', NF)\n                 .Conv2D(\'conv4.2\', NF)\n                 .Conv2D(\'conv4.3\', 3, activation=tf.identity)())\n        return l\n\n    @auto_reuse_variable_scope\n    def encoder(self, imgs):\n        with argscope(Conv2D, activation=tf.nn.elu, kernel_size=3, strides=1):\n            l = (LinearWrap(imgs)\n                 .Conv2D(\'conv1.1\', NF)\n                 .Conv2D(\'conv1.2\', NF)\n                 .Conv2D(\'conv1.3\', NF * 2)\n                 .AvgPooling(\'pool1\', 2)\n                 # 32\n                 .Conv2D(\'conv2.1\', NF * 2)\n                 .Conv2D(\'conv2.2\', NF * 3)\n                 .AvgPooling(\'pool2\', 2)\n                 # 16\n                 .Conv2D(\'conv3.1\', NF * 3)\n                 .Conv2D(\'conv3.2\', NF * 4)\n                 .AvgPooling(\'pool3\', 2)\n                 # 8\n                 .Conv2D(\'conv4.1\', NF * 4)\n                 .Conv2D(\'conv4.2\', NF * 4)\n\n                 .FullyConnected(\'fc\', NH)())\n        return l\n\n    def build_graph(self, image_pos):\n        image_pos = image_pos / 128.0 - 1\n\n        z = tf.random_uniform([args.batch, args.z_dim], minval=-1, maxval=1, name=\'z_train\')\n        z = tf.placeholder_with_default(z, [None, args.z_dim], name=\'z\')\n\n        def summary_image(name, x):\n            x = (x + 1.0) * 128.0\n            x = tf.clip_by_value(x, 0, 255)\n            tf.summary.image(name, tf.cast(x, tf.uint8), max_outputs=30)\n\n        with argscope([Conv2D, FullyConnected],\n                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n            with tf.variable_scope(\'gen\'):\n                image_gen = self.decoder(z)\n\n            with tf.variable_scope(\'discrim\'):\n                with tf.variable_scope(\'enc\'):\n                    hidden_pos = self.encoder(image_pos)\n                    hidden_neg = self.encoder(image_gen)\n\n                with tf.variable_scope(\'dec\'):\n                    recon_pos = self.decoder(hidden_pos)\n                    recon_neg = self.decoder(hidden_neg)\n\n        with tf.name_scope(\'viz\'):\n            summary_image(\'generated-samples\', image_gen)\n            summary_image(\'reconstruct-real\', recon_pos)\n            summary_image(\'reconstruct-fake\', recon_neg)\n\n        with tf.name_scope(\'losses\'):\n            L_pos = tf.reduce_mean(tf.abs(recon_pos - image_pos), name=\'loss_pos\')\n            L_neg = tf.reduce_mean(tf.abs(recon_neg - image_gen), name=\'loss_neg\')\n\n            eq = tf.subtract(GAMMA * L_pos, L_neg, name=\'equilibrium\')\n            measure = tf.add(L_pos, tf.abs(eq), name=\'measure\')\n\n            kt = tf.get_variable(\'kt\', dtype=tf.float32, initializer=0.0)\n\n            update_kt = kt.assign_add(1e-3 * eq)\n            with tf.control_dependencies([update_kt]):\n                self.d_loss = tf.subtract(L_pos, kt * L_neg, name=\'loss_D\')\n                self.g_loss = L_neg\n\n        add_moving_summary(L_pos, L_neg, eq, measure, self.d_loss)\n        tf.summary.scalar(\'kt\', kt)\n\n        self.collect_variables()\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=1e-4, trainable=False)\n        opt = tf.train.AdamOptimizer(lr, beta1=0.5, beta2=0.9)\n        return opt\n\n\nif __name__ == \'__main__\':\n    args = DCGAN.get_args(default_batch=32, default_z_dim=64)\n    if args.sample:\n        DCGAN.sample(Model(), args.load, \'gen/conv4.3/output\')\n    else:\n        logger.auto_set_dir()\n\n        input = QueueInput(DCGAN.get_data())\n        model = Model()\n        nr_tower = max(get_num_gpu(), 1)\n        trainer = GANTrainer(input, model, num_gpu=nr_tower)\n        trainer.train_with_defaults(\n            callbacks=[\n                ModelSaver(),\n                StatMonitorParamSetter(\n                    \'learning_rate\', \'losses/measure\', lambda x: x * 0.5, 0, 10)\n            ],\n            session_init=SmartInit(args.load),\n            steps_per_epoch=500, max_epoch=400)\n'"
examples/GAN/ConditionalGAN-mnist.py,29,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: ConditionalGAN-mnist.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.utils.viz import interactive_imshow, stack_patches\n\nfrom GAN import GANModelDesc, GANTrainer, RandomZData\n\n""""""\nTo train:\n    ./ConditionalGAN-mnist.py\n\nTo visualize:\n    ./ConditionalGAN-mnist.py --sample --load path/to/model\n\nA pretrained model is at http://models.tensorpack.com/GAN/\n""""""\n\nBATCH = 128\n\n\ndef batch_flatten(x):\n    """"""\n    Flatten the tensor except the first dimension.\n    """"""\n    shape = x.get_shape().as_list()[1:]\n    if None not in shape:\n        return tf.reshape(x, [-1, int(np.prod(shape))])\n    return tf.reshape(x, tf.stack([tf.shape(x)[0], -1]))\n\n\nclass Model(GANModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, 28, 28), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def generator(self, z, y):\n        l = FullyConnected(\'fc0\', tf.concat([z, y], 1), 1024, activation=BNReLU)\n        l = FullyConnected(\'fc1\', tf.concat([l, y], 1), 64 * 2 * 7 * 7, activation=BNReLU)\n        l = tf.reshape(l, [-1, 7, 7, 64 * 2])\n\n        y = tf.reshape(y, [-1, 1, 1, 10])\n        l = tf.concat([l, tf.tile(y, [1, 7, 7, 1])], 3)\n        l = Conv2DTranspose(\'deconv1\', l, 64 * 2, 5, 2, activation=BNReLU)\n\n        l = tf.concat([l, tf.tile(y, [1, 14, 14, 1])], 3)\n        l = Conv2DTranspose(\'deconv2\', l, 1, 5, 2, activation=tf.identity)\n        l = tf.nn.tanh(l, name=\'gen\')\n        return l\n\n    @auto_reuse_variable_scope\n    def discriminator(self, imgs, y):\n        """""" return a (b, 1) logits""""""\n        yv = y\n        y = tf.reshape(y, [-1, 1, 1, 10])\n        with argscope(Conv2D, kernel_size=5, strides=1):\n            l = (LinearWrap(imgs)\n                 .ConcatWith(tf.tile(y, [1, 28, 28, 1]), 3)\n                 .Conv2D(\'conv0\', 11)\n                 .tf.nn.leaky_relu()\n\n                 .ConcatWith(tf.tile(y, [1, 14, 14, 1]), 3)\n                 .Conv2D(\'conv1\', 74)\n                 .BatchNorm(\'bn1\')\n                 .tf.nn.leaky_relu()\n\n                 .apply(batch_flatten)\n                 .ConcatWith(yv, 1)\n                 .FullyConnected(\'fc1\', 1024, activation=tf.identity)\n                 .BatchNorm(\'bn2\')\n                 .tf.nn.leaky_relu()\n\n                 .ConcatWith(yv, 1)\n                 .FullyConnected(\'fct\', 1, activation=tf.identity)())\n        return l\n\n    def build_graph(self, image_pos, y):\n        image_pos = tf.expand_dims(image_pos * 2.0 - 1, -1)\n        y = tf.one_hot(y, 10, name=\'label_onehot\')\n\n        z = tf.random_uniform([BATCH, 100], -1, 1, name=\'z_train\')\n        z = tf.placeholder_with_default(z, [None, 100], name=\'z\')   # clear the static shape\n\n        with argscope([Conv2D, Conv2DTranspose, FullyConnected],\n                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n            with tf.variable_scope(\'gen\'):\n                image_gen = self.generator(z, y)\n                tf.summary.image(\'gen\', image_gen, 30)\n            with tf.variable_scope(\'discrim\'):\n                vecpos = self.discriminator(image_pos, y)\n                vecneg = self.discriminator(image_gen, y)\n\n        self.build_losses(vecpos, vecneg)\n        self.collect_variables()\n\n    def optimizer(self):\n        return tf.train.AdamOptimizer(2e-4, beta1=0.5, epsilon=1e-3)\n\n\ndef get_data():\n    ds = ConcatData([dataset.Mnist(\'train\'), dataset.Mnist(\'test\')])\n    return BatchData(ds, BATCH)\n\n\ndef sample(model_path):\n    pred = PredictConfig(\n        session_init=SmartInit(model_path),\n        model=Model(),\n        input_names=[\'label\', \'z\'],\n        output_names=[\'gen/gen\'])\n\n    ds = MapData(RandomZData((100, 100)),\n                 lambda dp: [np.arange(100) % 10, dp[0]])\n    pred = SimpleDatasetPredictor(pred, ds)\n    for o in pred.get_result():\n        o = o[0] * 255.0\n        viz = stack_patches(o, nr_row=10, nr_col=10)\n        viz = cv2.resize(viz, (800, 800))\n        interactive_imshow(viz)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--sample\', action=\'store_true\')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    if args.sample:\n        sample(args.load)\n    else:\n        logger.auto_set_dir()\n        GANTrainer(QueueInput(get_data()), Model()).train_with_defaults(\n            callbacks=[ModelSaver()],\n            steps_per_epoch=500,\n            max_epoch=100,\n            session_init=SmartInit(args.load),\n        )\n'"
examples/GAN/CycleGAN.py,43,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: CycleGAN.py\n# Author: Yuxin Wu\n\nimport argparse\nimport glob\nimport os\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\n\nfrom GAN import GANModelDesc, GANTrainer\n\n""""""\n1. Download the dataset following the original project: https://github.com/junyanz/CycleGAN#train\n2. ./CycleGAN.py --data /path/to/datasets/horse2zebra\nTraining and testing visualizations will be in tensorboard.\n\nThis implementation doesn\'t use fake sample buffer.\nIt\'s not hard to add but I didn\'t observe any difference with it.\n""""""\n\nSHAPE = 256\nBATCH = 1\nTEST_BATCH = 32\nNF = 64  # channel size\n\n\ndef INReLU(x, name=None):\n    x = InstanceNorm(\'inorm\', x)\n    return tf.nn.relu(x, name=name)\n\n\ndef INLReLU(x, name=None):\n    x = InstanceNorm(\'inorm\', x)\n    return tf.nn.leaky_relu(x, alpha=0.2, name=name)\n\n\nclass Model(GANModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, SHAPE, SHAPE, 3), tf.float32, \'inputA\'),\n                tf.TensorSpec((None, SHAPE, SHAPE, 3), tf.float32, \'inputB\')]\n\n    @staticmethod\n    def build_res_block(x, name, chan, first=False):\n        with tf.variable_scope(name):\n            input = x\n            return (LinearWrap(x)\n                    .tf.pad([[0, 0], [0, 0], [1, 1], [1, 1]], mode=\'SYMMETRIC\')\n                    .Conv2D(\'conv0\', chan, 3, padding=\'VALID\')\n                    .tf.pad([[0, 0], [0, 0], [1, 1], [1, 1]], mode=\'SYMMETRIC\')\n                    .Conv2D(\'conv1\', chan, 3, padding=\'VALID\', activation=tf.identity)\n                    .InstanceNorm(\'inorm\')()) + input\n\n    @auto_reuse_variable_scope\n    def generator(self, img):\n        assert img is not None\n        with argscope([Conv2D, Conv2DTranspose], activation=INReLU):\n            l = (LinearWrap(img)\n                 .tf.pad([[0, 0], [0, 0], [3, 3], [3, 3]], mode=\'SYMMETRIC\')\n                 .Conv2D(\'conv0\', NF, 7, padding=\'VALID\')\n                 .Conv2D(\'conv1\', NF * 2, 3, strides=2)\n                 .Conv2D(\'conv2\', NF * 4, 3, strides=2)())\n            for k in range(9):\n                l = Model.build_res_block(l, \'res{}\'.format(k), NF * 4, first=(k == 0))\n            l = (LinearWrap(l)\n                 .Conv2DTranspose(\'deconv0\', NF * 2, 3, strides=2)\n                 .Conv2DTranspose(\'deconv1\', NF * 1, 3, strides=2)\n                 .tf.pad([[0, 0], [0, 0], [3, 3], [3, 3]], mode=\'SYMMETRIC\')\n                 .Conv2D(\'convlast\', 3, 7, padding=\'VALID\', activation=tf.tanh, use_bias=True)())\n        return l\n\n    @auto_reuse_variable_scope\n    def discriminator(self, img):\n        with argscope(Conv2D, activation=INLReLU, kernel_size=4, strides=2):\n            l = (LinearWrap(img)\n                 .Conv2D(\'conv0\', NF, activation=tf.nn.leaky_relu)\n                 .Conv2D(\'conv1\', NF * 2)\n                 .Conv2D(\'conv2\', NF * 4)\n                 .Conv2D(\'conv3\', NF * 8, strides=1)\n                 .Conv2D(\'conv4\', 1, strides=1, activation=tf.identity, use_bias=True)())\n        return l\n\n    def build_graph(self, A, B):\n        with tf.name_scope(\'preprocess\'):\n            A = tf.transpose(A / 128.0 - 1.0, [0, 3, 1, 2])\n            B = tf.transpose(B / 128.0 - 1.0, [0, 3, 1, 2])\n\n        def viz3(name, a, b, c):\n            with tf.name_scope(name):\n                im = tf.concat([a, b, c], axis=3)\n                im = tf.transpose(im, [0, 2, 3, 1])\n                im = (im + 1.0) * 128\n                im = tf.clip_by_value(im, 0, 255)\n                im = tf.cast(im, tf.uint8, name=\'viz\')\n            tf.summary.image(name, im, max_outputs=50)\n\n        # use the initializers from torch\n        with argscope([Conv2D, Conv2DTranspose], use_bias=False,\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.02)), \\\n                argscope([Conv2D, Conv2DTranspose, InstanceNorm], data_format=\'channels_first\'):\n            with tf.variable_scope(\'gen\'):\n                with tf.variable_scope(\'B\'):\n                    AB = self.generator(A)\n                with tf.variable_scope(\'A\'):\n                    BA = self.generator(B)\n                    ABA = self.generator(AB)\n                with tf.variable_scope(\'B\'):\n                    BAB = self.generator(BA)\n\n            viz3(\'A_recon\', A, AB, ABA)\n            viz3(\'B_recon\', B, BA, BAB)\n\n            with tf.variable_scope(\'discrim\'):\n                with tf.variable_scope(\'A\'):\n                    A_dis_real = self.discriminator(A)\n                    A_dis_fake = self.discriminator(BA)\n\n                with tf.variable_scope(\'B\'):\n                    B_dis_real = self.discriminator(B)\n                    B_dis_fake = self.discriminator(AB)\n\n        def LSGAN_losses(real, fake):\n            d_real = tf.reduce_mean(tf.squared_difference(real, 1), name=\'d_real\')\n            d_fake = tf.reduce_mean(tf.square(fake), name=\'d_fake\')\n            d_loss = tf.multiply(d_real + d_fake, 0.5, name=\'d_loss\')\n\n            g_loss = tf.reduce_mean(tf.squared_difference(fake, 1), name=\'g_loss\')\n            add_moving_summary(g_loss, d_loss)\n            return g_loss, d_loss\n\n        with tf.name_scope(\'losses\'):\n            with tf.name_scope(\'LossA\'):\n                # reconstruction loss\n                recon_loss_A = tf.reduce_mean(tf.abs(A - ABA), name=\'recon_loss\')\n                # gan loss\n                G_loss_A, D_loss_A = LSGAN_losses(A_dis_real, A_dis_fake)\n\n            with tf.name_scope(\'LossB\'):\n                recon_loss_B = tf.reduce_mean(tf.abs(B - BAB), name=\'recon_loss\')\n                G_loss_B, D_loss_B = LSGAN_losses(B_dis_real, B_dis_fake)\n\n            LAMBDA = 10.0\n            self.g_loss = tf.add((G_loss_A + G_loss_B),\n                                 (recon_loss_A + recon_loss_B) * LAMBDA, name=\'G_loss_total\')\n            self.d_loss = tf.add(D_loss_A, D_loss_B, name=\'D_loss_total\')\n        self.collect_variables(\'gen\', \'discrim\')\n\n        add_moving_summary(recon_loss_A, recon_loss_B, self.g_loss, self.d_loss)\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=2e-4, trainable=False)\n        return tf.train.AdamOptimizer(lr, beta1=0.5, epsilon=1e-3)\n\n\ndef get_data(datadir, isTrain=True):\n    if isTrain:\n        augs = [\n            imgaug.Resize(int(SHAPE * 1.12)),\n            imgaug.RandomCrop(SHAPE),\n            imgaug.Flip(horiz=True),\n        ]\n    else:\n        augs = [imgaug.Resize(SHAPE)]\n\n    def get_image_pairs(dir1, dir2):\n        def get_df(dir):\n            files = sorted(glob.glob(os.path.join(dir, \'*.jpg\')))\n            df = ImageFromFile(files, channel=3, shuffle=isTrain)\n            return AugmentImageComponent(df, augs)\n        return JoinData([get_df(dir1), get_df(dir2)])\n\n    names = [\'trainA\', \'trainB\'] if isTrain else [\'testA\', \'testB\']\n    df = get_image_pairs(*[os.path.join(datadir, n) for n in names])\n    df = BatchData(df, BATCH if isTrain else TEST_BATCH)\n    df = MultiProcessRunnerZMQ(df, 2 if isTrain else 1)\n    return df\n\n\nclass VisualizeTestSet(Callback):\n    def _setup_graph(self):\n        self.pred = self.trainer.get_predictor(\n            [\'inputA\', \'inputB\'], [\'A_recon/viz\', \'B_recon/viz\'])\n\n    def _before_train(self):\n        global args\n        self.val_ds = get_data(args.data, isTrain=False)\n        self.val_ds.reset_state()\n\n    def _trigger(self):\n        idx = 0\n        for iA, iB in self.val_ds:\n            vizA, vizB = self.pred(iA, iB)\n            self.trainer.monitors.put_image(\'testA-{}\'.format(idx), vizA)\n            self.trainer.monitors.put_image(\'testB-{}\'.format(idx), vizB)\n            idx += 1\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--data\', required=True,\n        help=\'the image directory. should contain trainA/trainB/testA/testB\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    args = parser.parse_args()\n\n    logger.auto_set_dir()\n\n    df = get_data(args.data)\n    df = PrintData(df)\n    data = QueueInput(df)\n\n    GANTrainer(data, Model()).train_with_defaults(\n        callbacks=[\n            ModelSaver(),\n            ScheduledHyperParamSetter(\n                \'learning_rate\',\n                [(100, 2e-4), (200, 0)], interp=\'linear\'),\n            PeriodicTrigger(VisualizeTestSet(), every_k_epochs=3),\n        ],\n        max_epoch=195,\n        steps_per_epoch=data.size(),\n        session_init=SmartInit(args.load)\n    )\n'"
examples/GAN/DCGAN.py,17,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: DCGAN.py\n# Author: Yuxin Wu\n\nimport argparse\nimport glob\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.utils.viz import stack_patches\n\nfrom GAN import GANModelDesc, GANTrainer, RandomZData\n\n\n""""""\n1. Download the \'aligned&cropped\' version of CelebA dataset\n   from http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\n2. Start training:\n    ./DCGAN-CelebA.py --data /path/to/img_align_celeba/ --crop-size 140\n    Generated samples will be available through tensorboard\n\n3. Visualize samples with an existing model:\n    ./DCGAN-CelebA.py --load path/to/model --sample\n\nYou can also train on other images (just use any directory of jpg files in\n`--data`). But you may need to change the preprocessing.\n\nA pretrained model on CelebA is at http://models.tensorpack.com/#GAN\n""""""\n\n\nclass Model(GANModelDesc):\n    def __init__(self, shape, batch, z_dim):\n        self.shape = shape\n        self.batch = batch\n        self.zdim = z_dim\n\n    def inputs(self):\n        return [tf.TensorSpec((None, self.shape, self.shape, 3), tf.float32, \'input\')]\n\n    def generator(self, z):\n        """""" return an image generated from z""""""\n        nf = 64\n        l = FullyConnected(\'fc0\', z, nf * 8 * 4 * 4, activation=tf.identity)\n        l = tf.reshape(l, [-1, 4, 4, nf * 8])\n        l = BNReLU(l)\n        with argscope(Conv2DTranspose, activation=BNReLU, kernel_size=4, strides=2):\n            l = Conv2DTranspose(\'deconv1\', l, nf * 4)\n            l = Conv2DTranspose(\'deconv2\', l, nf * 2)\n            l = Conv2DTranspose(\'deconv3\', l, nf)\n            l = Conv2DTranspose(\'deconv4\', l, 3, activation=tf.identity)\n            l = tf.tanh(l, name=\'gen\')\n        return l\n\n    @auto_reuse_variable_scope\n    def discriminator(self, imgs):\n        """""" return a (b, 1) logits""""""\n        nf = 64\n        with argscope(Conv2D, kernel_size=4, strides=2):\n            l = (LinearWrap(imgs)\n                 .Conv2D(\'conv0\', nf, activation=tf.nn.leaky_relu)\n                 .Conv2D(\'conv1\', nf * 2)\n                 .BatchNorm(\'bn1\')\n                 .tf.nn.leaky_relu()\n                 .Conv2D(\'conv2\', nf * 4)\n                 .BatchNorm(\'bn2\')\n                 .tf.nn.leaky_relu()\n                 .Conv2D(\'conv3\', nf * 8)\n                 .BatchNorm(\'bn3\')\n                 .tf.nn.leaky_relu()\n                 .FullyConnected(\'fct\', 1)())\n        return l\n\n    def build_graph(self, image_pos):\n        image_pos = image_pos / 128.0 - 1\n\n        z = tf.random_uniform([self.batch, self.zdim], -1, 1, name=\'z_train\')\n        z = tf.placeholder_with_default(z, [None, self.zdim], name=\'z\')\n\n        with argscope([Conv2D, Conv2DTranspose, FullyConnected],\n                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n            with tf.variable_scope(\'gen\'):\n                image_gen = self.generator(z)\n            tf.summary.image(\'generated-samples\', image_gen, max_outputs=30)\n            with tf.variable_scope(\'discrim\'):\n                vecpos = self.discriminator(image_pos)\n                vecneg = self.discriminator(image_gen)\n\n        self.build_losses(vecpos, vecneg)\n        self.collect_variables()\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=2e-4, trainable=False)\n        return tf.train.AdamOptimizer(lr, beta1=0.5, epsilon=1e-3)\n\n\ndef get_augmentors():\n    augs = []\n    if args.load_size:\n        augs.append(imgaug.Resize(args.load_size))\n    if args.crop_size:\n        augs.append(imgaug.CenterCrop(args.crop_size))\n    augs.append(imgaug.Resize(args.final_size))\n    return augs\n\n\ndef get_data():\n    assert args.data\n    imgs = glob.glob(args.data + \'/*.jpg\')\n    ds = ImageFromFile(imgs, channel=3, shuffle=True)\n    ds = AugmentImageComponent(ds, get_augmentors())\n    ds = BatchData(ds, args.batch)\n    ds = MultiProcessRunnerZMQ(ds, 5)\n    return ds\n\n\ndef sample(model, model_path, output_name=\'gen/gen\'):\n    pred = PredictConfig(\n        session_init=SmartInit(model_path),\n        model=model,\n        input_names=[\'z\'],\n        output_names=[output_name, \'z\'])\n    pred = SimpleDatasetPredictor(pred, RandomZData((100, args.z_dim)))\n    for o in pred.get_result():\n        o = o[0] + 1\n        o = o * 128.0\n        o = np.clip(o, 0, 255)\n        o = o[:, :, :, ::-1]\n        stack_patches(o, nr_row=10, nr_col=10, viz=True)\n\n\ndef get_args(default_batch=128, default_z_dim=100):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--sample\', action=\'store_true\', help=\'view generated examples\')\n    parser.add_argument(\'--data\', help=\'a jpeg directory\')\n    parser.add_argument(\'--load-size\', help=\'size to load the original images\', type=int)\n    parser.add_argument(\'--crop-size\', help=\'crop the original images\', type=int)\n    parser.add_argument(\n        \'--final-size\', default=64, type=int,\n        help=\'resize to this shape as inputs to network\')\n    parser.add_argument(\'--z-dim\', help=\'hidden dimension\', type=int, default=default_z_dim)\n    parser.add_argument(\'--batch\', help=\'batch size\', type=int, default=default_batch)\n    global args\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    return args\n\n\nif __name__ == \'__main__\':\n    args = get_args()\n    M = Model(shape=args.final_size, batch=args.batch, z_dim=args.z_dim)\n    if args.sample:\n        sample(M, args.load)\n    else:\n        logger.auto_set_dir()\n        GANTrainer(\n            input=QueueInput(get_data()),\n            model=M).train_with_defaults(\n            callbacks=[ModelSaver()],\n            steps_per_epoch=300,\n            max_epoch=200,\n            session_init=SmartInit(args.load),\n        )\n'"
examples/GAN/DiscoGAN-CelebA.py,37,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: DiscoGAN-CelebA.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\n\nfrom GAN import GANModelDesc, SeparateGANTrainer\n\n""""""\n1. Download ""aligned&cropped"" version of celebA to /path/to/img_align_celeba.\n2. Put list_attr_celeba.txt into that directory as well.\n3. Start training gender transfer:\n    ./DiscoGAN-CelebA.py --data /path/to/img_align_celeba --style-A Male\n4. Visualize the gender conversion images in tensorboard.\n""""""\n\nSHAPE = 64\nBATCH = 64\nNF = 64  # channel size\n\n\ndef BNLReLU(x, name=None):\n    x = BatchNorm(\'bn\', x)\n    return tf.nn.leaky_relu(x, alpha=0.2, name=name)\n\n\nclass Model(GANModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, SHAPE, SHAPE, 3), tf.float32, \'inputA\'),\n                tf.TensorSpec((None, SHAPE, SHAPE, 3), tf.float32, \'inputB\')]\n\n    @auto_reuse_variable_scope\n    def generator(self, img):\n        assert img is not None\n        with argscope([Conv2D, Conv2DTranspose],\n                      activation=BNLReLU, kernel_size=4, strides=2), \\\n                argscope(Conv2DTranspose, activation=BNReLU):\n            l = (LinearWrap(img)\n                 .Conv2D(\'conv0\', NF, activation=tf.nn.leaky_relu)\n                 .Conv2D(\'conv1\', NF * 2)\n                 .Conv2D(\'conv2\', NF * 4)\n                 .Conv2D(\'conv3\', NF * 8)\n                 .Conv2DTranspose(\'deconv0\', NF * 4)\n                 .Conv2DTranspose(\'deconv1\', NF * 2)\n                 .Conv2DTranspose(\'deconv2\', NF * 1)\n                 .Conv2DTranspose(\'deconv3\', 3, activation=tf.identity)\n                 .tf.sigmoid()())\n        return l\n\n    @auto_reuse_variable_scope\n    def discriminator(self, img):\n        with argscope(Conv2D, activation=BNLReLU, kernel_size=4, strides=2):\n            l = Conv2D(\'conv0\', img, NF, activation=tf.nn.leaky_relu)\n            relu1 = Conv2D(\'conv1\', l, NF * 2)\n            relu2 = Conv2D(\'conv2\', relu1, NF * 4)\n            relu3 = Conv2D(\'conv3\', relu2, NF * 8)\n            logits = FullyConnected(\'fc\', relu3, 1, activation=tf.identity)\n        return logits, [relu1, relu2, relu3]\n\n    def get_feature_match_loss(self, feats_real, feats_fake):\n        losses = []\n        for real, fake in zip(feats_real, feats_fake):\n            loss = tf.reduce_mean(tf.squared_difference(\n                tf.reduce_mean(real, 0),\n                tf.reduce_mean(fake, 0)),\n                name=\'mse_feat_\' + real.op.name)\n            losses.append(loss)\n        ret = tf.add_n(losses, name=\'feature_match_loss\')\n        add_moving_summary(ret)\n        return ret\n\n    def build_graph(self, A, B):\n        A = tf.transpose(A / 255.0, [0, 3, 1, 2])\n        B = tf.transpose(B / 255.0, [0, 3, 1, 2])\n\n        # use the torch initializers\n        with argscope([Conv2D, Conv2DTranspose, FullyConnected],\n                      kernel_initializer=tf.variance_scaling_initializer(scale=0.333, distribution=\'uniform\'),\n                      use_bias=False), \\\n                argscope(BatchNorm, gamma_init=tf.random_uniform_initializer()), \\\n                argscope([Conv2D, Conv2DTranspose, BatchNorm], data_format=\'NCHW\'):\n            with tf.variable_scope(\'gen\'):\n                with tf.variable_scope(\'B\'):\n                    AB = self.generator(A)\n                with tf.variable_scope(\'A\'):\n                    BA = self.generator(B)\n                    ABA = self.generator(AB)\n                with tf.variable_scope(\'B\'):\n                    BAB = self.generator(BA)\n\n            viz_A_recon = tf.concat([A, AB, ABA], axis=3, name=\'viz_A_recon\')\n            viz_B_recon = tf.concat([B, BA, BAB], axis=3, name=\'viz_B_recon\')\n            tf.summary.image(\'Arecon\', tf.transpose(viz_A_recon, [0, 2, 3, 1]), max_outputs=50)\n            tf.summary.image(\'Brecon\', tf.transpose(viz_B_recon, [0, 2, 3, 1]), max_outputs=50)\n\n            with tf.variable_scope(\'discrim\'):\n                with tf.variable_scope(\'A\'):\n                    A_dis_real, A_feats_real = self.discriminator(A)\n                    A_dis_fake, A_feats_fake = self.discriminator(BA)\n\n                with tf.variable_scope(\'B\'):\n                    B_dis_real, B_feats_real = self.discriminator(B)\n                    B_dis_fake, B_feats_fake = self.discriminator(AB)\n\n        with tf.name_scope(\'LossA\'):\n            # reconstruction loss\n            recon_loss_A = tf.reduce_mean(tf.squared_difference(A, ABA), name=\'recon_loss\')\n            # gan loss\n            self.build_losses(A_dis_real, A_dis_fake)\n            G_loss_A = self.g_loss\n            D_loss_A = self.d_loss\n            # feature matching loss\n            fm_loss_A = self.get_feature_match_loss(A_feats_real, A_feats_fake)\n\n        with tf.name_scope(\'LossB\'):\n            recon_loss_B = tf.reduce_mean(tf.squared_difference(B, BAB), name=\'recon_loss\')\n            self.build_losses(B_dis_real, B_dis_fake)\n            G_loss_B = self.g_loss\n            D_loss_B = self.d_loss\n            fm_loss_B = self.get_feature_match_loss(B_feats_real, B_feats_fake)\n\n        global_step = get_global_step_var()\n        rate = tf.train.piecewise_constant(global_step, [np.int64(10000)], [0.01, 0.5])\n        rate = tf.identity(rate, name=\'rate\')   # TF issue#8594\n        g_loss = tf.add_n([\n            ((G_loss_A + G_loss_B) * 0.1 +\n             (fm_loss_A + fm_loss_B) * 0.9) * (1 - rate),\n            (recon_loss_A + recon_loss_B) * rate], name=\'G_loss_total\')\n        d_loss = tf.add_n([D_loss_A, D_loss_B], name=\'D_loss_total\')\n\n        self.collect_variables(\'gen\', \'discrim\')\n        # weight decay\n        wd_g = regularize_cost(\'gen/.*/W\', l2_regularizer(1e-5), name=\'G_regularize\')\n        wd_d = regularize_cost(\'discrim/.*/W\', l2_regularizer(1e-5), name=\'D_regularize\')\n\n        self.g_loss = g_loss + wd_g\n        self.d_loss = d_loss + wd_d\n\n        add_moving_summary(recon_loss_A, recon_loss_B, rate, g_loss, d_loss, wd_g, wd_d)\n\n    def optimizer(self):\n        return tf.train.AdamOptimizer(2e-4, beta1=0.5, epsilon=1e-3)\n\n\ndef get_celebA_data(datadir, styleA, styleB=None):\n    def read_attr(attrfname):\n        with open(attrfname) as f:\n            nr_record = int(f.readline())\n            headers = f.readline().strip().split()\n            data = []\n            for line in f:\n                line = line.strip().split()[1:]\n                line = list(map(int, line))\n                assert len(line) == len(headers)\n                data.append(line)\n            assert len(data) == nr_record\n            return headers, np.asarray(data, dtype=\'int8\')\n\n    headers, attrs = read_attr(os.path.join(datadir, \'list_attr_celeba.txt\'))\n    idxA = headers.index(styleA)\n    listA = np.nonzero(attrs[:, idxA] == 1)[0]\n    if styleB is not None:\n        idxB = headers.index(styleB)\n        listB = np.nonzero(attrs[:, idxB] == 1)[0]\n    else:\n        listB = np.nonzero(attrs[:, idxA] == -1)[0]\n\n    def get_filelist(idxlist):\n        return [os.path.join(datadir, \'{:06d}.jpg\'.format(x + 1))\n                for x in idxlist]\n\n    dfA = ImageFromFile(get_filelist(listA), channel=3, shuffle=True)\n    dfB = ImageFromFile(get_filelist(listB), channel=3, shuffle=True)\n    df = JoinData([dfA, dfB])\n    augs = [\n        imgaug.CenterCrop(160),\n        imgaug.Resize(64)]\n    df = AugmentImageComponents(df, augs, (0, 1))\n    df = BatchData(df, BATCH)\n    df = MultiProcessRunnerZMQ(df, 3)\n    return df\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--data\', required=True,\n        help=\'the img_align_celeba directory. should also contain list_attr_celeba.txt\')\n    parser.add_argument(\'--style-A\', help=\'style of A\', default=\'Male\')\n    parser.add_argument(\'--style-B\', help=\'style of B, default to ""not A""\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    args = parser.parse_args()\n\n    assert tf.test.is_gpu_available()\n    logger.auto_set_dir()\n\n    data = get_celebA_data(args.data, args.style_A, args.style_B)\n\n    # train 1 D after 2 G\n    SeparateGANTrainer(\n        QueueInput(data), Model(), d_period=3).train_with_defaults(\n        callbacks=[ModelSaver()],\n        steps_per_epoch=300,\n        max_epoch=250,\n        session_init=SmartInit(args.load),\n    )\n'"
examples/GAN/GAN.py,30,"b'# -*- coding: utf-8 -*-\n# File: GAN.py\n# Author: Yuxin Wu\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorpack import BatchNorm, DataFlow, ModelDescBase, StagingInput, TowerTrainer, argscope\nfrom tensorpack.graph_builder import DataParallelBuilder, LeastLoadedDeviceSetter\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.tfutils.tower import TowerContext, TowerFunc\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.argtools import memoized_method\n\n\nclass GANModelDesc(ModelDescBase):\n    def collect_variables(self, g_scope=\'gen\', d_scope=\'discrim\'):\n        """"""\n        Assign `self.g_vars` to the parameters under scope `g_scope`,\n        and same with `self.d_vars`.\n        """"""\n        self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, g_scope)\n        assert self.g_vars\n        self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, d_scope)\n        assert self.d_vars\n\n    def build_losses(self, logits_real, logits_fake):\n        """"""\n        Build standard GAN loss and set `self.g_loss` and `self.d_loss`.\n\n        D and G play two-player minimax game with value function V(G,D)\n\n          min_G max _D V(D, G) = IE_{x ~ p_data} [log D(x)] + IE_{z ~ p_fake} [log (1 - D(G(z)))]\n\n        Args:\n            logits_real (tf.Tensor): discrim logits from real samples\n            logits_fake (tf.Tensor): discrim logits from fake samples produced by generator\n        """"""\n        with tf.name_scope(""GAN_loss""):\n            score_real = tf.sigmoid(logits_real)\n            score_fake = tf.sigmoid(logits_fake)\n            tf.summary.histogram(\'score-real\', score_real)\n            tf.summary.histogram(\'score-fake\', score_fake)\n\n            with tf.name_scope(""discrim""):\n                d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_real, labels=tf.ones_like(logits_real)), name=\'loss_real\')\n                d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_fake, labels=tf.zeros_like(logits_fake)), name=\'loss_fake\')\n\n                d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name=\'accuracy_real\')\n                d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name=\'accuracy_fake\')\n\n                d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name=\'accuracy\')\n                self.d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name=\'loss\')\n\n            with tf.name_scope(""gen""):\n                self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_fake, labels=tf.ones_like(logits_fake)), name=\'loss\')\n                g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name=\'accuracy\')\n\n            add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)\n\n    def build_graph(self, *inputs):\n        """"""\n        Have to build one tower and set the following attributes:\n        g_loss, d_loss, g_vars, d_vars.\n        """"""\n        pass\n\n    @memoized_method\n    def get_optimizer(self):\n        return self.optimizer()\n\n\nclass GANTrainer(TowerTrainer):\n\n    def __init__(self, input, model, num_gpu=1):\n        """"""\n        Args:\n            input (InputSource):\n            model (GANModelDesc):\n        """"""\n        super(GANTrainer, self).__init__()\n        assert isinstance(model, GANModelDesc), model\n\n        if num_gpu > 1:\n            input = StagingInput(input)\n\n        # Setup input\n        cbs = input.setup(model.get_input_signature())\n        self.register_callback(cbs)\n\n        if num_gpu <= 1:\n            self._build_gan_trainer(input, model)\n        else:\n            self._build_multigpu_gan_trainer(input, model, num_gpu)\n\n    def _build_gan_trainer(self, input, model):\n        """"""\n        We need to set tower_func because it\'s a TowerTrainer,\n        and only TowerTrainer supports automatic graph creation for inference during training.\n\n        If we don\'t care about inference during training, using tower_func is\n        not needed. Just calling model.build_graph directly is OK.\n        """"""\n        # Build the graph\n        self.tower_func = TowerFunc(model.build_graph, model.inputs())\n        with TowerContext(\'\', is_training=True):\n            self.tower_func(*input.get_input_tensors())\n        opt = model.get_optimizer()\n\n        # Define the training iteration\n        # by default, run one d_min after one g_min\n        with tf.name_scope(\'optimize\'):\n            g_min = opt.minimize(model.g_loss, var_list=model.g_vars, name=\'g_op\')\n            with tf.control_dependencies([g_min]):\n                d_min = opt.minimize(model.d_loss, var_list=model.d_vars, name=\'d_op\')\n        self.train_op = d_min\n\n    def _build_multigpu_gan_trainer(self, input, model, num_gpu):\n        assert num_gpu > 1\n        raw_devices = [\'/gpu:{}\'.format(k) for k in range(num_gpu)]\n\n        # Build the graph with multi-gpu replication\n        def get_cost(*inputs):\n            model.build_graph(*inputs)\n            return [model.d_loss, model.g_loss]\n\n        self.tower_func = TowerFunc(get_cost, model.get_input_signature())\n        devices = [LeastLoadedDeviceSetter(d, raw_devices) for d in raw_devices]\n        cost_list = DataParallelBuilder.call_for_each_tower(\n            list(range(num_gpu)),\n            lambda: self.tower_func(*input.get_input_tensors()),\n            devices)\n        # For simplicity, average the cost here. It might be faster to average the gradients\n        with tf.name_scope(\'optimize\'):\n            d_loss = tf.add_n([x[0] for x in cost_list]) * (1.0 / num_gpu)\n            g_loss = tf.add_n([x[1] for x in cost_list]) * (1.0 / num_gpu)\n\n            opt = model.get_optimizer()\n            # run one d_min after one g_min\n            g_min = opt.minimize(g_loss, var_list=model.g_vars,\n                                 colocate_gradients_with_ops=True, name=\'g_op\')\n            with tf.control_dependencies([g_min]):\n                d_min = opt.minimize(d_loss, var_list=model.d_vars,\n                                     colocate_gradients_with_ops=True, name=\'d_op\')\n        # Define the training iteration\n        self.train_op = d_min\n\n\nclass SeparateGANTrainer(TowerTrainer):\n    """""" A GAN trainer which runs two optimization ops with a certain ratio.""""""\n    def __init__(self, input, model, d_period=1, g_period=1):\n        """"""\n        Args:\n            d_period(int): period of each d_opt run\n            g_period(int): period of each g_opt run\n        """"""\n        super(SeparateGANTrainer, self).__init__()\n        self._d_period = int(d_period)\n        self._g_period = int(g_period)\n        assert min(d_period, g_period) == 1\n\n        # Setup input\n        cbs = input.setup(model.get_input_signature())\n        self.register_callback(cbs)\n\n        # Build the graph\n        self.tower_func = TowerFunc(model.build_graph, model.inputs())\n        with TowerContext(\'\', is_training=True), \\\n                argscope(BatchNorm, ema_update=\'internal\'):\n            # should not hook the EMA updates to both train_op, it will hurt training speed.\n            self.tower_func(*input.get_input_tensors())\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        if len(update_ops):\n            logger.warn(""Found {} ops in UPDATE_OPS collection!"".format(len(update_ops)))\n            logger.warn(""Using SeparateGANTrainer with UPDATE_OPS may hurt your training speed a lot!"")\n\n        opt = model.get_optimizer()\n        with tf.name_scope(\'optimize\'):\n            self.d_min = opt.minimize(\n                model.d_loss, var_list=model.d_vars, name=\'d_min\')\n            self.g_min = opt.minimize(\n                model.g_loss, var_list=model.g_vars, name=\'g_min\')\n\n    def run_step(self):\n        # Define the training iteration\n        if self.global_step % (self._d_period) == 0:\n            self.hooked_sess.run(self.d_min)\n        if self.global_step % (self._g_period) == 0:\n            self.hooked_sess.run(self.g_min)\n\n\nclass RandomZData(DataFlow):\n    def __init__(self, shape):\n        super(RandomZData, self).__init__()\n        self.shape = shape\n\n    def __iter__(self):\n        while True:\n            yield [np.random.uniform(-1, 1, size=self.shape)]\n'"
examples/GAN/Image2Image.py,21,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: Image2Image.py\n# Author: Yuxin Wu\n\nimport argparse\nimport functools\nimport glob\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.utils.gpu import get_num_gpu\nfrom tensorpack.utils.viz import stack_patches\n\nfrom GAN import GANModelDesc, GANTrainer\n\n""""""\nTo train Image-to-Image translation model with image pairs:\n    ./Image2Image.py --data /path/to/datadir --mode {AtoB,BtoA}\n    # datadir should contain jpg images of shpae 2s x s, formed by A and B\n    # you can download some data from the original authors:\n    # https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/\n\nTraining visualization will appear be in tensorboard.\nTo visualize on test set:\n    ./Image2Image.py --sample --data /path/to/test/datadir --mode {AtoB,BtoA} --load model\n\n""""""\n\nBATCH = 1\nIN_CH = 3\nOUT_CH = 3\nLAMBDA = 100\nNF = 64  # number of filter\n\n\ndef BNLReLU(x, name=None):\n    x = BatchNorm(\'bn\', x)\n    return tf.nn.leaky_relu(x, alpha=0.2, name=name)\n\n\ndef visualize_tensors(name, imgs, scale_func=lambda x: (x + 1.) * 128., max_outputs=1):\n    """"""Generate tensor for TensorBoard (casting, clipping)\n\n    Args:\n        name: name for visualization operation\n        *imgs: multiple tensors as list\n        scale_func: scale input tensors to fit range [0, 255]\n\n    Example:\n        visualize_tensors(\'viz1\', [img1])\n        visualize_tensors(\'viz2\', [img1, img2, img3], max_outputs=max(30, BATCH))\n    """"""\n    xy = scale_func(tf.concat(imgs, axis=2))\n    xy = tf.cast(tf.clip_by_value(xy, 0, 255), tf.uint8, name=\'viz\')\n    tf.summary.image(name, xy, max_outputs=30)\n\n\nclass Model(GANModelDesc):\n    def inputs(self):\n        SHAPE = 256\n        return [tf.TensorSpec((None, SHAPE, SHAPE, IN_CH), tf.float32, \'input\'),\n                tf.TensorSpec((None, SHAPE, SHAPE, OUT_CH), tf.float32, \'output\')]\n\n    def generator(self, imgs):\n        # imgs: input: 256x256xch\n        # U-Net structure, it\'s slightly different from the original on the location of relu/lrelu\n        with argscope(BatchNorm, training=True), \\\n                argscope(Dropout, is_training=True):\n            # always use local stat for BN, and apply dropout even in testing\n            with argscope(Conv2D, kernel_size=4, strides=2, activation=BNLReLU):\n                e1 = Conv2D(\'conv1\', imgs, NF, activation=tf.nn.leaky_relu)\n                e2 = Conv2D(\'conv2\', e1, NF * 2)\n                e3 = Conv2D(\'conv3\', e2, NF * 4)\n                e4 = Conv2D(\'conv4\', e3, NF * 8)\n                e5 = Conv2D(\'conv5\', e4, NF * 8)\n                e6 = Conv2D(\'conv6\', e5, NF * 8)\n                e7 = Conv2D(\'conv7\', e6, NF * 8)\n                e8 = Conv2D(\'conv8\', e7, NF * 8, activation=BNReLU)  # 1x1\n            with argscope(Conv2DTranspose, activation=BNReLU, kernel_size=4, strides=2):\n                return (LinearWrap(e8)\n                        .Conv2DTranspose(\'deconv1\', NF * 8)\n                        .Dropout()\n                        .ConcatWith(e7, 3)\n                        .Conv2DTranspose(\'deconv2\', NF * 8)\n                        .Dropout()\n                        .ConcatWith(e6, 3)\n                        .Conv2DTranspose(\'deconv3\', NF * 8)\n                        .Dropout()\n                        .ConcatWith(e5, 3)\n                        .Conv2DTranspose(\'deconv4\', NF * 8)\n                        .ConcatWith(e4, 3)\n                        .Conv2DTranspose(\'deconv5\', NF * 4)\n                        .ConcatWith(e3, 3)\n                        .Conv2DTranspose(\'deconv6\', NF * 2)\n                        .ConcatWith(e2, 3)\n                        .Conv2DTranspose(\'deconv7\', NF * 1)\n                        .ConcatWith(e1, 3)\n                        .Conv2DTranspose(\'deconv8\', OUT_CH, activation=tf.tanh)())\n\n    @auto_reuse_variable_scope\n    def discriminator(self, inputs, outputs):\n        """""" return a (b, 1) logits""""""\n        l = tf.concat([inputs, outputs], 3)\n        with argscope(Conv2D, kernel_size=4, strides=2, activation=BNLReLU):\n            l = (LinearWrap(l)\n                 .Conv2D(\'conv0\', NF, activation=tf.nn.leaky_relu)\n                 .Conv2D(\'conv1\', NF * 2)\n                 .Conv2D(\'conv2\', NF * 4)\n                 .Conv2D(\'conv3\', NF * 8, strides=1, padding=\'VALID\')\n                 .Conv2D(\'convlast\', 1, strides=1, padding=\'VALID\', activation=tf.identity)())\n        return l\n\n    def build_graph(self, input, output):\n        input, output = input / 128.0 - 1, output / 128.0 - 1\n\n        with argscope([Conv2D, Conv2DTranspose], kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n            with tf.variable_scope(\'gen\'):\n                fake_output = self.generator(input)\n            with tf.variable_scope(\'discrim\'):\n                real_pred = self.discriminator(input, output)\n                fake_pred = self.discriminator(input, fake_output)\n\n        self.build_losses(real_pred, fake_pred)\n        errL1 = tf.reduce_mean(tf.abs(fake_output - output), name=\'L1_loss\')\n        self.g_loss = tf.add(self.g_loss, LAMBDA * errL1, name=\'total_g_loss\')\n        add_moving_summary(errL1, self.g_loss)\n\n        # tensorboard visualization\n        if IN_CH == 1:\n            input = tf.image.grayscale_to_rgb(input)\n        if OUT_CH == 1:\n            output = tf.image.grayscale_to_rgb(output)\n            fake_output = tf.image.grayscale_to_rgb(fake_output)\n\n        visualize_tensors(\'input,output,fake\', [input, output, fake_output], max_outputs=max(30, BATCH))\n\n        self.collect_variables()\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=2e-4, trainable=False)\n        return tf.train.AdamOptimizer(lr, beta1=0.5, epsilon=1e-3)\n\n\ndef split_input(mode, dp):\n    """"""\n    dp: the datapoint. first component is an RGB image of shape (s, 2s, 3).\n    :return: [input, output]\n    """"""\n    img = dp[0]\n    # split the image into left + right pairs\n    s = img.shape[0]\n    assert img.shape[1] == 2 * s\n    input, output = img[:, :s, :], img[:, s:, :]\n    if mode == \'BtoA\':\n        input, output = output, input\n    if IN_CH == 1:\n        input = cv2.cvtColor(input, cv2.COLOR_RGB2GRAY)[:, :, np.newaxis]\n    if OUT_CH == 1:\n        output = cv2.cvtColor(output, cv2.COLOR_RGB2GRAY)[:, :, np.newaxis]\n    return [input, output]\n\n\ndef get_data(args):\n    datadir = args.data\n    imgs = glob.glob(os.path.join(datadir, \'*.jpg\'))\n    ds = ImageFromFile(imgs, channel=3, shuffle=True)\n\n    ds = MapData(ds, functools.partial(split_input, args.mode))\n    augs = [imgaug.Resize(286), imgaug.RandomCrop(256)]\n    ds = AugmentImageComponents(ds, augs, (0, 1))\n    ds = BatchData(ds, BATCH)\n    ds = MultiProcessRunner(ds, 100, 1)\n    return ds\n\n\ndef sample(datadir, model_path):\n    pred = PredictConfig(\n        session_init=SmartInit(model_path),\n        model=Model(),\n        input_names=[\'input\', \'output\'],\n        output_names=[\'viz\'])\n\n    imgs = glob.glob(os.path.join(datadir, \'*.jpg\'))\n    ds = ImageFromFile(imgs, channel=3, shuffle=True)\n    ds = MapData(ds, split_input)\n    ds = AugmentImageComponents(ds, [imgaug.Resize(256)], (0, 1))\n    ds = BatchData(ds, 6)\n\n    pred = SimpleDatasetPredictor(pred, ds)\n    for o in pred.get_result():\n        o = o[0][:, :, :, ::-1]\n        stack_patches(o, nr_row=3, nr_col=2, viz=True)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--sample\', action=\'store_true\', help=\'run sampling\')\n    parser.add_argument(\'--data\', help=\'Image directory\', required=True)\n    parser.add_argument(\'--mode\', choices=[\'AtoB\', \'BtoA\'], default=\'AtoB\')\n    parser.add_argument(\'-b\', \'--batch\', type=int, default=1)\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    BATCH = args.batch\n\n    if args.sample:\n        assert args.load\n        sample(args.data, args.load)\n    else:\n        logger.auto_set_dir()\n\n        data = QueueInput(get_data(args))\n        trainer = GANTrainer(data, Model(), get_num_gpu())\n\n        trainer.train_with_defaults(\n            callbacks=[\n                PeriodicTrigger(ModelSaver(), every_k_epochs=3),\n                ScheduledHyperParamSetter(\'learning_rate\', [(200, 1e-4)])\n            ],\n            steps_per_epoch=data.size(),\n            max_epoch=300,\n            session_init=SmartInit(args.load)\n        )\n'"
examples/GAN/Improved-WGAN.py,22,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: Improved-WGAN.py\n# Author: Yuxin Wu\n\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils import get_tf_version_tuple\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\n\nimport DCGAN\nfrom GAN import SeparateGANTrainer\n\n""""""\nImproved Wasserstein-GAN.\nSee the docstring in DCGAN.py for usage.\n""""""\n\n# Don\'t want to mix two examples together, but want to reuse the code.\n# So here just import stuff from DCGAN.\n\n\nclass Model(DCGAN.Model):\n    # replace BatchNorm by LayerNorm\n    @auto_reuse_variable_scope\n    def discriminator(self, imgs):\n        nf = 64\n        with argscope(Conv2D, activation=tf.identity, kernel_size=4, strides=2):\n            l = (LinearWrap(imgs)\n                 .Conv2D(\'conv0\', nf, activation=tf.nn.leaky_relu)\n                 .Conv2D(\'conv1\', nf * 2)\n                 .LayerNorm(\'ln1\')\n                 .tf.nn.leaky_relu()\n                 .Conv2D(\'conv2\', nf * 4)\n                 .LayerNorm(\'ln2\')\n                 .tf.nn.leaky_relu()\n                 .Conv2D(\'conv3\', nf * 8)\n                 .LayerNorm(\'ln3\')\n                 .tf.nn.leaky_relu()\n                 .FullyConnected(\'fct\', 1, activation=tf.identity)())\n        return tf.reshape(l, [-1])\n\n    def build_graph(self, image_pos):\n        image_pos = image_pos / 128.0 - 1\n\n        z = tf.random_normal([self.batch, self.zdim], name=\'z_train\')\n        z = tf.placeholder_with_default(z, [None, self.zdim], name=\'z\')\n\n        with argscope([Conv2D, Conv2DTranspose, FullyConnected],\n                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n            with tf.variable_scope(\'gen\'):\n                image_gen = self.generator(z)\n            tf.summary.image(\'generated-samples\', image_gen, max_outputs=30)\n\n            alpha = tf.random_uniform(shape=[self.batch, 1, 1, 1],\n                                      minval=0., maxval=1., name=\'alpha\')\n            interp = image_pos + alpha * (image_gen - image_pos)\n\n            with tf.variable_scope(\'discrim\'):\n                vecpos = self.discriminator(image_pos)\n                vecneg = self.discriminator(image_gen)\n                vec_interp = self.discriminator(interp)\n\n        # the Wasserstein-GAN losses\n        self.d_loss = tf.reduce_mean(vecneg - vecpos, name=\'d_loss\')\n        self.g_loss = tf.negative(tf.reduce_mean(vecneg), name=\'g_loss\')\n\n        # the gradient penalty loss\n        gradients = tf.gradients(vec_interp, [interp])[0]\n        gradients = tf.sqrt(tf.reduce_sum(tf.square(gradients), [1, 2, 3]))\n        gradients_rms = tf.sqrt(tf.reduce_mean(tf.square(gradients)), name=\'gradient_rms\')\n        gradient_penalty = tf.reduce_mean(tf.square(gradients - 1), name=\'gradient_penalty\')\n        add_moving_summary(self.d_loss, self.g_loss, gradient_penalty, gradients_rms)\n\n        self.d_loss = tf.add(self.d_loss, 10 * gradient_penalty)\n\n        self.collect_variables()\n\n    def optimizer(self):\n        opt = tf.train.AdamOptimizer(1e-4, beta1=0.5, beta2=0.9)\n        return opt\n\n\nif __name__ == \'__main__\':\n    assert get_tf_version_tuple() >= (1, 4)\n    args = DCGAN.get_args(default_batch=64, default_z_dim=128)\n    M = Model(shape=args.final_size, batch=args.batch, z_dim=args.z_dim)\n    if args.sample:\n        DCGAN.sample(M, args.load)\n    else:\n        logger.auto_set_dir()\n        SeparateGANTrainer(\n            QueueInput(DCGAN.get_data()),\n            M, g_period=5).train_with_defaults(\n            callbacks=[ModelSaver()],\n            steps_per_epoch=300,\n            max_epoch=200,\n            session_init=SmartInit(args.load)\n        )\n'"
examples/GAN/InfoGAN-mnist.py,35,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: InfoGAN-mnist.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils import gradproc, optimizer, summary\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope, under_name_scope\nfrom tensorpack.utils import viz\n\nfrom GAN import GANModelDesc, GANTrainer\n\n""""""\nTo train:\n    ./InfoGAN-mnist.py\n\nTo visualize:\n    ./InfoGAN-mnist.py --sample --load path/to/model\n\nA pretrained model is at http://models.tensorpack.com/#GAN\n""""""\n\nBATCH = 128\n# latent space is cat(10) x uni(2) x noise(NOISE_DIM)\nNUM_CLASS = 10\nNUM_UNIFORM = 2\nDIST_PARAM_DIM = NUM_CLASS + NUM_UNIFORM\nNOISE_DIM = 62\n# prior: the assumption how the latent factors are presented in the dataset\nDIST_PRIOR_PARAM = [1.] * NUM_CLASS + [0.] * NUM_UNIFORM\n\n\ndef shapeless_placeholder(x, axis, name):\n    """"""\n    Make the static shape of a tensor less specific.\n\n    If you want to feed to a tensor, the shape of the feed value must match\n    the tensor\'s static shape. This function creates a placeholder which\n    defaults to x if not fed, but has a less specific static shape than x.\n    See also `tensorflow#5680 <https://github.com/tensorflow/tensorflow/issues/5680>`_.\n\n    Args:\n        x: a tensor\n        axis(int or list of ints): these axes of ``x.get_shape()`` will become\n            None in the output.\n        name(str): name of the output tensor\n\n    Returns:\n        a tensor equal to x, but shape information is partially cleared.\n    """"""\n    shp = x.get_shape().as_list()\n    if not isinstance(axis, list):\n        axis = [axis]\n    for a in axis:\n        if shp[a] is None:\n            raise ValueError(""Axis {} of shape {} is already unknown!"".format(a, shp))\n        shp[a] = None\n    x = tf.placeholder_with_default(x, shape=shp, name=name)\n    return x\n\n\ndef get_distributions(vec_cat, vec_uniform):\n    cat = tf.distributions.Categorical(logits=vec_cat, validate_args=True, name=\'cat\')\n    uni = tf.distributions.Normal(vec_uniform, scale=1., validate_args=True, allow_nan_stats=False, name=\'uni_a\')\n    return cat, uni\n\n\ndef entropy_from_samples(samples, vec):\n    """"""\n    Estimate H(x|s) ~= -E_{x \\sim P(x|s)}[\\log Q(x|s)], where x are samples, and Q is parameterized by vec.\n    """"""\n    samples_cat = tf.argmax(samples[:, :NUM_CLASS], axis=1, output_type=tf.int32)\n    samples_uniform = samples[:, NUM_CLASS:]\n    cat, uniform = get_distributions(vec[:, :NUM_CLASS], vec[:, NUM_CLASS:])\n\n    def neg_logprob(dist, sample, name):\n        nll = -dist.log_prob(sample)\n        # average over batch\n        return tf.reduce_sum(tf.reduce_mean(nll, axis=0), name=name)\n\n    entropies = [neg_logprob(cat, samples_cat, \'nll_cat\'),\n                 neg_logprob(uniform, samples_uniform, \'nll_uniform\')]\n    return entropies\n\n\n@under_name_scope()\ndef sample_prior(batch_size):\n    cat, _ = get_distributions(DIST_PRIOR_PARAM[:NUM_CLASS], DIST_PRIOR_PARAM[NUM_CLASS:])\n    sample_cat = tf.one_hot(cat.sample(batch_size), NUM_CLASS)\n\n    """"""\n    OpenAI official code actually models the ""uniform"" latent code as\n    a Gaussian distribution, but obtain the samples from a uniform distribution.\n    """"""\n    sample_uni = tf.random_uniform([batch_size, NUM_UNIFORM], -1, 1)\n    samples = tf.concat([sample_cat, sample_uni], axis=1)\n    return samples\n\n\nclass Model(GANModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, 28, 28), tf.float32, \'input\')]\n\n    def generator(self, z):\n        l = FullyConnected(\'fc0\', z, 1024, activation=BNReLU)\n        l = FullyConnected(\'fc1\', l, 128 * 7 * 7, activation=BNReLU)\n        l = tf.reshape(l, [-1, 7, 7, 128])\n        l = Conv2DTranspose(\'deconv1\', l, 64, 4, 2, activation=BNReLU)\n        l = Conv2DTranspose(\'deconv2\', l, 1, 4, 2, activation=tf.identity)\n        l = tf.sigmoid(l, name=\'gen\')\n        return l\n\n    @auto_reuse_variable_scope\n    def discriminator(self, imgs):\n        with argscope(Conv2D, kernel_size=4, strides=2):\n            l = (LinearWrap(imgs)\n                 .Conv2D(\'conv0\', 64)\n                 .tf.nn.leaky_relu()\n                 .Conv2D(\'conv1\', 128)\n                 .BatchNorm(\'bn1\')\n                 .tf.nn.leaky_relu()\n                 .FullyConnected(\'fc1\', 1024)\n                 .BatchNorm(\'bn2\')\n                 .tf.nn.leaky_relu()())\n\n            logits = FullyConnected(\'fct\', l, 1)\n            encoder = (LinearWrap(l)\n                       .FullyConnected(\'fce1\', 128)\n                       .BatchNorm(\'bne\')\n                       .tf.nn.leaky_relu()\n                       .FullyConnected(\'fce-out\', DIST_PARAM_DIM)())\n        return logits, encoder\n\n    def build_graph(self, real_sample):\n        real_sample = tf.expand_dims(real_sample, -1)\n\n        # sample the latent code:\n        zc = shapeless_placeholder(sample_prior(BATCH), 0, name=\'z_code\')\n        z_noise = shapeless_placeholder(\n            tf.random_uniform([BATCH, NOISE_DIM], -1, 1), 0, name=\'z_noise\')\n        z = tf.concat([zc, z_noise], 1, name=\'z\')\n\n        with argscope([Conv2D, Conv2DTranspose, FullyConnected],\n                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n            with tf.variable_scope(\'gen\'):\n                fake_sample = self.generator(z)\n                fake_sample_viz = tf.cast((fake_sample) * 255.0, tf.uint8, name=\'viz\')\n                tf.summary.image(\'gen\', fake_sample_viz, max_outputs=30)\n\n            # may need to investigate how bn stats should be updated across two discrim\n            with tf.variable_scope(\'discrim\'):\n                real_pred, _ = self.discriminator(real_sample)\n                fake_pred, dist_param = self.discriminator(fake_sample)\n\n        """"""\n        Mutual information between x (i.e. zc in this case) and some\n        information s (the generated samples in this case):\n\n                    I(x;s) = H(x) - H(x|s)\n                           = H(x) + E[\\log P(x|s)]\n\n        The distribution from which zc is sampled, in this case, is set to a fixed prior already.\n        So the first term is a constant.\n        For the second term, we can maximize its variational lower bound:\n                    E_{x \\sim P(x|s)}[\\log Q(x|s)]\n        where Q(x|s) is a proposal distribution to approximate P(x|s).\n\n        Here, Q(x|s) is assumed to be a distribution which shares the form\n        of P, and whose parameters are predicted by the discriminator network.\n        """"""\n        with tf.name_scope(""mutual_information""):\n            with tf.name_scope(\'prior_entropy\'):\n                cat, uni = get_distributions(DIST_PRIOR_PARAM[:NUM_CLASS], DIST_PRIOR_PARAM[NUM_CLASS:])\n                ents = [cat.entropy(name=\'cat_entropy\'), tf.reduce_sum(uni.entropy(), name=\'uni_entropy\')]\n                entropy = tf.add_n(ents, name=\'total_entropy\')\n                # Note that the entropy of prior is a constant. The paper mentioned it but didn\'t use it.\n\n            with tf.name_scope(\'conditional_entropy\'):\n                cond_ents = entropy_from_samples(zc, dist_param)\n                cond_entropy = tf.add_n(cond_ents, name=""total_entropy"")\n\n            MI = tf.subtract(entropy, cond_entropy, name=\'mutual_information\')\n            summary.add_moving_summary(entropy, cond_entropy, MI, *cond_ents)\n\n        # default GAN objective\n        self.build_losses(real_pred, fake_pred)\n\n        # subtract mutual information for latent factors (we want to maximize them)\n        self.g_loss = tf.subtract(self.g_loss, MI, name=\'total_g_loss\')\n        self.d_loss = tf.subtract(self.d_loss, MI, name=\'total_d_loss\')\n\n        summary.add_moving_summary(self.g_loss, self.d_loss)\n\n        # distinguish between variables of generator and discriminator updates\n        self.collect_variables()\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=2e-4, dtype=tf.float32, trainable=False)\n        opt = tf.train.AdamOptimizer(lr, beta1=0.5, epsilon=1e-6)\n        # generator learns 5 times faster\n        return optimizer.apply_grad_processors(\n            opt, [gradproc.ScaleGradient((\'gen/.*\', 5))])\n\n\ndef get_data():\n    ds = ConcatData([dataset.Mnist(\'train\'), dataset.Mnist(\'test\')])\n    ds = BatchData(ds, BATCH)\n    ds = MapData(ds, lambda dp: [dp[0]])  # only use the image\n    return ds\n\n\ndef sample(model_path):\n    pred = OfflinePredictor(PredictConfig(\n        session_init=SmartInit(model_path),\n        model=Model(),\n        input_names=[\'z_code\', \'z_noise\'],\n        output_names=[\'gen/viz\']))\n\n    # sample all one-hot encodings (10 times)\n    z_cat = np.tile(np.eye(10), [10, 1])\n    # sample continuos variables from -2 to +2 as mentioned in the paper\n    z_uni = np.linspace(-2.0, 2.0, num=100)\n    z_uni = z_uni[:, None]\n\n    IMG_SIZE = 400\n\n    while True:\n        # only categorical turned on\n        z_noise = np.random.uniform(-1, 1, (100, NOISE_DIM))\n        zc = np.concatenate((z_cat, z_uni * 0, z_uni * 0), axis=1)\n        o = pred(zc, z_noise)[0]\n        viz1 = viz.stack_patches(o, nr_row=10, nr_col=10)\n        viz1 = cv2.resize(viz1, (IMG_SIZE, IMG_SIZE))\n\n        # show effect of first continous variable with fixed noise\n        zc = np.concatenate((z_cat, z_uni, z_uni * 0), axis=1)\n        o = pred(zc, z_noise * 0)[0]\n        viz2 = viz.stack_patches(o, nr_row=10, nr_col=10)\n        viz2 = cv2.resize(viz2, (IMG_SIZE, IMG_SIZE))\n\n        # show effect of second continous variable with fixed noise\n        zc = np.concatenate((z_cat, z_uni * 0, z_uni), axis=1)\n        o = pred(zc, z_noise * 0)[0]\n        viz3 = viz.stack_patches(o, nr_row=10, nr_col=10)\n        viz3 = cv2.resize(viz3, (IMG_SIZE, IMG_SIZE))\n\n        canvas = viz.stack_patches(\n            [viz1, viz2, viz3],\n            nr_row=1, nr_col=3, border=5, bgcolor=(255, 0, 0))\n\n        viz.interactive_imshow(canvas)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--sample\', action=\'store_true\', help=\'visualize the space of the 10 latent codes\')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    if args.sample:\n        BATCH = 100\n        sample(args.load)\n    else:\n        logger.auto_set_dir()\n        GANTrainer(QueueInput(get_data()),\n                   Model()).train_with_defaults(\n            callbacks=[ModelSaver(keep_checkpoint_every_n_hours=0.1)],\n            steps_per_epoch=500,\n            max_epoch=100,\n            session_init=SmartInit(args.load)\n        )\n'"
examples/GAN/WGAN.py,7,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: WGAN.py\n# Author: Yuxin Wu\n\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.summary import add_moving_summary\n\nimport DCGAN\nfrom GAN import SeparateGANTrainer\n\n""""""\nWasserstein-GAN.\nSee the docstring in DCGAN.py for usage.\n""""""\n\n# Don\'t want to mix two examples together, but want to reuse the code.\n# So here just import stuff from DCGAN\n\n\nclass Model(DCGAN.Model):\n    # def generator(self, z):\n    # you can override generator to remove BatchNorm, it will still work in WGAN\n\n    def build_losses(self, vecpos, vecneg):\n        # the Wasserstein-GAN losses\n        self.d_loss = tf.reduce_mean(vecneg - vecpos, name=\'d_loss\')\n        self.g_loss = tf.negative(tf.reduce_mean(vecneg), name=\'g_loss\')\n        add_moving_summary(self.d_loss, self.g_loss)\n\n    def optimizer(self):\n        opt = tf.train.RMSPropOptimizer(1e-4)\n        return opt\n\n        # An alternative way to implement the clipping:\n        """"""\n        from tensorpack.tfutils import optimizer\n        def clip(v):\n            n = v.op.name\n            if not n.startswith(\'discrim/\'):\n                return None\n            logger.info(""Clip {}"".format(n))\n            return tf.clip_by_value(v, -0.01, 0.01)\n        return optimizer.VariableAssignmentOptimizer(opt, clip)\n        """"""\n\n\nclass ClipCallback(Callback):\n    def _setup_graph(self):\n        vars = tf.trainable_variables()\n        ops = []\n        for v in vars:\n            n = v.op.name\n            if not n.startswith(\'discrim/\'):\n                continue\n            logger.info(""Clip {}"".format(n))\n            ops.append(tf.assign(v, tf.clip_by_value(v, -0.01, 0.01)))\n        self._op = tf.group(*ops, name=\'clip\')\n\n    def _trigger_step(self):\n        self._op.run()\n\n\nif __name__ == \'__main__\':\n    args = DCGAN.get_args(default_batch=64)\n\n    M = Model(shape=args.final_size, batch=args.batch, z_dim=args.z_dim)\n    if args.sample:\n        DCGAN.sample(M, args.load)\n    else:\n        logger.auto_set_dir()\n\n        # The original code uses a different schedule, but this seems to work well.\n        # Train 1 D after 2 G\n        SeparateGANTrainer(\n            input=QueueInput(DCGAN.get_data()),\n            model=M, d_period=3).train_with_defaults(\n            callbacks=[ModelSaver(), ClipCallback()],\n            steps_per_epoch=500,\n            max_epoch=200,\n            session_init=SmartInit(args.load)\n        )\n'"
examples/HED/hed.py,36,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: hed.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils import gradproc, optimizer\nfrom tensorpack.tfutils.summary import add_moving_summary, add_param_summary\nfrom tensorpack.utils.gpu import get_num_gpu\nfrom tensorpack.utils import logger\n\n\ndef class_balanced_sigmoid_cross_entropy(logits, label, name=\'cross_entropy_loss\'):\n    """"""\n    The class-balanced cross entropy loss,\n    as in `Holistically-Nested Edge Detection\n    <http://arxiv.org/abs/1504.06375>`_.\n\n    Args:\n        logits: of shape (b, ...).\n        label: of the same shape. the ground truth in {0,1}.\n    Returns:\n        class-balanced cross entropy loss.\n    """"""\n    with tf.name_scope(\'class_balanced_sigmoid_cross_entropy\'):\n        y = tf.cast(label, tf.float32)\n\n        count_neg = tf.reduce_sum(1. - y)\n        count_pos = tf.reduce_sum(y)\n        beta = count_neg / (count_neg + count_pos)\n\n        pos_weight = beta / (1 - beta)\n        cost = tf.nn.weighted_cross_entropy_with_logits(logits=logits, targets=y, pos_weight=pos_weight)\n        cost = tf.reduce_mean(cost * (1 - beta))\n        zero = tf.equal(count_pos, 0.0)\n    return tf.where(zero, 0.0, cost, name=name)\n\n\n@layer_register(log_shape=True)\ndef CaffeBilinearUpSample(x, shape):\n    """"""\n    Deterministic bilinearly-upsample the input images.\n    It is implemented by deconvolution with ""BilinearFiller"" in Caffe.\n    It is aimed to mimic caffe behavior.\n\n    Args:\n        x (tf.Tensor): a NCHW tensor\n        shape (int): the upsample factor\n\n    Returns:\n        tf.Tensor: a NCHW tensor.\n    """"""\n    inp_shape = x.shape.as_list()\n    ch = inp_shape[1]\n    assert ch == 1, ""This layer only works for channel=1""\n    # for a version that supports >1 channels, see:\n    # https://github.com/tensorpack/tensorpack/issues/1040#issuecomment-452798180\n\n    shape = int(shape)\n    filter_shape = 2 * shape\n\n    def bilinear_conv_filler(s):\n        """"""\n        s: width, height of the conv filter\n        https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/filler.hpp#L219-L268\n        """"""\n        f = np.ceil(float(s) / 2)\n        c = float(2 * f - 1 - f % 2) / (2 * f)\n        ret = np.zeros((s, s), dtype=\'float32\')\n        for x in range(s):\n            for y in range(s):\n                ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n        return ret\n\n    w = bilinear_conv_filler(filter_shape)\n    w = np.repeat(w, ch * ch).reshape((filter_shape, filter_shape, ch, ch))\n\n    weight_var = tf.constant(w, tf.float32,\n                             shape=(filter_shape, filter_shape, ch, ch),\n                             name=\'bilinear_upsample_filter\')\n    x = tf.pad(x, [[0, 0], [0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1]], mode=\'SYMMETRIC\')\n    out_shape = tf.shape(x) * tf.constant([1, 1, shape, shape], tf.int32)\n    deconv = tf.nn.conv2d_transpose(x, weight_var, out_shape,\n                                    [1, 1, shape, shape], \'SAME\', data_format=\'NCHW\')\n    edge = shape * (shape - 1)\n    deconv = deconv[:, :, edge:-edge, edge:-edge]\n\n    if inp_shape[2]:\n        inp_shape[2] *= shape\n    if inp_shape[3]:\n        inp_shape[3] *= shape\n    deconv.set_shape(inp_shape)\n    return deconv\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec([None, None, None, 3], tf.float32, \'image\'),\n                tf.TensorSpec([None, None, None], tf.int32, \'edgemap\')]\n\n    def build_graph(self, image, edgemap):\n        image = image - tf.constant([104, 116, 122], dtype=\'float32\')\n        image = tf.transpose(image, [0, 3, 1, 2])\n        edgemap = tf.expand_dims(edgemap, 3, name=\'edgemap4d\')\n\n        def branch(name, l, up):\n            with tf.variable_scope(name):\n                l = Conv2D(\'convfc\', l, 1, kernel_size=1, activation=tf.identity,\n                           use_bias=True,\n                           kernel_initializer=tf.constant_initializer())\n                while up != 1:\n                    l = CaffeBilinearUpSample(\'upsample{}\'.format(up), l, 2)\n                    up = up // 2\n                return l\n\n        with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu), \\\n                argscope([Conv2D, MaxPooling], data_format=\'NCHW\'):\n            l = Conv2D(\'conv1_1\', image, 64)\n            l = Conv2D(\'conv1_2\', l, 64)\n            b1 = branch(\'branch1\', l, 1)\n            l = MaxPooling(\'pool1\', l, 2)\n\n            l = Conv2D(\'conv2_1\', l, 128)\n            l = Conv2D(\'conv2_2\', l, 128)\n            b2 = branch(\'branch2\', l, 2)\n            l = MaxPooling(\'pool2\', l, 2)\n\n            l = Conv2D(\'conv3_1\', l, 256)\n            l = Conv2D(\'conv3_2\', l, 256)\n            l = Conv2D(\'conv3_3\', l, 256)\n            b3 = branch(\'branch3\', l, 4)\n            l = MaxPooling(\'pool3\', l, 2)\n\n            l = Conv2D(\'conv4_1\', l, 512)\n            l = Conv2D(\'conv4_2\', l, 512)\n            l = Conv2D(\'conv4_3\', l, 512)\n            b4 = branch(\'branch4\', l, 8)\n            l = MaxPooling(\'pool4\', l, 2)\n\n            l = Conv2D(\'conv5_1\', l, 512)\n            l = Conv2D(\'conv5_2\', l, 512)\n            l = Conv2D(\'conv5_3\', l, 512)\n            b5 = branch(\'branch5\', l, 16)\n\n            final_map = Conv2D(\'convfcweight\',\n                               tf.concat([b1, b2, b3, b4, b5], 1), 1, kernel_size=1,\n                               kernel_initializer=tf.constant_initializer(0.2),\n                               use_bias=False, activation=tf.identity)\n        costs = []\n        for idx, b in enumerate([b1, b2, b3, b4, b5, final_map]):\n            b = tf.transpose(b, [0, 2, 3, 1])\n            output = tf.nn.sigmoid(b, name=\'output{}\'.format(idx + 1))\n            xentropy = class_balanced_sigmoid_cross_entropy(\n                b, edgemap,\n                name=\'xentropy{}\'.format(idx + 1))\n            costs.append(xentropy)\n\n        # some magic threshold\n        pred = tf.cast(tf.greater(output, 0.5), tf.int32, name=\'prediction\')\n        wrong = tf.cast(tf.not_equal(pred, edgemap), tf.float32)\n        wrong = tf.reduce_mean(wrong, name=\'train_error\')\n\n        wd_w = tf.train.exponential_decay(2e-4, get_global_step_var(),\n                                          80000, 0.7, True)\n        wd_cost = tf.multiply(wd_w, regularize_cost(\'.*/W\', tf.nn.l2_loss), name=\'wd_cost\')\n        costs.append(wd_cost)\n\n        add_param_summary((\'.*/W\', [\'histogram\']))   # monitor W\n        total_cost = tf.add_n(costs, name=\'cost\')\n        add_moving_summary(wrong, total_cost, *costs)\n        return total_cost\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=3e-5, trainable=False)\n        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n        return optimizer.apply_grad_processors(\n            opt, [gradproc.ScaleGradient(\n                [(\'convfcweight.*\', 0.1), (\'conv5_.*\', 5)])])\n\n\ndef get_data(name):\n    isTrain = name == \'train\'\n    ds = dataset.BSDS500(name, shuffle=True)\n\n    class CropMultiple16(imgaug.ImageAugmentor):\n        def get_transform(self, img):\n            newh = img.shape[0] // 16 * 16\n            neww = img.shape[1] // 16 * 16\n            assert newh > 0 and neww > 0\n            diffh = img.shape[0] - newh\n            h0 = 0 if diffh == 0 else self.rng.randint(diffh)\n            diffw = img.shape[1] - neww\n            w0 = 0 if diffw == 0 else self.rng.randint(diffw)\n            return imgaug.CropTransform(h0, w0, newh, neww)\n\n    if isTrain:\n        shape_aug = [\n            imgaug.RandomResize(xrange=(0.7, 1.5), yrange=(0.7, 1.5),\n                                aspect_ratio_thres=0.15),\n            imgaug.RotationAndCropValid(90),\n            CropMultiple16(),\n            imgaug.Flip(horiz=True),\n            imgaug.Flip(vert=True)\n        ]\n    else:\n        # the original image shape (321x481) in BSDS is not a multiple of 16\n        IMAGE_SHAPE = (320, 480)\n        shape_aug = [imgaug.CenterCrop(IMAGE_SHAPE)]\n    ds = AugmentImageComponents(ds, shape_aug, (0, 1), copy=False)\n\n    def f(m):   # thresholding\n        m[m >= 0.50] = 1\n        m[m < 0.50] = 0\n        return m\n    ds = MapDataComponent(ds, f, 1)\n\n    if isTrain:\n        augmentors = [\n            imgaug.Brightness(63, clip=False),\n            imgaug.Contrast((0.4, 1.5)),\n        ]\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        ds = BatchDataByShape(ds, 8, idx=0)\n        ds = MultiProcessRunnerZMQ(ds, 1)\n    else:\n        ds = BatchData(ds, 1)\n    return ds\n\n\ndef view_data():\n    ds = RepeatedData(get_data(\'train\'), -1)\n    ds.reset_state()\n    for ims, edgemaps in ds:\n        for im, edgemap in zip(ims, edgemaps):\n            assert im.shape[0] % 16 == 0 and im.shape[1] % 16 == 0, im.shape\n            cv2.imshow(""im"", im / 255.0)\n            cv2.waitKey(1000)\n            cv2.imshow(""edge"", edgemap)\n            cv2.waitKey(1000)\n\n\ndef get_config():\n    logger.auto_set_dir()\n    dataset_train = get_data(\'train\')\n    steps_per_epoch = len(dataset_train) * 40\n    dataset_val = get_data(\'val\')\n\n    return TrainConfig(\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(),\n            ScheduledHyperParamSetter(\'learning_rate\', [(30, 6e-6), (45, 1e-6), (60, 8e-7)]),\n            HumanHyperParamSetter(\'learning_rate\'),\n            InferenceRunner(dataset_val,\n                            BinaryClassificationStats(\'prediction\', \'edgemap4d\'))\n        ],\n        model=Model(),\n        steps_per_epoch=steps_per_epoch,\n        max_epoch=100,\n    )\n\n\ndef run(model_path, image_path, output):\n    pred_config = PredictConfig(\n        model=Model(),\n        session_init=SmartInit(model_path),\n        input_names=[\'image\'],\n        output_names=[\'output\' + str(k) for k in range(1, 7)])\n    predictor = OfflinePredictor(pred_config)\n    im = cv2.imread(image_path)\n    assert im is not None\n    im = cv2.resize(\n        im, (im.shape[1] // 16 * 16, im.shape[0] // 16 * 16)\n    )[None, :, :, :].astype(\'float32\')\n    outputs = predictor(im)\n    if output is None:\n        for k in range(6):\n            pred = outputs[k][0]\n            cv2.imwrite(""out{}.png"".format(\n                \'-fused\' if k == 5 else str(k + 1)), pred * 255)\n        logger.info(""Results saved to out*.png"")\n    else:\n        pred = outputs[5][0]\n        cv2.imwrite(output, pred * 255)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--view\', help=\'view dataset\', action=\'store_true\')\n    parser.add_argument(\'--run\', help=\'run model on images\')\n    parser.add_argument(\'--output\', help=\'fused output filename. default to out-fused.png\')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    if args.view:\n        view_data()\n    elif args.run:\n        run(args.load, args.run, args.output)\n    else:\n        config = get_config()\n        config.session_init = SmartInit(args.load)\n        launch_train_with_config(\n            config,\n            SyncMultiGPUTrainer(max(get_num_gpu(), 1)))\n'"
examples/ImageNetModels/alexnet.py,12,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: alexnet.py\n\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import imgaug\nfrom tensorpack.tfutils import argscope\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom imagenet_utils import ImageNetModel, get_imagenet_dataflow\n\n\ndef visualize_conv1_weights(filters):\n    ctx = get_current_tower_context()\n    if not ctx.is_main_training_tower:\n        return\n    with tf.name_scope(\'visualize_conv1\'):\n        filters = tf.reshape(filters, [11, 11, 3, 8, 12])\n        filters = tf.transpose(filters, [3, 0, 4, 1, 2])    # 8,11,12,11,3\n        filters = tf.reshape(filters, [1, 88, 132, 3])\n    tf.summary.image(\'visualize_conv1\', filters, max_outputs=1, collections=[\'AAA\'])\n\n\nclass Model(ImageNetModel):\n    weight_decay = 5e-4\n    data_format = \'NHWC\'  # LRN only supports NHWC\n\n    def get_logits(self, image):\n        gauss_init = tf.random_normal_initializer(stddev=0.01)\n        with argscope(Conv2D,\n                      kernel_initializer=tf.variance_scaling_initializer(scale=2.)), \\\n                argscope([Conv2D, FullyConnected], activation=tf.nn.relu), \\\n                argscope([Conv2D, MaxPooling], data_format=\'channels_last\'):\n            # necessary padding to get 55x55 after conv1\n            image = tf.pad(image, [[0, 0], [2, 2], [2, 2], [0, 0]])\n            l = Conv2D(\'conv1\', image, filters=96, kernel_size=11, strides=4, padding=\'VALID\')\n            # size: 55\n            visualize_conv1_weights(l.variables.W)\n            l = tf.nn.lrn(l, 2, bias=1.0, alpha=2e-5, beta=0.75, name=\'norm1\')\n            l = MaxPooling(\'pool1\', l, 3, strides=2, padding=\'VALID\')\n            # 27\n            l = Conv2D(\'conv2\', l, filters=256, kernel_size=5, split=2)\n            l = tf.nn.lrn(l, 2, bias=1.0, alpha=2e-5, beta=0.75, name=\'norm2\')\n            l = MaxPooling(\'pool2\', l, 3, strides=2, padding=\'VALID\')\n            # 13\n            l = Conv2D(\'conv3\', l, filters=384, kernel_size=3)\n            l = Conv2D(\'conv4\', l, filters=384, kernel_size=3, split=2)\n            l = Conv2D(\'conv5\', l, filters=256, kernel_size=3, split=2)\n            l = MaxPooling(\'pool3\', l, 3, strides=2, padding=\'VALID\')\n\n            l = FullyConnected(\'fc6\', l, 4096,\n                               kernel_initializer=gauss_init,\n                               bias_initializer=tf.ones_initializer())\n            l = Dropout(l, rate=0.5)\n            l = FullyConnected(\'fc7\', l, 4096, kernel_initializer=gauss_init)\n            l = Dropout(l, rate=0.5)\n        logits = FullyConnected(\'fc8\', l, 1000, kernel_initializer=gauss_init)\n        return logits\n\n\ndef get_data(name, batch):\n    isTrain = name == \'train\'\n    if isTrain:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),\n            imgaug.RandomCrop(224),\n            imgaug.Lighting(0.1,\n                            eigval=np.asarray(\n                                [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                            eigvec=np.array(\n                                [[-0.5675, 0.7192, 0.4009],\n                                 [-0.5808, -0.0045, -0.8140],\n                                 [-0.5836, -0.6948, 0.4203]],\n                                dtype=\'float32\')[::-1, ::-1]),\n            imgaug.Flip(horiz=True)]\n    else:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),\n            imgaug.CenterCrop((224, 224))]\n    return get_imagenet_dataflow(args.data, name, batch, augmentors)\n\n\ndef get_config():\n    nr_tower = max(get_num_gpu(), 1)\n    batch = args.batch\n    total_batch = batch * nr_tower\n    if total_batch != 128:\n        logger.warn(""AlexNet needs to be trained with a total batch size of 128."")\n    BASE_LR = 0.01 * (total_batch / 128.)\n\n    logger.info(""Running on {} towers. Batch size per tower: {}"".format(nr_tower, batch))\n    dataset_train = get_data(\'train\', batch)\n    dataset_val = get_data(\'val\', batch)\n\n    infs = [ClassificationError(\'wrong-top1\', \'val-error-top1\'),\n            ClassificationError(\'wrong-top5\', \'val-error-top5\')]\n    callbacks = [\n        ModelSaver(),\n        GPUUtilizationTracker(),\n        EstimatedTimeLeft(),\n        ScheduledHyperParamSetter(\n            \'learning_rate\',\n            [(0, BASE_LR), (30, BASE_LR * 1e-1), (60, BASE_LR * 1e-2), (80, BASE_LR * 1e-3)]),\n        DataParallelInferenceRunner(\n            dataset_val, infs, list(range(nr_tower))),\n    ]\n\n    return TrainConfig(\n        model=Model(),\n        data=StagingInput(QueueInput(dataset_train)),\n        callbacks=callbacks,\n        steps_per_epoch=1281167 // total_batch,\n        max_epoch=100,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--data\', help=\'ILSVRC dataset dir\')\n    parser.add_argument(\'--batch\', type=int, default=32, help=\'batch per GPU\')\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    logger.set_logger_dir(os.path.join(\'train_log\', \'AlexNet\'))\n\n    config = get_config()\n    nr_tower = max(get_num_gpu(), 1)\n    trainer = SyncMultiGPUTrainerReplicated(nr_tower)\n    launch_train_with_config(config, trainer)\n'"
examples/ImageNetModels/imagenet_utils.py,66,"b'# -*- coding: utf-8 -*-\n# File: imagenet_utils.py\n\n\nimport multiprocessing\nimport numpy as np\nimport os\nfrom abc import abstractmethod\nimport cv2\nimport tensorflow as tf\nimport tqdm\n\nfrom tensorpack import ModelDesc\nfrom tensorpack.dataflow import (\n    AugmentImageComponent, BatchData, MultiThreadMapData,\n    MultiProcessRunnerZMQ, dataset, imgaug)\nfrom tensorpack.input_source import QueueInput, StagingInput\nfrom tensorpack.models import regularize_cost, l2_regularizer\nfrom tensorpack.predict import FeedfreePredictor, PredictConfig\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.tfutils.optimizer import AccumGradOptimizer\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.stats import RatioCounter\n\n""""""\n====== DataFlow =======\n""""""\n\n\ndef fbresnet_augmentor(isTrain):\n    """"""\n    Augmentor used in fb.resnet.torch, for BGR images in range [0,255].\n    """"""\n    interpolation = cv2.INTER_CUBIC\n    # linear seems to have more stable performance.\n    # but we keep cubic for compatibility with old models\n    if isTrain:\n        augmentors = [\n            imgaug.GoogleNetRandomCropAndResize(interp=interpolation),\n            imgaug.ToFloat32(),  # avoid frequent casting in each color augmentation\n            # It\'s OK to remove the following augs if your CPU is not fast enough.\n            # Removing brightness/contrast/saturation does not have a significant effect on accuracy.\n            # Removing lighting leads to a tiny drop in accuracy.\n            imgaug.RandomOrderAug(\n                [imgaug.BrightnessScale((0.6, 1.4)),\n                 imgaug.Contrast((0.6, 1.4), rgb=False),\n                 imgaug.Saturation(0.4, rgb=False),\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\n                 imgaug.Lighting(0.1,\n                                 eigval=np.asarray(\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                                 eigvec=np.array(\n                                     [[-0.5675, 0.7192, 0.4009],\n                                      [-0.5808, -0.0045, -0.8140],\n                                      [-0.5836, -0.6948, 0.4203]],\n                                     dtype=\'float32\')[::-1, ::-1]\n                                 )]),\n            imgaug.ToUint8(),\n            imgaug.Flip(horiz=True),\n        ]\n    else:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, interp=interpolation),\n            imgaug.CenterCrop((224, 224)),\n        ]\n    return augmentors\n\n\ndef get_imagenet_dataflow(\n        datadir, name, batch_size,\n        augmentors=None, parallel=None):\n    """"""\n    Args:\n        augmentors (list[imgaug.Augmentor]): Defaults to `fbresnet_augmentor(isTrain)`\n\n    Returns: A DataFlow which produces BGR images and labels.\n\n    See explanations in the tutorial:\n    http://tensorpack.readthedocs.io/tutorial/efficient-dataflow.html\n    """"""\n    assert name in [\'train\', \'val\', \'test\']\n    isTrain = name == \'train\'\n    assert datadir is not None\n    if augmentors is None:\n        augmentors = fbresnet_augmentor(isTrain)\n    assert isinstance(augmentors, list)\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n\n    if isTrain:\n        ds = dataset.ILSVRC12(datadir, name, shuffle=True)\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        if parallel < 16:\n            logger.warn(""DataFlow may become the bottleneck when too few processes are used."")\n        ds = MultiProcessRunnerZMQ(ds, parallel)\n        ds = BatchData(ds, batch_size, remainder=False)\n    else:\n        ds = dataset.ILSVRC12Files(datadir, name, shuffle=False)\n        aug = imgaug.AugmentorList(augmentors)\n\n        def mapf(dp):\n            fname, cls = dp\n            im = cv2.imread(fname, cv2.IMREAD_COLOR)\n            im = aug.augment(im)\n            return im, cls\n        ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000, strict=True)\n        ds = BatchData(ds, batch_size, remainder=True)\n        ds = MultiProcessRunnerZMQ(ds, 1)\n    return ds\n\n\n""""""\n====== tf.data =======\n""""""\n\n\ndef get_imagenet_tfdata(datadir, name, batch_size, mapper=None, parallel=None):\n    """"""\n    Args:\n        mapper: a symbolic function that takes a tf.string (the raw bytes read from file) and produces a BGR image.\n            Defaults to `fbresnet_mapper(isTrain)`.\n\n    Returns:\n        A `tf.data.Dataset`. If training, the dataset is infinite.\n        The dataset contains BGR images and labels.\n    """"""\n\n    def get_imglist(dir, name):\n        """"""\n        Returns:\n            [(full filename, label)]\n        """"""\n        dir = os.path.join(dir, name)\n        meta = dataset.ILSVRCMeta()\n        imglist = meta.get_image_list(\n            name,\n            dataset.ILSVRCMeta.guess_dir_structure(dir))\n\n        def _filter(fname):\n            # png\n            return \'n02105855_2933.JPEG\' in fname\n\n        ret = []\n        for fname, label in imglist:\n            if _filter(fname):\n                logger.info(""Image {} was filtered out."".format(fname))\n                continue\n            fname = os.path.join(dir, fname)\n            ret.append((fname, label))\n        return ret\n\n    assert name in [\'train\', \'val\', \'test\']\n    assert datadir is not None\n    isTrain = name == \'train\'\n    if mapper is None:\n        mapper = fbresnet_mapper(isTrain)\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n    imglist = get_imglist(datadir, name)\n\n    N = len(imglist)\n    filenames = tf.constant([k[0] for k in imglist], name=\'filenames\')\n    labels = tf.constant([k[1] for k in imglist], dtype=tf.int32, name=\'labels\')\n\n    ds = tf.data.Dataset.from_tensor_slices((filenames, labels))\n\n    if isTrain:\n        ds = ds.shuffle(N, reshuffle_each_iteration=True).repeat()\n\n    ds = ds.apply(\n        tf.data.experimental.map_and_batch(\n            lambda fname, label: (mapper(tf.read_file(fname)), label),\n            batch_size=batch_size,\n            num_parallel_batches=parallel))\n    ds = ds.prefetch(100)\n    return ds\n\n\ndef fbresnet_mapper(isTrain):\n    """"""\n    Note: compared to fbresnet_augmentor, it\n    lacks some photometric augmentation that may have a small effect (0.1~0.2%) on accuracy.\n    """"""\n    JPEG_OPT = {\'fancy_upscaling\': True, \'dct_method\': \'INTEGER_ACCURATE\'}\n\n    def uint8_resize_bicubic(image, shape):\n        ret = tf.image.resize_bicubic([image], shape)\n        return tf.cast(tf.clip_by_value(ret, 0, 255), tf.uint8)[0]\n\n    def resize_shortest_edge(image, image_shape, size):\n        shape = tf.cast(image_shape, tf.float32)\n        w_greater = tf.greater(image_shape[0], image_shape[1])\n        shape = tf.cond(w_greater,\n                        lambda: tf.cast([shape[0] / shape[1] * size, size], tf.int32),\n                        lambda: tf.cast([size, shape[1] / shape[0] * size], tf.int32))\n\n        return uint8_resize_bicubic(image, shape)\n\n    def center_crop(image, size):\n        image_height = tf.shape(image)[0]\n        image_width = tf.shape(image)[1]\n\n        offset_height = (image_height - size) // 2\n        offset_width = (image_width - size) // 2\n        image = tf.slice(image, [offset_height, offset_width, 0], [size, size, -1])\n        return image\n\n    def lighting(image, std, eigval, eigvec):\n        v = tf.random_normal(shape=[3], stddev=std) * eigval\n        inc = tf.matmul(eigvec, tf.reshape(v, [3, 1]))\n        image = tf.cast(tf.cast(image, tf.float32) + tf.reshape(inc, [3]), image.dtype)\n        return image\n\n    def validation_mapper(byte):\n        image = tf.image.decode_jpeg(\n            tf.reshape(byte, shape=[]), 3, **JPEG_OPT)\n        image = resize_shortest_edge(image, tf.shape(image), 256)\n        image = center_crop(image, 224)\n        image = tf.reverse(image, axis=[2])  # to BGR\n        return image\n\n    def training_mapper(byte):\n        jpeg_shape = tf.image.extract_jpeg_shape(byte)  # hwc\n        bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n            jpeg_shape,\n            bounding_boxes=tf.zeros(shape=[0, 0, 4]),\n            min_object_covered=0,\n            aspect_ratio_range=[0.75, 1.33],\n            area_range=[0.08, 1.0],\n            max_attempts=10,\n            use_image_if_no_bounding_boxes=True)\n\n        is_bad = tf.reduce_sum(tf.cast(tf.equal(bbox_size, jpeg_shape), tf.int32)) >= 2\n\n        def good():\n            offset_y, offset_x, _ = tf.unstack(bbox_begin)\n            target_height, target_width, _ = tf.unstack(bbox_size)\n            crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n            image = tf.image.decode_and_crop_jpeg(\n                byte, crop_window, channels=3, **JPEG_OPT)\n            image = uint8_resize_bicubic(image, [224, 224])\n            return image\n\n        def bad():\n            image = tf.image.decode_jpeg(\n                tf.reshape(byte, shape=[]), 3, **JPEG_OPT)\n            image = resize_shortest_edge(image, jpeg_shape, 224)\n            image = center_crop(image, 224)\n            return image\n\n        image = tf.cond(is_bad, bad, good)\n        # TODO other imgproc\n        image = lighting(image, 0.1,\n                         eigval=np.array([0.2175, 0.0188, 0.0045], dtype=\'float32\') * 255.0,\n                         eigvec=np.array([[-0.5675, 0.7192, 0.4009],\n                                          [-0.5808, -0.0045, -0.8140],\n                                          [-0.5836, -0.6948, 0.4203]], dtype=\'float32\'))\n        image = tf.image.random_flip_left_right(image)\n        image = tf.reverse(image, axis=[2])  # to BGR\n        return image\n\n    return training_mapper if isTrain else validation_mapper\n\n\n""""""\n====== Model & Evaluation =======\n""""""\n\n\ndef eval_classification(model, sessinit, dataflow):\n    """"""\n    Eval a classification model on the dataset. It assumes the model inputs are\n    named ""input"" and ""label"", and contains ""wrong-top1"" and ""wrong-top5"" in the graph.\n    """"""\n    pred_config = PredictConfig(\n        model=model,\n        session_init=sessinit,\n        input_names=[\'input\', \'label\'],\n        output_names=[\'wrong-top1\', \'wrong-top5\']\n    )\n    acc1, acc5 = RatioCounter(), RatioCounter()\n\n    # This does not have a visible improvement over naive predictor,\n    # but will have an improvement if image_dtype is set to float32.\n    pred = FeedfreePredictor(pred_config, StagingInput(QueueInput(dataflow), device=\'/gpu:0\'))\n    for _ in tqdm.trange(dataflow.size()):\n        top1, top5 = pred()\n        batch_size = top1.shape[0]\n        acc1.feed(top1.sum(), batch_size)\n        acc5.feed(top5.sum(), batch_size)\n\n    print(""Top1 Error: {}"".format(acc1.ratio))\n    print(""Top5 Error: {}"".format(acc5.ratio))\n\n\nclass ImageNetModel(ModelDesc):\n    image_shape = 224\n\n    """"""\n    uint8 instead of float32 is used as input type to reduce copy overhead.\n    It might hurt the performance a liiiitle bit.\n    The pretrained models were trained with float32.\n    """"""\n    image_dtype = tf.uint8\n\n    """"""\n    Either \'NCHW\' or \'NHWC\'\n    """"""\n    data_format = \'NCHW\'\n\n    """"""\n    Whether the image is BGR or RGB. If using DataFlow, then it should be BGR.\n    """"""\n    image_bgr = True\n\n    weight_decay = 1e-4\n\n    """"""\n    To apply on normalization parameters, use \'.*/W|.*/gamma|.*/beta\'\n    """"""\n    weight_decay_pattern = \'.*/W\'\n\n    """"""\n    Scale the loss, for whatever reasons (e.g., gradient averaging, fp16 training, etc)\n    """"""\n    loss_scale = 1.\n\n    """"""\n    Label smoothing (See tf.losses.softmax_cross_entropy)\n    """"""\n    label_smoothing = 0.\n\n    """"""\n    Accumulate gradients across several steps (by default 1, which means no accumulation across steps).\n    """"""\n    accum_grad = 1\n\n    def inputs(self):\n        return [tf.TensorSpec([None, self.image_shape, self.image_shape, 3], self.image_dtype, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = self.image_preprocess(image)\n        assert self.data_format in [\'NCHW\', \'NHWC\']\n        if self.data_format == \'NCHW\':\n            image = tf.transpose(image, [0, 3, 1, 2])\n\n        logits = self.get_logits(image)\n        tf.nn.softmax(logits, name=\'prob\')\n        loss = ImageNetModel.compute_loss_and_error(\n            logits, label, label_smoothing=self.label_smoothing)\n\n        if self.weight_decay > 0:\n            wd_loss = regularize_cost(self.weight_decay_pattern,\n                                      l2_regularizer(self.weight_decay),\n                                      name=\'l2_regularize_loss\')\n            add_moving_summary(loss, wd_loss)\n            total_cost = tf.add_n([loss, wd_loss], name=\'cost\')\n        else:\n            total_cost = tf.identity(loss, name=\'cost\')\n            add_moving_summary(total_cost)\n\n        if self.loss_scale != 1.:\n            logger.info(""Scaling the total loss by {} ..."".format(self.loss_scale))\n            return total_cost * self.loss_scale\n        else:\n            return total_cost\n\n    @abstractmethod\n    def get_logits(self, image):\n        """"""\n        Args:\n            image: 4D tensor of ``self.input_shape`` in ``self.data_format``\n\n        Returns:\n            Nx#class logits\n        """"""\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.1, trainable=False)\n        tf.summary.scalar(\'learning_rate-summary\', lr)\n        opt = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n        if self.accum_grad != 1:\n            opt = AccumGradOptimizer(opt, self.accum_grad)\n        return opt\n\n    def image_preprocess(self, image):\n        with tf.name_scope(\'image_preprocess\'):\n            if image.dtype.base_dtype != tf.float32:\n                image = tf.cast(image, tf.float32)\n            mean = [0.485, 0.456, 0.406]    # rgb\n            std = [0.229, 0.224, 0.225]\n            if self.image_bgr:\n                mean = mean[::-1]\n                std = std[::-1]\n            image_mean = tf.constant(mean, dtype=tf.float32) * 255.\n            image_std = tf.constant(std, dtype=tf.float32) * 255.\n            image = (image - image_mean) / image_std\n            return image\n\n    @staticmethod\n    def compute_loss_and_error(logits, label, label_smoothing=0.):\n        if label_smoothing != 0.:\n            nclass = logits.shape[-1]\n            label = tf.one_hot(label, nclass) if label.shape.ndims == 1 else label\n\n        if label.shape.ndims == 1:\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        else:\n            loss = tf.losses.softmax_cross_entropy(\n                label, logits, label_smoothing=label_smoothing,\n                reduction=tf.losses.Reduction.NONE)\n        loss = tf.reduce_mean(loss, name=\'xentropy-loss\')\n\n        def prediction_incorrect(logits, label, topk=1, name=\'incorrect_vector\'):\n            with tf.name_scope(\'prediction_incorrect\'):\n                x = tf.logical_not(tf.nn.in_top_k(logits, label, topk))\n            return tf.cast(x, tf.float32, name=name)\n\n        wrong = prediction_incorrect(logits, label, 1, name=\'wrong-top1\')\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top1\'))\n\n        wrong = prediction_incorrect(logits, label, 5, name=\'wrong-top5\')\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top5\'))\n        return loss\n\n    def create_predict_config(self, session_init):\n        """"""\n        Returns:\n            a :class:`PredictConfig` to be used for inference.\n            The predictor will take inputs and return probabilities.\n\n        Examples:\n\n            pred = OfflinePredictor(model.create_predict_config(SmartInit(args.load)))\n            prob = pred(NCHW_image)[0]  # Nx1000 probabilities\n        """"""\n        return PredictConfig(model=self, input_names=[\'input\'], output_names=[\'prob\'], session_init=session_init)\n\n\nif __name__ == \'__main__\':\n    import argparse\n    from tensorpack.dataflow import TestDataSpeed\n    from tensorpack.tfutils import get_default_sess_config\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data\', required=True)\n    parser.add_argument(\'--batch\', type=int, default=32)\n    parser.add_argument(\'--aug\', choices=[\'train\', \'val\'], default=\'val\')\n    parser.add_argument(\'--symbolic\', action=\'store_true\')\n    args = parser.parse_args()\n\n    if not args.symbolic:\n        augs = fbresnet_augmentor(args.aug == \'train\')\n        df = get_imagenet_dataflow(\n            args.data, \'train\', args.batch, augs)\n        # For val augmentor, Should get >100 it/s (i.e. 3k im/s) here on a decent E5 server.\n        TestDataSpeed(df).start()\n    else:\n        assert args.aug == \'train\'\n        data = get_imagenet_tfdata(args.data, \'train\', args.batch)\n\n        itr = data.make_initializable_iterator()\n        dp = itr.get_next()\n        dpop = tf.group(*dp)\n        with tf.Session(config=get_default_sess_config()) as sess:\n            sess.run(itr.initializer)\n            for _ in tqdm.trange(200):\n                sess.run(dpop)\n            for _ in tqdm.trange(5000, smoothing=0.1):\n                sess.run(dpop)\n'"
examples/ImageNetModels/inception-bn.py,25,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: inception-bn.py\n# Author: Yuxin Wu\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom imagenet_utils import fbresnet_augmentor, get_imagenet_dataflow\n\n# Change them if using different number of GPUs.\nTOTAL_BATCH_SIZE = 64 * 6\nNUM_GPU = 6\nBATCH_SIZE = TOTAL_BATCH_SIZE // NUM_GPU\nINPUT_SHAPE = 224\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec([None, INPUT_SHAPE, INPUT_SHAPE, 3], tf.float32, 'input'),\n                tf.TensorSpec([None], tf.int32, 'label')]\n\n    def build_graph(self, image, label):\n        image = image / 128.0\n\n        def inception(name, x, nr1x1, nr3x3r, nr3x3, nr233r, nr233, nrpool, pooltype):\n            stride = 2 if nr1x1 == 0 else 1\n            with tf.variable_scope(name):\n                outs = []\n                if nr1x1 != 0:\n                    outs.append(Conv2D('conv1x1', x, nr1x1, 1))\n                x2 = Conv2D('conv3x3r', x, nr3x3r, 1)\n                outs.append(Conv2D('conv3x3', x2, nr3x3, 3, strides=stride))\n\n                x3 = Conv2D('conv233r', x, nr233r, 1)\n                x3 = Conv2D('conv233a', x3, nr233, 3)\n                outs.append(Conv2D('conv233b', x3, nr233, 3, strides=stride))\n\n                if pooltype == 'max':\n                    x4 = MaxPooling('mpool', x, 3, stride, padding='SAME')\n                else:\n                    assert pooltype == 'avg'\n                    x4 = AvgPooling('apool', x, 3, stride, padding='SAME')\n                if nrpool != 0:  # pool + passthrough if nrpool == 0\n                    x4 = Conv2D('poolproj', x4, nrpool, 1)\n                outs.append(x4)\n                return tf.concat(outs, 3, name='concat')\n\n        with argscope(Conv2D, activation=BNReLU, use_bias=False):\n            l = (LinearWrap(image)\n                 .Conv2D('conv0', 64, 7, strides=2)\n                 .MaxPooling('pool0', 3, 2, padding='SAME')\n                 .Conv2D('conv1', 64, 1)\n                 .Conv2D('conv2', 192, 3)\n                 .MaxPooling('pool2', 3, 2, padding='SAME')())\n            # 28\n            l = inception('incep3a', l, 64, 64, 64, 64, 96, 32, 'avg')\n            l = inception('incep3b', l, 64, 64, 96, 64, 96, 64, 'avg')\n            l = inception('incep3c', l, 0, 128, 160, 64, 96, 0, 'max')\n\n            br1 = (LinearWrap(l)\n                   .Conv2D('loss1conv', 128, 1)\n                   .FullyConnected('loss1fc', 1024, activation=tf.nn.relu)\n                   .FullyConnected('loss1logit', 1000, activation=tf.identity)())\n            loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=br1, labels=label)\n            loss1 = tf.reduce_mean(loss1, name='loss1')\n\n            # 14\n            l = inception('incep4a', l, 224, 64, 96, 96, 128, 128, 'avg')\n            l = inception('incep4b', l, 192, 96, 128, 96, 128, 128, 'avg')\n            l = inception('incep4c', l, 160, 128, 160, 128, 160, 128, 'avg')\n            l = inception('incep4d', l, 96, 128, 192, 160, 192, 128, 'avg')\n            l = inception('incep4e', l, 0, 128, 192, 192, 256, 0, 'max')\n\n            br2 = Conv2D('loss2conv', l, 128, 1)\n            br2 = FullyConnected('loss2fc', br2, 1024, activation=tf.nn.relu)\n            br2 = FullyConnected('loss2logit', br2, 1000, activation=tf.identity)\n            loss2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=br2, labels=label)\n            loss2 = tf.reduce_mean(loss2, name='loss2')\n\n            # 7\n            l = inception('incep5a', l, 352, 192, 320, 160, 224, 128, 'avg')\n            l = inception('incep5b', l, 352, 192, 320, 192, 224, 128, 'max')\n            l = GlobalAvgPooling('gap', l)\n\n            logits = FullyConnected('linear', l, 1000, activation=tf.identity)\n        tf.nn.softmax(logits, name='output')\n        loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        loss3 = tf.reduce_mean(loss3, name='loss3')\n\n        cost = tf.add_n([loss3, 0.3 * loss2, 0.3 * loss1], name='weighted_cost')\n        add_moving_summary(cost, loss1, loss2, loss3)\n\n        def prediction_incorrect(logits, label, topk, name):\n            return tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, topk)), tf.float32, name=name)\n\n        wrong = prediction_incorrect(logits, label, 1, name='wrong-top1')\n        add_moving_summary(tf.reduce_mean(wrong, name='train_error_top1'))\n\n        wrong = prediction_incorrect(logits, label, 5, name='wrong-top5')\n        add_moving_summary(tf.reduce_mean(wrong, name='train_error_top5'))\n\n        # weight decay on all W of fc layers\n        wd_w = tf.train.exponential_decay(0.0002, get_global_step_var(),\n                                          80000, 0.7, True)\n        wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')\n\n        total_cost = tf.add_n([cost, wd_cost], name='cost')\n        add_moving_summary(wd_cost, total_cost)\n        return total_cost\n\n    def optimizer(self):\n        lr = tf.get_variable('learning_rate', initializer=0.045, trainable=False)\n        return tf.train.MomentumOptimizer(lr, 0.9)\n\n\ndef get_data(train_or_test):\n    isTrain = train_or_test == 'train'\n    augs = fbresnet_augmentor(isTrain)\n\n    meta = dataset.ILSVRCMeta()\n    pp_mean = meta.get_per_pixel_mean()\n    augs.append(imgaug.MapImage(lambda x: x - pp_mean[16:-16, 16:-16]))\n\n    ds = get_imagenet_dataflow(args.data, train_or_test, BATCH_SIZE, augs)\n    return ds\n\n\ndef get_config():\n    logger.auto_set_dir()\n    dataset_train = get_data('train')\n    dataset_val = get_data('val')\n\n    return TrainConfig(\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(dataset_val, [\n                ClassificationError('wrong-top1', 'val-top1-error'),\n                ClassificationError('wrong-top5', 'val-top5-error')]),\n            ScheduledHyperParamSetter('learning_rate',\n                                      [(8, 0.03), (14, 0.02), (17, 5e-3),\n                                       (19, 3e-3), (24, 1e-3), (26, 2e-4),\n                                       (30, 5e-5)])\n        ],\n        model=Model(),\n        steps_per_epoch=5000,\n        max_epoch=80,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')\n    parser.add_argument('--load', help='load model')\n    parser.add_argument('--data', help='ImageNet data root directory', required=True)\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\n    config = get_config()\n    config.session_init = SmartInit(args.load)\n    nr_tower = get_num_gpu()\n    assert nr_tower == NUM_GPU\n    launch_train_with_config(config, SyncMultiGPUTrainer(NUM_GPU))\n"""
examples/ImageNetModels/shufflenet.py,16,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: shufflenet.py\n\nimport argparse\nimport math\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import imgaug\nfrom tensorpack.tfutils import argscope, SmartInit, model_utils\nfrom tensorpack.tfutils.scope_utils import under_name_scope\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom imagenet_utils import ImageNetModel, eval_classification, get_imagenet_dataflow\n\n\n@layer_register(log_shape=True)\ndef DepthConv(x, out_channel, kernel_shape, padding=\'SAME\', stride=1,\n              W_init=None, activation=tf.identity):\n    in_shape = x.get_shape().as_list()\n    in_channel = in_shape[1]\n    assert out_channel % in_channel == 0, (out_channel, in_channel)\n    channel_mult = out_channel // in_channel\n\n    if W_init is None:\n        W_init = tf.variance_scaling_initializer(2.0)\n    kernel_shape = [kernel_shape, kernel_shape]\n    filter_shape = kernel_shape + [in_channel, channel_mult]\n\n    W = tf.get_variable(\'W\', filter_shape, initializer=W_init)\n    conv = tf.nn.depthwise_conv2d(x, W, [1, 1, stride, stride], padding=padding, data_format=\'NCHW\')\n    return activation(conv, name=\'output\')\n\n\n@under_name_scope()\ndef channel_shuffle(l, group):\n    in_shape = l.get_shape().as_list()\n    in_channel = in_shape[1]\n    assert in_channel % group == 0, in_channel\n    l = tf.reshape(l, [-1, in_channel // group, group] + in_shape[-2:])\n    l = tf.transpose(l, [0, 2, 1, 3, 4])\n    l = tf.reshape(l, [-1, in_channel] + in_shape[-2:])\n    return l\n\n\n@layer_register()\ndef shufflenet_unit(l, out_channel, group, stride):\n    in_shape = l.get_shape().as_list()\n    in_channel = in_shape[1]\n    shortcut = l\n\n    # ""We do not apply group convolution on the first pointwise layer\n    #  because the number of input channels is relatively small.""\n    first_split = group if in_channel > 24 else 1\n    l = Conv2D(\'conv1\', l, out_channel // 4, 1, split=first_split, activation=BNReLU)\n    l = channel_shuffle(l, group)\n    l = DepthConv(\'dconv\', l, out_channel // 4, 3, stride=stride)\n    l = BatchNorm(\'dconv_bn\', l)\n\n    l = Conv2D(\'conv2\', l,\n               out_channel if stride == 1 else out_channel - in_channel,\n               1, split=group)\n    l = BatchNorm(\'conv2_bn\', l)\n    if stride == 1:     # unit (b)\n        output = tf.nn.relu(shortcut + l)\n    else:   # unit (c)\n        shortcut = AvgPooling(\'avgpool\', shortcut, 3, 2, padding=\'SAME\')\n        output = tf.concat([shortcut, tf.nn.relu(l)], axis=1)\n    return output\n\n\n@layer_register()\ndef shufflenet_unit_v2(l, out_channel, stride):\n    if stride == 1:\n        shortcut, l = tf.split(l, 2, axis=1)\n    else:\n        shortcut, l = l, l\n    shortcut_channel = int(shortcut.shape[1])\n\n    l = Conv2D(\'conv1\', l, out_channel // 2, 1, activation=BNReLU)\n    l = DepthConv(\'dconv\', l, out_channel // 2, 3, stride=stride)\n    l = BatchNorm(\'dconv_bn\', l)\n    l = Conv2D(\'conv2\', l, out_channel - shortcut_channel, 1, activation=BNReLU)\n\n    if stride == 2:\n        shortcut = DepthConv(\'shortcut_dconv\', shortcut, shortcut_channel, 3, stride=2)\n        shortcut = BatchNorm(\'shortcut_dconv_bn\', shortcut)\n        shortcut = Conv2D(\'shortcut_conv\', shortcut, shortcut_channel, 1, activation=BNReLU)\n    output = tf.concat([shortcut, l], axis=1)\n    output = channel_shuffle(output, 2)\n    return output\n\n\n@layer_register(log_shape=True)\ndef shufflenet_stage(input, channel, num_blocks, group):\n    l = input\n    for i in range(num_blocks):\n        name = \'block{}\'.format(i)\n        if args.v2:\n            l = shufflenet_unit_v2(name, l, channel, 2 if i == 0 else 1)\n        else:\n            l = shufflenet_unit(name, l, channel, group, 2 if i == 0 else 1)\n    return l\n\n\nclass Model(ImageNetModel):\n    weight_decay = 4e-5\n\n    def get_logits(self, image):\n\n        with argscope([Conv2D, MaxPooling, AvgPooling, GlobalAvgPooling, BatchNorm], data_format=\'channels_first\'), \\\n                argscope(Conv2D, use_bias=False):\n\n            group = args.group\n            if not args.v2:\n                # Copied from the paper\n                channels = {\n                    3: [240, 480, 960],\n                    4: [272, 544, 1088],\n                    8: [384, 768, 1536]\n                }\n                mul = group * 4  # #chan has to be a multiple of this number\n                channels = [int(math.ceil(x * args.ratio / mul) * mul)\n                            for x in channels[group]]\n                # The first channel must be a multiple of group\n                first_chan = int(math.ceil(24 * args.ratio / group) * group)\n            else:\n                # Copied from the paper\n                channels = {\n                    0.5: [48, 96, 192],\n                    1.: [116, 232, 464]\n                }[args.ratio]\n                first_chan = 24\n\n            logger.info(""#Channels: "" + str([first_chan] + channels))\n\n            l = Conv2D(\'conv1\', image, first_chan, 3, strides=2, activation=BNReLU)\n            l = MaxPooling(\'pool1\', l, 3, 2, padding=\'SAME\')\n\n            l = shufflenet_stage(\'stage2\', l, channels[0], 4, group)\n            l = shufflenet_stage(\'stage3\', l, channels[1], 8, group)\n            l = shufflenet_stage(\'stage4\', l, channels[2], 4, group)\n\n            if args.v2:\n                l = Conv2D(\'conv5\', l, 1024, 1, activation=BNReLU)\n\n            l = GlobalAvgPooling(\'gap\', l)\n            logits = FullyConnected(\'linear\', l, 1000)\n            return logits\n\n\ndef get_data(name, batch):\n    isTrain = name == \'train\'\n\n    if isTrain:\n        augmentors = [\n            # use lighter augs if model is too small\n            imgaug.GoogleNetRandomCropAndResize(crop_area_fraction=(0.49 if args.ratio < 1 else 0.08, 1.)),\n            imgaug.RandomOrderAug(\n                [imgaug.BrightnessScale((0.6, 1.4), clip=False),\n                 imgaug.Contrast((0.6, 1.4), clip=False),\n                 imgaug.Saturation(0.4, rgb=False),\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\n                 imgaug.Lighting(0.1,\n                                 eigval=np.asarray(\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                                 eigvec=np.array(\n                                     [[-0.5675, 0.7192, 0.4009],\n                                      [-0.5808, -0.0045, -0.8140],\n                                      [-0.5836, -0.6948, 0.4203]],\n                                     dtype=\'float32\')[::-1, ::-1]\n                                 )]),\n            imgaug.Flip(horiz=True),\n        ]\n    else:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),\n            imgaug.CenterCrop((224, 224)),\n        ]\n    return get_imagenet_dataflow(\n        args.data, name, batch, augmentors)\n\n\ndef get_config(model, nr_tower):\n    batch = TOTAL_BATCH_SIZE // nr_tower\n\n    logger.info(""Running on {} towers. Batch size per tower: {}"".format(nr_tower, batch))\n    dataset_train = get_data(\'train\', batch)\n    dataset_val = get_data(\'val\', batch)\n\n    step_size = 1280000 // TOTAL_BATCH_SIZE\n    max_iter = 3 * 10**5\n    max_epoch = (max_iter // step_size) + 1\n    callbacks = [\n        ModelSaver(),\n        ScheduledHyperParamSetter(\'learning_rate\',\n                                  [(0, 0.5), (max_iter, 0)],\n                                  interp=\'linear\', step_based=True),\n        EstimatedTimeLeft()\n    ]\n    infs = [ClassificationError(\'wrong-top1\', \'val-error-top1\'),\n            ClassificationError(\'wrong-top5\', \'val-error-top5\')]\n    if nr_tower == 1:\n        # single-GPU inference with queue prefetch\n        callbacks.append(InferenceRunner(QueueInput(dataset_val), infs))\n    else:\n        # multi-GPU inference (with mandatory queue prefetch)\n        callbacks.append(DataParallelInferenceRunner(\n            dataset_val, infs, list(range(nr_tower))))\n\n    return TrainConfig(\n        model=model,\n        dataflow=dataset_train,\n        callbacks=callbacks,\n        steps_per_epoch=step_size,\n        max_epoch=max_epoch,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--data\', help=\'ILSVRC dataset dir\')\n    parser.add_argument(\'-r\', \'--ratio\', type=float, default=0.5, choices=[1., 0.5])\n    parser.add_argument(\'--group\', type=int, default=8, choices=[3, 4, 8],\n                        help=""Number of groups for ShuffleNetV1"")\n    parser.add_argument(\'--v2\', action=\'store_true\', help=\'Use ShuffleNetV2\')\n    parser.add_argument(\'--batch\', type=int, default=1024, help=\'total batch size\')\n    parser.add_argument(\'--load\', help=\'path to load a model from\')\n    parser.add_argument(\'--eval\', action=\'store_true\')\n    parser.add_argument(\'--flops\', action=\'store_true\', help=\'print flops and exit\')\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    if args.v2 and args.group != parser.get_default(\'group\'):\n        logger.error(""group= is not used in ShuffleNetV2!"")\n\n    if args.batch != 1024:\n        logger.warn(""Total batch size != 1024, you need to change other hyperparameters to get the same results."")\n    TOTAL_BATCH_SIZE = args.batch\n\n    model = Model()\n\n    if args.eval:\n        batch = 128    # something that can run on one gpu\n        ds = get_data(\'val\', batch)\n        eval_classification(model, SmartInit(args.load), ds)\n    elif args.flops:\n        # manually build the graph with batch=1\n        with TowerContext(\'\', is_training=False):\n            model.build_graph(\n                tf.placeholder(tf.float32, [1, 224, 224, 3], \'input\'),\n                tf.placeholder(tf.int32, [1], \'label\')\n            )\n        model_utils.describe_trainable_vars()\n\n        tf.profiler.profile(\n            tf.get_default_graph(),\n            cmd=\'op\',\n            options=tf.profiler.ProfileOptionBuilder.float_operation())\n        logger.info(""Note that TensorFlow counts flops in a different way from the paper."")\n        logger.info(""TensorFlow counts multiply+add as two flops, however the paper counts them ""\n                    ""as 1 flop because it can be executed in one instruction."")\n    else:\n        if args.v2:\n            name = ""ShuffleNetV2-{}x"".format(args.ratio)\n        else:\n            name = ""ShuffleNetV1-{}x-g{}"".format(args.ratio, args.group)\n        logger.set_logger_dir(os.path.join(\'train_log\', name))\n\n        nr_tower = max(get_num_gpu(), 1)\n        config = get_config(model, nr_tower)\n        config.session_init = SmartInit(args.load)\n        launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_tower))\n'"
examples/ImageNetModels/vgg16.py,18,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: vgg16.py\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils import argscope\nfrom tensorpack.tfutils.summary import *\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom imagenet_utils import ImageNetModel, fbresnet_augmentor, get_imagenet_dataflow\n\n\ndef GroupNorm(x, group, gamma_initializer=tf.constant_initializer(1.)):\n    """"""\n    https://arxiv.org/abs/1803.08494\n    More code that reproduces the paper can be found at https://github.com/ppwwyyxx/GroupNorm-reproduce/.\n    """"""\n    shape = x.get_shape().as_list()\n    ndims = len(shape)\n    assert ndims == 4, shape\n    chan = shape[1]\n    assert chan % group == 0, chan\n    group_size = chan // group\n\n    orig_shape = tf.shape(x)\n    h, w = orig_shape[2], orig_shape[3]\n\n    x = tf.reshape(x, tf.stack([-1, group, group_size, h, w]))\n\n    mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\n\n    new_shape = [1, group, group_size, 1, 1]\n\n    beta = tf.get_variable(\'beta\', [chan], initializer=tf.constant_initializer())\n    beta = tf.reshape(beta, new_shape)\n\n    gamma = tf.get_variable(\'gamma\', [chan], initializer=gamma_initializer)\n    gamma = tf.reshape(gamma, new_shape)\n\n    out = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-5, name=\'output\')\n    return tf.reshape(out, orig_shape, name=\'output\')\n\n\ndef convnormrelu(x, name, chan):\n    x = Conv2D(name, x, chan, 3)\n    if args.norm == \'bn\':\n        x = BatchNorm(name + \'_bn\', x)\n    elif args.norm == \'gn\':\n        with tf.variable_scope(name + \'_gn\'):\n            x = GroupNorm(x, 32)\n    x = tf.nn.relu(x, name=name + \'_relu\')\n    return x\n\n\nclass Model(ImageNetModel):\n    weight_decay = 5e-4\n\n    def get_logits(self, image):\n        with argscope(Conv2D, kernel_initializer=tf.variance_scaling_initializer(scale=2.)), \\\n                argscope([Conv2D, MaxPooling, BatchNorm], data_format=\'channels_first\'):\n            logits = (LinearWrap(image)\n                      .apply(convnormrelu, \'conv1_1\', 64)\n                      .apply(convnormrelu, \'conv1_2\', 64)\n                      .MaxPooling(\'pool1\', 2)\n                      # 112\n                      .apply(convnormrelu, \'conv2_1\', 128)\n                      .apply(convnormrelu, \'conv2_2\', 128)\n                      .MaxPooling(\'pool2\', 2)\n                      # 56\n                      .apply(convnormrelu, \'conv3_1\', 256)\n                      .apply(convnormrelu, \'conv3_2\', 256)\n                      .apply(convnormrelu, \'conv3_3\', 256)\n                      .MaxPooling(\'pool3\', 2)\n                      # 28\n                      .apply(convnormrelu, \'conv4_1\', 512)\n                      .apply(convnormrelu, \'conv4_2\', 512)\n                      .apply(convnormrelu, \'conv4_3\', 512)\n                      .MaxPooling(\'pool4\', 2)\n                      # 14\n                      .apply(convnormrelu, \'conv5_1\', 512)\n                      .apply(convnormrelu, \'conv5_2\', 512)\n                      .apply(convnormrelu, \'conv5_3\', 512)\n                      .MaxPooling(\'pool5\', 2)\n                      # 7\n                      .FullyConnected(\'fc6\', 4096,\n                                      kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n                      .tf.nn.relu(name=\'fc6_relu\')\n                      .Dropout(\'drop0\', rate=0.5)\n                      .FullyConnected(\'fc7\', 4096,\n                                      kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n                      .tf.nn.relu(name=\'fc7_relu\')\n                      .Dropout(\'drop1\', rate=0.5)\n                      .FullyConnected(\'fc8\', 1000,\n                                      kernel_initializer=tf.random_normal_initializer(stddev=0.01))())\n        add_param_summary((\'.*\', [\'histogram\', \'rms\']))\n        return logits\n\n\ndef get_data(name, batch):\n    isTrain = name == \'train\'\n    augmentors = fbresnet_augmentor(isTrain)\n    return get_imagenet_dataflow(args.data, name, batch, augmentors)\n\n\ndef get_config():\n    nr_tower = max(get_num_gpu(), 1)\n    batch = args.batch\n    total_batch = batch * nr_tower\n    assert total_batch >= 256   # otherwise the learning rate warmup is wrong.\n    BASE_LR = 0.01 * (total_batch / 256.)\n\n    logger.info(""Running on {} towers. Batch size per tower: {}"".format(nr_tower, batch))\n    dataset_train = get_data(\'train\', batch)\n    dataset_val = get_data(\'val\', batch)\n\n    infs = [ClassificationError(\'wrong-top1\', \'val-error-top1\'),\n            ClassificationError(\'wrong-top5\', \'val-error-top5\')]\n    callbacks = [\n        ModelSaver(),\n        GPUUtilizationTracker(),\n        EstimatedTimeLeft(),\n        ScheduledHyperParamSetter(\n            \'learning_rate\',\n            [(0, 0.01), (3, max(BASE_LR, 0.01))], interp=\'linear\'),\n        ScheduledHyperParamSetter(\n            \'learning_rate\',\n            [(30, BASE_LR * 1e-1), (60, BASE_LR * 1e-2), (80, BASE_LR * 1e-3)]),\n        DataParallelInferenceRunner(\n            dataset_val, infs, list(range(nr_tower))),\n    ]\n\n    input = QueueInput(dataset_train)\n    input = StagingInput(input, nr_stage=1)\n    return TrainConfig(\n        model=Model(),\n        data=input,\n        callbacks=callbacks,\n        steps_per_epoch=1281167 // total_batch,\n        max_epoch=100,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--data\', help=\'ILSVRC dataset dir\')\n    parser.add_argument(\'--batch\', type=int, default=32, help=\'batch per GPU\')\n    parser.add_argument(\'--norm\', choices=[\'none\', \'bn\', \'gn\'], default=\'none\')\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    logger.set_logger_dir(os.path.join(\'train_log\', \'vgg16-norm={}\'.format(args.norm)))\n\n    config = get_config()\n    nr_tower = max(get_num_gpu(), 1)\n    trainer = SyncMultiGPUTrainerReplicated(nr_tower)\n    launch_train_with_config(config, trainer)\n'"
examples/OpticalFlow/flownet2.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Patrick Wieschollek <mail@patwie.com>\n\nimport argparse\nimport glob\nimport os\nimport cv2\n\nfrom tensorpack import *\nfrom tensorpack.utils import viz\n\nimport flownet_models as models\nfrom helper import Flow\n\n\ndef apply(model, model_path, images, ground_truth=None):\n    left = cv2.imread(images[0])\n    h, w = left.shape[:2]\n    newh = (h // 64) * 64\n    neww = (w // 64) * 64\n    aug = imgaug.CenterCrop((newh, neww))\n    left = aug.augment(left)\n\n    predict_func = OfflinePredictor(PredictConfig(\n        model=model(height=newh, width=neww),\n        session_init=SmartInit(model_path),\n        input_names=[\'left\', \'right\'],\n        output_names=[\'prediction\']))\n\n    for idx, right in enumerate(images[1:]):\n        right = aug.augment(cv2.imread(right))\n\n        left_input, right_input = [x.astype(\'float32\').transpose(2, 0, 1)[None, ...]\n                                   for x in [left, right]]\n        output = predict_func(left_input, right_input)[0].transpose(0, 2, 3, 1)\n        flow = Flow()\n\n        img = flow.visualize(output[0])\n        patches = [left, right, img * 255.]\n        if ground_truth is not None:\n            patches.append(flow.visualize(Flow.read(ground_truth)) * 255.)\n        img = viz.stack_patches(patches, 2, 2)\n\n        cv2.imshow(\'flow output\', img)\n        cv2.imwrite(\'flow_output{:03d}.png\'.format(idx), img)\n        cv2.waitKey(0)\n\n        left = right\n\n\nclass SintelData(DataFlow):\n\n    def __init__(self, data_path):\n        super(SintelData, self).__init__()\n        self.data_path = data_path\n        self.path_prefix = os.path.join(data_path, \'flow\')\n        assert os.path.isdir(self.path_prefix), self.path_prefix\n        self.flows = glob.glob(os.path.join(self.path_prefix, \'*\', \'*.flo\'))\n\n    def size(self):\n        return len(self.flows)\n\n    def __iter__(self):\n        for flow_path in self.flows:\n            input_path = flow_path.replace(\n                self.path_prefix, os.path.join(self.data_path, \'clean\'))\n            frame_id = int(input_path[-8:-4])\n            input_a_path = \'%s%04i.png\' % (input_path[:-8], frame_id)\n            input_b_path = \'%s%04i.png\' % (input_path[:-8], frame_id + 1)\n\n            input_a = cv2.imread(input_a_path)\n            input_b = cv2.imread(input_b_path)\n            flow = Flow.read(flow_path)\n\n            # most implementation just crop the center\n            # which seems to be accepted practise\n            h, w = input_a.shape[:2]\n            newh = (h // 64) * 64\n            neww = (w // 64) * 64\n            aug = imgaug.CenterCrop((newh, neww))\n            input_a = aug.augment(input_a)\n            input_b = aug.augment(input_b)\n            flow = aug.augment(flow)\n            yield [input_a, input_b, flow]\n\n\ndef inference(model, model_path, sintel_path):\n    ds = SintelData(sintel_path)\n\n    def nhwc2nchw(dp):\n        return [dp[0].transpose(2, 0, 1),\n                dp[1].transpose(2, 0, 1),\n                dp[2].transpose(2, 0, 1)]\n\n    ds = MapData(ds, nhwc2nchw)\n    ds = BatchData(ds, 1)\n    ds.reset_state()\n\n    # look at shape information (all images in Sintel has the same shape)\n    h, w = next(ds.__iter__())[0].shape[2:]\n\n    pred = PredictConfig(\n        model=model(height=h, width=w),\n        session_init=SmartInit(model_path),\n        input_names=[\'left\', \'right\', \'gt_flow\'],\n        output_names=[\'epe\', \'prediction\'])\n    pred = SimpleDatasetPredictor(pred, ds)\n\n    avg_epe, count_epe = 0, 0\n\n    for o in pred.get_result():\n        avg_epe += o[0]\n        count_epe += 1\n\n    print(\'average endpoint error (AEE): %f\' % (float(avg_epe) / float(count_epe)))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--load\', help=\'path to the model\', required=True)\n    parser.add_argument(\'--model\', help=\'model\',\n                        choices=[\'flownet2\', \'flownet2-s\', \'flownet2-c\'], required=True)\n    parser.add_argument(\'--images\', nargs=""+"",\n                        help=\'a list of equally-sized images. FlowNet will be applied to all consecutive pairs\')\n    parser.add_argument(\'--gt\', help=\'path to ground truth flow\')\n    parser.add_argument(\'--sintel_path\', help=\'path to sintel dataset\')\n    args = parser.parse_args()\n\n    model = {\'flownet2-s\': models.FlowNet2S,\n             \'flownet2-c\': models.FlowNet2C,\n             \'flownet2\': models.FlowNet2}[args.model]\n\n    if args.sintel_path:\n        inference(model, args.load, args.sintel_path)\n    else:\n        assert len(args.images) >= 2\n        apply(model, args.load, args.images, args.gt)\n'"
examples/OpticalFlow/flownet_models.py,176,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Patrick Wieschollek <mail@patwie.com>\n\n\nimport tensorflow as tf\n\nfrom tensorpack import ModelDesc, argscope, enable_argscope_for_module\n\nenable_argscope_for_module(tf.layers)\n\n# FlowNet2 follows the convention of FlowNet and scales the flow prediction by\n# factor 20 (note: max_displacement is 20)\nDISP_SCALE = 20.\n\n\ndef pad(x, p=3):\n    """"""Pad tensor in H, W\n\n    Remarks:\n        TensorFlow uses ""ceil(input_spatial_shape[i] / strides[i])"" rather than explicit padding\n        like Caffe, pyTorch does. Hence, we need to pad here beforehand.\n\n    Args:\n        x (tf.tensor): incoming tensor\n        p (int, optional): padding for H, W\n\n    Returns:\n        tf.tensor: padded tensor\n    """"""\n    return tf.pad(x, [[0, 0], [0, 0], [p, p], [p, p]])\n\n\ndef channel_norm(x):\n    return tf.norm(x, axis=1, keep_dims=True)\n\n\ndef correlation(ina, inb,\n                kernel_size, max_displacement,\n                stride_1, stride_2,\n                pad, data_format):\n    """"""\n    Correlation Cost Volume computation.\n\n    This is a fallback Python-only implementation, specialized just for FlowNet2.\n    It takes a lot of memory and is slow.\n\n    If you know to compile a custom op yourself, it\'s better to use the cuda implementation here:\n    https://github.com/PatWie/tensorflow-recipes/tree/master/OpticalFlow/user_ops\n    """"""\n    assert pad == max_displacement\n    assert kernel_size == 1\n    assert data_format == \'NCHW\'\n    assert max_displacement % stride_2 == 0\n    assert stride_1 == 1\n\n    D = int(max_displacement / stride_2 * 2) + 1  # D^2 == number of correlations per spatial location\n\n    b, c, h, w = ina.shape.as_list()\n\n    inb = tf.pad(inb, [[0, 0], [0, 0], [pad, pad], [pad, pad]])\n\n    res = []\n    for k1 in range(0, D):\n        start_h = k1 * stride_2\n        for k2 in range(0, D):\n            start_w = k2 * stride_2\n            s = tf.slice(inb, [0, 0, start_h, start_w], [-1, -1, h, w])\n            ans = tf.reduce_mean(ina * s, axis=1, keepdims=True)\n            res.append(ans)\n    res = tf.concat(res, axis=1)   # ND^2HW\n    return res\n\n\ndef resample(img, flow):\n    # img, NCHW\n    # flow, N2HW\n    B = tf.shape(img)[0]\n    c = tf.shape(img)[1]\n    h = tf.shape(img)[2]\n    w = tf.shape(img)[3]\n    img_flat = tf.reshape(tf.transpose(img, [0, 2, 3, 1]), [-1, c])\n\n    dx, dy = tf.unstack(flow, axis=1)\n    xf, yf = tf.meshgrid(tf.cast(tf.range(w), tf.float32), tf.cast(tf.range(h), tf.float32))\n    xf = xf + dx\n    yf = yf + dy\n\n    alpha = tf.expand_dims(xf - tf.floor(xf), axis=-1)\n    beta = tf.expand_dims(yf - tf.floor(yf), axis=-1)\n\n    xL = tf.clip_by_value(tf.cast(tf.floor(xf), dtype=tf.int32), 0, w - 1)\n    xR = tf.clip_by_value(tf.cast(tf.floor(xf) + 1, dtype=tf.int32), 0, w - 1)\n    yT = tf.clip_by_value(tf.cast(tf.floor(yf), dtype=tf.int32), 0, h - 1)\n    yB = tf.clip_by_value(tf.cast(tf.floor(yf) + 1, dtype=tf.int32), 0, h - 1)\n\n    batch_ids = tf.tile(tf.expand_dims(tf.expand_dims(tf.range(B), axis=-1), axis=-1), [1, h, w])\n\n    def get(y, x):\n        idx = tf.reshape(batch_ids * h * w + y * w + x, [-1])\n        idx = tf.cast(idx, tf.int32)\n        return tf.gather(img_flat, idx)\n\n    val = tf.zeros_like(alpha)\n    val += (1 - alpha) * (1 - beta) * tf.reshape(get(yT, xL), [-1, h, w, c])\n    val += (0 + alpha) * (1 - beta) * tf.reshape(get(yT, xR), [-1, h, w, c])\n    val += (1 - alpha) * (0 + beta) * tf.reshape(get(yB, xL), [-1, h, w, c])\n    val += (0 + alpha) * (0 + beta) * tf.reshape(get(yB, xR), [-1, h, w, c])\n\n    # we need to enforce the channel_dim known during compile-time here\n    shp = img.shape.as_list()\n    return tf.reshape(tf.transpose(val, [0, 3, 1, 2]), [-1, shp[1], h, w])\n\n\ndef resize(x, mode, factor=4):\n    """"""Resize input tensor with unkown input-shape by a factor\n\n    Args:\n        x (tf.Tensor): tensor NCHW\n        factor (int, optional): resize factor for H, W\n\n    Note:\n        Differences here against Caffe have huge impacts on the\n        quality of the predictions.\n\n    Returns:\n        tf.Tensor: resized tensor NCHW\n    """"""\n    assert mode in [\'bilinear\', \'nearest\'], mode\n    shp = tf.shape(x)[2:] * factor\n    # NCHW -> NHWC\n    x = tf.transpose(x, [0, 2, 3, 1])\n    if mode == \'bilinear\':\n        x = tf.image.resize_bilinear(x, shp, align_corners=True)\n    else:\n        # better approximation of what Caffe is doing\n        x = tf.image.resize_nearest_neighbor(x, shp, align_corners=False)\n    # NHWC -> NCHW\n    return tf.transpose(x, [0, 3, 1, 2])\n\n\nclass FlowNetBase(ModelDesc):\n    def __init__(self, height=None, width=None):\n        self.height = height\n        self.width = width\n\n    def inputs(self):\n        return [tf.TensorSpec((1, 3, self.height, self.width), tf.float32, \'left\'),\n                tf.TensorSpec((1, 3, self.height, self.width), tf.float32, \'right\'),\n                tf.TensorSpec((1, 2, self.height, self.width), tf.float32, \'gt_flow\')]\n\n    def graph_structure(self, inputs):\n        """"""\n        Args:\n            inputs: [2, C, H, W]\n        """"""\n        raise NotImplementedError()\n\n    def preprocess(self, left, right):\n        x = tf.concat([left, right], axis=0)  # 2CHW\n        rgb_mean = tf.reduce_mean(x, axis=[0, 2, 3], keep_dims=True)\n        return (x - rgb_mean) / 255.\n\n    def postprocess(self, prediction):\n        return resize(prediction * DISP_SCALE, mode=\'bilinear\')\n\n    def build_graph(self, left, right, gt_flow):\n        x = self.preprocess(left, right)\n        prediction = self.graph_structure(x)\n        prediction = self.postprocess(prediction)\n        tf.identity(prediction, name=""prediction"")\n        # endpoint error\n        tf.reduce_mean(tf.norm(prediction - gt_flow, axis=1), name=\'epe\')\n\n\nclass FlowNet2(FlowNetBase):\n\n    def postprocess(self, prediction):\n        return prediction\n\n    def graph_structure(self, x):\n        x1, x2 = tf.split(x, 2, axis=0)\n        x1x2 = tf.reshape(x, [1, -1, self.height, self.width])  # 1(2C)HW\n\n        # FlowNet-C\n        flownetc_flow2 = FlowNet2C().graph_structure(x)\n        flownetc_flow = resize(flownetc_flow2 * DISP_SCALE, mode=\'bilinear\')\n\n        resampled_img1 = resample(x2, flownetc_flow)\n        norm_diff_img0 = channel_norm(x1 - resampled_img1)\n\n        # FlowNet-S\n        concat1 = tf.concat([x1x2, resampled_img1, flownetc_flow / DISP_SCALE, norm_diff_img0], axis=1)\n        with tf.variable_scope(\'flownet_s1\'):\n            flownets1_flow2 = FlowNet2S().graph_structure(concat1, standalone=False)\n        flownets1_flow = resize(flownets1_flow2 * DISP_SCALE, mode=\'bilinear\')\n\n        resampled_img1 = resample(x2, flownets1_flow)\n        norm_diff_img0 = channel_norm(x1 - resampled_img1)\n\n        # FlowNet-S\n        concat2 = tf.concat([x1x2, resampled_img1, flownets1_flow / DISP_SCALE, norm_diff_img0], axis=1)\n        with tf.variable_scope(\'flownet_s2\'):\n            flownets2_flow2 = FlowNet2S().graph_structure(concat2, standalone=False)\n        flownets2_flow = resize(flownets2_flow2 * DISP_SCALE, mode=\'nearest\')\n\n        norm_flownets2_flow = channel_norm(flownets2_flow)\n        diff_flownets2_flow = resample(x2, flownets2_flow)\n        diff_flownets2_img1 = channel_norm(x1 - diff_flownets2_flow)\n\n        # FlowNet-SD\n        with tf.variable_scope(\'flownet_sd\'):\n            flownetsd_flow = self.flownet2_sd(x1x2)\n\n        norm_flownetsd_flow = channel_norm(flownetsd_flow)\n        diff_flownetsd_flow = resample(x2, flownetsd_flow)\n        diff_flownetsd_img1 = channel_norm(x1 - diff_flownetsd_flow)\n\n        concat3 = tf.concat([x1,\n                            flownetsd_flow, flownets2_flow,\n                            norm_flownetsd_flow, norm_flownets2_flow,\n                            diff_flownetsd_img1, diff_flownets2_img1], axis=1)\n\n        # FlowNet-Fusion\n        with tf.variable_scope(\'flownet_fusion\'):\n            flownetfusion_flow = self.flownet2_fusion(concat3)\n\n        return flownetfusion_flow\n\n    def flownet2_fusion(self, x):\n        """"""\n        Architecture in Table 4 of FlowNet 2.0.\n\n        Args:\n            x: NCHW tensor, where C=11 is the concatenation of 7 items of [3, 2, 2, 1, 1, 1, 1] channels.\n        """"""\n        with argscope([tf.layers.conv2d], activation=lambda x: tf.nn.leaky_relu(x, 0.1),\n                      padding=\'valid\', strides=2, kernel_size=3,\n                      data_format=\'channels_first\'), \\\n            argscope([tf.layers.conv2d_transpose], padding=\'same\', activation=tf.identity,\n                     data_format=\'channels_first\', strides=2, kernel_size=4):\n            conv0 = tf.layers.conv2d(pad(x, 1), 64, name=\'conv0\', strides=1)\n\n            x = tf.layers.conv2d(pad(conv0, 1), 64, name=\'conv1\')\n            conv1 = tf.layers.conv2d(pad(x, 1), 128, name=\'conv1_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv1, 1), 128, name=\'conv2\')\n            conv2 = tf.layers.conv2d(pad(x, 1), 128, name=\'conv2_1\', strides=1)\n\n            flow2 = tf.layers.conv2d(pad(conv2, 1), 2, name=\'predict_flow2\', strides=1, activation=tf.identity)\n            flow2_up = tf.layers.conv2d_transpose(flow2, 2, name=\'upsampled_flow2_to_1\')\n            x = tf.layers.conv2d_transpose(conv2, 32, name=\'deconv1\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat1 = tf.concat([conv1, x, flow2_up], axis=1, name=\'concat1\')\n            interconv1 = tf.layers.conv2d(pad(concat1, 1), 32, strides=1, name=\'inter_conv1\', activation=tf.identity)\n\n            flow1 = tf.layers.conv2d(pad(interconv1, 1), 2, name=\'predict_flow1\', strides=1, activation=tf.identity)\n            flow1_up = tf.layers.conv2d_transpose(flow1, 2, name=\'upsampled_flow1_to_0\')\n            x = tf.layers.conv2d_transpose(concat1, 16, name=\'deconv0\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat0 = tf.concat([conv0, x, flow1_up], axis=1, name=\'concat0\')\n            interconv0 = tf.layers.conv2d(pad(concat0, 1), 16, strides=1, name=\'inter_conv0\', activation=tf.identity)\n            flow0 = tf.layers.conv2d(pad(interconv0, 1), 2, name=\'predict_flow0\', strides=1, activation=tf.identity)\n\n            return tf.identity(flow0, name=\'flow2\')\n\n    def flownet2_sd(self, x):\n        """"""\n        Architecture in Table 3 of FlowNet 2.0.\n\n        Args:\n            x: concatenation of two inputs, of shape [1, 2xC, H, W]\n        """"""\n        with argscope([tf.layers.conv2d], activation=lambda x: tf.nn.leaky_relu(x, 0.1),\n                      padding=\'valid\', strides=2, kernel_size=3,\n                      data_format=\'channels_first\'), \\\n            argscope([tf.layers.conv2d_transpose], padding=\'same\', activation=tf.identity,\n                     data_format=\'channels_first\', strides=2, kernel_size=4):\n            x = tf.layers.conv2d(pad(x, 1), 64, name=\'conv0\', strides=1)\n\n            x = tf.layers.conv2d(pad(x, 1), 64, name=\'conv1\')\n            conv1 = tf.layers.conv2d(pad(x, 1), 128, name=\'conv1_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv1, 1), 128, name=\'conv2\')\n            conv2 = tf.layers.conv2d(pad(x, 1), 128, name=\'conv2_1\', strides=1)\n\n            x = tf.layers.conv2d(pad(conv2, 1), 256, name=\'conv3\')\n            conv3 = tf.layers.conv2d(pad(x, 1), 256, name=\'conv3_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv3, 1), 512, name=\'conv4\')\n            conv4 = tf.layers.conv2d(pad(x, 1), 512, name=\'conv4_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv4, 1), 512, name=\'conv5\')\n            conv5 = tf.layers.conv2d(pad(x, 1), 512, name=\'conv5_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv5, 1), 1024, name=\'conv6\')\n            conv6 = tf.layers.conv2d(pad(x, 1), 1024, name=\'conv6_1\', strides=1)\n\n            flow6 = tf.layers.conv2d(pad(conv6, 1), 2, name=\'predict_flow6\', strides=1, activation=tf.identity)\n            flow6_up = tf.layers.conv2d_transpose(flow6, 2, name=\'upsampled_flow6_to_5\')\n            x = tf.layers.conv2d_transpose(conv6, 512, name=\'deconv5\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat5 = tf.concat([conv5, x, flow6_up], axis=1, name=\'concat5\')\n            interconv5 = tf.layers.conv2d(pad(concat5, 1), 512, strides=1, name=\'inter_conv5\', activation=tf.identity)\n            flow5 = tf.layers.conv2d(pad(interconv5, 1), 2, name=\'predict_flow5\', strides=1, activation=tf.identity)\n            flow5_up = tf.layers.conv2d_transpose(flow5, 2, name=\'upsampled_flow5_to_4\')\n            x = tf.layers.conv2d_transpose(concat5, 256, name=\'deconv4\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat4 = tf.concat([conv4, x, flow5_up], axis=1, name=\'concat4\')\n            interconv4 = tf.layers.conv2d(pad(concat4, 1), 256, strides=1, name=\'inter_conv4\', activation=tf.identity)\n            flow4 = tf.layers.conv2d(pad(interconv4, 1), 2, name=\'predict_flow4\', strides=1, activation=tf.identity)\n            flow4_up = tf.layers.conv2d_transpose(flow4, 2, name=\'upsampled_flow4_to_3\')\n            x = tf.layers.conv2d_transpose(concat4, 128, name=\'deconv3\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat3 = tf.concat([conv3, x, flow4_up], axis=1, name=\'concat3\')\n            interconv3 = tf.layers.conv2d(pad(concat3, 1), 128, strides=1, name=\'inter_conv3\', activation=tf.identity)\n            flow3 = tf.layers.conv2d(pad(interconv3, 1), 2, name=\'predict_flow3\', strides=1, activation=tf.identity)\n            flow3_up = tf.layers.conv2d_transpose(flow3, 2, name=\'upsampled_flow3_to_2\')\n            x = tf.layers.conv2d_transpose(concat3, 64, name=\'deconv2\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat2 = tf.concat([conv2, x, flow3_up], axis=1, name=\'concat2\')\n            interconv2 = tf.layers.conv2d(pad(concat2, 1), 64, strides=1, name=\'inter_conv2\', activation=tf.identity)\n            flow2 = tf.layers.conv2d(pad(interconv2, 1), 2, name=\'predict_flow2\', strides=1, activation=tf.identity)\n\n            return resize(flow2 / DISP_SCALE, mode=\'nearest\')\n\n\nclass FlowNet2S(FlowNetBase):\n    def graph_structure(self, x, standalone=True):\n        """"""\n        Architecture of FlowNetSimple in Figure 2 of FlowNet 1.0.\n\n        Args:\n            x: 2CHW if standalone==True, else NCHW where C=12 is a concatenation\n                of 5 tensors of [3, 3, 3, 2, 1] channels.\n            standalone: If True, this model is used to predict flow from two inputs.\n                If False, this model is used as part of the FlowNet2.\n        """"""\n        if standalone:\n            x = tf.concat(tf.split(x, 2, axis=0), axis=1)\n\n        with argscope([tf.layers.conv2d], activation=lambda x: tf.nn.leaky_relu(x, 0.1),\n                      padding=\'valid\', strides=2, kernel_size=3,\n                      data_format=\'channels_first\'), \\\n            argscope([tf.layers.conv2d_transpose], padding=\'same\', activation=tf.identity,\n                     data_format=\'channels_first\', strides=2, kernel_size=4):\n            x = tf.layers.conv2d(pad(x, 3), 64, kernel_size=7, name=\'conv1\')\n            conv2 = tf.layers.conv2d(pad(x, 2), 128, kernel_size=5, name=\'conv2\')\n            x = tf.layers.conv2d(pad(conv2, 2), 256, kernel_size=5, name=\'conv3\')\n            conv3 = tf.layers.conv2d(pad(x, 1), 256, name=\'conv3_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv3, 1), 512, name=\'conv4\')\n            conv4 = tf.layers.conv2d(pad(x, 1), 512, name=\'conv4_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv4, 1), 512, name=\'conv5\')\n            conv5 = tf.layers.conv2d(pad(x, 1), 512, name=\'conv5_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv5, 1), 1024, name=\'conv6\')\n            conv6 = tf.layers.conv2d(pad(x, 1), 1024, name=\'conv6_1\', strides=1)\n\n            flow6 = tf.layers.conv2d(pad(conv6, 1), 2, name=\'predict_flow6\', strides=1, activation=tf.identity)\n            flow6_up = tf.layers.conv2d_transpose(flow6, 2, name=\'upsampled_flow6_to_5\', use_bias=False)\n            x = tf.layers.conv2d_transpose(conv6, 512, name=\'deconv5\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat5 = tf.concat([conv5, x, flow6_up], axis=1, name=\'concat5\')\n            flow5 = tf.layers.conv2d(pad(concat5, 1), 2, name=\'predict_flow5\', strides=1, activation=tf.identity)\n            flow5_up = tf.layers.conv2d_transpose(flow5, 2, name=\'upsampled_flow5_to_4\', use_bias=False)\n            x = tf.layers.conv2d_transpose(concat5, 256, name=\'deconv4\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat4 = tf.concat([conv4, x, flow5_up], axis=1, name=\'concat4\')\n            flow4 = tf.layers.conv2d(pad(concat4, 1), 2, name=\'predict_flow4\', strides=1, activation=tf.identity)\n            flow4_up = tf.layers.conv2d_transpose(flow4, 2, name=\'upsampled_flow4_to_3\', use_bias=False)\n            x = tf.layers.conv2d_transpose(concat4, 128, name=\'deconv3\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat3 = tf.concat([conv3, x, flow4_up], axis=1, name=\'concat3\')\n            flow3 = tf.layers.conv2d(pad(concat3, 1), 2, name=\'predict_flow3\', strides=1, activation=tf.identity)\n            flow3_up = tf.layers.conv2d_transpose(flow3, 2, name=\'upsampled_flow3_to_2\', use_bias=False)\n            x = tf.layers.conv2d_transpose(concat3, 64, name=\'deconv2\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat2 = tf.concat([conv2, x, flow3_up], axis=1, name=\'concat2\')\n            flow2 = tf.layers.conv2d(pad(concat2, 1), 2, name=\'predict_flow2\', strides=1, activation=tf.identity)\n\n            return tf.identity(flow2, name=\'flow2\')\n\n\nclass FlowNet2C(FlowNetBase):\n    def graph_structure(self, x1x2):\n        """"""\n        Architecture of FlowNetCorr in Figure 2 of FlowNet 1.0.\n        Args:\n            x: 2CHW.\n        """"""\n        with argscope([tf.layers.conv2d], activation=lambda x: tf.nn.leaky_relu(x, 0.1),\n                      padding=\'valid\', strides=2, kernel_size=3,\n                      data_format=\'channels_first\'), \\\n            argscope([tf.layers.conv2d_transpose], padding=\'same\', activation=tf.identity,\n                     data_format=\'channels_first\', strides=2, kernel_size=4):\n\n            # extract features\n            x = tf.layers.conv2d(pad(x1x2, 3), 64, kernel_size=7, name=\'conv1\')\n            conv2 = tf.layers.conv2d(pad(x, 2), 128, kernel_size=5, name=\'conv2\')\n            conv3 = tf.layers.conv2d(pad(conv2, 2), 256, kernel_size=5, name=\'conv3\')\n\n            conv2a, _ = tf.split(conv2, 2, axis=0)\n            conv3a, conv3b = tf.split(conv3, 2, axis=0)\n\n            corr = correlation(conv3a, conv3b,\n                               kernel_size=1,\n                               max_displacement=20,\n                               stride_1=1,\n                               stride_2=2,\n                               pad=20, data_format=\'NCHW\')\n            corr = tf.nn.leaky_relu(corr, 0.1)\n\n            conv_redir = tf.layers.conv2d(conv3a, 32, kernel_size=1, strides=1, name=\'conv_redir\')\n\n            in_conv3_1 = tf.concat([conv_redir, corr], axis=1, name=\'in_conv3_1\')\n            conv3_1 = tf.layers.conv2d(pad(in_conv3_1, 1), 256, name=\'conv3_1\', strides=1)\n\n            x = tf.layers.conv2d(pad(conv3_1, 1), 512, name=\'conv4\')\n            conv4 = tf.layers.conv2d(pad(x, 1), 512, name=\'conv4_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv4, 1), 512, name=\'conv5\')\n            conv5 = tf.layers.conv2d(pad(x, 1), 512, name=\'conv5_1\', strides=1)\n            x = tf.layers.conv2d(pad(conv5, 1), 1024, name=\'conv6\')\n            conv6 = tf.layers.conv2d(pad(x, 1), 1024, name=\'conv6_1\', strides=1)\n\n            flow6 = tf.layers.conv2d(pad(conv6, 1), 2, name=\'predict_flow6\', strides=1, activation=tf.identity)\n            flow6_up = tf.layers.conv2d_transpose(flow6, 2, name=\'upsampled_flow6_to_5\')\n            x = tf.layers.conv2d_transpose(conv6, 512, name=\'deconv5\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            # return flow6\n            concat5 = tf.concat([conv5, x, flow6_up], axis=1, name=\'concat5\')\n            flow5 = tf.layers.conv2d(pad(concat5, 1), 2, name=\'predict_flow5\', strides=1, activation=tf.identity)\n            flow5_up = tf.layers.conv2d_transpose(flow5, 2, name=\'upsampled_flow5_to_4\')\n            x = tf.layers.conv2d_transpose(concat5, 256, name=\'deconv4\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat4 = tf.concat([conv4, x, flow5_up], axis=1, name=\'concat4\')\n            flow4 = tf.layers.conv2d(pad(concat4, 1), 2, name=\'predict_flow4\', strides=1, activation=tf.identity)\n            flow4_up = tf.layers.conv2d_transpose(flow4, 2, name=\'upsampled_flow4_to_3\')\n            x = tf.layers.conv2d_transpose(concat4, 128, name=\'deconv3\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat3 = tf.concat([conv3_1, x, flow4_up], axis=1, name=\'concat3\')\n            flow3 = tf.layers.conv2d(pad(concat3, 1), 2, name=\'predict_flow3\', strides=1, activation=tf.identity)\n            flow3_up = tf.layers.conv2d_transpose(flow3, 2, name=\'upsampled_flow3_to_2\')\n            x = tf.layers.conv2d_transpose(concat3, 64, name=\'deconv2\', activation=lambda x: tf.nn.leaky_relu(x, 0.1))\n\n            concat2 = tf.concat([conv2a, x, flow3_up], axis=1, name=\'concat2\')\n            flow2 = tf.layers.conv2d(pad(concat2, 1), 2, name=\'predict_flow2\', strides=1, activation=tf.identity)\n\n            return tf.identity(flow2, name=\'flow2\')\n'"
examples/OpticalFlow/helper.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nHelper for Optical Flow visualization\n""""""\n\nimport numpy as np\n\n\nclass Flow(object):\n    """"""\n    based on https://github.com/cgtuebingen/learning-blind-motion-deblurring/blob/master/synthblur/src/flow.cpp#L44\n    """"""\n    def __init__(self):\n        super(Flow, self).__init__()\n        self.wheel = None\n        self._construct_wheel()\n\n    @staticmethod\n    def read(file):\n        # https://stackoverflow.com/a/44906777/7443104\n        with open(file, \'rb\') as f:\n            magic = np.fromfile(f, np.float32, count=1)\n            if 202021.25 != magic:\n                raise Exception(\'Magic number incorrect. Invalid .flo file\')\n            else:\n                w = np.fromfile(f, np.int32, count=1)[0]\n                h = np.fromfile(f, np.int32, count=1)[0]\n                data = np.fromfile(f, np.float32, count=2 * w * h)\n                return np.resize(data, (h, w, 2))\n\n    def _construct_wheel(self):\n        k = 0\n\n        RY, YG, GC = 15, 6, 4\n        YG, GC, CB = 6, 4, 11\n        BM, MR = 13, 6\n\n        self.wheel = np.zeros((55, 3), dtype=np.float32)\n\n        for i in range(RY):\n            self.wheel[k] = np.array([255., 255. * i / float(RY), 0])\n            k += 1\n\n        for i in range(YG):\n            self.wheel[k] = np.array([255. - 255. * i / float(YG), 255., 0])\n            k += 1\n\n        for i in range(GC):\n            self.wheel[k] = np.array([0, 255., 255. * i / float(GC)])\n            k += 1\n\n        for i in range(CB):\n            self.wheel[k] = np.array([0, 255. - 255. * i / float(CB), 255.])\n            k += 1\n\n        for i in range(BM):\n            self.wheel[k] = np.array([255. * i / float(BM), 0, 255.])\n            k += 1\n\n        for i in range(MR):\n            self.wheel[k] = np.array([255., 0, 255. - 255. * i / float(MR)])\n            k += 1\n\n        self.wheel = self.wheel / 255.\n\n    def visualize(self, nnf):\n        assert len(nnf.shape) == 3\n        assert nnf.shape[2] == 2\n\n        RY, YG, GC = 15, 6, 4\n        YG, GC, CB = 6, 4, 11\n        BM, MR = 13, 6\n        NCOLS = RY + YG + GC + CB + BM + MR\n\n        fx = nnf[:, :, 0].astype(np.float32)\n        fy = nnf[:, :, 1].astype(np.float32)\n\n        h, w = fx.shape[:2]\n        fx = fx.reshape([-1])\n        fy = fy.reshape([-1])\n\n        rad = np.sqrt(fx * fx + fy * fy)\n\n        normalizer = max(rad.max(), 1)\n        # This parameter controls how sensitive the visualization is to small displacement\n        # The smaller it is, the more sensitive the visualization is.\n        # We don\'t let it be smaller than 1 since we don\'t want to be sensitive to noise.\n\n        a = np.arctan2(-fy, -fx) / np.pi\n        fk = (a + 1.0) / 2.0 * (NCOLS - 1)\n        k0 = fk.astype(np.int32)\n        k1 = (k0 + 1) % NCOLS\n        f = (fk - k0).astype(np.float32)\n\n        color0 = self.wheel[k0, :]\n        color1 = self.wheel[k1, :]\n\n        f = np.stack([f, f, f], axis=-1)\n        color = (1 - f) * color0 + f * color1\n\n        color = 1 - (np.expand_dims(rad, axis=-1) / normalizer) * (1 - color)\n\n        return color.reshape(h, w, 3)[:, :, ::-1]\n\n\nif __name__ == \'__main__\':\n    import cv2\n    nnf = Flow.read(\'/tmp/data2/07446_flow.flo\')\n    v = Flow()\n    rgb = v.visualize(nnf)\n    cv2.imshow(\'rgb\', rgb)\n    cv2.waitKey(0)\n'"
examples/PennTreebank/PTB-LSTM.py,25,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: PTB-LSTM.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils import gradproc, optimizer, summary\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.argtools import memoized_ignoreargs\nfrom tensorpack.utils.fs import download, get_dataset_path\n\nimport reader as tfreader\nfrom reader import ptb_producer\n\nrnn = tf.contrib.rnn\n\nSEQ_LEN = 35\nHIDDEN_SIZE = 650\nNUM_LAYER = 2\nBATCH = 20\nDROPOUT = 0.5\nVOCAB_SIZE = None\nTRAIN_URL = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'\nVALID_URL = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'\nTEST_URL = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'\n\n\n@memoized_ignoreargs\ndef get_PennTreeBank(data_dir=None):\n    if data_dir is None:\n        data_dir = get_dataset_path('ptb_data')\n    if not os.path.isfile(os.path.join(data_dir, 'ptb.train.txt')):\n        download(TRAIN_URL, data_dir)\n        download(VALID_URL, data_dir)\n        download(TEST_URL, data_dir)\n    word_to_id = tfreader._build_vocab(os.path.join(data_dir, 'ptb.train.txt'))\n    data3 = [np.asarray(tfreader._file_to_word_ids(os.path.join(data_dir, fname), word_to_id))\n             for fname in ['ptb.train.txt', 'ptb.valid.txt', 'ptb.test.txt']]\n    return data3, word_to_id\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, SEQ_LEN), tf.int32, 'input'),\n                tf.TensorSpec((None, SEQ_LEN), tf.int32, 'nextinput')]\n\n    def build_graph(self, input, nextinput):\n        initializer = tf.random_uniform_initializer(-0.05, 0.05)\n\n        def get_basic_cell():\n            cell = rnn.BasicLSTMCell(num_units=HIDDEN_SIZE, forget_bias=0.0, reuse=tf.get_variable_scope().reuse)\n            if self.training:\n                cell = rnn.DropoutWrapper(cell, output_keep_prob=1 - DROPOUT)\n            return cell\n\n        cell = rnn.MultiRNNCell([get_basic_cell() for _ in range(NUM_LAYER)])\n\n        def get_v(n):\n            return tf.get_variable(n, [BATCH, HIDDEN_SIZE],\n                                   trainable=False,\n                                   initializer=tf.constant_initializer())\n\n        state_var = [rnn.LSTMStateTuple(\n            get_v('c{}'.format(k)), get_v('h{}'.format(k))) for k in range(NUM_LAYER)]\n        self.state = state_var = tuple(state_var)\n\n        embeddingW = tf.get_variable('embedding', [VOCAB_SIZE, HIDDEN_SIZE], initializer=initializer)\n        input_feature = tf.nn.embedding_lookup(embeddingW, input)  # B x seqlen x hiddensize\n        input_feature = Dropout(input_feature, keep_prob=1 - DROPOUT)\n\n        with tf.variable_scope('LSTM', initializer=initializer):\n            input_list = tf.unstack(input_feature, num=SEQ_LEN, axis=1)  # seqlen x (Bxhidden)\n            outputs, last_state = rnn.static_rnn(cell, input_list, state_var, scope='rnn')\n\n        # update the hidden state after a rnn loop completes\n        update_state_ops = []\n        for k in range(NUM_LAYER):\n            update_state_ops.extend([\n                tf.assign(state_var[k].c, last_state[k].c),\n                tf.assign(state_var[k].h, last_state[k].h)])\n\n        # seqlen x (Bxrnnsize)\n        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])  # (Bxseqlen) x hidden\n        logits = FullyConnected('fc', output, VOCAB_SIZE,\n                                activation=tf.identity, kernel_initializer=initializer,\n                                bias_initializer=initializer)\n        xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=tf.reshape(nextinput, [-1]))\n\n        with tf.control_dependencies(update_state_ops):\n            cost = tf.truediv(tf.reduce_sum(xent_loss),\n                              tf.cast(BATCH, tf.float32), name='cost')  # log-perplexity\n\n        perpl = tf.exp(cost / SEQ_LEN, name='perplexity')\n        summary.add_moving_summary(perpl, cost)\n        return cost\n\n    def reset_lstm_state(self):\n        s = self.state\n        z = tf.zeros_like(s[0].c)\n        ops = []\n        for k in range(NUM_LAYER):\n            ops.append(s[k].c.assign(z))\n            ops.append(s[k].h.assign(z))\n        return tf.group(*ops, name='reset_lstm_state')\n\n    def optimizer(self):\n        lr = tf.get_variable('learning_rate', initializer=1.0, trainable=False)\n        opt = tf.train.GradientDescentOptimizer(lr)\n        return optimizer.apply_grad_processors(\n            opt, [gradproc.GlobalNormClip(5)])\n\n\ndef get_config():\n    logger.auto_set_dir()\n\n    data3, wd2id = get_PennTreeBank()\n    global VOCAB_SIZE\n    VOCAB_SIZE = len(wd2id)\n    steps_per_epoch = (data3[0].shape[0] // BATCH - 1) // SEQ_LEN\n\n    train_data = TensorInput(\n        lambda: ptb_producer(data3[0], BATCH, SEQ_LEN),\n        steps_per_epoch)\n    val_data = TensorInput(\n        lambda: ptb_producer(data3[1], BATCH, SEQ_LEN),\n        (data3[1].shape[0] // BATCH - 1) // SEQ_LEN)\n\n    test_data = TensorInput(\n        lambda: ptb_producer(data3[2], BATCH, SEQ_LEN),\n        (data3[2].shape[0] // BATCH - 1) // SEQ_LEN)\n\n    M = Model()\n    return TrainConfig(\n        data=train_data,\n        model=M,\n        callbacks=[\n            ModelSaver(),\n            HyperParamSetterWithFunc(\n                'learning_rate',\n                lambda e, x: x * 0.80 if e > 6 else x),\n            RunOp(lambda: M.reset_lstm_state()),\n            InferenceRunner(val_data, [ScalarStats(['cost'])]),\n            RunOp(lambda: M.reset_lstm_state()),\n            InferenceRunner(\n                test_data,\n                [ScalarStats(['cost'], prefix='test')], tower_name='InferenceTowerTest'),\n            RunOp(lambda: M.reset_lstm_state()),\n            CallbackFactory(\n                trigger=lambda self:\n                [self.trainer.monitors.put_scalar(\n                    'validation_perplexity',\n                    np.exp(self.trainer.monitors.get_latest('validation_cost') / SEQ_LEN)),\n                 self.trainer.monitors.put_scalar(\n                     'test_perplexity',\n                     np.exp(self.trainer.monitors.get_latest('test_cost') / SEQ_LEN))]\n            ),\n        ],\n        max_epoch=70,\n    )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', type=int, help='the GPU to use')\n    parser.add_argument('--load', help='load model')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\n    config = get_config()\n    config.session_init = SmartInit(args.load)\n    launch_train_with_config(config, SimpleTrainer())\n"""
examples/PennTreebank/reader.py,12,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# flake8: noqa\n\n\n""""""Utilities for parsing PTB text files.""""""\nfrom __future__ import absolute_import, division, print_function\nimport collections\nimport os\nimport tensorflow as tf\n\n\ndef _read_words(filename):\n  with tf.gfile.GFile(filename, ""rb"") as f:\n    return f.read().decode(""utf-8"").replace(""\\n"", ""<eos>"").split()\n\n\ndef _build_vocab(filename):\n  data = _read_words(filename)\n\n  counter = collections.Counter(data)\n  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n  words, _ = list(zip(*count_pairs))\n  word_to_id = dict(zip(words, range(len(words))))\n\n  return word_to_id\n\n\ndef _file_to_word_ids(filename, word_to_id):\n  data = _read_words(filename)\n  return [word_to_id[word] for word in data if word in word_to_id]\n\n\ndef ptb_raw_data(data_path=None):\n  """"""Load PTB raw data from data directory ""data_path"".\n\n  Reads PTB text files, converts strings to integer ids,\n  and performs mini-batching of the inputs.\n\n  The PTB dataset comes from Tomas Mikolov\'s webpage:\n\n  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n\n  Returns:\n    tuple (train_data, valid_data, test_data, vocabulary)\n    where each of the data objects can be passed to PTBIterator.\n  """"""\n\n  train_path = os.path.join(data_path, ""ptb.train.txt"")\n  valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n  test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n  word_to_id = _build_vocab(train_path)\n  train_data = _file_to_word_ids(train_path, word_to_id)\n  valid_data = _file_to_word_ids(valid_path, word_to_id)\n  test_data = _file_to_word_ids(test_path, word_to_id)\n  vocabulary = len(word_to_id)\n  return train_data, valid_data, test_data, vocabulary\n\n\ndef ptb_producer(raw_data, batch_size, num_steps, name=None):\n  """"""Iterate on the raw PTB data.\n\n  This chunks up raw_data into batches of examples and returns Tensors that\n  are drawn from these batches.\n\n  Args:\n    raw_data: one of the raw data outputs from ptb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n    name: the name of this operation (optional).\n\n  Returns:\n    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n    of the tuple is the same data time-shifted to the right by one.\n\n  Raises:\n    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n  """"""\n  with tf.name_scope(name, ""PTBProducer"", [raw_data, batch_size, num_steps]):\n    raw_data = tf.convert_to_tensor(raw_data, name=""raw_data"", dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n                      [batch_size, batch_len])\n\n    epoch_size = (batch_len - 1) // num_steps\n    assertion = tf.assert_positive(\n        epoch_size,\n        message=""epoch_size == 0, decrease batch_size or num_steps"")\n    with tf.control_dependencies([assertion]):\n      epoch_size = tf.identity(epoch_size, name=""epoch_size"")\n\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n    x = tf.strided_slice(data, [0, i * num_steps],\n                         [batch_size, (i + 1) * num_steps])\n    x.set_shape([batch_size, num_steps])\n    y = tf.strided_slice(data, [0, i * num_steps + 1],\n                         [batch_size, (i + 1) * num_steps + 1])\n    y.set_shape([batch_size, num_steps])\n    return x, y\n'"
examples/ResNet/cifar10-preact18-mixup.py,18,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: cifar10-preact18-mixup.py\n# Author: Tao Hu <taohu620@gmail.com>,  Yauheni Selivonchyk <y.selivonchyk@gmail.com>\n\nimport argparse\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.summary import *\n\nBATCH_SIZE = 128\nCLASS_NUM = 10\n\nLR_SCHEDULE = [(0, 0.1), (100, 0.01), (150, 0.001)]\nWEIGHT_DECAY = 1e-4\n\nFILTER_SIZES = [64, 128, 256, 512]\nMODULE_SIZES = [2, 2, 2, 2]\n\n\ndef preactivation_block(input, num_filters, stride=1):\n    num_filters_in = input.get_shape().as_list()[1]\n\n    # residual\n    net = BNReLU(input)\n    residual = Conv2D(\'conv1\', net, num_filters, kernel_size=3, strides=stride, use_bias=False, activation=BNReLU)\n    residual = Conv2D(\'conv2\', residual, num_filters, kernel_size=3, strides=1, use_bias=False)\n\n    # identity\n    shortcut = input\n    if stride != 1 or num_filters_in != num_filters:\n        shortcut = Conv2D(\'shortcut\', net, num_filters, kernel_size=1, strides=stride, use_bias=False)\n\n    return shortcut + residual\n\n\nclass ResNet_Cifar(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec([None, 32, 32, 3], tf.float32, \'input\'),\n                tf.TensorSpec([None, CLASS_NUM], tf.float32, \'label\')]\n\n    def build_graph(self, image, label):\n        assert tf.test.is_gpu_available()\n\n        MEAN_IMAGE = tf.constant([0.4914, 0.4822, 0.4465], dtype=tf.float32)\n        STD_IMAGE = tf.constant([0.2023, 0.1994, 0.2010], dtype=tf.float32)\n        image = ((image / 255.0) - MEAN_IMAGE) / STD_IMAGE\n        image = tf.transpose(image, [0, 3, 1, 2])\n\n        pytorch_default_init = tf.variance_scaling_initializer(scale=1.0 / 3, mode=\'fan_in\', distribution=\'uniform\')\n        with argscope([Conv2D, BatchNorm, GlobalAvgPooling], data_format=\'channels_first\'), \\\n                argscope(Conv2D, kernel_initializer=pytorch_default_init):\n            net = Conv2D(\'conv0\', image, 64, kernel_size=3, strides=1, use_bias=False)\n            for i, blocks_in_module in enumerate(MODULE_SIZES):\n                for j in range(blocks_in_module):\n                    stride = 2 if j == 0 and i > 0 else 1\n                    with tf.variable_scope(""res%d.%d"" % (i, j)):\n                        net = preactivation_block(net, FILTER_SIZES[i], stride)\n            net = GlobalAvgPooling(\'gap\', net)\n            logits = FullyConnected(\'linear\', net, CLASS_NUM,\n                                    kernel_initializer=tf.random_normal_initializer(stddev=1e-3))\n\n        ce_cost = tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)\n        ce_cost = tf.reduce_mean(ce_cost, name=\'cross_entropy_loss\')\n\n        single_label = tf.cast(tf.argmax(label, axis=1), tf.int32)\n        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), tf.float32, name=\'wrong_vector\')\n        # monitor training error\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train_error\'), ce_cost)\n        add_param_summary((\'.*/W\', [\'histogram\']))\n\n        # weight decay on all W matrixes. including convolutional layers\n        wd_cost = tf.multiply(WEIGHT_DECAY, regularize_cost(\'.*\', tf.nn.l2_loss), name=\'wd_cost\')\n\n        return tf.add_n([ce_cost, wd_cost], name=\'cost\')\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.1, trainable=False)\n        opt = tf.train.MomentumOptimizer(lr, 0.9)\n        return opt\n\n\ndef get_data(train_or_test, isMixup, alpha):\n    isTrain = train_or_test == \'train\'\n    ds = dataset.Cifar10(train_or_test)\n    if isTrain:\n        augmentors = [\n            imgaug.CenterPaste((40, 40)),\n            imgaug.RandomCrop((32, 32)),\n            imgaug.Flip(horiz=True),\n        ]\n        ds = AugmentImageComponent(ds, augmentors)\n\n    batch = BATCH_SIZE\n    ds = BatchData(ds, batch, remainder=not isTrain)\n\n    def f(dp):\n        images, labels = dp\n        one_hot_labels = np.eye(CLASS_NUM)[labels]  # one hot coding\n        if not isTrain or not isMixup:\n            return [images, one_hot_labels]\n\n        # mixup implementation:\n        # Note that for larger images, it\'s more efficient to do mixup on GPUs (i.e. in the graph)\n        weight = np.random.beta(alpha, alpha, BATCH_SIZE)\n        x_weight = weight.reshape(BATCH_SIZE, 1, 1, 1)\n        y_weight = weight.reshape(BATCH_SIZE, 1)\n        index = np.random.permutation(BATCH_SIZE)\n\n        x1, x2 = images, images[index]\n        x = x1 * x_weight + x2 * (1 - x_weight)\n        y1, y2 = one_hot_labels, one_hot_labels[index]\n        y = y1 * y_weight + y2 * (1 - y_weight)\n        return [x, y]\n\n    ds = MapData(ds, f)\n    return ds\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--mixup\', help=\'enable mixup\', action=\'store_true\')\n    parser.add_argument(\'--alpha\', default=1, type=float, help=\'alpha in mixup\')\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    log_folder = \'train_log/cifar10-preact18%s\' % (\'-mixup\' if args.mixup else \'\')\n    logger.set_logger_dir(os.path.join(log_folder))\n\n    dataset_train = get_data(\'train\', args.mixup, args.alpha)\n    dataset_test = get_data(\'test\', args.mixup, args.alpha)\n\n    config = TrainConfig(\n        model=ResNet_Cifar(),\n        data=QueueInput(dataset_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                            [ScalarStats(\'cost\'), ClassificationError(\'wrong_vector\')]),\n            ScheduledHyperParamSetter(\'learning_rate\', LR_SCHEDULE)\n        ],\n        max_epoch=200,\n        steps_per_epoch=len(dataset_train),\n        session_init=SmartInit(args.load)\n    )\n    launch_train_with_config(config, SimpleTrainer())\n'"
examples/ResNet/cifar10-resnet.py,17,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: cifar10-resnet.py\n# Author: Yuxin Wu\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.summary import add_moving_summary, add_param_summary\nfrom tensorpack.utils.gpu import get_num_gpu\n\n""""""\nCIFAR10 ResNet example. See:\nDeep Residual Learning for Image Recognition, arxiv:1512.03385\nThis implementation uses the variants proposed in:\nIdentity Mappings in Deep Residual Networks, arxiv:1603.05027\n\nI can reproduce the results on 2 TitanX for\nn=5, about 7.1% val error after 67k steps (20.4 step/s)\nn=18, about 5.95% val error after 80k steps (5.6 step/s, not converged)\nn=30: a 182-layer network, about 5.6% val error after 51k steps (3.4 step/s)\nThis model uses the whole training set instead of a train-val split.\n\nTo train:\n    ./cifar10-resnet.py --gpu 0,1\n""""""\n\nBATCH_SIZE = 128\nNUM_UNITS = None\n\n\nclass Model(ModelDesc):\n\n    def __init__(self, n):\n        super(Model, self).__init__()\n        self.n = n\n\n    def inputs(self):\n        return [tf.TensorSpec([None, 32, 32, 3], tf.float32, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = image / 128.0\n        assert tf.test.is_gpu_available()\n        image = tf.transpose(image, [0, 3, 1, 2])\n\n        def residual(name, l, increase_dim=False, first=False):\n            shape = l.get_shape().as_list()\n            in_channel = shape[1]\n\n            if increase_dim:\n                out_channel = in_channel * 2\n                stride1 = 2\n            else:\n                out_channel = in_channel\n                stride1 = 1\n\n            with tf.variable_scope(name):\n                b1 = l if first else BNReLU(l)\n                c1 = Conv2D(\'conv1\', b1, out_channel, strides=stride1, activation=BNReLU)\n                c2 = Conv2D(\'conv2\', c1, out_channel)\n                if increase_dim:\n                    l = AvgPooling(\'pool\', l, 2)\n                    l = tf.pad(l, [[0, 0], [in_channel // 2, in_channel // 2], [0, 0], [0, 0]])\n\n                l = c2 + l\n                return l\n\n        with argscope([Conv2D, AvgPooling, BatchNorm, GlobalAvgPooling], data_format=\'channels_first\'), \\\n                argscope(Conv2D, use_bias=False, kernel_size=3,\n                         kernel_initializer=tf.variance_scaling_initializer(scale=2.0, mode=\'fan_out\')):\n            l = Conv2D(\'conv0\', image, 16, activation=BNReLU)\n            l = residual(\'res1.0\', l, first=True)\n            for k in range(1, self.n):\n                l = residual(\'res1.{}\'.format(k), l)\n            # 32,c=16\n\n            l = residual(\'res2.0\', l, increase_dim=True)\n            for k in range(1, self.n):\n                l = residual(\'res2.{}\'.format(k), l)\n            # 16,c=32\n\n            l = residual(\'res3.0\', l, increase_dim=True)\n            for k in range(1, self.n):\n                l = residual(\'res3.\' + str(k), l)\n            l = BNReLU(\'bnlast\', l)\n            # 8,c=64\n            l = GlobalAvgPooling(\'gap\', l)\n\n        logits = FullyConnected(\'linear\', l, 10)\n        tf.nn.softmax(logits, name=\'output\')\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')\n\n        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name=\'wrong_vector\')\n        # monitor training error\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train_error\'))\n\n        # weight decay on all W of fc layers\n        wd_w = tf.train.exponential_decay(0.0002, get_global_step_var(),\n                                          480000, 0.2, True)\n        wd_cost = tf.multiply(wd_w, regularize_cost(\'.*/W\', tf.nn.l2_loss), name=\'wd_cost\')\n        add_moving_summary(cost, wd_cost)\n\n        add_param_summary((\'.*/W\', [\'histogram\']))   # monitor W\n        return tf.add_n([cost, wd_cost], name=\'cost\')\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.01, trainable=False)\n        opt = tf.train.MomentumOptimizer(lr, 0.9)\n        return opt\n\n\ndef get_data(train_or_test):\n    isTrain = train_or_test == \'train\'\n    ds = dataset.Cifar10(train_or_test)\n    pp_mean = ds.get_per_pixel_mean((\'train\',))\n    if isTrain:\n        augmentors = [\n            imgaug.CenterPaste((40, 40)),\n            imgaug.RandomCrop((32, 32)),\n            imgaug.Flip(horiz=True),\n            imgaug.MapImage(lambda x: x - pp_mean),\n        ]\n    else:\n        augmentors = [\n            imgaug.MapImage(lambda x: x - pp_mean)\n        ]\n    ds = AugmentImageComponent(ds, augmentors)\n    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n    if isTrain:\n        ds = MultiProcessRunner(ds, 3, 2)\n    return ds\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'-n\', \'--num_units\',\n                        help=\'number of units in each stage\',\n                        type=int, default=18)\n    parser.add_argument(\'--load\', help=\'load model for training\')\n    args = parser.parse_args()\n    NUM_UNITS = args.num_units\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    logger.auto_set_dir()\n\n    dataset_train = get_data(\'train\')\n    dataset_test = get_data(\'test\')\n\n    config = TrainConfig(\n        model=Model(n=NUM_UNITS),\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                            [ScalarStats(\'cost\'), ClassificationError(\'wrong_vector\')]),\n            ScheduledHyperParamSetter(\'learning_rate\',\n                                      [(1, 0.1), (82, 0.01), (123, 0.001), (300, 0.0002)])\n        ],\n        max_epoch=400,\n        session_init=SmartInit(args.load),\n    )\n    num_gpu = max(get_num_gpu(), 1)\n    launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(num_gpu))\n'"
examples/ResNet/imagenet-resnet.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: imagenet-resnet.py\n\nimport argparse\nimport os\n\nfrom tensorpack import QueueInput, TFDatasetInput, logger\nfrom tensorpack.callbacks import *\nfrom tensorpack.dataflow import FakeData\nfrom tensorpack.models import *\nfrom tensorpack.tfutils import argscope, SmartInit\nfrom tensorpack.train import SyncMultiGPUTrainerReplicated, TrainConfig, launch_train_with_config\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom imagenet_utils import ImageNetModel, eval_classification, get_imagenet_dataflow, get_imagenet_tfdata\nimport resnet_model\nfrom resnet_model import preact_group, resnet_backbone, resnet_group\n\n\nclass Model(ImageNetModel):\n    def __init__(self, depth, mode=\'resnet\'):\n        self.mode = mode\n        basicblock = getattr(resnet_model, mode + \'_basicblock\', None)\n        bottleneck = getattr(resnet_model, mode + \'_bottleneck\', None)\n        self.num_blocks, self.block_func = {\n            18: ([2, 2, 2, 2], basicblock),\n            34: ([3, 4, 6, 3], basicblock),\n            50: ([3, 4, 6, 3], bottleneck),\n            101: ([3, 4, 23, 3], bottleneck),\n            152: ([3, 8, 36, 3], bottleneck)\n        }[depth]\n        assert self.block_func is not None, \\\n            ""(mode={}, depth={}) not implemented!"".format(mode, depth)\n\n    def get_logits(self, image):\n        with argscope([Conv2D, MaxPooling, GlobalAvgPooling, BatchNorm], data_format=self.data_format):\n            return resnet_backbone(\n                image, self.num_blocks,\n                preact_group if self.mode == \'preact\' else resnet_group, self.block_func)\n\n\ndef get_config(model):\n    nr_tower = max(get_num_gpu(), 1)\n    assert args.batch % nr_tower == 0\n    batch = args.batch // nr_tower\n\n    logger.info(""Running on {} towers. Batch size per tower: {}"".format(nr_tower, batch))\n    if batch < 32 or batch > 64:\n        logger.warn(""Batch size per tower not in [32, 64]. This probably will lead to worse accuracy than reported."")\n    if args.fake:\n        data = QueueInput(FakeData(\n            [[batch, 224, 224, 3], [batch]], 1000, random=False, dtype=\'uint8\'))\n        callbacks = []\n    else:\n        if args.symbolic:\n            data = TFDatasetInput(get_imagenet_tfdata(args.data, \'train\', batch))\n        else:\n            data = QueueInput(get_imagenet_dataflow(args.data, \'train\', batch))\n\n        START_LR = 0.1\n        BASE_LR = START_LR * (args.batch / 256.0)\n        callbacks = [\n            ModelSaver(),\n            EstimatedTimeLeft(),\n            ScheduledHyperParamSetter(\n                \'learning_rate\', [\n                    (0, min(START_LR, BASE_LR)), (30, BASE_LR * 1e-1), (60, BASE_LR * 1e-2),\n                    (90, BASE_LR * 1e-3), (100, BASE_LR * 1e-4)]),\n        ]\n        if BASE_LR > START_LR:\n            callbacks.append(\n                ScheduledHyperParamSetter(\n                    \'learning_rate\', [(0, START_LR), (5, BASE_LR)], interp=\'linear\'))\n\n        infs = [ClassificationError(\'wrong-top1\', \'val-error-top1\'),\n                ClassificationError(\'wrong-top5\', \'val-error-top5\')]\n        dataset_val = get_imagenet_dataflow(args.data, \'val\', batch)\n        if nr_tower == 1:\n            # single-GPU inference with queue prefetch\n            callbacks.append(InferenceRunner(QueueInput(dataset_val), infs))\n        else:\n            # multi-GPU inference (with mandatory queue prefetch)\n            callbacks.append(DataParallelInferenceRunner(\n                dataset_val, infs, list(range(nr_tower))))\n\n    if get_num_gpu() > 0:\n        callbacks.append(GPUUtilizationTracker())\n\n    return TrainConfig(\n        model=model,\n        data=data,\n        callbacks=callbacks,\n        steps_per_epoch=100 if args.fake else 1281167 // args.batch,\n        max_epoch=105,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    # generic:\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use. Default to use all available ones\')\n    parser.add_argument(\'--eval\', action=\'store_true\', help=\'run offline evaluation instead of training\')\n    parser.add_argument(\'--load\', help=\'load a model for training or evaluation\')\n\n    # data:\n    parser.add_argument(\'--data\', help=\'ILSVRC dataset dir\')\n    parser.add_argument(\'--fake\', help=\'use FakeData to debug or benchmark this model\', action=\'store_true\')\n    parser.add_argument(\'--symbolic\', help=\'use symbolic data loader\', action=\'store_true\')\n\n    # model:\n    parser.add_argument(\'--data-format\', help=\'the image data layout used by the model\',\n                        default=\'NCHW\', choices=[\'NCHW\', \'NHWC\'])\n    parser.add_argument(\'-d\', \'--depth\', help=\'ResNet depth\',\n                        type=int, default=50, choices=[18, 34, 50, 101, 152])\n    parser.add_argument(\'--weight-decay-norm\', action=\'store_true\',\n                        help=""apply weight decay on normalization layers (gamma & beta).""\n                             ""This is used in torch/pytorch, and slightly ""\n                             ""improves validation accuracy of large models."")\n    parser.add_argument(\'--batch\', default=256, type=int,\n                        help=""total batch size. ""\n                        ""Note that it\'s best to keep per-GPU batch size in [32, 64] to obtain the best accuracy.""\n                        ""Pretrained models listed in README were trained with batch=32x8."")\n    parser.add_argument(\'--mode\', choices=[\'resnet\', \'preact\', \'se\', \'resnext32x4d\'],\n                        help=\'variants of resnet to use\', default=\'resnet\')\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    model = Model(args.depth, args.mode)\n    model.data_format = args.data_format\n    if args.weight_decay_norm:\n        model.weight_decay_pattern = "".*/W|.*/gamma|.*/beta""\n\n    if args.eval:\n        batch = 128    # something that can run on one gpu\n        ds = get_imagenet_dataflow(args.data, \'val\', batch)\n        eval_classification(model, SmartInit(args.load), ds)\n    else:\n        if args.fake:\n            logger.set_logger_dir(os.path.join(\'train_log\', \'tmp\'), \'d\')\n        else:\n            logger.set_logger_dir(\n                os.path.join(\'train_log\',\n                             \'imagenet-{}-d{}-batch{}\'.format(\n                                 args.mode, args.depth, args.batch)))\n\n        config = get_config(model)\n        config.session_init = SmartInit(args.load)\n        trainer = SyncMultiGPUTrainerReplicated(max(get_num_gpu(), 1))\n        launch_train_with_config(config, trainer)\n'"
examples/ResNet/imagenet_utils.py,66,"b'# -*- coding: utf-8 -*-\n# File: imagenet_utils.py\n\n\nimport multiprocessing\nimport numpy as np\nimport os\nfrom abc import abstractmethod\nimport cv2\nimport tensorflow as tf\nimport tqdm\n\nfrom tensorpack import ModelDesc\nfrom tensorpack.dataflow import (\n    AugmentImageComponent, BatchData, MultiThreadMapData,\n    MultiProcessRunnerZMQ, dataset, imgaug)\nfrom tensorpack.input_source import QueueInput, StagingInput\nfrom tensorpack.models import regularize_cost, l2_regularizer\nfrom tensorpack.predict import FeedfreePredictor, PredictConfig\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.tfutils.optimizer import AccumGradOptimizer\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.stats import RatioCounter\n\n""""""\n====== DataFlow =======\n""""""\n\n\ndef fbresnet_augmentor(isTrain):\n    """"""\n    Augmentor used in fb.resnet.torch, for BGR images in range [0,255].\n    """"""\n    interpolation = cv2.INTER_CUBIC\n    # linear seems to have more stable performance.\n    # but we keep cubic for compatibility with old models\n    if isTrain:\n        augmentors = [\n            imgaug.GoogleNetRandomCropAndResize(interp=interpolation),\n            imgaug.ToFloat32(),  # avoid frequent casting in each color augmentation\n            # It\'s OK to remove the following augs if your CPU is not fast enough.\n            # Removing brightness/contrast/saturation does not have a significant effect on accuracy.\n            # Removing lighting leads to a tiny drop in accuracy.\n            imgaug.RandomOrderAug(\n                [imgaug.BrightnessScale((0.6, 1.4)),\n                 imgaug.Contrast((0.6, 1.4), rgb=False),\n                 imgaug.Saturation(0.4, rgb=False),\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\n                 imgaug.Lighting(0.1,\n                                 eigval=np.asarray(\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                                 eigvec=np.array(\n                                     [[-0.5675, 0.7192, 0.4009],\n                                      [-0.5808, -0.0045, -0.8140],\n                                      [-0.5836, -0.6948, 0.4203]],\n                                     dtype=\'float32\')[::-1, ::-1]\n                                 )]),\n            imgaug.ToUint8(),\n            imgaug.Flip(horiz=True),\n        ]\n    else:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, interp=interpolation),\n            imgaug.CenterCrop((224, 224)),\n        ]\n    return augmentors\n\n\ndef get_imagenet_dataflow(\n        datadir, name, batch_size,\n        augmentors=None, parallel=None):\n    """"""\n    Args:\n        augmentors (list[imgaug.Augmentor]): Defaults to `fbresnet_augmentor(isTrain)`\n\n    Returns: A DataFlow which produces BGR images and labels.\n\n    See explanations in the tutorial:\n    http://tensorpack.readthedocs.io/tutorial/efficient-dataflow.html\n    """"""\n    assert name in [\'train\', \'val\', \'test\']\n    isTrain = name == \'train\'\n    assert datadir is not None\n    if augmentors is None:\n        augmentors = fbresnet_augmentor(isTrain)\n    assert isinstance(augmentors, list)\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n\n    if isTrain:\n        ds = dataset.ILSVRC12(datadir, name, shuffle=True)\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        if parallel < 16:\n            logger.warn(""DataFlow may become the bottleneck when too few processes are used."")\n        ds = MultiProcessRunnerZMQ(ds, parallel)\n        ds = BatchData(ds, batch_size, remainder=False)\n    else:\n        ds = dataset.ILSVRC12Files(datadir, name, shuffle=False)\n        aug = imgaug.AugmentorList(augmentors)\n\n        def mapf(dp):\n            fname, cls = dp\n            im = cv2.imread(fname, cv2.IMREAD_COLOR)\n            im = aug.augment(im)\n            return im, cls\n        ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000, strict=True)\n        ds = BatchData(ds, batch_size, remainder=True)\n        ds = MultiProcessRunnerZMQ(ds, 1)\n    return ds\n\n\n""""""\n====== tf.data =======\n""""""\n\n\ndef get_imagenet_tfdata(datadir, name, batch_size, mapper=None, parallel=None):\n    """"""\n    Args:\n        mapper: a symbolic function that takes a tf.string (the raw bytes read from file) and produces a BGR image.\n            Defaults to `fbresnet_mapper(isTrain)`.\n\n    Returns:\n        A `tf.data.Dataset`. If training, the dataset is infinite.\n        The dataset contains BGR images and labels.\n    """"""\n\n    def get_imglist(dir, name):\n        """"""\n        Returns:\n            [(full filename, label)]\n        """"""\n        dir = os.path.join(dir, name)\n        meta = dataset.ILSVRCMeta()\n        imglist = meta.get_image_list(\n            name,\n            dataset.ILSVRCMeta.guess_dir_structure(dir))\n\n        def _filter(fname):\n            # png\n            return \'n02105855_2933.JPEG\' in fname\n\n        ret = []\n        for fname, label in imglist:\n            if _filter(fname):\n                logger.info(""Image {} was filtered out."".format(fname))\n                continue\n            fname = os.path.join(dir, fname)\n            ret.append((fname, label))\n        return ret\n\n    assert name in [\'train\', \'val\', \'test\']\n    assert datadir is not None\n    isTrain = name == \'train\'\n    if mapper is None:\n        mapper = fbresnet_mapper(isTrain)\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n    imglist = get_imglist(datadir, name)\n\n    N = len(imglist)\n    filenames = tf.constant([k[0] for k in imglist], name=\'filenames\')\n    labels = tf.constant([k[1] for k in imglist], dtype=tf.int32, name=\'labels\')\n\n    ds = tf.data.Dataset.from_tensor_slices((filenames, labels))\n\n    if isTrain:\n        ds = ds.shuffle(N, reshuffle_each_iteration=True).repeat()\n\n    ds = ds.apply(\n        tf.data.experimental.map_and_batch(\n            lambda fname, label: (mapper(tf.read_file(fname)), label),\n            batch_size=batch_size,\n            num_parallel_batches=parallel))\n    ds = ds.prefetch(100)\n    return ds\n\n\ndef fbresnet_mapper(isTrain):\n    """"""\n    Note: compared to fbresnet_augmentor, it\n    lacks some photometric augmentation that may have a small effect (0.1~0.2%) on accuracy.\n    """"""\n    JPEG_OPT = {\'fancy_upscaling\': True, \'dct_method\': \'INTEGER_ACCURATE\'}\n\n    def uint8_resize_bicubic(image, shape):\n        ret = tf.image.resize_bicubic([image], shape)\n        return tf.cast(tf.clip_by_value(ret, 0, 255), tf.uint8)[0]\n\n    def resize_shortest_edge(image, image_shape, size):\n        shape = tf.cast(image_shape, tf.float32)\n        w_greater = tf.greater(image_shape[0], image_shape[1])\n        shape = tf.cond(w_greater,\n                        lambda: tf.cast([shape[0] / shape[1] * size, size], tf.int32),\n                        lambda: tf.cast([size, shape[1] / shape[0] * size], tf.int32))\n\n        return uint8_resize_bicubic(image, shape)\n\n    def center_crop(image, size):\n        image_height = tf.shape(image)[0]\n        image_width = tf.shape(image)[1]\n\n        offset_height = (image_height - size) // 2\n        offset_width = (image_width - size) // 2\n        image = tf.slice(image, [offset_height, offset_width, 0], [size, size, -1])\n        return image\n\n    def lighting(image, std, eigval, eigvec):\n        v = tf.random_normal(shape=[3], stddev=std) * eigval\n        inc = tf.matmul(eigvec, tf.reshape(v, [3, 1]))\n        image = tf.cast(tf.cast(image, tf.float32) + tf.reshape(inc, [3]), image.dtype)\n        return image\n\n    def validation_mapper(byte):\n        image = tf.image.decode_jpeg(\n            tf.reshape(byte, shape=[]), 3, **JPEG_OPT)\n        image = resize_shortest_edge(image, tf.shape(image), 256)\n        image = center_crop(image, 224)\n        image = tf.reverse(image, axis=[2])  # to BGR\n        return image\n\n    def training_mapper(byte):\n        jpeg_shape = tf.image.extract_jpeg_shape(byte)  # hwc\n        bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n            jpeg_shape,\n            bounding_boxes=tf.zeros(shape=[0, 0, 4]),\n            min_object_covered=0,\n            aspect_ratio_range=[0.75, 1.33],\n            area_range=[0.08, 1.0],\n            max_attempts=10,\n            use_image_if_no_bounding_boxes=True)\n\n        is_bad = tf.reduce_sum(tf.cast(tf.equal(bbox_size, jpeg_shape), tf.int32)) >= 2\n\n        def good():\n            offset_y, offset_x, _ = tf.unstack(bbox_begin)\n            target_height, target_width, _ = tf.unstack(bbox_size)\n            crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n            image = tf.image.decode_and_crop_jpeg(\n                byte, crop_window, channels=3, **JPEG_OPT)\n            image = uint8_resize_bicubic(image, [224, 224])\n            return image\n\n        def bad():\n            image = tf.image.decode_jpeg(\n                tf.reshape(byte, shape=[]), 3, **JPEG_OPT)\n            image = resize_shortest_edge(image, jpeg_shape, 224)\n            image = center_crop(image, 224)\n            return image\n\n        image = tf.cond(is_bad, bad, good)\n        # TODO other imgproc\n        image = lighting(image, 0.1,\n                         eigval=np.array([0.2175, 0.0188, 0.0045], dtype=\'float32\') * 255.0,\n                         eigvec=np.array([[-0.5675, 0.7192, 0.4009],\n                                          [-0.5808, -0.0045, -0.8140],\n                                          [-0.5836, -0.6948, 0.4203]], dtype=\'float32\'))\n        image = tf.image.random_flip_left_right(image)\n        image = tf.reverse(image, axis=[2])  # to BGR\n        return image\n\n    return training_mapper if isTrain else validation_mapper\n\n\n""""""\n====== Model & Evaluation =======\n""""""\n\n\ndef eval_classification(model, sessinit, dataflow):\n    """"""\n    Eval a classification model on the dataset. It assumes the model inputs are\n    named ""input"" and ""label"", and contains ""wrong-top1"" and ""wrong-top5"" in the graph.\n    """"""\n    pred_config = PredictConfig(\n        model=model,\n        session_init=sessinit,\n        input_names=[\'input\', \'label\'],\n        output_names=[\'wrong-top1\', \'wrong-top5\']\n    )\n    acc1, acc5 = RatioCounter(), RatioCounter()\n\n    # This does not have a visible improvement over naive predictor,\n    # but will have an improvement if image_dtype is set to float32.\n    pred = FeedfreePredictor(pred_config, StagingInput(QueueInput(dataflow), device=\'/gpu:0\'))\n    for _ in tqdm.trange(dataflow.size()):\n        top1, top5 = pred()\n        batch_size = top1.shape[0]\n        acc1.feed(top1.sum(), batch_size)\n        acc5.feed(top5.sum(), batch_size)\n\n    print(""Top1 Error: {}"".format(acc1.ratio))\n    print(""Top5 Error: {}"".format(acc5.ratio))\n\n\nclass ImageNetModel(ModelDesc):\n    image_shape = 224\n\n    """"""\n    uint8 instead of float32 is used as input type to reduce copy overhead.\n    It might hurt the performance a liiiitle bit.\n    The pretrained models were trained with float32.\n    """"""\n    image_dtype = tf.uint8\n\n    """"""\n    Either \'NCHW\' or \'NHWC\'\n    """"""\n    data_format = \'NCHW\'\n\n    """"""\n    Whether the image is BGR or RGB. If using DataFlow, then it should be BGR.\n    """"""\n    image_bgr = True\n\n    weight_decay = 1e-4\n\n    """"""\n    To apply on normalization parameters, use \'.*/W|.*/gamma|.*/beta\'\n    """"""\n    weight_decay_pattern = \'.*/W\'\n\n    """"""\n    Scale the loss, for whatever reasons (e.g., gradient averaging, fp16 training, etc)\n    """"""\n    loss_scale = 1.\n\n    """"""\n    Label smoothing (See tf.losses.softmax_cross_entropy)\n    """"""\n    label_smoothing = 0.\n\n    """"""\n    Accumulate gradients across several steps (by default 1, which means no accumulation across steps).\n    """"""\n    accum_grad = 1\n\n    def inputs(self):\n        return [tf.TensorSpec([None, self.image_shape, self.image_shape, 3], self.image_dtype, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = self.image_preprocess(image)\n        assert self.data_format in [\'NCHW\', \'NHWC\']\n        if self.data_format == \'NCHW\':\n            image = tf.transpose(image, [0, 3, 1, 2])\n\n        logits = self.get_logits(image)\n        tf.nn.softmax(logits, name=\'prob\')\n        loss = ImageNetModel.compute_loss_and_error(\n            logits, label, label_smoothing=self.label_smoothing)\n\n        if self.weight_decay > 0:\n            wd_loss = regularize_cost(self.weight_decay_pattern,\n                                      l2_regularizer(self.weight_decay),\n                                      name=\'l2_regularize_loss\')\n            add_moving_summary(loss, wd_loss)\n            total_cost = tf.add_n([loss, wd_loss], name=\'cost\')\n        else:\n            total_cost = tf.identity(loss, name=\'cost\')\n            add_moving_summary(total_cost)\n\n        if self.loss_scale != 1.:\n            logger.info(""Scaling the total loss by {} ..."".format(self.loss_scale))\n            return total_cost * self.loss_scale\n        else:\n            return total_cost\n\n    @abstractmethod\n    def get_logits(self, image):\n        """"""\n        Args:\n            image: 4D tensor of ``self.input_shape`` in ``self.data_format``\n\n        Returns:\n            Nx#class logits\n        """"""\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.1, trainable=False)\n        tf.summary.scalar(\'learning_rate-summary\', lr)\n        opt = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n        if self.accum_grad != 1:\n            opt = AccumGradOptimizer(opt, self.accum_grad)\n        return opt\n\n    def image_preprocess(self, image):\n        with tf.name_scope(\'image_preprocess\'):\n            if image.dtype.base_dtype != tf.float32:\n                image = tf.cast(image, tf.float32)\n            mean = [0.485, 0.456, 0.406]    # rgb\n            std = [0.229, 0.224, 0.225]\n            if self.image_bgr:\n                mean = mean[::-1]\n                std = std[::-1]\n            image_mean = tf.constant(mean, dtype=tf.float32) * 255.\n            image_std = tf.constant(std, dtype=tf.float32) * 255.\n            image = (image - image_mean) / image_std\n            return image\n\n    @staticmethod\n    def compute_loss_and_error(logits, label, label_smoothing=0.):\n        if label_smoothing != 0.:\n            nclass = logits.shape[-1]\n            label = tf.one_hot(label, nclass) if label.shape.ndims == 1 else label\n\n        if label.shape.ndims == 1:\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        else:\n            loss = tf.losses.softmax_cross_entropy(\n                label, logits, label_smoothing=label_smoothing,\n                reduction=tf.losses.Reduction.NONE)\n        loss = tf.reduce_mean(loss, name=\'xentropy-loss\')\n\n        def prediction_incorrect(logits, label, topk=1, name=\'incorrect_vector\'):\n            with tf.name_scope(\'prediction_incorrect\'):\n                x = tf.logical_not(tf.nn.in_top_k(logits, label, topk))\n            return tf.cast(x, tf.float32, name=name)\n\n        wrong = prediction_incorrect(logits, label, 1, name=\'wrong-top1\')\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top1\'))\n\n        wrong = prediction_incorrect(logits, label, 5, name=\'wrong-top5\')\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top5\'))\n        return loss\n\n    def create_predict_config(self, session_init):\n        """"""\n        Returns:\n            a :class:`PredictConfig` to be used for inference.\n            The predictor will take inputs and return probabilities.\n\n        Examples:\n\n            pred = OfflinePredictor(model.create_predict_config(SmartInit(args.load)))\n            prob = pred(NCHW_image)[0]  # Nx1000 probabilities\n        """"""\n        return PredictConfig(model=self, input_names=[\'input\'], output_names=[\'prob\'], session_init=session_init)\n\n\nif __name__ == \'__main__\':\n    import argparse\n    from tensorpack.dataflow import TestDataSpeed\n    from tensorpack.tfutils import get_default_sess_config\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data\', required=True)\n    parser.add_argument(\'--batch\', type=int, default=32)\n    parser.add_argument(\'--aug\', choices=[\'train\', \'val\'], default=\'val\')\n    parser.add_argument(\'--symbolic\', action=\'store_true\')\n    args = parser.parse_args()\n\n    if not args.symbolic:\n        augs = fbresnet_augmentor(args.aug == \'train\')\n        df = get_imagenet_dataflow(\n            args.data, \'train\', args.batch, augs)\n        # For val augmentor, Should get >100 it/s (i.e. 3k im/s) here on a decent E5 server.\n        TestDataSpeed(df).start()\n    else:\n        assert args.aug == \'train\'\n        data = get_imagenet_tfdata(args.data, \'train\', args.batch)\n\n        itr = data.make_initializable_iterator()\n        dp = itr.get_next()\n        dpop = tf.group(*dp)\n        with tf.Session(config=get_default_sess_config()) as sess:\n            sess.run(itr.initializer)\n            for _ in tqdm.trange(200):\n                sess.run(dpop)\n            for _ in tqdm.trange(5000, smoothing=0.1):\n                sess.run(dpop)\n'"
examples/ResNet/load-resnet.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: load-resnet.py\n# Author: Eric Yujia Huang <yujiah1@andrew.cmu.edu>\n#         Yuxin Wu\n\nimport argparse\nimport functools\nimport numpy as np\nimport re\nimport cv2\nimport six\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow.dataset import ILSVRCMeta\nfrom tensorpack.utils import logger\n\nfrom imagenet_utils import ImageNetModel, eval_classification, get_imagenet_dataflow\nfrom resnet_model import resnet_bottleneck, resnet_group\n\nDEPTH = None\nCFG = {\n    50: ([3, 4, 6, 3]),\n    101: ([3, 4, 23, 3]),\n    152: ([3, 8, 36, 3])\n}\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec([None, 224, 224, 3], tf.float32, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        blocks = CFG[DEPTH]\n\n        bottleneck = functools.partial(resnet_bottleneck, stride_first=True)\n\n        # tensorflow with padding=SAME will by default pad [2,3] here.\n        # but caffe conv with stride will pad [3,2]\n        image = tf.pad(image, [[0, 0], [3, 2], [3, 2], [0, 0]])\n        image = tf.transpose(image, [0, 3, 1, 2])\n        with argscope([Conv2D, MaxPooling, GlobalAvgPooling, BatchNorm],\n                      data_format=\'channels_first\'), \\\n                argscope(Conv2D, use_bias=False):\n            logits = (LinearWrap(image)\n                      .Conv2D(\'conv0\', 64, 7, strides=2, activation=BNReLU, padding=\'VALID\')\n                      .MaxPooling(\'pool0\', 3, strides=2, padding=\'SAME\')\n                      .apply2(resnet_group, \'group0\', bottleneck, 64, blocks[0], 1)\n                      .apply2(resnet_group, \'group1\', bottleneck, 128, blocks[1], 2)\n                      .apply2(resnet_group, \'group2\', bottleneck, 256, blocks[2], 2)\n                      .apply2(resnet_group, \'group3\', bottleneck, 512, blocks[3], 2)\n                      .GlobalAvgPooling(\'gap\')\n                      .FullyConnected(\'linear\', 1000)())\n        tf.nn.softmax(logits, name=\'prob\')\n        ImageNetModel.compute_loss_and_error(logits, label)\n\n\ndef get_inference_augmentor():\n    # load ResNet mean from Kaiming:\n    # from tensorpack.utils.loadcaffe import get_caffe_pb\n    # obj = get_caffe_pb().BlobProto()\n    # obj.ParseFromString(open(\'ResNet_mean.binaryproto\').read())\n    # pp_mean_224 = np.array(obj.data).reshape(3, 224, 224).transpose(1,2,0)\n\n    meta = ILSVRCMeta()\n    pp_mean = meta.get_per_pixel_mean()\n    pp_mean_224 = pp_mean[16:-16, 16:-16, :]\n\n    transformers = [\n        imgaug.ResizeShortestEdge(256),\n        imgaug.CenterCrop((224, 224)),\n        imgaug.MapImage(lambda x: x - pp_mean_224),\n    ]\n    return transformers\n\n\ndef run_test(params, input):\n    pred_config = PredictConfig(\n        model=Model(),\n        session_init=SmartInit(params),\n        input_names=[\'input\'],\n        output_names=[\'prob\']\n    )\n    predict_func = OfflinePredictor(pred_config)\n\n    prepro = imgaug.AugmentorList(get_inference_augmentor())\n    im = cv2.imread(input).astype(\'float32\')\n    im = prepro.augment(im)\n    im = np.reshape(im, (1, 224, 224, 3))\n    outputs = predict_func(im)\n    prob = outputs[0]\n\n    ret = prob[0].argsort()[-10:][::-1]\n    print(ret)\n    meta = ILSVRCMeta().get_synset_words_1000()\n    print([meta[k] for k in ret])\n\n\ndef name_conversion(caffe_layer_name):\n    """""" Convert a caffe parameter name to a tensorflow parameter name as\n        defined in the above model """"""\n    # beginning & end mapping\n    NAME_MAP = {\'bn_conv1/beta\': \'conv0/bn/beta\',\n                \'bn_conv1/gamma\': \'conv0/bn/gamma\',\n                \'bn_conv1/mean/EMA\': \'conv0/bn/mean/EMA\',\n                \'bn_conv1/variance/EMA\': \'conv0/bn/variance/EMA\',\n                \'conv1/W\': \'conv0/W\', \'conv1/b\': \'conv0/b\',\n                \'fc1000/W\': \'linear/W\', \'fc1000/b\': \'linear/b\'}\n    if caffe_layer_name in NAME_MAP:\n        return NAME_MAP[caffe_layer_name]\n\n    s = re.search(\'([a-z]+)([0-9]+)([a-z]+)_\', caffe_layer_name)\n    if s is None:\n        s = re.search(\'([a-z]+)([0-9]+)([a-z]+)([0-9]+)_\', caffe_layer_name)\n        layer_block_part1 = s.group(3)\n        layer_block_part2 = s.group(4)\n        assert layer_block_part1 in [\'a\', \'b\']\n        layer_block = 0 if layer_block_part1 == \'a\' else int(layer_block_part2)\n    else:\n        layer_block = ord(s.group(3)) - ord(\'a\')\n    layer_type = s.group(1)\n    layer_group = s.group(2)\n\n    layer_branch = int(re.search(\'_branch([0-9])\', caffe_layer_name).group(1))\n    assert layer_branch in [1, 2]\n    if layer_branch == 2:\n        layer_id = re.search(\'_branch[0-9]([a-z])/\', caffe_layer_name).group(1)\n        layer_id = ord(layer_id) - ord(\'a\') + 1\n\n    TYPE_DICT = {\'res\': \'conv{}\', \'bn\': \'conv{}/bn\'}\n    layer_type = TYPE_DICT[layer_type].format(layer_id if layer_branch == 2 else \'shortcut\')\n\n    tf_name = caffe_layer_name[caffe_layer_name.index(\'/\'):]\n    tf_name = \'group{}/block{}/{}\'.format(\n        int(layer_group) - 2, layer_block, layer_type) + tf_name\n    return tf_name\n\n\ndef convert_param_name(param):\n    resnet_param = {}\n    for k, v in six.iteritems(param):\n        try:\n            newname = name_conversion(k)\n        except Exception:\n            logger.error(""Exception when processing caffe layer {}"".format(k))\n            raise\n        logger.info(""Name Transform: "" + k + \' --> \' + newname)\n        resnet_param[newname] = v\n    return resnet_param\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--load\', required=True,\n                        help=\'.npz model file generated by tensorpack.utils.loadcaffe\')\n    parser.add_argument(\'-d\', \'--depth\', help=\'resnet depth\', required=True, type=int, choices=[50, 101, 152])\n    parser.add_argument(\'--input\', help=\'an input image\')\n    parser.add_argument(\'--convert\', help=\'npz output file to save the converted model\')\n    parser.add_argument(\'--eval\', help=\'ILSVRC dir to run validation on\')\n\n    args = parser.parse_args()\n    DEPTH = args.depth\n\n    param = dict(np.load(args.load))\n    param = convert_param_name(param)\n\n    if args.convert:\n        assert args.convert.endswith(\'.npz\')\n        np.savez_compressed(args.convert, **param)\n\n    if args.eval:\n        ds = get_imagenet_dataflow(args.eval, \'val\', 128, get_inference_augmentor())\n        eval_classification(Model(), SmartRestore(param), ds)\n    elif args.input:\n        run_test(param, args.input)\n'"
examples/ResNet/resnet_model.py,15,"b'# -*- coding: utf-8 -*-\n# File: resnet_model.py\n\nimport tensorflow as tf\n\nfrom tensorpack.models import BatchNorm, BNReLU, Conv2D, FullyConnected, GlobalAvgPooling, MaxPooling\nfrom tensorpack.tfutils.argscope import argscope, get_arg_scope\n\n\ndef resnet_shortcut(l, n_out, stride, activation=tf.identity):\n    data_format = get_arg_scope()[\'Conv2D\'][\'data_format\']\n    n_in = l.get_shape().as_list()[1 if data_format in [\'NCHW\', \'channels_first\'] else 3]\n    if n_in != n_out:   # change dimension when channel is not the same\n        return Conv2D(\'convshortcut\', l, n_out, 1, strides=stride, activation=activation)\n    else:\n        return l\n\n\ndef get_bn(zero_init=False):\n    """"""\n    Zero init gamma is good for resnet. See https://arxiv.org/abs/1706.02677.\n    """"""\n    if zero_init:\n        return lambda x, name=None: BatchNorm(\'bn\', x, gamma_initializer=tf.zeros_initializer())\n    else:\n        return lambda x, name=None: BatchNorm(\'bn\', x)\n\n\n# ----------------- pre-activation resnet ----------------------\ndef apply_preactivation(l, preact):\n    if preact == \'bnrelu\':\n        shortcut = l    # preserve identity mapping\n        l = BNReLU(\'preact\', l)\n    else:\n        shortcut = l\n    return l, shortcut\n\n\ndef preact_basicblock(l, ch_out, stride, preact):\n    l, shortcut = apply_preactivation(l, preact)\n    l = Conv2D(\'conv1\', l, ch_out, 3, strides=stride, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3)\n    return l + resnet_shortcut(shortcut, ch_out, stride)\n\n\ndef preact_bottleneck(l, ch_out, stride, preact):\n    # stride is applied on the second conv, following fb.resnet.torch\n    l, shortcut = apply_preactivation(l, preact)\n    l = Conv2D(\'conv1\', l, ch_out, 1, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3, strides=stride, activation=BNReLU)\n    l = Conv2D(\'conv3\', l, ch_out * 4, 1)\n    return l + resnet_shortcut(shortcut, ch_out * 4, stride)\n\n\ndef preact_group(name, l, block_func, features, count, stride):\n    with tf.variable_scope(name):\n        for i in range(0, count):\n            with tf.variable_scope(\'block{}\'.format(i)):\n                # first block doesn\'t need activation\n                l = block_func(l, features,\n                               stride if i == 0 else 1,\n                               \'no_preact\' if i == 0 else \'bnrelu\')\n        # end of each group need an extra activation\n        l = BNReLU(\'bnlast\', l)\n    return l\n# ----------------- pre-activation resnet ----------------------\n\n\ndef resnet_basicblock(l, ch_out, stride):\n    shortcut = l\n    l = Conv2D(\'conv1\', l, ch_out, 3, strides=stride, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3, activation=get_bn(zero_init=True))\n    out = l + resnet_shortcut(shortcut, ch_out, stride, activation=get_bn(zero_init=False))\n    return tf.nn.relu(out)\n\n\ndef resnet_bottleneck(l, ch_out, stride, stride_first=False):\n    """"""\n    stride_first: original resnet put stride on first conv. fb.resnet.torch put stride on second conv.\n    """"""\n    shortcut = l\n    l = Conv2D(\'conv1\', l, ch_out, 1, strides=stride if stride_first else 1, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3, strides=1 if stride_first else stride, activation=BNReLU)\n    l = Conv2D(\'conv3\', l, ch_out * 4, 1, activation=get_bn(zero_init=True))\n    out = l + resnet_shortcut(shortcut, ch_out * 4, stride, activation=get_bn(zero_init=False))\n    return tf.nn.relu(out)\n\n\ndef se_bottleneck(l, ch_out, stride):\n    shortcut = l\n    l = Conv2D(\'conv1\', l, ch_out, 1, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3, strides=stride, activation=BNReLU)\n    l = Conv2D(\'conv3\', l, ch_out * 4, 1, activation=get_bn(zero_init=True))\n\n    squeeze = GlobalAvgPooling(\'gap\', l)\n    squeeze = FullyConnected(\'fc1\', squeeze, ch_out // 4, activation=tf.nn.relu)\n    squeeze = FullyConnected(\'fc2\', squeeze, ch_out * 4, activation=tf.nn.sigmoid)\n    data_format = get_arg_scope()[\'Conv2D\'][\'data_format\']\n    ch_ax = 1 if data_format in [\'NCHW\', \'channels_first\'] else 3\n    shape = [-1, 1, 1, 1]\n    shape[ch_ax] = ch_out * 4\n    l = l * tf.reshape(squeeze, shape)\n    out = l + resnet_shortcut(shortcut, ch_out * 4, stride, activation=get_bn(zero_init=False))\n    return tf.nn.relu(out)\n\n\ndef resnext32x4d_bottleneck(l, ch_out, stride):\n    shortcut = l\n    l = Conv2D(\'conv1\', l, ch_out * 2, 1, strides=1, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out * 2, 3, strides=stride, activation=BNReLU, split=32)\n    l = Conv2D(\'conv3\', l, ch_out * 4, 1, activation=get_bn(zero_init=True))\n    out = l + resnet_shortcut(shortcut, ch_out * 4, stride, activation=get_bn(zero_init=False))\n    return tf.nn.relu(out)\n\n\ndef resnet_group(name, l, block_func, features, count, stride):\n    with tf.variable_scope(name):\n        for i in range(0, count):\n            with tf.variable_scope(\'block{}\'.format(i)):\n                l = block_func(l, features, stride if i == 0 else 1)\n    return l\n\n\ndef resnet_backbone(image, num_blocks, group_func, block_func):\n    with argscope(Conv2D, use_bias=False,\n                  kernel_initializer=tf.variance_scaling_initializer(scale=2.0, mode=\'fan_out\')):\n        # Note that TF pads the image by [2, 3] instead of [3, 2].\n        # Similar things happen in later stride=2 layers as well.\n        l = Conv2D(\'conv0\', image, 64, 7, strides=2, activation=BNReLU)\n        l = MaxPooling(\'pool0\', l, pool_size=3, strides=2, padding=\'SAME\')\n        l = group_func(\'group0\', l, block_func, 64, num_blocks[0], 1)\n        l = group_func(\'group1\', l, block_func, 128, num_blocks[1], 2)\n        l = group_func(\'group2\', l, block_func, 256, num_blocks[2], 2)\n        l = group_func(\'group3\', l, block_func, 512, num_blocks[3], 2)\n        l = GlobalAvgPooling(\'gap\', l)\n        logits = FullyConnected(\'linear\', l, 1000,\n                                kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n    return logits\n'"
examples/Saliency/CAM-resnet.py,3,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: CAM-resnet.py\n\nimport argparse\nimport multiprocessing\nimport numpy as np\nimport os\nimport sys\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils import gradproc, optimizer\nfrom tensorpack.tfutils.summary import *\nfrom tensorpack.utils import viz\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom imagenet_utils import ImageNetModel, fbresnet_augmentor\nfrom resnet_model import preresnet_basicblock, preresnet_group\n\nTOTAL_BATCH_SIZE = 256\nDEPTH = None\n\n\nclass Model(ImageNetModel):\n\n    def get_logits(self, image):\n        cfg = {\n            18: ([2, 2, 2, 2], preresnet_basicblock),\n            34: ([3, 4, 6, 3], preresnet_basicblock),\n        }\n        defs, block_func = cfg[DEPTH]\n\n        with argscope(Conv2D, use_bias=False,\n                      kernel_initializer=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')), \\\n                argscope([Conv2D, MaxPooling, GlobalAvgPooling, BatchNorm], data_format='channels_first'):\n            convmaps = (LinearWrap(image)\n                        .Conv2D('conv0', 64, 7, strides=2, activation=BNReLU)\n                        .MaxPooling('pool0', 3, strides=2, padding='SAME')\n                        .apply2(preresnet_group, 'group0', block_func, 64, defs[0], 1)\n                        .apply2(preresnet_group, 'group1', block_func, 128, defs[1], 2)\n                        .apply2(preresnet_group, 'group2', block_func, 256, defs[2], 2)\n                        .apply2(preresnet_group, 'group3new', block_func, 512, defs[3], 1)())\n            print(convmaps)\n            convmaps = GlobalAvgPooling('gap', convmaps)\n            logits = FullyConnected('linearnew', convmaps, 1000)\n        return logits\n\n    def optimizer(self):\n        lr = tf.get_variable('learning_rate', initializer=0.1, trainable=False)\n        opt = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n        gradprocs = [gradproc.ScaleGradient(\n            [('conv0.*', 0.1), ('group[0-2].*', 0.1)])]\n        return optimizer.apply_grad_processors(opt, gradprocs)\n\n\ndef get_data(train_or_test):\n    # completely copied from imagenet-resnet.py example\n    isTrain = train_or_test == 'train'\n\n    datadir = args.data\n    ds = dataset.ILSVRC12(datadir, train_or_test, shuffle=isTrain)\n    augmentors = fbresnet_augmentor(isTrain)\n    augmentors.append(imgaug.ToUint8())\n\n    ds = AugmentImageComponent(ds, augmentors, copy=False)\n    if isTrain:\n        ds = MultiProcessRunnerZMQ(ds, min(25, multiprocessing.cpu_count()))\n    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n    return ds\n\n\ndef get_config():\n    dataset_train = get_data('train')\n    dataset_val = get_data('val')\n\n    return TrainConfig(\n        model=Model(),\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(),\n            PeriodicTrigger(InferenceRunner(dataset_val, [\n                ClassificationError('wrong-top1', 'val-error-top1'),\n                ClassificationError('wrong-top5', 'val-error-top5')]),\n                every_k_epochs=2),\n            ScheduledHyperParamSetter('learning_rate',\n                                      [(30, 1e-2), (55, 1e-3), (75, 1e-4), (95, 1e-5)]),\n        ],\n        steps_per_epoch=5000,\n        max_epoch=105,\n    )\n\n\ndef viz_cam(model_file, data_dir):\n    ds = get_data('val')\n    pred_config = PredictConfig(\n        model=Model(),\n        session_init=SmartInit(model_file),\n        input_names=['input', 'label'],\n        output_names=['wrong-top1', 'group3new/bnlast/Relu', 'linearnew/W'],\n        return_input=True\n    )\n    meta = dataset.ILSVRCMeta().get_synset_words_1000()\n\n    pred = SimpleDatasetPredictor(pred_config, ds)\n    cnt = 0\n    for inp, outp in pred.get_result():\n        images, labels = inp\n        wrongs, convmaps, W = outp\n        batch = wrongs.shape[0]\n        for i in range(batch):\n            if wrongs[i]:\n                continue\n            weight = W[:, [labels[i]]].T    # 512x1\n            convmap = convmaps[i, :, :, :]  # 512xhxw\n            mergedmap = np.matmul(weight, convmap.reshape((512, -1))).reshape(14, 14)\n            mergedmap = cv2.resize(mergedmap, (224, 224))\n            heatmap = viz.intensity_to_rgb(mergedmap, normalize=True)\n            blend = images[i] * 0.5 + heatmap * 0.5\n            concat = np.concatenate((images[i], heatmap, blend), axis=1)\n\n            classname = meta[labels[i]].split(',')[0]\n            cv2.imwrite('cam{}-{}.jpg'.format(cnt, classname), concat)\n            cnt += 1\n            if cnt == 500:\n                return\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')\n    parser.add_argument('--data', help='ILSVRC dataset dir')\n    parser.add_argument('--depth', type=int, default=18)\n    parser.add_argument('--load', help='load model')\n    parser.add_argument('--cam', action='store_true', help='run visualization')\n    args = parser.parse_args()\n\n    DEPTH = args.depth\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\n    num_gpu = get_num_gpu()\n    BATCH_SIZE = TOTAL_BATCH_SIZE // num_gpu\n\n    if args.cam:\n        BATCH_SIZE = 128    # something that can run on one gpu\n        viz_cam(args.load, args.data)\n        sys.exit()\n\n    logger.auto_set_dir()\n    config = get_config()\n    config.session_init = SmartInit(args.load)\n    launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(num_gpu))\n"""
examples/Saliency/imagenet_utils.py,66,"b'# -*- coding: utf-8 -*-\n# File: imagenet_utils.py\n\n\nimport multiprocessing\nimport numpy as np\nimport os\nfrom abc import abstractmethod\nimport cv2\nimport tensorflow as tf\nimport tqdm\n\nfrom tensorpack import ModelDesc\nfrom tensorpack.dataflow import (\n    AugmentImageComponent, BatchData, MultiThreadMapData,\n    MultiProcessRunnerZMQ, dataset, imgaug)\nfrom tensorpack.input_source import QueueInput, StagingInput\nfrom tensorpack.models import regularize_cost, l2_regularizer\nfrom tensorpack.predict import FeedfreePredictor, PredictConfig\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.tfutils.optimizer import AccumGradOptimizer\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.stats import RatioCounter\n\n""""""\n====== DataFlow =======\n""""""\n\n\ndef fbresnet_augmentor(isTrain):\n    """"""\n    Augmentor used in fb.resnet.torch, for BGR images in range [0,255].\n    """"""\n    interpolation = cv2.INTER_CUBIC\n    # linear seems to have more stable performance.\n    # but we keep cubic for compatibility with old models\n    if isTrain:\n        augmentors = [\n            imgaug.GoogleNetRandomCropAndResize(interp=interpolation),\n            imgaug.ToFloat32(),  # avoid frequent casting in each color augmentation\n            # It\'s OK to remove the following augs if your CPU is not fast enough.\n            # Removing brightness/contrast/saturation does not have a significant effect on accuracy.\n            # Removing lighting leads to a tiny drop in accuracy.\n            imgaug.RandomOrderAug(\n                [imgaug.BrightnessScale((0.6, 1.4)),\n                 imgaug.Contrast((0.6, 1.4), rgb=False),\n                 imgaug.Saturation(0.4, rgb=False),\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\n                 imgaug.Lighting(0.1,\n                                 eigval=np.asarray(\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                                 eigvec=np.array(\n                                     [[-0.5675, 0.7192, 0.4009],\n                                      [-0.5808, -0.0045, -0.8140],\n                                      [-0.5836, -0.6948, 0.4203]],\n                                     dtype=\'float32\')[::-1, ::-1]\n                                 )]),\n            imgaug.ToUint8(),\n            imgaug.Flip(horiz=True),\n        ]\n    else:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, interp=interpolation),\n            imgaug.CenterCrop((224, 224)),\n        ]\n    return augmentors\n\n\ndef get_imagenet_dataflow(\n        datadir, name, batch_size,\n        augmentors=None, parallel=None):\n    """"""\n    Args:\n        augmentors (list[imgaug.Augmentor]): Defaults to `fbresnet_augmentor(isTrain)`\n\n    Returns: A DataFlow which produces BGR images and labels.\n\n    See explanations in the tutorial:\n    http://tensorpack.readthedocs.io/tutorial/efficient-dataflow.html\n    """"""\n    assert name in [\'train\', \'val\', \'test\']\n    isTrain = name == \'train\'\n    assert datadir is not None\n    if augmentors is None:\n        augmentors = fbresnet_augmentor(isTrain)\n    assert isinstance(augmentors, list)\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n\n    if isTrain:\n        ds = dataset.ILSVRC12(datadir, name, shuffle=True)\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        if parallel < 16:\n            logger.warn(""DataFlow may become the bottleneck when too few processes are used."")\n        ds = MultiProcessRunnerZMQ(ds, parallel)\n        ds = BatchData(ds, batch_size, remainder=False)\n    else:\n        ds = dataset.ILSVRC12Files(datadir, name, shuffle=False)\n        aug = imgaug.AugmentorList(augmentors)\n\n        def mapf(dp):\n            fname, cls = dp\n            im = cv2.imread(fname, cv2.IMREAD_COLOR)\n            im = aug.augment(im)\n            return im, cls\n        ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000, strict=True)\n        ds = BatchData(ds, batch_size, remainder=True)\n        ds = MultiProcessRunnerZMQ(ds, 1)\n    return ds\n\n\n""""""\n====== tf.data =======\n""""""\n\n\ndef get_imagenet_tfdata(datadir, name, batch_size, mapper=None, parallel=None):\n    """"""\n    Args:\n        mapper: a symbolic function that takes a tf.string (the raw bytes read from file) and produces a BGR image.\n            Defaults to `fbresnet_mapper(isTrain)`.\n\n    Returns:\n        A `tf.data.Dataset`. If training, the dataset is infinite.\n        The dataset contains BGR images and labels.\n    """"""\n\n    def get_imglist(dir, name):\n        """"""\n        Returns:\n            [(full filename, label)]\n        """"""\n        dir = os.path.join(dir, name)\n        meta = dataset.ILSVRCMeta()\n        imglist = meta.get_image_list(\n            name,\n            dataset.ILSVRCMeta.guess_dir_structure(dir))\n\n        def _filter(fname):\n            # png\n            return \'n02105855_2933.JPEG\' in fname\n\n        ret = []\n        for fname, label in imglist:\n            if _filter(fname):\n                logger.info(""Image {} was filtered out."".format(fname))\n                continue\n            fname = os.path.join(dir, fname)\n            ret.append((fname, label))\n        return ret\n\n    assert name in [\'train\', \'val\', \'test\']\n    assert datadir is not None\n    isTrain = name == \'train\'\n    if mapper is None:\n        mapper = fbresnet_mapper(isTrain)\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n    imglist = get_imglist(datadir, name)\n\n    N = len(imglist)\n    filenames = tf.constant([k[0] for k in imglist], name=\'filenames\')\n    labels = tf.constant([k[1] for k in imglist], dtype=tf.int32, name=\'labels\')\n\n    ds = tf.data.Dataset.from_tensor_slices((filenames, labels))\n\n    if isTrain:\n        ds = ds.shuffle(N, reshuffle_each_iteration=True).repeat()\n\n    ds = ds.apply(\n        tf.data.experimental.map_and_batch(\n            lambda fname, label: (mapper(tf.read_file(fname)), label),\n            batch_size=batch_size,\n            num_parallel_batches=parallel))\n    ds = ds.prefetch(100)\n    return ds\n\n\ndef fbresnet_mapper(isTrain):\n    """"""\n    Note: compared to fbresnet_augmentor, it\n    lacks some photometric augmentation that may have a small effect (0.1~0.2%) on accuracy.\n    """"""\n    JPEG_OPT = {\'fancy_upscaling\': True, \'dct_method\': \'INTEGER_ACCURATE\'}\n\n    def uint8_resize_bicubic(image, shape):\n        ret = tf.image.resize_bicubic([image], shape)\n        return tf.cast(tf.clip_by_value(ret, 0, 255), tf.uint8)[0]\n\n    def resize_shortest_edge(image, image_shape, size):\n        shape = tf.cast(image_shape, tf.float32)\n        w_greater = tf.greater(image_shape[0], image_shape[1])\n        shape = tf.cond(w_greater,\n                        lambda: tf.cast([shape[0] / shape[1] * size, size], tf.int32),\n                        lambda: tf.cast([size, shape[1] / shape[0] * size], tf.int32))\n\n        return uint8_resize_bicubic(image, shape)\n\n    def center_crop(image, size):\n        image_height = tf.shape(image)[0]\n        image_width = tf.shape(image)[1]\n\n        offset_height = (image_height - size) // 2\n        offset_width = (image_width - size) // 2\n        image = tf.slice(image, [offset_height, offset_width, 0], [size, size, -1])\n        return image\n\n    def lighting(image, std, eigval, eigvec):\n        v = tf.random_normal(shape=[3], stddev=std) * eigval\n        inc = tf.matmul(eigvec, tf.reshape(v, [3, 1]))\n        image = tf.cast(tf.cast(image, tf.float32) + tf.reshape(inc, [3]), image.dtype)\n        return image\n\n    def validation_mapper(byte):\n        image = tf.image.decode_jpeg(\n            tf.reshape(byte, shape=[]), 3, **JPEG_OPT)\n        image = resize_shortest_edge(image, tf.shape(image), 256)\n        image = center_crop(image, 224)\n        image = tf.reverse(image, axis=[2])  # to BGR\n        return image\n\n    def training_mapper(byte):\n        jpeg_shape = tf.image.extract_jpeg_shape(byte)  # hwc\n        bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n            jpeg_shape,\n            bounding_boxes=tf.zeros(shape=[0, 0, 4]),\n            min_object_covered=0,\n            aspect_ratio_range=[0.75, 1.33],\n            area_range=[0.08, 1.0],\n            max_attempts=10,\n            use_image_if_no_bounding_boxes=True)\n\n        is_bad = tf.reduce_sum(tf.cast(tf.equal(bbox_size, jpeg_shape), tf.int32)) >= 2\n\n        def good():\n            offset_y, offset_x, _ = tf.unstack(bbox_begin)\n            target_height, target_width, _ = tf.unstack(bbox_size)\n            crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n            image = tf.image.decode_and_crop_jpeg(\n                byte, crop_window, channels=3, **JPEG_OPT)\n            image = uint8_resize_bicubic(image, [224, 224])\n            return image\n\n        def bad():\n            image = tf.image.decode_jpeg(\n                tf.reshape(byte, shape=[]), 3, **JPEG_OPT)\n            image = resize_shortest_edge(image, jpeg_shape, 224)\n            image = center_crop(image, 224)\n            return image\n\n        image = tf.cond(is_bad, bad, good)\n        # TODO other imgproc\n        image = lighting(image, 0.1,\n                         eigval=np.array([0.2175, 0.0188, 0.0045], dtype=\'float32\') * 255.0,\n                         eigvec=np.array([[-0.5675, 0.7192, 0.4009],\n                                          [-0.5808, -0.0045, -0.8140],\n                                          [-0.5836, -0.6948, 0.4203]], dtype=\'float32\'))\n        image = tf.image.random_flip_left_right(image)\n        image = tf.reverse(image, axis=[2])  # to BGR\n        return image\n\n    return training_mapper if isTrain else validation_mapper\n\n\n""""""\n====== Model & Evaluation =======\n""""""\n\n\ndef eval_classification(model, sessinit, dataflow):\n    """"""\n    Eval a classification model on the dataset. It assumes the model inputs are\n    named ""input"" and ""label"", and contains ""wrong-top1"" and ""wrong-top5"" in the graph.\n    """"""\n    pred_config = PredictConfig(\n        model=model,\n        session_init=sessinit,\n        input_names=[\'input\', \'label\'],\n        output_names=[\'wrong-top1\', \'wrong-top5\']\n    )\n    acc1, acc5 = RatioCounter(), RatioCounter()\n\n    # This does not have a visible improvement over naive predictor,\n    # but will have an improvement if image_dtype is set to float32.\n    pred = FeedfreePredictor(pred_config, StagingInput(QueueInput(dataflow), device=\'/gpu:0\'))\n    for _ in tqdm.trange(dataflow.size()):\n        top1, top5 = pred()\n        batch_size = top1.shape[0]\n        acc1.feed(top1.sum(), batch_size)\n        acc5.feed(top5.sum(), batch_size)\n\n    print(""Top1 Error: {}"".format(acc1.ratio))\n    print(""Top5 Error: {}"".format(acc5.ratio))\n\n\nclass ImageNetModel(ModelDesc):\n    image_shape = 224\n\n    """"""\n    uint8 instead of float32 is used as input type to reduce copy overhead.\n    It might hurt the performance a liiiitle bit.\n    The pretrained models were trained with float32.\n    """"""\n    image_dtype = tf.uint8\n\n    """"""\n    Either \'NCHW\' or \'NHWC\'\n    """"""\n    data_format = \'NCHW\'\n\n    """"""\n    Whether the image is BGR or RGB. If using DataFlow, then it should be BGR.\n    """"""\n    image_bgr = True\n\n    weight_decay = 1e-4\n\n    """"""\n    To apply on normalization parameters, use \'.*/W|.*/gamma|.*/beta\'\n    """"""\n    weight_decay_pattern = \'.*/W\'\n\n    """"""\n    Scale the loss, for whatever reasons (e.g., gradient averaging, fp16 training, etc)\n    """"""\n    loss_scale = 1.\n\n    """"""\n    Label smoothing (See tf.losses.softmax_cross_entropy)\n    """"""\n    label_smoothing = 0.\n\n    """"""\n    Accumulate gradients across several steps (by default 1, which means no accumulation across steps).\n    """"""\n    accum_grad = 1\n\n    def inputs(self):\n        return [tf.TensorSpec([None, self.image_shape, self.image_shape, 3], self.image_dtype, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = self.image_preprocess(image)\n        assert self.data_format in [\'NCHW\', \'NHWC\']\n        if self.data_format == \'NCHW\':\n            image = tf.transpose(image, [0, 3, 1, 2])\n\n        logits = self.get_logits(image)\n        tf.nn.softmax(logits, name=\'prob\')\n        loss = ImageNetModel.compute_loss_and_error(\n            logits, label, label_smoothing=self.label_smoothing)\n\n        if self.weight_decay > 0:\n            wd_loss = regularize_cost(self.weight_decay_pattern,\n                                      l2_regularizer(self.weight_decay),\n                                      name=\'l2_regularize_loss\')\n            add_moving_summary(loss, wd_loss)\n            total_cost = tf.add_n([loss, wd_loss], name=\'cost\')\n        else:\n            total_cost = tf.identity(loss, name=\'cost\')\n            add_moving_summary(total_cost)\n\n        if self.loss_scale != 1.:\n            logger.info(""Scaling the total loss by {} ..."".format(self.loss_scale))\n            return total_cost * self.loss_scale\n        else:\n            return total_cost\n\n    @abstractmethod\n    def get_logits(self, image):\n        """"""\n        Args:\n            image: 4D tensor of ``self.input_shape`` in ``self.data_format``\n\n        Returns:\n            Nx#class logits\n        """"""\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.1, trainable=False)\n        tf.summary.scalar(\'learning_rate-summary\', lr)\n        opt = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n        if self.accum_grad != 1:\n            opt = AccumGradOptimizer(opt, self.accum_grad)\n        return opt\n\n    def image_preprocess(self, image):\n        with tf.name_scope(\'image_preprocess\'):\n            if image.dtype.base_dtype != tf.float32:\n                image = tf.cast(image, tf.float32)\n            mean = [0.485, 0.456, 0.406]    # rgb\n            std = [0.229, 0.224, 0.225]\n            if self.image_bgr:\n                mean = mean[::-1]\n                std = std[::-1]\n            image_mean = tf.constant(mean, dtype=tf.float32) * 255.\n            image_std = tf.constant(std, dtype=tf.float32) * 255.\n            image = (image - image_mean) / image_std\n            return image\n\n    @staticmethod\n    def compute_loss_and_error(logits, label, label_smoothing=0.):\n        if label_smoothing != 0.:\n            nclass = logits.shape[-1]\n            label = tf.one_hot(label, nclass) if label.shape.ndims == 1 else label\n\n        if label.shape.ndims == 1:\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        else:\n            loss = tf.losses.softmax_cross_entropy(\n                label, logits, label_smoothing=label_smoothing,\n                reduction=tf.losses.Reduction.NONE)\n        loss = tf.reduce_mean(loss, name=\'xentropy-loss\')\n\n        def prediction_incorrect(logits, label, topk=1, name=\'incorrect_vector\'):\n            with tf.name_scope(\'prediction_incorrect\'):\n                x = tf.logical_not(tf.nn.in_top_k(logits, label, topk))\n            return tf.cast(x, tf.float32, name=name)\n\n        wrong = prediction_incorrect(logits, label, 1, name=\'wrong-top1\')\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top1\'))\n\n        wrong = prediction_incorrect(logits, label, 5, name=\'wrong-top5\')\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top5\'))\n        return loss\n\n    def create_predict_config(self, session_init):\n        """"""\n        Returns:\n            a :class:`PredictConfig` to be used for inference.\n            The predictor will take inputs and return probabilities.\n\n        Examples:\n\n            pred = OfflinePredictor(model.create_predict_config(SmartInit(args.load)))\n            prob = pred(NCHW_image)[0]  # Nx1000 probabilities\n        """"""\n        return PredictConfig(model=self, input_names=[\'input\'], output_names=[\'prob\'], session_init=session_init)\n\n\nif __name__ == \'__main__\':\n    import argparse\n    from tensorpack.dataflow import TestDataSpeed\n    from tensorpack.tfutils import get_default_sess_config\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data\', required=True)\n    parser.add_argument(\'--batch\', type=int, default=32)\n    parser.add_argument(\'--aug\', choices=[\'train\', \'val\'], default=\'val\')\n    parser.add_argument(\'--symbolic\', action=\'store_true\')\n    args = parser.parse_args()\n\n    if not args.symbolic:\n        augs = fbresnet_augmentor(args.aug == \'train\')\n        df = get_imagenet_dataflow(\n            args.data, \'train\', args.batch, augs)\n        # For val augmentor, Should get >100 it/s (i.e. 3k im/s) here on a decent E5 server.\n        TestDataSpeed(df).start()\n    else:\n        assert args.aug == \'train\'\n        data = get_imagenet_tfdata(args.data, \'train\', args.batch)\n\n        itr = data.make_initializable_iterator()\n        dp = itr.get_next()\n        dpop = tf.group(*dp)\n        with tf.Session(config=get_default_sess_config()) as sess:\n            sess.run(itr.initializer)\n            for _ in tqdm.trange(200):\n                sess.run(dpop)\n            for _ in tqdm.trange(5000, smoothing=0.1):\n                sess.run(dpop)\n'"
examples/Saliency/resnet_model.py,15,"b'# -*- coding: utf-8 -*-\n# File: resnet_model.py\n\nimport tensorflow as tf\n\nfrom tensorpack.models import BatchNorm, BNReLU, Conv2D, FullyConnected, GlobalAvgPooling, MaxPooling\nfrom tensorpack.tfutils.argscope import argscope, get_arg_scope\n\n\ndef resnet_shortcut(l, n_out, stride, activation=tf.identity):\n    data_format = get_arg_scope()[\'Conv2D\'][\'data_format\']\n    n_in = l.get_shape().as_list()[1 if data_format in [\'NCHW\', \'channels_first\'] else 3]\n    if n_in != n_out:   # change dimension when channel is not the same\n        return Conv2D(\'convshortcut\', l, n_out, 1, strides=stride, activation=activation)\n    else:\n        return l\n\n\ndef get_bn(zero_init=False):\n    """"""\n    Zero init gamma is good for resnet. See https://arxiv.org/abs/1706.02677.\n    """"""\n    if zero_init:\n        return lambda x, name=None: BatchNorm(\'bn\', x, gamma_initializer=tf.zeros_initializer())\n    else:\n        return lambda x, name=None: BatchNorm(\'bn\', x)\n\n\n# ----------------- pre-activation resnet ----------------------\ndef apply_preactivation(l, preact):\n    if preact == \'bnrelu\':\n        shortcut = l    # preserve identity mapping\n        l = BNReLU(\'preact\', l)\n    else:\n        shortcut = l\n    return l, shortcut\n\n\ndef preact_basicblock(l, ch_out, stride, preact):\n    l, shortcut = apply_preactivation(l, preact)\n    l = Conv2D(\'conv1\', l, ch_out, 3, strides=stride, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3)\n    return l + resnet_shortcut(shortcut, ch_out, stride)\n\n\ndef preact_bottleneck(l, ch_out, stride, preact):\n    # stride is applied on the second conv, following fb.resnet.torch\n    l, shortcut = apply_preactivation(l, preact)\n    l = Conv2D(\'conv1\', l, ch_out, 1, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3, strides=stride, activation=BNReLU)\n    l = Conv2D(\'conv3\', l, ch_out * 4, 1)\n    return l + resnet_shortcut(shortcut, ch_out * 4, stride)\n\n\ndef preact_group(name, l, block_func, features, count, stride):\n    with tf.variable_scope(name):\n        for i in range(0, count):\n            with tf.variable_scope(\'block{}\'.format(i)):\n                # first block doesn\'t need activation\n                l = block_func(l, features,\n                               stride if i == 0 else 1,\n                               \'no_preact\' if i == 0 else \'bnrelu\')\n        # end of each group need an extra activation\n        l = BNReLU(\'bnlast\', l)\n    return l\n# ----------------- pre-activation resnet ----------------------\n\n\ndef resnet_basicblock(l, ch_out, stride):\n    shortcut = l\n    l = Conv2D(\'conv1\', l, ch_out, 3, strides=stride, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3, activation=get_bn(zero_init=True))\n    out = l + resnet_shortcut(shortcut, ch_out, stride, activation=get_bn(zero_init=False))\n    return tf.nn.relu(out)\n\n\ndef resnet_bottleneck(l, ch_out, stride, stride_first=False):\n    """"""\n    stride_first: original resnet put stride on first conv. fb.resnet.torch put stride on second conv.\n    """"""\n    shortcut = l\n    l = Conv2D(\'conv1\', l, ch_out, 1, strides=stride if stride_first else 1, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3, strides=1 if stride_first else stride, activation=BNReLU)\n    l = Conv2D(\'conv3\', l, ch_out * 4, 1, activation=get_bn(zero_init=True))\n    out = l + resnet_shortcut(shortcut, ch_out * 4, stride, activation=get_bn(zero_init=False))\n    return tf.nn.relu(out)\n\n\ndef se_bottleneck(l, ch_out, stride):\n    shortcut = l\n    l = Conv2D(\'conv1\', l, ch_out, 1, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out, 3, strides=stride, activation=BNReLU)\n    l = Conv2D(\'conv3\', l, ch_out * 4, 1, activation=get_bn(zero_init=True))\n\n    squeeze = GlobalAvgPooling(\'gap\', l)\n    squeeze = FullyConnected(\'fc1\', squeeze, ch_out // 4, activation=tf.nn.relu)\n    squeeze = FullyConnected(\'fc2\', squeeze, ch_out * 4, activation=tf.nn.sigmoid)\n    data_format = get_arg_scope()[\'Conv2D\'][\'data_format\']\n    ch_ax = 1 if data_format in [\'NCHW\', \'channels_first\'] else 3\n    shape = [-1, 1, 1, 1]\n    shape[ch_ax] = ch_out * 4\n    l = l * tf.reshape(squeeze, shape)\n    out = l + resnet_shortcut(shortcut, ch_out * 4, stride, activation=get_bn(zero_init=False))\n    return tf.nn.relu(out)\n\n\ndef resnext32x4d_bottleneck(l, ch_out, stride):\n    shortcut = l\n    l = Conv2D(\'conv1\', l, ch_out * 2, 1, strides=1, activation=BNReLU)\n    l = Conv2D(\'conv2\', l, ch_out * 2, 3, strides=stride, activation=BNReLU, split=32)\n    l = Conv2D(\'conv3\', l, ch_out * 4, 1, activation=get_bn(zero_init=True))\n    out = l + resnet_shortcut(shortcut, ch_out * 4, stride, activation=get_bn(zero_init=False))\n    return tf.nn.relu(out)\n\n\ndef resnet_group(name, l, block_func, features, count, stride):\n    with tf.variable_scope(name):\n        for i in range(0, count):\n            with tf.variable_scope(\'block{}\'.format(i)):\n                l = block_func(l, features, stride if i == 0 else 1)\n    return l\n\n\ndef resnet_backbone(image, num_blocks, group_func, block_func):\n    with argscope(Conv2D, use_bias=False,\n                  kernel_initializer=tf.variance_scaling_initializer(scale=2.0, mode=\'fan_out\')):\n        # Note that TF pads the image by [2, 3] instead of [3, 2].\n        # Similar things happen in later stride=2 layers as well.\n        l = Conv2D(\'conv0\', image, 64, 7, strides=2, activation=BNReLU)\n        l = MaxPooling(\'pool0\', l, pool_size=3, strides=2, padding=\'SAME\')\n        l = group_func(\'group0\', l, block_func, 64, num_blocks[0], 1)\n        l = group_func(\'group1\', l, block_func, 128, num_blocks[1], 2)\n        l = group_func(\'group2\', l, block_func, 256, num_blocks[2], 2)\n        l = group_func(\'group3\', l, block_func, 512, num_blocks[3], 2)\n        l = GlobalAvgPooling(\'gap\', l)\n        logits = FullyConnected(\'linear\', l, 1000,\n                                kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n    return logits\n'"
examples/Saliency/saliency-maps.py,12,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport sys\nfrom contextlib import contextmanager\nimport cv2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim.nets import resnet_v1\n\nimport tensorpack as tp\nimport tensorpack.utils.viz as viz\nfrom tensorpack.tfutils import get_tf_version_tuple\n\nIMAGE_SIZE = 224\n\n\n@contextmanager\ndef guided_relu():\n    """"""\n    Returns:\n        A context where the gradient of :meth:`tf.nn.relu` is replaced by\n        guided back-propagation, as described in the paper:\n        `Striving for Simplicity: The All Convolutional Net\n        <https://arxiv.org/abs/1412.6806>`_\n    """"""\n    from tensorflow.python.ops import gen_nn_ops   # noqa\n\n    @tf.RegisterGradient(""GuidedReLU"")\n    def GuidedReluGrad(op, grad):\n        return tf.where(0. < grad,\n                        gen_nn_ops.relu_grad(grad, op.outputs[0]),\n                        tf.zeros(grad.get_shape()))\n\n    g = tf.get_default_graph()\n    with g.gradient_override_map({\'Relu\': \'GuidedReLU\'}):\n        yield\n\n\ndef saliency_map(output, input, name=""saliency_map""):\n    """"""\n    Produce a saliency map as described in the paper:\n    `Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n    <https://arxiv.org/abs/1312.6034>`_.\n    The saliency map is the gradient of the max element in output w.r.t input.\n\n    Returns:\n        tf.Tensor: the saliency map. Has the same shape as input.\n    """"""\n    max_outp = tf.reduce_max(output, 1)\n    saliency_op = tf.gradients(max_outp, input)[:][0]\n    return tf.identity(saliency_op, name=name)\n\n\nclass Model(tp.ModelDescBase):\n    def inputs(self):\n        return [tf.TensorSpec((IMAGE_SIZE, IMAGE_SIZE, 3), tf.float32, \'image\')]\n\n    def build_graph(self, orig_image):\n        mean = tf.get_variable(\'resnet_v1_50/mean_rgb\', shape=[3])\n        with guided_relu():\n            with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n                image = tf.expand_dims(orig_image - mean, 0)\n                logits, _ = resnet_v1.resnet_v1_50(image, 1000, is_training=False)\n            saliency_map(logits, orig_image, name=""saliency"")\n\n\ndef run(model_path, image_path):\n    predictor = tp.OfflinePredictor(tp.PredictConfig(\n        model=Model(),\n        session_init=tp.SmartInit(model_path),\n        input_names=[\'image\'],\n        output_names=[\'saliency\']))\n    im = cv2.imread(image_path)\n    assert im is not None and im.ndim == 3, image_path\n\n    # resnet expect RGB inputs of 224x224x3\n    im = cv2.resize(im, (IMAGE_SIZE, IMAGE_SIZE))\n    im = im.astype(np.float32)[:, :, ::-1]\n\n    saliency_images = predictor(im)[0]\n\n    abs_saliency = np.abs(saliency_images).max(axis=-1)\n    pos_saliency = np.maximum(0, saliency_images)\n    neg_saliency = np.maximum(0, -saliency_images)\n\n    pos_saliency -= pos_saliency.min()\n    pos_saliency /= pos_saliency.max()\n    cv2.imwrite(\'pos.jpg\', pos_saliency * 255)\n\n    neg_saliency -= neg_saliency.min()\n    neg_saliency /= neg_saliency.max()\n    cv2.imwrite(\'neg.jpg\', neg_saliency * 255)\n\n    abs_saliency = viz.intensity_to_rgb(abs_saliency, normalize=True)[:, :, ::-1]  # bgr\n    cv2.imwrite(""abs-saliency.jpg"", abs_saliency)\n\n    rsl = im * 0.2 + abs_saliency * 0.8\n    cv2.imwrite(""blended.jpg"", rsl)\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) != 2:\n        tp.logger.error(""Usage: {} image.jpg"".format(sys.argv[0]))\n        sys.exit(1)\n    assert get_tf_version_tuple() >= (1, 7), ""requires TF >= 1.7""\n    run(""resnet_v1_50.ckpt"", sys.argv[1])\n'"
examples/SimilarityLearning/embedding_data.py,0,"b'# -*- coding: utf-8 -*-\n# File: embedding_data.py\n# Author: tensorpack contributors\n\nimport numpy as np\n\nfrom tensorpack.dataflow import BatchData, dataset\n\n\ndef get_test_data(batch=128):\n    ds = dataset.Mnist(\'test\')\n    ds = BatchData(ds, batch)\n    return ds\n\n\ndef get_digits_by_label(images, labels):\n    data_dict = []\n    for clazz in range(0, 10):\n        clazz_filter = np.where(labels == clazz)\n        data_dict.append(list(images[clazz_filter].reshape((-1, 28, 28))))\n    return data_dict\n\n\nclass MnistPairs(dataset.Mnist):\n    """"""We could also write\n\n    .. code::\n\n        ds = dataset.Mnist(\'train\')\n        ds = JoinData([ds, ds])\n        ds = MapData(ds, lambda dp: [dp[0], dp[2], dp[1] == dp[3]])\n        ds = BatchData(ds, 128 // 2)\n\n    but then the positives pairs would be really rare (p=0.1).\n    """"""\n    def __init__(self, train_or_test):\n        super(MnistPairs, self).__init__(train_or_test, shuffle=False)\n        # now categorize these digits\n        self.data_dict = get_digits_by_label(self.images, self.labels)\n\n    def pick(self, label):\n        idx = self.rng.randint(len(self.data_dict[label]))\n        return self.data_dict[label][idx].astype(np.float32)\n\n    def __iter__(self):\n        while True:\n            y = self.rng.randint(2)\n            if y == 0:\n                pick_label, pick_other = self.rng.choice(10, size=2, replace=False)\n            else:\n                pick_label = self.rng.randint(10)\n                pick_other = pick_label\n\n            yield [self.pick(pick_label), self.pick(pick_other), y]\n\n\nclass MnistTriplets(MnistPairs):\n    def __iter__(self):\n        while True:\n            pick_label, pick_other = self.rng.choice(10, size=2, replace=False)\n            yield [self.pick(pick_label), self.pick(pick_label), self.pick(pick_other)]\n'"
examples/SimilarityLearning/mnist-embeddings.py,84,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-embeddings.py\n\nimport argparse\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.utils.gpu import change_gpu\n\nfrom embedding_data import MnistPairs, MnistTriplets, get_test_data\n\nMATPLOTLIB_AVAIBLABLE = False\ntry:\n    from matplotlib import offsetbox\n    import matplotlib.pyplot as plt\n    MATPLOTLIB_AVAIBLABLE = True\nexcept ImportError:\n    MATPLOTLIB_AVAIBLABLE = False\n\n\ndef contrastive_loss(left, right, y, margin, extra=False, scope=""constrastive_loss""):\n    r""""""Loss for Siamese networks as described in the paper:\n    `Learning a Similarity Metric Discriminatively, with Application to Face\n    Verification <http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf>`_ by Chopra et al.\n\n    .. math::\n        \\frac{1}{2} [y \\cdot d^2 + (1-y) \\cdot \\max(0, m - d)^2], d = \\Vert l - r \\Vert_2\n\n    Args:\n        left (tf.Tensor): left feature vectors of shape [Batch, N].\n        right (tf.Tensor): right feature vectors of shape [Batch, N].\n        y (tf.Tensor): binary labels of shape [Batch]. 1: similar, 0: not similar.\n        margin (float): horizon for negative examples (y==0).\n        extra (bool): also return distances for pos and neg.\n\n    Returns:\n        tf.Tensor: constrastive_loss (averaged over the batch), (and optionally average_pos_dist, average_neg_dist)\n    """"""\n    with tf.name_scope(scope):\n        y = tf.cast(y, tf.float32)\n\n        delta = tf.reduce_sum(tf.square(left - right), 1)\n        delta_sqrt = tf.sqrt(delta + 1e-10)\n\n        match_loss = delta\n        missmatch_loss = tf.square(tf.nn.relu(margin - delta_sqrt))\n\n        loss = tf.reduce_mean(0.5 * (y * match_loss + (1 - y) * missmatch_loss))\n\n        if extra:\n            num_pos = tf.count_nonzero(y)\n            num_neg = tf.count_nonzero(1 - y)\n            pos_dist = tf.where(tf.equal(num_pos, 0), 0.,\n                                tf.reduce_sum(y * delta_sqrt) / tf.cast(num_pos, tf.float32),\n                                name=""pos-dist"")\n            neg_dist = tf.where(tf.equal(num_neg, 0), 0.,\n                                tf.reduce_sum((1 - y) * delta_sqrt) / tf.cast(num_neg, tf.float32),\n                                name=""neg-dist"")\n            return loss, pos_dist, neg_dist\n        else:\n            return loss\n\n\ndef siamese_cosine_loss(left, right, y, scope=""cosine_loss""):\n    r""""""Loss for Siamese networks (cosine version).\n    Same as :func:`contrastive_loss` but with different similarity measurement.\n\n    .. math::\n        [\\frac{l \\cdot r}{\\lVert l\\rVert \\lVert r\\rVert} - (2y-1)]^2\n\n    Args:\n        left (tf.Tensor): left feature vectors of shape [Batch, N].\n        right (tf.Tensor): right feature vectors of shape [Batch, N].\n        y (tf.Tensor): binary labels of shape [Batch]. 1: similar, 0: not similar.\n\n    Returns:\n        tf.Tensor: cosine-loss as a scalar tensor.\n    """"""\n\n    def l2_norm(t, eps=1e-12):\n        """"""\n        Returns:\n            tf.Tensor: norm of 2D input tensor on axis 1\n        """"""\n        with tf.name_scope(""l2_norm""):\n            return tf.sqrt(tf.reduce_sum(tf.square(t), 1) + eps)\n\n    with tf.name_scope(scope):\n        y = 2 * tf.cast(y, tf.float32) - 1\n        pred = tf.reduce_sum(left * right, 1) / (l2_norm(left) * l2_norm(right) + 1e-10)\n\n        return tf.nn.l2_loss(y - pred) / tf.cast(tf.shape(left)[0], tf.float32)\n\n\ndef triplet_loss(anchor, positive, negative, margin, extra=False, scope=""triplet_loss""):\n    r""""""Loss for Triplet networks as described in the paper:\n    `FaceNet: A Unified Embedding for Face Recognition and Clustering\n    <https://arxiv.org/abs/1503.03832>`_\n    by Schroff et al.\n\n    Learn embeddings from an anchor point and a similar input (positive) as\n    well as a not-similar input (negative).\n    Intuitively, a matching pair (anchor, positive) should have a smaller relative distance\n    than a non-matching pair (anchor, negative).\n\n    .. math::\n        \\max(0, m + \\Vert a-p\\Vert^2 - \\Vert a-n\\Vert^2)\n\n    Args:\n        anchor (tf.Tensor): anchor feature vectors of shape [Batch, N].\n        positive (tf.Tensor): features of positive match of the same shape.\n        negative (tf.Tensor): features of negative match of the same shape.\n        margin (float): horizon for negative examples\n        extra (bool): also return distances for pos and neg.\n\n    Returns:\n        tf.Tensor: triplet-loss as scalar (and optionally average_pos_dist, average_neg_dist)\n    """"""\n\n    with tf.name_scope(scope):\n        d_pos = tf.reduce_sum(tf.square(anchor - positive), 1)\n        d_neg = tf.reduce_sum(tf.square(anchor - negative), 1)\n\n        loss = tf.reduce_mean(tf.maximum(0., margin + d_pos - d_neg))\n\n        if extra:\n            pos_dist = tf.reduce_mean(tf.sqrt(d_pos + 1e-10), name=\'pos-dist\')\n            neg_dist = tf.reduce_mean(tf.sqrt(d_neg + 1e-10), name=\'neg-dist\')\n            return loss, pos_dist, neg_dist\n        else:\n            return loss\n\n\ndef soft_triplet_loss(anchor, positive, negative, extra=True, scope=""soft_triplet_loss""):\n    r""""""Loss for triplet networks as described in the paper:\n    `Deep Metric Learning using Triplet Network\n    <https://arxiv.org/abs/1412.6622>`_ by Hoffer et al.\n\n    It is a softmax loss using :math:`(anchor-positive)^2` and\n    :math:`(anchor-negative)^2` as logits.\n\n    Args:\n        anchor (tf.Tensor): anchor feature vectors of shape [Batch, N].\n        positive (tf.Tensor): features of positive match of the same shape.\n        negative (tf.Tensor): features of negative match of the same shape.\n        extra (bool): also return distances for pos and neg.\n\n    Returns:\n        tf.Tensor: triplet-loss as scalar (and optionally average_pos_dist, average_neg_dist)\n    """"""\n\n    eps = 1e-10\n    with tf.name_scope(scope):\n        d_pos = tf.sqrt(tf.reduce_sum(tf.square(anchor - positive), 1) + eps)\n        d_neg = tf.sqrt(tf.reduce_sum(tf.square(anchor - negative), 1) + eps)\n\n        logits = tf.stack([d_pos, d_neg], axis=1)\n        ones = tf.ones_like(tf.squeeze(d_pos), dtype=""int32"")\n\n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=ones))\n\n        if extra:\n            pos_dist = tf.reduce_mean(d_pos, name=\'pos-dist\')\n            neg_dist = tf.reduce_mean(d_neg, name=\'neg-dist\')\n            return loss, pos_dist, neg_dist\n        else:\n            return loss\n\n\ndef center_loss(embedding, label, num_classes, alpha=0.1, scope=""center_loss""):\n    r""""""Center-Loss as described in the paper\n    `A Discriminative Feature Learning Approach for Deep Face Recognition`\n    <http://ydwen.github.io/papers/WenECCV16.pdf> by Wen et al.\n\n    Args:\n        embedding (tf.Tensor): features produced by the network\n        label (tf.Tensor): ground-truth label for each feature\n        num_classes (int): number of different classes\n        alpha (float): learning rate for updating the centers\n\n    Returns:\n        tf.Tensor: center loss\n    """"""\n    nrof_features = embedding.get_shape()[1]\n    centers = tf.get_variable(\'centers\', [num_classes, nrof_features], dtype=tf.float32,\n                              initializer=tf.constant_initializer(0), trainable=False)\n    label = tf.reshape(label, [-1])\n    centers_batch = tf.gather(centers, label)\n    diff = (1 - alpha) * (centers_batch - embedding)\n    centers = tf.scatter_sub(centers, label, diff)\n    loss = tf.reduce_mean(tf.square(embedding - centers_batch), name=scope)\n    return loss\n\n\nclass EmbeddingModel(ModelDesc):\n    def embed(self, x, nfeatures=2):\n        """"""Embed all given tensors into an nfeatures-dim space.  """"""\n        list_split = 0\n        if isinstance(x, list):\n            list_split = len(x)\n            x = tf.concat(x, 0)\n\n        # pre-process MNIST dataflow data\n        x = tf.expand_dims(x, 3)\n        x = x * 2 - 1\n\n        # the embedding network\n        net = slim.layers.conv2d(x, 20, 5, scope=\'conv1\')\n        net = slim.layers.max_pool2d(net, 2, scope=\'pool1\')\n        net = slim.layers.conv2d(net, 50, 5, scope=\'conv2\')\n        net = slim.layers.max_pool2d(net, 2, scope=\'pool2\')\n        net = slim.layers.flatten(net, scope=\'flatten3\')\n        net = slim.layers.fully_connected(net, 500, scope=\'fully_connected4\')\n        embeddings = slim.layers.fully_connected(net, nfeatures, activation_fn=None, scope=\'fully_connected5\')\n\n        # if ""x"" was a list of tensors, then split the embeddings\n        if list_split > 0:\n            embeddings = tf.split(embeddings, list_split, 0)\n\n        return embeddings\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=1e-4, trainable=False)\n        return tf.train.GradientDescentOptimizer(lr)\n\n\nclass SiameseModel(EmbeddingModel):\n    @staticmethod\n    def get_data():\n        ds = MnistPairs(\'train\')\n        ds = BatchData(ds, 128 // 2)\n        return ds\n\n    def inputs(self):\n        return [tf.TensorSpec((None, 28, 28), tf.float32, \'input\'),\n                tf.TensorSpec((None, 28, 28), tf.float32, \'input_y\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, x, y, label):\n        # embed them\n        single_input = x\n        x, y = self.embed([x, y])\n\n        # tag the embedding of \'input\' with name \'emb\', just for inference later on\n        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n            tf.identity(self.embed(single_input), name=""emb"")\n\n        # compute the actual loss\n        cost, pos_dist, neg_dist = contrastive_loss(x, y, label, 5., extra=True, scope=""loss"")\n        cost = tf.identity(cost, name=""cost"")\n\n        # track these values during training\n        add_moving_summary(pos_dist, neg_dist, cost)\n        return cost\n\n\nclass CosineModel(SiameseModel):\n    def build_graph(self, x, y, label):\n        single_input = x\n        x, y = self.embed([x, y])\n\n        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n            tf.identity(self.embed(single_input), name=""emb"")\n\n        cost = siamese_cosine_loss(x, y, label, scope=""loss"")\n        cost = tf.identity(cost, name=""cost"")\n        add_moving_summary(cost)\n        return cost\n\n\nclass TripletModel(EmbeddingModel):\n    @staticmethod\n    def get_data():\n        ds = MnistTriplets(\'train\')\n        ds = BatchData(ds, 128 // 3)\n        return ds\n\n    def inputs(self):\n        return [tf.TensorSpec((None, 28, 28), tf.float32, \'input\'),\n                tf.TensorSpec((None, 28, 28), tf.float32, \'input_p\'),\n                tf.TensorSpec((None, 28, 28), tf.float32, \'input_n\')]\n\n    def loss(self, a, p, n):\n        return triplet_loss(a, p, n, 5., extra=True, scope=""loss"")\n\n    def build_graph(self, a, p, n):\n        single_input = a\n        a, p, n = self.embed([a, p, n])\n\n        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n            tf.identity(self.embed(single_input), name=""emb"")\n\n        cost, pos_dist, neg_dist = self.loss(a, p, n)\n\n        cost = tf.identity(cost, name=""cost"")\n        add_moving_summary(pos_dist, neg_dist, cost)\n        return cost\n\n\nclass SoftTripletModel(TripletModel):\n    def loss(self, a, p, n):\n        return soft_triplet_loss(a, p, n, scope=""loss"")\n\n\nclass CenterModel(EmbeddingModel):\n    @staticmethod\n    def get_data():\n        ds = dataset.Mnist(\'train\')\n        ds = BatchData(ds, 128)\n        return ds\n\n    def inputs(self):\n        return [tf.TensorSpec((None, 28, 28), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, x, label):\n        # embed them\n        x = self.embed(x)\n        x = tf.identity(x, name=\'emb\')\n\n        # compute the embedding loss\n        emb_cost = center_loss(x, label, 10, 0.01)\n        # compute the classification loss\n        logits = slim.layers.fully_connected(tf.nn.relu(x), 10, activation_fn=None, scope=\'logits\')\n\n        cls_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label),\n                                  name=\'classification_costs\')\n        total_cost = tf.add(emb_cost, 100 * cls_cost, name=""cost"")\n\n        # track these values during training\n        add_moving_summary(total_cost, cls_cost, emb_cost)\n        return total_cost\n\n\ndef get_config(model, algorithm_name):\n\n    extra_display = [""cost""]\n    if not algorithm_name == ""cosine"" and not algorithm_name == ""center"":\n        extra_display = extra_display + [""loss/pos-dist"", ""loss/neg-dist""]\n\n    return TrainConfig(\n        dataflow=model.get_data(),\n        model=model(),\n        callbacks=[\n            ModelSaver(),\n            ScheduledHyperParamSetter(\'learning_rate\', [(10, 1e-5), (20, 1e-6)])\n        ],\n        extra_callbacks=[\n            MovingAverageSummary(),\n            ProgressBar(extra_display),\n            MergeAllSummaries(),\n            RunUpdateOps()\n        ],\n        max_epoch=20,\n    )\n\n\ndef visualize(model_path, model, algo_name):\n    if not MATPLOTLIB_AVAIBLABLE:\n        logger.error(""visualize requires matplotlib package ..."")\n        return\n    pred = OfflinePredictor(PredictConfig(\n        session_init=SmartInit(model_path),\n        model=model(),\n        input_names=[\'input\'],\n        output_names=[\'emb\']))\n\n    NUM_BATCHES = 6\n    BATCH_SIZE = 128\n    images = np.zeros((BATCH_SIZE * NUM_BATCHES, 28, 28))  # the used digits\n    embed = np.zeros((BATCH_SIZE * NUM_BATCHES, 2))  # the actual embeddings in 2-d\n\n    # get only the embedding model data (MNIST test)\n    ds = get_test_data()\n    ds.reset_state()\n\n    for offset, dp in enumerate(ds):\n        digit, label = dp\n        prediction = pred(digit)[0]\n        embed[offset * BATCH_SIZE:offset * BATCH_SIZE + BATCH_SIZE, ...] = prediction\n        images[offset * BATCH_SIZE:offset * BATCH_SIZE + BATCH_SIZE, ...] = digit\n        offset += 1\n        if offset == NUM_BATCHES:\n            break\n\n    plt.figure()\n    ax = plt.subplot(111)\n    ax_min = np.min(embed, 0)\n    ax_max = np.max(embed, 0)\n\n    ax_dist_sq = np.sum((ax_max - ax_min)**2)\n    ax.axis(\'off\')\n    shown_images = np.array([[1., 1.]])\n    for i in range(embed.shape[0]):\n        dist = np.sum((embed[i] - shown_images)**2, 1)\n        if np.min(dist) < 3e-4 * ax_dist_sq:     # don\'t show points that are too close\n            continue\n        shown_images = np.r_[shown_images, [embed[i]]]\n        imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(np.reshape(images[i, ...], [28, 28]),\n                                            zoom=0.6, cmap=plt.cm.gray_r), xy=embed[i], frameon=False)\n        ax.add_artist(imagebox)\n\n    plt.axis([ax_min[0], ax_max[0], ax_min[1], ax_max[1]])\n    plt.xticks([]), plt.yticks([])\n    plt.title(\'Embedding using %s-loss\' % algo_name)\n    plt.savefig(\'%s.jpg\' % algo_name)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'-a\', \'--algorithm\', help=\'used algorithm\', required=True,\n                        choices=[""siamese"", ""cosine"", ""triplet"", ""softtriplet"", ""center""])\n    parser.add_argument(\'--visualize\', help=\'export embeddings into an image\', action=\'store_true\')\n    args = parser.parse_args()\n\n    ALGO_CONFIGS = {""siamese"": SiameseModel,\n                    ""cosine"": CosineModel,\n                    ""triplet"": TripletModel,\n                    ""softtriplet"": SoftTripletModel,\n                    ""center"": CenterModel}\n\n    logger.auto_set_dir(name=args.algorithm)\n\n    with change_gpu(args.gpu):\n        if args.visualize:\n            visualize(args.load, ALGO_CONFIGS[args.algorithm], args.algorithm)\n        else:\n            config = get_config(ALGO_CONFIGS[args.algorithm], args.algorithm)\n            config.session_init = SmartInit(args.load)\n            launch_train_with_config(config, SimpleTrainer())\n'"
examples/SpatialTransformer/mnist-addition.py,62,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-addition.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils import gradproc, optimizer, summary\n\nIMAGE_SIZE = 42\nWARP_TARGET_SIZE = 28\nHALF_DIFF = (IMAGE_SIZE - WARP_TARGET_SIZE) // 2\n\n\ndef sample(img, coords):\n    """"""\n    Args:\n        img: bxhxwxc\n        coords: bxh2xw2x2. each coordinate is (y, x) integer.\n            Out of boundary coordinates will be clipped.\n    Return:\n        bxh2xw2xc image\n    """"""\n    shape = img.get_shape().as_list()[1:]   # h, w, c\n    batch = tf.shape(img)[0]\n    shape2 = coords.get_shape().as_list()[1:3]  # h2, w2\n    assert None not in shape2, coords.get_shape()\n    max_coor = tf.constant([shape[0] - 1, shape[1] - 1], dtype=tf.float32)\n\n    coords = tf.clip_by_value(coords, 0., max_coor)  # borderMode==repeat\n    coords = tf.cast(coords, tf.int32)\n\n    batch_index = tf.range(batch, dtype=tf.int32)\n    batch_index = tf.reshape(batch_index, [-1, 1, 1, 1])\n    batch_index = tf.tile(batch_index, [1, shape2[0], shape2[1], 1])    # bxh2xw2x1\n    indices = tf.concat([batch_index, coords], axis=3)  # bxh2xw2x3\n    sampled = tf.gather_nd(img, indices)\n    return sampled\n\n\n@layer_register(log_shape=True)\ndef GridSample(inputs, borderMode=\'repeat\'):\n    """"""\n    Sample the images using the given coordinates, by bilinear interpolation.\n    This was described in the paper:\n    `Spatial Transformer Networks <http://arxiv.org/abs/1506.02025>`_.\n\n    This is equivalent to `torch.nn.functional.grid_sample`,\n    up to some non-trivial coordinate transformation.\n\n    This implementation returns pixel value at pixel (1, 1) for a floating point coordinate (1.0, 1.0).\n    Note that this may not be what you need.\n\n    Args:\n        inputs (list): [images, coords]. images has shape NHWC.\n            coords has shape (N, H\', W\', 2), where each pair of the last dimension is a (y, x) real-value\n            coordinate.\n        borderMode: either ""repeat"" or ""constant"" (zero-filled)\n\n    Returns:\n        tf.Tensor: a tensor named ``output`` of shape (N, H\', W\', C).\n    """"""\n    image, mapping = inputs\n    assert image.get_shape().ndims == 4 and mapping.get_shape().ndims == 4\n    input_shape = image.get_shape().as_list()[1:]\n    assert None not in input_shape, \\\n        ""Images in GridSample layer must have fully-defined shape""\n    assert borderMode in [\'repeat\', \'constant\']\n\n    orig_mapping = mapping\n    mapping = tf.maximum(mapping, 0.0)\n    lcoor = tf.floor(mapping)\n    ucoor = lcoor + 1\n\n    diff = mapping - lcoor\n    neg_diff = 1.0 - diff  # bxh2xw2x2\n\n    lcoory, lcoorx = tf.split(lcoor, 2, 3)\n    ucoory, ucoorx = tf.split(ucoor, 2, 3)\n\n    lyux = tf.concat([lcoory, ucoorx], 3)\n    uylx = tf.concat([ucoory, lcoorx], 3)\n\n    diffy, diffx = tf.split(diff, 2, 3)\n    neg_diffy, neg_diffx = tf.split(neg_diff, 2, 3)\n\n    ret = tf.add_n([sample(image, lcoor) * neg_diffx * neg_diffy,\n                    sample(image, ucoor) * diffx * diffy,\n                    sample(image, lyux) * neg_diffy * diffx,\n                    sample(image, uylx) * diffy * neg_diffx], name=\'sampled\')\n    if borderMode == \'constant\':\n        max_coor = tf.constant([input_shape[0] - 1, input_shape[1] - 1], dtype=tf.float32)\n        mask = tf.greater_equal(orig_mapping, 0.0)\n        mask2 = tf.less_equal(orig_mapping, max_coor)\n        mask = tf.logical_and(mask, mask2)  # bxh2xw2x2\n        mask = tf.reduce_all(mask, [3])  # bxh2xw2 boolean\n        mask = tf.expand_dims(mask, 3)\n        ret = ret * tf.cast(mask, tf.float32)\n    return tf.identity(ret, name=\'output\')\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, IMAGE_SIZE, IMAGE_SIZE, 2), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        xys = np.array([(y, x, 1) for y in range(WARP_TARGET_SIZE)\n                        for x in range(WARP_TARGET_SIZE)], dtype=\'float32\')\n        xys = tf.constant(xys, dtype=tf.float32, name=\'xys\')    # p x 3\n\n        image = image / 255.0 - 0.5  # bhw2\n\n        def get_stn(image):\n            stn = (LinearWrap(image)\n                   .AvgPooling(\'downsample\', 2)\n                   .Conv2D(\'conv0\', 20, 5, padding=\'VALID\')\n                   .MaxPooling(\'pool0\', 2)\n                   .Conv2D(\'conv1\', 20, 5, padding=\'VALID\')\n                   .FullyConnected(\'fc1\', 32)\n                   .FullyConnected(\'fct\', 6, activation=tf.identity,\n                                   kernel_initializer=tf.constant_initializer(),\n                                   bias_initializer=tf.constant_initializer([1, 0, HALF_DIFF, 0, 1, HALF_DIFF]))())\n            # output 6 parameters for affine transformation\n            stn = tf.reshape(stn, [-1, 2, 3], name=\'affine\')  # bx2x3\n            stn = tf.reshape(tf.transpose(stn, [2, 0, 1]), [3, -1])  # 3 x (bx2)\n            coor = tf.reshape(tf.matmul(xys, stn),\n                              [WARP_TARGET_SIZE, WARP_TARGET_SIZE, -1, 2])\n            coor = tf.transpose(coor, [2, 0, 1, 3], \'sampled_coords\')  # b h w 2\n            sampled = GridSample(\'warp\', [image, coor], borderMode=\'constant\')\n            return sampled\n\n        with argscope([Conv2D, FullyConnected], activation=tf.nn.relu):\n            with tf.variable_scope(\'STN1\'):\n                sampled1 = get_stn(image)\n            with tf.variable_scope(\'STN2\'):\n                sampled2 = get_stn(image)\n\n        # For visualization in tensorboard\n        with tf.name_scope(\'visualization\'):\n            padded1 = tf.pad(sampled1, [[0, 0], [HALF_DIFF, HALF_DIFF], [HALF_DIFF, HALF_DIFF], [0, 0]])\n            padded2 = tf.pad(sampled2, [[0, 0], [HALF_DIFF, HALF_DIFF], [HALF_DIFF, HALF_DIFF], [0, 0]])\n            img_orig = tf.concat([image[:, :, :, 0], image[:, :, :, 1]], 1)  # b x 2h  x w\n            transform1 = tf.concat([padded1[:, :, :, 0], padded1[:, :, :, 1]], 1)\n            transform2 = tf.concat([padded2[:, :, :, 0], padded2[:, :, :, 1]], 1)\n            stacked = tf.concat([img_orig, transform1, transform2], 2, \'viz\')\n            tf.summary.image(\'visualize\',\n                             tf.expand_dims(stacked, -1), max_outputs=30)\n\n        sampled = tf.concat([sampled1, sampled2], 3, \'sampled_concat\')\n        logits = (LinearWrap(sampled)\n                  .FullyConnected(\'fc1\', 256, activation=tf.nn.relu)\n                  .FullyConnected(\'fc2\', 128, activation=tf.nn.relu)\n                  .FullyConnected(\'fct\', 19, activation=tf.identity)())\n        tf.nn.softmax(logits, name=\'prob\')\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')\n\n        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name=\'incorrect_vector\')\n        summary.add_moving_summary(tf.reduce_mean(wrong, name=\'train_error\'))\n\n        wd_cost = tf.multiply(1e-5, regularize_cost(\'fc.*/W\', tf.nn.l2_loss),\n                              name=\'regularize_loss\')\n        summary.add_moving_summary(cost, wd_cost)\n        return tf.add_n([wd_cost, cost], name=\'cost\')\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=5e-4, trainable=False)\n        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n        return optimizer.apply_grad_processors(\n            opt, [\n                gradproc.ScaleGradient((\'STN.*\', 0.1)),\n                gradproc.SummaryGradient()])\n\n\ndef get_data(isTrain):\n    ds = dataset.Mnist(\'train\' if isTrain else \'test\')\n    # create augmentation for both training and testing\n    augs = [\n        imgaug.MapImage(lambda x: x * 255.0),\n        imgaug.RandomResize((0.7, 1.2), (0.7, 1.2)),\n        imgaug.RotationAndCropValid(45),\n        imgaug.RandomPaste((IMAGE_SIZE, IMAGE_SIZE)),\n        imgaug.SaltPepperNoise(white_prob=0.01, black_prob=0.01)\n    ]\n    ds = AugmentImageComponent(ds, augs)\n\n    ds = JoinData([ds, ds])\n    # stack the two digits into two channels, and label it with the sum\n    ds = MapData(ds, lambda dp: [np.stack([dp[0], dp[2]], axis=2), dp[1] + dp[3]])\n    ds = BatchData(ds, 128)\n    return ds\n\n\ndef view_warp(modelpath):\n    pred = OfflinePredictor(PredictConfig(\n        session_init=SmartInit(modelpath),\n        model=Model(),\n        input_names=[\'input\'],\n        output_names=[\'visualization/viz\', \'STN1/affine\', \'STN2/affine\']))\n\n    xys = np.array([[0, 0, 1],\n                    [WARP_TARGET_SIZE, 0, 1],\n                    [WARP_TARGET_SIZE, WARP_TARGET_SIZE, 1],\n                    [0, WARP_TARGET_SIZE, 1]], dtype=\'float32\')\n\n    def draw_rect(img, affine, c, offset=[0, 0]):\n        a = np.transpose(affine)  # 3x2\n        a = (np.matmul(xys, a) + offset).astype(\'int32\')\n        cv2.line(img, tuple(a[0][::-1]), tuple(a[1][::-1]), c)\n        cv2.line(img, tuple(a[1][::-1]), tuple(a[2][::-1]), c)\n        cv2.line(img, tuple(a[2][::-1]), tuple(a[3][::-1]), c)\n        cv2.line(img, tuple(a[3][::-1]), tuple(a[0][::-1]), c)\n\n    ds = get_data(False)\n    ds.reset_state()\n    for k in ds:\n        img, label = k\n        outputs, affine1, affine2 = pred(img)\n        for idx, viz in enumerate(outputs):\n            viz = cv2.cvtColor(viz, cv2.COLOR_GRAY2BGR)\n            # Here we assume the second branch focuses on the first digit\n            draw_rect(viz, affine2[idx], (0, 0, 255))\n            draw_rect(viz, affine1[idx], (0, 0, 255), offset=[IMAGE_SIZE, 0])\n            cv2.imwrite(\'{:03d}.png\'.format(idx), (viz + 0.5) * 255)\n        break\n\n\ndef get_config():\n    logger.auto_set_dir()\n\n    dataset_train, dataset_test = get_data(True), get_data(False)\n    steps_per_epoch = len(dataset_train) * 5\n\n    return TrainConfig(\n        model=Model(),\n        data=QueueInput(dataset_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                            [ScalarStats(\'cost\'), ClassificationError()]),\n            ScheduledHyperParamSetter(\'learning_rate\', [(200, 1e-4)])\n        ],\n        steps_per_epoch=steps_per_epoch,\n        max_epoch=500,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--view\', action=\'store_true\')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    if args.view:\n        view_warp(args.load)\n    else:\n        config = get_config()\n        config.session_init = SmartInit(args.load)\n        launch_train_with_config(config, SimpleTrainer())\n'"
examples/SuperResolution/GAN.py,30,"b'# -*- coding: utf-8 -*-\n# File: GAN.py\n# Author: Yuxin Wu\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorpack import BatchNorm, DataFlow, ModelDescBase, StagingInput, TowerTrainer, argscope\nfrom tensorpack.graph_builder import DataParallelBuilder, LeastLoadedDeviceSetter\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.tfutils.tower import TowerContext, TowerFunc\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.argtools import memoized_method\n\n\nclass GANModelDesc(ModelDescBase):\n    def collect_variables(self, g_scope=\'gen\', d_scope=\'discrim\'):\n        """"""\n        Assign `self.g_vars` to the parameters under scope `g_scope`,\n        and same with `self.d_vars`.\n        """"""\n        self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, g_scope)\n        assert self.g_vars\n        self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, d_scope)\n        assert self.d_vars\n\n    def build_losses(self, logits_real, logits_fake):\n        """"""\n        Build standard GAN loss and set `self.g_loss` and `self.d_loss`.\n\n        D and G play two-player minimax game with value function V(G,D)\n\n          min_G max _D V(D, G) = IE_{x ~ p_data} [log D(x)] + IE_{z ~ p_fake} [log (1 - D(G(z)))]\n\n        Args:\n            logits_real (tf.Tensor): discrim logits from real samples\n            logits_fake (tf.Tensor): discrim logits from fake samples produced by generator\n        """"""\n        with tf.name_scope(""GAN_loss""):\n            score_real = tf.sigmoid(logits_real)\n            score_fake = tf.sigmoid(logits_fake)\n            tf.summary.histogram(\'score-real\', score_real)\n            tf.summary.histogram(\'score-fake\', score_fake)\n\n            with tf.name_scope(""discrim""):\n                d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_real, labels=tf.ones_like(logits_real)), name=\'loss_real\')\n                d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_fake, labels=tf.zeros_like(logits_fake)), name=\'loss_fake\')\n\n                d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name=\'accuracy_real\')\n                d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name=\'accuracy_fake\')\n\n                d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name=\'accuracy\')\n                self.d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name=\'loss\')\n\n            with tf.name_scope(""gen""):\n                self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                    logits=logits_fake, labels=tf.ones_like(logits_fake)), name=\'loss\')\n                g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name=\'accuracy\')\n\n            add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)\n\n    def build_graph(self, *inputs):\n        """"""\n        Have to build one tower and set the following attributes:\n        g_loss, d_loss, g_vars, d_vars.\n        """"""\n        pass\n\n    @memoized_method\n    def get_optimizer(self):\n        return self.optimizer()\n\n\nclass GANTrainer(TowerTrainer):\n\n    def __init__(self, input, model, num_gpu=1):\n        """"""\n        Args:\n            input (InputSource):\n            model (GANModelDesc):\n        """"""\n        super(GANTrainer, self).__init__()\n        assert isinstance(model, GANModelDesc), model\n\n        if num_gpu > 1:\n            input = StagingInput(input)\n\n        # Setup input\n        cbs = input.setup(model.get_input_signature())\n        self.register_callback(cbs)\n\n        if num_gpu <= 1:\n            self._build_gan_trainer(input, model)\n        else:\n            self._build_multigpu_gan_trainer(input, model, num_gpu)\n\n    def _build_gan_trainer(self, input, model):\n        """"""\n        We need to set tower_func because it\'s a TowerTrainer,\n        and only TowerTrainer supports automatic graph creation for inference during training.\n\n        If we don\'t care about inference during training, using tower_func is\n        not needed. Just calling model.build_graph directly is OK.\n        """"""\n        # Build the graph\n        self.tower_func = TowerFunc(model.build_graph, model.inputs())\n        with TowerContext(\'\', is_training=True):\n            self.tower_func(*input.get_input_tensors())\n        opt = model.get_optimizer()\n\n        # Define the training iteration\n        # by default, run one d_min after one g_min\n        with tf.name_scope(\'optimize\'):\n            g_min = opt.minimize(model.g_loss, var_list=model.g_vars, name=\'g_op\')\n            with tf.control_dependencies([g_min]):\n                d_min = opt.minimize(model.d_loss, var_list=model.d_vars, name=\'d_op\')\n        self.train_op = d_min\n\n    def _build_multigpu_gan_trainer(self, input, model, num_gpu):\n        assert num_gpu > 1\n        raw_devices = [\'/gpu:{}\'.format(k) for k in range(num_gpu)]\n\n        # Build the graph with multi-gpu replication\n        def get_cost(*inputs):\n            model.build_graph(*inputs)\n            return [model.d_loss, model.g_loss]\n\n        self.tower_func = TowerFunc(get_cost, model.get_input_signature())\n        devices = [LeastLoadedDeviceSetter(d, raw_devices) for d in raw_devices]\n        cost_list = DataParallelBuilder.call_for_each_tower(\n            list(range(num_gpu)),\n            lambda: self.tower_func(*input.get_input_tensors()),\n            devices)\n        # For simplicity, average the cost here. It might be faster to average the gradients\n        with tf.name_scope(\'optimize\'):\n            d_loss = tf.add_n([x[0] for x in cost_list]) * (1.0 / num_gpu)\n            g_loss = tf.add_n([x[1] for x in cost_list]) * (1.0 / num_gpu)\n\n            opt = model.get_optimizer()\n            # run one d_min after one g_min\n            g_min = opt.minimize(g_loss, var_list=model.g_vars,\n                                 colocate_gradients_with_ops=True, name=\'g_op\')\n            with tf.control_dependencies([g_min]):\n                d_min = opt.minimize(d_loss, var_list=model.d_vars,\n                                     colocate_gradients_with_ops=True, name=\'d_op\')\n        # Define the training iteration\n        self.train_op = d_min\n\n\nclass SeparateGANTrainer(TowerTrainer):\n    """""" A GAN trainer which runs two optimization ops with a certain ratio.""""""\n    def __init__(self, input, model, d_period=1, g_period=1):\n        """"""\n        Args:\n            d_period(int): period of each d_opt run\n            g_period(int): period of each g_opt run\n        """"""\n        super(SeparateGANTrainer, self).__init__()\n        self._d_period = int(d_period)\n        self._g_period = int(g_period)\n        assert min(d_period, g_period) == 1\n\n        # Setup input\n        cbs = input.setup(model.get_input_signature())\n        self.register_callback(cbs)\n\n        # Build the graph\n        self.tower_func = TowerFunc(model.build_graph, model.inputs())\n        with TowerContext(\'\', is_training=True), \\\n                argscope(BatchNorm, ema_update=\'internal\'):\n            # should not hook the EMA updates to both train_op, it will hurt training speed.\n            self.tower_func(*input.get_input_tensors())\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        if len(update_ops):\n            logger.warn(""Found {} ops in UPDATE_OPS collection!"".format(len(update_ops)))\n            logger.warn(""Using SeparateGANTrainer with UPDATE_OPS may hurt your training speed a lot!"")\n\n        opt = model.get_optimizer()\n        with tf.name_scope(\'optimize\'):\n            self.d_min = opt.minimize(\n                model.d_loss, var_list=model.d_vars, name=\'d_min\')\n            self.g_min = opt.minimize(\n                model.g_loss, var_list=model.g_vars, name=\'g_min\')\n\n    def run_step(self):\n        # Define the training iteration\n        if self.global_step % (self._d_period) == 0:\n            self.hooked_sess.run(self.d_min)\n        if self.global_step % (self._g_period) == 0:\n            self.hooked_sess.run(self.g_min)\n\n\nclass RandomZData(DataFlow):\n    def __init__(self, shape):\n        super(RandomZData, self).__init__()\n        self.shape = shape\n\n    def __iter__(self):\n        while True:\n            yield [np.random.uniform(-1, 1, size=self.shape)]\n'"
examples/SuperResolution/data_sampler.py,0,"b'import argparse\nimport numpy as np\nimport os\nimport zipfile\nimport cv2\n\nfrom tensorpack import LMDBSerializer, MapDataComponent, RNGDataFlow\n\n\nclass ImageDataFromZIPFile(RNGDataFlow):\n    """""" Produce images read from a list of zip files. """"""\n    def __init__(self, zip_file, shuffle=False):\n        """"""\n        Args:\n            zip_file (list): list of zip file paths.\n        """"""\n        assert os.path.isfile(zip_file)\n        self._file = zip_file\n        self.shuffle = shuffle\n        self.open()\n\n    def open(self):\n        self.archivefiles = []\n        archive = zipfile.ZipFile(self._file)\n        imagesInArchive = archive.namelist()\n        for img_name in imagesInArchive:\n            if img_name.endswith(\'.jpg\'):\n                self.archivefiles.append((archive, img_name))\n\n    def reset_state(self):\n        super(ImageDataFromZIPFile, self).reset_state()\n        # Seems necessary to reopen the zip file in forked processes.\n        self.open()\n\n    def size(self):\n        return len(self.archivefiles)\n\n    def __iter__(self):\n        if self.shuffle:\n            self.rng.shuffle(self.archivefiles)\n        for archive in self.archivefiles:\n            im_data = archive[0].read(archive[1])\n            im_data = np.asarray(bytearray(im_data), dtype=\'uint8\')\n            yield [im_data]\n\n\nclass ImageEncode(MapDataComponent):\n    def __init__(self, ds, mode=\'.jpg\', dtype=np.uint8, index=0):\n        def func(img):\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            return np.asarray(bytearray(cv2.imencode(mode, img)[1].tostring()), dtype=dtype)\n        super(ImageEncode, self).__init__(ds, func, index=index)\n\n\nclass ImageDecode(MapDataComponent):\n    def __init__(self, ds, index=0):\n        def func(im_data):\n            img = cv2.imdecode(im_data, cv2.IMREAD_COLOR)\n            return img\n        super(ImageDecode, self).__init__(ds, func, index=index)\n\n\nclass RejectTooSmallImages(MapDataComponent):\n    def __init__(self, ds, thresh=384, index=0):\n        def func(img):\n            h, w, _ = img.shape\n            if (h < thresh) or (w < thresh):\n                return None\n            else:\n                return img\n        super(RejectTooSmallImages, self).__init__(ds, func, index=index)\n\n\nclass CenterSquareResize(MapDataComponent):\n    def __init__(self, ds, index=0):\n        """"""See section 5.3\n        """"""\n        def func(img):\n            try:\n                h, w, _ = img.shape\n                if h > w:\n                    off = (h - w) // 2\n                    if off > 0:\n                        img = img[off:-off, :, :]\n                if w > h:\n                    off = (w - h) // 2\n                    if off > 0:\n                        img = img[:, off:-off, :]\n\n                img = cv2.resize(img, (256, 256))\n                return img\n            except Exception:\n                return None\n        super(CenterSquareResize, self).__init__(ds, func, index=index)\n\n\n# Testcode for encode/decode.\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--create\', action=\'store_true\', help=\'create lmdb\')\n    parser.add_argument(\'--debug\', action=\'store_true\', help=\'debug images\')\n    parser.add_argument(\'--input\', type=str, help=\'path to coco zip\', required=True)\n    parser.add_argument(\'--lmdb\', type=str, help=\'path to output lmdb\', required=True)\n    args = parser.parse_args()\n\n    ds = ImageDataFromZIPFile(args.input)\n    ds = ImageDecode(ds, index=0)\n    ds = RejectTooSmallImages(ds, index=0)\n    ds = CenterSquareResize(ds, index=0)\n    if args.create:\n        ds = ImageEncode(ds, index=0)\n        LMDBSerializer.save(ds, args.lmdb)\n    if args.debug:\n        ds.reset_state()\n        for i in ds:\n            cv2.imshow(\'example\', i[0])\n            cv2.waitKey(0)\n'"
examples/SuperResolution/enet-pat.py,57,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Patrick Wieschollek <mail@patwie.com>\n\nimport argparse\nimport numpy as np\nimport os\nimport cv2\nimport six\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom data_sampler import CenterSquareResize, ImageDataFromZIPFile, ImageDecode, RejectTooSmallImages\nfrom GAN import GANModelDesc, SeparateGANTrainer\n\nReduction = tf.losses.Reduction\n\nBATCH_SIZE = 16\nCHANNELS = 3\nSHAPE_LR = 32\nNF = 64\nVGG_MEAN = np.array([123.68, 116.779, 103.939])  # RGB\nGAN_FACTOR_PARAMETER = 2.\n\n\ndef normalize(v):\n    assert isinstance(v, tf.Tensor)\n    v.get_shape().assert_has_rank(4)\n    return v / tf.reduce_mean(v, axis=[1, 2, 3], keepdims=True)\n\n\ndef gram_matrix(v):\n    assert isinstance(v, tf.Tensor)\n    v.get_shape().assert_has_rank(4)\n    dim = v.get_shape().as_list()\n    v = tf.reshape(v, [-1, dim[1] * dim[2], dim[3]])\n    return tf.matmul(v, v, transpose_a=True)\n\n\nclass Model(GANModelDesc):\n\n    def __init__(self, height=SHAPE_LR, width=SHAPE_LR):\n        super(Model, self).__init__()\n        self.height = height\n        self.width = width\n\n    def inputs(self):\n        return [tf.TensorSpec((None, self.height * 1, self.width * 1, CHANNELS), tf.float32, \'Ilr\'),\n                tf.TensorSpec((None, self.height * 4, self.width * 4, CHANNELS), tf.float32, \'Ihr\')]\n\n    def build_graph(self, Ilr, Ihr):\n        Ilr, Ihr = Ilr / 255.0, Ihr / 255.0\n        Ibicubic = tf.image.resize_bicubic(\n            Ilr, [4 * self.height, 4 * self.width], align_corners=True,\n            name=\'bicubic_baseline\')    # (0,1)\n\n        VGG_MEAN_TENSOR = tf.constant(VGG_MEAN, dtype=tf.float32)\n\n        def resnet_block(x, name):\n            with tf.variable_scope(name):\n                y = Conv2D(\'conv0\', x, NF, activation=tf.nn.relu)\n                y = Conv2D(\'conv1\', y, NF, activation=tf.identity)\n            return x + y\n\n        def upsample(x, factor=2):\n            _, h, w, _ = x.get_shape().as_list()\n            x = tf.image.resize_nearest_neighbor(x, [factor * h, factor * w], align_corners=True)\n            return x\n\n        def generator(x, Ibicubic):\n            x = x - VGG_MEAN_TENSOR / 255.0\n            with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu):\n                x = Conv2D(\'conv1\', x, NF)\n                for i in range(10):\n                    x = resnet_block(x, \'block_%i\' % i)\n                x = upsample(x)\n                x = Conv2D(\'conv_post_1\', x, NF)\n                x = upsample(x)\n                x = Conv2D(\'conv_post_2\', x, NF)\n                x = Conv2D(\'conv_post_3\', x, NF)\n                Ires = Conv2D(\'conv_post_4\', x, 3, activation=tf.identity)\n                Iest = tf.add(Ibicubic, Ires, name=\'Iest\')\n                return Iest     # [0,1]\n\n        @auto_reuse_variable_scope\n        def discriminator(x):\n            x = x - VGG_MEAN_TENSOR / 255.0\n            with argscope(Conv2D, kernel_size=3, activation=tf.nn.leaky_relu):\n                x = Conv2D(\'conv0\', x, 32)\n                x = Conv2D(\'conv0b\', x, 32, strides=2)\n                x = Conv2D(\'conv1\', x, 64)\n                x = Conv2D(\'conv1b\', x, 64, strides=2)\n                x = Conv2D(\'conv2\', x, 128)\n                x = Conv2D(\'conv2b\', x, 128, strides=2)\n                x = Conv2D(\'conv3\', x, 256)\n                x = Conv2D(\'conv3b\', x, 256, strides=2)\n                x = Conv2D(\'conv4\', x, 512)\n                x = Conv2D(\'conv4b\', x, 512, strides=2)\n\n            x = FullyConnected(\'fc0\', x, 1024, activation=tf.nn.leaky_relu)\n            x = FullyConnected(\'fc1\', x, 1, activation=tf.identity)\n            return x\n\n        def additional_losses(a, b):\n            with tf.variable_scope(\'VGG19\'):\n                x = tf.concat([a, b], axis=0)\n                x = tf.reshape(x, [2 * BATCH_SIZE, SHAPE_LR * 4, SHAPE_LR * 4, 3]) * 255.0\n                x = x - VGG_MEAN_TENSOR\n                # VGG 19\n                with varreplace.freeze_variables():\n                    with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu):\n                        conv1_1 = Conv2D(\'conv1_1\', x, 64)\n                        conv1_2 = Conv2D(\'conv1_2\', conv1_1, 64)\n                        pool1 = MaxPooling(\'pool1\', conv1_2, 2)  # 64\n                        conv2_1 = Conv2D(\'conv2_1\', pool1, 128)\n                        conv2_2 = Conv2D(\'conv2_2\', conv2_1, 128)\n                        pool2 = MaxPooling(\'pool2\', conv2_2, 2)  # 32\n                        conv3_1 = Conv2D(\'conv3_1\', pool2, 256)\n                        conv3_2 = Conv2D(\'conv3_2\', conv3_1, 256)\n                        conv3_3 = Conv2D(\'conv3_3\', conv3_2, 256)\n                        conv3_4 = Conv2D(\'conv3_4\', conv3_3, 256)\n                        pool3 = MaxPooling(\'pool3\', conv3_4, 2)  # 16\n                        conv4_1 = Conv2D(\'conv4_1\', pool3, 512)\n                        conv4_2 = Conv2D(\'conv4_2\', conv4_1, 512)\n                        conv4_3 = Conv2D(\'conv4_3\', conv4_2, 512)\n                        conv4_4 = Conv2D(\'conv4_4\', conv4_3, 512)\n                        pool4 = MaxPooling(\'pool4\', conv4_4, 2)  # 8\n                        conv5_1 = Conv2D(\'conv5_1\', pool4, 512)\n                        conv5_2 = Conv2D(\'conv5_2\', conv5_1, 512)\n                        conv5_3 = Conv2D(\'conv5_3\', conv5_2, 512)\n                        conv5_4 = Conv2D(\'conv5_4\', conv5_3, 512)\n                        pool5 = MaxPooling(\'pool5\', conv5_4, 2)  # 4\n\n            # perceptual loss\n            with tf.name_scope(\'perceptual_loss\'):\n                pool2 = normalize(pool2)\n                pool5 = normalize(pool5)\n                phi_a_1, phi_b_1 = tf.split(pool2, 2, axis=0)\n                phi_a_2, phi_b_2 = tf.split(pool5, 2, axis=0)\n\n                logger.info(\'Create perceptual loss for layer {} with shape {}\'.format(pool2.name, pool2.get_shape()))\n                pool2_loss = tf.losses.mean_squared_error(phi_a_1, phi_b_1, reduction=Reduction.MEAN)\n                logger.info(\'Create perceptual loss for layer {} with shape {}\'.format(pool5.name, pool5.get_shape()))\n                pool5_loss = tf.losses.mean_squared_error(phi_a_2, phi_b_2, reduction=Reduction.MEAN)\n\n            # texture loss\n            with tf.name_scope(\'texture_loss\'):\n                def texture_loss(x, p=16):\n                    _, h, w, c = x.get_shape().as_list()\n                    x = normalize(x)\n                    assert h % p == 0 and w % p == 0\n                    logger.info(\'Create texture loss for layer {} with shape {}\'.format(x.name, x.get_shape()))\n\n                    x = tf.space_to_batch_nd(x, [p, p], [[0, 0], [0, 0]])  # [b * ?, h/p, w/p, c]\n                    x = tf.reshape(x, [p, p, -1, h // p, w // p, c])       # [p, p, b, h/p, w/p, c]\n                    x = tf.transpose(x, [2, 3, 4, 0, 1, 5])                # [b * ?, p, p, c]\n                    patches_a, patches_b = tf.split(x, 2, axis=0)          # each is b,h/p,w/p,p,p,c\n\n                    patches_a = tf.reshape(patches_a, [-1, p, p, c])       # [b * ?, p, p, c]\n                    patches_b = tf.reshape(patches_b, [-1, p, p, c])       # [b * ?, p, p, c]\n                    return tf.losses.mean_squared_error(\n                        gram_matrix(patches_a),\n                        gram_matrix(patches_b),\n                        reduction=Reduction.MEAN\n                    )\n\n                texture_loss_conv1_1 = tf.identity(texture_loss(conv1_1), name=\'normalized_conv1_1\')\n                texture_loss_conv2_1 = tf.identity(texture_loss(conv2_1), name=\'normalized_conv2_1\')\n                texture_loss_conv3_1 = tf.identity(texture_loss(conv3_1), name=\'normalized_conv3_1\')\n\n            return [pool2_loss, pool5_loss, texture_loss_conv1_1, texture_loss_conv2_1, texture_loss_conv3_1]\n\n        with tf.variable_scope(\'gen\'):\n            fake_hr = generator(Ilr, Ibicubic)\n            real_hr = Ihr\n\n        tf.multiply(fake_hr, 255.0, name=\'prediction\')\n\n        if self.training:\n            with tf.variable_scope(\'discrim\'):\n                real_score = discriminator(real_hr)\n                fake_score = discriminator(fake_hr)\n\n            self.build_losses(real_score, fake_score)\n\n            additional_losses = additional_losses(fake_hr, real_hr)\n            with tf.name_scope(\'additional_losses\'):\n                # see table 2 from appendix\n                loss = []\n                loss.append(tf.multiply(GAN_FACTOR_PARAMETER, self.g_loss, name=""loss_LA""))\n                loss.append(tf.multiply(2e-1, additional_losses[0], name=""loss_LP1""))\n                loss.append(tf.multiply(2e-2, additional_losses[1], name=""loss_LP2""))\n                loss.append(tf.multiply(3e-7, additional_losses[2], name=""loss_LT1""))\n                loss.append(tf.multiply(1e-6, additional_losses[3], name=""loss_LT2""))\n                loss.append(tf.multiply(1e-6, additional_losses[4], name=""loss_LT3""))\n\n            self.g_loss = tf.add_n(loss, name=\'total_g_loss\')\n            self.d_loss = tf.multiply(self.d_loss, GAN_FACTOR_PARAMETER, name=\'d_loss\')\n            add_moving_summary(self.g_loss, self.d_loss, *loss)\n\n            # visualization\n            viz = (tf.concat([Ibicubic, fake_hr, real_hr], 2)) * 255.\n            viz = tf.cast(tf.clip_by_value(viz, 0, 255), tf.uint8, name=\'viz\')\n            tf.summary.image(\'input,fake,real\', viz,\n                             max_outputs=max(30, BATCH_SIZE))\n\n            self.collect_variables()\n\n    def optimizer(self):\n        lr = tf.get_variable(\n            \'learning_rate\', initializer=1e-4, trainable=False)\n        opt = tf.train.AdamOptimizer(lr)\n        return opt\n\n\ndef apply(model_path, lowres_path="""", output_path=\'.\'):\n    assert os.path.isfile(lowres_path)\n    assert os.path.isdir(output_path)\n    lr = cv2.imread(lowres_path).astype(np.float32)\n    baseline = cv2.resize(lr, (0, 0), fx=4, fy=4, interpolation=cv2.INTER_CUBIC)\n    LR_SIZE_H, LR_SIZE_W = lr.shape[:2]\n\n    predict_func = OfflinePredictor(PredictConfig(\n        model=Model(LR_SIZE_H, LR_SIZE_W),\n        session_init=SmartInit(model_path),\n        input_names=[\'Ilr\'],\n        output_names=[\'prediction\']))\n\n    pred = predict_func(lr[None, ...])\n    p = np.clip(pred[0][0, ...], 0, 255)\n\n    cv2.imwrite(os.path.join(output_path, ""predition.png""), p)\n    cv2.imwrite(os.path.join(output_path, ""baseline.png""), baseline)\n\n\ndef get_data(file_name):\n    if file_name.endswith(\'.lmdb\'):\n        ds = LMDBSerializer.load(file_name, shuffle=True)\n        ds = ImageDecode(ds, index=0)\n    elif file_name.endswith(\'.zip\'):\n        ds = ImageDataFromZIPFile(file_name, shuffle=True)\n        ds = ImageDecode(ds, index=0)\n        ds = RejectTooSmallImages(ds, index=0)\n        ds = CenterSquareResize(ds, index=0)\n    else:\n        raise ValueError(""Unknown file format "" + file_name)\n    augmentors = [imgaug.RandomCrop(128),\n                  imgaug.Flip(horiz=True)]\n    ds = AugmentImageComponent(ds, augmentors, index=0, copy=True)\n    ds = MapData(ds, lambda x: [cv2.resize(x[0], (32, 32), interpolation=cv2.INTER_CUBIC), x[0]])\n    ds = MultiProcessRunnerZMQ(ds, 3)\n    ds = BatchData(ds, BATCH_SIZE)\n    return ds\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--apply\', action=\'store_true\')\n    parser.add_argument(\'--data\', help=\'path to the dataset. \'\n                        \'Can be either a LMDB generated by `data_sampler.py` or the original COCO zip.\')\n    parser.add_argument(\'--vgg19\', help=\'load model\', default="""")\n    parser.add_argument(\'--lowres\', help=\'low resolution image as input\', default="""", type=str)\n    parser.add_argument(\'--output\', help=\'directory for saving predicted high-res image\', default=""."", type=str)\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    if args.apply:\n        apply(args.load, args.lowres, args.output)\n    else:\n        logger.auto_set_dir()\n\n        if args.load:\n            session_init = SmartInit(args.load)\n        else:\n            assert os.path.isfile(args.vgg19)\n            param_dict = dict(np.load(args.vgg19))\n            param_dict = {\'VGG19/\' + name: value for name, value in six.iteritems(param_dict)}\n            session_init = SmartInit(param_dict)\n\n        nr_tower = max(get_num_gpu(), 1)\n        data = QueueInput(get_data(args.data))\n        model = Model()\n\n        trainer = SeparateGANTrainer(data, model, d_period=3)\n\n        trainer.train_with_defaults(\n            callbacks=[\n                ModelSaver(keep_checkpoint_every_n_hours=2)\n            ],\n            session_init=session_init,\n            steps_per_epoch=data.size() // 4,\n            max_epoch=300\n        )\n'"
examples/basics/cifar-convnet.py,17,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: cifar-convnet.py\n# Author: Yuxin Wu\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.summary import *\nfrom tensorpack.utils.gpu import get_num_gpu\n\n\n""""""\nA small convnet model for Cifar10 or Cifar100 dataset.\n\nCifar10 trained on 1 GPU:\n    91% accuracy after 50k iterations.\n    79 itr/s on P100\n\nNot a good model for Cifar100, just for demonstration.\n""""""\n\n\nclass Model(ModelDesc):\n    def __init__(self, cifar_classnum):\n        super(Model, self).__init__()\n        self.cifar_classnum = cifar_classnum\n\n    def inputs(self):\n        return [tf.TensorSpec((None, 30, 30, 3), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        drop_rate = tf.constant(0.5 if self.training else 0.0)\n\n        if self.training:\n            tf.summary.image(""train_image"", image, 10)\n        if tf.test.is_gpu_available():\n            image = tf.transpose(image, [0, 3, 1, 2])\n            data_format = \'channels_first\'\n        else:\n            data_format = \'channels_last\'\n\n        image = image / 4.0     # just to make range smaller\n        with argscope(Conv2D, activation=BNReLU, use_bias=False, kernel_size=3), \\\n                argscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format):\n            logits = LinearWrap(image) \\\n                .Conv2D(\'conv1.1\', filters=64) \\\n                .Conv2D(\'conv1.2\', filters=64) \\\n                .MaxPooling(\'pool1\', 3, stride=2, padding=\'SAME\') \\\n                .Conv2D(\'conv2.1\', filters=128) \\\n                .Conv2D(\'conv2.2\', filters=128) \\\n                .MaxPooling(\'pool2\', 3, stride=2, padding=\'SAME\') \\\n                .Conv2D(\'conv3.1\', filters=128, padding=\'VALID\') \\\n                .Conv2D(\'conv3.2\', filters=128, padding=\'VALID\') \\\n                .FullyConnected(\'fc0\', 1024 + 512, activation=tf.nn.relu) \\\n                .Dropout(rate=drop_rate) \\\n                .FullyConnected(\'fc1\', 512, activation=tf.nn.relu) \\\n                .FullyConnected(\'linear\', out_dim=self.cifar_classnum)()\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')\n\n        correct = tf.cast(tf.nn.in_top_k(predictions=logits, targets=label, k=1), tf.float32, name=\'correct\')\n        # monitor training error\n        add_moving_summary(tf.reduce_mean(correct, name=\'accuracy\'))\n\n        # weight decay on all W of fc layers\n        wd_cost = regularize_cost(\'fc.*/W\', l2_regularizer(4e-4), name=\'regularize_loss\')\n        add_moving_summary(cost, wd_cost)\n\n        add_param_summary((\'.*/W\', [\'histogram\']))   # monitor W\n        return tf.add_n([cost, wd_cost], name=\'cost\')\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=1e-2, trainable=False)\n        tf.summary.scalar(\'lr\', lr)\n        return tf.train.AdamOptimizer(lr, epsilon=1e-3)\n\n\ndef get_data(train_or_test, cifar_classnum):\n    isTrain = train_or_test == \'train\'\n    if cifar_classnum == 10:\n        ds = dataset.Cifar10(train_or_test)\n    else:\n        ds = dataset.Cifar100(train_or_test)\n    if isTrain:\n        augmentors = [\n            imgaug.RandomCrop((30, 30)),\n            imgaug.Flip(horiz=True),\n            imgaug.Brightness(63),\n            imgaug.Contrast((0.2, 1.8)),\n            imgaug.MeanVarianceNormalize(all_channel=True)\n        ]\n    else:\n        augmentors = [\n            imgaug.CenterCrop((30, 30)),\n            imgaug.MeanVarianceNormalize(all_channel=True)\n        ]\n    ds = AugmentImageComponent(ds, augmentors)\n    ds = BatchData(ds, 128, remainder=not isTrain)\n    if isTrain:\n        ds = MultiProcessRunnerZMQ(ds, 5)\n    return ds\n\n\ndef get_config(cifar_classnum):\n    # prepare dataset\n    dataset_train = get_data(\'train\', cifar_classnum)\n    dataset_test = get_data(\'test\', cifar_classnum)\n\n    def lr_func(lr):\n        if lr < 3e-5:\n            raise StopTraining()\n        return lr * 0.31\n    return TrainConfig(\n        model=Model(cifar_classnum),\n        data=QueueInput(dataset_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                            ScalarStats([\'accuracy\', \'cost\'])),\n            StatMonitorParamSetter(\'learning_rate\', \'validation_accuracy\', lr_func,\n                                   threshold=0.001, last_k=10, reverse=True),\n        ],\n        max_epoch=150,\n    )\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--classnum\', help=\'10 for cifar10 or 100 for cifar100\',\n                        type=int, default=10)\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    with tf.Graph().as_default():\n        logger.set_logger_dir(os.path.join(\'train_log\', \'cifar\' + str(args.classnum)))\n        config = get_config(args.classnum)\n        config.session_init = SmartInit(args.load)\n\n        num_gpu = get_num_gpu()\n        trainer = SimpleTrainer() if num_gpu <= 1 \\\n            else SyncMultiGPUTrainerParameterServer(num_gpu)\n        launch_train_with_config(config, trainer)\n'"
examples/basics/export-model.py,24,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport argparse\nimport cv2\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.export import ModelExporter\n\n""""""\nThis example illustrates the process of exporting a model trained in Tensorpack to:\n- SavedModel format for TensorFlow Serving\n- A frozen and pruned inference graph (compact)\n\nThe model applies a laplace filter to the input image.\n\nThe steps are:\n\n1. train the model by\n\n    python export-model.py\n\n2. export the model by\n\n    python export-model.py --export serving --load train_log/export/checkpoint\n    python export-model.py --export compact --load train_log/export/checkpoint\n\n3. run inference by\n\n    python export-model.py --apply default --load train_log/export/checkpoint\n    python export-model.py --apply inference_graph --load train_log/export/checkpoint\n    python export-model.py --apply compact --load /tmp/compact_graph.pb\n""""""\n\n\nSHAPE = 256\nCHANNELS = 3\n\n\nclass Model(ModelDesc):\n    """"""Just a simple model, which applies the Laplacian-operation to images to showcase\n    the usage of variables, and alternating the inference-graph later.\n    """"""\n\n    def inputs(self):\n        return [tf.TensorSpec((None, SHAPE, SHAPE, CHANNELS), tf.uint8, \'input_img\'),\n                tf.TensorSpec((None, SHAPE, SHAPE, CHANNELS), tf.uint8, \'target_img\')]\n\n    def make_prediction(self, img):\n\n        img = tf.cast(img, tf.float32)\n        img = tf.image.rgb_to_grayscale(img)\n\n        k = tf.get_variable(\'filter\', dtype=tf.float32,\n                            initializer=[[[[0.]], [[1.]], [[0.]]], [\n                                [[1.]], [[-4.]], [[1.]]], [[[0.]], [[1.]], [[0.]]]])\n        prediction_img = tf.nn.conv2d(img, k, strides=[1, 1, 1, 1], padding=\'SAME\')\n        return prediction_img\n\n    def build_graph(self, input_img, target_img):\n\n        target_img = tf.cast(target_img, tf.float32)\n        target_img = tf.image.rgb_to_grayscale(target_img)\n\n        self.prediction_img = tf.identity(self.make_prediction(input_img), name=\'prediction_img\')\n\n        cost = tf.losses.mean_squared_error(target_img, self.prediction_img,\n                                            reduction=tf.losses.Reduction.MEAN)\n        return tf.identity(cost, name=\'total_costs\')\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.0, trainable=False)\n        return tf.train.AdamOptimizer(lr)\n\n\ndef get_data(subset):\n    ds = FakeData([[SHAPE, SHAPE, CHANNELS], [SHAPE, SHAPE, CHANNELS]], 1000, random=False,\n                  dtype=[\'uint8\', \'uint8\'], domain=[(0, 255), (0, 10)])\n    ds = BatchData(ds, 1)\n    return ds\n\n\nclass InferenceOnlyModel(Model):\n    """"""Recreate a different inference graph to accept images encoded as png. """"""\n\n    def inputs(self):\n        # The inference graph only accepts a single image, which is different to the training model.\n        return [tf.TensorSpec((None,), tf.string, \'input_img_bytes\')]\n\n    def build_graph(self, input_img_bytes):\n        # prepare input (png encoded strings to images)\n        input_img = tf.map_fn(lambda x: tf.image.decode_png(x, channels=3), input_img_bytes, dtype=tf.uint8)\n\n        # just copy the relevant parts to this graph.\n        prediction_img = self.make_prediction(input_img)\n\n        # outputs should be png encoded strings agains\n        prediction_img = tf.clip_by_value(prediction_img, 0, 255)\n        prediction_img = tf.cast(prediction_img, tf.uint8)\n        prediction_img_bytes = tf.map_fn(tf.image.encode_png, prediction_img, dtype=tf.string)\n\n        tf.identity(prediction_img_bytes, name=\'prediction_img_bytes\')\n\n\ndef export_serving(model_path):\n    """"""Export trained model to use it in TensorFlow Serving or cloudML. """"""\n    pred_config = PredictConfig(\n        session_init=SmartInit(model_path),\n        model=InferenceOnlyModel(),\n        input_names=[\'input_img_bytes\'],\n        output_names=[\'prediction_img_bytes\'])\n    ModelExporter(pred_config).export_serving(\'/tmp/exported\')\n\n\ndef export_compact(model_path):\n    """"""Export trained model to use it as a frozen and pruned inference graph in\n       mobile applications. """"""\n    pred_config = PredictConfig(\n        session_init=SmartInit(model_path),\n        model=Model(),\n        input_names=[\'input_img\'],\n        output_names=[\'prediction_img\'])\n    ModelExporter(pred_config).export_compact(\'/tmp/compact_graph.pb\')\n\n\ndef apply(model_path):\n    """"""Run inference from a training model checkpoint. """"""\n    pred_config = PredictConfig(\n        session_init=SmartInit(model_path),\n        model=Model(),\n        input_names=[\'input_img\'],\n        output_names=[\'prediction_img\'])\n\n    pred = OfflinePredictor(pred_config)\n    img = cv2.imread(\'lena.png\')\n    prediction = pred([img])[0]\n    cv2.imwrite(\'applied_default.jpg\', prediction[0])\n\n\ndef apply_inference_graph(model_path):\n    """"""Run inference from a different graph, which receives encoded images buffers. """"""\n    pred_config = PredictConfig(\n        session_init=SmartInit(model_path),\n        model=InferenceOnlyModel(),\n        input_names=[\'input_img_bytes\'],\n        output_names=[\'prediction_img_bytes\'])\n\n    pred = OfflinePredictor(pred_config)\n    buf = open(\'lena.png\', \'rb\').read()\n    prediction = pred([buf])[0]\n    with open(\'applied_inference_graph.png\', \'wb\') as f:\n        f.write(prediction[0])\n\n\ndef apply_compact(graph_path):\n    """"""Run the pruned and frozen inference graph. """"""\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        # Note, we just load the graph and do *not* need to initialize anything.\n        with tf.gfile.GFile(graph_path, ""rb"") as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def)\n\n        input_img = sess.graph.get_tensor_by_name(\'import/input_img:0\')\n        prediction_img = sess.graph.get_tensor_by_name(\'import/prediction_img:0\')\n\n        prediction = sess.run(prediction_img, {input_img: cv2.imread(\'lena.png\')[None, ...]})\n        cv2.imwrite(\'applied_compact.png\', prediction[0])\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--apply\', help=\'run sampling\', default=\'\',\n                        choices=[\'default\', \'inference_graph\', \'compact\'])\n    parser.add_argument(\'--export\', help=\'export the model\', default=\'\',\n                        choices=[\'serving\', \'compact\'])\n\n    args = parser.parse_args()\n\n    if args.apply != \'\':\n        if args.apply == \'default\':\n            apply(args.load)\n        elif args.apply == \'inference_graph\':\n            apply_inference_graph(args.load)\n        else:\n            apply_compact(args.load)\n    elif args.export != \'\':\n        if args.export == \'serving\':\n            export_serving(args.load)\n        else:\n            export_compact(args.load)\n    else:\n        logger.auto_set_dir()\n\n        ds_train = get_data(\'train\')\n\n        config = TrainConfig(\n            model=Model(),\n            data=QueueInput(ds_train),\n            callbacks=[\n                ModelSaver(),\n            ],\n            steps_per_epoch=1,\n            max_epoch=1,\n        )\n        launch_train_with_config(config, SimpleTrainer())\n'"
examples/basics/mnist-convnet.py,18,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-convnet.py\n\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils import summary\n\n""""""\nMNIST ConvNet example.\nabout 0.6% validation error after 30 epochs.\n""""""\n\nIMAGE_SIZE = 28\n\n\nclass Model(ModelDesc):\n    # See tutorial at https://tensorpack.readthedocs.io/tutorial/training-interface.html#with-modeldesc-and-trainconfig\n    def inputs(self):\n        """"""\n        Define all the inputs (with type, shape, name) that the graph will need.\n        """"""\n        return [tf.TensorSpec((None, IMAGE_SIZE, IMAGE_SIZE), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        """"""This function should build the model which takes the input variables (defined above)\n        and return cost at the end.""""""\n\n        # In tensorflow, inputs to convolution function are assumed to be\n        # NHWC. Add a single channel here.\n        image = tf.expand_dims(image, 3)\n\n        image = image * 2 - 1   # center the pixels values at zero\n        # The context manager `argscope` sets the default option for all the layers under\n        # this context. Here we use 32 channel convolution with shape 3x3\n        # See tutorial at https://tensorpack.readthedocs.io/tutorial/symbolic.html\n        with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu, filters=32):\n            # LinearWrap is just a syntax sugar.\n            # See tutorial at https://tensorpack.readthedocs.io/tutorial/symbolic.html\n            logits = (LinearWrap(image)\n                      .Conv2D(\'conv0\')\n                      .MaxPooling(\'pool0\', 2)\n                      .Conv2D(\'conv1\')\n                      .Conv2D(\'conv2\')\n                      .MaxPooling(\'pool1\', 2)\n                      .Conv2D(\'conv3\')\n                      .FullyConnected(\'fc0\', 512, activation=tf.nn.relu)\n                      .Dropout(\'dropout\', rate=0.5)\n                      .FullyConnected(\'fc1\', 10, activation=tf.identity)())\n\n        # a vector of length B with loss of each sample\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')  # the average cross-entropy loss\n\n        correct = tf.cast(tf.nn.in_top_k(predictions=logits, targets=label, k=1), tf.float32, name=\'correct\')\n        accuracy = tf.reduce_mean(correct, name=\'accuracy\')\n\n        # This will monitor training error & accuracy (in a moving average fashion). The value will be automatically\n        # 1. written to tensosrboard\n        # 2. written to stat.json\n        # 3. printed after each epoch\n        # You can also just call `tf.summary.scalar`. But moving summary has some other benefits.\n        # See tutorial at https://tensorpack.readthedocs.io/tutorial/summary.html\n        train_error = tf.reduce_mean(1 - correct, name=\'train_error\')\n        summary.add_moving_summary(train_error, accuracy)\n\n        # Use a regex to find parameters to apply weight decay.\n        # Here we apply a weight decay on all W (weight matrix) of all fc layers\n        # If you don\'t like regex, you can certainly define the cost in any other methods.\n        wd_cost = tf.multiply(1e-5,\n                              regularize_cost(\'fc.*/W\', tf.nn.l2_loss),\n                              name=\'regularize_loss\')\n        total_cost = tf.add_n([wd_cost, cost], name=\'total_cost\')\n        summary.add_moving_summary(cost, wd_cost, total_cost)\n\n        # monitor histogram of all weight (of conv and fc layers) in tensorboard\n        summary.add_param_summary((\'.*/W\', [\'histogram\', \'rms\']))\n        # the function should return the total cost to be optimized\n        return total_cost\n\n    def optimizer(self):\n        lr = tf.train.exponential_decay(\n            learning_rate=1e-3,\n            global_step=get_global_step_var(),\n            decay_steps=468 * 10,\n            decay_rate=0.3, staircase=True, name=\'learning_rate\')\n        # This will also put the summary in tensorboard, stat.json and print in terminal,\n        # but this time without moving average\n        tf.summary.scalar(\'lr\', lr)\n        return tf.train.AdamOptimizer(lr)\n\n\ndef get_data():\n    # We don\'t need any fancy data loading for this simple example.\n    # See dataflow tutorial at https://tensorpack.readthedocs.io/tutorial/dataflow.html\n    train = BatchData(dataset.Mnist(\'train\'), 128)\n    test = BatchData(dataset.Mnist(\'test\'), 256, remainder=True)\n\n    train = PrintData(train)\n\n    return train, test\n\n\nif __name__ == \'__main__\':\n    # automatically setup the directory train_log/mnist-convnet for logging\n    logger.auto_set_dir()\n\n    dataset_train, dataset_test = get_data()\n\n    # How many iterations you want in each epoch.\n    # This len(data) is the default value.\n    steps_per_epoch = len(dataset_train)\n\n    # get the config which contains everything necessary in a training\n    config = TrainConfig(\n        model=Model(),\n        # The input source for training. FeedInput is slow, this is just for demo purpose.\n        # In practice it\'s best to use QueueInput or others.\n        # See tutorial at https://tensorpack.readthedocs.io/tutorial/extend/input-source.html\n        data=FeedInput(dataset_train),\n        # We use a few simple callbacks in this demo.\n        # See tutorial at https://tensorpack.readthedocs.io/tutorial/callback.html\n        callbacks=[\n            ModelSaver(),   # save the model after every epoch\n            InferenceRunner(    # run inference(for validation) after every epoch\n                dataset_test,   # the DataFlow instance used for validation\n                ScalarStats(    # produce `val_accuracy` and `val_cross_entropy_loss`\n                    [\'cross_entropy_loss\', \'accuracy\'], prefix=\'val\')),\n            # MaxSaver needs to come after InferenceRunner to obtain its score\n            MaxSaver(\'val_accuracy\'),  # save the model with highest accuracy\n        ],\n        steps_per_epoch=steps_per_epoch,\n        max_epoch=100,\n    )\n    # Use a simple trainer in this demo.\n    # More trainers with multi-gpu or distributed functionalities are available.\n    # See tutorial at https://tensorpack.readthedocs.io/tutorial/trainer.html\n    launch_train_with_config(config, SimpleTrainer())\n'"
examples/basics/mnist-tflayers.py,30,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-tflayers.py\n\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils import summary\n\n""""""\nMNIST ConvNet example using tf.layers\nMostly the same as \'mnist-convnet.py\',\nthe only differences are:\n    1. use tf.layers\n    2. use tf.layers variable names to summarize weights\n""""""\n\nIMAGE_SIZE = 28\n# Monkey-patch tf.layers to support argscope.\nenable_argscope_for_module(tf.layers)\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        """"""\n        Define all the inputs (with type, shape, name) that the graph will need.\n        """"""\n        return [tf.TensorSpec((None, IMAGE_SIZE, IMAGE_SIZE), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        """"""This function should build the model which takes the input variables\n        and return cost at the end""""""\n\n        # In tensorflow, inputs to convolution function are assumed to be\n        # NHWC. Add a single channel here.\n        image = tf.expand_dims(image, 3)\n\n        image = image * 2 - 1   # center the pixels values at zero\n\n        # The context manager `argscope` sets the default option for all the layers under\n        # this context. Here we use 32 channel convolution with shape 3x3\n        with argscope([tf.layers.conv2d], padding=\'same\', activation=tf.nn.relu):\n            l = tf.layers.conv2d(image, 32, 3, name=\'conv0\')\n            l = tf.layers.max_pooling2d(l, 2, 2, padding=\'valid\')\n            l = tf.layers.conv2d(l, 32, 3, name=\'conv1\')\n            l = tf.layers.conv2d(l, 32, 3, name=\'conv2\')\n            l = tf.layers.max_pooling2d(l, 2, 2, padding=\'valid\')\n            l = tf.layers.conv2d(l, 32, 3, name=\'conv3\')\n            l = tf.layers.flatten(l)\n            l = tf.layers.dense(l, 512, activation=tf.nn.relu, name=\'fc0\')\n            l = tf.layers.dropout(l, rate=0.5, training=self.training)\n        logits = tf.layers.dense(l, 10, activation=tf.identity, name=\'fc1\')\n\n        # a vector of length B with loss of each sample\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')  # the average cross-entropy loss\n\n        correct = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32, name=\'correct\')\n        accuracy = tf.reduce_mean(correct, name=\'accuracy\')\n\n        # This will monitor training error & accuracy (in a moving average fashion). The value will be automatically\n        # 1. written to tensosrboard\n        # 2. written to stat.json\n        # 3. printed after each epoch\n        train_error = tf.reduce_mean(1 - correct, name=\'train_error\')\n        summary.add_moving_summary(train_error, accuracy)\n\n        # Use a regex to find parameters to apply weight decay.\n        # Here we apply a weight decay on all W (weight matrix) of all fc layers\n        # If you don\'t like regex, you can certainly define the cost in any other methods.\n        wd_cost = tf.multiply(1e-5,\n                              regularize_cost(\'fc.*/kernel\', tf.nn.l2_loss),\n                              name=\'regularize_loss\')\n        total_cost = tf.add_n([wd_cost, cost], name=\'total_cost\')\n        summary.add_moving_summary(cost, wd_cost, total_cost)\n\n        # monitor histogram of all weight (of conv and fc layers) in tensorboard\n        summary.add_param_summary((\'.*/kernel\', [\'histogram\', \'rms\']))\n        # the function should return the total cost to be optimized\n        return total_cost\n\n    def optimizer(self):\n        lr = tf.train.exponential_decay(\n            learning_rate=1e-3,\n            global_step=get_global_step_var(),\n            decay_steps=468 * 10,\n            decay_rate=0.3, staircase=True, name=\'learning_rate\')\n        # This will also put the summary in tensorboard, stat.json and print in terminal\n        # but this time without moving average\n        tf.summary.scalar(\'lr\', lr)\n        return tf.train.AdamOptimizer(lr)\n\n\ndef get_data():\n    train = BatchData(dataset.Mnist(\'train\'), 128)\n    test = BatchData(dataset.Mnist(\'test\'), 256, remainder=True)\n    return train, test\n\n\nif __name__ == \'__main__\':\n    # automatically setup the directory train_log/mnist-convnet for logging\n    logger.auto_set_dir()\n\n    dataset_train, dataset_test = get_data()\n\n    # How many iterations you want in each epoch.\n    # This len(data) is the default value.\n    steps_per_epoch = len(dataset_train)\n\n    # get the config which contains everything necessary in a training\n    config = TrainConfig(\n        model=Model(),\n        # The input source for training. FeedInput is slow, this is just for demo purpose.\n        # In practice it\'s best to use QueueInput or others. See tutorials for details.\n        data=FeedInput(dataset_train),\n        callbacks=[\n            ModelSaver(),   # save the model after every epoch\n            InferenceRunner(    # run inference(for validation) after every epoch\n                dataset_test,   # the DataFlow instance used for validation\n                ScalarStats([\'cross_entropy_loss\', \'accuracy\'])),\n            MaxSaver(\'validation_accuracy\'),  # save the model with highest accuracy (prefix \'validation_\')\n        ],\n        steps_per_epoch=steps_per_epoch,\n        max_epoch=100,\n    )\n    launch_train_with_config(config, SimpleTrainer())\n'"
examples/basics/mnist-tfslim.py,10,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-tfslim.py\n\n""""""\nMNIST ConvNet example using TensorFlow-slim.\nMostly the same as \'mnist-convnet.py\',\nthe only differences are:\n    1. use slim.layers, slim.arg_scope, etc\n    2. use slim names to summarize weights\n""""""\n\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\n\nIMAGE_SIZE = 28\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, IMAGE_SIZE, IMAGE_SIZE), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = tf.expand_dims(image, 3)\n\n        image = image * 2 - 1\n\n        with slim.arg_scope([slim.layers.fully_connected],\n                            weights_regularizer=slim.l2_regularizer(1e-5)):\n            l = slim.layers.conv2d(image, 32, [3, 3], scope=\'conv0\')\n            l = slim.layers.max_pool2d(l, [2, 2], scope=\'pool0\')\n            l = slim.layers.conv2d(l, 32, [3, 3], padding=\'SAME\', scope=\'conv1\')\n            l = slim.layers.conv2d(l, 32, [3, 3], scope=\'conv2\')\n            l = slim.layers.max_pool2d(l, [2, 2], scope=\'pool1\')\n            l = slim.layers.conv2d(l, 32, [3, 3], scope=\'conv3\')\n            l = slim.layers.flatten(l, scope=\'flatten\')\n            l = slim.layers.fully_connected(l, 512, scope=\'fc0\')\n            l = slim.layers.dropout(l, is_training=self.training)\n            logits = slim.layers.fully_connected(l, 10, activation_fn=None, scope=\'fc1\')\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')\n\n        acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)\n\n        acc = tf.reduce_mean(acc, name=\'accuracy\')\n        summary.add_moving_summary(acc)\n\n        summary.add_moving_summary(cost)\n        summary.add_param_summary((\'.*/weights\', [\'histogram\', \'rms\']))  # slim uses different variable names\n        return cost + regularize_cost_from_collection()\n\n    def optimizer(self):\n        lr = tf.train.exponential_decay(\n            learning_rate=1e-3,\n            global_step=get_global_step_var(),\n            decay_steps=468 * 10,\n            decay_rate=0.3, staircase=True, name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', lr)\n        return tf.train.AdamOptimizer(lr)\n\n\ndef get_data():\n    train = BatchData(dataset.Mnist(\'train\'), 128)\n    test = BatchData(dataset.Mnist(\'test\'), 256, remainder=True)\n    return train, test\n\n\nif __name__ == \'__main__\':\n    logger.auto_set_dir()\n    dataset_train, dataset_test = get_data()\n\n    config = TrainConfig(\n        model=Model(),\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(\n                dataset_test,\n                ScalarStats([\'cross_entropy_loss\', \'accuracy\'])),\n        ],\n        max_epoch=100,\n    )\n    launch_train_with_config(config, SimpleTrainer())\n'"
examples/basics/mnist-visualizations.py,31,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-visualizations.py\n\n""""""\nThe same MNIST ConvNet example, but with weights/activations visualization.\n""""""\n\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\n\nIMAGE_SIZE = 28\n\n\ndef visualize_conv_weights(filters, name):\n    """"""Visualize use weights in convolution filters.\n\n    Args:\n        filters: tensor containing the weights [H,W,Cin,Cout]\n        name: label for tensorboard\n\n    Returns:\n        image of all weight\n    """"""\n    with tf.name_scope(\'visualize_w_\' + name):\n        filters = tf.transpose(filters, (3, 2, 0, 1))   # [h, w, cin, cout] -> [cout, cin, h, w]\n        filters = tf.unstack(filters)                   # --> cout * [cin, h, w]\n        filters = tf.concat(filters, 1)                 # --> [cin, cout * h, w]\n        filters = tf.unstack(filters)                   # --> cin * [cout * h, w]\n        filters = tf.concat(filters, 1)                 # --> [cout * h, cin * w]\n        filters = tf.expand_dims(filters, 0)\n        filters = tf.expand_dims(filters, -1)\n\n    tf.summary.image(\'visualize_w_\' + name, filters)\n\n\ndef visualize_conv_activations(activation, name):\n    """"""Visualize activations for convolution layers.\n\n    Remarks:\n        This tries to place all activations into a square.\n\n    Args:\n        activation: tensor with the activation [B,H,W,C]\n        name: label for tensorboard\n\n    Returns:\n        image of almost all activations\n    """"""\n    import math\n    with tf.name_scope(\'visualize_act_\' + name):\n        _, h, w, c = activation.get_shape().as_list()\n        rows = []\n        c_per_row = int(math.sqrt(c))\n        for y in range(0, c - c_per_row, c_per_row):\n            row = activation[:, :, :, y:y + c_per_row]  # [?, H, W, 32] --> [?, H, W, 5]\n            cols = tf.unstack(row, axis=3)              # [?, H, W, 5] --> 5 * [?, H, W]\n            row = tf.concat(cols, 1)\n            rows.append(row)\n\n        viz = tf.concat(rows, 2)\n    tf.summary.image(\'visualize_act_\' + name, tf.expand_dims(viz, -1))\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, IMAGE_SIZE, IMAGE_SIZE), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = tf.expand_dims(image * 2 - 1, 3)\n\n        with argscope(Conv2D, kernel_shape=3, nl=tf.nn.relu, out_channel=32):\n            c0 = Conv2D(\'conv0\', image)\n            p0 = MaxPooling(\'pool0\', c0, 2)\n            c1 = Conv2D(\'conv1\', p0)\n            c2 = Conv2D(\'conv2\', c1)\n            p1 = MaxPooling(\'pool1\', c2, 2)\n            c3 = Conv2D(\'conv3\', p1)\n            fc1 = FullyConnected(\'fc0\', c3, 512, nl=tf.nn.relu)\n            fc1 = Dropout(\'dropout\', fc1, 0.5)\n            logits = FullyConnected(\'fc1\', fc1, out_dim=10, nl=tf.identity)\n\n        with tf.name_scope(\'visualizations\'):\n            visualize_conv_weights(c0.variables.W, \'conv0\')\n            visualize_conv_activations(c0, \'conv0\')\n            visualize_conv_weights(c1.variables.W, \'conv1\')\n            visualize_conv_activations(c1, \'conv1\')\n            visualize_conv_weights(c2.variables.W, \'conv2\')\n            visualize_conv_activations(c2, \'conv2\')\n            visualize_conv_weights(c3.variables.W, \'conv3\')\n            visualize_conv_activations(c3, \'conv3\')\n\n            tf.summary.image(\'input\', (image + 1.0) * 128., 3)\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')\n\n        tf.reduce_mean(tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32), name=\'accuracy\')\n\n        wd_cost = tf.multiply(1e-5,\n                              regularize_cost(\'fc.*/W\', tf.nn.l2_loss),\n                              name=\'regularize_loss\')\n        return tf.add_n([wd_cost, cost], name=\'total_cost\')\n\n    def optimizer(self):\n        lr = tf.train.exponential_decay(\n            learning_rate=1e-3,\n            global_step=get_global_step_var(),\n            decay_steps=468 * 10,\n            decay_rate=0.3, staircase=True, name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', lr)\n        return tf.train.AdamOptimizer(lr)\n\n\ndef get_data():\n    train = BatchData(dataset.Mnist(\'train\'), 128)\n    test = BatchData(dataset.Mnist(\'test\'), 256, remainder=True)\n    return train, test\n\n\nif __name__ == \'__main__\':\n    logger.auto_set_dir()\n    dataset_train, dataset_test = get_data()\n\n    config = TrainConfig(\n        model=Model(),\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(\n                dataset_test, ScalarStats([\'cross_entropy_loss\', \'accuracy\'])),\n        ],\n        steps_per_epoch=len(dataset_train),\n        max_epoch=100,\n    )\n\n    launch_train_with_config(config, SimpleTrainer())\n'"
examples/basics/svhn-digit-convnet.py,13,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: svhn-digit-convnet.py\n# Author: Yuxin Wu\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.summary import *\n\n\n""""""\nA very small SVHN convnet model (only 0.8m parameters).\nAbout 2.3% validation error after 70 epochs. 2.15% after 150 epochs.\n\nEach epoch iterates over the whole training set (4721 iterations), and takes about 24s on a P100.\n""""""\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec([None, 40, 40, 3], tf.float32, \'input\'),\n                tf.TensorSpec([None], tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = image / 128.0 - 1\n\n        with argscope(Conv2D, activation=BNReLU, use_bias=False):\n            logits = (LinearWrap(image)\n                      .Conv2D(\'conv1\', 24, 5, padding=\'VALID\')\n                      .MaxPooling(\'pool1\', 2, padding=\'SAME\')\n                      .Conv2D(\'conv2\', 32, 3, padding=\'VALID\')\n                      .Conv2D(\'conv3\', 32, 3, padding=\'VALID\')\n                      .MaxPooling(\'pool2\', 2, padding=\'SAME\')\n                      .Conv2D(\'conv4\', 64, 3, padding=\'VALID\')\n                      .Dropout(\'drop\', rate=0.5)\n                      .FullyConnected(\'fc0\', 512,\n                                      bias_initializer=tf.constant_initializer(0.1),\n                                      activation=tf.nn.relu)\n                      .FullyConnected(\'linear\', units=10)())\n        tf.nn.softmax(logits, name=\'output\')\n\n        accuracy = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)\n        add_moving_summary(tf.reduce_mean(accuracy, name=\'accuracy\'))\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')\n\n        wd_cost = regularize_cost(\'fc.*/W\', l2_regularizer(0.00001))\n        add_moving_summary(cost, wd_cost)\n\n        add_param_summary((\'.*/W\', [\'histogram\', \'rms\']))   # monitor W\n        return tf.add_n([cost, wd_cost], name=\'cost\')\n\n    def optimizer(self):\n        lr = tf.train.exponential_decay(\n            learning_rate=1e-3,\n            global_step=get_global_step_var(),\n            decay_steps=4721 * 60,\n            decay_rate=0.2, staircase=True, name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', lr)\n        return tf.train.AdamOptimizer(lr)\n\n\ndef get_data():\n    d1 = dataset.SVHNDigit(\'train\')\n    d2 = dataset.SVHNDigit(\'extra\')\n    data_train = RandomMixData([d1, d2])\n    data_test = dataset.SVHNDigit(\'test\', shuffle=False)\n\n    augmentors = [\n        imgaug.Resize((40, 40)),\n        imgaug.Brightness(30),\n        imgaug.Contrast((0.5, 1.5)),\n    ]\n    data_train = AugmentImageComponent(data_train, augmentors)\n    data_train = BatchData(data_train, 128)\n    data_train = MultiProcessRunner(data_train, 5, 5)\n\n    augmentors = [imgaug.Resize((40, 40))]\n    data_test = AugmentImageComponent(data_test, augmentors)\n    data_test = BatchData(data_test, 128, remainder=True)\n    return data_train, data_test\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\n    parser.add_argument(\'--load\', help=\'load model\')\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    logger.auto_set_dir()\n    data_train, data_test = get_data()\n\n    config = TrainConfig(\n        model=Model(),\n        data=QueueInput(data_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(data_test,\n                            ScalarStats([\'cost\', \'accuracy\']))\n        ],\n        max_epoch=350,\n        session_init=SmartInit(args.load)\n    )\n    launch_train_with_config(config, SimpleTrainer())\n'"
examples/keras/imagenet-resnet-keras.py,16,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: imagenet-resnet-keras.py\n# Author: Yuxin Wu\n\nimport argparse\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.python.keras.layers import *\n\nfrom tensorpack import SyncMultiGPUTrainerReplicated\nfrom tensorpack.callbacks import *\nfrom tensorpack.contrib.keras import KerasModel\nfrom tensorpack.dataflow import FakeData, MapDataComponent\nfrom tensorpack.tfutils.common import get_tf_version_tuple\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.gpu import get_num_gpu\n\nfrom imagenet_utils import fbresnet_augmentor, get_imagenet_dataflow\n\nTOTAL_BATCH_SIZE = 512\nBASE_LR = 0.1 * (TOTAL_BATCH_SIZE // 256)\n\n\ndef bn(x, name, zero_init=False):\n    return BatchNormalization(\n        axis=1, name=name, fused=True,\n        momentum=0.9, epsilon=1e-5,\n        gamma_initializer=\'zeros\' if zero_init else \'ones\')(x)\n\n\ndef conv(x, filters, kernel, strides=1, name=None):\n    return Conv2D(filters, kernel, name=name,\n                  strides=strides, use_bias=False, padding=\'same\',\n                  kernel_initializer=tf.keras.initializers.VarianceScaling(\n                      scale=2.0, mode=\'fan_out\',\n                      distribution=\'untruncated_normal\' if get_tf_version_tuple() >= (1, 12) else \'normal\'),\n                  kernel_regularizer=tf.keras.regularizers.l2(5e-5))(x)\n\n\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\n    filters1, filters2, filters3 = filters\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n    x = conv(input_tensor, filters1, 1, name=conv_name_base + \'2a\')\n    x = bn(x, name=bn_name_base + \'2a\')\n    x = Activation(\'relu\')(x)\n\n    x = conv(x, filters2, kernel_size, name=conv_name_base + \'2b\')\n    x = bn(x, name=bn_name_base + \'2b\')\n    x = Activation(\'relu\')(x)\n\n    x = conv(x, filters3, (1, 1), name=conv_name_base + \'2c\')\n    x = bn(x, name=bn_name_base + \'2c\', zero_init=True)\n\n    x = tf.keras.layers.add([x, input_tensor])\n    x = Activation(\'relu\')(x)\n    return x\n\n\ndef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n    filters1, filters2, filters3 = filters\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n    x = conv(input_tensor, filters1, (1, 1), name=conv_name_base + \'2a\')\n    x = bn(x, name=bn_name_base + \'2a\')\n    x = Activation(\'relu\')(x)\n\n    x = conv(x, filters2, kernel_size, strides=strides, name=conv_name_base + \'2b\')\n    x = bn(x, name=bn_name_base + \'2b\')\n    x = Activation(\'relu\')(x)\n\n    x = conv(x, filters3, (1, 1), name=conv_name_base + \'2c\')\n    x = bn(x, name=bn_name_base + \'2c\', zero_init=True)\n\n    shortcut = conv(\n        input_tensor,\n        filters3, (1, 1), strides=strides,\n        name=conv_name_base + \'1\')\n    shortcut = bn(shortcut, name=bn_name_base + \'1\')\n\n    x = tf.keras.layers.add([x, shortcut])\n    x = Activation(\'relu\')(x)\n    return x\n\n\ndef resnet50(image):\n    input = Input(tensor=image)\n\n    def image_preprocess(image):\n        image = tf.cast(image, tf.float32)\n        image = image * (1.0 / 255)\n        mean = [0.485, 0.456, 0.406][::-1]\n        std = [0.229, 0.224, 0.225][::-1]\n        image = (image - tf.constant(mean, dtype=tf.float32)) / tf.constant(std, dtype=tf.float32)\n        image = tf.transpose(image, [0, 3, 1, 2])\n        return image\n\n    x = Lambda(image_preprocess)(input)\n\n    x = conv(x, 64, (7, 7), strides=(2, 2), name=\'conv0\')\n    x = bn(x, name=\'bn_conv1\')\n    x = Activation(\'relu\')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block=\'a\', strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'b\')\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'c\')\n\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block=\'a\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'b\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'c\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'d\')\n\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block=\'a\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'b\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'c\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'d\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'e\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'f\')\n\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block=\'a\')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'b\')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'c\')\n\n    x = GlobalAveragePooling2D(name=\'avg_pool\')(x)\n    x = Flatten()(x)\n    x = Dense(1000, activation=\'softmax\', name=\'fc1000\',\n              kernel_initializer=tf.keras.initializers.VarianceScaling(\n                  scale=2.0, mode=\'fan_in\'),\n              kernel_regularizer=tf.keras.regularizers.l2(5e-5))(x)\n\n    M = tf.keras.models.Model(input, x, name=\'resnet50\')\n    return M\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data\', help=\'ILSVRC dataset dir\')\n    parser.add_argument(\'--fake\', help=\'use fakedata to test or benchmark this model\', action=\'store_true\')\n    args = parser.parse_args()\n    logger.set_logger_dir(os.path.join(""train_log"", ""imagenet-resnet-keras""))\n\n    tf.keras.backend.set_image_data_format(\'channels_first\')\n\n    num_gpu = get_num_gpu()\n    if args.fake:\n        df_train = FakeData([[32, 224, 224, 3], [32, 1000]], 5000, random=False, dtype=\'uint8\')\n        df_val = FakeData([[32, 224, 224, 3], [32, 1000]], 5000, random=False)\n    else:\n        batch_size = TOTAL_BATCH_SIZE // num_gpu\n        assert args.data is not None\n        df_train = get_imagenet_dataflow(\n            args.data, \'train\', batch_size, fbresnet_augmentor(True))\n        df_val = get_imagenet_dataflow(\n            args.data, \'val\', batch_size, fbresnet_augmentor(False))\n\n        def one_hot(label):\n            return np.eye(1000)[label]\n\n        df_train = MapDataComponent(df_train, one_hot, 1)\n        df_val = MapDataComponent(df_val, one_hot, 1)\n\n    M = KerasModel(\n        resnet50,\n        input_signature=[tf.TensorSpec([None, 224, 224, 3], tf.uint8, \'images\')],\n        target_signature=[tf.TensorSpec([None, 1000], tf.float32, \'labels\')],\n        input=df_train,\n        trainer=SyncMultiGPUTrainerReplicated(num_gpu))\n\n    lr = tf.get_variable(\'learning_rate\', initializer=0.1, trainable=False)\n    tf.summary.scalar(\'lr\', lr)\n\n    M.compile(\n        optimizer=tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True),\n        loss=\'categorical_crossentropy\',\n        metrics=\'categorical_accuracy\'\n    )\n\n    callbacks = [\n        ModelSaver(),\n        ScheduledHyperParamSetter(\n            \'learning_rate\',\n            [(0, 0.1), (3, BASE_LR)], interp=\'linear\'),  # warmup\n        ScheduledHyperParamSetter(\n            \'learning_rate\',\n            [(30, BASE_LR * 0.1), (60, BASE_LR * 1e-2), (85, BASE_LR * 1e-3)]),\n        GPUUtilizationTracker()\n    ]\n    if not args.fake:\n        callbacks.append(\n            DataParallelInferenceRunner(\n                df_val, ScalarStats([\'categorical_accuracy\']), num_gpu))\n\n    M.fit(\n        steps_per_epoch=100 if args.fake else 1281167 // TOTAL_BATCH_SIZE,\n        max_epoch=100,\n        callbacks=callbacks\n    )\n'"
examples/keras/mnist-keras-v2.py,3,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-keras-v2.py\n# Author: Yuxin Wu\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorpack import QueueInput\nfrom tensorpack.callbacks import ModelSaver\nfrom tensorpack.contrib.keras import KerasModel\nfrom tensorpack.dataflow import BatchData, MapData, dataset\nfrom tensorpack.utils import logger\n\nKL = keras.layers\nIMAGE_SIZE = 28\n\n\ndef get_data():\n    def f(dp):\n        im = dp[0][:, :, None]\n        onehot = np.eye(10)[dp[1]]\n        return [im, onehot]\n\n    train = BatchData(MapData(dataset.Mnist(\'train\'), f), 128)\n    test = BatchData(MapData(dataset.Mnist(\'test\'), f), 256)\n    return train, test\n\n\nif __name__ == \'__main__\':\n    logger.auto_set_dir(\'d\')\n\n    def model_func(image):\n        """"""\n        Keras model has to be created inside this function to be used with tensorpack.\n        """"""\n        M = keras.models.Sequential()\n        # input_tensor have to be used here for tensorpack trainer to function properly.\n        # Just use inputs[1], inputs[2] if you have multiple inputs.\n        M.add(KL.InputLayer(input_tensor=image))\n        M.add(KL.Conv2D(32, 3, activation=\'relu\', padding=\'same\'))\n        M.add(KL.MaxPooling2D())\n        M.add(KL.Conv2D(32, 3, activation=\'relu\', padding=\'same\'))\n        M.add(KL.Conv2D(32, 3, activation=\'relu\', padding=\'same\'))\n        M.add(KL.MaxPooling2D())\n        M.add(KL.Conv2D(32, 3, padding=\'same\', activation=\'relu\'))\n\n        M.add(KL.Flatten())\n        M.add(KL.Dense(512, activation=\'relu\', kernel_regularizer=keras.regularizers.l2(1e-5)))\n        M.add(KL.Dropout(0.5))\n        M.add(KL.Dense(10, activation=None, kernel_regularizer=keras.regularizers.l2(1e-5)))\n        M.add(KL.Activation(\'softmax\'))\n        return M\n\n    dataset_train, dataset_test = get_data()\n\n    M = KerasModel(\n        model_func,\n        input_signature=[tf.TensorSpec([None, IMAGE_SIZE, IMAGE_SIZE, 1], tf.float32, \'images\')],\n        target_signature=[tf.TensorSpec([None, 10], tf.float32, \'labels\')],\n        input=QueueInput(dataset_train))\n    M.compile(\n        optimizer=tf.train.AdamOptimizer(1e-3),\n        loss=\'categorical_crossentropy\',\n        metrics=\'categorical_accuracy\'\n    )\n    M.fit(\n        validation_data=dataset_test,\n        steps_per_epoch=len(dataset_train),\n        callbacks=[\n            ModelSaver()\n        ]\n    )\n'"
examples/keras/mnist-keras.py,15,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist-keras.py\n# Author: Yuxin Wu\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom contextlib import contextmanager\n\nfrom tensorpack import *\nfrom tensorpack.contrib.keras import KerasPhaseCallback\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.utils.argtools import memoized\nfrom tensorpack.utils.gpu import get_num_gpu\nfrom tensorpack.tfutils.tower import get_current_tower_context\n\nKL = keras.layers\n\n""""""\nThis is an mnist example demonstrating how to use Keras symbolic function inside tensorpack.\nThis way you can define models in Keras-style, and benefit from the more efficeint trainers in tensorpack.\n\nNote: this example does not work for replicated-style data-parallel trainers, so may be less efficient\nfor some models.\n""""""\n\nIMAGE_SIZE = 28\n\n\n# Work around a Keras issue: it append name scopes to variable names..\n# May not work well if you use Keras layers inside other name scopes.\n@contextmanager\ndef clear_tower0_name_scope():\n    ns = tf.get_default_graph().get_name_scope()\n    if ns == \'tower0\':\n        with tf.name_scope(\'/\'):\n            yield\n    else:\n        yield\n\n\n@memoized  # this is necessary for sonnet/keras to work under tensorpack\ndef get_keras_model():\n    with clear_tower0_name_scope():\n        M = keras.models.Sequential()\n        M.add(KL.Conv2D(32, 3, activation=\'relu\', padding=\'same\'))\n        M.add(KL.MaxPooling2D())\n        M.add(KL.Conv2D(32, 3, activation=\'relu\', padding=\'same\'))\n        M.add(KL.Conv2D(32, 3, activation=\'relu\', padding=\'same\'))\n        M.add(KL.MaxPooling2D())\n        M.add(KL.Conv2D(32, 3, padding=\'same\', activation=\'relu\'))\n        M.add(KL.Flatten())\n        M.add(KL.Dense(512, activation=\'relu\', kernel_regularizer=keras.regularizers.l2(1e-5)))\n        M.add(KL.Dropout(rate=0.5))\n        M.add(KL.Dense(10, activation=None, kernel_regularizer=keras.regularizers.l2(1e-5)))\n    return M\n\n\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.TensorSpec((None, IMAGE_SIZE, IMAGE_SIZE), tf.float32, \'input\'),\n                tf.TensorSpec((None,), tf.int32, \'label\')]\n\n    def build_graph(self, image, label):\n        image = tf.expand_dims(image, 3) * 2 - 1\n        ctx = get_current_tower_context()\n\n        M = get_keras_model()\n        logits = M(image)\n        if ctx.is_main_training_tower:\n            for op in M.updates:\n                tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)\n\n        # build cost function by tensorflow\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')  # the average cross-entropy loss\n\n        # for tensorpack validation\n        acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)\n        acc = tf.reduce_mean(acc, name=\'accuracy\')\n        summary.add_moving_summary(acc)\n\n        wd_cost = tf.add_n(M.losses, name=\'regularize_loss\')    # this is how Keras manage regularizers\n        cost = tf.add_n([wd_cost, cost], name=\'total_cost\')\n        summary.add_moving_summary(cost, wd_cost)\n        return cost\n\n    def optimizer(self):\n        lr = tf.train.exponential_decay(\n            learning_rate=1e-3,\n            global_step=get_global_step_var(),\n            decay_steps=468 * 10,\n            decay_rate=0.3, staircase=True, name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', lr)\n        return tf.train.AdamOptimizer(lr)\n\n\ndef get_data():\n    train = BatchData(dataset.Mnist(\'train\'), 128)\n    test = BatchData(dataset.Mnist(\'test\'), 256, remainder=True)\n    return train, test\n\n\nif __name__ == \'__main__\':\n    logger.auto_set_dir()\n    dataset_train, dataset_test = get_data()\n\n    cfg = TrainConfig(\n        model=Model(),\n        dataflow=dataset_train,\n        callbacks=[\n            KerasPhaseCallback(True),   # for Keras training\n            ModelSaver(),\n            InferenceRunner(\n                dataset_test,\n                ScalarStats([\'cross_entropy_loss\', \'accuracy\'])),\n        ],\n        max_epoch=100,\n    )\n\n    if get_num_gpu() <= 1:\n        # single GPU:\n        launch_train_with_config(cfg, SimpleTrainer())\n    else:\n        # multi GPU:\n        launch_train_with_config(cfg, SyncMultiGPUTrainerParameterServer(2))\n        # ""Replicated"" multi-gpu trainer is not supported for Keras model\n        # since Keras does not respect variable scopes.\n'"
tensorpack/callbacks/__init__.py,0,"b""#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()['kcah_acitats'[::-1].upper()] = False\nif STATICA_HACK:\n    from .base import *\n    from .concurrency import *\n    from .graph import *\n    from .group import *\n    from .hooks import *\n    from .inference import *\n    from .inference_runner import *\n    from .monitor import *\n    from .param import *\n    from .prof import *\n    from .saver import *\n    from .misc import *\n    from .steps import *\n    from .summary import *\n    from .trigger import *\n\n\nfrom pkgutil import iter_modules\nimport os\n\n\n__all__ = []\n\n\ndef _global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if '__all__' in dir(p) else dir(p)\n    if lst:\n        del globals()[name]\n        for k in lst:\n            if not k.startswith('__'):\n                globals()[k] = p.__dict__[k]\n                __all__.append(k)\n\n\n_CURR_DIR = os.path.dirname(__file__)\nfor _, module_name, _ in iter_modules(\n       [_CURR_DIR]):\n    srcpath = os.path.join(_CURR_DIR, module_name + '.py')\n    if not os.path.isfile(srcpath):\n        continue\n    if module_name.endswith('_test'):\n        continue\n    if not module_name.startswith('_'):\n        _global_import(module_name)\n"""
tensorpack/callbacks/base.py,12,"b'# -*- coding: utf-8 -*-\n# File: base.py\n\n\nfrom abc import ABCMeta\nimport six\nfrom ..compat import tfv1 as tf\n\nfrom ..tfutils.common import get_op_or_tensor_by_name\n\n__all__ = [\'Callback\', \'ProxyCallback\', \'CallbackFactory\']\n\n\n@six.add_metaclass(ABCMeta)\nclass Callback(object):\n    """""" Base class for all callbacks. See\n    `Write a Callback\n    <http://tensorpack.readthedocs.io/tutorial/extend/callback.html>`_\n    for more detailed explanation of the callback methods.\n\n    Attributes:\n        epoch_num(int): trainer.epoch_num\n        global_step(int): trainer.global_step\n        local_step(int): trainer.local_step\n        trainer(Trainer): the trainer.\n        graph(tf.Graph): the graph.\n\n    Note:\n        These attributes are available only after (and including)\n        :meth:`_setup_graph`.\n\n    .. document private functions\n    .. automethod:: _setup_graph\n    .. automethod:: _before_train\n    .. automethod:: _after_train\n    .. automethod:: _before_run\n    .. automethod:: _after_run\n    .. automethod:: _before_epoch\n    .. automethod:: _after_epoch\n    .. automethod:: _trigger_step\n    .. automethod:: _trigger_epoch\n    .. automethod:: _trigger\n    """"""\n\n    _chief_only = True\n\n    name_scope = """"\n    """"""\n    A name scope for ops created inside this callback.\n    By default to the name of the class, but can be set per-instance.\n    """"""\n\n    def setup_graph(self, trainer):\n        self.trainer = trainer\n        self.graph = tf.get_default_graph()\n        scope_name = self.name_scope or type(self).__name__\n        scope_name = scope_name.replace(\'_\', \'\')\n        with tf.name_scope(scope_name):\n            self._setup_graph()\n\n    def _setup_graph(self):\n        """"""\n        Called before finalizing the graph.\n        Override this method to setup the ops used in the callback.\n        This is the same as ``tf.train.SessionRunHook.begin()``.\n        """"""\n        pass\n\n    def before_train(self):\n        self._before_train()\n\n    def _before_train(self):\n        """"""\n        Called right before the first iteration. The main difference to\n        `setup_graph` is that at this point the graph is finalized and a default session is initialized.\n        Override this method to, e.g. run some operations under the session.\n\n        This is similar to ``tf.train.SessionRunHook.after_create_session()``, but different:\n        it is called after the session is initialized by :class:`tfutils.SessionInit`.\n        """"""\n        pass\n\n    def before_epoch(self):\n        self._before_epoch()\n\n    def _before_epoch(self):\n        """"""\n        Called right before each epoch.\n        Usually you should use the :meth:`trigger` callback to run something between epochs.\n        Use this method only when something really needs to be run **immediately** before each epoch.\n        """"""\n        pass\n\n    def after_epoch(self):\n        self._after_epoch()\n\n    def _after_epoch(self):\n        """"""\n        Called right after each epoch.\n        Usually you should use the :meth:`trigger` callback to run something between epochs.\n        Use this method only when something really needs to be run **immediately** after each epoch.\n        """"""\n        pass\n\n    def before_run(self, ctx):\n        fetches = self._before_run(ctx)\n        if fetches is None:\n            return None\n        if isinstance(fetches, tf.train.SessionRunArgs):\n            return fetches\n\n        # also support list of names\n        assert isinstance(fetches, list), fetches\n        ret = []\n        for f in fetches:\n            if isinstance(f, (tf.Tensor, tf.Operation)):\n                ret.append(f)\n            else:\n                # warn about speed\n                ret.append(get_op_or_tensor_by_name(f))\n        return tf.train.SessionRunArgs(fetches=ret)\n\n    def _before_run(self, ctx):\n        """"""\n        It is called before every ``hooked_sess.run()`` call, and it\n        registers some extra op/tensors to run in the next call.\n        This method is the same as ``tf.train.SessionRunHook.before_run``.\n        Refer to TensorFlow docs for more details.\n        """"""\n        return None\n\n    def after_run(self, run_context, run_values):\n        self._after_run(run_context, run_values)\n\n    def _after_run(self, run_context, run_values):\n        """"""\n        It is called after every ``hooked_sess.run()`` call, and it\n        processes the values requested by the corresponding :meth:`before_run`.\n        It is equivalent to ``tf.train.SessionRunHook.after_run()``, refer to\n        TensorFlow docs for more details.\n        """"""\n        pass\n\n    def trigger_step(self):\n        self._trigger_step()\n\n    def _trigger_step(self):\n        """"""\n        Called after each :meth:`Trainer.run_step()` completes. Defaults to no-op.\n\n        You can override it to implement, e.g. a ProgressBar.\n        """"""\n        pass\n\n    def trigger_epoch(self):\n        self._trigger_epoch()\n\n    def _trigger_epoch(self):\n        """"""\n        Called after the completion of every epoch. Defaults to call ``self.trigger()``\n        """"""\n        self.trigger()\n\n    def trigger(self):\n        self._trigger()\n\n    def _trigger(self):\n        """"""\n        Override this method to define a general trigger behavior, to be used with trigger schedulers.\n        Note that the schedulers (e.g. :class:`PeriodicTrigger`) might call this\n        method both inside an epoch and after an epoch.\n\n        When used without the scheduler, this method by default will be called by `trigger_epoch()`.\n        """"""\n        pass\n\n    def after_train(self):\n        self._after_train()\n\n    def _after_train(self):\n        """"""\n        Called after training.\n        """"""\n        pass\n\n    @property\n    def epoch_num(self):\n        return self.trainer.epoch_num\n\n    @property\n    def global_step(self):\n        return self.trainer.global_step\n\n    @property\n    def local_step(self):\n        return self.trainer.local_step\n\n    @property\n    def chief_only(self):\n        """"""\n        Only run this callback on chief training process.\n\n        Returns: bool\n        """"""\n        return self._chief_only\n\n    @chief_only.setter\n    def chief_only(self, v):\n        self._chief_only = v\n\n    def set_chief_only(self, v=True):\n        """"""\n        Set chief_only property, and returns the callback itself.\n        """"""\n        self._chief_only = v\n        return self\n\n    def __str__(self):\n        return type(self).__name__\n\n    # TODO RENAME: same function to be used to get ops as well\n    def get_tensors_maybe_in_tower(self, names):\n        """"""\n        Get tensors in the graph with the given names.\n        Will automatically check for the *first training tower*\n        if no existing tensor is found with the name.\n\n        Returns:\n            [tf.Tensor]\n        """"""\n        from ..train.tower import TowerTrainer  # noqa\n\n        def get_tensor(name):\n            msg = ""Tensor {} not found in the graph!"".format(name)\n            try:\n                return get_op_or_tensor_by_name(name)\n            except KeyError:\n                pass\n            if not isinstance(self.trainer, TowerTrainer):\n                raise KeyError(msg)\n            towers = self.trainer.towers\n            try:\n                return towers.training()[0][name]\n            except KeyError:\n                raise KeyError(msg)\n        return [get_tensor(name) for name in names]\n\n\nclass ProxyCallback(Callback):\n    """""" A callback which proxy all methods to another callback.\n        It\'s useful as a base class of callbacks which decorate other callbacks.\n    """"""\n\n    def __init__(self, cb):\n        """"""\n        Args:\n            cb(Callback): the underlying callback\n        """"""\n        assert isinstance(cb, Callback), type(cb)\n        self.chief_only = cb.chief_only\n        self.cb = cb\n\n    def _before_train(self):\n        self.cb.before_train()\n\n    def _setup_graph(self):\n        with tf.name_scope(None):\n            self.cb.setup_graph(self.trainer)\n\n    def _trigger_epoch(self):\n        self.cb.trigger_epoch()\n\n    def _trigger(self):\n        self.cb.trigger()\n\n    def _trigger_step(self):\n        self.cb.trigger_step()\n\n    def _after_train(self):\n        self.cb.after_train()\n\n    def _before_epoch(self):\n        self.cb.before_epoch()\n\n    def _after_epoch(self):\n        self.cb.after_epoch()\n\n    def _before_run(self, ctx):\n        return self.cb._before_run(ctx)\n\n    def _after_run(self, ctx, run_values):\n        self.cb._after_run(ctx, run_values)\n\n    def __str__(self):\n        return ""Proxy-"" + str(self.cb)\n\n\nclass CallbackFactory(Callback):\n    """"""\n    Create a callback with some lambdas.\n    """"""\n    def __init__(self, setup_graph=None, before_train=None, trigger=None,\n                 after_train=None):\n        """"""\n        Each lambda takes ``self`` as the only argument.\n        """"""\n\n        self._cb_setup_graph = setup_graph\n        self._cb_before_train = before_train\n        self._cb_trigger = trigger\n        self._cb_after_train = after_train\n\n    def _setup_graph(self):\n        if self._cb_setup_graph:\n            self._cb_setup_graph(self)\n\n    def _before_train(self):\n        if self._cb_before_train:\n            self._cb_before_train(self)\n\n    def _trigger(self):\n        if self._cb_trigger:\n            self._cb_trigger(self)\n\n    def _after_train(self):\n        if self._cb_after_train:\n            self._cb_after_train(self)\n\n    def __str__(self):\n        strs = []\n        if self._cb_setup_graph is not None:\n            strs.append(""setup_graph="" + str(self._cb_setup_graph))\n        if self._cb_before_train is not None:\n            strs.append(""before_train="" + str(self._cb_before_train))\n        if self._cb_trigger is not None:\n            strs.append(""trigger="" + str(self._cb_trigger))\n        if self._cb_after_train is not None:\n            strs.append(""after_train="" + str(self._cb_after_train))\n        return ""CallbackFactory({})"".format(\', \'.join(strs))\n'"
tensorpack/callbacks/concurrency.py,0,"b'# -*- coding: utf-8 -*-\n# File: concurrency.py\n\nimport multiprocessing as mp\n\nfrom ..utils import logger\nfrom ..utils.concurrency import StoppableThread, start_proc_mask_signal\nfrom .base import Callback\n\n__all__ = [\'StartProcOrThread\']\n\n\nclass StartProcOrThread(Callback):\n    """"""\n    Start some threads or processes before training.\n    """"""\n\n    _chief_only = False\n\n    def __init__(self, startable, stop_at_last=True):\n        """"""\n        Args:\n            startable (list): list of processes or threads which have ``start()`` method.\n                Can also be a single instance of process of thread.\n            stop_at_last (bool): whether to stop the processes or threads\n                after training. It will use :meth:`Process.terminate()` or\n                :meth:`StoppableThread.stop()`, but will do nothing on normal\n                ``threading.Thread`` or other startable objects.\n        """"""\n        if not isinstance(startable, list):\n            startable = [startable]\n        self._procs_threads = startable\n        self._stop_at_last = stop_at_last\n\n    def _before_train(self):\n        logger.info(""Starting "" +\n                    \', \'.join([k.name for k in self._procs_threads]) + \' ...\')\n        # avoid sigint get handled by other processes\n        start_proc_mask_signal(self._procs_threads)\n\n    def _after_train(self):\n        if not self._stop_at_last:\n            return\n        for k in self._procs_threads:\n            if not k.is_alive():\n                continue\n            if isinstance(k, mp.Process):\n                logger.info(""Stopping {} ..."".format(k.name))\n                k.terminate()\n                k.join(5.0)\n                if k.is_alive():\n                    logger.error(""Cannot join process {}."".format(k.name))\n            elif isinstance(k, StoppableThread):\n                logger.info(""Stopping {} ..."".format(k.name))\n                k.stop()\n                k.join(5.0)\n                if k.is_alive():\n                    logger.error(""Cannot join thread {}."".format(k.name))\n'"
tensorpack/callbacks/graph.py,13,"b'# -*- coding: utf-8 -*-\n# File: graph.py\n\n\n"""""" Graph related callbacks""""""\n\nimport numpy as np\nimport os\n\nfrom ..compat import tfv1 as tf\nfrom ..tfutils.common import get_op_tensor_name\nfrom ..utils import logger\nfrom .base import Callback\n\n__all__ = [\'RunOp\', \'RunUpdateOps\', \'ProcessTensors\', \'DumpTensors\',\n           \'DumpTensor\', \'DumpTensorAsImage\', \'DumpParamAsImage\', \'CheckNumerics\']\n\n\nclass RunOp(Callback):\n    """""" Run an Op. """"""\n\n    _chief_only = False\n\n    def __init__(self, op,\n                 run_before=True, run_as_trigger=True,\n                 run_step=False, verbose=False):\n        """"""\n        Args:\n            op (tf.Operation or function): an Op, or a function that returns the Op in the graph.\n                The function will be called after the main graph has been created (in the :meth:`setup_graph` callback).\n            run_before (bool): run the Op before training\n            run_as_trigger (bool): run the Op on every :meth:`trigger()` call.\n            run_step (bool): run the Op every step (along with training)\n            verbose (bool): print logs when the op is run.\n\n        Example:\n            The `DQN Example\n            <https://github.com/tensorpack/tensorpack/blob/master/examples/DeepQNetwork/>`_\n            uses this callback to update target network.\n        """"""\n        if not callable(op):\n            self.setup_func = lambda: op  # noqa\n        else:\n            self.setup_func = op\n        self.run_before = run_before\n        self.run_as_trigger = run_as_trigger\n        self.run_step = run_step\n        self.verbose = verbose\n\n    def _setup_graph(self):\n        self._op = self.setup_func()\n        if self.run_step:\n            self._fetch = tf.train.SessionRunArgs(fetches=self._op)\n\n    def _before_train(self):\n        if self.run_before:\n            self._print()\n            self._op.run()\n\n    def _trigger(self):\n        if self.run_as_trigger:\n            self._print()\n            self._op.run()\n\n    def _before_run(self, _):\n        if self.run_step:\n            self._print()\n            return self._fetch\n\n    def _print(self):\n        if self.verbose:\n            logger.info(""Running Op {} ..."".format(self._op.name))\n\n\nclass RunUpdateOps(RunOp):\n    """"""\n    Run ops from the collection UPDATE_OPS every step.\n    The ops will be hooked to ``trainer.hooked_sess`` and run along with\n    each ``hooked_sess.run`` call.\n\n    Be careful when using ``UPDATE_OPS`` if your model contains more than one sub-networks.\n    Perhaps not all updates are supposed to be executed in every iteration.\n\n    This callback is one of the :func:`DEFAULT_CALLBACKS()`.\n    """"""\n\n    def __init__(self, collection=None):\n        """"""\n        Args:\n            collection (str): collection of ops to run. Defaults to ``tf.GraphKeys.UPDATE_OPS``\n        """"""\n        if collection is None:\n            collection = tf.GraphKeys.UPDATE_OPS\n        name = \'UPDATE_OPS\' if collection == tf.GraphKeys.UPDATE_OPS else collection\n\n        def f():\n            ops = tf.get_collection(collection)\n            if ops:\n                logger.info(""Applying collection {} of {} ops."".format(name, len(ops)))\n                return tf.group(*ops, name=\'update_ops\')\n            else:\n                return tf.no_op(name=\'empty_update_ops\')\n\n        super(RunUpdateOps, self).__init__(\n            f, run_before=False, run_as_trigger=False, run_step=True)\n\n\nclass ProcessTensors(Callback):\n    """"""\n    Fetch extra tensors **along with** each training step,\n    and call some function over the values.\n    It uses ``_{before,after}_run`` method to inject ``tf.train.SessionRunHooks``\n    to the session.\n    You can use it to print tensors, save tensors to file, etc.\n\n    Example:\n\n    .. code-block:: python\n\n        ProcessTensors([\'mycost1\', \'mycost2\'], lambda c1, c2: print(c1, c2, c1 + c2))\n    """"""\n    def __init__(self, names, fn):\n        """"""\n        Args:\n            names (list[str]): names of tensors\n            fn: a function taking all requested tensors as input\n        """"""\n        assert isinstance(names, (list, tuple)), names\n        self._names = names\n        self._fn = fn\n\n    def _setup_graph(self):\n        tensors = self.get_tensors_maybe_in_tower(self._names)\n        self._fetch = tf.train.SessionRunArgs(fetches=tensors)\n\n    def _before_run(self, _):\n        return self._fetch\n\n    def _after_run(self, _, rv):\n        results = rv.results\n        self._fn(*results)\n\n\nclass DumpTensors(ProcessTensors):\n    """"""\n    Dump some tensors to a file.\n    Every step this callback fetches tensors and write them to a npz file\n    under ``logger.get_logger_dir``.\n    The dump can be loaded by ``dict(np.load(filename).items())``.\n    """"""\n    def __init__(self, names):\n        """"""\n        Args:\n            names (list[str]): names of tensors\n        """"""\n        assert isinstance(names, (list, tuple)), names\n        self._names = names\n        dir = logger.get_logger_dir()\n\n        def fn(*args):\n            dic = {}\n            for name, val in zip(self._names, args):\n                dic[name] = val\n            fname = os.path.join(\n                dir, \'DumpTensor-{}.npz\'.format(self.global_step))\n            np.savez(fname, **dic)\n        super(DumpTensors, self).__init__(names, fn)\n\n\nclass DumpTensorAsImage(Callback):\n    """"""\n    Dump a tensor to image(s) to ``logger.get_logger_dir()`` once triggered.\n\n    Note that it requires the tensor is directly evaluable, i.e. either inputs\n    are not its dependency (e.g. the weights of the model), or the inputs are\n    feedfree (in which case this callback will take an extra datapoint from the input pipeline).\n    """"""\n\n    def __init__(self, tensor_name, prefix=None, map_func=None, scale=255):\n        """"""\n        Args:\n            tensor_name (str): the name of the tensor.\n            prefix (str): the filename prefix for saved images. Defaults to the Op name.\n            map_func: map the value of the tensor to an image or list of\n                 images of shape [h, w] or [h, w, c]. If None, will use identity.\n            scale (float): a multiplier on pixel values, applied after map_func.\n        """"""\n        op_name, self.tensor_name = get_op_tensor_name(tensor_name)\n        self.func = map_func\n        if prefix is None:\n            self.prefix = op_name\n        else:\n            self.prefix = prefix\n        self.log_dir = logger.get_logger_dir()\n        self.scale = scale\n\n    def _before_train(self):\n        self._tensor = self.graph.get_tensor_by_name(self.tensor_name)\n\n    def _trigger(self):\n        val = self.trainer.sess.run(self._tensor)\n        if self.func is not None:\n            val = self.func(val)\n        if isinstance(val, list) or val.ndim == 4:\n            for idx, im in enumerate(val):\n                self._dump_image(im, idx)\n        else:\n            self._dump_image(val)\n        self.trainer.monitors.put_image(self.prefix, val)\n\n    def _dump_image(self, im, idx=None):\n        assert im.ndim in [2, 3], str(im.ndim)\n        fname = os.path.join(\n            self.log_dir,\n            self.prefix + \'-ep{:03d}{}.png\'.format(\n                self.epoch_num, \'-\' + str(idx) if idx else \'\'))\n        res = im * self.scale\n        res = np.clip(res, 0, 255)\n        cv2.imwrite(fname, res.astype(\'uint8\'))\n\n\nclass CheckNumerics(Callback):\n    """"""\n    When triggered, check variables in the graph for NaN and Inf.\n    Raise exceptions if such an error is found.\n    """"""\n    def _setup_graph(self):\n        vars = tf.trainable_variables()\n        ops = [tf.check_numerics(v, ""CheckNumerics[\'{}\']"".format(v.op.name)).op for v in vars]\n        self._check_op = tf.group(*ops)\n\n    def _trigger(self):\n        self._check_op.run()\n\n\ntry:\n    import cv2\nexcept ImportError:\n    from ..utils.develop import create_dummy_class\n    DumpTensorAsImage = create_dummy_class(\'DumpTensorAsImage\', \'cv2\')  # noqa\n\n# alias\nDumpParamAsImage = DumpTensorAsImage\nDumpTensor = DumpTensors\n'"
tensorpack/callbacks/group.py,1,"b'# -*- coding: utf-8 -*-\n# File: group.py\n\n\nimport traceback\nfrom contextlib import contextmanager\nfrom time import perf_counter as timer  # noqa\nfrom ..compat import tfv1 as tf\n\nfrom ..utils import logger\nfrom ..utils.utils import humanize_time_delta\nfrom .base import Callback\nfrom .hooks import CallbackToHook\n\n__all__ = [\'Callbacks\']\n\n\nclass CallbackTimeLogger(object):\n    def __init__(self):\n        self.times = []\n        self.tot = 0\n\n    def add(self, name, time):\n        self.tot += time\n        self.times.append((name, time))\n\n    @contextmanager\n    def timed_callback(self, name):\n        s = timer()\n        yield\n        self.add(name, timer() - s)\n\n    def log(self):\n\n        """""" log the time of some heavy callbacks """"""\n        if self.tot < 3:\n            return\n        msgs = []\n        for name, t in self.times:\n            if t / self.tot > 0.3 and t > 1:\n                msgs.append(name + "": "" + humanize_time_delta(t))\n        logger.info(\n            ""Callbacks took {:.3f} sec in total. {}"".format(\n                self.tot, \'; \'.join(msgs)))\n\n\nclass Callbacks(Callback):\n    """"""\n    A container to hold all callbacks, and trigger them iteratively.\n\n    This is only used by the base trainer to run all the callbacks.\n    Users do not need to use this class.\n    """"""\n\n    def __init__(self, cbs):\n        """"""\n        Args:\n            cbs(list): a list of :class:`Callback` instances.\n        """"""\n        # check type\n        for cb in cbs:\n            assert isinstance(cb, Callback), cb.__class__\n        self.cbs = cbs\n\n    def _setup_graph(self):\n        with tf.name_scope(None):   # clear the name scope\n            for cb in self.cbs:\n                cb.setup_graph(self.trainer)\n\n    def _before_train(self):\n        for cb in self.cbs:\n            cb.before_train()\n\n    def _after_train(self):\n        for cb in self.cbs:\n            # make sure callbacks are properly finalized\n            try:\n                cb.after_train()\n            except Exception:\n                traceback.print_exc()\n\n    def get_hooks(self):\n        return [CallbackToHook(cb) for cb in self.cbs]\n\n    def trigger_step(self):\n        for cb in self.cbs:\n            cb.trigger_step()\n\n    def _trigger_epoch(self):\n        tm = CallbackTimeLogger()\n\n        for cb in self.cbs:\n            display_name = str(cb)\n            with tm.timed_callback(display_name):\n                cb.trigger_epoch()\n        tm.log()\n\n    def _before_epoch(self):\n        for cb in self.cbs:\n            cb.before_epoch()\n\n    def _after_epoch(self):\n        for cb in self.cbs:\n            cb.after_epoch()\n'"
tensorpack/callbacks/hooks.py,5,"b'# -*- coding: utf-8 -*-\n# File: hooks.py\n\n\n"""""" Compatible layers between tf.train.SessionRunHook and Callback""""""\n\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..utils.develop import HIDE_DOC\n\nfrom .base import Callback\n\n__all__ = [\'CallbackToHook\', \'HookToCallback\', \'TFLocalCLIDebugHook\']\n\n\nclass CallbackToHook(tfv1.train.SessionRunHook):\n    """"""\n    Hooks are less powerful than callbacks so the conversion is incomplete.\n    It only converts the ``before_run/after_run`` calls.\n\n    This is only for internal implementation of\n    ``before_run/after_run`` callbacks.\n    You shouldn\'t need to use this.\n    """"""\n\n    def __init__(self, cb):\n        self._cb = cb\n\n    @HIDE_DOC\n    def before_run(self, ctx):\n        return self._cb.before_run(ctx)\n\n    @HIDE_DOC\n    def after_run(self, ctx, vals):\n        self._cb.after_run(ctx, vals)\n\n\nclass HookToCallback(Callback):\n    """"""\n    Make a ``tf.train.SessionRunHook`` into a callback.\n    Note that when ``SessionRunHook.after_create_session`` is called, the ``coord`` argument will be None.\n    """"""\n\n    _chief_only = False\n\n    def __init__(self, hook):\n        """"""\n        Args:\n            hook (tf.train.SessionRunHook):\n        """"""\n        self._hook = hook\n\n    def _setup_graph(self):\n        with tf.name_scope(None):   # jump out of the name scope\n            self._hook.begin()\n\n    def _before_train(self):\n        sess = tf.get_default_session()\n        # coord is set to None when converting\n        self._hook.after_create_session(sess, None)\n\n    def _before_run(self, ctx):\n        return self._hook.before_run(ctx)\n\n    def _after_run(self, ctx, run_values):\n        self._hook.after_run(ctx, run_values)\n\n    def _after_train(self):\n        self._hook.end(self.trainer.sess)\n\n\nclass TFLocalCLIDebugHook(HookToCallback):\n    """"""\n    Use the hook `tfdbg.LocalCLIDebugHook` in tensorpack.\n    """"""\n\n    _chief_only = True\n\n    def __init__(self, *args, **kwargs):\n        """"""\n        Args:\n            args, kwargs: arguments to create `tfdbg.LocalCLIDebugHook`.\n                Refer to tensorflow documentation for details.\n        """"""\n        from tensorflow.python import debug as tfdbg\n        super(TFLocalCLIDebugHook, self).__init__(tfdbg.LocalCLIDebugHook(*args, **kwargs))\n\n    def add_tensor_filter(self, *args, **kwargs):\n        """"""\n        Wrapper of `tfdbg.LocalCLIDebugHook.add_tensor_filter`.\n        Refer to tensorflow documentation for details.\n        """"""\n        self._hook.add_tensor_filter(*args, **kwargs)\n'"
tensorpack/callbacks/inference.py,1,"b'# -*- coding: utf-8 -*-\n# File: inference.py\n\n\nimport numpy as np\nfrom abc import ABCMeta\nimport six\n\nfrom ..tfutils.common import get_op_tensor_name\nfrom ..utils import logger\nfrom ..utils.stats import BinaryStatistics, RatioCounter\nfrom .base import Callback\n\n__all__ = [\'ScalarStats\', \'Inferencer\',\n           \'ClassificationError\', \'BinaryClassificationStats\']\n\n\n@six.add_metaclass(ABCMeta)\nclass Inferencer(Callback):\n    """""" Base class of Inferencer.\n    Inferencer is a special kind of callback that should be called by :class:`InferenceRunner`.\n    It has the methods ``_get_fetches`` and ``_on_fetches`` which are like\n    :class:`SessionRunHooks`, except that they will be used only by :class:`InferenceRunner`.\n\n    .. document private functions\n    .. automethod:: _before_inference\n    .. automethod:: _after_inference\n    .. automethod:: _get_fetches\n    .. automethod:: _on_fetches\n    """"""\n\n    def _before_epoch(self):\n        self._before_inference()\n\n    def _before_inference(self):\n        """"""\n        Called before a new round of inference starts.\n        """"""\n        pass\n\n    def _trigger_epoch(self):\n        ret = self._after_inference()\n        if ret is None:\n            return\n        for k, v in six.iteritems(ret):\n            try:\n                v = float(v)\n            except ValueError:\n                logger.warn(""{} returns a non-scalar statistics!"".format(type(self).__name__))\n                continue\n            else:\n                self.trainer.monitors.put_scalar(k, v)\n\n    def _after_inference(self):\n        """"""\n        Called after a round of inference ends.\n        Returns a dict of scalar statistics which will be logged to monitors.\n        """"""\n        pass\n\n    def get_fetches(self):\n        """"""\n        Return a list of tensor names (guaranteed not op name) this inferencer needs.\n        """"""\n        ret = self._get_fetches()\n        return [get_op_tensor_name(n)[1] for n in ret]\n\n    def _get_fetches(self):\n        """"""\n        To be implemented by subclasses\n        """"""\n        raise NotImplementedError()\n\n    def on_fetches(self, results):\n        """"""\n        Called after each new datapoint finished the forward inference.\n\n        Args:\n            results(list): list of results this inferencer fetched. Has the same\n                length as ``self._get_fetches()``.\n        """"""\n        self._on_fetches(results)\n\n    def _on_fetches(self, results):\n        """"""\n        To be implemented by subclasses\n        """"""\n        raise NotImplementedError()\n\n\nclass ScalarStats(Inferencer):\n    """"""\n    Statistics of some scalar tensor.\n    The value will be averaged over all given datapoints.\n\n    Note that the average of accuracy over all batches is not necessarily the\n    accuracy of the whole dataset. See :class:`ClassificationError` for details.\n    """"""\n\n    def __init__(self, names, prefix=\'validation\'):\n        """"""\n        Args:\n            names(list or str): list of names or just one name. The\n                corresponding tensors have to be scalar.\n            prefix(str): a prefix for logging\n        """"""\n        if not isinstance(names, list):\n            self.names = [names]\n        else:\n            self.names = names\n        self.prefix = prefix\n\n    def _before_inference(self):\n        self.stats = []\n\n    def _get_fetches(self):\n        return self.names\n\n    def _on_fetches(self, output):\n        self.stats.append(output)\n\n    def _after_inference(self):\n        if len(self.stats):\n            self.stats = np.mean(self.stats, axis=0)\n            assert len(self.stats) == len(self.names)\n\n        ret = {}\n        for stat, name in zip(self.stats, self.names):\n            opname, _ = get_op_tensor_name(name)\n            name = \'{}_{}\'.format(self.prefix, opname) if self.prefix else opname\n            ret[name] = stat\n        return ret\n\n\nclass ClassificationError(Inferencer):\n    """"""\n    Compute **true** classification error in batch mode, from a ``wrong`` tensor.\n\n    The ``wrong`` tensor is supposed to be an binary vector containing\n    whether each sample in the batch is *incorrectly* classified.\n    You can use ``tf.nn.in_top_k`` to produce this vector.\n\n    This Inferencer produces the ""true"" error, which could be different from\n    ``ScalarStats(\'error_rate\')``.\n    It takes account of the fact that batches might not have the same size in\n    testing (because the size of test set might not be a multiple of batch size).\n    Therefore the result can be different from averaging the error rate of each batch.\n\n    You can also use the ""correct prediction"" tensor, then this inferencer will\n    give you ""classification accuracy"" instead of error.\n    """"""\n\n    def __init__(self, wrong_tensor_name=\'incorrect_vector\', summary_name=\'validation_error\'):\n        """"""\n        Args:\n            wrong_tensor_name(str): name of the ``wrong`` binary vector tensor.\n            summary_name(str): the name to log the error with.\n        """"""\n        self.wrong_tensor_name = wrong_tensor_name\n        self.summary_name = summary_name\n\n    def _before_inference(self):\n        self.err_stat = RatioCounter()\n\n    def _get_fetches(self):\n        return [self.wrong_tensor_name]\n\n    def _on_fetches(self, outputs):\n        vec = outputs[0]\n        # TODO put shape assertion into inference-runner\n        assert vec.ndim == 1, ""{} is not a vector!"".format(self.wrong_tensor_name)\n        batch_size = len(vec)\n        wrong = np.sum(vec)\n        self.err_stat.feed(wrong, batch_size)\n\n    def _after_inference(self):\n        return {self.summary_name: self.err_stat.ratio}\n\n\nclass BinaryClassificationStats(Inferencer):\n    """"""\n    Compute precision / recall in binary classification, given the\n    prediction vector and the label vector.\n    """"""\n\n    def __init__(self, pred_tensor_name, label_tensor_name, prefix=\'val\'):\n        """"""\n        Args:\n            pred_tensor_name(str): name of the 0/1 prediction tensor.\n            label_tensor_name(str): name of the 0/1 label tensor.\n        """"""\n        self.pred_tensor_name = pred_tensor_name\n        self.label_tensor_name = label_tensor_name\n        self.prefix = prefix\n\n    def _before_inference(self):\n        self.stat = BinaryStatistics()\n\n    def _get_fetches(self):\n        return [self.pred_tensor_name, self.label_tensor_name]\n\n    def _on_fetches(self, outputs):\n        pred, label = outputs\n        self.stat.feed(pred, label)\n\n    def _after_inference(self):\n        return {self.prefix + \'_precision\': self.stat.precision,\n                self.prefix + \'_recall\': self.stat.recall}\n'"
tensorpack/callbacks/inference_runner.py,9,"b'# -*- coding: utf-8 -*-\n# File: inference_runner.py\n\n\nimport itertools\nimport sys\nfrom contextlib import contextmanager\nimport tqdm\nfrom tensorflow.python.training.monitored_session import _HookedSession as HookedSession\n\nfrom ..compat import tfv1 as tf\nfrom ..dataflow.base import DataFlow\nfrom ..input_source import FeedInput, InputSource, QueueInput, StagingInput\nfrom ..tfutils.tower import PredictTowerContext\nfrom ..utils import logger\nfrom ..utils.utils import get_tqdm_kwargs\nfrom .base import Callback\nfrom .group import Callbacks\nfrom .inference import Inferencer\n\n__all__ = [\'InferenceRunnerBase\', \'InferenceRunner\',\n           \'DataParallelInferenceRunner\']\n\n\ndef _device_from_int(dev):\n    return \'/gpu:{}\'.format(dev) if dev >= 0 else \'/cpu:0\'\n\n\nclass InferencerToHook(tf.train.SessionRunHook):\n    def __init__(self, inf, fetches):\n        self._inf = inf\n        self._fetches = fetches\n\n    def before_run(self, _):\n        return tf.train.SessionRunArgs(fetches=self._fetches)\n\n    def after_run(self, _, run_values):\n        self._inf.on_fetches(run_values.results)\n\n\n@contextmanager\ndef _inference_context():\n    msg = ""You might need to check your input implementation.""\n    try:\n        yield\n    except (StopIteration, tf.errors.CancelledError):\n        logger.error(\n            ""[InferenceRunner] input stopped before reaching its __len__()! "" + msg)\n        raise\n    except tf.errors.OutOfRangeError:   # tf.data reaches an end\n        pass\n\n\nclass InferenceRunnerBase(Callback):\n    """""" Base class for inference runner.\n\n    Note:\n        1. InferenceRunner will use `input.size()` to determine\n           how much iterations to run, so you\'re responsible to ensure that\n           `input.size()` is accurate.\n        2. Only works with instances of `TowerTrainer`.\n    """"""\n    def __init__(self, input, infs):\n        """"""\n        Args:\n            input (InputSource): the input to use. Must have an accurate ``size()``.\n            infs (list[Inferencer]): list of :class:`Inferencer` to run.\n        """"""\n        self._input_source = input\n        if not isinstance(infs, list):\n            self.infs = [infs]\n        else:\n            self.infs = infs\n        for v in self.infs:\n            assert isinstance(v, Inferencer), v\n\n        try:\n            self._size = input.size()\n        except NotImplementedError:\n            self._size = 0\n\n        self._hooks = []\n\n    def register_hook(self, hook):\n        """"""\n        Args:\n            hook (tf.train.SessionRunHook):\n        """"""\n        self._hooks.append(hook)\n\n    def _before_train(self):\n        self._hooked_sess = HookedSession(self.trainer.sess, self._hooks)\n        self._input_callbacks.before_train()\n        if self._size > 0:\n            logger.info(""[InferenceRunner] Will eval {} iterations"".format(self._size))\n        else:\n            logger.warn(""[InferenceRunner] Got an InputSource with unknown size! Will iterate until OutOfRangeError!"")\n\n    def _after_train(self):\n        self._input_callbacks.after_train()\n\n\nclass InferenceRunner(InferenceRunnerBase):\n    """"""\n    A callback that runs a list of :class:`Inferencer` on some :class:`InputSource`.\n    """"""\n\n    def __init__(self, input, infs, tower_name=\'InferenceTower\', tower_func=None, device=0):\n        """"""\n        Args:\n            input (InputSource or DataFlow): The :class:`InputSource` to run\n                inference on.  If given a DataFlow, will use :class:`FeedInput`.\n            infs (list): a list of :class:`Inferencer` instances.\n            tower_name (str): the name scope of the tower to build.\n                If multiple InferenceRunner are used, each needs a different tower_name.\n            tower_func (tfutils.TowerFunc or None): the tower function to be used to build the graph.\n                By defaults to call `trainer.tower_func` under a `training=False` TowerContext,\n                but you can change it to a different tower function\n                if you need to inference with several different graphs.\n            device (int): the device to use\n        """"""\n        if isinstance(input, DataFlow):\n            # use infinite=False so that a dataflow without size will stop normally\n            # TODO a better way to handle inference size\n            input = FeedInput(input, infinite=False)\n        assert isinstance(input, InputSource), input\n        assert not isinstance(input, StagingInput), input\n        self._tower_name = tower_name\n        self._device_id = device\n        self._device = _device_from_int(device)\n        self._tower_func = tower_func\n        super(InferenceRunner, self).__init__(input, infs)\n\n    def _build_hook(self, inf):\n        out_names = inf.get_fetches()\n        fetches = self._tower_handle.get_tensors(out_names)\n        return InferencerToHook(inf, fetches)\n\n    def _setup_graph(self):\n        if self._tower_func is None:\n            assert self.trainer.tower_func is not None, ""You must set tower_func of the trainer to use InferenceRunner!""\n            self._tower_func = self.trainer.tower_func\n        input_callbacks = self._input_source.setup(self._tower_func.input_signature)\n\n        vs_name = self.trainer._vs_name_for_predictor(self._device_id)\n        logger.info(""[InferenceRunner] Building tower \'{}\' on device {} {}..."".format(\n            self._tower_name, self._device,\n            ""with variable scope \'{}\'"".format(vs_name) if vs_name else \'\'))\n        with tf.variable_scope(tf.get_variable_scope(), reuse=True), \\\n                tf.device(self._device), \\\n                PredictTowerContext(self._tower_name, vs_name=vs_name):\n            self._tower_func(*self._input_source.get_input_tensors())\n            self._tower_handle = self._tower_func.towers[-1]\n\n        for h in [self._build_hook(inf) for inf in self.infs]:\n            self.register_hook(h)\n        # trigger_{step,epoch}, {before,after}_epoch is ignored.\n        # We assume that InputSource callbacks won\'t use these methods\n        self._input_callbacks = Callbacks(input_callbacks)\n        for h in self._input_callbacks.get_hooks():\n            self.register_hook(h)\n\n        for inf in self.infs:\n            inf.setup_graph(self.trainer)\n        self._input_callbacks.setup_graph(self.trainer)\n\n    def _trigger(self):\n        for inf in self.infs:\n            inf.before_epoch()\n\n        self._input_source.reset_state()\n        # iterate over the data, and run the hooked session\n        with _inference_context(), \\\n                tqdm.tqdm(total=self._size, **get_tqdm_kwargs()) as pbar:\n            num_itr = self._size if self._size > 0 else sys.maxsize\n            for _ in range(num_itr):\n                self._hooked_sess.run(fetches=[])\n                pbar.update()\n        for inf in self.infs:\n            inf.trigger_epoch()\n\n\nclass DataParallelInferenceRunner(InferenceRunnerBase):\n    """"""\n    Inference with data-parallel support on multiple GPUs.\n    It will build one predict tower on each GPU, and run prediction\n    with a large total batch in parallel on all GPUs.\n    It will run the remainder (when the total size of input is not a multiple of #GPU)\n    sequentially.\n    """"""\n    def __init__(self, input, infs, gpus, tower_name=\'InferenceTower\', tower_func=None):\n        """"""\n        Args:\n            input (DataFlow or QueueInput)\n            gpus (int or list[int]): #gpus, or list of GPU id\n            tower_name (str): the name scope of the tower to build.\n                If multiple InferenceRunner are used, each needs a different tower_name.\n            tower_func (tfutils.TowerFunc or None): the tower function to be used to build the graph.\n                The tower function will be called under a `training=False` TowerContext.\n                The default is `trainer.tower_func`,\n                but you can change it to a different tower function\n                if you need to inference with several different models.\n        """"""\n        if isinstance(gpus, int):\n            gpus = list(range(gpus))\n        self._devices = [_device_from_int(k) for k in gpus]\n        self._tower_names = [\'{}{}\'.format(tower_name, k) for k in range(len(gpus))]\n\n        if isinstance(input, DataFlow):\n            input = QueueInput(input)\n        assert isinstance(input, QueueInput), input\n        super(DataParallelInferenceRunner, self).__init__(input, infs)\n        assert self._size > 0, ""Input for DataParallelInferenceRunner must have a size!""\n\n        self._hooks = []\n        self._hooks_parallel = []\n        self._tower_func = tower_func\n\n    def _setup_graph(self):\n        self._handles = []\n        if self._tower_func is None:\n            assert self.trainer.tower_func is not None, ""You must set tower_func of the trainer to use InferenceRunner!""\n            self._tower_func = self.trainer.tower_func\n\n        input_callbacks = self._input_source.setup(self._tower_func.input_signature)\n        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n            for idx, dev in enumerate(self._devices):\n                vs_name = self.trainer._vs_name_for_predictor(idx)\n                with tf.device(dev), PredictTowerContext(\n                        self._tower_names[idx], vs_name=vs_name):\n                    logger.info(""[InferenceRunner] Building tower \'{}\' on device {} {}..."".format(\n                        self._tower_names[idx], dev,\n                        ""with variable scope \'{}\'"".format(vs_name) if vs_name else \'\'))\n                    # TODO log for tower creation, here or in tower.py?\n                    self._tower_func(*self._input_source.get_input_tensors())\n                    self._handles.append(self._tower_func.towers[-1])\n\n        # setup callbacks and hooks\n        self._input_callbacks = Callbacks(input_callbacks)\n\n        # TODO InputSource might have hooks which break us.\n        # e.g. hooks from StagingInput will force the consumption\n        # of nr_tower datapoints in every run.\n        input_hooks = self._input_callbacks.get_hooks()\n        self._hooks.extend([self._build_hook(inf) for inf in self.infs] + input_hooks)\n        self._hooks_parallel.extend([self._build_hook_parallel(inf) for inf in self.infs] + input_hooks)\n\n        for inf in self.infs:\n            inf.setup_graph(self.trainer)\n        self._input_callbacks.setup_graph(self.trainer)\n\n    def register_hook(self, h):\n        logger.info(\n            ""[DataParallelInferenceRunner] Registering hook {} on both parallel and sequential inference."")\n        self._hooks.append(h)\n        self._hooks_parallel.append(h)\n\n    class _InferencerToHookDataParallel(InferencerToHook):\n        def __init__(self, inf, fetches, size):\n            """"""\n            Args:\n                size(int): number of tensors to fetch per tower\n            """"""\n            super(DataParallelInferenceRunner._InferencerToHookDataParallel, self).__init__(inf, fetches)\n            assert len(self._fetches) % size == 0\n            self._sz = size\n\n        def after_run(self, _, run_values):\n            res = run_values.results\n            for i in range(0, len(res), self._sz):\n                vals = res[i:i + self._sz]\n                self._inf.on_fetches(vals)\n\n    def _build_hook_parallel(self, inf):\n        out_names = inf.get_fetches()\n        sz = len(out_names)\n        fetches = list(itertools.chain(*[t.get_tensors(out_names) for t in self._handles]))\n        return self._InferencerToHookDataParallel(inf, fetches, sz)\n\n    def _build_hook(self, inf):\n        out_names = inf.get_fetches()\n        fetches = self._handles[0].get_tensors(out_names)\n        return InferencerToHook(inf, fetches)\n\n    def _before_train(self):\n        super(DataParallelInferenceRunner, self)._before_train()\n        self._parallel_hooked_sess = HookedSession(self.trainer.sess, self._hooks_parallel)\n\n    def _trigger(self):\n        for inf in self.infs:\n            inf.before_epoch()\n\n        total = self._size\n        nr_tower = len(self._devices)\n        self._input_source.reset_state()\n        with _inference_context():\n            with tqdm.tqdm(total=total, **get_tqdm_kwargs()) as pbar:\n                while total >= nr_tower:\n                    self._parallel_hooked_sess.run(fetches=[])\n                    pbar.update(nr_tower)\n                    total -= nr_tower\n                # take care of the rest\n                for _ in range(total):\n                    self._hooked_sess.run(fetches=[])\n                    pbar.update(1)\n        for inf in self.infs:\n            inf.trigger_epoch()\n'"
tensorpack/callbacks/misc.py,0,"b'# -*- coding: utf-8 -*-\n# File: misc.py\n\n\nimport numpy as np\nimport os\nimport time\nfrom collections import deque\n\nfrom ..utils import logger\nfrom ..utils.utils import humanize_time_delta\nfrom .base import Callback\n\n__all__ = [\'SendStat\', \'InjectShell\', \'EstimatedTimeLeft\']\n\n\nclass SendStat(Callback):\n    """""" An equivalent of :class:`SendMonitorData`, but as a normal callback. """"""\n    def __init__(self, command, names):\n        self.command = command\n        if not isinstance(names, list):\n            names = [names]\n        self.names = names\n\n    def _trigger(self):\n        M = self.trainer.monitors\n        v = {k: M.get_latest(k) for k in self.names}\n        cmd = self.command.format(**v)\n        ret = os.system(cmd)\n        if ret != 0:\n            logger.error(""Command {} failed with ret={}!"".format(cmd, ret))\n\n\nclass InjectShell(Callback):\n    """"""\n    Allow users to create a specific file as a signal to pause\n    and iteratively debug the training.\n    Once the :meth:`trigger` method is called, it detects whether the file exists, and opens an\n    IPython/pdb shell if yes.\n    In the shell, ``self`` is this callback, ``self.trainer`` is the trainer, and\n    from that you can access everything else.\n\n    Example:\n\n    .. code-block:: none\n\n        callbacks=[InjectShell(\'/path/to/pause-training.tmp\'), ...]\n\n        # the following command will pause the training and start a shell when the epoch finishes:\n        $ touch /path/to/pause-training.tmp\n\n    """"""\n\n    def __init__(self, file=\'INJECT_SHELL.tmp\', shell=\'ipython\'):\n        """"""\n        Args:\n           file (str): if this file exists, will open a shell.\n           shell (str): one of \'ipython\', \'pdb\'\n        """"""\n        self._file = file\n        assert shell in [\'ipython\', \'pdb\']\n        self._shell = shell\n        logger.info(""Create a file \'{}\' to open {} shell."".format(file, shell))\n\n    def _trigger(self):\n        if os.path.isfile(self._file):\n            logger.info(""File {} exists, entering shell."".format(self._file))\n            self._inject()\n\n    def _inject(self):\n        trainer = self.trainer   # noqa\n        if self._shell == \'ipython\':\n            import IPython as IP    # noqa\n            IP.embed()\n        elif self._shell == \'pdb\':\n            import pdb   # noqa\n            pdb.set_trace()\n\n    def _after_train(self):\n        if os.path.isfile(self._file):\n            os.unlink(self._file)\n\n\nclass EstimatedTimeLeft(Callback):\n    """"""\n    Estimate the time left until completion of training.\n    """"""\n    def __init__(self, last_k_epochs=5, median=True):\n        """"""\n        Args:\n            last_k_epochs (int): Use the time spent on last k epochs to estimate total time left.\n            median (bool): Use the mean or median time spent on last k epochs.\n        """"""\n        self._times = deque(maxlen=last_k_epochs)\n        self._median = median\n\n    def _before_train(self):\n        self._max_epoch = self.trainer.max_epoch\n        self._last_time = time.time()\n\n    def _trigger_epoch(self):\n        duration = time.time() - self._last_time\n        self._last_time = time.time()\n        self._times.append(duration)\n\n        epoch_time = np.median(self._times) if self._median else np.mean(self._times)\n        time_left = (self._max_epoch - self.epoch_num) * epoch_time\n        if time_left > 0:\n            logger.info(""Estimated Time Left: "" + humanize_time_delta(time_left))\n'"
tensorpack/callbacks/monitor.py,14,"b'# -*- coding: utf-8 -*-\n# File: monitor.py\n\n\nimport json\nimport numpy as np\nimport operator\nimport os\nimport re\nimport shutil\nimport time\nfrom collections import defaultdict\nfrom datetime import datetime\nimport six\nimport threading\n\nfrom ..compat import tfv1 as tf\nfrom ..libinfo import __git_version__\nfrom ..tfutils.summary import create_image_summary, create_scalar_summary\nfrom ..utils import fs, logger\nfrom ..utils.develop import HIDE_DOC\nfrom .base import Callback\n\n__all__ = [\'MonitorBase\', \'Monitors\',\n           \'TFEventWriter\', \'JSONWriter\',\n           \'ScalarPrinter\', \'SendMonitorData\',\n           \'CometMLMonitor\']\n\n\ndef image_to_nhwc(arr):\n    if arr.ndim == 4:\n        pass\n    elif arr.ndim == 3:\n        if arr.shape[-1] in [1, 3, 4]:\n            arr = arr[np.newaxis, :]\n        else:\n            arr = arr[:, :, :, np.newaxis]\n    elif arr.ndim == 2:\n        arr = arr[np.newaxis, :, :, np.newaxis]\n    else:\n        raise ValueError(""Array of shape {} is not an image!"".format(arr.shape))\n    return arr\n\n\nclass MonitorBase(Callback):\n    """"""\n    Base class for monitors which monitor a training progress, by processing different types of\n    summary/statistics from trainer.\n\n    .. document private functions\n    .. automethod:: _setup_graph\n    """"""\n\n    _chief_only = False\n\n    def setup_graph(self, trainer):\n        # Set attributes following Callback.setup_graph\n        self.trainer = trainer\n        self.graph = tf.get_default_graph()\n        self._setup_graph()\n\n    def _setup_graph(self):\n        """""" Override this method to setup the monitor.""""""\n        pass\n\n    def process_summary(self, summary):\n        """"""\n        Process a tf.Summary.\n        """"""\n        pass\n\n    def process(self, name, val):\n        """"""\n        Process a key-value pair.\n        """"""\n        pass\n\n    def process_scalar(self, name, val):\n        """"""\n        Args:\n            val: a scalar\n        """"""\n        pass\n\n    def process_image(self, name, val):\n        """"""\n        Args:\n            val (np.ndarray): 4D (NHWC) numpy array of images in range [0,255].\n                If channel is 3, assumed to be RGB.\n        """"""\n        pass\n\n    def process_event(self, evt):\n        """"""\n        Args:\n            evt (tf.Event): the most basic format acceptable by tensorboard.\n                It could include Summary, RunMetadata, LogMessage, and more.\n        """"""\n        pass\n    # TODO process other types\n\n\nclass NoOpMonitor(MonitorBase):\n    def __init__(self, name=None):\n        self._name = name\n\n    def __str__(self):\n        if self._name is None:\n            return ""NoOpMonitor""\n        return ""NoOpMonitor({})"".format(self._name)\n\n\nclass Monitors(Callback):\n    """"""\n    Merge monitors together for trainer to use.\n\n    In training, each trainer will create a :class:`Monitors` instance,\n    and you can access it through ``trainer.monitors``.\n    You should use ``trainer.monitors`` for logging and it will dispatch your\n    logs to each sub-monitor.\n    """"""\n\n    _chief_only = False\n\n    def __init__(self, monitors):\n        self._scalar_history = ScalarHistory()\n        self._monitors = monitors + [self._scalar_history]\n        for m in self._monitors:\n            assert isinstance(m, MonitorBase), m\n\n    def _setup_graph(self):\n        # scalar_history\'s other methods were not called.\n        # but they are not useful for now\n        self._scalar_history.setup_graph(self.trainer)\n\n    def _dispatch(self, func):\n        for m in self._monitors:\n            func(m)\n\n    def put_summary(self, summary):\n        """"""\n        Put a `tf.Summary`.\n        """"""\n        if isinstance(summary, six.binary_type):\n            summary = tf.Summary.FromString(summary)\n        assert isinstance(summary, tf.Summary), type(summary)\n\n        # TODO other types\n        for val in summary.value:\n            if val.WhichOneof(\'value\') == \'simple_value\':\n                val.tag = re.sub(\'tower[0-9]+/\', \'\', val.tag)   # TODO move to subclasses\n\n                # TODO This hack is still needed, seem to disappear only when\n                # compiled from source.\n                suffix = \'-summary\'  # tensorflow#6150, tensorboard#59\n                if val.tag.endswith(suffix):\n                    val.tag = val.tag[:-len(suffix)]\n\n                self._dispatch(lambda m: m.process_scalar(val.tag, val.simple_value))\n\n        self._dispatch(lambda m: m.process_summary(summary))\n\n    def put_scalar(self, name, val):\n        """"""\n        Put a scalar.\n        """"""\n        if isinstance(val, np.floating):\n            val = float(val)\n        if isinstance(val, np.integer):\n            val = int(val)\n        self._dispatch(lambda m: m.process_scalar(name, val))\n        s = create_scalar_summary(name, val)\n        self._dispatch(lambda m: m.process_summary(s))\n\n    def put_image(self, name, val):\n        """"""\n        Put an image.\n\n        Args:\n            name (str):\n            val (np.ndarray): 2D, 3D (HWC) or 4D (NHWC) numpy array of images\n                in range [0,255]. If channel is 3, assumed to be RGB.\n        """"""\n        assert isinstance(val, np.ndarray)\n        arr = image_to_nhwc(val)\n        self._dispatch(lambda m: m.process_image(name, arr))\n        s = create_image_summary(name, arr)\n        self._dispatch(lambda m: m.process_summary(s))\n\n    def put_event(self, evt):\n        """"""\n        Put an :class:`tf.Event`.\n        `step` and `wall_time` fields of :class:`tf.Event` will be filled automatically.\n\n        Args:\n            evt (tf.Event):\n        """"""\n        evt.step = self.global_step\n        evt.wall_time = time.time()\n        self._dispatch(lambda m: m.process_event(evt))\n\n    def get_latest(self, name):\n        """"""\n        Get latest scalar value of some data.\n\n        If you run multiprocess training, keep in mind that\n        the data is perhaps only available on chief process.\n\n        Returns:\n            scalar\n        """"""\n        return self._scalar_history.get_latest(name)[1]\n\n    def get_history(self, name):\n        """"""\n        Get a history of the scalar value of some data.\n\n        If you run multiprocess training, keep in mind that\n        the data is perhaps only available on chief process.\n\n        Returns:\n            a list of (global_step, value) pairs: history data for this scalar\n        """"""\n        return self._scalar_history.get_history(name)\n\n\nclass TFEventWriter(MonitorBase):\n    """"""\n    Write summaries to TensorFlow event file.\n    """"""\n    def __init__(self, logdir=None, max_queue=10, flush_secs=120, split_files=False):\n        """"""\n        Args:\n            logdir: ``logger.get_logger_dir()`` by default.\n            max_queue, flush_secs: Same as in :class:`tf.summary.FileWriter`.\n            split_files: if True, split events to multiple files rather than\n                append to a single file. Useful on certain filesystems where append is expensive.\n        """"""\n        if logdir is None:\n            logdir = logger.get_logger_dir()\n        assert tf.gfile.IsDirectory(logdir), logdir\n        self._logdir = fs.normpath(logdir)\n        self._max_queue = max_queue\n        self._flush_secs = flush_secs\n        self._split_files = split_files\n\n    def __new__(cls, logdir=None, max_queue=10, flush_secs=120, **kwargs):\n        if logdir is None:\n            logdir = logger.get_logger_dir()\n\n        if logdir is not None:\n            return super(TFEventWriter, cls).__new__(cls)\n        else:\n            logger.warn(""logger directory was not set. Ignore TFEventWriter."")\n            return NoOpMonitor(""TFEventWriter"")\n\n    def _setup_graph(self):\n        self._writer = tf.summary.FileWriter(\n            self._logdir, max_queue=self._max_queue, flush_secs=self._flush_secs)\n\n    def _write_graph(self):\n        self._writer.add_graph(self.graph)\n\n    def _before_train(self):\n        # Writing the graph is expensive (takes ~2min) when the graph is large.\n        # Therefore use a separate thread. It will then run in the\n        # background while TF is warming up in the first several iterations.\n        self._write_graph_thread = threading.Thread(target=self._write_graph)\n        self._write_graph_thread.daemon = True\n        self._write_graph_thread.start()\n\n    @HIDE_DOC\n    def process_summary(self, summary):\n        self._writer.add_summary(summary, self.global_step)\n\n    @HIDE_DOC\n    def process_event(self, evt):\n        self._writer.add_event(evt)\n\n    def _trigger(self):     # flush every epoch\n        self._writer.flush()\n        if self._split_files:\n            self._writer.close()\n            self._writer.reopen()  # open new file\n\n    def _after_train(self):\n        self._writer.close()\n\n\nclass JSONWriter(MonitorBase):\n    """"""\n    Write all scalar data to a json file under ``logger.get_logger_dir()``, grouped by their global step.\n    If found an earlier json history file, will append to it.\n    """"""\n\n    FILENAME = \'stats.json\'\n    """"""\n    The name of the json file. Do not change it.\n    """"""\n\n    def __new__(cls):\n        if logger.get_logger_dir():\n            return super(JSONWriter, cls).__new__(cls)\n        else:\n            logger.warn(""logger directory was not set. Ignore JSONWriter."")\n            return NoOpMonitor(""JSONWriter"")\n\n    @staticmethod\n    def load_existing_json(dir=None):\n        """"""\n        Look for an existing json under dir (defaults to\n        :meth:`logger.get_logger_dir()`) named ""stats.json"",\n        and return the loaded list of statistics if found. Returns None otherwise.\n        """"""\n        if dir is None:\n            dir = logger.get_logger_dir()\n        fname = os.path.join(dir, JSONWriter.FILENAME)\n        if tf.gfile.Exists(fname):\n            with open(fname) as f:\n                stats = json.load(f)\n                assert isinstance(stats, list), type(stats)\n                return stats\n        return None\n\n    @staticmethod\n    def load_existing_epoch_number(dir=None):\n        """"""\n        Try to load the latest epoch number from an existing json stats file (if any).\n        Returns None if not found.\n        """"""\n        stats = JSONWriter.load_existing_json(dir)\n        try:\n            return int(stats[-1][\'epoch_num\'])\n        except Exception:\n            return None\n\n    # initialize the stats here, because before_train from other callbacks may use it\n    def _setup_graph(self):\n        self._stats = []\n        self._stat_now = {}\n        self._last_gs = -1\n\n    def _before_train(self):\n        stats = JSONWriter.load_existing_json()\n        self._fname = os.path.join(logger.get_logger_dir(), JSONWriter.FILENAME)\n        if stats is not None:\n            try:\n                epoch = stats[-1][\'epoch_num\'] + 1\n            except Exception:\n                epoch = None\n\n            # check against the current training settings\n            # therefore this logic needs to be in before_train stage\n            starting_epoch = self.trainer.loop.starting_epoch\n            if epoch is None or epoch == starting_epoch:\n                logger.info(""Found existing JSON inside {}, will append to it."".format(logger.get_logger_dir()))\n                self._stats = stats\n            else:\n                logger.warn(\n                    ""History epoch={} from JSON is not the predecessor of the current starting_epoch={}"".format(\n                        epoch - 1, starting_epoch))\n                logger.warn(""If you want to resume old training, either use `AutoResumeTrainConfig` ""\n                            ""or correctly set the new starting_epoch yourself to avoid inconsistency. "")\n\n                backup_fname = JSONWriter.FILENAME + \'.\' + datetime.now().strftime(\'%m%d-%H%M%S\')\n                backup_fname = os.path.join(logger.get_logger_dir(), backup_fname)\n\n                logger.warn(""Now, we will train with starting_epoch={} and backup old json to {}"".format(\n                    self.trainer.loop.starting_epoch, backup_fname))\n                shutil.move(self._fname, backup_fname)\n\n        # in case we have something to log here.\n        self._trigger()\n\n    def _trigger_step(self):\n        # will do this in trigger_epoch\n        if self.local_step != self.trainer.steps_per_epoch - 1:\n            self._trigger()\n\n    def _trigger_epoch(self):\n        self._trigger()\n\n    @HIDE_DOC\n    def process_scalar(self, name, val):\n        self._stat_now[name] = val\n\n    def _trigger(self):\n        """"""\n        Add stats to json and dump to disk.\n        Note that this method is idempotent.\n        """"""\n        if len(self._stat_now):\n            self._stat_now[\'epoch_num\'] = self.epoch_num\n            self._stat_now[\'global_step\'] = self.global_step\n\n            self._stats.append(self._stat_now)\n            self._stat_now = {}\n            self._write_stat()\n\n    def _write_stat(self):\n        tmp_filename = self._fname + \'.tmp\'\n        try:\n            with open(tmp_filename, \'w\') as f:\n                json.dump(self._stats, f)\n            shutil.move(tmp_filename, self._fname)\n        except IOError:  # disk error sometimes..\n            logger.exception(""Exception in JSONWriter._write_stat()!"")\n\n\nclass ScalarPrinter(MonitorBase):\n    """"""\n    Print scalar data into terminal.\n    """"""\n\n    def __init__(self, enable_step=False, enable_epoch=True,\n                 whitelist=None, blacklist=None):\n        """"""\n        Args:\n            enable_step, enable_epoch (bool): whether to print the\n                monitor data (if any) between steps or between epochs.\n            whitelist (list[str] or None): A list of regex. Only names\n                matching some regex will be allowed for printing.\n                Defaults to match all names.\n            blacklist (list[str] or None): A list of regex. Names matching\n                any regex will not be printed. Defaults to match no names.\n        """"""\n        def compile_regex(rs):\n            if rs is None:\n                return None\n            rs = {re.compile(r) for r in rs}\n            return rs\n\n        self._whitelist = compile_regex(whitelist)\n        if blacklist is None:\n            blacklist = []\n        self._blacklist = compile_regex(blacklist)\n\n        self._enable_step = enable_step\n        self._enable_epoch = enable_epoch\n        self._dic = {}\n\n    # in case we have something to log here.\n    def _before_train(self):\n        self._trigger()\n\n    def _trigger_step(self):\n        if self._enable_step:\n            if self.local_step != self.trainer.steps_per_epoch - 1:\n                # not the last step\n                self._trigger()\n            else:\n                if not self._enable_epoch:\n                    self._trigger()\n                # otherwise, will print them together\n\n    def _trigger_epoch(self):\n        if self._enable_epoch:\n            self._trigger()\n\n    @HIDE_DOC\n    def process_scalar(self, name, val):\n        self._dic[name] = float(val)\n\n    def _trigger(self):\n        # Print stats here\n        def match_regex_list(regexs, name):\n            for r in regexs:\n                if r.search(name) is not None:\n                    return True\n            return False\n\n        for k, v in sorted(self._dic.items(), key=operator.itemgetter(0)):\n            if self._whitelist is None or \\\n                    match_regex_list(self._whitelist, k):\n                if not match_regex_list(self._blacklist, k):\n                    logger.info(\'{}: {:.5g}\'.format(k, v))\n        self._dic = {}\n\n\nclass ScalarHistory(MonitorBase):\n    """"""\n    Only internally used by monitors.\n    """"""\n\n    def __init__(self):\n        self._dic = defaultdict(list)\n\n    @HIDE_DOC\n    def process_scalar(self, name, val):\n        self._dic[name].append((self.global_step, float(val)))\n\n    def get_latest(self, name):\n        hist = self._dic[name]\n        if len(hist) == 0:\n            raise KeyError(""No available data for the key: {}"".format(name))\n        else:\n            return hist[-1]\n\n    def get_history(self, name):\n        return self._dic[name]\n\n\nclass SendMonitorData(MonitorBase):\n    """"""\n    Execute a command with some specific scalar monitor data.\n    This is useful for, e.g. building a custom statistics monitor.\n\n    It will try to send once receiving all the stats\n    """"""\n    def __init__(self, command, names):\n        """"""\n        Args:\n            command(str): a command to execute. Use format string with stat\n                names as keys.\n            names(list or str): data name(s) to use.\n\n        Example:\n            Send the stats to your phone through pushbullet:\n\n            .. code-block:: python\n\n                SendMonitorData(\'curl -u your_id: https://api.pushbullet.com/v2/pushes \\\\\n                         -d type=note -d title=""validation error"" \\\\\n                         -d body={validation_error} > /dev/null 2>&1\',\n                         \'validation_error\')\n        """"""\n        self.command = command\n        if not isinstance(names, list):\n            names = [names]\n        self.names = names\n        self.dic = {}\n\n    @HIDE_DOC\n    def process_scalar(self, name, val):\n        if name in self.names:\n            self.dic[name] = val\n\n    def _trigger_step(self):\n        self._trigger()\n\n    def _trigger(self):\n        try:\n            v = {k: self.dic[k] for k in self.names}\n        except KeyError:\n            return\n        cmd = self.command.format(**v)\n        ret = os.system(cmd)\n        if ret != 0:\n            logger.error(""Command \'{}\' failed with ret={}!"".format(cmd, ret))\n        self.dic = {}\n\n\nclass CometMLMonitor(MonitorBase):\n    """"""\n    Send scalar data and the graph to https://www.comet.ml.\n\n    Note:\n        1. comet_ml requires you to `import comet_ml` before importing tensorflow or tensorpack.\n        2. The ""automatic output logging"" feature of comet_ml will make the training progress bar appear to freeze.\n           Therefore the feature is disabled by default.\n    """"""\n    def __init__(self, experiment=None, tags=None, **kwargs):\n        """"""\n        Args:\n            experiment (comet_ml.Experiment): if provided, invalidate all other arguments\n            tags (list[str]): experiment tags\n            kwargs: arguments used to initialize :class:`comet_ml.Experiment`,\n                such as project name, API key, etc.\n                Refer to its documentation for details.\n        """"""\n        if experiment is not None:\n            self._exp = experiment\n            assert tags is None and len(kwargs) == 0\n        else:\n            from comet_ml import Experiment\n            kwargs.setdefault(\'log_code\', True)  # though it\'s not functioning, git patch logging requires it\n            kwargs.setdefault(\'auto_output_logging\', None)\n            self._exp = Experiment(**kwargs)\n            if tags is not None:\n                self._exp.add_tags(tags)\n\n        self._exp.set_code(""Code logging is impossible ..."")\n        self._exp.log_dependency(\'tensorpack\', __git_version__)\n\n    @property\n    def experiment(self):\n        """"""\n        The :class:`comet_ml.Experiment` instance.\n        """"""\n        return self._exp\n\n    def _before_train(self):\n        self._exp.set_model_graph(tf.get_default_graph())\n\n    @HIDE_DOC\n    def process_scalar(self, name, val):\n        self._exp.log_metric(name, val, step=self.global_step)\n\n    @HIDE_DOC\n    def process_image(self, name, val):\n        self._exp.set_step(self.global_step)\n        for idx, v in enumerate(val):\n            log_name = ""{}_step{}{}"".format(\n                name,\n                self.global_step,\n                ""_"" + str(idx) if len(val) > 1 else """")\n\n            self._exp.log_image(v, image_format=""jpeg"", name=log_name, image_minmax=(0, 255))\n\n    def _after_train(self):\n        self._exp.end()\n\n    def _after_epoch(self):\n        self._exp.log_epoch_end(self.epoch_num)\n'"
tensorpack/callbacks/param.py,0,"b'# -*- coding: utf-8 -*-\n# File: param.py\n\n\nimport operator\nimport os\nimport numpy as np\nfrom abc import ABCMeta, abstractmethod\nfrom collections import deque\nimport six\n\nfrom ..compat import tfv1\nfrom ..tfutils.common import get_op_tensor_name\nfrom ..utils import logger\nfrom .base import Callback\n\n__all__ = [\'HyperParam\', \'GraphVarParam\', \'ObjAttrParam\',\n           \'HyperParamSetter\', \'HumanHyperParamSetter\',\n           \'ScheduledHyperParamSetter\',\n           \'StatMonitorParamSetter\', \'HyperParamSetterWithFunc\',\n           ]\n\n\n@six.add_metaclass(ABCMeta)\nclass HyperParam(object):\n    """""" Base class for a hyperparam. """"""\n\n    def setup_graph(self):\n        """""" setup the graph in ``setup_graph`` callback stage, if necessary""""""\n        pass\n\n    @abstractmethod\n    def set_value(self, v):\n        """"""\n        Set the value of the param.\n\n        Args:\n            v: the value to be set\n        """"""\n        pass\n\n    @abstractmethod\n    def get_value(self):\n        """"""\n        Get the value of the param.\n        """"""\n        pass\n\n    @property\n    def readable_name(self):\n        """""" A name to display """"""\n        return self._readable_name\n\n\nclass GraphVarParam(HyperParam):\n    """""" A variable in the graph (e.g. learning_rate) can be a hyperparam.""""""\n\n    def __init__(self, name, shape=()):\n        """"""\n        Args:\n            name(str): name of the variable.\n            shape(tuple): shape of the variable.\n        """"""\n        self.name = name\n        self.shape = shape\n        self._readable_name, self.var_name = get_op_tensor_name(name)\n\n    def setup_graph(self):\n        """""" Will setup the assign operator for that variable. """"""\n        all_vars = tfv1.global_variables() + tfv1.local_variables()\n        for v in all_vars:\n            if v.name == self.var_name:\n                self.var = v\n                break\n        else:\n            raise ValueError(""{} is not a variable in the graph!"".format(self.var_name))\n\n    def set_value(self, v):\n        """""" Assign the variable a new value. """"""\n        if not self.var.dtype.is_floating and isinstance(v, float):\n            raise ValueError(\n                ""HyperParam {} has type \'{}\'. Cannot update it using float values."".format(\n                    self.name, self.var.dtype))\n        self.var.load(v)\n\n    def get_value(self):\n        """""" Evaluate the variable. """"""\n        return self.var.eval()\n\n\nclass ObjAttrParam(HyperParam):\n    """""" An attribute of an object can be a hyperparam. """"""\n\n    def __init__(self, obj, attrname, readable_name=None):\n        """"""\n        Args:\n            obj: the object\n            attrname (str): the attribute\n            readable_name(str): The name to display and set with. Defaults to be ``attrname``.\n        """"""\n        self.obj = obj\n        self.attrname = attrname\n        if readable_name is None:\n            self._readable_name = attrname\n        else:\n            self._readable_name = readable_name\n\n    def set_value(self, v):\n        setattr(self.obj, self.attrname, v)\n\n    def get_value(self):\n        return getattr(self.obj, self.attrname)\n\n\nclass HyperParamSetter(Callback):\n    """"""\n    An abstract base callback to set hyperparameters.\n\n    Once the :meth:`trigger()` method is called,\n    the method :meth:`_get_value_to_set` will be used to get a new value for the hyperparameter.\n    """"""\n\n    _chief_only = False\n\n    """"""\n    Also enable this hyperparam setter in the :meth:`before_train` method.\n    """"""\n    _enable_before_train = True\n\n    def __init__(self, param):\n        """"""\n        Args:\n            param(HyperParam or str): if is a :class:`str`, it is assumed to\n                be a :class:`GraphVarParam`.\n        """"""\n        # if a string, assumed to be a scalar graph variable\n        if isinstance(param, six.string_types):\n            param = GraphVarParam(param)\n        assert isinstance(param, HyperParam), type(param)\n        self.param = param\n        self._last_value = None\n        self._last_epoch_set = -1\n\n    def _setup_graph(self):\n        self.param.setup_graph()\n\n    def get_value_to_set(self):\n        """"""\n        Returns:\n            The value to assign to the variable.\n\n        Note:\n            Subclasses will implement the abstract method\n            :meth:`_get_value_to_set`, which should return a new value to\n            set, or return None to do nothing.\n        """"""\n        ret = self._get_value_to_set()\n        if ret is not None and ret != self._last_value:\n            if self.epoch_num != self._last_epoch_set:  # Print this message at most once every epoch\n                if self._last_value is None:\n                    logger.info(""[HyperParamSetter] At global_step={}, {} is set to {:.6f}"".format(\n                        self.global_step, self.param.readable_name, ret))\n                else:\n                    logger.info(""[HyperParamSetter] At global_step={}, {} changes from {:.6f} to {:.6f}"".format(\n                        self.global_step, self.param.readable_name, self._last_value, ret))\n            self._last_epoch_set = self.epoch_num\n            self._last_value = ret\n        return ret\n\n    @abstractmethod\n    def _get_value_to_set(self):\n        pass\n\n    def get_current_value(self):\n        """"""\n        Returns:\n            The current value of the param.\n        """"""\n        return self.param.get_value()\n\n    def _trigger(self):\n        self._set_param()\n\n    def _before_train(self):\n        if self._enable_before_train:\n            self._set_param()\n\n    def _set_param(self):\n        v = self.get_value_to_set()\n        if v is not None:\n            self.param.set_value(v)\n\n\nclass HumanHyperParamSetter(HyperParamSetter):\n    """"""\n    Set hyperparameter by loading the value from a file each time it get called.\n    This is useful for manually tuning some parameters (e.g. learning_rate)\n    without interrupting the training.\n    """"""\n\n    def __init__(self, param, file_name=\'hyper.txt\'):\n        """"""\n        Args:\n            param: same as in :class:`HyperParamSetter`.\n            file_name(str): a file containing the new value of the parameter.\n                Each line in the file is a ``k:v`` pair, for example, ``learning_rate:1e-4``.\n                If the pair is not found, the param will not be changed.\n        """"""\n        super(HumanHyperParamSetter, self).__init__(param)\n        self.file_name = os.path.join(logger.get_logger_dir(), file_name)\n        logger.info(""Use {} to set hyperparam: \'{}\'."".format(\n            self.file_name, self.param.readable_name))\n\n    def _get_value_to_set(self):\n        # ignore if no such file exists\n        if not os.path.isfile(self.file_name):\n            return None\n        try:\n            with open(self.file_name) as f:\n                lines = f.readlines()\n            lines = [s.strip().split(\':\') for s in lines]\n            dic = {str(k): float(v) for k, v in lines}\n            ret = dic[self.param.readable_name]\n            return ret\n        except Exception:\n            logger.warn(\n                ""Cannot find {} in {}"".format(\n                    self.param.readable_name, self.file_name))\n            return None\n\n\nclass ScheduledHyperParamSetter(HyperParamSetter):\n    """"""\n    Set hyperparameters by a predefined epoch-based schedule.\n    """"""\n\n    def __init__(self, param, schedule, interp=None, step_based=False):\n        """"""\n        Args:\n            param: same as in :class:`HyperParamSetter`.\n            schedule (list): with the format ``[(epoch1, val1), (epoch2, val2), (epoch3, val3)]``.\n                Each ``(ep, val)`` pair means to set the param\n                to ""val"" **after** the completion of epoch `ep`.\n                If ep == 0, the value will be set before the first epoch\n                (because by default the first is epoch 1).\n                The epoch numbers have to be increasing.\n            interp (str or None): Either None or \'linear\'.\n                If None, the parameter will only be set when the specific epoch or steps\n                is reached exactly. If \'linear\', perform linear interpolation (but no extrapolation)\n                every time this callback is triggered.\n            step_based (bool): interpret ``schedule`` as (step, value) instead\n                of (epoch, value).\n\n        Example:\n            .. code-block:: python\n\n                ScheduledHyperParamSetter(\'learning_rate\',\n                                          [(30, 1e-2), (60, 1e-3), (85, 1e-4), (95, 1e-5)]),\n        """"""\n        schedule = [(int(a), float(b)) for a, b in schedule]\n        self.schedule = sorted(schedule, key=operator.itemgetter(0))\n        if interp is not None:\n            assert interp == \'linear\'\n        self.interp = interp\n        self._step = step_based\n        super(ScheduledHyperParamSetter, self).__init__(param)\n\n    def _get_value_to_set(self):  # override parent\n        return self._get_value_to_set_at_point(self._current_point())\n\n    def _current_point(self):\n        return self.global_step if self._step else self.epoch_num\n\n    def _check_value_at_beginning(self):\n        v = None\n        # we are at `before_train`, therefore the epoch/step associated with `current_point` has finished.\n        for p in range(0, self._current_point() + 1):\n            v = self._get_value_to_set_at_point(p) or v\n        actual_value = self.param.get_value()\n        if v is not None and not np.isclose(v, actual_value):\n            logger.warn(""According to scheduler {}, parameter \'{}\' should become {} at the current point. ""\n                        ""However its current value is {}. ""\n                        ""If this is the only scheduler being used, you may want to check whether your ""\n                        ""initialization of the parameter is as expected"".format(\n                            self, self.param.readable_name, v, actual_value))\n\n    def _get_value_to_set_at_point(self, point):\n        """"""\n        Using schedule, compute the value to be set at a given point.\n        """"""\n        laste, lastv = None, None\n        for e, v in self.schedule:\n            if e == point:\n                return v    # meet the exact boundary, return directly\n            if e > point:\n                break\n            laste, lastv = e, v\n        if laste is None or laste == e:\n            # hasn\'t reached the first scheduled point, or reached the end of all scheduled points\n            return None\n        if self.interp is None:\n            # If no interpolation, nothing to do.\n            return None\n        v = (point - laste) * 1. / (e - laste) * (v - lastv) + lastv\n        return v\n\n    def _before_train(self):\n        super(ScheduledHyperParamSetter, self)._before_train()\n        self._check_value_at_beginning()\n\n    def _trigger_epoch(self):\n        if not self._step:\n            self.trigger()\n\n    def _trigger_step(self):\n        if self._step:\n            self.trigger()\n\n    def __str__(self):\n        return ""ScheduledHyperParamSetter(schedule={})"".format(self.schedule)\n\n\nclass HyperParamSetterWithFunc(HyperParamSetter):\n    """""" Set the parameter by a function of epoch num and old value. """"""\n    def __init__(self, param, func):\n        """"""\n        Args:\n            param: same as in :class:`HyperParamSetter`.\n            func: ``param`` will be set by ``new_value = func(epoch_num, old_value)``.\n                ``epoch_num`` is the number of epochs that have finished.\n\n        Example:\n            Decrease by a factor of 0.9 every two epochs:\n\n            .. code-block:: python\n\n                HyperParamSetterWithFunc(\'learning_rate\',\n                                         lambda e, x: x * 0.9 if e % 2 == 0 else x)\n        """"""\n        super(HyperParamSetterWithFunc, self).__init__(param)\n        self.f = func\n\n    def _get_value_to_set(self):\n        return self.f(self.epoch_num, self.get_current_value())\n\n\nclass StatMonitorParamSetter(HyperParamSetter):\n    """"""\n    Change the param by monitoring the change of a scalar statistics.\n    The param will be changed when the scalar does not decrease/increase enough.\n\n    Once triggered, this callback observes the latest **one** value of ``stat_name``, from the monitor backend.\n\n    This callback will then change a hyperparameter ``param`` by ``new_value = value_func(old_value)``, if:\n    ``min(history) >= history[0] - threshold``, where\n    ``history = [the most recent k observations of stat_name]``\n\n    Note:\n        The statistics of interest must be created at a frequency higher than or equal to this callback.\n        For example, using ``PeriodicTrigger(StatMonitorParamSetter(...), every_k_steps=100)``\n        is meaningless if the statistics to be monitored is only updated every 500 steps.\n\n        Callbacks are executed in order. Therefore, if the statistics to be monitored\n        is created after this callback, the behavior of this callback may get delayed.\n\n    Example:\n\n        If validation error wasn\'t decreasing for 5 epochs, decay the learning rate by 0.2:\n\n        .. code-block:: python\n\n            StatMonitorParamSetter(\'learning_rate\', \'val-error\',\n                                    lambda x: x * 0.2, threshold=0, last_k=5)\n    """"""\n\n    _enable_before_train = False\n\n    def __init__(self, param, stat_name, value_func, threshold,\n                 last_k, reverse=False):\n        """"""\n        Args:\n            param: same as in :class:`HyperParamSetter`.\n            stat_name (str): name of the statistics.\n            value_func (float -> float): a function which returns a new value\n                taking the old value.\n            threshold (float): change threshold.\n            last_k (int): use last k observations of statistics.\n            reverse (bool): monitor increasing instead of decreasing.\n                If True, ``param`` will be changed when ``max(history) <= history[0] + threshold``.\n        """"""\n        super(StatMonitorParamSetter, self).__init__(param)\n        self.stat_name = stat_name\n        self.value_func = value_func\n        self.history = deque(maxlen=last_k)\n        self.threshold = threshold\n        self.reverse = reverse\n\n    def _get_value_to_set(self):\n        try:\n            last = self.trainer.monitors.get_history(self.stat_name)[-1]\n        except (KeyError, IndexError):\n            logger.warn(\n                ""[StatMonitorParamSetter] No history data available for key \'{}\'."".format(self.stat_name))\n            return None\n        if len(self.history) and last[0] == self.history[-1][0]:\n            logger.warn(""StatMonitorParamSetter is triggered, but no new data has been added since last time."")\n            return None\n\n        self.history.append(last)\n\n        if len(self.history) < self.history.maxlen:\n            return None\n\n        values = [k[1] for k in self.history]\n        hist_first = values[0]\n        if not self.reverse:\n            hist_min = min(values)\n            if hist_min < hist_first - self.threshold:  # small enough\n                return None\n        else:\n            hist_max = max(values)\n            if hist_max > hist_first + self.threshold:  # large enough\n                return None\n        self.history.clear()\n        logger.info(\n            ""[StatMonitorParamSetter] Triggered, history of {}: "".format(\n                self.stat_name) + \',\'.join([str(round(x, 3)) for x in values]))\n        return self.value_func(self.get_current_value())\n'"
tensorpack/callbacks/param_test.py,2,"b'# -*- coding: utf-8 -*-\nimport unittest\nimport tensorflow as tf\n\nfrom ..utils import logger\nfrom ..train.trainers import NoOpTrainer\nfrom .param import ScheduledHyperParamSetter, ObjAttrParam\n\n\nclass ParamObject(object):\n    """"""\n    An object that holds the param to be set, for testing purposes.\n    """"""\n    PARAM_NAME = \'param\'\n\n    def __init__(self):\n        self.param_history = {}\n        self.__dict__[self.PARAM_NAME] = 1.0\n\n    def __setattr__(self, name, value):\n        if name == self.PARAM_NAME:\n            self._set_param(value)\n        super(ParamObject, self).__setattr__(name, value)\n\n    def _set_param(self, value):\n        self.param_history[self.trainer.global_step] = value\n\n\nclass ScheduledHyperParamSetterTest(unittest.TestCase):\n    def setUp(self):\n        self._param_obj = ParamObject()\n\n    def tearDown(self):\n        tf.reset_default_graph()\n\n    def _create_trainer_with_scheduler(self, scheduler,\n                                       steps_per_epoch, max_epoch, starting_epoch=1):\n        trainer = NoOpTrainer()\n        tf.get_variable(name=\'test_var\', shape=[])\n        self._param_obj.trainer = trainer\n        trainer.train_with_defaults(\n            callbacks=[scheduler],\n            extra_callbacks=[],\n            monitors=[],\n            steps_per_epoch=steps_per_epoch,\n            max_epoch=max_epoch,\n            starting_epoch=starting_epoch\n        )\n        return self._param_obj.param_history\n\n    def testInterpolation(self):\n        scheduler = ScheduledHyperParamSetter(\n            ObjAttrParam(self._param_obj, ParamObject.PARAM_NAME),\n            [(30, 0.3), (40, 0.4), (50, 0.5)], interp=\'linear\', step_based=True)\n        history = self._create_trainer_with_scheduler(scheduler, 10, 50, starting_epoch=20)\n        self.assertEqual(min(history.keys()), 30)\n        self.assertEqual(history[30], 0.3)\n        self.assertEqual(history[40], 0.4)\n        self.assertEqual(history[45], 0.45)\n\n    def testSchedule(self):\n        scheduler = ScheduledHyperParamSetter(\n            ObjAttrParam(self._param_obj, ParamObject.PARAM_NAME),\n            [(10, 0.3), (20, 0.4), (30, 0.5)])\n        history = self._create_trainer_with_scheduler(scheduler, 1, 50)\n        self.assertEqual(min(history.keys()), 10)\n        self.assertEqual(len(history), 3)\n\n    def testStartAfterSchedule(self):\n        scheduler = ScheduledHyperParamSetter(\n            ObjAttrParam(self._param_obj, ParamObject.PARAM_NAME),\n            [(10, 0.3), (20, 0.4), (30, 0.5)])\n        history = self._create_trainer_with_scheduler(scheduler, 1, 92, starting_epoch=90)\n        self.assertEqual(len(history), 0)\n\n    def testWarningStartInTheMiddle(self):\n        scheduler = ScheduledHyperParamSetter(\n            ObjAttrParam(self._param_obj, ParamObject.PARAM_NAME),\n            [(10, 0.3), (20, 0.4), (30, 0.5)])\n        with self.assertLogs(logger=logger._logger, level=\'WARNING\'):\n            self._create_trainer_with_scheduler(scheduler, 1, 21, starting_epoch=20)\n\n    def testNoWarningStartInTheMiddle(self):\n        scheduler = ScheduledHyperParamSetter(\n            ObjAttrParam(self._param_obj, ParamObject.PARAM_NAME),\n            [(10, 0.3), (20, 1.0), (30, 1.5)])\n        with unittest.mock.patch(\'tensorpack.utils.logger.warning\') as warning:\n            self._create_trainer_with_scheduler(scheduler, 1, 22, starting_epoch=21)\n        self.assertFalse(warning.called)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tensorpack/callbacks/prof.py,8,"b'# -*- coding: utf-8 -*-\n# File: prof.py\n\n\nimport multiprocessing as mp\nimport numpy as np\nimport os\nimport time\nimport tensorflow as tf\nfrom six.moves import map, queue\nimport psutil\n\nfrom ..tfutils.common import gpu_available_in_session\nfrom ..utils import logger\nfrom ..utils.timer import Timer\nfrom ..utils.concurrency import ensure_proc_terminate, start_proc_mask_signal\nfrom ..utils.gpu import get_num_gpu\nfrom ..utils.nvml import NVMLContext\nfrom .base import Callback\n\n__all__ = [\'GPUUtilizationTracker\', \'GraphProfiler\', \'PeakMemoryTracker\',\n           \'GPUMemoryTracker\', \'HostMemoryTracker\', \'ThroughputTracker\']\n\n\nclass GPUUtilizationTracker(Callback):\n    """""" Summarize the average GPU utilization within an epoch.\n\n    It will start a process to obtain GPU utilization through NVML every second\n    within the epoch (the trigger_epoch time was not included),\n    and write average utilization to monitors.\n\n    This callback creates a process, therefore it\'s not safe to be used with MPI.\n    """"""\n\n    _chief_only = False\n\n    def __init__(self, devices=None):\n        """"""\n        Args:\n            devices (list[int]): physical GPU ids to monitor. If None, will guess from the environment.\n        """"""\n        assert os.name != \'nt\', ""GPUUtilizationTracker does not support windows!""\n        self._devices = devices\n        self._enabled = True\n\n    def _guess_devices(self):\n        env = os.environ.get(\'CUDA_VISIBLE_DEVICES\')\n        if env is None:\n            devices = list(range(get_num_gpu()))\n            if len(devices) > 1:\n                logger.warn(""[GPUUtilizationTracker] Both devices and CUDA_VISIBLE_DEVICES are None! ""\n                            ""Will monitor all {} visible GPUs!"".format(len(devices)))\n        else:\n            if len(env):\n                devices = list(map(int, env.split(\',\')))\n            else:\n                devices = []\n        return devices\n\n    def _setup_graph(self):\n        # special heuristics for Horovod\n        from ..train import HorovodTrainer\n        if isinstance(self.trainer, HorovodTrainer):\n            if self.trainer.mpi_enabled():\n                logger.warn(""GPUUtilizationTracker is disabled under MPI."")\n                self._enabled = False\n                return\n            else:\n                self._devices = [self.trainer.hvd.local_rank()]\n\n        if self._devices is None:\n            self._devices = self._guess_devices()\n        assert len(self._devices), ""[GPUUtilizationTracker] No GPU device given!""\n\n        self._evt = mp.Event()\n        self._stop_evt = mp.Event()\n        self._queue = mp.Queue()\n        self._proc = mp.Process(target=self.worker, args=(\n            self._evt, self._queue, self._stop_evt, self._devices))\n        ensure_proc_terminate(self._proc)\n        start_proc_mask_signal(self._proc)\n\n    def _before_train(self):\n        assert gpu_available_in_session(), ""[GPUUtilizationTracker] needs GPU!""\n\n    def _before_epoch(self):\n        if self._enabled:\n            self._evt.set()\n\n    def _after_epoch(self):\n        if self._enabled:\n            while self._evt.is_set():   # unlikely, unless the epoch is extremely fast\n                pass\n            self._evt.set()\n\n    def _trigger_epoch(self):\n        # Don\'t do this in after_epoch because\n        # before,after_epoch are supposed to be extremely fast by design.\n        if not self._enabled:\n            return\n        try:\n            stats = self._queue.get(timeout=60)\n        except queue.Empty:\n            if self._proc.is_alive():\n                raise RuntimeError(""GPUUtilization.worker() is stuck. This is a bug."")\n            else:\n                raise RuntimeError(""GPUUtilization.worker() process is killed unexpectedly."")\n\n        if isinstance(stats, int) and stats == -1:\n            from ..train.base import StopTraining\n            raise StopTraining(""GPUUtilizationTracker.worker has failed."")\n        for idx, dev in enumerate(self._devices):\n            self.trainer.monitors.put_scalar(\'GPUUtil/{}\'.format(dev), stats[idx])\n\n    def _after_train(self):\n        if self._enabled:\n            self._stop_evt.set()\n            self._evt.set()\n            self._proc.terminate()\n\n    @staticmethod\n    def worker(evt, rst_queue, stop_evt, devices):\n        """"""\n        Args:\n            devices (list[int])\n        """"""\n        with NVMLContext() as ctx:\n            devices = [ctx.device(i) for i in devices]\n            while True:\n                try:\n                    evt.wait()  # start epoch\n                    evt.clear()\n                    if stop_evt.is_set():   # or on exit\n                        return\n\n                    stats = np.zeros((len(devices),), dtype=\'f4\')\n                    cnt = 0\n                    while True:\n                        time.sleep(1)\n\n                        data = [d.utilization()[\'gpu\'] for d in devices]\n                        data = list(map(float, data))\n                        stats += data\n                        cnt += 1\n\n                        if evt.is_set():    # stop epoch\n                            if stop_evt.is_set():   # or on exit\n                                return\n                            evt.clear()\n                            if cnt > 1:\n                                # Ignore the last datapoint. Usually is zero, makes us underestimate the util.\n                                stats -= data\n                                cnt -= 1\n                            rst_queue.put(stats / cnt)\n                            break\n                except Exception:\n                    logger.exception(""Exception in GPUUtilizationTracker.worker"")\n                    rst_queue.put(-1)\n                    return\n\n\n# Can add more features from tfprof\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md\n\nclass GraphProfiler(Callback):\n    """"""\n    Enable profiling by installing session hooks,\n    and write tracing files / events / metadata to ``logger.get_logger_dir()``.\n\n    The tracing files can be loaded from ``chrome://tracing``.\n    The metadata files can be processed by\n    `tfprof command line utils\n    <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md>`_.\n    The event is viewable from tensorboard.\n\n    Tips:\n\n    Note that the profiling is by default enabled for every step and is expensive.\n    You probably want to schedule it less frequently, e.g.:\n\n    .. code-block:: none\n\n        EnableCallbackIf(\n            GraphProfiler(dump_tracing=True, dump_event=True),\n            lambda self: self.trainer.global_step > 20 and self.trainer.global_step < 30)\n    """"""\n    def __init__(self, dump_metadata=False, dump_tracing=True, dump_event=False):\n        """"""\n        Args:\n            dump_metadata(bool): Dump :class:`tf.RunMetadata` to be used with tfprof.\n            dump_tracing(bool): Dump chrome tracing files.\n            dump_event(bool): Dump to an event processed by FileWriter and\n                will be shown in TensorBoard.\n        """"""\n        self._dir = logger.get_logger_dir()\n        self._dump_meta = bool(dump_metadata)\n        self._dump_tracing = bool(dump_tracing)\n        self._dump_event = bool(dump_event)\n        assert os.path.isdir(self._dir), self._dir\n\n    def _before_run(self, _):\n        opt = tf.RunOptions()\n        opt.trace_level = tf.RunOptions.FULL_TRACE\n        return tf.train.SessionRunArgs(fetches=None, options=opt)\n\n    def _after_run(self, _, run_values):\n        meta = run_values.run_metadata\n        if self._dump_meta:\n            self._write_meta(meta)\n        if self._dump_tracing:\n            self._write_tracing(meta)\n        if self._dump_event:\n            self._write_event(meta)\n\n    def _write_meta(self, metadata):\n        fname = os.path.join(\n            self._dir, \'runmetadata-{}.pb\'.format(self.global_step))\n        with open(fname, \'wb\') as f:\n            f.write(metadata.SerializeToString())\n\n    def _write_tracing(self, metadata):\n        from tensorflow.python.client import timeline\n        tl = timeline.Timeline(step_stats=metadata.step_stats)\n        fname = os.path.join(\n            self._dir, \'chrome-trace-{}.json\'.format(self.global_step))\n        with open(fname, \'w\') as f:\n            f.write(tl.generate_chrome_trace_format(\n                show_dataflow=True, show_memory=True))\n\n    def _write_event(self, metadata):\n        evt = tf.Event()\n        evt.tagged_run_metadata.tag = \'trace-{}\'.format(self.global_step)\n        evt.tagged_run_metadata.run_metadata = metadata.SerializeToString()\n        self.trainer.monitors.put_event(evt)\n\n\nclass GPUMemoryTracker(Callback):\n    """"""\n    Track peak memory used on each GPU device every epoch, by :mod:`tf.contrib.memory_stats`.\n    The peak memory comes from the ``MaxBytesInUse`` op, which is the peak memory used\n    in recent ``session.run`` calls.\n    See https://github.com/tensorflow/tensorflow/pull/13107.\n    """"""\n\n    _chief_only = False\n\n    def __init__(self, devices=(0,)):\n        """"""\n        Args:\n            devices([int] or [str]): list of GPU devices to track memory on.\n        """"""\n        assert isinstance(devices, (list, tuple)), devices\n        devices = [\'/gpu:{}\'.format(x) if isinstance(x, int) else x for x in devices]\n        self._devices = devices\n\n    def _setup_graph(self):\n        from tensorflow.contrib.memory_stats import MaxBytesInUse\n        ops = []\n        for dev in self._devices:\n            with tf.device(dev):\n                ops.append(MaxBytesInUse())\n        self._fetches = tf.train.SessionRunArgs(fetches=ops)\n\n    def _before_train(self):\n        assert gpu_available_in_session(), ""PeakMemoryTracker only supports GPU!""\n\n    def _before_run(self, _):\n        if self.local_step == self.trainer.steps_per_epoch - 1:\n            return self._fetches\n        return None\n\n    def _after_run(self, _, rv):\n        results = rv.results\n        if results is not None:\n            for mem, dev in zip(results, self._devices):\n                self.trainer.monitors.put_scalar(\'PeakMemory(MB)\' + dev, mem / 1e6)\n\n\nPeakMemoryTracker = GPUMemoryTracker\n\n\nclass HostMemoryTracker(Callback):\n    """"""\n    Track free RAM on the host.\n\n    When triggered, it writes the size of free RAM into monitors.\n    """"""\n    _chief_only = False\n\n    def _setup_graph(self):\n        logger.info(""[HostMemoryTracker] Free RAM in setup_graph() is {:.2f} GB."".format(self._free_ram_gb()))\n\n    def _before_train(self):\n        logger.info(""[HostMemoryTracker] Free RAM in before_train() is {:.2f} GB."".format(self._free_ram_gb()))\n\n    def _trigger(self):\n        ram_gb = self._free_ram_gb()\n        self.trainer.monitors.put_scalar(\'HostFreeMemory (GB)\', ram_gb)\n\n    def _free_ram_gb(self):\n        return psutil.virtual_memory().available / 1024**3\n\n\nclass ThroughputTracker(Callback):\n    """"""\n    This callback writes the training throughput (in terms of either steps/sec, or samples/sec)\n    to the monitors everytime it is triggered.\n    The throughput is computed based on the duration between the consecutive triggers.\n\n    The time spent on callbacks after each epoch is excluded.\n    """"""\n\n    _chief_only = False\n\n    def __init__(self, samples_per_step=None):\n        """"""\n        Args:\n            samples_per_step (int or None): total number of samples processed in each step\n                (i.e., your total batch size in each step).\n                If not provided, this callback will record ""steps/sec"" instead of ""samples/sec"".\n        """"""\n        if samples_per_step is not None:\n            samples_per_step = int(samples_per_step)\n        self._samples_per_step = samples_per_step\n        self._timer = Timer()\n        self._timer.pause()\n\n    # only include the time between before_epoch/after_epoch\n    def _before_epoch(self):\n        self._timer.resume()\n\n    def _after_epoch(self):\n        self._timer.pause()\n\n    def _before_train(self):\n        self._update_last()\n\n    def _update_last(self):\n        old_pause = self._timer.is_paused()\n        self._timer.reset()\n        if old_pause:\n            self._timer.pause()\n        self._last_step = self.global_step\n\n    def _trigger(self):\n        steps_per_sec = (self.global_step - self._last_step) / self._timer.seconds()\n        self._update_last()\n\n        if self._samples_per_step is None:\n            self.trainer.monitors.put_scalar(""Throughput (steps/sec)"", steps_per_sec)\n        else:\n            self.trainer.monitors.put_scalar(""Throughput (samples/sec)"", steps_per_sec * self._samples_per_step)\n'"
tensorpack/callbacks/saver.py,17,"b'# -*- coding: utf-8 -*-\n# File: saver.py\n\n\nimport os\nfrom datetime import datetime\n\nfrom ..compat import tfv1 as tf\nfrom ..utils import fs, logger\nfrom .base import Callback\n\n__all__ = [\'ModelSaver\', \'MinSaver\', \'MaxSaver\']\n\n\nclass ModelSaver(Callback):\n    """"""\n    Save the model once triggered.\n    """"""\n\n    def __init__(self, max_to_keep=10,\n                 keep_checkpoint_every_n_hours=0.5,\n                 checkpoint_dir=None,\n                 var_collections=None):\n        """"""\n        Args:\n            max_to_keep (int): the same as in ``tf.train.Saver``.\n            keep_checkpoint_every_n_hours (float): the same as in ``tf.train.Saver``.\n                Note that ""keep"" does not mean ""create"", but means ""don\'t delete"".\n            checkpoint_dir (str): Defaults to ``logger.get_logger_dir()``.\n            var_collections (str or list of str): collection of the variables (or list of collections) to save.\n        """"""\n        if var_collections is None:\n            var_collections = [tf.GraphKeys.GLOBAL_VARIABLES]\n        self._max_to_keep = max_to_keep\n        self._keep_every_n_hours = keep_checkpoint_every_n_hours\n\n        if not isinstance(var_collections, list):\n            var_collections = [var_collections]\n        self.var_collections = var_collections\n        if checkpoint_dir is None:\n            checkpoint_dir = logger.get_logger_dir()\n        if checkpoint_dir is not None:\n            if not tf.gfile.IsDirectory(checkpoint_dir):  # v2: tf.io.gfile.isdir\n                tf.gfile.MakeDirs(checkpoint_dir)  # v2: tf.io.gfile.makedirs\n        # If None, allow it to be init, but fail later if used\n        # For example, if chief_only=True, it can still be safely initialized\n        # in non-chief workers which don\'t have logger dir\n        self.checkpoint_dir = fs.normpath(checkpoint_dir) if checkpoint_dir is not None else checkpoint_dir\n\n    def _setup_graph(self):\n        assert self.checkpoint_dir is not None, \\\n            ""Please provide \'checkpoint_dir\' for ModelSaver, or use logger.set_logger_dir()""\n        vars = []\n        for key in self.var_collections:\n            vars.extend(tf.get_collection(key))\n        vars = list(set(vars))\n        self.path = os.path.join(self.checkpoint_dir, \'model\')\n        self.saver = tf.train.Saver(\n            var_list=vars,\n            max_to_keep=self._max_to_keep,\n            keep_checkpoint_every_n_hours=self._keep_every_n_hours,\n            write_version=tf.train.SaverDef.V2,\n            save_relative_paths=True)\n        # Scaffold will call saver.build from this collection\n        tf.add_to_collection(tf.GraphKeys.SAVERS, self.saver)\n\n    def _before_train(self):\n        # graph is finalized, OK to write it now.\n        time = datetime.now().strftime(\'%m%d-%H%M%S\')\n        self.saver.export_meta_graph(\n            os.path.join(self.checkpoint_dir,\n                         \'graph-{}.meta\'.format(time)),\n            collection_list=self.graph.get_all_collection_keys())\n\n    def _trigger(self):\n        try:\n            self.saver.save(\n                tf.get_default_session(),\n                self.path,\n                global_step=tf.train.get_global_step(),\n                write_meta_graph=False)\n            logger.info(""Model saved to %s."" % tf.train.get_checkpoint_state(self.checkpoint_dir).model_checkpoint_path)\n        except (IOError, tf.errors.PermissionDeniedError,\n                tf.errors.ResourceExhaustedError):   # disk error sometimes.. just ignore it\n            logger.exception(""Exception in ModelSaver!"")\n\n\nclass MinSaver(Callback):\n    """"""\n    Separately save the model with minimum value of some statistics.\n    """"""\n    def __init__(self, monitor_stat, reverse=False, filename=None, checkpoint_dir=None):\n        """"""\n        Args:\n            monitor_stat(str): the name of the statistics.\n            reverse (bool): if True, will save the maximum.\n            filename (str): the name for the saved model.\n                Defaults to ``min-{monitor_stat}.tfmodel``.\n            checkpoint_dir (str): the directory containing checkpoints.\n\n        Example:\n            Save the model with minimum validation error to\n            ""min-val-error.tfmodel"":\n\n            .. code-block:: python\n\n                MinSaver(\'val-error\')\n\n        Note:\n            1. It assumes that :class:`ModelSaver` is used with the same ``checkpoint_dir``\n               and appears earlier in the callback list.\n               The default for both :class:`ModelSaver` and :class:`MinSaver`\n               is ``checkpoint_dir=logger.get_logger_dir()``\n            2. Callbacks are executed in the order they are defined. Therefore you\'d want to\n               use this callback after the callback (e.g. InferenceRunner) that produces the statistics.\n        """"""\n        self.monitor_stat = monitor_stat\n        self.reverse = reverse\n        self.filename = filename\n        self.best = None\n        self.checkpoint_dir = checkpoint_dir\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = logger.get_logger_dir()\n        self.checkpoint_dir = fs.normpath(self.checkpoint_dir)\n\n    def _get_stat(self):\n        try:\n            v = self.trainer.monitors.get_history(self.monitor_stat)[-1]\n        except (KeyError, IndexError):\n            v = None, None\n        return v\n\n    def _trigger(self):\n        curr_step, curr_val = self._get_stat()\n        if curr_step is None:\n            return\n\n        if self.best is None or (curr_val > self.best[1] if self.reverse else curr_val < self.best[1]):\n            self.best = (curr_step, curr_val)\n            self._save()\n\n    def _save(self):\n        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n        if ckpt is None:\n            raise RuntimeError(\n                ""[MinSaver] Cannot find a checkpoint state. Do you forget to use ModelSaver?"")\n        path = ckpt.model_checkpoint_path\n\n        extreme_name = \'maximum\' if self.reverse else \'minimum\'\n        if not path.endswith(str(self.best[0])):\n            logger.warn(""[MinSaver] New {} \'{}\' found at global_step={}, but the latest checkpoint is {}."".format(\n                extreme_name, self.monitor_stat, self.best[0], path\n            ))\n            logger.warn(""MinSaver will do nothing this time. ""\n                        ""The callbacks may have inconsistent frequency or wrong order."")\n            return\n\n        newname = os.path.join(self.checkpoint_dir,\n                               self.filename or\n                               (\'max-\' + self.monitor_stat if self.reverse else \'min-\' + self.monitor_stat))\n        files_to_copy = tf.gfile.Glob(path + \'*\')\n        for file_to_copy in files_to_copy:\n            tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)\n        logger.info(""Model at global_step={} with {} {}={:.5g} saved."".format(\n            self.best[0], extreme_name, self.monitor_stat, self.best[1]))\n\n\nclass MaxSaver(MinSaver):\n    """"""\n    Separately save the model with maximum value of some statistics.\n\n    See docs of :class:`MinSaver` for details.\n    """"""\n    def __init__(self, monitor_stat, filename=None, checkpoint_dir=None):\n        """"""\n        Args:\n            monitor_stat(str): the name of the statistics.\n            filename (str): the name for the saved model.\n                Defaults to ``max-{monitor_stat}.tfmodel``.\n        """"""\n        super(MaxSaver, self).__init__(monitor_stat, True, filename=filename, checkpoint_dir=checkpoint_dir)\n'"
tensorpack/callbacks/stats.py,0,"b'# -*- coding: utf-8 -*-\n# File: stats.py\n\nfrom .graph import DumpParamAsImage  # noqa\n# for compatibility only\nfrom .misc import InjectShell, SendStat  # noqa\n\n__all__ = []\n'"
tensorpack/callbacks/steps.py,7,"b'# -*- coding: utf-8 -*-\n# File: steps.py\n\n"""""" Some common step callbacks. """"""\n\nimport tqdm\nfrom six.moves import zip\n\nfrom ..compat import tfv1 as tf\nfrom ..tfutils.common import get_global_step_var, get_op_tensor_name\nfrom ..utils import logger\nfrom ..utils.naming import GLOBAL_STEP_INCR_OP_NAME\nfrom ..utils.utils import get_tqdm_kwargs\nfrom .base import Callback\n\n__all__ = [\'TensorPrinter\', \'ProgressBar\', \'SessionRunTimeout\']\n\n\nclass TensorPrinter(Callback):\n    """""" Prints the value of some tensors in each step.\n        It\'s an example of how ``before_run/after_run`` works.\n    """"""\n\n    def __init__(self, names):\n        """"""\n        Args:\n            names(list): list of string, the names of the tensors to print.\n        """"""\n        names = [get_op_tensor_name(n)[1] for n in names]\n        logger.warn(""Using tf.Print in the graph is much faster than TensorPrinter!"")\n        self._names = names\n\n    def _setup_graph(self):\n        self._fetches = self.get_tensors_maybe_in_tower(self._names)\n\n    def _before_run(self, _):\n        return self._fetches\n\n    def _after_run(self, _, vals):\n        args = vals.results\n        assert len(args) == len(self._names), len(args)\n        for n, v in zip(self._names, args):\n            logger.info(""{}: {}"".format(n, v))\n\n\nclass ProgressBar(Callback):\n    """""" A progress bar based on tqdm.\n\n    This callback is one of the :func:`DEFAULT_CALLBACKS()`.\n    """"""\n\n    _chief_only = False\n\n    def __init__(self, names=()):\n        """"""\n        Args:\n            names(tuple[str]): the names of the tensors to monitor\n                on the progress bar.\n        """"""\n        super(ProgressBar, self).__init__()\n        self._names = [get_op_tensor_name(n)[1] for n in names]\n        self._tags = [get_op_tensor_name(n)[0].split(""/"")[-1] for n in names]\n        self._bar = None\n\n    def _before_train(self):\n        self._last_updated = self.local_step\n\n        self._total = self.trainer.steps_per_epoch\n        self._tqdm_args = get_tqdm_kwargs(leave=True)\n\n        self._fetches = self.get_tensors_maybe_in_tower(self._names) or None\n        if self._fetches:\n            for t in self._fetches:\n                assert t.shape.ndims == 0, ""ProgressBar can only print scalars, not {}"".format(t)\n            self._fetches = tf.train.SessionRunArgs(self._fetches)\n            self._tqdm_args[\'bar_format\'] = self._tqdm_args[\'bar_format\'] + ""{postfix} ""\n\n    def _before_epoch(self):\n        self._bar = tqdm.trange(self._total, **self._tqdm_args)\n\n    def _after_epoch(self):\n        self._bar.close()\n\n    def _before_run(self, _):\n        # update progress bar when local step changed (one step is finished)\n        if self.local_step != self._last_updated:\n            self._last_updated = self.local_step\n            return self._fetches\n        else:\n            return None\n\n    def _after_run(self, _, run_values):\n        res = run_values.results\n        if res:\n            self._bar.set_postfix(zip(self._tags, res))\n\n    def _trigger_step(self):\n        self._bar.update()\n\n    def _after_train(self):\n        if self._bar:       # training may get killed before the first step\n            self._bar.close()\n\n\nclass MaintainStepCounter(Callback):\n    """"""\n    It maintains the global step in the graph, making sure it\'s increased by one at every `hooked_sess.run`.\n    This callback is used internally by the trainer, you don\'t need to worry about it.\n    """"""\n\n    _chief_only = False\n    """"""\n    In distributed training, we let each worker maintain its local global_step.\n    """"""\n\n    def _setup_graph(self):\n        # ensure it exists\n        gs_var = get_global_step_var()\n        with tf.name_scope(None):\n            self.gs_incr_op = tf.assign_add(\n                gs_var, 1,\n                name=GLOBAL_STEP_INCR_OP_NAME).op\n        self._fetches = tf.train.SessionRunArgs(self.gs_incr_op)\n\n    def _before_train(self):\n        if self.global_step != 0:\n            logger.info(""Start training with global_step={}"".format(self.global_step))\n\n    def _before_run(self, _):\n        # always increase global_step when hooked_sess.run is called\n        return self._fetches\n\n    def _after_run(self, _, __):\n        # Keep python-side global_step in agreement with TF-side\n        self.trainer.loop._global_step += 1\n\n\nclass SessionRunTimeout(Callback):\n    """"""\n    Add timeout option to each sess.run call.\n    """"""\n    def __init__(self, timeout_in_ms):\n        """"""\n        Args:\n            timeout_in_ms (int):\n        """"""\n        self._timeout = int(timeout_in_ms)\n\n        opt = tf.RunOptions(timeout_in_ms=timeout_in_ms)\n        self._runargs = tf.train.SessionRunArgs(fetches=[], options=opt)\n\n    def _before_run(self, _):\n        return self._runargs\n'"
tensorpack/callbacks/summary.py,16,"b'# -*- coding: utf-8 -*-\n# File: summary.py\n\n\nimport numpy as np\nfrom collections import deque\n\nfrom ..compat import tfv1 as tf\nfrom ..tfutils.common import get_op_tensor_name\nfrom ..utils import logger\nfrom ..utils.naming import MOVING_SUMMARY_OPS_KEY\nfrom .base import Callback\n\n__all__ = [\'MovingAverageSummary\', \'MergeAllSummaries\', \'SimpleMovingAverage\']\n\n\nclass MovingAverageSummary(Callback):\n    """"""\n    Maintain the moving average of summarized tensors in every step,\n    by ops added to the collection.\n    Note that it only **maintains** the moving averages by updating\n    the relevant variables in the graph,\n    the actual summary should be done in other callbacks.\n\n    This callback is one of the :func:`DEFAULT_CALLBACKS()`.\n    """"""\n    def __init__(self, collection=MOVING_SUMMARY_OPS_KEY, train_op=None):\n        """"""\n        Args:\n            collection(str): the collection of EMA-maintaining ops.\n                The default value would work with\n                the tensors you added by :func:`tfutils.summary.add_moving_summary()`,\n                but you can use other collections as well.\n            train_op (tf.Operation or str): the (name of) training op to associate the maintaing ops with.\n                If not provided, the EMA-maintaining ops will be hooked to\n                `trainer.hooked_session` and be executed in every iteration.\n                Otherwise, the EMA-maintaining ops will be executed whenever\n                the training op is executed.\n        """"""\n        self._collection = collection\n        self._train_op = train_op\n\n    def _setup_graph(self):\n        ops = [k.op for k in tf.get_collection(self._collection)]\n        if self._train_op is None:\n            logger.info(""[MovingAverageSummary] {} operations in collection \'{}\' ""\n                        ""will be run with session hooks."".format(len(ops), self._collection))\n\n            self.ema_op = tf.group(*ops, name=\'maintain_moving_average_summary\')\n            self._fetch = tf.train.SessionRunArgs(fetches=self.ema_op)\n        else:\n            if isinstance(self._train_op, tf.Tensor):\n                self._train_op = self._train_op.op\n            if not isinstance(self._train_op, tf.Operation):\n                self._train_op = self.graph.get_operation_by_name(self._train_op)\n            self._train_op._add_control_inputs(ops)\n            logger.info(""[MovingAverageSummary] {} operations in collection \'{}\'""\n                        "" will be run together with operation \'{}\'."".format(\n                            len(ops), self._collection, self._train_op.name))\n\n    def _before_run(self, _):\n        if self._train_op is None:\n            return self._fetch\n\n\nclass MergeAllSummaries_RunAlone(Callback):\n    def __init__(self, period, key):\n        self._period = period\n        self._key = key\n\n    def _setup_graph(self):\n        size = len(tf.get_collection(self._key))\n        logger.info(""Summarizing collection \'{}\' of size {}."".format(self._key, size))\n        self.summary_op = tf.summary.merge_all(self._key)\n\n    def _trigger_step(self):\n        if self._period:\n            if (self.local_step + 1) % self._period == 0:\n                self._trigger()\n\n    def _trigger(self):\n        if self.summary_op:\n            summary = self.summary_op.eval()\n            self.trainer.monitors.put_summary(summary)\n\n\nclass MergeAllSummaries_RunWithOp(Callback):\n    def __init__(self, period, key):\n        self._period = period\n        self._key = key\n\n    def _setup_graph(self):\n        size = len(tf.get_collection(self._key))\n        logger.info(""Summarizing collection \'{}\' of size {}."".format(self._key, size))\n        self.summary_op = tf.summary.merge_all(self._key)\n        if self.summary_op is not None:\n            self._fetches = tf.train.SessionRunArgs(self.summary_op)\n        else:\n            self._fetches = None\n\n    def _need_run(self):\n        if self.local_step == self.trainer.steps_per_epoch - 1:\n            return True\n        if self._period > 0 and (self.local_step + 1) % self._period == 0:\n            return True\n        return False\n\n    def _before_run(self, ctx):\n        if self._need_run():\n            return self._fetches\n        return None\n\n    def _after_run(self, _, run_values):\n        summary = run_values.results\n        if summary is None:\n            return\n        self.trainer.monitors.put_summary(summary)\n\n\ndef MergeAllSummaries(period=0, run_alone=False, key=None):\n    """"""\n    Evaluate all summaries by ``tf.summary.merge_all``, and write them to logs.\n\n    This callback is one of the :func:`DEFAULT_CALLBACKS()`.\n\n    Args:\n        period (int): by default the callback summarizes once every epoch.\n            This option (if not set to 0) makes it additionally summarize every ``period`` steps.\n        run_alone (bool): whether to evaluate the summaries alone.\n            If True, summaries will be evaluated after each epoch alone.\n            If False, summaries will be evaluated together with the\n            `sess.run` calls, in the last step of each epoch.\n            For :class:`SimpleTrainer`, it needs to be False because summary may\n            depend on inputs.\n        key (str): the collection of summary tensors. Same as in ``tf.summary.merge_all``.\n            Default is ``tf.GraphKeys.SUMMARIES``.\n    """"""\n    if key is None:\n        key = tf.GraphKeys.SUMMARIES\n    period = int(period)\n    if run_alone:\n        return MergeAllSummaries_RunAlone(period, key)\n    else:\n        return MergeAllSummaries_RunWithOp(period, key)\n\n\nclass SimpleMovingAverage(Callback):\n    """"""\n    Monitor Simple Moving Average (SMA), i.e. an average within a sliding window,\n    of some tensors.\n    """"""\n    def __init__(self, tensors, window_size):\n        """"""\n        Args:\n            tensors (str or [str]): names of tensors\n            window_size (int): size of the moving window\n        """"""\n\n        self._tensor_names = [get_op_tensor_name(x)[1] for x in tensors]\n        self._display_names = [get_op_tensor_name(x)[0] for x in tensors]\n        self._window = int(window_size)\n        self._queue = deque(maxlen=window_size)\n\n    def _setup_graph(self):\n        tensors = self.get_tensors_maybe_in_tower(self._tensor_names)\n        for t in tensors:\n            assert t.get_shape().ndims == 0, \\\n                ""SimpleMovingAverage only accepts scalar tensor! Got one with {}"".format(t.get_shape())\n        self._fetch = tf.train.SessionRunArgs(fetches=tensors)\n\n    def _before_run(self, _):\n        return self._fetch\n\n    def _after_run(self, _, rv):\n        results = rv.results\n        self._queue.append(results)\n\n    def _trigger_step(self):\n        if self.global_step % self._window == 0:\n            averages = np.asarray(self._queue).mean(axis=0)\n            for name, avg in zip(self._display_names, averages):\n                self.trainer.monitors.put_scalar(name + \'/SMA\', avg)\n'"
tensorpack/callbacks/trigger.py,0,"b'# -*- coding: utf-8 -*-\n# File: trigger.py\n\n\nfrom .base import Callback, ProxyCallback\n\n__all__ = [\'PeriodicTrigger\', \'PeriodicCallback\', \'EnableCallbackIf\']\n\n\nclass PeriodicTrigger(ProxyCallback):\n    """"""\n    Trigger a callback every k global steps or every k epochs by its :meth:`trigger()` method.\n\n    Most existing callbacks which do something every epoch are implemented\n    with :meth:`trigger()` method. By default the :meth:`trigger()` method will be called every epoch.\n    This wrapper can make the callback run at a different frequency.\n\n    All other methods (``before/after_run``, ``trigger_step``, etc) of the given callback\n    are unaffected. They will still be called as-is.\n    """"""\n\n    def __init__(self, triggerable, every_k_steps=None, every_k_epochs=None, before_train=False):\n        """"""\n        Args:\n            triggerable (Callback): a Callback instance with a trigger method to be called.\n            every_k_steps (int): trigger when ``global_step % k == 0``. Set to\n                None to ignore.\n            every_k_epochs (int): trigger when ``epoch_num % k == 0``. Set to\n                None to ignore.\n            before_train (bool): trigger in the :meth:`before_train` method.\n\n        every_k_steps and every_k_epochs can be both set, but cannot be both None unless before_train is True.\n        """"""\n        assert isinstance(triggerable, Callback), type(triggerable)\n        super(PeriodicTrigger, self).__init__(triggerable)\n        if before_train is False:\n            assert (every_k_epochs is not None) or (every_k_steps is not None), \\\n                ""Arguments to PeriodicTrigger have disabled the triggerable!""\n        self._step_k = every_k_steps\n        self._epoch_k = every_k_epochs\n        self._do_before_train = before_train\n\n    def _before_train(self):\n        self.cb.before_train()\n        if self._do_before_train:\n            self.cb.trigger()\n\n    def _trigger_step(self):\n        self.cb.trigger_step()\n        if self._step_k is None:\n            return\n        if self.global_step % self._step_k == 0:\n            self.cb.trigger()\n\n    def _trigger_epoch(self):\n        if self._epoch_k is None:\n            return\n        if self.epoch_num % self._epoch_k == 0:\n            self.cb.trigger()\n\n    def __str__(self):\n        return ""PeriodicTrigger-"" + str(self.cb)\n\n\nclass EnableCallbackIf(ProxyCallback):\n    """"""\n    Disable the ``{before,after}_epoch``, ``{before,after}_run``,\n    ``trigger_{epoch,step}``\n    methods of a callback, unless some condition satisfies.\n    The other methods are unaffected.\n\n    A more accurate name for this callback should be ""DisableCallbackUnless"", but that\'s too ugly.\n\n    Note:\n        If you use ``{before,after}_run``,\n        ``pred`` will be evaluated only in ``before_run``.\n    """"""\n\n    def __init__(self, callback, pred):\n        """"""\n        Args:\n            callback (Callback):\n            pred (self -> bool): a callable predicate. Has to be a pure function.\n                The callback is disabled unless this predicate returns True.\n        """"""\n        self._pred = pred\n        super(EnableCallbackIf, self).__init__(callback)\n\n    def _before_run(self, ctx):\n        if self._pred(self):\n            self._enabled = True\n            return super(EnableCallbackIf, self)._before_run(ctx)\n        else:\n            self._enabled = False\n\n    def _after_run(self, ctx, rv):\n        if self._enabled:\n            super(EnableCallbackIf, self)._after_run(ctx, rv)\n\n    def _before_epoch(self):\n        if self._pred(self):\n            super(EnableCallbackIf, self)._before_epoch()\n\n    def _after_epoch(self):\n        if self._pred(self):\n            super(EnableCallbackIf, self)._after_epoch()\n\n    def _trigger_epoch(self):\n        if self._pred(self):\n            super(EnableCallbackIf, self)._trigger_epoch()\n\n    def _trigger_step(self):\n        if self._pred(self):\n            super(EnableCallbackIf, self)._trigger_step()\n\n    def __str__(self):\n        return ""EnableCallbackIf-"" + str(self.cb)\n\n\nclass PeriodicCallback(EnableCallbackIf):\n    """"""\n    The ``{before,after}_epoch``, ``{before,after}_run``, ``trigger_{epoch,step}``\n    methods of the given callback will be enabled only when ``global_step % every_k_steps == 0`\n    or ``epoch_num % every_k_epochs == 0``. The other methods are unaffected.\n\n    Note that this can only makes a callback **less** frequent than itself.\n    If you have a callback that by default runs every epoch by its :meth:`trigger()` method,\n    use :class:`PeriodicTrigger` to schedule it more frequent than itself.\n    """"""\n\n    def __init__(self, callback, every_k_steps=None, every_k_epochs=None):\n        """"""\n        Args:\n            callback (Callback): a Callback instance.\n            every_k_steps (int): enable the callback when ``global_step % k == 0``. Set to\n                None to ignore.\n            every_k_epochs (int): enable the callback when ``epoch_num % k == 0``.\n                Also enable when the last step finishes (``epoch_num == max_epoch``\n                and ``local_step == steps_per_epoch - 1``). Set to None to ignore.\n\n        every_k_steps and every_k_epochs can be both set, but cannot be both None.\n        """"""\n        assert isinstance(callback, Callback), type(callback)\n        assert (every_k_epochs is not None) or (every_k_steps is not None), \\\n            ""every_k_steps and every_k_epochs cannot be both None!""\n        self._step_k = every_k_steps\n        self._epoch_k = every_k_epochs\n        super(PeriodicCallback, self).__init__(callback, PeriodicCallback.predicate)\n\n    def predicate(self):\n        if self._step_k is not None and self.global_step % self._step_k == 0:\n            return True\n        if self._epoch_k is not None and self.epoch_num % self._epoch_k == 0:\n            return True\n        if self._epoch_k is not None:\n            if self.local_step == self.trainer.steps_per_epoch - 1 and \\\n                    self.epoch_num == self.trainer.max_epoch:\n                return True\n        return False\n\n    def __str__(self):\n        return ""PeriodicCallback-"" + str(self.cb)\n'"
tensorpack/compat/__init__.py,6,"b""#!/usr/bin/env python\n\nimport tensorflow as tf\n\n\ndef backport_tensor_spec():\n    if hasattr(tf, 'TensorSpec'):\n        return tf.TensorSpec\n    try:\n        # available since 1.7\n        from tensorflow.python.framework.tensor_spec import TensorSpec\n    except ImportError:\n        pass\n    else:\n        tf.TensorSpec = TensorSpec\n        return TensorSpec\n\n    from .tensor_spec import TensorSpec\n    tf.TensorSpec = TensorSpec\n    return TensorSpec\n\n\ndef is_tfv2():\n    try:\n        from tensorflow.python import tf2\n        return tf2.enabled()\n    except Exception:\n        return False\n\n\nif is_tfv2():\n    tfv1 = tf.compat.v1\n    if not hasattr(tf, 'layers'):\n        # promised at https://github.com/tensorflow/community/pull/24#issuecomment-440453886\n        tf.layers = tf.keras.layers\nelse:\n    try:\n        tfv1 = tf.compat.v1  # this will silent some warnings\n    except AttributeError:\n        tfv1 = tf\n"""
tensorpack/compat/tensor_spec.py,9,"b'\n""""""\nCopied from tensorflow/python/framework/tensor_spec.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.python.framework import common_shapes\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\n\n\nclass TensorSpec(object):\n  """"""Describes a tf.Tensor.\n\n  Metadata for describing the `tf.Tensor` objects accepted or returned\n  by some TensorFlow APIs.\n  """"""\n\n  __slots__ = [""_shape"", ""_shape_tuple"", ""_dtype"", ""_name""]\n\n  def __init__(self, shape, dtype=dtypes.float32, name=None):\n    """"""Creates a TensorSpec.\n\n    Args:\n      shape: Value convertible to `tf.TensorShape`. The shape of the tensor.\n      dtype: Value convertible to `tf.DType`. The type of the tensor values.\n      name: Optional name for the Tensor.\n\n    Raises:\n      TypeError: If shape is not convertible to a `tf.TensorShape`, or dtype is\n        not convertible to a `tf.DType`.\n    """"""\n    self._shape = tensor_shape.TensorShape(shape)\n    try:\n      self._shape_tuple = tuple(self.shape.as_list())\n    except ValueError:\n      self._shape_tuple = None\n    self._dtype = dtypes.as_dtype(dtype)\n    self._name = name\n\n  @classmethod\n  def from_spec(cls, spec, name=None):\n    return cls(spec.shape, spec.dtype, name or spec.name)\n\n  @classmethod\n  def from_tensor(cls, tensor, name=None):\n    if isinstance(tensor, ops.EagerTensor):\n      return TensorSpec(tensor.shape, tensor.dtype, name)\n    elif isinstance(tensor, ops.Tensor):\n      return TensorSpec(tensor.shape, tensor.dtype, name or tensor.op.name)\n    else:\n      raise ValueError(""`tensor` should be a tf.Tensor"")\n\n  @property\n  def shape(self):\n    """"""Returns the `TensorShape` that represents the shape of the tensor.""""""\n    return self._shape\n\n  @property\n  def dtype(self):\n    """"""Returns the `dtype` of elements in the tensor.""""""\n    return self._dtype\n\n  @property\n  def name(self):\n    """"""Returns the (optionally provided) name of the described tensor.""""""\n    return self._name\n\n  def is_compatible_with(self, spec_or_tensor):\n    """"""Returns True if spec_or_tensor is compatible with this TensorSpec.\n\n    Two tensors are considered compatible if they have the same dtype\n    and their shapes are compatible (see `tf.TensorShape.is_compatible_with`).\n\n    Args:\n      spec_or_tensor: A tf.TensorSpec or a tf.Tensor\n\n    Returns:\n      True if spec_or_tensor is compatible with self.\n    """"""\n    return (self._dtype.is_compatible_with(spec_or_tensor.dtype) and\n            self._shape.is_compatible_with(spec_or_tensor.shape))\n\n  def __repr__(self):\n    return ""TensorSpec(shape={}, dtype={}, name={})"".format(\n        self.shape, repr(self.dtype), repr(self.name))\n\n  def __hash__(self):\n    return hash((self._shape_tuple, self.dtype))\n\n  def __eq__(self, other):\n    return (self._shape_tuple == other._shape_tuple  # pylint: disable=protected-access\n            and self.dtype == other.dtype\n            and self._name == other._name)  # pylint: disable=protected-access\n\n  def __ne__(self, other):\n    return not self == other\n\n  def __reduce__(self):\n    return TensorSpec, (self._shape, self._dtype, self._name)\n'"
tensorpack/contrib/__init__.py,0,b''
tensorpack/contrib/keras.py,27,"b'# -*- coding: utf-8 -*-\n# File: keras.py\n\nfrom contextlib import contextmanager\nimport six\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom ..callbacks import Callback, CallbackToHook, InferenceRunner, InferenceRunnerBase, ScalarStats\nfrom ..models.regularize import regularize_cost_from_collection\nfrom ..tfutils.collection import backup_collection, restore_collection\nfrom ..tfutils.common import get_op_tensor_name\nfrom ..tfutils.scope_utils import cached_name_scope\nfrom ..tfutils.summary import add_moving_summary\nfrom ..tfutils.tower import get_current_tower_context\nfrom ..train import SimpleTrainer, SyncMultiGPUTrainerParameterServer, Trainer\nfrom ..train.interface import apply_default_prefetch\nfrom ..train.trainers import DistributedTrainerBase\nfrom ..utils import logger\nfrom ..utils.gpu import get_nr_gpu\n\n__all__ = [\'KerasPhaseCallback\', \'setup_keras_trainer\', \'KerasModel\']\n\n\nTOTAL_LOSS_NAME = \'total_loss\'\n\n\ndef _check_name(tensor, name):\n    tensorname = get_op_tensor_name(tensor.name)[0]\n    assert tensorname.split(\'/\')[-1] == name, \\\n        ""{} does not match {}, you may have name conflict somewhere!"".format(tensor.name, name)\n\n\nclass KerasModelCaller(object):\n    """"""\n    Keras model doesn\'t support variable scope reuse.\n    This is a wrapper around keras model to mimic reuse.\n    """"""\n    def __init__(self, get_model):\n        self.get_model = get_model\n        self.cached_model = None\n\n    def __call__(self, *input_tensors):\n        """"""\n        Args:\n            input_tensors ([tf.Tensor])\n        Returns:\n            output tensors of this tower, evaluated with the input tensors.\n        """"""\n        reuse = tf.get_variable_scope().reuse\n\n        old_trainable_names = {x.name for x in tf.trainable_variables()}\n        trainable_backup = backup_collection([tf.GraphKeys.TRAINABLE_VARIABLES])\n        update_ops_backup = backup_collection([tf.GraphKeys.UPDATE_OPS])\n\n        def post_process_model(model):\n            added_trainable_names = {x.name for x in tf.trainable_variables()}\n            restore_collection(trainable_backup)\n\n            for v in model.weights:\n                # In Keras, the collection is not respected and could contain non-trainable vars.\n                # We put M.weights into the collection instead.\n                if v.name not in old_trainable_names and v.name in added_trainable_names:\n                    tf.add_to_collection(tf.GraphKeys.TRAINABLE_VARIABLES, v)\n            new_trainable_names = {x.name for x in tf.trainable_variables()}\n\n            for n in added_trainable_names:\n                if n not in new_trainable_names:\n                    logger.warn(""Keras created trainable variable \'{}\' which is actually not trainable. ""\n                                ""This was automatically corrected."".format(n))\n\n            # Keras models might not use this collection at all (in some versions).\n            # This is a BC-breaking change of tf.keras: https://github.com/tensorflow/tensorflow/issues/19643\n            restore_collection(update_ops_backup)\n            for op in model.updates:\n                tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)\n\n        if self.cached_model is None:\n            assert not reuse\n\n            # starting from some versions, tf.keras starts to prepend name scope to variable names ..\n            @contextmanager\n            def clear_tower0_name_scope():\n                ns = tf.get_default_graph().get_name_scope()\n                if ns == \'tower0\':\n                    with tf.name_scope(\'/\'):\n                        yield\n                else:\n                    yield\n\n            with clear_tower0_name_scope():\n                model = self.cached_model = self.get_model(*input_tensors)\n                assert isinstance(model, keras.Model), \\\n                    ""Your get_model function should return a `tf.keras.Model`!""\n            outputs = model.outputs\n        elif reuse:\n            # use the cached Keras model to mimic reuse\n            # NOTE: ctx.is_training won\'t be useful inside model,\n            # because inference will always use the cached Keras model\n            model = self.cached_model\n            outputs = model.call(*input_tensors)\n        else:\n            # create new Keras model if not reuse\n            model = self.get_model(*input_tensors)\n            outputs = model.outputs\n\n        post_process_model(model)\n\n        if isinstance(outputs, list) and len(outputs) == 1:\n            return outputs[0]\n        return outputs\n\n\nclass KerasPhaseCallback(Callback):\n    """"""\n    Keras needs an extra input if learning_phase is used by the model\n    This callback will be used:\n    1. By the trainer with isTrain=True\n    2. By InferenceRunner with isTrain=False, in the form of hooks\n\n    If you use :class:`KerasModel` or :func:`setup_keras_trainer`,\n    this callback will be automatically added when needed.\n    """"""\n    def __init__(self, isTrain):\n        assert isinstance(isTrain, bool), isTrain\n        self._isTrain = isTrain\n        self._learning_phase = keras.backend.learning_phase()\n\n    def _setup_graph(self):\n        logger.info(""Using Keras learning phase {} in the graph!"".format(\n            self._learning_phase.name))\n        cbs = self.trainer._callbacks.cbs\n        for cb in cbs:\n            # XXX HACK\n            if isinstance(cb, InferenceRunnerBase):\n                h = CallbackToHook(KerasPhaseCallback(False))\n                cb.register_hook(h)\n\n    def _before_run(self, ctx):\n        return tf.train.SessionRunArgs(\n            fetches=[], feed_dict={self._learning_phase: int(self._isTrain)})\n\n\ndef setup_keras_trainer(\n        trainer, get_model,\n        input_signature, target_signature,\n        input, optimizer, loss, metrics):\n    """"""\n    Args:\n        trainer (SingleCostTrainer):\n        get_model (input1, input2, ... -> tf.keras.Model):\n            A function which takes tensors, builds and returns a Keras model.\n            It will be part of the tower function.\n        input (InputSource):\n        optimizer (tf.train.Optimizer):\n        loss, metrics: list of strings\n    """"""\n    assert isinstance(optimizer, tf.train.Optimizer), optimizer\n    assert isinstance(loss, list), loss\n    assert len(loss) >= 1, ""No loss was given!""\n    assert isinstance(metrics, list), metrics\n    model_caller = KerasModelCaller(get_model)\n\n    nr_inputs = len(input_signature)\n\n    def get_cost(*inputs):\n        ctx = get_current_tower_context()\n        input_tensors = list(inputs[:nr_inputs])\n        target_tensors = list(inputs[nr_inputs:])\n        # TODO mapping between target tensors & output tensors\n\n        outputs = model_caller(*input_tensors)\n\n        if isinstance(outputs, tf.Tensor):\n            outputs = [outputs]\n        assert len(outputs) == len(target_tensors), \\\n            ""len({}) != len({})"".format(str(outputs), str(target_tensors))\n        assert len(outputs) == len(loss), \\\n            ""len({}) != len({})"".format(str(outputs), str(loss))\n\n        loss_tensors = []\n        for idx, loss_name in enumerate(loss):\n            with cached_name_scope(\'keras_loss\', top_level=False):\n                loss_fn = keras.losses.get(loss_name)\n                curr_loss = loss_fn(target_tensors[idx], outputs[idx])\n            curr_loss = tf.reduce_mean(curr_loss, name=loss_name)\n            _check_name(curr_loss, loss_name)\n            loss_tensors.append(curr_loss)\n\n        loss_reg = regularize_cost_from_collection()\n        if loss_reg is not None:\n            total_loss = tf.add_n(loss_tensors + [loss_reg], name=TOTAL_LOSS_NAME)\n            add_moving_summary(loss_reg, total_loss, *loss_tensors)\n        else:\n            total_loss = tf.add_n(loss_tensors, name=TOTAL_LOSS_NAME)\n            add_moving_summary(total_loss, *loss_tensors)\n\n        if metrics and (ctx.is_main_training_tower or not ctx.is_training):\n            # for list: one metric for each output\n            metric_tensors = []\n            for oid, metric_name in enumerate(metrics):\n                output_tensor = outputs[oid]\n                target_tensor = target_tensors[oid]  # TODO may not have the same mapping?\n                with cached_name_scope(\'keras_metric\', top_level=False):\n                    metric_fn = keras.metrics.get(metric_name)\n                    metric_tensor = metric_fn(target_tensor, output_tensor)\n                metric_tensor = tf.reduce_mean(metric_tensor, name=metric_name)\n                _check_name(metric_tensor, metric_name)\n                # check name conflict here\n                metric_tensors.append(metric_tensor)\n            add_moving_summary(*metric_tensors)\n\n        return total_loss\n\n    trainer.setup_graph(\n        input_signature + target_signature,\n        input,\n        get_cost,\n        lambda: optimizer)\n    if isinstance(keras.backend.learning_phase(), tf.Tensor) and len(keras.backend.learning_phase().consumers()) > 0:\n        # check if learning_phase is used in this model\n        trainer.register_callback(KerasPhaseCallback(True))\n\n\nclass KerasModel(object):\n    def __init__(self, get_model, input_signature=None, target_signature=None,\n                 input=None, trainer=None):\n        """"""\n        Args:\n            get_model (input1, input2, ... -> keras.Model):\n                A function which takes tensors, builds and returns a Keras model.\n                It will be part of the tower function.\n            input_signature ([tf.TensorSpec]): required. The signature for inputs.\n            target_signature ([tf.TensorSpec]): required. The signature for the targets tensors.\n            input (InputSource | DataFlow): the InputSource or DataFlow where the input data comes from.\n            trainer (Trainer): the default will check the number of available GPUs and use them all.\n        """"""\n        self.get_model = get_model\n        assert callable(get_model), get_model\n        self.input_signature = input_signature\n        self.target_signature = target_signature\n        if trainer is None:\n            nr_gpu = get_nr_gpu()\n            if nr_gpu <= 1:\n                trainer = SimpleTrainer()\n            else:\n                # the default multi-gpu trainer\n                trainer = SyncMultiGPUTrainerParameterServer(nr_gpu)\n        assert isinstance(trainer, Trainer), trainer\n        assert not isinstance(trainer, DistributedTrainerBase)\n\n        assert input is not None, ""Argument \'input\' is required!""\n        self.input = apply_default_prefetch(input, trainer)\n        self.trainer = trainer\n\n    def compile(self, optimizer, loss, metrics=None):\n        """"""\n        Args:\n            optimizer (tf.train.Optimizer):\n            loss, metrics: string or list of strings\n        """"""\n        if isinstance(loss, six.string_types):\n            loss = [loss]\n        if metrics is None:\n            metrics = []\n        if isinstance(metrics, six.string_types):\n            metrics = [metrics]\n\n        self._stats_to_inference = loss + metrics + [TOTAL_LOSS_NAME]\n        setup_keras_trainer(\n            self.trainer, get_model=self.get_model,\n            input_signature=self.input_signature,\n            target_signature=self.target_signature,\n            input=self.input,\n            optimizer=optimizer,\n            loss=loss,\n            metrics=metrics)\n\n    def fit(self, validation_data=None, **kwargs):\n        """"""\n        Args:\n            validation_data (DataFlow or InputSource): to be used for inference.\n                The inference callback is added as the first in the callback list.\n                If you need to use it in a different order, please write it in the callback list manually.\n            kwargs: same arguments as :meth:`Trainer.train_with_defaults`.\n        """"""\n        callbacks = kwargs.pop(\'callbacks\', [])\n        if validation_data is not None:\n            # There is no way to guess where users want this callback. So we have to choose one.\n            # MinSaver may need results from this callback,\n            # so we put this callback at first.\n            callbacks.insert(0, InferenceRunner(\n                validation_data, ScalarStats(self._stats_to_inference)))\n        self.trainer.train_with_defaults(callbacks=callbacks, **kwargs)\n'"
tensorpack/dataflow/__init__.py,0,"b'#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()[\'kcah_acitats\'[::-1].upper()] = False\nif STATICA_HACK:\n    from .base import *\n    from .common import *\n    from .format import *\n    from .image import *\n    from .parallel_map import *\n    from .parallel import *\n    from .raw import *\n    from .remote import *\n    from .serialize import *\n    from . import imgaug\n    from . import dataset\n\n\nfrom pkgutil import iter_modules\nimport os\nimport os.path\nfrom ..utils.develop import LazyLoader\n\n__all__ = []\n\n\ndef _global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if \'__all__\' in dir(p) else dir(p)\n    if lst:\n        del globals()[name]\n        for k in lst:\n            if not k.startswith(\'__\'):\n                globals()[k] = p.__dict__[k]\n                __all__.append(k)\n\n\n__SKIP = set([\'dataset\', \'imgaug\'])\n_CURR_DIR = os.path.dirname(__file__)\nfor _, module_name, __ in iter_modules(\n        [os.path.dirname(__file__)]):\n    srcpath = os.path.join(_CURR_DIR, module_name + \'.py\')\n    if not os.path.isfile(srcpath):\n        continue\n    if ""_test"" not in module_name and \\\n       not module_name.startswith(\'_\') and \\\n            module_name not in __SKIP:\n        _global_import(module_name)\n\n\nglobals()[\'dataset\'] = LazyLoader(\'dataset\', globals(), __name__ + \'.dataset\')\nglobals()[\'imgaug\'] = LazyLoader(\'imgaug\', globals(), __name__ + \'.imgaug\')\n\ndel LazyLoader\n\n__all__.extend([\'imgaug\', \'dataset\'])\n'"
tensorpack/dataflow/base.py,0,"b'# -*- coding: utf-8 -*-\n# File: base.py\n\n\nimport threading\nfrom abc import ABCMeta, abstractmethod\nimport six\n\nfrom ..utils.utils import get_rng\n\n__all__ = [\'DataFlow\', \'ProxyDataFlow\', \'RNGDataFlow\', \'DataFlowTerminated\']\n\n\nclass DataFlowTerminated(BaseException):\n    """"""\n    An exception indicating that the DataFlow is unable to produce any more\n    data, i.e. something wrong happened so that calling :meth:`get_data`\n    cannot give a valid iterator any more.\n    In most DataFlow this will never be raised.\n    """"""\n    pass\n\n\nclass DataFlowReentrantGuard(object):\n    """"""\n    A tool to enforce non-reentrancy.\n    Mostly used on DataFlow whose :meth:`get_data` is stateful,\n    so that multiple instances of the iterator cannot co-exist.\n    """"""\n    def __init__(self):\n        self._lock = threading.Lock()\n\n    def __enter__(self):\n        self._succ = self._lock.acquire(False)\n        if not self._succ:\n            raise threading.ThreadError(""This DataFlow is not reentrant!"")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self._lock.release()\n        return False\n\n\nclass DataFlowMeta(ABCMeta):\n    """"""\n    DataFlow uses ""__iter__()"" and ""__len__()"" instead of\n    ""get_data()"" and ""size()"". This add back-compatibility.\n    """"""\n    def __new__(mcls, name, bases, namespace, **kwargs):\n\n        def hot_patch(required, existing):\n            if required not in namespace and existing in namespace:\n                namespace[required] = namespace[existing]\n\n        hot_patch(\'__iter__\', \'get_data\')\n        hot_patch(\'__len__\', \'size\')\n\n        return ABCMeta.__new__(mcls, name, bases, namespace, **kwargs)\n\n\n@six.add_metaclass(DataFlowMeta)\nclass DataFlow(object):\n    """""" Base class for all DataFlow """"""\n\n    @abstractmethod\n    def __iter__(self):\n        """"""\n        * A dataflow is an iterable. The :meth:`__iter__` method should yield a list or dict each time.\n          Note that dict is **partially** supported at the moment: certain dataflow does not support dict.\n\n        * The :meth:`__iter__` method can be either finite (will stop iteration) or infinite\n          (will not stop iteration). For a finite dataflow, :meth:`__iter__` can be called\n          again immediately after the previous call returned.\n\n        * For many dataflow, the :meth:`__iter__` method is non-reentrant, which means for an dataflow\n          instance ``df``, :meth:`df.__iter__` cannot be called before the previous\n          :meth:`df.__iter__` call has finished (iteration has stopped).\n          When a dataflow is non-reentrant, :meth:`df.__iter__` should throw an exception if\n          called before the previous call has finished.\n          For such non-reentrant dataflows, if you need to use the same dataflow in two places,\n          you need to create two dataflow instances.\n\n        Yields:\n            list/dict: The datapoint, i.e. list/dict of components.\n        """"""\n\n    def __len__(self):\n        """"""\n        * A dataflow can optionally implement :meth:`__len__`. If not implemented, it will\n          throw :class:`NotImplementedError`.\n\n        * It returns an integer representing the size of the dataflow.\n          The return value **may not be accurate or meaningful** at all.\n          When saying the length is ""accurate"", it means that\n          :meth:`__iter__` will always yield this many of datapoints before it stops iteration.\n\n        * There could be many reasons why :meth:`__len__` is inaccurate.\n          For example, some dataflow has dynamic size, if it throws away datapoints on the fly.\n          Some dataflow mixes the datapoints between consecutive passes over\n          the dataset, due to parallelism and buffering.\n          In this case it does not make sense to stop the iteration anywhere.\n\n        * Due to the above reasons, the length is only a rough guidance.\n          And it\'s up to the user how to interpret it.\n          Inside tensorpack it\'s only used in these places:\n\n          + A default ``steps_per_epoch`` in training, but you probably want to customize\n            it yourself, especially when using data-parallel trainer.\n          + The length of progress bar when processing a dataflow.\n          + Used by :class:`InferenceRunner` to get the number of iterations in inference.\n            In this case users are **responsible** for making sure that :meth:`__len__` is ""accurate"".\n            This is to guarantee that inference is run on a fixed set of images.\n\n        Returns:\n            int: rough size of this dataflow.\n\n        Raises:\n            :class:`NotImplementedError` if this DataFlow doesn\'t have a size.\n        """"""\n        raise NotImplementedError()\n\n    def reset_state(self):\n        """"""\n        * The caller must guarantee that :meth:`reset_state` should be called **once and only once**\n          by the **process that uses the dataflow** before :meth:`__iter__` is called.\n          The caller thread of this method should stay alive to keep this dataflow alive.\n\n        * It is meant for certain initialization that involves processes,\n          e.g., initialize random number generators (RNG), create worker processes.\n\n          Because it\'s very common to use RNG in data processing,\n          developers of dataflow can also subclass :class:`RNGDataFlow` to have easier access to\n          a properly-initialized RNG.\n\n        * A dataflow is not fork-safe after :meth:`reset_state` is called (because this will violate the guarantee).\n          There are a few other dataflows that are not fork-safe anytime, which will be mentioned in the docs.\n\n        * You should take the responsibility and follow the above guarantee if you\'re the caller of a dataflow yourself\n          (either when you\'re using dataflow outside of tensorpack, or if you\'re writing a wrapper dataflow).\n\n        * Tensorpack\'s built-in forking dataflows (:class:`MultiProcessRunner`, :class:`MultiProcessMapData`, etc)\n          and other component that uses dataflows (:class:`InputSource`)\n          already take care of the responsibility of calling this method.\n        """"""\n        pass\n\n    # These are the old (overly verbose) names for the methods:\n    def get_data(self):\n        return self.__iter__()\n\n    def size(self):\n        return self.__len__()\n\n\nclass RNGDataFlow(DataFlow):\n    """""" A DataFlow with RNG""""""\n\n    rng = None\n    """"""\n    ``self.rng`` is a ``np.random.RandomState`` instance that is initialized\n    correctly (with different seeds in each process) in ``RNGDataFlow.reset_state()``.\n    """"""\n\n    def reset_state(self):\n        """""" Reset the RNG """"""\n        self.rng = get_rng(self)\n\n\nclass ProxyDataFlow(DataFlow):\n    """""" Base class for DataFlow that proxies another.\n        Every method is proxied to ``self.ds`` unless overriden by a subclass.\n    """"""\n\n    def __init__(self, ds):\n        """"""\n        Args:\n            ds (DataFlow): DataFlow to proxy.\n        """"""\n        self.ds = ds\n\n    def reset_state(self):\n        self.ds.reset_state()\n\n    def __len__(self):\n        return self.ds.__len__()\n\n    def __iter__(self):\n        return self.ds.__iter__()\n'"
tensorpack/dataflow/common.py,0,"b'# -*- coding: utf-8 -*-\n# File: common.py\n\nfrom __future__ import division\nimport itertools\nimport numpy as np\nimport pprint\nfrom collections import defaultdict, deque\nfrom copy import copy\nimport six\nimport tqdm\nfrom termcolor import colored\n\nfrom ..utils import logger\nfrom ..utils.utils import get_rng, get_tqdm, get_tqdm_kwargs\nfrom .base import DataFlow, DataFlowReentrantGuard, ProxyDataFlow, RNGDataFlow\n\ntry:\n    from collections.abc import Mapping\nexcept ImportError:\n    from collections import Mapping\n\n__all__ = [\'TestDataSpeed\', \'PrintData\', \'BatchData\', \'BatchDataByShape\', \'FixedSizeData\', \'MapData\',\n           \'MapDataComponent\', \'RepeatedData\', \'RepeatedDataPoint\', \'RandomChooseData\',\n           \'RandomMixData\', \'JoinData\', \'ConcatData\', \'SelectComponent\',\n           \'LocallyShuffleData\', \'CacheData\']\n\n\nclass TestDataSpeed(ProxyDataFlow):\n    """""" Test the speed of a DataFlow """"""\n    def __init__(self, ds, size=5000, warmup=0):\n        """"""\n        Args:\n            ds (DataFlow): the DataFlow to test.\n            size (int): number of datapoints to fetch.\n            warmup (int): warmup iterations\n        """"""\n        super(TestDataSpeed, self).__init__(ds)\n        self.test_size = int(size)\n        self.warmup = int(warmup)\n        self._reset_called = False\n\n    def reset_state(self):\n        self._reset_called = True\n        super(TestDataSpeed, self).reset_state()\n\n    def __iter__(self):\n        """""" Will run testing at the beginning, then produce data normally. """"""\n        self.start()\n        yield from self.ds\n\n    def start(self):\n        """"""\n        Start testing with a progress bar.\n        """"""\n        if not self._reset_called:\n            self.ds.reset_state()\n        itr = self.ds.__iter__()\n        if self.warmup:\n            for _ in tqdm.trange(self.warmup, **get_tqdm_kwargs()):\n                next(itr)\n        # add smoothing for speed benchmark\n        with get_tqdm(total=self.test_size,\n                      leave=True, smoothing=0.2) as pbar:\n            for idx, dp in enumerate(itr):\n                pbar.update()\n                if idx == self.test_size - 1:\n                    break\n\n\nclass BatchData(ProxyDataFlow):\n    """"""\n    Stack datapoints into batches.\n    It produces datapoints of the same number of components as ``ds``, but\n    each component has one new extra dimension of size ``batch_size``.\n    The batch can be either a list of original components, or (by default)\n    a numpy array of original components.\n    """"""\n\n    def __init__(self, ds, batch_size, remainder=False, use_list=False):\n        """"""\n        Args:\n            ds (DataFlow): A dataflow that produces either list or dict.\n                When ``use_list=False``, the components of ``ds``\n                must be either scalars or :class:`np.ndarray`, and have to be consistent in shapes.\n            batch_size(int): batch size\n            remainder (bool): When the remaining datapoints in ``ds`` is not\n                enough to form a batch, whether or not to also produce the remaining\n                data as a smaller batch.\n                If set to False, all produced datapoints are guaranteed to have the same batch size.\n                If set to True, `len(ds)` must be accurate.\n            use_list (bool): if True, each component will contain a list\n                of datapoints instead of an numpy array of an extra dimension.\n        """"""\n        super(BatchData, self).__init__(ds)\n        if not remainder:\n            try:\n                assert batch_size <= len(ds)\n            except NotImplementedError:\n                pass\n        self.batch_size = int(batch_size)\n        assert self.batch_size > 0\n        self.remainder = remainder\n        self.use_list = use_list\n\n    def __len__(self):\n        ds_size = len(self.ds)\n        div = ds_size // self.batch_size\n        rem = ds_size % self.batch_size\n        if rem == 0:\n            return div\n        return div + int(self.remainder)\n\n    def __iter__(self):\n        """"""\n        Yields:\n            Batched data by stacking each component on an extra 0th dimension.\n        """"""\n        holder = []\n        for data in self.ds:\n            holder.append(data)\n            if len(holder) == self.batch_size:\n                yield BatchData.aggregate_batch(holder, self.use_list)\n                del holder[:]\n        if self.remainder and len(holder) > 0:\n            yield BatchData.aggregate_batch(holder, self.use_list)\n\n    @staticmethod\n    def _batch_numpy(data_list):\n        data = data_list[0]\n        if isinstance(data, six.integer_types):\n            dtype = \'int32\'\n        elif type(data) == bool:\n            dtype = \'bool\'\n        elif type(data) == float:\n            dtype = \'float32\'\n        elif isinstance(data, (six.binary_type, six.text_type)):\n            dtype = \'str\'\n        else:\n            try:\n                dtype = data.dtype\n            except AttributeError:\n                raise TypeError(""Unsupported type to batch: {}"".format(type(data)))\n        try:\n            return np.asarray(data_list, dtype=dtype)\n        except Exception as e:  # noqa\n            logger.exception(""Cannot batch data. Perhaps they are of inconsistent shape?"")\n            if isinstance(data, np.ndarray):\n                s = pprint.pformat([x.shape for x in data_list])\n                logger.error(""Shape of all arrays to be batched: "" + s)\n            try:\n                # open an ipython shell if possible\n                import IPython as IP; IP.embed()    # noqa\n            except ImportError:\n                pass\n\n    @staticmethod\n    def aggregate_batch(data_holder, use_list=False):\n        """"""\n        Aggregate a list of datapoints to one batched datapoint.\n\n        Args:\n            data_holder (list[dp]): each dp is either a list or a dict.\n            use_list (bool): whether to batch data into a list or a numpy array.\n\n        Returns:\n            dp:\n                either a list or a dict, depend on the inputs.\n                Each item is a batched version of the corresponding inputs.\n        """"""\n        first_dp = data_holder[0]\n        if isinstance(first_dp, (list, tuple)):\n            result = []\n            for k in range(len(first_dp)):\n                data_list = [x[k] for x in data_holder]\n                if use_list:\n                    result.append(data_list)\n                else:\n                    result.append(BatchData._batch_numpy(data_list))\n        elif isinstance(first_dp, dict):\n            result = {}\n            for key in first_dp.keys():\n                data_list = [x[key] for x in data_holder]\n                if use_list:\n                    result[key] = data_list\n                else:\n                    result[key] = BatchData._batch_numpy(data_list)\n        else:\n            raise ValueError(""Data point has to be list/tuple/dict. Got {}"".format(type(first_dp)))\n        return result\n\n\nclass BatchDataByShape(BatchData):\n    """"""\n    Group datapoints of the same shape together to batches.\n    It doesn\'t require input DataFlow to be homogeneous anymore: it can have\n    datapoints of different shape, and batches will be formed from those who\n    have the same shape.\n\n    Note:\n        It is implemented by a dict{shape -> datapoints}.\n        Therefore, datapoints of uncommon shapes may never be enough to form a batch and\n        never get generated.\n    """"""\n    def __init__(self, ds, batch_size, idx):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow. ``dp[idx]`` has to be an :class:`np.ndarray`.\n            batch_size (int): batch size\n            idx (int): ``dp[idx].shape`` will be used to group datapoints.\n                Other components are assumed to be batch-able.\n        """"""\n        super(BatchDataByShape, self).__init__(ds, batch_size, remainder=False)\n        self.idx = idx\n\n    def reset_state(self):\n        super(BatchDataByShape, self).reset_state()\n        self.holder = defaultdict(list)\n        self._guard = DataFlowReentrantGuard()\n\n    def __iter__(self):\n        with self._guard:\n            for dp in self.ds:\n                shp = dp[self.idx].shape\n                holder = self.holder[shp]\n                holder.append(dp)\n                if len(holder) == self.batch_size:\n                    yield BatchData.aggregate_batch(holder)\n                    del holder[:]\n\n\nclass FixedSizeData(ProxyDataFlow):\n    """""" Generate data from another DataFlow, but with a fixed total count.\n    """"""\n    def __init__(self, ds, size, keep_state=True):\n        """"""\n        Args:\n            ds (DataFlow): input dataflow\n            size (int): size\n            keep_state (bool): keep the iterator state of ``ds``\n                between calls to :meth:`__iter__()`, so that the\n                next call will continue the previous iteration over ``ds``,\n                instead of reinitializing an iterator.\n\n        Example:\n\n        .. code-block:: none\n\n            ds produces: 1, 2, 3, 4, 5; 1, 2, 3, 4, 5; ...\n            FixedSizeData(ds, 3, True): 1, 2, 3; 4, 5, 1; 2, 3, 4; ...\n            FixedSizeData(ds, 3, False): 1, 2, 3; 1, 2, 3; ...\n            FixedSizeData(ds, 6, False): 1, 2, 3, 4, 5, 1; 1, 2, 3, 4, 5, 1;...\n        """"""\n        super(FixedSizeData, self).__init__(ds)\n        self._size = int(size)\n        self.itr = None\n        self._keep = keep_state\n\n    def __len__(self):\n        return self._size\n\n    def reset_state(self):\n        super(FixedSizeData, self).reset_state()\n        self.itr = self.ds.__iter__()\n        self._guard = DataFlowReentrantGuard()\n\n    def __iter__(self):\n        with self._guard:\n            if self.itr is None:\n                self.itr = self.ds.__iter__()\n            cnt = 0\n            while True:\n                try:\n                    dp = next(self.itr)\n                except StopIteration:\n                    self.itr = self.ds.__iter__()\n                    dp = next(self.itr)\n\n                cnt += 1\n                yield dp\n                if cnt == self._size:\n                    if not self._keep:\n                        self.itr = None\n                    return\n\n\nclass MapData(ProxyDataFlow):\n    """"""\n    Apply a mapper/filter on the datapoints of a DataFlow.\n\n    Note:\n        1. Please make sure func doesn\'t modify its arguments in place,\n           unless you\'re certain it\'s safe.\n        2. If you discard some datapoints, ``len(MapData(ds))`` will be incorrect.\n\n    Example:\n\n        .. code-block:: none\n\n            ds = Mnist(\'train\')  # each datapoint is [img, label]\n            ds = MapData(ds, lambda dp: [dp[0] * 255, dp[1]])\n    """"""\n\n    def __init__(self, ds, func):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow\n            func (datapoint -> datapoint | None): takes a datapoint and returns a new\n                datapoint. Return None to discard/skip this datapoint.\n        """"""\n        super(MapData, self).__init__(ds)\n        self.func = func\n\n    def __iter__(self):\n        for dp in self.ds:\n            ret = self.func(copy(dp))  # shallow copy the list\n            if ret is not None:\n                yield ret\n\n\nclass MapDataComponent(MapData):\n    """"""\n    Apply a mapper/filter on a datapoint component.\n\n    Note:\n        1. This dataflow itself doesn\'t modify the datapoints.\n           But please make sure func doesn\'t modify its arguments in place,\n           unless you\'re certain it\'s safe.\n        2. If you discard some datapoints, ``len(MapDataComponent(ds, ..))`` will be incorrect.\n\n    Example:\n\n        .. code-block:: none\n\n            ds = Mnist(\'train\')  # each datapoint is [img, label]\n            ds = MapDataComponent(ds, lambda img: img * 255, 0)  # map the 0th component\n    """"""\n    def __init__(self, ds, func, index=0):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow which produces either list or dict.\n            func (TYPE -> TYPE|None): takes ``dp[index]``, returns a new value for ``dp[index]``.\n                Return None to discard/skip this datapoint.\n            index (int or str): index or key of the component.\n        """"""\n        self._index = index\n        self._func = func\n        super(MapDataComponent, self).__init__(ds, self._mapper)\n\n    def _mapper(self, dp):\n        r = self._func(dp[self._index])\n        if r is None:\n            return None\n        dp = copy(dp)   # shallow copy to avoid modifying the datapoint\n        if isinstance(dp, tuple):\n            dp = list(dp)  # to be able to modify it in the next line\n        dp[self._index] = r\n        return dp\n\n\nclass RepeatedData(ProxyDataFlow):\n    """""" Take data points from another DataFlow and produce them until\n        it\'s exhausted for certain amount of times. i.e.:\n        dp1, dp2, .... dpn, dp1, dp2, ....dpn\n    """"""\n\n    def __init__(self, ds, num):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow\n            num (int): number of times to repeat ds.\n                Set to -1 to repeat ``ds`` infinite times.\n        """"""\n        self.num = num\n        super(RepeatedData, self).__init__(ds)\n\n    def __len__(self):\n        """"""\n        Raises:\n            :class:`ValueError` when num == -1.\n        """"""\n        if self.num == -1:\n            raise NotImplementedError(""__len__() is unavailable for infinite dataflow"")\n        return len(self.ds) * self.num\n\n    def __iter__(self):\n        if self.num == -1:\n            while True:\n                yield from self.ds\n        else:\n            for _ in range(self.num):\n                yield from self.ds\n\n\nclass RepeatedDataPoint(ProxyDataFlow):\n    """""" Take data points from another DataFlow and produce them a\n    certain number of times. i.e.:\n    dp1, dp1, ..., dp1, dp2, ..., dp2, ...\n    """"""\n\n    def __init__(self, ds, num):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow\n            num (int): number of times to repeat each datapoint.\n        """"""\n        self.num = int(num)\n        assert self.num >= 1, self.num\n        super(RepeatedDataPoint, self).__init__(ds)\n\n    def __len__(self):\n        return len(self.ds) * self.num\n\n    def __iter__(self):\n        for dp in self.ds:\n            for _ in range(self.num):\n                yield dp\n\n\nclass RandomChooseData(RNGDataFlow):\n    """"""\n    Randomly choose from several DataFlow.\n    Stop producing when any of them is exhausted.\n    """"""\n\n    def __init__(self, df_lists):\n        """"""\n        Args:\n            df_lists (list): a list of DataFlow, or a list of (DataFlow, probability) tuples.\n                Probabilities must sum to 1 if used.\n        """"""\n        super(RandomChooseData, self).__init__()\n        if isinstance(df_lists[0], (tuple, list)):\n            assert sum(v[1] for v in df_lists) == 1.0\n            self.df_lists = df_lists\n        else:\n            prob = 1.0 / len(df_lists)\n            self.df_lists = [(k, prob) for k in df_lists]\n\n    def reset_state(self):\n        super(RandomChooseData, self).reset_state()\n        for d in self.df_lists:\n            if isinstance(d, tuple):\n                d[0].reset_state()\n            else:\n                d.reset_state()\n\n    def __iter__(self):\n        itrs = [v[0].__iter__() for v in self.df_lists]\n        probs = np.array([v[1] for v in self.df_lists])\n        try:\n            while True:\n                itr = self.rng.choice(itrs, p=probs)\n                yield next(itr)\n        except StopIteration:\n            return\n\n\nclass RandomMixData(RNGDataFlow):\n    """"""\n    Perfectly mix datapoints from several DataFlow using their\n    :meth:`__len__()`. Will stop when all DataFlow exhausted.\n    """"""\n\n    def __init__(self, df_lists):\n        """"""\n        Args:\n            df_lists (list): a list of DataFlow.\n                All DataFlow must implement ``__len__()``.\n        """"""\n        super(RandomMixData, self).__init__()\n        self.df_lists = df_lists\n        self.sizes = [len(k) for k in self.df_lists]\n\n    def reset_state(self):\n        super(RandomMixData, self).reset_state()\n        for d in self.df_lists:\n            d.reset_state()\n\n    def __len__(self):\n        return sum(self.sizes)\n\n    def __iter__(self):\n        sums = np.cumsum(self.sizes)\n        idxs = np.arange(self.__len__())\n        self.rng.shuffle(idxs)\n        idxs = np.array(list(map(\n            lambda x: np.searchsorted(sums, x, \'right\'), idxs)))\n        itrs = [k.__iter__() for k in self.df_lists]\n        assert idxs.max() == len(itrs) - 1, ""{}!={}"".format(idxs.max(), len(itrs) - 1)\n        for k in idxs:\n            yield next(itrs[k])\n        # TODO run till exception\n\n\nclass ConcatData(DataFlow):\n    """"""\n    Concatenate several DataFlow.\n    Produce datapoints from each DataFlow and start the next when one\n    DataFlow is exhausted.\n    """"""\n\n    def __init__(self, df_lists):\n        """"""\n        Args:\n            df_lists (list): a list of DataFlow.\n        """"""\n        self.df_lists = df_lists\n\n    def reset_state(self):\n        for d in self.df_lists:\n            d.reset_state()\n\n    def __len__(self):\n        return sum(len(x) for x in self.df_lists)\n\n    def __iter__(self):\n        for d in self.df_lists:\n            yield from d\n\n\nclass JoinData(DataFlow):\n    """"""\n    Join the components from each DataFlow. See below for its behavior.\n\n    Note that you can\'t join a DataFlow that produces lists with one that produces dicts.\n\n    Example:\n\n    .. code-block:: none\n\n        df1 produces: [c1, c2]\n        df2 produces: [c3, c4]\n        joined: [c1, c2, c3, c4]\n\n        df1 produces: {""a"":c1, ""b"":c2}\n        df2 produces: {""c"":c3}\n        joined: {""a"":c1, ""b"":c2, ""c"":c3}\n    """"""\n\n    def __init__(self, df_lists):\n        """"""\n        Args:\n            df_lists (list): a list of DataFlow.\n                When these dataflows have different sizes, JoinData will stop when any\n                of them is exhausted.\n                The list could contain the same DataFlow instance more than once,\n                but note that in that case `__iter__` will then also be called many times.\n        """"""\n        self.df_lists = df_lists\n\n        try:\n            self._size = len(self.df_lists[0])\n            for d in self.df_lists:\n                assert len(d) == self._size, \\\n                    ""All DataFlow must have the same size! {} != {}"".format(len(d), self._size)\n        except Exception:\n            logger.info(""[JoinData] Size check failed for the list of dataflow to be joined!"")\n\n    def reset_state(self):\n        for d in set(self.df_lists):\n            d.reset_state()\n\n    def __len__(self):\n        """"""\n        Return the minimum size among all.\n        """"""\n        return min(len(k) for k in self.df_lists)\n\n    def __iter__(self):\n        itrs = [k.__iter__() for k in self.df_lists]\n        try:\n            while True:\n                all_dps = [next(itr) for itr in itrs]\n                if isinstance(all_dps[0], (list, tuple)):\n                    dp = list(itertools.chain(*all_dps))\n                else:\n                    dp = {}\n                    for x in all_dps:\n                        dp.update(x)\n                yield dp\n        except StopIteration:   # some of them are exhausted\n            pass\n\n\ndef SelectComponent(ds, idxs):\n    """"""\n    Select / reorder components from datapoints.\n\n    Args:\n        ds (DataFlow): input DataFlow.\n        idxs (list[int] or list[str]): a list of component indices/keys.\n\n    Example:\n\n    .. code-block:: none\n\n        original df produces: [c1, c2, c3]\n        idxs: [2,1]\n        this df: [c3, c2]\n    """"""\n    return MapData(ds, lambda dp: [dp[i] for i in idxs])\n\n\nclass LocallyShuffleData(ProxyDataFlow, RNGDataFlow):\n    """""" Buffer the datapoints from a given dataflow, and shuffle them before producing them.\n        This can be used as an alternative when a complete random shuffle is too expensive\n        or impossible for the data source.\n\n        This dataflow has the following behavior:\n\n        1. It takes datapoints from the given dataflow `ds` to an internal buffer of fixed size.\n           Each datapoint is duplicated for `num_reuse` times.\n        2. Once the buffer is full, this dataflow starts to yield data from the beginning of the buffer,\n           and new datapoints will be added to the end of the buffer. This is like a FIFO queue.\n        3. The internal buffer is shuffled after every `shuffle_interval` datapoints that come from `ds`.\n\n        To maintain shuffling states, this dataflow is not reentrant.\n\n        Datapoints from one pass of `ds` will get mixed with datapoints from a different pass.\n        As a result, the iterator of this dataflow will run indefinitely\n        because it does not make sense to stop the iteration anywhere.\n    """"""\n\n    def __init__(self, ds, buffer_size, num_reuse=1, shuffle_interval=None):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow.\n            buffer_size (int): size of the buffer.\n            num_reuse (int): duplicate each datapoints several times into the buffer to improve\n                speed, but duplication may hurt your model.\n            shuffle_interval (int): shuffle the buffer after this many\n                datapoints were produced from the given dataflow. Frequent shuffle on large buffer\n                may affect speed, but infrequent shuffle may not provide enough randomness.\n                Defaults to buffer_size / 3\n        """"""\n        ProxyDataFlow.__init__(self, ds)\n        self.q = deque(maxlen=buffer_size)\n        if shuffle_interval is None:\n            shuffle_interval = int(buffer_size // 3)\n        self.shuffle_interval = shuffle_interval\n        self.num_reuse = num_reuse\n        self._inf_ds = RepeatedData(ds, -1)\n\n    def reset_state(self):\n        self._guard = DataFlowReentrantGuard()\n        ProxyDataFlow.reset_state(self)\n        RNGDataFlow.reset_state(self)\n        self._iter_cnt = 0\n        self._inf_iter = iter(self._inf_ds)\n\n    def __len__(self):\n        return len(self.ds) * self.num_reuse\n\n    def __iter__(self):\n        with self._guard:\n            for dp in self._inf_iter:\n                self._iter_cnt = (self._iter_cnt + 1) % self.shuffle_interval\n                # fill queue\n                if self._iter_cnt == 0:\n                    self.rng.shuffle(self.q)\n                for _ in range(self.num_reuse):\n                    if self.q.maxlen == len(self.q):\n                        yield self.q.popleft()\n                    self.q.append(dp)\n\n\nclass CacheData(ProxyDataFlow):\n    """"""\n    Completely cache the first pass of a DataFlow in memory,\n    and produce from the cache thereafter.\n\n    NOTE: The user should not stop the iterator before it has reached the end.\n        Otherwise the cache may be incomplete.\n    """"""\n    def __init__(self, ds, shuffle=False):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow.\n            shuffle (bool): whether to shuffle the cache before yielding from it.\n        """"""\n        self.shuffle = shuffle\n        super(CacheData, self).__init__(ds)\n\n    def reset_state(self):\n        super(CacheData, self).reset_state()\n        self._guard = DataFlowReentrantGuard()\n        if self.shuffle:\n            self.rng = get_rng(self)\n        self.buffer = []\n\n    def __iter__(self):\n        with self._guard:\n            if len(self.buffer):\n                if self.shuffle:\n                    self.rng.shuffle(self.buffer)\n                yield from self.buffer\n            else:\n                for dp in self.ds:\n                    yield dp\n                    self.buffer.append(dp)\n\n\nclass PrintData(ProxyDataFlow):\n    """"""\n    Behave like an identity proxy, but print shape and range of the first few datapoints.\n    Good for debugging.\n\n    Example:\n        Place it somewhere in your dataflow like\n\n        .. code-block:: python\n\n            def create_my_dataflow():\n                ds = SomeDataSource(\'path/to/lmdb\')\n                ds = SomeInscrutableMappings(ds)\n                ds = PrintData(ds, num=2, max_list=2)\n                return ds\n            ds = create_my_dataflow()\n            # other code that uses ds\n\n        When datapoints are taken from the dataflow, it will print outputs like:\n\n        .. code-block:: none\n\n            [0110 09:22:21 @common.py:589] DataFlow Info:\n            datapoint 0<2 with 4 components consists of\n               0: float with value 0.0816501893251\n               1: ndarray:int32 of shape (64,) in range [0, 10]\n               2: ndarray:float32 of shape (64, 64) in range [-1.2248, 1.2177]\n               3: list of len 50\n                  0: ndarray:int32 of shape (64, 64) in range [-128, 80]\n                  1: ndarray:float32 of shape (64, 64) in range [0.8400, 0.6845]\n                  ...\n            datapoint 1<2 with 4 components consists of\n               0: float with value 5.88252075399\n               1: ndarray:int32 of shape (64,) in range [0, 10]\n               2: ndarray:float32 of shape (64, 64) with range [-0.9011, 0.8491]\n               3: list of len 50\n                  0: ndarray:int32 of shape (64, 64) in range [-70, 50]\n                  1: ndarray:float32 of shape (64, 64) in range [0.7400, 0.3545]\n                  ...\n    """"""\n\n    def __init__(self, ds, num=1, name=None, max_depth=3, max_list=3):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow.\n            num (int): number of dataflow points to print.\n            name (str, optional): name to identify this DataFlow.\n            max_depth (int, optional): stop output when too deep recursion in sub elements\n            max_list (int, optional): stop output when too many sub elements\n        """"""\n        super(PrintData, self).__init__(ds)\n        self.num = num\n        self.name = name\n        self.cnt = 0\n        self.max_depth = max_depth\n        self.max_list = max_list\n\n    def _analyze_input_data(self, entry, k, depth=1, max_depth=3, max_list=3):\n        """"""\n        Gather useful debug information from a datapoint.\n\n        Args:\n            entry: the datapoint component, either a list or a dict\n            k (int): index of this component in current datapoint\n            depth (int, optional): recursion depth\n            max_depth, max_list: same as in :meth:`__init__`.\n\n        Returns:\n            string: debug message\n        """"""\n\n        class _elementInfo(object):\n            def __init__(self, el, pos, depth=0, max_list=3):\n                self.shape = """"\n                self.type = type(el).__name__\n                self.dtype = """"\n                self.range = """"\n\n                self.sub_elements = []\n\n                self.ident = "" "" * (depth * 2)\n                self.pos = pos\n\n                numpy_scalar_types = list(itertools.chain(*np.sctypes.values()))\n\n                if isinstance(el, (int, float, bool)):\n                    self.range = "" with value {}"".format(el)\n                elif type(el) is np.ndarray:\n                    self.shape = "" of shape {}"".format(el.shape)\n                    self.dtype = "":{}"".format(str(el.dtype))\n                    self.range = "" in range [{}, {}]"".format(el.min(), el.max())\n                elif type(el) in numpy_scalar_types:\n                    self.range = "" with value {}"".format(el)\n                elif isinstance(el, (list, tuple)):\n                    self.shape = "" of len {}"".format(len(el))\n\n                    if depth < max_depth:\n                        for k, subel in enumerate(el):\n                            if k < max_list:\n                                self.sub_elements.append(_elementInfo(subel, k, depth + 1, max_list))\n                            else:\n                                self.sub_elements.append("" "" * ((depth + 1) * 2) + \'...\')\n                                break\n                    else:\n                        if len(el) > 0:\n                            self.sub_elements.append("" "" * ((depth + 1) * 2) + \' ...\')\n\n            def __str__(self):\n                strings = []\n                vals = (self.ident, self.pos, self.type, self.dtype, self.shape, self.range)\n                strings.append(""{}{}: {}{}{}{}"".format(*vals))\n\n                for k, el in enumerate(self.sub_elements):\n                    strings.append(str(el))\n                return ""\\n"".join(strings)\n\n        return str(_elementInfo(entry, k, depth, max_list))\n\n    def _get_msg(self, dp):\n        msg = [colored(u""datapoint %i/%i with %i components consists of"" %\n               (self.cnt, self.num, len(dp)), ""cyan"")]\n        is_dict = isinstance(dp, Mapping)\n        for k, entry in enumerate(dp):\n            if is_dict:\n                key, value = entry, dp[entry]\n            else:\n                key, value = k, entry\n            msg.append(self._analyze_input_data(value, key, max_depth=self.max_depth, max_list=self.max_list))\n        return u\'\\n\'.join(msg)\n\n    def __iter__(self):\n        for dp in self.ds:\n            # it is important to place this here! otherwise it mixes the output of multiple PrintData\n            if self.cnt == 0:\n                label = \' (%s)\' % self.name if self.name is not None else """"\n                logger.info(colored(""Contents of DataFlow%s:"" % label, \'cyan\'))\n\n            if self.cnt < self.num:\n                print(self._get_msg(dp))\n                self.cnt += 1\n            yield dp\n\n    def reset_state(self):\n        super(PrintData, self).reset_state()\n        self.cnt = 0\n'"
tensorpack/dataflow/format.py,0,"b'# -*- coding: utf-8 -*-\n# File: format.py\n\n\nimport numpy as np\nimport os\nimport six\n\nfrom ..utils import logger\nfrom ..utils.argtools import log_once\nfrom ..utils.serialize import loads\nfrom ..utils.develop import create_dummy_class  # noqa\nfrom ..utils.loadcaffe import get_caffe_pb\nfrom ..utils.timer import timed_operation\nfrom ..utils.utils import get_tqdm\nfrom .base import DataFlowReentrantGuard, RNGDataFlow\nfrom .common import MapData\n\n__all__ = [\'HDF5Data\', \'LMDBData\', \'LMDBDataDecoder\',\n           \'CaffeLMDB\', \'SVMLightData\']\n\n""""""\nAdapters for different data format.\n""""""\n\n\nclass HDF5Data(RNGDataFlow):\n    """"""\n    Zip data from different paths in an HDF5 file.\n\n    Warning:\n        The current implementation will load all data into memory. (TODO)\n    """"""\n# TODO\n\n    def __init__(self, filename, data_paths, shuffle=True):\n        """"""\n        Args:\n            filename (str): h5 data file.\n            data_paths (list): list of h5 paths to zipped.\n                For example `[\'images\', \'labels\']`.\n            shuffle (bool): shuffle all data.\n        """"""\n        self.f = h5py.File(filename, \'r\')\n        logger.info(""Loading {} to memory..."".format(filename))\n        self.dps = [self.f[k].value for k in data_paths]\n        lens = [len(k) for k in self.dps]\n        assert all(k == lens[0] for k in lens)\n        self._size = lens[0]\n        self.shuffle = shuffle\n\n    def __len__(self):\n        return self._size\n\n    def __iter__(self):\n        idxs = list(range(self._size))\n        if self.shuffle:\n            self.rng.shuffle(idxs)\n        for k in idxs:\n            yield [dp[k] for dp in self.dps]\n\n\nclass LMDBData(RNGDataFlow):\n    """"""\n    Read a LMDB database and produce (k,v) raw bytes pairs.\n    The raw bytes are usually not what you\'re interested in.\n    You might want to use\n    :class:`LMDBDataDecoder` or apply a\n    mapper function after :class:`LMDBData`.\n    """"""\n    def __init__(self, lmdb_path, shuffle=True, keys=None):\n        """"""\n        Args:\n            lmdb_path (str): a directory or a file.\n            shuffle (bool): shuffle the keys or not.\n            keys (list[str] or str): list of str as the keys, used only when shuffle is True.\n                It can also be a format string e.g. ``{:0>8d}`` which will be\n                formatted with the indices from 0 to *total_size - 1*.\n\n                If not given, it will then look in the database for ``__keys__`` which\n                :func:`LMDBSerializer.save` used to store the list of keys.\n                If still not found, it will iterate over the database to find\n                all the keys.\n        """"""\n        self._lmdb_path = lmdb_path\n        self._shuffle = shuffle\n\n        self._open_lmdb()\n        self._size = self._txn.stat()[\'entries\']\n        self._set_keys(keys)\n        logger.info(""Found {} entries in {}"".format(self._size, self._lmdb_path))\n\n        # Clean them up after finding the list of keys, since we don\'t want to fork them\n        self._close_lmdb()\n\n    def _set_keys(self, keys=None):\n        def find_keys(txn, size):\n            logger.warn(""Traversing the database to find keys is slow. Your should specify the keys."")\n            keys = []\n            with timed_operation(""Loading LMDB keys ..."", log_start=True), \\\n                    get_tqdm(total=size) as pbar:\n                for k in self._txn.cursor():\n                    assert k[0] != b\'__keys__\'\n                    keys.append(k[0])\n                    pbar.update()\n            return keys\n\n        self.keys = self._txn.get(b\'__keys__\')\n        if self.keys is not None:\n            self.keys = loads(self.keys)\n            self._size -= 1     # delete this item\n\n        if self._shuffle:   # keys are necessary when shuffle is True\n            if keys is None:\n                if self.keys is None:\n                    self.keys = find_keys(self._txn, self._size)\n            else:\n                # check if key-format like \'{:0>8d}\' was given\n                if isinstance(keys, six.string_types):\n                    self.keys = map(lambda x: keys.format(x), list(np.arange(self._size)))\n                else:\n                    self.keys = keys\n\n    def _open_lmdb(self):\n        self._lmdb = lmdb.open(self._lmdb_path,\n                               subdir=os.path.isdir(self._lmdb_path),\n                               readonly=True, lock=False, readahead=True,\n                               map_size=1099511627776 * 2, max_readers=100)\n        self._txn = self._lmdb.begin()\n\n    def _close_lmdb(self):\n        self._lmdb.close()\n        del self._lmdb\n        del self._txn\n\n    def reset_state(self):\n        self._guard = DataFlowReentrantGuard()\n        super(LMDBData, self).reset_state()\n        self._open_lmdb()  # open the LMDB in the worker process\n\n    def __len__(self):\n        return self._size\n\n    def __iter__(self):\n        with self._guard:\n            if not self._shuffle:\n                c = self._txn.cursor()\n                for k, v in c:\n                    if k != b\'__keys__\':\n                        yield [k, v]\n            else:\n                self.rng.shuffle(self.keys)\n                for k in self.keys:\n                    v = self._txn.get(k)\n                    yield [k, v]\n\n\nclass LMDBDataDecoder(MapData):\n    """""" Read a LMDB database with a custom decoder and produce decoded outputs.""""""\n    def __init__(self, lmdb_data, decoder):\n        """"""\n        Args:\n            lmdb_data: a :class:`LMDBData` instance.\n            decoder (k,v -> dp | None): a function taking k, v and returning a datapoint,\n                or return None to discard.\n        """"""\n        def f(dp):\n            return decoder(dp[0], dp[1])\n        super(LMDBDataDecoder, self).__init__(lmdb_data, f)\n\n\ndef CaffeLMDB(lmdb_path, shuffle=True, keys=None):\n    """"""\n    Read a Caffe-format LMDB file where each value contains a ``caffe.Datum`` protobuf.\n    Produces datapoints of the format: [HWC image, label].\n\n    Note that Caffe LMDB format is not efficient: it stores serialized raw\n    arrays rather than JPEG images.\n\n    Args:\n        lmdb_path, shuffle, keys: same as :class:`LMDBData`.\n\n    Example:\n        .. code-block:: python\n\n            ds = CaffeLMDB(""/tmp/validation"", keys=\'{:0>8d}\')\n    """"""\n\n    cpb = get_caffe_pb()\n    lmdb_data = LMDBData(lmdb_path, shuffle, keys)\n\n    def decoder(k, v):\n        try:\n            datum = cpb.Datum()\n            datum.ParseFromString(v)\n            img = np.fromstring(datum.data, dtype=np.uint8)\n            img = img.reshape(datum.channels, datum.height, datum.width)\n        except Exception:\n            log_once(""Cannot read key {}"".format(k), \'warn\')\n            return None\n        return [img.transpose(1, 2, 0), datum.label]\n    logger.warn(""Caffe LMDB format doesn\'t store jpeg-compressed images, \\\n        it\'s not recommended due to its inferior performance."")\n    return LMDBDataDecoder(lmdb_data, decoder)\n\n\nclass SVMLightData(RNGDataFlow):\n    """""" Read X,y from an SVMlight file, and produce [X_i, y_i] pairs. """"""\n\n    def __init__(self, filename, shuffle=True):\n        """"""\n        Args:\n            filename (str): input file\n            shuffle (bool): shuffle the data\n        """"""\n        import sklearn.datasets  # noqa\n        self.X, self.y = sklearn.datasets.load_svmlight_file(filename)\n        self.X = np.asarray(self.X.todense())\n        self.shuffle = shuffle\n\n    def __len__(self):\n        return len(self.y)\n\n    def __iter__(self):\n        idxs = np.arange(self.__len__())\n        if self.shuffle:\n            self.rng.shuffle(idxs)\n        for id in idxs:\n            yield [self.X[id, :], self.y[id]]\n\n\ntry:\n    import h5py\nexcept ImportError:\n    HDF5Data = create_dummy_class(\'HDF5Data\', \'h5py\')   # noqa\n\ntry:\n    import lmdb\nexcept ImportError:\n    for klass in [\'LMDBData\', \'LMDBDataDecoder\', \'CaffeLMDB\']:\n        globals()[klass] = create_dummy_class(klass, \'lmdb\')\n'"
tensorpack/dataflow/image.py,0,"b'# -*- coding: utf-8 -*-\n# File: image.py\n\n\nimport copy as copy_mod\nimport numpy as np\nfrom contextlib import contextmanager\n\nfrom ..utils import logger\nfrom ..utils.argtools import shape2d\nfrom .base import RNGDataFlow\nfrom .common import MapData, MapDataComponent\n\n__all__ = [\'ImageFromFile\', \'AugmentImageComponent\', \'AugmentImageCoordinates\', \'AugmentImageComponents\']\n\n\ndef check_dtype(img):\n    assert isinstance(img, np.ndarray), ""[Augmentor] Needs an numpy array, but got a {}!"".format(type(img))\n    assert not isinstance(img.dtype, np.integer) or (img.dtype == np.uint8), \\\n        ""[Augmentor] Got image of type {}, use uint8 or floating points instead!"".format(img.dtype)\n\n\ndef validate_coords(coords):\n    assert coords.ndim == 2, coords.ndim\n    assert coords.shape[1] == 2, coords.shape\n    assert np.issubdtype(coords.dtype, np.float), coords.dtype\n\n\nclass ExceptionHandler:\n    def __init__(self, catch_exceptions=False):\n        self._nr_error = 0\n        self.catch_exceptions = catch_exceptions\n\n    @contextmanager\n    def catch(self):\n        try:\n            yield\n        except Exception:\n            self._nr_error += 1\n            if not self.catch_exceptions:\n                raise\n            else:\n                if self._nr_error % 100 == 0 or self._nr_error < 10:\n                    logger.exception(""Got {} augmentation errors."".format(self._nr_error))\n\n\nclass ImageFromFile(RNGDataFlow):\n    """""" Produce images read from a list of files as (h, w, c) arrays. """"""\n    def __init__(self, files, channel=3, resize=None, shuffle=False):\n        """"""\n        Args:\n            files (list): list of file paths.\n            channel (int): 1 or 3. Will convert grayscale to RGB images if channel==3.\n                Will produce (h, w, 1) array if channel==1.\n            resize (tuple): int or (h, w) tuple. If given, resize the image.\n        """"""\n        assert len(files), ""No image files given to ImageFromFile!""\n        self.files = files\n        self.channel = int(channel)\n        assert self.channel in [1, 3], self.channel\n        self.imread_mode = cv2.IMREAD_GRAYSCALE if self.channel == 1 else cv2.IMREAD_COLOR\n        if resize is not None:\n            resize = shape2d(resize)\n        self.resize = resize\n        self.shuffle = shuffle\n\n    def __len__(self):\n        return len(self.files)\n\n    def __iter__(self):\n        if self.shuffle:\n            self.rng.shuffle(self.files)\n        for f in self.files:\n            im = cv2.imread(f, self.imread_mode)\n            assert im is not None, f\n            if self.channel == 3:\n                im = im[:, :, ::-1]\n            if self.resize is not None:\n                im = cv2.resize(im, tuple(self.resize[::-1]))\n            if self.channel == 1:\n                im = im[:, :, np.newaxis]\n            yield [im]\n\n\nclass AugmentImageComponent(MapDataComponent):\n    """"""\n    Apply image augmentors on 1 image component.\n    """"""\n\n    def __init__(self, ds, augmentors, index=0, copy=True, catch_exceptions=False):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow.\n            augmentors (AugmentorList): a list of :class:`imgaug.ImageAugmentor` to be applied in order.\n            index (int or str): the index or key of the image component to be augmented in the datapoint.\n            copy (bool): Some augmentors modify the input images. When copy is\n                True, a copy will be made before any augmentors are applied,\n                to keep the original images not modified.\n                Turn it off to save time when you know it\'s OK.\n            catch_exceptions (bool): when set to True, will catch\n                all exceptions and only warn you when there are too many (>100).\n                Can be used to ignore occasion errors in data.\n        """"""\n        if isinstance(augmentors, AugmentorList):\n            self.augs = augmentors\n        else:\n            self.augs = AugmentorList(augmentors)\n        self._copy = copy\n\n        self._exception_handler = ExceptionHandler(catch_exceptions)\n        super(AugmentImageComponent, self).__init__(ds, self._aug_mapper, index)\n\n    def reset_state(self):\n        self.ds.reset_state()\n        self.augs.reset_state()\n\n    def _aug_mapper(self, x):\n        check_dtype(x)\n        with self._exception_handler.catch():\n            if self._copy:\n                x = copy_mod.deepcopy(x)\n            return self.augs.augment(x)\n\n\nclass AugmentImageCoordinates(MapData):\n    """"""\n    Apply image augmentors on an image and a list of coordinates.\n    Coordinates must be a Nx2 floating point array, each row is (x, y).\n    """"""\n\n    def __init__(self, ds, augmentors, img_index=0, coords_index=1, copy=True, catch_exceptions=False):\n\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow.\n            augmentors (AugmentorList): a list of :class:`imgaug.ImageAugmentor` to be applied in order.\n            img_index (int or str): the index/key of the image component to be augmented.\n            coords_index (int or str): the index/key of the coordinate component to be augmented.\n            copy, catch_exceptions: same as in :class:`AugmentImageComponent`\n        """"""\n        if isinstance(augmentors, AugmentorList):\n            self.augs = augmentors\n        else:\n            self.augs = AugmentorList(augmentors)\n\n        self._img_index = img_index\n        self._coords_index = coords_index\n        self._copy = copy\n        self._exception_handler = ExceptionHandler(catch_exceptions)\n\n        super(AugmentImageCoordinates, self).__init__(ds, self._aug_mapper)\n\n    def reset_state(self):\n        self.ds.reset_state()\n        self.augs.reset_state()\n\n    def _aug_mapper(self, dp):\n        with self._exception_handler.catch():\n            img, coords = dp[self._img_index], dp[self._coords_index]\n            check_dtype(img)\n            validate_coords(coords)\n            if self._copy:\n                img, coords = copy_mod.deepcopy((img, coords))\n            tfms = self.augs.get_transform(img)\n            dp[self._img_index] = tfms.apply_image(img)\n            dp[self._coords_index] = tfms.apply_coords(coords)\n            return dp\n\n\nclass AugmentImageComponents(MapData):\n    """"""\n    Apply image augmentors on several components, with shared augmentation parameters.\n\n    Example:\n\n        .. code-block:: python\n\n            ds = MyDataFlow()   # produce [image(HWC), segmask(HW), keypoint(Nx2)]\n            ds = AugmentImageComponents(\n                ds, augs,\n                index=(0,1), coords_index=(2,))\n\n    """"""\n\n    def __init__(self, ds, augmentors, index=(0, 1), coords_index=(), copy=True, catch_exceptions=False):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow.\n            augmentors (AugmentorList): a list of :class:`imgaug.ImageAugmentor` instance to be applied in order.\n            index: tuple of indices of the image components.\n            coords_index: tuple of indices of the coordinates components.\n            copy, catch_exceptions: same as in :class:`AugmentImageComponent`\n        """"""\n        if isinstance(augmentors, AugmentorList):\n            self.augs = augmentors\n        else:\n            self.augs = AugmentorList(augmentors)\n        self.ds = ds\n        self._exception_handler = ExceptionHandler(catch_exceptions)\n        self._copy = copy\n        self._index = index\n        self._coords_index = coords_index\n\n        super(AugmentImageComponents, self).__init__(ds, self._aug_mapper)\n\n    def reset_state(self):\n        self.ds.reset_state()\n        self.augs.reset_state()\n\n    def _aug_mapper(self, dp):\n        dp = copy_mod.copy(dp)  # always do a shallow copy, make sure the list is intact\n        copy_func = copy_mod.deepcopy if self._copy else lambda x: x  # noqa\n        with self._exception_handler.catch():\n            major_image = self._index[0]  # image to be used to get params. TODO better design?\n            im = copy_func(dp[major_image])\n            check_dtype(im)\n            tfms = self.augs.get_transform(im)\n            dp[major_image] = tfms.apply_image(im)\n            for idx in self._index[1:]:\n                check_dtype(dp[idx])\n                dp[idx] = tfms.apply_image(copy_func(dp[idx]))\n            for idx in self._coords_index:\n                coords = copy_func(dp[idx])\n                validate_coords(coords)\n                dp[idx] = tfms.apply_coords(coords)\n            return dp\n\n\ntry:\n    import cv2\n    from .imgaug import AugmentorList\nexcept ImportError:\n    from ..utils.develop import create_dummy_class\n    ImageFromFile = create_dummy_class(\'ImageFromFile\', \'cv2\')  # noqa\n    AugmentImageComponent = create_dummy_class(\'AugmentImageComponent\', \'cv2\')  # noqa\n    AugmentImageCoordinates = create_dummy_class(\'AugmentImageCoordinates\', \'cv2\') # noqa\n    AugmentImageComponents = create_dummy_class(\'AugmentImageComponents\', \'cv2\')  # noqa\n'"
tensorpack/dataflow/parallel.py,0,"b'# -*- coding: utf-8 -*-\n# File: parallel.py\n\nimport atexit\nimport pickle\nimport errno\nimport traceback\nimport itertools\nimport multiprocessing as mp\nimport os\nimport sys\nimport uuid\nimport weakref\nfrom contextlib import contextmanager\nimport zmq\nfrom six.moves import queue, range\n\nfrom ..utils import logger\nfrom ..utils.concurrency import (\n    StoppableThread, enable_death_signal, ensure_proc_terminate, start_proc_mask_signal)\nfrom ..utils.serialize import dumps_once as dumps, loads_once as loads\nfrom .base import DataFlow, DataFlowReentrantGuard, DataFlowTerminated, ProxyDataFlow\n\n__all__ = [\'PrefetchData\', \'MultiProcessPrefetchData\',\n           \'MultiProcessRunner\', \'MultiProcessRunnerZMQ\', \'MultiThreadRunner\',\n           \'PrefetchDataZMQ\', \'MultiThreadPrefetchData\']\n\n\n# from https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/__init__.py\nclass _ExceptionWrapper:\n    MAGIC = b""EXC_MAGIC""\n    """"""Wraps an exception plus traceback to communicate across threads""""""\n    def __init__(self, exc_info):\n        # It is important that we don\'t store exc_info, see\n        # NOTE [ Python Traceback Reference Cycle Problem ]\n        self.exc_type = exc_info[0]\n        self.exc_msg = """".join(traceback.format_exception(*exc_info))\n\n    def pack(self):\n        return self.MAGIC + pickle.dumps(self)\n\n    @staticmethod\n    def unpack(dp):\n        if isinstance(dp, bytes) and dp.startswith(_ExceptionWrapper.MAGIC):\n            return pickle.loads(dp[len(_ExceptionWrapper.MAGIC):])\n\n\ndef _repeat_iter(get_itr):\n    while True:\n        yield from get_itr()\n\n\ndef _bind_guard(sock, name):\n    try:\n        sock.bind(name)\n    except zmq.ZMQError:\n        logger.error(\n            ""ZMQError in socket.bind(\'{}\'). Perhaps you\'re \\\nusing pipes on a non-local file system. See documentation of MultiProcessRunnerZMQ \\\nfor more information."".format(name))\n        raise\n\n\ndef _get_pipe_name(name):\n    if sys.platform.startswith(\'linux\'):\n        # linux supports abstract sockets: http://api.zeromq.org/4-1:zmq-ipc\n        pipename = ""ipc://@{}-pipe-{}"".format(name, str(uuid.uuid1())[:8])\n        pipedir = os.environ.get(\'TENSORPACK_PIPEDIR\', None)\n        if pipedir is not None:\n            logger.warn(""TENSORPACK_PIPEDIR is not used on Linux any more! Abstract sockets will be used."")\n    else:\n        pipedir = os.environ.get(\'TENSORPACK_PIPEDIR\', None)\n        if pipedir is not None:\n            logger.info(""ZMQ uses TENSORPACK_PIPEDIR={}"".format(pipedir))\n        else:\n            pipedir = \'.\'\n        assert os.path.isdir(pipedir), pipedir\n        filename = \'{}/{}-pipe-{}\'.format(pipedir.rstrip(\'/\'), name, str(uuid.uuid1())[:6])\n        assert not os.path.exists(filename), ""Pipe {} exists! You may be unlucky."".format(filename)\n        pipename = ""ipc://{}"".format(filename)\n    return pipename\n\n\ndef del_weakref(x):\n    o = x()\n    if o is not None:\n        o.__del__()\n\n\n@contextmanager\ndef _zmq_catch_error(name):\n    try:\n        yield\n    except zmq.ContextTerminated:\n        logger.info(""[{}] Context terminated."".format(name))\n        raise DataFlowTerminated()\n    except zmq.ZMQError as e:\n        if e.errno == errno.ENOTSOCK:       # socket closed\n            logger.info(""[{}] Socket closed."".format(name))\n            raise DataFlowTerminated()\n        else:\n            raise\n    except Exception:\n        raise\n\n\nclass _MultiProcessZMQDataFlow(DataFlow):\n    def __init__(self):\n        assert os.name != \'nt\', ""ZMQ IPC doesn\'t support windows!""\n        self._reset_done = False\n        self._procs = []\n\n    def reset_state(self):\n        """"""\n        All forked dataflows should only be reset **once and only once** in spawned processes.\n        Subclasses should call this method with super.\n        """"""\n        assert not self._reset_done, ""reset_state() was called twice! This violates the API of DataFlow!""\n        self._reset_done = True\n\n        # __del__ not guaranteed to get called at exit\n        atexit.register(del_weakref, weakref.ref(self))\n\n    def _start_processes(self):\n        start_proc_mask_signal(self._procs)\n\n    def __del__(self):\n        try:\n            if not self._reset_done:\n                return\n            if not self.context.closed:\n                self.socket.close(0)\n                self.context.destroy(0)\n            for x in self._procs:\n                x.terminate()\n                x.join(5)\n            print(""{} successfully cleaned-up."".format(type(self).__name__))\n        except Exception:\n            pass\n\n\nclass MultiProcessRunner(ProxyDataFlow):\n    """"""\n    Running a DataFlow in >=1 processes using Python multiprocessing utilities.\n    It will fork the process that calls :meth:`__init__`, collect datapoints from `ds` in each\n    process by a Python :class:`multiprocessing.Queue`.\n\n    Note:\n        1. (Data integrity) An iterator cannot run faster automatically -- what\'s happening is\n           that the process will be forked ``num_proc`` times.\n           There will be ``num_proc`` dataflow running in parallel and **independently**.\n           As a result, we have the following guarantee on the dataflow correctness:\n\n           a. When ``num_proc=1``, this dataflow produces the same data as the\n              given dataflow in the same order.\n           b. When ``num_proc>1``, if each sample from the given dataflow is i.i.d.,\n              then this dataflow produces the **same distribution** of data as the given dataflow.\n              This implies that there will be duplication, reordering, etc.\n              You probably only want to use it for training.\n\n              For example, if your original dataflow contains no randomness and produces the same first datapoint,\n              then after parallel prefetching, the datapoint will be produced ``num_proc`` times\n              at the beginning.\n              Even when your original dataflow is fully shuffled, you still need to be aware of the\n              `Birthday Paradox <https://en.wikipedia.org/wiki/Birthday_problem>`_\n              and know that you\'ll likely see duplicates.\n\n           To utilize parallelism with more strict data integrity, you can use\n           the parallel versions of :class:`MapData`: :class:`MultiThreadMapData`, :class:`MultiProcessMapData`.\n        2. This has more serialization overhead than :class:`MultiProcessRunnerZMQ` when data is large.\n        3. You can nest like this: ``MultiProcessRunnerZMQ(MultiProcessRunner(df, num_proc=a), num_proc=b)``.\n           A total of ``a`` instances of ``df`` worker processes will be created.\n        4. Fork happens in `__init__`. `reset_state()` is a no-op.\n           DataFlow in the worker processes will be reset at the time of fork.\n        5. This DataFlow does support windows. However, Windows requires more strict picklability on processes,\n           which means that some code that\'s forkable on Linux may not be forkable on Windows. If that happens you\'ll\n           need to re-organize some part of code that\'s not forkable.\n    """"""\n\n    class _Worker(mp.Process):\n        def __init__(self, ds, queue, idx):\n            super(MultiProcessRunner._Worker, self).__init__()\n            self.ds = ds\n            self.queue = queue\n            self.idx = idx\n\n        def run(self):\n            enable_death_signal(_warn=self.idx == 0)\n            # reset all ds so each process will produce different data\n            self.ds.reset_state()\n            while True:\n                for dp in self.ds:\n                    self.queue.put(dp)\n\n    def __init__(self, ds, num_prefetch, num_proc):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow.\n            num_prefetch (int): size of the queue to hold prefetched datapoints.\n                Required.\n            num_proc (int): number of processes to use. Required.\n        """"""\n        # https://docs.python.org/3.6/library/multiprocessing.html?highlight=process#the-spawn-and-forkserver-start-methods\n        if os.name == \'nt\':\n            logger.warn(""MultiProcessRunner does support Windows. \\\nHowever, Windows requires more strict picklability on processes, which may \\\nlead of failure on some of the code."")\n        super(MultiProcessRunner, self).__init__(ds)\n        try:\n            self._size = len(ds)\n        except NotImplementedError:\n            self._size = -1\n        assert num_proc > 0, num_proc\n        assert num_prefetch > 0, num_prefetch\n        self.num_proc = num_proc\n        self.num_prefetch = num_prefetch\n\n        if num_proc > 1:\n            logger.info(""[MultiProcessRunner] Will fork a dataflow more than one times. ""\n                        ""This assumes the datapoints are i.i.d."")\n\n        self.queue = mp.Queue(self.num_prefetch)\n        self.procs = [MultiProcessRunner._Worker(self.ds, self.queue, idx)\n                      for idx in range(self.num_proc)]\n        ensure_proc_terminate(self.procs)\n        self._reset_done = False\n\n    def __iter__(self):\n        for k in itertools.count():\n            if self._size > 0 and k >= self._size:\n                break\n            dp = self.queue.get()\n            yield dp\n\n    def reset_state(self):\n        assert not self._reset_done, ""reset_state() was called twice! This violates the API of DataFlow!""\n        self._reset_done = True\n        start_proc_mask_signal(self.procs)\n\n\nclass MultiProcessRunnerZMQ(_MultiProcessZMQDataFlow):\n    """"""\n    Run a DataFlow in >=1 processes, with ZeroMQ for communication.\n    It will fork the calling process of :meth:`reset_state()`,\n    and collect datapoints from the given dataflow in each process by ZeroMQ IPC pipe.\n    This is typically faster than :class:`MultiProcessRunner`.\n\n    Note:\n        1. (Data integrity) An iterator cannot run faster automatically -- what\'s happening is\n           that the process will be forked ``num_proc`` times.\n           There will be ``num_proc`` dataflow running in parallel and **independently**.\n           As a result, we have the following guarantee on the dataflow correctness:\n\n           a. When ``num_proc=1``, this dataflow produces the same data as the\n              given dataflow in the same order.\n           b. When ``num_proc>1``, if each sample from the given dataflow is i.i.d.,\n              then this dataflow produces the **same distribution** of data as the given dataflow.\n              This implies that there will be duplication, reordering, etc.\n              You probably only want to use it for training.\n\n              For example, if your original dataflow contains no randomness and produces the same first datapoint,\n              then after parallel prefetching, the datapoint will be produced ``num_proc`` times\n              at the beginning.\n              Even when your original dataflow is fully shuffled, you still need to be aware of the\n              `Birthday Paradox <https://en.wikipedia.org/wiki/Birthday_problem>`_\n              and know that you\'ll likely see duplicates.\n\n           To utilize parallelism with more strict data integrity, you can use\n           the parallel versions of :class:`MapData`: :class:`MultiThreadMapData`, :class:`MultiProcessMapData`.\n        2. `reset_state()` of the given dataflow will be called **once and only once** in the worker processes.\n        3. The fork of processes happened in this dataflow\'s `reset_state()` method.\n           Please note that forking a TensorFlow GPU session may be unsafe.\n           If you\'re managing this dataflow on your own,\n           it\'s better to fork before creating the session.\n        4. (Fork-safety) After the fork has happened, this dataflow becomes not fork-safe.\n           i.e., if you fork an already reset instance of this dataflow,\n           it won\'t be usable in the forked process. Therefore, do not nest two `MultiProcessRunnerZMQ`.\n        5. (Thread-safety) ZMQ is not thread safe. Therefore, do not call :meth:`get_data` of the same dataflow in\n           more than 1 threads.\n        6. This dataflow does not support windows. Use `MultiProcessRunner` which works on windows.\n        7. (For Mac only) A UNIX named pipe will be created in the current directory.\n           However, certain non-local filesystem such as NFS/GlusterFS/AFS doesn\'t always support pipes.\n           You can change the directory by ``export TENSORPACK_PIPEDIR=/other/dir``.\n           In particular, you can use somewhere under \'/tmp\' which is usually local.\n\n           Note that some non-local FS may appear to support pipes and code\n           may appear to run but crash with bizarre error.\n           Also note that ZMQ limits the maximum length of pipe path.\n           If you hit the limit, you can set the directory to a softlink\n           which points to a local directory.\n    """"""\n\n    class _Worker(mp.Process):\n        def __init__(self, ds, conn_name, hwm, idx):\n            super(MultiProcessRunnerZMQ._Worker, self).__init__()\n            self.ds = ds\n            self.conn_name = conn_name\n            self.hwm = hwm\n            self.idx = idx\n\n        def run(self):\n            enable_death_signal(_warn=self.idx == 0)\n            self.ds.reset_state()\n            itr = _repeat_iter(lambda: self.ds)\n\n            context = zmq.Context()\n            socket = context.socket(zmq.PUSH)\n            socket.set_hwm(self.hwm)\n            socket.connect(self.conn_name)\n            try:\n                while True:\n                    try:\n                        dp = next(itr)\n                        socket.send(dumps(dp), copy=False)\n                    except Exception:\n                        dp = _ExceptionWrapper(sys.exc_info()).pack()\n                        socket.send(dumps(dp), copy=False)\n                        raise\n            # sigint could still propagate here, e.g. when nested\n            except KeyboardInterrupt:\n                pass\n            finally:\n                socket.close(0)\n                context.destroy(0)\n\n    def __init__(self, ds, num_proc=1, hwm=50):\n        """"""\n        Args:\n            ds (DataFlow): input DataFlow.\n            num_proc (int): number of processes to use.\n            hwm (int): the zmq ""high-water mark"" (queue size) for both sender and receiver.\n        """"""\n        super(MultiProcessRunnerZMQ, self).__init__()\n\n        self.ds = ds\n        self.num_proc = num_proc\n        self._hwm = hwm\n\n        if num_proc > 1:\n            logger.info(""[MultiProcessRunnerZMQ] Will fork a dataflow more than one times. ""\n                        ""This assumes the datapoints are i.i.d."")\n        try:\n            self._size = ds.__len__()\n        except NotImplementedError:\n            self._size = -1\n\n    def _recv(self):\n        ret = loads(self.socket.recv(copy=False))\n        exc = _ExceptionWrapper.unpack(ret)\n        if exc is not None:\n            logger.error(""Exception \'{}\' in worker:"".format(str(exc.exc_type)))\n            raise exc.exc_type(exc.exc_msg)\n        return ret\n\n    def __len__(self):\n        return self.ds.__len__()\n\n    def __iter__(self):\n        with self._guard, _zmq_catch_error(\'MultiProcessRunnerZMQ\'):\n            for k in itertools.count():\n                if self._size > 0 and k >= self._size:\n                    break\n                yield self._recv()\n\n    def reset_state(self):\n        super(MultiProcessRunnerZMQ, self).reset_state()\n        self._guard = DataFlowReentrantGuard()\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.PULL)\n        self.socket.set_hwm(self._hwm)\n        pipename = _get_pipe_name(\'dataflow\')\n        _bind_guard(self.socket, pipename)\n\n        self._procs = [MultiProcessRunnerZMQ._Worker(self.ds, pipename, self._hwm, idx)\n                       for idx in range(self.num_proc)]\n        self._start_processes()\n\n\nclass MultiThreadRunner(DataFlow):\n    """"""\n    Create multiple dataflow instances and run them each in one thread.\n    Collect outputs from them with a queue.\n\n    Note:\n        1. (Data integrity) An iterator cannot run faster automatically -- what\'s happening is\n           that each thread will create a dataflow iterator.\n           There will be ``num_thread`` dataflow running in parallel and **independently**.\n           As a result, we have the following guarantee on the dataflow correctness:\n\n           a. When ``num_thread=1``, this dataflow produces the same data as the\n              given dataflow in the same order.\n           b. When ``num_thread>1``, if each sample from the given dataflow is i.i.d.,\n              then this dataflow produces the **same distribution** of data as the given dataflow.\n              This implies that there will be duplication, reordering, etc.\n              You probably only want to use it for training.\n\n              For example, if your original dataflow contains no randomness and produces the same first datapoint,\n              then after parallel prefetching, the datapoint will be produced ``num_thread`` times\n              at the beginning.\n              Even when your original dataflow is fully shuffled, you still need to be aware of the\n              `Birthday Paradox <https://en.wikipedia.org/wiki/Birthday_problem>`_\n              and know that you\'ll likely see duplicates.\n\n           To utilize parallelism with more strict data integrity, you can use\n           the parallel versions of :class:`MapData`: :class:`MultiThreadMapData`, :class:`MultiProcessMapData`.\n    """"""\n\n    class _Worker(StoppableThread):\n        def __init__(self, get_df, queue):\n            super(MultiThreadRunner._Worker, self).__init__()\n            self.df = get_df()\n            assert isinstance(self.df, DataFlow), self.df\n            self.queue = queue\n            self.daemon = True\n\n        def run(self):\n            self.df.reset_state()\n            try:\n                while True:\n                    for dp in self.df:\n                        if self.stopped():\n                            return\n                        self.queue_put_stoppable(self.queue, dp)\n            except Exception:\n                if self.stopped():\n                    pass        # skip duplicated error messages\n                else:\n                    raise\n            finally:\n                self.stop()\n\n    def __init__(self, get_df, num_prefetch, num_thread):\n        """"""\n        Args:\n            get_df ( -> DataFlow): a callable which returns a DataFlow.\n                Each thread will call this function to get the DataFlow to use.\n                Therefore do not return the same DataFlow object for each call,\n                unless your dataflow is stateless.\n            num_prefetch (int): size of the queue\n            num_thread (int): number of threads\n        """"""\n        assert num_thread > 0, num_thread\n        assert num_prefetch > 0, num_prefetch\n        self.num_thread = num_thread\n        self.queue = queue.Queue(maxsize=num_prefetch)\n        self.threads = [\n            MultiThreadRunner._Worker(get_df, self.queue)\n            for _ in range(num_thread)]\n\n        try:\n            self._size = self.__len__()\n        except NotImplementedError:\n            self._size = -1\n\n    def reset_state(self):\n        for th in self.threads:\n            th.df.reset_state()\n            th.start()\n\n    def __len__(self):\n        return self.threads[0].df.__len__()\n\n    def __iter__(self):\n        for k in itertools.count():\n            if self._size > 0 and k >= self._size:\n                break\n            yield self.queue.get()\n\n    def __del__(self):\n        for p in self.threads:\n            if p.is_alive():\n                p.stop()\n                p.join()\n\n\nclass PlasmaPutData(ProxyDataFlow):\n    """"""\n    Put each data point to plasma shared memory object store, and yield the object id instead.\n\n    Experimental.\n    """"""\n    def __init__(self, ds, socket=""/tmp/plasma""):\n        self._socket = socket\n        super(PlasmaPutData, self).__init__(ds)\n\n    def reset_state(self):\n        super(PlasmaPutData, self).reset_state()\n        self.client = plasma.connect(self._socket, """", 0)\n\n    def __iter__(self):\n        for dp in self.ds:\n            oid = self.client.put(dp)\n            yield [oid.binary()]\n\n\nclass PlasmaGetData(ProxyDataFlow):\n    """"""\n    Take plasma object id from a DataFlow, and retrieve it from plasma shared\n    memory object store.\n\n    Experimental.\n    """"""\n    def __init__(self, ds, socket=""/tmp/plasma""):\n        self._socket = socket\n        super(PlasmaGetData, self).__init__(ds)\n\n    def reset_state(self):\n        super(PlasmaGetData, self).reset_state()\n        self.client = plasma.connect(self._socket, """", 0)\n\n    def __iter__(self):\n        for dp in self.ds:\n            oid = plasma.ObjectID(dp[0])\n            dp = self.client.get(oid)\n            yield dp\n\n\nplasma = None\n# These plasma code is only experimental\n# try:\n#     import pyarrow.plasma as plasma\n# except ImportError:\n#     from ..utils.develop import create_dummy_class\n#     PlasmaPutData = create_dummy_class(\'PlasmaPutData\', \'pyarrow\')   # noqa\n#     PlasmaGetData = create_dummy_class(\'PlasmaGetData\', \'pyarrow\')   # noqa\n\n\n# The old inappropriate names:\nPrefetchData = MultiProcessRunner\nMultiProcessPrefetchData = MultiProcessRunner\nPrefetchDataZMQ = MultiProcessRunnerZMQ\nMultiThreadPrefetchData = MultiThreadRunner\n\nif __name__ == \'__main__\':\n    import time\n    from .raw import DataFromGenerator\n    from .common import FixedSizeData\n    x = DataFromGenerator(itertools.count())\n    x = FixedSizeData(x, 100)\n    x = MultiProcessRunnerZMQ(x, 2)\n    x.reset_state()\n    for idx, dp in enumerate(x):\n        print(dp)\n        time.sleep(0.1)\n'"
tensorpack/dataflow/parallel_map.py,0,"b'# -*- coding: utf-8 -*-\n# File: parallel_map.py\nimport copy\nimport ctypes\nimport multiprocessing as mp\nimport numpy as np\nimport threading\nimport zmq\nfrom six.moves import queue\n\nfrom ..utils.concurrency import StoppableThread, enable_death_signal\nfrom ..utils.serialize import dumps_once as dumps, loads_once as loads\nfrom .base import DataFlow, DataFlowReentrantGuard, ProxyDataFlow\nfrom .common import RepeatedData, BatchData\nfrom .parallel import _bind_guard, _get_pipe_name, _MultiProcessZMQDataFlow, _repeat_iter, _zmq_catch_error\n\n__all__ = [\'MultiThreadMapData\',\n           \'MultiProcessMapData\', \'MultiProcessMapDataZMQ\',\n           \'MultiProcessMapAndBatchData\', \'MultiProcessMapAndBatchDataZMQ\']\n\n\nclass _ParallelMapData(ProxyDataFlow):\n    def __init__(self, ds, buffer_size, strict=False):\n        super(_ParallelMapData, self).__init__(ds)\n        assert buffer_size > 0, buffer_size\n        self._buffer_size = buffer_size\n        self._buffer_occupancy = 0  # actual #elements in buffer, only useful in strict mode\n        self._strict = strict\n\n    def reset_state(self):\n        super(_ParallelMapData, self).reset_state()\n        if not self._strict:\n            ds = RepeatedData(self.ds, -1)\n        else:\n            ds = self.ds\n        self._iter = ds.__iter__()\n\n    def _recv(self):\n        pass\n\n    def _send(self, dp):\n        pass\n\n    def _recv_filter_none(self):\n        ret = self._recv()\n        assert ret is not None, \\\n            ""[{}] Map function cannot return None when strict mode is used."".format(type(self).__name__)\n        return ret\n\n    def _fill_buffer(self, cnt=None):\n        if cnt is None:\n            cnt = self._buffer_size - self._buffer_occupancy\n        try:\n            for _ in range(cnt):\n                dp = next(self._iter)\n                self._send(dp)\n        except StopIteration:\n            raise RuntimeError(\n                ""[{}] buffer_size cannot be larger than the size of the DataFlow when strict=True! ""\n                ""Please use a smaller buffer_size!"".format(type(self).__name__))\n        self._buffer_occupancy += cnt\n\n    def get_data_non_strict(self):\n        for dp in self._iter:\n            self._send(dp)\n            ret = self._recv()\n            if ret is not None:\n                yield ret\n\n    def get_data_strict(self):\n        self._fill_buffer()\n        for dp in self._iter:\n            self._send(dp)\n            yield self._recv_filter_none()\n        self._iter = self.ds.__iter__()   # refresh\n\n        # first clear the buffer, then fill\n        for k in range(self._buffer_size):\n            dp = self._recv_filter_none()\n            self._buffer_occupancy -= 1\n            if k == self._buffer_size - 1:\n                self._fill_buffer()\n            yield dp\n\n    def __iter__(self):\n        if self._strict:\n            yield from self.get_data_strict()\n        else:\n            yield from self.get_data_non_strict()\n\n\nclass MultiThreadMapData(_ParallelMapData):\n    """"""\n    Same as :class:`MapData`, but start threads to run the mapping function.\n    This is useful when the mapping function is the bottleneck, but you don\'t\n    want to start processes for the entire dataflow pipeline.\n\n    The semantics of this class is **identical** to :class:`MapData` except for the ordering.\n    Threads run in parallel and can take different time to run the\n    mapping function. Therefore the order of datapoints won\'t be preserved.\n\n    When ``strict=True``, ``MultiThreadMapData(df, ...)``\n    is guaranteed to produce the exact set of data as ``MapData(df, ...)``,\n    if both are iterated until ``StopIteration``. But the produced data will have different ordering.\n    The behavior of strict mode is undefined if the given dataflow ``df`` is infinite.\n\n    When ``strict=False``, the data that\'s produced by ``MultiThreadMapData(df, ...)``\n    is a reordering of the data produced by ``RepeatedData(MapData(df, ...), -1)``.\n    In other words, first pass of ``MultiThreadMapData.__iter__`` may contain\n    datapoints from the second pass of ``df.__iter__``.\n\n\n    Note:\n        1. You should avoid starting many threads in your main process to reduce GIL contention.\n\n           The threads will only start in the process which calls :meth:`reset_state()`.\n           Therefore you can use ``MultiProcessRunnerZMQ(MultiThreadMapData(...), 1)``\n           to reduce GIL contention.\n    """"""\n    class _Worker(StoppableThread):\n        def __init__(self, inq, outq, evt, map_func):\n            super(MultiThreadMapData._Worker, self).__init__(evt)\n            self.inq = inq\n            self.outq = outq\n            self.func = map_func\n            self.daemon = True\n\n        def run(self):\n            try:\n                while True:\n                    dp = self.queue_get_stoppable(self.inq)\n                    if self.stopped():\n                        return\n                    # cannot ignore None here. will lead to unsynced send/recv\n                    obj = self.func(dp)\n                    self.queue_put_stoppable(self.outq, obj)\n            except Exception:\n                if self.stopped():\n                    pass        # skip duplicated error messages\n                else:\n                    raise\n            finally:\n                self.stop()\n\n    def __init__(self, ds, num_thread=None, map_func=None, *, buffer_size=200, strict=False):\n        """"""\n        Args:\n            ds (DataFlow): the dataflow to map\n            num_thread (int): number of threads to use\n            map_func (callable): datapoint -> datapoint | None. Return None to\n                discard/skip the datapoint.\n            buffer_size (int): number of datapoints in the buffer\n            strict (bool): use ""strict mode"", see notes above.\n        """"""\n        if strict:\n            # In strict mode, buffer size cannot be larger than the total number of datapoints\n            try:\n                buffer_size = min(buffer_size, len(ds))\n            except Exception:  # ds may not have a length\n                pass\n\n        super(MultiThreadMapData, self).__init__(ds, buffer_size, strict)\n        assert num_thread > 0, num_thread\n\n        self._strict = strict\n        self.num_thread = num_thread\n        self.map_func = map_func\n        self._threads = []\n        self._evt = None\n\n    def reset_state(self):\n        super(MultiThreadMapData, self).reset_state()\n        if self._threads:\n            self._threads[0].stop()\n            for t in self._threads:\n                t.join()\n\n        self._in_queue = queue.Queue()\n        self._out_queue = queue.Queue()\n        self._evt = threading.Event()\n        self._threads = [MultiThreadMapData._Worker(\n            self._in_queue, self._out_queue, self._evt, self.map_func)\n            for _ in range(self.num_thread)]\n        for t in self._threads:\n            t.start()\n\n        self._guard = DataFlowReentrantGuard()\n\n        # Call once at the beginning, to ensure inq+outq has a total of buffer_size elements\n        self._fill_buffer()\n\n    def _recv(self):\n        return self._out_queue.get()\n\n    def _send(self, dp):\n        self._in_queue.put(dp)\n\n    def __iter__(self):\n        with self._guard:\n            yield from super(MultiThreadMapData, self).__iter__()\n\n    def __del__(self):\n        if self._evt is not None:\n            self._evt.set()\n        for p in self._threads:\n            p.stop()\n            p.join(timeout=5.0)\n            # if p.is_alive():\n            #     logger.warn(""Cannot join thread {}."".format(p.name))\n\n\nclass MultiProcessMapDataZMQ(_ParallelMapData, _MultiProcessZMQDataFlow):\n    """"""\n    Same as :class:`MapData`, but start processes to run the mapping function,\n    and communicate with ZeroMQ pipe.\n\n    The semantics of this class is **identical** to :class:`MapData` except for the ordering.\n    Processes run in parallel and can take different time to run the\n    mapping function. Therefore the order of datapoints won\'t be preserved.\n\n    When ``strict=True``, ``MultiProcessMapData(df, ...)``\n    is guaranteed to produce the exact set of data as ``MapData(df, ...)``,\n    if both are iterated until ``StopIteration``. But the produced data will have different ordering.\n    The behavior of strict mode is undefined if the given dataflow ``df`` is infinite.\n\n    When ``strict=False``, the data that\'s produced by ``MultiProcessMapData(df, ...)``\n    is a reordering of the data produced by ``RepeatedData(MapData(df, ...), -1)``.\n    In other words, first pass of ``MultiProcessMapData.__iter__`` may contain\n    datapoints from the second pass of ``df.__iter__``.\n    """"""\n    class _Worker(mp.Process):\n        def __init__(self, identity, map_func, pipename, hwm):\n            super(MultiProcessMapDataZMQ._Worker, self).__init__()\n            self.identity = identity\n            self.map_func = map_func\n            self.pipename = pipename\n            self.hwm = hwm\n\n        def run(self):\n            enable_death_signal(_warn=self.identity == b\'0\')\n            ctx = zmq.Context()\n            socket = ctx.socket(zmq.REP)\n            socket.setsockopt(zmq.IDENTITY, self.identity)\n            socket.set_hwm(self.hwm)\n            socket.connect(self.pipename)\n\n            while True:\n                dp = loads(socket.recv(copy=False))\n                dp = self.map_func(dp)\n                socket.send(dumps(dp), copy=False)\n\n    def __init__(self, ds, num_proc=None, map_func=None, *, buffer_size=200, strict=False):\n        """"""\n        Args:\n            ds (DataFlow): the dataflow to map\n            num_proc(int): number of threads to use\n            map_func (callable): datapoint -> datapoint | None. Return None to\n                discard/skip the datapoint.\n            buffer_size (int): number of datapoints in the buffer\n            strict (bool): use ""strict mode"", see notes above.\n        """"""\n        if strict:\n            # In strict mode, buffer size cannot be larger than the total number of datapoints\n            try:\n                buffer_size = min(buffer_size, len(ds))\n            except Exception:  # ds may not have a length\n                pass\n\n        _ParallelMapData.__init__(self, ds, buffer_size, strict)\n        _MultiProcessZMQDataFlow.__init__(self)\n        assert num_proc > 0, num_proc\n        self.num_proc = num_proc\n        self.map_func = map_func\n        self._strict = strict\n        self._procs = []\n\n    def _create_worker(self, id, pipename, hwm):\n        return MultiProcessMapDataZMQ._Worker(id, self.map_func, pipename, hwm)\n\n    def reset_state(self):\n        _MultiProcessZMQDataFlow.reset_state(self)\n        _ParallelMapData.reset_state(self)\n        self._guard = DataFlowReentrantGuard()\n\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.DEALER)\n        self.socket.set_hwm(self._buffer_size * 2)\n        pipename = _get_pipe_name(\'dataflow-map\')\n        _bind_guard(self.socket, pipename)\n\n        self._proc_ids = [u\'{}\'.format(k).encode(\'utf-8\') for k in range(self.num_proc)]\n        worker_hwm = int(self._buffer_size * 2 // self.num_proc)\n        self._procs = [self._create_worker(self._proc_ids[k], pipename, worker_hwm)\n                       for k in range(self.num_proc)]\n\n        self._start_processes()\n        self._fill_buffer()     # pre-fill the bufer\n\n    def _send(self, dp):\n        msg = [b"""", dumps(dp)]\n        self.socket.send_multipart(msg, copy=False)\n\n    def _recv(self):\n        msg = self.socket.recv_multipart(copy=False)\n        dp = loads(msg[1])\n        return dp\n\n    def __iter__(self):\n        with self._guard, _zmq_catch_error(type(self).__name__):\n            yield from super(MultiProcessMapDataZMQ, self).__iter__()\n\n\nclass MultiProcessMapAndBatchDataZMQ(_MultiProcessZMQDataFlow):\n    """"""\n    Similar to :class:`MultiProcessMapDataZMQ`, except that this DataFlow\n    also does batching in parallel in the worker processes.\n    Therefore it can be helpful if you wish to hide the latency of batching.\n\n    When `nr_proc==1`, the behavior of this class is identical to\n    `BatchData(MapData(ds, map_func), batch_size)`.\n\n    When `nr_proc>1`, the datapoints may be grouped in arbitrary order,\n    or grouped with datapoints from a different pass of the given dataflow.\n    """"""\n\n    class _Dispatcher(mp.Process):\n        def __init__(self, ds, pipename, hwm):\n            super(MultiProcessMapAndBatchDataZMQ._Dispatcher, self).__init__()\n            self.ds = RepeatedData(ds, -1)\n            self.pipename = pipename\n            self.hwm = hwm\n\n        def run(self):\n            enable_death_signal()\n            ctx = zmq.Context()\n            socket = ctx.socket(zmq.PUSH)\n            socket.set_hwm(self.hwm)\n            socket.bind(self.pipename)\n            self.ds.reset_state()\n            for dp in self.ds:\n                socket.send(dumps(dp), copy=False)\n\n    class _Worker(mp.Process):\n        def __init__(self, identity, map_func, input_pipe, result_pipe, hwm, batch_size):\n            super(MultiProcessMapAndBatchDataZMQ._Worker, self).__init__()\n            self.identity = identity\n            self.map_func = map_func\n            self.input_pipe = input_pipe\n            self.result_pipe = result_pipe\n            self.hwm = hwm\n            self.batch_size = batch_size\n\n        def run(self):\n            enable_death_signal(_warn=self.identity == b\'0\')\n            ctx = zmq.Context()\n\n            # recv jobs\n            socket = ctx.socket(zmq.PULL)\n            socket.setsockopt(zmq.IDENTITY, self.identity)\n            socket.set_hwm(self.hwm * self.batch_size)\n            socket.connect(self.input_pipe)\n\n            # send results\n            out_socket = ctx.socket(zmq.PUSH)\n            out_socket.set_hwm(max(self.hwm, 5))\n            out_socket.connect(self.result_pipe)\n\n            batch = []\n            while True:\n                dp = loads(socket.recv(copy=False))\n                dp = self.map_func(dp)\n                if dp is not None:\n                    batch.append(dp)\n                    if len(batch) == self.batch_size:\n                        dp = BatchData.aggregate_batch(batch)\n                        out_socket.send(dumps(dp), copy=False)\n                        del batch[:]\n\n    def __init__(self, ds, num_proc, map_func, batch_size, buffer_size=None):\n        """"""\n        Args:\n            ds (DataFlow): the dataflow to map\n            num_proc(int): number of threads to use\n            map_func (callable): datapoint -> datapoint | None. Return None to\n                discard/skip the datapoint.\n            batch_size (int): batch size\n            buffer_size (int): number of datapoints (not batched) in the buffer.\n                Defaults to batch_size * 10\n        """"""\n        super(MultiProcessMapAndBatchDataZMQ, self).__init__()\n        assert batch_size < buffer_size\n        self.ds = ds\n        self.num_proc = num_proc\n        self.map_func = map_func\n        self.batch_size = batch_size\n        if buffer_size is None:\n            buffer_size = batch_size * 10\n        self.buffer_size = buffer_size\n\n    def reset_state(self):\n        _MultiProcessZMQDataFlow.reset_state(self)\n        self._guard = DataFlowReentrantGuard()\n\n        job_pipe = _get_pipe_name(""dataflow_MaB_job"")\n        result_pipe = _get_pipe_name(""dataflow_MaB_result"")\n\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.PULL)\n        self.socket.set_hwm(max(5, self.buffer_size // self.batch_size))\n        _bind_guard(self.socket, result_pipe)\n\n        dispatcher = MultiProcessMapAndBatchDataZMQ._Dispatcher(self.ds, job_pipe, self.buffer_size)\n\n        self._proc_ids = [u\'{}\'.format(k).encode(\'utf-8\') for k in range(self.num_proc)]\n        worker_hwm = max(3, self.buffer_size // self.num_proc // self.batch_size)\n        self._procs = [MultiProcessMapAndBatchDataZMQ._Worker(\n            self._proc_ids[k], self.map_func, job_pipe, result_pipe, worker_hwm, self.batch_size)\n            for k in range(self.num_proc)]\n\n        self._procs.append(dispatcher)\n        self._start_processes()\n\n    def __iter__(self):\n        with self._guard, _zmq_catch_error(type(self).__name__):\n            while True:\n                yield loads(self.socket.recv(copy=False))\n\n\ndef _pool_map(data):\n    global SHARED_ARR, WORKER_ID, MAP_FUNC\n    res = MAP_FUNC(data)\n    if res is None:\n        return None\n    shared = np.reshape(SHARED_ARR, res.shape)\n    assert shared.dtype == res.dtype\n    shared[:] = res\n    return WORKER_ID\n\n\n# TODO shutdown pool, improve speed.\nclass MultiProcessMapDataComponentSharedArray(DataFlow):\n    """"""\n    Similar to :class:`MapDataComponent`, but perform IPC by shared memory,\n    therefore more efficient when data (result of map_func) is large.\n    It requires `map_func` to always return a numpy array of fixed shape and dtype, or None.\n    """"""\n    def __init__(self, ds, nr_proc, map_func, output_shape, output_dtype, index=0):\n        """"""\n        Args:\n            ds (DataFlow): the dataflow to map on\n            nr_proc(int): number of processes\n            map_func (data component -> ndarray | None): the mapping function\n            output_shape (tuple): the shape of the output of map_func\n            output_dtype (np.dtype): the type of the output of map_func\n            index (int): the index of the datapoint component to map on.\n        """"""\n        self.ds = ds\n        self.nr_proc = nr_proc\n        self.map_func = map_func\n        self.output_shape = output_shape\n        self.output_dtype = np.dtype(output_dtype).type\n        self.index = index\n\n        self._shared_mem = [self._create_shared_arr() for k in range(nr_proc)]\n        id_queue = mp.Queue()\n        for k in range(nr_proc):\n            id_queue.put(k)\n\n        def _init_pool(arrs, queue, map_func):\n            id = queue.get()\n            global SHARED_ARR, WORKER_ID, MAP_FUNC\n            SHARED_ARR = arrs[id]\n            WORKER_ID = id\n            MAP_FUNC = map_func\n\n        self._pool = mp.pool.Pool(\n            processes=nr_proc,\n            initializer=_init_pool,\n            initargs=(self._shared_mem, id_queue, map_func))\n\n    def _create_shared_arr(self):\n        TYPE = {\n            np.float32: ctypes.c_float,\n            np.float64: ctypes.c_double,\n            np.uint8: ctypes.c_uint8,\n            np.int8: ctypes.c_int8,\n            np.int32: ctypes.c_int32,\n        }\n        ctype = TYPE[self.output_dtype]\n        arr = mp.RawArray(ctype, int(np.prod(self.output_shape)))\n        return arr\n\n    def __len__(self):\n        return len(self.ds)\n\n    def reset_state(self):\n        self.ds.reset_state()\n        self._guard = DataFlowReentrantGuard()\n\n    def __iter__(self):\n        ds_itr = _repeat_iter(self.ds.get_data)\n        with self._guard:\n            while True:\n                dps = []\n                for k in range(self.nr_proc):\n                    dps.append(copy.copy(next(ds_itr)))\n                to_map = [x[self.index] for x in dps]\n                res = self._pool.map_async(_pool_map, to_map)\n\n                for index in res.get():\n                    if index is None:\n                        continue\n                    arr = np.reshape(self._shared_mem[index], self.output_shape)\n                    dp = dps[index]\n                    dp[self.index] = arr.copy()\n                    yield dp\n\n\n# alias\nMultiProcessMapData = MultiProcessMapDataZMQ\nMultiProcessMapAndBatchData = MultiProcessMapAndBatchDataZMQ\n\n\nif __name__ == \'__main__\':\n    import time\n\n    class Zero(DataFlow):\n        def __init__(self, size):\n            self._size = size\n\n        def __iter__(self):\n            for k in range(self._size):\n                yield [k]\n\n        def __len__(self):\n            return self._size\n\n    def f(x):\n        if x[0] < 10:\n            time.sleep(1)\n        return x\n\n    ds = Zero(100)\n    ds = MultiThreadMapData(ds, 50, f, buffer_size=50, strict=True)\n    ds.reset_state()\n    for idx, k in enumerate(ds):\n        print(""Bang!"", k)\n        if idx == 100:\n            break\n    print(""END!"")\n'"
tensorpack/dataflow/raw.py,0,"b'# -*- coding: utf-8 -*-\n# File: raw.py\n\n\nimport copy\nimport numpy as np\nimport six\n\nfrom .base import DataFlow, RNGDataFlow\n\n__all__ = [\'FakeData\', \'DataFromQueue\', \'DataFromList\', \'DataFromGenerator\', \'DataFromIterable\']\n\n\nclass FakeData(RNGDataFlow):\n    """""" Generate fake data of given shapes""""""\n\n    def __init__(self, shapes, size=1000, random=True, dtype=\'float32\', domain=(0, 1)):\n        """"""\n        Args:\n            shapes (list): a list of lists/tuples. Shapes of each component.\n            size (int): size of this DataFlow.\n            random (bool): whether to randomly generate data every iteration.\n                Note that merely generating the data could sometimes be time-consuming!\n            dtype (str or list): data type as string, or a list of data types.\n            domain (tuple or list): (min, max) tuple, or a list of such tuples\n        """"""\n        super(FakeData, self).__init__()\n        self.shapes = shapes\n        self._size = int(size)\n        self.random = random\n        self.dtype = [dtype] * len(shapes) if isinstance(dtype, six.string_types) else dtype\n        self.domain = [domain] * len(shapes) if isinstance(domain, tuple) else domain\n        assert len(self.dtype) == len(self.shapes)\n        assert len(self.domain) == len(self.domain)\n\n    def __len__(self):\n        return self._size\n\n    def __iter__(self):\n        if self.random:\n            for _ in range(self._size):\n                val = []\n                for k in range(len(self.shapes)):\n                    v = self.rng.rand(*self.shapes[k]) * (self.domain[k][1] - self.domain[k][0]) + self.domain[k][0]\n                    val.append(v.astype(self.dtype[k]))\n                yield val\n        else:\n            val = []\n            for k in range(len(self.shapes)):\n                v = self.rng.rand(*self.shapes[k]) * (self.domain[k][1] - self.domain[k][0]) + self.domain[k][0]\n                val.append(v.astype(self.dtype[k]))\n            for _ in range(self._size):\n                yield copy.copy(val)\n\n\nclass DataFromQueue(DataFlow):\n    """""" Produce data from a queue """"""\n    def __init__(self, queue):\n        """"""\n        Args:\n            queue (queue): a queue with ``get()`` method.\n        """"""\n        self.queue = queue\n\n    def __iter__(self):\n        while True:\n            yield self.queue.get()\n\n\nclass DataFromList(RNGDataFlow):\n    """""" Wrap a list of datapoints to a DataFlow""""""\n\n    def __init__(self, lst, shuffle=True):\n        """"""\n        Args:\n            lst (list): input list. Each element is a datapoint.\n            shuffle (bool): shuffle data.\n        """"""\n        super(DataFromList, self).__init__()\n        self.lst = lst\n        self.shuffle = shuffle\n\n    def __len__(self):\n        return len(self.lst)\n\n    def __iter__(self):\n        if not self.shuffle:\n            yield from self.lst\n        else:\n            idxs = np.arange(len(self.lst))\n            self.rng.shuffle(idxs)\n            for k in idxs:\n                yield self.lst[k]\n\n\nclass DataFromGenerator(DataFlow):\n    """"""\n    Wrap a generator to a DataFlow.\n    The dataflow will not have length.\n    """"""\n    def __init__(self, gen):\n        """"""\n        Args:\n            gen: iterable, or a callable that returns an iterable\n        """"""\n        self._gen = gen\n\n    def __iter__(self):\n        if not callable(self._gen):\n            yield from self._gen\n        else:\n            yield from self._gen()\n\n    def __len__(self):\n        return len(self._gen)\n\n\nclass DataFromIterable(DataFlow):\n    """""" Wrap an iterable of datapoints to a DataFlow""""""\n    def __init__(self, iterable):\n        """"""\n        Args:\n            iterable: an iterable object\n        """"""\n        self._itr = iterable\n        try:\n            self._len = len(iterable)\n        except Exception:\n            self._len = None\n\n    def __len__(self):\n        if self._len is None:\n            raise NotImplementedError\n        return self._len\n\n    def __iter__(self):\n        yield from self._itr\n'"
tensorpack/dataflow/remote.py,0,"b'# -*- coding: utf-8 -*-\n# File: remote.py\n\n\nimport multiprocessing as mp\nimport time\nfrom collections import deque\nimport tqdm\n\nfrom ..utils import logger\nfrom ..utils.concurrency import DIE\nfrom ..utils.serialize import dumps, loads\nfrom ..utils.utils import get_tqdm_kwargs\nfrom .base import DataFlow, DataFlowReentrantGuard\n\ntry:\n    import zmq\nexcept ImportError:\n    logger.warn(""Error in \'import zmq\'. remote feature won\'t be available"")\n    __all__ = []\nelse:\n    __all__ = [\'send_dataflow_zmq\', \'RemoteDataZMQ\']\n\n\ndef send_dataflow_zmq(df, addr, hwm=50, format=None, bind=False):\n    """"""\n    Run DataFlow and send data to a ZMQ socket addr.\n    It will serialize and send each datapoint to this address with a PUSH socket.\n    This function never returns.\n\n    Args:\n        df (DataFlow): Will infinitely loop over the DataFlow.\n        addr: a ZMQ socket endpoint.\n        hwm (int): ZMQ high-water mark (buffer size)\n        format (str): The serialization format.\n             Default format uses :mod:`utils.serialize`.\n             This format works with :class:`dataflow.RemoteDataZMQ`.\n             An alternate format is \'zmq_ops\', used by https://github.com/tensorpack/zmq_ops\n             and :class:`input_source.ZMQInput`.\n        bind (bool): whether to bind or connect to the endpoint address.\n    """"""\n    assert format in [None, \'zmq_op\', \'zmq_ops\']\n    if format is None:\n        dump_fn = dumps\n    else:\n        from zmq_ops import dump_arrays\n        dump_fn = dump_arrays\n\n    ctx = zmq.Context()\n    socket = ctx.socket(zmq.PUSH)\n    socket.set_hwm(hwm)\n    if bind:\n        socket.bind(addr)\n    else:\n        socket.connect(addr)\n    try:\n        df.reset_state()\n        logger.info(""Serving data to {} with {} format ..."".format(\n            addr, \'default\' if format is None else \'zmq_ops\'))\n        INTERVAL = 200\n        q = deque(maxlen=INTERVAL)\n\n        try:\n            total = len(df)\n        except NotImplementedError:\n            total = 0\n        tqdm_args = get_tqdm_kwargs(leave=True, smoothing=0.8)\n        tqdm_args[\'bar_format\'] = tqdm_args[\'bar_format\'] + ""{postfix}""\n        while True:\n            with tqdm.trange(total, **tqdm_args) as pbar:\n                for dp in df:\n                    start = time.time()\n                    socket.send(dump_fn(dp), copy=False)\n                    q.append(time.time() - start)\n                    pbar.update(1)\n                    if pbar.n % INTERVAL == 0:\n                        avg = ""{:.3f}"".format(sum(q) / len(q))\n                        pbar.set_postfix({\'AvgSendLat\': avg})\n    finally:\n        logger.info(""Exiting send_dataflow_zmq ..."")\n        socket.setsockopt(zmq.LINGER, 0)\n        socket.close()\n        if not ctx.closed:\n            ctx.destroy(0)\n\n\nclass RemoteDataZMQ(DataFlow):\n    """"""\n    Produce data from ZMQ PULL socket(s).\n    It is the receiver-side counterpart of :func:`send_dataflow_zmq`, which uses :mod:`tensorpack.utils.serialize`\n    for serialization.\n    See http://tensorpack.readthedocs.io/tutorial/efficient-dataflow.html#distributed-dataflow\n\n    Attributes:\n        cnt1, cnt2 (int): number of data points received from addr1 and addr2\n    """"""\n    def __init__(self, addr1, addr2=None, hwm=50, bind=True):\n        """"""\n        Args:\n            addr1,addr2 (str): addr of the zmq endpoint to connect to.\n                Use both if you need two protocols (e.g. both IPC and TCP).\n                I don\'t think you\'ll ever need 3.\n            hwm (int): ZMQ high-water mark (buffer size)\n            bind (bool): whether to connect or bind the endpoint\n        """"""\n        assert addr1\n        self._addr1 = addr1\n        self._addr2 = addr2\n        self._hwm = int(hwm)\n        self._guard = DataFlowReentrantGuard()\n        self._bind = bind\n\n    def reset_state(self):\n        self.cnt1 = 0\n        self.cnt2 = 0\n\n    def bind_or_connect(self, socket, addr):\n        if self._bind:\n            socket.bind(addr)\n        else:\n            socket.connect(addr)\n\n    def __iter__(self):\n        with self._guard:\n            try:\n                ctx = zmq.Context()\n                if self._addr2 is None:\n                    socket = ctx.socket(zmq.PULL)\n                    socket.set_hwm(self._hwm)\n                    self.bind_or_connect(socket, self._addr1)\n\n                    while True:\n                        dp = loads(socket.recv(copy=False))\n                        yield dp\n                        self.cnt1 += 1\n                else:\n                    socket1 = ctx.socket(zmq.PULL)\n                    socket1.set_hwm(self._hwm)\n                    self.bind_or_connect(socket1, self._addr1)\n\n                    socket2 = ctx.socket(zmq.PULL)\n                    socket2.set_hwm(self._hwm)\n                    self.bind_or_connect(socket2, self._addr2)\n\n                    poller = zmq.Poller()\n                    poller.register(socket1, zmq.POLLIN)\n                    poller.register(socket2, zmq.POLLIN)\n\n                    while True:\n                        evts = poller.poll()\n                        for sock, evt in evts:\n                            dp = loads(sock.recv(copy=False))\n                            yield dp\n                            if sock == socket1:\n                                self.cnt1 += 1\n                            else:\n                                self.cnt2 += 1\n            finally:\n                ctx.destroy(linger=0)\n\n\n# for internal use only\ndef dump_dataflow_to_process_queue(df, size, nr_consumer):\n    """"""\n    Convert a DataFlow to a :class:`multiprocessing.Queue`.\n    The DataFlow will only be reset in the spawned process.\n\n    Args:\n        df (DataFlow): the DataFlow to dump.\n        size (int): size of the queue\n        nr_consumer (int): number of consumer of the queue.\n            The producer will add this many of ``DIE`` sentinel to the end of the queue.\n\n    Returns:\n        tuple(queue, process):\n            The process will take data from ``df`` and fill\n            the queue, once you start it. Each element in the queue is (idx,\n            dp). idx can be the ``DIE`` sentinel when ``df`` is exhausted.\n    """"""\n    q = mp.Queue(size)\n\n    class EnqueProc(mp.Process):\n\n        def __init__(self, df, q, nr_consumer):\n            super(EnqueProc, self).__init__()\n            self.df = df\n            self.q = q\n\n        def run(self):\n            self.df.reset_state()\n            try:\n                for idx, dp in enumerate(self.df):\n                    self.q.put((idx, dp))\n            finally:\n                for _ in range(nr_consumer):\n                    self.q.put((DIE, None))\n\n    proc = EnqueProc(df, q, nr_consumer)\n    return q, proc\n\n\nif __name__ == \'__main__\':\n    from argparse import ArgumentParser\n    from .raw import FakeData\n    from .common import TestDataSpeed\n\n    """"""\n    Test the multi-producer single-consumer model\n    """"""\n    parser = ArgumentParser()\n    parser.add_argument(\'-t\', \'--task\', choices=[\'send\', \'recv\'], required=True)\n    parser.add_argument(\'-a\', \'--addr1\', required=True)\n    parser.add_argument(\'-b\', \'--addr2\', default=None)\n    args = parser.parse_args()\n\n    # tcp addr like ""tcp://127.0.0.1:8877""\n    # ipc addr like ""ipc://@ipc-test""\n    if args.task == \'send\':\n        # use random=True to make it slow and cpu-consuming\n        ds = FakeData([(128, 244, 244, 3)], 1000, random=True)\n        send_dataflow_zmq(ds, args.addr1)\n    else:\n        ds = RemoteDataZMQ(args.addr1, args.addr2)\n        logger.info(""Each DP is 73.5MB"")\n        TestDataSpeed(ds).start_test()\n'"
tensorpack/dataflow/serialize.py,2,"b'# -*- coding: utf-8 -*-\n# File: serialize.py\n\nimport numpy as np\nimport os\nimport platform\nfrom collections import defaultdict\n\nfrom ..utils import logger\nfrom ..utils.serialize import dumps, loads\nfrom ..utils.develop import create_dummy_class  # noqa\nfrom ..utils.utils import get_tqdm\nfrom .base import DataFlow\nfrom .common import FixedSizeData, MapData\nfrom .format import HDF5Data, LMDBData\nfrom .raw import DataFromGenerator, DataFromList\n\n__all__ = [\'LMDBSerializer\', \'NumpySerializer\', \'TFRecordSerializer\', \'HDF5Serializer\']\n\n\ndef _reset_df_and_get_size(df):\n    df.reset_state()\n    try:\n        sz = len(df)\n    except NotImplementedError:\n        sz = 0\n    return sz\n\n\nclass LMDBSerializer():\n    """"""\n    Serialize a Dataflow to a lmdb database, where the keys are indices and values\n    are serialized datapoints.\n\n    You will need to ``pip install lmdb`` to use it.\n\n    Example:\n\n    .. code-block:: python\n\n        LMDBSerializer.save(my_df, ""output.lmdb"")\n\n        new_df = LMDBSerializer.load(""output.lmdb"", shuffle=True)\n    """"""\n    @staticmethod\n    def save(df, path, write_frequency=5000):\n        """"""\n        Args:\n            df (DataFlow): the DataFlow to serialize.\n            path (str): output path. Either a directory or an lmdb file.\n            write_frequency (int): the frequency to write back data to disk.\n                A smaller value reduces memory usage.\n        """"""\n        assert isinstance(df, DataFlow), type(df)\n        isdir = os.path.isdir(path)\n        if isdir:\n            assert not os.path.isfile(os.path.join(path, \'data.mdb\')), ""LMDB file exists!""\n        else:\n            assert not os.path.isfile(path), ""LMDB file {} exists!"".format(path)\n        # It\'s OK to use super large map_size on Linux, but not on other platforms\n        # See: https://github.com/NVIDIA/DIGITS/issues/206\n        map_size = 1099511627776 * 2 if platform.system() == \'Linux\' else 128 * 10**6\n        db = lmdb.open(path, subdir=isdir,\n                       map_size=map_size, readonly=False,\n                       meminit=False, map_async=True)    # need sync() at the end\n        size = _reset_df_and_get_size(df)\n\n        # put data into lmdb, and doubling the size if full.\n        # Ref: https://github.com/NVIDIA/DIGITS/pull/209/files\n        def put_or_grow(txn, key, value):\n            try:\n                txn.put(key, value)\n                return txn\n            except lmdb.MapFullError:\n                pass\n            txn.abort()\n            curr_size = db.info()[\'map_size\']\n            new_size = curr_size * 2\n            logger.info(""Doubling LMDB map_size to {:.2f}GB"".format(new_size / 10**9))\n            db.set_mapsize(new_size)\n            txn = db.begin(write=True)\n            txn = put_or_grow(txn, key, value)\n            return txn\n\n        with get_tqdm(total=size) as pbar:\n            idx = -1\n\n            # LMDB transaction is not exception-safe!\n            # although it has a context manager interface\n            txn = db.begin(write=True)\n            for idx, dp in enumerate(df):\n                txn = put_or_grow(txn, u\'{:08}\'.format(idx).encode(\'ascii\'), dumps(dp))\n                pbar.update()\n                if (idx + 1) % write_frequency == 0:\n                    txn.commit()\n                    txn = db.begin(write=True)\n            txn.commit()\n\n            keys = [u\'{:08}\'.format(k).encode(\'ascii\') for k in range(idx + 1)]\n            with db.begin(write=True) as txn:\n                txn = put_or_grow(txn, b\'__keys__\', dumps(keys))\n\n            logger.info(""Flushing database ..."")\n            db.sync()\n        db.close()\n\n    @staticmethod\n    def load(path, shuffle=True):\n        """"""\n        Note:\n            If you found deserialization being the bottleneck, you can use :class:`LMDBData` as the reader\n            and run deserialization as a mapper in parallel.\n        """"""\n        df = LMDBData(path, shuffle=shuffle)\n        return MapData(df, LMDBSerializer._deserialize_lmdb)\n\n    @staticmethod\n    def _deserialize_lmdb(dp):\n        return loads(dp[1])\n\n\nclass NumpySerializer():\n    """"""\n    Serialize the entire dataflow to a npz dict.\n    Note that this would have to store the entire dataflow in memory,\n    and is also >10x slower than LMDB/TFRecord serializers.\n    """"""\n\n    @staticmethod\n    def save(df, path):\n        """"""\n        Args:\n            df (DataFlow): the DataFlow to serialize.\n            path (str): output npz file.\n        """"""\n        buffer = []\n        size = _reset_df_and_get_size(df)\n        with get_tqdm(total=size) as pbar:\n            for dp in df:\n                buffer.append(dp)\n                pbar.update()\n        np.savez_compressed(path, buffer=np.asarray(buffer, dtype=np.object))\n\n    @staticmethod\n    def load(path, shuffle=True):\n        # allow_pickle defaults to False since numpy 1.16.3\n        # (https://www.numpy.org/devdocs/release.html#unpickling-while-loading-requires-explicit-opt-in)\n        buffer = np.load(path, allow_pickle=True)[\'buffer\']\n        return DataFromList(buffer, shuffle=shuffle)\n\n\nclass TFRecordSerializer():\n    """"""\n    Serialize datapoints to bytes (by tensorpack\'s default serializer) and write to a TFRecord file.\n\n    Note that TFRecord does not support random access and is in fact not very performant.\n    It\'s better to use :class:`LMDBSerializer`.\n    """"""\n    @staticmethod\n    def save(df, path):\n        """"""\n        Args:\n            df (DataFlow): the DataFlow to serialize.\n            path (str): output tfrecord file.\n        """"""\n        size = _reset_df_and_get_size(df)\n        with tf.python_io.TFRecordWriter(path) as writer, get_tqdm(total=size) as pbar:\n            for dp in df:\n                writer.write(dumps(dp))\n                pbar.update()\n\n    @staticmethod\n    def load(path, size=None):\n        """"""\n        Args:\n            size (int): total number of records. If not provided, the returned dataflow will have no `__len__()`.\n                It\'s needed because this metadata is not stored in the TFRecord file.\n        """"""\n        gen = tf.python_io.tf_record_iterator(path)\n        ds = DataFromGenerator(gen)\n        ds = MapData(ds, loads)\n        if size is not None:\n            ds = FixedSizeData(ds, size)\n        return ds\n\n\nclass HDF5Serializer():\n    """"""\n    Write datapoints to a HDF5 file.\n\n    Note that HDF5 files are in fact not very performant and currently do not support lazy loading.\n    It\'s better to use :class:`LMDBSerializer`.\n    """"""\n    @staticmethod\n    def save(df, path, data_paths):\n        """"""\n        Args:\n            df (DataFlow): the DataFlow to serialize.\n            path (str): output hdf5 file.\n            data_paths (list[str]): list of h5 paths. It should have the same\n                length as each datapoint, and each path should correspond to one\n                component of the datapoint.\n        """"""\n        size = _reset_df_and_get_size(df)\n        buffer = defaultdict(list)\n\n        with get_tqdm(total=size) as pbar:\n            for dp in df:\n                assert len(dp) == len(data_paths), ""Datapoint has {} components!"".format(len(dp))\n                for k, el in zip(data_paths, dp):\n                    buffer[k].append(el)\n                pbar.update()\n\n        with h5py.File(path, \'w\') as hf, get_tqdm(total=len(data_paths)) as pbar:\n            for data_path in data_paths:\n                hf.create_dataset(data_path, data=buffer[data_path])\n                pbar.update()\n\n    @staticmethod\n    def load(path, data_paths, shuffle=True):\n        """"""\n        Args:\n            data_paths (list): list of h5 paths to be zipped.\n        """"""\n        return HDF5Data(path, data_paths, shuffle)\n\n\ntry:\n    import lmdb\nexcept ImportError:\n    LMDBSerializer = create_dummy_class(\'LMDBSerializer\', \'lmdb\')   # noqa\n\ntry:\n    import tensorflow as tf\nexcept ImportError:\n    TFRecordSerializer = create_dummy_class(\'TFRecordSerializer\', \'tensorflow\')   # noqa\n\ntry:\n    import h5py\nexcept ImportError:\n    HDF5Serializer = create_dummy_class(\'HDF5Serializer\', \'h5py\')   # noqa\n\n\nif __name__ == \'__main__\':\n    from .raw import FakeData\n    import time\n    ds = FakeData([[300, 300, 3], [1]], 1000)\n\n    print(time.time())\n    TFRecordSerializer.save(ds, \'out.tfrecords\')\n    print(time.time())\n    df = TFRecordSerializer.load(\'out.tfrecords\', size=1000)\n    df.reset_state()\n    for idx, dp in enumerate(df):\n        pass\n    print(""TF Finished, "", idx)\n    print(time.time())\n\n    LMDBSerializer.save(ds, \'out.lmdb\')\n    print(time.time())\n    df = LMDBSerializer.load(\'out.lmdb\')\n    df.reset_state()\n    for idx, dp in enumerate(df):\n        pass\n    print(""LMDB Finished, "", idx)\n    print(time.time())\n\n    NumpySerializer.save(ds, \'out.npz\')\n    print(time.time())\n    df = NumpySerializer.load(\'out.npz\')\n    df.reset_state()\n    for idx, dp in enumerate(df):\n        pass\n    print(""Numpy Finished, "", idx)\n    print(time.time())\n\n    paths = [\'p1\', \'p2\']\n    HDF5Serializer.save(ds, \'out.h5\', paths)\n    print(time.time())\n    df = HDF5Serializer.load(\'out.h5\', paths)\n    df.reset_state()\n    for idx, dp in enumerate(df):\n        pass\n    print(""HDF5 Finished, "", idx)\n    print(time.time())\n'"
tensorpack/dataflow/serialize_test.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tempfile\nimport numpy as np\nimport os\nimport unittest\n\nfrom tensorpack.dataflow import HDF5Serializer, LMDBSerializer, NumpySerializer, TFRecordSerializer\nfrom tensorpack.dataflow.base import DataFlow\n\n\ndef delete_file_if_exists(fn):\n    try:\n        os.remove(fn)\n    except OSError:\n        pass\n\n\nclass SeededFakeDataFlow(DataFlow):\n    """"""docstring for SeededFakeDataFlow""""""\n\n    def __init__(self, seed=42, size=32):\n        super(SeededFakeDataFlow, self).__init__()\n        self.seed = seed\n        self._size = size\n        self.cache = []\n\n    def reset_state(self):\n        np.random.seed(self.seed)\n        for _ in range(self._size):\n            label = np.random.randint(low=0, high=10)\n            img = np.random.randn(28, 28, 3)\n            self.cache.append([label, img])\n\n    def __len__(self):\n        return self._size\n\n    def __iter__(self):\n        for dp in self.cache:\n            yield dp\n\n\nclass SerializerTest(unittest.TestCase):\n\n    def run_write_read_test(self, file, serializer, w_args, w_kwargs, r_args, r_kwargs, error_msg):\n        try:\n            delete_file_if_exists(file)\n\n            ds_expected = SeededFakeDataFlow()\n            serializer.save(ds_expected, file, *w_args, **w_kwargs)\n            ds_actual = serializer.load(file, *r_args, **r_kwargs)\n\n            ds_actual.reset_state()\n            ds_expected.reset_state()\n\n            for dp_expected, dp_actual in zip(ds_expected.__iter__(), ds_actual.__iter__()):\n                self.assertEqual(dp_expected[0], dp_actual[0])\n                self.assertTrue(np.allclose(dp_expected[1], dp_actual[1]))\n        except ImportError:\n            print(error_msg)\n\n    def test_lmdb(self):\n        with tempfile.TemporaryDirectory() as f:\n            self.run_write_read_test(\n                os.path.join(f, \'test.lmdb\'),\n                LMDBSerializer,\n                {}, {},\n                {}, {\'shuffle\': False},\n                \'Skip test_lmdb, no lmdb available\')\n\n    def test_tfrecord(self):\n        with tempfile.TemporaryDirectory() as f:\n            self.run_write_read_test(\n                os.path.join(f, \'test.tfrecord\'),\n                TFRecordSerializer,\n                {}, {},\n                {}, {\'size\': 32},\n                \'Skip test_tfrecord, no tensorflow available\')\n\n    def test_numpy(self):\n        with tempfile.TemporaryDirectory() as f:\n            self.run_write_read_test(\n                os.path.join(f, \'test.npz\'),\n                NumpySerializer,\n                {}, {},\n                {}, {\'shuffle\': False},\n                \'Skip test_numpy, no numpy available\')\n\n    def test_hdf5(self):\n        args = [[\'label\', \'image\']]\n        with tempfile.TemporaryDirectory() as f:\n            self.run_write_read_test(\n                os.path.join(f, \'test.h5\'),\n                HDF5Serializer,\n                args, {},\n                args, {\'shuffle\': False},\n                \'Skip test_hdf5, no h5py available\')\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tensorpack/graph_builder/__init__.py,0,"b""#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()['kcah_acitats'[::-1].upper()] = False\nif STATICA_HACK:\n    from .model_desc import *\n    from .training import *\n    from .distributed import *\n    from .utils import *\n\nfrom .model_desc import ModelDesc, ModelDescBase\n\nfrom pkgutil import iter_modules\nimport os\nimport os.path\n\n__all__ = []\n\ndef global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if '__all__' in dir(p) else []\n    del globals()[name]\n    for k in lst:\n        if not k.startswith('__'):\n            globals()[k] = p.__dict__[k]\n            __all__.append(k)\n\n\n_CURR_DIR = os.path.dirname(__file__)\n_SKIP = ['distributed']\nfor _, module_name, _ in iter_modules(\n        [_CURR_DIR]):\n    srcpath = os.path.join(_CURR_DIR, module_name + '.py')\n    if not os.path.isfile(srcpath):\n        continue\n    if module_name.startswith('_'):\n        continue\n    if module_name not in _SKIP:\n        global_import(module_name)\n"""
tensorpack/graph_builder/distributed.py,33,"b'# -*- coding: utf-8 -*-\n# File: distributed.py\n\nimport re\nimport tensorflow as tf\n\nfrom ..tfutils.common import get_global_step_var, get_op_tensor_name\nfrom ..utils import logger\nfrom ..utils.argtools import memoized\nfrom .training import DataParallelBuilder, GraphBuilder\nfrom .utils import OverrideCachingDevice, aggregate_grads, override_to_local_variable\n\n__all__ = []\n\n\nclass DistributedBuilderBase(GraphBuilder):\n\n    _sync_queue_counter = 0\n\n    def __init__(self, server):\n        self.server = server\n        server_def = server.server_def\n        self.cluster = tf.train.ClusterSpec(server_def.cluster)\n        self.task_index = server_def.task_index\n\n        self.num_ps = self.cluster.num_tasks(\'ps\')\n        self.num_worker = self.cluster.num_tasks(\'worker\')\n\n    def _add_sync_queues_and_barrier(self, name, dependencies):\n        """"""Adds ops to enqueue on all worker queues.\n\n        Args:\n            name: prefixed for the shared_name of ops.\n            dependencies: control dependency from ops.\n\n        Returns:\n            an op that should be used as control dependency before starting next step.\n        """"""\n        self._sync_queue_counter += 1\n        with tf.device(self.sync_queue_devices[self._sync_queue_counter % len(self.sync_queue_devices)]):\n            sync_queues = [\n                tf.FIFOQueue(self.num_worker, [tf.bool], shapes=[[]],\n                             shared_name=\'%s%s\' % (name, i))\n                for i in range(self.num_worker)]\n            queue_ops = []\n            # For each other worker, add an entry in a queue, signaling that it can finish this step.\n            token = tf.constant(False)\n            with tf.control_dependencies(dependencies):\n                for i, q in enumerate(sync_queues):\n                    if i != self.task_index:\n                        queue_ops.append(q.enqueue(token))\n\n            # Drain tokens off queue for this worker, one for each other worker.\n            queue_ops.append(\n                sync_queues[self.task_index].dequeue_many(len(sync_queues) - 1))\n\n            return tf.group(*queue_ops, name=name)\n\n\nclass DistributedParameterServerBuilder(DataParallelBuilder, DistributedBuilderBase):\n    """"""\n    Distributed parameter server training.\n    A single copy of parameters are scattered around PS.\n    Gradients across GPUs are averaged within the worker, and applied to PS.\n    Each worker also caches the variables for reading.\n\n    It is an equivalent of ``--variable_update=parameter_server`` in\n    `tensorflow/benchmarks <https://github.com/tensorflow/benchmarks>`_.\n    However this implementation hasn\'t been well tested.\n    It probably still has issues in model saving, etc.\n    Also, TensorFlow team is not actively maintaining distributed training features.\n    Check :class:`HorovodTrainer` and\n    `ResNet-Horovod <https://github.com/tensorpack/benchmarks/tree/master/ResNet-Horovod>`_\n    for better distributed training support.\n\n    Note:\n        1. Gradients are not averaged across workers, but applied to PS variables\n           directly (either with or without locking depending on the optimizer).\n    """"""\n\n    def __init__(self, towers, server, caching_device):\n        """"""\n        Args:\n            towers (list[int]): list of GPU ids.\n            server (tf.train.Server): the server with ps and workers.\n                job_name must be \'worker\'.\n            caching_device (str): either \'cpu\' or \'gpu\'\n        """"""\n        DataParallelBuilder.__init__(self, towers)\n        DistributedBuilderBase.__init__(self, server)\n\n        assert caching_device in [\'cpu\', \'gpu\'], caching_device\n        self.caching_device = caching_device\n\n        self.is_chief = (self.task_index == 0)\n\n        worker_prefix = \'/job:worker/task:%s\' % self.task_index\n        self.param_server_device = tf.train.replica_device_setter(\n            worker_device=worker_prefix + \'/cpu:0\', cluster=self.cluster)\n        self.cpu_device = \'%s/cpu:0\' % worker_prefix\n        self.raw_devices = [\'{}/gpu:{}\'.format(worker_prefix, k) for k in self.towers]\n\n        self.sync_queue_devices = [\'/job:ps/task:%s/cpu:0\' % i for i in range(self.num_ps)]\n\n    def build(self, get_grad_fn, get_opt_fn):\n        ps_strategy = tf.contrib.training.GreedyLoadBalancingStrategy(\n            self.num_ps, tf.contrib.training.byte_size_load_fn)\n        devices = [\n            tf.train.replica_device_setter(\n                worker_device=d,\n                cluster=self.cluster,\n                ps_strategy=ps_strategy) for d in self.raw_devices]\n\n        if self.caching_device == \'gpu\':\n            caching_devices = self.raw_devices\n        else:\n            caching_devices = [self.cpu_device]\n        custom_getter = OverrideCachingDevice(\n            caching_devices, self.cpu_device, 1024 * 64)\n\n        with tf.variable_scope(tf.get_variable_scope(), custom_getter=custom_getter):\n            grad_list = DataParallelBuilder.build_on_towers(self.towers, get_grad_fn, devices)\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        with tf.device(self.param_server_device):\n            grads = aggregate_grads(grad_list, colocation=False)\n            opt = get_opt_fn()\n            train_op = opt.apply_gradients(grads, name=\'train_op\')\n        train_op = self._add_sync_queues_and_barrier(\'all_workers_sync_barrier\', [train_op])\n        return train_op\n\n\nclass DistributedReplicatedBuilder(DataParallelBuilder, DistributedBuilderBase):\n    """"""\n    Distributed replicated training.\n    Each worker process builds the same model on one or more GPUs.\n    Gradients across GPUs are averaged within the worker,\n    and get synchronously applied to the global copy of variables located on PS.\n    Then each worker copy the latest variables from PS back to local.\n\n    It is an equivalent of ``--variable_update=distributed_replicated`` in\n    `tensorflow/benchmarks <https://github.com/tensorflow/benchmarks>`_.\n    Note that the performance of this trainer is still not satisfactory,\n    and TensorFlow team is not actively maintaining distributed training features.\n    Check :class:`HorovodTrainer` and\n    `ResNet-Horovod <https://github.com/tensorpack/benchmarks/tree/master/ResNet-Horovod>`_\n    for better distributed training support.\n\n    Note:\n        1. Gradients are not averaged across workers, but applied to PS variables\n           directly (either with or without locking depending on the optimizer).\n        2. Some details about collections: all variables created inside tower\n           will become local variables,\n           and a clone will be made in global variables for all trainable/model variables.\n\n    Example:\n\n        .. code-block:: python\n\n            # Create the server object like this:\n            hosts = [\'host1.com\', \'host2.com\']\n            cluster_spec = tf.train.ClusterSpec({\n                \'ps\': [h + \':2222\' for h in hosts],\n                \'worker\': [h + \':2223\' for h in hosts]\n            })\n            server = tf.train.Server(\n                cluster_spec, job_name=args.job, task_index=args.task,\n                config=get_default_sess_config())\n            # initialize trainer with this server object\n\n        .. code-block:: none\n\n            # Start training like this:\n            (host1)$ ./train.py --job worker --task 0\n            (host1)$ CUDA_VISIBLE_DEVICES= ./train.py --job ps --task 0\n            (host2)$ ./train.py --job worker --task 1\n            (host2)$ CUDA_VISIBLE_DEVICES= ./train.py --job ps --task 1\n    """"""\n\n    def __init__(self, towers, server):\n        """"""\n        Args:\n            towers (list[int]): list of GPU ids.\n            server (tf.train.Server): the server with ps and workers.\n                job_name must be \'worker\'.\n        """"""\n        DataParallelBuilder.__init__(self, towers)\n        DistributedBuilderBase.__init__(self, server)\n\n        self.is_chief = (self.task_index == 0)\n\n        worker_prefix = \'/job:worker/task:%s\' % self.task_index\n        self.param_server_device = tf.train.replica_device_setter(\n            worker_device=worker_prefix + \'/cpu:0\', cluster=self.cluster)\n\n        self.nr_gpu = len(self.towers)\n        self.cpu_device = \'%s/cpu:0\' % worker_prefix\n        self.raw_devices = [\'%s/gpu:%i\' % (worker_prefix, i) for i in towers]\n\n        # Device for queues for managing synchronization between servers\n        self.sync_queue_devices = [\'/job:ps/task:%s/cpu:0\' % i for i in range(self.num_ps)]\n\n    @staticmethod\n    def _apply_shadow_vars(avg_grads):\n        """"""\n        Create shadow variables on PS, and replace variables in avg_grads\n        by these shadow variables.\n\n        Args:\n            avg_grads: list of (grad, var) tuples\n        """"""\n        ps_var_grads = []\n        for grad, var in avg_grads:\n            assert var.name.startswith(\'tower\'), var.name\n            my_name = \'/\'.join(var.name.split(\'/\')[1:])\n            my_name = get_op_tensor_name(my_name)[0]\n            new_v = tf.get_variable(my_name, dtype=var.dtype.base_dtype,\n                                    initializer=var.initial_value,\n                                    trainable=True)\n            # (g, v) to be applied, where v is global (ps vars)\n            ps_var_grads.append((grad, new_v))\n        return ps_var_grads\n\n    @staticmethod\n    def _shadow_model_variables(shadow_vars):\n        """"""\n        Create shadow vars for model_variables as well, and add to the list of ``shadow_vars``.\n\n        Returns:\n            list of (shadow_model_var, local_model_var) used for syncing.\n        """"""\n        G = tf.get_default_graph()\n        curr_shadow_vars = {v.name for v in shadow_vars}\n        model_vars = tf.model_variables()\n        shadow_model_vars = []\n        for v in model_vars:\n            assert v.name.startswith(\'tower\'), ""Found some MODEL_VARIABLES created outside of the tower function!""\n            stripped_op_name, stripped_var_name = get_op_tensor_name(re.sub(\'^tower[0-9]+/\', \'\', v.name))\n            if stripped_op_name in curr_shadow_vars:\n                continue\n            try:\n                G.get_tensor_by_name(stripped_var_name)\n                logger.warn(""Model Variable {} also appears in other collections."".format(stripped_var_name))\n                continue\n            except KeyError:\n                pass\n            new_v = tf.get_variable(stripped_op_name, dtype=v.dtype.base_dtype,\n                                    initializer=v.initial_value,\n                                    trainable=False)\n\n            curr_shadow_vars.add(stripped_op_name)  # avoid duplicated shadow_model_vars\n            shadow_vars.append(new_v)\n            shadow_model_vars.append((new_v, v))  # only need to sync model_var from one tower\n        return shadow_model_vars\n\n    def build(self, get_grad_fn, get_opt_fn):\n        """"""\n        Args:\n            get_grad_fn (-> [(grad, var)]):\n            get_opt_fn (-> tf.train.Optimizer): callable which returns an optimizer\n\n        Returns:\n            (tf.Operation, tf.Operation, tf.Operation):\n\n            1. the training op.\n\n            2. the op which sync all the local variables from PS.\n            This op should be run before training.\n\n            3. the op which sync all the local `MODEL_VARIABLES` from PS.\n            You can choose how often to run it by yourself.\n        """"""\n        with override_to_local_variable():\n            get_global_step_var()\n\n        get_opt_fn = memoized(get_opt_fn)\n        # Build the optimizer first, before entering any tower.\n        # This makes sure that learning_rate is a global variable (what we expect)\n        get_opt_fn()    # TODO get_opt_fn called before main graph was built\n\n        # Ngpu * Nvar * 2\n        grad_list = DataParallelBuilder.build_on_towers(\n            self.towers, get_grad_fn,\n            devices=self.raw_devices,\n            use_vs=[True] * len(self.towers))  # open vs at each tower\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        avg_grads = aggregate_grads(\n            grad_list, colocation=False, devices=self.raw_devices)\n        with tf.device(self.param_server_device):\n            ps_var_grads = DistributedReplicatedBuilder._apply_shadow_vars(avg_grads)\n            var_update_ops = self._apply_gradients_and_copy(\n                get_opt_fn(), grad_list, ps_var_grads)\n            self._shadow_vars = [v for (__, v) in ps_var_grads]\n            self._shadow_model_vars = DistributedReplicatedBuilder._shadow_model_variables(self._shadow_vars)\n\n        # TODO add options to synchronize less\n        main_fetch = tf.group(*var_update_ops, name=\'main_fetches\')\n        train_op = self._add_sync_queues_and_barrier(\n            \'post_copy_barrier\', [main_fetch])\n\n        # initial local_vars syncing\n        with tf.name_scope(\'initial_sync_variables\'):\n            initial_sync_op = self._get_initial_sync_op()\n        if len(self._shadow_model_vars) and self.is_chief:\n            with tf.name_scope(\'sync_model_variables\'):\n                model_sync_op = self._get_sync_model_vars_op()\n        else:\n            model_sync_op = None\n        return train_op, initial_sync_op, model_sync_op\n\n    def _apply_gradients_and_copy(self, opt, raw_grad_list, ps_var_grads):\n        """"""\n        Apply averaged gradients to ps vars, and then copy the updated\n        variables back to each tower.\n\n        Args:\n            raw_grad_list: Ngpu x Nvar x 2 gradient list from all towers\n            ps_var_grads: Nvar x 2 (grad, ps_var)\n\n        Returns:\n            list of copy ops\n        """"""\n        # TODO do this for variables together?\n        with tf.name_scope(\'apply_gradients\'):\n            var_update_ops = []\n            for vid, (g, v) in enumerate(ps_var_grads):\n                # TODO do we put momentum variables into local or global?\n                apply_gradient_op = opt.apply_gradients([(g, v)])\n                barrier = self._add_sync_queues_and_barrier(\n                    \'param_update_barrier_{}\'.format(vid), [apply_gradient_op])\n                with tf.control_dependencies([barrier]), \\\n                        tf.device(self.cpu_device):\n                    updated_value = v.read_value()\n                    for towerid in range(self.nr_gpu):\n                        var_update_ops.append(\n                            raw_grad_list[towerid][vid][1].assign(updated_value))\n            return var_update_ops\n\n    def _get_initial_sync_op(self):\n        """"""\n        Get the op to copy-initialized all local variables from PS.\n        """"""\n        def strip_port(s):\n            if s.endswith(\':0\'):\n                return s[:-2]\n            return s\n        local_vars = tf.local_variables()\n        local_var_by_name = {strip_port(v.name): v for v in local_vars}\n        ops = []\n        nr_shadow_vars = len(self._shadow_vars)\n        for v in self._shadow_vars:\n            vname = strip_port(v.name)\n            for i in range(self.nr_gpu):\n                name = \'tower%s/%s\' % (i, vname)\n                assert name in local_var_by_name, \\\n                    ""Shadow variable {} doesn\'t match a corresponding local variable!"".format(v.name)\n                copy_to = local_var_by_name[name]\n                # logger.info(""{} -> {}"".format(v.name, copy_to.name))\n                ops.append(copy_to.assign(v.read_value()))\n        return tf.group(*ops, name=\'sync_{}_variables_from_ps\'.format(nr_shadow_vars))\n\n    def _get_sync_model_vars_op(self):\n        """"""\n        Get the op to sync local model_variables to PS.\n        """"""\n        ops = []\n        for (shadow_v, local_v) in self._shadow_model_vars:\n            ops.append(shadow_v.assign(local_v.read_value()))\n        assert len(ops)\n        return tf.group(*ops, name=\'sync_{}_model_variables_to_ps\'.format(len(ops)))\n'"
tensorpack/graph_builder/model_desc.py,0,"b'# -*- coding: utf-8 -*-\n# File: model_desc.py\n\n\nfrom ..train.model_desc import ModelDesc, ModelDescBase  # kept for BC # noqa\n\n\n__all__ = []\n'"
tensorpack/graph_builder/training.py,24,"b'# -*- coding: utf-8 -*-\n# File: training.py\n\nimport copy\nimport pprint\nimport re\nfrom abc import ABCMeta, abstractmethod\nfrom contextlib import contextmanager\nimport six\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..tfutils.common import get_tf_version_tuple\nfrom ..tfutils.gradproc import ScaleGradient\nfrom ..tfutils.tower import TrainTowerContext\nfrom ..utils import logger\nfrom ..utils.develop import HIDE_DOC\nfrom .utils import (\n    GradientPacker, LeastLoadedDeviceSetter, aggregate_grads, allreduce_grads, allreduce_grads_hierarchical,\n    merge_grad_list, override_to_local_variable, split_grad_list)\n\n__all__ = [""DataParallelBuilder""]\n\n\n@six.add_metaclass(ABCMeta)\nclass GraphBuilder(object):\n    @abstractmethod\n    def build(*args, **kwargs):\n        pass\n\n\n@contextmanager\ndef _maybe_reuse_vs(reuse):\n    if reuse:\n        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n            yield\n    else:\n        yield\n\n\nclass DataParallelBuilder(GraphBuilder):\n    def __init__(self, towers):\n        """"""\n        Args:\n            towers(list[int]): list of GPU ids.\n        """"""\n        if len(towers) > 1:\n            logger.info(""[DataParallel] Training a model of {} towers."".format(len(towers)))\n            if not tf.test.is_built_with_cuda():\n                logger.error(""[DataParallel] TensorFlow was not built with CUDA support!"")\n\n        self.towers = towers\n\n    @staticmethod\n    def _check_grad_list(grad_list):\n        """"""\n        Args:\n            grad_list: list of list of tuples, shape is Ngpu x Nvar x 2\n        """"""\n        nvars = [len(k) for k in grad_list]\n\n        def basename(x):\n            return re.sub(\'tower[0-9]+/\', \'\', x.op.name)\n\n        if len(set(nvars)) != 1:\n            names_per_gpu = [{basename(k[1]) for k in grad_and_vars} for grad_and_vars in grad_list]\n            inters = copy.copy(names_per_gpu[0])\n            for s in names_per_gpu:\n                inters &= s\n            for s in names_per_gpu:\n                s -= inters\n            logger.error(""Unique trainable variables on towers: "" + pprint.pformat(names_per_gpu))\n            raise ValueError(""Number of gradients from each tower is different! "" + str(nvars))\n\n    @staticmethod\n    def call_for_each_tower(\n            towers, func, devices=None, use_vs=None):\n        """"""\n        Run `func` on all GPUs (towers) and return the results.\n\n        Args:\n            towers (list[int]): a list of GPU id.\n            func: a lambda to be called inside each tower\n            devices: a list of devices to be used. By default will use \'/gpu:{tower}\'\n            use_vs (list[bool]): list of use_vs to passed to TowerContext\n\n        Returns:\n            List of outputs of ``func``, evaluated on each tower.\n        """"""\n\n        ret = []\n        if devices is not None:\n            assert len(devices) == len(towers)\n        if use_vs is not None:\n            assert len(use_vs) == len(towers)\n\n        tower_names = [\'tower{}\'.format(idx) for idx in range(len(towers))]\n\n        for idx, t in enumerate(towers):\n            device = devices[idx] if devices is not None else \'/gpu:{}\'.format(t)\n            usevs = use_vs[idx] if use_vs is not None else False\n            reuse = not usevs and idx > 0\n            with tfv1.device(device), _maybe_reuse_vs(reuse), TrainTowerContext(\n                    tower_names[idx],\n                    vs_name=tower_names[idx] if usevs else \'\',\n                    index=idx, total=len(towers)):\n                if len(str(device)) < 10:   # a device function doesn\'t have good string description\n                    logger.info(""Building graph for training tower {} on device {} ..."".format(idx, device))\n                else:\n                    logger.info(""Building graph for training tower {} ..."".format(idx))\n\n                # When use_vs is True, use LOCAL_VARIABLES,\n                # so these duplicated variables won\'t be saved by default.\n                with override_to_local_variable(enable=usevs):\n                    ret.append(func())\n        return ret\n\n    @staticmethod\n    @HIDE_DOC\n    def build_on_towers(*args, **kwargs):\n        return DataParallelBuilder.call_for_each_tower(*args, **kwargs)\n\n\nclass SyncMultiGPUParameterServerBuilder(DataParallelBuilder):\n    """"""\n    Data-parallel training in \'ParameterServer\' mode.\n    It builds one tower on each GPU with\n    shared variable scope. It synchronizes the gradients computed\n    from each tower, averages them and applies to the shared variables.\n\n    It is an equivalent of ``--variable_update=parameter_server`` in\n    `tensorflow/benchmarks <https://github.com/tensorflow/benchmarks>`_.\n    """"""\n    def __init__(self, towers, ps_device):\n        """"""\n        Args:\n            towers(list[int]): list of GPU id\n            ps_device (str): either \'gpu\' or \'cpu\', where variables are stored.\n        """"""\n        super(SyncMultiGPUParameterServerBuilder, self).__init__(towers)\n        assert ps_device in [\'cpu\', \'gpu\']\n        self.ps_device = ps_device\n\n    def call_for_each_tower(self, tower_fn):\n        """"""\n        Call the function `tower_fn` under :class:`TowerContext` for each tower.\n\n        Returns:\n            a list, contains the return values of `tower_fn` on each tower.\n        """"""\n        raw_devices = [\'/gpu:{}\'.format(k) for k in self.towers]\n        if self.ps_device == \'gpu\':\n            devices = [LeastLoadedDeviceSetter(d, raw_devices) for d in raw_devices]\n        else:\n            devices = [tf.train.replica_device_setter(\n                worker_device=d, ps_device=\'/cpu:0\', ps_tasks=1) for d in raw_devices]\n\n        return DataParallelBuilder.build_on_towers(self.towers, tower_fn, devices)\n\n    def build(self, grad_list, get_opt_fn):\n        """"""\n        Reduce the gradients, apply them with the optimizer,\n        and set self.grads to a list of (g, v), containing the averaged gradients.\n\n        Args:\n            grad_list ([[(grad, var), ...], ...]): #GPU lists to be reduced. Each is the gradients computed on each GPU.\n            get_opt_fn (-> tf.train.Optimizer): callable which returns an optimizer\n\n        Returns:\n            tf.Operation: the training op\n        """"""\n        assert len(grad_list) == len(self.towers)\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        # debug tower performance (without update):\n        # ops = [k[0] for k in grad_list[1]] + [k[0] for k in grad_list[0]]\n        # self.train_op = tf.group(*ops)\n        # return\n\n        self.grads = aggregate_grads(grad_list, colocation=True)\n        # grads = grad_list[0]\n\n        opt = get_opt_fn()\n        if self.ps_device == \'cpu\':\n            with tf.device(\'/cpu:0\'):\n                train_op = opt.apply_gradients(self.grads, name=\'train_op\')\n        else:\n            train_op = opt.apply_gradients(self.grads, name=\'train_op\')\n        return train_op\n\n\nclass SyncMultiGPUReplicatedBuilder(DataParallelBuilder):\n    """"""\n    Data-parallel training in ""replicated"" mode,\n    where each GPU contains a replicate of the whole model.\n    It will build one tower on each GPU under its own variable scope.\n    Each gradient update is averaged or summed across or GPUs through NCCL.\n\n    It is an equivalent of ``--variable_update=replicated`` in\n    `tensorflow/benchmarks <https://github.com/tensorflow/benchmarks>`_.\n    """"""\n\n    def __init__(self, towers, average, mode):\n        super(SyncMultiGPUReplicatedBuilder, self).__init__(towers)\n        self._average = average\n        assert mode in [\'nccl\', \'cpu\', \'hierarchical\'], mode\n        self._mode = mode\n\n        if self._mode == \'hierarchical\' and len(towers) != 8:\n            logger.warn(""mode=\'hierarchical\' require >= 8 GPUs. Fallback to mode=\'nccl\'."")\n            self._mode = \'nccl\'\n\n    def call_for_each_tower(self, tower_fn):\n        """"""\n        Call the function `tower_fn` under :class:`TowerContext` for each tower.\n\n        Returns:\n            a list, contains the return values of `tower_fn` on each tower.\n        """"""\n        # if tower_fn returns [(grad, var), ...], this returns #GPU x #VAR x 2\n        return DataParallelBuilder.build_on_towers(\n            self.towers,\n            tower_fn,\n            # use no variable scope for the first tower\n            use_vs=[False] + [True] * (len(self.towers) - 1))\n\n    def build(self, grad_list, get_opt_fn):\n        """"""\n        Reduce the gradients, apply them with the optimizer,\n        and set self.grads to #GPU number of lists of (g, v), containing the all-reduced gradients on each device.\n\n        Args:\n            grad_list ([[(grad, var), ...], ...]): #GPU lists to be reduced. Each is the gradients computed on each GPU.\n            get_opt_fn (-> tf.train.Optimizer): callable which returns an optimizer\n\n        Returns:\n            (tf.Operation, tf.Operation)\n\n            1. the training op.\n\n            2. the op which sync variables from GPU 0 to other GPUs.\n                It has to be run before the training has started.\n                And you can optionally run it later to sync non-trainable variables.\n        """"""\n        assert len(grad_list) == len(self.towers)\n        raw_devices = [\'/gpu:{}\'.format(k) for k in self.towers]\n\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        dtypes = {x[0].dtype.base_dtype for x in grad_list[0]}\n        dtypes_nccl_supported = [tf.float32, tf.float64]\n        if get_tf_version_tuple() >= (1, 8):\n            dtypes_nccl_supported.append(tf.float16)\n        valid_for_nccl = all(k in dtypes_nccl_supported for k in dtypes)\n        if self._mode == \'nccl\' and not valid_for_nccl:\n            logger.warn(""Cannot use mode=\'nccl\' because some gradients have unsupported types. Fallback to mode=\'cpu\'"")\n            self._mode = \'cpu\'\n\n        if self._mode in [\'nccl\', \'hierarchical\']:\n            all_grads, all_vars = split_grad_list(grad_list)\n            # use allreduce from tf-benchmarks\n            # from .batch_allreduce import AllReduceSpecAlgorithm\n            # algo = AllReduceSpecAlgorithm(\'nccl\', list(range(8)), 0, 10)\n            # all_grads, warmup_ops = algo.batch_all_reduce(all_grads, 1, True, False)\n            # print(""WARMUP OPS"", warmup_ops)\n\n            if self._mode == \'nccl\':\n                all_grads = allreduce_grads(all_grads, average=self._average)  # #gpu x #param\n            else:\n                packer = GradientPacker(len(raw_devices))\n                succ = packer.compute_strategy(all_grads[0])\n                if succ:\n                    packed_grads = packer.pack_all(all_grads, raw_devices)\n                    packed_grads_aggr = allreduce_grads_hierarchical(\n                        packed_grads, raw_devices, average=self._average)\n                    all_grads = packer.unpack_all(packed_grads_aggr, raw_devices)\n                else:\n                    all_grads = allreduce_grads_hierarchical(all_grads, raw_devices, average=self._average)\n\n            self.grads = merge_grad_list(all_grads, all_vars)\n        elif self._mode == \'cpu\':\n            agg_grad_and_vars = aggregate_grads(\n                grad_list, colocation=False,\n                devices=[\'/cpu:0\'], average=self._average)    # #param x 2\n            self.grads = []  # #gpu x #param x 2\n            for grad_and_vars in grad_list:   # grad_and_vars: #paramx2\n                # take v from each tower, and g from average.\n                self.grads.append(\n                    [(g, v) for (_, v), (g, _) in zip(grad_and_vars, agg_grad_and_vars)])\n\n        train_ops = []\n        opt = get_opt_fn()\n        with tf.name_scope(\'apply_gradients\'):\n            for idx, grad_and_vars in enumerate(self.grads):\n                with tf.device(raw_devices[idx]):\n                    # apply_gradients may create variables. Make them LOCAL_VARIABLES\n                    with override_to_local_variable(enable=idx > 0):\n                        train_ops.append(opt.apply_gradients(\n                            grad_and_vars, name=\'apply_grad_{}\'.format(idx)))\n        train_op = tf.group(*train_ops, name=\'train_op\')\n\n        if len(self.towers) > 1:\n            with tf.name_scope(\'sync_variables\'):\n                post_init_op = SyncMultiGPUReplicatedBuilder.get_post_init_ops()\n        else:\n            post_init_op = None\n        return train_op, post_init_op\n\n# Adopt from https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/variable_mgr.py\n    @staticmethod\n    def get_post_init_ops():\n        """"""\n        Copy values of variables on GPU 0 to other GPUs.\n        """"""\n        # literally all variables, because it\'s better to sync optimizer-internal variables as well\n        all_vars = tf.global_variables() + tf.local_variables()\n        var_by_name = {v.name: v for v in all_vars}\n        trainable_names = {x.name for x in tf.trainable_variables()}\n        post_init_ops = []\n\n        def log_failure(name, reason):\n            logger.warn(""[ReplicatedTrainer] Do not know how to sync variable \'{}\' across GPUs. ""\n                        ""Reason: {} "".format(name, reason))\n            assert name not in trainable_names, \\\n                ""The aforementioned variable is trainable, so this is probably a fatal error.""\n            logger.warn(\n                ""[ReplicatedTrainer] This variable is non-trainable. ""\n                ""Ignore this warning if you know it\'s OK to leave it out-of-sync."")\n\n        for v in all_vars:\n            if not v.name.startswith(\'tower\'):\n                continue\n            if v.name.startswith(\'tower0\'):\n                # in this trainer, the master name doesn\'t have the towerx/ prefix\n                log_failure(v.name, ""Name should not have prefix \'tower0\' in this trainer!"")\n                continue        # TODO some vars (EMA) may still startswith tower0\n\n            split_name = v.name.split(\'/\')\n            prefix = split_name[0]\n            realname = \'/\'.join(split_name[1:])\n            if prefix in realname:\n                log_failure(v.name, ""Prefix {} appears multiple times in its name!"".format(prefix))\n                continue\n            copy_from = var_by_name.get(realname)\n            if copy_from is not None:\n                post_init_ops.append(v.assign(copy_from.read_value()))\n            else:\n                log_failure(v.name, ""Cannot find {} in the graph!"".format(realname))\n        logger.info(\n            ""\'sync_variables_from_main_tower\' includes {} operations."".format(len(post_init_ops)))\n        return tf.group(*post_init_ops, name=\'sync_variables_from_main_tower\')\n\n\nclass AsyncMultiGPUBuilder(DataParallelBuilder):\n    """"""\n    Data-parallel training with async update.\n    It builds one tower on each GPU with shared variable scope.\n    Every tower computes the gradients and independently applies them to the\n    variables, without synchronizing and averaging across towers.\n    """"""\n\n    def __init__(self, towers, scale_gradient=True):\n        """"""\n        Args:\n            towers(list[int]): list of GPU ids.\n            scale_gradient (bool): if True, will scale each gradient by ``1.0/nr_gpu``.\n        """"""\n        super(AsyncMultiGPUBuilder, self).__init__(towers)\n        self._scale_gradient = scale_gradient\n\n    def call_for_each_tower(self, tower_fn):\n        """"""\n        Call the function `tower_fn` under :class:`TowerContext` for each tower.\n\n        Returns:\n            a list, contains the return values of `tower_fn` on each tower.\n        """"""\n        ps_device = \'cpu\' if len(self.towers) >= 4 else \'gpu\'\n\n        raw_devices = [\'/gpu:{}\'.format(k) for k in self.towers]\n        if ps_device == \'gpu\':\n            devices = [LeastLoadedDeviceSetter(d, raw_devices) for d in raw_devices]\n        else:\n            devices = [tf.train.replica_device_setter(\n                worker_device=d, ps_device=\'/cpu:0\', ps_tasks=1) for d in raw_devices]\n\n        return DataParallelBuilder.build_on_towers(self.towers, tower_fn, devices)\n\n    def build(self, grad_list, get_opt_fn):\n        """"""\n        Args:\n            grad_list ([[(grad, var), ...], ...]): #GPU lists to be reduced. Each is the gradients computed on each GPU.\n            get_opt_fn (-> tf.train.Optimizer): callable which returns an optimizer\n\n        Returns:\n            tf.Operation: the training op\n        """"""\n        assert len(grad_list) == len(self.towers)\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        if self._scale_gradient and len(self.towers) > 1:\n            # pretend to average the grads, in order to make async and\n            # sync have consistent effective learning rate\n            gradproc = ScaleGradient((\'.*\', 1.0 / len(self.towers)), verbose=False)\n            grad_list = [gradproc.process(gv) for gv in grad_list]\n        # Ngpu x Nvar x 2\n\n        train_ops = []\n        opt = get_opt_fn()\n        with tf.name_scope(\'async_apply_gradients\'):\n            for i, grad_and_vars in enumerate(zip(*grad_list)):\n                # Ngpu x 2\n                v = grad_and_vars[0][1]\n                with tf.device(v.device):\n                    # will call apply_gradients (therefore gradproc) multiple times\n                    train_ops.append(opt.apply_gradients(\n                        grad_and_vars, name=\'apply_grad_{}\'.format(i)))\n        return tf.group(*train_ops, name=\'train_op\')\n'"
tensorpack/graph_builder/utils.py,30,"b'# -*- coding: utf-8 -*-\n# File: utils.py\n\n\nimport operator\nfrom contextlib import contextmanager\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..tfutils.common import get_tf_version_tuple\nfrom ..tfutils.scope_utils import cached_name_scope, under_name_scope\nfrom ..tfutils.varreplace import custom_getter_scope\nfrom ..utils import logger\nfrom ..utils.argtools import call_only_once\n\n__all__ = [""LeastLoadedDeviceSetter"", ""allreduce_grads"", ""aggregate_grads""]\n\n\n""""""\nSome utilities for building the graph.\n""""""\n\n\ndef _replace_global_by_local(kwargs):\n    if \'collections\' in kwargs:\n        collections = kwargs[\'collections\']\n    if not collections:\n        collections = {tf.GraphKeys.GLOBAL_VARIABLES}\n    else:\n        collections = set(collections.copy())\n    collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)\n    collections.add(tf.GraphKeys.LOCAL_VARIABLES)\n    kwargs[\'collections\'] = list(collections)\n\n\n@contextmanager\ndef override_to_local_variable(enable=True):\n    """"""\n    Returns:\n        a context where all variables will be created as local.\n    """"""\n    if enable:\n\n        def custom_getter(getter, name, *args, **kwargs):\n            _replace_global_by_local(kwargs)\n            return getter(name, *args, **kwargs)\n\n        with custom_getter_scope(custom_getter):\n            yield\n    else:\n        yield\n\n\n# https://github.com/tensorflow/benchmarks/blob/48cbef14a592e02a14beee8e9aef3ad22cadaed1/scripts/tf_cnn_benchmarks/variable_mgr_util.py#L192-L218\nclass LeastLoadedDeviceSetter(object):\n    """"""\n    Helper class to assign variables on the least loaded ps-device.\n\n    Usage:\n\n        .. code-block:: python\n\n            with tf.device(LeastLoadedDeviceSetter(...)):\n                ...\n    """"""\n    def __init__(self, worker_device, ps_devices):\n        """"""\n        Args:\n            worker_device: the device to use for compute ops.\n            ps_devices: a list of device to use for Variable ops.\n        """"""\n        self.ps_devices = ps_devices\n        self.worker_device = worker_device\n        self.ps_sizes = [0] * len(self.ps_devices)\n\n    def __call__(self, op):\n        # from tensorflow.python.training.device_util import canonicalize\n        # from tensorflow.python.distribute.device_util import canonicalize\n        def canonicalize(name):    # tensorflow/tensorflow#11484\n            return tfv1.DeviceSpec.from_string(name).to_string()\n\n        if op.device:\n            return op.device\n        if op.type not in [\'Variable\', \'VariableV2\']:\n            return canonicalize(self.worker_device)\n\n        device_index, _ = min(enumerate(\n            self.ps_sizes), key=operator.itemgetter(1))\n        device_name = self.ps_devices[device_index]\n        var_size = op.outputs[0].get_shape().num_elements()\n        if var_size is None:\n            logger.warn(""[LeastLoadedDeviceSetter] Shape of variable {} is not fully defined!"".format(op.name))\n            var_size = 0\n\n        self.ps_sizes[device_index] += var_size\n\n        return canonicalize(device_name)\n\n    def __str__(self):\n        return ""LeastLoadedDeviceSetter-{}"".format(self.worker_device)\n\n\ndef split_grad_list(grad_list):\n    """"""\n    Args:\n        grad_list: K x N x 2\n\n    Returns:\n        K x N: gradients\n        K x N: variables\n    """"""\n    g = []\n    v = []\n    for tower in grad_list:\n        g.append([x[0] for x in tower])\n        v.append([x[1] for x in tower])\n    return g, v\n\n\ndef merge_grad_list(all_grads, all_vars):\n    """"""\n    Args:\n        all_grads (K x N): gradients\n        all_vars(K x N): variables\n\n    Return:\n        K x N x 2: list of list of (grad, var) pairs\n    """"""\n    return [list(zip(gs, vs)) for gs, vs in zip(all_grads, all_vars)]\n\n\n@under_name_scope(\'AllReduceGrads\')\ndef allreduce_grads(all_grads, average):\n    """"""\n    All-reduce average the gradients among K devices. Results are broadcasted to all devices.\n\n    Args:\n        all_grads (K x N): List of list of gradients. N is the number of variables.\n        average (bool): average gradients or not.\n\n    Returns:\n        K x N: same as input, but each grad is replaced by the average over K devices.\n    """"""\n\n    if get_tf_version_tuple() <= (1, 12):\n        from tensorflow.contrib import nccl  # deprecated\n    else:\n        from tensorflow.python.ops import nccl_ops as nccl\n    nr_tower = len(all_grads)\n    if nr_tower == 1:\n        return all_grads\n    new_all_grads = []  # N x K\n    for grads in zip(*all_grads):\n        summed = nccl.all_sum(grads)\n\n        grads_for_devices = []  # K\n        for g in summed:\n            with tf.device(g.device):\n                # tensorflow/benchmarks didn\'t average gradients\n                if average:\n                    g = tf.multiply(g, 1.0 / nr_tower)\n            grads_for_devices.append(g)\n        new_all_grads.append(grads_for_devices)\n\n    # transpose to K x N\n    ret = list(zip(*new_all_grads))\n    return ret\n\n\n@under_name_scope(\'AllReduceGradsHierachical\')\ndef allreduce_grads_hierarchical(all_grads, devices, average=False):\n    """"""\n    Hierarchical allreduce for DGX-1 system.\n\n    Args:\n        all_grads (K x N): List of list of gradients. N is the number of variables.\n        devices ([str]): K str for the K devices.\n        average (bool): average gradients or not.\n\n    Returns:\n        (K x N): same as input, but each grad is replaced by the average over K lists.\n    """"""\n    num_gpu = len(devices)\n    assert num_gpu == 8, num_gpu\n    assert len(all_grads) == num_gpu, len(all_grads)\n    group_size = num_gpu // 2\n\n    agg_all_grads = []  # N x K\n    for varid, grads in enumerate(zip(*all_grads)):\n        # grads: K gradients\n        g0_main_gpu = varid % num_gpu\n        g1_main_gpu = (g0_main_gpu + group_size) % num_gpu\n        g0_start = 0 if g0_main_gpu < group_size else group_size\n        g1_start = 0 if g1_main_gpu < group_size else group_size\n        assert g0_start != g1_start\n        g0_grads = grads[g0_start: g0_start + group_size]\n        g1_grads = grads[g1_start: g1_start + group_size]\n\n        with tf.device(devices[g0_main_gpu]):\n            g0_agg = tf.add_n(g0_grads, name=\'group0_agg\')\n\n        with tf.device(devices[g1_main_gpu]):\n            g1_agg = tf.add_n(g1_grads, name=\'group1_agg\')\n            g1_total_agg = tf.add(g0_agg, g1_agg, name=\'group1_total_agg\')\n\n        with tf.device(devices[g0_main_gpu]):\n            g0_total_agg = tf.identity(g1_total_agg, name=\'group0_total_agg\')\n\n        agg_grads = []  # K aggregated grads\n        for k in range(num_gpu):\n            if (k < group_size) == (g0_main_gpu < group_size):\n                main_gpu = g0_total_agg\n            else:\n                main_gpu = g1_total_agg\n            with tf.device(devices[k]):\n                if not average:\n                    device_total_agg = tf.identity(\n                        main_gpu, name=\'device{}_total_agg\'.format(k))\n                else:\n                    # TODO where to put average?\n                    device_total_agg = tf.multiply(\n                        main_gpu, 1.0 / num_gpu, name=\'device{}_total_agg\'.format(k))\n                agg_grads.append(device_total_agg)\n\n        agg_all_grads.append(agg_grads)\n\n    # transpose\n    agg_all_grads = list(zip(*agg_all_grads))   # K x Nvar\n    return agg_all_grads\n\n\n@under_name_scope(\'AggregateGrads\')\ndef aggregate_grads(all_grads,\n                    colocation=False,\n                    devices=None,\n                    average=True):\n    """"""\n    Average the gradients.\n\n    Args:\n        all_grads (K x N x 2): A list of K lists. Each of the list is a list of N (grad, var) tuples.\n            The variables have to be the same across the K lists.\n        colocation (bool): colocate gradient averaging on the device of the variable.\n        devices (list[str]): assign the averaging to these device in\n            round-robin. Cannot be used together with ``colocation``.\n        average (bool): do average or sum\n\n    Returns:\n        (N x 2): A list of N (grad, var) tuples, where grad is averaged or summed over K.\n    """"""\n    assert not (devices is not None and colocation)\n    if devices is not None:\n        assert isinstance(devices, list), devices\n\n    nr_tower = len(all_grads)\n    if nr_tower == 1:\n        return all_grads[0]\n\n    def aggregate(grads):\n        if average:\n            return tf.multiply(tf.add_n(grads), 1.0 / nr_tower)\n        else:\n            return tf.add_n(grads)\n\n    ret = []\n    for idx, grad_and_vars in enumerate(zip(*all_grads)):\n        # Ngpu * 2\n        v = grad_and_vars[0][1]\n        grads = [g for (g, _) in grad_and_vars]\n\n        if colocation:\n            with tf.device(v.device):       # colocate summed grad with var\n                grad = aggregate(grads)\n        elif devices is None:\n            grad = aggregate(grads)\n        else:\n            dev = devices[idx % len(devices)]\n            with tf.device(dev):\n                grad = aggregate(grads)\n        ret.append((grad, v))\n    return ret\n\n\naverage_grads = aggregate_grads\n\n\n# https://github.com/tensorflow/benchmarks/blob/48cbef14a592e02a14beee8e9aef3ad22cadaed1/scripts/tf_cnn_benchmarks/variable_mgr_util.py#L140-L166\nclass OverrideCachingDevice(object):\n    """"""Variable getter which caches variables on the least loaded device.\n\n    Variables smaller than a certain threshold are cached on a single specific\n    device, as specified in the constructor. All other variables are load balanced\n    across a pool of devices, by caching each variable on the least loaded device.\n    """"""\n\n    def __init__(self, devices, device_for_small_variables,\n                 small_variable_size_threshold):\n        self.devices = devices\n        self.sizes = [0] * len(self.devices)\n        self.device_for_small_variables = device_for_small_variables\n        self.small_variable_size_threshold = small_variable_size_threshold\n\n    def __call__(self, getter, *args, **kwargs):\n        size = tf.TensorShape(kwargs[\'shape\']).num_elements()\n        if size is None or not kwargs.get(\'trainable\', True):\n            # TODO a lot of vars won\'t be saved then\n            _replace_global_by_local(kwargs)\n            return getter(*args, **kwargs)\n\n        if size < self.small_variable_size_threshold:\n            device_name = self.device_for_small_variables\n        else:\n            device_index, _ = min(enumerate(self.sizes), key=operator.itemgetter(1))\n            device_name = self.devices[device_index]\n            self.sizes[device_index] += size\n\n        kwargs[\'caching_device\'] = device_name\n        var = getter(*args, **kwargs)\n        return var\n\n\nclass GradientPacker(object):\n    """"""\n    Concat gradients together to optimize transfer.\n    """"""\n\n    def __init__(self, num_split=8):\n        self._num_split = num_split\n\n    @call_only_once\n    def compute_strategy(self, grads):\n        """"""\n        Returns:\n            bool - False if grads cannot be packed due to various reasons.\n        """"""\n        for g in grads:\n            assert g.shape.is_fully_defined(), ""Shape of {} is {}!"".format(g.name, g.shape)\n\n        self._shapes = [g.shape for g in grads]\n        self._sizes = [g.shape.num_elements() for g in grads]\n        self._total_size = sum(self._sizes)\n        if self._total_size / self._num_split < 1024:\n            logger.info(""Skip GradientPacker due to too few gradients."")\n            return False\n        # should have the same dtype\n        dtypes = {g.dtype for g in grads}\n        if len(dtypes) != 1:\n            logger.info(""Skip GradientPacker due to inconsistent gradient types."")\n            return False\n        self._grad_dtype = grads[0].dtype\n\n        split_size = self._total_size // self._num_split\n        split_size_last = self._total_size - split_size * (self._num_split - 1)\n        self._split_sizes = [split_size] * (self._num_split - 1) + [split_size_last]\n        logger.info(\n            ""Will pack {} gradients of total dimension={} into {} splits."".format(\n                len(self._sizes), self._total_size, self._num_split))\n        return True\n\n    def pack(self, grads):\n        """"""\n        Args:\n            grads (list): list of gradient tensors\n\n        Returns:\n            packed list of gradient tensors to be aggregated.\n        """"""\n        for i, g in enumerate(grads):\n            assert g.shape == self._shapes[i]\n\n        with cached_name_scope(""GradientPacker"", top_level=False):\n            concat_grads = tf.concat([tf.reshape(g, [-1]) for g in grads], 0, name=\'concatenated_grads\')\n            # concat_grads = tf.cast(concat_grads, tf.float16)\n            grad_packs = tf.split(concat_grads, self._split_sizes)\n            return grad_packs\n\n    def unpack(self, grad_packs):\n        with cached_name_scope(""GradientPacker"", top_level=False):\n            concat_grads = tf.concat(grad_packs, 0, name=\'concatenated_packs\')\n            # concat_grads = tf.cast(concat_grads, self._grad_dtype)\n            flattened_grads = tf.split(concat_grads, self._sizes)\n            grads = [tf.reshape(g, shape) for g, shape in zip(flattened_grads, self._shapes)]\n            return grads\n\n    def pack_all(self, all_grads, devices):\n        """"""\n        Args:\n            all_grads: K x N, K lists of gradients to be packed\n        """"""\n        ret = []    # #GPU x #split\n        for dev, grads in zip(devices, all_grads):\n            with tf.device(dev):\n                ret.append(self.pack(grads))\n        return ret\n\n    def unpack_all(self, all_packed, devices):\n        """"""\n        Args:\n            all_packed: K lists of packed gradients.\n        """"""\n        all_grads = []  # #GPU x #Var\n        for dev, packed_grads_single_device in zip(devices, all_packed):\n            with tf.device(dev):\n                all_grads.append(self.unpack(packed_grads_single_device))\n        return all_grads\n'"
tensorpack/input_source/__init__.py,0,"b""#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()['kcah_acitats'[::-1].upper()] = False\nif STATICA_HACK:\n    from .input_source_base import *\n    from .input_source import *\n\nfrom pkgutil import iter_modules\nimport os\nimport os.path\n\n__all__ = []\n\n\ndef global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if '__all__' in dir(p) else []\n    del globals()[name]\n    for k in lst:\n        if not k.startswith('__'):\n            globals()[k] = p.__dict__[k]\n            __all__.append(k)\n\n\n_CURR_DIR = os.path.dirname(__file__)\n_SKIP = []\nfor _, module_name, _ in iter_modules(\n        [_CURR_DIR]):\n    srcpath = os.path.join(_CURR_DIR, module_name + '.py')\n    if not os.path.isfile(srcpath):\n        continue\n    if module_name.startswith('_'):\n        continue\n    if module_name not in _SKIP:\n        global_import(module_name)\n"""
tensorpack/input_source/input_source.py,30,"b'# -*- coding: utf-8 -*-\n# File: input_source.py\n\n\nimport threading\nfrom contextlib import contextmanager\nfrom itertools import chain\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..callbacks.base import Callback, CallbackFactory\nfrom ..callbacks.graph import RunOp\nfrom ..dataflow import DataFlow, MapData, RepeatedData, DataFlowTerminated\nfrom ..tfutils.common import get_op_tensor_name\nfrom ..tfutils.dependency import dependency_of_fetches\nfrom ..tfutils.summary import add_moving_summary\nfrom ..tfutils.tower import get_current_tower_context\nfrom ..utils import logger\nfrom ..utils.concurrency import ShareSessionThread\nfrom .input_source_base import InputSource, build_or_reuse_placeholder\n\ntry:\n    from tensorflow.python.ops.data_flow_ops import StagingArea\nexcept ImportError:\n    pass\n\n\n__all__ = [\'PlaceholderInput\', \'FeedInput\', \'FeedfreeInput\',\n           \'QueueInput\', \'BatchQueueInput\',\n           \'DummyConstantInput\', \'TensorInput\',\n           \'ZMQInput\', \'TFDatasetInput\',\n           \'StagingInput\']\n\n\ndef _get_reset_callback(df):\n    return CallbackFactory(setup_graph=lambda _: df.reset_state())\n\n\ndef _make_feeds(placeholders, datapoint):\n    assert len(datapoint) == len(placeholders), \\\n        ""Size of datapoint and placeholders are different: {} != {}"".format(\n            len(datapoint), len(placeholders))\n\n    if isinstance(datapoint, (list, tuple)):\n        return dict(zip(placeholders, datapoint))\n    elif isinstance(datapoint, dict):\n        ret = {p: datapoint[p.op.name] for p in placeholders}\n        return ret\n    else:\n        raise TypeError(""Got a datapoint of type {}!"".format(type(datapoint)))\n\n\nclass PlaceholderInput(InputSource):\n    """"""\n    Just produce placeholders as input tensors.\n    """"""\n    def __init__(self):\n        pass\n\n    def _setup(self, inputs):\n        self._all_placehdrs = [build_or_reuse_placeholder(v) for v in inputs]\n\n    def _get_input_tensors(self):\n        return self._all_placehdrs\n\n\nclass FeedInput(InputSource):\n    """"""\n    Input by iterating over a DataFlow and feed datapoints.\n\n    Note:\n        If `get_input_tensors()` is called more than one time, it will return the same placeholders (i.e. feed points)\n        as the first time.\n        Therefore you can\'t use it for data-parallel training.\n    """"""\n\n    class _FeedCallback(Callback):\n        def __init__(self, ds, placeholders):\n            self._ds = ds\n            self._itr = self._ds.__iter__()\n            self._placeholders = placeholders\n\n        def _before_run(self, _):\n            dp = next(self._itr)\n            assert len(dp) == len(self._placeholders), ""[FeedInput] datapoints and inputs are of different length!""\n            feed = _make_feeds(self._placeholders, dp)\n            return tfv1.train.SessionRunArgs(fetches=[], feed_dict=feed)\n\n        def _reset(self):\n            self._itr = self._ds.__iter__()\n\n    def __init__(self, ds, infinite=True):\n        """"""\n        Args:\n            ds (DataFlow): the input DataFlow.\n            infinite (bool): When set to False, will raise StopIteration when\n                ds is exhausted.\n        """"""\n        if not isinstance(ds, DataFlow):\n            raise ValueError(""FeedInput takes a DataFlow! Got {}"".format(ds))\n        self.ds = ds\n        if infinite:\n            self._iter_ds = RepeatedData(self.ds, -1)\n        else:\n            self._iter_ds = self.ds\n\n    def _size(self):\n        return len(self.ds)\n\n    def _setup(self, inputs):\n        # placeholders as input are always safe to reuse.\n        self._all_placehdrs = [build_or_reuse_placeholder(v) for v in inputs]\n        self._cb = self._FeedCallback(self._iter_ds, self._all_placehdrs)\n\n    def _get_input_tensors(self):\n        return self._all_placehdrs\n\n    def _reset_state(self):\n        self._cb._reset()\n\n    def _get_callbacks(self):\n        return [self._cb, _get_reset_callback(self._iter_ds)]\n\n\nclass FeedfreeInput(InputSource):\n    """""" Abstract base for input without feed,\n    e.g. by queue or other operations. """"""\n\n    def _reset_state(self):\n        pass\n\n\n# TODO enqueue_many? https://github.com/tensorflow/tensorflow/issues/7817#issuecomment-282053155\nclass EnqueueThread(ShareSessionThread):\n    def __init__(self, queue, ds, placehdrs):\n        super(EnqueueThread, self).__init__()\n        self.name = \'EnqueueThread: enqueue dataflow to TF queue ""{}""\'.format(queue.name)\n        self.daemon = True\n        self.dataflow = ds\n        self.queue = queue\n        self.placehdrs = placehdrs\n\n        self.op = self.queue.enqueue(self.placehdrs)\n        self.close_op = self.queue.close(cancel_pending_enqueues=True)\n\n        self._running = threading.Event()\n        self._running.set()\n        # self._size = queue.size()\n\n    def run(self):\n        with self.default_sess():\n            try:\n                self.reinitialize_dataflow()\n                while True:\n                    # pausable loop\n                    if not self._running.is_set():\n                        self._running.wait()\n\n                    dp = next(self._itr)\n                    feed = _make_feeds(self.placehdrs, dp)\n                    # _, sz = sess.run([self.op, self._sz], feed_dict=feed)\n                    self.op.run(feed_dict=feed)\n            except (tf.errors.CancelledError, tf.errors.OutOfRangeError):\n                pass\n            except DataFlowTerminated:\n                logger.info(""[EnqueueThread] DataFlow has terminated."")\n            except Exception as e:\n                if isinstance(e, RuntimeError) and \'closed Session\' in str(e):\n                    pass\n                else:\n                    logger.exception(""[EnqueueThread] Exception in thread {}:"".format(self.name))\n            finally:\n                try:\n                    self.close_op.run()\n                except Exception:\n                    pass\n                logger.info(""[EnqueueThread] Thread {} Exited."".format(self.name))\n\n    def reinitialize_dataflow(self):\n        self._itr = self.dataflow.__iter__()\n\n    def pause(self):\n        self._running.clear()\n\n    def resume(self):\n        self._running.set()\n\n\nclass QueueInput(FeedfreeInput):\n    """""" Enqueue datapoints from a DataFlow to a TF queue.\n        And the model receives dequeued tensors.\n    """"""\n\n    def __init__(self, ds, queue=None):\n        """"""\n        Args:\n            ds(DataFlow): the input DataFlow.\n            queue (tf.QueueBase): A :class:`tf.QueueBase` whose type\n                should match the corresponding input signature of the model.\n                Defaults to a FIFO queue of size 50.\n        """"""\n        if not isinstance(ds, DataFlow):\n            raise ValueError(""QueueInput takes a DataFlow! Got {}"".format(ds))\n        self.queue = queue\n        self.ds = ds\n        self._inf_ds = RepeatedData(ds, -1)\n        self._started = False\n\n    def _size(self):\n        return len(self.ds)\n\n    def _setup(self, inputs):\n        self._input_placehdrs = [build_or_reuse_placeholder(v) for v in inputs]\n        assert len(self._input_placehdrs) > 0, \\\n            ""QueueInput has to be used with some inputs!""\n        with self.cached_name_scope():\n            if self.queue is None:\n                self.queue = tfv1.FIFOQueue(\n                    50, [x.dtype for x in self._input_placehdrs],\n                    name=\'input_queue\')\n            logger.info(""Setting up the queue \'{}\' for CPU prefetching ..."".format(self.queue.name))\n            self.thread = EnqueueThread(self.queue, self._inf_ds, self._input_placehdrs)\n\n            self._dequeue_op = self.queue.dequeue(name=\'dequeue_for_reset\')\n\n    def refill_queue(self):\n        """"""\n        Clear the queue, then call dataflow.__iter__() again and fill into the queue.\n        """"""\n        self.thread.pause()     # pause enqueue\n\n        opt = tfv1.RunOptions()\n        opt.timeout_in_ms = 2000   # 2s\n        sess = tfv1.get_default_session()\n        # dequeue until empty\n        try:\n            while True:\n                sess.run(self._dequeue_op, options=opt)\n        except tf.errors.DeadlineExceededError:\n            pass\n\n        # reset dataflow, start thread\n        self.thread.reinitialize_dataflow()\n        self.thread.resume()\n\n    def _create_ema_callback(self):\n        """"""\n        Create a hook-only callback which maintain EMA of the queue size.\n        Also tf.summary.scalar the EMA.\n        """"""\n        with self.cached_name_scope():\n            # in TF there is no API to get queue capacity, so we can only summary the size\n            size = tf.cast(self.queue.size(), tf.float32, name=\'queue_size\')\n        size_ema_op = add_moving_summary(size, collection=None, decay=0.5)[0].op\n        ret = RunOp(\n            lambda: size_ema_op,\n            run_before=False,\n            run_as_trigger=False,\n            run_step=True)\n        ret.name_scope = ""InputSource/EMA""\n        return ret\n\n    def _get_callbacks(self):\n        from ..callbacks.concurrency import StartProcOrThread\n        cb = StartProcOrThread(self.thread)\n        return [cb, self._create_ema_callback(), _get_reset_callback(self._inf_ds)]\n\n    def _get_input_tensors(self):\n        with tf.device(\'/cpu:0\'), self.cached_name_scope():\n            ret = self.queue.dequeue(name=\'input_deque\')\n            if isinstance(ret, tf.Tensor):  # only one input\n                ret = [ret]\n            assert len(ret) == len(self._input_placehdrs)\n            for qv, v in zip(ret, self._input_placehdrs):\n                qv.set_shape(v.get_shape())\n            return ret\n\n\nclass BatchQueueInput(QueueInput):\n    """""" Enqueue datapoints from a DataFlow to a TF queue.\n        And the model receives batches formed by concatenating\n        dequeued tensors.\n    """"""\n    def __init__(self, ds, batch_size, queue=None):\n        """"""\n        Args:\n            ds(DataFlow): the input DataFlow.\n            batch_size(int): the batch size.\n            queue (tf.QueueBase): A :class:`tf.QueueBase` whose type\n                should match the corresponding input signature of the model.\n                Defaults to a FIFO queue of size 3000.\n        """"""\n        super(BatchQueueInput, self).__init__(ds, queue)\n        self.batch_size = int(batch_size)\n\n    def _size(self):\n        return len(self.ds) // self.batch_size\n\n    def _setup(self, inputs):\n        logger.info(""Setting up the queue for CPU prefetching ..."")\n        self.input_placehdrs = [build_or_reuse_placeholder(v) for v in inputs]\n        assert len(self.input_placehdrs) > 0, \\\n            ""BatchQueueInput has to be used with some input signature!""\n\n        # prepare placeholders without the first dimension\n        placehdrs_nobatch = []\n        for p in self.input_placehdrs:\n            placehdrs_nobatch.append(tfv1.placeholder(\n                dtype=p.dtype, shape=p.get_shape().as_list()[1:],\n                name=get_op_tensor_name(p.name)[0] + \'-nobatch\'))\n\n        # dequeue_many requires fully-defined shapes\n        shape_err = ""Use of BatchQueueInput requires inputs to have fully-defined ""\n        ""shapes except for the batch dimension""\n        shapes = []\n        for p in placehdrs_nobatch:\n            assert p.get_shape().is_fully_defined(), shape_err\n            shapes.append(p.get_shape())\n\n        with self.cached_name_scope():\n            if self.queue is None:\n                self.queue = tf.FIFOQueue(\n                    3000, [x.dtype for x in self.input_placehdrs],\n                    shapes=shapes,\n                    name=\'input_queue\')\n            for shp in self.queue.shapes:\n                assert shp.is_fully_defined(), shape_err\n\n            self.thread = EnqueueThread(self.queue, self._inf_ds, placehdrs_nobatch)\n\n    def _get_input_tensors(self):\n        with tf.device(\'/cpu:0\'), self.cached_name_scope():\n            ret = self.queue.dequeue_many(self.batch_size, name=\'input_deque\')\n            if isinstance(ret, tf.Tensor):  # only one input\n                ret = [ret]\n            assert len(ret) == len(self.input_placehdrs)\n            for qv, v in zip(ret, self.input_placehdrs):\n                shp = v.get_shape().as_list()\n                shp[0] = self.batch_size\n                qv.set_shape(shp)\n            return ret\n\n\n# TODO tensor inputs can be drained? look at the new dataset API.\nclass TensorInput(FeedfreeInput):\n    """""" Use inputs from a list of tensors, e.g. a TF data reading pipeline.\n        The PTB training example shows how to use it.\n    """"""\n\n    def __init__(self, get_tensor_fn, size=None):\n        """"""\n        Args:\n            get_tensor_fn ( -> [tf.Tensor]): a function which returns a list of input tensors\n                (for example, [image, label]) when called.\n                It will be called under a TowerContext and should return the inputs to be used in that tower.\n                The returned tensors will be evaluated every iteration, it\'s your job to make sure it\'s possible.\n            size(int): size of this input. Use None to leave it undefined.\n        """"""\n        if not callable(get_tensor_fn):\n            raise ValueError(""get_tensor_fn has to be a function! Got {}"".format(get_tensor_fn))\n        self.get_tensor_fn = get_tensor_fn\n        if size is not None:\n            size = int(size)\n            assert size > 0\n        self._fixed_size = size\n\n    def _setup(self, input_signature):\n        self._spec = input_signature\n\n    def _size(self):\n        if self._fixed_size is None:\n            raise NotImplementedError(""size of TensorInput is undefined!"")\n        return self._fixed_size\n\n    def _get_input_tensors(self):\n        with self.cached_name_scope():\n            ret = self.get_tensor_fn()\n        assert isinstance(ret, (list, tuple)), ""get_tensor_fn needs to return a list!""\n        assert len(ret) == len(self._spec), \\\n            ""get_tensor_fn returns {} tensors but there are {} inputs"".format(len(ret), len(self._spec))\n        return ret\n\n\nclass DummyConstantInput(TensorInput):\n    """""" Input with a constant zero tensor placed on GPU.\n        Useful for debugging performance issues """"""\n    def __init__(self, shapes):\n        """"""\n        Args:\n            shapes (list[list]): a list of fully-specified shapes.\n        """"""\n        self.shapes = shapes\n        logger.warn(""Using dummy input for debug!"")\n\n        def fn():\n            tlist = []\n            ctx = get_current_tower_context()\n            assert ctx is not None\n            assert len(self.shapes) == len(self._spec)\n            for idx, p in enumerate(self._spec):\n                tlist.append(tf.constant(\n                    0, dtype=p.dtype,\n                    name=\'dummy-{}-{}\'.format(p.name, ctx.index),\n                    shape=self.shapes[idx]))\n            return tlist\n        super(DummyConstantInput, self).__init__(fn)\n\n\nclass ZMQInput(TensorInput):\n    """"""\n    Receive tensors from a ZMQ endpoint, with ops from https://github.com/tensorpack/zmq_ops.\n    It works with :func:`dataflow.remote.send_dataflow_zmq(format=\'zmq_ops\')`.\n    """"""\n    def __init__(self, end_point, hwm, bind=True):\n        """"""\n        Args:\n            end_point (str): the ZMQ endpoint\n            hwm (int): the ZMQ high-water-mark\n        """"""\n        self._end_point = end_point\n        self._hwm = int(hwm)\n        self._bind = bind\n\n        def fn():\n            ret = self._zmq_pull_socket.pull()\n            assert len(ret) == len(self._spec)\n            for qv, v in zip(ret, self._spec):\n                qv.set_shape(v.shape)\n            return ret\n        super(ZMQInput, self).__init__(fn)\n\n    def _setup(self, input_signature):\n        super(ZMQInput, self)._setup(input_signature)\n        assert len(input_signature) > 0, \\\n            ""ZMQInput has to be used with input signature!""\n\n        import zmq_ops\n        self._zmq_pull_socket = zmq_ops.ZMQPullSocket(\n            self._end_point,\n            [x.dtype for x in input_signature],\n            hwm=self._hwm,\n            bind=self._bind)\n\n    def to_dataset(self, input_signature):\n        """"""\n        Convert to a TF dataset.\n\n        Args:\n            input_signature (list[InputSpec]):\n\n        Returns:\n            tf.data.Dataset\n        """"""\n        import zmq_ops\n        zmq_pull_socket = zmq_ops.ZMQPullSocket(\n            self._end_point, [x.dtype for x in input_signature],\n            hwm=self._hwm, bind=self._bind)\n\n        def mapper(_):\n            inputs = list(zmq_pull_socket.pull())\n            for v, sig in zip(inputs, input_signature):\n                v.set_shape(sig.shape)\n            return inputs\n\n        # Is there a better way to construct from stateful tensor?\n        dataset = tf.data.Dataset.from_tensors([1])  # just a placeholder\n        return dataset.map(mapper)\n\n\nclass TFDatasetInput(FeedfreeInput):\n    """"""\n    Use a :class:`tf.data.Dataset` instance as input.\n\n    Note:\n        1. In training, the given dataset or dataflow has to be infinite\n            (you can use :func:`repeat()`, or :class:`RepeatedData` ).\n\n        2. TensorFlow may keep the dataflow alive even if the dataset is no\n           longer used.\n    """"""\n    def __init__(self, dataset):\n        """"""\n        Args:\n            dataset (tf.data.Dataset or DataFlow):\n        """"""\n        if isinstance(dataset, tf.data.Dataset):\n            self._dataset = dataset\n            self._dataflow = None\n        elif isinstance(dataset, DataFlow):\n            self._dataset = None\n            self._dataflow = dataset\n        else:\n            raise ValueError(""TFDatasetInput takes a tf.data.Dataset or DataFlow! Got {}"".format(dataset))\n\n    def _setup(self, input_signature):\n        self._spec = input_signature\n        if self._dataset is not None:\n            types = self._dataset.output_types\n            if len(types) == 1:\n                types = (types,)\n            spec_types = tuple(k.dtype for k in input_signature)\n            assert len(types) == len(spec_types), \\\n                ""Dataset and input signature have different length! {} != {}"".format(\n                    len(types), len(spec_types))\n            assert types == spec_types, \\\n                ""Data types of dataset and input signature don\'t match! {} != {}"".format(\n                    str(types), str(spec_types))\n\n            shapes = self._dataset.output_shapes\n            spec_shapes = [k.shape for k in input_signature]\n            for idx, (s1, s2) in enumerate(zip(shapes, spec_shapes)):\n                s2 = tf.TensorShape(s2)\n                assert s2.is_compatible_with(s1), \\\n                    ""Input signature \'{}\' has incompatible shape with dataset! {} vs {}"".format(\n                        input_signature[idx].name, s2, s1)\n        else:\n            self._dataset = TFDatasetInput.dataflow_to_dataset(self._dataflow, [x.dtype for x in input_signature])\n\n        self._iterator = self._dataset.make_initializable_iterator()\n        self._init_op = self._iterator.initializer\n\n    def _reset_state(self):\n        self._init_op.run()\n\n    def _get_input_tensors(self):\n        spec_shapes = [k.shape for k in self._spec]\n        ret = self._iterator.get_next()\n        assert len(ret) == len(spec_shapes), \\\n            ""Dataset returns {} tensors but there are {} inputs!"".format(len(ret), len(spec_shapes))\n        for t, shp in zip(ret, spec_shapes):\n            t.set_shape(shp)\n        return ret\n\n    @staticmethod\n    def dataflow_to_dataset(df, types):\n        """"""\n        Wrap a dataflow to tf.data.Dataset.\n        This function will also reset the dataflow.\n\n        If the dataflow itself is finite, the returned dataset is also finite.\n        Therefore, if used for training, you\'ll need to add `.repeat()` on the returned\n        dataset.\n\n        Args:\n            df (DataFlow): a dataflow which produces lists\n            types([tf.DType]): list of types\n\n        Returns:\n            (tf.data.Dataset)\n\n        Note:\n            TensorFlow may keep the dataflow alive even if the dataset is no\n            longer used.\n        """"""\n        # TODO theoretically it can support dict\n        assert isinstance(df, DataFlow), df\n        assert isinstance(types, (list, tuple)), types\n        df = MapData(df, tuple)\n        df.reset_state()\n        ds = tf.data.Dataset.from_generator(\n            df.get_data, tuple(types))\n        return ds\n\n\nclass StagingInput(FeedfreeInput):\n    """"""\n    A wrapper around a feedfree input,\n    to prefetch the input in StagingArea (on GPUs).\n\n    It works by registering hooks to put & get tensors into the StagingArea.\n    If `get_input_tensors` gets called multiple times,\n    it requires that all outputs ever produced by this InputSource will be fetched together.\n\n    This means that in multi-GPU training, you should ensure that each call on `hooked_sess.run`\n    depends on either all input tensors on all GPUs, or no input tensors at all.\n    As a result you cannot use this InputSource for :class:`InferenceRunner`.\n\n    More than one StagingInput cannot be used together.\n    """"""\n    class StagingCallback(Callback):\n        """"""\n        A callback registered by this input source, to make sure stage/unstage\n        is run at each step.\n        """"""\n        def __init__(self, input, nr_stage):\n            self.nr_stage = nr_stage\n            self._input = input\n            self._initialized = False\n\n        def _setup_graph(self):\n            self.stage_op = self._input._get_stage_op()\n            unstage_ops = self._input._get_unstage_ops()\n            unstage_op = tf.group(*unstage_ops, name=\'unstage_all\')\n            self._check_dependency_op = unstage_ops[0]\n            self.fetches = tfv1.train.SessionRunArgs(\n                fetches=[self.stage_op, unstage_op])\n\n        def _prefill(self, sess):\n            logger.info(""Pre-filling StagingArea ..."")\n            for _ in range(self.nr_stage):\n                self.stage_op.run(session=sess)\n            logger.info(""{} element{} put into StagingArea on each tower."".format(\n                self.nr_stage, ""s were"" if self.nr_stage > 1 else "" was""))\n\n        def _before_run(self, ctx):\n            # This has to happen once, right before the first iteration.\n            # doing it in `before_train` may not work because QueueInput happens in before_train.\n            if not self._initialized:\n                self._initialized = True\n                self._prefill(ctx.session)\n            # Only step the stagingarea when the input is evaluated in this sess.run\n            fetches = ctx.original_args.fetches\n            if dependency_of_fetches(fetches, self._check_dependency_op):\n                # note: this disable nesting of StagingInput\n                return self.fetches\n\n    def __init__(self, input, nr_stage=1, device=None):\n        """"""\n        Args:\n            input (FeedfreeInput):\n            nr_stage (int): number of elements to prefetch into each StagingArea, at the beginning.\n                Since enqueue and dequeue are synchronized, prefetching 1 element should be sufficient.\n            device (str or None): if not None, place the StagingArea on a specific device. e.g., \'/cpu:0\'.\n                Otherwise, they are placed under where `get_inputs_tensors`\n                gets called, which could be unspecified in case of simple trainers.\n        """"""\n        if not isinstance(input, FeedfreeInput):\n            raise ValueError(""StagingInput takes a FeedfreeInput! Got {}"".format(input))\n        if isinstance(input, StagingInput):\n            raise ValueError(""StagingInput cannot be nested!"")\n\n        self._input = input\n\n        self._nr_stage = nr_stage\n        self._areas = []\n        self._stage_ops = []\n        self._unstage_ops = []\n        self._device = device\n\n    def _setup(self, inputs):\n        self._input.setup(inputs)\n        with self.cached_name_scope():\n            pass    # just to cache the correct ns to use\n\n    def _get_callbacks(self):\n        cbs = self._input.get_callbacks()\n\n        # this callback has to happen after others, so StagingInput can be stacked together\n        cbs.append(\n            StagingInput.StagingCallback(self, self._nr_stage))\n        return cbs\n\n    def _size(self):\n        return self._input.size()\n\n    @contextmanager\n    def _device_ctx(self):\n        if not self._device:\n            yield\n        else:\n            with tf.device(self._device):\n                yield\n\n    def _get_input_tensors(self):\n        inputs = self._input.get_input_tensors()\n\n        with self._device_ctx():\n            with self.cached_name_scope():\n                # Putting variables to stagingarea will cause trouble\n                dtypes = []\n                for idx in range(len(inputs)):\n                    dtype = inputs[idx].dtype\n                    if dtype.base_dtype != dtype:     # is reference type\n                        inputs[idx] = tf.identity(inputs[idx])\n                    dtypes.append(dtype.base_dtype)\n\n                # TODO tensorflow/benchmarks use static shapes here,\n                # though it doesn\'t seem to help. We can use it when it\'s known.\n                # Setting capacity to 1 to potentially save some memory, because we should\n                # expect the consumers to run slower than the producer.\n                stage = StagingArea(dtypes, shapes=None, capacity=1)\n\n            # put & get automatically inherit the name scope from the area\n            self._stage_ops.append(stage.put(inputs))\n            self._areas.append(stage)\n            outputs = stage.get()\n            if isinstance(outputs, tf.Tensor):  # when size=1, TF doesn\'t return a list\n                outputs = [outputs]\n\n            for vin, vout in zip(inputs, outputs):\n                vout.set_shape(vin.get_shape())\n            self._unstage_ops.append(outputs)\n            # self._size_ops.append(stage.size())\n            return outputs\n\n    def _get_stage_op(self):\n        with self.cached_name_scope():\n            return tf.group(*self._stage_ops)\n\n    def _get_unstage_ops(self):\n        with self.cached_name_scope():\n            all_outputs = list(chain.from_iterable(self._unstage_ops))\n            return all_outputs\n\n    # for debugging only\n    def _create_ema_callback(self):\n        def create_ema_op():\n            with self.cached_name_scope():\n                avg_size = tf.truediv(tf.add_n(self._size_ops), len(self._size_ops), name=\'avg_stagingarea_size\')\n                return add_moving_summary(avg_size, collection=None)[0].op\n        return RunOp(\n            create_ema_op,\n            run_before=False,\n            run_as_trigger=False,\n            run_step=True)\n'"
tensorpack/input_source/input_source_base.py,12,"b'# -*- coding: utf-8 -*-\n# File: input_source_base.py\n\nimport copy\nfrom abc import ABCMeta, abstractmethod\nfrom contextlib import contextmanager\nimport six\nimport tensorflow as tf\n\nfrom ..callbacks.base import CallbackFactory\nfrom ..tfutils.common import get_op_tensor_name\nfrom ..utils import logger\nfrom ..utils.argtools import call_only_once, memoized_method\nfrom ..compat import tfv1\n\n__all__ = [\'InputSource\', \'remap_input_source\']\n\n\ndef build_or_reuse_placeholder(tensor_spec):\n    """"""\n    Build a tf.placeholder from the metadata in the given tensor spec, or return an existing one.\n\n    Args:\n        tensor_spec (tf.TensorSpec):\n\n    Returns:\n        tf.Tensor:\n    """"""\n    g = tfv1.get_default_graph()\n    name = tensor_spec.name\n    try:\n        tensor = g.get_tensor_by_name(name + \':0\')\n        assert ""Placeholder"" in tensor.op.type, ""Tensor {} exists but is not a placeholder!"".format(name)\n        assert tensor_spec.is_compatible_with(tensor), \\\n            ""Tensor {} exists but is not compatible with the signature!"".format(tensor)\n        if tensor.shape.as_list() == tensor_spec.shape.as_list():\n            # It might be desirable to use a placeholder of a different shape in some tower\n            # (e.g., a less specific shape)\n\n            # Comparing `tensor.shape` directly doesn\'t work, because\n            # tensorflow thinks `tf.Dimension(None)` and `tf.Dimension(None)` are not equal.\n            return tensor\n    except KeyError:\n        pass\n    with tfv1.name_scope(None):   # clear any name scope it might get called in\n        ret = tfv1.placeholder(\n            tensor_spec.dtype, shape=tensor_spec.shape, name=tensor_spec.name)\n    return ret\n\n\ndef get_tensors_inputs(placeholders, tensors, names):\n    """"""\n    Args:\n        placeholders (list[Tensor]):\n        tensors (list[Tensor]): list of tf.Tensor\n        names (list[str]): names matching the given tensors\n\n    Returns:\n        list[Tensor]: inputs to used for the tower function,\n            with the corresponding placeholders replaced by tensors.\n    """"""\n    assert len(tensors) == len(names), \\\n        ""Input tensors {} and input names {} have different length!"".format(\n            tensors, names)\n    ret = copy.copy(placeholders)\n    placeholder_names = [p.name for p in placeholders]\n    for name, tensor in zip(names, tensors):\n        tensorname = get_op_tensor_name(name)[1]\n        try:\n            idx = placeholder_names.index(tensorname)\n        except ValueError:\n            logger.error(""Name {} is not a model input!"".format(tensorname))\n            raise\n        ret[idx] = tensor\n    return ret\n\n\ndef get_sublist_by_names(lst, names):\n    """"""\n    Args:\n        lst (list): list of objects with ""name"" property.\n\n    Returns:\n        list: a sublist of objects, matching names\n    """"""\n    orig_names = [p.name for p in lst]\n    ret = []\n    for name in names:\n        try:\n            idx = orig_names.index(name)\n        except ValueError:\n            logger.error(""Name {} doesn\'t appear in lst {}!"".format(\n                name, str(orig_names)))\n            raise\n        ret.append(lst[idx])\n    return ret\n\n\n@six.add_metaclass(ABCMeta)\nclass InputSource(object):\n    """""" Base class for the abstract InputSource. """"""\n\n    _name_scope = None\n    _setup_done = False\n\n    def get_input_tensors(self):\n        """"""\n        Returns:\n            list[Tensor]: A list of tensors corresponding to the inputs of the model.\n                Will be used as input for the tower function.\n                This method should always create and return new tensors when called,\n                unless it returns placeholders.\n        """"""\n        return self._get_input_tensors()\n\n    @abstractmethod\n    def _get_input_tensors(self):\n        pass\n\n    @call_only_once\n    def setup(self, input_signature):\n        """"""\n        Args:\n            input_signature (list[tf.TensorSpec]): list of specs for each input tensor\n\n        Returns:\n            list[Callback]: extra callbacks needed by this InputSource.\n            callbacks of InputSource cannot use any `trigger*()` method.\n        """"""\n        self._setup(input_signature)\n        self._setup_done = True\n        return self.get_callbacks()\n\n    def _setup(self, input_signature):\n        pass\n\n    def setup_done(self):\n        """"""\n        Returns:\n            bool: whether :meth:`setup()` has been called.\n        """"""\n        return self._setup_done\n\n    @memoized_method\n    def get_callbacks(self):\n        """"""\n        An InputSource might need some extra maintenance during training,\n        which is done also through the Callback interface.\n        This method returns the callbacks and the return value will be memoized.\n\n        All callbacks will be automatically marked as `chief_only=False`,\n        so they will run on all nodes.\n\n        Callbacks returned by :class:`InputSource` only supports a subset of callback\'s functionalities:\n\n        1. It cannot access the trainer, because an :class:`InputSource` can be used in pure inference.\n        2. It cannot use the following methods: `trigger_{step,epoch}, {before,after}_epoch`.\n\n        In other words, these callbacks should only have the basic functionality of `tf.train.SessionRunHooks`.\n\n        Returns:\n            list[Callback]: extra callbacks needed by this InputSource.\n        """"""\n        assert self.setup_done()\n        ret = [CallbackFactory(\n            before_train=lambda _: self.reset_state())] + self._get_callbacks()\n\n        for r in ret:\n            r.set_chief_only(False)    # no input callbacks should be chief-only\n        return ret\n\n    def _get_callbacks(self):\n        return []\n\n    def reset_state(self):\n        """"""\n        Initialize/reinitialize this InputSource.\n        Must be called under a default session.\n\n        For training, it will get called once by the trainer in `before_train` callbacks.\n        For inference, the :class:`InferenceRunner` will call this method each time it is triggered.\n        """"""\n        self._reset_state()\n\n    def _reset_state(self):\n        pass\n\n    def size(self):\n        """"""\n        Returns:\n            int: epoch size of the InputSource\n        """"""\n        return self._size()\n\n    def _size(self):\n        raise NotImplementedError()\n\n    @contextmanager\n    def cached_name_scope(self):\n        """"""\n        Yield a context under a cached name scope, whose name is the name of\n        this InputSource class.\n        """"""\n        if self._name_scope:\n            with tf.name_scope(self._name_scope):\n                yield self._name_scope\n        else:\n            name = type(self).__name__\n            with tf.name_scope(name) as ns:\n                self._name_scope = ns\n                yield ns\n\n\nclass ProxyInputSource(InputSource):\n    """"""\n    An InputSource which proxy every method to ``self._input``.\n    """"""\n    def __init__(self, input):\n        assert isinstance(input, InputSource), input\n        self._input = input\n\n    def _get_input_tensors(self):\n        return self._input.get_input_tensors()\n\n    def _setup(self, input_signature):\n        self._input.setup(input_signature)\n\n    def _get_callbacks(self):\n        return self._input.get_callbacks()\n\n    def _size(self):\n        return self._input.size()\n\n    def _reset_state(self):\n        self._input.reset_state()\n\n\ndef remap_input_source(input, names):\n    """"""\n    When you have some :class:`InputSource` which doesn\'t match the inputs of\n    your tower function, use `RemapInputSource`.\n    It produces placeholders for all the inputs in your model,\n    except that the corresponding ones are replaced with the tensor produced\n    by the given :class:`InputSource`.\n\n    Example:\n\n    .. code-block:: python\n\n        input1 = QueueInput(ds)\n        # assume ds produces data that should be fed to \'image\' and \'label\',\n        # but the graph takes more inputs for some reasons, or takes inputs\n        # of a different order, for example like the following:\n\n        # input_signature = [tf.TensorSpec((None,10), tf.float32, \'score\'),\n        #                    tf.TensorSpec((None,20,20,3), tf.float32, \'label\'),\n        #                    tf.TensorSpec((None,), tf.int32, \'image\') ]\n\n        input2 = remap_input_source(input1, [\'image\', \'label\'])\n        # now, if input2 is used with the above input_signature, it will return a\n        # placeholder for \'score\', plus the tensors returned by input1\n    """"""\n    def __init__(self, input, names):\n        """"""\n        Args:\n            input(InputSource): a :class:`InputSource`, whose tensors will get mapped.\n            names(list[str]): list of input names corresponding to the tensors\n                produced by ``input``.\n\n        Returns:\n            InputSource:\n        """"""\n        ProxyInputSource.__init__(self, input)\n        assert isinstance(names, (list, tuple)), names\n        self._names = tuple(names)\n\n    def _setup(self, inputs):\n        self._all_placehdrs = [build_or_reuse_placeholder(v) for v in inputs]\n        inputs_subset = get_sublist_by_names(inputs, self._names)\n        self._input.setup(inputs_subset)\n\n    def _get_input_tensors(self):\n        ret = self._input.get_input_tensors()\n        assert len(ret) == len(self._names)\n        return get_tensors_inputs(\n            self._all_placehdrs, ret, self._names)\n\n    oldcls = type(input)\n    # inherit oldcls so that type check in various places would work\n    cls = type(\'Remapped\' + oldcls.__name__, (ProxyInputSource, oldcls), {\n        \'__init__\': __init__,\n        \'_setup\': _setup,\n        \'_get_input_tensors\': _get_input_tensors})\n    return cls(input, names)\n'"
tensorpack/models/__init__.py,0,"b'#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()[\'kcah_acitats\'[::-1].upper()] = False\nif STATICA_HACK:\n    from .batch_norm import *\n    from .common import *\n    from .conv2d import *\n    from .fc import *\n    from .layer_norm import *\n    from .linearwrap import *\n    from .nonlin import *\n    from .pool import *\n    from .regularize import *\n\n\nfrom pkgutil import iter_modules\nimport os\nimport os.path\n# this line is necessary for _TFModuleFunc to work\nimport tensorflow as tf  # noqa: F401\n\n__all__ = []\n\n\ndef _global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if \'__all__\' in dir(p) else dir(p)\n    del globals()[name]\n    for k in lst:\n        if not k.startswith(\'__\'):\n            globals()[k] = p.__dict__[k]\n            __all__.append(k)\n\n\n_CURR_DIR = os.path.dirname(__file__)\n_SKIP = [\'utils\', \'registry\', \'tflayer\']\nfor _, module_name, _ in iter_modules(\n        [_CURR_DIR]):\n    srcpath = os.path.join(_CURR_DIR, module_name + \'.py\')\n    if not os.path.isfile(srcpath):\n        continue\n    if module_name.startswith(\'_\'):\n        continue\n    if ""_test"" in module_name:\n        continue\n    if module_name not in _SKIP:\n        _global_import(module_name)\n'"
tensorpack/models/_old_batch_norm.py,23,"b'# -*- coding: utf-8 -*-\n# File: _old_batch_norm.py\n\nimport tensorflow as tf\nfrom tensorflow.contrib.framework import add_model_variable\nfrom tensorflow.python.training import moving_averages\n\nfrom ..tfutils.common import get_tf_version_tuple\nfrom ..tfutils.tower import get_current_tower_context\nfrom ..utils import logger\nfrom ..utils.argtools import get_data_format\nfrom .common import VariableHolder, layer_register\nfrom .tflayer import convert_to_tflayer_args\n\n\n""""""\nOld Custom BN Implementation, Kept Here For Future Reference\n""""""\n\n\ndef get_bn_variables(n_out, use_scale, use_bias, gamma_init):\n    if use_bias:\n        beta = tf.get_variable(\'beta\', [n_out], initializer=tf.constant_initializer())\n    else:\n        beta = tf.zeros([n_out], name=\'beta\')\n    if use_scale:\n        gamma = tf.get_variable(\'gamma\', [n_out], initializer=gamma_init)\n    else:\n        gamma = tf.ones([n_out], name=\'gamma\')\n    # x * gamma + beta\n\n    moving_mean = tf.get_variable(\'mean/EMA\', [n_out],\n                                  initializer=tf.constant_initializer(), trainable=False)\n    moving_var = tf.get_variable(\'variance/EMA\', [n_out],\n                                 initializer=tf.constant_initializer(1.0), trainable=False)\n    return beta, gamma, moving_mean, moving_var\n\n\ndef update_bn_ema(xn, batch_mean, batch_var,\n                  moving_mean, moving_var, decay, internal_update):\n    update_op1 = moving_averages.assign_moving_average(\n        moving_mean, batch_mean, decay, zero_debias=False,\n        name=\'mean_ema_op\')\n    update_op2 = moving_averages.assign_moving_average(\n        moving_var, batch_var, decay, zero_debias=False,\n        name=\'var_ema_op\')\n\n    if internal_update:\n        with tf.control_dependencies([update_op1, update_op2]):\n            return tf.identity(xn, name=\'output\')\n    else:\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_op1)\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_op2)\n        return tf.identity(xn, name=\'output\')\n\n\n@layer_register()\n@convert_to_tflayer_args(\n    args_names=[],\n    name_mapping={\n        \'use_bias\': \'center\',\n        \'use_scale\': \'scale\',\n        \'gamma_init\': \'gamma_initializer\',\n        \'decay\': \'momentum\',\n        \'use_local_stat\': \'training\'\n    })\ndef BatchNorm(inputs, training=None, momentum=0.9, epsilon=1e-5,\n              center=True, scale=True,\n              gamma_initializer=tf.ones_initializer(),\n              data_format=\'channels_last\',\n              internal_update=False):\n    """"""\n    Mostly equivalent to `tf.layers.batch_normalization`, but difference in\n    the following:\n    1. Accepts `data_format` rather than `axis`. For 2D input, this argument will be ignored.\n    2. Default value for `momentum` and `epsilon` is different.\n    3. Default value for `training` is automatically obtained from `TowerContext`.\n    4. Support the `internal_update` option.\n    Args:\n        internal_update (bool): if False, add EMA update ops to\n            `tf.GraphKeys.UPDATE_OPS`. If True, update EMA inside the layer\n            by control dependencies.\n    Variable Names:\n    * ``beta``: the bias term. Will be zero-inited by default.\n    * ``gamma``: the scale term. Will be one-inited by default. Input will be transformed by ``x * gamma + beta``.\n    * ``mean/EMA``: the moving average of mean.\n    * ``variance/EMA``: the moving average of variance.\n    Note:\n        1. About multi-GPU training: moving averages across GPUs are not aggregated.\n           Batch statistics are computed independently.  This is consistent with most frameworks.\n        2. Combinations of ``training`` and ``ctx.is_training``:\n            * ``training == ctx.is_training``: standard BN, EMA are\n                maintained during training and used during inference. This is\n                the default.\n            * ``training and not ctx.is_training``: still use batch statistics in inference.\n            * ``not training and ctx.is_training``: use EMA to normalize in\n                training. This is useful when you load a pre-trained BN and\n                don\'t want to fine tune the EMA. EMA will not be updated in\n                this case.\n    """"""\n    data_format = get_data_format(data_format, keras_mode=False)\n    shape = inputs.get_shape().as_list()\n    ndims = len(shape)\n    assert ndims in [2, 4]\n    if ndims == 2:\n        data_format = \'NHWC\'\n    if data_format == \'NCHW\':\n        n_out = shape[1]\n    else:\n        n_out = shape[-1]  # channel\n    assert n_out is not None, ""Input to BatchNorm cannot have unknown channels!""\n    beta, gamma, moving_mean, moving_var = get_bn_variables(n_out, scale, center, gamma_initializer)\n\n    ctx = get_current_tower_context()\n    use_local_stat = training\n    if use_local_stat is None:\n        use_local_stat = ctx.is_training\n    use_local_stat = bool(use_local_stat)\n\n    if use_local_stat:\n        if ndims == 2:\n            inputs = tf.reshape(inputs, [-1, 1, 1, n_out])    # fused_bn only takes 4D input\n            # fused_bn has error using NCHW? (see #190)\n\n        xn, batch_mean, batch_var = tf.nn.fused_batch_norm(\n            inputs, gamma, beta, epsilon=epsilon,\n            is_training=True, data_format=data_format)\n\n        if ndims == 2:\n            xn = tf.squeeze(xn, [1, 2])\n    else:\n        if ctx.is_training:\n            assert get_tf_version_tuple() >= (1, 4), \\\n                ""Fine tuning a BatchNorm model with fixed statistics is only "" \\\n                ""supported after https://github.com/tensorflow/tensorflow/pull/12580 ""\n            if ctx.is_main_training_tower:  # only warn in first tower\n                logger.warn(""[BatchNorm] Using moving_mean/moving_variance in training."")\n            # Using moving_mean/moving_variance in training, which means we\n            # loaded a pre-trained BN and only fine-tuning the affine part.\n            xn, _, _ = tf.nn.fused_batch_norm(\n                inputs, gamma, beta,\n                mean=moving_mean, variance=moving_var, epsilon=epsilon,\n                data_format=data_format, is_training=False)\n        else:\n            if ndims == 4:\n                xn, _, _ = tf.nn.fused_batch_norm(\n                    inputs, gamma, beta,\n                    mean=moving_mean, variance=moving_var, epsilon=epsilon,\n                    data_format=data_format, is_training=False)\n            else:\n                xn = tf.nn.batch_normalization(\n                    inputs, moving_mean, moving_var, beta, gamma, epsilon)\n\n    # maintain EMA only on one GPU is OK, even in replicated mode.\n    # because training time doesn\'t use EMA\n    if ctx.is_main_training_tower:\n        add_model_variable(moving_mean)\n        add_model_variable(moving_var)\n    if ctx.is_main_training_tower and use_local_stat:\n        ret = update_bn_ema(xn, batch_mean, batch_var, moving_mean, moving_var, momentum, internal_update)\n    else:\n        ret = tf.identity(xn, name=\'output\')\n\n    vh = ret.variables = VariableHolder(mean=moving_mean, variance=moving_var)\n    if scale:\n        vh.gamma = gamma\n    if center:\n        vh.beta = beta\n    return ret\n'"
tensorpack/models/batch_norm.py,57,"b'# Copyright (c) Tensorpack Contributors. All Rights Reserved\n# -*- coding: utf-8 -*-\n# File: batch_norm.py\n\n\nimport re\nfrom ..compat import tfv1 as tf  # this should be avoided first in model code\nfrom tensorflow.python.training import moving_averages\n\nfrom ..tfutils.collection import backup_collection, restore_collection\nfrom ..tfutils.common import get_tf_version_tuple\nfrom ..tfutils.tower import get_current_tower_context\nfrom ..utils import logger\nfrom ..utils.argtools import get_data_format, log_once\nfrom .common import VariableHolder, layer_register\nfrom .tflayer import convert_to_tflayer_args, rename_get_variable\nfrom .utils import disable_autograph\n\n__all__ = [\'BatchNorm\', \'BatchRenorm\']\n\n# decay: being too close to 1 leads to slow start-up. torch use 0.9.\n# eps: torch: 1e-5. Lasagne: 1e-4\n\n\ndef get_bn_variables(n_out, use_scale, use_bias, beta_init, gamma_init):\n    if use_bias:\n        beta = tf.get_variable(\'beta\', [n_out], initializer=beta_init)\n    else:\n        beta = tf.zeros([n_out], name=\'beta\')\n    if use_scale:\n        gamma = tf.get_variable(\'gamma\', [n_out], initializer=gamma_init)\n    else:\n        gamma = tf.ones([n_out], name=\'gamma\')\n    # x * gamma + beta\n\n    moving_mean = tf.get_variable(\'mean/EMA\', [n_out],\n                                  initializer=tf.constant_initializer(), trainable=False)\n    moving_var = tf.get_variable(\'variance/EMA\', [n_out],\n                                 initializer=tf.constant_initializer(1.0), trainable=False)\n\n    if get_current_tower_context().is_main_training_tower:\n        for v in [moving_mean, moving_var]:\n            tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, v)\n    return beta, gamma, moving_mean, moving_var\n\n\ndef internal_update_bn_ema(xn, batch_mean, batch_var,\n                           moving_mean, moving_var, decay):\n    update_op1 = moving_averages.assign_moving_average(\n        moving_mean, batch_mean, decay, zero_debias=False,\n        name=\'mean_ema_op\')\n    update_op2 = moving_averages.assign_moving_average(\n        moving_var, batch_var, decay, zero_debias=False,\n        name=\'var_ema_op\')\n\n    # When sync_statistics is True, always enable internal_update.\n    # Otherwise the update ops (only executed on main tower)\n    # will hang when some BatchNorm layers are unused (https://github.com/tensorpack/tensorpack/issues/1078)\n    with tf.control_dependencies([update_op1, update_op2]):\n        return tf.identity(xn, name=\'output\')\n\n\ndef get_sync_bn_mean_var(inputs, red_axis, sync_statistics):\n    ctx = get_current_tower_context()\n    batch_mean = tf.reduce_mean(inputs, axis=red_axis)\n    batch_mean_square = tf.reduce_mean(tf.square(inputs), axis=red_axis)\n\n    TF_version = get_tf_version_tuple()\n\n    if sync_statistics == \'nccl\':\n        num_dev = ctx.total\n        if num_dev == 1:\n            logger.warn(""BatchNorm(sync_statistics=\'nccl\') is used with only one tower!"")\n        else:\n            assert TF_version >= (1, 10), \\\n                ""Cross-GPU BatchNorm is only supported in TF>=1.10 ."" \\\n                ""Upgrade TF or apply this patch manually: https://github.com/tensorflow/tensorflow/pull/20360""\n\n            if TF_version <= (1, 12):\n                try:\n                    from tensorflow.contrib.nccl.python.ops.nccl_ops import _validate_and_load_nccl_so  # deprecated\n                except Exception:\n                    pass\n                else:\n                    _validate_and_load_nccl_so()\n                from tensorflow.contrib.nccl.ops import gen_nccl_ops  # deprecated\n            else:\n                from tensorflow.python.ops import gen_nccl_ops\n            shared_name = re.sub(\'tower[0-9]+/\', \'\', tf.get_variable_scope().name)\n            batch_mean = gen_nccl_ops.nccl_all_reduce(\n                input=batch_mean,\n                reduction=\'sum\',\n                num_devices=num_dev,\n                shared_name=shared_name + \'_NCCL_mean\') * (1.0 / num_dev)\n            batch_mean_square = gen_nccl_ops.nccl_all_reduce(\n                input=batch_mean_square,\n                reduction=\'sum\',\n                num_devices=num_dev,\n                shared_name=shared_name + \'_NCCL_mean_square\') * (1.0 / num_dev)\n    elif sync_statistics == \'horovod\':\n        # Require https://github.com/uber/horovod/pull/331\n        import horovod.tensorflow as hvd\n        if hvd.size() == 1:\n            logger.warn(""BatchNorm(sync_statistics=\'horovod\') is used with only one process!"")\n        else:\n            import horovod\n            hvd_version = tuple(map(int, horovod.__version__.split(\'.\')[:3]))\n            assert hvd_version >= (0, 13, 6), ""sync_statistics=horovod needs horovod>=0.13.6 !""\n\n            batch_mean = hvd.allreduce(batch_mean, average=True)\n            batch_mean_square = hvd.allreduce(batch_mean_square, average=True)\n    batch_var = batch_mean_square - tf.square(batch_mean)\n    return batch_mean, batch_var\n\n\n@layer_register()\n@convert_to_tflayer_args(\n    args_names=[],\n    name_mapping={\n        \'use_bias\': \'center\',\n        \'use_scale\': \'scale\',\n        \'gamma_init\': \'gamma_initializer\',\n        \'decay\': \'momentum\',\n        \'use_local_stat\': \'training\'\n    })\n@disable_autograph()\ndef BatchNorm(inputs, axis=None, *, training=None, momentum=0.9, epsilon=1e-5,\n              center=True, scale=True,\n              beta_initializer=tf.zeros_initializer(),\n              gamma_initializer=tf.ones_initializer(),\n              virtual_batch_size=None,\n              data_format=\'channels_last\',\n              ema_update=\'default\',\n              sync_statistics=None):\n    """"""\n    A more powerful version of `tf.layers.batch_normalization`. It differs from\n    the offical one in the following aspects:\n\n    1. Accepts an alternative ``data_format`` option when ``axis`` is None. For 2D input, this argument will be ignored.\n    2. Default value for ``momentum`` and ``epsilon`` is different.\n    3. Default value for ``training`` is automatically obtained from tensorpack\'s ``TowerContext``.\n       User-provided value can overwrite this behavior.\n    4. Support the ``ema_update`` option, which covers broader use cases than the standard EMA update.\n    5. Support the ``sync_statistics`` option, which implements ""SyncBN"" and is very useful in small-batch models.\n    6. Better support of the ``virtual_batch_size`` option that does not have the bugs in ``tf.layers``.\n\n    Args:\n        training (bool): if True, use per-batch statistics to normalize. Otherwise, use stored EMA\n            to normalize. By default, it is equal to `get_current_tower_context().is_training`.\n            This is not a good argument name, but it is what the Tensorflow layer uses.\n        virtual_batch_size (int): implement ""Ghost BatchNorm"" that normalizes\n            the data with a smaller batch size than the input. Only effective when training is True.\n            The value has to be a divisor of the actual batch size.\n\n            It does not use the buggy TensorFlow implementation which has the\n            problems of (1) wrong behavior at inference; (2) create variables with unnecessary size=1 dimensions.\n            Corresponding TF issue: https://github.com/tensorflow/tensorflow/issues/23050\n        ema_update (str): Only effective when ``training=True``. It has the following options:\n\n          * ""default"": same as ""collection"". Because this is the default behavior in TensorFlow.\n          * ""skip"": do not update EMA. This can be useful when you reuse a batch norm layer in several places\n            but do not want them to all update your EMA.\n          * ""collection"": Add EMA update ops to collection `tf.GraphKeys.UPDATE_OPS`.\n            The ops in the collection will be run automatically by the callback :class:`RunUpdateOps`, along with\n            your training iterations. This can waste compute if your training iterations do not always depend\n            on the BatchNorm layer.\n          * ""internal"": EMA is updated inside this layer itself by control dependencies.\n            In standard scenarios, it has similar speed to ""collection"". But it has some more benefits:\n\n            1. BatchNorm is used inside dynamic control flow.\n               The collection-based update does not support dynamic control flows.\n            2. BatchNorm layer is sometimes unused (e.g., in GANs you have two networks to train alternatively).\n               Putting all update ops into a single collection will waste a lot of compute.\n            3. Other part of the model relies on the ""updated"" EMA. The collection-based method does not update\n               EMA immediately.\n            4. It has less chance to cause TensorFlow bugs in a graph with complicated control flow.\n\n            Therefore this option is preferred over TensorFlow default.\n            Corresponding TF issue: https://github.com/tensorflow/tensorflow/issues/14699\n        sync_statistics (str or None): one of None, ""nccl"", or ""horovod"". It determines how to compute the\n          ""per-batch statistics"" when ``training==True``.\n\n          * None: it uses statistics of the input tensor to normalize during training.\n            This is the standard way BatchNorm was implemented in most frameworks.\n\n          * ""nccl"": this layer must be used under tensorpack\'s multi-GPU trainers.\n            It uses the aggregated statistics of the whole batch (across all GPUs) to normalize.\n\n          * ""horovod"": this layer must be used under tensorpack\'s :class:`HorovodTrainer`.\n            It uses the aggregated statistics of the whole batch (across all MPI ranks) to normalize.\n            Note that on a single machine this is found to be slower than the ""nccl"" implementation.\n\n          When not None, each GPU computes its own E[x] and E[x^2],\n          which are then averaged among all GPUs to compute global mean & variance.\n          Therefore each GPU needs to have the same batch size.\n\n          The synchronization is based on the current variable scope + the name of the layer\n          (`BatchNorm(\'name\', input)`). Therefore, you need to make sure that:\n\n          1. The BatchNorm layer on different GPUs needs to have the same name, so that\n             statistics can be synchronized. If names do not match, this layer will hang.\n          2. A BatchNorm layer cannot be reused within one tower.\n          3. A BatchNorm layer needs to be executed for the same number of times by all GPUs.\n             If different GPUs execute one BatchNorm layer for different number of times\n             (e.g., if some GPUs do not execute it), this layer may hang.\n\n          This option is also known as ""SyncBN"" or ""Cross-GPU BatchNorm"" as mentioned in:\n          `MegDet: A Large Mini-Batch Object Detector <https://arxiv.org/abs/1711.07240>`_.\n          Corresponding TF issue: https://github.com/tensorflow/tensorflow/issues/18222.\n\n          When `sync_statistics` is enabled, `ema_update` is set to ""internal"" automatically.\n          This is to avoid running `UPDATE_OPS`, which requires synchronization.\n\n    Variable Names:\n\n    * ``beta``: the bias term. Will be zero-inited by default.\n    * ``gamma``: the scale term. Will be one-inited by default.\n    * ``mean/EMA``: the moving average of mean.\n    * ``variance/EMA``: the moving average of variance.\n\n    Note:\n        This layer is more flexible than the standard ""BatchNorm"" layer and provides more features:\n\n        1. No matter whether you\'re doing training or not, you can set the ``training`` argument\n           to use batch statistics or EMA statistics.\n           i.e., you can use batch statistics during inference, or use EMA statistics during training.\n           Using EMA statistics in training is useful when you load a pre-trained BN and\n           don\'t want to update it.\n        2. As long as `training=True`, `sync_statistics` and `ema_update` option will take effect.\n    """"""\n    # parse training/ctx\n    ctx = get_current_tower_context()\n    if training is None:\n        training = ctx.is_training\n    training = bool(training)\n    if not training:\n        virtual_batch_size = None\n\n    # parse shapes\n    data_format = get_data_format(data_format, keras_mode=False)\n    shape = inputs.get_shape().as_list()\n    ndims = len(shape)\n    assert ndims in [2, 4], ndims\n    if sync_statistics is not None:\n        sync_statistics = sync_statistics.lower()\n    assert sync_statistics in [None, \'nccl\', \'horovod\'], sync_statistics\n\n    assert ema_update in [""default"", ""collection"", ""internal"", ""skip""]\n    if ema_update == ""default"":\n        ema_update = ""collection""\n    # Logic:\n    # 1. EMA update is possible only when we compute batch statistics (training=True)\n    # 2. We know that in training, non-main training tower does not need EMA\n    #    update (unless you need, e.g., inference during training on all towers)\n    #    We don\'t know about what to do in prediction context, so be conservative and do the update.\n    # 3. User can explicit disable update by ""skip"".\n    do_ema_update = training and \\\n        (ctx.is_main_training_tower or not ctx.is_training) \\\n        and (ema_update != ""skip"")\n\n    if axis is None:\n        if ndims == 2:\n            axis = 1\n        else:\n            axis = 1 if data_format == \'NCHW\' else 3\n    assert axis in [1, 3], axis\n    num_chan = shape[axis]\n\n    freeze_bn_backward = not training and ctx.is_training\n    if freeze_bn_backward:\n        if ctx.is_main_training_tower:  # only warn in first tower\n            log_once(""Some BatchNorm layer uses moving_mean/moving_variance in training."", func=\'warn\')\n        # Using moving_mean/moving_variance in training, which means we\n        # loaded a pre-trained BN and only fine-tuning the affine part.\n\n    do_sync_bn = (sync_statistics is not None) and training\n\n    if not do_sync_bn and not virtual_batch_size:\n        # Use the builtin layer for regular per-GPU BN.\n        # Use our own implementation for SyncBN and GhostBN\n        coll_bk = backup_collection([tf.GraphKeys.UPDATE_OPS])\n        with rename_get_variable(\n                {\'moving_mean\': \'mean/EMA\',\n                 \'moving_variance\': \'variance/EMA\'}):\n            tf_args = dict(\n                axis=axis,\n                momentum=momentum, epsilon=epsilon,\n                center=center, scale=scale,\n                beta_initializer=beta_initializer,\n                gamma_initializer=gamma_initializer,\n                fused=(ndims == 4 and axis in [1, 3]),\n                _reuse=tf.get_variable_scope().reuse)\n            use_fp16 = inputs.dtype == tf.float16\n            if use_fp16:\n                # non-fused does not support fp16; fused does not support all layouts.\n                # we made our best guess here\n                tf_args[\'fused\'] = True\n            layer = tf.layers.BatchNormalization(**tf_args)\n            xn = layer.apply(inputs, training=training, scope=tf.get_variable_scope())\n\n        # Add EMA variables to the correct collection\n        if ctx.is_main_training_tower:\n            for v in layer.non_trainable_variables:\n                if isinstance(v, tf.Variable):\n                    tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, v)\n\n        if not do_ema_update:\n            restore_collection(coll_bk)\n        if do_ema_update and ema_update == ""internal"":\n            # Implement ""internal"" update.\n            restore_collection(coll_bk)\n            assert layer.updates\n            with tf.control_dependencies(layer.updates):\n                ret = tf.identity(xn, name=\'output\')\n        else:\n            ret = tf.identity(xn, name=\'output\')\n\n        vh = ret.variables = VariableHolder(\n            moving_mean=layer.moving_mean,\n            mean=layer.moving_mean,  # for backward-compatibility\n            moving_variance=layer.moving_variance,\n            variance=layer.moving_variance)  # for backward-compatibility\n        if scale:\n            vh.gamma = layer.gamma\n        if center:\n            vh.beta = layer.beta\n    else:\n        red_axis = [0] if ndims == 2 else ([0, 2, 3] if axis == 1 else [0, 1, 2])\n        beta, gamma, moving_mean, moving_var = get_bn_variables(\n            num_chan, scale, center, beta_initializer, gamma_initializer)\n        assert sync_statistics is None or virtual_batch_size is None, ""Cannot use SyncBN and GhostBN together!""\n        new_shape = None  # don\'t need to reshape unless ...\n\n        if sync_statistics is not None:\n            # sync bn\n            batch_mean, batch_var = get_sync_bn_mean_var(inputs, red_axis, sync_statistics)\n            batch_mean_vec = batch_mean\n            batch_var_vec = batch_var\n\n            if ndims == 4 and axis == 1:\n                new_shape = [1, num_chan, 1, 1]\n                batch_mean = tf.reshape(batch_mean, new_shape)\n                batch_var = tf.reshape(batch_var, new_shape)\n        else:\n            orig_shape = tf.shape(inputs)\n            inputs = tf.reshape(\n                inputs,\n                tf.concat([[-1, virtual_batch_size],\n                           tf.shape(inputs)[1:]], axis=0))\n            # B/V, V, ...\n            red_axis = [x + 1 for x in red_axis]\n            new_shape = [1] * (ndims + 1)\n            new_shape[axis + 1] = num_chan\n\n            batch_mean, batch_var = tf.nn.moments(inputs, red_axis, keepdims=True)\n            # B/V, C\n            # vec for EMA update: use the first one only to mimic per-GPU BN\n            batch_mean_vec = tf.reshape(batch_mean[0], [num_chan])\n            batch_var_vec = tf.reshape(batch_var[0], [num_chan])\n\n        if new_shape is not None:\n            # Using fused_batch_norm(is_training=False) is actually slightly faster,\n            # but hopefully this call will be JITed in the future.\n            xn = tf.nn.batch_normalization(\n                inputs, batch_mean, batch_var,\n                tf.reshape(beta, new_shape),\n                tf.reshape(gamma, new_shape), epsilon)\n        else:\n            xn = tf.nn.batch_normalization(\n                inputs, batch_mean, batch_var,\n                beta, gamma, epsilon)\n        if virtual_batch_size is not None:\n            xn = tf.reshape(xn, orig_shape)\n\n        if do_ema_update:\n            ret = internal_update_bn_ema(\n                xn, batch_mean_vec, batch_var_vec, moving_mean, moving_var, momentum)\n        else:\n            ret = tf.identity(xn, name=\'output\')\n\n        vh = ret.variables = VariableHolder(\n            moving_mean=moving_mean,\n            mean=moving_mean,  # for backward-compatibility\n            moving_variance=moving_var,\n            variance=moving_var)  # for backward-compatibility\n        if scale:\n            vh.gamma = gamma\n        if center:\n            vh.beta = beta\n    return ret\n\n\n@layer_register()\n@convert_to_tflayer_args(\n    args_names=[],\n    name_mapping={\n        \'use_bias\': \'center\',\n        \'use_scale\': \'scale\',\n        \'gamma_init\': \'gamma_initializer\',\n        \'decay\': \'momentum\'\n    })\ndef BatchRenorm(x, rmax, dmax, *, momentum=0.9, epsilon=1e-5,\n                center=True, scale=True, gamma_initializer=None,\n                data_format=\'channels_last\'):\n    """"""\n    Batch Renormalization layer, as described in the paper:\n    `Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models\n    <https://arxiv.org/abs/1702.03275>`_.\n    This implementation is a wrapper around `tf.layers.batch_normalization`.\n\n    Args:\n        x (tf.Tensor): a NHWC or NC tensor.\n        rmax, dmax (tf.Tensor): a scalar tensor, the maximum allowed corrections.\n        decay (float): decay rate of moving average.\n        epsilon (float): epsilon to avoid divide-by-zero.\n        use_scale, use_bias (bool): whether to use the extra affine transformation or not.\n\n    Returns:\n        tf.Tensor: a tensor named ``output`` with the same shape of x.\n\n    Variable Names:\n\n    * ``beta``: the bias term.\n    * ``gamma``: the scale term. Input will be transformed by ``x * gamma + beta``.\n    * ``moving_mean, renorm_mean, renorm_mean_weight``: See TF documentation.\n    * ``moving_variance, renorm_stddev, renorm_stddev_weight``: See TF documentation.\n    """"""\n\n    shape = x.get_shape().as_list()\n    ndims = len(shape)\n    assert ndims in [2, 4]\n    if ndims == 2:\n        data_format = \'channels_first\'\n\n    ctx = get_current_tower_context()\n    coll_bk = backup_collection([tf.GraphKeys.UPDATE_OPS])\n    layer = tf.layers.BatchNormalization(\n        axis=1 if data_format == \'channels_first\' else 3,\n        momentum=momentum, epsilon=epsilon,\n        center=center, scale=scale,\n        renorm=True,\n        renorm_clipping={\n            \'rmin\': 1.0 / rmax,\n            \'rmax\': rmax,\n            \'dmax\': dmax},\n        renorm_momentum=0.99,\n        gamma_initializer=gamma_initializer,\n        fused=False,\n        _reuse=tf.get_variable_scope().reuse)\n    xn = layer.apply(x, training=ctx.is_training, scope=tf.get_variable_scope())\n\n    if ctx.is_main_training_tower:\n        for v in layer.non_trainable_variables:\n            if isinstance(v, tf.Variable):\n                tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, v)\n    else:\n        # only run UPDATE_OPS in the first tower\n        restore_collection(coll_bk)\n\n    if ndims == 2:\n        xn = tf.squeeze(xn, [1, 2])\n    ret = tf.identity(xn, name=\'output\')\n\n    # TODO not sure whether to add moving_mean/moving_var to VH now\n    vh = ret.variables = VariableHolder()\n    if scale:\n        vh.gamma = layer.gamma\n    if center:\n        vh.beta = layer.beta\n    return ret\n'"
tensorpack/models/common.py,0,"b""# -*- coding: utf-8 -*-\n# File: common.py\n\nfrom .registry import layer_register, disable_layer_logging  # noqa\nfrom .tflayer import rename_tflayer_get_variable\nfrom .utils import VariableHolder  # noqa\n\n__all__ = ['layer_register', 'VariableHolder', 'rename_tflayer_get_variable',\n           'disable_layer_logging']\n"""
tensorpack/models/conv2d.py,37,"b'# -*- coding: utf-8 -*-\n# File: conv2d.py\n\n\nfrom ..compat import tfv1 as tf  # this should be avoided first in model code\n\nfrom ..tfutils.common import get_tf_version_tuple\nfrom ..utils.argtools import get_data_format, shape2d, shape4d, log_once\nfrom .common import VariableHolder, layer_register\nfrom .tflayer import convert_to_tflayer_args, rename_get_variable\n\n__all__ = [\'Conv2D\', \'Deconv2D\', \'Conv2DTranspose\']\n\n\n@layer_register(log_shape=True)\n@convert_to_tflayer_args(\n    args_names=[\'filters\', \'kernel_size\'],\n    name_mapping={\n        \'out_channel\': \'filters\',\n        \'kernel_shape\': \'kernel_size\',\n        \'stride\': \'strides\',\n    })\ndef Conv2D(\n        inputs,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding=\'same\',\n        data_format=\'channels_last\',\n        dilation_rate=(1, 1),\n        activation=None,\n        use_bias=True,\n        kernel_initializer=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        split=1):\n    """"""\n    Similar to `tf.layers.Conv2D`, but with some differences:\n\n    1. Default kernel initializer is variance_scaling_initializer(2.0).\n    2. Default padding is \'same\'.\n    3. Support \'split\' argument to do group convolution.\n\n    Variable Names:\n\n    * ``W``: weights\n    * ``b``: bias\n    """"""\n    if kernel_initializer is None:\n        if get_tf_version_tuple() <= (1, 12):\n            kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)  # deprecated\n        else:\n            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution=\'untruncated_normal\')\n    dilation_rate = shape2d(dilation_rate)\n\n    if split == 1 and dilation_rate == [1, 1]:\n        # tf.layers.Conv2D has bugs with dilations (https://github.com/tensorflow/tensorflow/issues/26797)\n        with rename_get_variable({\'kernel\': \'W\', \'bias\': \'b\'}):\n            layer = tf.layers.Conv2D(\n                filters,\n                kernel_size,\n                strides=strides,\n                padding=padding,\n                data_format=data_format,\n                dilation_rate=dilation_rate,\n                activation=activation,\n                use_bias=use_bias,\n                kernel_initializer=kernel_initializer,\n                bias_initializer=bias_initializer,\n                kernel_regularizer=kernel_regularizer,\n                bias_regularizer=bias_regularizer,\n                activity_regularizer=activity_regularizer,\n                _reuse=tf.get_variable_scope().reuse)\n            ret = layer.apply(inputs, scope=tf.get_variable_scope())\n            ret = tf.identity(ret, name=\'output\')\n\n        ret.variables = VariableHolder(W=layer.kernel)\n        if use_bias:\n            ret.variables.b = layer.bias\n\n    else:\n        # group conv implementation\n        data_format = get_data_format(data_format, keras_mode=False)\n        in_shape = inputs.get_shape().as_list()\n        channel_axis = 3 if data_format == \'NHWC\' else 1\n        in_channel = in_shape[channel_axis]\n        assert in_channel is not None, ""[Conv2D] Input cannot have unknown channel!""\n        assert in_channel % split == 0\n\n        assert kernel_regularizer is None and bias_regularizer is None and activity_regularizer is None, \\\n            ""Not supported by group conv or dilated conv!""\n\n        out_channel = filters\n        assert out_channel % split == 0\n        assert dilation_rate == [1, 1] or get_tf_version_tuple() >= (1, 5), \'TF>=1.5 required for dilated conv.\'\n\n        kernel_shape = shape2d(kernel_size)\n        filter_shape = kernel_shape + [in_channel / split, out_channel]\n        stride = shape4d(strides, data_format=data_format)\n\n        kwargs = {""data_format"": data_format}\n        if get_tf_version_tuple() >= (1, 5):\n            kwargs[\'dilations\'] = shape4d(dilation_rate, data_format=data_format)\n\n        # matching input dtype (ex. tf.float16) since the default dtype of variable if tf.float32\n        inputs_dtype = inputs.dtype\n        W = tf.get_variable(\n            \'W\', filter_shape, dtype=inputs_dtype, initializer=kernel_initializer)\n\n        if use_bias:\n            b = tf.get_variable(\'b\', [out_channel], dtype=inputs_dtype, initializer=bias_initializer)\n\n        if split == 1:\n            conv = tf.nn.conv2d(inputs, W, stride, padding.upper(), **kwargs)\n        else:\n            conv = None\n            if get_tf_version_tuple() >= (1, 13):\n                try:\n                    conv = tf.nn.conv2d(inputs, W, stride, padding.upper(), **kwargs)\n                except ValueError:\n                    log_once(""CUDNN group convolution support is only available with ""\n                             ""https://github.com/tensorflow/tensorflow/pull/25818 . ""\n                             ""Will fall back to a loop-based slow implementation instead!"", \'warn\')\n            if conv is None:\n                inputs = tf.split(inputs, split, channel_axis)\n                kernels = tf.split(W, split, 3)\n                outputs = [tf.nn.conv2d(i, k, stride, padding.upper(), **kwargs)\n                           for i, k in zip(inputs, kernels)]\n                conv = tf.concat(outputs, channel_axis)\n\n        ret = tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv\n        if activation is not None:\n            ret = activation(ret)\n        ret = tf.identity(ret, name=\'output\')\n\n        ret.variables = VariableHolder(W=W)\n        if use_bias:\n            ret.variables.b = b\n    return ret\n\n\n@layer_register(log_shape=True)\n@convert_to_tflayer_args(\n    args_names=[\'filters\', \'kernel_size\', \'strides\'],\n    name_mapping={\n        \'out_channel\': \'filters\',\n        \'kernel_shape\': \'kernel_size\',\n        \'stride\': \'strides\',\n    })\ndef Conv2DTranspose(\n        inputs,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding=\'same\',\n        data_format=\'channels_last\',\n        activation=None,\n        use_bias=True,\n        kernel_initializer=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None):\n    """"""\n    A wrapper around `tf.layers.Conv2DTranspose`.\n    Some differences to maintain backward-compatibility:\n\n    1. Default kernel initializer is variance_scaling_initializer(2.0).\n    2. Default padding is \'same\'\n\n    Variable Names:\n\n    * ``W``: weights\n    * ``b``: bias\n    """"""\n    if kernel_initializer is None:\n        if get_tf_version_tuple() <= (1, 12):\n            kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)  # deprecated\n        else:\n            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution=\'untruncated_normal\')\n\n    if get_tf_version_tuple() <= (1, 12):\n        with rename_get_variable({\'kernel\': \'W\', \'bias\': \'b\'}):\n            layer = tf.layers.Conv2DTranspose(\n                filters,\n                kernel_size,\n                strides=strides,\n                padding=padding,\n                data_format=data_format,\n                activation=activation,\n                use_bias=use_bias,\n                kernel_initializer=kernel_initializer,\n                bias_initializer=bias_initializer,\n                kernel_regularizer=kernel_regularizer,\n                bias_regularizer=bias_regularizer,\n                activity_regularizer=activity_regularizer,\n                _reuse=tf.get_variable_scope().reuse)\n            ret = layer.apply(inputs, scope=tf.get_variable_scope())\n            ret = tf.identity(ret, name=\'output\')\n        ret.variables = VariableHolder(W=layer.kernel)\n        if use_bias:\n            ret.variables.b = layer.bias\n    else:\n        # Our own implementation, to avoid Keras bugs. https://github.com/tensorflow/tensorflow/issues/25946\n        assert kernel_regularizer is None and bias_regularizer is None and activity_regularizer is None, \\\n            ""Unsupported arguments due to Keras bug in TensorFlow 1.13""\n        data_format = get_data_format(data_format, keras_mode=False)\n        shape_dyn = tf.shape(inputs)\n        shape_sta = inputs.shape.as_list()\n        strides2d = shape2d(strides)\n        kernel_shape = shape2d(kernel_size)\n\n        assert padding.lower() in [\'valid\', \'same\'], ""Padding {} is not supported!"".format(padding)\n\n        if padding.lower() == \'valid\':\n            shape_res2d = [max(kernel_shape[0] - strides2d[0], 0),\n                           max(kernel_shape[1] - strides2d[1], 0)]\n        else:\n            shape_res2d = shape2d(0)\n\n        if data_format == \'NCHW\':\n            channels_in = shape_sta[1]\n            out_shape_dyn = tf.stack(\n                [shape_dyn[0], filters,\n                 shape_dyn[2] * strides2d[0] + shape_res2d[0],\n                 shape_dyn[3] * strides2d[1] + shape_res2d[1]])\n            out_shape3_sta = [filters,\n                              None if shape_sta[2] is None else shape_sta[2] * strides2d[0] + shape_res2d[0],\n                              None if shape_sta[3] is None else shape_sta[3] * strides2d[1] + shape_res2d[1]]\n        else:\n            channels_in = shape_sta[-1]\n            out_shape_dyn = tf.stack(\n                [shape_dyn[0],\n                 shape_dyn[1] * strides2d[0] + shape_res2d[0],\n                 shape_dyn[2] * strides2d[1] + shape_res2d[1],\n                 filters])\n            out_shape3_sta = [None if shape_sta[1] is None else shape_sta[1] * strides2d[0] + shape_res2d[0],\n                              None if shape_sta[2] is None else shape_sta[2] * strides2d[1] + shape_res2d[1],\n                              filters]\n\n        inputs_dtype = inputs.dtype\n        W = tf.get_variable(\'W\', kernel_shape + [filters, channels_in],\n                            dtype=inputs_dtype, initializer=kernel_initializer)\n        if use_bias:\n            b = tf.get_variable(\'b\', [filters], dtype=inputs_dtype, initializer=bias_initializer)\n        conv = tf.nn.conv2d_transpose(\n            inputs, W, out_shape_dyn,\n            shape4d(strides, data_format=data_format),\n            padding=padding.upper(),\n            data_format=data_format)\n        conv.set_shape(tf.TensorShape([shape_sta[0]] + out_shape3_sta))\n\n        ret = tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv\n        if activation is not None:\n            ret = activation(ret)\n        ret = tf.identity(ret, name=\'output\')\n\n        ret.variables = VariableHolder(W=W)\n        if use_bias:\n            ret.variables.b = b\n\n    return ret\n\n\nDeconv2D = Conv2DTranspose\n'"
tensorpack/models/fc.py,10,"b'# -*- coding: utf-8 -*-\n# File: fc.py\n\n\nimport numpy as np\nfrom ..compat import tfv1 as tf  # this should be avoided first in model code\n\nfrom ..tfutils.common import get_tf_version_tuple\nfrom .common import VariableHolder, layer_register\nfrom .tflayer import convert_to_tflayer_args, rename_get_variable\n\n__all__ = [\'FullyConnected\']\n\n\ndef batch_flatten(x):\n    """"""\n    Flatten the tensor except the first dimension.\n    """"""\n    shape = x.get_shape().as_list()[1:]\n    if None not in shape:\n        return tf.reshape(x, [-1, int(np.prod(shape))])\n    return tf.reshape(x, tf.stack([tf.shape(x)[0], -1]))\n\n\n@layer_register(log_shape=True)\n@convert_to_tflayer_args(\n    args_names=[\'units\'],\n    name_mapping={\'out_dim\': \'units\'})\ndef FullyConnected(\n        inputs,\n        units,\n        activation=None,\n        use_bias=True,\n        kernel_initializer=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None):\n    """"""\n    A wrapper around `tf.layers.Dense`.\n    One difference to maintain backward-compatibility:\n    Default weight initializer is variance_scaling_initializer(2.0).\n\n    Variable Names:\n\n    * ``W``: weights of shape [in_dim, out_dim]\n    * ``b``: bias\n    """"""\n    if kernel_initializer is None:\n        if get_tf_version_tuple() <= (1, 12):\n            kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)  # deprecated\n        else:\n            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution=\'untruncated_normal\')\n\n    inputs = batch_flatten(inputs)\n    with rename_get_variable({\'kernel\': \'W\', \'bias\': \'b\'}):\n        layer = tf.layers.Dense(\n            units=units,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            _reuse=tf.get_variable_scope().reuse)\n        ret = layer.apply(inputs, scope=tf.get_variable_scope())\n        ret = tf.identity(ret, name=\'output\')\n\n    ret.variables = VariableHolder(W=layer.kernel)\n    if use_bias:\n        ret.variables.b = layer.bias\n    return ret\n'"
tensorpack/models/layer_norm.py,20,"b'# -*- coding: utf-8 -*-\n# File: layer_norm.py\n\n\nfrom ..compat import tfv1 as tf  # this should be avoided first in model code\n\nfrom ..utils.argtools import get_data_format\nfrom ..utils.develop import log_deprecated\nfrom .common import VariableHolder, layer_register\nfrom .tflayer import convert_to_tflayer_args\n\n__all__ = [\'LayerNorm\', \'InstanceNorm\']\n\n\n@layer_register()\n@convert_to_tflayer_args(\n    args_names=[],\n    name_mapping={\n        \'use_bias\': \'center\',\n        \'use_scale\': \'scale\',\n        \'gamma_init\': \'gamma_initializer\',\n    })\ndef LayerNorm(\n        x, epsilon=1e-5, *,\n        center=True, scale=True,\n        gamma_initializer=tf.ones_initializer(),\n        data_format=\'channels_last\'):\n    """"""\n    Layer Normalization layer, as described in the paper:\n    `Layer Normalization <https://arxiv.org/abs/1607.06450>`_.\n\n    Args:\n        x (tf.Tensor): a 4D or 2D tensor. When 4D, the layout should match data_format.\n        epsilon (float): epsilon to avoid divide-by-zero.\n        center, scale (bool): whether to use the extra affine transformation or not.\n    """"""\n    data_format = get_data_format(data_format, keras_mode=False)\n    shape = x.get_shape().as_list()\n    ndims = len(shape)\n    assert ndims in [2, 4]\n\n    mean, var = tf.nn.moments(x, list(range(1, len(shape))), keep_dims=True)\n\n    if data_format == \'NCHW\':\n        chan = shape[1]\n        new_shape = [1, chan, 1, 1]\n    else:\n        chan = shape[-1]\n        new_shape = [1, 1, 1, chan]\n    if ndims == 2:\n        new_shape = [1, chan]\n\n    if center:\n        beta = tf.get_variable(\'beta\', [chan], initializer=tf.constant_initializer())\n        beta = tf.reshape(beta, new_shape)\n    else:\n        beta = tf.zeros([1] * ndims, name=\'beta\')\n    if scale:\n        gamma = tf.get_variable(\'gamma\', [chan], initializer=gamma_initializer)\n        gamma = tf.reshape(gamma, new_shape)\n    else:\n        gamma = tf.ones([1] * ndims, name=\'gamma\')\n\n    ret = tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon, name=\'output\')\n\n    vh = ret.variables = VariableHolder()\n    if scale:\n        vh.gamma = gamma\n    if center:\n        vh.beta = beta\n    return ret\n\n\n@layer_register()\n@convert_to_tflayer_args(\n    args_names=[],\n    name_mapping={\n        \'gamma_init\': \'gamma_initializer\',\n    })\ndef InstanceNorm(x, epsilon=1e-5, *, center=True, scale=True,\n                 gamma_initializer=tf.ones_initializer(),\n                 data_format=\'channels_last\', use_affine=None):\n    """"""\n    Instance Normalization, as in the paper:\n    `Instance Normalization: The Missing Ingredient for Fast Stylization\n    <https://arxiv.org/abs/1607.08022>`_.\n\n    Args:\n        x (tf.Tensor): a 4D tensor.\n        epsilon (float): avoid divide-by-zero\n        center, scale (bool): whether to use the extra affine transformation or not.\n        use_affine: deprecated. Don\'t use.\n    """"""\n    data_format = get_data_format(data_format, keras_mode=False)\n    shape = x.get_shape().as_list()\n    assert len(shape) == 4, ""Input of InstanceNorm has to be 4D!""\n\n    if use_affine is not None:\n        log_deprecated(""InstanceNorm(use_affine=)"", ""Use center= or scale= instead!"", ""2020-06-01"")\n        center = scale = use_affine\n\n    if data_format == \'NHWC\':\n        axis = [1, 2]\n        ch = shape[3]\n        new_shape = [1, 1, 1, ch]\n    else:\n        axis = [2, 3]\n        ch = shape[1]\n        new_shape = [1, ch, 1, 1]\n    assert ch is not None, ""Input of InstanceNorm require known channel!""\n\n    mean, var = tf.nn.moments(x, axis, keep_dims=True)\n\n    if center:\n        beta = tf.get_variable(\'beta\', [ch], initializer=tf.constant_initializer())\n        beta = tf.reshape(beta, new_shape)\n    else:\n        beta = tf.zeros([1, 1, 1, 1], name=\'beta\', dtype=x.dtype)\n    if scale:\n        gamma = tf.get_variable(\'gamma\', [ch], initializer=gamma_initializer)\n        gamma = tf.reshape(gamma, new_shape)\n    else:\n        gamma = tf.ones([1, 1, 1, 1], name=\'gamma\', dtype=x.dtype)\n    ret = tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon, name=\'output\')\n\n    vh = ret.variables = VariableHolder()\n    if scale:\n        vh.gamma = gamma\n    if center:\n        vh.beta = beta\n    return ret\n'"
tensorpack/models/linearwrap.py,3,"b'# -*- coding: utf-8 -*-\n# File: linearwrap.py\n\n\nfrom types import ModuleType\nimport six\n\nfrom .registry import get_registered_layer\n\n__all__ = [\'LinearWrap\']\n\n\nclass LinearWrap(object):\n    """""" A simple wrapper to easily create ""linear"" graph,\n        consisting of layers / symbolic functions with only one input & output.\n    """"""\n\n    class _TFModuleFunc(object):\n        def __init__(self, mod, tensor):\n            self._mod = mod\n            self._t = tensor\n\n        def __getattr__(self, name):\n            ret = getattr(self._mod, name)\n            if isinstance(ret, ModuleType):\n                return LinearWrap._TFModuleFunc(ret, self._t)\n            else:\n                # assume to be a tf function\n                def f(*args, **kwargs):\n                    o = ret(self._t, *args, **kwargs)\n                    return LinearWrap(o)\n                return f\n\n    def __init__(self, tensor):\n        """"""\n        Args:\n            tensor (tf.Tensor): the tensor to wrap\n        """"""\n        self._t = tensor\n\n    def __getattr__(self, layer_name):\n        layer = get_registered_layer(layer_name)\n        if layer is not None:\n            # this is a registered tensorpack layer\n            # parse arguments by tensorpack model convention\n            if layer.use_scope:\n                def layer_func(name, *args, **kwargs):\n                    ret = layer(name, self._t, *args, **kwargs)\n                    return LinearWrap(ret)\n            else:\n                def layer_func(*args, **kwargs):\n                    if len(args) and isinstance(args[0], six.string_types):\n                        name, args = args[0], args[1:]\n                        ret = layer(name, self._t, *args, **kwargs)\n                    else:\n                        ret = layer(self._t, *args, **kwargs)\n                    return LinearWrap(ret)\n            return layer_func\n        else:\n            assert layer_name == \'tf\', \\\n                ""Calling LinearWrap.{}:"" \\\n                "" neither a layer nor \'tf\'! "" \\\n                ""Did you forget to extract tensor from LinearWrap?"".format(layer_name)\n            import tensorflow as layer  # noqa\n            assert isinstance(layer, ModuleType), layer\n            return LinearWrap._TFModuleFunc(layer, self._t)\n\n    def apply(self, func, *args, **kwargs):\n        """"""\n        Apply a function on the wrapped tensor.\n\n        Returns:\n            LinearWrap: ``LinearWrap(func(self.tensor(), *args, **kwargs))``.\n        """"""\n        ret = func(self._t, *args, **kwargs)\n        return LinearWrap(ret)\n\n    def apply2(self, func, *args, **kwargs):\n        """"""\n        Apply a function on the wrapped tensor. The tensor\n        will be the second argument of func.\n\n        This is because many symbolic functions\n        (such as tensorpack\'s layers) takes \'scope\' as the first argument.\n\n        Returns:\n            LinearWrap: ``LinearWrap(func(args[0], self.tensor(), *args[1:], **kwargs))``.\n        """"""\n        ret = func(args[0], self._t, *(args[1:]), **kwargs)\n        return LinearWrap(ret)\n\n    def __call__(self):\n        """"""\n        Returns:\n            tf.Tensor: the underlying wrapped tensor.\n        """"""\n        return self._t\n\n    def tensor(self):\n        """"""\n        Equivalent to ``self.__call__()``.\n\n        Returns:\n            tf.Tensor: the underlying wrapped tensor.\n        """"""\n        return self._t\n\n    def print_tensor(self):\n        """"""\n        Print the underlying tensor and return self. Can be useful to get the\n        name of tensors inside :class:`LinearWrap`.\n\n        :return: self\n        """"""\n        print(self._t)\n        return self\n'"
tensorpack/models/models_test.py,8,"b'# -*- coding: utf-8 -*-\n# File: _test.py\n\n\nimport logging\nimport unittest\nimport tensorflow as tf\nimport numpy as np\n\nfrom .conv2d import Conv2DTranspose\nfrom .pool import FixedUnPooling\n\n\nclass TestModel(unittest.TestCase):\n\n    def eval(self, x, feed_dict=None):\n        sess = tf.Session()\n        sess.run(tf.global_variables_initializer())\n        if isinstance(x, list):\n            return sess.run(x, feed_dict=feed_dict)\n        else:\n            return sess.run([x], feed_dict=feed_dict)[0]\n\n    def make_variable(self, *args):\n        if len(args) > 1:\n            return [tf.Variable(k) for k in args]\n        else:\n            return tf.Variable(args[0])\n\n\nclass TestPool(TestModel):\n    def test_FixedUnPooling(self):\n        h, w = 3, 4\n        scale = 2\n        mat = np.random.rand(h, w, 3).astype(\'float32\')\n        input = self.make_variable(mat)\n        input = tf.reshape(input, [1, h, w, 3])\n        output = FixedUnPooling(\'unpool\', input, scale)\n        res = self.eval(output)\n        self.assertEqual(res.shape, (1, scale * h, scale * w, 3))\n\n        # mat is on corner\n        ele = res[0, ::scale, ::scale, 0]\n        self.assertTrue((ele == mat[:, :, 0]).all())\n        # the rest are zeros\n        res[0, ::scale, ::scale, :] = 0\n        self.assertTrue((res == 0).all())\n\n# Below was originally for the BilinearUpsample layer used in the HED example\n#     def test_BilinearUpSample(self):\n#         h, w = 12, 12\n#         scale = 2\n#\n#         mat = np.random.rand(h, w).astype(\'float32\')\n#         inp = self.make_variable(mat)\n#         inp = tf.reshape(inp, [1, h, w, 1])\n#\n#         output = BilinearUpSample(inp, scale)\n#         res = self.eval(output)[0, :, :, 0]\n#\n#         from skimage.transform import rescale\n#         res2 = rescale(mat, scale, mode=\'edge\')\n#\n#         diff = np.abs(res2 - res)\n#\n#         # if not diff.max() < 1e-4:\n#         #     import IPython\n#         #     IPython.embed(config=IPython.terminal.ipapp.load_default_config())\n#         self.assertTrue(diff.max() < 1e-4, diff.max())\n\n\nclass TestConv2DTranspose(TestModel):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def test_shape_match(self):\n        h, w = 12, 18\n        input = self.make_variable(np.random.rand(1, h, w, 3).astype(""float32""))\n        for padding in [""same"", ""valid""]:\n            for stride in [1, 2]:\n                output = Conv2DTranspose(\n                    \'deconv_s{}_pad{}\'.format(stride, padding),\n                    input, 20, 3, strides=stride, padding=padding)\n\n                static_shape = output.shape\n                dynamic_shape = self.eval(output).shape\n                self.assertTrue(static_shape == dynamic_shape)\n\n    def test_unspecified_shape_match(self):\n        h, w = 12, 18\n        input = tf.placeholder(shape=(1, h, None, 3), dtype=tf.float32)\n        for padding in [""same"", ""valid""]:\n            for stride in [1, 2]:\n                output = Conv2DTranspose(\n                    \'deconv_s{}_pad{}\'.format(stride, padding),\n                    input, 20, 3, strides=stride, padding=padding)\n\n                static_shape = tuple(output.shape.as_list())\n                dynamic_shape = self.eval(\n                    output,\n                    feed_dict={input: np.random.rand(1, h, w, 3)}).shape\n                self.assertTrue(static_shape[2] is None)\n                self.assertTrue(static_shape[:2] == dynamic_shape[:2])\n                self.assertTrue(static_shape[3] == dynamic_shape[3])\n\n\ndef run_test_case(case):\n    suite = unittest.TestLoader().loadTestsFromTestCase(case)\n    unittest.TextTestRunner(verbosity=2).run(suite)\n\n\nif __name__ == \'__main__\':\n    from tensorpack.utils import logger\n    logger.setLevel(logging.CRITICAL)\n    unittest.main()\n'"
tensorpack/models/nonlin.py,10,"b'# -*- coding: utf-8 -*-\n# File: nonlin.py\n\n\nimport tensorflow as tf\n\nfrom ..utils.develop import log_deprecated\nfrom ..compat import tfv1\nfrom .batch_norm import BatchNorm\nfrom .common import VariableHolder, layer_register\nfrom .utils import disable_autograph\n\n__all__ = [\'Maxout\', \'PReLU\', \'BNReLU\']\n\n\n@layer_register(use_scope=None)\ndef Maxout(x, num_unit):\n    """"""\n    Maxout as in the paper `Maxout Networks <http://arxiv.org/abs/1302.4389>`_.\n\n    Args:\n        x (tf.Tensor): a NHWC or NC tensor. Channel has to be known.\n        num_unit (int): a int. Must be divisible by C.\n\n    Returns:\n        tf.Tensor: of shape NHW(C/num_unit) named ``output``.\n    """"""\n    input_shape = x.get_shape().as_list()\n    ndim = len(input_shape)\n    assert ndim == 4 or ndim == 2\n    ch = input_shape[-1]\n    assert ch is not None and ch % num_unit == 0\n    if ndim == 4:\n        x = tf.reshape(x, [-1, input_shape[1], input_shape[2], ch / num_unit, num_unit])\n    else:\n        x = tf.reshape(x, [-1, ch / num_unit, num_unit])\n    return tf.reduce_max(x, ndim, name=\'output\')\n\n\n@layer_register()\n@disable_autograph()\ndef PReLU(x, init=0.001, name=None):\n    """"""\n    Parameterized ReLU as in the paper `Delving Deep into Rectifiers: Surpassing\n    Human-Level Performance on ImageNet Classification\n    <http://arxiv.org/abs/1502.01852>`_.\n\n    Args:\n        x (tf.Tensor): input\n        init (float): initial value for the learnable slope.\n        name (str): deprecated argument. Don\'t use\n\n    Variable Names:\n\n    * ``alpha``: learnable slope.\n    """"""\n    if name is not None:\n        log_deprecated(""PReLU(name=...)"", ""The output tensor will be named `output`."")\n    init = tfv1.constant_initializer(init)\n    alpha = tfv1.get_variable(\'alpha\', [], initializer=init)\n    x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))\n    ret = tf.multiply(x, 0.5, name=name or None)\n\n    ret.variables = VariableHolder(alpha=alpha)\n    return ret\n\n\n@layer_register(use_scope=None)\ndef BNReLU(x, name=None):\n    """"""\n    A shorthand of BatchNormalization + ReLU.\n\n    Args:\n        x (tf.Tensor): the input\n        name: deprecated, don\'t use.\n    """"""\n    if name is not None:\n        log_deprecated(""BNReLU(name=...)"", ""The output tensor will be named `output`."")\n\n    x = BatchNorm(\'bn\', x)\n    x = tf.nn.relu(x, name=name)\n    return x\n'"
tensorpack/models/pool.py,29,"b'# -*- coding: utf-8 -*-\n# File: pool.py\n\nimport numpy as np\nfrom ..compat import tfv1 as tf  # this should be avoided first in model code\n\nfrom ..utils.argtools import get_data_format, shape2d\nfrom .common import layer_register\nfrom .shape_utils import StaticDynamicShape\nfrom .tflayer import convert_to_tflayer_args\n\n__all__ = [\'MaxPooling\', \'FixedUnPooling\', \'AvgPooling\', \'GlobalAvgPooling\']\n\n\n@layer_register(log_shape=True)\n@convert_to_tflayer_args(\n    args_names=[\'pool_size\', \'strides\'],\n    name_mapping={\'shape\': \'pool_size\', \'stride\': \'strides\'})\ndef MaxPooling(\n        inputs,\n        pool_size,\n        strides=None,\n        padding=\'valid\',\n        data_format=\'channels_last\'):\n    """"""\n    Same as `tf.layers.MaxPooling2D`. Default strides is equal to pool_size.\n    """"""\n    if strides is None:\n        strides = pool_size\n    layer = tf.layers.MaxPooling2D(pool_size, strides, padding=padding, data_format=data_format)\n    ret = layer.apply(inputs, scope=tf.get_variable_scope())\n    return tf.identity(ret, name=\'output\')\n\n\n@layer_register(log_shape=True)\n@convert_to_tflayer_args(\n    args_names=[\'pool_size\', \'strides\'],\n    name_mapping={\'shape\': \'pool_size\', \'stride\': \'strides\'})\ndef AvgPooling(\n        inputs,\n        pool_size,\n        strides=None,\n        padding=\'valid\',\n        data_format=\'channels_last\'):\n    """"""\n    Same as `tf.layers.AveragePooling2D`. Default strides is equal to pool_size.\n    """"""\n    if strides is None:\n        strides = pool_size\n    layer = tf.layers.AveragePooling2D(pool_size, strides, padding=padding, data_format=data_format)\n    ret = layer.apply(inputs, scope=tf.get_variable_scope())\n    return tf.identity(ret, name=\'output\')\n\n\n@layer_register(log_shape=True)\ndef GlobalAvgPooling(x, data_format=\'channels_last\'):\n    """"""\n    Global average pooling as in the paper `Network In Network\n    <http://arxiv.org/abs/1312.4400>`_.\n\n    Args:\n        x (tf.Tensor): a 4D tensor.\n\n    Returns:\n        tf.Tensor: a NC tensor named ``output``.\n    """"""\n    assert x.shape.ndims == 4\n    data_format = get_data_format(data_format)\n    axis = [1, 2] if data_format == \'channels_last\' else [2, 3]\n    return tf.reduce_mean(x, axis, name=\'output\')\n\n\ndef UnPooling2x2ZeroFilled(x):\n    # https://github.com/tensorflow/tensorflow/issues/2169\n    out = tf.concat([x, tf.zeros_like(x)], 3)\n    out = tf.concat([out, tf.zeros_like(out)], 2)\n\n    sh = x.get_shape().as_list()\n    if None not in sh[1:]:\n        out_size = [-1, sh[1] * 2, sh[2] * 2, sh[3]]\n        return tf.reshape(out, out_size)\n    else:\n        shv = tf.shape(x)\n        ret = tf.reshape(out, tf.stack([-1, shv[1] * 2, shv[2] * 2, sh[3]]))\n        return ret\n\n\n@layer_register(log_shape=True)\ndef FixedUnPooling(x, shape, unpool_mat=None, data_format=\'channels_last\'):\n    """"""\n    Unpool the input with a fixed matrix to perform kronecker product with.\n\n    Args:\n        x (tf.Tensor): a 4D image tensor\n        shape: int or (h, w) tuple\n        unpool_mat: a tf.Tensor or np.ndarray 2D matrix with size=shape.\n            If is None, will use a matrix with 1 at top-left corner.\n\n    Returns:\n        tf.Tensor: a 4D image tensor.\n    """"""\n    data_format = get_data_format(data_format, keras_mode=False)\n    shape = shape2d(shape)\n\n    output_shape = StaticDynamicShape(x)\n    output_shape.apply(1 if data_format == \'NHWC\' else 2, lambda x: x * shape[0])\n    output_shape.apply(2 if data_format == \'NHWC\' else 3, lambda x: x * shape[1])\n\n    # a faster implementation for this special case\n    if shape[0] == 2 and shape[1] == 2 and unpool_mat is None and data_format == \'NHWC\':\n        ret = UnPooling2x2ZeroFilled(x)\n    else:\n        # check unpool_mat\n        if unpool_mat is None:\n            mat = np.zeros(shape, dtype=\'float32\')\n            mat[0][0] = 1\n            unpool_mat = tf.constant(mat, name=\'unpool_mat\')\n        elif isinstance(unpool_mat, np.ndarray):\n            unpool_mat = tf.constant(unpool_mat, name=\'unpool_mat\')\n        assert unpool_mat.shape.as_list() == list(shape)\n\n        if data_format == \'NHWC\':\n            x = tf.transpose(x, [0, 3, 1, 2])\n        # perform a tensor-matrix kronecker product\n        x = tf.expand_dims(x, -1)       # bchwx1\n        mat = tf.expand_dims(unpool_mat, 0)  # 1xshxsw\n        ret = tf.tensordot(x, mat, axes=1)  # bxcxhxwxshxsw\n\n        if data_format == \'NHWC\':\n            ret = tf.transpose(ret, [0, 2, 4, 3, 5, 1])\n        else:\n            ret = tf.transpose(ret, [0, 1, 2, 4, 3, 5])\n\n        shape3_dyn = [output_shape.get_dynamic(k) for k in range(1, 4)]\n        ret = tf.reshape(ret, tf.stack([-1] + shape3_dyn))\n\n    ret.set_shape(tf.TensorShape(output_shape.get_static()))\n    return ret\n'"
tensorpack/models/registry.py,3,"b'# -*- coding: utf-8 -*-\n# File: registry.py\n\n\nimport copy\nimport re\nimport collections\nfrom functools import wraps\nimport six\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..tfutils.argscope import get_arg_scope\nfrom ..tfutils.model_utils import get_shape_str\nfrom ..utils import logger\n\n# make sure each layer is only logged once\n_LAYER_LOGGED = set()\n_LAYER_REGISTRY = {}\n\n__all__ = [\'layer_register\', \'disable_layer_logging\']\n\n\n_NameConflict = ""LAYER_NAME_CONFLICT!!""\n\n\ndef _register(name, func):\n    if name in _LAYER_REGISTRY:\n        _LAYER_REGISTRY[name] = _NameConflict\n        return\n    if name in [\'tf\']:\n        raise ValueError(logger.error(""A layer cannot be named {}"".format(name)))\n    _LAYER_REGISTRY[name] = func\n\n    # handle alias\n    if name == \'Conv2DTranspose\':\n        _register(\'Deconv2D\', func)\n\n\ndef get_registered_layer(name):\n    """"""\n    Args:\n        name (str): the name of the layer, e.g. \'Conv2D\'\n    Returns:\n        the wrapped layer function, or None if not registered.\n    """"""\n    ret = _LAYER_REGISTRY.get(name, None)\n    if ret == _NameConflict:\n        raise KeyError(""Layer named \'{}\' is registered with `@layer_register` more than once!"".format(name))\n    return ret\n\n\ndef disable_layer_logging():\n    """"""\n    Disable the shape logging for all layers from this moment on. Can be\n    useful when creating multiple towers.\n    """"""\n    class ContainEverything:\n        def __contains__(self, x):\n            return True\n    # can use nonlocal in python3, but how\n    globals()[\'_LAYER_LOGGED\'] = ContainEverything()\n\n\nclass LayerShapeLogger():\n    """"""\n    A class that logs shapes of inputs/outputs of layers,\n    during the possibly-nested calls to them.\n    """"""\n    def __init__(self):\n        self.stack = collections.deque()\n        self.depth = 0\n\n    def _indent(self):\n        return "" "" * (self.depth * 2)\n\n    def push_inputs(self, name, message):\n        while len(self.stack):\n            item = self.stack.pop()\n            logger.info(self._indent() + ""\'{}\' input: {}"".format(item[0], item[1]))\n            self.depth += 1\n\n        self.stack.append((name, message))\n\n    def push_outputs(self, name, message):\n        if len(self.stack):\n            assert len(self.stack) == 1, self.stack\n            assert self.stack[-1][0] == name, self.stack\n            item = self.stack.pop()\n            logger.info(self._indent() + ""\'{}\': {} --> {}"".format(name, item[1], message))\n        else:\n            self.depth -= 1\n            logger.info(self._indent() + ""\'{}\' output: {}"".format(name, message))\n\n\n_SHAPE_LOGGER = LayerShapeLogger()\n\n\ndef layer_register(\n        log_shape=False,\n        use_scope=True):\n    """"""\n    Args:\n        log_shape (bool): log input/output shape of this layer\n        use_scope (bool or None):\n            Whether to call this layer with an extra first argument as variable scope.\n            When set to None, it can be called either with or without\n            the scope name argument, depend on whether the first argument\n            is string or not.\n\n    Returns:\n        A decorator used to register a layer.\n\n    Example:\n\n    .. code-block:: python\n\n        @layer_register(use_scope=True)\n        def add10(x):\n            return x + tf.get_variable(\'W\', shape=[10])\n\n        # use it:\n        output = add10(\'layer_name\', input)  # the function will be called under variable scope ""layer_name"".\n    """"""\n\n    def wrapper(func):\n        @wraps(func)\n        def wrapped_func(*args, **kwargs):\n            assert args[0] is not None, args\n            if use_scope:\n                name, inputs = args[0], args[1]\n                args = args[1:]  # actual positional args used to call func\n                assert isinstance(name, six.string_types), ""First argument for \\""{}\\"" should be a string. "".format(\n                    func.__name__) + ""Did you forget to specify the name of the layer?""\n            else:\n                assert not log_shape\n                if isinstance(args[0], six.string_types):\n                    if use_scope is False:\n                        logger.warn(\n                            ""Please call layer {} without the first scope name argument, ""\n                            ""or register the layer with use_scope=None to allow calling it ""\n                            ""with scope names."".format(func.__name__))\n                    name, inputs = args[0], args[1]\n                    args = args[1:]  # actual positional args used to call func\n                else:\n                    inputs = args[0]\n                    name = None\n            if not (isinstance(inputs, (tf.Tensor, tf.Variable)) or\n                    (isinstance(inputs, (list, tuple)) and\n                        isinstance(inputs[0], (tf.Tensor, tf.Variable)))):\n                raise ValueError(""Invalid inputs to layer: "" + str(inputs))\n\n            # use kwargs from current argument scope\n            actual_args = copy.copy(get_arg_scope()[func.__name__])\n            # explicit kwargs overwrite argscope\n            actual_args.update(kwargs)\n            # if six.PY3:\n            #     # explicit positional args also override argscope. only work in PY3\n            #     posargmap = inspect.signature(func).bind_partial(*args).arguments\n            #     for k in six.iterkeys(posargmap):\n            #         if k in actual_args:\n            #             del actual_args[k]\n\n            if name is not None:        # use scope\n                with tfv1.variable_scope(name) as scope:\n                    # this name is only used to surpress logging, doesn\'t hurt to do some heuristics\n                    scope_name = re.sub(\'tower[0-9]+/\', \'\', scope.name)\n                    do_log_shape = log_shape and scope_name not in _LAYER_LOGGED\n                    if do_log_shape:\n                        _SHAPE_LOGGER.push_inputs(scope.name, get_shape_str(inputs))\n\n                    # run the actual function\n                    outputs = func(*args, **actual_args)\n\n                    if do_log_shape:\n                        _SHAPE_LOGGER.push_outputs(scope.name, get_shape_str(outputs))\n                        _LAYER_LOGGED.add(scope_name)\n            else:\n                # run the actual function\n                outputs = func(*args, **actual_args)\n            return outputs\n\n        wrapped_func.use_scope = use_scope\n        wrapped_func.__argscope_enabled__ = True\n        _register(func.__name__, wrapped_func)\n        return wrapped_func\n\n    return wrapper\n'"
tensorpack/models/regularize.py,23,"b'# -*- coding: utf-8 -*-\n# File: regularize.py\n\n\nimport re\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..tfutils.common import get_tf_version_tuple\nfrom ..tfutils.tower import get_current_tower_context\nfrom ..utils import logger\nfrom ..utils.argtools import graph_memoized\nfrom .common import layer_register\n\n__all__ = [\'regularize_cost\', \'regularize_cost_from_collection\',\n           \'l2_regularizer\', \'l1_regularizer\', \'Dropout\']\n\n\n@graph_memoized\ndef _log_once(msg):\n    logger.info(msg)\n\n\nif get_tf_version_tuple() <= (1, 12):\n    l2_regularizer = tf.contrib.layers.l2_regularizer  # deprecated\n    l1_regularizer = tf.contrib.layers.l1_regularizer  # deprecated\nelse:\n    # oh these little dirty details\n    l2_regularizer = lambda x: tf.keras.regularizers.l2(x * 0.5)  # noqa\n    l1_regularizer = tf.keras.regularizers.l1\n\n\ndef regularize_cost(regex, func, name=\'regularize_cost\'):\n    """"""\n    Apply a regularizer on trainable variables matching the regex, and print\n    the matched variables (only print once in multi-tower training).\n    In replicated mode, it will only regularize variables within the current tower.\n\n    If called under a TowerContext with `is_training==False`, this function returns a zero constant tensor.\n\n    Args:\n        regex (str): a regex to match variable names, e.g. ""conv.*/W""\n        func: the regularization function, which takes a tensor and returns a scalar tensor.\n            E.g., ``tf.nn.l2_loss, tf.contrib.layers.l1_regularizer(0.001)``.\n\n    Returns:\n        tf.Tensor: a scalar, the total regularization cost.\n\n    Example:\n        .. code-block:: python\n\n            cost = cost + regularize_cost(""fc.*/W"", l2_regularizer(1e-5))\n    """"""\n    assert len(regex)\n    ctx = get_current_tower_context()\n    if not ctx.is_training:\n        # Currently cannot build the wd_cost correctly at inference,\n        # because ths vs_name used in inference can be \'\', therefore the\n        # variable filter will fail\n        return tf.constant(0, dtype=tf.float32, name=\'empty_\' + name)\n\n    # If vars are shared, regularize all of them\n    # If vars are replicated, only regularize those in the current tower\n    if ctx.has_own_variables:\n        params = ctx.get_collection_in_tower(tfv1.GraphKeys.TRAINABLE_VARIABLES)\n    else:\n        params = tfv1.trainable_variables()\n\n    names = []\n\n    with tfv1.name_scope(name + \'_internals\'):\n        costs = []\n        for p in params:\n            para_name = p.op.name\n            if re.search(regex, para_name):\n                regloss = func(p)\n                assert regloss.dtype.is_floating, regloss\n                # Some variables may not be fp32, but it should\n                # be fine to assume regularization in fp32\n                if regloss.dtype != tf.float32:\n                    regloss = tf.cast(regloss, tf.float32)\n                costs.append(regloss)\n                names.append(p.name)\n        if not costs:\n            return tf.constant(0, dtype=tf.float32, name=\'empty_\' + name)\n\n    # remove tower prefix from names, and print\n    if len(ctx.vs_name):\n        prefix = ctx.vs_name + \'/\'\n        prefixlen = len(prefix)\n\n        def f(name):\n            if name.startswith(prefix):\n                return name[prefixlen:]\n            return name\n        names = list(map(f, names))\n    logger.info(""regularize_cost() found {} variables to regularize."".format(len(names)))\n    _log_once(""The following tensors will be regularized: {}"".format(\', \'.join(names)))\n\n    return tf.add_n(costs, name=name)\n\n\ndef regularize_cost_from_collection(name=\'regularize_cost\'):\n    """"""\n    Get the cost from the regularizers in ``tf.GraphKeys.REGULARIZATION_LOSSES``.\n    If in replicated mode, will only regularize variables created within the current tower.\n\n    Args:\n        name (str): the name of the returned tensor\n\n    Returns:\n        tf.Tensor: a scalar, the total regularization cost.\n    """"""\n    ctx = get_current_tower_context()\n    if not ctx.is_training:\n        # TODO Currently cannot build the wd_cost correctly at inference,\n        # because ths vs_name used in inference can be \'\', therefore the\n        # variable filter will fail\n        return tf.constant(0, dtype=tf.float32, name=\'empty_\' + name)\n\n    # NOTE: this collection doesn\'t always grow with towers.\n    # It only grows with actual variable creation, but not get_variable call.\n    if ctx.has_own_variables:   # be careful of the first tower (name=\'\')\n        losses = ctx.get_collection_in_tower(tfv1.GraphKeys.REGULARIZATION_LOSSES)\n    else:\n        losses = tfv1.get_collection(tfv1.GraphKeys.REGULARIZATION_LOSSES)\n    if len(losses) > 0:\n        logger.info(""regularize_cost_from_collection() found {} regularizers ""\n                    ""in REGULARIZATION_LOSSES collection."".format(len(losses)))\n\n        def maploss(l):\n            assert l.dtype.is_floating, l\n            if l.dtype != tf.float32:\n                l = tf.cast(l, tf.float32)\n            return l\n\n        losses = [maploss(l) for l in losses]\n        reg_loss = tf.add_n(losses, name=name)\n        return reg_loss\n    else:\n        return tf.constant(0, dtype=tf.float32, name=\'empty_\' + name)\n\n\n@layer_register(use_scope=None)\ndef Dropout(x, *args, **kwargs):\n    """"""\n    Same as `tf.layers.dropout`.\n    However, for historical reasons, the first positional argument is\n    interpreted as keep_prob rather than drop_prob.\n    Explicitly use `rate=` keyword arguments to ensure things are consistent.\n    """"""\n    if \'is_training\' in kwargs:\n        kwargs[\'training\'] = kwargs.pop(\'is_training\')\n    if len(args) > 0:\n        if args[0] != 0.5:\n            logger.warn(\n                ""The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. ""\n                ""This is different from the rate argument in tf.layers.Dropout due to historical reasons. ""\n                ""To mimic tf.layers.Dropout, explicitly use keyword argument \'rate\' instead"")\n        rate = 1 - args[0]\n    elif \'keep_prob\' in kwargs:\n        assert \'rate\' not in kwargs, ""Cannot set both keep_prob and rate!""\n        rate = 1 - kwargs.pop(\'keep_prob\')\n    elif \'rate\' in kwargs:\n        rate = kwargs.pop(\'rate\')\n    else:\n        rate = 0.5\n\n    if kwargs.get(\'training\', None) is None:\n        kwargs[\'training\'] = get_current_tower_context().is_training\n\n    if get_tf_version_tuple() <= (1, 12):\n        return tf.layers.dropout(x, rate=rate, **kwargs)\n    else:\n        return tf.nn.dropout(x, rate=rate if kwargs[\'training\'] else 0.)\n'"
tensorpack/models/shape_utils.py,3,"b'# -*- coding: utf-8 -*-\n# File: shape_utils.py\n\nimport tensorflow as tf\n\n__all__ = []\n\n\nclass StaticDynamicAxis(object):\n    def __init__(self, static, dynamic):\n        self.static = static\n        self.dynamic = dynamic\n\n    def apply(self, f):\n        try:\n            st = f(self.static)\n            return StaticDynamicAxis(st, st)\n        except TypeError:\n            return StaticDynamicAxis(None, f(self.dynamic))\n\n    def __str__(self):\n        return ""S={}, D={}"".format(str(self.static), str(self.dynamic))\n\n\ndef DynamicLazyAxis(shape, idx):\n    return lambda: shape[idx]\n\n\ndef StaticLazyAxis(dim):\n    return lambda: dim\n\n\nclass StaticDynamicShape(object):\n    def __init__(self, tensor):\n        assert isinstance(tensor, tf.Tensor), tensor\n        ndims = tensor.shape.ndims\n        self.static = tensor.shape.as_list()\n        if tensor.shape.is_fully_defined():\n            self.dynamic = self.static[:]\n        else:\n            dynamic = tf.shape(tensor)\n            self.dynamic = [DynamicLazyAxis(dynamic, k) for k in range(ndims)]\n\n        for k in range(ndims):\n            if self.static[k] is not None:\n                self.dynamic[k] = StaticLazyAxis(self.static[k])\n\n    def apply(self, axis, f):\n        if self.static[axis] is not None:\n            try:\n                st = f(self.static[axis])\n                self.static[axis] = st\n                self.dynamic[axis] = StaticLazyAxis(st)\n                return\n            except TypeError:\n                pass\n        self.static[axis] = None\n        dyn = self.dynamic[axis]\n        self.dynamic[axis] = lambda: f(dyn())\n\n    def get_static(self):\n        return self.static\n\n    @property\n    def ndims(self):\n        return len(self.static)\n\n    def get_dynamic(self, axis=None):\n        if axis is None:\n            return [self.dynamic[k]() for k in range(self.ndims)]\n        return self.dynamic[axis]()\n\n\nif __name__ == \'__main__\':\n    x = tf.placeholder(tf.float32, shape=[None, 3, None, 10])\n    shape = StaticDynamicShape(x)\n    shape.apply(1, lambda x: x * 3)\n    shape.apply(2, lambda x: x + 5)\n    print(shape.get_static())\n    print(shape.get_dynamic())\n'"
tensorpack/models/shapes.py,5,"b'# -*- coding: utf-8 -*-\n# File: shapes.py\n\n\nimport tensorflow as tf\n\nfrom .common import layer_register\n\n__all__ = [\'ConcatWith\']\n\n\n@layer_register(use_scope=None)\ndef ConcatWith(x, tensor, dim):\n    """"""\n    A wrapper around ``tf.concat`` to cooperate with :class:`LinearWrap`.\n\n    Args:\n        x (tf.Tensor): input\n        tensor (list[tf.Tensor]): a tensor or list of tensors to concatenate with x.\n            x will be at the beginning\n        dim (int): the dimension along which to concatenate\n\n    Returns:\n        tf.Tensor: ``tf.concat([x] + tensor, dim)``\n    """"""\n    if type(tensor) != list:\n        tensor = [tensor]\n    return tf.concat([x] + tensor, dim)\n'"
tensorpack/models/tflayer.py,11,"b'# -*- coding: utf-8 -*-\n# File: tflayer.py\n\nimport functools\nimport six\nimport tensorflow as tf\n\nfrom ..tfutils.common import get_tf_version_tuple\nfrom ..tfutils.varreplace import custom_getter_scope\nfrom ..utils.argtools import get_data_format\n\n__all__ = []\n\n\ndef map_common_tfargs(kwargs):\n    df = kwargs.pop(\'data_format\', None)\n    if df is not None:\n        df = get_data_format(df, keras_mode=True)\n        kwargs[\'data_format\'] = df\n\n    old_nl = kwargs.pop(\'nl\', None)\n    if old_nl is not None:\n        kwargs[\'activation\'] = lambda x, name=None: old_nl(x, name=name)\n\n    if \'W_init\' in kwargs:\n        kwargs[\'kernel_initializer\'] = kwargs.pop(\'W_init\')\n\n    if \'b_init\' in kwargs:\n        kwargs[\'bias_initializer\'] = kwargs.pop(\'b_init\')\n    return kwargs\n\n\ndef convert_to_tflayer_args(args_names, name_mapping):\n    """"""\n    After applying this decorator:\n    1. data_format becomes tf.layers style\n    2. nl becomes activation\n    3. initializers are renamed\n    4. positional args are transformed to corresponding kwargs, according to args_names\n    5. kwargs are mapped to tf.layers names if needed, by name_mapping\n    """"""\n\n    def decorator(func):\n        @functools.wraps(func)\n        def decorated_func(inputs, *args, **kwargs):\n            kwargs = map_common_tfargs(kwargs)\n\n            posarg_dic = {}\n            assert len(args) <= len(args_names), \\\n                ""Please use kwargs instead of positional args to call this model, "" \\\n                ""except for the following arguments: {}"".format(\', \'.join(args_names))\n            for pos_arg, name in zip(args, args_names):\n                posarg_dic[name] = pos_arg\n\n            ret = {}\n            for name, arg in six.iteritems(kwargs):\n                newname = name_mapping.get(name, None)\n                if newname is not None:\n                    assert newname not in kwargs, \\\n                        ""Argument {} and {} conflicts!"".format(name, newname)\n                else:\n                    newname = name\n                ret[newname] = arg\n            ret.update(posarg_dic)  # Let pos arg overwrite kw arg, for argscope to work\n\n            return func(inputs, **ret)\n\n        return decorated_func\n\n    return decorator\n\n\ndef rename_get_variable(mapping):\n    """"""\n    Args:\n        mapping(dict): an old -> new mapping for variable basename. e.g. {\'kernel\': \'W\'}\n\n    Returns:\n        A context where the variables are renamed.\n    """"""\n    def custom_getter(getter, name, *args, **kwargs):\n        splits = name.split(\'/\')\n        basename = splits[-1]\n        if basename in mapping:\n            basename = mapping[basename]\n            splits[-1] = basename\n            name = \'/\'.join(splits)\n        return getter(name, *args, **kwargs)\n    return custom_getter_scope(custom_getter)\n\n\ndef rename_tflayer_get_variable():\n    """"""\n    Rename all :func:`tf.get_variable` with rules that transforms tflayer style to tensorpack style.\n\n    Returns:\n        A context where the variables are renamed.\n\n    Example:\n\n    .. code-block:: python\n\n        with rename_tflayer_get_variable():\n            x = tf.layer.conv2d(input, 3, 3, name=\'conv0\')\n            # variables will be named \'conv0/W\', \'conv0/b\'\n    """"""\n    mapping = {\n        \'kernel\': \'W\',\n        \'bias\': \'b\',\n        \'moving_mean\': \'mean/EMA\',\n        \'moving_variance\': \'variance/EMA\',\n    }\n    return rename_get_variable(mapping)\n\n\ndef monkeypatch_tf_layers():\n    if get_tf_version_tuple() < (1, 4):\n        if not hasattr(tf.layers, \'Dense\'):\n            from tensorflow.python.layers.core import Dense\n            tf.layers.Dense = Dense\n\n            from tensorflow.python.layers.normalization import BatchNormalization\n            tf.layers.BatchNormalization = BatchNormalization\n\n            from tensorflow.python.layers.convolutional import Conv2DTranspose, Conv2D\n            tf.layers.Conv2DTranspose = Conv2DTranspose\n            tf.layers.Conv2D = Conv2D\n\n            from tensorflow.python.layers.pooling import MaxPooling2D, AveragePooling2D\n            tf.layers.MaxPooling2D = MaxPooling2D\n            tf.layers.AveragePooling2D = AveragePooling2D\n\n\nmonkeypatch_tf_layers()\n'"
tensorpack/models/utils.py,0,"b'# -*- coding: utf-8 -*-\n# File: utils.py\n\nimport six\n\n\nclass VariableHolder(object):\n    """""" A proxy to access variables defined in a layer. """"""\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n            kwargs: {name:variable}\n        """"""\n        self._vars = {}\n        for k, v in six.iteritems(kwargs):\n            self._add_variable(k, v)\n\n    def _add_variable(self, name, var):\n        assert name not in self._vars\n        self._vars[name] = var\n\n    def __setattr__(self, name, var):\n        if not name.startswith(\'_\'):\n            self._add_variable(name, var)\n        else:\n            # private attributes\n            super(VariableHolder, self).__setattr__(name, var)\n\n    def __getattr__(self, name):\n        return self._vars[name]\n\n    def all(self):\n        """"""\n        Returns:\n            list of all variables\n        """"""\n        return list(six.itervalues(self._vars))\n\n\ntry:\n    # When BN is used as an activation, keras layers try to autograph.convert it\n    # This leads to massive warnings so we disable it.\n    from tensorflow.python.autograph.impl.api import do_not_convert as disable_autograph\nexcept ImportError:\n    def disable_autograph():\n        return lambda x: x\n'"
tensorpack/predict/__init__.py,0,"b""#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()['kcah_acitats'[::-1].upper()] = False\nif STATICA_HACK:\n    from .base import *\n    from .concurrency import *\n    from .config import *\n    from .dataset import *\n    from .multigpu import *\n\n\nfrom pkgutil import iter_modules\nimport os\nimport os.path\n\n__all__ = []\n\n\ndef global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if '__all__' in dir(p) else dir(p)\n    if lst:\n        del globals()[name]\n        for k in lst:\n            globals()[k] = p.__dict__[k]\n            __all__.append(k)\n\n\n_CURR_DIR = os.path.dirname(__file__)\nfor _, module_name, _ in iter_modules(\n        [_CURR_DIR]):\n    srcpath = os.path.join(_CURR_DIR, module_name + '.py')\n    if not os.path.isfile(srcpath):\n        continue\n    if module_name.startswith('_'):\n        continue\n    global_import(module_name)\n"""
tensorpack/predict/base.py,5,"b'# -*- coding: utf-8 -*-\n# File: base.py\n\n\nfrom abc import ABCMeta, abstractmethod\nimport six\nimport tensorflow as tf\n\nfrom ..input_source import PlaceholderInput\nfrom ..tfutils.common import get_tensors_by_names, get_op_tensor_name\nfrom ..tfutils.tower import PredictTowerContext\n\n__all__ = [\'PredictorBase\',\n           \'OnlinePredictor\', \'OfflinePredictor\']\n\n\n@six.add_metaclass(ABCMeta)\nclass PredictorBase(object):\n    """"""\n    Base class for all predictors.\n\n    Attributes:\n        return_input (bool): whether the call will also return (inputs, outputs)\n            or just outputs\n    """"""\n\n    def __call__(self, *dp):\n        """"""\n        Call the predictor on some inputs.\n\n        Example:\n            When you have a predictor defined with two inputs, call it with:\n\n            .. code-block:: python\n\n                predictor(e1, e2)\n\n        Returns:\n            list[array]: list of outputs\n        """"""\n        output = self._do_call(dp)\n        if self.return_input:\n            return (dp, output)\n        else:\n            return output\n\n    @abstractmethod\n    def _do_call(self, dp):\n        """"""\n        Args:\n            dp: input datapoint.  must have the same length as input_names\n        Returns:\n            output as defined by the config\n        """"""\n\n\nclass AsyncPredictorBase(PredictorBase):\n    """""" Base class for all async predictors. """"""\n\n    @abstractmethod\n    def put_task(self, dp, callback=None):\n        """"""\n        Args:\n            dp (list): A datapoint as inputs. It could be either batched or not\n                batched depending on the predictor implementation).\n            callback: a thread-safe callback to get called with\n                either outputs or (inputs, outputs), if `return_input` is True.\n        Returns:\n            concurrent.futures.Future: a Future of results\n        """"""\n\n    @abstractmethod\n    def start(self):\n        """""" Start workers """"""\n\n    def _do_call(self, dp):\n        fut = self.put_task(dp)\n        # in Tornado, Future.result() doesn\'t wait\n        return fut.result()\n\n\nclass OnlinePredictor(PredictorBase):\n    """"""\n    A predictor which directly use an existing session and given tensors.\n\n    Attributes:\n        sess: The tf.Session object associated with this predictor.\n    """"""\n\n    ACCEPT_OPTIONS = False\n    """""" See Session.make_callable """"""\n\n    def __init__(self, input_tensors, output_tensors,\n                 return_input=False, sess=None):\n        """"""\n        Args:\n            input_tensors (list): list of names.\n            output_tensors (list): list of names.\n            return_input (bool): same as :attr:`PredictorBase.return_input`.\n            sess (tf.Session): the session this predictor runs in. If None,\n                will use the default session at the first call.\n                Note that in TensorFlow, default session is thread-local.\n        """"""\n        def normalize_name(t):\n            if isinstance(t, six.string_types):\n                return get_op_tensor_name(t)[1]\n            return t\n\n        self.return_input = return_input\n        self.input_tensors = [normalize_name(x) for x in input_tensors]\n        self.output_tensors = [normalize_name(x) for x in output_tensors]\n        self.sess = sess\n\n        if sess is not None:\n            self._callable = sess.make_callable(\n                fetches=output_tensors,\n                feed_list=input_tensors,\n                accept_options=self.ACCEPT_OPTIONS)\n        else:\n            self._callable = None\n\n    def _do_call(self, dp):\n        assert len(dp) == len(self.input_tensors), \\\n            ""{} != {}"".format(len(dp), len(self.input_tensors))\n        if self.sess is None:\n            self.sess = tf.get_default_session()\n            assert self.sess is not None, ""Predictor isn\'t called under a default session!""\n\n        if self._callable is None:\n            self._callable = self.sess.make_callable(\n                fetches=self.output_tensors,\n                feed_list=self.input_tensors,\n                accept_options=self.ACCEPT_OPTIONS)\n        # run_metadata = tf.RunMetadata()\n        # options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        return self._callable(*dp)\n\n\nclass OfflinePredictor(OnlinePredictor):\n    """""" A predictor built from a given config.\n        A single-tower model will be built without any prefix.\n\n        Example:\n\n        .. code-block:: python\n\n            config = PredictConfig(model=my_model,\n                                   inputs_names=[\'image\'],\n                                   output_names=[\'linear/output\', \'prediction\'])\n            predictor = OfflinePredictor(config)\n            batch_image = np.random.rand(1, 100, 100, 3)\n            batch_output, batch_prediction = predictor(batch_image)\n    """"""\n\n    def __init__(self, config):\n        """"""\n        Args:\n            config (PredictConfig): the config to use.\n        """"""\n        self.graph = config._maybe_create_graph()\n        with self.graph.as_default():\n            input = PlaceholderInput()\n            input.setup(config.input_signature)\n            with PredictTowerContext(\'\'):\n                config.tower_func(*input.get_input_tensors())\n\n            input_tensors = get_tensors_by_names(config.input_names)\n            output_tensors = get_tensors_by_names(config.output_names)\n\n            config.session_init._setup_graph()\n            sess = config.session_creator.create_session()\n            config.session_init._run_init(sess)\n            super(OfflinePredictor, self).__init__(\n                input_tensors, output_tensors, config.return_input, sess)\n'"
tensorpack/predict/concurrency.py,1,"b'# -*- coding: utf-8 -*-\n# File: concurrency.py\n\n\nimport multiprocessing\nimport numpy as np\nfrom concurrent.futures import Future\nimport tensorflow as tf\nfrom six.moves import queue, range\n\nfrom ..compat import tfv1\nfrom ..tfutils.model_utils import describe_trainable_vars\nfrom ..utils import logger\nfrom ..utils.concurrency import DIE, ShareSessionThread, StoppableThread\nfrom .base import AsyncPredictorBase, OfflinePredictor, OnlinePredictor\n\n__all__ = [\'MultiThreadAsyncPredictor\']\n\n\nclass MultiProcessPredictWorker(multiprocessing.Process):\n    """""" Base class for predict worker that runs offline in multiprocess""""""\n\n    def __init__(self, idx, config):\n        """"""\n        Args:\n            idx (int): index of the worker. the 0th worker will print log.\n            config (PredictConfig): the config to use.\n        """"""\n        super(MultiProcessPredictWorker, self).__init__()\n        self.name = ""MultiProcessPredictWorker-{}"".format(idx)\n        self.idx = idx\n        self.config = config\n\n    def _init_runtime(self):\n        """""" Call _init_runtime under different CUDA_VISIBLE_DEVICES, you\'ll\n            have workers that run on multiGPUs\n        """"""\n        if self.idx != 0:\n            from tensorpack.models.registry import disable_layer_logging\n            disable_layer_logging()\n        self.predictor = OfflinePredictor(self.config)\n        if self.idx == 0:\n            with self.predictor.graph.as_default():\n                describe_trainable_vars()\n\n\nclass MultiProcessQueuePredictWorker(MultiProcessPredictWorker):\n    """"""\n    An offline predictor worker that takes input and produces output by queue.\n    Each process will exit when they see :class:`DIE`.\n    """"""\n\n    def __init__(self, idx, inqueue, outqueue, config):\n        """"""\n        Args:\n            idx, config: same as in :class:`MultiProcessPredictWorker`.\n            inqueue (multiprocessing.Queue): input queue to get data point. elements are (task_id, dp)\n            outqueue (multiprocessing.Queue): output queue to put result. elements are (task_id, output)\n        """"""\n        super(MultiProcessQueuePredictWorker, self).__init__(idx, config)\n        self.inqueue = inqueue\n        self.outqueue = outqueue\n        assert isinstance(self.inqueue, multiprocessing.queues.Queue)\n        assert isinstance(self.outqueue, multiprocessing.queues.Queue)\n\n    def run(self):\n        self._init_runtime()\n        while True:\n            tid, dp = self.inqueue.get()\n            if tid == DIE:\n                self.outqueue.put((DIE, None))\n                return\n            else:\n                self.outqueue.put((tid, self.predictor(*dp)))\n\n\nclass PredictorWorkerThread(StoppableThread, ShareSessionThread):\n    def __init__(self, queue, pred_func, id, batch_size=5):\n        super(PredictorWorkerThread, self).__init__()\n        self.name = ""PredictorWorkerThread-{}"".format(id)\n        self.queue = queue\n        self.func = pred_func\n        self.daemon = True\n        self.batch_size = batch_size\n        self.id = id\n\n    def run(self):\n        with self.default_sess():\n            while not self.stopped():\n                batched, futures = self.fetch_batch()\n                try:\n                    outputs = self.func(*batched)\n                except tf.errors.CancelledError:\n                    for f in futures:\n                        f.cancel()\n                    logger.warn(""In PredictorWorkerThread id={}, call was cancelled."".format(self.id))\n                    return\n                # print ""Worker {} batched {} Queue {}"".format(\n                #         self.id, len(futures), self.queue.qsize())\n                #  debug, for speed testing\n                # if not hasattr(self, \'xxx\'):\n                    # self.xxx = outputs = self.func(batched)\n                # else:\n                    # outputs = [[self.xxx[0][0]] * len(batched[0]), [self.xxx[1][0]] * len(batched[0])]\n\n                for idx, f in enumerate(futures):\n                    f.set_result([k[idx] for k in outputs])\n\n    def fetch_batch(self):\n        """""" Fetch a batch of data without waiting""""""\n        inp, f = self.queue.get()\n        nr_input_var = len(inp)\n        batched, futures = [[] for _ in range(nr_input_var)], []\n        for k in range(nr_input_var):\n            batched[k].append(inp[k])\n        futures.append(f)\n        while len(futures) < self.batch_size:\n            try:\n                inp, f = self.queue.get_nowait()\n                for k in range(nr_input_var):\n                    batched[k].append(inp[k])\n                futures.append(f)\n            except queue.Empty:\n                break   # do not wait\n\n        for k in range(nr_input_var):\n            batched[k] = np.asarray(batched[k])\n        return batched, futures\n\n\nclass MultiThreadAsyncPredictor(AsyncPredictorBase):\n    """"""\n    An multithreaded online async predictor which runs a list of OnlinePredictor.\n    It would do an extra batching internally.\n    """"""\n\n    def __init__(self, predictors, batch_size=5):\n        """"""\n        Args:\n            predictors (list): a list of OnlinePredictor available to use.\n            batch_size (int): the maximum of an internal batch.\n        """"""\n        assert len(predictors)\n        self._need_default_sess = False\n        for k in predictors:\n            assert isinstance(k, OnlinePredictor), type(k)\n            if k.sess is None:\n                self._need_default_sess = True\n            # TODO support predictors.return_input here\n            assert not k.return_input\n        self.input_queue = queue.Queue(maxsize=len(predictors) * 100)\n        self.threads = [\n            PredictorWorkerThread(\n                self.input_queue, f, id, batch_size=batch_size)\n            for id, f in enumerate(predictors)]\n\n    def start(self):\n        if self._need_default_sess:\n            assert tfv1.get_default_session() is not None, \\\n                ""Not session is bind to predictors, "" \\\n                ""MultiThreadAsyncPredictor.start() has to be called under a default session!""\n        for t in self.threads:\n            t.start()\n\n    def put_task(self, dp, callback=None):\n        """"""\n        Args:\n            dp (list): A datapoint as inputs. It could be either batched or not\n                batched depending on the predictor implementation).\n            callback: a thread-safe callback. When the results are ready, it will be called\n                with the ""future"" object.\n        Returns:\n            concurrent.futures.Future: a Future of results.\n        """"""\n        f = Future()\n        if callback is not None:\n            f.add_done_callback(callback)\n        self.input_queue.put((dp, f))\n        return f\n'"
tensorpack/predict/config.py,4,"b'# -*- coding: utf-8 -*-\n# File: config.py\n\n\nimport six\nfrom ..compat import tfv1 as tf\n\nfrom ..train.model_desc import ModelDescBase\nfrom ..tfutils.sessinit import JustCurrentSession, SessionInit\nfrom ..tfutils.sesscreate import NewSessionCreator\nfrom ..tfutils.tower import TowerFunc\nfrom ..utils import logger\n\n__all__ = [\'PredictConfig\']\n\n\nclass PredictConfig(object):\n    def __init__(self,\n                 model=None,\n                 tower_func=None,\n                 input_signature=None,\n\n                 input_names=None,\n                 output_names=None,\n\n                 session_creator=None,\n                 session_init=None,\n                 return_input=False,\n                 create_graph=True,\n                 ):\n        """"""\n        Users need to provide enough arguments to create a tower function,\n        which will be used to construct the graph.\n        This can be provided in the following ways:\n\n        1. `model`: a :class:`ModelDesc` instance. It will contain a tower function by itself.\n        2. `tower_func`: a :class:`tfutils.TowerFunc` instance.\n            Provide a tower function instance directly.\n        3. `tower_func`: a symbolic function and `input_signature`: the signature of the function.\n            Provide both a function and its signature.\n\n        Example:\n\n        .. code-block:: python\n\n            config = PredictConfig(model=my_model,\n                                   inputs_names=[\'image\'],\n                                   output_names=[\'linear/output\', \'prediction\'])\n\n        Args:\n            model (ModelDescBase): to be used to construct a tower function.\n            tower_func: a callable which takes input tensors (by positional args) and construct a tower.\n                or a :class:`tfutils.TowerFunc` instance.\n            input_signature ([tf.TensorSpec]): if tower_func is a plain function (instead of a TowerFunc),\n                this describes the list of inputs it takes.\n\n            input_names (list): a list of input tensor names. Defaults to match input_signature.\n                The name can be either the name of a tensor, or the name of one input of the tower.\n            output_names (list): a list of names of the output tensors to predict, the\n                tensors can be any tensor in the graph that\'s computable from the tensors correponding to `input_names`.\n\n            session_creator (tf.train.SessionCreator): how to create the\n                session. Defaults to :class:`NewSessionCreator()`.\n            session_init (SessionInit): how to initialize variables of the session.\n                Defaults to do nothing.\n\n            return_input (bool): same as in :attr:`PredictorBase.return_input`.\n            create_graph (bool): create a new graph, or use the default graph\n                when predictor is first initialized.\n        """"""\n        def assert_type(v, tp, name):\n            assert isinstance(v, tp), \\\n                ""Argument \'{}\' has to be type \'{}\', but an object of type \'{}\' found."".format(\n                    name, tp.__name__, v.__class__.__name__)\n\n        if model is not None:\n            assert_type(model, ModelDescBase, \'model\')\n            assert input_signature is None and tower_func is None\n            self.input_signature = model.get_input_signature()\n            self.tower_func = TowerFunc(model.build_graph, self.input_signature)\n        else:\n            if isinstance(tower_func, TowerFunc):\n                input_signature = tower_func.input_signature\n            assert input_signature is not None and tower_func is not None\n            self.input_signature = input_signature\n            self.tower_func = TowerFunc(tower_func, input_signature)\n\n        if session_init is None:\n            session_init = JustCurrentSession()\n        self.session_init = session_init\n        assert_type(self.session_init, SessionInit, \'session_init\')\n\n        if session_creator is None:\n            self.session_creator = NewSessionCreator()\n        else:\n            self.session_creator = session_creator\n\n        # inputs & outputs\n        self.input_names = input_names\n        if self.input_names is None:\n            self.input_names = [k.name for k in self.input_signature]\n        assert output_names is not None, ""Argument \'output_names\' is not provided!""\n        self.output_names = output_names\n        assert_type(self.output_names, list, \'output_names\')\n        assert_type(self.input_names, list, \'input_names\')\n        if len(self.input_names) == 0:\n            logger.warn(\'PredictConfig receives empty ""input_names"".\')\n        for v in self.input_names:\n            assert_type(v, six.string_types, \'Each item in input_names\')\n        assert len(self.output_names), ""Argument \'output_names\' cannot be empty!""\n\n        self.return_input = bool(return_input)\n        self.create_graph = bool(create_graph)\n\n    def _maybe_create_graph(self):\n        if self.create_graph:\n            return tf.Graph()\n        return tf.get_default_graph()\n'"
tensorpack/predict/dataset.py,0,"b'# -*- coding: utf-8 -*-\n# File: dataset.py\n\n\nimport multiprocessing\nimport os\nfrom abc import ABCMeta, abstractmethod\nimport six\n\nfrom ..dataflow import DataFlow\nfrom ..dataflow.remote import dump_dataflow_to_process_queue\nfrom ..utils import logger\nfrom ..utils.develop import HIDE_DOC\nfrom ..utils.concurrency import DIE, OrderedResultGatherProc, ensure_proc_terminate\nfrom ..utils.gpu import change_gpu, get_num_gpu\nfrom ..utils.utils import get_tqdm\nfrom .base import OfflinePredictor\nfrom .concurrency import MultiProcessQueuePredictWorker\nfrom .config import PredictConfig\n\n__all__ = [\'DatasetPredictorBase\', \'SimpleDatasetPredictor\',\n           \'MultiProcessDatasetPredictor\']\n\n\n@six.add_metaclass(ABCMeta)\nclass DatasetPredictorBase(object):\n    """""" Base class for dataset predictors.\n        These are predictors which run over a :class:`DataFlow`.\n    """"""\n\n    def __init__(self, config, dataset):\n        """"""\n        Args:\n            config (PredictConfig): the config of predictor.\n            dataset (DataFlow): the DataFlow to run on.\n        """"""\n        assert isinstance(dataset, DataFlow)\n        assert isinstance(config, PredictConfig)\n        self.config = config\n        self.dataset = dataset\n\n    @abstractmethod\n    def get_result(self):\n        """"""\n        Yields:\n            output for each datapoint in the DataFlow.\n        """"""\n        pass\n\n    def get_all_result(self):\n        """"""\n        Returns:\n            list: all outputs for all datapoints in the DataFlow.\n        """"""\n        return list(self.get_result())\n\n\nclass SimpleDatasetPredictor(DatasetPredictorBase):\n    """"""\n    Simply create one predictor and run it on the DataFlow.\n    """"""\n    def __init__(self, config, dataset):\n        super(SimpleDatasetPredictor, self).__init__(config, dataset)\n        self.predictor = OfflinePredictor(config)\n\n    @HIDE_DOC\n    def get_result(self):\n        self.dataset.reset_state()\n        try:\n            sz = len(self.dataset)\n        except NotImplementedError:\n            sz = 0\n        with get_tqdm(total=sz, disable=(sz == 0)) as pbar:\n            for dp in self.dataset:\n                res = self.predictor(*dp)\n                yield res\n                pbar.update()\n\n\nclass MultiProcessDatasetPredictor(DatasetPredictorBase):\n    """"""\n    Run prediction in multiple processes, on either CPU or GPU.\n    Each process fetch datapoints as tasks and run predictions independently.\n    """"""\n    # TODO allow unordered\n\n    def __init__(self, config, dataset, nr_proc, use_gpu=True, ordered=True):\n        """"""\n        Args:\n            config: same as in :class:`DatasetPredictorBase`.\n            dataset: same as in :class:`DatasetPredictorBase`.\n            nr_proc (int): number of processes to use\n            use_gpu (bool): use GPU or CPU.\n                If GPU, then ``nr_proc`` cannot be more than what\'s in\n                CUDA_VISIBLE_DEVICES.\n            ordered (bool): produce outputs in the original order of the\n                datapoints. This will be a bit slower. Otherwise, :meth:`get_result` will produce\n                outputs in any order.\n        """"""\n        if config.return_input:\n            logger.warn(""Using the option `return_input` in MultiProcessDatasetPredictor might be slow"")\n        assert nr_proc >= 1, nr_proc\n        super(MultiProcessDatasetPredictor, self).__init__(config, dataset)\n\n        self.nr_proc = nr_proc\n        self.ordered = ordered\n\n        self.inqueue, self.inqueue_proc = dump_dataflow_to_process_queue(\n            self.dataset, nr_proc * 2, self.nr_proc)    # put (idx, dp) to inqueue\n\n        if use_gpu:\n            try:\n                gpus = os.environ[\'CUDA_VISIBLE_DEVICES\'].split(\',\')\n            except KeyError:\n                gpus = list(range(get_num_gpu()))\n            assert len(gpus) >= self.nr_proc, \\\n                ""nr_proc={} while only {} gpus available"".format(\n                self.nr_proc, len(gpus))\n        else:\n            gpus = [\'-1\'] * self.nr_proc\n        # worker produces (idx, result) to outqueue\n        self.outqueue = multiprocessing.Queue()\n        self.workers = [MultiProcessQueuePredictWorker(\n            i, self.inqueue, self.outqueue, self.config)\n            for i in range(self.nr_proc)]\n\n        # start inqueue and workers\n        self.inqueue_proc.start()\n        for p, gpuid in zip(self.workers, gpus):\n            if gpuid == \'-1\':\n                logger.info(""Worker {} uses CPU"".format(p.idx))\n            else:\n                logger.info(""Worker {} uses GPU {}"".format(p.idx, gpuid))\n            with change_gpu(gpuid):\n                p.start()\n\n        if ordered:\n            self.result_queue = OrderedResultGatherProc(\n                self.outqueue, nr_producer=self.nr_proc)\n            self.result_queue.start()\n            ensure_proc_terminate(self.result_queue)\n        else:\n            self.result_queue = self.outqueue\n        ensure_proc_terminate(self.workers + [self.inqueue_proc])\n\n    @HIDE_DOC\n    def get_result(self):\n        try:\n            sz = len(self.dataset)\n        except NotImplementedError:\n            sz = 0\n        with get_tqdm(total=sz, disable=(sz == 0)) as pbar:\n            die_cnt = 0\n            while True:\n                res = self.result_queue.get()\n                pbar.update()\n                if res[0] != DIE:\n                    yield res[1]\n                else:\n                    die_cnt += 1\n                    if die_cnt == self.nr_proc:\n                        break\n        self.inqueue_proc.join()\n        self.inqueue_proc.terminate()\n        if self.ordered:    # if ordered, than result_queue is a Process\n            self.result_queue.join()\n            self.result_queue.terminate()\n        for p in self.workers:\n            p.join()\n            p.terminate()\n'"
tensorpack/predict/feedfree.py,1,"b'#!/usr/bin/env python\n\nfrom tensorflow.python.training.monitored_session import _HookedSession as HookedSession\n\nfrom ..callbacks import Callbacks\nfrom ..tfutils.tower import PredictTowerContext\nfrom .base import PredictorBase\n\n__all__ = [\'FeedfreePredictor\']\n\n\nclass FeedfreePredictor(PredictorBase):\n    """"""\n    Create a predictor that takes inputs from an :class:`InputSource`, instead of from feeds.\n    An instance `pred` of :class:`FeedfreePredictor` can be called only by `pred()`, which returns\n    a list of output values as defined in config.output_names.\n    """"""\n\n    def __init__(self, config, input_source):\n        """"""\n        Args:\n            config (PredictConfig): the config to use.\n            input_source (InputSource): the feedfree InputSource to use.\n                Must match the signature of the tower function in config.\n        """"""\n        self._config = config\n        self._input_source = input_source\n        assert config.return_input is False, \\\n            ""return_input is not supported in FeedfreePredictor! "" \\\n            ""If you need to fetch inputs, add the names to the output_names!""\n\n        self._hooks = []\n        self.graph = config._maybe_create_graph()\n        with self.graph.as_default():\n            self._input_callbacks = Callbacks(\n                self._input_source.setup(config.input_signature))\n            with PredictTowerContext(\'\'):\n                self._input_tensors = self._input_source.get_input_tensors()\n                config.tower_func(*self._input_tensors)\n                self._tower_handle = config.tower_func.towers[-1]\n\n            self._output_tensors = self._tower_handle.get_tensors(config.output_names)\n\n            self._input_callbacks.setup_graph(None)\n\n            for h in self._input_callbacks.get_hooks():\n                self._register_hook(h)\n            self._initialize_session()\n\n    def _register_hook(self, hook):\n        """"""\n        Args:\n            hook (tf.train.SessionRunHook):\n        """"""\n        self._hooks.append(hook)\n\n    def _initialize_session(self):\n        # init the session\n        self._config.session_init._setup_graph()\n        self._sess = self._config.session_creator.create_session()\n        self._config.session_init._run_init(self._sess)\n\n        with self._sess.as_default():\n            self._input_callbacks.before_train()\n            self._hooked_sess = HookedSession(self._sess, self._hooks)\n\n    def __call__(self):\n        return self._hooked_sess.run(self._output_tensors)\n\n    def _do_call(self):\n        raise NotImplementedError(""You\'re calling the wrong function!"")\n'"
tensorpack/predict/multigpu.py,5,"b'# -*- coding: utf-8 -*-\n# File: multigpu.py\n\n\nimport tensorflow as tf\n\nfrom ..input_source import PlaceholderInput\nfrom ..tfutils.tower import PredictTowerContext\nfrom ..utils import logger\nfrom .base import OnlinePredictor\n\n__all__ = [\'MultiTowerOfflinePredictor\',\n           \'DataParallelOfflinePredictor\']\n\n\nclass MultiTowerOfflinePredictor(OnlinePredictor):\n    """""" A multi-tower multi-GPU predictor.\n        It builds one predictor for each tower.\n    """"""\n\n    def __init__(self, config, towers):\n        """"""\n        Args:\n            config (PredictConfig): the config to use.\n            towers: a list of relative GPU id.\n        """"""\n        assert len(towers) > 0\n        self.graph = config._maybe_create_graph()\n        self.predictors = []\n        self.return_input = config.return_input\n        with self.graph.as_default():\n            handles = []\n\n            input = PlaceholderInput()\n            input.setup(config.input_signature)\n\n            for idx, t in enumerate(towers):\n                tower_name = \'tower\' + str(t)\n\n                device = \'/gpu:{}\'.format(t)\n                with tf.variable_scope(tf.get_variable_scope(), reuse=idx > 0), \\\n                        tf.device(device), \\\n                        PredictTowerContext(tower_name):\n                    logger.info(""Building graph for predict tower \'{}\' on device {} ..."".format(tower_name, device))\n                    config.tower_func(*input.get_input_tensors())\n                    handles.append(config.tower_func.towers[-1])\n\n            config.session_init._setup_graph()\n            self.sess = config.session_creator.create_session()\n            config.session_init._run_init(self.sess)\n\n            for h in handles:\n                input_tensors = h.get_tensors(config.input_names)\n                output_tensors = h.get_tensors(config.output_names)\n                self.predictors.append(OnlinePredictor(\n                    input_tensors, output_tensors, config.return_input, self.sess))\n\n    def _do_call(self, dp):\n        # use the first tower for compatible PredictorBase interface\n        return self.predictors[0]._do_call(dp)\n\n    def get_predictor(self, n):\n        """"""\n        Returns:\n            OnlinePredictor: the nth predictor on the nth tower.\n        """"""\n        l = len(self.predictors)\n        if n >= l:\n            logger.warn(""n > #towers, will assign predictor to GPU by round-robin"")\n        return [self.predictors[k % l] for k in range(n)]\n\n    def get_predictors(self):\n        """"""\n        Returns:\n            list[OnlinePredictor]: a list of predictor\n        """"""\n        return self.predictors\n\n\nclass DataParallelOfflinePredictor(OnlinePredictor):\n    """"""\n    A data-parallel predictor. It builds one predictor that utilizes all GPUs.\n\n    Note that it doesn\'t split/concat inputs/outputs automatically.\n    Instead, its inputs are:\n    ``[input[0] in tower[0], input[1] in tower[0], ..., input[0] in tower[1], input[1] in tower[1], ...]``\n    Similar for the outputs.\n    """"""\n\n    def __init__(self, config, towers):\n        """"""\n        Args:\n            config (PredictConfig): the config to use.\n            towers: a list of relative GPU id.\n        """"""\n        self.graph = config._maybe_create_graph()\n        with self.graph.as_default():\n            input_tensors = []\n            output_tensors = []\n\n            for idx, t in enumerate(towers):\n                tower_name = \'tower\' + str(t)\n\n                new_sig = [tf.TensorSpec(dtype=p.dtype, shape=p.shape, name=tower_name + \'_\' + p.name)\n                           for p in config.input_signature]\n                input = PlaceholderInput()\n                input.setup(new_sig)\n\n                with tf.variable_scope(tf.get_variable_scope(), reuse=idx > 0), \\\n                        tf.device(\'/gpu:{}\'.format(t)), \\\n                        PredictTowerContext(tower_name):\n                    config.tower_func(*input.get_input_tensors())\n                    h = config.tower_func.towers[-1]\n                    input_tensors.extend(h.get_tensors(config.input_names))\n                    output_tensors.extend(h.get_tensors(config.output_names))\n\n            config.session_init._setup_graph()\n            sess = config.session_creator.create_session()\n            config.session_init._run_init(sess)\n            super(DataParallelOfflinePredictor, self).__init__(\n                input_tensors, output_tensors, config.return_input, sess)\n'"
tensorpack/tfutils/__init__.py,0,"b'#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n\nfrom .tower import get_current_tower_context, TowerContext\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()[\'kcah_acitats\'[::-1].upper()] = False\nif STATICA_HACK:\n    from .common import *\n    from .sessinit import *\n    from .argscope import *\n\n\n# don\'t want to include everything from .tower\n__all__ = [\'get_current_tower_context\', \'TowerContext\']\n\n\ndef _global_import(name):\n    p = __import__(name, globals(), None, level=1)\n    lst = p.__all__ if \'__all__\' in dir(p) else dir(p)\n    for k in lst:\n        if not k.startswith(\'__\'):\n            globals()[k] = p.__dict__[k]\n            __all__.append(k)\n\n\n_TO_IMPORT = frozenset([\n    \'common\',\n    \'sessinit\',\n    \'argscope\',\n])\n\nfor module_name in _TO_IMPORT:\n    _global_import(module_name)\n\n""""""\nTODO remove this line in the future.\nBetter to keep submodule names (sesscreate, varmanip, etc) out of __all__,\nso that these names will be invisible under `tensorpack.` namespace.\n\nTo use these utilities, users are expected to import them explicitly, e.g.:\n\nimport tensorpack.tfutils.sessinit as sessinit\n""""""\n__all__.extend([\'sessinit\', \'summary\', \'optimizer\',\n                \'sesscreate\', \'gradproc\', \'varreplace\',\n                \'tower\'])\n'"
tensorpack/tfutils/argscope.py,5,"b'# -*- coding: utf-8 -*-\n# File: argscope.py\n\nimport copy\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nfrom functools import wraps\nfrom inspect import getmembers, isfunction\nimport tensorflow as tf\n\nfrom ..compat import is_tfv2\nfrom ..utils import logger\nfrom .model_utils import get_shape_str\nfrom .tower import get_current_tower_context\n\n__all__ = [\'argscope\', \'get_arg_scope\', \'enable_argscope_for_module\',\n           \'enable_argscope_for_function\']\n\n_ArgScopeStack = []\n\n\n@contextmanager\ndef argscope(layers, **kwargs):\n    """"""\n    Args:\n        layers (list or layer): layer or list of layers to apply the arguments.\n\n    Returns:\n        a context where all appearance of these layer will by default have the\n        arguments specified by kwargs.\n\n    Example:\n        .. code-block:: python\n\n            with argscope(Conv2D, kernel_shape=3, nl=tf.nn.relu, out_channel=32):\n                x = Conv2D(\'conv0\', x)\n                x = Conv2D(\'conv1\', x)\n                x = Conv2D(\'conv2\', x, out_channel=64)  # override argscope\n\n    """"""\n    if not isinstance(layers, list):\n        layers = [layers]\n\n    for l in layers:\n        assert hasattr(l, \'__argscope_enabled__\'), ""Argscope not supported for {}"".format(l)\n\n    # need to deepcopy so that changes to new_scope does not affect outer scope\n    new_scope = copy.deepcopy(get_arg_scope())\n    for l in layers:\n        new_scope[l.__name__].update(kwargs)\n    _ArgScopeStack.append(new_scope)\n    yield\n    del _ArgScopeStack[-1]\n\n\ndef get_arg_scope():\n    """"""\n    Returns:\n        dict: the current argscope.\n\n    An argscope is a dict of dict: ``dict[layername] = {arg: val}``\n    """"""\n    if len(_ArgScopeStack) > 0:\n        return _ArgScopeStack[-1]\n    else:\n        return defaultdict(dict)\n\n\ndef enable_argscope_for_function(func, log_shape=True):\n    """"""Decorator for function to support argscope\n\n    Example:\n\n        .. code-block:: python\n\n            from mylib import myfunc\n            myfunc = enable_argscope_for_function(myfunc)\n\n    Args:\n        func: A function mapping one or multiple tensors to one or multiple\n            tensors.\n        log_shape (bool): Specify whether the first input resp. output tensor\n            shape should be printed once.\n\n    Remarks:\n        If the function ``func`` returns multiple input or output tensors,\n        only the first input/output tensor shape is displayed during logging.\n\n    Returns:\n        The decorated function.\n\n    """"""\n\n    assert callable(func), ""func should be a callable""\n\n    @wraps(func)\n    def wrapped_func(*args, **kwargs):\n        actual_args = copy.copy(get_arg_scope()[func.__name__])\n        actual_args.update(kwargs)\n        out_tensor = func(*args, **actual_args)\n        in_tensor = args[0]\n\n        ctx = get_current_tower_context()\n        name = func.__name__ if \'name\' not in kwargs else kwargs[\'name\']\n        if log_shape:\n            if (\'tower\' not in ctx.ns_name.lower()) or ctx.is_main_training_tower:\n                # we assume the first parameter is the most interesting\n                if isinstance(out_tensor, tuple):\n                    out_tensor_descr = out_tensor[0]\n                else:\n                    out_tensor_descr = out_tensor\n                logger.info(""{:<12}: {} --> {}"".format(\n                    ""\'"" + name + ""\'"",\n                    get_shape_str(in_tensor),\n                    get_shape_str(out_tensor_descr)))\n\n        return out_tensor\n    wrapped_func.__argscope_enabled__ = True\n    return wrapped_func\n\n\ndef enable_argscope_for_module(module, log_shape=True):\n    """"""\n    Overwrite all functions of a given module to support argscope.\n    Note that this function monkey-patches the module and therefore could\n    have unexpected consequences.\n    It has been only tested to work well with ``tf.layers`` module.\n\n    Example:\n\n        .. code-block:: python\n\n            import tensorflow as tf\n            enable_argscope_for_module(tf.layers)\n\n    Args:\n        log_shape (bool): print input/output shapes of each function.\n    """"""\n    if is_tfv2() and module == tf.layers:\n        module = tf.compat.v1.layers\n    for name, obj in getmembers(module):\n        if isfunction(obj):\n            setattr(module, name, enable_argscope_for_function(obj,\n                    log_shape=log_shape))\n'"
tensorpack/tfutils/collection.py,13,"b'# -*- coding: utf-8 -*-\n# File: collection.py\n\n\nfrom contextlib import contextmanager\nfrom copy import copy\nimport six\n\nfrom ..compat import tfv1 as tf\n\nfrom ..utils import logger\nfrom ..utils.argtools import memoized\n\n__all__ = [\'backup_collection\',\n           \'restore_collection\',\n           \'freeze_collection\']\n\n\ndef backup_collection(keys=None):\n    """"""\n    Args:\n        keys (list): list of collection keys to backup.\n            Defaults to all keys in the graph.\n\n    Returns:\n        dict: the backup\n    """"""\n    if keys is None:\n        keys = tf.get_default_graph().get_all_collection_keys()\n    ret = {}\n    assert isinstance(keys, (list, tuple, set))\n    for k in keys:\n        ret[k] = copy(tf.get_collection(k))\n    return ret\n\n\ndef restore_collection(backup):\n    """"""\n    Restore from a collection backup.\n\n    Args:\n        backup (dict):\n    """"""\n    for k, v in six.iteritems(backup):\n        del tf.get_collection_ref(k)[:]\n        tf.get_collection_ref(k).extend(v)\n\n\n@contextmanager\ndef freeze_collection(keys):\n    """"""\n    Args:\n        keys(list): list of collection keys to freeze.\n\n    Returns:\n        a context where the collections are in the end restored to its initial state.\n    """"""\n    backup = backup_collection(keys)\n    yield\n    restore_collection(backup)\n\n\n@memoized\ndef get_inverse_graphkeys():\n    ret = {}\n    for name in dir(tf.GraphKeys):\n        if name.startswith(\'_\'):\n            continue\n        if name in [\'VARIABLES\']:   # will produce deprecated warning\n            continue\n        ret[getattr(tf.GraphKeys, name)] = ""tf.GraphKeys.{}"".format(name)\n    return ret\n\n\nclass CollectionGuard(object):\n    """"""\n    A context to maintain collection change in a tower.\n    """"""\n\n    original = None\n\n    def __init__(self, name, check_diff,\n                 freeze_keys=(),\n                 diff_whitelist=None):\n        """"""\n        Args:\n           name (str): name of the tower\n           check_diff (bool): whether to check and print about collection change\n                when leaving this guard.\n           freeze_keys (list): list of keys to backup when entering and restore when leaving this guard.\n           diff_whitelist (list): list of keys to ignore, when check_diff is True.\n                Defaults to some collections that are normally changed,\n                including variables, losses, contexts, queue runners.\n        """"""\n        self._name = name\n        self._check_diff = check_diff\n        if diff_whitelist is None:\n            diff_whitelist = CollectionGuard._default_diff_whitelist()\n        self._whitelist = set(diff_whitelist)\n        self._freeze_keys = freeze_keys\n        self._inverse_graphkeys = get_inverse_graphkeys()\n\n    @staticmethod\n    def _default_diff_whitelist():\n        ret = [tf.GraphKeys.TRAINABLE_VARIABLES,\n               tf.GraphKeys.GLOBAL_VARIABLES,\n               tf.GraphKeys.QUEUE_RUNNERS,\n               tf.GraphKeys.LOCAL_VARIABLES]\n        for newkey in [\'COND_CONTEXT\', \'WHILE_CONTEXT\', \'LOSSES\']:\n            if hasattr(tf.GraphKeys, newkey):\n                ret.append(getattr(tf.GraphKeys, newkey))\n        return ret\n\n    def _key_name(self, name):\n        return self._inverse_graphkeys.get(name, name)\n\n    def __enter__(self):\n        self.original = backup_collection()\n        self._freeze_backup = backup_collection(self._freeze_keys)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is not None:\n            return False\n        new_coll = backup_collection()\n\n        if self._check_diff:\n            self._print_diff(new_coll)\n        self._restore_freeze(new_coll)\n        return False\n\n    def _print_diff(self, new):\n        newly_created = []\n        size_change = []\n        for k, v in six.iteritems(new):\n            if k in self._whitelist or k in self._freeze_keys:\n                continue\n            if k not in self.original:\n                newly_created.append((self._key_name(k), len(v)))\n            else:\n                old_v = self.original[k]\n                if len(old_v) != len(v):\n                    size_change.append((self._key_name(k), len(old_v), len(v)))\n        if newly_created:\n            logger.info(\n                ""New collections created in tower {}: "".format(self._name) +\n                \', \'.join([""{} of size {}"".format(key, size) for key, size in newly_created]))\n        if size_change:\n            logger.info(\n                ""Size of these collections were changed in {}: {}"".format(\n                    self._name, \', \'.join(\n                        map(lambda t: ""({}: {}->{})"".format(*t),\n                            size_change))))\n\n    def _restore_freeze(self, new):\n        size_change = []\n        for k, v in six.iteritems(self._freeze_backup):\n            newv = new.get(k, [])\n            if len(v) != len(newv):\n                size_change.append((self._key_name(k), len(v), len(newv)))\n        if size_change:\n            logger.info(\n                ""These collections were modified but restored in {}: {}"".format(\n                    self._name, \', \'.join(\n                        map(lambda t: ""({}: {}->{})"".format(*t),\n                            size_change))))\n        restore_collection(self._freeze_backup)\n\n    def get_collection_in_tower(self, key):\n        """"""\n        Get items from this collection that are added in the current tower.\n        """"""\n        new = tf.get_collection(key)\n        old = set(self.original.get(key, []))\n        # persist the order in new\n        return [x for x in new if x not in old]\n'"
tensorpack/tfutils/common.py,7,"b'# -*- coding: utf-8 -*-\n# File: common.py\n\nfrom collections import defaultdict\nfrom six.moves import map\nfrom tabulate import tabulate\nimport os\nimport sys\nimport psutil\nimport tensorflow as tf\nimport numpy as np\n\nfrom ..compat import tfv1\nfrom ..utils.argtools import graph_memoized\nfrom ..utils.utils import find_library_full_path as find_library\nfrom ..utils.nvml import NVMLContext\nfrom ..libinfo import __git_version__\n\n__all__ = [\'get_default_sess_config\',\n           \'get_global_step_value\',\n           \'get_global_step_var\',\n           \'get_tf_version_tuple\',\n           \'collect_env_info\'\n           # \'get_op_tensor_name\',\n           # \'get_tensors_by_names\',\n           # \'get_op_or_tensor_by_name\',\n           ]\n\n\ndef get_default_sess_config(mem_fraction=0.99):\n    """"""\n    Return a tf.ConfigProto to use as default session config.\n    You can modify the returned config to fit your needs.\n\n    Args:\n        mem_fraction(float): see the `per_process_gpu_memory_fraction` option\n            in TensorFlow\'s GPUOptions protobuf:\n            https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto\n\n    Returns:\n        tf.ConfigProto: the config to use.\n    """"""\n    conf = tfv1.ConfigProto()\n\n    conf.allow_soft_placement = True\n    # conf.log_device_placement = True\n\n    conf.intra_op_parallelism_threads = 1\n    conf.inter_op_parallelism_threads = 0\n    # TF benchmark use cpu_count() - gpu_thread_count(), e.g. 80 - 8 * 2\n    # Didn\'t see much difference.\n\n    conf.gpu_options.per_process_gpu_memory_fraction = mem_fraction\n\n    # This hurt performance of large data pipeline:\n    # https://github.com/tensorflow/benchmarks/commit/1528c46499cdcff669b5d7c006b7b971884ad0e6\n    # conf.gpu_options.force_gpu_compatible = True\n\n    conf.gpu_options.allow_growth = True\n\n    # from tensorflow.core.protobuf import rewriter_config_pb2 as rwc\n    # conf.graph_options.rewrite_options.memory_optimization = \\\n    #     rwc.RewriterConfig.HEURISTICS\n\n    # May hurt performance?\n    # conf.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n    # conf.graph_options.place_pruned_graph = True\n    return conf\n\n\n@graph_memoized\ndef get_global_step_var():\n    """"""\n    Returns:\n        tf.Tensor: the global_step variable in the current graph. Create if doesn\'t exist.\n    """"""\n    scope = tfv1.VariableScope(reuse=False, name=\'\')  # the root vs\n    with tfv1.variable_scope(scope):\n        var = tfv1.train.get_or_create_global_step()\n    return var\n\n\ndef get_global_step_value():\n    """"""\n    Returns:\n        int: global_step value in current graph and session\n\n    Has to be called under a default session.\n    """"""\n\n    return tfv1.train.global_step(\n        tfv1.get_default_session(),\n        get_global_step_var())\n\n\ndef get_op_tensor_name(name):\n    """"""\n    Will automatically determine if ``name`` is a tensor name (ends with \':x\')\n    or a op name.\n    If it is an op name, the corresponding tensor name is assumed to be ``op_name + \':0\'``.\n\n    Args:\n        name(str): name of an op or a tensor\n    Returns:\n        tuple: (op_name, tensor_name)\n    """"""\n    if len(name) >= 3 and name[-2] == \':\':\n        return name[:-2], name\n    else:\n        return name, name + \':0\'\n\n\ndef get_tensors_by_names(names):\n    """"""\n    Get a list of tensors in the default graph by a list of names.\n\n    Args:\n        names (list):\n    """"""\n    ret = []\n    G = tfv1.get_default_graph()\n    for n in names:\n        opn, varn = get_op_tensor_name(n)\n        ret.append(G.get_tensor_by_name(varn))\n    return ret\n\n\ndef get_op_or_tensor_by_name(name):\n    """"""\n    Get either tf.Operation of tf.Tensor from names.\n\n    Args:\n        name (list[str] or str): names of operations or tensors.\n\n    Raises:\n        KeyError, if the name doesn\'t exist\n    """"""\n    G = tfv1.get_default_graph()\n\n    def f(n):\n        if len(n) >= 3 and n[-2] == \':\':\n            return G.get_tensor_by_name(n)\n        else:\n            return G.get_operation_by_name(n)\n\n    if not isinstance(name, list):\n        return f(name)\n    else:\n        return list(map(f, name))\n\n\ndef gpu_available_in_session():\n    sess = tfv1.get_default_session()\n    for dev in sess.list_devices():\n        if dev.device_type.lower() == \'gpu\':\n            return True\n    return False\n\n\ndef get_tf_version_tuple():\n    """"""\n    Return TensorFlow version as a 2-element tuple (for comparison).\n    """"""\n    return tuple(map(int, tf.__version__.split(\'.\')[:2]))\n\n\ndef collect_env_info():\n    """"""\n    Returns:\n        str - a table contains important information about the environment\n    """"""\n    data = []\n    data.append((""sys.platform"", sys.platform))\n    data.append((""Python"", sys.version.replace(""\\n"", """")))\n    data.append((""Tensorpack"", __git_version__))\n    data.append((""Numpy"", np.__version__))\n\n    data.append((""TensorFlow"", tfv1.VERSION + ""/"" + tfv1.GIT_VERSION))\n    data.append((""TF Compiler Version"", tfv1.COMPILER_VERSION))\n    has_cuda = tf.test.is_built_with_cuda()\n    data.append((""TF CUDA support"", has_cuda))\n\n    try:\n        from tensorflow.python.framework import test_util\n        data.append((""TF MKL support"", test_util.IsMklEnabled()))\n    except Exception:\n        pass\n\n    try:\n        from tensorflow.python.framework import test_util\n        data.append((""TF XLA support"", test_util.is_xla_enabled()))\n    except Exception:\n        pass\n\n    if has_cuda:\n        data.append((""Nvidia Driver"", find_library(""nvidia-ml"")))\n        data.append((""CUDA"", find_library(""cudart"")))\n        data.append((""CUDNN"", find_library(""cudnn"")))\n        data.append((""NCCL"", find_library(""nccl"")))\n\n        # List devices with NVML\n        data.append(\n            (""CUDA_VISIBLE_DEVICES"",\n             os.environ.get(""CUDA_VISIBLE_DEVICES"", ""Unspecified"")))\n        try:\n            devs = defaultdict(list)\n            with NVMLContext() as ctx:\n                for idx, dev in enumerate(ctx.devices()):\n                    devs[dev.name()].append(str(idx))\n\n            for devname, devids in devs.items():\n                data.append(\n                    (""GPU "" + "","".join(devids), devname))\n        except Exception:\n            data.append((""GPU"", ""Not found with NVML""))\n\n    vram = psutil.virtual_memory()\n    data.append((""Free RAM"", ""{:.2f}/{:.2f} GB"".format(vram.available / 1024**3, vram.total / 1024**3)))\n    data.append((""CPU Count"", psutil.cpu_count()))\n\n    # Other important dependencies:\n    try:\n        import horovod\n        data.append((""Horovod"", horovod.__version__))\n    except ImportError:\n        pass\n\n    try:\n        import cv2\n        data.append((""cv2"", cv2.__version__))\n    except ImportError:\n        pass\n\n    import msgpack\n    data.append((""msgpack"", ""."".join([str(x) for x in msgpack.version])))\n\n    has_prctl = True\n    try:\n        import prctl\n        _ = prctl.set_pdeathsig  # noqa\n    except Exception:\n        has_prctl = False\n    data.append((""python-prctl"", has_prctl))\n\n    return tabulate(data)\n\n\nif __name__ == \'__main__\':\n    print(collect_env_info())\n'"
tensorpack/tfutils/dependency.py,6,"b'\nimport tensorflow as tf\n\nfrom ..utils.argtools import graph_memoized\n\n""""""\nUtils about parsing dependencies in the graph.\n""""""\n\n__all__ = [\n    \'dependency_of_targets\', \'dependency_of_fetches\'\n]\n\n\n@graph_memoized\ndef dependency_of_targets(targets, op):\n    """"""\n    Check that op is in the subgraph induced by the dependencies of targets.\n    The result is memoized.\n\n    This is useful if some SessionRunHooks should be run only together with certain ops.\n\n    Args:\n        targets: a tuple of ops or tensors. The targets to find dependencies of.\n        op (tf.Operation or tf.Tensor):\n\n    Returns:\n        bool: True if any one of `targets` depend on `op`.\n    """"""\n    # TODO tensorarray? sparsetensor?\n    if isinstance(op, tf.Tensor):\n        op = op.op\n    assert isinstance(op, tf.Operation), op\n\n    try:\n        from tensorflow.contrib.graph_editor import get_backward_walk_ops  # deprecated\n    except ImportError:\n        from tensorflow.python.ops.op_selector import get_backward_walk_ops\n    # alternative implementation can use graph_util.extract_sub_graph\n    dependent_ops = get_backward_walk_ops(targets, control_inputs=True)\n    return op in dependent_ops\n\n\ndef dependency_of_fetches(fetches, op):\n    """"""\n    Check that op is in the subgraph induced by the dependencies of fetches.\n    fetches may have more general structure.\n\n    Args:\n        fetches: An argument to `sess.run`. Nested structure will affect performance.\n        op (tf.Operation or tf.Tensor):\n\n    Returns:\n        bool: True if any of `fetches` depend on `op`.\n    """"""\n    try:\n        from tensorflow.python.client.session import _FetchHandler as FetchHandler\n        # use the graph of the op, so that this function can be called without being under a default graph\n        handler = FetchHandler(op.graph, fetches, {})\n        targets = tuple(handler.fetches() + handler.targets())\n    except ImportError:\n        if isinstance(fetches, list):\n            targets = tuple(fetches)\n        elif isinstance(fetches, dict):\n            raise ValueError(""Don\'t know how to parse dictionary to fetch list! ""\n                             ""This is a bug of tensorpack."")\n        else:\n            targets = (fetches, )\n    return dependency_of_targets(targets, op)\n\n\nif __name__ == \'__main__\':\n    a = tf.random_normal(shape=[3, 3])\n    b = tf.random_normal(shape=[3, 3])\n    print(dependency_of_fetches(a, a))\n    print(dependency_of_fetches([a, b], a))\n'"
tensorpack/tfutils/distributed.py,13,"b'# -*- coding: utf-8 -*-\n# File: distributed.py\n\n\nimport tensorflow as tf\n\n\ndef get_distributed_session_creator(server):\n    """"""\n    Args:\n       server (tf.train.Server):\n\n    Returns:\n        tf.train.SessionCreator\n    """"""\n\n    server_def = server.server_def\n    is_chief = (server_def.job_name == \'worker\') and (server_def.task_index == 0)\n\n    init_op = tf.global_variables_initializer()\n    local_init_op = tf.local_variables_initializer()\n    ready_op = tf.report_uninitialized_variables()\n    ready_for_local_init_op = tf.report_uninitialized_variables(tf.global_variables())\n    sm = tf.train.SessionManager(\n        local_init_op=local_init_op,\n        ready_op=ready_op,\n        ready_for_local_init_op=ready_for_local_init_op,\n        graph=tf.get_default_graph())\n\n    # to debug wrong variable collection\n    # from pprint import pprint\n    # print(""GLOBAL:"")\n    # pprint([(k.name, k.device) for k in tf.global_variables()])\n    # print(""LOCAL:"")\n    # pprint([(k.name, k.device) for k in tf.local_variables()])\n\n    class _Creator(tf.train.SessionCreator):\n        def create_session(self):\n            if is_chief:\n                return sm.prepare_session(master=server.target, init_op=init_op)\n            else:\n                tf.logging.set_verbosity(tf.logging.INFO)   # print message about uninitialized vars\n                ret = sm.wait_for_session(master=server.target)\n                tf.logging.set_verbosity(tf.logging.WARN)\n                return ret\n\n    return _Creator()\n'"
tensorpack/tfutils/export.py,2,"b'# -*- coding: utf-8 -*-\n# File: export.py\n\n""""""\nA collection of functions to ease the process of exporting\na model for production.\n\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.tools import optimize_for_inference_lib\n\nfrom ..compat import tfv1\nfrom ..input_source import PlaceholderInput\nfrom ..tfutils.common import get_tensors_by_names, get_tf_version_tuple\nfrom ..tfutils.tower import PredictTowerContext\nfrom ..utils import logger\n\n__all__ = [\'ModelExporter\']\n\n\nclass ModelExporter(object):\n    """"""Export models for inference.""""""\n\n    def __init__(self, config):\n        """"""Initialise the export process.\n\n        Args:\n            config (PredictConfig): the config to use.\n                The graph will be built with the tower function defined by this `PredictConfig`.\n                Then the input / output names will be used to export models for inference.\n        """"""\n        super(ModelExporter, self).__init__()\n        self.config = config\n\n    def export_compact(self, filename, optimize=True, toco_compatible=False):\n        """"""Create a self-contained inference-only graph and write final graph (in pb format) to disk.\n\n        Args:\n            filename (str): path to the output graph\n            optimize (bool): whether to use TensorFlow\'s `optimize_for_inference`\n                to prune and optimize the graph. This does not work on all types of graphs.\n            toco_compatible (bool): See TensorFlow\'s\n                `optimize_for_inference\n                <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py>`_\n                for details. Only available after TF 1.8.\n        """"""\n        if toco_compatible:\n            assert optimize, ""toco_compatible is only effective when optimize=True!""\n        self.graph = self.config._maybe_create_graph()\n        with self.graph.as_default():\n            input = PlaceholderInput()\n            input.setup(self.config.input_signature)\n            with PredictTowerContext(\'\'):\n                self.config.tower_func(*input.get_input_tensors())\n\n            input_tensors = get_tensors_by_names(self.config.input_names)\n            output_tensors = get_tensors_by_names(self.config.output_names)\n\n            self.config.session_init._setup_graph()\n            # we cannot use ""self.config.session_creator.create_session()"" here since it finalizes the graph\n            sess = tfv1.Session(config=tfv1.ConfigProto(allow_soft_placement=True))\n            self.config.session_init._run_init(sess)\n\n            dtypes = [n.dtype for n in input_tensors]\n\n            # freeze variables to constants\n            frozen_graph_def = graph_util.convert_variables_to_constants(\n                sess,\n                self.graph.as_graph_def(),\n                [n.name[:-2] for n in output_tensors],\n                variable_names_whitelist=None,\n                variable_names_blacklist=None)\n\n            # prune unused nodes from graph\n            if optimize:\n                toco_args = () if get_tf_version_tuple() < (1, 8) else (toco_compatible, )\n                frozen_graph_def = optimize_for_inference_lib.optimize_for_inference(\n                    frozen_graph_def,\n                    [n.name[:-2] for n in input_tensors],\n                    [n.name[:-2] for n in output_tensors],\n                    [dtype.as_datatype_enum for dtype in dtypes],\n                    *toco_args)\n\n            with gfile.FastGFile(filename, ""wb"") as f:\n                f.write(frozen_graph_def.SerializeToString())\n                logger.info(""Output graph written to {}."".format(filename))\n\n    def export_serving(self, filename,\n                       tags=None,\n                       signature_name=\'prediction_pipeline\'):\n        """"""\n        Converts a checkpoint and graph to a servable for TensorFlow Serving.\n        Use TF\'s `SavedModelBuilder` to export a trained model without tensorpack dependency.\n\n        Args:\n            filename (str): path for export directory\n            tags (tuple): tuple of user specified tags. Defaults to just ""SERVING"".\n            signature_name (str): name of signature for prediction\n\n        Note:\n            This produces\n\n            .. code-block:: none\n\n                variables/       # output from the vanilla Saver\n                    variables.data-?????-of-?????\n                    variables.index\n                saved_model.pb   # a `SavedModel` protobuf\n\n            Currently, we only support a single signature, which is the general PredictSignatureDef:\n            https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/signature_defs.md\n        """"""\n        if tags is None:\n            tags = (tf.saved_model.SERVING if get_tf_version_tuple() >= (1, 12)\n                    else tf.saved_model.tag_constants.SERVING, )\n\n        self.graph = self.config._maybe_create_graph()\n        with self.graph.as_default():\n            input = PlaceholderInput()\n            input.setup(self.config.input_signature)\n            with PredictTowerContext(\'\'):\n                self.config.tower_func(*input.get_input_tensors())\n\n            input_tensors = get_tensors_by_names(self.config.input_names)\n            saved_model = tfv1.saved_model.utils\n            inputs_signatures = {t.name: saved_model.build_tensor_info(t) for t in input_tensors}\n            output_tensors = get_tensors_by_names(self.config.output_names)\n            outputs_signatures = {t.name: saved_model.build_tensor_info(t) for t in output_tensors}\n\n            self.config.session_init._setup_graph()\n            # we cannot use ""self.config.session_creator.create_session()"" here since it finalizes the graph\n            sess = tfv1.Session(config=tfv1.ConfigProto(allow_soft_placement=True))\n            self.config.session_init._run_init(sess)\n\n            builder = tfv1.saved_model.builder.SavedModelBuilder(filename)\n\n            prediction_signature = tfv1.saved_model.signature_def_utils.build_signature_def(\n                inputs=inputs_signatures,\n                outputs=outputs_signatures,\n                method_name=tfv1.saved_model.signature_constants.PREDICT_METHOD_NAME)\n\n            builder.add_meta_graph_and_variables(\n                sess, list(tags),\n                signature_def_map={signature_name: prediction_signature})\n            builder.save()\n            logger.info(""SavedModel created at {}."".format(filename))\n'"
tensorpack/tfutils/gradproc.py,5,"b'# -*- coding: utf-8 -*-\n# File: gradproc.py\n\n\nimport inspect\nimport re\nfrom abc import ABCMeta, abstractmethod\nimport six\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..utils import logger\nfrom .summary import add_moving_summary\nfrom .symbolic_functions import print_stat, rms\n\n__all__ = [\'GradientProcessor\',\n           \'FilterNoneGrad\', \'GlobalNormClip\', \'MapGradient\', \'SummaryGradient\',\n           \'PrintGradient\', \'CheckGradient\', \'ScaleGradient\']\n\n\n@six.add_metaclass(ABCMeta)\nclass GradientProcessor(object):\n    """"""\n    Base class for all gradient processors.\n    Gradient processors can be applied to optimizers by\n    :func:`optimizer.apply_grad_processors`.\n\n    Subclass should override the ``_process()`` method.\n    """"""\n    _name_scope = None\n\n    def process(self, grads):\n        """"""\n        Process the symbolic gradients.\n\n        Args:\n            grads (list): list of (grad, var).\n        Returns:\n            list: processed gradients, with the same type as input.\n        """"""\n\n        # reuse the old name_scope, if process() is called multiple times\n        if self._name_scope is None:\n            with tfv1.name_scope(type(self).__name__) as scope:\n                self._name_scope = scope\n                return self._process(grads)\n        else:\n            with tfv1.name_scope(self._name_scope):\n                return self._process(grads)\n\n    @abstractmethod\n    def _process(self, grads):\n        pass\n\n\nclass FilterNoneGrad(GradientProcessor):\n    """"""\n    Skip the update and print a warning (instead of crashing),\n    when the gradient of certain variable is None.\n    """"""\n    def __init__(self, verbose=True):\n        """"""\n        Args:\n            verbose (bool): whether to print warning about None gradients.\n        """"""\n        super(FilterNoneGrad, self).__init__()\n        self._verbose = verbose\n\n    def _process(self, grads):\n        g = []\n        to_print = []\n        for grad, var in grads:\n            if grad is None:\n                to_print.append(var.op.name)\n            else:\n                g.append((grad, var))\n        if self._verbose and len(to_print):\n            message = \', \'.join(to_print)\n            logger.warn(""No gradient w.r.t {} trainable variables: {}"".format(len(to_print), message))\n        return g\n\n\nclass GlobalNormClip(GradientProcessor):\n    """""" Clip by global norm.\n        The global norm is the sum of norm for **all** gradients.\n\n        See :func:`tf.clip_by_global_norm` for more information.\n    """"""\n\n    def __init__(self, global_norm):\n        """"""\n        Args:\n            global_norm(float): the threshold to clip with.\n        """"""\n        super(GlobalNormClip, self).__init__()\n        self._norm = float(global_norm)\n\n    def _process(self, grads):\n        g = [k[0] for k in grads]\n        v = [k[1] for k in grads]\n        g, _ = tf.clip_by_global_norm(g, self._norm, name=\'clip_by_global_norm\')\n        return list(zip(g, v))\n\n\nclass MapGradient(GradientProcessor):\n    """"""\n    Apply a function on all gradient if the name matches regex.\n    Keep the other gradients unchanged.\n\n    It can be used for gradient clipping, etc.\n    """"""\n\n    def __init__(self, func, regex=\'.*\'):\n        """"""\n        Args:\n            func: a user-supplied function which takes one or two arguments.\n                The argument(s) can be either a `grad` tensor, or `grad` and `var`.\n                The function should return the new gradient to be used.\n                If it return None, the gradient is discarded (hence no update to the variable will happen).\n            regex (str): used to match variables. Defaults to match all variables.\n        """"""\n        args = inspect.getfullargspec(func).args\n        arg_num = len(args) - inspect.ismethod(func)\n        assert arg_num in [1, 2], \\\n            ""The function must take 1 or 2 arguments!  ({})"".format(args)\n        if arg_num == 1:\n            self.func = lambda grad, var: func(grad)\n        else:\n            self.func = func\n\n        if not regex.endswith(\'$\'):\n            regex = regex + \'$\'\n        self.regex = regex\n        super(MapGradient, self).__init__()\n\n    def _process(self, grads):\n        ret = []\n        matched = False\n        for grad, var in grads:\n            if re.match(self.regex, var.op.name):\n                matched = True\n                grad = self.func(grad, var)\n                if grad is not None:\n                    ret.append((grad, var))\n            else:\n                ret.append((grad, var))\n        if not matched:\n            logger.warn(""[MapGradient] No match was found for regex {}."".format(self.regex))\n        return ret\n\n\n# TODO has dependency problems: sess.run may not depend on grad\n# maybe group maintain op and grad ?\nclass SummaryGradient(MapGradient):\n    """"""\n    For each gradient tensor, summary its histogram and add it to moving\n    summaries.\n    """"""\n    # avoid duplicate summaries from towers\n    # TODO this is global. not good.\n    _summaried_gradient = set()\n\n    def __init__(self, regex=\'.*\', collections=None):\n        """"""\n        Args:\n            regex(str): same as in :class:`MapGradient`.\n            collections (list[str]): list of collection names\n        """"""\n        super(SummaryGradient, self).__init__(self._mapper, regex)\n        self._coll = collections\n\n    def _mapper(self, grad, var):\n        name = var.op.name\n        if re.match(\'tower[0-9]+/\', name):\n            # replicated training, var may come from different towers\n            return grad\n        if name not in SummaryGradient._summaried_gradient:\n            SummaryGradient._summaried_gradient.add(name)\n            tfv1.summary.histogram(name + \'-grad\', grad, collections=self._coll)\n            add_moving_summary(rms(grad, name=name + \'/rms\'))\n        return grad\n\n\nclass PrintGradient(MapGradient):\n    """"""\n    Print the gradients every step with :func:`symbolic_functions.print_stat`.\n    """"""\n    _printed = set()\n    # TODO this is global. not good.\n\n    def __init__(self, regex=\'.*\'):\n        """"""\n        Args:\n            regex(str): same as in :class:`MapGradient`.\n        """"""\n        super(PrintGradient, self).__init__(self._mapper, regex)\n\n    def _mapper(self, grad, var):\n        name = var.op.name\n        if name not in PrintGradient._printed:\n            PrintGradient._printed.add(name)\n            grad = print_stat(grad, message=name + \'-grad\')\n        return grad\n\n\nclass CheckGradient(MapGradient):\n    """"""\n    Run :func:`tf.check_numerics` for each gradient.\n    """"""\n\n    def __init__(self):\n        super(CheckGradient, self).__init__(self._mapper)\n\n    def _mapper(self, grad, var):\n        # this was very slow.... see #3649\n        # op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)\n        grad = tf.check_numerics(grad, \'CheckGradient/\' + var.op.name)\n        return grad\n\n\nclass ScaleGradient(MapGradient):\n    """"""\n    Scale certain gradient by a multiplier.\n    """"""\n\n    def __init__(self, multipliers, verbose=True):\n        """"""\n        Args:\n            multipliers (tuple or list): tuple of (regex, float), or list of such tuples.\n            verbose (bool): whether to print logs or not\n\n        Example:\n            Use double learning rate for all the bias (as in caffe), and freeze layer0:\n\n            .. code-block:: python\n\n                from tensorpack.tfutils import optimizer, gradproc\n                opt = optimizer.apply_grad_processors(\n                    opt, [gradproc.ScaleGradient(\n                        [(\'.*/b\', 2.), (\'layer0/.*\', 0.)]\n                    )])\n        """"""\n        if not isinstance(multipliers, list):\n            multipliers = [multipliers]\n        self.multipliers = multipliers\n        assert verbose in [True, False], verbose\n        self._verbose = verbose\n        super(ScaleGradient, self).__init__(self._mapper)\n\n    def _mapper(self, grad, var):\n        varname = var.op.name\n        for regex, val in self.multipliers:\n            # always match against the whole name\n            if not regex.endswith(\'$\'):\n                regex = regex + \'$\'\n\n            if re.match(regex, varname):\n                if self._verbose:\n                    logger.info(""Gradient of \'{}\' is multipled by {}"".format(varname, val))\n                if val != 0:    # skip zero to speed up\n                    return grad * val\n                else:\n                    return None\n        return grad\n'"
tensorpack/tfutils/model_utils.py,4,"b'# -*- coding: utf-8 -*-\n# File: model_utils.py\n# Author: tensorpack contributors\n\nfrom ..compat import tfv1 as tf\nfrom tabulate import tabulate\nfrom termcolor import colored\n\nfrom .common import get_op_tensor_name\nfrom ..utils import logger\n\n__all__ = []\n\n\ndef describe_trainable_vars():\n    """"""\n    Print a description of the current model parameters.\n    Skip variables starting with ""tower"", as they are just duplicates built by data-parallel logic.\n    """"""\n    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    if len(train_vars) == 0:\n        logger.warn(""No trainable variables in the graph!"")\n        return\n    total = 0\n    total_bytes = 0\n    data = []\n    for v in train_vars:\n        if v.name.startswith(\'tower\'):\n            continue\n        shape = v.get_shape()\n        ele = shape.num_elements()\n        if ele is None:\n            logger.warn(""Shape of variable {} is not fully defined but {}."".format(v.name, shape))\n            ele = 0\n        try:\n            shape = shape.as_list()\n        except ValueError:\n            shape = \'<unknown>\'\n\n        total += ele\n        total_bytes += ele * v.dtype.size\n        data.append([get_op_tensor_name(v.name)[0], shape, ele, v.device, v.dtype.base_dtype.name])\n    headers = [\'name\', \'shape\', \'#elements\', \'device\', \'dtype\']\n\n    dtypes = list({x[4] for x in data})\n    if len(dtypes) == 1 and dtypes[0] == ""float32"":\n        # don\'t log the dtype if all vars are float32 (default dtype)\n        for x in data:\n            del x[4]\n        del headers[4]\n\n    devices = {x[3] for x in data}\n    if len(devices) == 1:\n        # don\'t log the device if all vars on the same device\n        for x in data:\n            del x[3]\n        del headers[3]\n\n    table = tabulate(data, headers=headers)\n\n    size_mb = total_bytes / 1024.0**2\n    summary_msg = colored(\n        ""\\nNumber of trainable variables: {}"".format(len(data)) +\n        ""\\nNumber of parameters (elements): {}"".format(total) +\n        ""\\nStorage space needed for all trainable variables: {:.02f}MB"".format(size_mb),\n        \'cyan\')\n    logger.info(colored(""List of Trainable Variables: \\n"", \'cyan\') + table + summary_msg)\n\n\ndef get_shape_str(tensors):\n    """"""\n    Internally used by layer registry, to print shapes of inputs/outputs of layers.\n\n    Args:\n        tensors (list or tf.Tensor): a tensor or a list of tensors\n    Returns:\n        str: a string to describe the shape\n    """"""\n    if isinstance(tensors, (list, tuple)):\n        for v in tensors:\n            assert isinstance(v, (tf.Tensor, tf.Variable)), ""Not a tensor: {}"".format(type(v))\n        shape_str = "", "".join(map(get_shape_str, tensors))\n    else:\n        assert isinstance(tensors, (tf.Tensor, tf.Variable)), ""Not a tensor: {}"".format(type(tensors))\n        shape_str = str(tensors.get_shape().as_list()).replace(""None"", ""?"")\n    return shape_str\n'"
tensorpack/tfutils/optimizer.py,38,"b'# -*- coding: utf-8 -*-\n# File: optimizer.py\n\n\nfrom contextlib import contextmanager\nimport tensorflow as tf\n\nfrom ..tfutils.common import get_tf_version_tuple\nfrom ..compat import tfv1\nfrom ..utils.develop import HIDE_DOC\nfrom .gradproc import FilterNoneGrad, GradientProcessor\n\n__all__ = [\'apply_grad_processors\', \'ProxyOptimizer\',\n           \'PostProcessOptimizer\', \'VariableAssignmentOptimizer\',\n           \'AccumGradOptimizer\']\n\n\nclass ProxyOptimizer(tfv1.train.Optimizer):\n    """"""\n    A transparent proxy which delegates all methods of :class:`tf.train.Optimizer`\n    """"""\n    def __init__(self, opt, name=\'ProxyOptimizer\'):\n        assert isinstance(opt, tfv1.train.Optimizer), opt\n        super(ProxyOptimizer, self).__init__(False, name)\n        self._opt = opt\n\n    @HIDE_DOC\n    def compute_gradients(self, *args, **kwargs):\n        return self._opt.compute_gradients(*args, **kwargs)\n\n    @HIDE_DOC\n    def get_slot(self, *args, **kwargs):\n        return self._opt.get_slot(*args, **kwargs)\n\n    @HIDE_DOC\n    def get_slot_names(self, *args, **kwargs):\n        return self._opt.get_slot_names(*args, **kwargs)\n\n    @HIDE_DOC\n    def apply_gradients(self, *args, **kwargs):\n        return self._opt.apply_gradients(*args, **kwargs)\n\n\ndef apply_grad_processors(opt, gradprocs):\n    """"""\n    Wrapper around optimizers to apply gradient processors.\n\n    Args:\n        opt (tf.train.Optimizer):\n        gradprocs (list[GradientProcessor]): gradient processors to add to the\n            optimizer.\n\n    Returns:\n        a :class:`tf.train.Optimizer` instance which runs the gradient\n        processors before updating the variables.\n    """"""\n    assert isinstance(gradprocs, (list, tuple)), gradprocs\n    for gp in gradprocs:\n        assert isinstance(gp, GradientProcessor), gp\n\n    class _ApplyGradientProcessor(ProxyOptimizer):\n        def __init__(self, opt, gradprocs):\n            self._gradprocs = gradprocs[:]\n            super(_ApplyGradientProcessor, self).__init__(opt)\n\n        def apply_gradients(self, grads_and_vars,\n                            global_step=None, name=None):\n            g = self._apply(grads_and_vars)\n            return self._opt.apply_gradients(g, global_step, name)\n\n        def _apply(self, g):\n            for proc in self._gradprocs:\n                g = proc.process(g)\n            return g\n\n    return _ApplyGradientProcessor(opt, gradprocs)\n\n\nclass PostProcessOptimizer(ProxyOptimizer):\n    """"""\n    An optimizer which applies some ""post-processing operation"" per variable\n    (e.g. clipping, quantization) after the gradient update.\n    """"""\n    def __init__(self, opt, func, colocate=True):\n        """"""\n        Args:\n            opt (tf.train.Optimizer):\n            func (tf.Variable -> tf.Operation or None): the operation needed\n                to perform for this variable after the gradient update.\n            colocate (boolean): colocate the function with the variable. No effect since TF 1.13.\n        """"""\n        super(PostProcessOptimizer, self).__init__(opt)\n        self._func = func\n        self._colocate = colocate\n\n    @HIDE_DOC\n    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n        update_op = super(PostProcessOptimizer, self).apply_gradients(\n            grads_and_vars, global_step)\n        ops = []\n        with tf.control_dependencies([update_op]):\n            for _, var in grads_and_vars:\n                with self._maybe_colocate(var):\n                    op = self._func(var)\n                    if op is not None:\n                        assert isinstance(op, tf.Operation), op\n                        ops.append(op)\n        update_op = tf.group(update_op, *ops, name=name)\n        return update_op\n\n    @contextmanager\n    def _maybe_colocate(self, var):\n        G = tf.get_default_graph()\n        if self._colocate and get_tf_version_tuple() <= (1, 12):\n            with G.colocate_with(var):\n                yield\n        else:\n            yield\n\n\nclass VariableAssignmentOptimizer(PostProcessOptimizer):\n    """"""\n    An optimizer which assigns each variable a new value (e.g. clipping,\n    quantization) after the gradient update.\n    """"""\n    def __init__(self, opt, func):\n        """"""\n        Args:\n            opt (tf.train.Optimizer):\n            func (tf.Variable -> tf.Tensor or None): the new value to be\n                assigned to this variable after the gradient update.\n        """"""\n        def f(v):\n            t = func(v)\n            if t is None:\n                return t\n            return tf.assign(v, t, use_locking=False).op\n        super(VariableAssignmentOptimizer, self).__init__(opt, f)\n\n\nclass AccumGradOptimizer(ProxyOptimizer):\n    """"""\n    An optimizer which accumulates gradients across :math:`k` :meth:`minimize` executions,\n    and apply them together in every :math:`k` th :meth:`minimize` execution.\n    This is roughly the same as using a :math:`k` times larger batch size plus a\n    :math:`k` times larger learning rate, but uses much less memory.\n\n    This optimizer can be used in any TensorFlow code (with or without tensorpack).\n\n    Example:\n\n    .. code-block:: python\n\n        from tensorpack.tfutils.optimizer import AccumGradOptimizer\n        myopt = tf.train.GradientDescentOptimizer(0.01)\n        myopt = AccumGradOptimizer(myopt, niter=5)\n        train_op = myopt.minimize(loss)\n\n    """"""\n\n    def __init__(self, opt, niter):\n        """"""\n        Args:\n            opt (tf.train.Optimizer): the underlying sub-optimizer.\n            niter (int): number of iterations to accumulate gradients.\n        """"""\n        super(AccumGradOptimizer, self).__init__(opt, \'AccumGrad\')\n        self._niter = int(niter)\n\n    def _create_accum_slots(self, var_list):\n        slots = []\n        for v in var_list:\n            # TODO an option to not colocate the accumulators with variables (to save more memory)\n            s = self._zeros_slot(v, ""accum"", self._name)\n            slots.append(s)\n        return slots\n\n    @HIDE_DOC\n    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n        grads_and_vars = FilterNoneGrad().process(grads_and_vars)\n        vs = []\n        for g, v in grads_and_vars:\n            assert isinstance(g, (tf.Tensor, tf.IndexedSlices)) and isinstance(v, tf.Variable), \\\n                ""AccumGradOptimizer does not work for the gradient of {}! "" \\\n                ""Types of v and g are {} and {}"".format(v.op.name, type(v), type(g))\n            vs.append(v)\n\n        with tf.control_dependencies(None):\n            slots = self._create_accum_slots(vs)\n            slots_and_vars = [(s, gv[1]) for s, gv in zip(slots, grads_and_vars)]\n\n            with tf.variable_scope(self._name), tf.device(\'/cpu:0\'):\n                counter = tf.Variable(\n                    0, name=""counter"", trainable=False, dtype=tf.int32)\n\n        with tf.name_scope(\'AccumGradOptimizer\'):\n            ops = []\n            for s, gv in zip(slots, grads_and_vars):\n                g, v = gv\n                ops.append(s.assign_add(g))\n            update_counter = tf.assign_add(counter, 1, name=\'update_counter\')\n            update_slot_op = tf.group(update_counter, *ops, name=\'update_slot\')\n\n            def update_grad():\n                update_op = self._opt.apply_gradients(slots_and_vars)\n                with tf.control_dependencies([update_op]):\n                    clear_ops = [tf.assign(s, tf.zeros_like(s)) for s in slots]\n                return tf.group(*clear_ops, name=\'update_grad\')\n\n            pred = tf.equal(tf.mod(counter, self._niter), 0)\n            with tf.control_dependencies([update_slot_op]):\n                if name is None:\n                    name = \'cond_update_grad\'\n                op = tf.cond(pred, update_grad, tf.no_op)\n\n            if global_step is not None:\n                # Tensorpack maintains global_step by other means,\n                # so this option is useless in tensorpack trainers.\n                # But we include the implementation here for completeness\n                global_step_increment = tf.assign_add(global_step, 1)\n                op = tf.group(op, global_step_increment, name=name)\n            else:\n                op = tf.identity(op, name=name).op\n        return op\n\n\nif __name__ == \'__main__\':\n    # run it with ""python -m tensorpack.tfutils.optimizer""\n\n    x = tf.get_variable(\'x\', shape=[6])\n    cost = tf.reduce_sum(tf.abs(x), name=\'cost\')\n    opt = tf.train.GradientDescentOptimizer(0.01)\n    opt = AccumGradOptimizer(opt, 5)\n    min_op = opt.minimize(cost, global_step=tf.train.get_or_create_global_step())\n\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    with sess.as_default():\n        for _ in range(20):\n            min_op.run()\n            print(x.eval())\n        print(tf.train.get_or_create_global_step().eval())\n'"
tensorpack/tfutils/scope_utils.py,16,"b'# -*- coding: utf-8 -*-\n# File: scope_utils.py\n\n\nimport functools\nfrom contextlib import contextmanager\n\nfrom ..compat import tfv1 as tf\nfrom ..utils.argtools import graph_memoized\nfrom ..utils import logger\nfrom .common import get_tf_version_tuple\n\n__all__ = [\'auto_reuse_variable_scope\', \'cached_name_scope\', \'under_name_scope\']\n\n\ndef auto_reuse_variable_scope(func):\n    """"""\n    A decorator which automatically reuses the current variable scope if the\n    function has been called with the same variable scope before.\n\n    Example:\n\n    .. code-block:: python\n\n        @auto_reuse_variable_scope\n        def myfunc(x):\n            return tf.layers.conv2d(x, 128, 3)\n\n        myfunc(x1)  # will inherit parent scope reuse\n        myfunc(x2)  # will reuse\n        with tf.variable_scope(\'newscope\'):\n            myfunc(x3)  # will inherit parent scope reuse\n            myfunc(x4)  # will reuse\n    """"""\n    used_scope = set()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        scope = tf.get_variable_scope()\n        h = hash((tf.get_default_graph(), scope.name))\n        # print(""Entering "" + scope.name + "" reuse: "" + str(h in used_scope))\n        if h in used_scope:\n            if get_tf_version_tuple() >= (1, 5):\n                with tf.variable_scope(scope, reuse=True, auxiliary_name_scope=False):\n                    return func(*args, **kwargs)\n            else:\n                ns = tf.get_default_graph().get_name_scope()\n                with tf.variable_scope(scope, reuse=True), \\\n                        tf.name_scope(ns + \'/\' if ns else \'\'):\n                    return func(*args, **kwargs)\n        else:\n            used_scope.add(h)\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef under_name_scope(name_scope=None):\n    """"""\n    Args:\n        name_scope(str): the default scope to use. If None, will use the name of the function.\n\n    Returns:\n        A decorator which makes the function run under a name scope.\n        The name scope is obtained by the following:\n        1. The \'name_scope\' keyword argument when the decorated function is called.\n        2. The \'name_scope\' argument of the decorator.\n        3. (default) The name of the decorated function itself.\n\n        If the name is taken and cannot be used, a warning will be\n        printed in the first case.\n\n    Example:\n\n    .. code-block:: python\n\n        @under_name_scope()\n        def rms(x):\n            return tf.sqrt(\n                tf.reduce_mean(tf.square(x)))\n\n        rms(tensor)  # will be called under name scope \'rms\'\n        rms(tensor, name_scope=\'scope\')  # will be called under name scope \'scope\'\n\n\n    Todo:\n        Add a reuse option.\n    """"""\n\n    def _impl(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warn_incorrect_scope = \'name_scope\' in kwargs\n            scopename = kwargs.pop(\'name_scope\', name_scope)\n            if scopename is None:\n                scopename = func.__name__\n\n            if warn_incorrect_scope:\n                # cached_name_scope will try to reenter the existing scope\n                with cached_name_scope(scopename, top_level=False) as scope:\n                    scope = scope.strip(\'/\')\n                    # but it can still conflict with an existing tensor\n                    if not scope.endswith(scopename):\n                        logger.warn("""""" \\\nCalling function {} with name_scope=\'{}\', but actual name scope becomes \'{}\'.  \\\nThe name \'{}\' might be taken."""""".format(func.__name__, scopename, scope.split(\'/\')[-1], scopename))\n                    return func(*args, **kwargs)\n            else:\n                with tf.name_scope(scopename):\n                    return func(*args, **kwargs)\n        return wrapper\n    return _impl\n\n\ndef under_variable_scope():\n    """"""\n    Returns:\n        A decorator which makes the function happen under a variable scope,\n        which is named by the function itself.\n\n    Example:\n\n    .. code-block:: python\n\n        @under_variable_scope()\n        def mid_level(x):\n            with argscope(Conv2D, kernel_shape=3, nl=BNReLU):\n                x = Conv2D(\'conv1\', x, 512, stride=1)\n                x = Conv2D(\'conv2\', x, 256, stride=1)\n            return x\n\n    """"""\n\n    def _impl(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            name = func.__name__\n            with tf.variable_scope(name):\n                return func(*args, **kwargs)\n        return wrapper\n    return _impl\n\n\n@graph_memoized\ndef _get_cached_ns(name):\n    with tf.name_scope(None):\n        with tf.name_scope(name) as scope:\n            return scope\n\n\n@contextmanager\ndef cached_name_scope(name, top_level=True):\n    """"""\n    Return a context which either opens and caches a new name scope,\n    or reenter an existing one.\n\n    Args:\n        top_level(bool): if True, the name scope will always be top-level.\n            It will not be nested under any existing name scope of the caller.\n    """"""\n    if not top_level:\n        current_ns = tf.get_default_graph().get_name_scope()\n        if current_ns:\n            name = current_ns + \'/\' + name\n    ns = _get_cached_ns(name)\n    with tf.name_scope(ns):\n        yield ns\n'"
tensorpack/tfutils/sesscreate.py,11,"b'# -*- coding: utf-8 -*-\n# File: sesscreate.py\n\n\nfrom ..compat import tfv1 as tf\nfrom ..utils import logger\nfrom .common import get_default_sess_config\n\n__all__ = [\'NewSessionCreator\', \'ReuseSessionCreator\', \'SessionCreatorAdapter\']\n\n""""""\nA SessionCreator should:\n    create the session\n    initialize all variables\n    return a session that is ready to use\n    not finalize the graph\n""""""\n\n\n_WRN1 = """"""User-provided custom session config may not work due to TF bugs. If you saw logs like\n```\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\n```\nbefore this line, then your GPU has been initialized and custom GPU options may not take effect. """"""\n\n_WRN2 = """"""To workaround this issue, you can do one of the following:\n1. Avoid initializing the GPU too early. Find code that initializes the GPU and skip it.\n   Typically examples are: creating a session; check GPU availability; check GPU number.\n2. Manually set your GPU options earlier. You can create a session with custom\n   GPU options at the beginning of your program, as described in\n   https://github.com/tensorpack/tensorpack/issues/497\n""""""\n\n\nclass NewSessionCreator(tf.train.SessionCreator):\n    def __init__(self, target=\'\', config=None):\n        """"""\n        Args:\n            target, config: same as :meth:`Session.__init__()`.\n            config: a :class:`tf.ConfigProto` instance, defaults to :func:`tfutils.get_default_sess_config()`\n        """"""\n        self.target = target\n\n        if config is None:\n            # distributed trainer doesn\'t support user-provided config\n            # we set this attribute so that they can check\n            self.user_provided_config = False\n            config = get_default_sess_config()\n        else:\n            self.user_provided_config = True\n            logger.warn(_WRN1)\n            logger.warn(_WRN2)\n        self.config = config\n\n    def create_session(self):\n        sess = tf.Session(target=self.target, config=self.config)\n\n        def blocking_op(x):\n            """"""\n            Whether an op is possibly blocking.\n            """"""\n            if x.op_def is not None and not x.op_def.is_stateful:\n                return False\n            if ""Dequeue"" in x.type or ""Enqueue"" in x.type:\n                return True\n            if ""Unstage"" in x.type:\n                return True\n            if x.type in [""ZMQPull""]:\n                return True\n            return False\n\n        def run(op):\n            try:\n                from tensorflow.contrib.graph_editor import get_backward_walk_ops  # deprecated\n            except ImportError:\n                from tensorflow.python.ops.op_selector import get_backward_walk_ops\n\n            deps = get_backward_walk_ops(op, control_inputs=True)\n            for dep_op in deps:\n                if blocking_op(dep_op):\n                    logger.warn(\n                        ""Initializer \'{}\' depends on a blocking op \'{}\'. ""\n                        ""This initializer is likely to hang!"".format(\n                            op.name, dep_op.name))\n\n            sess.run(op)\n\n        run(tf.global_variables_initializer())\n        run(tf.local_variables_initializer())\n        run(tf.tables_initializer())\n        return sess\n\n\nclass ReuseSessionCreator(tf.train.SessionCreator):\n    """"""\n    Returns an existing session.\n    """"""\n    def __init__(self, sess):\n        """"""\n        Args:\n            sess (tf.Session): the session to reuse\n        """"""\n        self.sess = sess\n\n    def create_session(self):\n        return self.sess\n\n\nclass SessionCreatorAdapter(tf.train.SessionCreator):\n    """"""\n    Apply a function on the output of a SessionCreator. Can be used to create a debug session.\n\n    Note:\n    Since TF 1.6, debug session may not work properly with Monitored session.\n    This is a tensorflow bug. To use tfdbg, use the :class:`TFLocalCLIDebugHook` callback instead.\n    """"""\n    def __init__(self, session_creator, func):\n        """"""\n        Args:\n            session_creator (tf.train.SessionCreator): a session creator\n            func (tf.Session -> tf.Session): takes a session created by\n            ``session_creator``, and return a new session to be returned by ``self.create_session``\n        """"""\n        self._creator = session_creator\n        self._func = func\n\n    def create_session(self):\n        sess = self._creator.create_session()\n        return self._func(sess)\n'"
tensorpack/tfutils/sessinit.py,8,"b'# -*- coding: utf-8 -*-\n# File: sessinit.py\n\nimport os\nimport numpy as np\nimport six\n\nfrom ..compat import tfv1 as tf\nfrom ..utils import logger\nfrom .common import get_op_tensor_name\nfrom .varmanip import SessionUpdate, get_checkpoint_path, get_savename_from_varname, is_training_name\n\n__all__ = [\'SessionInit\', \'ChainInit\',\n           \'SaverRestore\', \'SaverRestoreRelaxed\', \'DictRestore\',\n           \'JustCurrentSession\', \'get_model_loader\', \'SmartInit\']\n\n\nclass SessionInit(object):\n    """""" Base class for utilities to load variables to a (existing) session. """"""\n    def init(self, sess):\n        """"""\n        Initialize a session\n\n        Args:\n            sess (tf.Session): the session\n        """"""\n        self._setup_graph()\n        self._run_init(sess)\n\n    def _setup_graph(self):\n        pass\n\n    def _run_init(self, sess):\n        pass\n\n\nclass JustCurrentSession(SessionInit):\n    """""" This is a no-op placeholder""""""\n    pass\n\n\nclass CheckpointReaderAdapter(object):\n    """"""\n    An adapter to work around old checkpoint format, where the keys are op\n    names instead of tensor names (with :0).\n    """"""\n    def __init__(self, reader):\n        self._reader = reader\n        m = self._reader.get_variable_to_shape_map()\n        self._map = {k if k.endswith(\':0\') else k + \':0\': v\n                     for k, v in six.iteritems(m)}\n\n    def get_variable_to_shape_map(self):\n        return self._map\n\n    def get_tensor(self, name):\n        if self._reader.has_tensor(name):\n            return self._reader.get_tensor(name)\n        if name in self._map:\n            assert name.endswith(\':0\'), name\n            name = name[:-2]\n        return self._reader.get_tensor(name)\n\n    def has_tensor(self, name):\n        return name in self._map\n\n    # some checkpoint might not have \':0\'\n    def get_real_name(self, name):\n        if self._reader.has_tensor(name):\n            return name\n        assert self.has_tensor(name)\n        return name[:-2]\n\n\nclass MismatchLogger(object):\n    def __init__(self, exists, nonexists):\n        self._exists = exists\n        self._nonexists = nonexists\n        self._names = []\n\n    def add(self, name):\n        self._names.append(get_op_tensor_name(name)[0])\n\n    def log(self):\n        if len(self._names):\n            logger.warn(""The following variables are in the {}, but not found in the {}: {}"".format(\n                self._exists, self._nonexists, \', \'.join(self._names)))\n\n\nclass SaverRestore(SessionInit):\n    """"""\n    Restore a tensorflow checkpoint saved by :class:`tf.train.Saver` or :class:`ModelSaver`.\n    """"""\n    def __init__(self, model_path, prefix=None, ignore=()):\n        """"""\n        Args:\n            model_path (str): a model name (model-xxxx) or a ``checkpoint`` file.\n            prefix (str): during restore, add a ``prefix/`` for every variable in this checkpoint.\n            ignore (tuple[str]): tensor names that should be ignored during loading, e.g. learning-rate\n        """"""\n        if model_path.endswith(\'.npy\') or model_path.endswith(\'.npz\'):\n            logger.warn(""SaverRestore expect a TF checkpoint, but got a model path \'{}\'."".format(model_path) +\n                        "" To load from a dict, use \'DictRestore\'."")\n        model_path = get_checkpoint_path(model_path)\n        self.path = model_path  # attribute used by AutoResumeTrainConfig!\n        self.prefix = prefix\n        self.ignore = [i if i.endswith(\':0\') else i + \':0\' for i in ignore]\n\n    def _setup_graph(self):\n        dic = self._get_restore_dict()\n        self.saver = tf.train.Saver(var_list=dic, name=str(id(dic)))\n\n    def _run_init(self, sess):\n        logger.info(""Restoring checkpoint from {} ..."".format(self.path))\n        self.saver.restore(sess, self.path)\n\n    @staticmethod\n    def _read_checkpoint_vars(model_path):\n        """""" return a set of strings """"""\n        reader = tf.train.NewCheckpointReader(model_path)\n        reader = CheckpointReaderAdapter(reader)    # use an adapter to standardize the name\n        ckpt_vars = reader.get_variable_to_shape_map().keys()\n        return reader, set(ckpt_vars)\n\n    def _match_vars(self, func):\n        reader, chkpt_vars = SaverRestore._read_checkpoint_vars(self.path)\n        graph_vars = tf.global_variables()\n        chkpt_vars_used = set()\n\n        mismatch = MismatchLogger(\'graph\', \'checkpoint\')\n        for v in graph_vars:\n            name = get_savename_from_varname(v.name, varname_prefix=self.prefix)\n            if name in self.ignore and reader.has_tensor(name):\n                logger.info(""Variable {} in the graph will not be loaded from the checkpoint!"".format(name))\n            else:\n                if reader.has_tensor(name):\n                    func(reader, name, v)\n                    chkpt_vars_used.add(name)\n                else:\n                    # use tensor name (instead of op name) for logging, to be consistent with the reverse case\n                    if not is_training_name(v.name):\n                        mismatch.add(v.name)\n        mismatch.log()\n        mismatch = MismatchLogger(\'checkpoint\', \'graph\')\n        if len(chkpt_vars_used) < len(chkpt_vars):\n            unused = chkpt_vars - chkpt_vars_used\n            for name in sorted(unused):\n                if not is_training_name(name):\n                    mismatch.add(name)\n        mismatch.log()\n\n    def _get_restore_dict(self):\n        var_dict = {}\n\n        def f(reader, name, v):\n            name = reader.get_real_name(name)\n            assert name not in var_dict, ""Restore conflict: {} and {}"".format(v.name, var_dict[name].name)\n            var_dict[name] = v\n        self._match_vars(f)\n        return var_dict\n\n\nclass SaverRestoreRelaxed(SaverRestore):\n    """""" Same as :class:`SaverRestore`, but has more relaxed constraints.\n\n        It allows upcasting certain variables, or reshape certain\n        variables when there is a mismatch that can be fixed.\n\n        When variable shape and value shape do not match, it will print a\n        warning but will not crash.\n\n        Another advantage is that it doesn\'t add any new ops to the graph.\n    """"""\n    def _run_init(self, sess):\n        logger.info(\n            ""Restoring checkpoint from {} ..."".format(self.path))\n\n        matched_pairs = []\n\n        def f(reader, name, v):\n            val = reader.get_tensor(name)\n            val = SessionUpdate.relaxed_value_for_var(val, v, ignore_mismatch=True)\n            if val is not None:\n                matched_pairs.append((v, val))\n\n        with sess.as_default():\n            self._match_vars(f)\n            upd = SessionUpdate(sess, [x[0] for x in matched_pairs])\n            upd.update({x[0].name: x[1] for x in matched_pairs})\n\n\nclass DictRestore(SessionInit):\n    """"""\n    Restore variables from a dictionary.\n    """"""\n\n    def __init__(self, variable_dict, ignore_mismatch=False):\n        """"""\n        Args:\n            variable_dict (dict): a dict of {name: value}\n            ignore_mismatch (bool): ignore failures when the value and the\n                variable does not match in their shapes.\n                If False, it will throw exception on such errors.\n                If True, it will only print a warning.\n        """"""\n        assert isinstance(variable_dict, dict), type(variable_dict)\n        # use varname (with :0) for consistency\n        self._prms = {get_op_tensor_name(n)[1]: v for n, v in six.iteritems(variable_dict)}\n        self._ignore_mismatch = ignore_mismatch\n\n    def _run_init(self, sess):\n        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n        variable_names_list = [k.name for k in variables]\n\n        variable_names = set(variable_names_list)\n        param_names = set(six.iterkeys(self._prms))\n\n        # intersect has the original ordering of variables\n        intersect = [v for v in variable_names_list if v in param_names]\n\n        # use opname (without :0) for clarity in logging\n        logger.info(""Variables to restore from dict: {}"".format(\n            \', \'.join(get_op_tensor_name(x)[0] for x in intersect)))\n\n        mismatch = MismatchLogger(\'graph\', \'dict\')\n        for k in sorted(variable_names - param_names):\n            if not is_training_name(k):\n                mismatch.add(k)\n        mismatch.log()\n        mismatch = MismatchLogger(\'dict\', \'graph\')\n        for k in sorted(param_names - variable_names):\n            mismatch.add(k)\n        mismatch.log()\n\n        upd = SessionUpdate(sess, [v for v in variables if v.name in intersect], ignore_mismatch=self._ignore_mismatch)\n        logger.info(""Restoring {} variables from dict ..."".format(len(intersect)))\n        upd.update({name: value for name, value in six.iteritems(self._prms) if name in intersect})\n\n\nclass ChainInit(SessionInit):\n    """"""\n    Initialize a session by a list of :class:`SessionInit` instance, executed one by one.\n    This can be useful for, e.g., loading several models from different files\n    to form a composition of models.\n    """"""\n\n    def __init__(self, sess_inits):\n        """"""\n        Args:\n            sess_inits (list[SessionInit]): list of :class:`SessionInit` instances.\n        """"""\n        self.inits = sess_inits\n\n    def _setup_graph(self):\n        for i in self.inits:\n            i._setup_graph()\n\n    def _run_init(self, sess):\n        for i in self.inits:\n            i._run_init(sess)\n\n\ndef SmartInit(obj, *, ignore_mismatch=False):\n    """"""\n    Create a :class:`SessionInit` to be loaded to a session,\n    automatically from any supported objects, with some smart heuristics.\n    The object can be:\n\n    + A TF checkpoint\n    + A dict of numpy arrays\n    + A npz file, to be interpreted as a dict\n    + An empty string or None, in which case the sessinit will be a no-op\n    + A list of supported objects, to be initialized one by one\n\n    Args:\n        obj: a supported object\n        ignore_mismatch (bool): ignore failures when the value and the\n            variable does not match in their shapes.\n            If False, it will throw exception on such errors.\n            If True, it will only print a warning.\n\n    Returns:\n        SessionInit:\n    """"""\n    if not obj:\n        return JustCurrentSession()\n    if isinstance(obj, list):\n        return ChainInit([SmartInit(x, ignore_mismatch=ignore_mismatch) for x in obj])\n    if isinstance(obj, six.string_types):\n        obj = os.path.expanduser(obj)\n        if obj.endswith("".npy"") or obj.endswith("".npz""):\n            assert tf.gfile.Exists(obj), ""File {} does not exist!"".format(obj)\n            filename = obj\n            logger.info(""Loading dictionary from {} ..."".format(filename))\n            if filename.endswith(\'.npy\'):\n                obj = np.load(filename, encoding=\'latin1\').item()\n            elif filename.endswith(\'.npz\'):\n                obj = dict(np.load(filename))\n        elif len(tf.gfile.Glob(obj + ""*"")):\n            # Assume to be a TF checkpoint.\n            # A TF checkpoint must be a prefix of an actual file.\n            return (SaverRestoreRelaxed if ignore_mismatch else SaverRestore)(obj)\n        else:\n            raise ValueError(""Invalid argument to SmartInit: "" + obj)\n\n    if isinstance(obj, dict):\n        return DictRestore(obj, ignore_mismatch=ignore_mismatch)\n    raise ValueError(""Invalid argument to SmartInit: "" + type(obj))\n\n\nget_model_loader = SmartInit\n'"
tensorpack/tfutils/summary.py,33,"b'# -*- coding: utf-8 -*-\n# File: summary.py\n\n\nimport re\nfrom contextlib import contextmanager\nimport six\nfrom six.moves import range\nfrom tensorflow.python.training import moving_averages\n\nfrom ..compat import tfv1 as tf\nfrom ..utils import logger\nfrom ..utils.argtools import graph_memoized\nfrom ..utils.naming import MOVING_SUMMARY_OPS_KEY\nfrom .scope_utils import cached_name_scope\nfrom .symbolic_functions import rms\nfrom .tower import get_current_tower_context\n\n__all__ = [\'add_tensor_summary\', \'add_param_summary\',\n           \'add_activation_summary\', \'add_moving_summary\',\n           ]\n\n\n# some scope stuff to use internally...\n@graph_memoized\ndef _get_cached_vs(name):\n    with tf.variable_scope(name) as scope:\n        return scope\n\n\n@contextmanager\ndef _enter_vs_reuse_ns(name):\n    vs = _get_cached_vs(name)\n    # XXX Not good to enter the cached vs directly, because this will clean-up custom getter\n    # with tf.variable_scope(name, reuse=tf.AUTO_REUSE):    # available in 1.4 only\n    with tf.variable_scope(vs):\n        with tf.name_scope(vs.original_name_scope):\n            yield vs\n\n\ndef create_scalar_summary(name, v):\n    """"""\n    Args:\n        name (str):\n        v (float): scalar value\n    Returns:\n        tf.Summary: a tf.Summary object with name and simple scalar value v.\n    """"""\n    assert isinstance(name, six.string_types), type(name)\n    v = float(v)\n    s = tf.Summary()\n    s.value.add(tag=name, simple_value=v)\n    return s\n\n\ndef create_image_summary(name, val):\n    """"""\n    Args:\n        name(str):\n        val(np.ndarray): 4D tensor of NHWC. assume RGB if C==3.\n            Can be either float or uint8. Range has to be [0,255].\n\n    Returns:\n        tf.Summary:\n    """"""\n    assert isinstance(name, six.string_types), type(name)\n    n, h, w, c = val.shape\n    val = val.astype(\'uint8\')\n    s = tf.Summary()\n    imparams = [cv2.IMWRITE_PNG_COMPRESSION, 9]\n    for k in range(n):\n        arr = val[k]\n        # CV2 will only write correctly in BGR chanel order\n        if c == 3:\n            arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n        elif c == 4:\n            arr = cv2.cvtColor(arr, cv2.COLOR_RGBA2BGRA)\n        tag = name if n == 1 else \'{}/{}\'.format(name, k)\n        retval, img_str = cv2.imencode(\'.png\', arr, imparams)\n        if not retval:\n            # Encoding has failed.\n            continue\n        img_str = img_str.tostring()\n\n        img = tf.Summary.Image()\n        img.height = h\n        img.width = w\n        # 1 - grayscale 3 - RGB 4 - RGBA\n        img.colorspace = c\n        img.encoded_image_string = img_str\n        s.value.add(tag=tag, image=img)\n    return s\n\n\ndef add_tensor_summary(x, types, name=None, collections=None,\n                       main_tower_only=True):\n    """"""\n    Summarize a tensor by different methods.\n\n    Args:\n        x (tf.Tensor): a tensor to summarize\n        types (list[str]): summary types, can be scalar/histogram/sparsity/mean/rms\n        name (str): summary name. Defaults to be the op name.\n        collections (list[str]): collections of the summary ops.\n        main_tower_only (bool): Only run under main training tower. If\n            set to True, calling this function under other TowerContext\n            has no effect.\n\n    Example:\n\n    .. code-block:: python\n\n        with tf.name_scope(\'mysummaries\'):  # to not mess up tensorboard\n            add_tensor_summary(\n                tensor, [\'histogram\', \'rms\', \'sparsity\'], name=\'mytensor\')\n    """"""\n    types = set(types)\n    if name is None:\n        name = x.op.name\n    ctx = get_current_tower_context()\n    if main_tower_only and ctx is not None and not ctx.is_main_training_tower:\n        return\n\n    SUMMARY_TYPES_DIC = {\n        \'scalar\': lambda: tf.summary.scalar(name + \'-summary\', x, collections=collections),\n        \'histogram\': lambda: tf.summary.histogram(name + \'-histogram\', x, collections=collections),\n        \'sparsity\': lambda: tf.summary.scalar(\n            name + \'-sparsity\', tf.nn.zero_fraction(x),\n            collections=collections),\n        \'mean\': lambda: tf.summary.scalar(\n            name + \'-mean\', tf.reduce_mean(x),\n            collections=collections),\n        \'rms\': lambda: tf.summary.scalar(\n            name + \'-rms\', rms(x), collections=collections)\n    }\n    for typ in types:\n        SUMMARY_TYPES_DIC[typ]()\n\n\ndef add_activation_summary(x, types=None, name=None, collections=None):\n    """"""\n    Call :func:`add_tensor_summary` under a reused \'activation-summary\' name scope.\n    This function is a no-op if not calling from main training tower.\n\n    Args:\n        x (tf.Tensor): the tensor to summary.\n        types (list[str]): summary types, defaults to ``[\'sparsity\', \'rms\', \'histogram\']``.\n        name (str): if is None, use x.name.\n        collections (list[str]): collections of the summary ops.\n    """"""\n    ndim = x.get_shape().ndims\n    if ndim < 2:\n        logger.warn(""Cannot summarize scalar activation {}"".format(x.name))\n        return\n    if types is None:\n        types = [\'sparsity\', \'rms\', \'histogram\']\n    with cached_name_scope(\'activation-summary\'):\n        add_tensor_summary(x, types, name=name, collections=collections)\n\n\ndef add_param_summary(*summary_lists, **kwargs):\n    """"""\n    Add summary ops for all trainable variables matching the regex, under a\n    reused \'param-summary\' name scope.\n    This function is a no-op if not calling from main training tower.\n\n    Args:\n        summary_lists (list): each is (regex, [list of summary type]).\n            Summary type is defined in :func:`add_tensor_summary`.\n        collections (list[str]): collections of the summary ops.\n\n    Example:\n\n    .. code-block:: python\n\n        add_param_summary(\n            (\'.*/W\', [\'histogram\', \'rms\']),\n            (\'.*/gamma\', [\'scalar\']),\n        )\n    """"""\n    collections = kwargs.pop(\'collections\', None)\n    assert len(kwargs) == 0, ""Unknown kwargs: "" + str(kwargs)\n    ctx = get_current_tower_context()\n    if ctx is not None and not ctx.is_main_training_tower:\n        return\n\n    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    with cached_name_scope(\'param-summary\'):\n        for p in params:\n            name = p.op.name\n            for rgx, actions in summary_lists:\n                if not rgx.endswith(\'$\'):\n                    rgx = rgx + \'$\'\n                if re.match(rgx, name):\n                    add_tensor_summary(p, actions, name=name, collections=collections)\n\n\ndef add_moving_summary(*args, **kwargs):\n    """"""\n    Summarize the moving average for scalar tensors.\n    This function is a no-op if not calling from main training tower.\n    See tutorial at https://tensorpack.readthedocs.io/tutorial/summary.html\n\n    Args:\n        args: scalar tensors to summarize\n        decay (float): the decay rate. Defaults to 0.95.\n        collection (str or None): the name of the collection to add EMA-maintaining ops.\n            The default will work together with the default\n            :class:`MovingAverageSummary` callback.\n        summary_collections ([str]): the names of collections to add the\n            summary op. Default is TF\'s default (`tf.GraphKeys.SUMMARIES`).\n\n    Returns:\n        [tf.Tensor]:\n            list of tensors returned by assign_moving_average,\n            which can be used to maintain the EMA.\n    """"""\n    decay = kwargs.pop(\'decay\', 0.95)\n    coll = kwargs.pop(\'collection\', MOVING_SUMMARY_OPS_KEY)\n    summ_coll = kwargs.pop(\'summary_collections\', None)\n    assert len(kwargs) == 0, ""Unknown arguments: "" + str(kwargs)\n\n    ctx = get_current_tower_context()\n    # allow ctx to be none\n    if ctx is not None and not ctx.is_main_training_tower:\n        return []\n\n    graph = tf.get_default_graph()\n    try:\n        control_flow_ctx = graph._get_control_flow_context()\n        # XLA does not support summaries anyway\n        # However, this function will generate unnecessary dependency edges,\n        # which makes the tower function harder to compile under XLA, so we skip it\n        if control_flow_ctx is not None and control_flow_ctx.IsXLAContext():\n            return\n    except Exception:\n        pass\n\n    if tf.get_variable_scope().reuse is True:\n        logger.warn(""add_moving_summary() called under reuse=True scope, ignored."")\n        return []\n\n    for x in args:\n        assert isinstance(x, (tf.Tensor, tf.Variable)), x\n        assert x.get_shape().ndims == 0, \\\n            ""add_moving_summary() only accepts scalar tensor! Got one with {}"".format(x.get_shape())\n\n    ema_ops = []\n    for c in args:\n        name = re.sub(\'tower[0-9]+/\', \'\', c.op.name)\n        with tf.name_scope(None):\n            if not c.dtype.is_floating:\n                c = tf.cast(c, tf.float32)\n            # assign_moving_average creates variables with op names, therefore clear ns first.\n            with _enter_vs_reuse_ns(\'EMA\') as vs:\n                ema_var = tf.get_variable(name, shape=c.shape, dtype=c.dtype,\n                                          initializer=tf.constant_initializer(),\n                                          trainable=False)\n                ns = vs.original_name_scope\n            with tf.name_scope(ns):     # reuse VS&NS so that EMA_1 won\'t appear\n                ema_op = moving_averages.assign_moving_average(\n                    ema_var, c, decay,\n                    zero_debias=True, name=name + \'_EMA_apply\')\n            ema_ops.append(ema_op)\n        with tf.name_scope(None):\n            tf.summary.scalar(\n                name + \'-summary\', ema_op,\n                collections=summ_coll)    # write the EMA value as a summary\n    if coll is not None:\n        for op in ema_ops:\n            tf.add_to_collection(coll, op)\n    return ema_ops\n\n\ntry:\n    import cv2\nexcept ImportError:\n    from ..utils.develop import create_dummy_func\n    create_image_summary = create_dummy_func(\'create_image_summary\', \'cv2\')  # noqa\n'"
tensorpack/tfutils/symbolic_functions.py,14,"b'# -*- coding: utf-8 -*-\n# File: symbolic_functions.py\n\n\nimport tensorflow as tf\n\nfrom ..compat import tfv1\n\n__all__ = [\'print_stat\', \'rms\']\n\n\ndef print_stat(x, message=None):\n    """""" A simple print Op that might be easier to use than :meth:`tf.Print`.\n        Use it like: ``x = print_stat(x, message=\'This is x\')``.\n    """"""\n    if message is None:\n        message = x.op.name\n    lst = [tf.shape(x), tf.reduce_mean(x)]\n    if x.dtype.is_floating:\n        lst.append(rms(x))\n    return tf.Print(x, lst + [x], summarize=20,\n                    message=message, name=\'print_\' + x.op.name)\n\n\n# for internal use only\ndef rms(x, name=None):\n    """"""\n    Returns:\n        root mean square of tensor x.\n    """"""\n    if name is None:\n        name = x.op.name + \'/rms\'\n        with tfv1.name_scope(None):   # name already contains the scope\n            return tf.sqrt(tf.reduce_mean(tf.square(x)), name=name)\n    return tf.sqrt(tf.reduce_mean(tf.square(x)), name=name)\n\n\n# don\'t hurt to leave it here\ndef psnr(prediction, ground_truth, maxp=None, name=\'psnr\'):\n    """"""`Peak Signal to Noise Ratio <https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio>`_.\n\n    .. math::\n\n        PSNR = 20 \\cdot \\log_{10}(MAX_p) - 10 \\cdot \\log_{10}(MSE)\n\n    Args:\n        prediction: a :class:`tf.Tensor` representing the prediction signal.\n        ground_truth: another :class:`tf.Tensor` with the same shape.\n        maxp: maximum possible pixel value of the image (255 in in 8bit images)\n\n    Returns:\n        A scalar tensor representing the PSNR\n    """"""\n\n    maxp = float(maxp)\n\n    def log10(x):\n        with tf.name_scope(""log10""):\n            numerator = tf.log(x)\n            denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n            return numerator / denominator\n\n    mse = tf.reduce_mean(tf.square(prediction - ground_truth))\n    if maxp is None:\n        psnr = tf.multiply(log10(mse), -10., name=name)\n    else:\n        psnr = tf.multiply(log10(mse), -10.)\n        psnr = tf.add(tf.multiply(20., log10(maxp)), psnr, name=name)\n\n    return psnr\n'"
tensorpack/tfutils/tower.py,12,"b'# -*- coding: utf-8 -*-\n# File: tower.py\n\n\nfrom abc import ABCMeta, abstractmethod, abstractproperty\nimport six\n\nfrom ..compat import tfv1 as tf\nfrom ..utils import logger\nfrom ..utils.argtools import call_only_once\nfrom ..utils.develop import HIDE_DOC\nfrom ..utils.naming import MOVING_SUMMARY_OPS_KEY\nfrom .collection import CollectionGuard\nfrom .common import get_op_or_tensor_by_name, get_op_tensor_name\n\n__all__ = [\'get_current_tower_context\', \'BaseTowerContext\', \'TowerContext\',\n           \'TowerFuncWrapper\', \'TowerFunc\',\n           \'TowerTensorHandle\', \'TowerTensorHandles\']\n\n_CurrentTowerContext = None\n\n\n@six.add_metaclass(ABCMeta)\nclass BaseTowerContext(object):\n    """""" A context where the current model is built in.\n        You need to use :func:`TowerContext` to create a :class:`BaseTowerContext`.\n    """"""\n\n    @HIDE_DOC\n    def __init__(self, ns_name, vs_name=\'\'):\n        """"""\n        This is not supposed to be used by users.\n        You need to use :func:`TowerContext` to create a :class:`BaseTowerContext`.\n\n        Args:\n            ns_name (str): The name scope of the tower.\n            vs_name (str): Open a new variable scope with this name.\n        """"""\n        self._name = ns_name\n\n        self._vs_name = vs_name\n        if len(vs_name):\n            assert len(ns_name), ""TowerContext(vs_name) cannot be used with an empty name!""\n\n    @abstractproperty\n    def is_main_training_tower(self):\n        """"""\n        bool: Whether this tower is the main (i.e., the first) training tower.\n        """"""\n        pass\n\n    @abstractproperty\n    def has_own_variables(self):\n        """"""\n        bool: Whether this tower is supposed to have its own trainable variables.\n        """"""\n        pass\n\n    @property\n    def name(self):\n        """"""\n        str: The name scope of the tower.\n        """"""\n        return self._name\n\n    @property\n    def vs_name(self):\n        """"""\n        str: The variable scope of the tower.\n        """"""\n        return self._vs_name\n\n    @property\n    def ns_name(self):\n        """"""\n        str: The name scope of the tower.\n        """"""\n        return self._name\n\n    def get_collection_in_tower(self, key):\n        """"""\n        From a collection, get items that are __added__ to the collection in this tower.\n\n        Note that it works by tracking the collection at the beginning and end of\n        the tower function.\n        Therefore it does not guarantee that the items are __created__ in this tower.\n        """"""\n        return self._collection_guard.get_collection_in_tower(key)\n\n    @call_only_once\n    def _get_scopes(self):\n        """"""\n        Returns the ns and vs for this tower.\n        """"""\n        if not len(self._name):\n            # work around https://github.com/tensorflow/tensorflow/issues/14703\n            return [tf.variable_scope(tf.get_variable_scope())]\n\n        ret = []\n\n        if len(self._vs_name):\n            ret.append(tf.variable_scope(self._vs_name))\n        else:\n            # caller should have handled reuse outside of TowerContext\n            ret.append(tf.variable_scope(tf.get_variable_scope()))\n\n        # always clear existing ns  # TODO check existing ns\n        if len(self._name):\n            ret.append(tf.name_scope(self._name + \'/\'))\n        return ret\n\n    @abstractmethod\n    def _keys_to_freeze(self):\n        pass\n\n    def __enter__(self):\n        global _CurrentTowerContext\n        assert _CurrentTowerContext is None, ""Cannot nest TowerContext!""\n        _CurrentTowerContext = self\n\n        self._collection_guard = CollectionGuard(\n            self._name,\n            check_diff=not self.is_main_training_tower,\n            freeze_keys=self._keys_to_freeze())\n\n        self._ctxs = self._get_scopes()\n        self._ctxs.append(self._collection_guard)\n        for c in self._ctxs:\n            c.__enter__()\n\n        # check that ns_name is always the same as _name\n        ns = tf.get_default_graph().get_name_scope()\n        assert ns == self._name, \\\n            ""Name conflict: name_scope inside tower \'{}\' becomes \'{}\'!"".format(self._name, ns) \\\n            + "" You may need a different name for the tower!""\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        global _CurrentTowerContext\n        _CurrentTowerContext = None\n\n        if not self.has_own_variables:\n            diff_trainable_vars = self._collection_guard.get_collection_in_tower(tf.GraphKeys.TRAINABLE_VARIABLES)\n            assert len(diff_trainable_vars) == 0,  \\\n                ""New TRAINABLE_VARIABLES shouldn\'t be created in {}: "".format(\n                    self._name) + \', \'.join([k.name for k in diff_trainable_vars])\n        for c in self._ctxs[::-1]:\n            c.__exit__(exc_type, exc_val, exc_tb)\n        return False\n\n    def __str__(self):\n        return ""TowerContext(name={}, is_training={})"".format(\n            self._name, self._is_training)\n\n    @property\n    def is_training(self):\n        """"""\n        bool: whether the context is training or not\n        """"""\n        return self._is_training\n\n\nclass TrainTowerContext(BaseTowerContext):\n\n    def __init__(self, ns_name, vs_name=\'\', index=0, total=1):\n        """"""\n        Args:\n            index (int): index of this tower, only used in training.\n            total (int): total number of towers to be built.\n        """"""\n        super(TrainTowerContext, self).__init__(ns_name, vs_name)\n        self._is_training = True\n\n        self.index = int(index)\n        self.total = int(total)\n        if self.index > 0:\n            assert self.total > self.index, ""(index, total) = ({}, {})"".format(self.index, self.total)\n\n        vs = tf.get_variable_scope()\n        assert vs.name == \'\', ""Cannot nest TrainTowerContext with an existing variable scope!""\n        if vs_name:\n            assert not vs.reuse, \\\n                ""Cannot create tower {} with vs_name={} under reuse=True!"".format(ns_name, vs_name)\n        self._original_vs_reuse = vs.reuse\n\n    @property\n    def is_main_training_tower(self):\n        return self.index == 0\n\n    @property\n    def has_own_variables(self):\n        if self._original_vs_reuse:\n            return False\n        return self.index == 0 or len(self._vs_name) > 0\n\n    def _keys_to_freeze(self):\n        if self.index == 0:\n            return []\n        return [tf.GraphKeys.SUMMARIES, MOVING_SUMMARY_OPS_KEY]\n\n\nclass PredictTowerContext(BaseTowerContext):\n    def __init__(self, ns_name, vs_name=\'\'):\n        super(PredictTowerContext, self).__init__(ns_name, vs_name)\n        self._is_training = False\n\n        self._initial_vs_reuse = tf.get_variable_scope().reuse\n\n    @property\n    def has_own_variables(self):\n        return not self._initial_vs_reuse\n\n    @property\n    def is_main_training_tower(self):\n        return False\n\n    def _keys_to_freeze(self):\n        # freeze UPDATE_OPS during inference because they should never be used\n        return [tf.GraphKeys.SUMMARIES, MOVING_SUMMARY_OPS_KEY, tf.GraphKeys.UPDATE_OPS]\n\n\ndef get_current_tower_context():\n    """"""\n    When called inside a TowerContext, returns the TowerContext.\n\n    Returns:\n        a :class:`BaseTowerContext` instance or None, if not called under a TowerContext.\n    """"""\n    return _CurrentTowerContext\n\n\ndef TowerContext(tower_name, is_training, vs_name=\'\'):\n    """"""\n    The context for a tower function, containing metadata about the current tower.\n    Tensorpack trainers use :class:`TowerContext` to manage tower function.\n    Many tensorpack layers have to be called under a :class:`TowerContext`.\n\n    Example:\n\n    .. code-block:: python\n\n        with TowerContext(\'\', is_training=True):\n            # call a tensorpack layer or a tower function\n    """"""\n    if is_training:\n        return TrainTowerContext(tower_name, vs_name=vs_name)\n    else:\n        return PredictTowerContext(tower_name, vs_name=vs_name)\n\n\nclass TowerFunc(object):\n    """"""\n    A tower function (see\n    `tutorial on tower function\n    <http://tensorpack.readthedocs.io/tutorial/extend/trainer.html#tower-trainer>`_)\n    It keeps track of the name scope, variable scope and input/output tensors\n    each time the function is called.\n\n    :class:`TowerTrainer` needs this so that it knows how to build a predictor.\n\n    Conceptually, this class is roughly equivalent to `tf.function` with input signature, introduced in TF 2.0.\n    """"""\n\n    def __init__(self, tower_fn, input_signature):\n        """"""\n        Args:\n            tower_func: a function which builds one tower in the graph.\n                It takes several input tensors and could return anything.\n            input_signature ([TensorSpec]): list of :class:`tf.TensorSpec`.\n                They are used to figure out the names for the input tensors.\n        """"""\n        assert callable(tower_fn), tower_fn\n        self._inputs_names = [k.name for k in input_signature]\n        assert len(set(self._inputs_names)) == len(self._inputs_names), \\\n            ""Duplicated names in input_signature! "" + str(self._inputs_names)\n        for name in self._inputs_names:\n            if any(k in name for k in [\':\', \'/\', \' \']):\n                raise ValueError(""Invalid input name: \'{}\'"".format(name))\n        self._tower_fn = tower_fn\n        self._input_signature = input_signature\n\n        self._handles = []\n\n    def __new__(cls, tower_fn, _):\n        # to avoid double-wrapping a function\n        if isinstance(tower_fn, TowerFunc):\n            return tower_fn\n        else:\n            return super(TowerFunc, cls).__new__(cls)\n\n    def __call__(self, *args):\n        ctx = get_current_tower_context()\n        assert ctx is not None, ""Function must be called under TowerContext!""\n        output = self._tower_fn(*args)\n        handle = TowerTensorHandle(ctx, args, output, self._input_signature)\n        self._handles.append(handle)\n        return output\n\n    @property\n    def towers(self):\n        """"""\n        TowerTensorHandles: a :class:`TowerTensorHandles` object, that can\n            access the tower handles by either indices or names.\n        """"""\n        return TowerTensorHandles(self._handles)\n\n    @property\n    def input_signature(self):\n        return self._input_signature\n\n\nTowerFuncWrapper = TowerFunc\n\n\nclass TowerTensorHandles(object):\n    """"""\n    Wrap a list of :class:`TowerTensorHandle`,\n    to support access to them by index or names.\n    """"""\n    def __init__(self, handles):\n        self._handles = handles\n        self._name_to_handle = {k.ns_name: k for k in handles}\n\n    def __len__(self):\n        return len(self._handles)\n\n    def __getitem__(self, name_or_index):\n        """"""\n        Args:\n            name_or_index (str or int):\n\n        Returns:\n            a :class:`TowerTensorHandle`.\n        """"""\n        if isinstance(name_or_index, int):\n            return self._handles[name_or_index]\n        return self._name_to_handle[name_or_index]\n\n    def training(self):\n        """"""\n        Returns:\n            A :class:`TowerTensorHandles`, containing only the training towers.\n        """"""\n        handles = [h for h in self._handles if h.is_training]\n        return TowerTensorHandles(handles)\n\n    def inference(self):\n        """"""\n        Returns:\n            A :class:`TowerTensorHandles`, containing only the inference towers.\n        """"""\n        handles = [h for h in self._handles if not h.is_training]\n        return TowerTensorHandles(handles)\n\n\nclass TowerTensorHandle(object):\n    """"""\n    When a function is called multiple times under each tower,\n    it becomes hard to keep track of the scope and access those tensors\n    in each tower.\n    This class provides easy access to the tensors as well as the\n    inputs/outputs created in each tower.\n    """"""\n\n    @HIDE_DOC\n    def __init__(self, ctx, inputs, outputs, input_signature=None):\n        self._ctx = ctx\n\n        self._extra_tensor_names = {}\n        if input_signature is not None:\n            assert len(input_signature) == len(inputs)\n            self._extra_tensor_names = {\n                get_op_tensor_name(x.name)[1]: y for x, y in zip(input_signature, inputs)}\n        self._inputs = inputs\n        self._outputs = outputs\n\n        # TODO: deprecated. Remove them later\n        self.input = inputs\n        self.output = outputs\n\n    @property\n    def vs_name(self):\n        return self._ctx.vs_name\n\n    @property\n    def ns_name(self):\n        return self._ctx.ns_name\n\n    def get_tensor(self, name):\n        """"""\n        Get a tensor in this tower. The name argument can be:\n\n        1. The name of a tensor/variable without any tower prefix.\n\n        2. A name in the input signature, if it is used when building the tower.\n\n        In the second case, this method will return the tensor that\'s used as the corresponding\n        input to the tower. Note that this tensor may have a different name (e.g. may be an output of a queue).\n        """"""\n        name = get_op_tensor_name(name)[1]\n        if len(self.ns_name):\n            name_with_ns = self.ns_name + ""/"" + name\n        else:\n            name_with_ns = name\n\n        try:\n            ret = get_op_or_tensor_by_name(name_with_ns)\n        except KeyError:\n            if name in self._extra_tensor_names:\n                return self._extra_tensor_names[name]\n        else:\n            if name in self._extra_tensor_names:\n                mapped_tensor = self._extra_tensor_names[name]\n                logger.info(\n                    ""\'{}\' may refer to both the Tensor/Placeholder \'{}\' or the input to the tower \'{}\'."".format(\n                        name, ret.name, mapped_tensor.name) +\n                    "" Assuming it is the input \'{}\'."".format(mapped_tensor.name))\n                return mapped_tensor\n            return ret\n        # should also allow variables in get_tensor\n        return self.get_variable(name)\n\n    def get_tensors(self, names):\n        """"""\n        Like :meth:`get_tensor`, but takes a list and returns a list.\n        """"""\n        return [self.get_tensor(name) for name in names]\n\n    def __getitem__(self, name):\n        """"""\n        The same as :meth:`get_tensor`.\n        """"""\n        return self.get_tensor(name)\n\n    def get_variable(self, name):\n        """"""\n        Get a variable used in this tower.\n        The name should not contain the variable scope prefix of the tower.\n\n        When the tower has the same variable scope and name scope, this is equivalent to\n        :meth:`get_tensor`.\n        """"""\n        name = get_op_tensor_name(name)[1]\n        if len(self.vs_name):\n            name_with_vs = self.vs_name + ""/"" + name\n        else:\n            name_with_vs = name\n        return get_op_or_tensor_by_name(name_with_vs)\n\n    def get_variables(self, names):\n        """"""\n        Like :meth:`get_variable`, but takes a list and returns a list.\n        """"""\n        return [self.get_variable(name) for name in names]\n\n    def get_collection(self, key=None, name=None):\n        """"""\n        See :meth:`BaseTowerContext.get_collection_in_tower`.\n\n        Args:\n            key (str): the key of the collection\n            name: deprecated\n        """"""\n        if name is not None:\n            logger.warn(""TowerTensorHandle.get_collection(name=..) was renamed to (key=..) !"")\n            key = name\n        return self._ctx.get_collection_in_tower(key)\n\n    @property\n    def inputs(self):\n        """"""\n        list[Tensor]: The list of input tensors used to build the tower.\n        """"""\n        return self._inputs\n\n    @property\n    def outputs(self):\n        """"""\n        list[Tensor]: The outputs returned by the tower function.\n        """"""\n        return self._outputs\n\n    @property\n    def is_training(self):\n        return self._ctx.is_training\n'"
tensorpack/tfutils/unit_tests.py,3,"b""# -*- coding: utf-8 -*-\n\nimport unittest\nimport tensorflow as tf\n\nfrom ..utils import logger\nfrom .scope_utils import under_name_scope\n\n\nclass ScopeUtilsTest(unittest.TestCase):\n\n    @under_name_scope(name_scope='s')\n    def _f(self, check=True):\n        if check:\n            assert tf.get_default_graph().get_name_scope().endswith('s')\n        return True\n\n    def test_under_name_scope(self):\n        self.assertTrue(self._f())\n        with self.assertRaises(AssertionError):\n            self._f()  # name conflict\n\n    def test_under_name_scope_warning(self):\n        x = tf.placeholder(tf.float32, [3])\n        tf.nn.relu(x, name='s')\n        with self.assertLogs(logger=logger._logger, level='WARNING'):\n            self._f(check=False, name_scope='s')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tensorpack/tfutils/varmanip.py,13,"b'# -*- coding: utf-8 -*-\n# File: varmanip.py\n\nimport numpy as np\nimport os\nimport pprint\nimport six\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..utils import logger\nfrom .common import get_op_tensor_name\n\n__all__ = [\'SessionUpdate\', \'dump_session_params\',\n           \'load_chkpt_vars\', \'save_chkpt_vars\', \'get_checkpoint_path\']\n\n\ndef get_savename_from_varname(\n        varname, varname_prefix=None,\n        savename_prefix=None):\n    """"""\n    Args:\n        varname(str): a variable name in the graph\n        varname_prefix(str): an optional prefix that may need to be removed in varname\n        savename_prefix(str): an optional prefix to append to all savename\n    Returns:\n        str: the name used to save the variable\n    """"""\n    name = varname\n    if varname_prefix is not None \\\n            and name.startswith(varname_prefix):\n        name = name[len(varname_prefix) + 1:]\n    if savename_prefix is not None:\n        name = savename_prefix + \'/\' + name\n    return name\n\n\nclass SessionUpdate(object):\n    """""" Update the variables in a session """"""\n\n    def __init__(self, sess, vars_to_update, ignore_mismatch=False):\n        """"""\n        Args:\n            sess (tf.Session): a session object\n            vars_to_update: a collection of variables to update\n            ignore_mismatch (bool): ignore failures when the value and the\n                variable does not match.\n        """"""\n        self.sess = sess\n        self.name_map = {v.name: v for v in vars_to_update}\n        self.ignore_mismatch = ignore_mismatch\n\n    @staticmethod\n    def relaxed_value_for_var(value, var, ignore_mismatch=False):\n        """"""\n        Returns a relaxed (possibly reshaped/upcast-ed) version of value,\n        to be loaded to the given variable.\n\n        Args:\n            value (ndarray): an numpy array to be loaded to var\n            var (tf.Variable):\n            ignore_mismatch (bool): ignore failures when the value and the\n                variable does not match.\n\n        Returns:\n            ndarray: a possibly reshaped or casted version of value.\n            Returns None if `ignore_mismatch==True` and the value and the variable\n            mismatch.\n        """"""\n        assert isinstance(var, tf.Variable)\n        name = var.op.name\n\n        # check incompatible shape\n        varshape = tuple(var.get_shape().as_list())\n        if varshape != value.shape:\n            if np.prod(varshape) != np.prod(value.shape):\n                if ignore_mismatch:\n                    logger.warn(\n                        ""Cannot load an array of shape {} into variable \'{}\' whose shape is {}."".format(\n                            value.shape, name, varshape))\n                    return None\n                else:\n                    raise ValueError(\n                        ""Trying to load an array of shape {} into variable \'{}\' whose shape is {}."".format(\n                            value.shape, name, varshape))\n            # TODO only allow reshape when shape different by empty axis\n            logger.warn(""The tensor is reshaped from {} to {} when assigned to \'{}\'"".format(\n                value.shape, varshape, name))\n            value = value.reshape(varshape)\n\n        # Be permissive, and allow some common type incompatibility problems\n        def allow_cast(to_type, from_type):\n            # to_type: a tf dtype\n            # from_type: a numpy dtype\n            from_type = tf.as_dtype(from_type)\n\n            # allow up/down casting between floating points\n            if from_type.is_floating and to_type.is_floating:\n                return True\n\n            if from_type.is_integer and to_type.is_integer:\n                # only allow up-casting between integers\n                if to_type.min <= from_type.min and to_type.max >= from_type.max:\n                    return True\n            return False\n\n        if hasattr(value, \'dtype\'):\n            vartype = var.dtype.as_numpy_dtype\n            if vartype != value.dtype:\n                msg = ""Variable {} has dtype {} but was given a value of dtype {}."".format(name, var.dtype, value.dtype)\n\n                if allow_cast(var.dtype.base_dtype, value.dtype):\n                    value = vartype(value)\n                    logger.warn(msg + "" The value will be loaded after casting!"")\n                else:\n                    assert vartype == value.dtype, msg\n        return value\n\n    def update(self, prms):\n        """"""\n        Args:\n            prms(dict): dict of {variable name: value}\n                Any name in prms must be in the graph and in vars_to_update.\n        """"""\n        with self.sess.as_default():\n            fetches = []\n            feeds = {}\n            for name, value in six.iteritems(prms):\n                assert name in self.name_map\n                var = self.name_map[name]\n                value = SessionUpdate.relaxed_value_for_var(\n                    value, var, ignore_mismatch=self.ignore_mismatch)\n                # This is the implementation of `var.load`\n                if value is not None:\n                    fetches.append(var.initializer)\n                    feeds[var.initializer.inputs[1]] = value\n            self.sess.run(fetches, feed_dict=feeds)\n\n\ndef dump_session_params(path):\n    """"""\n    Dump value of all TRAINABLE + MODEL variables to a dict, and save as\n    npz format (loadable by :func:`sessinit.SmartInit`).\n\n    Args:\n        path(str): the file name to save the parameters. Must ends with npz.\n    """"""\n    # save variables that are GLOBAL, and either TRAINABLE or MODEL\n    var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    var.extend(tf.get_collection(tf.GraphKeys.MODEL_VARIABLES))\n    # TODO dedup\n    assert len(set(var)) == len(var), ""TRAINABLE and MODEL variables have duplication!""\n    gvars = {k.name for k in tf.global_variables()}\n    var = [v for v in var if v.name in gvars]\n    result = {}\n    for v in var:\n        result[v.name] = v.eval()\n    save_chkpt_vars(result, path)\n\n\ndef save_chkpt_vars(dic, path):\n    """"""\n    Save variables in dic to path.\n\n    Args:\n        dic: {name: value}\n        path: save as npz if the name ends with \'.npz\', otherwise save as a checkpoint.\n    """"""\n    logger.info(""Variables to save to {}:"".format(path))\n    keys = sorted(dic.keys())\n    logger.info(pprint.pformat(keys))\n\n    assert not path.endswith(\'.npy\')\n    if path.endswith(\'.npz\'):\n        np.savez_compressed(path, **dic)\n    else:\n        with tf.Graph().as_default(), \\\n                tf.Session() as sess:\n            for k, v in six.iteritems(dic):\n                k = get_op_tensor_name(k)[0]\n                _ = tf.Variable(name=k, initial_value=v)    # noqa\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n            saver.save(sess, path, write_meta_graph=False)\n\n\ndef get_checkpoint_path(path):\n    """"""\n    Work around TF problems in checkpoint path handling.\n\n    Args:\n        path: a user-input path\n    Returns:\n        str: the argument that can be passed to NewCheckpointReader\n    """"""\n    if os.path.basename(path) == path:\n        path = os.path.join(\'.\', path)  # avoid #4921 and #6142\n    if os.path.basename(path) == \'checkpoint\':\n        assert tfv1.gfile.Exists(path), path\n        path = tf.train.latest_checkpoint(os.path.dirname(path))\n        # to be consistent with either v1 or v2\n\n    # fix paths if provided a wrong one\n    new_path = path\n    if \'00000-of-00001\' in path:\n        new_path = path.split(\'.data\')[0]\n    elif path.endswith(\'.index\'):\n        new_path = path.split(\'.index\')[0]\n    if new_path != path:\n        logger.info(\n            ""Checkpoint path {} is auto-corrected to {}."".format(path, new_path))\n        path = new_path\n    assert tfv1.gfile.Exists(path) or tfv1.gfile.Exists(path + \'.index\'), path\n    return path\n\n\ndef load_chkpt_vars(path):\n    """""" Load all variables from a checkpoint to a dict.\n\n    Args:\n        path(str): path to a checkpoint.\n\n    Returns:\n        dict: a name:value dict\n    """"""\n    path = get_checkpoint_path(path)\n    reader = tfv1.train.NewCheckpointReader(path)\n    var_names = reader.get_variable_to_shape_map().keys()\n    result = {}\n    for n in var_names:\n        result[n] = reader.get_tensor(n)\n    return result\n\n\ndef is_training_name(name):\n    """"""\n    **Guess** if this variable is only used in training.\n    Only used internally to avoid too many logging. Do not use it.\n    """"""\n    # TODO: maybe simply check against TRAINABLE_VARIABLES and MODEL_VARIABLES?\n    # TODO or use get_slot_names()\n    name = get_op_tensor_name(name)[0]\n    if name.endswith(\'/Adam\') or name.endswith(\'/Adam_1\'):\n        return True\n    if name.endswith(\'/Momentum\'):\n        return True\n    if name.endswith(\'/Adadelta\') or name.endswith(\'/Adadelta_1\'):\n        return True\n    if name.endswith(\'/RMSProp\') or name.endswith(\'/RMSProp_1\'):\n        return True\n    if name.endswith(\'/Adagrad\'):\n        return True\n    if name.startswith(\'EMA/\') or \'/EMA/\' in name:  # all the moving average summaries\n        return True\n    if name.startswith(\'AccumGrad\') or name.endswith(\'/AccumGrad\'):\n        return True\n    if name.startswith(\'apply_gradients\'):\n        return True\n    return False\n'"
tensorpack/tfutils/varreplace.py,13,"b'# -*- coding: utf-8 -*-\n# File: varreplace.py\n# Credit: Qinyao He\n\nfrom contextlib import contextmanager\n\nfrom ..compat import tfv1 as tf\nfrom .common import get_tf_version_tuple\n\n__all__ = [\'custom_getter_scope\', \'freeze_variables\', \'remap_variables\']\n\n\n@contextmanager\ndef custom_getter_scope(custom_getter):\n    """"""\n    Args:\n        custom_getter: the same as in :func:`tf.get_variable`\n\n    Returns:\n        The current variable scope with a custom_getter.\n    """"""\n    scope = tf.get_variable_scope()\n    if get_tf_version_tuple() >= (1, 5):\n        with tf.variable_scope(\n                scope, custom_getter=custom_getter,\n                auxiliary_name_scope=False):\n            yield\n    else:\n        ns = tf.get_default_graph().get_name_scope()\n        with tf.variable_scope(\n                scope, custom_getter=custom_getter):\n            with tf.name_scope(ns + \'/\' if ns else \'\'):\n                yield\n\n\ndef remap_variables(fn):\n    """"""\n    Use fn to map the output of any variable getter.\n\n    Args:\n        fn (tf.Variable -> tf.Tensor)\n\n    Returns:\n        The current variable scope with a custom_getter that maps\n        all the variables by fn.\n\n    Example:\n        .. code-block:: python\n\n            from tensorpack.tfutils import varreplace\n            with varreplace.remap_variables(lambda var: quantize(var)):\n                x = FullyConnected(\'fc\', x, 1000)   # fc/{W,b} will be quantized\n    """"""\n    def custom_getter(getter, *args, **kwargs):\n        v = getter(*args, **kwargs)\n        return fn(v)\n    return custom_getter_scope(custom_getter)\n\n\ndef freeze_variables(stop_gradient=True, skip_collection=False):\n    """"""\n    Return a context to freeze variables,\n    by wrapping ``tf.get_variable`` with a custom getter.\n    It works by either applying ``tf.stop_gradient`` on the variables,\n    or keeping them out of the ``TRAINABLE_VARIABLES`` collection, or\n    both. Both options have their own pros and cons.\n\n    Example:\n        .. code-block:: python\n\n            from tensorpack.tfutils import varreplace\n            with varreplace.freeze_variable(stop_gradient=False, skip_collection=True):\n                x = FullyConnected(\'fc\', x, 1000)   # fc/* will not be trained\n\n    Args:\n        stop_gradient (bool): if True, variables returned from `get_variable`\n            will be wrapped with `tf.stop_gradient`.\n\n            Note that the created variables may still have gradient when accessed\n            by other approaches (e.g. by name, or by collection).\n            For example, they may still have a gradient in weight decay.\n            Also note that this makes `tf.get_variable` returns a Tensor instead of a Variable,\n            which may break existing contract.\n            Therefore, it\'s recommended to use the `skip_collection` option instead.\n        skip_collection (bool): if True, do not add the variable to\n            ``TRAINABLE_VARIABLES`` collection, but to ``MODEL_VARIABLES``\n            collection. As a result they will not be trained by default.\n\n    Note:\n\n    `stop_gradient` only stops variables returned by `get_variable` **within the context** to\n    contribute no gradient in this context. Therefore it may not completely freeze the variables.\n    For example:\n\n        1. If a variable is created, or reused outside of the context, it can still contribute to the\n           gradient of other tensors.\n        2. If a freezed variable is accessed by other approaches (e.g., by names, by collections),\n           it can still contribute to the gradient of other tensors.\n           For example, weight decay cannot be stopped by a `stop_gradient` context.\n\n    `skip_collection` has to be used the first time the variable is created.\n    Once `skip_collection` is used, the variable is not a trainable variable anymore,\n    and will be completely freezed from gradient update in tensorpack\'s single-cost trainer.\n\n    Choose the option carefully depend on what you need.\n    """"""\n    def custom_getter(getter, *args, **kwargs):\n        trainable = kwargs.get(\'trainable\', True)\n        name = args[0] if len(args) else kwargs.get(\'name\')\n        if skip_collection:\n            kwargs[\'trainable\'] = False\n        v = getter(*args, **kwargs)\n        # do not perform unnecessary changes if it\'s not originally trainable\n        # otherwise the variable may get added to MODEL_VARIABLES twice\n        if trainable and skip_collection:\n            tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, v)\n        if trainable and stop_gradient:\n            v = tf.stop_gradient(v, name=\'freezed_\' + name)\n        return v\n    return custom_getter_scope(custom_getter)\n'"
tensorpack/train/__init__.py,0,"b""#  -*- coding: utf-8 -*-\n#  File: __init__.py\n# flake8: noqa\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()['kcah_acitats'[::-1].upper()] = False\nif STATICA_HACK:\n    from .base import *\n    from .config import *\n    from .interface import *\n    from .tower import *\n    from .trainers import *\n\n\nfrom pkgutil import iter_modules\nimport os\nimport os.path\n\n__all__ = []\n\n\ndef global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if '__all__' in dir(p) else []\n    if lst:\n        del globals()[name]\n        for k in lst:\n            globals()[k] = p.__dict__[k]\n            __all__.append(k)\n\n\n_CURR_DIR = os.path.dirname(__file__)\n_SKIP = ['utility']\nfor _, module_name, _ in iter_modules(\n        [_CURR_DIR]):\n    srcpath = os.path.join(_CURR_DIR, module_name + '.py')\n    if not os.path.isfile(srcpath):\n        continue\n    if module_name.startswith('_'):\n        continue\n    if module_name not in _SKIP:\n        global_import(module_name)\n"""
tensorpack/train/base.py,4,"b'# -*- coding: utf-8 -*-\n# File: base.py\n\nimport copy\nimport time\nimport weakref\nimport tensorflow as tf\n\nfrom ..compat import tfv1\nfrom ..callbacks import Callback, Callbacks, Monitors, MonitorBase\nfrom ..callbacks.steps import MaintainStepCounter\nfrom ..tfutils import get_global_step_value\nfrom ..tfutils.model_utils import describe_trainable_vars\nfrom ..tfutils.sesscreate import NewSessionCreator, ReuseSessionCreator\nfrom ..tfutils.sessinit import JustCurrentSession, SessionInit\nfrom ..utils import logger\nfrom ..utils.argtools import call_only_once\nfrom ..utils.utils import humanize_time_delta\nfrom .config import DEFAULT_CALLBACKS, DEFAULT_MONITORS, TrainConfig\n\n__all__ = [\'StopTraining\', \'Trainer\']\n\n\nclass StopTraining(Exception):\n    """"""\n    An exception thrown to stop training.\n    """"""\n    pass\n\n\nclass TrainLoop(object):\n    """"""\n    Manage the double for loop.\n    """"""\n\n    def __init__(self):\n        self._epoch_num = 0\n        self._global_step = 0\n        self._local_step = -1\n\n    def config(self, steps_per_epoch, starting_epoch, max_epoch):\n        """"""\n        Configure the loop given the settings.\n        """"""\n        self.starting_epoch = int(starting_epoch)\n        self.max_epoch = int(max_epoch)\n        self.steps_per_epoch = int(steps_per_epoch)\n        # Allow empty epoch (no steps), if we want to run the callbacks only.\n        assert self.steps_per_epoch >= 0 and self.max_epoch >= 0\n\n        self._epoch_num = starting_epoch - 1\n\n    def update_global_step(self):\n        """"""\n        Update the Python-side global_step from TF.\n        This must be called under initialized default session.\n        """"""\n        self._global_step = get_global_step_value()\n\n    @property\n    def epoch_num(self):\n        """"""\n        The number of the currently ongoing epoch.\n\n        An epoch is defined to cover the moment before calling `before_epoch` until after calling `trigger_epoch`.\n        i.e., in the `trigger_epoch` of epoch 3, `self.epoch_num` is 3.\n        If you need use `self.epoch_num` in your callback, you\'ll need to know this.\n        """"""\n        return self._epoch_num\n\n    @property\n    def global_step(self):\n        """"""\n        The tensorflow global_step, i.e. how many times ``hooked_sess.run`` has been called.\n\n        Note:\n            1. global_step is incremented **after** each ``hooked_sess.run`` returns from TF runtime.\n            2. If you make zero or more than one calls to ``hooked_sess.run`` in one\n               :meth:`run_step`, local_step and global_step may increment at different speed.\n        """"""\n        return self._global_step\n\n    @property\n    def local_step(self):\n        """"""\n        The number of steps that have finished in the current epoch.\n        """"""\n        return self._local_step\n\n\nclass Trainer(object):\n    """""" Base class for a trainer.\n    """"""\n\n    is_chief = True\n    """"""\n    Whether this process is the chief worker in distributed training.\n    Certain callbacks will only be run by chief worker.\n    """"""\n\n    sess = None\n    """"""\n    The ``tf.Session`` object the trainer is using.\n    Available after :meth:`initialize()`.\n\n    Using ``trainer.sess.run`` to evaluate tensors that depend on the training\n    ``InputSource`` may have unexpected effect:\n\n    For example, if you use ``trainer.sess.run`` to evaluate a tensor that depends on the\n    inputs coming from a ``StagingArea``,\n    it will take a datapoint from the ``StagingArea``, making the ``StagingArea`` empty, and as a result\n    make the training hang.\n    """"""\n\n    hooked_sess = None\n    """"""\n    The ``tf.train.MonitoredSession`` object the trainer is using.\n    It contains all the ``before_run/after_run`` hooks the callbacks have registered.\n    It is used for running the training iterations.\n    Available after :meth:`initialize()`.\n\n    Note that using ``hooked_sess.run`` will evaluate all the hooks,\n    just like running a training iteration. It may do the following:\n\n    1. Take a datapoint from the InputSource\n    2. Increase the global_step\n    3. Evaluate some summaries\n\n    Typically you **should not** use ``hooked_sess.run`` in callbacks,\n    because it is for the ""training iteration"". If you just want to evaluate\n    some tensors, use ``sess.run`` if the tensors does not depend on the inputs,\n    or more generally, use `before_run/after_run` to evaluate the tensors **along with**\n    the training iterations.\n    """"""\n\n    def __init__(self):\n        self._callbacks = []\n        self.loop = TrainLoop()\n\n    def _register_callback(self, cb):\n        """"""\n        Register callbacks to the trainer.\n        It can only be called before :meth:`Trainer.train()`.\n\n        Args:\n            cb (Callback or [Callback]): a callback or a list of callbacks\n\n        Returns:\n            succeed or not\n        """"""\n        if isinstance(cb, (list, tuple)):\n            for x in cb:\n                self._register_callback(x)\n            return\n        assert isinstance(cb, Callback), cb\n        assert not isinstance(self._callbacks, Callbacks), \\\n            ""Cannot register more callbacks after trainer was setup!""\n        if not self.is_chief and cb.chief_only:\n            logger.warn(""Callback {} is chief-only, skipped."".format(str(cb)))\n            return False\n        else:\n            self._callbacks.append(cb)\n            return True\n\n    register_callback = _register_callback\n\n    def run_step(self):\n        """"""\n        Defines what to do in one iteration. The default is:\n        ``self.hooked_sess.run(self.train_op)``.\n\n        The behavior of each iteration can be changed by either setting ``trainer.train_op``,\n        or overriding this method.\n        """"""\n        if not hasattr(self, \'train_op\'):\n            raise NotImplementedError(\n                ""Please either set `Trainer.train_op` or provide an implementation ""\n                ""of Trainer.run_step()!"")\n        self.hooked_sess.run(self.train_op)\n\n    @call_only_once\n    def setup_callbacks(self, callbacks, monitors):\n        """"""\n        Setup callbacks and monitors. Must be called after the main graph is built.\n\n        Args:\n            callbacks ([Callback]):\n            monitors ([MonitorBase]):\n        """"""\n        assert isinstance(callbacks, list), callbacks\n        assert isinstance(monitors, list), monitors\n        describe_trainable_vars()   # TODO weird\n\n        self.register_callback(MaintainStepCounter())\n        for cb in callbacks:\n            self.register_callback(cb)\n        for cb in self._callbacks:\n            assert not isinstance(cb, MonitorBase), ""Monitor cannot be pre-registered for now!""\n        registered_monitors = []\n        for m in monitors:\n            if self.register_callback(m):\n                registered_monitors.append(m)\n        self.monitors = Monitors(registered_monitors)\n        self.register_callback(self.monitors)   # monitors is also a callback\n\n        # some final operations that might modify the graph\n        logger.info(""Setup callbacks graph ..."")\n        self._callbacks = Callbacks(self._callbacks)\n        self._callbacks.setup_graph(weakref.proxy(self))\n\n    @call_only_once\n    def initialize(self, session_creator, session_init):\n        """"""\n        Create the session and set `self.sess`.\n        Call `self.initiailize_hooks()`\n        Finalize the graph.\n\n        It must be called after callbacks are setup.\n\n        Args:\n            session_creator (tf.train.SessionCreator):\n            session_init (sessinit.SessionInit):\n        """"""\n        assert isinstance(session_creator, tfv1.train.SessionCreator), session_creator\n        assert isinstance(session_init, SessionInit), session_init\n        session_init._setup_graph()\n\n        logger.info(""Creating the session ..."")\n\n        self.sess = session_creator.create_session()\n        self.initialize_hooks()\n\n        if self.is_chief:\n            logger.info(""Initializing the session ..."")\n            session_init._run_init(self.sess)\n        else:\n            if not isinstance(session_init, JustCurrentSession):\n                logger.warn(""This is not a chief worker, \'session_init\' was ignored!"")\n\n        self.sess.graph.finalize()\n        logger.info(""Graph Finalized."")\n\n    @call_only_once\n    def initialize_hooks(self):\n        """"""\n        Create SessionRunHooks for all callbacks, and hook it onto `self.sess` to create `self.hooked_sess`.\n\n        A new trainer may override this method to create multiple groups of hooks,\n        which can be useful when the training is not done by a single `train_op`.\n        """"""\n        hooks = self._callbacks.get_hooks()\n        self.hooked_sess = tfv1.train.MonitoredSession(\n            session_creator=ReuseSessionCreator(self.sess), hooks=hooks)\n\n    @call_only_once\n    def main_loop(self, steps_per_epoch, starting_epoch, max_epoch):\n        """"""\n        Run the main training loop.\n\n        Args:\n            steps_per_epoch, starting_epoch, max_epoch (int):\n        """"""\n        with self.sess.as_default():\n            self.loop.config(steps_per_epoch, starting_epoch, max_epoch)\n            self.loop.update_global_step()\n            try:\n                self._callbacks.before_train()\n                # refresh global step (might have changed by callbacks) TODO ugly\n                # what if gs is changed later?\n                self.loop.update_global_step()\n                for self.loop._epoch_num in range(\n                        self.loop.starting_epoch, self.loop.max_epoch + 1):\n                    logger.info(""Start Epoch {} ..."".format(self.loop.epoch_num))\n                    self._callbacks.before_epoch()\n                    start_time = time.time()\n                    for self.loop._local_step in range(self.loop.steps_per_epoch):\n                        if self.hooked_sess.should_stop():\n                            return\n                        self.run_step()  # implemented by subclass\n                        self._callbacks.trigger_step()\n                    self._callbacks.after_epoch()\n                    logger.info(""Epoch {} (global_step {}) finished, time:{}."".format(\n                        self.loop.epoch_num, self.loop.global_step, humanize_time_delta(time.time() - start_time)))\n\n                    # trigger epoch outside the timing region.\n                    self._callbacks.trigger_epoch()\n                logger.info(""Training has finished!"")\n            except (StopTraining, tf.errors.OutOfRangeError) as e:\n                logger.info(""Training was stopped by exception {}."".format(str(e)))\n            except KeyboardInterrupt:\n                logger.info(""Detected Ctrl-C and exiting main loop."")\n                raise\n            finally:\n                self._callbacks.after_train()\n                self.hooked_sess.close()\n\n    def train(self,\n              callbacks, monitors,\n              session_creator, session_init,\n              steps_per_epoch, starting_epoch=1, max_epoch=9999999):\n        """"""\n        Implemented by three lines:\n\n        .. code-block:: python\n\n            self.setup_callbacks(callbacks, monitors)\n            self.initialize(session_creator, session_init)\n            self.main_loop(steps_per_epoch, starting_epoch, max_epoch)\n\n        You can call those methods by yourself to have better control on details if needed.\n        """"""\n        self.setup_callbacks(callbacks, monitors)\n        self.initialize(session_creator, session_init)\n        self.main_loop(steps_per_epoch, starting_epoch, max_epoch)\n\n    def train_with_defaults(\n            self, _sentinel=None,\n            callbacks=None, monitors=None,\n            session_creator=None, session_init=None,\n            steps_per_epoch=None, starting_epoch=1, max_epoch=9999999,\n            extra_callbacks=None):\n        """"""\n        Same as :meth:`train()`, except:\n\n        1. Add `extra_callbacks` to callbacks. The default value for\n           `extra_callbacks` is :meth:`DEFAULT_CALLBACKS()`.\n        2. Default value for `monitors` is :meth:`DEFAULT_MONITORS()`.\n        3. Provide default values for every option except `steps_per_epoch`.\n        """"""\n        assert _sentinel is None, ""Please call `train_with_defaults` with keyword arguments only!""\n        callbacks = copy.copy(callbacks or [])\n        monitors = DEFAULT_MONITORS() if monitors is None else monitors\n        extra_callbacks = DEFAULT_CALLBACKS() if extra_callbacks is None else extra_callbacks\n        callbacks.extend(extra_callbacks)\n\n        assert steps_per_epoch is not None\n        session_creator = session_creator or NewSessionCreator()\n        session_init = session_init or JustCurrentSession()\n\n        self.train(callbacks, monitors,\n                   session_creator, session_init,\n                   steps_per_epoch, starting_epoch, max_epoch)\n\n    def __new__(cls, *args, **kwargs):\n        if (len(args) > 0 and isinstance(args[0], TrainConfig)) \\\n                or \'config\' in kwargs:\n            logger.error(""You\'re calling new trainers with old trainer API!"")\n            logger.error(""See https://github.com/tensorpack/tensorpack/issues/458 for more information."")\n            import sys\n            sys.exit(1)\n        else:\n            return super(Trainer, cls).__new__(cls)\n\n\ndef _get_property(name):\n    """"""\n    Delegate property to self.loop\n    """"""\n    ret = property(\n        lambda self: getattr(self.loop, name))\n    try:\n        ret.__doc__ = getattr(TrainLoop, name).__doc__\n    except AttributeError:\n        pass\n    return ret\n\n\nfor name in [\'global_step\', \'local_step\', \'steps_per_epoch\',\n             \'epoch_num\', \'starting_epoch\', \'max_epoch\']:\n    setattr(Trainer, name, _get_property(name))\n'"
tensorpack/train/config.py,3,"b'# -*- coding: utf-8 -*-\n# File: config.py\n\nimport os\nimport tensorflow as tf\n\nfrom ..callbacks import (\n    JSONWriter, MergeAllSummaries, MovingAverageSummary, ProgressBar, RunUpdateOps, ScalarPrinter, TFEventWriter)\nfrom ..dataflow.base import DataFlow\nfrom ..input_source import InputSource\nfrom ..tfutils.sesscreate import NewSessionCreator\nfrom ..tfutils.sessinit import SaverRestore, SessionInit\nfrom ..utils import logger\n\nfrom .model_desc import ModelDescBase\n\n__all__ = [\'TrainConfig\', \'AutoResumeTrainConfig\', \'DEFAULT_CALLBACKS\', \'DEFAULT_MONITORS\']\n\n\ndef DEFAULT_CALLBACKS():\n    """"""\n    Return the default callbacks,\n    which will be used in :class:`TrainConfig` and :meth:`Trainer.train_with_defaults`.\n    They are:\n\n    1. MovingAverageSummary()\n    2. ProgressBar()\n    3. MergeAllSummaries()\n    4. RunUpdateOps()\n    """"""\n    return [\n        MovingAverageSummary(),\n        ProgressBar(),\n        MergeAllSummaries(),\n        RunUpdateOps()]\n\n\ndef DEFAULT_MONITORS():\n    """"""\n    Return the default monitors,\n    which will be used in :class:`TrainConfig` and :meth:`Trainer.train_with_defaults`.\n    They are:\n\n    1. TFEventWriter()\n    2. JSONWriter()\n    3. ScalarPrinter()\n    """"""\n    return [TFEventWriter(), JSONWriter(), ScalarPrinter()]\n\n\nclass TrainConfig(object):\n    """"""\n    A collection of options to be used for single-cost trainers.\n\n    Note that you do not have to use :class:`TrainConfig`.\n    You can use the API of :class:`Trainer` directly, to have more fine-grained control of the training.\n    """"""\n\n    def __init__(self,\n                 dataflow=None, data=None,\n                 model=None,\n                 callbacks=None, extra_callbacks=None, monitors=None,\n                 session_creator=None, session_config=None, session_init=None,\n                 starting_epoch=1, steps_per_epoch=None, max_epoch=99999,\n                 **kwargs):\n        """"""\n        Args:\n            dataflow (DataFlow):\n            data (InputSource):\n            model (ModelDesc):\n\n            callbacks (list[Callback]): a list of :class:`Callback` to use during training.\n            extra_callbacks (list[Callback]): This argument\n                is only used to provide the defaults in addition to ``callbacks``.\n                The list of callbacks that will be used in the end is simply ``callbacks + extra_callbacks``.\n\n                It is usually left as None, and the default value for this argument is :func:`DEFAULT_CALLBACKS()`.\n                You can override it when you don\'t like any of the default callbacks.\n                For example, if you\'d like to let the progress bar print tensors, you can use\n\n                .. code-block:: none\n\n                    extra_callbacks=[ProgressBar(names=[\'name\']),\n                                     MovingAverageSummary(),\n                                     MergeAllSummaries(),\n                                     RunUpdateOps()]\n\n            monitors (list[MonitorBase]): Defaults to :func:`DEFAULT_MONITORS()`.\n\n            session_creator (tf.train.SessionCreator): Defaults to :class:`sesscreate.NewSessionCreator()`\n                with the config returned by :func:`tfutils.get_default_sess_config()`.\n            session_config (tf.ConfigProto): when session_creator is None, use this to create the session.\n            session_init (SessionInit): how to initialize variables of a session. Defaults to do nothing.\n\n            starting_epoch (int): The index of the first epoch.\n            steps_per_epoch (int): the number of steps (defined by :meth:`Trainer.run_step`) to run in each epoch.\n                Defaults to the input data size. You may want to divide it by the #GPUs in multi-GPU training.\n\n                Number of steps per epoch only affects the schedule of callbacks.\n                It does not affect the sequence of input data seen by the model.\n            max_epoch (int): maximum number of epoch to run training.\n        """"""\n\n        # TODO type checker decorator\n        def assert_type(v, tp, name):\n            assert isinstance(v, tp), \\\n                ""{} has to be type \'{}\', but an object of type \'{}\' found."".format(\n                    name, tp.__name__, v.__class__.__name__)\n\n        # process data & model\n        assert data is None or dataflow is None, ""dataflow and data cannot be both presented in TrainConfig!""\n        if dataflow is not None:\n            assert_type(dataflow, DataFlow, \'dataflow\')\n        if data is not None:\n            assert_type(data, InputSource, \'data\')\n        self.dataflow = dataflow\n        self.data = data\n\n        if model is not None:\n            assert_type(model, ModelDescBase, \'model\')\n        self.model = model\n\n        if callbacks is not None:\n            assert_type(callbacks, list, \'callbacks\')\n        self.callbacks = callbacks\n        if extra_callbacks is not None:\n            assert_type(extra_callbacks, list, \'extra_callbacks\')\n        self.extra_callbacks = extra_callbacks\n        if monitors is not None:\n            assert_type(monitors, list, \'monitors\')\n        self.monitors = monitors\n        if session_init is not None:\n            assert_type(session_init, SessionInit, \'session_init\')\n        self.session_init = session_init\n\n        if session_creator is None:\n            if session_config is not None:\n                self.session_creator = NewSessionCreator(config=session_config)\n            else:\n                self.session_creator = NewSessionCreator(config=None)\n        else:\n            self.session_creator = session_creator\n            assert session_config is None, ""Cannot set both session_creator and session_config!""\n\n        if steps_per_epoch is None:\n            try:\n                if dataflow is not None:\n                    steps_per_epoch = len(dataflow)\n                elif data is not None:\n                    steps_per_epoch = data.size()\n                else:\n                    raise NotImplementedError()\n            except NotImplementedError:\n                logger.error(""You must set `TrainConfig(steps_per_epoch)` if the size of your input is not available."")\n                raise\n        else:\n            steps_per_epoch = int(steps_per_epoch)\n        self.steps_per_epoch = steps_per_epoch\n\n        self.starting_epoch = int(starting_epoch)\n        self.max_epoch = int(max_epoch)\n\n\nclass AutoResumeTrainConfig(TrainConfig):\n    """"""\n    Same as :class:`TrainConfig`, but does the following to automatically\n    resume from training:\n\n    1. If a checkpoint was found in :meth:`logger.get_logger_dir()`, set\n       `session_init` option to load it.\n    2. If a JSON history was found in :meth:`logger.get_logger_dir()`, try to\n       load the epoch number from it and set the `starting_epoch` option to\n       continue training.\n\n    You can choose to let the above two option to either overwrite or\n    not overwrite user-provided arguments, as explained below.\n\n    Note that the functionality requires the logging directory to obtain\n    necessary information from a previous run.\n    If you have unconventional setup of logging directory, this class will not\n    work for you, for example:\n\n        1. If you save the checkpoint to a different directory rather than the\n           logging directory.\n\n        2. If in distributed training the directory is not\n           available to every worker, or the directories are different for different workers.\n    """"""\n    def __init__(self, always_resume=True, **kwargs):\n        """"""\n        Args:\n            always_resume (bool): If False, user-provided arguments\n                `session_init` and `starting_epoch` will take priority.\n                Otherwise, resume will take priority.\n            kwargs: same as in :class:`TrainConfig`.\n\n        Note:\n            The main goal of this class is to let a training job resume\n            without changing any line of code or command line arguments.\n            So it\'s useful to let resume take priority over user-provided arguments sometimes.\n\n            For example: if your training starts from a pre-trained model,\n            you would want it to use user-provided model loader at the\n            beginning, but a ""resume"" model loader when the job was\n            interrupted and restarted.\n        """"""\n        found_sessinit = False\n        if always_resume or \'session_init\' not in kwargs:\n            sessinit = self.get_sessinit_resume()\n            if sessinit is not None:\n                found_sessinit = True\n                path = sessinit.path\n                if \'session_init\' in kwargs:\n                    logger.info(""Found checkpoint at {}. ""\n                                ""session_init arguments will be overwritten."".format(path))\n                else:\n                    logger.info(""Will load checkpoint at {}."".format(path))\n                kwargs[\'session_init\'] = sessinit\n\n        found_last_epoch = False\n        if always_resume or \'starting_epoch\' not in kwargs:\n            last_epoch = JSONWriter.load_existing_epoch_number()\n            if last_epoch is not None:\n                found_last_epoch = True\n                now_epoch = last_epoch + 1\n                logger.info(""Found history statistics from JSON. ""\n                            ""Setting starting_epoch to {}."".format(now_epoch))\n                kwargs[\'starting_epoch\'] = now_epoch\n        assert found_sessinit == found_last_epoch, \\\n            ""Found SessionInit={}, Found Last Epoch={}"".format(found_sessinit, found_last_epoch)\n\n        super(AutoResumeTrainConfig, self).__init__(**kwargs)\n\n    @staticmethod\n    def get_sessinit_resume(dir=None):\n        if dir is None:\n            dir = logger.get_logger_dir()\n        if not dir:\n            return None\n        path = os.path.join(dir, \'checkpoint\')\n        if not tf.gfile.Exists(path):\n            return None\n        return SaverRestore(path)\n'"
tensorpack/train/interface.py,1,"b'# -*- coding: utf-8 -*-\n# File: interface.py\n\nfrom ..compat import tfv1\nfrom ..input_source import FeedInput, InputSource, QueueInput, StagingInput\nfrom ..utils import logger\nfrom ..compat import is_tfv2\nfrom .config import TrainConfig\nfrom .tower import SingleCostTrainer\nfrom .trainers import SimpleTrainer\n\n__all__ = [\'launch_train_with_config\']\n\n\ndef apply_default_prefetch(input_source_or_dataflow, trainer):\n    """"""\n    Apply a set of default rules to make a fast :class:`InputSource`.\n\n    Args:\n        input_source_or_dataflow(InputSource | DataFlow):\n        trainer (Trainer):\n\n    Returns:\n        InputSource\n    """"""\n    if not isinstance(input_source_or_dataflow, InputSource):\n        # to mimic same behavior of the old trainer interface\n        if type(trainer) == SimpleTrainer:\n            input = FeedInput(input_source_or_dataflow)\n        else:\n            logger.info(""Automatically applying QueueInput on the DataFlow."")\n            input = QueueInput(input_source_or_dataflow)\n    else:\n        input = input_source_or_dataflow\n    if hasattr(trainer, \'devices\'):\n        towers = trainer.devices\n        if len(towers) > 1:  # seem to only help on >1 GPUs\n            assert not isinstance(trainer, SimpleTrainer)\n\n            if isinstance(input, QueueInput):\n                logger.info(""Automatically applying StagingInput on the DataFlow."")\n                input = StagingInput(input)\n    return input\n\n\ndef launch_train_with_config(config, trainer):\n    """"""\n    Train with a :class:`TrainConfig` and a :class:`Trainer`, to\n    present the simple and old training interface. It basically does the following\n    3 things (and you can easily do them by yourself if you need more control):\n\n    1. Setup the input with automatic prefetching heuristics,\n       from `config.data` or `config.dataflow`.\n    2. Call `trainer.setup_graph` with the input as well as `config.model`.\n    3. Call `trainer.train` with rest of the attributes of config.\n\n    See the `related tutorial\n    <https://tensorpack.readthedocs.io/tutorial/training-interface.html#with-modeldesc-and-trainconfig>`_\n    to learn more.\n\n    Args:\n        config (TrainConfig):\n        trainer (Trainer): an instance of :class:`SingleCostTrainer`.\n\n    Example:\n\n    .. code-block:: python\n\n        launch_train_with_config(\n            config, SyncMultiGPUTrainerParameterServer(8, ps_device=\'gpu\'))\n    """"""\n    if is_tfv2():\n        tfv1.disable_eager_execution()\n\n    assert isinstance(trainer, SingleCostTrainer), trainer\n    assert isinstance(config, TrainConfig), config\n    assert config.model is not None\n    assert config.dataflow is not None or config.data is not None\n\n    model = config.model\n    input = config.data or config.dataflow\n    input = apply_default_prefetch(input, trainer)\n\n    # This is the only place where the `ModelDesc` abstraction is useful.\n    # We should gradually stay away from this unuseful abstraction.\n    # TowerFunc is a better abstraction (similar to tf.function in the future)\n    trainer.setup_graph(\n        model.get_input_signature(), input,\n        model.build_graph, model.get_optimizer)\n    _check_unused_regularization()\n    trainer.train_with_defaults(\n        callbacks=config.callbacks,\n        monitors=config.monitors,\n        session_creator=config.session_creator,\n        session_init=config.session_init,\n        steps_per_epoch=config.steps_per_epoch,\n        starting_epoch=config.starting_epoch,\n        max_epoch=config.max_epoch,\n        extra_callbacks=config.extra_callbacks)\n\n\ndef _check_unused_regularization():\n    coll = tfv1.get_collection(tfv1.GraphKeys.REGULARIZATION_LOSSES)\n    unconsumed_reg = []\n    for c in coll:\n        if len(c.consumers()) == 0:\n            unconsumed_reg.append(c)\n    if unconsumed_reg:\n        logger.warn(""The following tensors appear in REGULARIZATION_LOSSES collection but have no ""\n                    ""consumers! You may have forgotten to add regularization to total cost."")\n        logger.warn(""Unconsumed regularization: {}"".format(\', \'.join([x.name for x in unconsumed_reg])))\n'"
tensorpack/train/model_desc.py,9,"b'# -*- coding: utf-8 -*-\n# File: model_desc.py\n\n\nimport tensorflow as tf\n\nfrom ..utils.argtools import memoized_method\nfrom ..tfutils.common import get_op_tensor_name\nfrom ..tfutils.tower import get_current_tower_context\nfrom ..compat import backport_tensor_spec, tfv1\n\nTensorSpec = backport_tensor_spec()\n\n\n__all__ = [\'ModelDesc\', \'ModelDescBase\']\n\n\nclass ModelDescBase(object):\n    """"""\n    Base class for a model description.\n\n    It is used for the simple training interface described in\n    `Training Interface Tutorial <https://tensorpack.readthedocs.io/tutorial/training-interface.html>`_.\n\n    Subclass is expected to implement :meth:`inputs` and :meth:`build_graph`, as they\n    together define a tower function.\n    """"""\n\n    @memoized_method\n    def get_input_signature(self):\n        """"""\n        Returns:\n            A list of :class:`tf.TensorSpec`, which describes the inputs of this model.\n            The result is cached for each instance of :class:`ModelDescBase`.\n        """"""\n        with tf.Graph().as_default() as G:   # create these placeholder in a temporary graph\n            inputs = self.inputs()\n            assert isinstance(inputs, (list, tuple)), \\\n                ""ModelDesc.inputs() should return a list of tf.TensorSpec objects! Got {} instead."".format(str(inputs))\n            if isinstance(inputs[0], tf.Tensor):\n                for p in inputs:\n                    assert ""Placeholder"" in p.op.type, \\\n                        ""inputs() have to return TensorSpec or placeholders! Found {} instead."".format(p)\n                    assert p.graph == G, ""Placeholders returned by inputs() should be created inside inputs()!""\n            return [TensorSpec(shape=p.shape, dtype=p.dtype, name=get_op_tensor_name(p.name)[0]) for p in inputs]\n\n    @property\n    def input_names(self):\n        """"""\n        list[str]: the names of all the inputs.\n        """"""\n        return [k.name for k in self.get_input_signature()]\n\n    def inputs(self):\n        """"""\n        A subclass is expected to implement this method.\n\n        If returning placeholders,\n        the placeholders **have to** be created inside this method.\n        Don\'t return placeholders created in other places.\n\n        Also, users should never call this method by yourself.\n\n        Returns:\n            list[tf.TensorSpec or tf.placeholder].\n        """"""\n        raise NotImplementedError()\n\n    def build_graph(self, *args):\n        """"""\n        A subclass is expected to implement this method.\n\n        Build the whole symbolic graph.\n        This is supposed to be part of the ""tower function"" when used with :class:`TowerTrainer`.\n\n        Args:\n            args ([tf.Tensor]): tensors that matches the list of inputs defined by ``inputs()``.\n\n        Returns:\n            In general it returns nothing, but a subclass\n            may require it to return necessary information to build the trainer.\n            For example, `SingleCostTrainer` expect this method to return the cost tensor.\n        """"""\n        raise NotImplementedError()\n\n    @property\n    def training(self):\n        """"""\n        bool: whether the caller is under a training context or not.\n        """"""\n        return get_current_tower_context().is_training\n\n\nclass ModelDesc(ModelDescBase):\n    """"""\n    One subclass of :class:`ModelDescBase` with the assupmtion of\n    **single cost** and **single optimizer** training.\n    It has the following constraints in addition to :class:`ModelDescBase`:\n\n    1. `build_graph(...)` method should return a cost tensor when called under a training context.\n       The cost will be the final cost to be optimized by the optimizer.\n       Therefore it should include necessary regularization.\n\n    2. Subclass is expected to implement :meth:`optimizer()` method.\n    """"""\n\n    @memoized_method\n    def get_optimizer(self):\n        """"""\n        Return the memoized optimizer returned by `optimizer()`.\n\n        Users of :class:`ModelDesc` will need to implement `optimizer()`,\n        which will only be called once per each model.\n\n        Returns:\n            a :class:`tf.train.Optimizer` instance.\n        """"""\n        ret = self.optimizer()\n        assert isinstance(ret, tfv1.train.Optimizer), \\\n            ""ModelDesc.optimizer() must return a tf.train.Optimizer! Got {} instead."".format(str(ret))\n        return ret\n\n    def optimizer(self):\n        """"""\n        A subclass is expected to implement this method.\n\n        Returns:\n            a `tf.train.Optimizer` instance.\n        """"""\n        raise NotImplementedError()\n'"
tensorpack/train/tower.py,13,"b'# -*- coding: utf-8 -*-\n# File: tower.py\n\nfrom abc import ABCMeta, abstractmethod\nimport six\nimport tensorflow as tf\n\nfrom ..compat import tfv1, is_tfv2\nfrom ..input_source import PlaceholderInput\nfrom ..predict.base import OnlinePredictor\nfrom ..tfutils.gradproc import FilterNoneGrad\nfrom ..tfutils.tower import PredictTowerContext, TowerFunc, get_current_tower_context\nfrom ..utils import logger\nfrom ..utils.argtools import call_only_once, memoized\nfrom ..utils.develop import HIDE_DOC\nfrom .base import Trainer\n\n__all__ = [\'SingleCostTrainer\', \'TowerTrainer\']\n\n\nclass TowerTrainer(Trainer):\n    """"""\n    Base trainers for models that can be built by calling a tower function under a :class:`TowerContext`.\n\n    The assumption of tower function is required by some features that replicates the model\n    automatically. For example, TowerTrainer can create a predictor for you automatically,\n    by calling the tower function.\n\n    To use :class:`TowerTrainer`, set `tower_func` and use it to build the graph.\n    Note that `tower_func` can only be set once per instance of `TowerTrainer`.\n    """"""\n\n    _tower_func = None\n    _predictors = []\n    """"""\n    List of OnlinePredictor ever created for this trainer.\n    It is maintained for internal use.\n    """"""\n\n    @call_only_once\n    def _set_tower_func(self, tower_func):\n        assert isinstance(tower_func, TowerFunc), tower_func\n        self._tower_func = tower_func\n\n    @property\n    def tower_func(self):\n        """"""\n        A :class:`TowerFunc` instance.\n        See `tutorial on tower function\n        <http://tensorpack.readthedocs.io/tutorial/trainer.html#tower-trainer>`_\n        for more information.\n        """"""\n        return self._tower_func\n\n    @tower_func.setter\n    def tower_func(self, val):\n        self._set_tower_func(val)\n\n    @property\n    def input_signature(self):\n        """"""\n        list[tf.TensorSpec]: metainfo about the inputs to the tower.\n        """"""\n        return self.tower_func.input_signature\n\n    @property\n    def towers(self):\n        """"""\n        TowerTensorHandles: used to access the tower handles by either indices or names.\n\n        This property is accessbile only after the graph is set up.\n        With :meth:`towers`, you can then access many attributes of each tower:\n\n        Example:\n\n        .. code-block:: python\n\n            # Access the conv1/output tensor in the first training tower\n            trainer.towers.training()[0].get_tensor(\'conv1/output\')\n        """"""\n        return self.tower_func.towers\n\n    def get_predictor(self, input_names, output_names, device=0):\n        """"""\n        This method will build the trainer\'s tower function under ``TowerContext(is_training=False)``,\n        and returns a callable predictor with input placeholders & output tensors in this tower.\n\n        This method handles the common case where you inference with the same tower function\n        you provide to the trainer.\n        If you want to do inference with a different tower function, you can always build the tower by yourself,\n        under a ""reuse"" variable scope and a `TowerContext(is_training=False)`.\n\n        Args:\n            input_names (list): list of input names, matching the inputs declared for the trainer.\n            output_names(list): list of tensor names without the tower prefix.\n            device (int): build the predictor on device \'/gpu:{device}\' or use -1 for \'/cpu:0\'.\n\n        Returns:\n            an :class:`OnlinePredictor`.\n\n        Example:\n\n        .. code-block:: none\n\n            # in the graph:\n            interesting_tensor = tf.identity(x, name=\'fun\')\n            # in _setup_graph callback method:\n            self._predictor = self.trainer.get_predictor([\'input1\', \'input2\'], [\'fun\'])\n            # After session is initialized (see Tutorials - Write a Callback), can use it by:\n            outputs = self._predictor(input1, input2)\n\n        The CycleGAN example and DQN example have more concrete use of this method.\n        """"""\n        assert self.tower_func is not None, ""Must set tower_func on the trainer to use get_predictor()!""\n        tower_name = \'tower-pred-{}\'.format(device) if device >= 0 else \'tower-pred-cpu\'\n        device_id = device\n        device = \'/gpu:{}\'.format(device_id) if device_id >= 0 else \'/cpu:0\'\n\n        try:\n            tower = self.tower_func.towers[tower_name]\n            assert tower is not None, ""This is a bug!""\n        except KeyError:\n            tower = None\n\n        if tower is None:\n            input = PlaceholderInput()\n            input.setup(self.input_signature)\n\n            vs_name = self._vs_name_for_predictor(device_id)\n            with tfv1.variable_scope(tfv1.get_variable_scope(), reuse=True), \\\n                    tf.device(device), PredictTowerContext(\n                        tower_name, vs_name=vs_name):\n                logger.info(""Building graph for predict tower \'{}\' on device {} {}..."".format(\n                    tower_name, device,\n                    ""with variable scope \'{}\'"".format(vs_name) if vs_name else \'\'))\n                self.tower_func(*input.get_input_tensors())\n            tower = self.tower_func.towers[tower_name]\n        input_tensors = tower.get_tensors(input_names)\n        output_tensors = tower.get_tensors(output_names)\n        predictor = OnlinePredictor(input_tensors, output_tensors)\n        self._predictors.append(predictor)\n        return predictor\n\n    @HIDE_DOC\n    @call_only_once\n    def initialize(self, session_creator, session_init):\n        super(TowerTrainer, self).initialize(session_creator, session_init)\n        # Predictors are created before creating the session, so they don\'t have an associated session.\n        for pred in self._predictors:\n            pred.sess = self.sess\n\n    def _vs_name_for_predictor(self, device):\n        towers = self.towers.training()\n        available_ids = list(range(len(towers)))\n        if device in available_ids:\n            return towers[device].vs_name\n        else:\n            return towers[0].vs_name\n\n\n@six.add_metaclass(ABCMeta)\nclass SingleCostTrainer(TowerTrainer):\n    """"""\n    Base class for single-cost trainer.\n\n    Single-cost trainer has a :meth:`setup_graph` method which takes\n    (input_signature, input, get_cost_fn, get_opt_fn), and build the training graph from them.\n\n    To use a :class:`SingleCostTrainer` object, call `trainer.setup_graph(...); trainer.train(...)`.\n    """"""\n\n    COLOCATE_GRADIENTS_WITH_OPS = True\n    """"""\n    See `tf.gradients`. It sometimes can heavily affect performance when backward op does\n    not support the device of forward op.\n    """"""\n\n    GATE_GRADIENTS = False\n    """"""See `tf.gradients`. """"""\n\n    AGGREGATION_METHOD = tf.AggregationMethod.DEFAULT\n    """"""See `tf.gradients`. """"""\n\n    XLA_COMPILE = False\n    """""" Use :func:`xla.compile` to compile the tower function.\n    Note that XLA has very strong requirements on the tower function, e.g.:\n\n    1. limited op support\n    2. inferrable shape\n    3. no summary support\n\n    and many tower functions cannot be compiled by XLA.\n    Don\'t use it if you don\'t understand it.\n    """"""\n\n    @call_only_once\n    def setup_graph(self, input_signature, input, get_cost_fn, get_opt_fn):\n        """"""\n        Responsible for building the main training graph for single-cost training.\n\n        Args:\n            input_signature ([TensorSpec]): list of TensorSpec that describe the inputs\n            input (InputSource): an InputSource which has to match the input signature\n            get_cost_fn ([tf.Tensor] -> tf.Tensor): callable, takes some input tensors and return a cost tensor.\n            get_opt_fn (-> tf.train.Optimizer): callable which returns an\n                optimizer. Will only be called once.\n\n        Note:\n            `get_cost_fn` will be part of the tower function.\n            It must follows the `rules of tower function.\n            <http://tensorpack.readthedocs.io/tutorial/trainer.html#tower-trainer>`_.\n        """"""\n        get_cost_fn = TowerFunc(get_cost_fn, input_signature)\n        get_opt_fn = memoized(get_opt_fn)\n        self.tower_func = get_cost_fn\n\n        # TODO setup may want to register monitor as well??\n        input_callbacks = self._setup_input(input_signature, input)\n        train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n        self.register_callback(input_callbacks + train_callbacks)\n\n    @abstractmethod\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        """"""\n        Implement the logic to build the graph, with an :class:`InputSource`\n        that\'s been setup already.\n\n        Returns:\n            [Callback]: list of callbacks needed\n        """"""\n\n    def _setup_input(self, input_signature, input):\n        assert not input.setup_done()\n        return input.setup(input_signature)\n\n    def _make_get_grad_fn(self, input, get_cost_fn, get_opt_fn):\n        """"""\n        Internal use only.\n\n        Returns:\n            a get_grad_fn for GraphBuilder to use.\n        """"""\n        assert input.setup_done()\n\n        def get_grad_fn():\n            ctx = get_current_tower_context()\n            inputs = input.get_input_tensors()\n\n            def compute_grad_from_inputs(*inputs):\n                cost = get_cost_fn(*inputs)\n                assert isinstance(cost, tf.Tensor), \\\n                    ""Expect the given function to return a cost, but got {} instead"".format(str(cost))\n                assert cost.shape.ndims == 0, ""Cost must be a scalar, but found {}!"".format(cost)\n\n                if not ctx.is_training:\n                    return None     # this is the tower function, could be called for inference\n\n                if ctx.has_own_variables:\n                    varlist = ctx.get_collection_in_tower(tfv1.GraphKeys.TRAINABLE_VARIABLES)\n                else:\n                    varlist = tfv1.trainable_variables()\n                opt = get_opt_fn()\n                if is_tfv2() and isinstance(opt, tf.optimizers.Optimizer):\n                    grads = opt.get_gradients(cost, varlist)\n                    grads = list(zip(grads, varlist))\n                else:\n                    grads = opt.compute_gradients(\n                        cost, var_list=varlist,\n                        gate_gradients=self.GATE_GRADIENTS,\n                        colocate_gradients_with_ops=self.COLOCATE_GRADIENTS_WITH_OPS,\n                        aggregation_method=self.AGGREGATION_METHOD)\n                grads = FilterNoneGrad().process(grads)\n                return grads\n\n            if not self.XLA_COMPILE:\n                return compute_grad_from_inputs(*inputs)\n            else:\n                try:\n                    from tensorflow.contrib.compiler import xla  # deprecated\n                except ImportError:\n                    from tensorflow.python.compiler.xla import xla\n\n                def xla_func():\n                    grads = compute_grad_from_inputs(*inputs)\n                    # unpack, because the return value\n                    # of xla function cannot have nested structure\n                    grads = [x[0] for x in grads]\n                    return grads\n\n                grads_no_vars = xla.compile(xla_func)\n                if ctx.has_own_variables:\n                    varlist = ctx.get_collection_in_tower(tf.GraphKeys.TRAINABLE_VARIABLES)\n                else:\n                    varlist = tf.trainable_variables()\n                return list(zip(grads_no_vars, varlist))\n\n        return get_grad_fn\n'"
tensorpack/train/trainers.py,5,"b'# -*- coding: utf-8 -*-\n# File: trainers.py\n\nimport multiprocessing as mp\nimport os\nimport sys\nimport tensorflow as tf\n\nfrom ..callbacks import CallbackFactory, RunOp\nfrom ..graph_builder.distributed import DistributedParameterServerBuilder, DistributedReplicatedBuilder\nfrom ..graph_builder.training import (\n    AsyncMultiGPUBuilder, SyncMultiGPUParameterServerBuilder, SyncMultiGPUReplicatedBuilder)\nfrom ..graph_builder.utils import override_to_local_variable\nfrom ..input_source import FeedfreeInput, QueueInput\nfrom ..tfutils import get_global_step_var\nfrom ..tfutils.distributed import get_distributed_session_creator\nfrom ..tfutils.sesscreate import NewSessionCreator\nfrom ..tfutils.tower import TrainTowerContext\nfrom ..utils import logger\nfrom ..utils.argtools import map_arg\nfrom ..utils.develop import HIDE_DOC, deprecated\nfrom .tower import SingleCostTrainer\n\n__all__ = [\'NoOpTrainer\', \'SimpleTrainer\',\n           \'QueueInputTrainer\',\n           \'SyncMultiGPUTrainer\',\n           \'SyncMultiGPUTrainerReplicated\',\n           \'SyncMultiGPUTrainerParameterServer\',\n           \'AsyncMultiGPUTrainer\',\n           \'DistributedTrainerParameterServer\',\n           \'DistributedTrainerReplicated\',\n           \'HorovodTrainer\', \'BytePSTrainer\']\n\n\ndef _int_to_range(x):\n    if isinstance(x, int):\n        assert x > 0, ""Argument cannot be {}!"".format(x)\n        return list(range(x))\n    return x\n\n\nclass SimpleTrainer(SingleCostTrainer):\n    """"""\n    Single-GPU single-cost single-tower trainer.\n    """"""\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        logger.info(""Building graph for a single training tower ..."")\n        with TrainTowerContext(\'\'):\n            grads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()\n            opt = get_opt_fn()\n            self.train_op = opt.apply_gradients(grads, name=\'train_op\')\n        return []\n\n\nclass NoOpTrainer(SimpleTrainer):\n    """"""\n    A special trainer that builds the graph (if given a tower function)\n    and does nothing in each step.\n    It is used to only run the callbacks.\n\n    Note that `steps_per_epoch` and `max_epochs` are still valid options.\n    """"""\n    def run_step(self):\n        self.hooked_sess.run([])\n\n\n# Only exists for type check & back-compatibility\nclass QueueInputTrainer(SimpleTrainer):\n    @deprecated(""SimpleTrainer is sufficient!"", ""2019-12-31"")\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        assert isinstance(input, QueueInput), input\n        return super(QueueInputTrainer, self)._setup_graph(input, get_cost_fn, get_opt_fn)\n\n\nclass SyncMultiGPUTrainerParameterServer(SingleCostTrainer):\n\n    __doc__ = SyncMultiGPUParameterServerBuilder.__doc__ + """"""\n\n    Attributes:\n        devices (list[int]): List of GPU ids.\n\n    """"""\n\n    @map_arg(gpus=_int_to_range)\n    def __init__(self, gpus, ps_device=None):\n        """"""\n        Args:\n            gpus ([int]): list of GPU ids.\n            ps_device: either \'gpu\' or \'cpu\', where variables are stored.\n                The default value is subject to change.\n        """"""\n        self.devices = gpus\n        if ps_device is None:\n            ps_device = \'gpu\' if len(gpus) <= 2 else \'cpu\'\n        self._builder = SyncMultiGPUParameterServerBuilder(gpus, ps_device)\n        super(SyncMultiGPUTrainerParameterServer, self).__init__()\n\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        if len(self.devices) > 1:\n            assert isinstance(input, FeedfreeInput), input\n        tower_fn = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)\n        grad_list = self._builder.call_for_each_tower(tower_fn)\n        self.train_op = self._builder.build(grad_list, get_opt_fn)\n        return []\n\n\ndef SyncMultiGPUTrainer(gpus):\n    """"""\n    Return a default multi-GPU trainer, if you don\'t care about the details.\n    It may not be the most efficient one for your task.\n\n    Args:\n        gpus (list[int]): list of GPU ids.\n    """"""\n    return SyncMultiGPUTrainerParameterServer(gpus, ps_device=\'cpu\')\n\n\nclass AsyncMultiGPUTrainer(SingleCostTrainer):\n\n    __doc__ = AsyncMultiGPUBuilder.__doc__ + """"""\n\n    Attributes:\n        devices (list[int]): List of GPU ids.\n\n    """"""\n\n    @map_arg(gpus=_int_to_range)\n    def __init__(self, gpus, scale_gradient=True):\n        """"""\n        Args:\n            gpus ([int]): list of GPU ids.\n            scale_gradient (bool): if True, will scale each gradient by ``1.0/nr_gpu``.\n        """"""\n        self.devices = gpus\n        self._builder = AsyncMultiGPUBuilder(gpus, scale_gradient)\n        super(AsyncMultiGPUTrainer, self).__init__()\n\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        if len(self.devices) > 1:\n            assert isinstance(input, FeedfreeInput), input\n        tower_fn = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)\n        grad_list = self._builder.call_for_each_tower(tower_fn)\n        self.train_op = self._builder.build(grad_list, get_opt_fn)\n        return []\n\n\nclass SyncMultiGPUTrainerReplicated(SingleCostTrainer):\n\n    __doc__ = SyncMultiGPUReplicatedBuilder.__doc__ + """"""\n\n    Attributes:\n        devices (list[int]): List of GPU ids.\n\n        BROADCAST_EVERY_EPOCH (bool):\n            Whether to broadcast the variables every epoch.\n            Theoretically this is a no-op (because the variables\n            are supposed to be in-sync).\n            But this cheap operation may help prevent\n            certain numerical issues in practice.\n            Note that in cases such as BatchNorm, the variables may not be in sync.\n    """"""\n\n    @map_arg(gpus=_int_to_range)\n    def __init__(self, gpus, average=True, mode=None):\n        """"""\n        Args:\n            gpus (int or [int]): list of GPU ids.\n            average (bool): whether to average or sum gradients.\n            mode (str or None): Gradient aggregation mode.\n                Supported values: [\'nccl\', \'hierarchical\', \'cpu\'].\n                Default to pick automatically by heuristics.\n                These modes may have slight (within 5%) differences in speed.\n                ""hierarchical"" mode was designed for DGX-like 8GPU machines.\n        """"""\n        self.devices = gpus\n\n        if mode is None:\n            mode = \'hierarchical\' if len(gpus) == 8 else \'nccl\'\n        mode = mode.lower()\n\n        self._builder = SyncMultiGPUReplicatedBuilder(gpus, average, mode)\n        self.BROADCAST_EVERY_EPOCH = True\n\n        super(SyncMultiGPUTrainerReplicated, self).__init__()\n\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        if len(self.devices) > 1:\n            assert isinstance(input, FeedfreeInput), input\n        tower_fn = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)\n        grad_list = self._builder.call_for_each_tower(tower_fn)\n        self.train_op, post_init_op = self._builder.build(grad_list, get_opt_fn)\n\n        if post_init_op is not None:\n            cb = RunOp(\n                post_init_op,\n                run_before=True,\n                run_as_trigger=self.BROADCAST_EVERY_EPOCH,\n                verbose=True)\n            cb.name_scope = ""SyncVariables""\n            return [cb]\n        else:\n            return []\n\n\nclass DistributedTrainerBase(SingleCostTrainer):\n\n    devices = None\n\n    def __init__(self, gpus, server):\n        super(DistributedTrainerBase, self).__init__()\n        self.devices = gpus\n        self.server = server\n        self.job_name = server.server_def.job_name\n        logger.info(""Distributed training on cluster:\\n"" + str(server.server_def.cluster))\n\n    def join(self):\n        logger.info(""Calling server.join() on {}:{}"".format(self.job_name, self.server.server_def.task_index))\n        logger.info(""Kill me with \'kill {}\'"".format(os.getpid()))\n        self.server.join()  # this function will never return tensorflow#4713\n        raise RuntimeError(""This is a bug. Server.join() for should never return!"")\n\n    @HIDE_DOC\n    def initialize(self, session_creator, session_init):\n        if not isinstance(session_creator, NewSessionCreator) or \\\n                session_creator.user_provided_config:\n            raise ValueError(\n                ""You are not allowed to set session_creator or session_config for distributed training! ""\n                ""To use a custom session config, pass it to tf.train.Server."")\n        super(DistributedTrainerBase, self).initialize(\n            get_distributed_session_creator(self.server), session_init)\n\n\n# This is slow. deprecated in favor of horovod\nclass DistributedTrainerParameterServer(DistributedTrainerBase):\n\n    __doc__ = DistributedParameterServerBuilder.__doc__\n\n    @map_arg(gpus=_int_to_range)\n    def __init__(self, gpus, server, caching_device=\'cpu\'):\n        """"""\n        Args:\n            gpus ([int]): list of GPU ids.\n            server (tf.train.Server): the server with ps and workers.\n            caching_device (str): either \'cpu\' or \'gpu\'. The device to cache variables copied from PS\n        """"""\n        super(DistributedTrainerParameterServer, self).__init__(gpus, server)\n        assert self.job_name in [\'ps\', \'worker\'], self.job_name\n        if self.job_name == \'ps\':\n            self.join()\n\n        self._builder = DistributedParameterServerBuilder(gpus, server, caching_device)\n        self.is_chief = self._builder.is_chief\n\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        assert isinstance(input, FeedfreeInput), input\n        self.train_op = self._builder.build(\n            self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n        return []\n\n\n# This is slow. deprecated in favor of horovod\nclass DistributedTrainerReplicated(DistributedTrainerBase):\n\n    __doc__ = DistributedReplicatedBuilder.__doc__\n\n    @map_arg(gpus=_int_to_range)\n    def __init__(self, gpus, server):\n        """"""\n        Args:\n            gpus (list[int]): list of GPU ids.\n            server (tf.train.Server): the server with ps and workers.\n        """"""\n        super(DistributedTrainerReplicated, self).__init__(gpus, server)\n        assert self.job_name in [\'ps\', \'worker\'], self.job_name\n        if self.job_name == \'ps\':\n            self.join()\n\n        self._builder = DistributedReplicatedBuilder(gpus, server)\n        self.is_chief = self._builder.is_chief\n\n    def _setup_input(self, input_signature, input):\n        with override_to_local_variable():\n            get_global_step_var()  # gs should be local\n            # input source may create variables (queue size summary)\n            # TODO This is not good because we don\'t know from here\n            # whether something should be global or local. We now assume\n            # they should be local.\n            assert not input.setup_done()\n            return input.setup(input_signature)\n\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        assert isinstance(input, FeedfreeInput), input\n        self.train_op, initial_sync_op, model_sync_op = self._builder.build(\n            self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n\n        callbacks = []\n        # Initial syncing vars from PS\n        cb = RunOp(lambda: initial_sync_op,\n                   run_before=True, run_as_trigger=False, verbose=True)\n        cb.chief_only = False\n        callbacks.append(cb)\n\n        # Sync model_variables to PS, only chief needs to do this\n        if model_sync_op:\n            cb = RunOp(lambda: model_sync_op,\n                       run_before=False, run_as_trigger=True, verbose=True)\n            logger.warn(""For efficiency, local MODEL_VARIABLES are only synced to PS once ""\n                        ""every epoch. Be careful if you save the model more frequently than this."")\n            callbacks.append(cb)\n        return callbacks\n\n    @property\n    def _main_tower_vs_name(self):\n        return ""tower0""\n\n\nclass HorovodTrainer(SingleCostTrainer):\n    """"""\n    Horovod trainer, support both multi-GPU and distributed training.\n\n    To use for multi-GPU training:\n\n    .. code-block:: bash\n\n        # First, change trainer to HorovodTrainer(), then\n        CUDA_VISIBLE_DEVICES=0,1,2,3 NCCL_DEBUG=INFO mpirun -np 4 --output-filename mylog python train.py\n\n    To use for distributed training:\n\n    .. code-block:: bash\n\n        # First, change trainer to HorovodTrainer(), then\n        mpirun -np 8 -H server1:4,server2:4  \\\\\n            -bind-to none -map-by slot \\\\\n            --output-filename mylog -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH \\\\\n            python train.py\n        # Add other environment variables you need by -x, e.g. PYTHONPATH, PATH.\n        # If using all GPUs, you can always skip the `CUDA_VISIBLE_DEVICES` option.\n        # There are other MPI options that can potentially improve performance especially on special hardwares.\n\n    Horovod can also be launched without MPI. See\n    `its documentation <https://github.com/horovod/horovod#running-horovod>`_\n    for more details.\n\n    Note:\n        1. To reach the maximum speed in your system, there are many options to tune\n           for Horovod installation and in the MPI command line.\n           See Horovod docs for details.\n\n        2. Due to a TF bug (#8136), you must not initialize CUDA context before the trainer starts training.\n           Therefore TF functions like `is_gpu_available()` or `list_local_devices()`\n           must be avoided.\n           You can, however, use `tf.config.experimental.list_physical_devices(\'GPU\')`, introduced in TF 1.14.\n\n        3. Horovod supports both MPI and gloo. There are a few drawbacks of the MPI backend:\n\n            + MPI does not like `fork()`. If your code (e.g. dataflow) contains multiprocessing, it may cause problems.\n            + MPI sometimes fails to kill all processes in the end. Be sure to check it afterwards.\n\n        4. Keep in mind that there is one process running the script per GPU, therefore:\n\n           + Make sure your InputSource has reasonable randomness.\n\n           + If your data processing is heavy, doing it in a single dedicated process might be\n             a better choice than doing them repeatedly in each process.\n\n           + You need to make sure log directories in each process won\'t conflict.\n             You can set it only for the chief process, or set a different one for each process.\n\n           + Callbacks have an option to be run only in the chief process, or in all processes.\n             See :meth:`Callback.set_chief_only()`. Most callbacks have a reasonable\n             default already, but certain callbacks may need your customization.\n             Report an issue if you find any bad defaults.\n\n           + You can use Horovod API such as `hvd.rank()` to know which process you are and choose\n             different code path. Chief process has rank 0.\n\n        5. Due to these caveats, see\n           `ResNet-Horovod <https://github.com/tensorpack/benchmarks/tree/master/ResNet-Horovod>`_\n           for a full example which has handled these common issues.\n           This example can train ImageNet in roughly an hour following the paper\'s setup.\n\n    Attributes:\n        BROADCAST_EVERY_EPOCH (bool):\n            Whether to broadcast the variables every epoch.\n            Theoretically this is a no-op (because the variables\n            are supposed to be in-sync).\n            But this cheap operation may help prevent certain numerical issues in practice.\n            Note that in cases such as BatchNorm, the variables may not be in sync.\n    """"""\n\n    def __init__(self, average=True, compression=None):\n        """"""\n        Args:\n            average (bool): whether to average or sum the gradients across processes.\n            compression: `hvd.Compression.fp16` or `hvd.Compression.none`\n        """"""\n        if \'pyarrow\' in sys.modules:\n            logger.warn(""Horovod and pyarrow may conflict due to pyarrow bugs."")\n        # lazy import\n        import horovod.tensorflow as hvd\n        import horovod\n        hvd_version = tuple(map(int, horovod.__version__.split(\'.\')[:3]))\n        self.hvd = hvd\n\n        hvd.init()\n        self.is_chief = hvd.rank() == 0\n        self._local_rank = hvd.local_rank()\n        self._rank = hvd.rank()\n        self._average = average\n        self._compression = compression\n        self._has_compression = hvd_version >= (0, 15, 0)\n        logger.info(""[HorovodTrainer] local rank={}"".format(self._local_rank))\n        super(HorovodTrainer, self).__init__()\n\n        self.BROADCAST_EVERY_EPOCH = True\n\n    def mpi_enabled(self):\n        """"""\n        Returns:\n            bool: whether hvd is currently running under MPI\n        """"""\n        try:\n            return self.hvd.mpi_enabled()\n        except AttributeError:\n            return False\n\n    def allreduce(self, grads):\n        if self.hvd.size() == 1:\n            return grads\n        # copied from https://github.com/uber/horovod/blob/master/horovod/tensorflow/__init__.py\n        averaged_gradients = []\n        with tf.name_scope(""AllReduce""):\n            for grad, var in grads:\n                if grad is not None:\n                    if self._compression is not None and self._has_compression:\n                        avg_grad = self.hvd.allreduce(grad, average=self._average, compression=self._compression)\n                    else:\n                        avg_grad = self.hvd.allreduce(grad, average=self._average)\n                    averaged_gradients.append((avg_grad, var))\n                else:\n                    averaged_gradients.append((None, var))\n        return averaged_gradients\n\n    def _setup_graph(self, input, get_cost_fn, get_opt_fn):\n        with TrainTowerContext(\'\'):\n            grads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()\n            grads = self.allreduce(grads)\n\n            opt = get_opt_fn()\n            self.train_op = opt.apply_gradients(grads, name=\'train_op\')\n\n        cb = CallbackFactory(\n            before_train=self.broadcast,\n            trigger=self.broadcast if self.BROADCAST_EVERY_EPOCH else None\n        ).set_chief_only(False)\n        return [cb]\n\n    def broadcast(self, _):\n        logger.info(""Running broadcast ..."")\n        # the op will be created in initialize()\n        self.sess.run(self._broadcast_op)\n\n    @HIDE_DOC\n    def initialize(self, session_creator, session_init):\n        # broadcast_op should be the last setup_graph: it needs to be created\n        # ""right before"" the graph is finalized,\n        # because it needs to capture all the variables (which may be created by callbacks).\n        self._broadcast_op = self.hvd.broadcast_global_variables(0)\n\n        # it\'s important that our NewSessionCreator does not finalize the graph\n        if not isinstance(session_creator, NewSessionCreator):\n            raise ValueError(\n                ""session_creator has to be `NewSessionCreator` for horovod/byteps training! "")\n        # NOTE It will fail if GPU was already detected before initializing the session\n        # https://github.com/tensorflow/tensorflow/issues/8136\n        session_creator.config.gpu_options.visible_device_list = str(self._local_rank)\n        try:\n            session_creator.config.inter_op_parallelism_threads = mp.cpu_count() // self.hvd.local_size()\n        except AttributeError:  # old horovod does not have local_size\n            pass\n        super(HorovodTrainer, self).initialize(session_creator, session_init)\n\n        # This broadcast belongs to the ""intialize"" stage\n        # It should not be delayed to the ""before_train"" stage.\n        # TODO:\n        # 1. a allgather helper to concat strings\n        # 2. check variables on each rank match each other, print warnings, and broadcast the common set.\n        if self.is_chief:\n            logger.info(""Broadcasting initialized variables ..."")\n        else:\n            logger.info(""Rank {} waiting for initialization broadcasting ..."".format(self._rank))\n        self.sess.run(self._broadcast_op)\n\n\nclass BytePSTrainer(HorovodTrainer):\n    """"""\n    BytePS trainer. Supports both multi-GPU and distributed training.\n    It achieves better scalability than horovod in distributed training, if the model is communication\n    intensive and you have properly set up the machines following its\n    `best practices <https://github.com/bytedance/byteps/blob/master/docs/best-practice.md>`_\n    which requires a few extra bandwidth servers than horovod.\n\n    To use it, switch the trainer, and refer to BytePS documentation on how to\n    launch server/scheduler/workers.\n\n    Attributes:\n        hvd (module): the byteps module that contains horovod-compatible APIs\n            like `rank(),size()`.\n            This attribute exists so that downstream code that uses these APIs\n            does not need to worry about which library is being used under the hood.\n    """"""\n    def __init__(self, average=True):\n        """"""\n        Args:\n            average (bool): whether to average or sum the gradients across processes.\n        """"""\n        import byteps.tensorflow as bps\n        self.hvd = bps  # BytePS has the same interface as Horovod\n        self.hvd.allreduce = bps.push_pull  # https://github.com/bytedance/byteps/issues/8\n        assert os.environ.get(""DMLC_ROLE"", None) == ""worker""\n        assert ""DMLC_WORKER_ID"" in os.environ and ""DMLC_NUM_WORKER"" in os.environ\n        bps.init()\n        self.is_chief = bps.rank() == 0\n\n        self._local_rank = bps.local_rank()\n        self._rank = bps.rank()\n        self._average = average\n\n        self._compression = None\n        self._has_compression = False\n        logger.info(""[BytePSTrainer] local rank={}"".format(self._local_rank))\n        SingleCostTrainer.__init__(self)\n\n    def mpi_enabled(self):\n        """"""\n        Returns:\n            bool: whether hvd is currently running under MPI\n        """"""\n        return False\n'"
tensorpack/train/utility.py,0,"b'# -*- coding: utf-8 -*-\n# File: utility.py\n\n# for backwards-compatibility\nfrom ..graph_builder.utils import LeastLoadedDeviceSetter, OverrideToLocalVariable, override_to_local_variable  # noqa\n'"
tensorpack/utils/__init__.py,0,"b'#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n""""""\nCommon utils.\nThese utils should be irrelevant to tensorflow.\n""""""\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()[\'kcah_acitats\'[::-1].upper()] = False\nif STATICA_HACK:\n    from .utils import *\n\n\n__all__ = []\n\n\ndef _global_import(name):\n    p = __import__(name, globals(), None, level=1)\n    lst = p.__all__ if \'__all__\' in dir(p) else dir(p)\n    for k in lst:\n        if not k.startswith(\'__\'):\n            globals()[k] = p.__dict__[k]\n            __all__.append(k)\n\n\n_global_import(\'utils\')\n\n# Import no other submodules. they are supposed to be explicitly imported by users.\n__all__.extend([\'logger\'])\n'"
tensorpack/utils/argtools.py,0,"b'# -*- coding: utf-8 -*-\n# File: argtools.py\n\n\nimport inspect\nimport functools\n\nfrom . import logger\n\n__all__ = [\'map_arg\', \'memoized\', \'memoized_method\', \'graph_memoized\', \'shape2d\', \'shape4d\',\n           \'memoized_ignoreargs\', \'log_once\']\n\n\ndef map_arg(**maps):\n    """"""\n    Apply a mapping on certain argument before calling the original function.\n\n    Args:\n        maps (dict): {argument_name: map_func}\n    """"""\n    def deco(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # getcallargs was deprecated since 3.5\n            sig = inspect.signature(func)\n            argmap = sig.bind_partial(*args, **kwargs).arguments\n            for k, map_func in maps.items():\n                if k in argmap:\n                    argmap[k] = map_func(argmap[k])\n            return func(**argmap)\n        return wrapper\n    return deco\n\n\nmemoized = functools.lru_cache(maxsize=None)\n"""""" Alias to :func:`functools.lru_cache`\nWARNING: memoization will keep keys and values alive!\n""""""\n\n\ndef graph_memoized(func):\n    """"""\n    Like memoized, but keep one cache per default graph.\n    """"""\n\n    # TODO it keeps the graph alive\n    from ..compat import tfv1\n    GRAPH_ARG_NAME = \'__IMPOSSIBLE_NAME_FOR_YOU__\'\n\n    @memoized\n    def func_with_graph_arg(*args, **kwargs):\n        kwargs.pop(GRAPH_ARG_NAME)\n        return func(*args, **kwargs)\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        assert GRAPH_ARG_NAME not in kwargs, ""No Way!!""\n        graph = tfv1.get_default_graph()\n        kwargs[GRAPH_ARG_NAME] = graph\n        return func_with_graph_arg(*args, **kwargs)\n    return wrapper\n\n\n_MEMOIZED_NOARGS = {}\n\n\ndef memoized_ignoreargs(func):\n    """"""\n    A decorator. It performs memoization ignoring the arguments used to call\n    the function.\n    """"""\n    def wrapper(*args, **kwargs):\n        if func not in _MEMOIZED_NOARGS:\n            res = func(*args, **kwargs)\n            _MEMOIZED_NOARGS[func] = res\n            return res\n        return _MEMOIZED_NOARGS[func]\n    return wrapper\n\n\ndef shape2d(a):\n    """"""\n    Ensure a 2D shape.\n\n    Args:\n        a: a int or tuple/list of length 2\n\n    Returns:\n        list: of length 2. if ``a`` is a int, return ``[a, a]``.\n    """"""\n    if type(a) == int:\n        return [a, a]\n    if isinstance(a, (list, tuple)):\n        assert len(a) == 2\n        return list(a)\n    raise RuntimeError(""Illegal shape: {}"".format(a))\n\n\ndef get_data_format(data_format, keras_mode=True):\n    if keras_mode:\n        dic = {\'NCHW\': \'channels_first\', \'NHWC\': \'channels_last\'}\n    else:\n        dic = {\'channels_first\': \'NCHW\', \'channels_last\': \'NHWC\'}\n    ret = dic.get(data_format, data_format)\n    if ret not in dic.values():\n        raise ValueError(""Unknown data_format: {}"".format(data_format))\n    return ret\n\n\ndef shape4d(a, data_format=\'NHWC\'):\n    """"""\n    Ensuer a 4D shape, to use with 4D symbolic functions.\n\n    Args:\n        a: a int or tuple/list of length 2\n\n    Returns:\n        list: of length 4. if ``a`` is a int, return ``[1, a, a, 1]``\n            or ``[1, 1, a, a]`` depending on data_format.\n    """"""\n    s2d = shape2d(a)\n    if get_data_format(data_format, False) == \'NHWC\':\n        return [1] + s2d + [1]\n    else:\n        return [1, 1] + s2d\n\n\n@memoized\ndef log_once(message, func=\'info\'):\n    """"""\n    Log certain message only once. Call this function more than one times with\n    the same message will result in no-op.\n\n    Args:\n        message(str): message to log\n        func(str): the name of the logger method. e.g. ""info"", ""warn"", ""error"".\n    """"""\n    getattr(logger, func)(message)\n\n\ndef call_only_once(func):\n    """"""\n    Decorate a method or property of a class, so that this method can only\n    be called once for every instance.\n    Calling it more than once will result in exception.\n    """"""\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        # cannot use hasattr here, because hasattr tries to getattr, which\n        # fails if func is a property\n        assert func.__name__ in dir(self), ""call_only_once can only be used on method or property!""\n\n        if not hasattr(self, \'_CALL_ONLY_ONCE_CACHE\'):\n            cache = self._CALL_ONLY_ONCE_CACHE = set()\n        else:\n            cache = self._CALL_ONLY_ONCE_CACHE\n\n        cls = type(self)\n        # cannot use ismethod(), because decorated method becomes a function\n        is_method = inspect.isfunction(getattr(cls, func.__name__))\n        assert func not in cache, \\\n            ""{} {}.{} can only be called once per object!"".format(\n                \'Method\' if is_method else \'Property\',\n                cls.__name__, func.__name__)\n        cache.add(func)\n\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef memoized_method(func):\n    """"""\n    A decorator that performs memoization on methods. It stores the cache on the object instance itself.\n    """"""\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        assert func.__name__ in dir(self), ""memoized_method can only be used on method!""\n\n        if not hasattr(self, \'_MEMOIZED_CACHE\'):\n            cache = self._MEMOIZED_CACHE = {}\n        else:\n            cache = self._MEMOIZED_CACHE\n\n        key = (func, ) + args[1:] + tuple(kwargs)\n        ret = cache.get(key, None)\n        if ret is not None:\n            return ret\n        value = func(*args, **kwargs)\n        cache[key] = value\n        return value\n\n    return wrapper\n\n\nif __name__ == \'__main__\':\n    class A():\n        def __init__(self):\n            self._p = 0\n\n        @call_only_once\n        def f(self, x):\n            print(x)\n\n        @property\n        def p(self):\n            return self._p\n\n        @p.setter\n        @call_only_once\n        def p(self, val):\n            self._p = val\n\n    a = A()\n    a.f(1)\n\n    b = A()\n    b.f(2)\n    b.f(1)\n\n    print(b.p)\n    print(b.p)\n    b.p = 2\n    print(b.p)\n    b.p = 3\n    print(b.p)\n'"
tensorpack/utils/compatible_serialize.py,0,"b'from .serialize import loads, dumps  # noqa\n\n# keep this file for BC\n'"
tensorpack/utils/concurrency.py,0,"b'# -*- coding: utf-8 -*-\n# File: concurrency.py\n\n# Some code taken from zxytim\n\nimport sys\nimport atexit\nimport bisect\nimport multiprocessing as mp\nimport platform\nimport signal\nimport threading\nimport weakref\nfrom contextlib import contextmanager\nimport six\nfrom six.moves import queue\nimport subprocess\n\nfrom . import logger\nfrom .argtools import log_once\n\n\n__all__ = [\'StoppableThread\', \'LoopThread\', \'ShareSessionThread\',\n           \'ensure_proc_terminate\',\n           \'start_proc_mask_signal\']\n\n\nclass StoppableThread(threading.Thread):\n    """"""\n    A thread that has a \'stop\' event.\n    """"""\n\n    def __init__(self, evt=None):\n        """"""\n        Args:\n            evt(threading.Event): if None, will create one.\n        """"""\n        super(StoppableThread, self).__init__()\n        if evt is None:\n            evt = threading.Event()\n        self._stop_evt = evt\n\n    def stop(self):\n        """""" Stop the thread""""""\n        self._stop_evt.set()\n\n    def stopped(self):\n        """"""\n        Returns:\n            bool: whether the thread is stopped or not\n        """"""\n        return self._stop_evt.isSet()\n\n    def queue_put_stoppable(self, q, obj):\n        """""" Put obj to queue, but will give up when the thread is stopped""""""\n        while not self.stopped():\n            try:\n                q.put(obj, timeout=5)\n                break\n            except queue.Full:\n                pass\n\n    def queue_get_stoppable(self, q):\n        """""" Take obj from queue, but will give up when the thread is stopped""""""\n        while not self.stopped():\n            try:\n                return q.get(timeout=5)\n            except queue.Empty:\n                pass\n\n\nclass LoopThread(StoppableThread):\n    """""" A pausable thread that simply runs a loop""""""\n\n    def __init__(self, func, pausable=True):\n        """"""\n        Args:\n            func: the function to run\n        """"""\n        super(LoopThread, self).__init__()\n        self._func = func\n        self._pausable = pausable\n        if pausable:\n            self._lock = threading.Lock()\n        self.daemon = True\n\n    def run(self):\n        while not self.stopped():\n            if self._pausable:\n                self._lock.acquire()\n                self._lock.release()\n            self._func()\n\n    def pause(self):\n        """""" Pause the loop """"""\n        assert self._pausable\n        self._lock.acquire()\n\n    def resume(self):\n        """""" Resume the loop """"""\n        assert self._pausable\n        self._lock.release()\n\n\nclass ShareSessionThread(threading.Thread):\n    """""" A wrapper around thread so that the thread\n        uses the default session at ""start()"" time.\n    """"""\n    def __init__(self, th=None):\n        """"""\n        Args:\n            th (threading.Thread or None):\n        """"""\n        super(ShareSessionThread, self).__init__()\n        if th is not None:\n            assert isinstance(th, threading.Thread), th\n            self._th = th\n            self.name = th.name\n            self.daemon = th.daemon\n\n    @contextmanager\n    def default_sess(self):\n        if self._sess:\n            with self._sess.as_default():\n                yield self._sess\n        else:\n            logger.warn(""ShareSessionThread {} wasn\'t under a default session!"".format(self.name))\n            yield None\n\n    def start(self):\n        from ..compat import tfv1\n        self._sess = tfv1.get_default_session()\n        super(ShareSessionThread, self).start()\n\n    def run(self):\n        if not self._th:\n            raise NotImplementedError()\n        with self._sess.as_default():\n            self._th.run()\n\n\nclass DIE(object):\n    """""" A placeholder class indicating end of queue """"""\n    pass\n\n\ndef ensure_proc_terminate(proc):\n    """"""\n    Make sure processes terminate when main process exit.\n\n    Args:\n        proc (multiprocessing.Process or list)\n    """"""\n    if isinstance(proc, list):\n        for p in proc:\n            ensure_proc_terminate(p)\n        return\n\n    def stop_proc_by_weak_ref(ref):\n        proc = ref()\n        if proc is None:\n            return\n        if not proc.is_alive():\n            return\n        proc.terminate()\n        proc.join()\n\n    assert isinstance(proc, mp.Process)\n    atexit.register(stop_proc_by_weak_ref, weakref.ref(proc))\n\n\ndef enable_death_signal(_warn=True):\n    """"""\n    Set the ""death signal"" of the current process, so that\n    the current process will be cleaned with guarantee\n    in case the parent dies accidentally.\n    """"""\n    if platform.system() != \'Linux\':\n        return\n    try:\n        import prctl    # pip install python-prctl\n    except ImportError:\n        if _warn:\n            log_once(\'""import prctl"" failed! Install python-prctl so that processes can be cleaned with guarantee.\',\n                     \'warn\')\n        return\n    else:\n        assert hasattr(prctl, \'set_pdeathsig\'), \\\n            ""prctl.set_pdeathsig does not exist! Note that you need to install \'python-prctl\' instead of \'prctl\'.""\n        # is SIGHUP a good choice?\n        prctl.set_pdeathsig(signal.SIGHUP)\n\n\ndef is_main_thread():\n    if six.PY2:\n        return isinstance(threading.current_thread(), threading._MainThread)\n    else:\n        # a nicer solution with py3\n        return threading.current_thread() == threading.main_thread()\n\n\n@contextmanager\ndef mask_sigint():\n    """"""\n    Returns:\n        If called in main thread, returns a context where ``SIGINT`` is ignored, and yield True.\n        Otherwise yield False.\n    """"""\n    if is_main_thread():\n        sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n        yield True\n        signal.signal(signal.SIGINT, sigint_handler)\n    else:\n        yield False\n\n\ndef start_proc_mask_signal(proc):\n    """"""\n    Start process(es) with SIGINT ignored.\n\n    Args:\n        proc: (mp.Process or list)\n\n    Note:\n        The signal mask is only applied when called from main thread.\n    """"""\n    if not isinstance(proc, list):\n        proc = [proc]\n\n    with mask_sigint():\n        for p in proc:\n            if isinstance(p, mp.Process):\n                if sys.version_info < (3, 4) or mp.get_start_method() == \'fork\':\n                    log_once(""""""\nStarting a process with \'fork\' method is efficient but not safe and may cause deadlock or crash.\nUse \'forkserver\' or \'spawn\' method instead if you run into such issues.\nSee https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods on how to set them.\n"""""".replace(""\\n"", """"),\n\'warn\')  # noqa\n            p.start()\n\n\ndef subproc_call(cmd, timeout=None):\n    """"""\n    Execute a command with timeout, and return STDOUT and STDERR\n\n    Args:\n        cmd(str): the command to execute.\n        timeout(float): timeout in seconds.\n\n    Returns:\n        output(bytes), retcode(int). If timeout, retcode is -1.\n    """"""\n    try:\n        output = subprocess.check_output(\n            cmd, stderr=subprocess.STDOUT,\n            shell=True, timeout=timeout)\n        return output, 0\n    except subprocess.TimeoutExpired as e:\n        logger.warn(""Command \'{}\' timeout!"".format(cmd))\n        if e.output:\n            logger.warn(e.output.decode(\'utf-8\'))\n            return e.output, -1\n        else:\n            return """", -1\n    except subprocess.CalledProcessError as e:\n        logger.warn(""Command \'{}\' failed, return code={}"".format(cmd, e.returncode))\n        logger.warn(e.output.decode(\'utf-8\'))\n        return e.output, e.returncode\n    except Exception:\n        logger.warn(""Command \'{}\' failed to run."".format(cmd))\n        return """", -2\n\n\nclass OrderedContainer(object):\n    """"""\n    Like a queue, but will always wait to receive item with rank\n    (x+1) and produce (x+1) before producing (x+2).\n\n    Warning:\n        It is not thread-safe.\n    """"""\n\n    def __init__(self, start=0):\n        """"""\n        Args:\n            start(int): the starting rank.\n        """"""\n        self.ranks = []\n        self.data = []\n        self.wait_for = start\n\n    def put(self, rank, val):\n        """"""\n        Args:\n            rank(int): rank of th element. All elements must have different ranks.\n            val: an object\n        """"""\n        idx = bisect.bisect(self.ranks, rank)\n        self.ranks.insert(idx, rank)\n        self.data.insert(idx, val)\n\n    def has_next(self):\n        if len(self.ranks) == 0:\n            return False\n        return self.ranks[0] == self.wait_for\n\n    def get(self):\n        assert self.has_next()\n        ret = self.data[0]\n        rank = self.ranks[0]\n        del self.ranks[0]\n        del self.data[0]\n        self.wait_for += 1\n        return rank, ret\n\n\nclass OrderedResultGatherProc(mp.Process):\n    """"""\n    Gather indexed data from a data queue, and produce results with the\n    original index-based order.\n    """"""\n\n    def __init__(self, data_queue, nr_producer, start=0):\n        """"""\n        Args:\n            data_queue(mp.Queue): a queue which contains datapoints.\n            nr_producer(int): number of producer processes. This process will\n                terminate after receiving this many of :class:`DIE` sentinel.\n            start(int): the rank of the first object\n        """"""\n        super(OrderedResultGatherProc, self).__init__()\n        self.data_queue = data_queue\n        self.ordered_container = OrderedContainer(start=start)\n        self.result_queue = mp.Queue()\n        self.nr_producer = nr_producer\n\n    def run(self):\n        nr_end = 0\n        try:\n            while True:\n                task_id, data = self.data_queue.get()\n                if task_id == DIE:\n                    self.result_queue.put((task_id, data))\n                    nr_end += 1\n                    if nr_end == self.nr_producer:\n                        return\n                else:\n                    self.ordered_container.put(task_id, data)\n                    while self.ordered_container.has_next():\n                        self.result_queue.put(self.ordered_container.get())\n        except Exception as e:\n            import traceback\n            traceback.print_exc()\n            raise e\n\n    def get(self):\n        return self.result_queue.get()\n'"
tensorpack/utils/debug.py,0,"b'# -*- coding: utf-8 -*-\n# File: debug.py\n\n\nimport sys\n\n\ndef enable_call_trace():\n    """""" Enable trace for calls to any function. """"""\n    def tracer(frame, event, arg):\n        if event == \'call\':\n            co = frame.f_code\n            func_name = co.co_name\n            if func_name == \'write\' or func_name == \'print\':\n                # ignore write() calls from print statements\n                return\n            func_line_no = frame.f_lineno\n            func_filename = co.co_filename\n            caller = frame.f_back\n            if caller:\n                caller_line_no = caller.f_lineno\n                caller_filename = caller.f_code.co_filename\n                print(\'Call to `%s` on line %s:%s from %s:%s\' %\n                      (func_name, func_filename, func_line_no,\n                       caller_filename, caller_line_no))\n            return\n    sys.settrace(tracer)\n\n\nif __name__ == \'__main__\':\n    enable_call_trace()\n\n    def b(a):\n        print(2)\n\n    def a():\n        print(1)\n        b(1)\n\n    a()\n'"
tensorpack/utils/develop.py,0,"b'# -*- coding: utf-8 -*-\n# File: develop.py\n# Author: tensorpack contributors\n\n\n"""""" Utilities for developers only.\nThese are not visible to users (not automatically imported). And should not\nappeared in docs.""""""\nimport functools\nimport importlib\nimport os\nimport types\nfrom collections import defaultdict\nfrom datetime import datetime\nimport six\n\nfrom . import logger\n\n__all__ = []\n\n\ndef create_dummy_class(klass, dependency):\n    """"""\n    When a dependency of a class is not available, create a dummy class which throws ImportError when used.\n\n    Args:\n        klass (str): name of the class.\n        dependency (str): name of the dependency.\n\n    Returns:\n        class: a class object\n    """"""\n    assert not building_rtfd()\n\n    class _DummyMetaClass(type):\n        # throw error on class attribute access\n        def __getattr__(_, __):\n            raise AttributeError(""Cannot import \'{}\', therefore \'{}\' is not available"".format(dependency, klass))\n\n    @six.add_metaclass(_DummyMetaClass)\n    class _Dummy(object):\n        # throw error on constructor\n        def __init__(self, *args, **kwargs):\n            raise ImportError(""Cannot import \'{}\', therefore \'{}\' is not available"".format(dependency, klass))\n\n    return _Dummy\n\n\ndef create_dummy_func(func, dependency):\n    """"""\n    When a dependency of a function is not available, create a dummy function which throws ImportError when used.\n\n    Args:\n        func (str): name of the function.\n        dependency (str or list[str]): name(s) of the dependency.\n\n    Returns:\n        function: a function object\n    """"""\n    assert not building_rtfd()\n\n    if isinstance(dependency, (list, tuple)):\n        dependency = \',\'.join(dependency)\n\n    def _dummy(*args, **kwargs):\n        raise ImportError(""Cannot import \'{}\', therefore \'{}\' is not available"".format(dependency, func))\n    return _dummy\n\n\ndef building_rtfd():\n    """"""\n    Returns:\n        bool: if the library is being imported to generate docs now.\n    """"""\n    return os.environ.get(\'READTHEDOCS\') == \'True\' \\\n        or os.environ.get(\'DOC_BUILDING\')\n\n\n_DEPRECATED_LOG_NUM = defaultdict(int)\n\n\ndef log_deprecated(name="""", text="""", eos="""", max_num_warnings=None):\n    """"""\n    Log deprecation warning.\n\n    Args:\n        name (str): name of the deprecated item.\n        text (str, optional): information about the deprecation.\n        eos (str, optional): end of service date such as ""YYYY-MM-DD"".\n        max_num_warnings (int, optional): the maximum number of times to print this warning\n    """"""\n    assert name or text\n    if eos:\n        eos = ""after "" + datetime(*map(int, eos.split(""-""))).strftime(""%d %b"")\n    if name:\n        if eos:\n            warn_msg = ""%s will be deprecated %s. %s"" % (name, eos, text)\n        else:\n            warn_msg = ""%s was deprecated. %s"" % (name, text)\n    else:\n        warn_msg = text\n        if eos:\n            warn_msg += "" Legacy period ends %s"" % eos\n\n    if max_num_warnings is not None:\n        if _DEPRECATED_LOG_NUM[warn_msg] >= max_num_warnings:\n            return\n        _DEPRECATED_LOG_NUM[warn_msg] += 1\n    logger.warn(""[Deprecated] "" + warn_msg)\n\n\ndef deprecated(text="""", eos="""", max_num_warnings=None):\n    """"""\n    Args:\n        text, eos, max_num_warnings: same as :func:`log_deprecated`.\n\n    Returns:\n        a decorator which deprecates the function.\n\n    Example:\n        .. code-block:: python\n\n            @deprecated(""Explanation of what to do instead."", ""2017-11-4"")\n            def foo(...):\n                pass\n    """"""\n\n    def get_location():\n        import inspect\n        frame = inspect.currentframe()\n        if frame:\n            callstack = inspect.getouterframes(frame)[-1]\n            return \'%s:%i\' % (callstack[1], callstack[2])\n        else:\n            stack = inspect.stack(0)\n            entry = stack[2]\n            return \'%s:%i\' % (entry[1], entry[2])\n\n    def deprecated_inner(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            name = ""{} [{}]"".format(func.__name__, get_location())\n            log_deprecated(name, text, eos, max_num_warnings=max_num_warnings)\n            return func(*args, **kwargs)\n        return new_func\n    return deprecated_inner\n\n\ndef HIDE_DOC(func):\n    func.__HIDE_SPHINX_DOC__ = True\n    return func\n\n\n# Copied from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/lazy_loader.py\nclass LazyLoader(types.ModuleType):\n    def __init__(self, local_name, parent_module_globals, name):\n        self._local_name = local_name\n        self._parent_module_globals = parent_module_globals\n        super(LazyLoader, self).__init__(name)\n\n    def _load(self):\n        # Import the target module and insert it into the parent\'s namespace\n        module = importlib.import_module(self.__name__)\n        self._parent_module_globals[self._local_name] = module\n\n        # Update this object\'s dict so that if someone keeps a reference to the\n        #   LazyLoader, lookups are efficient (__getattr__ is only called on lookups\n        #   that fail).\n        self.__dict__.update(module.__dict__)\n\n        return module\n\n    def __getattr__(self, item):\n        module = self._load()\n        return getattr(module, item)\n\n    def __dir__(self):\n        module = self._load()\n        return dir(module)\n'"
tensorpack/utils/fs.py,0,"b'# -*- coding: utf-8 -*-\n# File: fs.py\n\n\nimport errno\nimport os\nimport tqdm\nfrom six.moves import urllib\n\nfrom . import logger\nfrom .utils import execute_only_once\n\n__all__ = [\'mkdir_p\', \'download\', \'recursive_walk\', \'get_dataset_path\', \'normpath\']\n\n\ndef mkdir_p(dirname):\n    """""" Like ""mkdir -p"", make a dir recursively, but do nothing if the dir exists\n\n    Args:\n        dirname(str):\n    """"""\n    assert dirname is not None\n    if dirname == \'\' or os.path.isdir(dirname):\n        return\n    try:\n        os.makedirs(dirname)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise e\n\n\ndef download(url, dir, filename=None, expect_size=None):\n    """"""\n    Download URL to a directory.\n    Will figure out the filename automatically from URL, if not given.\n    """"""\n    mkdir_p(dir)\n    if filename is None:\n        filename = url.split(\'/\')[-1]\n    fpath = os.path.join(dir, filename)\n\n    if os.path.isfile(fpath):\n        if expect_size is not None and os.stat(fpath).st_size == expect_size:\n            logger.info(""File {} exists! Skip download."".format(filename))\n            return fpath\n        else:\n            logger.warn(""File {} exists. Will overwrite with a new download!"".format(filename))\n\n    def hook(t):\n        last_b = [0]\n\n        def inner(b, bsize, tsize=None):\n            if tsize is not None:\n                t.total = tsize\n            t.update((b - last_b[0]) * bsize)\n            last_b[0] = b\n        return inner\n    try:\n        with tqdm.tqdm(unit=\'B\', unit_scale=True, miniters=1, desc=filename) as t:\n            fpath, _ = urllib.request.urlretrieve(url, fpath, reporthook=hook(t))\n        statinfo = os.stat(fpath)\n        size = statinfo.st_size\n    except IOError:\n        logger.error(""Failed to download {}"".format(url))\n        raise\n    assert size > 0, ""Downloaded an empty file from {}!"".format(url)\n\n    if expect_size is not None and size != expect_size:\n        logger.error(""File downloaded from {} does not match the expected size!"".format(url))\n        logger.error(""You may have downloaded a broken file, or the upstream may have modified the file."")\n\n    # TODO human-readable size\n    logger.info(\'Succesfully downloaded \' + filename + "". "" + str(size) + \' bytes.\')\n    return fpath\n\n\ndef recursive_walk(rootdir):\n    """"""\n    Yields:\n        str: All files in rootdir, recursively.\n    """"""\n    for r, dirs, files in os.walk(rootdir):\n        for f in files:\n            yield os.path.join(r, f)\n\n\ndef get_dataset_path(*args):\n    """"""\n    Get the path to some dataset under ``$TENSORPACK_DATASET``.\n\n    Args:\n        args: strings to be joined to form path.\n\n    Returns:\n        str: path to the dataset.\n    """"""\n    d = os.environ.get(\'TENSORPACK_DATASET\', None)\n    if d is None:\n        d = os.path.join(os.path.expanduser(\'~\'), \'tensorpack_data\')\n        if execute_only_once():\n            logger.warn(""Env var $TENSORPACK_DATASET not set, using {} for datasets."".format(d))\n        if not os.path.isdir(d):\n            mkdir_p(d)\n            logger.info(""Created the directory {}."".format(d))\n    assert os.path.isdir(d), d\n    return os.path.join(d, *args)\n\n\ndef normpath(path):\n    """"""\n    Normalizes a path to a folder by taking into consideration remote storages like Cloud storaged\n    referenced by \'://\' at the beginning of the path.\n\n    Args:\n        args: path to be normalized.\n\n    Returns:\n        str: normalized path.\n    """"""\n    return path if \'://\' in path else os.path.normpath(path)\n\n\nif __name__ == \'__main__\':\n    download(\'http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz\', \'.\')\n'"
tensorpack/utils/gpu.py,2,"b'# -*- coding: utf-8 -*-\n# File: gpu.py\n\n\nimport os\n\nfrom . import logger\nfrom .concurrency import subproc_call\nfrom .nvml import NVMLContext\nfrom .utils import change_env\n\n__all__ = [\'change_gpu\', \'get_nr_gpu\', \'get_num_gpu\']\n\n\ndef change_gpu(val):\n    """"""\n    Args:\n        val: an integer, the index of the GPU or -1 to disable GPU.\n\n    Returns:\n        a context where ``CUDA_VISIBLE_DEVICES=val``.\n    """"""\n    val = str(val)\n    if val == \'-1\':\n        val = \'\'\n    return change_env(\'CUDA_VISIBLE_DEVICES\', val)\n\n\ndef get_num_gpu():\n    """"""\n    Returns:\n        int: #available GPUs in CUDA_VISIBLE_DEVICES, or in the system.\n    """"""\n\n    def warn_return(ret, message):\n        try:\n            import tensorflow as tf\n        except ImportError:\n            return ret\n\n        built_with_cuda = tf.test.is_built_with_cuda()\n        if not built_with_cuda and ret > 0:\n            logger.warn(message + ""But TensorFlow was not built with CUDA support and could not use GPUs!"")\n        return ret\n\n    env = os.environ.get(\'CUDA_VISIBLE_DEVICES\', None)\n    if env:\n        return warn_return(len(env.split(\',\')), ""Found non-empty CUDA_VISIBLE_DEVICES. "")\n    output, code = subproc_call(""nvidia-smi -L"", timeout=5)\n    if code == 0:\n        output = output.decode(\'utf-8\')\n        return warn_return(len(output.strip().split(\'\\n\')), ""Found nvidia-smi. "")\n    try:\n        # Use NVML to query device properties\n        with NVMLContext() as ctx:\n            return warn_return(ctx.num_devices(), ""NVML found nvidia devices. "")\n    except Exception:\n        # Fallback\n        logger.info(""Loading local devices by TensorFlow ..."")\n\n        try:\n            import tensorflow as tf\n            # available since TF 1.14\n            gpu_devices = tf.config.experimental.list_physical_devices(\'GPU\')\n        except AttributeError:\n            from tensorflow.python.client import device_lib\n            local_device_protos = device_lib.list_local_devices()\n            # Note this will initialize all GPUs and therefore has side effect\n            # https://github.com/tensorflow/tensorflow/issues/8136\n            gpu_devices = [x.name for x in local_device_protos if x.device_type == \'GPU\']\n        return len(gpu_devices)\n\n\nget_nr_gpu = get_num_gpu\n'"
tensorpack/utils/loadcaffe.py,0,"b'# -*- coding: utf-8 -*-\n# File: loadcaffe.py\n\n\nimport numpy as np\nimport os\nimport sys\n\nfrom . import logger\nfrom .concurrency import subproc_call\nfrom .fs import download, get_dataset_path\nfrom .utils import change_env\n\n__all__ = [\'load_caffe\', \'get_caffe_pb\']\n\nCAFFE_PROTO_URL = ""https://github.com/BVLC/caffe/raw/master/src/caffe/proto/caffe.proto""\n\n\nclass CaffeLayerProcessor(object):\n\n    def __init__(self, net):\n        self.net = net\n        self.layer_names = net._layer_names\n        self.param_dict = {}\n        self.processors = {\n            \'Convolution\': self.proc_conv,\n            \'InnerProduct\': self.proc_fc,\n            \'BatchNorm\': self.proc_bn,\n            \'Scale\': self.proc_scale\n        }\n\n    def process(self):\n        for idx, layer in enumerate(self.net.layers):\n            param = layer.blobs\n            name = self.layer_names[idx]\n            if layer.type in self.processors:\n                logger.info(""Processing layer {} of type {}"".format(\n                    name, layer.type))\n                dic = self.processors[layer.type](idx, name, param)\n                self.param_dict.update(dic)\n            elif len(layer.blobs) != 0:\n                logger.warn(\n                    ""{} layer contains parameters but is not supported!"".format(layer.type))\n        return self.param_dict\n\n    def proc_conv(self, idx, name, param):\n        assert len(param) <= 2\n        assert param[0].data.ndim == 4\n        # caffe: ch_out, ch_in, h, w\n        W = param[0].data.transpose(2, 3, 1, 0)\n        if len(param) == 1:\n            return {name + \'/W\': W}\n        else:\n            return {name + \'/W\': W,\n                    name + \'/b\': param[1].data}\n\n    def proc_fc(self, idx, name, param):\n        # TODO caffe has an \'transpose\' option for fc/W\n        assert len(param) == 2\n        prev_layer_name = self.net.bottom_names[name][0]\n        prev_layer_output = self.net.blobs[prev_layer_name].data\n        if prev_layer_output.ndim == 4:\n            logger.info(""FC layer {} takes spatial data."".format(name))\n            W = param[0].data\n            # original: outx(CxHxW)\n            W = W.reshape((-1,) + prev_layer_output.shape[1:]).transpose(2, 3, 1, 0)\n            # become: (HxWxC)xout\n        else:\n            W = param[0].data.transpose()\n        return {name + \'/W\': W,\n                name + \'/b\': param[1].data}\n\n    def proc_bn(self, idx, name, param):\n        scale_factor = param[2].data[0]\n        return {name + \'/mean/EMA\': param[0].data / scale_factor,\n                name + \'/variance/EMA\': param[1].data / scale_factor}\n\n    def proc_scale(self, idx, name, param):\n        bottom_name = self.net.bottom_names[name][0]\n        # find the bn layer before this scaling\n        for i, layer in enumerate(self.net.layers):\n            if layer.type == \'BatchNorm\':\n                name2 = self.layer_names[i]\n                bottom_name2 = self.net.bottom_names[name2][0]\n                if bottom_name2 == bottom_name:\n                    # scaling and BN share the same bottom, should merge\n                    logger.info(""Merge {} and {} into one BatchNorm layer"".format(\n                        name, name2))\n                    return {name2 + \'/beta\': param[1].data,\n                            name2 + \'/gamma\': param[0].data}\n        # assume this scaling layer is part of some BN\n        logger.error(""Could not find a BN layer corresponding to this Scale layer!"")\n        raise ValueError()\n\n\ndef load_caffe(model_desc, model_file):\n    """"""\n    Load a caffe model. You must be able to ``import caffe`` to use this\n    function.\n\n    Args:\n        model_desc (str): path to caffe model description file (.prototxt).\n        model_file (str): path to caffe model parameter file (.caffemodel).\n    Returns:\n        dict: the parameters.\n    """"""\n    with change_env(\'GLOG_minloglevel\', \'2\'):\n        import caffe\n        caffe.set_mode_cpu()\n        net = caffe.Net(model_desc, model_file, caffe.TEST)\n    param_dict = CaffeLayerProcessor(net).process()\n    logger.info(""Model loaded from caffe. Params: "" +\n                "", "".join(sorted(param_dict.keys())))\n    return param_dict\n\n\ndef get_caffe_pb():\n    """"""\n    Get caffe protobuf.\n\n    Returns:\n        The imported caffe protobuf module.\n    """"""\n    dir = get_dataset_path(\'caffe\')\n    caffe_pb_file = os.path.join(dir, \'caffe_pb2.py\')\n    if not os.path.isfile(caffe_pb_file):\n        download(CAFFE_PROTO_URL, dir)\n        assert os.path.isfile(os.path.join(dir, \'caffe.proto\'))\n\n        cmd = ""protoc --version""\n        version, ret = subproc_call(cmd, timeout=3)\n        if ret != 0:\n            sys.exit(1)\n        try:\n            version = version.decode(\'utf-8\')\n            version = float(\'.\'.join(version.split(\' \')[1].split(\'.\')[:2]))\n            assert version >= 2.7, ""Require protoc>=2.7 for Python3""\n        except Exception:\n            logger.exception(""protoc --version gives: "" + str(version))\n            raise\n\n        cmd = \'cd {} && protoc caffe.proto --python_out .\'.format(dir)\n        ret = os.system(cmd)\n        assert ret == 0, \\\n            ""Command `{}` failed!"".format(cmd)\n        assert os.path.isfile(caffe_pb_file), caffe_pb_file\n    import imp\n    return imp.load_source(\'caffepb\', caffe_pb_file)\n\n\nif __name__ == \'__main__\':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'model\', help=\'.prototxt file\')\n    parser.add_argument(\'weights\', help=\'.caffemodel file\')\n    parser.add_argument(\'output\', help=\'output npz file\')\n    args = parser.parse_args()\n    ret = load_caffe(args.model, args.weights)\n\n    if args.output.endswith(\'.npz\'):\n        np.savez_compressed(args.output, **ret)\n    elif args.output.endswith(\'.npy\'):\n        logger.warn(""Please use npz format instead!"")\n        np.save(args.output, ret)\n    else:\n        raise ValueError(""Unknown format {}"".format(args.output))\n'"
tensorpack/utils/logger.py,0,"b'# -*- coding: utf-8 -*-\n# File: logger.py\n\n""""""\nThe logger module itself has the common logging functions of Python\'s\n:class:`logging.Logger`. For example:\n\n.. code-block:: python\n\n    from tensorpack.utils import logger\n    logger.set_logger_dir(\'train_log/test\')\n    logger.info(""Test"")\n    logger.error(""Error happened!"")\n""""""\n\n\nimport logging\nimport os\nimport os.path\nimport shutil\nimport sys\nfrom datetime import datetime\nfrom six.moves import input\nfrom termcolor import colored\n\n__all__ = [\'set_logger_dir\', \'auto_set_dir\', \'get_logger_dir\']\n\n\nclass _MyFormatter(logging.Formatter):\n    def format(self, record):\n        date = colored(\'[%(asctime)s @%(filename)s:%(lineno)d]\', \'green\')\n        msg = \'%(message)s\'\n        if record.levelno == logging.WARNING:\n            fmt = date + \' \' + colored(\'WRN\', \'red\', attrs=[\'blink\']) + \' \' + msg\n        elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:\n            fmt = date + \' \' + colored(\'ERR\', \'red\', attrs=[\'blink\', \'underline\']) + \' \' + msg\n        elif record.levelno == logging.DEBUG:\n            fmt = date + \' \' + colored(\'DBG\', \'yellow\', attrs=[\'blink\']) + \' \' + msg\n        else:\n            fmt = date + \' \' + msg\n        if hasattr(self, \'_style\'):\n            # Python3 compatibility\n            self._style._fmt = fmt\n        self._fmt = fmt\n        return super(_MyFormatter, self).format(record)\n\n\ndef _getlogger():\n    # this file is synced to ""dataflow"" package as well\n    package_name = ""dataflow"" if __name__.startswith(""dataflow"") else ""tensorpack""\n    logger = logging.getLogger(package_name)\n    logger.propagate = False\n    logger.setLevel(logging.INFO)\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(_MyFormatter(datefmt=\'%m%d %H:%M:%S\'))\n    logger.addHandler(handler)\n    return logger\n\n\n_logger = _getlogger()\n_LOGGING_METHOD = [\'info\', \'warning\', \'error\', \'critical\', \'exception\', \'debug\', \'setLevel\', \'addFilter\']\n# export logger functions\nfor func in _LOGGING_METHOD:\n    locals()[func] = getattr(_logger, func)\n    __all__.append(func)\n# \'warn\' is deprecated in logging module\nwarn = _logger.warning\n__all__.append(\'warn\')\n\n\ndef _get_time_str():\n    return datetime.now().strftime(\'%m%d-%H%M%S\')\n\n\n# globals: logger file and directory:\nLOG_DIR = None\n_FILE_HANDLER = None\n\n\ndef _set_file(path):\n    global _FILE_HANDLER\n    if os.path.isfile(path):\n        backup_name = path + \'.\' + _get_time_str()\n        shutil.move(path, backup_name)\n        _logger.info(""Existing log file \'{}\' backuped to \'{}\'"".format(path, backup_name))  # noqa: F821\n    hdl = logging.FileHandler(\n        filename=path, encoding=\'utf-8\', mode=\'w\')\n    hdl.setFormatter(_MyFormatter(datefmt=\'%m%d %H:%M:%S\'))\n\n    _FILE_HANDLER = hdl\n    _logger.addHandler(hdl)\n    _logger.info(""Argv: "" + \' \'.join(sys.argv))\n\n\ndef set_logger_dir(dirname, action=None):\n    """"""\n    Set the directory for global logging.\n\n    Args:\n        dirname(str): log directory\n        action(str): an action of [""k"",""d"",""q""] to be performed\n            when the directory exists. Will ask user by default.\n\n                ""d"": delete the directory. Note that the deletion may fail when\n                the directory is used by tensorboard.\n\n                ""k"": keep the directory. This is useful when you resume from a\n                previous training and want the directory to look as if the\n                training was not interrupted.\n                Note that this option does not load old models or any other\n                old states for you. It simply does nothing.\n\n    """"""\n    dirname = os.path.normpath(dirname)\n    global LOG_DIR, _FILE_HANDLER\n    if _FILE_HANDLER:\n        # unload and close the old file handler, so that we may safely delete the logger directory\n        _logger.removeHandler(_FILE_HANDLER)\n        del _FILE_HANDLER\n\n    def dir_nonempty(dirname):\n        # If directory exists and nonempty (ignore hidden files), prompt for action\n        return os.path.isdir(dirname) and len([x for x in os.listdir(dirname) if x[0] != \'.\'])\n\n    if dir_nonempty(dirname):\n        if not action:\n            _logger.warning(""""""\\\nLog directory {} exists! Use \'d\' to delete it. """""".format(dirname))\n            _logger.warning(""""""\\\nIf you\'re resuming from a previous run, you can choose to keep it.\nPress any other key to exit. """""")\n        while not action:\n            action = input(""Select Action: k (keep) / d (delete) / q (quit):"").lower().strip()\n        act = action\n        if act == \'b\':\n            backup_name = dirname + _get_time_str()\n            shutil.move(dirname, backup_name)\n            info(""Directory \'{}\' backuped to \'{}\'"".format(dirname, backup_name))  # noqa: F821\n        elif act == \'d\':\n            shutil.rmtree(dirname, ignore_errors=True)\n            if dir_nonempty(dirname):\n                shutil.rmtree(dirname, ignore_errors=False)\n        elif act == \'n\':\n            dirname = dirname + _get_time_str()\n            info(""Use a new log directory {}"".format(dirname))  # noqa: F821\n        elif act == \'k\':\n            pass\n        else:\n            raise OSError(""Directory {} exits!"".format(dirname))\n    LOG_DIR = dirname\n    from .fs import mkdir_p\n    mkdir_p(dirname)\n    _set_file(os.path.join(dirname, \'log.log\'))\n\n\ndef auto_set_dir(action=None, name=None):\n    """"""\n    Use :func:`logger.set_logger_dir` to set log directory to\n    ""./train_log/{scriptname}:{name}"". ""scriptname"" is the name of the main python file currently running""""""\n    mod = sys.modules[\'__main__\']\n    basename = os.path.basename(mod.__file__)\n    auto_dirname = os.path.join(\'train_log\', basename[:basename.rfind(\'.\')])\n    if name:\n        auto_dirname += \'_%s\' % name if os.name == \'nt\' else \':%s\' % name\n    set_logger_dir(auto_dirname, action=action)\n\n\ndef get_logger_dir():\n    """"""\n    Returns:\n        The logger directory, or None if not set.\n        The directory is used for general logging, tensorboard events, checkpoints, etc.\n    """"""\n    return LOG_DIR\n'"
tensorpack/utils/naming.py,0,"b""# -*- coding: utf-8 -*-\n# File: naming.py\n\n\nGLOBAL_STEP_INCR_OP_NAME = 'global_step_incr'\n\n# extra variables to summarize during training in a moving-average way\nMOVING_SUMMARY_OPS_KEY = 'MOVING_SUMMARY_OPS'\n"""
tensorpack/utils/nvml.py,0,"b'# -*- coding: utf-8 -*-\n# File: nvml.py\n\nimport threading\nfrom ctypes import (\n    CDLL, POINTER, Structure, byref, c_uint,\n    c_ulonglong, create_string_buffer)\n\n__all__ = [\'NVMLContext\']\n\n\nNVML_ERROR_FUNCTION_NOT_FOUND = 13\n\n\nNvmlErrorCodes = {""0"": ""NVML_SUCCESS"",\n                  ""1"": ""NVML_ERROR_UNINITIALIZED"",\n                  ""2"": ""NVML_ERROR_INVALID_ARGUMENT"",\n                  ""3"": ""NVML_ERROR_NOT_SUPPORTED"",\n                  ""4"": ""NVML_ERROR_NO_PERMISSION"",\n                  ""5"": ""NVML_ERROR_ALREADY_INITIALIZED"",\n                  ""6"": ""NVML_ERROR_NOT_FOUND"",\n                  ""7"": ""NVML_ERROR_INSUFFICIENT_SIZE"",\n                  ""8"": ""NVML_ERROR_INSUFFICIENT_POWER"",\n                  ""9"": ""NVML_ERROR_DRIVER_NOT_LOADED"",\n                  ""10"": ""NVML_ERROR_TIMEOUT"",\n                  ""11"": ""NVML_ERROR_IRQ_ISSUE"",\n                  ""12"": ""NVML_ERROR_LIBRARY_NOT_FOUND"",\n                  ""13"": ""NVML_ERROR_FUNCTION_NOT_FOUND"",\n                  ""14"": ""NVML_ERROR_CORRUPTED_INFOROM"",\n                  ""15"": ""NVML_ERROR_GPU_IS_LOST"",\n                  ""16"": ""NVML_ERROR_RESET_REQUIRED"",\n                  ""17"": ""NVML_ERROR_OPERATING_SYSTEM"",\n                  ""18"": ""NVML_ERROR_LIB_RM_VERSION_MISMATCH"",\n                  ""999"": ""NVML_ERROR_UNKNOWN""}\n\n\nclass NvmlException(Exception):\n    def __init__(self, error_code):\n        super(NvmlException, self).__init__(error_code)\n        self.error_code = error_code\n\n    def __str__(self):\n        return NvmlErrorCodes[str(self.error_code)]\n\n\ndef _check_return(ret):\n    if (ret != 0):\n        raise NvmlException(ret)\n    return ret\n\n\nclass NVML(object):\n    """"""\n    Loader for libnvidia-ml.so\n    """"""\n\n    _nvmlLib = None\n    _lib_lock = threading.Lock()\n\n    def load(self):\n        with self._lib_lock:\n            if self._nvmlLib is None:\n                self._nvmlLib = CDLL(""libnvidia-ml.so.1"")\n\n                function_pointers = [""nvmlDeviceGetName"", ""nvmlDeviceGetUUID"", ""nvmlDeviceGetMemoryInfo"",\n                                     ""nvmlDeviceGetUtilizationRates"", ""nvmlInit_v2"", ""nvmlShutdown"",\n                                     ""nvmlDeviceGetCount_v2"", ""nvmlDeviceGetHandleByIndex_v2""]\n\n                self.func_ptr = {n: self._function_pointer(n) for n in function_pointers}\n\n    def _function_pointer(self, name):\n        try:\n            return getattr(self._nvmlLib, name)\n        except AttributeError:\n            raise NvmlException(NVML_ERROR_FUNCTION_NOT_FOUND)\n\n    def get_function(self, name):\n        if name in self.func_ptr.keys():\n            return self.func_ptr[name]\n\n\n_NVML = NVML()\n\n\nclass NvidiaDevice(object):\n    """"""Represent a single GPUDevice""""""\n\n    def __init__(self, hnd):\n        super(NvidiaDevice, self).__init__()\n        self.hnd = hnd\n\n    def memory(self):\n        """"""Memory information in bytes\n\n        Example:\n\n            >>> print(ctx.device(0).memory())\n            {\'total\': 4238016512L, \'used\': 434831360L, \'free\': 3803185152L}\n\n        Returns:\n            total/used/free memory in bytes\n        """"""\n        class GpuMemoryInfo(Structure):\n            _fields_ = [\n                (\'total\', c_ulonglong),\n                (\'free\', c_ulonglong),\n                (\'used\', c_ulonglong),\n            ]\n\n        c_memory = GpuMemoryInfo()\n        _check_return(_NVML.get_function(\n            ""nvmlDeviceGetMemoryInfo"")(self.hnd, byref(c_memory)))\n        return {\'total\': c_memory.total, \'free\': c_memory.free, \'used\': c_memory.used}\n\n    def utilization(self):\n        """"""Percent of time over the past second was utilized.\n\n        Details:\n           Percent of time over the past second during which one or more kernels was executing on the GPU.\n           Percent of time over the past second during which global (device) memory was being read or written\n\n        Example:\n\n            >>> print(ctx.device(0).utilization())\n            {\'gpu\': 4L, \'memory\': 6L}\n\n        """"""\n        class GpuUtilizationInfo(Structure):\n\n            _fields_ = [\n                (\'gpu\', c_uint),\n                (\'memory\', c_uint),\n            ]\n\n        c_util = GpuUtilizationInfo()\n        _check_return(_NVML.get_function(\n            ""nvmlDeviceGetUtilizationRates"")(self.hnd, byref(c_util)))\n        return {\'gpu\': c_util.gpu, \'memory\': c_util.memory}\n\n    def name(self):\n        buflen = 1024\n        buf = create_string_buffer(buflen)\n        fn = _NVML.get_function(""nvmlDeviceGetName"")\n        ret = fn(self.hnd, buf, c_uint(1024))\n        _check_return(ret)\n        return buf.value.decode(\'utf-8\')\n\n\nclass NVMLContext(object):\n    """"""Creates a context to query information\n\n    Example:\n\n        with NVMLContext() as ctx:\n            num_gpus = ctx.num_devices()\n            for device in ctx.devices():\n                print(device.memory())\n                print(device.utilization())\n\n    """"""\n    def __enter__(self):\n        """"""Create a new context """"""\n        _NVML.load()\n        _check_return(_NVML.get_function(""nvmlInit_v2"")())\n        return self\n\n    def __exit__(self, type, value, tb):\n        """"""Destroy current context""""""\n        _check_return(_NVML.get_function(""nvmlShutdown"")())\n\n    def num_devices(self):\n        """"""Get number of devices """"""\n        c_count = c_uint()\n        _check_return(_NVML.get_function(\n            ""nvmlDeviceGetCount_v2"")(byref(c_count)))\n        return c_count.value\n\n    def devices(self):\n        """"""\n        Returns:\n            [NvidiaDevice]: a list of devices\n        """"""\n        return [self.device(i) for i in range(self.num_devices())]\n\n    def device(self, idx):\n        """"""Get a specific GPU device\n\n        Args:\n            idx: index of device\n\n        Returns:\n            NvidiaDevice: single GPU device\n        """"""\n\n        class GpuDevice(Structure):\n            pass\n\n        c_nvmlDevice_t = POINTER(GpuDevice)\n\n        c_index = c_uint(idx)\n        device = c_nvmlDevice_t()\n        _check_return(_NVML.get_function(\n            ""nvmlDeviceGetHandleByIndex_v2"")(c_index, byref(device)))\n        return NvidiaDevice(device)\n\n\nif __name__ == \'__main__\':\n    with NVMLContext() as ctx:\n        for idx, dev in enumerate(ctx.devices()):\n            print(idx, dev.name())\n\n    with NVMLContext() as ctx:\n        print(ctx.devices())\n        print(ctx.devices()[0].utilization())\n'"
tensorpack/utils/palette.py,0,"b'# -*- coding: utf-8 -*-\n# File: palette.py\n\nimport numpy as np\n\n__all__ = [\'PALETTE_RGB\']\n\n# Copied from https://stackoverflow.com/questions/2328339/how-to-generate-n-different-colors-for-any-natural-number-n\nPALETTE_HEX = [\n    ""#000000"", ""#FFFF00"", ""#1CE6FF"", ""#FF34FF"", ""#FF4A46"", ""#008941"", ""#006FA6"", ""#A30059"",\n    ""#FFDBE5"", ""#7A4900"", ""#0000A6"", ""#63FFAC"", ""#B79762"", ""#004D43"", ""#8FB0FF"", ""#997D87"",\n    ""#5A0007"", ""#809693"", ""#FEFFE6"", ""#1B4400"", ""#4FC601"", ""#3B5DFF"", ""#4A3B53"", ""#FF2F80"",\n    ""#61615A"", ""#BA0900"", ""#6B7900"", ""#00C2A0"", ""#FFAA92"", ""#FF90C9"", ""#B903AA"", ""#D16100"",\n    ""#DDEFFF"", ""#000035"", ""#7B4F4B"", ""#A1C299"", ""#300018"", ""#0AA6D8"", ""#013349"", ""#00846F"",\n    ""#372101"", ""#FFB500"", ""#C2FFED"", ""#A079BF"", ""#CC0744"", ""#C0B9B2"", ""#C2FF99"", ""#001E09"",\n    ""#00489C"", ""#6F0062"", ""#0CBD66"", ""#EEC3FF"", ""#456D75"", ""#B77B68"", ""#7A87A1"", ""#788D66"",\n    ""#885578"", ""#FAD09F"", ""#FF8A9A"", ""#D157A0"", ""#BEC459"", ""#456648"", ""#0086ED"", ""#886F4C"",\n    ""#34362D"", ""#B4A8BD"", ""#00A6AA"", ""#452C2C"", ""#636375"", ""#A3C8C9"", ""#FF913F"", ""#938A81"",\n    ""#575329"", ""#00FECF"", ""#B05B6F"", ""#8CD0FF"", ""#3B9700"", ""#04F757"", ""#C8A1A1"", ""#1E6E00"",\n    ""#7900D7"", ""#A77500"", ""#6367A9"", ""#A05837"", ""#6B002C"", ""#772600"", ""#D790FF"", ""#9B9700"",\n    ""#549E79"", ""#FFF69F"", ""#201625"", ""#72418F"", ""#BC23FF"", ""#99ADC0"", ""#3A2465"", ""#922329"",\n    ""#5B4534"", ""#FDE8DC"", ""#404E55"", ""#0089A3"", ""#CB7E98"", ""#A4E804"", ""#324E72"", ""#6A3A4C"",\n    ""#83AB58"", ""#001C1E"", ""#D1F7CE"", ""#004B28"", ""#C8D0F6"", ""#A3A489"", ""#806C66"", ""#222800"",\n    ""#BF5650"", ""#E83000"", ""#66796D"", ""#DA007C"", ""#FF1A59"", ""#8ADBB4"", ""#1E0200"", ""#5B4E51"",\n    ""#C895C5"", ""#320033"", ""#FF6832"", ""#66E1D3"", ""#CFCDAC"", ""#D0AC94"",\n    ""#7ED379"", ""#012C58""]\n\n\n# Copied from https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/colormap.py\nDETECTRON_PALETTE = np.array(\n    [\n        0.000, 0.447, 0.741,\n        0.850, 0.325, 0.098,\n        0.929, 0.694, 0.125,\n        0.494, 0.184, 0.556,\n        0.466, 0.674, 0.188,\n        0.301, 0.745, 0.933,\n        0.635, 0.078, 0.184,\n        0.300, 0.300, 0.300,\n        0.600, 0.600, 0.600,\n        1.000, 0.000, 0.000,\n        1.000, 0.500, 0.000,\n        0.749, 0.749, 0.000,\n        0.000, 1.000, 0.000,\n        0.000, 0.000, 1.000,\n        0.667, 0.000, 1.000,\n        0.333, 0.333, 0.000,\n        0.333, 0.667, 0.000,\n        0.333, 1.000, 0.000,\n        0.667, 0.333, 0.000,\n        0.667, 0.667, 0.000,\n        0.667, 1.000, 0.000,\n        1.000, 0.333, 0.000,\n        1.000, 0.667, 0.000,\n        1.000, 1.000, 0.000,\n        0.000, 0.333, 0.500,\n        0.000, 0.667, 0.500,\n        0.000, 1.000, 0.500,\n        0.333, 0.000, 0.500,\n        0.333, 0.333, 0.500,\n        0.333, 0.667, 0.500,\n        0.333, 1.000, 0.500,\n        0.667, 0.000, 0.500,\n        0.667, 0.333, 0.500,\n        0.667, 0.667, 0.500,\n        0.667, 1.000, 0.500,\n        1.000, 0.000, 0.500,\n        1.000, 0.333, 0.500,\n        1.000, 0.667, 0.500,\n        1.000, 1.000, 0.500,\n        0.000, 0.333, 1.000,\n        0.000, 0.667, 1.000,\n        0.000, 1.000, 1.000,\n        0.333, 0.000, 1.000,\n        0.333, 0.333, 1.000,\n        0.333, 0.667, 1.000,\n        0.333, 1.000, 1.000,\n        0.667, 0.000, 1.000,\n        0.667, 0.333, 1.000,\n        0.667, 0.667, 1.000,\n        0.667, 1.000, 1.000,\n        1.000, 0.000, 1.000,\n        1.000, 0.333, 1.000,\n        1.000, 0.667, 1.000,\n        0.167, 0.000, 0.000,\n        0.333, 0.000, 0.000,\n        0.500, 0.000, 0.000,\n        0.667, 0.000, 0.000,\n        0.833, 0.000, 0.000,\n        1.000, 0.000, 0.000,\n        0.000, 0.167, 0.000,\n        0.000, 0.333, 0.000,\n        0.000, 0.500, 0.000,\n        0.000, 0.667, 0.000,\n        0.000, 0.833, 0.000,\n        0.000, 1.000, 0.000,\n        0.000, 0.000, 0.167,\n        0.000, 0.000, 0.333,\n        0.000, 0.000, 0.500,\n        0.000, 0.000, 0.667,\n        0.000, 0.000, 0.833,\n        0.000, 0.000, 1.000,\n        0.000, 0.000, 0.000,\n        0.143, 0.143, 0.143,\n        0.286, 0.286, 0.286,\n        0.429, 0.429, 0.429,\n        0.571, 0.571, 0.571,\n        0.714, 0.714, 0.714,\n        0.857, 0.857, 0.857,\n        1.000, 1.000, 1.000\n    ]\n).astype(np.float32).reshape(-1, 3) * 255\n\n\ndef _parse_hex_color(s):\n    r = int(s[1:3], 16)\n    g = int(s[3:5], 16)\n    b = int(s[5:7], 16)\n    return (r, g, b)\n\n\n# PALETTE_RGB = np.asarray(\n#     list(map(_parse_hex_color, PALETTE_HEX)),\n#     dtype=\'int32\')\n\n# This seems more beautiful\nPALETTE_RGB = DETECTRON_PALETTE\n'"
tensorpack/utils/serialize.py,0,"b'# -*- coding: utf-8 -*-\n# File: serialize.py\n\nimport os\n\nimport pickle\nfrom multiprocessing.reduction import ForkingPickler\nimport msgpack\nimport msgpack_numpy\n\nmsgpack_numpy.patch()\nassert msgpack.version >= (0, 5, 2)\n\n__all__ = [\'loads\', \'dumps\']\n\n\nMAX_MSGPACK_LEN = 1000000000\n\n\nclass MsgpackSerializer(object):\n\n    @staticmethod\n    def dumps(obj):\n        """"""\n        Serialize an object.\n\n        Returns:\n            Implementation-dependent bytes-like object.\n        """"""\n        return msgpack.dumps(obj, use_bin_type=True)\n\n    @staticmethod\n    def loads(buf):\n        """"""\n        Args:\n            buf: the output of `dumps`.\n        """"""\n        # Since 0.6, the default max size was set to 1MB.\n        # We change it to approximately 1G.\n        return msgpack.loads(buf, raw=False,\n                             max_bin_len=MAX_MSGPACK_LEN,\n                             max_array_len=MAX_MSGPACK_LEN,\n                             max_map_len=MAX_MSGPACK_LEN,\n                             max_str_len=MAX_MSGPACK_LEN)\n\n\nclass PyarrowSerializer(object):\n    @staticmethod\n    def dumps(obj):\n        """"""\n        Serialize an object.\n\n        Returns:\n            Implementation-dependent bytes-like object.\n            May not be compatible across different versions of pyarrow.\n        """"""\n        import pyarrow as pa\n        return pa.serialize(obj).to_buffer()\n\n    @staticmethod\n    def dumps_bytes(obj):\n        """"""\n        Returns:\n            bytes\n        """"""\n        return PyarrowSerializer.dumps(obj).to_pybytes()\n\n    @staticmethod\n    def loads(buf):\n        """"""\n        Args:\n            buf: the output of `dumps` or `dumps_bytes`.\n        """"""\n        import pyarrow as pa\n        return pa.deserialize(buf)\n\n\nclass PickleSerializer(object):\n    @staticmethod\n    def dumps(obj):\n        """"""\n        Returns:\n            bytes\n        """"""\n        return pickle.dumps(obj, protocol=-1)\n\n    @staticmethod\n    def loads(buf):\n        """"""\n        Args:\n            bytes\n        """"""\n        return pickle.loads(buf)\n\n\n# Define the default serializer to be used that dumps data to bytes\n_DEFAULT_S = os.environ.get(\'TENSORPACK_SERIALIZE\', \'pickle\')\n\nif _DEFAULT_S == ""pyarrow"":\n    dumps = PyarrowSerializer.dumps_bytes\n    loads = PyarrowSerializer.loads\nelif _DEFAULT_S == ""pickle"":\n    dumps = PickleSerializer.dumps\n    loads = PickleSerializer.loads\nelse:\n    dumps = MsgpackSerializer.dumps\n    loads = MsgpackSerializer.loads\n\n# Define the default serializer to be used for passing data\n# among a pair of peers. In this case the deserialization is\n# known to happen only once\n_DEFAULT_S = os.environ.get(\'TENSORPACK_ONCE_SERIALIZE\', \'pickle\')\n\nif _DEFAULT_S == ""pyarrow"":\n    dumps_once = PyarrowSerializer.dumps\n    loads_once = PyarrowSerializer.loads\nelif _DEFAULT_S == ""pickle"":\n    dumps_once = ForkingPickler.dumps\n    loads_once = ForkingPickler.loads\nelse:\n    dumps_once = MsgpackSerializer.dumps\n    loads_once = MsgpackSerializer.loads\n'"
tensorpack/utils/stats.py,0,"b'# -*- coding: utf-8 -*-\n# File: stats.py\n\nimport numpy as np\n\n__all__ = [\'StatCounter\', \'BinaryStatistics\', \'RatioCounter\', \'Accuracy\',\n           \'OnlineMoments\']\n\n\nclass StatCounter(object):\n    """""" A simple counter""""""\n\n    def __init__(self):\n        self.reset()\n\n    def feed(self, v):\n        """"""\n        Args:\n            v(float or np.ndarray): has to be the same shape between calls.\n        """"""\n        self._values.append(v)\n\n    def reset(self):\n        self._values = []\n\n    @property\n    def count(self):\n        return len(self._values)\n\n    @property\n    def average(self):\n        assert len(self._values)\n        return np.mean(self._values)\n\n    @property\n    def sum(self):\n        assert len(self._values)\n        return np.sum(self._values)\n\n    @property\n    def max(self):\n        assert len(self._values)\n        return max(self._values)\n\n    @property\n    def min(self):\n        assert len(self._values)\n        return min(self._values)\n\n    def samples(self):\n        """"""\n        Returns all samples.\n        """"""\n        return self._values\n\n\nclass RatioCounter(object):\n    """""" A counter to count ratio of something. """"""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self._tot = 0\n        self._cnt = 0\n\n    def feed(self, count, total=1):\n        """"""\n        Args:\n            cnt(int): the count of some event of interest.\n            tot(int): the total number of events.\n        """"""\n        self._tot += total\n        self._cnt += count\n\n    @property\n    def ratio(self):\n        if self._tot == 0:\n            return 0\n        return self._cnt * 1.0 / self._tot\n\n    @property\n    def total(self):\n        """"""\n        Returns:\n            int: the total\n        """"""\n        return self._tot\n\n    @property\n    def count(self):\n        """"""\n        Returns:\n            int: the total\n        """"""\n        return self._cnt\n\n\nclass Accuracy(RatioCounter):\n    """""" A RatioCounter with a fancy name """"""\n    @property\n    def accuracy(self):\n        return self.ratio\n\n\nclass BinaryStatistics(object):\n    """"""\n    Statistics for binary decision,\n    including precision, recall, false positive, false negative\n    """"""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.nr_pos = 0  # positive label\n        self.nr_neg = 0  # negative label\n        self.nr_pred_pos = 0\n        self.nr_pred_neg = 0\n        self.corr_pos = 0   # correct predict positive\n        self.corr_neg = 0   # correct predict negative\n\n    def feed(self, pred, label):\n        """"""\n        Args:\n            pred (np.ndarray): binary array.\n            label (np.ndarray): binary array of the same size.\n        """"""\n        assert pred.shape == label.shape, ""{} != {}"".format(pred.shape, label.shape)\n        self.nr_pos += (label == 1).sum()\n        self.nr_neg += (label == 0).sum()\n        self.nr_pred_pos += (pred == 1).sum()\n        self.nr_pred_neg += (pred == 0).sum()\n        self.corr_pos += ((pred == 1) & (pred == label)).sum()\n        self.corr_neg += ((pred == 0) & (pred == label)).sum()\n\n    @property\n    def precision(self):\n        if self.nr_pred_pos == 0:\n            return 0\n        return self.corr_pos * 1. / self.nr_pred_pos\n\n    @property\n    def recall(self):\n        if self.nr_pos == 0:\n            return 0\n        return self.corr_pos * 1. / self.nr_pos\n\n    @property\n    def false_positive(self):\n        if self.nr_pred_pos == 0:\n            return 0\n        return 1 - self.precision\n\n    @property\n    def false_negative(self):\n        if self.nr_pos == 0:\n            return 0\n        return 1 - self.recall\n\n\nclass OnlineMoments(object):\n    """"""Compute 1st and 2nd moments online (to avoid storing all elements).\n\n    See algorithm at: https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Online_algorithm\n    """"""\n\n    def __init__(self):\n        self._mean = 0\n        self._M2 = 0\n        self._n = 0\n\n    def feed(self, x):\n        """"""\n        Args:\n            x (float or np.ndarray): must have the same shape.\n        """"""\n        self._n += 1\n        delta = x - self._mean\n        self._mean += delta * (1.0 / self._n)\n        delta2 = x - self._mean\n        self._M2 += delta * delta2\n\n    @property\n    def mean(self):\n        return self._mean\n\n    @property\n    def variance(self):\n        return self._M2 / (self._n - 1)\n\n    @property\n    def std(self):\n        return np.sqrt(self.variance)\n'"
tensorpack/utils/timer.py,0,"b'# -*- coding: utf-8 -*-\n# File: timer.py\n\n\nimport atexit\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nfrom time import perf_counter as timer  # noqa\n\nfrom . import logger\nfrom .stats import StatCounter\n\n\n__all__ = [\'timed_operation\', \'IterSpeedCounter\', \'Timer\']\n\n\n@contextmanager\ndef timed_operation(msg, log_start=False):\n    """"""\n    Surround a context with a timer.\n\n    Args:\n        msg(str): the log to print.\n        log_start(bool): whether to print also at the beginning.\n\n    Example:\n        .. code-block:: python\n\n            with timed_operation(\'Good Stuff\'):\n                time.sleep(1)\n\n        Will print:\n\n        .. code-block:: python\n\n            Good stuff finished, time:1sec.\n    """"""\n    assert len(msg)\n    if log_start:\n        logger.info(\'Start {} ...\'.format(msg))\n    start = timer()\n    yield\n    msg = msg[0].upper() + msg[1:]\n    logger.info(\'{} finished, time:{:.4f} sec.\'.format(\n        msg, timer() - start))\n\n\n_TOTAL_TIMER_DATA = defaultdict(StatCounter)\n\n\n@contextmanager\ndef total_timer(msg):\n    """""" A context which add the time spent inside to the global TotalTimer. """"""\n    start = timer()\n    yield\n    t = timer() - start\n    _TOTAL_TIMER_DATA[msg].feed(t)\n\n\ndef print_total_timer():\n    """"""\n    Print the content of the global TotalTimer, if it\'s not empty. This function will automatically get\n    called when program exits.\n    """"""\n    if len(_TOTAL_TIMER_DATA) == 0:\n        return\n    for k, v in _TOTAL_TIMER_DATA.items():\n        logger.info(""Total Time: {} -> {:.2f} sec, {} times, {:.3g} sec/time"".format(\n            k, v.sum, v.count, v.average))\n\n\natexit.register(print_total_timer)\n\n\nclass IterSpeedCounter(object):\n    """""" Test how often some code gets reached.\n\n    Example:\n        Print the speed of the iteration every 100 times.\n\n        .. code-block:: python\n\n            speed = IterSpeedCounter(100)\n            for k in range(1000):\n                # do something\n                speed()\n    """"""\n\n    def __init__(self, print_every, name=None):\n        """"""\n        Args:\n            print_every(int): interval to print.\n            name(str): name to used when print.\n        """"""\n        self.cnt = 0\n        self.print_every = int(print_every)\n        self.name = name if name else \'IterSpeed\'\n\n    def reset(self):\n        self.start = timer()\n\n    def __call__(self):\n        if self.cnt == 0:\n            self.reset()\n        self.cnt += 1\n        if self.cnt % self.print_every != 0:\n            return\n        t = timer() - self.start\n        logger.info(""{}: {:.2f} sec, {} times, {:.3g} sec/time"".format(\n            self.name, t, self.cnt, t / self.cnt))\n\n\nclass Timer():\n    """"""\n    A timer class which computes the time elapsed since the start/reset of the timer.\n    """"""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        """"""\n        Reset the timer.\n        """"""\n        self._start = timer()\n        self._paused = False\n        self._total_paused = 0\n\n    def pause(self):\n        """"""\n        Pause the timer.\n        """"""\n        assert self._paused is False\n        self._paused = timer()\n\n    def is_paused(self):\n        return self._paused is not False\n\n    def resume(self):\n        """"""\n        Resume the timer.\n        """"""\n        assert self._paused is not False\n        self._total_paused += timer() - self._paused\n        self._paused = False\n\n    def seconds(self):\n        """"""\n        Returns:\n            float: the total number of seconds since the start/reset of the timer, excluding the\n                time in between when the timer is paused.\n        """"""\n        if self._paused:\n            self.resume()\n            self.pause()\n        return timer() - self._start - self._total_paused\n'"
tensorpack/utils/utils.py,0,"b'# -*- coding: utf-8 -*-\n# File: utils.py\n\n\nimport inspect\nimport numpy as np\nimport re\nimport os\nimport sys\nfrom contextlib import contextmanager\nfrom datetime import datetime, timedelta\nfrom tqdm import tqdm\n\nfrom . import logger\nfrom .concurrency import subproc_call\n\n__all__ = [\'change_env\',\n           \'get_rng\',\n           \'fix_rng_seed\',\n           \'get_tqdm\',\n           \'execute_only_once\',\n           \'humanize_time_delta\'\n           ]\n\n\ndef humanize_time_delta(sec):\n    """"""Humanize timedelta given in seconds\n\n    Args:\n        sec (float): time difference in seconds. Must be positive.\n\n    Returns:\n        str - time difference as a readable string\n\n    Example:\n\n    .. code-block:: python\n\n        print(humanize_time_delta(1))                                   # 1 second\n        print(humanize_time_delta(60 + 1))                              # 1 minute 1 second\n        print(humanize_time_delta(87.6))                                # 1 minute 27 seconds\n        print(humanize_time_delta(0.01))                                # 0.01 seconds\n        print(humanize_time_delta(60 * 60 + 1))                         # 1 hour 1 second\n        print(humanize_time_delta(60 * 60 * 24 + 1))                    # 1 day 1 second\n        print(humanize_time_delta(60 * 60 * 24 + 60 * 2 + 60*60*9 + 3)) # 1 day 9 hours 2 minutes 3 seconds\n    """"""\n    if sec < 0:\n        logger.warn(""humanize_time_delta() obtains negative seconds!"")\n        return ""{:.3g} seconds"".format(sec)\n    if sec == 0:\n        return ""0 second""\n    time = datetime(2000, 1, 1) + timedelta(seconds=int(sec))\n    units = [\'day\', \'hour\', \'minute\', \'second\']\n    vals = [int(sec // 86400), time.hour, time.minute, time.second]\n    if sec < 60:\n        vals[-1] = sec\n\n    def _format(v, u):\n        return ""{:.3g} {}{}"".format(v, u, ""s"" if v > 1 else """")\n\n    ans = []\n    for v, u in zip(vals, units):\n        if v > 0:\n            ans.append(_format(v, u))\n    return "" "".join(ans)\n\n\n@contextmanager\ndef change_env(name, val):\n    """"""\n    Args:\n        name(str): name of the env var\n        val(str or None): the value, or set to None to clear the env var.\n\n    Returns:\n        a context where the environment variable ``name`` being set to\n        ``val``. It will be set back after the context exits.\n    """"""\n    oldval = os.environ.get(name, None)\n\n    if val is None:\n        try:\n            del os.environ[name]\n        except KeyError:\n            pass\n    else:\n        os.environ[name] = val\n\n    yield\n\n    if oldval is None:\n        try:\n            del os.environ[name]\n        except KeyError:\n            pass\n    else:\n        os.environ[name] = oldval\n\n\n_RNG_SEED = None\n\n\ndef fix_rng_seed(seed):\n    """"""\n    Call this function at the beginning of program to fix rng seed within tensorpack.\n\n    Args:\n        seed (int):\n\n    Note:\n        See https://github.com/tensorpack/tensorpack/issues/196.\n\n    Example:\n\n        Fix random seed in both tensorpack and tensorflow.\n\n    .. code-block:: python\n\n            seed = 42\n            utils.fix_rng_seed(seed)\n            tesnorflow.set_random_seed(seed)\n            # run trainer\n    """"""\n    global _RNG_SEED\n    _RNG_SEED = int(seed)\n\n\ndef get_rng(obj=None):\n    """"""\n    Get a good RNG seeded with time, pid and the object.\n\n    Args:\n        obj: some object to use to generate random seed.\n    Returns:\n        np.random.RandomState: the RNG.\n    """"""\n    seed = (id(obj) + os.getpid() +\n            int(datetime.now().strftime(""%Y%m%d%H%M%S%f""))) % 4294967295\n    if _RNG_SEED is not None:\n        seed = _RNG_SEED\n    return np.random.RandomState(seed)\n\n\n_EXECUTE_HISTORY = set()\n\n\ndef execute_only_once():\n    """"""\n    Each called in the code to this function is guaranteed to return True the\n    first time and False afterwards.\n\n    Returns:\n        bool: whether this is the first time this function gets called from this line of code.\n\n    Example:\n        .. code-block:: python\n\n            if execute_only_once():\n                # do something only once\n    """"""\n    f = inspect.currentframe().f_back\n    ident = (f.f_code.co_filename, f.f_lineno)\n    if ident in _EXECUTE_HISTORY:\n        return False\n    _EXECUTE_HISTORY.add(ident)\n    return True\n\n\ndef _pick_tqdm_interval(file):\n    # Heuristics to pick a update interval for progress bar that\'s nice-looking for users.\n    isatty = file.isatty()\n    # Jupyter notebook should be recognized as tty.\n    # Wait for https://github.com/ipython/ipykernel/issues/268\n    try:\n        from ipykernel import iostream\n        if isinstance(file, iostream.OutStream):\n            isatty = True\n    except ImportError:\n        pass\n\n    if isatty:\n        return 0.5\n    else:\n        # When run under mpirun/slurm, isatty is always False.\n        # Here we apply some hacky heuristics for slurm.\n        if \'SLURM_JOB_ID\' in os.environ:\n            if int(os.environ.get(\'SLURM_JOB_NUM_NODES\', 1)) > 1:\n                # multi-machine job, probably not interactive\n                return 60\n            else:\n                # possibly interactive, so let\'s be conservative\n                return 15\n\n        if \'OMPI_COMM_WORLD_SIZE\' in os.environ:\n            return 60\n\n        # If not a tty, don\'t refresh progress bar that often\n        return 180\n\n\ndef get_tqdm_kwargs(**kwargs):\n    """"""\n    Return default arguments to be used with tqdm.\n\n    Args:\n        kwargs: extra arguments to be used.\n    Returns:\n        dict:\n    """"""\n    default = dict(\n        smoothing=0.5,\n        dynamic_ncols=True,\n        ascii=True,\n        bar_format=\'{l_bar}{bar}|{n_fmt}/{total_fmt}[{elapsed}<{remaining},{rate_noinv_fmt}]\'\n    )\n\n    try:\n        # Use this env var to override the refresh interval setting\n        interval = float(os.environ[\'TENSORPACK_PROGRESS_REFRESH\'])\n    except KeyError:\n        interval = _pick_tqdm_interval(kwargs.get(\'file\', sys.stderr))\n\n    default[\'mininterval\'] = interval\n    default.update(kwargs)\n    return default\n\n\ndef get_tqdm(*args, **kwargs):\n    """""" Similar to :func:`tqdm.tqdm()`,\n    but use tensorpack\'s default options to have consistent style. """"""\n    return tqdm(*args, **get_tqdm_kwargs(**kwargs))\n\n\ndef find_library_full_path(name):\n    """"""\n    Similar to `from ctypes.util import find_library`, but try\n    to return full path if possible.\n    """"""\n    from ctypes.util import find_library\n\n    if os.name == ""posix"" and sys.platform == ""darwin"":\n        # on Mac, ctypes already returns full path\n        return find_library(name)\n\n    def _use_proc_maps(name):\n        """"""\n        Find so from /proc/pid/maps\n        Only works with libraries that has already been loaded.\n        But this is the most accurate method -- it finds the exact library that\'s being used.\n        """"""\n        procmap = os.path.join(\'/proc\', str(os.getpid()), \'maps\')\n        if not os.path.isfile(procmap):\n            return None\n        try:\n            with open(procmap, \'r\') as f:\n                for line in f:\n                    line = line.strip().split(\' \')\n                    sofile = line[-1]\n\n                    basename = os.path.basename(sofile)\n                    if \'lib\' + name + \'.so\' in basename:\n                        if os.path.isfile(sofile):\n                            return os.path.realpath(sofile)\n        except IOError:\n            # can fail in certain environment (e.g. chroot)\n            # if the pids are incorrectly mapped\n            pass\n\n    # The following two methods come from https://github.com/python/cpython/blob/master/Lib/ctypes/util.py\n    def _use_ld(name):\n        """"""\n        Find so with `ld -lname -Lpath`.\n        It will search for files in LD_LIBRARY_PATH, but not in ldconfig.\n        """"""\n        cmd = ""ld -t -l{} -o {}"".format(name, os.devnull)\n        ld_lib_path = os.environ.get(\'LD_LIBRARY_PATH\', \'\')\n        for d in ld_lib_path.split(\':\'):\n            cmd = cmd + "" -L "" + d\n        result, ret = subproc_call(cmd + \'|| true\')\n        expr = r\'[^\\(\\)\\s]*lib%s\\.[^\\(\\)\\s]*\' % re.escape(name)\n        res = re.search(expr, result.decode(\'utf-8\'))\n        if res:\n            res = res.group(0)\n            if not os.path.isfile(res):\n                return None\n            return os.path.realpath(res)\n\n    def _use_ldconfig(name):\n        """"""\n        Find so in `ldconfig -p`.\n        It does not handle LD_LIBRARY_PATH.\n        """"""\n        with change_env(\'LC_ALL\', \'C\'), change_env(\'LANG\', \'C\'):\n            ldconfig, ret = subproc_call(""ldconfig -p"")\n            ldconfig = ldconfig.decode(\'utf-8\')\n            if ret != 0:\n                return None\n        expr = r\'\\s+(lib%s\\.[^\\s]+)\\s+\\(.*=>\\s+(.*)\' % (re.escape(name))\n        res = re.search(expr, ldconfig)\n        if not res:\n            return None\n        else:\n            ret = res.group(2)\n            return os.path.realpath(ret)\n\n    if sys.platform.startswith(\'linux\'):\n        return _use_proc_maps(name) or _use_ld(name) or _use_ldconfig(name) or find_library(name)\n\n    return find_library(name)  # don\'t know what to do\n'"
tensorpack/utils/viz.py,0,"b'# -*- coding: utf-8 -*-\n# File: viz.py\n# Credit: zxytim\n\nimport numpy as np\nimport os\nimport sys\n\nfrom ..utils.develop import create_dummy_func  # noqa\nfrom .argtools import shape2d\nfrom .fs import mkdir_p\n\ntry:\n    import cv2\nexcept ImportError:\n    pass\n\n\n__all__ = [\'interactive_imshow\',\n           \'stack_patches\', \'gen_stack_patches\',\n           \'dump_dataflow_images\', \'intensity_to_rgb\',\n           \'draw_boxes\']\n\n\ndef interactive_imshow(img, lclick_cb=None, rclick_cb=None, **kwargs):\n    """"""\n    Args:\n        img (np.ndarray): an image (expect BGR) to show.\n        lclick_cb, rclick_cb: a callback ``func(img, x, y)`` for left/right click event.\n        kwargs: can be {key_cb_a: callback_img, key_cb_b: callback_img}, to\n            specify a callback ``func(img)`` for keypress.\n\n    Some existing keypress event handler:\n\n    * q: destroy the current window\n    * x: execute ``sys.exit()``\n    * s: save image to ""out.png""\n    """"""\n    name = \'tensorpack_viz_window\'\n    cv2.imshow(name, img)\n\n    def mouse_cb(event, x, y, *args):\n        if event == cv2.EVENT_LBUTTONUP and lclick_cb is not None:\n            lclick_cb(img, x, y)\n        elif event == cv2.EVENT_RBUTTONUP and rclick_cb is not None:\n            rclick_cb(img, x, y)\n    cv2.setMouseCallback(name, mouse_cb)\n    key = cv2.waitKey(-1)\n    while key >= 128:\n        key = cv2.waitKey(-1)\n    key = chr(key & 0xff)\n    cb_name = \'key_cb_\' + key\n    if cb_name in kwargs:\n        kwargs[cb_name](img)\n    elif key == \'q\':\n        cv2.destroyWindow(name)\n    elif key == \'x\':\n        sys.exit()\n    elif key == \'s\':\n        cv2.imwrite(\'out.png\', img)\n    elif key in [\'+\', \'=\']:\n        img = cv2.resize(img, None, fx=1.3, fy=1.3, interpolation=cv2.INTER_CUBIC)\n        interactive_imshow(img, lclick_cb, rclick_cb, **kwargs)\n    elif key == \'-\':\n        img = cv2.resize(img, None, fx=0.7, fy=0.7, interpolation=cv2.INTER_CUBIC)\n        interactive_imshow(img, lclick_cb, rclick_cb, **kwargs)\n\n\ndef _preprocess_patch_list(plist):\n    plist = np.asarray(plist)\n    assert plist.dtype != np.object\n    if plist.ndim == 3:\n        plist = plist[:, :, :, np.newaxis]\n    assert plist.ndim == 4 and plist.shape[3] in [1, 3], plist.shape\n    return plist\n\n\ndef _pad_patch_list(plist, bgcolor):\n    if isinstance(bgcolor, int):\n        bgcolor = (bgcolor, bgcolor, bgcolor)\n\n    def _pad_channel(plist):\n        ret = []\n        for p in plist:\n            if len(p.shape) == 2:\n                p = p[:, :, np.newaxis]\n            if p.shape[2] == 1:\n                p = np.repeat(p, 3, 2)\n            ret.append(p)\n        return ret\n\n    plist = _pad_channel(plist)\n    shapes = [x.shape for x in plist]\n    ph = max(s[0] for s in shapes)\n    pw = max(s[1] for s in shapes)\n\n    ret = np.zeros((len(plist), ph, pw, 3), dtype=plist[0].dtype)\n    ret[:, :, :] = bgcolor\n    for idx, p in enumerate(plist):\n        s = p.shape\n        sh = (ph - s[0]) // 2\n        sw = (pw - s[1]) // 2\n        ret[idx, sh:sh + s[0], sw:sw + s[1], :] = p\n    return ret\n\n\nclass Canvas(object):\n    def __init__(self, ph, pw,\n                 nr_row, nr_col,\n                 channel, border, bgcolor):\n        self.ph = ph\n        self.pw = pw\n        self.nr_row = nr_row\n        self.nr_col = nr_col\n\n        if border is None:\n            border = int(0.05 * min(ph, pw))\n        self.border = border\n\n        if isinstance(bgcolor, int):\n            bgchannel = 1\n        else:\n            bgchannel = 3\n        self.bgcolor = bgcolor\n        self.channel = max(channel, bgchannel)\n\n        self.canvas = np.zeros((nr_row * (ph + border) - border,\n                               nr_col * (pw + border) - border,\n                               self.channel), dtype=\'uint8\')\n\n    def draw_patches(self, plist):\n        assert self.nr_row * self.nr_col >= len(plist), \\\n            ""{}*{} < {}"".format(self.nr_row, self.nr_col, len(plist))\n        if self.channel == 3 and plist.shape[3] == 1:\n            plist = np.repeat(plist, 3, axis=3)\n        cur_row, cur_col = 0, 0\n        if self.channel == 1:\n            self.canvas.fill(self.bgcolor)\n        else:\n            self.canvas[:, :, :] = self.bgcolor\n        for patch in plist:\n            r0 = cur_row * (self.ph + self.border)\n            c0 = cur_col * (self.pw + self.border)\n            self.canvas[r0:r0 + self.ph, c0:c0 + self.pw] = patch\n            cur_col += 1\n            if cur_col == self.nr_col:\n                cur_col = 0\n                cur_row += 1\n\n    def get_patchid_from_coord(self, x, y):\n        x = x // (self.pw + self.border)\n        y = y // (self.pw + self.border)\n        idx = y * self.nr_col + x\n        return idx\n\n\ndef stack_patches(\n        patch_list, nr_row, nr_col, border=None,\n        pad=False, bgcolor=255, viz=False, lclick_cb=None):\n    """"""\n    Stacked patches into grid, to produce visualizations like the following:\n\n    .. image:: https://github.com/tensorpack/tensorpack/raw/master/examples/GAN/demo/BEGAN-CelebA-samples.jpg\n\n    Args:\n        patch_list(list[ndarray] or ndarray): NHW or NHWC images in [0,255].\n        nr_row(int), nr_col(int): rows and cols of the grid.\n            ``nr_col * nr_row`` must be no less than ``len(patch_list)``.\n        border(int): border length between images.\n            Defaults to ``0.05 * min(patch_width, patch_height)``.\n        pad (boolean): when `patch_list` is a list, pad all patches to the maximum height and width.\n            This option allows stacking patches of different shapes together.\n        bgcolor(int or 3-tuple): background color in [0, 255]. Either an int\n            or a BGR tuple.\n        viz(bool): whether to use :func:`interactive_imshow` to visualize the results.\n        lclick_cb: A callback function ``f(patch, patch index in patch_list)``\n            to get called when a patch get clicked in imshow.\n\n    Returns:\n        np.ndarray: the stacked image.\n    """"""\n    if pad:\n        patch_list = _pad_patch_list(patch_list, bgcolor)\n    patch_list = _preprocess_patch_list(patch_list)\n\n    if lclick_cb is not None:\n        viz = True\n    ph, pw = patch_list.shape[1:3]\n\n    canvas = Canvas(ph, pw, nr_row, nr_col,\n                    patch_list.shape[-1], border, bgcolor)\n\n    if lclick_cb is not None:\n        def lclick_callback(img, x, y):\n            idx = canvas.get_patchid_from_coord(x, y)\n            lclick_cb(patch_list[idx], idx)\n    else:\n        lclick_callback = None\n\n    canvas.draw_patches(patch_list)\n    if viz:\n        interactive_imshow(canvas.canvas, lclick_cb=lclick_callback)\n    return canvas.canvas\n\n\ndef gen_stack_patches(patch_list,\n                      nr_row=None, nr_col=None, border=None,\n                      max_width=1000, max_height=1000,\n                      bgcolor=255, viz=False, lclick_cb=None):\n    """"""\n    Similar to :func:`stack_patches` but with a generator interface.\n    It takes a much-longer list and yields stacked results one by one.\n    For example, if ``patch_list`` contains 1000 images and ``nr_row==nr_col==10``,\n    this generator yields 10 stacked images.\n\n    Args:\n        nr_row(int), nr_col(int): rows and cols of each result.\n        max_width(int), max_height(int): Maximum allowed size of the\n            stacked image. If ``nr_row/nr_col`` are None, this number\n            will be used to infer the rows and cols. Otherwise the option is\n            ignored.\n        patch_list, border, viz, lclick_cb: same as in :func:`stack_patches`.\n\n    Yields:\n        np.ndarray: the stacked image.\n    """"""\n    # setup parameters\n    patch_list = _preprocess_patch_list(patch_list)\n    if lclick_cb is not None:\n        viz = True\n    ph, pw = patch_list.shape[1:3]\n\n    if border is None:\n        border = int(0.05 * min(ph, pw))\n    if nr_row is None:\n        nr_row = int(max_height / (ph + border))\n    if nr_col is None:\n        nr_col = int(max_width / (pw + border))\n    canvas = Canvas(ph, pw, nr_row, nr_col, patch_list.shape[-1], border, bgcolor)\n\n    nr_patch = nr_row * nr_col\n    start = 0\n\n    if lclick_cb is not None:\n        def lclick_callback(img, x, y):\n            idx = canvas.get_patchid_from_coord(x, y)\n            idx = idx + start\n            if idx < end:\n                lclick_cb(patch_list[idx], idx)\n    else:\n        lclick_callback = None\n\n    while True:\n        end = start + nr_patch\n        cur_list = patch_list[start:end]\n        if not len(cur_list):\n            return\n        canvas.draw_patches(cur_list)\n        if viz:\n            interactive_imshow(canvas.canvas, lclick_cb=lclick_callback)\n        yield canvas.canvas\n        start = end\n\n\ndef dump_dataflow_images(df, index=0, batched=True,\n                         number=1000, output_dir=None,\n                         scale=1, resize=None, viz=None,\n                         flipRGB=False):\n    """"""\n    Dump or visualize images of a :class:`DataFlow`.\n\n    Args:\n        df (DataFlow): the DataFlow.\n        index (int): the index of the image component.\n        batched (bool): whether the component contains batched images (NHW or\n            NHWC) or not (HW or HWC).\n        number (int): how many datapoint to take from the DataFlow.\n        output_dir (str): output directory to save images, default to not save.\n        scale (float): scale the value, usually either 1 or 255.\n        resize (tuple or None): tuple of (h, w) to resize the images to.\n        viz (tuple or None): tuple of (h, w) determining the grid size to use\n            with :func:`gen_stack_patches` for visualization. No visualization will happen by\n            default.\n        flipRGB (bool): apply a RGB<->BGR conversion or not.\n    """"""\n    if output_dir:\n        mkdir_p(output_dir)\n    if viz is not None:\n        viz = shape2d(viz)\n        vizsize = viz[0] * viz[1]\n    if resize is not None:\n        resize = tuple(shape2d(resize))\n    vizlist = []\n\n    df.reset_state()\n    cnt = 0\n    while True:\n        for dp in df:\n            if not batched:\n                imgbatch = [dp[index]]\n            else:\n                imgbatch = dp[index]\n            for img in imgbatch:\n                cnt += 1\n                if cnt == number:\n                    return\n                if scale != 1:\n                    img = img * scale\n                if resize is not None:\n                    img = cv2.resize(img, resize)\n                if flipRGB:\n                    img = img[:, :, ::-1]\n                if output_dir:\n                    fname = os.path.join(output_dir, \'{:03d}.jpg\'.format(cnt))\n                    cv2.imwrite(fname, img)\n                if viz is not None:\n                    vizlist.append(img)\n            if viz is not None and len(vizlist) >= vizsize:\n                stack_patches(\n                    vizlist[:vizsize],\n                    nr_row=viz[0], nr_col=viz[1], viz=True)\n                vizlist = vizlist[vizsize:]\n\n\ndef intensity_to_rgb(intensity, cmap=\'cubehelix\', normalize=False):\n    """"""\n    Convert a 1-channel matrix of intensities to an RGB image employing a colormap.\n    This function requires matplotlib. See `matplotlib colormaps\n    <http://matplotlib.org/examples/color/colormaps_reference.html>`_ for a\n    list of available colormap.\n\n    Args:\n        intensity (np.ndarray): array of intensities such as saliency.\n        cmap (str): name of the colormap to use.\n        normalize (bool): if True, will normalize the intensity so that it has\n            minimum 0 and maximum 1.\n\n    Returns:\n        np.ndarray: an RGB float32 image in range [0, 255], a colored heatmap.\n    """"""\n    assert intensity.ndim == 2, intensity.shape\n    intensity = intensity.astype(""float"")\n\n    if normalize:\n        intensity -= intensity.min()\n        intensity /= intensity.max()\n\n    cmap = plt.get_cmap(cmap)\n    intensity = cmap(intensity)[..., :3]\n    return intensity.astype(\'float32\') * 255.0\n\n\ndef draw_text(img, pos, text, color, font_scale=0.4):\n    """"""\n    Draw text on an image.\n\n    Args:\n        pos (tuple): x, y; the position of the text\n        text (str):\n        font_scale (float):\n        color (tuple): a 3-tuple BGR color in [0, 255]\n    """"""\n    img = img.astype(np.uint8)\n    x0, y0 = int(pos[0]), int(pos[1])\n    # Compute text size.\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    ((text_w, text_h), _) = cv2.getTextSize(text, font, font_scale, 1)\n    # Place text background.\n    if x0 + text_w > img.shape[1]:\n        x0 = img.shape[1] - text_w\n    if y0 - int(1.15 * text_h) < 0:\n        y0 = int(1.15 * text_h)\n    back_topleft = x0, y0 - int(1.3 * text_h)\n    back_bottomright = x0 + text_w, y0\n    cv2.rectangle(img, back_topleft, back_bottomright, color, -1)\n    # Show text.\n    text_bottomleft = x0, y0 - int(0.25 * text_h)\n    cv2.putText(img, text, text_bottomleft, font, font_scale, (222, 222, 222), lineType=cv2.LINE_AA)\n    return img\n\n\ndef draw_boxes(im, boxes, labels=None, color=None):\n    """"""\n    Args:\n        im (np.ndarray): a BGR image in range [0,255]. It will not be modified.\n        boxes (np.ndarray): a numpy array of shape Nx4 where each row is [x1, y1, x2, y2].\n        labels: (list[str] or None)\n        color: a 3-tuple BGR color (in range [0, 255])\n\n    Returns:\n        np.ndarray: a new image.\n    """"""\n    boxes = np.asarray(boxes, dtype=\'int32\')\n    if labels is not None:\n        assert len(labels) == len(boxes), ""{} != {}"".format(len(labels), len(boxes))\n    areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n    sorted_inds = np.argsort(-areas)    # draw large ones first\n    assert areas.min() > 0, areas.min()\n    # allow equal, because we are not very strict about rounding error here\n    assert boxes[:, 0].min() >= 0 and boxes[:, 1].min() >= 0 \\\n        and boxes[:, 2].max() <= im.shape[1] and boxes[:, 3].max() <= im.shape[0], \\\n        ""Image shape: {}\\n Boxes:\\n{}"".format(str(im.shape), str(boxes))\n\n    im = im.copy()\n    if color is None:\n        color = (15, 128, 15)\n    if im.ndim == 2 or (im.ndim == 3 and im.shape[2] == 1):\n        im = cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)\n    for i in sorted_inds:\n        box = boxes[i, :]\n        if labels is not None:\n            im = draw_text(im, (box[0], box[1]), labels[i], color=color)\n        cv2.rectangle(im, (box[0], box[1]), (box[2], box[3]),\n                      color=color, thickness=1)\n    return im\n\n\ntry:\n    import matplotlib.pyplot as plt\nexcept (ImportError, RuntimeError):\n    intensity_to_rgb = create_dummy_func(\'intensity_to_rgb\', \'matplotlib\')    # noqa\n\nif __name__ == \'__main__\':\n    if False:\n        imglist = []\n        for i in range(100):\n            fname = ""{:03d}.png"".format(i)\n            imglist.append(cv2.imread(fname))\n        for idx, patch in enumerate(gen_stack_patches(\n                imglist, max_width=500, max_height=200)):\n            of = ""patch{:02d}.png"".format(idx)\n            cv2.imwrite(of, patch)\n    if False:\n        imglist = []\n        img = cv2.imread(\'out.png\')\n        img2 = cv2.resize(img, (300, 300))\n        viz = stack_patches([img, img2], 1, 2, pad=True, viz=True)\n\n    if False:\n        img = cv2.imread(\'cat.jpg\')\n        boxes = np.asarray([\n            [10, 30, 200, 100],\n            [20, 80, 250, 250]\n        ])\n        img = draw_boxes(img, boxes, [\'asdfasdf\', \'11111111111111\'])\n        interactive_imshow(img)\n'"
examples/FasterRCNN/convert_d2/convert_d2.py,0,"b'#!/usr/bin/env python\n\nimport argparse\nimport numpy as np\nimport pickle\nfrom detectron2.config import get_cfg\n\n\ndef convert_config(cfg):\n    ret = []\n    ret.append((""MODE_MASK"", cfg.MODEL.MASK_ON))\n    has_fpn = ""fpn"" in cfg.MODEL.BACKBONE.NAME\n    ret.append((""MODE_FPN"", has_fpn))\n    if not has_fpn:\n        # we only support C4 and FPN\n        assert cfg.MODEL.ROI_HEADS.NAME == ""Res5ROIHeads""\n    else:\n        ret.append((""FPN.CASCADE"", cfg.MODEL.ROI_HEADS.NAME == ""CascadeROIHeads""))\n        assert len(cfg.MODEL.ROI_BOX_CASCADE_HEAD.IOUS) == 3\n    depth = cfg.MODEL.RESNETS.DEPTH\n    assert depth in [50, 101], depth\n    if depth == 101:\n        ret.append((""BACKBONE.RESNET_NUM_BLOCKS"", [3, 4, 23, 3]))\n    ret.append((""BACKBONE.STRIDE_1X1"", cfg.MODEL.RESNETS.STRIDE_IN_1X1))\n    ret.append((""PREPROC.PIXEL_MEAN"", cfg.MODEL.PIXEL_MEAN[::-1]))\n    ret.append((""PREPROC.PIXEL_STD"", cfg.MODEL.PIXEL_STD[::-1]))\n\n    assert cfg.MODEL.ROI_MASK_HEAD.POOLER_TYPE == ""ROIAlignV2""\n    assert cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE == ""ROIAlignV2""\n    return ret\n\n\ndef convert_weights(d, cfg):\n    has_fpn = ""fpn"" in cfg.MODEL.BACKBONE.NAME\n\n    ret = {}\n\n    def _convert_conv(src, dst):\n        src_w = d.pop(src + "".weight"").transpose(2, 3, 1, 0)\n        ret[dst + ""/W""] = src_w\n        if src + "".norm.weight"" in d:     # has norm\n            ret[dst + ""/bn/gamma""] = d.pop(src + "".norm.weight"")\n            ret[dst + ""/bn/beta""] = d.pop(src + "".norm.bias"")\n            ret[dst + ""/bn/variance/EMA""] = d.pop(src + "".norm.running_var"")\n            ret[dst + ""/bn/mean/EMA""] = d.pop(src + "".norm.running_mean"")\n        if src + "".bias"" in d:\n            ret[dst + ""/b""] = d.pop(src + "".bias"")\n\n    def _convert_fc(src, dst):\n        ret[dst + ""/W""] = d.pop(src + "".weight"").transpose()\n        ret[dst + ""/b""] = d.pop(src + "".bias"")\n\n    if has_fpn:\n        backbone_prefix = ""backbone.bottom_up.""\n    else:\n        backbone_prefix = ""backbone.""\n    _convert_conv(backbone_prefix + ""stem.conv1"", ""conv0"")\n    for grpid in range(4):\n        if not has_fpn and grpid == 3:\n            backbone_prefix = ""roi_heads.""\n        for blkid in range([3, 4, 6 if cfg.MODEL.RESNETS.DEPTH == 50 else 23, 3][grpid]):\n            _convert_conv(backbone_prefix + f""res{grpid + 2}.{blkid}.conv1"",\n                          f""group{grpid}/block{blkid}/conv1"")\n            _convert_conv(backbone_prefix + f""res{grpid + 2}.{blkid}.conv2"",\n                          f""group{grpid}/block{blkid}/conv2"")\n            _convert_conv(backbone_prefix + f""res{grpid + 2}.{blkid}.conv3"",\n                          f""group{grpid}/block{blkid}/conv3"")\n            if blkid == 0:\n                _convert_conv(backbone_prefix + f""res{grpid + 2}.{blkid}.shortcut"",\n                              f""group{grpid}/block{blkid}/convshortcut"")\n\n    if has_fpn:\n        for lvl in range(2, 6):\n            _convert_conv(f""backbone.fpn_lateral{lvl}"", f""fpn/lateral_1x1_c{lvl}"")\n            _convert_conv(f""backbone.fpn_output{lvl}"", f""fpn/posthoc_3x3_p{lvl}"")\n\n    # RPN:\n    _convert_conv(""proposal_generator.rpn_head.conv"", ""rpn/conv0"")\n    _convert_conv(""proposal_generator.rpn_head.objectness_logits"", ""rpn/class"")\n    _convert_conv(""proposal_generator.rpn_head.anchor_deltas"", ""rpn/box"")\n\n    def _convert_box_predictor(src, dst):\n        if cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG:\n            _convert_fc(src + "".bbox_pred"", dst + ""/box"")\n        else:\n            v = d.pop(src + "".bbox_pred.bias"")\n            ret[dst + ""/box/b""] = np.concatenate((v[:4], v))\n            v = d.pop(src + "".bbox_pred.weight"")\n            ret[dst + ""/box/W""] = np.concatenate((v[:4, :], v), axis=0).transpose()\n\n        _convert_fc(src + "".cls_score"", dst + ""/class"")\n\n        num_class = ret[dst + ""/class/W""].shape[1] - 1\n        idxs = np.concatenate(((num_class, ), np.arange(num_class)))\n        ret[dst + ""/class/W""] = ret[dst + ""/class/W""][:, idxs]\n        ret[dst + ""/class/b""] = ret[dst + ""/class/b""][idxs]\n\n    # Fast R-CNN: box head\n    has_cascade = cfg.MODEL.ROI_HEADS.NAME == ""CascadeROIHeads""\n    if has_cascade:\n        assert cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG\n        for k in range(3):\n            for i in range(cfg.MODEL.ROI_BOX_HEAD.NUM_FC):\n                _convert_fc(f""roi_heads.box_head.{k}.fc{i+1}"", f""cascade_rcnn_stage{k+1}/head/fc{i+6}"")\n            _convert_box_predictor(f""roi_heads.box_predictor.{k}"", f""cascade_rcnn_stage{k+1}/outputs"")\n    else:\n        for i in range(cfg.MODEL.ROI_BOX_HEAD.NUM_FC):\n            _convert_fc(f""roi_heads.box_head.fc{i+1}"", f""fastrcnn/fc{i+6}"")\n        _convert_box_predictor(""roi_heads.box_predictor"", ""fastrcnn/outputs"" if has_fpn else ""fastrcnn"")\n\n    # mask head\n    if cfg.MODEL.MASK_ON:\n        for fcn in range(cfg.MODEL.ROI_MASK_HEAD.NUM_CONV):\n            _convert_conv(f""roi_heads.mask_head.mask_fcn{fcn+1}"", f""maskrcnn/fcn{fcn}"")\n        _convert_conv(""roi_heads.mask_head.deconv"", ""maskrcnn/deconv"")\n        _convert_conv(""roi_heads.mask_head.predictor"", ""maskrcnn/conv"")\n\n    for k in list(d.keys()):\n        if ""cell_anchors"" in k:\n            d.pop(k)\n    assert len(d) == 0, d.keys()\n    return ret\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--d2-config"")\n    parser.add_argument(""--d2-pkl"")\n    parser.add_argument(""--output"")\n    args = parser.parse_args()\n\n    cfg = get_cfg()\n    cfg.merge_from_file(args.d2_config)\n\n    tp_cfg = convert_config(cfg)\n    for k, v in tp_cfg:\n        print(""\'{}={}\'"".format(k, v).replace(\' \', \'\'), end=\' \')\n\n    with open(args.d2_pkl, ""rb"") as f:\n        d2_dict = pickle.load(f)[""model""]\n    tp_dict = convert_weights(d2_dict, cfg)\n\n    np.savez_compressed(args.output, **tp_dict)\n'"
examples/FasterRCNN/dataset/__init__.py,0,b'from .dataset import *\nfrom .coco import *\nfrom .balloon import *\n'
examples/FasterRCNN/dataset/balloon.py,0,"b'import os\nimport numpy as np\nimport json\nfrom dataset import DatasetSplit, DatasetRegistry\n\n__all__ = [""register_balloon""]\n\n\nclass BalloonDemo(DatasetSplit):\n    def __init__(self, base_dir, split):\n        assert split in [""train"", ""val""]\n        base_dir = os.path.expanduser(base_dir)\n        self.imgdir = os.path.join(base_dir, split)\n        assert os.path.isdir(self.imgdir), self.imgdir\n\n    def training_roidbs(self):\n        json_file = os.path.join(self.imgdir, ""via_region_data.json"")\n        with open(json_file) as f:\n            obj = json.load(f)\n\n        ret = []\n        for _, v in obj.items():\n            fname = v[""filename""]\n            fname = os.path.join(self.imgdir, fname)\n\n            roidb = {""file_name"": fname}\n\n            annos = v[""regions""]\n\n            boxes = []\n            segs = []\n            for _, anno in annos.items():\n                assert not anno[""region_attributes""]\n                anno = anno[""shape_attributes""]\n                px = anno[""all_points_x""]\n                py = anno[""all_points_y""]\n                poly = np.stack((px, py), axis=1) + 0.5\n                maxxy = poly.max(axis=0)\n                minxy = poly.min(axis=0)\n\n                boxes.append([minxy[0], minxy[1], maxxy[0], maxxy[1]])\n                segs.append([poly])\n            N = len(annos)\n            roidb[""boxes""] = np.asarray(boxes, dtype=np.float32)\n            roidb[""segmentation""] = segs\n            roidb[""class""] = np.ones((N, ), dtype=np.int32)\n            roidb[""is_crowd""] = np.zeros((N, ), dtype=np.int8)\n            ret.append(roidb)\n        return ret\n\n\ndef register_balloon(basedir):\n    for split in [""train"", ""val""]:\n        name = ""balloon_"" + split\n        DatasetRegistry.register(name, lambda x=split: BalloonDemo(basedir, x))\n        DatasetRegistry.register_metadata(name, ""class_names"", [""BG"", ""balloon""])\n\n\nif __name__ == \'__main__\':\n    basedir = \'~/data/balloon\'\n    roidbs = BalloonDemo(basedir, ""train"").training_roidbs()\n    print(""#images:"", len(roidbs))\n\n    from viz import draw_annotation\n    from tensorpack.utils.viz import interactive_imshow as imshow\n    import cv2\n    for r in roidbs:\n        im = cv2.imread(r[""file_name""])\n        vis = draw_annotation(im, r[""boxes""], r[""class""], r[""segmentation""])\n        imshow(vis)\n'"
examples/FasterRCNN/dataset/coco.py,0,"b'# -*- coding: utf-8 -*-\n\nimport json\nimport numpy as np\nimport os\nimport tqdm\n\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.timer import timed_operation\n\nfrom config import config as cfg\nfrom dataset import DatasetRegistry, DatasetSplit\n\n__all__ = [\'register_coco\']\n\n\nclass COCODetection(DatasetSplit):\n    # handle a few special splits whose names do not match the directory names\n    _INSTANCE_TO_BASEDIR = {\n        \'valminusminival2014\': \'val2014\',\n        \'minival2014\': \'val2014\',\n        \'val2017_100\': \'val2017\',\n    }\n\n    """"""\n    Mapping from the incontinuous COCO category id to an id in [1, #category]\n    For your own coco-format, dataset, change this to an **empty dict**.\n    """"""\n    COCO_id_to_category_id = {13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72, 82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}  # noqa\n\n    def __init__(self, basedir, split):\n        """"""\n        Args:\n            basedir (str): root of the dataset which contains the subdirectories for each split and annotations\n            split (str): the name of the split, e.g. ""train2017"".\n                The split has to match an annotation file in ""annotations/"" and a directory of images.\n\n        Examples:\n            For a directory of this structure:\n\n            DIR/\n              annotations/\n                instances_XX.json\n                instances_YY.json\n              XX/\n              YY/\n\n            use `COCODetection(DIR, \'XX\')` and `COCODetection(DIR, \'YY\')`\n        """"""\n        basedir = os.path.expanduser(basedir)\n        self._imgdir = os.path.realpath(os.path.join(\n            basedir, self._INSTANCE_TO_BASEDIR.get(split, split)))\n        assert os.path.isdir(self._imgdir), ""{} is not a directory!"".format(self._imgdir)\n        annotation_file = os.path.join(\n            basedir, \'annotations/instances_{}.json\'.format(split))\n        assert os.path.isfile(annotation_file), annotation_file\n\n        from pycocotools.coco import COCO\n        self.coco = COCO(annotation_file)\n        self.annotation_file = annotation_file\n        logger.info(""Instances loaded from {}."".format(annotation_file))\n\n    # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n    def print_coco_metrics(self, results):\n        """"""\n        Args:\n            results(list[dict]): results in coco format\n        Returns:\n            dict: the evaluation metrics\n        """"""\n        from pycocotools.cocoeval import COCOeval\n        ret = {}\n        has_mask = ""segmentation"" in results[0]  # results will be modified by loadRes\n\n        cocoDt = self.coco.loadRes(results)\n        cocoEval = COCOeval(self.coco, cocoDt, \'bbox\')\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n        fields = [\'IoU=0.5:0.95\', \'IoU=0.5\', \'IoU=0.75\', \'small\', \'medium\', \'large\']\n        for k in range(6):\n            ret[\'mAP(bbox)/\' + fields[k]] = cocoEval.stats[k]\n\n        if len(results) > 0 and has_mask:\n            cocoEval = COCOeval(self.coco, cocoDt, \'segm\')\n            cocoEval.evaluate()\n            cocoEval.accumulate()\n            cocoEval.summarize()\n            for k in range(6):\n                ret[\'mAP(segm)/\' + fields[k]] = cocoEval.stats[k]\n        return ret\n\n    def load(self, add_gt=True, add_mask=False):\n        """"""\n        Args:\n            add_gt: whether to add ground truth bounding box annotations to the dicts\n            add_mask: whether to also add ground truth mask\n\n        Returns:\n            a list of dict, each has keys including:\n                \'image_id\', \'file_name\',\n                and (if add_gt is True) \'boxes\', \'class\', \'is_crowd\', and optionally\n                \'segmentation\'.\n        """"""\n        with timed_operation(\'Load annotations for {}\'.format(\n                os.path.basename(self.annotation_file))):\n            img_ids = self.coco.getImgIds()\n            img_ids.sort()\n            # list of dict, each has keys: height,width,id,file_name\n            imgs = self.coco.loadImgs(img_ids)\n\n            for idx, img in enumerate(tqdm.tqdm(imgs)):\n                img[\'image_id\'] = img.pop(\'id\')\n                img[\'file_name\'] = os.path.join(self._imgdir, img[\'file_name\'])\n                if idx == 0:\n                    # make sure the directories are correctly set\n                    assert os.path.isfile(img[""file_name""]), img[""file_name""]\n                if add_gt:\n                    self._add_detection_gt(img, add_mask)\n            return imgs\n\n    def _add_detection_gt(self, img, add_mask):\n        """"""\n        Add \'boxes\', \'class\', \'is_crowd\' of this image to the dict, used by detection.\n        If add_mask is True, also add \'segmentation\' in coco poly format.\n        """"""\n        # ann_ids = self.coco.getAnnIds(imgIds=img[\'image_id\'])\n        # objs = self.coco.loadAnns(ann_ids)\n        objs = self.coco.imgToAnns[img[\'image_id\']]  # equivalent but faster than the above two lines\n        if \'minival\' not in self.annotation_file:\n            # TODO better to check across the entire json, rather than per-image\n            ann_ids = [ann[""id""] for ann in objs]\n            assert len(set(ann_ids)) == len(ann_ids), \\\n                ""Annotation ids in \'{}\' are not unique!"".format(self.annotation_file)\n\n        # clean-up boxes\n        width = img.pop(\'width\')\n        height = img.pop(\'height\')\n\n        all_boxes = []\n        all_segm = []\n        all_cls = []\n        all_iscrowd = []\n        for objid, obj in enumerate(objs):\n            if obj.get(\'ignore\', 0) == 1:\n                continue\n            x1, y1, w, h = list(map(float, obj[\'bbox\']))\n            # bbox is originally in float\n            # x1/y1 means upper-left corner and w/h means true w/h. This can be verified by segmentation pixels.\n            # But we do make an assumption here that (0.0, 0.0) is upper-left corner of the first pixel\n            x2, y2 = x1 + w, y1 + h\n\n            # np.clip would be quite slow here\n            x1 = min(max(x1, 0), width)\n            x2 = min(max(x2, 0), width)\n            y1 = min(max(y1, 0), height)\n            y2 = min(max(y2, 0), height)\n            w, h = x2 - x1, y2 - y1\n            # Require non-zero seg area and more than 1x1 box size\n            if obj[\'area\'] > 1 and w > 0 and h > 0:\n                all_boxes.append([x1, y1, x2, y2])\n                all_cls.append(self.COCO_id_to_category_id.get(obj[\'category_id\'], obj[\'category_id\']))\n                iscrowd = obj.get(""iscrowd"", 0)\n                all_iscrowd.append(iscrowd)\n\n                if add_mask:\n                    segs = obj[\'segmentation\']\n                    if not isinstance(segs, list):\n                        assert iscrowd == 1\n                        all_segm.append(None)\n                    else:\n                        valid_segs = [np.asarray(p).reshape(-1, 2).astype(\'float32\') for p in segs if len(p) >= 6]\n                        if len(valid_segs) == 0:\n                            logger.error(""Object {} in image {} has no valid polygons!"".format(objid, img[\'file_name\']))\n                        elif len(valid_segs) < len(segs):\n                            logger.warn(""Object {} in image {} has invalid polygons!"".format(objid, img[\'file_name\']))\n                        all_segm.append(valid_segs)\n\n        # all geometrically-valid boxes are returned\n        if len(all_boxes):\n            img[\'boxes\'] = np.asarray(all_boxes, dtype=\'float32\')  # (n, 4)\n        else:\n            img[\'boxes\'] = np.zeros((0, 4), dtype=\'float32\')\n        cls = np.asarray(all_cls, dtype=\'int32\')  # (n,)\n        if len(cls):\n            assert cls.min() > 0, ""Category id in COCO format must > 0!""\n        img[\'class\'] = cls          # n, always >0\n        img[\'is_crowd\'] = np.asarray(all_iscrowd, dtype=\'int8\')  # n,\n        if add_mask:\n            # also required to be float32\n            img[\'segmentation\'] = all_segm\n\n    def training_roidbs(self):\n        return self.load(add_gt=True, add_mask=cfg.MODE_MASK)\n\n    def inference_roidbs(self):\n        return self.load(add_gt=False)\n\n    def eval_inference_results(self, results, output=None):\n        continuous_id_to_COCO_id = {v: k for k, v in self.COCO_id_to_category_id.items()}\n        for res in results:\n            # convert to COCO\'s incontinuous category id\n            if res[\'category_id\'] in continuous_id_to_COCO_id:\n                res[\'category_id\'] = continuous_id_to_COCO_id[res[\'category_id\']]\n            # COCO expects results in xywh format\n            box = res[\'bbox\']\n            box[2] -= box[0]\n            box[3] -= box[1]\n            res[\'bbox\'] = [round(float(x), 3) for x in box]\n\n        if output is not None:\n            with open(output, \'w\') as f:\n                json.dump(results, f)\n        if len(results):\n            # sometimes may crash if the results are empty?\n            return self.print_coco_metrics(results)\n        else:\n            return {}\n\n\ndef register_coco(basedir):\n    """"""\n    Add COCO datasets like ""coco_train201x"" to the registry,\n    so you can refer to them with names in `cfg.DATA.TRAIN/VAL`.\n\n    Note that train2017==trainval35k==train2014+val2014-minival2014, and val2017==minival2014.\n    """"""\n\n    # 80 names for COCO\n    # For your own coco-format dataset, change this.\n    class_names = [\n        ""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"", ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"", ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"", ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"", ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush""]  # noqa\n    class_names = [""BG""] + class_names\n\n    for split in [""train2017"", ""val2017"", ""train2014"", ""val2014"",\n                  ""valminusminival2014"", ""minival2014"", ""val2017_100""]:\n        name = ""coco_"" + split\n        DatasetRegistry.register(name, lambda x=split: COCODetection(basedir, x))\n        DatasetRegistry.register_metadata(name, \'class_names\', class_names)\n\n\nif __name__ == \'__main__\':\n    basedir = \'~/data/coco\'\n    c = COCODetection(basedir, \'train2014\')\n    roidb = c.load(add_gt=True, add_mask=True)\n    print(""#Images:"", len(roidb))\n'"
examples/FasterRCNN/dataset/dataset.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom collections import defaultdict\n\n__all__ = [\'DatasetRegistry\', \'DatasetSplit\']\n\n\nclass DatasetSplit():\n    """"""\n    A class to load datasets, evaluate results for a datast split (e.g., ""coco_train_2017"")\n\n    To use your own dataset that\'s not in COCO format, write a subclass that\n    implements the interfaces.\n    """"""\n    def training_roidbs(self):\n        """"""\n        Returns:\n            roidbs (list[dict]):\n\n        Produce ""roidbs"" as a list of dict, each dict corresponds to one image with k>=0 instances.\n        and the following keys are expected for training:\n\n        file_name: str, full path to the image\n        boxes: numpy array of kx4 floats, each row is [x1, y1, x2, y2]\n        class: numpy array of k integers, in the range of [1, #categories], NOT [0, #categories)\n        is_crowd: k booleans. Use k False if you don\'t know what it means.\n        segmentation: k lists of numpy arrays.\n            Each list of numpy arrays corresponds to the mask for one instance.\n            Each numpy array in the list is a polygon of shape Nx2,\n            because one mask can be represented by N polygons.\n            Each row in the Nx2 array is a (x, y) coordinate.\n\n            If your segmentation annotations are originally masks rather than polygons,\n            either convert it, or the augmentation will need to be changed or skipped accordingly.\n\n            Include this field only if training Mask R-CNN.\n\n        Coordinates in boxes & polygons are absolute coordinates in unit of pixels, unless\n        cfg.DATA.ABSOLUTE_COORD is False.\n        """"""\n        raise NotImplementedError()\n\n    def inference_roidbs(self):\n        """"""\n        Returns:\n            roidbs (list[dict]):\n\n            Each dict corresponds to one image to run inference on. The\n            following keys in the dict are expected:\n\n            file_name (str): full path to the image\n            image_id (str): an id for the image. The inference results will be stored with this id.\n        """"""\n        raise NotImplementedError()\n\n    def eval_inference_results(self, results, output=None):\n        """"""\n        Args:\n            results (list[dict]): the inference results as dicts.\n                Each dict corresponds to one __instance__. It contains the following keys:\n\n                image_id (str): the id that matches `inference_roidbs`.\n                category_id (int): the category prediction, in range [1, #category]\n                bbox (list[float]): x1, y1, x2, y2\n                score (float):\n                segmentation: the segmentation mask in COCO\'s rle format.\n            output (str): the output file or directory to optionally save the results to.\n\n        Returns:\n            dict: the evaluation results.\n        """"""\n        raise NotImplementedError()\n\n\nclass DatasetRegistry():\n    _registry = {}\n    _metadata_registry = defaultdict(dict)\n\n    @staticmethod\n    def register(name, func):\n        """"""\n        Args:\n            name (str): the name of the dataset split, e.g. ""coco_train2017""\n            func: a function which returns an instance of `DatasetSplit`\n        """"""\n        assert name not in DatasetRegistry._registry, ""Dataset {} was registered already!"".format(name)\n        DatasetRegistry._registry[name] = func\n\n    @staticmethod\n    def get(name):\n        """"""\n        Args:\n            name (str): the name of the dataset split, e.g. ""coco_train2017""\n\n        Returns:\n            DatasetSplit\n        """"""\n        assert name in DatasetRegistry._registry, ""Dataset {} was not registered!"".format(name)\n        return DatasetRegistry._registry[name]()\n\n    @staticmethod\n    def register_metadata(name, key, value):\n        """"""\n        Args:\n            name (str): the name of the dataset split, e.g. ""coco_train2017""\n            key: the key of the metadata, e.g., ""class_names""\n            value: the value of the metadata\n        """"""\n        DatasetRegistry._metadata_registry[name][key] = value\n\n    @staticmethod\n    def get_metadata(name, key):\n        """"""\n        Args:\n            name (str): the name of the dataset split, e.g. ""coco_train2017""\n            key: the key of the metadata, e.g., ""class_names""\n\n        Returns:\n            value\n        """"""\n        return DatasetRegistry._metadata_registry[name][key]\n'"
examples/FasterRCNN/modeling/__init__.py,0,b''
examples/FasterRCNN/modeling/backbone.py,32,"b'# -*- coding: utf-8 -*-\n# File: backbone.py\n\nimport numpy as np\nimport tensorflow as tf\nfrom contextlib import ExitStack, contextmanager\n\nfrom tensorpack.models import BatchNorm, Conv2D, MaxPooling, layer_register\nfrom tensorpack.tfutils import argscope\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\nfrom tensorpack.tfutils.varreplace import custom_getter_scope, freeze_variables\n\nfrom config import config as cfg\n\n\n@layer_register(log_shape=True)\ndef GroupNorm(x, group=32, gamma_initializer=tf.constant_initializer(1.)):\n    """"""\n    More code that reproduces the paper can be found at https://github.com/ppwwyyxx/GroupNorm-reproduce/.\n    """"""\n    shape = x.get_shape().as_list()\n    ndims = len(shape)\n    assert ndims == 4, shape\n    chan = shape[1]\n    assert chan % group == 0, chan\n    group_size = chan // group\n\n    orig_shape = tf.shape(x)\n    h, w = orig_shape[2], orig_shape[3]\n\n    x = tf.reshape(x, tf.stack([-1, group, group_size, h, w]))\n\n    mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\n\n    new_shape = [1, group, group_size, 1, 1]\n\n    beta = tf.get_variable(\'beta\', [chan], initializer=tf.constant_initializer())\n    beta = tf.reshape(beta, new_shape)\n\n    gamma = tf.get_variable(\'gamma\', [chan], initializer=gamma_initializer)\n    gamma = tf.reshape(gamma, new_shape)\n\n    out = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-5, name=\'output\')\n    return tf.reshape(out, orig_shape, name=\'output\')\n\n\ndef freeze_affine_getter(getter, *args, **kwargs):\n    # custom getter to freeze affine params inside bn\n    name = args[0] if len(args) else kwargs.get(\'name\')\n    if name.endswith(\'/gamma\') or name.endswith(\'/beta\'):\n        kwargs[\'trainable\'] = False\n        ret = getter(*args, **kwargs)\n        tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, ret)\n    else:\n        ret = getter(*args, **kwargs)\n    return ret\n\n\ndef maybe_reverse_pad(topleft, bottomright):\n    if cfg.BACKBONE.TF_PAD_MODE:\n        return [topleft, bottomright]\n    return [bottomright, topleft]\n\n\n@contextmanager\ndef backbone_scope(freeze):\n    """"""\n    Args:\n        freeze (bool): whether to freeze all the variables under the scope\n    """"""\n    def nonlin(x):\n        x = get_norm()(x)\n        return tf.nn.relu(x)\n\n    with argscope([Conv2D, MaxPooling, BatchNorm], data_format=\'channels_first\'), \\\n            argscope(Conv2D, use_bias=False, activation=nonlin,\n                     kernel_initializer=tf.variance_scaling_initializer(\n                         scale=2.0, mode=\'fan_out\')), \\\n            ExitStack() as stack:\n        if cfg.BACKBONE.NORM in [\'FreezeBN\', \'SyncBN\']:\n            if freeze or cfg.BACKBONE.NORM == \'FreezeBN\':\n                stack.enter_context(argscope(BatchNorm, training=False))\n            else:\n                stack.enter_context(argscope(\n                    BatchNorm, sync_statistics=\'nccl\' if cfg.TRAINER == \'replicated\' else \'horovod\'))\n\n        if freeze:\n            stack.enter_context(freeze_variables(stop_gradient=False, skip_collection=True))\n        else:\n            # the layers are not completely freezed, but we may want to only freeze the affine\n            if cfg.BACKBONE.FREEZE_AFFINE:\n                stack.enter_context(custom_getter_scope(freeze_affine_getter))\n        yield\n\n\ndef image_preprocess(image, bgr=True):\n    with tf.name_scope(\'image_preprocess\'):\n        if image.dtype.base_dtype != tf.float32:\n            image = tf.cast(image, tf.float32)\n\n        mean = cfg.PREPROC.PIXEL_MEAN\n        std = np.asarray(cfg.PREPROC.PIXEL_STD)\n        if bgr:\n            mean = mean[::-1]\n            std = std[::-1]\n        image_mean = tf.constant(mean, dtype=tf.float32)\n        image_invstd = tf.constant(1.0 / std, dtype=tf.float32)\n        image = (image - image_mean) * image_invstd\n        return image\n\n\ndef get_norm(zero_init=False):\n    if cfg.BACKBONE.NORM == \'None\':\n        return lambda x: x\n    if cfg.BACKBONE.NORM == \'GN\':\n        Norm = GroupNorm\n        layer_name = \'gn\'\n    else:\n        Norm = BatchNorm\n        layer_name = \'bn\'\n    return lambda x: Norm(layer_name, x, gamma_initializer=tf.zeros_initializer() if zero_init else None)\n\n\ndef resnet_shortcut(l, n_out, stride, activation=tf.identity):\n    n_in = l.shape[1]\n    if n_in != n_out:   # change dimension when channel is not the same\n        # TF\'s SAME mode output ceil(x/stride), which is NOT what we want when x is odd and stride is 2\n        # In FPN mode, the images are pre-padded already.\n        if not cfg.MODE_FPN and stride == 2:\n            l = l[:, :, :-1, :-1]\n        return Conv2D(\'convshortcut\', l, n_out, 1,\n                      strides=stride, activation=activation)\n    else:\n        return l\n\n\ndef resnet_bottleneck(l, ch_out, stride):\n    shortcut = l\n    if cfg.BACKBONE.STRIDE_1X1:\n        if stride == 2:\n            l = l[:, :, :-1, :-1]\n        l = Conv2D(\'conv1\', l, ch_out, 1, strides=stride)\n        l = Conv2D(\'conv2\', l, ch_out, 3, strides=1)\n    else:\n        l = Conv2D(\'conv1\', l, ch_out, 1, strides=1)\n        if stride == 2:\n            l = tf.pad(l, [[0, 0], [0, 0], maybe_reverse_pad(0, 1), maybe_reverse_pad(0, 1)])\n            l = Conv2D(\'conv2\', l, ch_out, 3, strides=2, padding=\'VALID\')\n        else:\n            l = Conv2D(\'conv2\', l, ch_out, 3, strides=stride)\n    if cfg.BACKBONE.NORM != \'None\':\n        l = Conv2D(\'conv3\', l, ch_out * 4, 1, activation=get_norm(zero_init=True))\n    else:\n        l = Conv2D(\'conv3\', l, ch_out * 4, 1, activation=tf.identity,\n                   kernel_initializer=tf.constant_initializer())\n    ret = l + resnet_shortcut(shortcut, ch_out * 4, stride, activation=get_norm(zero_init=False))\n    return tf.nn.relu(ret, name=\'output\')\n\n\ndef resnet_group(name, l, block_func, features, count, stride):\n    with tf.variable_scope(name):\n        for i in range(0, count):\n            with tf.variable_scope(\'block{}\'.format(i)):\n                l = block_func(l, features, stride if i == 0 else 1)\n    return l\n\n\ndef resnet_c4_backbone(image, num_blocks):\n    assert len(num_blocks) == 3\n    freeze_at = cfg.BACKBONE.FREEZE_AT\n    with backbone_scope(freeze=freeze_at > 0):\n        l = tf.pad(image, [[0, 0], [0, 0], maybe_reverse_pad(2, 3), maybe_reverse_pad(2, 3)])\n        l = Conv2D(\'conv0\', l, 64, 7, strides=2, padding=\'VALID\')\n        l = tf.pad(l, [[0, 0], [0, 0], maybe_reverse_pad(0, 1), maybe_reverse_pad(0, 1)])\n        l = MaxPooling(\'pool0\', l, 3, strides=2, padding=\'VALID\')\n\n    with backbone_scope(freeze=freeze_at > 1):\n        c2 = resnet_group(\'group0\', l, resnet_bottleneck, 64, num_blocks[0], 1)\n    with backbone_scope(freeze=False):\n        c3 = resnet_group(\'group1\', c2, resnet_bottleneck, 128, num_blocks[1], 2)\n        c4 = resnet_group(\'group2\', c3, resnet_bottleneck, 256, num_blocks[2], 2)\n    # 16x downsampling up to now\n    return c4\n\n\n@auto_reuse_variable_scope\ndef resnet_conv5(image, num_block):\n    with backbone_scope(freeze=False):\n        l = resnet_group(\'group3\', image, resnet_bottleneck, 512, num_block, 2)\n        return l\n\n\ndef resnet_fpn_backbone(image, num_blocks):\n    freeze_at = cfg.BACKBONE.FREEZE_AT\n    shape2d = tf.shape(image)[2:]\n    mult = float(cfg.FPN.RESOLUTION_REQUIREMENT)\n    new_shape2d = tf.cast(tf.ceil(tf.cast(shape2d, tf.float32) / mult) * mult, tf.int32)\n    pad_shape2d = new_shape2d - shape2d\n    assert len(num_blocks) == 4, num_blocks\n    with backbone_scope(freeze=freeze_at > 0):\n        chan = image.shape[1]\n        pad_base = maybe_reverse_pad(2, 3)\n        l = tf.pad(image, tf.stack(\n            [[0, 0], [0, 0],\n             [pad_base[0], pad_base[1] + pad_shape2d[0]],\n             [pad_base[0], pad_base[1] + pad_shape2d[1]]]))\n        l.set_shape([None, chan, None, None])\n        l = Conv2D(\'conv0\', l, 64, 7, strides=2, padding=\'VALID\')\n        l = tf.pad(l, [[0, 0], [0, 0], maybe_reverse_pad(0, 1), maybe_reverse_pad(0, 1)])\n        l = MaxPooling(\'pool0\', l, 3, strides=2, padding=\'VALID\')\n    with backbone_scope(freeze=freeze_at > 1):\n        c2 = resnet_group(\'group0\', l, resnet_bottleneck, 64, num_blocks[0], 1)\n    with backbone_scope(freeze=False):\n        c3 = resnet_group(\'group1\', c2, resnet_bottleneck, 128, num_blocks[1], 2)\n        c4 = resnet_group(\'group2\', c3, resnet_bottleneck, 256, num_blocks[2], 2)\n        c5 = resnet_group(\'group3\', c4, resnet_bottleneck, 512, num_blocks[3], 2)\n    # 32x downsampling up to now\n    # size of c5: ceil(input/32)\n    return c2, c3, c4, c5\n'"
examples/FasterRCNN/modeling/generalized_rcnn.py,41,"b'# -*- coding: utf-8 -*-\n# File:\n\nimport tensorflow as tf\n\nfrom tensorpack import ModelDesc\nfrom tensorpack.models import GlobalAvgPooling, l2_regularizer, regularize_cost\nfrom tensorpack.tfutils import optimizer\nfrom tensorpack.tfutils.summary import add_moving_summary\n\nfrom config import config as cfg\nfrom data import get_all_anchors, get_all_anchors_fpn\nfrom utils.box_ops import area as tf_area\n\nfrom . import model_frcnn\nfrom . import model_mrcnn\nfrom .backbone import image_preprocess, resnet_c4_backbone, resnet_conv5, resnet_fpn_backbone\nfrom .model_box import RPNAnchors, clip_boxes, crop_and_resize, roi_align\nfrom .model_cascade import CascadeRCNNHead\nfrom .model_fpn import fpn_model, generate_fpn_proposals, multilevel_roi_align, multilevel_rpn_losses\nfrom .model_frcnn import (\n    BoxProposals, FastRCNNHead, fastrcnn_outputs, fastrcnn_predictions, sample_fast_rcnn_targets)\nfrom .model_mrcnn import maskrcnn_loss, maskrcnn_upXconv_head, unpackbits_masks\nfrom .model_rpn import generate_rpn_proposals, rpn_head, rpn_losses\n\n\nclass GeneralizedRCNN(ModelDesc):\n    def preprocess(self, image):\n        image = tf.expand_dims(image, 0)\n        image = image_preprocess(image, bgr=True)\n        return tf.transpose(image, [0, 3, 1, 2])\n\n    def optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0., trainable=False)\n        tf.summary.scalar(\'learning_rate-summary\', lr)\n\n        # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False.\n        lr = lr / 8.\n        opt = tf.train.MomentumOptimizer(lr, 0.9)\n        if cfg.TRAIN.NUM_GPUS < 8:\n            opt = optimizer.AccumGradOptimizer(opt, 8 // cfg.TRAIN.NUM_GPUS)\n        return opt\n\n    def get_inference_tensor_names(self):\n        """"""\n        Returns two lists of tensor names to be used to create an inference callable.\n\n        `build_graph` must create tensors of these names when called under inference context.\n\n        Returns:\n            [str]: input names\n            [str]: output names\n        """"""\n        out = [\'output/boxes\', \'output/scores\', \'output/labels\']\n        if cfg.MODE_MASK:\n            out.append(\'output/masks\')\n        return [\'image\'], out\n\n    def build_graph(self, *inputs):\n        inputs = dict(zip(self.input_names, inputs))\n        if ""gt_masks_packed"" in inputs:\n            gt_masks = tf.cast(unpackbits_masks(inputs.pop(""gt_masks_packed"")), tf.uint8, name=""gt_masks"")\n            inputs[""gt_masks""] = gt_masks\n\n        image = self.preprocess(inputs[\'image\'])     # 1CHW\n\n        features = self.backbone(image)\n        anchor_inputs = {k: v for k, v in inputs.items() if k.startswith(\'anchor_\')}\n        proposals, rpn_losses = self.rpn(image, features, anchor_inputs)  # inputs?\n\n        targets = [inputs[k] for k in [\'gt_boxes\', \'gt_labels\', \'gt_masks\'] if k in inputs]\n        gt_boxes_area = tf.reduce_mean(tf_area(inputs[""gt_boxes""]), name=\'mean_gt_box_area\')\n        add_moving_summary(gt_boxes_area)\n        head_losses = self.roi_heads(image, features, proposals, targets)\n\n        if self.training:\n            wd_cost = regularize_cost(\n                \'.*/W\', l2_regularizer(cfg.TRAIN.WEIGHT_DECAY), name=\'wd_cost\')\n            total_cost = tf.add_n(\n                rpn_losses + head_losses + [wd_cost], \'total_cost\')\n            add_moving_summary(total_cost, wd_cost)\n            return total_cost\n        else:\n            # Check that the model defines the tensors it declares for inference\n            # For existing models, they are defined in ""fastrcnn_predictions(name_scope=\'output\')""\n            G = tf.get_default_graph()\n            ns = G.get_name_scope()\n            for name in self.get_inference_tensor_names()[1]:\n                try:\n                    name = \'/\'.join([ns, name]) if ns else name\n                    G.get_tensor_by_name(name + \':0\')\n                except KeyError:\n                    raise KeyError(""Your model does not define the tensor \'{}\' in inference context."".format(name))\n\n\nclass ResNetC4Model(GeneralizedRCNN):\n    def inputs(self):\n        ret = [\n            tf.TensorSpec((None, None, 3), tf.float32, \'image\'),\n            tf.TensorSpec((None, None, cfg.RPN.NUM_ANCHOR), tf.int32, \'anchor_labels\'),\n            tf.TensorSpec((None, None, cfg.RPN.NUM_ANCHOR, 4), tf.float32, \'anchor_boxes\'),\n            tf.TensorSpec((None, 4), tf.float32, \'gt_boxes\'),\n            tf.TensorSpec((None,), tf.int64, \'gt_labels\')]  # all > 0\n        if cfg.MODE_MASK:\n            ret.append(\n                tf.TensorSpec((None, None, None), tf.uint8, \'gt_masks_packed\')\n            )   # NR_GT x height x ceil(width/8), packed groundtruth masks\n        return ret\n\n    def backbone(self, image):\n        return [resnet_c4_backbone(image, cfg.BACKBONE.RESNET_NUM_BLOCKS[:3])]\n\n    def rpn(self, image, features, inputs):\n        featuremap = features[0]\n        rpn_label_logits, rpn_box_logits = rpn_head(\'rpn\', featuremap, cfg.RPN.HEAD_DIM, cfg.RPN.NUM_ANCHOR)\n        anchors = RPNAnchors(\n            get_all_anchors(\n                stride=cfg.RPN.ANCHOR_STRIDE, sizes=cfg.RPN.ANCHOR_SIZES,\n                ratios=cfg.RPN.ANCHOR_RATIOS, max_size=cfg.PREPROC.MAX_SIZE),\n            inputs[\'anchor_labels\'], inputs[\'anchor_boxes\'])\n        anchors = anchors.narrow_to(featuremap)\n\n        image_shape2d = tf.shape(image)[2:]     # h,w\n        pred_boxes_decoded = anchors.decode_logits(rpn_box_logits)  # fHxfWxNAx4, floatbox\n        proposal_boxes, proposal_scores = generate_rpn_proposals(\n            tf.reshape(pred_boxes_decoded, [-1, 4]),\n            tf.reshape(rpn_label_logits, [-1]),\n            image_shape2d,\n            cfg.RPN.TRAIN_PRE_NMS_TOPK if self.training else cfg.RPN.TEST_PRE_NMS_TOPK,\n            cfg.RPN.TRAIN_POST_NMS_TOPK if self.training else cfg.RPN.TEST_POST_NMS_TOPK)\n\n        if self.training:\n            losses = rpn_losses(\n                anchors.gt_labels, anchors.encoded_gt_boxes(), rpn_label_logits, rpn_box_logits)\n        else:\n            losses = []\n\n        return BoxProposals(proposal_boxes), losses\n\n    def roi_heads(self, image, features, proposals, targets):\n        image_shape2d = tf.shape(image)[2:]     # h,w\n        featuremap = features[0]\n\n        gt_boxes, gt_labels, *_ = targets\n\n        if self.training:\n            # sample proposal boxes in training\n            proposals = sample_fast_rcnn_targets(proposals.boxes, gt_boxes, gt_labels)\n        # The boxes to be used to crop RoIs.\n        # Use all proposal boxes in inference\n\n        boxes_on_featuremap = proposals.boxes * (1.0 / cfg.RPN.ANCHOR_STRIDE)\n        roi_resized = roi_align(featuremap, boxes_on_featuremap, 14)\n\n        feature_fastrcnn = resnet_conv5(roi_resized, cfg.BACKBONE.RESNET_NUM_BLOCKS[-1])    # nxcx7x7\n        # Keep C5 feature to be shared with mask branch\n        feature_gap = GlobalAvgPooling(\'gap\', feature_fastrcnn, data_format=\'channels_first\')\n        fastrcnn_label_logits, fastrcnn_box_logits = fastrcnn_outputs(\'fastrcnn\', feature_gap, cfg.DATA.NUM_CATEGORY)\n\n        fastrcnn_head = FastRCNNHead(proposals, fastrcnn_box_logits, fastrcnn_label_logits, gt_boxes,\n                                     tf.constant(cfg.FRCNN.BBOX_REG_WEIGHTS, dtype=tf.float32))\n\n        if self.training:\n            all_losses = fastrcnn_head.losses()\n\n            if cfg.MODE_MASK:\n                gt_masks = targets[2]\n                # maskrcnn loss\n                # In training, mask branch shares the same C5 feature.\n                fg_feature = tf.gather(feature_fastrcnn, proposals.fg_inds())\n                mask_logits = maskrcnn_upXconv_head(\n                    \'maskrcnn\', fg_feature, cfg.DATA.NUM_CATEGORY, num_convs=0)   # #fg x #cat x 14x14\n\n                target_masks_for_fg = crop_and_resize(\n                    tf.expand_dims(gt_masks, 1),\n                    proposals.fg_boxes(),\n                    proposals.fg_inds_wrt_gt, 14,\n                    pad_border=False)  # nfg x 1x14x14\n                target_masks_for_fg = tf.squeeze(target_masks_for_fg, 1, \'sampled_fg_mask_targets\')\n                all_losses.append(maskrcnn_loss(mask_logits, proposals.fg_labels(), target_masks_for_fg))\n            return all_losses\n        else:\n            decoded_boxes = fastrcnn_head.decoded_output_boxes()\n            decoded_boxes = clip_boxes(decoded_boxes, image_shape2d, name=\'fastrcnn_all_boxes\')\n            label_scores = fastrcnn_head.output_scores(name=\'fastrcnn_all_scores\')\n            final_boxes, final_scores, final_labels = fastrcnn_predictions(\n                decoded_boxes, label_scores, name_scope=\'output\')\n\n            if cfg.MODE_MASK:\n                roi_resized = roi_align(featuremap, final_boxes * (1.0 / cfg.RPN.ANCHOR_STRIDE), 14)\n                feature_maskrcnn = resnet_conv5(roi_resized, cfg.BACKBONE.RESNET_NUM_BLOCKS[-1])\n                mask_logits = maskrcnn_upXconv_head(\n                    \'maskrcnn\', feature_maskrcnn, cfg.DATA.NUM_CATEGORY, 0)   # #result x #cat x 14x14\n                indices = tf.stack([tf.range(tf.size(final_labels)), tf.cast(final_labels, tf.int32) - 1], axis=1)\n                final_mask_logits = tf.gather_nd(mask_logits, indices)   # #resultx14x14\n                tf.sigmoid(final_mask_logits, name=\'output/masks\')\n            return []\n\n\nclass ResNetFPNModel(GeneralizedRCNN):\n\n    def inputs(self):\n        ret = [\n            tf.TensorSpec((None, None, 3), tf.float32, \'image\')]\n        num_anchors = len(cfg.RPN.ANCHOR_RATIOS)\n        for k in range(len(cfg.FPN.ANCHOR_STRIDES)):\n            ret.extend([\n                tf.TensorSpec((None, None, num_anchors), tf.int32,\n                              \'anchor_labels_lvl{}\'.format(k + 2)),\n                tf.TensorSpec((None, None, num_anchors, 4), tf.float32,\n                              \'anchor_boxes_lvl{}\'.format(k + 2))])\n        ret.extend([\n            tf.TensorSpec((None, 4), tf.float32, \'gt_boxes\'),\n            tf.TensorSpec((None,), tf.int64, \'gt_labels\')])  # all > 0\n        if cfg.MODE_MASK:\n            ret.append(\n                tf.TensorSpec((None, None, None), tf.uint8, \'gt_masks_packed\')\n            )\n        return ret\n\n    def slice_feature_and_anchors(self, p23456, anchors):\n        for i, stride in enumerate(cfg.FPN.ANCHOR_STRIDES):\n            with tf.name_scope(\'FPN_slice_lvl{}\'.format(i)):\n                anchors[i] = anchors[i].narrow_to(p23456[i])\n\n    def backbone(self, image):\n        c2345 = resnet_fpn_backbone(image, cfg.BACKBONE.RESNET_NUM_BLOCKS)\n        p23456 = fpn_model(\'fpn\', c2345)\n        return p23456\n\n    def rpn(self, image, features, inputs):\n        assert len(cfg.RPN.ANCHOR_SIZES) == len(cfg.FPN.ANCHOR_STRIDES)\n\n        image_shape2d = tf.shape(image)[2:]     # h,w\n        all_anchors_fpn = get_all_anchors_fpn(\n            strides=cfg.FPN.ANCHOR_STRIDES,\n            sizes=cfg.RPN.ANCHOR_SIZES,\n            ratios=cfg.RPN.ANCHOR_RATIOS,\n            max_size=cfg.PREPROC.MAX_SIZE)\n        multilevel_anchors = [RPNAnchors(\n            all_anchors_fpn[i],\n            inputs[\'anchor_labels_lvl{}\'.format(i + 2)],\n            inputs[\'anchor_boxes_lvl{}\'.format(i + 2)]) for i in range(len(all_anchors_fpn))]\n        self.slice_feature_and_anchors(features, multilevel_anchors)\n\n        # Multi-Level RPN Proposals\n        rpn_outputs = [rpn_head(\'rpn\', pi, cfg.FPN.NUM_CHANNEL, len(cfg.RPN.ANCHOR_RATIOS))\n                       for pi in features]\n        multilevel_label_logits = [k[0] for k in rpn_outputs]\n        multilevel_box_logits = [k[1] for k in rpn_outputs]\n        multilevel_pred_boxes = [anchor.decode_logits(logits)\n                                 for anchor, logits in zip(multilevel_anchors, multilevel_box_logits)]\n\n        proposal_boxes, proposal_scores = generate_fpn_proposals(\n            multilevel_pred_boxes, multilevel_label_logits, image_shape2d)\n\n        if self.training:\n            losses = multilevel_rpn_losses(\n                multilevel_anchors, multilevel_label_logits, multilevel_box_logits)\n        else:\n            losses = []\n\n        return BoxProposals(proposal_boxes), losses\n\n    def roi_heads(self, image, features, proposals, targets):\n        image_shape2d = tf.shape(image)[2:]     # h,w\n        assert len(features) == 5, ""Features have to be P23456!""\n        gt_boxes, gt_labels, *_ = targets\n\n        if self.training:\n            proposals = sample_fast_rcnn_targets(proposals.boxes, gt_boxes, gt_labels)\n\n        fastrcnn_head_func = getattr(model_frcnn, cfg.FPN.FRCNN_HEAD_FUNC)\n        if not cfg.FPN.CASCADE:\n            roi_feature_fastrcnn = multilevel_roi_align(features[:4], proposals.boxes, 7)\n\n            head_feature = fastrcnn_head_func(\'fastrcnn\', roi_feature_fastrcnn)\n            fastrcnn_label_logits, fastrcnn_box_logits = fastrcnn_outputs(\n                \'fastrcnn/outputs\', head_feature, cfg.DATA.NUM_CATEGORY)\n            fastrcnn_head = FastRCNNHead(proposals, fastrcnn_box_logits, fastrcnn_label_logits,\n                                         gt_boxes, tf.constant(cfg.FRCNN.BBOX_REG_WEIGHTS, dtype=tf.float32))\n        else:\n            def roi_func(boxes):\n                return multilevel_roi_align(features[:4], boxes, 7)\n\n            fastrcnn_head = CascadeRCNNHead(\n                proposals, roi_func, fastrcnn_head_func,\n                (gt_boxes, gt_labels), image_shape2d, cfg.DATA.NUM_CATEGORY)\n\n        if self.training:\n            all_losses = fastrcnn_head.losses()\n\n            if cfg.MODE_MASK:\n                gt_masks = targets[2]\n                # maskrcnn loss\n                roi_feature_maskrcnn = multilevel_roi_align(\n                    features[:4], proposals.fg_boxes(), 14,\n                    name_scope=\'multilevel_roi_align_mask\')\n                maskrcnn_head_func = getattr(model_mrcnn, cfg.FPN.MRCNN_HEAD_FUNC)\n                mask_logits = maskrcnn_head_func(\n                    \'maskrcnn\', roi_feature_maskrcnn, cfg.DATA.NUM_CATEGORY)   # #fg x #cat x 28 x 28\n\n                target_masks_for_fg = crop_and_resize(\n                    tf.expand_dims(gt_masks, 1),\n                    proposals.fg_boxes(),\n                    proposals.fg_inds_wrt_gt, 28,\n                    pad_border=False)  # fg x 1x28x28\n                target_masks_for_fg = tf.squeeze(target_masks_for_fg, 1, \'sampled_fg_mask_targets\')\n                all_losses.append(maskrcnn_loss(mask_logits, proposals.fg_labels(), target_masks_for_fg))\n            return all_losses\n        else:\n            decoded_boxes = fastrcnn_head.decoded_output_boxes()\n            decoded_boxes = clip_boxes(decoded_boxes, image_shape2d, name=\'fastrcnn_all_boxes\')\n            label_scores = fastrcnn_head.output_scores(name=\'fastrcnn_all_scores\')\n            final_boxes, final_scores, final_labels = fastrcnn_predictions(\n                decoded_boxes, label_scores, name_scope=\'output\')\n            if cfg.MODE_MASK:\n                # Cascade inference needs roi transform with refined boxes.\n                roi_feature_maskrcnn = multilevel_roi_align(features[:4], final_boxes, 14)\n                maskrcnn_head_func = getattr(model_mrcnn, cfg.FPN.MRCNN_HEAD_FUNC)\n                mask_logits = maskrcnn_head_func(\n                    \'maskrcnn\', roi_feature_maskrcnn, cfg.DATA.NUM_CATEGORY)   # #fg x #cat x 28 x 28\n                indices = tf.stack([tf.range(tf.size(final_labels)), tf.cast(final_labels, tf.int32) - 1], axis=1)\n                final_mask_logits = tf.gather_nd(mask_logits, indices)   # #resultx28x28\n                tf.sigmoid(final_mask_logits, name=\'output/masks\')\n            return []\n'"
examples/FasterRCNN/modeling/model_box.py,47,"b'# -*- coding: utf-8 -*-\n# File: model_box.py\n\nimport numpy as np\nimport tensorflow as tf\nfrom collections import namedtuple\n\nfrom tensorpack.tfutils.scope_utils import under_name_scope\n\nfrom config import config\n\n\n@under_name_scope()\ndef clip_boxes(boxes, window, name=None):\n    """"""\n    Args:\n        boxes: nx4, xyxy\n        window: [h, w]\n    """"""\n    boxes = tf.maximum(boxes, 0.0)\n    m = tf.tile(tf.reverse(window, [0]), [2])    # (4,)\n    boxes = tf.minimum(boxes, tf.cast(m, tf.float32), name=name)\n    return boxes\n\n\n@under_name_scope()\ndef decode_bbox_target(box_predictions, anchors):\n    """"""\n    Args:\n        box_predictions: (..., 4), logits\n        anchors: (..., 4), floatbox. Must have the same shape\n\n    Returns:\n        box_decoded: (..., 4), float32. With the same shape.\n    """"""\n    orig_shape = tf.shape(anchors)\n    box_pred_txtytwth = tf.reshape(box_predictions, (-1, 2, 2))\n    box_pred_txty, box_pred_twth = tf.split(box_pred_txtytwth, 2, axis=1)\n    # each is (...)x1x2\n    anchors_x1y1x2y2 = tf.reshape(anchors, (-1, 2, 2))\n    anchors_x1y1, anchors_x2y2 = tf.split(anchors_x1y1x2y2, 2, axis=1)\n\n    waha = anchors_x2y2 - anchors_x1y1\n    xaya = (anchors_x2y2 + anchors_x1y1) * 0.5\n\n    clip = np.log(config.PREPROC.MAX_SIZE / 16.)\n    wbhb = tf.exp(tf.minimum(box_pred_twth, clip)) * waha\n    xbyb = box_pred_txty * waha + xaya\n    x1y1 = xbyb - wbhb * 0.5\n    x2y2 = xbyb + wbhb * 0.5    # (...)x1x2\n    out = tf.concat([x1y1, x2y2], axis=-2)\n    return tf.reshape(out, orig_shape)\n\n\n@under_name_scope()\ndef encode_bbox_target(boxes, anchors):\n    """"""\n    Args:\n        boxes: (..., 4), float32\n        anchors: (..., 4), float32\n\n    Returns:\n        box_encoded: (..., 4), float32 with the same shape.\n    """"""\n    anchors_x1y1x2y2 = tf.reshape(anchors, (-1, 2, 2))\n    anchors_x1y1, anchors_x2y2 = tf.split(anchors_x1y1x2y2, 2, axis=1)\n    waha = anchors_x2y2 - anchors_x1y1\n    xaya = (anchors_x2y2 + anchors_x1y1) * 0.5\n\n    boxes_x1y1x2y2 = tf.reshape(boxes, (-1, 2, 2))\n    boxes_x1y1, boxes_x2y2 = tf.split(boxes_x1y1x2y2, 2, axis=1)\n    wbhb = boxes_x2y2 - boxes_x1y1\n    xbyb = (boxes_x2y2 + boxes_x1y1) * 0.5\n\n    # Note that here not all boxes are valid. Some may be zero\n    txty = (xbyb - xaya) / waha\n    twth = tf.log(wbhb / waha)  # may contain -inf for invalid boxes\n    encoded = tf.concat([txty, twth], axis=1)  # (-1x2x2)\n    return tf.reshape(encoded, tf.shape(boxes))\n\n\n@under_name_scope()\ndef crop_and_resize(image, boxes, box_ind, crop_size, pad_border=True):\n    """"""\n    Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes.\n\n    Args:\n        image: NCHW\n        boxes: nx4, x1y1x2y2\n        box_ind: (n,)\n        crop_size (int):\n    Returns:\n        n,C,size,size\n    """"""\n    assert isinstance(crop_size, int), crop_size\n    boxes = tf.stop_gradient(boxes)\n\n    # TF\'s crop_and_resize produces zeros on border\n    if pad_border:\n        # this can be quite slow\n        image = tf.pad(image, [[0, 0], [0, 0], [1, 1], [1, 1]], mode=\'SYMMETRIC\')\n        boxes = boxes + 1\n\n    @under_name_scope()\n    def transform_fpcoor_for_tf(boxes, image_shape, crop_shape):\n        """"""\n        The way tf.image.crop_and_resize works (with normalized box):\n        Initial point (the value of output[0]): x0_box * (W_img - 1)\n        Spacing: w_box * (W_img - 1) / (W_crop - 1)\n        Use the above grid to bilinear sample.\n\n        However, what we want is (with fpcoor box):\n        Spacing: w_box / W_crop\n        Initial point: x0_box + spacing/2 - 0.5\n        (-0.5 because bilinear sample (in my definition) assumes floating point coordinate\n         (0.0, 0.0) is the same as pixel value (0, 0))\n\n        This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize\n\n        Returns:\n            y1x1y2x2\n        """"""\n        x0, y0, x1, y1 = tf.split(boxes, 4, axis=1)\n\n        spacing_w = (x1 - x0) / tf.cast(crop_shape[1], tf.float32)\n        spacing_h = (y1 - y0) / tf.cast(crop_shape[0], tf.float32)\n\n        imshape = [tf.cast(image_shape[0] - 1, tf.float32), tf.cast(image_shape[1] - 1, tf.float32)]\n        nx0 = (x0 + spacing_w / 2 - 0.5) / imshape[1]\n        ny0 = (y0 + spacing_h / 2 - 0.5) / imshape[0]\n\n        nw = spacing_w * tf.cast(crop_shape[1] - 1, tf.float32) / imshape[1]\n        nh = spacing_h * tf.cast(crop_shape[0] - 1, tf.float32) / imshape[0]\n\n        return tf.concat([ny0, nx0, ny0 + nh, nx0 + nw], axis=1)\n\n    image_shape = tf.shape(image)[2:]\n\n    boxes = transform_fpcoor_for_tf(boxes, image_shape, [crop_size, crop_size])\n    image = tf.transpose(image, [0, 2, 3, 1])   # nhwc\n    ret = tf.image.crop_and_resize(\n        image, boxes, tf.cast(box_ind, tf.int32),\n        crop_size=[crop_size, crop_size])\n    ret = tf.transpose(ret, [0, 3, 1, 2])   # ncss\n    return ret\n\n\n@under_name_scope()\ndef roi_align(featuremap, boxes, resolution):\n    """"""\n    Args:\n        featuremap: 1xCxHxW\n        boxes: Nx4 floatbox\n        resolution: output spatial resolution\n\n    Returns:\n        NxCx res x res\n    """"""\n    # sample 4 locations per roi bin\n    ret = crop_and_resize(\n        featuremap, boxes,\n        tf.zeros([tf.shape(boxes)[0]], dtype=tf.int32),\n        resolution * 2)\n    try:\n        avgpool = tf.nn.avg_pool2d\n    except AttributeError:\n        avgpool = tf.nn.avg_pool\n    ret = avgpool(ret, [1, 1, 2, 2], [1, 1, 2, 2], padding=\'SAME\', data_format=\'NCHW\')\n    return ret\n\n\nclass RPNAnchors(namedtuple(\'_RPNAnchors\', [\'boxes\', \'gt_labels\', \'gt_boxes\'])):\n    """"""\n    boxes (FS x FS x NA x 4): The anchor boxes.\n    gt_labels (FS x FS x NA):\n    gt_boxes (FS x FS x NA x 4): Groundtruth boxes corresponding to each anchor.\n    """"""\n    def encoded_gt_boxes(self):\n        return encode_bbox_target(self.gt_boxes, self.boxes)\n\n    def decode_logits(self, logits):\n        return decode_bbox_target(logits, self.boxes)\n\n    @under_name_scope()\n    def narrow_to(self, featuremap):\n        """"""\n        Slice anchors to the spatial size of this featuremap.\n        """"""\n        shape2d = tf.shape(featuremap)[2:]  # h,w\n        slice3d = tf.concat([shape2d, [-1]], axis=0)\n        slice4d = tf.concat([shape2d, [-1, -1]], axis=0)\n        boxes = tf.slice(self.boxes, [0, 0, 0, 0], slice4d)\n        gt_labels = tf.slice(self.gt_labels, [0, 0, 0], slice3d)\n        gt_boxes = tf.slice(self.gt_boxes, [0, 0, 0, 0], slice4d)\n        return RPNAnchors(boxes, gt_labels, gt_boxes)\n\n\nif __name__ == \'__main__\':\n    """"""\n    Demonstrate what\'s wrong with tf.image.crop_and_resize.\n    Also reported at https://github.com/tensorflow/tensorflow/issues/26278\n    """"""\n    import tensorflow.contrib.eager as tfe\n    tfe.enable_eager_execution()\n\n    # want to crop 2x2 out of a 5x5 image, and resize to 4x4\n    image = np.arange(25).astype(\'float32\').reshape(5, 5)\n    boxes = np.asarray([[1, 1, 3, 3]], dtype=\'float32\')\n    target = 4\n\n    print(crop_and_resize(\n        image[None, None, :, :], boxes, [0], target)[0][0])\n    """"""\n    Expected values:\n    4.5 5 5.5 6\n    7 7.5 8 8.5\n    9.5 10 10.5 11\n    12 12.5 13 13.5\n\n    You cannot easily get the above results with tf.image.crop_and_resize.\n    Try out yourself here:\n    """"""\n    print(tf.image.crop_and_resize(\n        image[None, :, :, None],\n        np.asarray([[1, 1, 2, 2]]) / 4.0, [0], [target, target])[0][:, :, 0])\n'"
examples/FasterRCNN/modeling/model_cascade.py,20,"b'import tensorflow as tf\n\nfrom tensorpack.tfutils import get_current_tower_context\n\nfrom config import config as cfg\nfrom utils.box_ops import pairwise_iou, area as tf_area\nfrom .model_box import clip_boxes\nfrom .model_frcnn import BoxProposals, FastRCNNHead, fastrcnn_outputs\n\n\nclass CascadeRCNNHead(object):\n    def __init__(self, proposals,\n                 roi_func, fastrcnn_head_func, gt_targets, image_shape2d,\n                 num_categories):\n        """"""\n        Args:\n            proposals: BoxProposals\n            roi_func (boxes -> features): a function to crop features with rois\n            fastrcnn_head_func (features -> features): the fastrcnn head to apply on the cropped features\n            gt_targets (gt_boxes, gt_labels):\n        """"""\n        for k, v in locals().items():\n            if k != \'self\':\n                setattr(self, k, v)\n        self.gt_boxes, self.gt_labels = gt_targets\n        del self.gt_targets\n\n        self.num_cascade_stages = len(cfg.CASCADE.IOUS)\n\n        self.training = get_current_tower_context().is_training\n        if self.training:\n            @tf.custom_gradient\n            def scale_gradient(x):\n                return x, lambda dy: dy * (1.0 / self.num_cascade_stages)\n            self.scale_gradient = scale_gradient\n        else:\n            self.scale_gradient = tf.identity\n\n        ious = cfg.CASCADE.IOUS\n        # It\'s unclear how to do >3 stages, so it does not make sense to implement them\n        assert self.num_cascade_stages == 3, ""Only 3-stage cascade was implemented!""\n        with tf.variable_scope(\'cascade_rcnn_stage1\'):\n            H1, B1 = self.run_head(self.proposals, 0)\n\n        with tf.variable_scope(\'cascade_rcnn_stage2\'):\n            B1_proposal = self.match_box_with_gt(B1, ious[1])\n            H2, B2 = self.run_head(B1_proposal, 1)\n\n        with tf.variable_scope(\'cascade_rcnn_stage3\'):\n            B2_proposal = self.match_box_with_gt(B2, ious[2])\n            H3, B3 = self.run_head(B2_proposal, 2)\n        self._cascade_boxes = [B1, B2, B3]\n        self._heads = [H1, H2, H3]\n\n    def run_head(self, proposals, stage):\n        """"""\n        Args:\n            proposals: BoxProposals\n            stage: 0, 1, 2\n\n        Returns:\n            FastRCNNHead\n            Nx4, updated boxes\n        """"""\n        reg_weights = tf.constant(cfg.CASCADE.BBOX_REG_WEIGHTS[stage], dtype=tf.float32)\n        pooled_feature = self.roi_func(proposals.boxes)  # N,C,S,S\n        pooled_feature = self.scale_gradient(pooled_feature)\n        head_feature = self.fastrcnn_head_func(\'head\', pooled_feature)\n        label_logits, box_logits = fastrcnn_outputs(\n            \'outputs\', head_feature, self.num_categories, class_agnostic_regression=True)\n        head = FastRCNNHead(proposals, box_logits, label_logits, self.gt_boxes, reg_weights)\n\n        refined_boxes = head.decoded_output_boxes_class_agnostic()\n        refined_boxes = clip_boxes(refined_boxes, self.image_shape2d)\n        if self.training:\n            refined_boxes = tf.boolean_mask(refined_boxes, tf_area(refined_boxes) > 0)\n        return head, tf.stop_gradient(refined_boxes, name=\'output_boxes\')\n\n    def match_box_with_gt(self, boxes, iou_threshold):\n        """"""\n        Args:\n            boxes: Nx4\n        Returns:\n            BoxProposals\n        """"""\n        if self.training:\n            with tf.name_scope(\'match_box_with_gt_{}\'.format(iou_threshold)):\n                iou = pairwise_iou(boxes, self.gt_boxes)  # NxM\n                max_iou_per_box = tf.reduce_max(iou, axis=1)  # N\n                best_iou_ind = tf.cond(tf.shape(iou)[1] > 0,\n                                       lambda: tf.argmax(iou, axis=1),   # #proposal, each in 0~m-1\n                                       lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.int64))\n                labels_per_box = tf.gather(self.gt_labels, best_iou_ind)\n                fg_mask = max_iou_per_box >= iou_threshold\n                fg_inds_wrt_gt = tf.boolean_mask(best_iou_ind, fg_mask)\n                labels_per_box = tf.stop_gradient(labels_per_box * tf.cast(fg_mask, tf.int64))\n                return BoxProposals(boxes, labels_per_box, fg_inds_wrt_gt)\n        else:\n            return BoxProposals(boxes)\n\n    def losses(self):\n        ret = []\n        for idx, head in enumerate(self._heads):\n            with tf.name_scope(\'cascade_loss_stage{}\'.format(idx + 1)):\n                ret.extend(head.losses())\n        return ret\n\n    def decoded_output_boxes(self):\n        """"""\n        Returns:\n            Nx#classx4\n        """"""\n        ret = self._cascade_boxes[-1]\n        ret = tf.expand_dims(ret, 1)     # class-agnostic\n        return tf.tile(ret, [1, self.num_categories + 1, 1])\n\n    def output_scores(self, name=None):\n        """"""\n        Returns:\n            Nx#class\n        """"""\n        scores = [head.output_scores(\'cascade_scores_stage{}\'.format(idx + 1))\n                  for idx, head in enumerate(self._heads)]\n        return tf.multiply(tf.add_n(scores), (1.0 / self.num_cascade_stages), name=name)\n'"
examples/FasterRCNN/modeling/model_fpn.py,47,"b'# -*- coding: utf-8 -*-\n\nimport itertools\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorpack.models import Conv2D, FixedUnPooling, MaxPooling, layer_register\nfrom tensorpack.tfutils.argscope import argscope\nfrom tensorpack.tfutils.scope_utils import under_name_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.tfutils.tower import get_current_tower_context\nfrom tensorpack.utils.argtools import memoized\n\nfrom config import config as cfg\nfrom utils.box_ops import area as tf_area\nfrom .backbone import GroupNorm\nfrom .model_box import roi_align\nfrom .model_rpn import generate_rpn_proposals, rpn_losses, get_all_anchors\n\n\n@layer_register(log_shape=True)\ndef fpn_model(features):\n    """"""\n    Args:\n        features ([tf.Tensor]): ResNet features c2-c5\n\n    Returns:\n        [tf.Tensor]: FPN features p2-p6\n    """"""\n    assert len(features) == 4, features\n    num_channel = cfg.FPN.NUM_CHANNEL\n\n    use_gn = cfg.FPN.NORM == \'GN\'\n\n    def upsample2x(name, x):\n        try:\n            resize = tf.compat.v2.image.resize_images\n            with tf.name_scope(name):\n                shp2d = tf.shape(x)[2:]\n                x = tf.transpose(x, [0, 2, 3, 1])\n                x = resize(x, shp2d * 2, \'nearest\')\n                x = tf.transpose(x, [0, 3, 1, 2])\n                return x\n        except AttributeError:\n            return FixedUnPooling(\n                name, x, 2, unpool_mat=np.ones((2, 2), dtype=\'float32\'),\n                data_format=\'channels_first\')\n\n    with argscope(Conv2D, data_format=\'channels_first\',\n                  activation=tf.identity, use_bias=True,\n                  kernel_initializer=tf.variance_scaling_initializer(scale=1.)):\n        lat_2345 = [Conv2D(\'lateral_1x1_c{}\'.format(i + 2), c, num_channel, 1)\n                    for i, c in enumerate(features)]\n        if use_gn:\n            lat_2345 = [GroupNorm(\'gn_c{}\'.format(i + 2), c) for i, c in enumerate(lat_2345)]\n        lat_sum_5432 = []\n        for idx, lat in enumerate(lat_2345[::-1]):\n            if idx == 0:\n                lat_sum_5432.append(lat)\n            else:\n                lat = lat + upsample2x(\'upsample_lat{}\'.format(6 - idx), lat_sum_5432[-1])\n                lat_sum_5432.append(lat)\n        p2345 = [Conv2D(\'posthoc_3x3_p{}\'.format(i + 2), c, num_channel, 3)\n                 for i, c in enumerate(lat_sum_5432[::-1])]\n        if use_gn:\n            p2345 = [GroupNorm(\'gn_p{}\'.format(i + 2), c) for i, c in enumerate(p2345)]\n        p6 = MaxPooling(\'maxpool_p6\', p2345[-1], pool_size=1, strides=2, data_format=\'channels_first\', padding=\'VALID\')\n        return p2345 + [p6]\n\n\n@under_name_scope()\ndef fpn_map_rois_to_levels(boxes):\n    """"""\n    Assign boxes to level 2~5.\n\n    Args:\n        boxes (nx4):\n\n    Returns:\n        [tf.Tensor]: 4 tensors for level 2-5. Each tensor is a vector of indices of boxes in its level.\n        [tf.Tensor]: 4 tensors, the gathered boxes in each level.\n\n    Be careful that the returned tensor could be empty.\n    """"""\n    sqrtarea = tf.sqrt(tf_area(boxes))\n    level = tf.cast(tf.floor(\n        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)\n\n    # RoI levels range from 2~5 (not 6)\n    level_ids = [\n        tf.where(level <= 2),\n        tf.where(tf.equal(level, 3)),   # == is not supported\n        tf.where(tf.equal(level, 4)),\n        tf.where(level >= 5)]\n    level_ids = [tf.reshape(x, [-1], name=\'roi_level{}_id\'.format(i + 2))\n                 for i, x in enumerate(level_ids)]\n    num_in_levels = [tf.size(x, name=\'num_roi_level{}\'.format(i + 2))\n                     for i, x in enumerate(level_ids)]\n    add_moving_summary(*num_in_levels)\n\n    level_boxes = [tf.gather(boxes, ids) for ids in level_ids]\n    return level_ids, level_boxes\n\n\n@under_name_scope()\ndef multilevel_roi_align(features, rcnn_boxes, resolution):\n    """"""\n    Args:\n        features ([tf.Tensor]): 4 FPN feature level 2-5\n        rcnn_boxes (tf.Tensor): nx4 boxes\n        resolution (int): output spatial resolution\n    Returns:\n        NxC x res x res\n    """"""\n    assert len(features) == 4, features\n    # Reassign rcnn_boxes to levels\n    level_ids, level_boxes = fpn_map_rois_to_levels(rcnn_boxes)\n    all_rois = []\n\n    # Crop patches from corresponding levels\n    for i, boxes, featuremap in zip(itertools.count(), level_boxes, features):\n        with tf.name_scope(\'roi_level{}\'.format(i + 2)):\n            boxes_on_featuremap = boxes * (1.0 / cfg.FPN.ANCHOR_STRIDES[i])\n            all_rois.append(roi_align(featuremap, boxes_on_featuremap, resolution))\n\n    # this can fail if using TF<=1.8 with MKL build\n    all_rois = tf.concat(all_rois, axis=0)  # NCHW\n    # Unshuffle to the original order, to match the original samples\n    level_id_perm = tf.concat(level_ids, axis=0)  # A permutation of 1~N\n    level_id_invert_perm = tf.invert_permutation(level_id_perm)\n    all_rois = tf.gather(all_rois, level_id_invert_perm, name=""output"")\n    return all_rois\n\n\ndef multilevel_rpn_losses(\n        multilevel_anchors, multilevel_label_logits, multilevel_box_logits):\n    """"""\n    Args:\n        multilevel_anchors: #lvl RPNAnchors\n        multilevel_label_logits: #lvl tensors of shape HxWxA\n        multilevel_box_logits: #lvl tensors of shape HxWxAx4\n\n    Returns:\n        label_loss, box_loss\n    """"""\n    num_lvl = len(cfg.FPN.ANCHOR_STRIDES)\n    assert len(multilevel_anchors) == num_lvl\n    assert len(multilevel_label_logits) == num_lvl\n    assert len(multilevel_box_logits) == num_lvl\n\n    losses = []\n    with tf.name_scope(\'rpn_losses\'):\n        for lvl in range(num_lvl):\n            anchors = multilevel_anchors[lvl]\n            label_loss, box_loss = rpn_losses(\n                anchors.gt_labels, anchors.encoded_gt_boxes(),\n                multilevel_label_logits[lvl], multilevel_box_logits[lvl],\n                name_scope=\'level{}\'.format(lvl + 2))\n            losses.extend([label_loss, box_loss])\n\n        total_label_loss = tf.add_n(losses[::2], name=\'label_loss\')\n        total_box_loss = tf.add_n(losses[1::2], name=\'box_loss\')\n        add_moving_summary(total_label_loss, total_box_loss)\n    return [total_label_loss, total_box_loss]\n\n\n@under_name_scope()\ndef generate_fpn_proposals(\n        multilevel_pred_boxes, multilevel_label_logits, image_shape2d):\n    """"""\n    Args:\n        multilevel_pred_boxes: #lvl HxWxAx4 boxes\n        multilevel_label_logits: #lvl tensors of shape HxWxA\n\n    Returns:\n        boxes: kx4 float\n        scores: k logits\n    """"""\n    num_lvl = len(cfg.FPN.ANCHOR_STRIDES)\n    assert len(multilevel_pred_boxes) == num_lvl\n    assert len(multilevel_label_logits) == num_lvl\n\n    training = get_current_tower_context().is_training\n    all_boxes = []\n    all_scores = []\n    if cfg.FPN.PROPOSAL_MODE == \'Level\':\n        fpn_nms_topk = cfg.RPN.TRAIN_PER_LEVEL_NMS_TOPK if training else cfg.RPN.TEST_PER_LEVEL_NMS_TOPK\n        for lvl in range(num_lvl):\n            with tf.name_scope(\'Lvl{}\'.format(lvl + 2)):\n                pred_boxes_decoded = multilevel_pred_boxes[lvl]\n\n                proposal_boxes, proposal_scores = generate_rpn_proposals(\n                    tf.reshape(pred_boxes_decoded, [-1, 4]),\n                    tf.reshape(multilevel_label_logits[lvl], [-1]),\n                    image_shape2d, fpn_nms_topk)\n                all_boxes.append(proposal_boxes)\n                all_scores.append(proposal_scores)\n\n        proposal_boxes = tf.concat(all_boxes, axis=0)  # nx4\n        proposal_scores = tf.concat(all_scores, axis=0)  # n\n        # Here we are different from Detectron.\n        # Detectron picks top-k within the batch, rather than within an image. However we do not have a batch.\n        proposal_topk = tf.minimum(tf.size(proposal_scores), fpn_nms_topk)\n        proposal_scores, topk_indices = tf.nn.top_k(proposal_scores, k=proposal_topk, sorted=False)\n        proposal_boxes = tf.gather(proposal_boxes, topk_indices, name=""all_proposals"")\n    else:\n        for lvl in range(num_lvl):\n            with tf.name_scope(\'Lvl{}\'.format(lvl + 2)):\n                pred_boxes_decoded = multilevel_pred_boxes[lvl]\n                all_boxes.append(tf.reshape(pred_boxes_decoded, [-1, 4]))\n                all_scores.append(tf.reshape(multilevel_label_logits[lvl], [-1]))\n        all_boxes = tf.concat(all_boxes, axis=0)\n        all_scores = tf.concat(all_scores, axis=0)\n        proposal_boxes, proposal_scores = generate_rpn_proposals(\n            all_boxes, all_scores, image_shape2d,\n            cfg.RPN.TRAIN_PRE_NMS_TOPK if training else cfg.RPN.TEST_PRE_NMS_TOPK,\n            cfg.RPN.TRAIN_POST_NMS_TOPK if training else cfg.RPN.TEST_POST_NMS_TOPK)\n\n    tf.sigmoid(proposal_scores, name=\'probs\')  # for visualization\n    return tf.stop_gradient(proposal_boxes, name=\'boxes\'), \\\n        tf.stop_gradient(proposal_scores, name=\'scores\')\n\n\n@memoized\ndef get_all_anchors_fpn(*, strides, sizes, ratios, max_size):\n    """"""\n    Returns:\n        [anchors]: each anchors is a SxSx NUM_ANCHOR_RATIOS x4 array.\n    """"""\n    assert len(strides) == len(sizes)\n    foas = []\n    for stride, size in zip(strides, sizes):\n        foa = get_all_anchors(stride=stride, sizes=(size,), ratios=ratios, max_size=max_size)\n        foas.append(foa)\n    return foas\n'"
examples/FasterRCNN/modeling/model_frcnn.py,89,"b'# -*- coding: utf-8 -*-\n# File: model_frcnn.py\n\nimport tensorflow as tf\n\nfrom tensorpack.models import Conv2D, FullyConnected, layer_register\nfrom tensorpack.tfutils.argscope import argscope\nfrom tensorpack.tfutils.common import get_tf_version_tuple\nfrom tensorpack.tfutils.scope_utils import under_name_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.utils.argtools import memoized_method\n\nfrom config import config as cfg\nfrom utils.box_ops import pairwise_iou\n\nfrom .model_box import decode_bbox_target, encode_bbox_target\nfrom .backbone import GroupNorm\n\n\n@under_name_scope()\ndef proposal_metrics(iou):\n    """"""\n    Add summaries for RPN proposals.\n\n    Args:\n        iou: nxm, #proposal x #gt\n    """"""\n    # find best roi for each gt, for summary only\n    best_iou = tf.reduce_max(iou, axis=0)\n    mean_best_iou = tf.reduce_mean(best_iou, name=\'best_iou_per_gt\')\n    summaries = [mean_best_iou]\n    with tf.device(\'/cpu:0\'):\n        for th in [0.3, 0.5]:\n            recall = tf.truediv(\n                tf.count_nonzero(best_iou >= th),\n                tf.size(best_iou, out_type=tf.int64),\n                name=\'recall_iou{}\'.format(th))\n            summaries.append(recall)\n    add_moving_summary(*summaries)\n\n\n@under_name_scope()\ndef sample_fast_rcnn_targets(boxes, gt_boxes, gt_labels):\n    """"""\n    Sample some boxes from all proposals for training.\n    #fg is guaranteed to be > 0, because ground truth boxes will be added as proposals.\n\n    Args:\n        boxes: nx4 region proposals, floatbox\n        gt_boxes: mx4, floatbox\n        gt_labels: m, int32\n\n    Returns:\n        A BoxProposals instance, with:\n            sampled_boxes: tx4 floatbox, the rois\n            sampled_labels: t int64 labels, in [0, #class). Positive means foreground.\n            fg_inds_wrt_gt: #fg indices, each in range [0, m-1].\n                It contains the matching GT of each foreground roi.\n    """"""\n    iou = pairwise_iou(boxes, gt_boxes)     # nxm\n    proposal_metrics(iou)\n\n    # add ground truth as proposals as well\n    boxes = tf.concat([boxes, gt_boxes], axis=0)    # (n+m) x 4\n    iou = tf.concat([iou, tf.eye(tf.shape(gt_boxes)[0])], axis=0)   # (n+m) x m\n    # #proposal=n+m from now on\n\n    def sample_fg_bg(iou):\n        fg_mask = tf.cond(tf.shape(iou)[1] > 0,\n                          lambda: tf.reduce_max(iou, axis=1) >= cfg.FRCNN.FG_THRESH,\n                          lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.bool))\n\n        fg_inds = tf.reshape(tf.where(fg_mask), [-1])\n        num_fg = tf.minimum(int(\n            cfg.FRCNN.BATCH_PER_IM * cfg.FRCNN.FG_RATIO),\n            tf.size(fg_inds), name=\'num_fg\')\n        fg_inds = tf.random_shuffle(fg_inds)[:num_fg]\n\n        bg_inds = tf.reshape(tf.where(tf.logical_not(fg_mask)), [-1])\n        num_bg = tf.minimum(\n            cfg.FRCNN.BATCH_PER_IM - num_fg,\n            tf.size(bg_inds), name=\'num_bg\')\n        bg_inds = tf.random_shuffle(bg_inds)[:num_bg]\n\n        add_moving_summary(num_fg, num_bg)\n        return fg_inds, bg_inds\n\n    fg_inds, bg_inds = sample_fg_bg(iou)\n    # fg,bg indices w.r.t proposals\n\n    best_iou_ind = tf.cond(tf.shape(iou)[1] > 0,\n                           lambda: tf.argmax(iou, axis=1),   # #proposal, each in 0~m-1\n                           lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.int64))\n    fg_inds_wrt_gt = tf.gather(best_iou_ind, fg_inds)   # num_fg\n\n    all_indices = tf.concat([fg_inds, bg_inds], axis=0)   # indices w.r.t all n+m proposal boxes\n    ret_boxes = tf.gather(boxes, all_indices)\n\n    ret_labels = tf.concat(\n        [tf.gather(gt_labels, fg_inds_wrt_gt),\n         tf.zeros_like(bg_inds, dtype=tf.int64)], axis=0)\n    # stop the gradient -- they are meant to be training targets\n    return BoxProposals(\n        tf.stop_gradient(ret_boxes, name=\'sampled_proposal_boxes\'),\n        tf.stop_gradient(ret_labels, name=\'sampled_labels\'),\n        tf.stop_gradient(fg_inds_wrt_gt))\n\n\n@layer_register(log_shape=True)\ndef fastrcnn_outputs(feature, num_categories, class_agnostic_regression=False):\n    """"""\n    Args:\n        feature (any shape):\n        num_categories (int):\n        class_agnostic_regression (bool): if True, regression to N x 1 x 4\n\n    Returns:\n        cls_logits: N x num_class classification logits\n        reg_logits: N x num_classx4 or Nx1x4 if class agnostic\n    """"""\n    num_classes = num_categories + 1\n    classification = FullyConnected(\n        \'class\', feature, num_classes,\n        kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n    num_classes_for_box = 1 if class_agnostic_regression else num_classes\n    box_regression = FullyConnected(\n        \'box\', feature, num_classes_for_box * 4,\n        kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n    box_regression = tf.reshape(box_regression, (-1, num_classes_for_box, 4), name=\'output_box\')\n    return classification, box_regression\n\n\n@under_name_scope()\ndef fastrcnn_losses(labels, label_logits, fg_boxes, fg_box_logits):\n    """"""\n    Args:\n        labels: n,\n        label_logits: nxC\n        fg_boxes: nfgx4, encoded\n        fg_box_logits: nfgxCx4 or nfgx1x4 if class agnostic\n\n    Returns:\n        label_loss, box_loss\n    """"""\n    label_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=labels, logits=label_logits)\n    label_loss = tf.reduce_mean(label_loss, name=\'label_loss\')\n\n    fg_inds = tf.where(labels > 0)[:, 0]\n    fg_labels = tf.gather(labels, fg_inds)\n    num_fg = tf.size(fg_inds, out_type=tf.int64)\n    empty_fg = tf.equal(num_fg, 0)\n    if int(fg_box_logits.shape[1]) > 1:\n        if get_tf_version_tuple() >= (1, 14):\n            fg_labels = tf.expand_dims(fg_labels, axis=1)  # nfg x 1\n            fg_box_logits = tf.gather(fg_box_logits, fg_labels, batch_dims=1)\n        else:\n            indices = tf.stack([tf.range(num_fg), fg_labels], axis=1)  # nfgx2\n            fg_box_logits = tf.gather_nd(fg_box_logits, indices)\n    fg_box_logits = tf.reshape(fg_box_logits, [-1, 4])  # nfg x 4\n\n    with tf.name_scope(\'label_metrics\'), tf.device(\'/cpu:0\'):\n        prediction = tf.argmax(label_logits, axis=1, name=\'label_prediction\')\n        correct = tf.cast(tf.equal(prediction, labels), tf.float32)  # boolean/integer gather is unavailable on GPU\n        accuracy = tf.reduce_mean(correct, name=\'accuracy\')\n        fg_label_pred = tf.argmax(tf.gather(label_logits, fg_inds), axis=1)\n        num_zero = tf.reduce_sum(tf.cast(tf.equal(fg_label_pred, 0), tf.int64), name=\'num_zero\')\n        false_negative = tf.where(\n            empty_fg, 0., tf.cast(tf.truediv(num_zero, num_fg), tf.float32), name=\'false_negative\')\n        fg_accuracy = tf.where(\n            empty_fg, 0., tf.reduce_mean(tf.gather(correct, fg_inds)), name=\'fg_accuracy\')\n\n    box_loss = tf.reduce_sum(tf.abs(fg_boxes - fg_box_logits))\n    box_loss = tf.truediv(\n        box_loss, tf.cast(tf.shape(labels)[0], tf.float32), name=\'box_loss\')\n\n    add_moving_summary(label_loss, box_loss, accuracy,\n                       fg_accuracy, false_negative, tf.cast(num_fg, tf.float32, name=\'num_fg_label\'))\n    return [label_loss, box_loss]\n\n\n@under_name_scope()\ndef fastrcnn_predictions(boxes, scores):\n    """"""\n    Generate final results from predictions of all proposals.\n\n    Args:\n        boxes: n#classx4 floatbox in float32\n        scores: nx#class\n\n    Returns:\n        boxes: Kx4\n        scores: K\n        labels: K\n    """"""\n    assert boxes.shape[1] == scores.shape[1]\n    boxes = tf.transpose(boxes, [1, 0, 2])[1:, :, :]  # #catxnx4\n    scores = tf.transpose(scores[:, 1:], [1, 0])  # #catxn\n\n    max_coord = tf.reduce_max(boxes)\n    filtered_ids = tf.where(scores > cfg.TEST.RESULT_SCORE_THRESH)  # Fx2\n    filtered_boxes = tf.gather_nd(boxes, filtered_ids)  # Fx4\n    filtered_scores = tf.gather_nd(scores, filtered_ids)  # F,\n    cls_per_box = tf.slice(filtered_ids, [0, 0], [-1, 1])\n    offsets = tf.cast(cls_per_box, tf.float32) * (max_coord + 1)  # F,1\n    nms_boxes = filtered_boxes + offsets\n    selection = tf.image.non_max_suppression(\n        nms_boxes,\n        filtered_scores,\n        cfg.TEST.RESULTS_PER_IM,\n        cfg.TEST.FRCNN_NMS_THRESH)\n    final_scores = tf.gather(filtered_scores, selection, name=\'scores\')\n    final_labels = tf.add(tf.gather(cls_per_box[:, 0], selection), 1, name=\'labels\')\n    final_boxes = tf.gather(filtered_boxes, selection, name=\'boxes\')\n    return final_boxes, final_scores, final_labels\n\n\n""""""\nFastRCNN heads for FPN:\n""""""\n\n\n@layer_register(log_shape=True)\ndef fastrcnn_2fc_head(feature):\n    """"""\n    Args:\n        feature (any shape):\n\n    Returns:\n        2D head feature\n    """"""\n    dim = cfg.FPN.FRCNN_FC_HEAD_DIM\n    init = tf.variance_scaling_initializer()\n    hidden = FullyConnected(\'fc6\', feature, dim, kernel_initializer=init, activation=tf.nn.relu)\n    hidden = FullyConnected(\'fc7\', hidden, dim, kernel_initializer=init, activation=tf.nn.relu)\n    return hidden\n\n\n@layer_register(log_shape=True)\ndef fastrcnn_Xconv1fc_head(feature, num_convs, norm=None):\n    """"""\n    Args:\n        feature (NCHW):\n        num_classes(int): num_category + 1\n        num_convs (int): number of conv layers\n        norm (str or None): either None or \'GN\'\n\n    Returns:\n        2D head feature\n    """"""\n    assert norm in [None, \'GN\'], norm\n    l = feature\n    with argscope(Conv2D, data_format=\'channels_first\',\n                  kernel_initializer=tf.variance_scaling_initializer(\n                      scale=2.0, mode=\'fan_out\',\n                      distribution=\'untruncated_normal\' if get_tf_version_tuple() >= (1, 12) else \'normal\')):\n        for k in range(num_convs):\n            l = Conv2D(\'conv{}\'.format(k), l, cfg.FPN.FRCNN_CONV_HEAD_DIM, 3, activation=tf.nn.relu)\n            if norm is not None:\n                l = GroupNorm(\'gn{}\'.format(k), l)\n        l = FullyConnected(\'fc\', l, cfg.FPN.FRCNN_FC_HEAD_DIM,\n                           kernel_initializer=tf.variance_scaling_initializer(), activation=tf.nn.relu)\n    return l\n\n\ndef fastrcnn_4conv1fc_head(*args, **kwargs):\n    return fastrcnn_Xconv1fc_head(*args, num_convs=4, **kwargs)\n\n\ndef fastrcnn_4conv1fc_gn_head(*args, **kwargs):\n    return fastrcnn_Xconv1fc_head(*args, num_convs=4, norm=\'GN\', **kwargs)\n\n\nclass BoxProposals(object):\n    """"""\n    A structure to manage box proposals and their relations with ground truth.\n    """"""\n    def __init__(self, boxes, labels=None, fg_inds_wrt_gt=None):\n        """"""\n        Args:\n            boxes: Nx4\n            labels: N, each in [0, #class), the true label for each input box\n            fg_inds_wrt_gt: #fg, each in [0, M)\n\n        The last four arguments could be None when not training.\n        """"""\n        for k, v in locals().items():\n            if k != \'self\' and v is not None:\n                setattr(self, k, v)\n\n    @memoized_method\n    def fg_inds(self):\n        """""" Returns: #fg indices in [0, N-1] """"""\n        return tf.reshape(tf.where(self.labels > 0), [-1], name=\'fg_inds\')\n\n    @memoized_method\n    def fg_boxes(self):\n        """""" Returns: #fg x4""""""\n        return tf.gather(self.boxes, self.fg_inds(), name=\'fg_boxes\')\n\n    @memoized_method\n    def fg_labels(self):\n        """""" Returns: #fg""""""\n        return tf.gather(self.labels, self.fg_inds(), name=\'fg_labels\')\n\n\nclass FastRCNNHead(object):\n    """"""\n    A class to process & decode inputs/outputs of a fastrcnn classification+regression head.\n    """"""\n    def __init__(self, proposals, box_logits, label_logits, gt_boxes, bbox_regression_weights):\n        """"""\n        Args:\n            proposals: BoxProposals\n            box_logits: Nx#classx4 or Nx1x4, the output of the head\n            label_logits: Nx#class, the output of the head\n            gt_boxes: Mx4\n            bbox_regression_weights: a 4 element tensor\n        """"""\n        for k, v in locals().items():\n            if k != \'self\' and v is not None:\n                setattr(self, k, v)\n        self._bbox_class_agnostic = int(box_logits.shape[1]) == 1\n        self._num_classes = box_logits.shape[1]\n\n    @memoized_method\n    def fg_box_logits(self):\n        """""" Returns: #fg x ? x 4 """"""\n        return tf.gather(self.box_logits, self.proposals.fg_inds(), name=\'fg_box_logits\')\n\n    @memoized_method\n    def losses(self):\n        encoded_fg_gt_boxes = encode_bbox_target(\n            tf.gather(self.gt_boxes, self.proposals.fg_inds_wrt_gt),\n            self.proposals.fg_boxes()) * self.bbox_regression_weights\n        return fastrcnn_losses(\n            self.proposals.labels, self.label_logits,\n            encoded_fg_gt_boxes, self.fg_box_logits()\n        )\n\n    @memoized_method\n    def decoded_output_boxes(self):\n        """""" Returns: N x #class x 4 """"""\n        anchors = tf.tile(tf.expand_dims(self.proposals.boxes, 1),\n                          [1, self._num_classes, 1])   # N x #class x 4\n        decoded_boxes = decode_bbox_target(\n            self.box_logits / self.bbox_regression_weights,\n            anchors\n        )\n        return decoded_boxes\n\n    @memoized_method\n    def decoded_output_boxes_for_true_label(self):\n        """""" Returns: Nx4 decoded boxes """"""\n        return self._decoded_output_boxes_for_label(self.proposals.labels)\n\n    @memoized_method\n    def decoded_output_boxes_for_predicted_label(self):\n        """""" Returns: Nx4 decoded boxes """"""\n        return self._decoded_output_boxes_for_label(self.predicted_labels())\n\n    @memoized_method\n    def decoded_output_boxes_for_label(self, labels):\n        assert not self._bbox_class_agnostic\n        indices = tf.stack([\n            tf.range(tf.size(labels, out_type=tf.int64)),\n            labels\n        ])\n        needed_logits = tf.gather_nd(self.box_logits, indices)\n        decoded = decode_bbox_target(\n            needed_logits / self.bbox_regression_weights,\n            self.proposals.boxes\n        )\n        return decoded\n\n    @memoized_method\n    def decoded_output_boxes_class_agnostic(self):\n        """""" Returns: Nx4 """"""\n        assert self._bbox_class_agnostic\n        box_logits = tf.reshape(self.box_logits, [-1, 4])\n        decoded = decode_bbox_target(\n            box_logits / self.bbox_regression_weights,\n            self.proposals.boxes\n        )\n        return decoded\n\n    @memoized_method\n    def output_scores(self, name=None):\n        """""" Returns: N x #class scores, summed to one for each box.""""""\n        return tf.nn.softmax(self.label_logits, name=name)\n\n    @memoized_method\n    def predicted_labels(self):\n        """""" Returns: N ints """"""\n        return tf.argmax(self.label_logits, axis=1, name=\'predicted_labels\')\n'"
examples/FasterRCNN/modeling/model_mrcnn.py,29,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom tensorpack.models import Conv2D, Conv2DTranspose, layer_register\nfrom tensorpack.tfutils.argscope import argscope\nfrom tensorpack.tfutils.common import get_tf_version_tuple\nfrom tensorpack.tfutils.scope_utils import under_name_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\n\nfrom .backbone import GroupNorm\nfrom config import config as cfg\n\n\n@under_name_scope()\ndef maskrcnn_loss(mask_logits, fg_labels, fg_target_masks):\n    """"""\n    Args:\n        mask_logits: #fg x #category xhxw\n        fg_labels: #fg, in 1~#class, int64\n        fg_target_masks: #fgxhxw, float32\n    """"""\n    if get_tf_version_tuple() >= (1, 14):\n        mask_logits = tf.gather(\n            mask_logits, tf.reshape(fg_labels - 1, [-1, 1]), batch_dims=1)\n        mask_logits = tf.squeeze(mask_logits, axis=1)\n    else:\n        indices = tf.stack([tf.range(tf.size(fg_labels, out_type=tf.int64)),\n                            fg_labels - 1], axis=1)  # #fgx2\n        mask_logits = tf.gather_nd(mask_logits, indices)  # #fg x h x w\n\n    mask_probs = tf.sigmoid(mask_logits)\n\n    # add some training visualizations to tensorboard\n    with tf.name_scope(\'mask_viz\'):\n        viz = tf.concat([fg_target_masks, mask_probs], axis=1)\n        viz = tf.expand_dims(viz, 3)\n        viz = tf.cast(viz * 255, tf.uint8, name=\'viz\')\n        tf.summary.image(\'mask_truth|pred\', viz, max_outputs=10)\n\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=fg_target_masks, logits=mask_logits)\n    loss = tf.reduce_mean(loss, name=\'maskrcnn_loss\')\n\n    pred_label = mask_probs > 0.5\n    truth_label = fg_target_masks > 0.5\n    accuracy = tf.reduce_mean(\n        tf.cast(tf.equal(pred_label, truth_label), tf.float32),\n        name=\'accuracy\')\n    pos_accuracy = tf.logical_and(\n        tf.equal(pred_label, truth_label),\n        tf.equal(truth_label, True))\n    pos_accuracy = tf.reduce_mean(tf.cast(pos_accuracy, tf.float32), name=\'pos_accuracy\')\n    fg_pixel_ratio = tf.reduce_mean(tf.cast(truth_label, tf.float32), name=\'fg_pixel_ratio\')\n\n    add_moving_summary(loss, accuracy, fg_pixel_ratio, pos_accuracy)\n    return loss\n\n\n@layer_register(log_shape=True)\ndef maskrcnn_upXconv_head(feature, num_category, num_convs, norm=None):\n    """"""\n    Args:\n        feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models.\n        num_category(int):\n        num_convs (int): number of convolution layers\n        norm (str or None): either None or \'GN\'\n\n    Returns:\n        mask_logits (N x num_category x 2s x 2s):\n    """"""\n    assert norm in [None, \'GN\'], norm\n    l = feature\n    with argscope([Conv2D, Conv2DTranspose], data_format=\'channels_first\',\n                  kernel_initializer=tf.variance_scaling_initializer(\n                      scale=2.0, mode=\'fan_out\',\n                      distribution=\'untruncated_normal\' if get_tf_version_tuple() >= (1, 12) else \'normal\')):\n        # c2\'s MSRAFill is fan_out\n        for k in range(num_convs):\n            l = Conv2D(\'fcn{}\'.format(k), l, cfg.MRCNN.HEAD_DIM, 3, activation=tf.nn.relu)\n            if norm is not None:\n                l = GroupNorm(\'gn{}\'.format(k), l)\n        l = Conv2DTranspose(\'deconv\', l, cfg.MRCNN.HEAD_DIM, 2, strides=2, activation=tf.nn.relu)\n        l = Conv2D(\'conv\', l, num_category, 1, kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n    return l\n\n\ndef maskrcnn_up4conv_head(*args, **kwargs):\n    return maskrcnn_upXconv_head(*args, num_convs=4, **kwargs)\n\n\ndef maskrcnn_up4conv_gn_head(*args, **kwargs):\n    return maskrcnn_upXconv_head(*args, num_convs=4, norm=\'GN\', **kwargs)\n\n\ndef unpackbits_masks(masks):\n    """"""\n    Args:\n        masks (Tensor): uint8 Tensor of shape N, H, W. The last dimension is packed bits.\n\n    Returns:\n        masks (Tensor): bool Tensor of shape N, H, 8*W.\n\n    This is a reverse operation of `np.packbits`\n    """"""\n    assert masks.dtype == tf.uint8, masks\n    bits = tf.constant((128, 64, 32, 16, 8, 4, 2, 1), dtype=tf.uint8)\n    unpacked = tf.bitwise.bitwise_and(tf.expand_dims(masks, -1), bits) > 0\n    unpacked = tf.reshape(\n        unpacked,\n        tf.concat([tf.shape(masks)[:-1], [8 * tf.shape(masks)[-1]]], axis=0))\n    return unpacked\n'"
examples/FasterRCNN/modeling/model_rpn.py,50,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorpack.models import Conv2D, layer_register\nfrom tensorpack.tfutils.argscope import argscope\nfrom tensorpack.tfutils.scope_utils import auto_reuse_variable_scope, under_name_scope\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.utils.argtools import memoized\n\nfrom config import config as cfg\nfrom .model_box import clip_boxes\n\n\n@layer_register(log_shape=True)\n@auto_reuse_variable_scope\ndef rpn_head(featuremap, channel, num_anchors):\n    """"""\n    Returns:\n        label_logits: fHxfWxNA\n        box_logits: fHxfWxNAx4\n    """"""\n    with argscope(Conv2D, data_format=\'channels_first\',\n                  kernel_initializer=tf.random_normal_initializer(stddev=0.01)):\n        hidden = Conv2D(\'conv0\', featuremap, channel, 3, activation=tf.nn.relu)\n\n        label_logits = Conv2D(\'class\', hidden, num_anchors, 1)\n        box_logits = Conv2D(\'box\', hidden, 4 * num_anchors, 1)\n        # 1, NA(*4), im/16, im/16 (NCHW)\n\n        label_logits = tf.transpose(label_logits, [0, 2, 3, 1])  # 1xfHxfWxNA\n        label_logits = tf.squeeze(label_logits, 0)  # fHxfWxNA\n\n        shp = tf.shape(box_logits)  # 1x(NAx4)xfHxfW\n        box_logits = tf.transpose(box_logits, [0, 2, 3, 1])  # 1xfHxfWx(NAx4)\n        box_logits = tf.reshape(box_logits, tf.stack([shp[2], shp[3], num_anchors, 4]))  # fHxfWxNAx4\n    return label_logits, box_logits\n\n\n@under_name_scope()\ndef rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):\n    """"""\n    Args:\n        anchor_labels: fHxfWxNA\n        anchor_boxes: fHxfWxNAx4, encoded\n        label_logits:  fHxfWxNA\n        box_logits: fHxfWxNAx4\n\n    Returns:\n        label_loss, box_loss\n    """"""\n    with tf.device(\'/cpu:0\'):\n        valid_mask = tf.stop_gradient(tf.not_equal(anchor_labels, -1))\n        pos_mask = tf.stop_gradient(tf.equal(anchor_labels, 1))\n        nr_valid = tf.stop_gradient(tf.count_nonzero(valid_mask, dtype=tf.int32), name=\'num_valid_anchor\')\n        nr_pos = tf.identity(tf.count_nonzero(pos_mask, dtype=tf.int32), name=\'num_pos_anchor\')\n        # nr_pos is guaranteed >0 in C4. But in FPN. even nr_valid could be 0.\n\n        valid_anchor_labels = tf.boolean_mask(anchor_labels, valid_mask)\n    valid_label_logits = tf.boolean_mask(label_logits, valid_mask)\n\n    with tf.name_scope(\'label_metrics\'):\n        valid_label_prob = tf.nn.sigmoid(valid_label_logits)\n        summaries = []\n        with tf.device(\'/cpu:0\'):\n            for th in [0.5, 0.2, 0.1]:\n                valid_prediction = tf.cast(valid_label_prob > th, tf.int32)\n                nr_pos_prediction = tf.reduce_sum(valid_prediction, name=\'num_pos_prediction\')\n                pos_prediction_corr = tf.count_nonzero(\n                    tf.logical_and(\n                        valid_label_prob > th,\n                        tf.equal(valid_prediction, valid_anchor_labels)),\n                    dtype=tf.int32)\n                placeholder = 0.5   # A small value will make summaries appear lower.\n                recall = tf.cast(tf.truediv(pos_prediction_corr, nr_pos), tf.float32)\n                recall = tf.where(tf.equal(nr_pos, 0), placeholder, recall, name=\'recall_th{}\'.format(th))\n                precision = tf.cast(tf.truediv(pos_prediction_corr, nr_pos_prediction), tf.float32)\n                precision = tf.where(tf.equal(nr_pos_prediction, 0),\n                                     placeholder, precision, name=\'precision_th{}\'.format(th))\n                summaries.extend([precision, recall])\n        add_moving_summary(*summaries)\n\n    # Per-level loss summaries in FPN may appear lower due to the use of a small placeholder.\n    # But the total RPN loss will be fine.  TODO make the summary op smarter\n    placeholder = 0.\n    label_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.cast(valid_anchor_labels, tf.float32), logits=valid_label_logits)\n    label_loss = tf.reduce_sum(label_loss) * (1. / cfg.RPN.BATCH_PER_IM)\n    label_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name=\'label_loss\')\n\n    pos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)\n    pos_box_logits = tf.boolean_mask(box_logits, pos_mask)\n    delta = 1.0 / 9\n    box_loss = tf.losses.huber_loss(\n        pos_anchor_boxes, pos_box_logits, delta=delta,\n        reduction=tf.losses.Reduction.SUM) / delta\n    box_loss = box_loss * (1. / cfg.RPN.BATCH_PER_IM)\n    box_loss = tf.where(tf.equal(nr_pos, 0), placeholder, box_loss, name=\'box_loss\')\n\n    add_moving_summary(label_loss, box_loss, nr_valid, nr_pos)\n    return [label_loss, box_loss]\n\n\n@under_name_scope()\ndef generate_rpn_proposals(boxes, scores, img_shape,\n                           pre_nms_topk, post_nms_topk=None):\n    """"""\n    Sample RPN proposals by the following steps:\n    1. Pick top k1 by scores\n    2. NMS them\n    3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output.\n\n    Args:\n        boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already\n        scores: n float, the logits\n        img_shape: [h, w]\n        pre_nms_topk, post_nms_topk (int): See above.\n\n    Returns:\n        boxes: kx4 float\n        scores: k logits\n    """"""\n    assert boxes.shape.ndims == 2, boxes.shape\n    if post_nms_topk is None:\n        post_nms_topk = pre_nms_topk\n\n    topk = tf.minimum(pre_nms_topk, tf.size(scores))\n    topk_scores, topk_indices = tf.nn.top_k(scores, k=topk, sorted=False)\n    topk_boxes = tf.gather(boxes, topk_indices)\n    topk_boxes = clip_boxes(topk_boxes, img_shape)\n\n    if cfg.RPN.MIN_SIZE > 0:\n        topk_boxes_x1y1x2y2 = tf.reshape(topk_boxes, (-1, 2, 2))\n        topk_boxes_x1y1, topk_boxes_x2y2 = tf.split(topk_boxes_x1y1x2y2, 2, axis=1)\n        # nx1x2 each\n        wbhb = tf.squeeze(topk_boxes_x2y2 - topk_boxes_x1y1, axis=1)\n        valid = tf.reduce_all(wbhb > cfg.RPN.MIN_SIZE, axis=1)  # n,\n        topk_valid_boxes = tf.boolean_mask(topk_boxes, valid)\n        topk_valid_scores = tf.boolean_mask(topk_scores, valid)\n    else:\n        topk_valid_boxes = topk_boxes\n        topk_valid_scores = topk_scores\n\n    nms_indices = tf.image.non_max_suppression(\n        topk_valid_boxes,\n        topk_valid_scores,\n        max_output_size=post_nms_topk,\n        iou_threshold=cfg.RPN.PROPOSAL_NMS_THRESH)\n\n    proposal_boxes = tf.gather(topk_valid_boxes, nms_indices)\n    proposal_scores = tf.gather(topk_valid_scores, nms_indices)\n    tf.sigmoid(proposal_scores, name=\'probs\')  # for visualization\n    return tf.stop_gradient(proposal_boxes, name=\'boxes\'), tf.stop_gradient(proposal_scores, name=\'scores\')\n\n\n@memoized\ndef get_all_anchors(*, stride, sizes, ratios, max_size):\n    """"""\n    Get all anchors in the largest possible image, shifted, floatbox\n    Args:\n        stride (int): the stride of anchors.\n        sizes (tuple[int]): the sizes (sqrt area) of anchors\n        ratios (tuple[int]): the aspect ratios of anchors\n        max_size (int): maximum size of input image\n\n    Returns:\n        anchors: SxSxNUM_ANCHORx4, where S == ceil(MAX_SIZE/STRIDE), floatbox\n        The layout in the NUM_ANCHOR dim is NUM_RATIO x NUM_SIZE.\n\n    """"""\n    # Generates a NAx4 matrix of anchor boxes in (x1, y1, x2, y2) format. Anchors\n    # are centered on 0, have sqrt areas equal to the specified sizes, and aspect ratios as given.\n    anchors = []\n    for sz in sizes:\n        for ratio in ratios:\n            w = np.sqrt(sz * sz / ratio)\n            h = ratio * w\n            anchors.append([-w, -h, w, h])\n    cell_anchors = np.asarray(anchors) * 0.5\n\n    field_size = int(np.ceil(max_size / stride))\n    shifts = (np.arange(0, field_size) * stride).astype(""float32"")\n    shift_x, shift_y = np.meshgrid(shifts, shifts)\n    shift_x = shift_x.flatten()\n    shift_y = shift_y.flatten()\n    shifts = np.vstack((shift_x, shift_y, shift_x, shift_y)).transpose()\n    # Kx4, K = field_size * field_size\n    K = shifts.shape[0]\n\n    A = cell_anchors.shape[0]\n    field_of_anchors = cell_anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n    field_of_anchors = field_of_anchors.reshape((field_size, field_size, A, 4))\n    # FSxFSxAx4\n    # Many rounding happens inside the anchor code anyway\n    # assert np.all(field_of_anchors == field_of_anchors.astype(\'int32\'))\n    field_of_anchors = field_of_anchors.astype(""float32"")\n    return field_of_anchors\n'"
examples/FasterRCNN/utils/__init__.py,0,b''
examples/FasterRCNN/utils/box_ops.py,14,"b'# -*- coding: utf-8 -*-\n# File: box_ops.py\n\nimport tensorflow as tf\n\nfrom tensorpack.tfutils.scope_utils import under_name_scope\n\n\n""""""\nThis file is modified from\nhttps://github.com/tensorflow/models/blob/master/object_detection/core/box_list_ops.py\n""""""\n\n\n@under_name_scope()\ndef area(boxes):\n    """"""\n    Args:\n      boxes: nx4 floatbox\n\n    Returns:\n      n\n    """"""\n    x_min, y_min, x_max, y_max = tf.split(boxes, 4, axis=1)\n    return tf.squeeze((y_max - y_min) * (x_max - x_min), [1])\n\n\n@under_name_scope()\ndef pairwise_intersection(boxlist1, boxlist2):\n    """"""Compute pairwise intersection areas between boxes.\n\n    Args:\n      boxlist1: Nx4 floatbox\n      boxlist2: Mx4\n\n    Returns:\n      a tensor with shape [N, M] representing pairwise intersections\n    """"""\n    x_min1, y_min1, x_max1, y_max1 = tf.split(boxlist1, 4, axis=1)\n    x_min2, y_min2, x_max2, y_max2 = tf.split(boxlist2, 4, axis=1)\n    all_pairs_min_ymax = tf.minimum(y_max1, tf.transpose(y_max2))\n    all_pairs_max_ymin = tf.maximum(y_min1, tf.transpose(y_min2))\n    intersect_heights = tf.maximum(0.0, all_pairs_min_ymax - all_pairs_max_ymin)\n    all_pairs_min_xmax = tf.minimum(x_max1, tf.transpose(x_max2))\n    all_pairs_max_xmin = tf.maximum(x_min1, tf.transpose(x_min2))\n    intersect_widths = tf.maximum(0.0, all_pairs_min_xmax - all_pairs_max_xmin)\n    return intersect_heights * intersect_widths\n\n\n@under_name_scope()\ndef pairwise_iou(boxlist1, boxlist2):\n    """"""Computes pairwise intersection-over-union between box collections.\n\n    Args:\n      boxlist1: Nx4 floatbox\n      boxlist2: Mx4\n\n    Returns:\n      a tensor with shape [N, M] representing pairwise iou scores.\n    """"""\n    intersections = pairwise_intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = (\n        tf.expand_dims(areas1, 1) + tf.expand_dims(areas2, 0) - intersections)\n    return tf.where(\n        tf.equal(intersections, 0.0),\n        tf.zeros_like(intersections), tf.truediv(intersections, unions))\n'"
examples/FasterRCNN/utils/np_box_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Operations for [N, 4] numpy arrays representing bounding boxes.\n\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\n\ndef area(boxes):\n  """"""Computes area of boxes.\n\n  Args:\n    boxes: Numpy array with shape [N, 4] holding N boxes\n\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  """"""\n  return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n\n\ndef intersection(boxes1, boxes2):\n  """"""Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes\n    boxes2: a numpy array with shape [M, 4] holding M boxes\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  """"""\n  [y_min1, x_min1, y_max1, x_max1] = np.split(boxes1, 4, axis=1)\n  [y_min2, x_min2, y_max2, x_max2] = np.split(boxes2, 4, axis=1)\n\n  all_pairs_min_ymax = np.minimum(y_max1, np.transpose(y_max2))\n  all_pairs_max_ymin = np.maximum(y_min1, np.transpose(y_min2))\n  intersect_heights = np.maximum(\n      np.zeros(all_pairs_max_ymin.shape, dtype=\'f4\'),\n      all_pairs_min_ymax - all_pairs_max_ymin)\n  all_pairs_min_xmax = np.minimum(x_max1, np.transpose(x_max2))\n  all_pairs_max_xmin = np.maximum(x_min1, np.transpose(x_min2))\n  intersect_widths = np.maximum(\n      np.zeros(all_pairs_max_xmin.shape, dtype=\'f4\'),\n      all_pairs_min_xmax - all_pairs_max_xmin)\n  return intersect_heights * intersect_widths\n\n\ndef iou(boxes1, boxes2):\n  """"""Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding M boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  """"""\n  intersect = intersection(boxes1, boxes2)\n  area1 = area(boxes1)\n  area2 = area(boxes2)\n  union = np.expand_dims(area1, axis=1) + np.expand_dims(\n      area2, axis=0) - intersect\n  return intersect / union\n\n\ndef ioa(boxes1, boxes2):\n  """"""Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  """"""\n  intersect = intersection(boxes1, boxes2)\n  inv_areas = np.expand_dims(1.0 / area(boxes2), axis=0)\n  return intersect * inv_areas\n'"
tensorpack/dataflow/dataset/__init__.py,0,"b""#  -*- coding: utf-8 -*-\n#  File: __init__.py\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()['kcah_acitats'[::-1].upper()] = False\nif STATICA_HACK:\n    from .bsds500 import *\n    from .cifar import *\n    from .ilsvrc import *\n    from .mnist import *\n    from .svhn import *\n    from .caltech101 import *\n\nfrom pkgutil import iter_modules\nimport os\nimport os.path\n\n__all__ = []\n\n\ndef global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if '__all__' in dir(p) else dir(p)\n    if lst:\n        del globals()[name]\n        for k in lst:\n            if not k.startswith('__'):\n                globals()[k] = p.__dict__[k]\n                __all__.append(k)\n\n\n_CURR_DIR = os.path.dirname(__file__)\nfor _, module_name, _ in iter_modules(\n        [_CURR_DIR]):\n    srcpath = os.path.join(_CURR_DIR, module_name + '.py')\n    if not os.path.isfile(srcpath):\n        continue\n    if not module_name.startswith('_'):\n        global_import(module_name)\n"""
tensorpack/dataflow/dataset/bsds500.py,0,"b'# -*- coding: utf-8 -*-\n# File: bsds500.py\n\n\nimport glob\nimport numpy as np\nimport os\n\nfrom ...utils.fs import download, get_dataset_path\nfrom ..base import RNGDataFlow\n\n__all__ = [\'BSDS500\']\n\n\nDATA_URL = ""http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz""\nDATA_SIZE = 70763455\nIMG_W, IMG_H = 481, 321\n\n\nclass BSDS500(RNGDataFlow):\n    """"""\n    `Berkeley Segmentation Data Set and Benchmarks 500 dataset\n    <http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html#bsds500>`_.\n\n    Produce ``(image, label)`` pair, where ``image`` has shape (321, 481, 3(BGR)) and\n    ranges in [0,255].\n    ``Label`` is a floating point image of shape (321, 481) in range [0, 1].\n    The value of each pixel is ``number of times it is annotated as edge / total number of annotators for this image``.\n    """"""\n\n    def __init__(self, name, data_dir=None, shuffle=True):\n        """"""\n        Args:\n            name (str): \'train\', \'test\', \'val\'\n            data_dir (str): a directory containing the original \'BSR\' directory.\n        """"""\n        # check and download data\n        if data_dir is None:\n            data_dir = get_dataset_path(\'bsds500_data\')\n        if not os.path.isdir(os.path.join(data_dir, \'BSR\')):\n            download(DATA_URL, data_dir, expect_size=DATA_SIZE)\n            filename = DATA_URL.split(\'/\')[-1]\n            filepath = os.path.join(data_dir, filename)\n            import tarfile\n            tarfile.open(filepath, \'r:gz\').extractall(data_dir)\n        self.data_root = os.path.join(data_dir, \'BSR\', \'BSDS500\', \'data\')\n        assert os.path.isdir(self.data_root)\n\n        self.shuffle = shuffle\n        assert name in [\'train\', \'test\', \'val\']\n        self._load(name)\n\n    def _load(self, name):\n        image_glob = os.path.join(self.data_root, \'images\', name, \'*.jpg\')\n        image_files = glob.glob(image_glob)\n        gt_dir = os.path.join(self.data_root, \'groundTruth\', name)\n        self.data = np.zeros((len(image_files), IMG_H, IMG_W, 3), dtype=\'uint8\')\n        self.label = np.zeros((len(image_files), IMG_H, IMG_W), dtype=\'float32\')\n\n        for idx, f in enumerate(image_files):\n            im = cv2.imread(f, cv2.IMREAD_COLOR)\n            assert im is not None\n            if im.shape[0] > im.shape[1]:\n                im = np.transpose(im, (1, 0, 2))\n            assert im.shape[:2] == (IMG_H, IMG_W), ""{} != {}"".format(im.shape[:2], (IMG_H, IMG_W))\n\n            imgid = os.path.basename(f).split(\'.\')[0]\n            gt_file = os.path.join(gt_dir, imgid)\n            gt = loadmat(gt_file)[\'groundTruth\'][0]\n            n_annot = gt.shape[0]\n            gt = sum(gt[k][\'Boundaries\'][0][0] for k in range(n_annot))\n            gt = gt.astype(\'float32\')\n            gt *= 1.0 / n_annot\n            if gt.shape[0] > gt.shape[1]:\n                gt = gt.transpose()\n            assert gt.shape == (IMG_H, IMG_W)\n\n            self.data[idx] = im\n            self.label[idx] = gt\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __iter__(self):\n        idxs = np.arange(self.data.shape[0])\n        if self.shuffle:\n            self.rng.shuffle(idxs)\n        for k in idxs:\n            yield [self.data[k], self.label[k]]\n\n\ntry:\n    from scipy.io import loadmat\n    import cv2\nexcept ImportError:\n    from ...utils.develop import create_dummy_class\n    BSDS500 = create_dummy_class(\'BSDS500\', [\'scipy.io\', \'cv2\'])  # noqa\n\nif __name__ == \'__main__\':\n    a = BSDS500(\'val\')\n    a.reset_state()\n    for k in a:\n        cv2.imshow(""haha"", k[1].astype(\'uint8\') * 255)\n        cv2.waitKey(1000)\n'"
tensorpack/dataflow/dataset/caltech101.py,0,"b'# -*- coding: utf-8 -*-\n# File: caltech101.py\n\n\nimport os\n\nfrom ...utils import logger\nfrom ...utils.fs import download, get_dataset_path\nfrom ..base import RNGDataFlow\n\n__all__ = [""Caltech101Silhouettes""]\n\n\ndef maybe_download(url, work_directory):\n    """"""Download the data from Marlin\'s website, unless it\'s already here.""""""\n    filename = url.split(""/"")[-1]\n    filepath = os.path.join(work_directory, filename)\n    if not os.path.exists(filepath):\n        logger.info(""Downloading to {}..."".format(filepath))\n        download(url, work_directory)\n    return filepath\n\n\nclass Caltech101Silhouettes(RNGDataFlow):\n    """"""\n    Produces [image, label] in Caltech101 Silhouettes dataset,\n    image is 28x28 in the range [0,1], label is an int in the range [0,100].\n    """"""\n\n    _DIR_NAME = ""caltech101_data""\n    _SOURCE_URL = ""https://people.cs.umass.edu/~marlin/data/""\n\n    def __init__(self, name, shuffle=True, dir=None):\n        """"""\n        Args:\n            name (str): \'train\', \'test\', \'val\'\n            shuffle (bool): shuffle the dataset\n        """"""\n        if dir is None:\n            dir = get_dataset_path(self._DIR_NAME)\n        assert name in [\'train\', \'test\', \'val\']\n        self.name = name\n        self.shuffle = shuffle\n\n        def get_images_and_labels(data_file):\n            f = maybe_download(self._SOURCE_URL + data_file, dir)\n            data = scipy.io.loadmat(f)\n            return data\n\n        self.data = get_images_and_labels(""caltech101_silhouettes_28_split1.mat"")\n\n        if self.name == ""train"":\n            self.images = self.data[""train_data""].reshape((4100, 28, 28))\n            self.labels = self.data[""train_labels""].ravel() - 1\n        elif self.name == ""test"":\n            self.images = self.data[""test_data""].reshape((2307, 28, 28))\n            self.labels = self.data[""test_labels""].ravel() - 1\n        else:\n            self.images = self.data[""val_data""].reshape((2264, 28, 28))\n            self.labels = self.data[""val_labels""].ravel() - 1\n\n    def __len__(self):\n        return self.images.shape[0]\n\n    def __iter__(self):\n        idxs = list(range(self.__len__()))\n        if self.shuffle:\n            self.rng.shuffle(idxs)\n        for k in idxs:\n            img = self.images[k]\n            label = self.labels[k]\n            yield [img, label]\n\n\ntry:\n    import scipy.io\nexcept ImportError:\n    from ...utils.develop import create_dummy_class\n    Caltech101Silhouettes = create_dummy_class(\'Caltech101Silhouettes\', \'scipy.io\') # noqa\n\n\nif __name__ == ""__main__"":\n    ds = Caltech101Silhouettes(""train"")\n    ds.reset_state()\n    for _ in ds:\n        from IPython import embed\n\n        embed()\n        break\n'"
tensorpack/dataflow/dataset/cifar.py,0,"b'# -*- coding: utf-8 -*-\n# File: cifar.py\n\n#         Yukun Chen <cykustc@gmail.com>\n\nimport numpy as np\nimport os\nimport pickle\nimport tarfile\n\nfrom ...utils import logger\nfrom ...utils.fs import download, get_dataset_path\nfrom ..base import RNGDataFlow\n\n__all__ = [\'CifarBase\', \'Cifar10\', \'Cifar100\']\n\n\nDATA_URL_CIFAR_10 = (\'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\', 170498071)\nDATA_URL_CIFAR_100 = (\'http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\', 169001437)\n\n\ndef maybe_download_and_extract(dest_directory, cifar_classnum):\n    """"""Download and extract the tarball from Alex\'s website. Copied from tensorflow example """"""\n    assert cifar_classnum == 10 or cifar_classnum == 100\n    if cifar_classnum == 10:\n        cifar_foldername = \'cifar-10-batches-py\'\n    else:\n        cifar_foldername = \'cifar-100-python\'\n    if os.path.isdir(os.path.join(dest_directory, cifar_foldername)):\n        logger.info(""Found cifar{} data in {}."".format(cifar_classnum, dest_directory))\n        return\n    else:\n        DATA_URL = DATA_URL_CIFAR_10 if cifar_classnum == 10 else DATA_URL_CIFAR_100\n        filename = DATA_URL[0].split(\'/\')[-1]\n        filepath = os.path.join(dest_directory, filename)\n        download(DATA_URL[0], dest_directory, expect_size=DATA_URL[1])\n        tarfile.open(filepath, \'r:gz\').extractall(dest_directory)\n\n\ndef read_cifar(filenames, cifar_classnum):\n    assert cifar_classnum == 10 or cifar_classnum == 100\n    ret = []\n    for fname in filenames:\n        fo = open(fname, \'rb\')\n        dic = pickle.load(fo, encoding=\'bytes\')\n        data = dic[b\'data\']\n        if cifar_classnum == 10:\n            label = dic[b\'labels\']\n            IMG_NUM = 10000  # cifar10 data are split into blocks of 10000\n        else:\n            label = dic[b\'fine_labels\']\n            IMG_NUM = 50000 if \'train\' in fname else 10000\n        fo.close()\n        for k in range(IMG_NUM):\n            img = data[k].reshape(3, 32, 32)\n            img = np.transpose(img, [1, 2, 0])\n            ret.append([img, label[k]])\n    return ret\n\n\ndef get_filenames(dir, cifar_classnum):\n    assert cifar_classnum == 10 or cifar_classnum == 100\n    if cifar_classnum == 10:\n        train_files = [os.path.join(\n            dir, \'cifar-10-batches-py\', \'data_batch_%d\' % i) for i in range(1, 6)]\n        test_files = [os.path.join(\n            dir, \'cifar-10-batches-py\', \'test_batch\')]\n        meta_file = os.path.join(dir, \'cifar-10-batches-py\', \'batches.meta\')\n    elif cifar_classnum == 100:\n        train_files = [os.path.join(dir, \'cifar-100-python\', \'train\')]\n        test_files = [os.path.join(dir, \'cifar-100-python\', \'test\')]\n        meta_file = os.path.join(dir, \'cifar-100-python\', \'meta\')\n    return train_files, test_files, meta_file\n\n\ndef _parse_meta(filename, cifar_classnum):\n    with open(filename, \'rb\') as f:\n        obj = pickle.load(f)\n        return obj[\'label_names\' if cifar_classnum == 10 else \'fine_label_names\']\n\n\nclass CifarBase(RNGDataFlow):\n    """"""\n    Produces [image, label] in Cifar10/100 dataset,\n    image is 32x32x3 in the range [0,255].\n    label is an int.\n    """"""\n    def __init__(self, train_or_test, shuffle=None, dir=None, cifar_classnum=10):\n        """"""\n        Args:\n            train_or_test (str): \'train\' or \'test\'\n            shuffle (bool): defaults to True for training set.\n            dir (str): path to the dataset directory\n            cifar_classnum (int): 10 or 100\n        """"""\n        assert train_or_test in [\'train\', \'test\']\n        assert cifar_classnum == 10 or cifar_classnum == 100\n        self.cifar_classnum = cifar_classnum\n        if dir is None:\n            dir = get_dataset_path(\'cifar{}_data\'.format(cifar_classnum))\n        maybe_download_and_extract(dir, self.cifar_classnum)\n        train_files, test_files, meta_file = get_filenames(dir, cifar_classnum)\n        if train_or_test == \'train\':\n            self.fs = train_files\n        else:\n            self.fs = test_files\n        for f in self.fs:\n            if not os.path.isfile(f):\n                raise ValueError(\'Failed to find file: \' + f)\n        self._label_names = _parse_meta(meta_file, cifar_classnum)\n        self.train_or_test = train_or_test\n        self.data = read_cifar(self.fs, cifar_classnum)\n        self.dir = dir\n\n        if shuffle is None:\n            shuffle = train_or_test == \'train\'\n        self.shuffle = shuffle\n\n    def __len__(self):\n        return 50000 if self.train_or_test == \'train\' else 10000\n\n    def __iter__(self):\n        idxs = np.arange(len(self.data))\n        if self.shuffle:\n            self.rng.shuffle(idxs)\n        for k in idxs:\n            # since cifar is quite small, just do it for safety\n            yield self.data[k]\n\n    def get_per_pixel_mean(self, names=(\'train\', \'test\')):\n        """"""\n        Args:\n            names (tuple[str]): the names (\'train\' or \'test\') of the datasets\n\n        Returns:\n            a mean image of all images in the given datasets, with size 32x32x3\n        """"""\n        for name in names:\n            assert name in [\'train\', \'test\'], name\n        train_files, test_files, _ = get_filenames(self.dir, self.cifar_classnum)\n        all_files = []\n        if \'train\' in names:\n            all_files.extend(train_files)\n        if \'test\' in names:\n            all_files.extend(test_files)\n        all_imgs = [x[0] for x in read_cifar(all_files, self.cifar_classnum)]\n        arr = np.array(all_imgs, dtype=\'float32\')\n        mean = np.mean(arr, axis=0)\n        return mean\n\n    def get_label_names(self):\n        """"""\n        Returns:\n            [str]: name of each class.\n        """"""\n        return self._label_names\n\n    def get_per_channel_mean(self, names=(\'train\', \'test\')):\n        """"""\n        Args:\n            names (tuple[str]): the names (\'train\' or \'test\') of the datasets\n\n        Returns:\n            An array of three values as mean of each channel, for all images in the given datasets.\n        """"""\n        mean = self.get_per_pixel_mean(names)\n        return np.mean(mean, axis=(0, 1))\n\n\nclass Cifar10(CifarBase):\n    """"""\n    Produces [image, label] in Cifar10 dataset,\n    image is 32x32x3 in the range [0,255].\n    label is an int.\n    """"""\n    def __init__(self, train_or_test, shuffle=None, dir=None):\n        """"""\n        Args:\n            train_or_test (str): either \'train\' or \'test\'.\n            shuffle (bool): shuffle the dataset, default to shuffle in training\n        """"""\n        super(Cifar10, self).__init__(train_or_test, shuffle, dir, 10)\n\n\nclass Cifar100(CifarBase):\n    """""" Similar to Cifar10""""""\n    def __init__(self, train_or_test, shuffle=None, dir=None):\n        super(Cifar100, self).__init__(train_or_test, shuffle, dir, 100)\n\n\nif __name__ == \'__main__\':\n    ds = Cifar10(\'train\')\n    mean = ds.get_per_channel_mean()\n    print(mean)\n\n    import cv2\n    ds.reset_state()\n    for i, dp in enumerate(ds):\n        if i == 100:\n            break\n        img = dp[0]\n        cv2.imwrite(""{:04d}.jpg"".format(i), img)\n'"
tensorpack/dataflow/dataset/ilsvrc.py,0,"b'# -*- coding: utf-8 -*-\n# File: ilsvrc.py\n\nimport numpy as np\nimport os\nimport tarfile\nimport tqdm\n\nfrom ...utils import logger\nfrom ...utils.fs import download, get_dataset_path, mkdir_p\nfrom ...utils.loadcaffe import get_caffe_pb\nfrom ...utils.timer import timed_operation\nfrom ..base import RNGDataFlow\n\n__all__ = [\'ILSVRCMeta\', \'ILSVRC12\', \'ILSVRC12Files\']\n\nCAFFE_ILSVRC12_URL = (""http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz"", 17858008)\n\n\nclass ILSVRCMeta(object):\n    """"""\n    Provide methods to access metadata for ILSVRC dataset.\n    """"""\n\n    def __init__(self, dir=None):\n        if dir is None:\n            dir = get_dataset_path(\'ilsvrc_metadata\')\n        self.dir = os.path.expanduser(dir)\n        mkdir_p(self.dir)\n        f = os.path.join(self.dir, \'synsets.txt\')\n        if not os.path.isfile(f):\n            self._download_caffe_meta()\n        self.caffepb = None\n\n    def get_synset_words_1000(self):\n        """"""\n        Returns:\n            dict: {cls_number: cls_name}\n        """"""\n        fname = os.path.join(self.dir, \'synset_words.txt\')\n        assert os.path.isfile(fname), fname\n        lines = [x.strip() for x in open(fname).readlines()]\n        return dict(enumerate(lines))\n\n    def get_synset_1000(self):\n        """"""\n        Returns:\n            dict: {cls_number: synset_id}\n        """"""\n        fname = os.path.join(self.dir, \'synsets.txt\')\n        assert os.path.isfile(fname)\n        lines = [x.strip() for x in open(fname).readlines()]\n        return dict(enumerate(lines))\n\n    def _download_caffe_meta(self):\n        fpath = download(CAFFE_ILSVRC12_URL[0], self.dir, expect_size=CAFFE_ILSVRC12_URL[1])\n        tarfile.open(fpath, \'r:gz\').extractall(self.dir)\n\n    def get_image_list(self, name, dir_structure=\'original\'):\n        """"""\n        Args:\n            name (str): \'train\' or \'val\' or \'test\'\n            dir_structure (str): same as in :meth:`ILSVRC12.__init__()`.\n        Returns:\n            list: list of (image filename, label)\n        """"""\n        assert name in [\'train\', \'val\', \'test\']\n        assert dir_structure in [\'original\', \'train\']\n        add_label_to_fname = (name != \'train\' and dir_structure != \'original\')\n        if add_label_to_fname:\n            synset = self.get_synset_1000()\n\n        fname = os.path.join(self.dir, name + \'.txt\')\n        assert os.path.isfile(fname), fname\n        with open(fname) as f:\n            ret = []\n            for line in f.readlines():\n                name, cls = line.strip().split()\n                cls = int(cls)\n\n                if add_label_to_fname:\n                    name = os.path.join(synset[cls], name)\n\n                ret.append((name.strip(), cls))\n        assert len(ret), fname\n        return ret\n\n    def get_per_pixel_mean(self, size=None):\n        """"""\n        Args:\n            size (tuple): image size in (h, w). Defaults to (256, 256).\n        Returns:\n            np.ndarray: per-pixel mean of shape (h, w, 3 (BGR)) in range [0, 255].\n        """"""\n        if self.caffepb is None:\n            self.caffepb = get_caffe_pb()\n        obj = self.caffepb.BlobProto()\n\n        mean_file = os.path.join(self.dir, \'imagenet_mean.binaryproto\')\n        with open(mean_file, \'rb\') as f:\n            obj.ParseFromString(f.read())\n        arr = np.array(obj.data).reshape((3, 256, 256)).astype(\'float32\')\n        arr = np.transpose(arr, [1, 2, 0])\n        if size is not None:\n            arr = cv2.resize(arr, size[::-1])\n        return arr\n\n    @staticmethod\n    def guess_dir_structure(dir):\n        """"""\n        Return the directory structure of ""dir"".\n\n        Args:\n            dir(str): something like \'/path/to/imagenet/val\'\n\n        Returns:\n            either \'train\' or \'original\'\n        """"""\n        subdir = os.listdir(dir)[0]\n        # find a subdir starting with \'n\'\n        if subdir.startswith(\'n\') and \\\n                os.path.isdir(os.path.join(dir, subdir)):\n            dir_structure = \'train\'\n        else:\n            dir_structure = \'original\'\n        logger.info(\n            ""[ILSVRC12] Assuming directory {} has \'{}\' structure."".format(\n                dir, dir_structure))\n        return dir_structure\n\n\nclass ILSVRC12Files(RNGDataFlow):\n    """"""\n    Same as :class:`ILSVRC12`, but produces filenames of the images instead of nparrays.\n    This could be useful when ``cv2.imread`` is a bottleneck and you want to\n    decode it in smarter ways (e.g. in parallel).\n    """"""\n    def __init__(self, dir, name, meta_dir=None,\n                 shuffle=None, dir_structure=None):\n        """"""\n        Same as in :class:`ILSVRC12`.\n        """"""\n        assert name in [\'train\', \'test\', \'val\'], name\n        dir = os.path.expanduser(dir)\n        assert os.path.isdir(dir), dir\n        self.full_dir = os.path.join(dir, name)\n        self.name = name\n        assert os.path.isdir(self.full_dir), self.full_dir\n        assert meta_dir is None or os.path.isdir(meta_dir), meta_dir\n        if shuffle is None:\n            shuffle = name == \'train\'\n        self.shuffle = shuffle\n\n        if name == \'train\':\n            dir_structure = \'train\'\n        if dir_structure is None:\n            dir_structure = ILSVRCMeta.guess_dir_structure(self.full_dir)\n\n        meta = ILSVRCMeta(meta_dir)\n        self.imglist = meta.get_image_list(name, dir_structure)\n\n        for fname, _ in self.imglist[:10]:\n            fname = os.path.join(self.full_dir, fname)\n            assert os.path.isfile(fname), fname\n\n    def __len__(self):\n        return len(self.imglist)\n\n    def __iter__(self):\n        idxs = np.arange(len(self.imglist))\n        if self.shuffle:\n            self.rng.shuffle(idxs)\n        for k in idxs:\n            fname, label = self.imglist[k]\n            fname = os.path.join(self.full_dir, fname)\n            yield [fname, label]\n\n\nclass ILSVRC12(ILSVRC12Files):\n    """"""\n    Produces uint8 ILSVRC12 images of shape [h, w, 3(BGR)], and a label between [0, 999].\n    The label map follows the synsets.txt file in http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz.\n    """"""\n    def __init__(self, dir, name, meta_dir=None,\n                 shuffle=None, dir_structure=None):\n        """"""\n        Args:\n            dir (str): A directory containing a subdir named ``name``,\n                containing the images in a structure described below.\n            name (str): One of \'train\' or \'val\' or \'test\'.\n            shuffle (bool): shuffle the dataset.\n                Defaults to True if name==\'train\'.\n            dir_structure (str): One of \'original\' or \'train\'.\n                The directory structure for the \'val\' directory.\n                \'original\' means the original decompressed directory, which only has list of image files (as below).\n                If set to \'train\', it expects the same two-level directory structure similar to \'dir/train/\'.\n                By default, it tries to automatically detect the structure.\n                You probably do not need to care about this option because \'original\' is what people usually have.\n\n        Example:\n\n        When `dir_structure==\'original\'`, `dir` should have the following structure:\n\n        .. code-block:: none\n\n            dir/\n              train/\n                n02134418/\n                  n02134418_198.JPEG\n                  ...\n                ...\n              val/\n                ILSVRC2012_val_00000001.JPEG\n                ...\n              test/\n                ILSVRC2012_test_00000001.JPEG\n                ...\n\n        With the downloaded ILSVRC12_img_*.tar, you can use the following\n        command to build the above structure:\n\n        .. code-block:: none\n\n            mkdir val && tar xvf ILSVRC12_img_val.tar -C val\n            mkdir test && tar xvf ILSVRC12_img_test.tar -C test\n            mkdir train && tar xvf ILSVRC12_img_train.tar -C train && cd train\n            find -type f -name \'*.tar\' | parallel -P 10 \'echo {} && mkdir -p {/.} && tar xf {} -C {/.}\'\n\n        When `dir_structure==\'train\'`, `dir` should have the following structure:\n\n        .. code-block:: none\n\n            dir/\n              train/\n                n02134418/\n                  n02134418_198.JPEG\n                  ...\n                ...\n              val/\n                n01440764/\n                  ILSVRC2012_val_00000293.JPEG\n                  ...\n                ...\n              test/\n                ILSVRC2012_test_00000001.JPEG\n                ...\n        """"""\n        super(ILSVRC12, self).__init__(\n            dir, name, meta_dir, shuffle, dir_structure)\n\n    """"""\n    There are some CMYK / png images, but cv2 seems robust to them.\n    https://github.com/tensorflow/models/blob/c0cd713f59cfe44fa049b3120c417cc4079c17e3/research/inception/inception/data/build_imagenet_data.py#L264-L300\n    """"""\n    def __iter__(self):\n        for fname, label in super(ILSVRC12, self).__iter__():\n            im = cv2.imread(fname, cv2.IMREAD_COLOR)\n            assert im is not None, fname\n            yield [im, label]\n\n    @staticmethod\n    def get_training_bbox(bbox_dir, imglist):\n        import xml.etree.ElementTree as ET\n        ret = []\n\n        def parse_bbox(fname):\n            root = ET.parse(fname).getroot()\n            size = root.find(\'size\').getchildren()\n            size = map(int, [size[0].text, size[1].text])\n\n            box = root.find(\'object\').find(\'bndbox\').getchildren()\n            box = map(lambda x: float(x.text), box)\n            return np.asarray(box, dtype=\'float32\')\n\n        with timed_operation(\'Loading Bounding Boxes ...\'):\n            cnt = 0\n            for k in tqdm.trange(len(imglist)):\n                fname = imglist[k][0]\n                fname = fname[:-4] + \'xml\'\n                fname = os.path.join(bbox_dir, fname)\n                try:\n                    ret.append(parse_bbox(fname))\n                    cnt += 1\n                except Exception:\n                    ret.append(None)\n            logger.info(""{}/{} images have bounding box."".format(cnt, len(imglist)))\n        return ret\n\n\ntry:\n    import cv2\nexcept ImportError:\n    from ...utils.develop import create_dummy_class\n    ILSVRC12 = create_dummy_class(\'ILSVRC12\', \'cv2\')  # noqa\n\nif __name__ == \'__main__\':\n    meta = ILSVRCMeta()\n    # print(meta.get_synset_words_1000())\n\n    ds = ILSVRC12(\'/home/wyx/data/fake_ilsvrc/\', \'train\', shuffle=False)\n    ds.reset_state()\n\n    for _ in ds:\n        from IPython import embed\n        embed()\n        break\n'"
tensorpack/dataflow/dataset/mnist.py,0,"b'# -*- coding: utf-8 -*-\n# File: mnist.py\n\n\nimport gzip\nimport numpy\nimport os\n\nfrom ...utils import logger\nfrom ...utils.fs import download, get_dataset_path\nfrom ..base import RNGDataFlow\n\n__all__ = [\'Mnist\', \'FashionMnist\']\n\n\ndef maybe_download(url, work_directory):\n    """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n    filename = url.split(\'/\')[-1]\n    filepath = os.path.join(work_directory, filename)\n    if not os.path.exists(filepath):\n        logger.info(""Downloading to {}..."".format(filepath))\n        download(url, work_directory)\n    return filepath\n\n\ndef _read32(bytestream):\n    dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n\n\ndef extract_images(filename):\n    """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n    with gzip.open(filename) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2051:\n            raise ValueError(\n                \'Invalid magic number %d in MNIST image file: %s\' %\n                (magic, filename))\n        num_images = _read32(bytestream)\n        rows = _read32(bytestream)\n        cols = _read32(bytestream)\n        buf = bytestream.read(rows * cols * num_images)\n        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n        data = data.reshape(num_images, rows, cols, 1)\n        data = data.astype(\'float32\') / 255.0\n        return data\n\n\ndef extract_labels(filename):\n    """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n    with gzip.open(filename) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2049:\n            raise ValueError(\n                \'Invalid magic number %d in MNIST label file: %s\' %\n                (magic, filename))\n        num_items = _read32(bytestream)\n        buf = bytestream.read(num_items)\n        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n        return labels\n\n\nclass Mnist(RNGDataFlow):\n    """"""\n    Produces [image, label] in MNIST dataset,\n    image is 28x28 in the range [0,1], label is an int.\n    """"""\n\n    _DIR_NAME = \'mnist_data\'\n    _SOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\n\n    def __init__(self, train_or_test, shuffle=True, dir=None):\n        """"""\n        Args:\n            train_or_test (str): either \'train\' or \'test\'\n            shuffle (bool): shuffle the dataset\n        """"""\n        if dir is None:\n            dir = get_dataset_path(self._DIR_NAME)\n        assert train_or_test in [\'train\', \'test\']\n        self.train_or_test = train_or_test\n        self.shuffle = shuffle\n\n        def get_images_and_labels(image_file, label_file):\n            f = maybe_download(self._SOURCE_URL + image_file, dir)\n            images = extract_images(f)\n            f = maybe_download(self._SOURCE_URL + label_file, dir)\n            labels = extract_labels(f)\n            assert images.shape[0] == labels.shape[0]\n            return images, labels\n\n        if self.train_or_test == \'train\':\n            self.images, self.labels = get_images_and_labels(\n                \'train-images-idx3-ubyte.gz\',\n                \'train-labels-idx1-ubyte.gz\')\n        else:\n            self.images, self.labels = get_images_and_labels(\n                \'t10k-images-idx3-ubyte.gz\',\n                \'t10k-labels-idx1-ubyte.gz\')\n\n    def __len__(self):\n        return self.images.shape[0]\n\n    def __iter__(self):\n        idxs = list(range(self.__len__()))\n        if self.shuffle:\n            self.rng.shuffle(idxs)\n        for k in idxs:\n            img = self.images[k].reshape((28, 28))\n            label = self.labels[k]\n            yield [img, label]\n\n\nclass FashionMnist(Mnist):\n    """"""\n    Same API as :class:`Mnist`, but more fashion.\n    """"""\n\n    _DIR_NAME = \'fashion_mnist_data\'\n    _SOURCE_URL = \'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\'\n\n    def get_label_names(self):\n        """"""\n        Returns:\n            [str]: the name of each class\n        """"""\n        # copied from https://github.com/zalandoresearch/fashion-mnist\n        return [\'T-shirt/top\', \'Trouser\', \'Pullover\', \'Dress\', \'Coat\',\n                \'Sandal\', \'Shirt\', \'Sneaker\', \'Bag\', \'Ankle boot\']\n\n\nif __name__ == \'__main__\':\n    ds = Mnist(\'train\')\n    ds.reset_state()\n    for _ in ds:\n        from IPython import embed\n        embed()\n        break\n'"
tensorpack/dataflow/dataset/svhn.py,0,"b'# -*- coding: utf-8 -*-\n# File: svhn.py\n\n\nimport numpy as np\nimport os\n\nfrom ...utils import logger\nfrom ...utils.fs import download, get_dataset_path\nfrom ..base import RNGDataFlow\n\n__all__ = [\'SVHNDigit\']\n\nSVHN_URL = ""http://ufldl.stanford.edu/housenumbers/""\n\n\nclass SVHNDigit(RNGDataFlow):\n    """"""\n    `SVHN <http://ufldl.stanford.edu/housenumbers/>`_ Cropped Digit Dataset.\n    Produces [img, label], img of 32x32x3 in range [0,255], label of 0-9\n    """"""\n    _Cache = {}\n\n    def __init__(self, name, data_dir=None, shuffle=True):\n        """"""\n        Args:\n            name (str): \'train\', \'test\', or \'extra\'.\n            data_dir (str): a directory containing the original {train,test,extra}_32x32.mat.\n            shuffle (bool): shuffle the dataset.\n        """"""\n        self.shuffle = shuffle\n\n        if name in SVHNDigit._Cache:\n            self.X, self.Y = SVHNDigit._Cache[name]\n            return\n        if data_dir is None:\n            data_dir = get_dataset_path(\'svhn_data\')\n        assert name in [\'train\', \'test\', \'extra\'], name\n        filename = os.path.join(data_dir, name + \'_32x32.mat\')\n        if not os.path.isfile(filename):\n            url = SVHN_URL + os.path.basename(filename)\n            logger.info(""File {} not found!"".format(filename))\n            logger.info(""Downloading from {} ..."".format(url))\n            download(url, os.path.dirname(filename))\n        logger.info(""Loading {} ..."".format(filename))\n        data = scipy.io.loadmat(filename)\n        self.X = data[\'X\'].transpose(3, 0, 1, 2)\n        self.Y = data[\'y\'].reshape((-1))\n        self.Y[self.Y == 10] = 0\n        SVHNDigit._Cache[name] = (self.X, self.Y)\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __iter__(self):\n        n = self.X.shape[0]\n        idxs = np.arange(n)\n        if self.shuffle:\n            self.rng.shuffle(idxs)\n        for k in idxs:\n            # since svhn is quite small, just do it for safety\n            yield [self.X[k], self.Y[k]]\n\n    @staticmethod\n    def get_per_pixel_mean(names=(\'train\', \'test\', \'extra\')):\n        """"""\n        Args:\n            names (tuple[str]): names of the dataset split\n\n        Returns:\n            a 32x32x3 image, the mean of all images in the given datasets\n        """"""\n        for name in names:\n            assert name in [\'train\', \'test\', \'extra\'], name\n        images = [SVHNDigit(x).X for x in names]\n        return np.concatenate(tuple(images)).mean(axis=0)\n\n\ntry:\n    import scipy.io\nexcept ImportError:\n    from ...utils.develop import create_dummy_class\n    SVHNDigit = create_dummy_class(\'SVHNDigit\', \'scipy.io\')  # noqa\n\nif __name__ == \'__main__\':\n    a = SVHNDigit(\'train\')\n    b = SVHNDigit.get_per_pixel_mean()\n'"
tensorpack/dataflow/imgaug/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File: __init__.py\n\n# https://github.com/celery/kombu/blob/7d13f9b95d0b50c94393b962e6def928511bfda6/kombu/__init__.py#L34-L36\nSTATICA_HACK = True\nglobals()[\'kcah_acitats\'[::-1].upper()] = False\nif STATICA_HACK:\n    from .base import *\n    from .convert import *\n    from .crop import *\n    from .deform import *\n    from .geometry import *\n    from .imgproc import *\n    from .meta import *\n    from .misc import *\n    from .noise import *\n    from .paste import *\n    from .transform import *\n    from .external import *\n\n\nimport os\nfrom pkgutil import iter_modules\n\n__all__ = []\n\n\ndef global_import(name):\n    p = __import__(name, globals(), locals(), level=1)\n    lst = p.__all__ if \'__all__\' in dir(p) else dir(p)\n    if lst:\n        del globals()[name]\n        for k in lst:\n            if not k.startswith(\'__\'):\n                globals()[k] = p.__dict__[k]\n                __all__.append(k)\n\n\ntry:\n    import cv2  # noqa\nexcept ImportError:\n    from ...utils import logger\n    logger.warn(""Cannot import \'cv2\', therefore image augmentation is not available."")\nelse:\n    _CURR_DIR = os.path.dirname(__file__)\n    for _, module_name, _ in iter_modules(\n            [os.path.dirname(__file__)]):\n        srcpath = os.path.join(_CURR_DIR, module_name + \'.py\')\n        if not os.path.isfile(srcpath):\n            continue\n        if not module_name.startswith(\'_\') and ""_test"" not in module_name:\n            global_import(module_name)\n'"
tensorpack/dataflow/imgaug/base.py,0,"b'# -*- coding: utf-8 -*-\n# File: base.py\n\nimport os\nimport inspect\nimport pprint\nfrom collections import namedtuple\nimport weakref\n\nfrom ...utils.argtools import log_once\nfrom ...utils.utils import get_rng\nfrom ...utils.develop import deprecated\nfrom ..image import check_dtype\n\n# Cannot import here if we want to keep backward compatibility.\n# Because this causes circular dependency\n# from .transform import TransformList, PhotometricTransform, TransformFactory\n\n__all__ = [\'Augmentor\', \'ImageAugmentor\', \'AugmentorList\', \'PhotometricAugmentor\']\n\n\ndef _reset_augmentor_after_fork(aug_ref):\n    aug = aug_ref()\n    if aug:\n        aug.reset_state()\n\n\ndef _default_repr(self):\n    """"""\n    Produce something like:\n    ""imgaug.MyAugmentor(field1={self.field1}, field2={self.field2})""\n\n    It assumes that the instance `self` contains attributes that match its constructor.\n    """"""\n    classname = type(self).__name__\n    argspec = inspect.getfullargspec(self.__init__)\n    assert argspec.varargs is None, ""The default __repr__ in {} doesn\'t work for varargs!"".format(classname)\n    assert argspec.varkw is None, ""The default __repr__ in {} doesn\'t work for kwargs!"".format(classname)\n    defaults = {}\n\n    fields = argspec.args[1:]\n    defaults_pos = argspec.defaults\n    if defaults_pos is not None:\n        for f, d in zip(fields[::-1], defaults_pos[::-1]):\n            defaults[f] = d\n\n    for k in argspec.kwonlyargs:\n        fields.append(k)\n        if k in argspec.kwonlydefaults:\n            defaults[k] = argspec.kwonlydefaults[k]\n\n    argstr = []\n    for f in fields:\n        assert hasattr(self, f), \\\n            ""Attribute {} in {} not found! Default __repr__ only works if "" \\\n            ""the instance has attributes that match the constructor."".format(f, classname)\n        attr = getattr(self, f)\n        if f in defaults and attr is defaults[f]:\n            continue\n        argstr.append(""{}={}"".format(f, pprint.pformat(attr)))\n    return ""imgaug.{}({})"".format(classname, \', \'.join(argstr))\n\n\nImagePlaceholder = namedtuple(""ImagePlaceholder"", [""shape""])\n\n\nclass ImageAugmentor(object):\n    """"""\n    Base class for an augmentor\n\n    ImageAugmentor should take images of type uint8 in range [0, 255], or\n    floating point images in range [0, 1] or [0, 255].\n\n    Attributes:\n        rng: a numpy :class:`RandomState`\n    """"""\n\n    def __init__(self):\n        self.reset_state()\n\n        # only available on Unix after Python 3.7\n        if hasattr(os, \'register_at_fork\'):\n            os.register_at_fork(\n                after_in_child=lambda: _reset_augmentor_after_fork(weakref.ref(self)))\n\n    def _init(self, params=None):\n        if params:\n            for k, v in params.items():\n                if k != \'self\' and not k.startswith(\'_\'):\n                    setattr(self, k, v)\n\n    def reset_state(self):\n        """"""\n        Reset rng and other state of the augmentor.\n\n        Similar to :meth:`DataFlow.reset_state`, the caller of Augmentor\n        is responsible for calling this method (once or more times) in the **process that uses the augmentor**\n        before using it.\n\n        If you use a built-in augmentation dataflow (:class:`AugmentImageComponent`, etc),\n        this method will be called in the dataflow\'s own `reset_state` method.\n\n        If you use Python\xe2\x89\xa53.7 on Unix, this method will be automatically called after fork,\n        and you do not need to bother calling it.\n        """"""\n        self.rng = get_rng(self)\n\n    def _rand_range(self, low=1.0, high=None, size=None):\n        """"""\n        Generate uniform float random number between low and high using `self.rng`.\n        """"""\n        if high is None:\n            low, high = 0, low\n        if size is None:\n            size = []\n        return self.rng.uniform(low, high, size).astype(""float32"")\n\n    def __str__(self):\n        try:\n            return _default_repr(self)\n        except AssertionError as e:\n            log_once(e.args[0], \'warn\')\n            return super(Augmentor, self).__repr__()\n\n    __repr__ = __str__\n\n    def get_transform(self, img):\n        """"""\n        Instantiate a :class:`Transform` object to be used given the input image.\n        Subclasses should implement this method.\n\n        The :class:`ImageAugmentor` often has random policies which generate deterministic transform.\n        Any of those random policies should happen inside this method and instantiate\n        an actual deterministic transform to be performed.\n        The returned :class:`Transform` object should perform deterministic transforms\n        through its :meth:`apply_*` method.\n\n        In this way, the returned :class:`Transform` object can be used to transform not only the\n        input image, but other images or coordinates associated with the image.\n\n        Args:\n            img (ndarray): see notes of this class on the requirements.\n\n        Returns:\n            Transform\n        """"""\n        # This should be an abstract method\n        # But we provide an implementation that uses the old interface,\n        # for backward compatibility\n        log_once(""The old augmentor interface was deprecated. ""\n                 ""Please implement {} with `get_transform` instead!"".format(self.__class__.__name__),\n                 ""warning"")\n\n        def legacy_augment_coords(self, coords, p):\n            try:\n                return self._augment_coords(coords, p)\n            except AttributeError:\n                pass\n            try:\n                return self.augment_coords(coords, p)\n            except AttributeError:\n                pass\n            return coords  # this is the old default\n\n        p = None  # the default return value for this method\n        try:\n            p = self._get_augment_params(img)\n        except AttributeError:\n            pass\n        try:\n            p = self.get_augment_params(img)\n        except AttributeError:\n            pass\n\n        from .transform import BaseTransform, TransformFactory\n        if isinstance(p, BaseTransform):  # some old augs return Transform already\n            return p\n\n        return TransformFactory(name=""LegacyConversion -- "" + str(self),\n                                apply_image=lambda img: self._augment(img, p),\n                                apply_coords=lambda coords: legacy_augment_coords(self, coords, p))\n\n    def augment(self, img):\n        """"""\n        Create a transform, and apply it to augment the input image.\n\n        This can save you one line of code, when you only care the augmentation of ""one image"".\n        It will not return the :class:`Transform` object to you\n        so you won\'t be able to apply the same transformation on\n        other data associated with the image.\n\n        Args:\n            img (ndarray): see notes of this class on the requirements.\n\n        Returns:\n            img: augmented image.\n        """"""\n        check_dtype(img)\n        t = self.get_transform(img)\n        return t.apply_image(img)\n\n    # ###########################\n    # Legacy interfaces:\n    # ###########################\n    @deprecated(""Please use `get_transform` instead!"", ""2020-06-06"", max_num_warnings=3)\n    def augment_return_params(self, d):\n        t = self.get_transform(d)\n        return t.apply_image(d), t\n\n    @deprecated(""Please use `transform.apply_image` instead!"", ""2020-06-06"", max_num_warnings=3)\n    def augment_with_params(self, d, param):\n        return param.apply_image(d)\n\n    @deprecated(""Please use `transform.apply_coords` instead!"", ""2020-06-06"", max_num_warnings=3)\n    def augment_coords(self, coords, param):\n        return param.apply_coords(coords)\n\n\nclass AugmentorList(ImageAugmentor):\n    """"""\n    Augment an image by a list of augmentors\n    """"""\n\n    def __init__(self, augmentors):\n        """"""\n        Args:\n            augmentors (list): list of :class:`ImageAugmentor` instance to be applied.\n        """"""\n        assert isinstance(augmentors, (list, tuple)), augmentors\n        self.augmentors = augmentors\n        super(AugmentorList, self).__init__()\n\n    def reset_state(self):\n        """""" Will reset state of each augmentor """"""\n        super(AugmentorList, self).reset_state()\n        for a in self.augmentors:\n            a.reset_state()\n\n    def get_transform(self, img):\n        check_dtype(img)\n        assert img.ndim in [2, 3], img.ndim\n\n        from .transform import LazyTransform, TransformList\n        # The next augmentor requires the previous one to finish.\n        # So we have to use LazyTransform\n        tfms = []\n        for idx, a in enumerate(self.augmentors):\n            if idx == 0:\n                t = a.get_transform(img)\n            else:\n                t = LazyTransform(a.get_transform)\n\n            if isinstance(t, TransformList):\n                tfms.extend(t.tfms)\n            else:\n                tfms.append(t)\n        return TransformList(tfms)\n\n    def __str__(self):\n        repr_each_aug = "",\\n"".join([""  "" + repr(x) for x in self.augmentors])\n        return ""imgaug.AugmentorList([\\n{}])"".format(repr_each_aug)\n\n    __repr__ = __str__\n\n\nAugmentor = ImageAugmentor\n""""""\nLegacy name. Augmentor and ImageAugmentor are now the same thing.\n""""""\n\n\nclass PhotometricAugmentor(ImageAugmentor):\n    """"""\n    A base class for ImageAugmentor which only affects pixels.\n\n    Subclass should implement `_get_params(img)` and `_impl(img, params)`.\n    """"""\n    def get_transform(self, img):\n        p = self._get_augment_params(img)\n        from .transform import PhotometricTransform\n        return PhotometricTransform(func=lambda img: self._augment(img, p),\n                                    name=""from "" + str(self))\n\n    def _get_augment_params(self, _):\n        return None\n'"
tensorpack/dataflow/imgaug/convert.py,0,"b'# -*- coding: utf-8 -*-\n# File: convert.py\n\nimport numpy as np\nimport cv2\n\nfrom .base import PhotometricAugmentor\n\n__all__ = [\'ColorSpace\', \'Grayscale\', \'ToUint8\', \'ToFloat32\']\n\n\nclass ColorSpace(PhotometricAugmentor):\n    """""" Convert into another color space.  """"""\n\n    def __init__(self, mode, keepdims=True):\n        """"""\n        Args:\n            mode: OpenCV color space conversion code (e.g., ``cv2.COLOR_BGR2HSV``)\n            keepdims (bool): keep the dimension of image unchanged if OpenCV\n                changes it.\n        """"""\n        super(ColorSpace, self).__init__()\n        self._init(locals())\n\n    def _augment(self, img, _):\n        transf = cv2.cvtColor(img, self.mode)\n        if self.keepdims:\n            if len(transf.shape) is not len(img.shape):\n                transf = transf[..., None]\n        return transf\n\n\nclass Grayscale(ColorSpace):\n    """""" Convert RGB or BGR image to grayscale. """"""\n\n    def __init__(self, keepdims=True, rgb=False, keepshape=False):\n        """"""\n        Args:\n            keepdims (bool): return image of shape [H, W, 1] instead of [H, W]\n            rgb (bool): interpret input as RGB instead of the default BGR\n            keepshape (bool): whether to duplicate the gray image into 3 channels\n                so the result has the same shape as input.\n        """"""\n        mode = cv2.COLOR_RGB2GRAY if rgb else cv2.COLOR_BGR2GRAY\n        if keepshape:\n            assert keepdims, ""keepdims must be True when keepshape==True""\n        super(Grayscale, self).__init__(mode, keepdims)\n        self.keepshape = keepshape\n        self.rgb = rgb\n\n    def _augment(self, img, _):\n        ret = super()._augment(img, _)\n        if self.keepshape:\n            return np.concatenate([ret] * 3, axis=2)\n        else:\n            return ret\n\n\nclass ToUint8(PhotometricAugmentor):\n    """""" Clip and convert image to uint8. Useful to reduce communication overhead. """"""\n    def _augment(self, img, _):\n        return np.clip(img, 0, 255).astype(np.uint8)\n\n\nclass ToFloat32(PhotometricAugmentor):\n    """""" Convert image to float32, may increase quality of the augmentor. """"""\n    def _augment(self, img, _):\n        return img.astype(np.float32)\n'"
tensorpack/dataflow/imgaug/crop.py,0,"b'# -*- coding: utf-8 -*-\n# File: crop.py\n\nimport numpy as np\nimport cv2\n\nfrom ...utils.argtools import shape2d\nfrom ...utils.develop import log_deprecated\nfrom .base import ImageAugmentor, ImagePlaceholder\nfrom .transform import CropTransform, TransformList, ResizeTransform, PhotometricTransform\nfrom .misc import ResizeShortestEdge\n\n__all__ = [\'RandomCrop\', \'CenterCrop\', \'RandomCropRandomShape\',\n           \'GoogleNetRandomCropAndResize\', \'RandomCutout\']\n\n\nclass RandomCrop(ImageAugmentor):\n    """""" Randomly crop the image into a smaller one """"""\n\n    def __init__(self, crop_shape):\n        """"""\n        Args:\n            crop_shape: (h, w), int or a tuple of int\n        """"""\n        crop_shape = shape2d(crop_shape)\n        crop_shape = (int(crop_shape[0]), int(crop_shape[1]))\n        super(RandomCrop, self).__init__()\n        self._init(locals())\n\n    def get_transform(self, img):\n        orig_shape = img.shape\n        assert orig_shape[0] >= self.crop_shape[0] \\\n            and orig_shape[1] >= self.crop_shape[1], orig_shape\n        diffh = orig_shape[0] - self.crop_shape[0]\n        h0 = self.rng.randint(diffh + 1)\n        diffw = orig_shape[1] - self.crop_shape[1]\n        w0 = self.rng.randint(diffw + 1)\n        return CropTransform(h0, w0, self.crop_shape[0], self.crop_shape[1])\n\n\nclass CenterCrop(ImageAugmentor):\n    """""" Crop the image at the center""""""\n\n    def __init__(self, crop_shape):\n        """"""\n        Args:\n            crop_shape: (h, w) tuple or a int\n        """"""\n        crop_shape = shape2d(crop_shape)\n        self._init(locals())\n\n    def get_transform(self, img):\n        orig_shape = img.shape\n        assert orig_shape[0] >= self.crop_shape[0] \\\n            and orig_shape[1] >= self.crop_shape[1], orig_shape\n        h0 = int((orig_shape[0] - self.crop_shape[0]) * 0.5)\n        w0 = int((orig_shape[1] - self.crop_shape[1]) * 0.5)\n        return CropTransform(h0, w0, self.crop_shape[0], self.crop_shape[1])\n\n\nclass RandomCropRandomShape(ImageAugmentor):\n    """""" Random crop with a random shape""""""\n\n    def __init__(self, wmin, hmin,\n                 wmax=None, hmax=None,\n                 max_aspect_ratio=None):\n        """"""\n        Randomly crop a box of shape (h, w), sampled from [min, max] (both inclusive).\n        If max is None, will use the input image shape.\n\n        Args:\n            wmin, hmin, wmax, hmax: range to sample shape.\n            max_aspect_ratio (float): this argument has no effect and is deprecated.\n        """"""\n        super(RandomCropRandomShape, self).__init__()\n        if max_aspect_ratio is not None:\n            log_deprecated(""RandomCropRandomShape(max_aspect_ratio)"", ""It is never implemented!"", ""2020-06-06"")\n        self._init(locals())\n\n    def get_transform(self, img):\n        hmax = self.hmax or img.shape[0]\n        wmax = self.wmax or img.shape[1]\n        h = self.rng.randint(self.hmin, hmax + 1)\n        w = self.rng.randint(self.wmin, wmax + 1)\n        diffh = img.shape[0] - h\n        diffw = img.shape[1] - w\n        assert diffh >= 0 and diffw >= 0, str(diffh) + "", "" + str(diffw)\n        y0 = 0 if diffh == 0 else self.rng.randint(diffh)\n        x0 = 0 if diffw == 0 else self.rng.randint(diffw)\n        return CropTransform(y0, x0, h, w)\n\n\nclass GoogleNetRandomCropAndResize(ImageAugmentor):\n    """"""\n    The random crop and resize augmentation proposed in\n    Sec. 6 of ""Going Deeper with Convolutions"" by Google.\n    This implementation follows the details in ``fb.resnet.torch``.\n\n    It attempts to crop a random rectangle with 8%~100% area of the original image,\n    and keep the aspect ratio between 3/4 to 4/3. Then it resize this crop to the target shape.\n    If such crop cannot be found in 10 iterations, it will do a ResizeShortestEdge + CenterCrop.\n    """"""\n    def __init__(self, crop_area_fraction=(0.08, 1.),\n                 aspect_ratio_range=(0.75, 1.333),\n                 target_shape=224, interp=cv2.INTER_LINEAR):\n        """"""\n        Args:\n            crop_area_fraction (tuple(float)): Defaults to crop 8%-100% area.\n            aspect_ratio_range (tuple(float)): Defaults to make aspect ratio in 3/4-4/3.\n            target_shape (int): Defaults to 224, the standard ImageNet image shape.\n        """"""\n        super(GoogleNetRandomCropAndResize, self).__init__()\n        self._init(locals())\n\n    def get_transform(self, img):\n        h, w = img.shape[:2]\n        area = h * w\n        for _ in range(10):\n            targetArea = self.rng.uniform(*self.crop_area_fraction) * area\n            aspectR = self.rng.uniform(*self.aspect_ratio_range)\n            ww = int(np.sqrt(targetArea * aspectR) + 0.5)\n            hh = int(np.sqrt(targetArea / aspectR) + 0.5)\n            if self.rng.uniform() < 0.5:\n                ww, hh = hh, ww\n            if hh <= h and ww <= w:\n                x1 = self.rng.randint(0, w - ww + 1)\n                y1 = self.rng.randint(0, h - hh + 1)\n                return TransformList([\n                    CropTransform(y1, x1, hh, ww),\n                    ResizeTransform(hh, ww, self.target_shape, self.target_shape, interp=self.interp)\n                ])\n        resize = ResizeShortestEdge(self.target_shape, interp=self.interp).get_transform(img)\n        out_shape = (resize.new_h, resize.new_w)\n        crop = CenterCrop(self.target_shape).get_transform(ImagePlaceholder(shape=out_shape))\n        return TransformList([resize, crop])\n\n\nclass RandomCutout(ImageAugmentor):\n    """"""\n    The cutout augmentation, as described in https://arxiv.org/abs/1708.04552\n    """"""\n    def __init__(self, h_range, w_range, fill=0.):\n        """"""\n        Args:\n            h_range (int or tuple): the height of rectangle to cut.\n                If a tuple, will randomly sample from this range [low, high)\n            w_range (int or tuple): similar to above\n            fill (float): the fill value\n        """"""\n        super(RandomCutout, self).__init__()\n        self._init(locals())\n\n    def _get_cutout_shape(self):\n        if isinstance(self.h_range, int):\n            h = self.h_range\n        else:\n            h = self.rng.randint(self.h_range)\n\n        if isinstance(self.w_range, int):\n            w = self.w_range\n        else:\n            w = self.rng.randint(self.w_range)\n        return h, w\n\n    @staticmethod\n    def _cutout(img, y0, x0, h, w, fill):\n        img[y0:y0 + h, x0:x0 + w] = fill\n        return img\n\n    def get_transform(self, img):\n        h, w = self._get_cutout_shape()\n        x0 = self.rng.randint(0, img.shape[1] + 1 - w)\n        y0 = self.rng.randint(0, img.shape[0] + 1 - h)\n        return PhotometricTransform(\n            lambda img: RandomCutout._cutout(img, y0, x0, h, w, self.fill),\n            ""cutout"")\n'"
tensorpack/dataflow/imgaug/deform.py,0,"b'# -*- coding: utf-8 -*-\n# File: deform.py\n\n\nimport numpy as np\n\nfrom ...utils import logger\nfrom .base import ImageAugmentor\nfrom .transform import TransformFactory\n\n__all__ = []\n\n# Code was temporarily kept here for a future reference in case someone needs it\n# But it was already deprecated,\n# because this augmentation is not a general one that people will often find helpful.\n\n\nclass GaussianMap(object):\n    """""" Generate Gaussian weighted deformation map""""""\n    # TODO really needs speedup\n\n    def __init__(self, image_shape, sigma=0.5):\n        assert len(image_shape) == 2\n        self.shape = image_shape\n        self.sigma = sigma\n\n    def get_gaussian_weight(self, anchor):\n        """"""\n        Args:\n            anchor: coordinate of the center\n        """"""\n        ret = np.zeros(self.shape, dtype=\'float32\')\n\n        y, x = np.mgrid[:self.shape[0], :self.shape[1]]\n        y = y.astype(\'float32\') / ret.shape[0] - anchor[0]\n        x = x.astype(\'float32\') / ret.shape[1] - anchor[1]\n        g = np.exp(-(x**2 + y ** 2) / self.sigma)\n        # cv2.imshow("" "", g)\n        # cv2.waitKey()\n        return g\n\n\ndef np_sample(img, coords):\n    # a numpy implementation of ImageSample layer\n    coords = np.maximum(coords, 0)\n    coords = np.minimum(coords, np.array([img.shape[0] - 1, img.shape[1] - 1]))\n\n    lcoor = np.floor(coords).astype(\'int32\')\n    ucoor = lcoor + 1\n    ucoor = np.minimum(ucoor, np.array([img.shape[0] - 1, img.shape[1] - 1]))\n    diff = coords - lcoor\n    neg_diff = 1.0 - diff\n\n    lcoory, lcoorx = np.split(lcoor, 2, axis=2)\n    ucoory, ucoorx = np.split(ucoor, 2, axis=2)\n    diff = np.repeat(diff, 3, 2).reshape((diff.shape[0], diff.shape[1], 2, 3))\n    neg_diff = np.repeat(neg_diff, 3, 2).reshape((diff.shape[0], diff.shape[1], 2, 3))\n    diffy, diffx = np.split(diff, 2, axis=2)\n    ndiffy, ndiffx = np.split(neg_diff, 2, axis=2)\n\n    ret = img[lcoory, lcoorx, :] * ndiffx * ndiffy + \\\n        img[ucoory, ucoorx, :] * diffx * diffy + \\\n        img[lcoory, ucoorx, :] * ndiffy * diffx + \\\n        img[ucoory, lcoorx, :] * diffy * ndiffx\n    return ret[:, :, 0, :]\n\n\nclass GaussianDeform(ImageAugmentor):\n    """"""\n    Some kind of slow deformation I made up. Don\'t count on it.\n    """"""\n\n    # TODO input/output with different shape\n\n    def __init__(self, anchors, shape, sigma=0.5, randrange=None):\n        """"""\n        Args:\n            anchors (list): list of center coordinates in range [0,1].\n            shape(list or tuple): image shape in [h, w].\n            sigma (float): sigma for Gaussian weight\n            randrange (int): offset range. Defaults to shape[0] / 8\n        """"""\n        logger.warn(""GaussianDeform is slow. Consider using it with 4 or more prefetching processes."")\n        super(GaussianDeform, self).__init__()\n        self.anchors = anchors\n        self.K = len(self.anchors)\n        self.shape = shape\n        self.grid = np.mgrid[0:self.shape[0], 0:self.shape[1]].transpose(1, 2, 0)\n        self.grid = self.grid.astype(\'float32\')  # HxWx2\n\n        gm = GaussianMap(self.shape, sigma=sigma)\n        self.gws = np.array([gm.get_gaussian_weight(ank)\n                             for ank in self.anchors], dtype=\'float32\')  # KxHxW\n        self.gws = self.gws.transpose(1, 2, 0)  # HxWxK\n        if randrange is None:\n            self.randrange = self.shape[0] / 8\n        else:\n            self.randrange = randrange\n        self.sigma = sigma\n\n    def get_transform(self, img):\n        v = self.rng.rand(self.K, 2).astype(\'float32\') - 0.5\n        v = v * 2 * self.randrange\n        return TransformFactory(name=str(self), apply_image=lambda img: self._augment(img, v))\n\n    def _augment(self, img, v):\n        grid = self.grid + np.dot(self.gws, v)\n        return np_sample(img, grid)\n'"
tensorpack/dataflow/imgaug/external.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\n\nfrom .base import ImageAugmentor\nfrom .transform import Transform\n\n__all__ = [\'IAAugmentor\', \'Albumentations\']\n\n\nclass IAATransform(Transform):\n    def __init__(self, aug, img_shape):\n        self._init(locals())\n\n    def apply_image(self, img):\n        return self.aug.augment_image(img)\n\n    def apply_coords(self, coords):\n        import imgaug as IA\n        points = [IA.Keypoint(x=x, y=y) for x, y in coords]\n        points = IA.KeypointsOnImage(points, shape=self.img_shape)\n        augmented = self.aug.augment_keypoints([points])[0].keypoints\n        return np.asarray([[p.x, p.y] for p in augmented])\n\n\nclass IAAugmentor(ImageAugmentor):\n    """"""\n    Wrap an augmentor form the IAA library: https://github.com/aleju/imgaug.\n    Both images and coordinates are supported.\n\n    Note:\n        1. It\'s NOT RECOMMENDED\n           to use coordinates because the IAA library does not handle coordinates accurately.\n\n        2. Only uint8 images are supported by the IAA library.\n\n        3. The IAA library can only produces images of the same shape.\n\n    Example:\n\n    .. code-block:: python\n\n        from imgaug import augmenters as iaa  # this is the aleju/imgaug library\n        from tensorpack import imgaug  # this is not the aleju/imgaug library\n        # or from dataflow import imgaug  # if you\'re using the standalone version of dataflow\n        myaug = imgaug.IAAugmentor(\n            iaa.Sequential([\n                iaa.Sharpen(alpha=(0, 1), lightness=(0.75, 1.5)),\n                iaa.Fliplr(0.5),\n                iaa.Crop(px=(0, 100)),\n            ])\n    """"""\n\n    def __init__(self, augmentor):\n        """"""\n        Args:\n            augmentor (iaa.Augmenter):\n        """"""\n        super(IAAugmentor, self).__init__()\n        self._aug = augmentor\n\n    def get_transform(self, img):\n        return IAATransform(self._aug.to_deterministic(), img.shape)\n\n\nclass AlbumentationsTransform(Transform):\n    def __init__(self, aug, param):\n        self._init(locals())\n\n    def apply_image(self, img):\n        return self.aug.apply(img, **self.param)\n\n\nclass Albumentations(ImageAugmentor):\n    """"""\n    Wrap an augmentor form the albumentations library: https://github.com/albu/albumentations.\n    Coordinate augmentation is not supported by the library.\n\n    Example:\n\n    .. code-block:: python\n\n        from tensorpack import imgaug\n        # or from dataflow import imgaug  # if you\'re using the standalone version of dataflow\n        import albumentations as AB\n        myaug = imgaug.Albumentations(AB.RandomRotate90(p=1))\n    """"""\n    def __init__(self, augmentor):\n        """"""\n        Args:\n            augmentor (albumentations.BasicTransform):\n        """"""\n        super(Albumentations, self).__init__()\n        self._aug = augmentor\n\n    def get_transform(self, img):\n        return AlbumentationsTransform(self._aug, self._aug.get_params())\n'"
tensorpack/dataflow/imgaug/geometry.py,0,"b'# -*- coding: utf-8 -*-\n# File: geometry.py\n\n\nimport math\nimport numpy as np\nimport cv2\n\nfrom .base import ImageAugmentor\nfrom .transform import WarpAffineTransform, CropTransform, TransformList\n\n__all__ = [\'Shift\', \'Rotation\', \'RotationAndCropValid\', \'Affine\']\n\n\nclass Shift(ImageAugmentor):\n    """""" Random horizontal and vertical shifts """"""\n\n    def __init__(self, horiz_frac=0, vert_frac=0,\n                 border=cv2.BORDER_REPLICATE, border_value=0):\n        """"""\n        Args:\n            horiz_frac (float): max abs fraction for horizontal shift\n            vert_frac (float): max abs fraction for horizontal shift\n            border: cv2 border method\n            border_value: cv2 border value for border=cv2.BORDER_CONSTANT\n        """"""\n        assert horiz_frac < 1.0 and vert_frac < 1.0\n        super(Shift, self).__init__()\n        self._init(locals())\n\n    def get_transform(self, img):\n        max_dx = self.horiz_frac * img.shape[1]\n        max_dy = self.vert_frac * img.shape[0]\n        dx = np.round(self._rand_range(-max_dx, max_dx))\n        dy = np.round(self._rand_range(-max_dy, max_dy))\n\n        mat = np.array([[1, 0, dx], [0, 1, dy]], dtype=\'float32\')\n        return WarpAffineTransform(\n            mat, img.shape[1::-1],\n            borderMode=self.border, borderValue=self.border_value)\n\n\nclass Rotation(ImageAugmentor):\n    """""" Random rotate the image w.r.t a random center""""""\n\n    def __init__(self, max_deg, center_range=(0, 1),\n                 interp=cv2.INTER_LINEAR,\n                 border=cv2.BORDER_REPLICATE, step_deg=None, border_value=0):\n        """"""\n        Args:\n            max_deg (float): max abs value of the rotation angle (in degree).\n            center_range (tuple): (min, max) range of the random rotation center.\n            interp: cv2 interpolation method\n            border: cv2 border method\n            step_deg (float): if not None, the stepping of the rotation\n                angle. The rotation angle will be a multiple of step_deg. This\n                option requires ``max_deg==180`` and step_deg has to be a divisor of 180)\n            border_value: cv2 border value for border=cv2.BORDER_CONSTANT\n        """"""\n        assert step_deg is None or (max_deg == 180 and max_deg % step_deg == 0)\n        super(Rotation, self).__init__()\n        self._init(locals())\n\n    def get_transform(self, img):\n        center = img.shape[1::-1] * self._rand_range(\n            self.center_range[0], self.center_range[1], (2,))\n        deg = self._rand_range(-self.max_deg, self.max_deg)\n        if self.step_deg:\n            deg = deg // self.step_deg * self.step_deg\n        """"""\n        The correct center is shape*0.5-0.5. This can be verified by:\n\n        SHAPE = 7\n        arr = np.random.rand(SHAPE, SHAPE)\n        orig = arr\n        c = SHAPE * 0.5 - 0.5\n        c = (c, c)\n        for k in range(4):\n            mat = cv2.getRotationMatrix2D(c, 90, 1)\n            arr = cv2.warpAffine(arr, mat, arr.shape)\n        assert np.all(arr == orig)\n        """"""\n        mat = cv2.getRotationMatrix2D(tuple(center - 0.5), deg, 1)\n        return WarpAffineTransform(\n            mat, img.shape[1::-1], interp=self.interp,\n            borderMode=self.border, borderValue=self.border_value)\n\n\nclass RotationAndCropValid(ImageAugmentor):\n    """""" Random rotate and then crop the largest possible rectangle.\n        Note that this will produce images of different shapes.\n    """"""\n\n    def __init__(self, max_deg, interp=cv2.INTER_LINEAR, step_deg=None):\n        """"""\n        Args:\n            max_deg, interp, step_deg: same as :class:`Rotation`\n        """"""\n        assert step_deg is None or (max_deg == 180 and max_deg % step_deg == 0)\n        super(RotationAndCropValid, self).__init__()\n        self._init(locals())\n\n    def _get_deg(self, img):\n        deg = self._rand_range(-self.max_deg, self.max_deg)\n        if self.step_deg:\n            deg = deg // self.step_deg * self.step_deg\n        return deg\n\n    def get_transform(self, img):\n        deg = self._get_deg(img)\n\n        h, w = img.shape[:2]\n        center = (img.shape[1] * 0.5, img.shape[0] * 0.5)\n        rot_m = cv2.getRotationMatrix2D((center[0] - 0.5, center[1] - 0.5), deg, 1)\n        tfm = WarpAffineTransform(rot_m, (w, h), interp=self.interp)\n\n        neww, newh = RotationAndCropValid.largest_rotated_rect(w, h, deg)\n        neww = min(neww, w)\n        newh = min(newh, h)\n        newx = int(center[0] - neww * 0.5)\n        newy = int(center[1] - newh * 0.5)\n        tfm2 = CropTransform(newy, newx, newh, neww)\n        return TransformList([tfm, tfm2])\n\n    @staticmethod\n    def largest_rotated_rect(w, h, angle):\n        """"""\n        Get largest rectangle after rotation.\n        http://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n        """"""\n        angle = angle / 180.0 * math.pi\n        if w <= 0 or h <= 0:\n            return 0, 0\n\n        width_is_longer = w >= h\n        side_long, side_short = (w, h) if width_is_longer else (h, w)\n\n        # since the solutions for angle, -angle and 180-angle are all the same,\n        # if suffices to look at the first quadrant and the absolute values of sin,cos:\n        sin_a, cos_a = abs(math.sin(angle)), abs(math.cos(angle))\n        if side_short <= 2. * sin_a * cos_a * side_long:\n            # half constrained case: two crop corners touch the longer side,\n            #   the other two corners are on the mid-line parallel to the longer line\n            x = 0.5 * side_short\n            wr, hr = (x / sin_a, x / cos_a) if width_is_longer else (x / cos_a, x / sin_a)\n        else:\n            # fully constrained case: crop touches all 4 sides\n            cos_2a = cos_a * cos_a - sin_a * sin_a\n            wr, hr = (w * cos_a - h * sin_a) / cos_2a, (h * cos_a - w * sin_a) / cos_2a\n        return int(np.round(wr)), int(np.round(hr))\n\n\nclass Affine(ImageAugmentor):\n    """"""\n    Random affine transform of the image w.r.t to the image center.\n    Transformations involve:\n\n    - Translation (""move"" image on the x-/y-axis)\n    - Rotation\n    - Scaling (""zoom"" in/out)\n    - Shear (move one side of the image, turning a square into a trapezoid)\n    """"""\n\n    def __init__(self, scale=None, translate_frac=None, rotate_max_deg=0.0, shear=0.0,\n                 interp=cv2.INTER_LINEAR, border=cv2.BORDER_REPLICATE, border_value=0):\n        """"""\n        Args:\n            scale (tuple of 2 floats): scaling factor interval, e.g (a, b), then scale is\n                randomly sampled from the range a <= scale <= b. Will keep\n                original scale by default.\n            translate_frac (tuple of 2 floats): tuple of max abs fraction for horizontal\n                and vertical translation. For example translate_frac=(a, b), then horizontal shift\n                is randomly sampled in the range 0 < dx < img_width * a and vertical shift is\n                randomly sampled in the range 0 < dy < img_height * b. Will\n                not translate by default.\n            shear (float): max abs shear value in degrees between 0 to 180\n            interp: cv2 interpolation method\n            border: cv2 border method\n            border_value: cv2 border value for border=cv2.BORDER_CONSTANT\n        """"""\n        if scale is not None:\n            assert isinstance(scale, tuple) and len(scale) == 2, \\\n                ""Argument scale should be a tuple of two floats, e.g (a, b)""\n\n        if translate_frac is not None:\n            assert isinstance(translate_frac, tuple) and len(translate_frac) == 2, \\\n                ""Argument translate_frac should be a tuple of two floats, e.g (a, b)""\n\n        assert shear >= 0.0, ""Argument shear should be between 0.0 and 180.0""\n\n        super(Affine, self).__init__()\n        self._init(locals())\n\n    def get_transform(self, img):\n        if self.scale is not None:\n            scale = self._rand_range(self.scale[0], self.scale[1])\n        else:\n            scale = 1.0\n\n        if self.translate_frac is not None:\n            max_dx = self.translate_frac[0] * img.shape[1]\n            max_dy = self.translate_frac[1] * img.shape[0]\n            dx = np.round(self._rand_range(-max_dx, max_dx))\n            dy = np.round(self._rand_range(-max_dy, max_dy))\n        else:\n            dx = 0\n            dy = 0\n\n        if self.shear > 0.0:\n            shear = self._rand_range(-self.shear, self.shear)\n            sin_shear = math.sin(math.radians(shear))\n            cos_shear = math.cos(math.radians(shear))\n        else:\n            sin_shear = 0.0\n            cos_shear = 1.0\n\n        center = (img.shape[1::-1] * np.array((0.5, 0.5))) - 0.5\n        deg = self._rand_range(-self.rotate_max_deg, self.rotate_max_deg)\n\n        transform_matrix = cv2.getRotationMatrix2D(tuple(center), deg, scale)\n\n        # Apply shear :\n        if self.shear > 0.0:\n            m00 = transform_matrix[0, 0]\n            m01 = transform_matrix[0, 1]\n            m10 = transform_matrix[1, 0]\n            m11 = transform_matrix[1, 1]\n            transform_matrix[0, 1] = m01 * cos_shear + m00 * sin_shear\n            transform_matrix[1, 1] = m11 * cos_shear + m10 * sin_shear\n            # Add correction term to keep the center unchanged\n            tx = center[0] * (1.0 - m00) - center[1] * transform_matrix[0, 1]\n            ty = -center[0] * m10 + center[1] * (1.0 - transform_matrix[1, 1])\n            transform_matrix[0, 2] = tx\n            transform_matrix[1, 2] = ty\n\n        # Apply shift :\n        transform_matrix[0, 2] += dx\n        transform_matrix[1, 2] += dy\n        return WarpAffineTransform(transform_matrix, img.shape[1::-1],\n                                   self.interp, self.border, self.border_value)\n'"
tensorpack/dataflow/imgaug/imgaug_test.py,0,"b'# -*- coding: utf-8 -*-\n# File: _test.py\n\n\nimport sys\nimport numpy as np\nimport cv2\nimport unittest\n\nfrom .base import ImageAugmentor, AugmentorList\nfrom .imgproc import Contrast\nfrom .noise import SaltPepperNoise\nfrom .misc import Flip, Resize\n\n\ndef _rand_image(shape=(20, 20)):\n    return np.random.rand(*shape).astype(""float32"")\n\n\nclass LegacyBrightness(ImageAugmentor):\n    def __init__(self, delta, clip=True):\n        super(LegacyBrightness, self).__init__()\n        assert delta > 0\n        self._init(locals())\n\n    def _get_augment_params(self, _):\n        v = self._rand_range(-self.delta, self.delta)\n        return v\n\n    def _augment(self, img, v):\n        old_dtype = img.dtype\n        img = img.astype(\'float32\')\n        img += v\n        if self.clip or old_dtype == np.uint8:\n            img = np.clip(img, 0, 255)\n        return img.astype(old_dtype)\n\n\nclass LegacyFlip(ImageAugmentor):\n    def __init__(self, horiz=False, vert=False, prob=0.5):\n        super(LegacyFlip, self).__init__()\n        if horiz and vert:\n            raise ValueError(""Cannot do both horiz and vert. Please use two Flip instead."")\n        elif horiz:\n            self.code = 1\n        elif vert:\n            self.code = 0\n        else:\n            raise ValueError(""At least one of horiz or vert has to be True!"")\n        self._init(locals())\n\n    def _get_augment_params(self, img):\n        h, w = img.shape[:2]\n        do = self._rand_range() < self.prob\n        return (do, h, w)\n\n    def _augment(self, img, param):\n        do, _, _ = param\n        if do:\n            ret = cv2.flip(img, self.code)\n            if img.ndim == 3 and ret.ndim == 2:\n                ret = ret[:, :, np.newaxis]\n        else:\n            ret = img\n        return ret\n\n    def _augment_coords(self, coords, param):\n        do, h, w = param\n        if do:\n            if self.code == 0:\n                coords[:, 1] = h - coords[:, 1]\n            elif self.code == 1:\n                coords[:, 0] = w - coords[:, 0]\n        return coords\n\n\nclass ImgAugTest(unittest.TestCase):\n    def _get_augs(self):\n        return AugmentorList([\n            Contrast((0.8, 1.2)),\n            Flip(horiz=True),\n            Resize((30, 30)),\n            SaltPepperNoise()\n        ])\n\n    def _get_augs_with_legacy(self):\n        return AugmentorList([\n            LegacyBrightness(0.5),\n            LegacyFlip(horiz=True),\n            Resize((30, 30)),\n            SaltPepperNoise()\n        ])\n\n    def test_augmentors(self):\n        augmentors = self._get_augs()\n\n        img = _rand_image()\n        orig = img.copy()\n        tfms = augmentors.get_transform(img)\n\n        # test printing\n        print(augmentors)\n        print(tfms)\n\n        newimg = tfms.apply_image(img)\n        print(tfms)  # lazy ones will instantiate after the first apply\n\n        newimg2 = tfms.apply_image(orig)\n        self.assertTrue(np.allclose(newimg, newimg2))\n        self.assertEqual(newimg2.shape[0], 30)\n\n        coords = np.asarray([[0, 0], [10, 12]], dtype=""float32"")\n        tfms.apply_coords(coords)\n\n    def test_legacy_usage(self):\n        augmentors = self._get_augs()\n\n        img = _rand_image()\n        orig = img.copy()\n        newimg, tfms = augmentors.augment_return_params(img)\n        newimg2 = augmentors.augment_with_params(orig, tfms)\n        self.assertTrue(np.allclose(newimg, newimg2))\n        self.assertEqual(newimg2.shape[0], 30)\n\n        coords = np.asarray([[0, 0], [10, 12]], dtype=""float32"")\n        augmentors.augment_coords(coords, tfms)\n\n    def test_legacy_augs_new_usage(self):\n        augmentors = self._get_augs_with_legacy()\n\n        img = _rand_image()\n        orig = img.copy()\n        tfms = augmentors.get_transform(img)\n        newimg = tfms.apply_image(img)\n        newimg2 = tfms.apply_image(orig)\n        self.assertTrue(np.allclose(newimg, newimg2))\n        self.assertEqual(newimg2.shape[0], 30)\n\n        coords = np.asarray([[0, 0], [10, 12]], dtype=""float32"")\n        tfms.apply_coords(coords)\n\n    def test_legacy_augs_legacy_usage(self):\n        augmentors = self._get_augs_with_legacy()\n\n        img = _rand_image()\n        orig = img.copy()\n        newimg, tfms = augmentors.augment_return_params(img)\n        newimg2 = augmentors.augment_with_params(orig, tfms)\n        self.assertTrue(np.allclose(newimg, newimg2))\n        self.assertEqual(newimg2.shape[0], 30)\n\n        coords = np.asarray([[0, 0], [10, 12]], dtype=""float32"")\n        augmentors.augment_coords(coords, tfms)\n\n\nif __name__ == \'__main__\':\n    anchors = [(0.2, 0.2), (0.7, 0.2), (0.8, 0.8), (0.5, 0.5), (0.2, 0.5)]\n    augmentors = AugmentorList([\n        Contrast((0.8, 1.2)),\n        Flip(horiz=True),\n        # RandomCropRandomShape(0.3),\n        SaltPepperNoise()\n    ])\n\n    img = cv2.imread(sys.argv[1])\n    newimg, prms = augmentors._augment_return_params(img)\n    cv2.imshow("" "", newimg.astype(\'uint8\'))\n    cv2.waitKey()\n\n    newimg = augmentors._augment(img, prms)\n    cv2.imshow("" "", newimg.astype(\'uint8\'))\n    cv2.waitKey()\n'"
tensorpack/dataflow/imgaug/imgproc.py,0,"b'# -*- coding: utf-8 -*-\n# File: imgproc.py\n\n\nimport numpy as np\nimport cv2\n\nfrom ...utils.develop import log_deprecated\nfrom .base import PhotometricAugmentor\n\n__all__ = [\'Hue\', \'Brightness\', \'BrightnessScale\', \'Contrast\', \'MeanVarianceNormalize\',\n           \'GaussianBlur\', \'Gamma\', \'Clip\', \'Saturation\', \'Lighting\', \'MinMaxNormalize\']\n\n\nclass Hue(PhotometricAugmentor):\n    """""" Randomly change color hue.\n    """"""\n\n    def __init__(self, range=(0, 180), rgb=True):\n        """"""\n        Args:\n            range(list or tuple): range from which the applied hue offset is selected\n                (maximum range can be [-90,90] for both uint8 and float32)\n            rgb (bool): whether input is RGB or BGR.\n        """"""\n        super(Hue, self).__init__()\n        rgb = bool(rgb)\n        self._init(locals())\n\n    def _get_augment_params(self, _):\n        return self._rand_range(*self.range)\n\n    def _augment(self, img, hue):\n        m = cv2.COLOR_BGR2HSV if not self.rgb else cv2.COLOR_RGB2HSV\n        hsv = cv2.cvtColor(img, m)\n        # https://docs.opencv.org/3.2.0/de/d25/imgproc_color_conversions.html#color_convert_rgb_hsv\n        if hsv.dtype.itemsize == 1:\n            # OpenCV uses 0-179 for 8-bit images\n            hsv[..., 0] = (hsv[..., 0] + hue) % 180\n        else:\n            # OpenCV uses 0-360 for floating point images\n            hsv[..., 0] = (hsv[..., 0] + 2 * hue) % 360\n        m = cv2.COLOR_HSV2BGR if not self.rgb else cv2.COLOR_HSV2RGB\n        img = cv2.cvtColor(hsv, m)\n        return img\n\n\nclass Brightness(PhotometricAugmentor):\n    """"""\n    Adjust brightness by adding a random number.\n    """"""\n    def __init__(self, delta, clip=True):\n        """"""\n        Args:\n            delta (float): Randomly add a value within [-delta,delta]\n            clip (bool): clip results to [0,255] even when data type is not uint8.\n        """"""\n        super(Brightness, self).__init__()\n        assert delta > 0\n        self._init(locals())\n\n    def _get_augment_params(self, _):\n        return self._rand_range(-self.delta, self.delta)\n\n    def _augment(self, img, v):\n        old_dtype = img.dtype\n        img = img.astype(\'float32\')\n        img += v\n        if self.clip or old_dtype == np.uint8:\n            img = np.clip(img, 0, 255)\n        return img.astype(old_dtype)\n\n\nclass BrightnessScale(PhotometricAugmentor):\n    """"""\n    Adjust brightness by scaling by a random factor.\n    """"""\n    def __init__(self, range, clip=True):\n        """"""\n        Args:\n            range (tuple): Randomly scale the image by a factor in (range[0], range[1])\n            clip (bool): clip results to [0,255] even when data type is not uint8.\n        """"""\n        super(BrightnessScale, self).__init__()\n        self._init(locals())\n\n    def _get_augment_params(self, _):\n        return self._rand_range(*self.range)\n\n    def _augment(self, img, v):\n        old_dtype = img.dtype\n        img = img.astype(\'float32\')\n        img *= v\n        if self.clip or old_dtype == np.uint8:\n            img = np.clip(img, 0, 255)\n        return img.astype(old_dtype)\n\n\nclass Contrast(PhotometricAugmentor):\n    """"""\n    Apply ``x = (x - mean) * contrast_factor + mean`` to each channel.\n    """"""\n\n    def __init__(self, factor_range, rgb=None, clip=True):\n        """"""\n        Args:\n            factor_range (list or tuple): an interval to randomly sample the `contrast_factor`.\n            rgb (bool or None): if None, use the mean per-channel.\n            clip (bool): clip to [0, 255] even when data type is not uint8.\n        """"""\n        super(Contrast, self).__init__()\n        self._init(locals())\n\n    def _get_augment_params(self, _):\n        return self._rand_range(*self.factor_range)\n\n    def _augment(self, img, r):\n        old_dtype = img.dtype\n\n        if img.ndim == 3:\n            if self.rgb is not None:\n                m = cv2.COLOR_RGB2GRAY if self.rgb else cv2.COLOR_BGR2GRAY\n                grey = cv2.cvtColor(img.astype(\'float32\'), m)\n                mean = np.mean(grey)\n            else:\n                mean = np.mean(img, axis=(0, 1), keepdims=True)\n        else:\n            mean = np.mean(img)\n\n        img = img * r + mean * (1 - r)\n        if self.clip or old_dtype == np.uint8:\n            img = np.clip(img, 0, 255)\n        return img.astype(old_dtype)\n\n\nclass MeanVarianceNormalize(PhotometricAugmentor):\n    """"""\n    Linearly scales the image to have zero mean and unit norm.\n    ``x = (x - mean) / adjusted_stddev``\n    where ``adjusted_stddev = max(stddev, 1.0/sqrt(num_pixels * channels))``\n\n    This augmentor always returns float32 images.\n    """"""\n\n    def __init__(self, all_channel=True):\n        """"""\n        Args:\n            all_channel (bool): if True, normalize all channels together. else separately.\n        """"""\n        self._init(locals())\n\n    def _augment(self, img, _):\n        img = img.astype(\'float32\')\n        if self.all_channel:\n            mean = np.mean(img)\n            std = np.std(img)\n        else:\n            mean = np.mean(img, axis=(0, 1), keepdims=True)\n            std = np.std(img, axis=(0, 1), keepdims=True)\n        std = np.maximum(std, 1.0 / np.sqrt(np.prod(img.shape)))\n        img = (img - mean) / std\n        return img\n\n\nclass GaussianBlur(PhotometricAugmentor):\n    """""" Gaussian blur the image with random window size""""""\n\n    def __init__(self, size_range=(0, 3), sigma_range=(0, 0), symmetric=True, max_size=None):\n        """"""\n        Args:\n            size_range (tuple[int]): Gaussian window size would be 2 * size +\n                1, where size is randomly sampled from this [low, high) range.\n            sigma_range (tuple[float]): min,max of the sigma value. 0 means\n                opencv\'s default.\n            symmetric (bool): whether to use the same size & sigma for x and y.\n            max_size (int): deprecated\n        """"""\n        super(GaussianBlur, self).__init__()\n        if not isinstance(size_range, (list, tuple)):\n            size_range = (0, size_range)\n        assert isinstance(sigma_range, (list, tuple)), sigma_range\n        if max_size is not None:\n            log_deprecated(""GaussianBlur(max_size=)"", ""Use size_range= instead!"", ""2020-09-01"")\n            size_range = (0, max_size)\n        self._init(locals())\n\n    def _get_augment_params(self, _):\n        size_xy = self.rng.randint(self.size_range[0], self.size_range[1], size=(2,)) * 2 + 1\n        sigma_xy = self._rand_range(*self.sigma_range, size=(2,))\n        if self.symmetric:\n            size_xy[1] = size_xy[0]\n            sigma_xy[1] = sigma_xy[0]\n        return tuple(size_xy), tuple(sigma_xy)\n\n    def _augment(self, img, prm):\n        size, sigma = prm\n        return np.reshape(cv2.GaussianBlur(img, size, sigmaX=sigma[0], sigmaY=sigma[1],\n                                           borderType=cv2.BORDER_REPLICATE), img.shape)\n\n\nclass Gamma(PhotometricAugmentor):\n    """""" Randomly adjust gamma """"""\n    def __init__(self, range=(-0.5, 0.5)):\n        """"""\n        Args:\n            range(list or tuple): gamma range\n        """"""\n        super(Gamma, self).__init__()\n        self._init(locals())\n\n    def _get_augment_params(self, _):\n        return self._rand_range(*self.range)\n\n    def _augment(self, img, gamma):\n        old_dtype = img.dtype\n        lut = ((np.arange(256, dtype=\'float32\') / 255) ** (1. / (1. + gamma)) * 255).astype(\'uint8\')\n        img = np.clip(img, 0, 255).astype(\'uint8\')\n        ret = cv2.LUT(img, lut).astype(old_dtype)\n        if img.ndim == 3 and ret.ndim == 2:\n            ret = ret[:, :, np.newaxis]\n        return ret\n\n\nclass Clip(PhotometricAugmentor):\n    """""" Clip the pixel values """"""\n\n    def __init__(self, min=0, max=255):\n        """"""\n        Args:\n            min, max: the clip range\n        """"""\n        self._init(locals())\n\n    def _augment(self, img, _):\n        return np.clip(img, self.min, self.max)\n\n\nclass Saturation(PhotometricAugmentor):\n    """""" Randomly adjust saturation.\n        Follows the implementation in `fb.resnet.torch\n        <https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L218>`__.\n    """"""\n\n    def __init__(self, alpha=0.4, rgb=True, clip=True):\n        """"""\n        Args:\n            alpha(float): maximum saturation change.\n            rgb (bool): whether input is RGB or BGR.\n            clip (bool): clip results to [0,255] even when data type is not uint8.\n        """"""\n        super().__init__()\n        rgb = bool(rgb)\n        assert alpha < 1\n        self._init(locals())\n\n    def _get_augment_params(self, _):\n        return 1 + self._rand_range(-self.alpha, self.alpha)\n\n    def _augment(self, img, v):\n        old_dtype = img.dtype\n        m = cv2.COLOR_RGB2GRAY if self.rgb else cv2.COLOR_BGR2GRAY\n        grey = cv2.cvtColor(img, m)\n        ret = img * v + (grey * (1 - v))[:, :, np.newaxis]\n        if self.clip or old_dtype == np.uint8:\n            ret = np.clip(ret, 0, 255)\n        return ret.astype(old_dtype)\n\n\nclass Lighting(PhotometricAugmentor):\n    """""" Lighting noise, as in the paper\n        `ImageNet Classification with Deep Convolutional Neural Networks\n        <https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf>`_.\n        The implementation follows `fb.resnet.torch\n        <https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L184>`__.\n    """"""\n\n    def __init__(self, std, eigval, eigvec, clip=True):\n        """"""\n        Args:\n            std (float): maximum standard deviation\n            eigval: a vector of (3,). The eigenvalues of 3 channels.\n            eigvec: a 3x3 matrix. Each column is one eigen vector.\n            clip (bool): clip results to [0,255] even when data type is not uint8.\n        """"""\n        super(Lighting, self).__init__()\n        eigval = np.asarray(eigval, dtype=""float32"")\n        eigvec = np.asarray(eigvec, dtype=""float32"")\n        assert eigval.shape == (3,)\n        assert eigvec.shape == (3, 3)\n        self._init(locals())\n\n    def _get_augment_params(self, img):\n        assert img.shape[2] == 3\n        return (self.rng.randn(3) * self.std).astype(""float32"")\n\n    def _augment(self, img, v):\n        old_dtype = img.dtype\n        v = v * self.eigval\n        v = v.reshape((3, 1))\n        inc = np.dot(self.eigvec, v).reshape((3,))\n        img = np.add(img, inc)\n        if self.clip or old_dtype == np.uint8:\n            img = np.clip(img, 0, 255)\n        return img.astype(old_dtype)\n\n\nclass MinMaxNormalize(PhotometricAugmentor):\n    """"""\n    Linearly scales the image to the range [min, max].\n\n    This augmentor always returns float32 images.\n    """"""\n    def __init__(self, min=0, max=255, all_channel=True):\n        """"""\n        Args:\n            max (float): The new maximum value\n            min (float): The new minimum value\n            all_channel (bool): if True, normalize all channels together. else separately.\n        """"""\n        self._init(locals())\n\n    def _augment(self, img, _):\n        img = img.astype(\'float32\')\n        if self.all_channel:\n            minimum = np.min(img)\n            maximum = np.max(img)\n        else:\n            minimum = np.min(img, axis=(0, 1), keepdims=True)\n            maximum = np.max(img, axis=(0, 1), keepdims=True)\n        img = (self.max - self.min) * (img - minimum) / (maximum - minimum) + self.min\n        return img\n'"
tensorpack/dataflow/imgaug/meta.py,0,"b'# -*- coding: utf-8 -*-\n# File: meta.py\n\n\nfrom .base import ImageAugmentor\nfrom .transform import NoOpTransform, TransformList, TransformFactory\n\n__all__ = [\'RandomChooseAug\', \'MapImage\', \'Identity\', \'RandomApplyAug\',\n           \'RandomOrderAug\']\n\n\nclass Identity(ImageAugmentor):\n    """""" A no-op augmentor """"""\n    def get_transform(self, img):\n        return NoOpTransform()\n\n\nclass RandomApplyAug(ImageAugmentor):\n    """""" Randomly apply the augmentor with a probability.\n        Otherwise do nothing\n    """"""\n\n    def __init__(self, aug, prob):\n        """"""\n        Args:\n            aug (ImageAugmentor): an augmentor.\n            prob (float): the probability to apply the augmentor.\n        """"""\n        self._init(locals())\n        super(RandomApplyAug, self).__init__()\n\n    def get_transform(self, img):\n        p = self.rng.rand()\n        if p < self.prob:\n            return self.aug.get_transform(img)\n        else:\n            return NoOpTransform()\n\n    def reset_state(self):\n        super(RandomApplyAug, self).reset_state()\n        self.aug.reset_state()\n\n\nclass RandomChooseAug(ImageAugmentor):\n    """""" Randomly choose one from a list of augmentors """"""\n    def __init__(self, aug_lists):\n        """"""\n        Args:\n            aug_lists (list): list of augmentors, or list of (augmentor, probability) tuples\n        """"""\n        if isinstance(aug_lists[0], (tuple, list)):\n            prob = [k[1] for k in aug_lists]\n            aug_lists = [k[0] for k in aug_lists]\n            self._init(locals())\n        else:\n            prob = [1.0 / len(aug_lists)] * len(aug_lists)\n            self._init(locals())\n        super(RandomChooseAug, self).__init__()\n\n    def reset_state(self):\n        super(RandomChooseAug, self).reset_state()\n        for a in self.aug_lists:\n            a.reset_state()\n\n    def get_transform(self, img):\n        aug_idx = self.rng.choice(len(self.aug_lists), p=self.prob)\n        return self.aug_lists[aug_idx].get_transform(img)\n\n\nclass RandomOrderAug(ImageAugmentor):\n    """"""\n    Apply the augmentors with randomized order.\n    """"""\n\n    def __init__(self, aug_lists):\n        """"""\n        Args:\n            aug_lists (list): list of augmentors.\n                The augmentors are assumed to not change the shape of images.\n        """"""\n        self._init(locals())\n        super(RandomOrderAug, self).__init__()\n\n    def reset_state(self):\n        super(RandomOrderAug, self).reset_state()\n        for a in self.aug_lists:\n            a.reset_state()\n\n    def get_transform(self, img):\n        # Note: this makes assumption that the augmentors do not make changes\n        # to the image that will affect how the transforms will be instantiated\n        # in the subsequent augmentors.\n        idxs = self.rng.permutation(len(self.aug_lists))\n        tfms = [self.aug_lists[k].get_transform(img)\n                for k in range(len(self.aug_lists))]\n        return TransformList([tfms[k] for k in idxs])\n\n\nclass MapImage(ImageAugmentor):\n    """"""\n    Map the image array by simple functions.\n    """"""\n\n    def __init__(self, func, coord_func=None):\n        """"""\n        Args:\n            func: a function which takes an image array and return an augmented one\n            coord_func: optional. A function which takes coordinates and return augmented ones.\n                Coordinates should be Nx2 array of (x, y)s.\n        """"""\n        super(MapImage, self).__init__()\n        self.func = func\n        self.coord_func = coord_func\n\n    def get_transform(self, img):\n        if self.coord_func:\n            return TransformFactory(name=""MapImage"", apply_image=self.func, apply_coords=self.coord_func)\n        else:\n            return TransformFactory(name=""MapImage"", apply_image=self.func)\n'"
tensorpack/dataflow/imgaug/misc.py,0,"b'# -*- coding: utf-8 -*-\n# File: misc.py\n\nimport cv2\n\nfrom ...utils import logger\nfrom ...utils.argtools import shape2d\nfrom .base import ImageAugmentor\nfrom .transform import ResizeTransform, NoOpTransform, FlipTransform, TransposeTransform\n\n__all__ = [\'Flip\', \'Resize\', \'RandomResize\', \'ResizeShortestEdge\', \'Transpose\']\n\n\nclass Flip(ImageAugmentor):\n    """"""\n    Random flip the image either horizontally or vertically.\n    """"""\n    def __init__(self, horiz=False, vert=False, prob=0.5):\n        """"""\n        Args:\n            horiz (bool): use horizontal flip.\n            vert (bool): use vertical flip.\n            prob (float): probability of flip.\n        """"""\n        super(Flip, self).__init__()\n        if horiz and vert:\n            raise ValueError(""Cannot do both horiz and vert. Please use two Flip instead."")\n        if not horiz and not vert:\n            raise ValueError(""At least one of horiz or vert has to be True!"")\n        self._init(locals())\n\n    def get_transform(self, img):\n        h, w = img.shape[:2]\n        do = self._rand_range() < self.prob\n        if not do:\n            return NoOpTransform()\n        else:\n            return FlipTransform(h, w, self.horiz)\n\n\nclass Resize(ImageAugmentor):\n    """""" Resize image to a target size""""""\n\n    def __init__(self, shape, interp=cv2.INTER_LINEAR):\n        """"""\n        Args:\n            shape: (h, w) tuple or a int\n            interp: cv2 interpolation method\n        """"""\n        shape = tuple(shape2d(shape))\n        self._init(locals())\n\n    def get_transform(self, img):\n        return ResizeTransform(\n            img.shape[0], img.shape[1],\n            self.shape[0], self.shape[1], self.interp)\n\n\nclass ResizeShortestEdge(ImageAugmentor):\n    """"""\n    Resize the shortest edge to a certain number while\n    keeping the aspect ratio.\n    """"""\n\n    def __init__(self, size, interp=cv2.INTER_LINEAR):\n        """"""\n        Args:\n            size (int): the size to resize the shortest edge to.\n        """"""\n        size = int(size)\n        self._init(locals())\n\n    def get_transform(self, img):\n        h, w = img.shape[:2]\n        scale = self.size * 1.0 / min(h, w)\n        if h < w:\n            newh, neww = self.size, int(scale * w + 0.5)\n        else:\n            newh, neww = int(scale * h + 0.5), self.size\n        return ResizeTransform(h, w, newh, neww, self.interp)\n\n\nclass RandomResize(ImageAugmentor):\n    """""" Randomly rescale width and height of the image.""""""\n\n    def __init__(self, xrange, yrange=None, minimum=(0, 0), aspect_ratio_thres=0.15,\n                 interp=cv2.INTER_LINEAR):\n        """"""\n        Args:\n            xrange (tuple): a (min, max) tuple. If is floating point, the\n                tuple defines the range of scaling ratio of new width, e.g. (0.9, 1.2).\n                If is integer, the tuple defines the range of new width in pixels, e.g. (200, 350).\n            yrange (tuple): similar to xrange, but for height. Should be None when aspect_ratio_thres==0.\n            minimum (tuple): (xmin, ymin) in pixels. To avoid scaling down too much.\n            aspect_ratio_thres (float): discard samples which change aspect ratio\n                larger than this threshold. Set to 0 to keep aspect ratio.\n            interp: cv2 interpolation method\n        """"""\n        super(RandomResize, self).__init__()\n        assert aspect_ratio_thres >= 0\n        self._init(locals())\n\n        def is_float(tp):\n            return isinstance(tp[0], float) or isinstance(tp[1], float)\n\n        if yrange is not None:\n            assert is_float(xrange) == is_float(yrange), ""xrange and yrange has different type!""\n        self._is_scale = is_float(xrange)\n\n        if aspect_ratio_thres == 0:\n            if self._is_scale:\n                assert xrange == yrange or yrange is None\n            else:\n                if yrange is not None:\n                    logger.warn(""aspect_ratio_thres==0, yrange is not used!"")\n\n    def get_transform(self, img):\n        cnt = 0\n        h, w = img.shape[:2]\n\n        def get_dest_size():\n            if self._is_scale:\n                sx = self._rand_range(*self.xrange)\n                if self.aspect_ratio_thres == 0:\n                    sy = sx\n                else:\n                    sy = self._rand_range(*self.yrange)\n                destX = max(sx * w, self.minimum[0])\n                destY = max(sy * h, self.minimum[1])\n            else:\n                sx = self._rand_range(*self.xrange)\n                if self.aspect_ratio_thres == 0:\n                    sy = sx * 1.0 / w * h\n                else:\n                    sy = self._rand_range(*self.yrange)\n                destX = max(sx, self.minimum[0])\n                destY = max(sy, self.minimum[1])\n            return (int(destX + 0.5), int(destY + 0.5))\n\n        while True:\n            destX, destY = get_dest_size()\n            if self.aspect_ratio_thres > 0:  # don\'t check when thres == 0\n                oldr = w * 1.0 / h\n                newr = destX * 1.0 / destY\n                diff = abs(newr - oldr) / oldr\n                if diff >= self.aspect_ratio_thres + 1e-5:\n                    cnt += 1\n                    if cnt > 50:\n                        logger.warn(""RandomResize failed to augment an image"")\n                        return ResizeTransform(h, w, h, w, self.interp)\n                    continue\n            return ResizeTransform(h, w, destY, destX, self.interp)\n\n\nclass Transpose(ImageAugmentor):\n    """"""\n    Random transpose the image\n    """"""\n    def __init__(self, prob=0.5):\n        """"""\n        Args:\n            prob (float): probability of transpose.\n        """"""\n        super(Transpose, self).__init__()\n        self.prob = prob\n\n    def get_transform(self, _):\n        if self.rng.rand() < self.prob:\n            return TransposeTransform()\n        else:\n            return NoOpTransform()\n'"
tensorpack/dataflow/imgaug/noise.py,0,"b'# -*- coding: utf-8 -*-\n# File: noise.py\n\n\nimport numpy as np\nimport cv2\n\nfrom .base import PhotometricAugmentor\n\n__all__ = [\'JpegNoise\', \'GaussianNoise\', \'SaltPepperNoise\']\n\n\nclass JpegNoise(PhotometricAugmentor):\n    """""" Random JPEG noise. """"""\n\n    def __init__(self, quality_range=(40, 100)):\n        """"""\n        Args:\n            quality_range (tuple): range to sample JPEG quality\n        """"""\n        super(JpegNoise, self).__init__()\n        self._init(locals())\n\n    def _get_augment_params(self, img):\n        return self.rng.randint(*self.quality_range)\n\n    def _augment(self, img, q):\n        enc = cv2.imencode(\'.jpg\', img, [cv2.IMWRITE_JPEG_QUALITY, q])[1]\n        return cv2.imdecode(enc, 1).astype(img.dtype)\n\n\nclass GaussianNoise(PhotometricAugmentor):\n    """"""\n    Add random Gaussian noise N(0, sigma^2) of the same shape to img.\n    """"""\n    def __init__(self, sigma=1, clip=True):\n        """"""\n        Args:\n            sigma (float): stddev of the Gaussian distribution.\n            clip (bool): clip the result to [0,255] in the end.\n        """"""\n        super(GaussianNoise, self).__init__()\n        self._init(locals())\n\n    def _get_augment_params(self, img):\n        return self.rng.randn(*img.shape)\n\n    def _augment(self, img, noise):\n        old_dtype = img.dtype\n        ret = img + noise * self.sigma\n        if self.clip or old_dtype == np.uint8:\n            ret = np.clip(ret, 0, 255)\n        return ret.astype(old_dtype)\n\n\nclass SaltPepperNoise(PhotometricAugmentor):\n    """""" Salt and pepper noise.\n        Randomly set some elements in image to 0 or 255, regardless of its channels.\n    """"""\n\n    def __init__(self, white_prob=0.05, black_prob=0.05):\n        """"""\n        Args:\n            white_prob (float), black_prob (float): probabilities setting an element to 255 or 0.\n        """"""\n        assert white_prob + black_prob <= 1, ""Sum of probabilities cannot be greater than 1""\n        super(SaltPepperNoise, self).__init__()\n        self._init(locals())\n\n    def _get_augment_params(self, img):\n        return self.rng.uniform(low=0, high=1, size=img.shape)\n\n    def _augment(self, img, param):\n        img[param > (1 - self.white_prob)] = 255\n        img[param < self.black_prob] = 0\n        return img\n'"
tensorpack/dataflow/imgaug/paste.py,0,"b'# -*- coding: utf-8 -*-\n# File: paste.py\n\n\nimport numpy as np\nfrom abc import abstractmethod\n\nfrom .base import ImageAugmentor\nfrom .transform import TransformFactory\n\n__all__ = [\'CenterPaste\', \'BackgroundFiller\', \'ConstantBackgroundFiller\',\n           \'RandomPaste\']\n\n\nclass BackgroundFiller(object):\n    """""" Base class for all BackgroundFiller""""""\n\n    def fill(self, background_shape, img):\n        """"""\n        Return a proper background image of background_shape, given img.\n\n        Args:\n            background_shape (tuple): a shape (h, w)\n            img: an image\n        Returns:\n            a background image\n        """"""\n        background_shape = tuple(background_shape)\n        return self._fill(background_shape, img)\n\n    @abstractmethod\n    def _fill(self, background_shape, img):\n        pass\n\n\nclass ConstantBackgroundFiller(BackgroundFiller):\n    """""" Fill the background by a constant """"""\n\n    def __init__(self, value):\n        """"""\n        Args:\n            value (float): the value to fill the background.\n        """"""\n        self.value = value\n\n    def _fill(self, background_shape, img):\n        assert img.ndim in [3, 2]\n        if img.ndim == 3:\n            return_shape = background_shape + (img.shape[2],)\n        else:\n            return_shape = background_shape\n        return np.zeros(return_shape, dtype=img.dtype) + self.value\n\n\n# NOTE:\n# apply_coords should be implemeted in paste transform, but not yet done\n\n\nclass CenterPaste(ImageAugmentor):\n    """"""\n    Paste the image onto the center of a background canvas.\n    """"""\n\n    def __init__(self, background_shape, background_filler=None):\n        """"""\n        Args:\n            background_shape (tuple): shape of the background canvas.\n            background_filler (BackgroundFiller): How to fill the background. Defaults to zero-filler.\n        """"""\n        if background_filler is None:\n            background_filler = ConstantBackgroundFiller(0)\n\n        self._init(locals())\n\n    def get_transform(self, _):\n        return TransformFactory(name=str(self), apply_image=lambda img: self._impl(img))\n\n    def _impl(self, img):\n        img_shape = img.shape[:2]\n        assert self.background_shape[0] >= img_shape[0] and self.background_shape[1] >= img_shape[1]\n\n        background = self.background_filler.fill(\n            self.background_shape, img)\n        y0 = int((self.background_shape[0] - img_shape[0]) * 0.5)\n        x0 = int((self.background_shape[1] - img_shape[1]) * 0.5)\n        background[y0:y0 + img_shape[0], x0:x0 + img_shape[1]] = img\n        return background\n\n\nclass RandomPaste(CenterPaste):\n    """"""\n    Randomly paste the image onto a background canvas.\n    """"""\n\n    def get_transform(self, img):\n        img_shape = img.shape[:2]\n        assert self.background_shape[0] > img_shape[0] and self.background_shape[1] > img_shape[1]\n\n        y0 = self._rand_range(self.background_shape[0] - img_shape[0])\n        x0 = self._rand_range(self.background_shape[1] - img_shape[1])\n        l = int(x0), int(y0)\n        return TransformFactory(name=str(self), apply_image=lambda img: self._impl(img, l))\n\n    def _impl(self, img, loc):\n        x0, y0 = loc\n        img_shape = img.shape[:2]\n        background = self.background_filler.fill(\n            self.background_shape, img)\n        background[y0:y0 + img_shape[0], x0:x0 + img_shape[1]] = img\n        return background\n'"
tensorpack/dataflow/imgaug/transform.py,0,"b'# -*- coding: utf-8 -*-\n# File: transform.py\n\nimport numpy as np\nimport cv2\n\nfrom ...utils.argtools import log_once\n\nfrom .base import ImageAugmentor, _default_repr\n\nTransformAugmentorBase = ImageAugmentor\n""""""\nLegacy alias. Please don\'t use.\n""""""\n# This legacy augmentor requires us to import base from here, causing circular dependency.\n# Should remove this in the future.\n\n__all__ = [""Transform"", ""ResizeTransform"", ""CropTransform"", ""FlipTransform"",\n           ""TransformList"", ""TransformFactory""]\n\n\n# class WrappedImgFunc(object):\n#     def __init__(self, func, need_float=False, cast_back=True, fix_ndim=True):\n#         self.func = func\n#         self.need_float = need_float\n#         self.cast_back = cast_back\n\n#     def __call__(self, img):\n#         old_dtype = img.dtype\n#         old_ndim = img.ndim\n\n#         if self.need_float:\n#             img = img.astype(""float32"")\n\n#         img = self.func(img)\n\n#         if self.cast_back and old_dtype == np.uint8 and img.dtype != np.uint8:\n#             img = np.clip(img, 0, 255.)\n\n#         if self.cast_back:\n#             img = img.astype(old_dtype)\n\n#         if self.fix_ndim and old_ndim == 3 and img.ndim == 2:\n#             img = img[:, :, np.newaxis]\n#         return img\n\n\nclass BaseTransform(object):\n    """"""\n    Base class for all transforms, for type-check only.\n\n    Users should never interact with this class.\n    """"""\n    def _init(self, params=None):\n        if params:\n            for k, v in params.items():\n                if k != \'self\' and not k.startswith(\'_\'):\n                    setattr(self, k, v)\n\n\nclass Transform(BaseTransform):\n    """"""\n    A deterministic image transformation, used to implement\n    the (probably random) augmentors.\n\n    This class is also the place to provide a default implementation to any\n    :meth:`apply_xxx` method.\n    The current default is to raise NotImplementedError in any such methods.\n\n    All subclasses should implement `apply_image`.\n    The image should be of type uint8 in range [0, 255], or\n    floating point images in range [0, 1] or [0, 255]\n\n    Some subclasses may implement `apply_coords`, when applicable.\n    It should take and return a numpy array of Nx2, where each row is the (x, y) coordinate.\n\n    The implementation of each method may choose to modify its input data\n    in-place for efficient transformation.\n    """"""\n\n    def __init__(self):\n        # provide an empty __init__, so that __repr__ will work nicely\n        pass\n\n    def __getattr__(self, name):\n        if name.startswith(""apply_""):\n\n            def f(x):\n                raise NotImplementedError(""{} does not implement method {}"".format(self.__class__.__name__, name))\n\n            return f\n        raise AttributeError(""Transform object has no attribute {}"".format(name))\n\n    def __repr__(self):\n        try:\n            return _default_repr(self)\n        except AssertionError as e:\n            log_once(e.args[0], \'warn\')\n            return super(Transform, self).__repr__()\n\n    __str__ = __repr__\n\n\nclass ResizeTransform(Transform):\n    """"""\n    Resize the image.\n    """"""\n    def __init__(self, h, w, new_h, new_w, interp):\n        """"""\n        Args:\n            h, w (int):\n            new_h, new_w (int):\n            interp (int): cv2 interpolation method\n        """"""\n        super(ResizeTransform, self).__init__()\n        self._init(locals())\n\n    def apply_image(self, img):\n        assert img.shape[:2] == (self.h, self.w)\n        ret = cv2.resize(\n            img, (self.new_w, self.new_h),\n            interpolation=self.interp)\n        if img.ndim == 3 and ret.ndim == 2:\n            ret = ret[:, :, np.newaxis]\n        return ret\n\n    def apply_coords(self, coords):\n        coords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)\n        coords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)\n        return coords\n\n\nclass CropTransform(Transform):\n    """"""\n    Crop a subimage from an image.\n    """"""\n    def __init__(self, y0, x0, h, w):\n        super(CropTransform, self).__init__()\n        self._init(locals())\n\n    def apply_image(self, img):\n        return img[self.y0:self.y0 + self.h, self.x0:self.x0 + self.w]\n\n    def apply_coords(self, coords):\n        coords[:, 0] -= self.x0\n        coords[:, 1] -= self.y0\n        return coords\n\n\nclass WarpAffineTransform(Transform):\n    def __init__(self, mat, dsize, interp=cv2.INTER_LINEAR,\n                 borderMode=cv2.BORDER_CONSTANT, borderValue=0):\n        super(WarpAffineTransform, self).__init__()\n        self._init(locals())\n\n    def apply_image(self, img):\n        ret = cv2.warpAffine(img, self.mat, self.dsize,\n                             flags=self.interp,\n                             borderMode=self.borderMode,\n                             borderValue=self.borderValue)\n        if img.ndim == 3 and ret.ndim == 2:\n            ret = ret[:, :, np.newaxis]\n        return ret\n\n    def apply_coords(self, coords):\n        coords = np.concatenate((coords, np.ones((coords.shape[0], 1), dtype=\'f4\')), axis=1)\n        coords = np.dot(coords, self.mat.T)\n        return coords\n\n\nclass FlipTransform(Transform):\n    """"""\n    Flip the image.\n    """"""\n    def __init__(self, h, w, horiz=True):\n        """"""\n        Args:\n            h, w (int):\n            horiz (bool): whether to flip horizontally or vertically.\n        """"""\n        self._init(locals())\n\n    def apply_image(self, img):\n        if self.horiz:\n            return img[:, ::-1]\n        else:\n            return img[::-1]\n\n    def apply_coords(self, coords):\n        if self.horiz:\n            coords[:, 0] = self.w - coords[:, 0]\n        else:\n            coords[:, 1] = self.h - coords[:, 1]\n        return coords\n\n\nclass TransposeTransform(Transform):\n    """"""\n    Transpose the image.\n    """"""\n    def apply_image(self, img):\n        ret = cv2.transpose(img)\n        if img.ndim == 3 and ret.ndim == 2:\n            ret = ret[:, :, np.newaxis]\n        return ret\n\n    def apply_coords(self, coords):\n        return coords[:, ::-1]\n\n\nclass NoOpTransform(Transform):\n    """"""\n    A Transform that does nothing.\n    """"""\n    def __getattr__(self, name):\n        if name.startswith(""apply_""):\n            return lambda x: x\n        raise AttributeError(""NoOpTransform object has no attribute {}"".format(name))\n\n\nclass PhotometricTransform(Transform):\n    """"""\n    A transform which only has `apply_image` but does nothing in `apply_coords`.\n    """"""\n    def __init__(self, func, name=None):\n        """"""\n        Args:\n            func (img -> img): a function to be used for :meth:`apply_image`\n            name (str, optional): the name of this transform\n        """"""\n        self._func = func\n        self._name = name\n\n    def apply_image(self, img):\n        return self._func(img)\n\n    def apply_coords(self, coords):\n        return coords\n\n    def __repr__(self):\n        return ""imgaug.PhotometricTransform({})"".format(self._name if self._name else """")\n\n    __str__ = __repr__\n\n\nclass TransformFactory(Transform):\n    """"""\n    Create a :class:`Transform` from user-provided functions.\n    """"""\n    def __init__(self, name=None, **kwargs):\n        """"""\n        Args:\n            name (str, optional): the name of this transform\n            **kwargs: mapping from `\'apply_xxx\'` to implementation of such functions.\n        """"""\n        for k, v in kwargs.items():\n            if k.startswith(\'apply_\'):\n                setattr(self, k, v)\n            else:\n                raise KeyError(""Unknown argument \'{}\' in TransformFactory!"".format(k))\n        self._name = name\n\n    def __str__(self):\n        return ""imgaug.TransformFactory({})"".format(self._name if self._name else """")\n\n    __repr__ = __str__\n\n\n""""""\nSome meta-transforms:\nthey do not perform actual transformation, but delegate to another Transform.\n""""""\n\n\nclass TransformList(BaseTransform):\n    """"""\n    Apply a list of transforms sequentially.\n    """"""\n    def __init__(self, tfms):\n        """"""\n        Args:\n            tfms (list[Transform]):\n        """"""\n        for t in tfms:\n            assert isinstance(t, BaseTransform), t\n        self.tfms = tfms\n\n    def _apply(self, x, meth):\n        for t in self.tfms:\n            x = getattr(t, meth)(x)\n        return x\n\n    def __getattr__(self, name):\n        if name.startswith(""apply_""):\n            return lambda x: self._apply(x, name)\n        raise AttributeError(""TransformList object has no attribute {}"".format(name))\n\n    def __str__(self):\n        repr_each_tfm = "",\\n"".join([""  "" + repr(x) for x in self.tfms])\n        return ""imgaug.TransformList([\\n{}])"".format(repr_each_tfm)\n\n    def __add__(self, other):\n        other = other.tfms if isinstance(other, TransformList) else [other]\n        return TransformList(self.tfms + other)\n\n    def __iadd__(self, other):\n        other = other.tfms if isinstance(other, TransformList) else [other]\n        self.tfms.extend(other)\n        return self\n\n    def __radd__(self, other):\n        other = other.tfms if isinstance(other, TransformList) else [other]\n        return TransformList(other + self.tfms)\n\n    __repr__ = __str__\n\n\nclass LazyTransform(BaseTransform):\n    """"""\n    A transform that\'s instantiated at the first call to `apply_image`.\n    """"""\n    def __init__(self, get_transform):\n        """"""\n        Args:\n            get_transform (img -> Transform): a function which will be used to instantiate a Transform.\n        """"""\n        self.get_transform = get_transform\n        self._transform = None\n\n    def apply_image(self, img):\n        if not self._transform:\n            self._transform = self.get_transform(img)\n        return self._transform.apply_image(img)\n\n    def _apply(self, x, meth):\n        assert self._transform is not None, \\\n            ""LazyTransform.{} can only be called after the transform has been applied on an image!""\n        return getattr(self._transform, meth)(x)\n\n    def __getattr__(self, name):\n        if name.startswith(""apply_""):\n            return lambda x: self._apply(x, name)\n        raise AttributeError(""TransformList object has no attribute {}"".format(name))\n\n    def __repr__(self):\n        if self._transform is None:\n            return ""LazyTransform(get_transform={})"".format(str(self.get_transform))\n        else:\n            return repr(self._transform)\n\n    __str__ = __repr__\n\n    def apply_coords(self, coords):\n        return self._apply(coords, ""apply_coords"")\n\n\nif __name__ == \'__main__\':\n    shape = (100, 100)\n    center = (10, 70)\n    mat = cv2.getRotationMatrix2D(center, 20, 1)\n    trans = WarpAffineTransform(mat, (130, 130))\n\n    def draw_points(img, pts):\n        for p in pts:\n            try:\n                img[int(p[1]), int(p[0])] = 0\n            except IndexError:\n                pass\n\n    image = cv2.imread(\'cat.jpg\')\n    image = cv2.resize(image, shape)\n    orig_image = image.copy()\n    coords = np.random.randint(100, size=(20, 2))\n\n    draw_points(orig_image, coords)\n    print(coords)\n\n    for _ in range(1):\n        coords = trans.apply_coords(coords)\n        image = trans.apply_image(image)\n    print(coords)\n    draw_points(image, coords)\n\n    # viz = cv2.resize(viz, (1200, 600))\n    orig_image = cv2.resize(orig_image, (600, 600))\n    image = cv2.resize(image, (600, 600))\n    viz = np.concatenate((orig_image, image), axis=1)\n    cv2.imshow(""mat"", viz)\n    cv2.waitKey()\n'"
